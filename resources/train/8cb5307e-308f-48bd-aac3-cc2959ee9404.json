[{"section_title": "Abstract", "text": "Medical predictive modeling is a challenging problem due to the heterogeneous nature of the patients. In order to build effective medical predictive models we need to address such heterogeneous nature during modeling and allow patients to have their own personalized models instead of using a one-size-fits-all model. However, building a personalized model for each patient is computationally expensive and the over-parametrization of the model makes it susceptible to the model overfitting problem. To address these challenges, we propose a novel approach called FactORized MUlti-task LeArning model (Formula), which learns the personalized model of each patient via a sparse multi-task learning method. The personalized models are assumed to share a low-rank representation, known as the base models. Formula is designed to simultaneously learn the base models as well as the personalized model of each patient, where the latter is a linear combination of the base models. We have performed extensive experiments to evaluate the proposed approach on a real medical data set. The proposed approach delivered superior predictive performance while the personalized models offered many useful medical insights."}, {"section_title": "Introduction", "text": "Predictive modeling has become an integral component of many industries to deliver accurate predictions for various purposes, such as decision making and risk management. With the growing development and availability of electronic medical records (EMR), the practitioners in many clinical decision support and care management systems resort to leveraging patients' medical records to perform various predictive modeling tasks for risk predictions and disease analysis. Moreover, in many clinical and pharmaceutical researches, predictive models such as disease progression models are used to study the pathologies of disease and evaluate the effectiveness of treatments, given the historical observations * Computer Science and Engineering Department, Michigan State University \u2020 Samsung Research America, San Jose, CA and medical records [4] . In the study of Alzheimer's disease (AD), for example, various predictive models are designed to study the courses of the disease and its progression patterns, to identity sensitive biomarkers that signal progression of the disease, and to build accurate models that identify high risk patients [25, 26] . Compared to standard data mining and machine learning applications, medical predictive modeling is especially challenging due to the heterogeneous nature of the patients. The heterogeneity arises from multiple factors: first of all, although some patients have similar phenotypes according to their health records, their medical conditions may vary. For example, in the study of dementia, patients with similar cognitive impairments may have different pathological causes. Another example is the study of heart failure (HF), where HF may be caused by coronary artery disease, hypertension, impaired glucose tolerance, and other factors [12, 16] . Secondly, it is well acknowledged that patients with the same disease may progress differently [17] . As such, one should address the heterogeneity of the patients in order to build accurate medical predictive models. It is widely accepted that building personalized models [15] is key to solving the problem, taking the inherent variability of the patients into account.\nOne simple way to implement the personalized models is to build a separate model for each patient independently. However, there are several drawbacks of this 'fully personalized' approach: First, it is not efficient in terms of time and space complexity. The task of building the personalized models is expensive and storing them is infeasible when the number of patients is large. More importantly, this approach requires solving a predictive modeling problem with a huge number of parameters. Because we have only limited amount of training data, such models are likely to severely overfit the data and result in models with poor generalization performance.\nInstead of building a different model for each patient, an alternative approach is to consider the similarity of the patients. Specifically, a two-stage modeling is performed-grouping the patients first based on their similarities and then building a separate model for patients in each group independently. This includes methods such as locally weighted learning [2] and localized support vector machine (LSVM) [9] . Locally weighted learning is a lazy learning scheme, in which the learning procedure only starts when the testing is performed. This approach would find the neighbors of the test instance, forming a group centered at the instance. It then builds a predictive model based on the training instances in the group. LSVM [9] is another approach, where supervised clustering is initially performed to group the training instances. It then trains a local SVM model for each cluster independently. One potential limitation of the two-stage approach is that the training of a model for patients within each group does not utilize potentially valuable information about patients from other groups since the grouping and model building steps are carried out separately. In addition, the approach is not exactly personalized since all the patients that belong to the same group have the same predictive model.\nTo address these limitations, this paper introduces a novel approach called FactORized MUlti-task LeArning model (Formula). Formula learns a personalized model for each patient in a tractable way by assuming the models share a low-rank representation, known as its 'base models'. The personalized model for each patient is a linear combination of a few of these base models. The base models can be regarded as features characterizing the underlying groups of the data while the coefficients of these base models denote memberships of the patients in these groups. To ensure the robustness of Formula, we enforce sparsity in both the 'base models' as well as the combination coefficients. As a result, each base model involves only a few relevant features, while each personalized model is a linear combination of only a few base models. Formula also enforces a graph Laplacian regularization to ensure that the personalized models for similar patients should be close to each other.\nIn short, the main contributions of this paper are summarized below:\n\u2022 We proposed a novel personalized medical model called Formula. Instead of building a single model for all the patients or applying a two-stage modeling, Formula extracts the base models of the patients and uses a linear combination of these models as the personalized model of a patient.\n\u2022 We employed a sparse matrix factorization formulation to perform base model selection for each patient and feature selection for each base model.\n\u2022 We designed an efficient optimization method to solve this non-convex problem.\n\u2022 We evaluated Formula on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The experimental results show the superiority of Formula over other baseline methods.\nThe remainder of this paper is organized as follows. A brief review of the related works is given in Section 2. In Section 3.1, we formalize the problem of learning personalized models. The proposed Formula approach is introduced in Section 3.2. We solve its corresponding optimization problem in Section 3.3. Section 4 evaluated the performance of Formula on a real world dataset. Finally, we conclude the paper in Section 5."}, {"section_title": "Related Works", "text": "As mentioned in Section 1, in order to avoid learning one model for each patient (data point), we might consider either locally weighted learning, or two-stage learning methods, such as clustering plus multi-task learning. In this section, we are going to review locally weighted learning and multi-task learning."}, {"section_title": "Locally Weighted Learning", "text": "Locally weighted learning is categorized as lazy learning method [2] , in which the model is learned only when the testing data point comes. Locally weighted learning has been imbedded into various kinds of fundamental approaches, such as locally weighted regression [2] , localized SVM [9] , etc. The drawback of locally weighted learning is that it needs to build one model for each testing data points [22] . To address this drawback, localized SVM proposed an efficient learning method by first clustering the training data points into different groups and then building an SVM for each group. However, localized SVM need to take into consideration all the testing data points in advance to do the clustering over the training data samples, but usually the testing data set might not be available in advance. Also it does not address the problem when a new testing point becomes available, whether the method will retrain all the models using all available testing points, or just use the already generated models for each group. As mentioned earlier, localized SVM is a two-stage model and it builds models independently between groups. Formula is different from locally weighted learning in that it benefits from learning the group and the models simultaneously, and also considers the relations between groups implicitly."}, {"section_title": "Multi-Task Learning (MTL)", "text": "Multi-task learning [6] is a methodology designed to improve predictive performance that learns different tasks simultaneously by taking into consideration the relations between tasks. The key difference between various MTLs lies in the way how they define the task relations. For example, the task relationships can be modeled using a common prior within a hierarchical Bayesian framework [3, 21] , or using different kinds of regularization techniques, such as Mean-regularized MTL [11] , low-rank regularized MTL [8] , MTL with joint feature learning [28, 23] , etc. Multi-task learning has also been used for feature learning and selection [1] or temporal learning [28, 20] by considering each time point as one task. An open-source multi-task learning software package MALSAR [24] has been developed to include efficient solvers for many state-of-the-art multi-task learning algorithms. As the second stage in the two-stage modeling scheme, multi-task learning can explicitly consider the relations between different groups/tasks. Although they can utilize the relations between tasks, the relations are mostly predefined. If the predefined task relations do not reflect the true underline relation, the performance will be degenerated. In this paper, we can address this problem by incorporating task relations implicitly, and identify the tasks and learn the task models simultaneously."}, {"section_title": "Learning Personalized Model via FORMULA", "text": "In this section, we formally introduce the problem of learning personalized models. We then discuss the technical challenges of the problem, which motivate the proposed Formula approach. Finally, we discuss how the problem can be efficiently solved."}, {"section_title": "Problem Formulation", "text": "In a typical predictive modeling setting, we are given a feature vector and a target variable for each data point. Our goal is to learn a model that predicts the value of the target variable given its feature vector. In the context of medical predictive modeling, the features can be extracted from various sources, including historical medical records or medical images. The target variable can be binary-valued, such as the onset of a certain disease, or continuous-valued, such as the cognitive score of a patient.\nLet\n} denote a collection of N training samples, where each sample is characterized by a D-dimensional feature vector, x i \u2208 R D , and a target response 1 y i \u2208 \u211c. We assume that the target can be approximated by a linear combination of the features, i.e., y i = w T i x i + \u03f5 i , where w i is the parameter vector associated with the i-th training sample and \u03f5 i is the Gaussian noise term. Since we are seeking for personalized models, the parameters w i are unique for each sample and are estimated by solving the following 1 In this paper, we focus on the regression problem.\noptimization problem: Since there are D \u00d7 N parameters that must be estimated from the N training samples, this leads to an underdetermined system of linear equations, which has either no solution or infinitely many solutions. The overparameterization of the model also makes it susceptible to model overfitting. To overcome these problems, the number of effective parameters must be significantly reduced. One way to achieve this is by identifying groups of similar samples and then build a separate model for each group. Let G = {\u03c0 1 , \u03c0 2 , \u00b7 \u00b7 \u00b7 , \u03c0 K } denote the set of K groups, where \u03c0 j denote the set of samples assigned to the j-th group. The problem of learning personalized models for each group can be formalized as follows:\nThe optimization problem can be solved using a twostage approach, where the group membership information is initially obtained by applying clustering techniques such as k-means. Once the clusters are found, a personalized model is derived for each cluster by solving the inner summation term of the objective function given in (3.2). However, since the clustering is performed independently of the predictive modeling step, this may lead to suboptimal performance as the construction of the model for each group does not utilize information from other groups. The multi-task learning approach to be described in the next section is designed to overcome this problem by solving the clustering and predictive modeling steps jointly in a unified learning framework, thus supporting knowledge transfer among the clusters. In addition, to improve robustness of the predictions, additional sparsity constraints were imposed to further reduce the number of effective parameters that the models depend upon."}, {"section_title": "The Proposed FORMULA Framework", "text": "This section presents the proposed Formula approach, which considers the development of personalized model for each patient as a single learning task. Unlike the two-stage approach given in Equation (3.2), Formula assumes the learning tasks are related. It therefore simultaneously learns the related tasks and utilize the shared information among tasks to improve its overall predictions. We achieve these goals by incorporating regularization terms into the personalized model formulation given in (3.1):\nwhere L(X, y;\nis the loss function and R(W ) is the regularization term, which encodes our modeling assumptions. To start with, we consider the following modeling assumptions of our formulation:\n\u2022 Model Clustering. One of the key assumptions behind our proposed approach is that the predictions of the target variables are governed by a set of K base models, which are collectively represented by the matrix\n, where each base model is represented by a column vector u i \u2208 R D . We further assume that each personalized model w i is represented by a linear combinations of the base models, i.e.,\nK is a vector denoting the coefficients of the linear combination, and\n. This assumption can be enforced by requiring the model matrix W to be as close as possible to the product of two matrices, i.e., W = U V .\n\u2022 Sparse Personalized Models. Depending on the nature of the data, the number of base models can be potentially large. However, the personalized model of each individual patient is assumed to be a linear combination of only a few base models. In other words, the number of non-zero elements in V should be as few as possible. This can be achieved by enforcing a sparse-inducing norm on the matrix V . In addition, to ensure interpretability of the cluster assignment, the elements in V should be non-negative.\n\u2022 Sparse Base Models. Each base model should be characterized by only a few relevant features, to ensure the model is robust to noise. A sparseinducing norm can be applied to U to obtain the sparse base models.\n\u2022 Local Smoothness and Recovery. Although each patient has its own personalized model, we assume the models for patients with similar phenotypes should be close to one another. Such a model smoothness criterion is helpful to infer the personalized model of a test patient by assuming it is similar to the weighted average of the personalized models for its neighbors. This can be achieved by incorporating a graph Laplacian regularization term into the proposed formulation.\nBased on the preceding assumptions, the objective function of Formula is given by:\nwhere V \u227d 0 denote all elements in V must be non negative and L \u2208 R N \u00d7N is the similarity matrix between the training instances. The parameters \u03bb 1 , \u03bb 2 , and \u03bb 3 control the tradeoffs among the various terms of the objective function. The last term in the objective function, \u2225W \u2212 W L\u2225 2 F , enforces the local smoothness constraint on the w i s. Note that L must be normalized such that the sum of each row or column is equal to 1. The number of base models K is assumed to be predefined by the user."}, {"section_title": "Optimization", "text": "This section describes how to solve the optimization problem for our proposed framework. In this work, we consider a squared loss function for regression problems, i.e., \u2113 i (x i , y i ;\n2 . However, the optimization strategies used in this paper can also be applied to other loss functions. The objective function for Formula with squared loss is given by:\nWe can simplify the problem by replacing W with the matrix product U V in the objective function, i.e., w i = U v i . This reduces the objective function to the following expression:\nThus, we only need to solve for U and V , and do not need to store the D \u00d7 N matrix W . Similar to [7, 27] , we propose to use the Block Coordinate Descent (BCD) algorithm to obtain a locally optimal solution. Specifically, we iteratively solve for U and V by fixing one of them to be constant, until the algorithm converges. Below we explain how each step can be solved efficiently.\nSolve U , given V . The objective function becomes\n. This is an \u2113 1 -regularized convex optimization problem, which can be efficiently solved using projected gradient methods, such as spectral projected gradient [19] , by considering the gradient of the smooth part of the objective function. Here, the gradient of the smooth part w.r.t. U is given by,\nThe problem can be solved in a similar way. The gradient of the smooth part of the objective function w.r.t. V is given by,\nwhere"}, {"section_title": "and v i is the i-th column of V .", "text": ""}, {"section_title": "Experimental Evaluation and Results", "text": "We have performed extensive experiments to evaluate the performance of Formula."}, {"section_title": "Dataset", "text": "Our experiments were performed on the ADNI dataset 2 , which contains images from MRI scans (M) and PET scans (P), as well as CSF measurements (C) and cognition-related clinical measurements such as Mini Mental State Examination (MMSE) scores and Alzheimer's Disease Assessment Scale-cognitive subscores (ADAS-Cog). ADNI is a longitudinal project, in which the measurements are collected repeatedly over a 6-month or 1-year interval. We call the time point when the patient came to the hospital for screening as baseline. The time point when the patient came to the hospital for evaluation is determined based on the elapsed time since the initial baseline. For example, M06 denote the time point 6 months after the first visit. There are altogether 5 time points, designated as M06, M12, M24, M36 and M48, respectively. We consider the samples collected for each time point as a separate data set. The sample sizes for the five data sets are shown in Table 1. Note that the data sets decrease in size due to the drop out of some patients for various reasons.\n2 Available at http://adni.loni.ucla.edu The features of each data set include those from M, P, C and META (E), which denote additional features other than M, P and C. The detailed list of the META features is given in [28] . We consider using these features to build models for predicting the ADAS cognitive scores or MMSE scores on each data set."}, {"section_title": "Baseline Algorithms", "text": "We compared the performance of Formula against the following baseline methods.\n\u2022 Single model(SM): This is a one-size-fits-all approach, assuming there is no inherent groupings in the data. We applied ridge regression to construct a single model for each data set.\n\u2022 Clustering + single task model with Ridge regression(CSTR): In this baseline algorithm, we first apply k-means clustering to generate k clusters. We then build a ridge regression model for each cluster.\n\u2022 Clustering + single task model with Lasso regression(CSTL): This approach is similar to CSTR except we use Lasso regression to build the model instead of ridge regression.\n\u2022 Clustering + sparse low rank mutli-task learning (CSL) [8] : First we cluster the data using k-means to generate k clusters, and then treat each cluster as a task to learn a multi-task model. Here we assume that all models share a low-rank representation in addition to a sparse property. The objective function for CSL is given by [8] ,\nWe use the implementation in MALSAR [24] to solve CSL.\n\u2022 Clustering + mean regularized multi-task learning (CMR) [11] : In this baseline algorithm, we first cluster the data using k-means into k clusters which are considered as k tasks. We consider a special task relation for the multi-task learning. The assumption is that the models for all tasks are close to their mean model. The objective function is given below.\nWe use the implementation in MALSAR [24] to solve CMR."}, {"section_title": "Evaluation Method", "text": "Each data set is partitioned into a training set, which contains 80% of the data points, and a test set, which contains the rest of the data. For SM, because there is only one model for the entire data set, we can apply the derived model directly on the test set for evaluation. For the two-stage methods (CSTR, CSTL, CSL and CMR), when the test data point becomes available, we first find the nearest cluster of each test point and apply its corresponding model to make the prediction. As previously mentioned, in Formula, the personalized model W are estimated for the training data only. For testing, we use the weighted average model of the nearest neighbors of each test point. Formally, the personalized model for the test point x i is:\nwhere T is the number of nearest neighbors of x i . We set T = 5 in our experiments. s i,j is the similarity between x i and x j . Here, s i,j is calculated using the Gaussian radial basis function. The model performance is evaluated based on its mean of squared error (MSE)."}, {"section_title": "Experimental Results", "text": "The experimental results are summarized in Table 2 . The results suggest that Formula outperforms other baselines in 6 out of 10 data sets and is consistently one of the top two algorithms for all the data sets. In the 6 data sets in which Formula has the lowest MSE, its average improvement over the second best performer is more than 6%. In ADAS-Cog M36 and MMSE M12, the performance of Formula is almost the same as the top performers. Comparing Formula against CSL and CMR, even though all three methods are multi-task learning algorithms, Formula outperforms the other two consistently on all the data sets. The reason is that by learning the clusters and models simultaneously, Formula can utilize more information in the learning process. Comparing CSTR/CSTL against CSL/CMR, observe that their performances are quite similar. However, there are several cases where CSTL outperforms CSL/CMR, which might be due to the inaccurate assumption of the task relations in the multi-task learning formulation.\nIn addition, it is worth noting that the performance for all the methods degrade from M06 to M48. This is because the sample size decreases over time, which makes the methods more susceptible to underfitting their models with inadequate data points."}, {"section_title": "Sensitivity Analysis", "text": "Since there are four parameters that must be tuned, namely, \u03bb 1 , \u03bb 2 , \u03bb 3 , and K, we need to analyze the sensitivity of the models to changes in the parameter values. By varying the value of one parameter and keeping the other three parameters constant, the sensitivity analysis results for \u03bb 1 , \u03bb 2 , \u03bb 3 , and K are shown in Figure 1 . From Figure 1 , we can see that the model is quite robust to changes in these parameter values. Nonetheless, in general, \u03bb 1 prefers larger values (see Figure 1a) whereas \u03bb 2 prefers smaller ones (see Figure 1b) . The model is also not that sensitive to changes in \u03bb 3 and K (Figure 1c, 1d ) within the range of parameter values investigated."}, {"section_title": "Model Analysis", "text": "One of the most attractive feature of the proposed model is that it learns a set of base models from the training data, where each base model corresponds to a column of the matrix U . The personalized models can be represented by a linear combination of these base model. In this section, we first investigate the base models obtained from the best solution in the last section and then analyze how each of the base models contribute to the personalized models. Base Models. In both ADAS-Cog and MMSE datasets, the rank of the best models obtained is 3. We sort the features in each base model and rank them according to their contribution. The top features for each base model in ADAS-Cog and MMSE are shown in Table 3 and  Table 4 , respectively. We observe that the top features in the base models look very different from each other. Due to the progression of Alzheimer's disease, it will eventually affect almost all parts of the brain. One possible explanation of these heterogeneous models is that, the patients may be at different stages of the disease, and at different stages, the contribution from the base model differs.\nIn the Base Model A of the ADAS-Cog task (Table 3), the leading feature is the cortical thickness average of the right lateral Occipital. The relationship between occipital and AD was studied by previous works [18, 5] , and found to be significant in the advanced AD patients. In both Base Model B and Base Model C, the leading feature is the cortical thickness average of the left middle temporal gyrus, which is an important area in the Temporal lobe, connected to multiple cognitive functions, such as accessing word meaning while reading. The area is found to be the first temporal lobe neocortical sites affected in AD [10] . As such, these two models may relate to early predictive patterns of AD. We also find that in Base Model B, the effects of Left Entorhinal is higher than White matter volumn of the left Hippocampus, while in the Base Model C the pattern is reversed. The two models may indicate different progression patterns in early stage AD patients.\nIn the base models obtained in MMSE task (Table 4), we find that the leading features have different patterns. In Base Model A, the leading feature is the average cortical thickness of the inferior parietal. The area of inferior parietal is related to the progression of AD in several studies. In [14] , the authors find its metabolism decreased early in the course of AD. The leading feature in Base Model B is the cortical parcellation volume of right precentral. The precentral gyrus is found to be significant in voxel analysis [13] . In Base Model C, the volume of left hippocampus dominates other features. This feature is considered as one of the most significant biomarkers of AD. Contribution to Personalized Models. As we have learned heterogeneous base models for both tasks, it is interesting to see how each of the models contribute to the personalized models. Since both models are at rank 3, we are able to plot the coefficients in V using a 3-dimensional coordinator system. For each point, the value at an axis means how much the corresponding base model contributes to its personalized model. The scatter plots of the contributions are given in Figure 2 . We are able to find very interesting patterns in these plots.\nIn Figure 2 .a, the size of the marker is propotion- model, patients with two base models, and those with three base models). This is probably because that due to the \u2113 1 thrinkage effects, small contributions turn to zeros. We are able to see much more patients with linear combinations of base model B and C (112 patients in total), as compared to other two groups (39 patients for A and B, 36 patients for A and C). And we also notice the personalize models for most patients (280 patients in total) are linear combination of three base model. The results for MMSE M06 task is given in Figure 2 .b, in which the size of the marker is propotional to the value of the patient's MMSE score. Cognitive normal patients usually have higher MMSE scores, which means smaller markers indicates patients affected more by Alzheimer's. We are able to see that the personalized models for the largest population are linear combination of the three models (230 patients). Only a few patients lie on the axises (16 patients for base model A, 32 for B, and 27 for C) and those patients usually have smaller MMSE values as compared to the rest of population. Considering the difference between ADASCog and MMSE, i.e., healthy patients typically have low ADAS-Cog score and high MMSE score, the finding in Figure 2 .a and Figure 2 .b are consistent: the models for patients with advanced Alzheimer's are more likely to be singleton (and heterogeneous). Also, the findings in this paper are consistent with our assumption that predictive models of the patients are not homogeneous, and for different set of patients, the models should be different. There are many other interesting findings we have, for example, the patterns of different latent modality over the course of progression of Alzheimer's. We leave a complete analysis to future publication."}, {"section_title": "Conclusion", "text": "Personalized modeling for medical use is one of the emerging research topics in machine learning and data mining area, and there are many challenges associated with it. To address these challenges, we propose a novel FactORized MUlti-task LeArning model (Formula) to learn low-rank personalized models, leveraging the shared information among patients. Specifically, the proposed approach learns a personalized model for each patient, assuming the models share a low-rank representation. The personalized models are computed as linear combinations of a few base models. Our experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) data set suggest that the proposed approach is superior than several baseline methods and provide many valuable medical insights."}]