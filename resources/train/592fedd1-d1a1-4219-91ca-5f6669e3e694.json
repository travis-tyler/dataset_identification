[{"section_title": "Abstract", "text": "When analyzing data sampled with unequal inclusion probabilities, correlations between the probability of selection and the sampled data can induce bias if the inclusion probabilities are ignored in the analysis. Weights equal to the inverse of the probability of inclusion are commonly used to correct possible bias. When weights are uncorrelated with the descriptive or model estimators of interest, highly disproportional sample designs resulting in large weights can introduce unnecessary variability, leading to an overall larger mean square error compared to unweighted methods.\nWe describe an approach we term 'weight smoothing' that models the interactions between the weights and the estimators as random effects, reducing the root mean square error (RMSE) by shrinking interactions toward zero when such shrinkage is allowed by the data. This article adapts a flexible Laplace prior distribution for the hierarchical Bayesian model to gain a more robust bias-variance tradeoff than previous approaches using normal priors. Simulation and application suggest that under a linear model setting, weight-smoothing models with Laplace priors yield robust results when weighting is necessary, and provide considerable reduction in RMSE otherwise. In logistic regression models, estimates using weight-smoothing models with Laplace priors are robust, but with less gain in efficiency than in linear regression settings."}, {"section_title": "Introduction", "text": "Studies based on data sampled with unequal inclusion probabilities typically apply case weights equal to the inverse of the probability of inclusion to reduce or remove bias in estimators of descriptive population quantities, such as means or totals (Horvitz and Thompson 1952) . This \"fully weighted\" approach can be extended to estimate analytical quantities that focus on association between risk factors and outcomes, such as population slopes in linear and generalized linear models, by applying sampling weights to score equations, and solving for the resulting \"pseudo-maximum likelihood\" estimators (PMLEs) (Binder 1983; Pfeffermann 1993) . Unweighted and weighted estimators generally correspond when the underlying model (either implicit or explicit) is correctly specified and the sampling scheme is noninformative. When the model is misspecified or the sampling scheme is informative, weighted estimators typically reduce bias, often (although not always) at the cost of increased variance. As model assumptions improve and/or sampling better approximates noninformativeness, the increase in variance from weighted analysis could overwhelm the reduction in bias, leading to an overall larger mean square error (MSE) than would be the case if the weights were ignored or at least controlled in some fashion.\nIn many, if not most cases, fully weighted estimators are used without concerns about such tradeoffs. When variability in weights is of concern, weight trimming, or \"winsorization,\" is used to control the variation in weights by capping the weights at some value w 0 , and redistributing the values above w 0 among the rest (Alexander et al. 1997; Kish 1992; Potter 1990 ). Various criteria have been used to determine the cap value based on data. Some examples include the National Assessment of Education Progress (NAEP) method by Potter (1988) , which set the cutoff point equal to ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi c P i[s w 2 i =n p , where c was chosen in an ad-hoc manner. Cox and McGrath (1981) q , where u\u02c6w is the fully weighted estimator, and u\u02c6t, t \u00bc 1, : : : , T, is the weight-trimmed estimator, with t denoting various trimming levels ordered from lowest to highest. These levels are ad hoc, except for w 0 \u00bc w for t \u00bc 1, which fixes all weights to their mean value and yields the unweighted estimator, and w 0 \u00bc max w i for t \u00bc T, which yields the fully weighted (untrimmed) estimator. Chowdhury et al. (2007) suggested treating the weights as coming from a skewed cumulative distribution (e.g., an exponential distribution), and using the upper one percent of the fitted distribution as a cut point for weight trimming. Beaumont (2008) proposed a generalized design-based method, replacing the actual weights with weights predicted using a function of response and design variables. Details of these design-based approaches are summarized in Henry and Valliant's (2012) review.\nAn alternative to standard design-based weighted estimation is a model-based approach that accommodates disproportional probability-of-selection design in a finite population Bayesian inference setting. By creating dummy variables stratified by equal or approximately equal case weights, a fully weighted data analysis is obtained by building a model with indicators for the weight strata together with interaction terms between the weight stratum indicators and model parameters of interest, then obtaining inference about the population quantity of interest from its posterior predictive distribution. Elliott and Little (2000) established two model-based approaches for weight-trimming: model averaging, or \"weight pooling\", and hierarchical modeling, or \"weight smoothing\". A weight pooling model collapses strata with similar weights together with their associated interaction terms, mimicking a data-driven weight-trimming process. Weight smoothing treats the underlying weight strata as random effects, and achieves a balance between fully weighted and unweighted estimates using a shrinkage estimator: thus the weight strata are smoothed if data provide little evidence of difference between strata, and are separated if data suggest that interactions with strata are present. Under a Bayesian framework, a twolevel model is implemented, assigning a multivariate normal prior for the random effects, with inference obtained from the posterior predictive distribution of the population parameter of interest. Elliott (2007) extended the application of weight-smoothing models to linear and generalized linear models, and discussed different structures for the randomeffect priors, namely exchangeable, autoregressive, linear and nonparametric random slopes. Both of these papers found that there was an efficient/robustness tradeoff with respect to structure in the prior mean and variance, with simple exchangeable models providing highly efficient estimates when the weights provided little bias correction, but being susceptible to \"oversmoothing\" and yielding biased estimators when weights really were necessary to provide substantial bias correction. Highly structured models such as smoothing splines provided robust bias correction, but had less dramatic gains in efficiency and were more complex to implement. Hence we are motivated to find alternative models that will induce weight smoothing under simple mean and covariance matrix settings, while improving the bias-variance tradeoff. A logical choice would be a Laplace prior, which can be viewed as providing a Bayesian version of a LASSO regression model (Park and Casella 2008) . This heavier-tailed prior might be expected to provide little or no shrinkage when bias correction is required, but still allow approximation to an unweighted estimator when the data suggest weak relationships with probability of selection.\nIn this article we extend the weight-smoothing approach by use of Laplace priors for the random-effect weight strata and interaction terms instead of multivariate normal priors, in order to achieve more robustness against \"oversmoothing\" in settings where weights are required to accommodate model misspecification or nonignorable sampling. We evaluate the performance of our proposed model in a simulation study, under both model misspecification and informative sampling, for both continuous and dichotomous outcomes, and compare it with competing methods. The article is organized as follows. In Section 2 we review the theory of model smoothing together with recently proposed model-assisted methods, and develop our model with Laplace priors. Section 3 provides simulation studies, and compares bias, coverage and MSE of the proposed method with competing methods. Section 4 demonstrates the method's performance for both linear and logistic scenarios by applications to dioxin data from the National Health and Nutrition Examination Survey (NHANES) and Partners for Child Passenger Safety dataset. Section 5 provides a summary discussion."}, {"section_title": "Weight-Smoothing Methodology", "text": ""}, {"section_title": "Finite Bayesian Population Inference", "text": "For finite Bayesian population inference, we model the population data Y: Y , f (Y j u,Z ), where Z are the variables associated with the sample design (probabilities of selection, cluster indicators, stratum variables). Note that the parametric model f can either be highly parametric with a low dimension u (e.g., a normal model with common mean and variance), or have a more semiparametric or nonparametric flavor with a high-dimension u (such as a spline or Dirichlet process model). Inference about some population quantity of interest Q(Y) is based on the posterior predictive distribution of p\u00f0Y nob jY obs ;I;Z\u00de\u00bc \u00d0 \u00d0 p\u00f0IjY;Z;u;f\u00dep\u00f0Y nob jY obs ;Z;u;f\u00dep\u00f0Y obs jZ;u\u00dep\u00f0u;f\u00dedudf \u00d0 \u00d0 \u00d0 p\u00f0IjY;Z;u;f\u00dep\u00f0Y nob jY obs ;Z;u;f\u00dep\u00f0Y obs jZ;u\u00dep\u00f0u;f\u00dedudfdY nobs \u00f01\u00de\nwhere Y nob consists of the N 2 n unobserved cases in the population, u models Y (possibly conditional on Z) and f models the inclusion indicator I (equal to 1 if the unit is sampled and observed and 0 otherwise). Thus p(I j Y, Z, u, f) refers to the distribution of the sample mechanism given the design variables and the data of interest, p(Y nob j Y obs , Z, u, f) gives the distribution of the unobserved elements in the sample given the observed elements in the sample and the (fully observed) design variables, and p(Y obs j Z, u) models the observed data given any design variables. Assuming that f and u have independent priors, the sampling mechanism is said to be \"noninformative\" if the distribution of I is independent of Y Z j , or \"ignorable\" if the distribution of I only depends on Y obs Z j . When the sampling design is ignorable, p(I j Y, Z, u, f) \u00bc p(I j Y obs , Z, f), and thus (1) allowing inference about Q(Y) to be made without explicitly modeling the sampling inclusion parameter I (Ericson 1969; Holt and Smith 1979; Little 1993; Rubin 1987; Skinner et al. 1989) . Note that if inference about quantities Q\u00f0Y X j \u00de involving covariates X is desired (e.g., regression slopes), noninformative or ignorable sample designs can be relaxed to have the distribution of I depend on X."}, {"section_title": "Weight Modeling", "text": "Beaumont (2008) \ni e i , and the exponential form,\n, where H i and v i . 0 are known functions of y i . (The exponential form prevents the predicted weights from being negative.) He presented two examples of H T i b, one-degree polynomial and five-degree polynomial of y i . The predicted weights are obtained by fitting the (unweighted) model on the sampled data, then the reweighted estimator of the survey-response variable of interest is obtained using the predicted weights. Extensions to regression settings can consider models of the form\ni e i , where J i and u i are functions of Y i and X i (possibly including interactions)."}, {"section_title": "Weight Smoothing", "text": "In general, weight smoothing stratifies the data by inclusion probability, and applies a hierarchical model treating strata means as random effects, thus achieving trimming via shrinkage. Considering the population mean as the quantity of interest, a general weightsmoothing model is as follows:\nwhere m \u00bc (m 1 , : : : , m H ), f \u00bc (f 1 , : : : , f H ), and h \u00bc 1, : : : , H indexes different \"weight strata\" defined, for example, by same or similar inclusion probabilities. In the case of stratified or poststratified sample designs, h indexes the actual strata. In more general designs, subjects can be formed into strata with equal or similar weights. We assume f, G, and s 2 all have weak or noninformative priors. Typically these strata are ordered from the smallest weight (highest probability of selection) to the largest weight (lowest probability of selection), but this is not required if a more natural ordering is available, for example, if the weight strata represent a disproportionately stratified sample by age. Based on this model, the posterior mean of the population mean is derived as:\nwhere N h and n h are the population and sample sizes in stratum h, respectively, and m h \u00bc E(m h jy). Various assumptions can be made for the prior distribution of m, such as\nSee Elliott and Little (2000) for a detailed review. The weight-smoothing mechanism can be easily intuited in the simplest case of the exchangeable random-effect (XRE) model (Holt and Smith 1979; Ghosh and Meeden 1986; Little 1991; Lazzaroni and Little 1998) , where f h \u00bc m for all h, and G \u00bc t \nh =N\u00de y h , the fully weighted estimator. On the other hand, as t 2 ! 0, w h ! 0, and the estimation shrinks toward the\nand is a measure of the information available to distinguish how the population means within a weight strata differ, the weight-smoothing model achieves a \"data-driven\" compromise between the weighted estimator, which is design consistent but may be highly inefficient, and the unweighted estimator, which is fully efficient when the assumption of independence between inclusion probability and mean of Y holds, but is likely biased otherwise."}, {"section_title": "Weight Smoothing for Linear and Generalized Linear Regression Models", "text": "Generalized linear regression models (McCullagh and Nelder 1989) \nNotice that the quantity B that satisfies U(B) \u00bc 0 is always a meaningful population quantity even if the model is misspecified, since it is a linear approximation of x i to h i . A first-order approximation of E(Bjy, X) is given based on B where\n, and E(Bjy, X) is obtained by solving the weighted score equation for population regression parameter B"}, {"section_title": "Laplace Prior for Weight Smoothing", "text": "Instead of using a multivariate normal distribution as the prior of bs, we propose using a multivariate Laplace distribution. Unlike the normal distribution prior which restricts the variation between random-effect term and prior mean in an L 2 -norm manner, Laplace measures by the L 1 distance. This should allow more severe downweighting of interactions between the regression parameters and the probability of selection for which there is only weak evidence in the data, while preserving those that we need for bias correction.\nThe general form of multivariate Laplace distribution is given by Eltoft et al. (2006) :\nwhere y is a d-dimensional random variable y \u00bc ( y 1 , : : : , y d ); K m (x) denotes the modified Bessel function of the second kind and order m, evaluated at x; q( y)\nis the vector of means, and l an overall scale parameter. However, this format is inconvenient for application. The alternative approach is to represent the Laplace distribution as a scale mixture of normals with an exponential mixing density: For linear regression, Y conditional on all other parameters follows a normal distribution. Assuming that the residual variance s 2 is independent from the latent mixing variables t i , the hierarchical model is as follows: \nFor logistic regression, the model is similar to that for linear regression, except that Y follows a binomial distribution, and estimation of s 2 is no longer necessary: \nh is the ML estimate of the logistic regression of y on z from strata h, and V b h the associated covariance matrix obtained from the expected information matrix evaluated at b h . The proposed b h is accepted with probability\nAll other parameters follow the Gibbs Sampler algorithm, and are directly drawn from their fully conditional distributions as below (full derivation is available online at www.doi.org/10.1515/jos-2016-0026, Appendix B):\nAt each step in the Gibbs Sampler chain, a draw of the logistic regression population slope B is obtained from the draw of b h by solving the weighted score equation "}, {"section_title": "Simulation Study", "text": "To evaluate the performance of weight-smoothing models using Laplace priors, we create several scenarios for ordinary linear regression and logistic regression, generating separate populations with normally distributed outcomes and dichotomized outcomes accordingly. We also consider scenarios where heteroscedasticity or multiple covariates occur. The target of interest is the population slope in a regression model. In addition to our Laplace prior estimator, we include an unweighted estimator, a fully weighted estimator, a normalprior (exchangable) estimator (Elliott and Little 2000; Elliott 2007) , and several variations of the estimator proposed by Beaumont (2008) for comparison. For each scenario and estimator, we compute bias, square root of mean square error (RMSE) and coverage of 95% confidence or credible intervals as follows:\nwhere s indexes the independent samples drawn for each simulation, B s is the point estimator of the regression coefficient of interest,B L s andB U s correspond to the lower and upper bounds of the 95% confidence or credible interval, and B to the regression coefficient computed using the population data (i.e., the inference target of interest).\nRelative root mean square error (RRMSE) is reported as the ratio of the estimator's RMSE to the fully weighted estimator's RMSE."}, {"section_title": "Hierarchical Weight Smoothing Model for Ordinary Linear Regression", "text": "We first generate a population of size N \u00bc 20,000. The predictor X is uniformly distributed between zero and ten, and is equally divided into 20 strata with intervals of 0.5 each. The response variable Y is generated as a spline function of X noted below, with knots located between strata. Three sets of coefficients of the spline are applied separately to represent the various patterns of Y jX from straight slope to accelerating and decelerating curves. We let the population variance s 2 vary across 10 1.5 , 10 3.5 and 10 5.5 to create varying levels of background noise compared to changes in slope. Figure 1 shows each of the 3 \u00a3 3 \u00bc 9 populations generated for the linear regression. From the population, samples of size n \u00bc 1,000 are repeatedly drawn without replacement, according to inclusion probabilities proportional to p h \u00bc (1 \u00fe h/30) * h for the hth stratum, which results in a ratio of 35 times between the maximum and minimum probabilities. We also ensure that the sample size of each stratum is greater than three for computation convenience. Z is created as Z \u00bc I^X, where I \u00bc c(I 1 , : : : , I h ) is an indicator vector marking that the current observation belongs to the ith stratum. It is also centered within each column with respect to each stratum, and used as predictor in the simulations. Thus b a corresponds to a linear model (no model misspecification); b b to a setting where the nonlinearity is greatest where the probability of selection is the highest, and b c to a setting where the nonlinearity is greatest where the probability of selection is the lowest.\nlinear approximation of Y to X. Under b b and b c , weights correct bias from model misspecification. Under b a , the model is correctly specified, suggesting that the unweighted estimator is most efficient. Also note that under b b , the curvature is largest where the data are most densely sampled, while the reverse is true under b c , suggesting that varying degrees of trimming will be required to optimize the bias-variance tradeoff. For the hyperprior parameters, s 2 0 is arbitrarily defined as 1,000 to approximate a noninformative prior; the prior for l follows a gamma hyperprior with parameter g \u00bc 1 and d \u00bc 1.78, as suggested by Park and Casella (2008) . All other parameters in simulation are initialized at zero, except for the variance estimator s 2 , which is initialized at one.\nA Gibbs Sampler method is applied, that is, all parameters are sequentially drawn from the full conditional distribution for each iteration. Then, to obtain the estimate from the posterior predictive distribution, the unobserved Y are generated based on sampled parameters from each iteration, and the target population slope B is obtained by fully weighted regression on observed and predicted Y. The process iterates 10,000 times, with a burn-in of 2,000. Diagnostic plots are generated to ensure the algorithm's convergence via visual inspection. Overall, 200 samples are generated from each population to provide the empirical distribution for the repeated measures properties. We compare the properties of our Laplace model (HWS) with major competitors, including the unweighted model (UNWT), fully weighted model (FWT), weightsmoothing model with normal prior and exchangeable random-slope assumption (XRS), and two variations of the estimators proposed by Beaumont (2008) : predicted weights on y and x (PREDYX) and predicted weights on degree 5 polynomial of y, together with x (PREDYX5). Bias and nominal 95% coverage are recorded directly, while RMSE is rescaled relative to the fully weighted estimator. Results are provided in Tables 1, 2, and 3. Under b a , where the model is correctly specified, all methods yield unbiased results, and the unweighted estimator maintains the best efficiency, with an approximate 30% decrease in RMSE compared to the fully weighted estimator. The original weight-smoothing method under XRS tends to provide unstable results, inflating the variance when the population signal is strong, but achieving similar RMSE as the unweighted estimator when the population signal is weak relative to the noise. Our model, under the same XRS assumption but with a Laplace prior, gives more stable results that resemble the fully weighted estimator when variance is low, but increase in efficiency as the population variance increases. Both the XRS and HWS estimators have correct to somewhat conservative coverage when the linear model is correctly specified. The Beaumont estimator PREDYX has improved RMSE compared to the fully weighted estimator, since it models the weight as a linear function of X; PREDYX5 yields estimates similar to the fully weighted estimator, as variance reduction was minimal due to the increased variability in the model weights.\nFor scenarios under b b and b c , the unweighted estimator of B is biased, and the fully weighted estimator strongly prevails over the unweighted estimator with respect to both RMSE and true coverage for small to moderate levels of residual variances. The weightsmoothing method under XRS remains biased at moderate levels of variance for b b and b c , and also at small levels of variance for b c , raising RMSE relative to FWT and destroying nominal coverage, suggesting that the exchangeable random-slope structure is not able to capture the relation in mean and variance among strata. The weight-smoothing estimator with Laplace prior has limited bias similar to that of the fully weighted estimator, but very substantially reduced RMSE, with correct to conservative coverage. The PREDYX estimator is insufficiently structured to reduce bias in the small-to-medium residualvariance settings; PREDYX5 mimics the fully weighted estimator and thus has little savings in relative RMSE under any of the scenarios."}, {"section_title": "Hierarchical Weight Smoothing Model for Logistic Regression", "text": "Following Elliott (2007), we set up populations in two settings: model misspecification and informative sampling. For model misspecification, the population is equally divided into 20 strata, and the predictor X is uniformly distributed within each stratum on an interval ranging from 0.5(h 2 1) to 0.5h. The binary response variable is generated as follows: . For C, we consider values of 0, .027, .045, .061, .080, corresponding to no model misspecification at c \u00bc 0 to increasing levels of model misspecification. The selection probability for each observation remains the same within each stratum, and increases linearly along strata, with a ratio between maximum and minimum probabilities equal to 20. For the informative sampling setting, we follow the same formula of\nX hi , UNI\u00f00:5 * \u00f0h 2 1\u00de; 0:5 * h\u00de; h \u00bc 1; : : : ; 20; i \u00bc 1; : : : ; 1000 but fix C \u00bc 0, so the model is correctly specified. We also create a vector of binary value Z * i such that Cor\u00f0Y i ; Z * i \u00de \u00bc r, and let r range from 0.05 to 0.95 to represent different levels of correlation with Y. Then we let Z i \u00bc Z * i U i \u00fe \u00f01 2 Z * i \u00deX i , where U i , U\u00f00; 10\u00de independent of X i , and the selection probability is proportional to Z i . Thus whether the selection probability is related to X or not is determined by the value of Z * , which is correlated with Y to some level. The process results in a ratio of roughly 30 between the maximum weight and minimum weight, with the correlation between selection probability and Y varying from 0 to 30% as the correlation between Z * and Y increases from .05 to .95. Twenty strata of equal size are created by pooling observations with similar selection probabilities together.\nFrom this population, samples with n \u00bc 1,000 are selected without replacement, with the selection probability stated above. We create weight strata using the values of h. A total of 200 samples are generated to create the empirical distribution for inference. A single MCMC chain is built for each data set, and for each iteration in the algorithm, all parameters are sequentially drawn from the full conditional distribution, except for b, which is drawn via a Metropolis step (proposed from a normal distribution centered at MLE with inverse expected information as covariance matrix, and accepted according to likelihood ratio times prior distribution). Then the predicted Y is calculated based on drawn parameters, and the target population slope is obtained by fully weighted logistic regression. The initial values of parameters are assigned the same as linear regression setting, and the process iterates 10,000 times, with a burn-in of 2,000.\nWe compare the properties of our Laplace model (HWS) with the same major competitors as in the linear regression setting. Bias and nominal 95% coverage are recorded directly, while RMSE is rescaled according to the fully weighted estimator. Results are provided in Table 4 and 5.\nWhile comparing different models under the model misspecification settings, the unweighted model has increased bias as the population model is less correctly specified, resulting in a change from an efficient estimate to a poor estimate (RMSE ratio from 69.7% to 281.9% of FWTs as C increases) and poor coverage as misspecification increases. The exchangeable random-slope model estimator is not robust, with bias similar to the unweighted model, and larger RMSE than the fully weighted estimator, although coverage is conservative. The hierarchical weight smoothing model with Laplace prior provides a more robust estimator, with minimal bias, and RMSE reduced by up to 14% compared to the FWT estimator, with true coverage similar to that of the fully weighted estimator. The weight-prediction model PREDYX performs similarly to the unweighted estimator, gaining efficiency when the model is correctly specified, and suffering as misspecification increases. PREDYX5, which predicts weights with a degree-five polynomial of x, essentially mimics the fully weighted estimator.\nUnder informative sampling, the unweighted estimator has only slightly larger RMSE than the fully weighted estimator, but is substantially biased with poor coverage. The exchangeable random-effect model has a similar degree of bias compared to the unweighted estimator, but has increased variability that, while providing conservative coverage, yields substantially increased RMSE over the fully weighted estimator. The hierarchical weight-smoothing model with Laplace prior again provides a more robust estimator, with minimal bias, and RMSE reduced by up to twelve percent compared to the FWT estimator, although true coverage suffers to a moderate degree except when the sampling is highly informative. PREDYX improves RMSE by up to 17% while having only slight undercoverage. PREDYX5 again mimics the fully weighted model."}, {"section_title": "Hierarchical Weight-Smoothing Model for Heteroscedasticity Scenario", "text": "In this section we evaluate the performance of the hierarchical weight-smoothing model compared to unweighted and fully weighted models under the heteroscedasticity setting. Here the main purpose of weighting is not to correct bias, but to adjust for the violation in the homoscedasticity assumption, and to yield correct inference on target quantities. We expect the data-driven method to capture the heteroscedasticity pattern in data well, and lead to proper inference.\nFirst, we create a population of size N \u00bc 20,000. The interval between zero and ten is evenly divided into ten strata with a length of one, and the predictor X is uniformly distributed within each stratum. The response variable Y is then generated from a normal distribution, with mean equal to twice of X, and variance as an increasing function of X: X i , UNI\u00f00; 10\u00de; i \u00bc 1; : : : ; N \u00bc 20;000\nwhere the population variance s 2 is set to 10 1 , 10 3 and 10 5 to adjust for different scales of population variance.\nWe repeatedly draw samples from the population without replacement by inclusion probabilities proportional to P, to link the corresponding weights to the heteroscedasticity pattern. The inferential target remains the population slope; all other settings for parameters and simulations remain the same. Altogether 200 samples are drawn, and the HWT model is fit using 10,000 iterations, with 2,000 as burn-in. To evaluate the results, we compare bias, relative RMSE and true coverage of the nominal 95% confidence interval or credible interval across the unweighted model, fully weighted model, and hierarchical weight-smoothing model in Table 6 .\nThe results suggest that violation in homoscedasticity undermines the performance of the unweighted estimator when the population variance is small. But as the population variance increases, this effect is quickly overwhelmed by concerns about efficiency, where the unweighted estimator has about a 35% reduction in RMSE compared to the fully weighted estimator. The hierarchical weight-smoothing model performs well in the heteroscedastic setting, where it correctly retains the weight interactions at low variances, yet \"tunes them out\" when efficiency is the dominating component."}, {"section_title": "Hierarchical Weight-Smoothing Model with Multiple Covariates", "text": "In the last simulation study we focus on the hierarchical weight-smoothing model's performance when multiple covariates exist in the model. The challenge lies in that some covariates may be related to the sampling scheme, and thus could benefit from weighting, but some may be independently distributed, and could lose efficiency in weighting. Simply applying the fully weighted or the unweighted model will sacrifice either variance for covariate associations unrelated to the sampling scheme, or bias for covariate associations related to the sampling scheme; we expect our data-driven method could reach a balance in between these options.\nTo replicate such a situation, we look into three different scenarios: independent covariates X 1 and X 2 with model misspecification on X 1 , and weight related to X 1 ; independent covariates X 1 and X 2 with model misspecification on X 1 , and weight related to X 2 ; and correlated covariates X 1 and X 2 , with model misspecification on X 1 , and weight related to X 1 .\nFor the first scenario, we generate a population of size N \u00bc 20,000, with two covariates X 1 and X 2 independently drawn from uniform distributions on an interval between zero and ten. The outcome Y is generated from a normal distribution with mean equal to a spline function of X 1 plus X 2 , thus our linear approximation to X 1 leads to model misspecification. The inclusion probabilities are proportional to p i \u00bc \u00f01 \u00fe dX 1i e=30\u00de * dX 1i e=2, and samples of size n \u00bc 1,000 are repeatedly drawn. Weights equal to inverse inclusion probabilities naturally create ten strata. For computation convenience, we also create Z as Z \u00bc I^(X 1 , X 2 ), where I \u00bc c(I 1 , : : : , I h ) indicates to which stratum an observation belongs. We assess the model performance through its inference on the population slopes of Y to X 1 and X 2 :\nWhen initializing the process, we retain all previous hyperprior parameters settings. The same Gibbs Sampler method is applied, and the predicted Y is generated to yield inference on target population slopes B 1 and B 2 . The process iterates 10,000 times, with a burn-in of 2,000. We conduct 200 simulations to provide the empirical distribution to estimate repeated measurement properties. Table 7 compares bias, relative RMSE and true coverage of the nominal 95% confidence interval or credible interval across the hierarchical weight-smoothing method, unweighted model and fully weighted model.\nThe results in Table 7 suggest that when the population variance is small, the unweighted model result suffers from model misspecification and has a substantially larger bias compared to the fully weighted estimator. Yet for the covariate on which the model is correctly specified, the unweighted estimator gains efficiency and yields an RMSE about half the size of the RMSE of the weighted estimator. As background variance increases, the influence of bias decreases and the unweighted estimator prevails over the fully weighted estimator in estimating both X 1 and X 2 . The hierarchical weight-smoothing model closely resembles the fully weighted estimator with minor improvements. That is, it 1 , B 2 when weight is related to X 1 using populations with different residual variances corresponding to various models under consideration correctly applies weight when model misspecification occurs, but fails to tune the result when efficiency is more important.\nFor the second scenario, we follow the same setting as the previous scenario, except that the inclusion probabilities are now proportional to p i \u00bc \u00f01 \u00fe dX 2i e=30\u00de * dX 2i e=2. Since the weights are related to the covariate that is correctly specified in the model, and independent from the covariate that requires adjustment, we expect no bias correction from the fully weighted estimator, and better efficiency from the unweighted estimator.\nWe keep the same parameter initialization and simulation setting. The simulation consists of 200 samples, 10,000 iterations within each sample, including 2,000 burn-in. The bias, relative RMSE and true coverage of the nominal 95% confidence interval or credible interval are reported in Table 8 .\nAs expected, the result from the unweighted model is more efficient than the fully weighted model, leading to a 30% to 70% reduction in RMSE. Since weighting is unnecessary for either X 1 or X 2 according to the population setup, the weight-smoothing model is able to limit the side effect of weighting, and achieves on average a 30% reduction in RMSE compared to the weighted model. Also note that the weight-smoothing model result suffers a moderate drop in the true coverage of a nominal 95% credible interval when the population variance is small relative to the misspecification.\nFor the last scenario, we study the model behavior when X 1 and X 2 are correlated. For this purpose, we create X 1 by the former approach, that is, from a uniform distribution on interval between zero and ten, but define X 2 as having a uniform distribution centered at X 1 to yield a correlation of about 0.45 between X 1 and X 2 . The rest of the settings stay the same: Similarly, three settings of variance are considered, and the inclusion probabilities are related to X 1 according to the formula p i \u00bc \u00f01 \u00fe dX 1i e=30\u00de * dX 1i e=2. The simulation again consists of 200 samples and 10,000 iterations within each sample, including 2,000 burn-in. The results are presented in Table 9 .\nAlthough X 1 and X 2 are correlated in this scenario, the results are very close to the first scenario. The weighted method is useful when model misspecification exists and the population variance is small, but loses to the unweighted method due to lack of efficiency when the model is correctly specified, or the variance is large compared to potential biasedness. The hierarchical weight-smoothing model fails to balance the two situations, closely resembling the fully weighted method.\nCombining all three scenarios, we conclude that the weight-smoothing model with Laplace prior has large gains in efficiencies when weighting is not necessary for any of the covariates, at the cost of a moderate drop in the true coverage rate when the residual variance is small. In other settings, its performance is similar to the fully weighted estimator. "}, {"section_title": "Application on Dioxin data from NHANES", "text": "To demonstrate the performance of our method in the linear regression setting, we consider its application on the dioxin dataset from the National Health and Nutrition Examination Survey (NHANES). During the 2003-2004 survey, 1,250 representative adult subjects were selected in a probability sample of the US, and had their blood biomarkers measured, including 2,3,7,8-tetrachlorodibenzo-p-dioxin (TCDD), a compound usually formed through incomplete combustion such as incineration, paper and plastics manufacturing, and smoking. Other demographic variables including age and gender are available from the survey. The sampled data are stratified into 25 strata, with each consisting of two Masked Variance Units (MVU's) to account for geographic clustering in the sample design without compromising confidentiality. Survey weights are provided as well. Due to technical limits, 674 readings are below limit of detection, and are imputed through multiple imputation using the model described in Chen et al. (2010) , resulting in five replicate data sets. Both survey structure and imputation are incorporated in analysis using a jackknife method and Rubin's formula (Rubin 1987) .\nTo determine the connection between log of TCDD level and individual demographic information, four linear regression models are fitted as log TCDD on age, log TCDD on gender, log TCDD on age and gender, and log TCDD on age, gender, and interaction. The hierarchical model is built as described before, with same initial value of parameters as those in the simulation. For each model setting, the unweighted (UNWT), fully weighted (FWT), and the hierarchical weight-smoothing (HWS) estimators are obtained (the exchangeable random-slope model fails to converge and is removed from the result). To estimate mean square error, the fully weighted version is treated as unbiased. Note that the fully weighted estimator is unbiased only in expectation, leading to an unbiased estimated square bias of regression coefficient b given by max(0,(b 2 b w ) 2 2 V 01 ), wher\u00ea (Little et al. 1997) . To fully account for the design features, all variance/covariance estimates are calculated via jackknife as\ndenotes the weighted b estimator from sample excluding i th MVU in h th stratum, and W (hi ) is a diagonal matrix consisting of case weight w j for all elements j \u00d3 h, j \u00d3 i,\nVar Var\u00f0b) and d Cov Cov\u00f0b w ;b) are calculated accordingly, and estimates from five imputed replicate datasets are combined using Rubin's formula (Rubin 1987) . All Gibbs Sampler estimates are based on 10,000 iterations after discarding 2,000 draws as burn-in. The resulting bias and RMSE estimates are summarized in Tables 10 through 13. For the first two models of log TCDD on age and gender separately, the estimation of the single predictor using an unweighted model appears to be biased compared to the fully weighted model, resulting in an estimated bias of about 40% and 70% of RMSE. However, the weighted model also fails to provide an efficient estimate for effect on age, supported by a RMSE of 3.888, larger than the RMSE of 3.265 from the unweighted model. Meanwhile, the hierarchical weight-smoothing model shows its ability to improve efficiency, both reducing the bias comparing to the unweighted model, and maintaining a RMSE similar to or smaller than the fully weighted model depending on the severity of variance inflation.\nAs more predictors enter the model, the estimated bias rapidly decreases in scale, leading to a scenario in which both bias and inflation in variance could dominate the overall RMSE, and neither the unweighted model nor the fully weighted model prevails in estimating all predictors. Hence the hierarchical weight-smoothing model cannot reduce bias further, yet it succeeds in reducing variance, resulting in overall smaller RMSE comparing to both the unweighted and fully weighted estimator (although the narrowness of the interval suggests that its coverage may be compromised to some degree, as in Subsection 3.4 of the simulation study)."}, {"section_title": "Application on Partners for Child Passenger Safety Data", "text": "In this section, we use a Partners for Child Passenger Safety dataset to demonstrate our method's performance under a logistic regression setting. Unit observations in the dataset are damaged vehicles disproportionally sampled from State Farm Insurance claims records between December 1998 and December 2005, when at least one child occupant less than 16 years of age was a passenger in a model year 1990 or newer State Farminsured vehicle with a damage claims report. The focus of the study is children's consequential injuries, defined by either facial lacerations or other injuries rated two or more on the Abbreviated Injury Scale (AIS) (Association for the Advancement of Automotive Medicine 1990). Due to the rare occurrence of the injury among all claims, to improve accuracy of the corresponding estimation of this rare outcome, the overall population was divided into three strata based on injury status -vehicles with at least one child occupant screened positive for injury at the time of the crash, vehicles with all child occupants reported receiving medical treatment but screened negative for injury, and vehicles with no occupants receiving medical treatment -and crossed with two strata defined by whether the vehicle was driveable or not. Since the stratification was associated with risk of injury (ascertained by follow-up survey), and cannot be fully explained by other auxiliary variables, the sampling design is informative, with weights varying from one to 50, and nine percent of weights lying outside three times their standard deviation. As determined by Winston et al. (2002) , children rear seated in compacted extended cab pickups are at greater risk of consequential injuries than children rear seated in other vehicles. To strengthen the conclusion, two models are applied, the unadjusted logistic model of injury status on car type (compacted extended cab pickups or others), and adjusted logistic model adapting control variables including child age (years), use of restraint (Y/N), intrusion into the passenger cabin in accident (Y/N), tow away after accident (Y/N), direction of impact (front/side/rear/other), and weight of the vehicle (pounds). The logistic hierarchical weight-smoothing model is set up as stated in the previous section, then the Gibbs sampler is executed for 10,000 iterations with 2,000 burnin, and odds ratios are compared with the unweighted and fully weighted model. As this is a disproportionally stratifed sample design, standard variance estimators are used for the unweighted and fully weighted estimators, while the posterior predictive distribution of the HWT model is used to compute point estimates and 95% credible intervals for the HWT estimator.\nThe estimated odds ratios for compacted extended cab pickups indicator did not vary much from the unadjusted model to the fully adjusted model, while unweighted regression and fully weighted regression lead to quite different results, from an OR of 3.534 to 11.317 for the unadjusted model, and from 3.448 to 13.890 when all other control variables are included (see Table 14 ). The hierarchical weight-smoothing model estimates lie in between the unweighted and weighted estimates, although much closer to the fully weighted model. It is also worth noting that with similar point estimates, the HWS model provides a considerable reduction in estimated standard deviation, leading to a narrower 95% confidence interval compared to the fully weighted model, a characteristic also presented in the previous simulation study."}, {"section_title": "Discussion", "text": "Model-based approaches to \"trimming\" survey weights attempt to formally balance bias and variance, resulting in an estimate usually lying between those from the unweighted model and fully weighted model. The weight-smoothing model using a Laplace prior shows the potential to provide a more efficient estimate than either the unweighted model or the fully weighted model, using an approach that is nearly as simple to implement in a regression setting as an exchangeable model, with equivalent or improved increases in efficiency but better robustness properties. Large increases in efficiency occur when bias is present due to model misspecification, and population variance is small so the weightsmoothing model is able to model the underlying data structure precisely, yielding an estimate with greatly reduced mean square error. However, this aggressive estimation comes at some cost of robustness, that is, the reduced variance could lead to lower than nominal coverage rates. As presented in the simulation, the HWS model suffers a moderate drop in the coverage rate when population variance is small, although it is usually competitive with the coverage of the fully weighted estimator. In future, it would be worth exploring the model's mechanism in reducing the overall RMSE, and the limit of the scenarios under which it still maintains reasonable coverage. The distribution of weights with a high degree of variability can itself vary considerably, from relatively uniform or heavy-tailed distributions, as we have seen in the simulations or the NHANES examples, to a small number of extreme outlying weights, to intermediate cases, as was the case in the Partners for Passenger Safety example. Since the method works by smoothing a given weight-stratum estimate inversely to its stability relative to other weight-stratum estimates, we would anticipate that there would not be consistent differences between these different types of weight distributions. If either the heavy-tailed or outlier weight strata are well estimated and sufficiently different from other weight strata, this interaction will be preserved and the fully weighted estimator will be approximated; otherwise the interaction will be shrunk and the estimator will move away from the fully weighted estimator.\nWe also note that the Laplace weight-smoothing model is largely agnostic to the construction of the weights. Instead, it focuses on whether there are enough data to support main effects and interactions between the weights and the parameter of interest (note that the main effects themselves can be viewed as interactions with the intercept in the case of estimating a population mean). While simple to implement, there may be settings where one wants to smooth some components of the weights (e.g., selection and nonresponse) while retaining others (e.g., post stratification or calibration). Such \"partial smoothing\" model-based approaches remain a topic for future research.\nComparing the results of the Laplace prior weight-smoothing models with the modelbased estimators of Beaumont (2008), we find that the Laplace estimators offer the promise of relatively simple estimators that can approximate fully weighted estimators (3.731,29.876) 13.268 (7.919,22.232) when weights are required for bias correction, but improve over weighted estimators in terms of variability while maintaining an approximately correct nominal coverage of credible intervals. In contrast, in some settings the Beaumont estimators can \"over smooth\" weights when bias correction is needed and yield unstable estimators when the weight prediction is weak. The predicted weights in the weight-modeling approach of Beaumont incorporate information from design variables, thus yielding better predictions for weighted mean and population total estimates than unweighted estimators. However, in some settings even a degree-five polynomial may fail to correctly approximate the relationship between the inverse of the probability of selection and the sample statistic of interest. Perhaps even more importantly, highly structured models for weight prediction such as high-degree polynomials may result in unstable estimates of weights, adding unnecessary variance rather than dampening it, although model-selection methods may reduce such impacts. Methods such as those proposed by Pfeffermann (2011) and Kim and Skinner (2013) , who proposed a form of \"stabilized\" weight models as w i \u00bc E M \u00f0w i jI; Y; X\u00de=E M \u00f0w i jI; Y\u00de; may be of use in informative sampling settings (in noninformative sampling settings, w i \u00bc w i if the model is correctly specified). Ultimately we find attempts to model weights rather than data misguided, as this focuses on design factors on which we should be conditioning, rather than assessing uncertainties in the data that may be fertile ground for mean square error reduction while preserving approximate nominal coverage: that is, calibrated Bayes estimators (Little 2011) . As a final note, there is an issue of whether a census estimate of a parameter of a misspecified model is a sensible inferential target. Our perspective is that statistical models are rarely perfect, and that complex sample designs can sometimes magnify the degree of these failures. We recognize, however, that there is controversy in this area. For example, Rothman et al. (2013) make a case that truly scientific endeavors attempt to make causal statements that should be independent of sample selection. Keiding and Louis (2015) replied to this with an argument perhaps close to the one we make here, which is that the transportability (in the formal sense of Pearl and Bareinboim 2014) of model results may still require attention to the effects of sample design."}]