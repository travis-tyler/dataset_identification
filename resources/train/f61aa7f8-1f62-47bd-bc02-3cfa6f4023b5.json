[{"section_title": "", "text": "of selected groups of students at the time the measures were taken. As such they serve to spotlight achievement discrepancies between selected groups of students. Additional state resources (generally as a research function) can then he focused on \"why\" such discrepancies do in fact exist and \"what\" can be done about them. Nothing but experimentation, if that, can serve to demonstrate the causes of these 'between group\" discrepancies in achievement. The proposed Minnesota Educational Assessment program is modeled after the National Assessment of Education Progress (NAEP), an ongoing educational project designed to give educators and the lay public a better look at those knowledges and skills that American youth have acquired. NAEP provides for a systematic, continuous, census-like survey of knowledges, skills, understandings, and attitudes as exhibited by students and young adults in four age levels and across ten different subject areas. By following the NAEP model, Minnesota can: (1) reduce the costs of developing and scoring assessment exercises; (2) compare the asses:ment results for Minnesota students to NAEP results for students in the Nation as a wf,ole, as well as in the Central Region* of the Nation (all comparisons will be made at the same student age levels); and (3) take.advantage of past and future exercise administration, data collection, sampling, and data analysis methodologies that have been (and will continue to be) developed by NAEP. Table 1 presents an overview of the curriculum coverage provided by the proposed Minnesota plan. (Curriculum coverage is defined to include the grade and age levels assessed, the subject matter areas included in the assessment program, and the reassessment cycle for measuring educational progress.) As shown in Table 1, the plan provides for the collection of assessment data from a sample of students in grades 4, 8, and 11, and from a sample of students who are in school and are 9, 13, and 17 years of age. However, grade and age samples will overlap considerably. For example, over 70 percent of the 9 year olds will be 4th grade students. The age level results will be used for making Minnesota versus NAEP comparisons; the grade: level results will be used to enhance state level decisionmaking. Assessment data will be collected from 8th graders and from 13-year-olds in October and November, from 4th graders and 9-year-olds in January, and from 11th graders and 17-year-olds in March and April. Subject area coverage would be initially limited to the ten NAEP subject matter areas as shown in Table 1; i.e., Art, Career and Occupational Development, Citizenship, The Central Region includes the following states: North Dakota, South Dakota, Iowa, Kansas, Minnesota, Missouri, Nebraska, Illinois, Michigan, Ohio, and Wisconsin.  The program is thus based on a five-year \"reassessment\" cycle. That is, each subject area is reassessed in five years (except for the'first six year cycle for Reading of 11th graders and in-school 17-yearolds). In addition to allowing a reasonable time for measuring educational progress, this cycle provides time for the preparation of exercise items for the reassessment. The utility of the assessment results depends upon how groups of students are defined for reporting purposes. State results for each of the three age groups would be used to report comparisons between Minnesota students and students in the NAEP National and Central Region samples. Within state results would be reported by additional groupings of students within each of the three grade levels (4, 8, and 11). A final delineation of these reporting groups is pending an exact determination of funds available for the Phase 1 assessment. However, a list of candidate reporting variables was developed and ranked by order of importance. This list provides for grouping student results by: (1) such student related variables as sex, SES, race, longevity in the Minnesota Educational System, and the type and degree of participation in extra-curricular activities; (2) such school related variables as public and/or non-public status, type and size of community in which school is located, size of enrollment, teacher/student ratio, per pupil expenditure levels, regional location, and selected characteristics of staff and student body; and (3) process related variables pertinent.to instructional approach and/or program participation. The proposed Minnesota assessment plan has two unique features that could be introduced on a pilot basis during the Phase 1 assessment. The first is an option whereby local school districts could linkup with, or \"piggyback\" onto, the state assessment to obtain results for their districts. The Department of Education, though it has endorsed this option, has decided to field test it with one or two districts before finalizing procedures and policies for its full implementation. Plans at this time anticipate that local school districts would be responsible for any additional costs associated with exercising this option. The second unique feature is the development of \"desired outcomes\" measures that would reflect desired performance levels for Minnesota students in the assessed subject matter areas. This judgmental information could assist educational decisionmakers in identifying areas of concern and commendation within the state. There are, however, several technical problems involved in developing these measures. As a result, implementation of this feature would be on an experimental basis, as limited by the availability of resources. B."}, {"section_title": "Managing and Staffing (Chapter 3)", "text": "The ten-year educational assessment program proposed for Minnesota involves the annual collection, analysis, and dissemination of a substantial amount of educational data. That is, with the exception of the first and last years of the assessment cycle, Department of Education personnel will be simultaneously involved in collecting data for the current year's assessment, analyzing and disseminating the results for the previous year's assessment, and preparing for next year's assessment (e.g., developing exercises and finalizing the data collection strategy). The recommended program, by following the NAEP model of collecting data on 13-year-olds in the fall (October-December), on 9-year-olds in the winter (January-February), and on in-school 17-yearolds in the spring (March-April), does serve to reduce the data collection work load by distributing it over the school year. The current staff of the MinneSota Assessment Project consists of a Director and typing assistance, plus two excellent advisory groups (the Assessment Advisory Council and the Technical Advisory Committee). However, an assessment program of this magnitude entails a wide range of work tasks, many of which require highly specialized technical and professional skills. That is, Minnesota would need a full-time staff of nine professionals, two junior professional/research assistants, one secretary, two clerk /typists, and four typists, plus eight part -time exercise administrators and some specialized contracting and/on consulting assistance to implement and sustain the proposed assessment program. (It is not always cost-effective to hire speCialized expertise in-house, especially when this expertise is required in brief, infrequent intervals throughout each assessment year. Hence, the additional contracting and/or consulting services would be used in exercise development, sampling, data analyses, and exercise scoring.) The full time Minnesota assessment staff, contracting/consulting services, and advisory groups would be organized as follows: Director's Office: A Director and a Secretary."}, {"section_title": "2)", "text": "Advisory Groups: An Assessment Advisory CoUneil and a Technical Advisory Committee.\nReview of NAEP objectives and released exercises in order to select those exercises which are relevant to Minnesota objectives.\nEach of the ten geographical reporting regions of the state (two of the eleven regions are combined to form a single region) are represented in the sample so that results can be reported for each of them with nearly equal statistical precision.\nTo designate one school official, preferably a counselor, to help arrange the testing schedule and to aid the District Supervisor and Exercise Administrators in having students at the administration site on schedule. 3) To provide adequate space for administering the exercises (exercise packages will be administered to an average of approximately 20 students from each school in the sample).\nSize and Type of Community--extreme rural areas, extreme inner cities, extreme affluent suburbs, inner city fringes, suburban fringes, medium cities, small cities.\nHave the Director also assume the role of Head, Dissemination Section. 3) Contract the responsibilities of the Instrumentation Section to a consulting team with expertise in the development of Reading exercises for 17-year-olds.\nReview available questionnaires for items that can be incorporated into the required questionnaires.\nTo betransformed into computerized form by optical scanning methods that eliminate the human element as much as possible from the data transformation process. 3) To be compatible with the data collection procedures outlined in Chapter 6.\nReview the NAEP objectives and released exercises and select those that are relevant to the Minnesota objectives. 3) Identify the gaps in those Minnesota objectives which are not adequately measured by the selected NAEP exercises.\nPreparation of exercises. 3) Review and revision of exercises.\nPer Pupil Expenditure by SchoOlDistrict. 3) Size of SchOol.\nVariability of exercise responses between pupils within school.\nOne school official, preferably a counselor, to help arrange the testing schedule and to aid the District Supervisor and Exercise Administrators in having students at the administration site on schedule.\nAcceptance of data as reported. 3) Deletioh of the specific data items in question or of the entire form. However; the appropriateness of each of these methods will depend onthe nature. of missing data. In some situations involving probability sampling, it may be advisable to adjust the weights used instead of resorting to imputation. The imputation procedure to be used will depend on.the nature of analyses, availability of auxiliary information available, and the, pattern of missing data. . _"}, {"section_title": "3)", "text": "Instrumentation Section: A Head, a ResearchrAssistant, two Typists, and a subcontracted team of six Exercise Writers.\nIdentification of the gaps for those Minnesota objectives which are not adequately measured by the selected NAEP exercises.\nEach reporting variable, or any combination of two reporting variables, can be reported for up to eight reporting groups. This requirement allows for the analysis of interaction effects of certain pairs of reporting variables; i.e., differences in the effects of one reporting variable for different levels of the remaining variable will be examined.\nSex.\nDevelop draft questionnaires.\nEach reporting variable or any combination of two reporting variables can be reported for up to eight orting'groups. This requirement,; will allow for the analysis of interactl,t effects of certainpairs of reporting variables. That is, differences in the effects of one reporting vP-iable for different levels of the remaining variable will be examined.'\nCost of including additional schools in sample.\nAdequate space for administering the exercises (exercise-packages will be administered to an average of approximately 20 students from each school the sample)."}, {"section_title": "4)", "text": "Survey Operation Section: A Head, a Clerk/Typist, two District Supervisors, and eight part-time Exercise Administrators.\nDevelopment of required exercises.\nA field test(s) to tryout the questionnaires.\nIf, for a given grade and/or age level, the amount of time it takes to administer the entire set of assessment exercises to a given student is longer than desirable, a matrix sampling approach will be developed to shorten the-length of time each pupil will be tested.\nTo cooperate in completing the school questionnaires and in providing some of the background information fo.7, elementary school students selected in the sample. Assessment data will be collected on machine readable/scorable forms in order to minimize errors inherent in reproducing or transferring data to magnetic tape files for computerized retrieval. However, editing and error resolution activities 1 that require various levels of judgment and involve more than one individual will also be conducted at several phases in the data handling process. Because it is important to safeguard the rights of participating students and school principals, certain measures are included in the data collection and processing plan to assure confidentiality with respect to all information collected on individual students and schools. As such, these data collection and processing procedures assure a hign degree of cooperation and a great degress of quality control with respect to both sample selection and data collection. \nColor.\nContract the responsibilities of the Survey Section and the Scoring, Data Reduction, Sampling, Data Analysis, and Report Preparation Section to one or more organizations with capabilities in these areas.  Figure 2. There are several factors, however, that should be taken into consideration before this recruitment strategy is employed.\nField test and revise the draft questionnaires. Student and school questionnaires can be prepared by the Instrumentation Section as described in Chapter 2. The initial development of the desired outcome questionnaires, however, is a potentially large and difficult task, the completion of which is beyond the scope of the resources planned for the Section. The ensuing discussion is thus directed only to the development of the student and school questionnaires. The first task, the delineation of the questionnaire's data elements, is dependent upon a final selection of reporting variables and a definition of the reporting groups for each selected variable. Given these reporting variables and groups, the information to be collected via the questionnaire will further depend upon the availability  So that the student questionnaire would take no longer than 15 minutes to complete, the school questionnaire no longer than 30-45 minutes.\nTo expedite the data analysis plan discussed in Chapter 7. As a final task, the draft questionnaires should be field tested on a small sample of students and principals to obtain timing estimates and to elicit critical appraisal with respect to interpreting the items and/or selecting item alternatives. The questionnaires should be revised as per the results of these limited field tests. These questionnaires as initially designed for the Phase 1 assessment will probably require slight annual revisions based on feedback received, changes made in reporting variables, and the inclusion of some items that might be specific to the subject matter areas being assessed. IV.\nDevelop exercises to fill in the identified gaps.\nField testing and revision of exercises.\nDevelopment. of required exercises.\nIf for a.giyen age and/or grade level.the amount of time it takes to administer the entire set of assessment exercises to a:given student is longer than desirable, a matrix sampling approach will be developed to shorten the length of time,each pupil will be tested.\nSize-of Community in which,SChool is Located.:\nCost of including additional pupils in sample within selected schools. Based upon estimates of these variances and cost components from other educational assessment surveys, a sample of 20 randomly selected students per, school is'recommended. Once the total pupil sample size, its allocation to the reporting regions, and the number of sample pupils per school are decided upon, then the--school sample sizes for each of the reporting regions is a straightforward calculation.. Using the pupil sample sizes of the disproportional allocation of column. 6 of Table 7 and allocating two.\"special\" schools* to each reporting region, columu 5 of Table 13 gives the total school sample.sizes by reporting regions. Column 8 of Table 8, gives the largest See footncite 1 of Table 6 for a definition of \"special\" school.  A \"special\" school is one with a small 17-year-old enrollment not in grades 11 or 12. Since 17-year-olds are enrolled in grades 9 through 12, this school population must be sampled in order to avoid statistical bias., It is expected that on the average these schools..will contain five 17-year-olds. When a.\"special\" school is selected for the sample all of its eligible students will be given an exercise package. number of school groups or strata within each region that is possible if two schools are selected from each stratum. This latter condition allows for estimates of ='sampling variability to be computed from the assessment data. D.\nCooperation of the principal in completing the school questionnaires (and of homeroom teachers in Phase 2 assessments to provide some of the background information for those elementary school students selected in the sample). More specifically, the district superintendent of each school selected in the sample will be advised of their selection by mail in early February. This letter will furnish general information about the assessment and indicate that an assessment District Supervisor will make telephone contact with the superintendent within a short time. The purpose of the telephone contact will be to answer any questions and to arrange a meeting between officials of the individual schools, the superintendent, and the District Supervisor. Three types of students would be excluded from the target population even though they do meet the age and the grade level definitions: (1) non-English speaking students: (2) educable mentally retarded students; and (3) functionally disabled students. Principals'will be provided with'guidelines for identifying these students. At this meeting, the school's involvement in the assessment program will be explained in detail. Space for administering the exercises and the aid of a school counselor will be arranged. Sampling proCedures will be discussed and possible . candidates for, the Exercise Administrator positions will be solicited. "}, {"section_title": "5)", "text": "Scoring, Data Reduction, Sampling, Data Analyses, and Report Preparation .\nPackaging of the exercises into booklets. The magnitude of the annual exercise development task would vary depending upon the degree to which Minnesota objectives are measured by the released NAEP exercises Reviews of exercises should take into account their potential offensiveness, ease of scoring, ease of administration, content validity, and content appropriateness. Using a form of matrix sampling, these exercises could be distributed over several exercise packages to minimize the exercise administration time per student; i.e., each student in the sample could take only a portion of the total number of exercises involved in the assessment. Major steps in the questionnaire development plan include: 1) A delineation of the variables and data elements to be included in the questionnaires. 2) A review of available questionnaires for items that can be incorporated into the required questionnaires. 3) The development of draft questionnaires.\nA revision and finalization of the draft questionnaires. As special considerations, all questionnaires should be designed to be brief, amenable to transformation to a computerized file by optical scanners, and compatible with the data collection and analysis plans. D.\nThe school and pupil school sample sizes will.be such that estimates of the sampling variability of the reported results can be estimated from the sample data. This sample design involves the use of the state's ten reporting regions as a stratification variable to give the sample \"representative credibility\" by geographically spreading it 'across the state. Selected school level variables which relate to educational performance are also used as stratification variables within each of the ten reporting regions. Stratifying by these achievement related school variables (per pupil expenditure and type and size of community in which the school is located) provides for more homogeneous groupings of schools, thus enhancing the statistical precision of assessment results. The sample design involves two sampling stages. The first stage would consist of selecting a random sample of schools within strata; the second stage would be the selection of a random sample of students within the schools selected at stage one. Using this sample design, the statewide probability sample for the Phase 1 assessment of in-school 17-year-olds would include approximately 5,100 students from 270 schools. The Phase 1 assessment of Reading would involve no more than two hours of total exercise administration time. Consequently, it will not be necessary to use a matrix sampling approach in this phase of the assessment. E.\nParental education--both parents with eighth grade or less, at least one parent with some high school but not graduated, at least one parent graduated from high school, and at least one parent with some post-high school training. [Ref. 5] These \"reported\" exercises are then released and can be used in state and/or local assessment programs. \nContract out only those 1975-76 assessment responsibilities that cannot be handled by the available in-house staff. If everything has proceeded as planned--assuming the strategy to develop a large in-house capability is not altered on the basis of experience--contractual and/or consulting services would be limited to those specified in Figure 2. Should unforeseen events occur, contracted services could be used to fill in staffing gaps and maintain program continuity. The strategy described above represents one reasonable approach; others can be designed by reducing or increasing the milestone date for staffing completion, or by rearranging the priorities for section head recruitment (e.g., the Department may decide that hiring a head for the dissemination section is first priority). school year assessment--keeping in mind that the exercises must be in the field for the 8th graders/13-year-olds in October, for the 4th graders/9-year-olds in January, and for 11th graders/17-year-olds in. March. Given these constraints, the exercise development year for each pair of subject matter areas extends roughly from November of one year to December of the next year. There is nothing magical or absolute about having \"six\" contracted exercise writers on this team. For planning purposes, it was felt that one person should be responsible for one subject area at one grade/age level--hence, six writers would be required to cover two subjects across three grade/age levels. It is not envisioned that these personnel work independently. In fact, it might be more appropriate for planning purposes to consider having a \"three-man\" exercise writing team in each subject area. Furthermore, the requirements for this contracted expertise could vary considerably year-by-year as a'function of (1) the number of new exercises that must be developed to supplement the NAEP exercises (could range from \"none\" to \"all\"), and (2) the expertise and production capabilities of the organized working groups of school personnel and curriculum specialists. 1.\nPackage the exercises. Important considerations in the performance of each of these tasks are discussed in separate sections below. B.\nFinal reviews and selections. As outlined in the management and staff plan in Chapter 3 and further discussed in Section II above, Minnesota would supplement the \"contracted\" local university expertise with an appropriate force of school personnel and curriculum specialists. The recommended approach, though usually less expensive, is more difficult to supervise and 'control to ensure satisfactory results. Three of the key features of the current NAEP plan are especially noteworthy. The-first is the involvement of student groups to review both the objectives developed in Task 1 and the exercises developed in this task. As a result, the exercise packages will be identical in that all the Reading exercises will be administered to all students. However, when exercises for two subject areas are administered during Phase 2 assessments, it will be necessary to distribute the total number of exercises over three or four exercise packages to reduce the exercise administration time required per student. Each package for the same grade/age level will contain identical student questionnaires; however, the exercise components will vary. A form of matrix sampling will be used whereby each exercise package will contain a portion of the total number of exercises required to assess both subject areas. One exercise package would be assigned to each student in the sample on a probability basis. The plan for developing the exercises required to supplement the released NIEP exercises involves the following subtasks: Definition of broad state goals and their translation into measurable, operational and behavioral terms. 2) Review of NAEP objectives and released exercises in order to select those exercises which are relevant to Minnesota objectives. 3) Identification of the gaps for those Minnesota objectives which are not adequately measured by the selected NAEP exercises.\nPackaging of the exercises into booklets. II.\nConfirmation of a computer.generated correction. Within these general guidelines, a specific set of error resolution rules should be established during th,: operational.phase of.the assessment for resolvingerrors in sets of specific data items. A tile should be kept of all receipt control statistics and 'summary data on the detection and resolution of errors: This type of information is extremely important to the refinement of data collection and processing procedures for subsequent assessments. IV."}, {"section_title": "Instrumentation Development (Chapter 4)", "text": "Two types of instrumentation are required for the proposed educational assessment program. The first is the exercises for measuring performance in subject areas; the second is the questionnaires for obtaining student and school background information. If the desired outcomes measure is also included as a component of the assessment program, the instrumentation requirments would be expanded to include additional questionnaires for gathering desired outcomes information. NAEP exercises are released in_December or January of the school year following their use in the National Assessment program. With few exceptions, the Minnesota curriculum coverage plan is designed to incorporate the released NAEP exercises into the Minnesota assessment program, beginning in the fall of the school year following their release. Given these constraints, the exercise development year for each pair of subject matter areas extends roughly from November of one year to December of the next year. At least two months is required after the exercises are developed for packaging, printing, and distributing the assessment materials in time for their scheduled fall, winter, and spring administrations. The plan for identifying and developing the exercises required to supplement the released NAEP items involves five major tasks: Definition of broad state goals and their translation into measurable, operational and behavioral objectives."}, {"section_title": "Sample Design (Chapter 5)", "text": "Sampling, which provides educational decisionmakers and those interested in education with results of sufficient precision at a reasonable cost, was selected over the alternative of collecting assessment data from every Minnesota student in the grade/age levels to be assessed. In general, the sample design recommended for Minnesota meets the following requirements: 1) The sample is a probability sample; i.e., each student in a given age class or grade in a public or non-public school in the State of Minnesota has a known positive chance of inclusion in the sample."}, {"section_title": "Data Collection and Processing (Chapter 6)", "text": "The quality of information gathered is greatly affected by how it is collected and who collects it. Since good decisions are rarely made on the basis of poor information, the task of collecting and processing data (includes editing and scoring) constitutes an integral aspect of the assessment plan. A field survey approach analogous to that used in the ongoing NAEP program is recommended as the most cost-effective way for Minnesota to collect their assessment' data. Specially trained survey teams, using a \"one day in--one day out\" approach, will administer exercises and collect background information on the students and schools"}, {"section_title": "_10", "text": "in the statewide sample. Data collection at each school will require no more than -one-half day and will be conducted with minimal disruption to school programs and slight impositions on students, teachers, principals, and other school officials. These survey teams of District Supervisors and Exercise Administrators will be supervised and coordinated by the Head of the Survey Operations Section. Exercise Administrators will use paced tapes to better ensure the standardization of all data collection and, except when assessing Reading, to help prevent the exercises from measuring reading ability as well as the subject area being assessed. Assistance and cooperation of each school selected in tL sample will generally be required as follows: To provide a roster of eligible students in the proper grade and age levels (this information would be used to draw the student sample)."}, {"section_title": "Data Analysis (Chapter 7)", "text": "The data analysis plan for the proposed assessment program focuses on various strategies of statistical analysis to extract the most important and relevant descriptive measures from the data, as well as to detect important differences in achievement measures between various subgroups of students. This data analysis plan consists of three general sets of analyses. The first is a descriptive analysis of the responses to the student and school questionnaires that would serve to describe the input characteristics of the students and schools in Minnesota. That is, estimated proportions of Minnesota students in each grade/age level who are in each of the discrete categories of the selected reporting variables would be computed; e.g., numbers of students attending schools in Large City, Large City Fringe/Medium City, and Small Town/Rural types of communities. Similar estimates would also be computed for the proportion of Minnesota schools in all reporting groups for the school related variables; e.g., numbers of schools with various average student/ teacher ratios. In addition, estimated proportions would also be compiled for various combinations of reporting variables. The second set of analyses involves comparisons of Minnesota Assessment results to National Assessment results. That is, the exercise p-values for various groups of Minnesota students would be compared to those of students in comparable age groups These analytical techniques involve the simultaneous examination of a number of independent variables and are useful for examining the effects of one independent variable while controlling or adjusting for the effects of other independent variables. These second level analyses are directed to investigating the relationships between achievement measures (exercise responses) and selected student, family, and schoolcharacteristics. One might, for example, pose the following question under this evel of analysis: What would be the differences in p-values between students in schools with large enrollment differences if the distributions of students by parental education, race, SES, type of community in.which the school is located, etc., had been the same for all schools? A great danger in tnis approach is attributing casualty to those variables that turn out to be significant in the statistical mode]. Nothing short of experimentation, if that, can demonstrate what the actual effects of these variables are. Statistical models based on survey data (non-experimental data) can only provide valuable hints and insights into the probable effects of these variables. G."}, {"section_title": "Reporting and Dissemination (Chapter 8)", "text": "The basic purpose of a dissemination plan for statewide assessment results is to insure that accurate information is made available to all interested people in the state, at a level of sophistication (detail) commensurate with their background, needs, and purposes. This information is needed so that people at all levels can evaluate properly the need for change, so that they have enough information of the right kind and type to make intelligent and data-based educational decisions. Thus a legislator or high-level policymaker would have a far different background, need, and purpose for assessment results than would a housewife with children in school. Too little information in the former case would be deleterious, while too much in the latter case would provide unneeded, unwanted, and perhaps misunderstood data. There are at least four levels which should be considered in understanding the various needs of the consumers of assessment program results. The policymaking level consists of legislators and members of the executive branch of the state government who are charged with the responsibility for establishing broad policies which, when carried out, will best meet the educational needs of the state. A decisionmaking level is comprised of personnel throughout the state, and in the Department of Education, who are charged with carrying out policy through making the basic operational decisions and allocations of resources. An operational level or the \"on the street\" level is one in Which educators work actively with children and parents, face the very real problems in modern education, and carry out all the policies and decisions which have been made. And, finally, a public level brings us back full circle to the policymaking level. The public level consists of local school board members, various special interest groups, both concerned and somewhat-less-thanconcerned parents of children in school, and a very large group of adults with no children in school. This group, through the democratic processes, can and should have a strong effect on the first level. The dissemination plan must be responsive to the informational needs of these various levels of consumers of educational assessment results and of educational decisionmake-.s who will use assessment information. All levels must have access to all results; but their specific information needs vary. This means, then, that report formats, audio-visual aids, presentation modes, and any other form of promulgation and publication decided upon and used should be tailored to some degree to decisionmaking groups, special interest groups, and the general public. A series of pre-assessment workshops is proposed in spring 1973, before actual Phase assessment administration, to acquaint the decisionmaking and operational levels with the project. In addition, wide public exposure to the assessment project is suggested through television, service clubs, and special interest group presentations. The primary vehicle for assessment dissemination is seen as a series of Fall Workshops, each of which serves to disseminate the results of the previous school year's assessment and to introduce future assessments. Actual dissemination of Phase 1 results will occur by means of the basic Technical Report, a widely distributed Highlights Report that would be written in popular language and format, and Fall Workshops for decisionmaking and operational persons in the subject area(s) assessed. These workshops will also serve to introduce the early Phase 2 assessments. Yearly Phase 2 dissemination will be in the same manner: Technical Report, Highlights Report, and dissemination Fall Workshops on subject-matter areas. Each year there will also be special presentations prepared for the policymaking level. The utility of the state assessment for educational decisionmaking depends to a large extent upon how various groups of students within the state are delineated for the purpose of reporting the assessment results. A general strategy for delineating these reporting variables, along with a candidate list of student, school, and process reporting variables, are also presented in this chapter. Additional features of the proposed assessment program that are covered in separate sections of this chapter include an option whereby local school districts can link up with or \"piggyback\" onto the state program to provide assessment results for their districts, and an experimental feature for exploring the development of performance criteri4 or desired outcomes against which state assessment results could be compared as one means of identifying areas of concern and commendation within the state educational program. II."}, {"section_title": "INTERPRETIVE LIMITATIONS OF STATE ASSESSMENT RESULTS", "text": "A statewide assessment program is an important component of the state department planning function. Properly conceived, the statewide ac.essment program can provide a means of periodically monitoring achievement in the cognitive, affective, and psychomotor domains to determine whether or not Minnesota children know and can do those things that they should be able to do in order to live a full life. In addition to providing status reports of performance with respect to desired outcomes at various stages in the student's scholastic career, the assessment program can serve to identify those groups of students, by certain general characteristics (e.g., sex, race, SES, and geographic location and resource levels of schools attended), who may or may not be realizing the educational objectives of the state. (This is an important factor if one adheres to the belief that the state should be accountable to all children, regardless of their race, geographic location, and family status.) Given such information, educators can formulate plans for the allocation of resources so that the attainment of desired outcomes can be enhanced among those groups of students having the greatest needs. However, it must be noted that assessment results generally serve only to describe the achievement status (by Reading or any other output variable) of selected groups of students at the time the measures were obtained. As such, they serve to spotlight achievement discrepancies between selected groups of students. Additional state resources (generally as a research function) can then be focused on \"why\" such discrepancies do in fact exist and \"what\" can be done about them. Most schoolmen regard Roger Freeman as one of the more mendacious authors on school finance in the U.S. A senior fellow at the Hoover Institution of War, Revolution, and Peace at Stanford University, Freeman said in a Wall Street Journal article last March 31 that \"the higher the expenditures per pupil--the smaller the class size--the lower are pupil achievements--and vice versa.\" Citing as his source the New York City School Fact Book, Freeman announced that in 1967068 there were 30 New York schools in which per-pupil expenditures averaged $1,330. Then there were 101 schools in which the average was $441. Thus the first group of schools spent about 2.5 times as much as the second. The. teacher -pupil ratio was 1:12.3 in the high-expenditure schools, 1:25.9 in the low-expenditure schools--or more than twice as high. But the reading skills of the students in the lowexpenditure, large-class schools averaged above grade level, Freeman said, while-in the high-expenditure, small-class schools they were below grade level. \"This is not just an accident,\" Freeman alleged. \"A review of . . reports from other cities shows that the high-expenditure, small-class city school typically is one wita low educational achievements.\" Albert Shanker says in his weekly New York Times column, \"Where We Stand,\" that Freeman's views would merit no more than casual attention if they were the views of one man; but the ominous fact is that these dangerous views are the hallmark of an odd coalition of the right and the left, the wealthy right embracing them as a justification for withholding adequate fiscal support for education, the new left proclaiming that \"relevancy,\" \"Community control,\" \"ethnic studies,\" and \"life-style\" rather than more money are the keys to educational success. \"The shoddiness of such reasonings is quite apparent,\" Shanker says. \"Pupils do not do poorly in reading and math because they are in small classes; rather, they are placed in small classes because they are doing poorly. If we were to extend Freeman's logic to [medicine], the more money an individual spends on doctors and hospitals, the poorer his health. The healthiest individuals spend little or no money on doctors and hospitals. Het-e the way to fight disease is to abolish Medicare, health insurance, and welfare programs.\" Shanker suggests that even though some cities are spending more than others, they are not spending enough. Or, he adds, it may be that, while more money will make some difference, the schools, no matter how effectively they function, cannot overcome all the nonschool factors which prevent students from achieving. . . . The results for about fifty percent of the NAEP exercises given each year are reported--for each exercise and each age group--by the following categories (beginning with the second assessment year, results are reported for each student area by theme, i.e., a set of exercises which share a common content but which may require diverse behavioral responses):"}, {"section_title": "III.", "text": ""}, {"section_title": "THE NAEP PROGRAM", "text": ""}, {"section_title": "1)", "text": "Gegoraphic region--Northeast, Southeast, Central and West.\nEven after the current freeze on hiring is lifted, the process of obtaining approval for the required manpower slots will further delay personnel recruitment. 2) Since few states and/or large school systems have undertaken assessment programs of this magnitude, it will be difficult to find experienced personnel who rossess those rather unique qualifications required to fill the fOur section head positions. 3) The  After being on the job a while, these heads may decide to reevaluate the in-house assessment capability reflected in Figure 2 by either expanding or reducing the work done \"out-of-house\" by subcontractors."}, {"section_title": "Carryover of NAEP Technology", "text": "Minnesota, by following the NAEP model, can take advantage of the test administration, data collection, sampling, and data analysis methodologies that have been developed, field tested, and perfected through the years by NAEP. Since NAEP is an ongoing project, further relevant NAEP innovations can likewise be adopted by the Minnesota program. IV."}, {"section_title": "CURRICULUM COVERAGE OF THE PROPOSED MINNESOTA PLAN", "text": "A. As a result, the Minnesota statewide assessment sample will be designed to provide for adequate statistical precision to compare Minnesota statewide results . for 9, 13, and in-school 17-year-olds with the results for these same age groups in the Nation and Central aegion."}, {"section_title": "General", "text": "Educational planning in the public schools is currently based more on information grouped by grade levels than by age levels. In addition, the grade level groupings are more conducive to gathering information relative to desired outcome measures for the state; that is, it would be easier for an educator or layman to formulate the desired education performance of 8th graders, as opposed to formulating desired outcomes for 13-year-olds who may be scattered in grades 6 through 9 (the concept of   9-year-olds will generally be distributed across grades 2-5;'13-year-olds across grades 5-9; and 17_yearolds across grades 9-12. Grade 8 and 13-year-old students are tested in October-November; Grade 4 and 9-year-old students are tested in January; and Grade 11 and 17-year-olds are tested in March-April. 3/ desired outcomes is discussed in Section VII below). The statewide sample size will, therefore, be designed so that assessment results can also be reported by grade levels. As indicated in Table 4, the greatest percentages of 9, 13, and 17-year-olds in the United States are found in grades 4, 8, and 11 respectively. Since these grade levels provide the highest overlap with the NAEP age groupings, the total state sample required for reporting results by these three grade levels and by the three NAEP age levels would be smaller than for any other combination of three grade levels. In addition to providing for assessment cost economies, these grade levels do represent key stages in the elementary and secondary education ladder. In order to increase the validity of the Minnesota and NAEP comparisons, Minnesota will follow the NAEP plan of collecting assessment data on 13-year-olds and 8th graders in the fall (October-December), on 9-year-olds and 4th graders in the winter (January-February), and on in school 17-year-olds and 11th graders in the spring (March-April). NAEP also collects data on, and reports results for, 17-year-olds who are out of school and young adults age 26-35. Minnesota, realizing the expenses involved in collecting data from these students and young adults, has given these options a low priority for the assessment program. C.\nA reporting variable is defined as a primary characteristic or set of characteristics that serves to define the group of students for which information is desired and for which output measures are to be reported. Each reporting variable has reporting groups. For example, sex is a reporting variable; males and females are reporting groups. A second consideration with respect to delineating reporting variables involves the precision of the output estimates being reported. When splitting the total state sample into reporting groups, the sample size of each group must be of sufficient size to make precise estimates from the sample for each population group to be analyzed and reported. The number and nature of. the reporting categories included in the program thus.controls the number of students to be assessed, a factor which greatly affects data collection costs. Each additional reporting category also adds to the assessment costs by increasing the time and effort involved in questionnaire development, data analysis, report writing and reproduction, and the dissemination of results. Since an exact determination of the amount of funds available for the Phase 1 assessment has not yet been made, it was not possible to make a final selection of the number and type of reporting categories. These categories are thus discussed in broad general terms, and their reporting groups have not been specified. Precise definitions would be made when Phase 1 becomes operational and exact funding constraints are known. The final specifications of these reporting variables must be completed, however, prior to the design of the questionnaires for collecting the supplementary student and school background information required to place, students in the proper reporting category. B.\nThe general exercise development tasks and the sequence of their performance are as follows: Define the broad state goals and translate them into measurable, operational and behavioral objectives.\nA field survey approach analogous to that used in the ongoing NAEP Program .These tapes will be paced to allow the proper amount of response time for each exercise. All information on the paced tapes will alsobe printed in the exercise booklets and student questionnaires. Students will mark their responseS directly into the packages. These paced tapes help to assure the uniformity of administration by different .exercise administrators and by the same exercise.administrator from one administration to another. They also help to prevent exercises for subject areas other than Reading from measuring reading ability instead of the subject area being assessed. The rse of this field survey approach to collect data from a statewide sample of students also minimizes disruptions to the daily instructional routines of participating schools. As previously indicated in Chapter 3.(Section II.B.), field survey operations will.be conducted under the geneial supervision of the Head of the Survey Operations Section. Two District Supervisors and eight Exercise AdminiStrators will be responsible for data collection. These personnel will form two. teams, each consisting of a District Supervisor and four Exercise Administrators. A team will be assigned to a geographic area which would constitute approximately one-half of the work load for each six-week exercise administration cycle. The general job requirement's for each of theSe positions were given in Chapter 3. Ensuing discussions as to how each survey team and each school in the sample would participate in data collection will provide a better understanding of the job skills required for these positions. The Head of the Survey Operations Section would be responsible for spelling out' the entire data collection process in a training manual, conducting training classes, and arranging work assignments and schedules for the District Supervisors. The remainder of this section is directed specifically to the Phase I assessment; however, the operations for each subsequent fall, winter, and spring data collection cycle would be similar. The Phase 1 administration schedule requires that the Reading exercises be available in a form ready for printing by mid-February 1973 at the latest. B.\nInformation will be gathered'on machine readable/scorable forms in order to minimize errors inherent. in reproducing or transferring data to magnetic tape files . for computerized retriev:l. However, editing and error resolution activities' that involVe more than one individual and require various levels of judgment will take place during several stages in data processing, both before and after the exercises have been scored. As mentioned earlier, it is important to safeguard the rights of should catch obvious omissions and errors. A \"spot check\" edit against the same instruction sheet will also be performed'by the District Supervisor as he collectS the booklets and questionnaires from eachExercise Administrator.. Following these two edits, the data would be delivered by the District Supervisor to the Department of Education for receipt control and final editing prior to being shipped to the scorer. As these materials are logged in for receipt control at the Department of Education, they will be given a third Scan edit for completeness. Follow-up activities would be initiated with schools for missing and/or incomplete materials and errors which cannot be reconciled by the central staff. All necessary coding on all questionnaires and booklets will be checked and, if necessary, altered to assure machine readability. After the return of all assessment Materials has been satisfactorily completed, they will be packaged and shipped to the scorer for scoring and data reduction. Note that the structure of the data.in these files precludes anyone from associating a student name, or the name of the school that he attended, to an individual -student data set. That is, each school data set and each data set for a student in that school will be coded by '' same number. These numbers, though they are unique,' cannot be related; by name, t, i specific student or school. C."}, {"section_title": "Subject Matter Areas Covered", "text": "The subject area coverage for the Minnesota assessment plan as outlined in Table   2 is initially limited to the ten NAEP subject matter areas. These ten subject areas The decision as to which subjects are to be assessed when was greatly influenced by NAEP policy regarding the release of exercises for state and/or local use. It is desirable to use these exercises as soon as possible after they have been released in order to better insure their content \"currency,\" as well as to reduce the time lapse between NAEP and Minnesota comparisons. NAEP exercises are usually not released before the middle of the school year following the school year in which they were used. Given the time required to screen these exercises for taeir relevancy to state objectives and to develop supplemental exercises, it is not feasible to plan to use the NAEP released exercises sooner than two years after they have been used in National Assessment. Hence, the Minnesota assessment schedule in Table 3 is generally two years behind the NAEP plan (refer to Table 2).  D."}, {"section_title": "Testing Cycle", "text": "By following each grade level diagonally in Table 3, it can be noted that the One can also compare the .performances of 4th, 8th, and 11th graders during the same year and on the same subject area. However, keep in mind that these groups are assessed at different times within the school year and such comparisons would have to be interpreted with this point in mind--especially when common exercises are given to different grade or age level groups. V."}, {"section_title": "REPORTING VARIABLES", "text": "A."}, {"section_title": "Candidate Variables", "text": "The major reporting variable is the state as a whole. It is these results which will be used in making the Minnesota versus NAEP National and Central Region comparisons. Figure 1 contains a map of the state outlining the eleven planning regions that could be used in defining the primary reporting variables for within the state. Ten potential reporting groups are constructed by combining planning Regions 1 and 2 to form one reporting region, with the nine remaining planning regions each serving as a reporting region. This ten region group was selected over the alternatives of using each of the state's 435 school districts as a reporting group, or of further collapsing the eleven planning regions into only five reporting groups. As discussed in detail in Chapter 5 of this report, the state sample will be selected to insure that the results reported for each of these ten regional groups have a adequate degree of statistical precision. In addition, using these ten regions as a stratification variable is a proper sampling strategy in that it also generates a state sample that is large enough to provide enough statistical precision for reporting statewide results by 20-25 other reporting variables--even though the Department of Education might decide at a later date not to use the ten planning regions as reporting groups. However, in order to maintain sufficient statistical precision, the number of reliable reporting groups within any single reporting variable, or within any combination of reporting variables, would not normally exceed eight (the true limiting factor would be the sample size for the group which has the smallest sample); e.g., one could report results by sex and four levels of SES. Type and degree of participation in extra-curricular activities. g. Race. 2."}, {"section_title": "School or School District Variables", "text": "a. Public and/or non-public status (use three reporting groups: Public and non-public together; public; non-public). b. Type and size of community in which school is located. c. Principal's perception of adequacy of personnel of his school. d. School district enrollment. e. School enrollment. f. Teacher/student ratio. g."}, {"section_title": "School fiscal categories.", "text": "h. Principal's perception of the adequacy of his school's resources. i. Regional location in terms of the ten planning regions described above. j. Racial composition of student body. k. SES measure of student body. 1. Percent teachers with graduate degrees or with varying years of teaching experience. 3."}, {"section_title": "Process Variables", "text": "a. Group students by special educational processes in the measured subject areas; e.g., code emphasis versus meaning emphasis in Reading, or traditional versus modern approach in Mathematics. b. Group by current and previous participation (within previous 3 or 4 years) in special remedial or nonremedial programs (e.g., Title I programs). VI."}, {"section_title": "THE PIGGYBACK OPTION", "text": "The piggyback option is defined as giving local school districts the opportunity 1ais judgmental information could be used by educational decisionmakers (includes lay people as well as professional educators and legislators) to identify areas of concern and commendation within the state's educational program. A general approach to obtaining relevant desired outcome information could involve obtaining estimates from Minnesota citizens (parents, students, and educators) as to the percent of the state's students who should be able to answer correctly each exercise used in the assessment. These estimates could be collected at the same time that the assessment exercises are being administered to students. In addition to getting a desired outcome measure for each exercise, it might also be desirable to obtain similar estimates on groups of homogeneous exercises that can be clustered into a major theme or major skill area; e.g., to be able to state as a desired outcome that fourth graders should be able to correctly answer 10 of the 12 exercises related to the \"Initial Consonant\" theme in Reading \"Word Attack Skills,\" or to state that Minnesota fourth grade students should be able to answer correctly 15 of the 20 exercises that measure Reading \"Word Attack Skills.\" The Technical Advisory Committee, however, posed several problems that are involved in collecting and summarizing the desired outcome data under such an approach. Some of these problems, though their solutions would be fairly time consuming and costly, could be solved by employing methodologies and procedures which are rather straight- The Technical Advisory Committee thus recommends that the initial \"desired outcomes\" effort be limited to a mall-experimental, pilot approach to the undertaken in conjunction with the Phase 1 assessment. 17-year-olds enrolled in school). Since state educational planning is currently based more on data grouped by grade levels than by age levels, the Minnesota plan also provides for an assessment of students in grades 4, 8, and 11. Two subject matter areas are to be assessed annually at the three age and three grade levels; after five years, the same assessment cycle would be repeated in order to provide an evaluation of educational progress throughout the state. State assessment data would also be collected as per the NAEP model by trained exercise administrators, using paced tapes to standardize the administration procedures. The utility of the assessment results depends upon how groups of students are defined for reporting purposes. The state as a whole is the major reporting variable. These state results will be compared to those of the Nation and Central Region. "}, {"section_title": "INTRODUCTION", "text": "The ten-year assessment program recommended in the previous chapter of this report involves the annual collection, analysis, and dissemination of a substantial amount of educational data. That is, with the exception of the first and last years of the assessment cycle, Department of Education personnel will be simultaneously involved in collecting data for the current year's assessment, analyzing and disseminating the results for the previous year's assessment, and preparing for next year's assessment (e.g., developing exercise items and finalizing the data collection strategy). The recommended program, by following the NAEP model of collecting data on 13-yearolds in the fall (October-December), on 9-year-olds in the winter (January-February), and on in-school 17-year-olds in the spring (March-April), does serve to reduce the data collection work load by distributing it over the school year. An assessment program of this magnitude entails a wide range of work tasks, many of which require highly specialized technical and professional skills. This chapter presents a brief description of these required work tasks and desired staff competencies, a plan whereby staff resources could be effectively organized and managed, and a general strategy for building and developing this assessment staff within the Minnesota Department of Education. II.\nThe basic purpose of a dissemination plan for statewide assessment results is to insure that accurate information is made available to all interested people in the state, at a level of sophistication (detail) commensurate with their background, needs, and purposes. This information is needed so that people at all levels can evaluate properly the need for change, so that they have enough information of the right kind and type to make intelligent and data-based educational decisions. Thus a legislator or high-level policymaker would have a far different background, need, and purpose for assessment results than would a housewife with children in school. Too little information in the former case would be deleterious while too much in the latter case would provide unneeded, unwanted, and perhaps misunderstood data. This means, then, that report formats, audio-visual aids, presentation modes, and any other form of promulgation and publication decided upon and used should be tailored to some degree to decisionmaking groups, special interest groups, and the general public. The final report of the Minnesota Educational Assessment Program for each year or cycle may be a somewhat bulky, detailed, statistical narrative which should be interpreted for a wide variety of potential audiences. There are at least four levels which should be considered in understanding the various needs of the consumers of assessment program results. The policymaking level-consists of legislators and members of the executive branch of the state government who are charged with the responsibility for establishing broad policies which, when carried out, will best meet the educational needs of the state. For example, policymakers may decide on the basis of statewide results that a major emphasis should be put on Citizenship rather than some other area which has been receiving emphasis but which exceeds expected levels. The decisionmaking level is comprised of personnel throughout the state in the Department of Education who are charged with carrying out policy through making the basic operational decisions and allocations of resources. These are the various District Superintendents, curriculum directors at the state level, curriculum specialists at various levels, and principals of schools. In the Citizenship example, decisionmakers would be concerned with finding and recruiting appropriate specialists in Civics and other social sciences, developing goals and objectives for new areas of instruction, and devising curriculums to meet these new objectives. The assessment results would provide these decisionmakers with some insight into specific areas on which Citizenship emphasis should be brought to bear. Third is the operational level, the \"on the street\" level in which educators work actively with children and parents, face the very real problems in modern education, and carry out all the policies and decisions which have been made. These are teachers, school-based curriculum and guidance specialists, and other teaching staff. People at this level are devising and carrying out lesson plans and classroom projects, and bringing outside experiences unique and meaningful to their own groups of students. The broad range of Citizenship needs revealed in the statewide assessment, for example, would have to be translated into concrete behavioral terms both for this level and by this level. The artifactual division or categorization into levels does not mean, however, that certain levels can have access to only certain bits of information interpreted in certain ways. Rather it means that disseminators must be alert to their needs and be ready to respond with appropriate detail when queried. Under the alternative selected by Minnesota of three grade/age levels, two subject-matter areas, and ten reporting regions for the assessment program, the spring 1973 (Phase 1) assessment can be considered as a gearing up effort, inasmuch as only one area and age group--Reading for 17-year-olds--is being assessed. Subsequent years and cycles (Phase 2) will have the full scope of three grade/age levels and two subject matter areas. Hence, for dissemination purposes, spring 1973 should also be viewed as developmental in that the strategies tried for effective dissemination could be changed (if required) for subsequent assessments. As a final comment, there is a difference in the desired impact of assessment dissemination and publicity in the beginning as compared to the end of a year or a cycle. The assessment program must be \"sold\" in different ways. Hence, the primary vehicle for wide dissemination and publicity is a series of Fall Workshops that consist of a results dissemination phase for just-completed administrations, and a planning and publicity phase for to-be-completed administrations. The exception to this will be-.Phase 1 which will have only a pre-assessment workshop."}, {"section_title": "MANAGEMENT AND STAFFING PLAN", "text": "A."}, {"section_title": "Staffing Plan", "text": "The general work tasks involved in the annual assessment operation are listed in Table 5. A wide range of staffing plans could be designed for performing these tasks. Minnesota could, for example, build an assessment staff that would have the capability of completing all assessment tasks \"in-house\"--or, at the other end of the spectrum, the state could elect to minimize staffing requirements by contracting with an organization to provide a \"turn-key\" assessment; i.e., completing all tasks outside the Department of Education. One could support the extreme \"turn-key\" approach by arguing that, instead of building a large staff of assessment technicians, the state should gear its staff requirements more toward the utilization of assessment findings to better assure that indicated program changes are designed and implemented. The ineffectiveness, Determine broad goals for education in the state."}, {"section_title": "2.", "text": "Define the broad state goals in operational and behavioral terms so the degree of attainment can be measured. Information Needs Prepare new exercises as required. 3. Conduct tryouts and finalize new exercises.\nReduce data on magnetic tape files for analysis. Sampling and Data Analysis 1. Design and select sample.\nSpecify format of magnetic tape files. 3. Analyze data.\nPrepare Highlight Reports. (continued) Design and implement pre-assessment strategy to gain public awareness, understanding, and support of assessment program.\nDesign report formats for different types of audiences."}, {"section_title": "4.", "text": "Design necessary school, teacher, and pupil questionnaires. Data Collection (Survey Operations) 1. Package exercises and print exercise booklets. 2. Print questionnaires.\nContact schools.\nDesign AV aides for disseminating results."}, {"section_title": "3.", "text": "Hire and train field administration staff.\nPrepare news releases."}, {"section_title": "5.", "text": "Distribute materials.\nDisseminate results to various decisionmaking groups, special interest groups, and general public. from a cost viewpoint, of hiring and retaining full time staff members who possess the variety of specialized expertise that is needed at infrequent intervals throughout the year is another effective argument against the \"in-house\" extreme. Arguments often used to oppose the \"turn-key\" extreme include the availability of funds for new staff positions as opposed to the nonavailability of contract monies, or the possibility of reducing assessment costs by developing a large and capable \"in-house\" assessment staff. One could pose similar arguments for the many combinations of \"staffing and contracting\" approaches that might lie between these two extremes. In effect, the most desirable management/staffing plan for a given state will depend to a large extent upon the state's policies, resources, geography, labor market, etc. (Minnesota, for example, is currently operating under a \"freeze\" policy that prohibiti the hiring of additional staff.) The Minnesota Department of Education has expressed a desire to move toward the development of an \"in-house\" assessment capability--even though the development of this capability might extend over a period of years. Given this charge, the management and staffing plan, as depicted in Figure 2, represents the minimal staffing requirements for the adequate completion of the work tasks listed in Table 5. The functions of the advisory staff and the general duties of key personnel are briefly described below. These general descriptions, used in conjunction with the methdology discussions presented in subsequent chapters, provide a good profile of the skills and competencies required in key staff positions. 1.\n"}, {"section_title": "6.", "text": "Collect data."}, {"section_title": "7.", "text": "Edit data. Scoring and Data Reduction 1. Scoreexercises (including open-ended)."}, {"section_title": "Report Preparation", "text": "1. Prepare Technical Report."}, {"section_title": "Director, State Educational Assessment", "text": "The director is responsible, for the overall management of the program. The director, in addition to maintaining close contact with the \"line\" staff, must anticipate those decision points which will require technical or special inputs from the advisory staff and make sure that these inputs are made at critical decision It is also essential that the director stay in touch with 1 ... legislators interested in education, leaders in state government who are interested in education, leaders among the local schools, and the leaders among the lay public who are interested in education. The purpose of keeping in touch is to sense the educational problems, the kinds of concerns about education, the notions that are held that may be true or false, and other-things in the general climate of the environment to suggest the strategic areas for assessment, the kinds of information that will need to be provided to deal with the concerns and apprehensions, and the people to be involved in order to get-support for an ongoing program. [Ref. 7] This information, along with the progress or lack of p: ess being made in the ongoing program, can be used in planning modifications to the long-range program. The director is also responsible for the technical planning of the assessment program; i.e., determining the technical procedures required to perform the assessment tasks. These technical procedures would, in turn, lead to the development of a step-by-step outline of specific jobs to be performed, as well as to suggest additional types of competencies that should be included on the advisory staffs. 2."}, {"section_title": "Advisory Councils", "text": "Both of these councils, the Assessment Advisory. Council (AAC) and the In addition to being responsible for those tasks listed in Table 5 under \"Instrumentation,\" this person would assume a leadership role in defining broad state goals in operational and behavioral terms .so that the degree of attainment can be measured. "}, {"section_title": "Head, Survey Operations Section", "text": "The head of the Survey Operations reports directly to the Director and is in charge of all aspects of data collection for those schools selected in the assessment sample. The specific work tasks for which this section is responsible are listed in Table 5 under \"Data Collection.\" This position requires expertise in survey operations and field interviever management, as well as knowledge of strategies for initiating and sustaining willing cooperation among participating schools. The head of this section will be assisted by a clerk/typist, two District Supervisors, and eight Exercise Administrators. "}, {"section_title": "Head, Dissemination Section", "text": "The dissemination of pre-assessment information and the dissemination of assessment results are key functions in assessment programs. The goal of this section is to put information into the hands of those who will make use of it. In addition to possessing a \"public relations\" personality, the head of this section should have a background in mass communications, speech, and journalism. This person will be assisted by a clerk/typist in performing those tasks listed under \"Dissemination\" in Table 5."}, {"section_title": "8.", "text": "Head, Scoring, Data Reduction, Sampling, Data Analysis, and Report Preparation Section This staff member must be able to wear many hats. He is responsible for the tasks listed in Table 5 under the \"Scoring and Data Reduction,\" \"Sampling and This section would be supported by an educational research analyst, an educational sampling statistician, a research assistant, and two typists. Use the established advisory staff."}, {"section_title": "Student Questionnaire", "text": "These questionnaires (one for each of the three grade/age levels) are to be included in the appropriate exercise package to provide for the collection of background information on each student included in the sample. These background data will be used to group students as per the reporting variables previously "}, {"section_title": "School Questionnaire", "text": "This questionnaire would be designed to collect background information on each school in the sample and would be filled out by the school principal. School questionnaires would be used to collect information on such school factors as: public or non-public status; enrollment data; per7pupil expenditure data; average teacher salaries; regional location; racial composition of student body; type of community in which school is located; school staff background data; and the principal's perceptions of the adequacy of his school's facilities and staff. 3."}, {"section_title": "Desired Outcome Questionnaires", "text": "If the desired outcomes concept is included in Phase 1 of the assessment program, the type and number of questionnaires required to collect relevant information will depend upon the nature and magnitude of the pilot approach to be undertaken. Pending such a decision, the brief discussion presented in Chapter 2 (Section VII) of the technical problems involved in implementing this component should suffice to provide a general description of the information to be collected by these instruments. B."}, {"section_title": "Questionnaire Development", "text": "Four major steps are involved in the development of the three questionnaires discussed above: Delineate the variables and data elements to be included in the questionnaires."}, {"section_title": "EXERCISE DEVELOPMENT", "text": "A."}, {"section_title": "Definition of Behavioral Objectives (Task 1)", "text": "Objectives written in behavioral terms are necessary to the production of good A revised set of objectives should be developed on the basis of these reviews. After these reformulated objectives are once again reviewed, they can be finalized. Completion of this task is a prerequisite to exercise development. However, it is expected that the \"exercise development\" team (i.e., the contracted exercise writers and assembled working groups of school personnel) will be required to \"polish up\" these objectives prior to beginning their work on Task 2. C."}, {"section_title": "Review of NAEP. Materials (Task 2)", "text": "Given the lists of behavioral objectives for Minnesota and for NAEP, those released NAEP exercises which are best suited to the needs of Minnesota must be selected. This review process could be conducted by groups of subject matter specialists only, groups of lay persons only, and groups containing both subject matter specialists and lay persons. The results of these reviews would then be presented to the staffi of the Instrumentation Section. The initial step in this review process is to screen out those exercises developed to measure NAEP objectives which are not compatible to Minnesota objectives. After this step has been completed, each exercise should be screened in greater detail. Some of the key factors to be considered in further reviewing these exercise items are presented below. [Ref. 6] 1."}, {"section_title": "Offensiveness", "text": "All potential exercises should be reviewed for their potential offensiveness to the Minnesota public. The involvement of lay people in this process is essential. Since all NAEP materials are subjected to a similar review, one would not expect to find anything among them that would be offensive to Minnesotans. However, the NAEP criteria for \"inoffensiveness,\" as covered in pages 42-46 of Reference 6, should be reviewed. "}, {"section_title": "Ease of Administration", "text": "Some NAEP items are designed to be administered to individuals; most are designed for group administrations. Minnesota would probably want to either exclude the \"individual\" exercises or adapt them for' group administration because of the additional costs involved in individualized administration. (Caution should be exercised when interpreting the results for any NAEP versus Minnesota comparisons that are made on those exercises which were individually administered by NAEP, but were modified by Minnesota and administered to groups of Minnesota students.) Some ofthe NAEP group exercises might also be excluded because of the equipment and materials required in their administration. 4."}, {"section_title": "Content Validity", "text": "The content of each NAEP exercise should be examined in terms of whether or not it is assessing something important and desirable for Minnesota children to know, and whether or not it is measuring the objectives for which it was-intended."}, {"section_title": "Content Appropriateness", "text": "Each exercise should be examined for its appropriateness for the age and grade.levels for which it is being considered. For example, is the exercise stated so that the student will understand what he is to do? Is the vocabulary appropriate? D."}, {"section_title": "Identification of Gaps (Task 3)", "text": "A comparison of the results for Tasks 1 and 2 serves to identify the \"exercise\" gaps to be filled. However, this task is not quite that straightforward in that the size of this gap must be weighed against the availability of resources. If the gaps cannot be filled with the resources available, the results of Tasks 1 and 2'should be reevaluated in order to establish priorities for Task 4. This review could also serve to identify for further considerations those NAEP exercises that were excluded on the basis of \"borderline\" decisions; i.e., some of these exercises could possibly be used to plug up the gaps that cannot be filled with existing resources. The review panels and groups involved in Task 2 should also be involved in this task. E."}, {"section_title": "Development of Exercises (Task 4)", "text": "This critical task consists of five essential components: Development of prototype exercises."}, {"section_title": "GENERAL SAMPLE DESIGN CONSIDERATIONS, PRINCIPLES AND REQUIREMENTS", "text": "A. Other statewide educational assessments may provide useful data in making these approximations."}, {"section_title": "D.", "text": "The Concept of a \"Design. Effect\" The concept of a \"design effect\" is most important in the evaluation of the expected statistical precision of the alternative sample designs.1 For a two-stage sample design, the standard error of a p-value is increased over that expected from . simple random sampling by virtue of the \"clustering effect\" built into the sample design. That is, pupils' within the same school (i.e., clustered), usually respond or perform more nearly alike than students in different schools. However, on the other hand, the stratification.process usually increases precision. If the allocation of the sample, to the strata is,so far fifrom optimum that large weights are required for unbiased estimates; sampling variability may also be increased. The net result of' all the.factors (e. g., clustering, weighting, and stratification) is measured by a'design effect index which is defined as the ratio of the sampling variance of the p-value for the sample design actually used (e.g:, two-stage sampling) to the variance of thep -value that would be obtained from simple random sampling. This can:be expressed as: A p-value is a statistic estimating the proportion of students who respond correctly to an assessment exercise. The standard error of a p-value is an estimate of the variability of the sample p-value 'in repeated sampling with a fixed sample size and sample design."}, {"section_title": "Design Effect -", "text": ""}, {"section_title": "Sampling Variance of p-Value for Two-Stage Sample Design Sampling Variance of p-Value for Simple Rarlom Sampling", "text": "Design effect values are usually greater than 1.00 indicating that the sample design actually used produces less statistical precision than simple random sampling. However, when the statistical precision of the sample design is considered relative to its cost, it is possible for a sample design with a design effect greater than 1.00 to be "}, {"section_title": "Requirements of'the Sample Design", "text": "In general, the requirements of the sample design are: 1) The sample should be a probability sample. That is, each.student in a given age class or grade in a.public or non-public school in the State of Minnesota should have a known positive chance of inclusion in the sample. -2). Each of the ten geographical reporting regions of tha state should be represented in the sample so that results can be reported for-them with nearly equal statistical precision. ."}, {"section_title": "Introduction", "text": "The sample designs discussed will be those for the 17-year-old and eleventh grade populations for the school year 1972-73 since it is planned that this "}, {"section_title": "Stratification of the School Population", "text": "The requirement that results be reportable by each of the ten reporting regions is a sample design specification that will be met through stratification. Figure 1 of Chapter 2 gives .a map of the State of Minnesota which geographically defines the ten reporting regions. Notice that planning regions one and two have been pooled together to form assessment reporting region.one. For the 17-year-old population and for the 1972-73 school year, Table .6 gives a description of the population fOr the ten reporting regions. The first step in the stratification process will involve grouping the schools into ten groups using the reporting region in which the school is located as a criterion. The next stp of the school stratification process will involve grouping the schools within each of the reporting regions by school level variables which are expected to relate to educational performance. Some candidate stratification variables that will be considered for this purpose and that will be used either individually or in an index are the following: State Income Tax Return Data by School District."}, {"section_title": "5Y", "text": "Type of Community in which School is, Located. . Variables available froM,the 1970 Census Tabulated by School District. Two sample allocations across the ten reporting regions will.be.considered. The first allocation consists of allocating the -sample'students tO,the regions in proportion to the total number of .students in that grade/age in the region. That is, smaller regions (fewer students) will have smaller samples and larger regions will have larger samples. Column 3 of Table-7 giveS the proportional allocation See Figure 1 for mapshowing definition'of planning regions."}, {"section_title": "2/ --", "text": "The source of the 17-year-old count data was published by State of Minnesota Planning Agency Office of Local and Urban Affairs, titled.\"Age and Sex of-MinnesotaPopulation.\" The source of:their data was 1970 U.S. Census. This data-was tabulated by State, the Eleven Planning Regions, and the 87 counties, of the State."}, {"section_title": "3/", "text": "The data for this column was tabulated from a computer printout supplied to RTI .by'the Minnesota State Department of EduCation dated September 28,.. 1972, listing all public schools in the State. where 5100 is the total sample size and 75,531 is the total number of 17-year-olds in Minnesota as given in Table 6. Column 5 of Table 7 gives the standard error of a p-value assuming a constant p-value and \"design effect\" from reporting region to reporting region.-Observe.that the expected standard errors in column 5 range from .0108 to .0391 while the expected standard error of a p-value for the state estimate is .00725. It-can be seen that allocating the sample proportionally to the ten reporting regions does not meet the sample design requirement that results of nearly equal statistical precision be reported for each of the ten regions. To meet this need, an alternative to the proportional.allocation, called disproportional allocation, is needed. The effects of disproportionate sampling on precision are shown in Table 7. The formula for the values of the standard errors of column 5 of Table 7  For region I, the standard error would be .00125 =,.0351. The formula to calculate the standard errors of column 8 of Table 7 is the same as that given in the Equation in the footnote on the previous page except for the value of the standard error of the p-value for the state (the number in the last row) which requires a modification due to the varying proportions of the sample in the populatiOn (note that the p-value and \"design effect\" are assumed to remain Constant from reporting region to reporting region)-. The major characteristics of this allocation relative to the proportional allocation is that the range of the expeCted standard errors over the ten reporting regions has reduced considerably. from [.0108 to .0391] for proportional allocation to [.0140 to .0258] for the disproportional allocation. Under the disproportional allocation, we expect the standard error of all regional-results not to exceed .0258 compared to-.0391 for proportional allocation. In addition, the standard errors for each:reporting region for disproportionate allocation are in most cases smaller than those for proportionate allocation. There is a tradeoff, however, in that as the regional estimates become more precise, the overall state estimate becomes less precise. In the present situation, the p-value for the state estimate increases from .00725 to .00805, an increase of 11.percent. The loss in precision in absolute magnitude .000E0 (.00805 minus .00725) is quite small and is not alarming from a practical viewpoint. Consequently, it is recommended that the disproportional allocation be u d in the Minnesbta Educational Assessment sample design. As discussed i.1 Section II, the important factors in determining the number of pupils to be selected per school are: Variability of p-values from school to school."}, {"section_title": "Stratification Sample and Stratification of Pupils Within Selected Schools", "text": "The final step of the sample design will consist of stratifying the list of pupils supplied by each school into groups or strata so as to adequately spread or control the sample across grade and age level groups. For example, in assessing the 17-year-olds and eleventh grade populations, the following four strata would be constructed and randomly sampled: 17-year-olds in 10th grade or less 2) 17-year-olds in 11th grade 3) 17-year-olds in 12th grade 4) 11th graders who are not 17-year-olds. Strata 1, 2, and 3 would be sampled for 17-year-olds while strata 2 and 4 would be sampled for llth graders. The allocation of the sample to the strata will be resolved after it is decided what statistical precision is expected for each of the two populations (17-year-olds and eleventh graders)."}, {"section_title": "E.", "text": "Summary of the Phase 1 Sample Design Figure 4 graphically describes the sample deSign in its hierarchical or nested structure. The sample design consists of the following steps: * Step 1 Group the schools by the ten geographical reporting regions. Step 2 Split the schools of each reporting region into homogeneous groupings with respect to educational performance using the best data that is economically available. The number of groups to be formed is specified in Table 8. Step 3 Select a random sample of two schools from each school group or strata formed in step 2 using a table of random numbers. Step 4 Group the pupil list of each selected school into four strata defined by age and grade level. Step 5 Select a random sample of pupils from each age.group by grade level strata within the selected school using a table of random numbers. This sequence of steps assumed that the sampling frame or list of all schools, containing relevant stratification and sample selection data, has been constructed. information is to be gathered by survey teams, will these teams be comprised of state or contracted personnel, or some combination of the two? When will assessment data be collected? Will completed exercise booklets be optically scanned? What edit checks will be performed and how will errors'be resolved? Will the assessment data be.stored on magnetic tape files? Answers to these and similar questions were also evaluated with respect to resource constraints (time, funds, and personnel). As a result, the approach to data collection and processing as outlined in this chapter is recommended as the most cost-effective method for collecting quality This data collection and processing plan presupposes that preassessment conferences and workshops for representatives of participating schools and districts would be conducted by the Minnesota Department of Education and/or the contracted agency responsible for data collection. The primary purpose of these workshops will be to brief the representatives on the history, goals, 'purposes, and procedures of the assessment plan. II."}, {"section_title": "DATA COLLECTION", "text": "A."}, {"section_title": "School Involvement", "text": "The field work would be scheduled and conducted so as to minimize school disruptions and demands imposed on school personnel. Every attempt would be made to collect as much data as possible from state and federal sources: i.e., those data routinely collected on reports, questionnaires, and surveys. Nevertheless, the following would be required of those schools selected in the Phase 1 assessment sample:  ) This information would be required in early February and would be used to draw the student sample."}, {"section_title": "Package Administration", "text": "The collection of assessment data from students within each school will be done primarily by Exercise Administrators who have been hired and trained by the Exercises would be administered to no more than 20-25 students, In the event that the number of assigned sample students exceeds this number, additional group sessions will be scheduled. The Exercise Administrator will also ha've specific instructions for scheduling make-up sessions if a minimum'number of students are'not available f6r testing. The.Exercise Administrator will leave the school questionnaire with, and explain it.to, the principal:On the same day that the exercises are scheduled for administration in:his school. This will be done prior to the administration.of exercises to the students. The completed school questionnaires are to, be picked up from the principal on the same day, after the exercise administrations have beewcompleted. After the administration, the Exercise Administrator will (1) code certain information on the completed exercise packages, thing identifying students by name sbould not be removed from the school premises. Only those students assigned to the sample will parLicipate in assessment. Other students will not be permitted co.see the assessment packages."}, {"section_title": "III.: DATA PROCESSING", "text": "A."}, {"section_title": "Error Resolution", "text": "The resolution.of errors identified in the editing process may take many forms, depending on the nature of the error and the editing stage in which it was noted; e.g., all errors noted by the Exercise Administrator at the test site should be corrected, whereas the time and expense involved in correcting an error noted at the scoring site might result in deleting the specific data item(s) in question. In addition,., the scoring and developing of the files should be conducted so that those schools falling in the sample with an early closing date are scored and edited first. ThiS allows for the resolution of any type of errors, particularly those of a critical nature, which must be resolved before the closing of schools. The, standard procedure for making all types of corrections would initially be the use of the source document (i.e., the exercise booklet and questionnaires) to determine what data should be appearing in the record. In addition to this initial _ check, the resolution of suspect data will generally take one or more of the following forms: Follow-up action with data\" source and/cr respondent."}, {"section_title": "SUMMARY", "text": "Data for the assessment program will be collected by specially trained survey teams using a \"one day in--one day out\" approach. As such, these data collection and processing procedures assure a high degree of cooperation and a great degree of quality control with respect to both sample selection and data collection. The ensuing discussion will be rather general with respect, to specific independent variables and combinations of independent variables to be used in the analysis since a final selection of reporting variables is still pending. In addition, the results of the preliminary analyses of the sample sizes in the various subgroups defined by the reporting variables will be important in selecting the various combinations of variables that can be used meaningfully-as independent variables. A final selection-of two-way and three-way tables to he presented should be based on their potential use in the explication of results andor their general idformational value."}, {"section_title": "*", "text": "More specifically, a p-value is an estimate of the total number of students in a group in terms of a population count who would have given an acceptable responsi,; divided by the estimated total number of students'in the group. Multiplied by 100, a p -value gives the percent-correct._of the exercise. Notice that.a p-value is not derived by dividing the number of respondents in the sample that giVe acceptable responses by the total sample size because all'p-vaques are calculated using weighted responses based on the chance each respondent was selected into the sample."}, {"section_title": "C.", "text": "Comparisons with National Assessment .Exercise p-values for various grotps-of Minnesbta students would be contrasted with analogous groups of students in the Nation and in the Central Region. Such contrasts can be made only for those released NAEP exercises that are adopted by Minnesota. P-values for exercises that were administered by NAEP in individual sessions should only be compared with extreme caution to MinnesOta p-values that were obtained by group administrations of the same exercises. The sample is designed to provide Minnesota versus Nation and Minnesota versus Central Retion comparisons for students at three age levels; i.e., 9, 13, and in-school 17-year-olds. In addition, the sample would he large enough to provide Minnesota versus NAEP. comparisons for subgroups of 9, 13, and in-school 17-year-olds defined bysex, by four levels of parent's education, and by size of community. If desired outcomes measures are developed for Minnesota in the form of individual exercise p-values, they could also be compared with the Minnesota and NAEP results. Table 9 is \"mock-up Differences between the-p-vo.lus of the state and the nation would also be analyzed for-impoTtant. statistical relationsh_.:1 and noted appropriately. "}, {"section_title": "Within Minnesota Comparisons", "text": "These analyses Would be .concerned with computing and contrasting p-values for the various' groups of students within Minnesota as defined by the reporting variables. TWo levels of analysis would be used in making these comparisons. The first. of these is similar to the-analysis discusSed in the previous section for making Minnesota versus NAEP contrasts, except that these comparisons will'be made for':the reporting groups within Minnesota._ The first level of analysis is primarily descriptive in character and involves the calculation of p-values and their respective standard errors for each exercise for the various reporting groups. In addition, analyses will be performed for evaluating the statistical relationship of certain comparisons across the groups of a reporting variable and between exercises within a reporting group. At the second level,. relationships between the achievement measures (exercise responses) and student and school background variables would be investigated by analytic techniques such as Tukey's method of balanced fits. [ Ref. 9] These analytical techniques involve the simultaneous examination of a number of independent variables and are useful for examining the effects of one independent variable while controlling or adjusting for the effects of other independent variables. 1."}, {"section_title": "Level One Analysis", "text": "Weighted p-values and their standard errors will be computed for subgroups of students as defined by the reporting variables. Differences between the p-values of these subgroups will be analyzed. Results for the Minnesota reporting groups could also be summarized by major themes as per a format similar to that shown.previously in Table 10. It is also desirable to report the achievement status of student groups that have been simultaneously classified by any two or more of the previously discussed reporting variables; e.g., race by type of community or type of community by region,, etc. Tables such as Table 12 and 13 can be extremely useful in further pinpointing groups of students who may or may not have particular educational needs; e.g., Whites atteng schools in Small Town/Rural 'communities may score much lower in \"Word Attack Skil., than Whites in the other two Types of Communities. One might also use these tables to.gain additional insight into possible interaction effects between two or aare reporting variables. 2."}, {"section_title": "Level Two Analysis", "text": "Whereas the level one analysis is directed at answering questions of the \"what\" dimension, the leveltwo analysis is directed to the \"why\" dimension. The results of the leVel two analysis will be displayed in tables similar to those of Tables 11 and 12, except that the p-values will have been adjusted for differences in selected school, student; and process variables lother_ than those used to define the particular reporting groups of that table. For example', the results in. Table 11 might       Reading for Facts  "}, {"section_title": "92", "text": "II."}, {"section_title": "PRE-ASSESSMENT WORKSHOPS FOR PHASE 1 (SPRING 1973)", "text": "As soon as possible after the formal approval of the assessment program, and before the actual administration of the Phase 1 assessment exercises in the schools, a series of 6-10 workshops should be conducted across the state for personnel at the decisionmaking level to insure that the goals, aims, and purposes of the program are fully understood. Reporting variables and reporting groups will be explained, as well as the basic elements of the Reading assessment program to be conducted for 17-year-'olds. These pre-assessment workshop& should include a slide and tape presentation so that consistent information would be promulgated. Appropriate officials from the Department of Education first should be trained on the overall assessment program, and then be available in these workshops to answer questions and expand on themes as necessary. Relevant assessment handouts should be distributed, and suggestions should be given for attendees to hold similar workshop discussions with teachers, specialists, and other personnel potentially involved with the program. Press releases should be generated so that the public would be informed. The In addition, there will be extensive tabulations of results, interpretations and analyses of these tabulations, and sufficient technical detail throughout to meet the needs of technically oriented readers. Based upon the preliminary draft of the Technical Report, which should be available three to four months after exercise administration has been completed, a Highlights Report in popular language and format can be drafted. This Highlights Report would be widely disseminated to all schools which participated in the program, appropriate curriculum specialists in reading and language arts, district superintendent offices, state level Department of Education personnel, etc. Included in the Highlights Report will be an order blank for obtaining the Technical Report. This will assure wide availability of the Technical Report on request. The ability of any and all persons to obtain the basic Technical Report is crucial. Misinterpretation of assessment results can occur on the basis of such necessarily condensed information as the Highlights Report will contain. The Technical Report will be initially distributed to such personnel and agencies manifestly having the need and ability to use and profit by it, such as the Commissioner, superintendents, legislative committee members, certain research and evaluation specialists on the Department of Education staff, and AAC and TAC members. It is vitally important that assessment results, in order to have any substantial effect on the educational process, be interpreted in operational terms by and with operational decisionmakers. Therefore, it is proposed that the primary vehicle for interpretive dissemination be a series of Fall Workshops analagous to the pre-assessment workshops. These workshops will be primarily geared to the needs and expectations of operational people in education; hence, assessment results should be interpreted on not only a local basis, but also in classroom and subject matter terms. For example in Citizenship a goal of understanding the rights and freedoms of individuals is by itself too broad to implement. Participants in workshops would have to interpret assessment results on this goal by generating new ideas to teach about, facts to impart, and attitudes to look for. Specifically in Phase 1, Reading assessment results will be worked over with reading, language arts, grammar, remedial reading, and composition instructors, to name but a few. These Fall Workshops will be held either during the pre-school sessions of teacher and administrator preparation before the beginning of each school year, or early in the fall immediately before the first field administration. The first such workshop will be based on the spring 1973 (Phase 1) Reading results for 17-year-olds; and the audio- Conflicting results may emerge which need clarification, e.g., word attack skills may be low in an area where there is no accompanying low comprehension level. Concepts such as these must be clarified and joint decisions made on local actions to ID:. taken where necessary. Nearly equal in importance to dissemination of Phase 1 Reading results at these workshops is the presentation of lead-in material for the subsequent assessments of Phase 2. This part of the workshop would be similar in purpose to the workshops held previously in Phase 1, but geared specifically to the assessment administrations to be conducted next. For example, the Phase 1 dissemination workshop would also introduce the first year of the Phase 2 assessment, i.e., Reading in two grade/age levels and Literature in three grade/age levels. About 6 to 10 such workshops again are envisioned, with perhaps 40 to 50 attendees each, spread over the 10 reporting districts of the state to minimize travel costs and time. Appropriate press releases should be created to publicize the workshops, and the media would be invited to attend. Finally, provisions will have to be made for a formal presentation to the Legislature and/or the Executive section of the government of the State, and other policymaking persons, near the end of each fiscal year so as to justify both the current assessment program expenditures and projected program expenditures. This presentation, though quite similar to the/workshop presentations, would include more data relevant to the types of decisions these bodies find it necessary to make."}, {"section_title": "95", "text": "V."}, {"section_title": "SUNNAPY", "text": "A dissemination plan must be responsive to the informational needs of various levels of consumers of educational assessment results and of educational decisionmakers who will use assessment information. These consumers are individuals at the policymaking level, the decisionmaking level, the operational level, and the public level. All levels must have access to all results; but their specific information needs vary. The Technical Report and the Highlights Report will meet some of these different needs. The primary vehicle for assessment dissemination and planning is seen as a series of Fall Workshops, each of which has a results dissemination aspect and a planning aspect. These two aspects will be split, however, for Phase 1. A series of pre-assessment workshops is proposed in spring 1973, before actual Phase 1 assessment administration, to acquaint the decisionmaking and operational levels with the project. In addition, wide public exposure to the assessment project is suggested through television, service clubs, and special interest group presentations. Actual dissemination of Phase 1 results will occur by means of the basic Technical Report, a widely distributed Highlights Report, and Fall Workshops for decisionmaking and operational persons in the subject area(s) assessed. These workshops will also serve a planning function for the early Phase 2 assessments. Yearly Phase 2 dissemination will be in the same manner: Technical Report, Highlights Report, and dissemination and planning Fall Workshops on subject-matter areas. Each year there will also be special presentations prepared for thq policymaking level."}]