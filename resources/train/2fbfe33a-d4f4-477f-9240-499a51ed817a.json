[{"section_title": "", "text": "A substantial body of evidence has shown large academic test score gaps between black and white students in early childhood (Brooks-Gunn, Klebanov, Smith, Duncan, & Lee, 2003;Phillips, Brooks-Gunn, Duncan, Klebanov, & Crane, 1998;Fryer & Levitt, 2004;Murnane, Willett, Bub, & McCartney, 2006). These gaps remain, and probably grow, as students progress through school (Reardon & Robinson, 2008). Many researchers have sought to explain these persistent test score gaps, and particularly, to understand the role of students' socio-economic status (SES) in the formation of these gaps. The earliest studies of racial test score gaps among young children found that controlling for indicators of SES (i.e. parental education, income, and occupational prestige) reduced blackwhite gaps by meaningful amounts, but sizeable gaps still remained (Brooks-Gunn, Klebanov, Smith, Duncan, & Lee, 2003;Phillips, Brooks-Gunn, Duncan, Klebanov, & Crane, 1998). One study that departed from this pattern was Fryer and Levitt's (2004). Using data from the nationally representative Early Childhood Longitudinal Study, Kindergarten Class of 1998-1999, Fryer and Levitt (2004) (FL from hereon) found large math and reading gaps at school entry, but a small number of SES-related variables explained all of the black-white reading gap and most of the math gap (with a standardized gap of 0.10 remaining). They concluded that real gains by the recent cohort of black students were \"an important part of the explanation\" (Fryer & Levitt, 2004, p. 448) of why their results differed from previous studies. However, using data from National Institute of Child Health and Human Development's Study of Early Child Care and Youth Development, which contained a sample of students from a similar age cohort as those in the ECLS-K: 1999, Murnane and colleagues (2006) were unable to replicate FL's SES finding; they, like previous researchers, were unable to control away gaps with the usual SES variables. Murnane et al. (2006) hypothesized that the discrepancy was due to differences in the assessments used in each of the data sets; while the NICHD study used more general tests of cognitive skills, the ECLS-K assessments focused on academic material taught in schools. It is unclear, then, whether FL's finding was due to general gains in cognitive skills by black students, gains specifically in math and reading skills, higher-quality covariates in the ECLS-K: 1999, or idiosyncrasies in the ECLS-K: 1999 sampling. The recent release of data from the National Center for Education Statistics' (NCES) ECLS-K: 2011 provides an opportunity to test whether FL's results replicate in a new nationallyrepresentative sample over a decade after the first ECLS-K. Additionally, unlike the ECLS-K: 1999, the ECLS-K: 2011 administered executive functioning assessments. This enables us to examine whether, as suggested by Murnane et al. (2006), the role of SES in explaining test score gaps differs depending on the extent to which the assessment measures curricular material."}, {"section_title": "Purpose / Objective / Research Question / Focus of Study:", "text": "Description of the focus of the research. In this study, I use data from the ECLS-K: 2011 to update the picture of black-white test score gaps and gap trends over kindergarten and to determine whether patterns found in the earlier ECLS-K: 1999 replicate. Specifically, I ask: 1) Do measures of socioeconomic status explain black-white math and reading test score gaps in the fall and spring of kindergarten? 2) Do measures of executive functioning show the same patterns of results as measures of math and reading skills?"}, {"section_title": "Setting and Population:", "text": "Description of the research location. The ECLS-K: 2011 is a nationally representative sample of students attending kindergarten over the 2010-2011 school year. In my analyses, I use data from black or white students who had non-missing data for the control variables and for both testing occasions (on all assessments; white n ~ 5280, black n ~1000)."}, {"section_title": "Research Design:", "text": "Description of the research design. The ECLS-K: 2011 is an ongoing study being conducted by the NCES that is similar to its predecessor, the ECLS-K: 1999. For the ECLS-K: 2011, the NCES used a three-stage sampling design in order to efficiently obtain a nationally representative sample of students attending kindergarten in 2010-2011 (Tourangeau et al., 2012). Data collection included parent surveys about the students' home environments, principal and teacher surveys about the students' school environments, and child cognitive assessments in reading, mathematics, science, and executive functioning. The first two rounds of data from the study, which I use here, were released in July 2013 as a restricted-use data file and contain data collected during the fall and spring of students' kindergarten year."}, {"section_title": "Data Collection and Analysis:", "text": "Description of the methods for collecting and analyzing data."}, {"section_title": "Assessments", "text": "I use data from three separate assessments: two subject area assessments (math and reading) and one executive functioning assessment testing working memory. These assessments were administered one-on-one by trained child assessors at two time points: first in the fall of kindergarten and again in the spring. Test dates varied by school site, with fall assessments occurring between August and December, and spring assessments occurring between January and July. Before each subject area assessment, students completed a set of routing items in order to determine the difficulty level of their full assessment. Each set of questions included common items so as to allow for item response theory (IRT) scaling, which (in theory, if the model assumptions hold), expresses all students' scores across item sets and test waves on a common scale (Tourangeau et al., 2012). Subject Area Assessments. Math and reading assessments were based on the existing NAEP frameworks, projected to earlier grades using curriculum frameworks from national and state performance standards. The reading assessment measured print awareness, letter recognition, phonemic awareness, word recognition, vocabulary, and reading comprehension, while the math assessment included items related to number sense, properties, and operations; measurement; geometry and spatial sense; data analysis; statistics and probability; and patterns, algebra, and functions. Executive Function Assessment. Students took the Numbers Reversed subtest of Woodcock-Johnson III Tests of Cognitive Abilities (Mather & Woodcock, 2001, as cited in Torangeau et al., 2012, which is designed to measure working memory. In this task, assessors read increasingly longer series of numbers and the child was asked to repeat the numbers back in reverse order. This test is reported to have a median split-half reliability of 0.87 (Schrank, McGrew, & Woodcock, 2001). In my main analyses, I use the standard score metric for this assessment, standardized to mean 0 and sd 1 at each test wave."}, {"section_title": "A-3", "text": ""}, {"section_title": "Analytic Plan", "text": "Models. I fit models of the form: , (1) where represents the test score of student i at time t (fall or spring) on assessment a, BLACK is a dummy variable indicating whether the student is black (1=yes, 0=no), is a vector of control variables, and is the student-level error term. In this model, can be interpreted as the black-white gap on assessment a at time t (controlling for the other variables in the model; some models include an interaction between BLACK and SES, making the BLACK coefficient the gap for students with mean SES composite scores). A negatively-signed indicates that black students, on average, score lower than white students. I test for, and include where significant, interactions between BLACK and the control variables. In all models, I use the recommended sampling weights and adjust standard errors for the complex sampling design. For consistency with FL, I use the \"scale score\" metric for the math and reading tests, standardized to mean 0 and standard deviation 1 at each testing wave. The scale scores are obtained by first applying item response theory to estimate students' latent ability and then transforming students' to represent an estimate of the total number of items the student would have answered correctly had he or she been given the chance to answer all of the questions on the test. The ECLS-K manual reports the reliabilities as 0.95 for both fall and spring reading scores, and 0.92 and 0.94 for fall and spring math scores, respectively (Torangeau et al., 2012). Like FL, my main conclusions are not sensitive to the use of scale scores versus the metric (I include results using the metric in the appendix). Control Variables. I use a similar set of covariates as those used by FL. Because of the variation in test administration dates described above, I control for the number of months of school the student experienced before testing. I also control for child's age at kindergarten entry, whether the child is a first-time kindergartener, and biological sex. As measures of children's socioeconomic status, I include a continuous composite measure of the child's family income, parental occupational status, and parent education (and the squared value of the composite, where appropriate), as well as separate variables indicating whether the child participates in Women, Infants, and Children (WIC) Food and Nutrition Services, whether the mother received WIC support while pregnant with the child, and a measure of the number of children's books in the child's home (and its squared value). Like FL, I also control for whether the mother was a teenager at her first child's birth and whether the mother was over 30 years old at her first child's birth."}, {"section_title": "Findings / Results:", "text": "Description of the main findings with specific details. In Table 1, I present weighted descriptive statistics by student race. Descriptively, black students score lower than white students, on average, on each test and test occasion. Black students also have lower average SES composite scores, higher rates of participation in WIC, and are more likely to have a mother who was a teen at the time of her first child's birth. The number of months of school experienced before each assessment is nearly identical for both black and white students. In Appendix C, I present fall and spring gaps for each assessment in different scales, standardization units, gap measures, and assumed test reliabilities. In Table 2, I present the raw and adjusted math and reading gaps in fall and spring of the 2010-2011 kindergarten year. These results show many of the same patterns found in the ECLS-K: 1999 by FL. Columns 1 and 5 show the unadjusted fall math and reading gaps, respectively. These gaps are somewhat smaller than the scale score gaps presented by FL, with math (-0.566, compared to -0.628 in FL) being larger than reading (-0.288, compared to -0.401 in FL). As in the ECLS-K: 1999, the unadjusted math and reading gaps are larger in the spring than in the fall, by approximately 0.10 wave-standard deviation units for both math and reading. Also similar to FL, a small set of SES-related control variables reduces these gaps considerably. For reading, the significant interaction between BLACK and SES indicates that the effect of SES on achievement is stronger for black students than for white students at both waves; the same is true for math, but only in the fall (FL do not report a BLACK*SES interaction). In reading, the adjusted gap switches signs in the fall, with black students of mean SES scoring significantly higher than white students of mean SES (0.142). By spring, however, black students of mean SES have lost their adjusted advantage, and there is no significant difference between black and white students of mean SES (similar to FL's results). In math, an adjusted fall gap of -0.115 remains for students of mean SES. The adjusted math gap grows by approximately 0.10 standard deviations during the school year, quite similar to the math gap growth observed by FL. In Table 3, I present the results for the Numbers Reversed executive functioning measure. These results differ somewhat from the math and reading results. The unadjusted gaps are similar in the fall and the spring (at -0.510 and -0.516, respectively). The SES controls do not explain as much of the working memory gap as they do the math and reading gaps, and the controls explain somewhat less of the spring gap than the fall gap (with an adjusted fall gap of -0.207 and an adjusted spring gap of -0.274). BLACK shows a marginally significant, positivelysigned interaction with SES in the fall."}, {"section_title": "Conclusions:", "text": "Description of conclusions, recommendations, and limitations based on findings. These math and reading analyses based on the newly-released ECLS-K: 2011 largely replicate the results that Fryer and Levitt (2004) found with the ECLS-K: 1999. The results for working memory show a somewhat different pattern than for math and reading, however, where the SES variables explain somewhat less of the black-white gap, and the gap does not change substantially over the school year. The differing pattern of results for math and reading compared to executive functioning is consistent with the hypothesis that gap estimates are sensitive to the measure used, and are affected by whether the material on the assessments is learned mostly inside or outside school. The math and reading gaps, which represent knowledge learned in school, grow from fall to spring, while the unadjusted executive functioning gap, which does not measure curricular objectives, does not grow from fall to spring. As with all research of this sort, causal inferences cannot be drawn about the relationship between the control variables and the outcomes. These estimates therefore likely represent an upper bound on the extent to which the included controls (as operationalized) explain blackwhite test score gaps.     Women, Infant, and Children vouchers; WIC (pregnant)=mother received WIC while pregnant with child. SES=continuous composite measure of income, occupational status, and education. Teen Mother=mother was <20 years old at 1 st child's birth; Mom age>30= mother was 30 or older when first child born; Entry age=child's age at kindergarten entry; BooksHome=number of children books in child's home; Months Sch = number of months of school before fall or spring assessment."}]