[{"section_title": "Executive Summary", "text": "State performance standards represent how much the state expects the student to learn in order to be considered proficient in reading, mathematics, and science. In the past, these performance standards have been used by each state to report adequate yearly progress (AYP) under No Child Left Behind federal legislation, and they are currently being used for federal reporting under the Department of Education's flexibility waivers. These standards are also used by the state to monitor progress from year to year, and to report on the success of each classroom, school, and district to parents and the public. This report uses international benchmarking as a common metric to examine and compare what students are expected to learn in some states with what students are expected to learn in other states. 1 The performance standards in each state were compared with the international benchmarks used in two international assessments, and it was assumed that each state's expectations are embodied in the stringency of the performance standards (also called achievement standards) it uses on its own state accountability tests. The international assessments were the Trends in International Mathematics and Science Study (TIMSS) and the Progress in International Reading Literacy Study (PIRLS). The data were obtained through a statistical linking study tying the National Assessment of Educational Progress (NAEP) to TIMSS and PIRLS (see Appendix A). The international benchmarking not only provided a mechanism for calibrating the difficulty and gauging the global competitiveness of each state standard but also yielded an international common metric with which to compare state expectations. The overall finding in the study is that there is considerable variance in state performance standards, exposing a large gap in expectations between the states with the highest standards and the states with the lowest standards. Although this gap in expectations is large, many policymakers may not be aware of just how large it is. In general, \uf0a7 The difference between the standards in the states with the highest standards and the states with the lowest standards is about 2 standard deviations. In many testing programs, a gap this large represents three to four grade levels. \uf0a7 This \"expectations gap\" is so large that it is more than twice the size of the national black-white achievement gap. Closing the achievement gap is important, but so is closing the larger expectation gap. Reducing the expectation gap will require consistently high expectations from all states. The 2011 percent proficient for each state test was obtained from EdFacts, U.S. Department of Education, http://www2.ed.gov/admins/lead/account/consolidated/index.html. The state NAEP Coordinators in 22 states were contacted to confirm or correct state results reported in EdFacts. Some states were excluded in some tables because AIR was unable to reliably confirm the state's percent proficient. The author would like to thank Jonathan Phelan and Steven Hummel at AIR for conducting this review of the data. \uf0a7 The report also found that success under No Child Left Behind is largely related to using low performance standards. The states reporting the highest numbers of proficient students have the lowest performance standards. More than two-thirds of the variation in state success reported by No Child Left Behind is related to how high or low the states set their performance standards. These results help explain why the United States does poorly in international comparisons. Many states think they have high standards and are doing well, and feel no urgency to improve because almost all their students are proficient. The report estimated how the 2011 state results reported to No Child Left Behind would have looked had all the states used a common metric. When the data were reanalyzed using a common metric, higher achievement was correlated with a higher performance standard. With a different metric used in each state, as encouraged by NCLB, higher achievement is obtained by setting low standards. When a common metric is used in each state, such as the state NAEP assessment, higher achievement is associated with setting higher standards. The data show that the No Child Left Behind paradigm of encouraging each state to establish its own unique performance standard is fundamentally flawed and misleading. The big policy problem associated with the current No Child Left Behind state testing paradigm is that the lack of a common metric results in a lack of transparency. Because test results across the 50 states are not comparable, any inference about national progress is impossible; we cannot even determine if progress in one state is greater than progress in another state. Clearly, 50 states going in 50 different directions cannot lead to national success that is globally competitive. Transparency in measurement (through use of a common metric) is the most fundamental requirement for scientific measurement and the first step in determining if our educational programs are succeeding. The lack of transparency among state performance standards leads to a kind of policy jabberwocky: the word proficiency means whatever one wants it to mean. This misleads the public, because low standards can be used to artificially rack up high numbers of \"proficient\" students. This looks good for federal reporting requirements, but it denies students the opportunity to learn college and career readiness skills. If we believe almost all students are already proficient, what is the motivation to teach them higher-level skills? This may be the main reason why less than 40 percent of 12th grade students are academically prepared for college. 2 Furthermore, over a third of students enrolled in college need remedial help. They thought that they were college ready because they passed their high school graduation test, but they were not. To reduce the expectations gap, this report recommends re-engineering the current standardsetting paradigm used by the states. Almost all states use test content-based standard setting methods such as the bookmark method (Mitzel, Lewis, Patz, & Green, 2001). The problem with this approach is that it uses an inward focus on internal state content standards and does not focus on how state expectations stack up against the expectations of other states, the nation, and other countries. Rather than deriving performance standards exclusively from internal state content considerations, this report recommends a different method of evidence-based standard setting that incorporates more empirical data. An example of this is the Benchmark Method (Phillips, 2011) of standard setting, which argues that performance standards are fundamentally a policy-judgment decision (not just a content decision) and that these standards need to be guided by knowledge of the real world around us and the requirements that our students will face as they compete in a global economic and technological world."}, {"section_title": "Introduction", "text": "For the past quarter century, we as a country have believed that our underachieving educational system has put our nation at risk (National Council for Excellence in Education, 1983). National policymakers have responded to this crisis with aspirational, far-reaching goals, such as \"being the first in the world in mathematics and science achievement by 2000\" (National Education Goals Panel, 1999), \"all students will be proficient in reading and mathematics by 2014\" (No Child Left Behind Act, 2001), or \"every student should graduate from high school ready for college and a career, regardless of their income, race, ethnic or language background, or disability status\" (U.S. Department of Education, 2010). Each of these national goals recognizes that in the 21st century, students must compete in a global economy, not just a local economy. The need for states to set high, internationally competitive standards has recently been emphasized by a number of policymakers. A recent report by the NGA, CCSSO, and Achieve 2008 The President of the United States (Obama, 2009), in a speech to the U.S. Hispanic Chamber of Commerce, recognized the need for high and consistent standards. He stated, Let's challenge our states to adopt world-class standards that will bring our curriculums into the 21st century. Today's system of 50 different sets of benchmarks for academic success means fourth-grade readers in Mississippi are scoring nearly 70 points lower than students in Wyoming-and getting the same grade. Over the last decade within the United States, many states have been busy developing new content standards and new criterion-referenced tests that measure success on those content standards. Much of this frenetic activity is related to the federal No Child Left Behind legislation that requires states to report annually on whether they are making adequate yearly progress (AYP) toward meeting state standards. When states set performance standards, however, they generally have little knowledge of how those state performance standards compare with international performance standards, such as those used on TIMSS, PIRLS, and PISA. Yet, states should care about how their students compare internationally and how their performance standards compare internationally. States compete with international companies, and their students will need to compete in an international market place."}, {"section_title": "International Benchmarking", "text": "International benchmarking is a way to calibrate the difficulty level of state performance standards to international standards. This type of benchmarking is similar to benchmarking in business and industry. For example, the fuel efficiency and quality of American-built cars are often benchmarked against those built in Japan and South Korea. Such benchmarking is important in education if we are to expect our students to compete in a global economy."}, {"section_title": "International Benchmarking Using TIMSS, PIRLS, and PISA", "text": "The international data already collected for three assessments could provide the data needed for international benchmarks. Two of these are used in this study: TIMSS and PIRLS. Both surveys are sponsored by the International Association for the Evaluation of Educational Achievement (IEA), currently located in the Netherlands. TIMSS is an assessment of Grade 4 and Grade 8 students in mathematics and science, and PIRLS is an assessment of Grade 4 students in reading. The third survey is PISA, sponsored by the Paris-headquartered OECD. PISA is an assessment of 15-year-old students in mathematics, science, and reading literacy. Statistical techniques for international benchmarking using PISA can be found in Phillips and Jiang (2014)."}, {"section_title": "Expressing International Benchmarks as Grades", "text": "International benchmarks using TIMSS and PIRLS can be obtained by states by statistically linking their state tests to the state NAEP, then linking national NAEP to national TIMSS or PIRLS. This process of chain linking places the state's own performance standards on the TIMSS or PIRLS scale. States can then determine how their own state performance standards compare with the international benchmarks on TIMSS and PIRLS. One of the primary ways TIMSS and PIRLS report their results is in terms of international benchmarks. The labels and cut-points on the TIMSS and PIRLS scales for the international benchmarks are Advanced (625), High (550), Intermediate (475), and Low (400). These performance standards apply to both the Grade 4 and Grade 8 mathematics assessment in TIMSS and the Grade 4 reading assessment in PIRLS. Full descriptions of the TIMSS and PIRLS international benchmarks are contained in Appendix D. To facilitate discussion, this report will relabel the international benchmarks as grades, with Advanced assigned an A, High assigned a B, Intermediate a C, and Low a D. These grades are indicated in Table 1.  Figure 3, we see that the state with the greatest percentage of students reported proficient under No Child Left Behind is Georgia, whereas the percentage of proficient students in Massachusetts is among the lowest across the states. If parents used No Child Left Behind data to choose a state in which to live so their children could attend the best schools, they might choose Georgia. But there is something wrong with this picture. We know that NAEP reports exactly the opposite, with Massachusetts the highest-achieving state and Georgia among the lowest-achieving states. If we look deeper into the state performance standards, we can begin to explain this contradiction. In each state, the number of proficient students is influenced by how high or low the state sets the performance standard for proficiency. The only way to compare the stringency or difficulty level of the performance standards across states is to express them in a common metric. This is done in Figures 5 through 8 by converting the state performance standards to the metric of TIMSS (i.e., the TIMSS equivalent of the state performance standard in mathematics and science) and converting the state performance standards to the metric of PIRLS (i.e., the PIRLS equivalent of the state performance standard in reading). The TIMSS equivalents and PIRLS equivalents are then expressed as a grade (see Table 1, above). These grades represent the international benchmark for the state performance standards. A state performance standard that is mapped to a TIMSS equivalent in the D range of the TIMSS scale (i.e., a Low international benchmark) is requiring only a minimal level of mathematics. On the other hand, a state performance standard that is mapped to a TIMSS equivalent in the B range of the TIMSS scale (i.e., a High international benchmark) is requiring a level of mathematics similar to that needed to perform at the TIMSS and PIRLS level of the typical student in the highest-performing countries. In Figures 5 through 8, the states have been ordered by their reported rates of percent proficient. The states with the lowest percent proficient are on the left and the states with the highest percent proficient on the right. The negatively sloping line shows that the performance standards drop as percent proficient increases. Comparing the international benchmarks in Figures 5 through 8 to the percent proficient in Figures 1 through 4 shows why so many states can claim so many proficient students for federal reporting requirements. These states are using low standards to define proficiency. For example, in Grade 8 mathematics, seven states require only the equivalent of a D or D+ to be considered proficient. Massachusetts, on the other hand, has the highest performance standard in the country, a B\u2212, which is why that state has fewer proficient students. The correlation between the difficulty of the state performance standard and the percent proficient is equal to \u2212.83 for Grade 4 mathematics, \u2212.83 for Grade 4 reading, \u2212.79 for Grade 8 mathematics, and \u2212.88 for Grade 8 science. This means that about two-thirds of the variance in No Child Left Behind reporting is due to how high-or low-the state sets the performance standard. In other words, high state performance as reported by No Child Left Behind is largely determined by how low a state sets its performance standards. Another important observation emerging from Figures 5 through 8 is that the difference between the highest and lowest performance standards represents a difference in expectations. The states with the highest standards are expecting more than the states with the lowest standards, and this expectation gap is huge. We can get a solid understanding of this expectation gap if we express it in terms of TIMSS and PIRLS standard deviation units. Expressed as units of the U.S. national standard deviations of TIMSS or PIRLS, the standard deviation differences between the highest and lowest performance standard are 2.0, 1.6, 1.6, and 2.1 for Grade 4 mathematics, Grade 4 reading, Grade 8 mathematics, and Grade 8 science, respectively. To get a feel for the magnitude of these differences, note that a difference of two standard deviations equals about a three-to four-grade-level difference in student proficiency. Also, two standard deviations is about twice the size of the black-white achievement gap, which is often characterized as about one standard deviation. For example, the average national scores on the 2013 Grade 8 NAEP mathematics assessment were 263 and 295, for blacks and white, respectively with a standard deviation equal to 33. Expressed as a standard deviation unit, the black-white achievement gap was (263-295) / 33 = -.97. This means black students scored almost one standard deviation below white students.        Note: The negatively sloping line of best fit represents the negative linear relationship between the state performance standard and the state percent proficient. Source: Phillips, G. (2014). International benchmarking: State and national education performance standards. Washington, DC: American Institutes for Research.    Note: The negatively sloping line of best fit represents the negative linear relationship between the state performance standard and the state percent proficient. Source: Phillips, G. (2014). International benchmarking: State and national education performance standards. Washington, DC: American Institutes for Research.  "}, {"section_title": "Expressing State Performance Standards with a Common Metric", "text": "As indicated above, there is a large negative correlation between the stringency of the state standards and the percent proficient reported to the federal government as required by NCLB. This implies that, based on NCLB reporting, setting higher state standards is associated with lower levels of student performance. So does that mean there is no benefit to setting higher expectations? Actually, there is a benefit, but we have to use a common metric to see it. For example, Tables 12 and 13 (in Appendix B) show the TIMSS equivalent of the state performance standards (column 2) and the estimates of how many students would be expected to reach the High international benchmark on TIMSS (column 6). The latter estimates are available for Grade 8 from the NCES 2011 NAEP-TIMSS linking study (NCES, 2013). The question becomes this: Are higher state performance standards associated with higher percentages of students estimated to achieve the High level of performance on TIMSS? The correlations are +.37 for mathematics and +.41 for science. The correlations are positive and statistically significant. If we compare the TIMSS equivalent of the state performance standards with the percent proficient on state NAEP, we find similar results. The correlations are +.35 for mathematics and +.39 for science. The correlations are again positive and statistically significant. These results show a positive association between raising state performance standards and improved student performance. Such results are visible only when the performance standards are expressed with a common metric across the states."}, {"section_title": "International Benchmarks for National Performance Standards", "text": "In addition to benchmarking state performance standards, TIMSS and PIRLS can be used to internationally benchmark NAEP national achievement levels. This can be seen in Figures 9  through 12. The general conclusion from the linking results is that the NAEP Proficient achievement level is higher than the TIMSS High benchmark and the NAEP Advanced level is higher than the TIMSS Advanced benchmarks for Grade 4 mathematics and Grade 8 science. Furthermore, the NAEP Proficient and Advanced standards are higher than the PIRLS international benchmarks in Grade 4 reading. However, the NAEP Proficient and Advanced achievement levels are lower than the TIMSS High and Advanced international benchmarks for Grade 8 mathematics. These results are graphed in Figures 9 through 12, where the NAEP performance standards are expressed in the TIMSS and PIRLS metric. The same results are displayed again in Figures 13 through 16, but with the TIMSS and PIRLS international benchmarks expressed in the NAEP metric. These findings may help explain several anomalies when comparing NAEP results with TIMSS and PIRLS results. For example, it is often reported that the United States does very well on international reading comparisons but has a low level of proficiency based on NAEP. For example, the 2011 PIRLS shows that 56 percent of U.S. students were reading at the High level and ranked sixth among the participating countries, implying that the United States produces students who are world-class readers. However, only 34 percent of students were reported proficient on the 2011 NAEP, suggesting that very few students in the United States are proficient readers. The reason for this discrepancy is that the NAEP Proficient standard is substantially higher than the PIRLS High international benchmark, as indicated in Figure 10 and Figure 14.      "}, {"section_title": "How to Get Higher and More Consistent Standards", "text": "The lack of transparency among state performance standards is beginning to dawn on national policymakers. Recent calls for fewer, clearer, and higher standards by Secretary of Education Arne Duncan are recognition of the need for transparency. The Common Core project by CCSSO and NGA is partly motivated by the nation's lack of progress toward internationally competitive educational excellence if the 50 states are going in 50 different directions. Both the Secretary of Education and the CCSSO-NGA project are primarily talking about fewer, clearer, and higher content standards. Content standards are statements about the scope and sequence of what students should learn in each grade and subject in school. Their concern is whether the state content standards are challenging and at least comparable to what is taught to students in the highest-performing countries in the world. This is an important first step, but it does not address the expectations gap discussed in this report. Many states already have highly challenging 21st-century content standards, but use low performance standards to increase the number of proficient students for federal reporting. States need a way to set consistently high performance standards. This can only happen if the current standard-setting paradigm in the testing industry is changed. One of the main reasons states set low performance standards is related to the methodology currently in vogue in state testing programs to establish performance standards. Frequently used techniques like the Bookmark Method (Mitzel, Lewis, Patz, & Green, 2001) set standards based primarily (and in some cases exclusively) on test content. Teachers and other stakeholders set standards by reviewing test items and relating them to descriptions of performance levels and state content standards. The use of external empirical data is usually relegated to secondary importance in the standard-setting process. The standard-setting process is content-based not evidence-based. The problem with narrowly focused content-based standard-setting methods is that nothing in the standard-setting process ensures that the performance standards are nationally or internationally challenging. The panelists usually believe that they are setting rigorous standards, basing their belief on the personal classroom experiences of the teachers and the anecdotal experiences of other stakeholders on the panel. Unfortunately, the panelists are flying without radar and have no clue as to whether they are setting standards that will help their students compete outside their state. Across the country, the strict emphasis on internal state content in setting performance standards has had the net effect of creating wide variations in rigor across all the states, and dumbed-down performance standards in many. These wide variations and low standards bespeak a lack of credibility and lack of transparency in state and federal education reporting, confuse policymakers, and mislead the public in some states into believing that their students are proficient when they are not. To correct this problem, this report recommends a more evidencebased approach to standard setting, such as the Benchmark Method (Phillips, 2013), in which panelists are guided by external data from other educational systems. In the near future, many states are likely to function as a consortium and adopt the Common Core standards developed by CCSSO and NGA. Eventually, the Common Core content standards will need to establish Common Core performance standards. The Benchmark Method of establishing performance standards represents a departure from the narrow focus on internal content standards currently used in most states. The Benchmark Method recognizes that performance standards are policy decisions, and that they need to be consistent and be set high enough to prepare students to compete for college and careers beyond the state borders. If the Benchmark Method were to be used in the future by individual states (or a consortium of states), state performance standards would be consistent and more on par with the high standards used by national and international assessments such as NAEP, TIMSS, PIRLS, and PISA."}, {"section_title": "Conclusion", "text": "The overall finding in the study is that the difference in the stringency of the performance standards used across the states is huge and probably far greater than most policymakers realize. The difference between the state with the highest standards and the state with the lowest standards was about 2 standard deviations. This difference is so great that it is more than twice the size of the national black-white achievement gap. In many state testing programs, a difference this great represents three to four grade levels. These large differences among states clearly indicate why we need more common assessments and the Common Core State Standards. It is not that each state should teach the same thing at the same time in every grade every year-instead, we need to reduce the extreme variability that we now have, whereby some low-achieving states have low expectations and higher-achieving states have higher expectations. These huge differences in expectations deny students in states with low performance standards the opportunity to learn from a challenging curriculum. Unfortunately, at the time of this report, much of the support for the Common Core State Standards has eroded. Initially, 46 states (including the District of Columbia) planned to conduct common assessments on the Common Core State Standards either through the Smarted Balanced Assessment Consortium (SBAC) or the Partnership for Assessment of Readiness for College and Careers (PARCC). Based on a recent tally, that number has now dropped to 27 states (including the District of Columbia), with 17 participating in the SBAC and 10 participating in PARCC (Gewertz & Ujifusa, 2014). In addition, recent 2014 polls by Education Next and the 46 th annual PDK/Gallup Poll have shown a drop in public support for the Common Core based on the public's misperception that that the Common Core was a federal initiative (Camera, 2014). Our analysis found that success under No Child Left Behind is largely related to using low performance standards. The stringency of state performance standards had a high negative correlation with the percentage of proficient students reported by the states. The states reporting the highest numbers of proficient students had the lowest performance standards. Another way of saying this is that high state performance reported by No Child Left Behind is significantly correlated with low state performance standards. About two-thirds of the variation in states' success reported by No Child Left Behind reflects differences in how individual states set their performance standards. This report also estimated how the 2011 state results reported to No Child Left Behind would have looked had all the states used performance standards expressed in a common metric. When the data were reanalyzed on this basis, higher expectations reported by states were correlated with higher achievement. This report argues that the No Child Left Behind paradigm of encouraging each state to set a different performance standard is fundamentally flawed, misleading, and lacking in transparency. Test results across the 50 states are not comparable, inferences about national progress are impossible, and we cannot even determine if progress in one state is greater than progress in another state. The lack of transparency among state performance standards misleads the public, because low standards can be used to artificially inflate the numbers of proficient students. This practice denies the nation's students the opportunity to learn college and career readiness skills."}, {"section_title": "Appendix A: Statistically Linking NAEP to TIMSS and PIRLS", "text": "This report uses the statistical linking procedures outlined in Johnson, Cohen, Chen, Jiang, and Zhang (2005). One major difference is that this report uses reported statistics from the NAEP 2011, TIMSS 2011, and PIRLS 2011 published reports, and the 2011 NAEP reports in mathematics and reading, rather than recalculating them from the public-use data files and plausible values available for the NAEP, TIMSS, and PIRLS assessments. The international benchmarking in this study is based on data obtained from several publically available reports. Data on mathematics, reading, and science NAEP were obtained from 2011 NAEP reports (National Center for Education Statistics, 2011a, 2011b, 2010. Data on TIMSS mathematics and science results were obtained from 2011 TIMSS reports (Mullis, Martin, Foy, & Arora, 2011;Martin, Mullis, Foy, & Stanco, 2011). Data on PIRLS were obtained from the 2011 PIRLS report (Mullis, Martin, Kennedy, Foy, & Drucker, 2012). In the following discussion, Y denotes TIMSS (or PIRLS) and X denotes NAEP. In statistical moderation, the estimated z score is a transformed x score expressed in the y metric The z is the TIMSS equivalent (or PIRLS equivalent) of the NAEP score x associated with the state performance standard. The NAEP score x is obtained from determining the scaled score on NAEP that is the equipercentile equivalent of the performance standard on the local state accountability test (that is used for federal reporting required by No Child Left Behind). In equation 1, \u00c2 is an estimate of the intercept of a straight line, and B is an estimate of the slope of the linear transformation of NAEP to TIMSS or PIRLS defined by \u02c6\u0176 In the above equations, \u02c6X \u00b5 and \u02c6Y \u00b5 are the national means of the U.S. NAEP and U.S. TIMSS (or PIRLS), respectively, while \u02c6X \u03c3 and \u02c6Y \u03c3 are the national standard deviations of the assessments."}, {"section_title": "Linking Error Variance", "text": "The linking error variance in the TIMSS equivalents and PIRLS equivalents can be determined through the following equation: According to Johnson et al. (2005), the error variances in this equation, 2"}, {"section_title": "2\u02c6,", "text": ""}, {"section_title": ", and", "text": "A AB B \u03c3 \u03c3 \u03c3 , can be approximated by Taylor-series linearization (Wolter, 1985). x B"}, {"section_title": "Var Var xB", "text": ""}, {"section_title": "Var", "text": "Var B (5) Equations (4) and (5) were used with data in the U.S. linking sample to derive the estimates of linking error variance in this paper. The statistics needed to use equations (1) through (3) are contained in the tables below.    The parameter estimates \u00c2 and B are indicated in Tables 6 through 9. These are the intercepts and slopes, respectively, needed to re-express NAEP results on the TIMSS or PIRLS scale.     Table 10, the TIMSS equivalent of the Massachusetts proficient standard in Grade 4 mathematics was 580. In other words, the Massachusetts proficient standard is comparable in difficulty to the TIMSS score of 580. A score of 580 on TIMSS is at the High international benchmark and is comparable to a B+, based on the grading system in Table 1 of this report (B+ is assigned if the TIMSS equivalent or PIRLS equivalent of the state proficient standard is between 575 and 599 on the TIMSS or PIRLS scale).    The results in Tables 14 and 15 indicate that the linking results were adequate for Grade 8 mathematics. In Grade 8 mathematics, the percent reaching the High TIMSS equivalents were statistically comparable to the actual percent reaching the High benchmark in five out of nine comparisons. The linking primarily underestimated the percent reaching High TIMSS benchmarks for states with very high performance standards. In Grade 8 science, the linking results were especially good and were comparable in seven out of nine comparisons. As can be seen from these tables, the estimates based on linking were not perfect, but they were adequate in most cases. The validity evidence for the NAEP-PIRLS linking was also encouraging (see Phillips, 2014). In 2011, Florida administered a statewide assessment in PIRLS. In general, PIRLS equivalents are not statistical significantly different from the actual PIRLS benchmarks. For example, the mean difference between the PIRLS equivalent and the actual PIRLS mean is not significant (see Error! Reference source not found.). The only significant difference between the PIRLS equivalent and the actual PIRLS result is for the percentage of advanced students (see Table 17). Even though there is only a 1 percent difference between the predicted and the actual percentage, the difference is statistically significant because the standard errors are so small.  "}, {"section_title": "International Benchmarks for Grade 8 TIMSS Mathematics", "text": ""}, {"section_title": "Advanced", "text": "Students can reason with information, draw conclusions, make generalizations, and solve linear equations. Students can solve a variety of fraction, proportion, and percent problems and justify their conclusions. Students can express generalizations algebraically and model situations. They can solve a variety of problems involving equations, formulas, and functions. Students can reason with geometric figures to solve problems. Students can reason with data from several sources or unfamiliar representations to solve multi-step problems.\nStudents communicate an understanding of complex and abstract concepts in biology, chemistry, physics, and earth science. Students demonstrate some conceptual knowledge about cells and the characteristics, classification, and life processes of organisms. They communicate an understanding of the complexity of ecosystems and adaptations of organisms, and apply an understanding of life cycles and heredity. Students also communicate an understanding of the structure of matter and physical and chemical properties and changes and apply knowledge of forces, pressure, motion, sound, and light. They reason about electrical circuits and properties of magnets. Students apply knowledge and communicate understanding of the solar system and Earth's processes, structures, and physical features. They understand basic features of scientific investigation. They also combine information from several sources to solve problems and draw conclusions, and they provide written explanations to communicate scientific knowledge."}, {"section_title": "High", "text": "Students can apply their understanding and knowledge in a variety of relatively complex situations. Students can use information from several sources to solve problems involving different types of numbers and operations. Students can relate fractions, decimals, and percents to each other. Students at this level show basic procedural knowledge related to algebraic expressions. They can use properties of lines, angles, triangles, rectangles, and rectangular prisms to solve problems. They can analyze data in a variety of graphs.\nStudents demonstrate understanding of concepts related to science cycles, systems, and principles. They demonstrate understanding of aspects of human biology, and of the characteristics, classification, and life processes of organisms. Students communicate understanding of processes and relationships in ecosystems. They show an understanding of the classification and compositions of matter and chemical and physical properties and changes. They apply knowledge to situations related to light and sound and demonstrate basic knowledge of heat and temperature, forces and motion, and electrical circuits and magnets. Students demonstrate an understanding of the solar system and of Earth's processes, physical features, and resources. They demonstrate some scientific inquiry skills. They also combine and interpret information from various types of diagrams, contour maps, graphs, and tables; select relevant information, analyze, and draw conclusions; and provide short explanations conveying scientific knowledge."}, {"section_title": "Intermediate", "text": "Students can apply basic mathematical knowledge in a variety of situations. Students can solve problems involving decimals, fractions, proportions, and percentages. They understand simple algebraic relationships. Students can relate a two-dimensional drawing to a three-dimensional object. They can read, interpret, and construct graphs and tables. They recognize basic notions of likelihood.\nStudents recognize and apply their understanding of basic scientific knowledge in various contexts. Students apply knowledge and communicate an understanding of human health, life cycles, adaptation, and heredity, and analyze information about ecosystems. They have some knowledge of chemistry in everyday life and elementary knowledge of properties of solutions and the concept of concentration. They are acquainted with some aspects of force, motion, and energy. They demonstrate an understanding of Earth's processes and physical features, including the water cycle and atmosphere. Students interpret information from tables, graphs, and pictorial diagrams and draw conclusions. They apply knowledge to practical situations and communicate their understanding through brief descriptive responses."}, {"section_title": "Low", "text": "Students have some knowledge of whole numbers and decimals, operations, and basic graphs.\nStudents can recognize some basic facts from the life and physical sciences. They have some knowledge of biology, and demonstrate some familiarity with physical phenomena. Students interpret simple pictorial diagrams, complete simple tables, and apply basic knowledge to practical situations."}, {"section_title": "International Benchmarks for", "text": "Grade 8 TIMSS Science"}]