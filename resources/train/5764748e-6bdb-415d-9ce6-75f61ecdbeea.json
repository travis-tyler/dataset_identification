[{"section_title": "EXECUTIVE SUMMARY", "text": ""}, {"section_title": "Executive Summary", "text": "The Trends in International Mathematics and Science Study (TIMSS) is an international comparative study of student achievement. TIMSS 2011 represents the fifth such study since TIMSS was first conducted in 1995. Developed and implemented at the international level by the International Association for the Evaluation of Educational Achievement (IEA)-an international organization of national research institutions and governmental research agencies-TIMSS assesses the mathematics and science knowledge and skills of 4th-and 8th-graders. TIMSS is designed to align broadly with mathematics and science curricula in the participating countries and education systems. This report focuses on the performance of U.S. students 1 relative to their peers around the world in countries and other education systems that participated in TIMSS 2011. For the purposes of this report, \"countries\" are complete, independent political entities, whereas \"other education systems\" represent a portion of a country, nation, kingdom, or emirate or are other non-national entities (e.g., U.S. states, Canadian provinces, Flemish Belgium, and Northern Ireland). In this report, these \"other education systems\" are designated as such by their national three-letter international abbreviation appended to their name (e.g., England-GBR, Ontario-CAN). This report also examines changes in mathematics and science achievement compared with TIMSS 1995 andTIMSS 2007. In 2011, TIMSS was administered at grade 4 in 57 countries and other education systems and, at grade 8, in 56 countries and other education systems. 2 These total counts include U.S. states that participated in TIMSS 2011 not only as part of the U.S. national sample of public and private schools but also individually with state-level public school samples. At grade 4, this was Florida and North Carolina, and at grade 8 this was Alabama, California, Colorado, Connecticut, Florida, Indiana, Massachusetts, Minnesota, and North Carolina. Note that, because all TIMSS participants are treated equally, these states are compared with the United States (national sample) throughout this report. All differences described in this report are statistically significant at the .05 level. No statistical adjustments to account for multiple comparisons were used. \u2022 At grade 4, the United States was among the top 15 education systems in mathematics (8 education systems had higher averages and 6 were not measurably different) and scored higher, on average, than 42 education systems. \u2022 The 8 education systems with average mathematics scores above the U.S. score were Singapore, Korea, Hong Kong-CHN, Chinese Taipei-CHN, Japan, Northern Ireland-GBR, North Carolina-USA, and Belgium (Flemish)-BEL. \u2022 Among the U.S. states that participated in TIMSS at grade 4, North Carolina scored above the TIMSS scale average and the U.S. national average in mathematics, while Florida scored above the TIMSS scale average but was not measurably different from the U.S. national average. \u2022 Compared with 1995, the U.S. average mathematics score at grade 4 was 23 score points higher in 2011 (541 vs. 518). \u2022 Compared with 2007, the U.S. average mathematics score at grade 4 was 12 score points higher in 2011 (541 vs. 529). \u2022 The percentage of 4th-graders performing at or above the Advanced international mathematics benchmark in 2011 was higher than in the United States in 7 education systems, was not different in 4 education systems, and was lower than in the United States in 45 education systems. 4 3 TIMSS provides two overall scales-mathematics and science-as well as several content and cognitive domain subscales for each of the overall scales. The scores are reported on a scale from 0 to 1,000, with the TIMSS scale average set at 500 and standard deviation set at 100. 4 TIMSS reports on four benchmarks to describe student performance in mathematics and science. Each benchmark is associated with a score on the achievement scale and a description of the knowledge and skills demonstrated by students at that level of achievement. The Advanced international benchmark indicates that students scored 625 or higher. More information on the benchmarks can be found in the main body of the report and appendix A. \u2022 Compared with 1995, the U.S. average mathematics score at grade 8 was 17 score points higher in 2011 (509 vs. 492). \u2022 There was no measurable difference between the U.S. average score in 2007 (508) and in 2011 (509). \u2022 The percentage of 8th-grade students performing at or above the Advanced international mathematics benchmark in 2011 was higher than in the United States in 11 education systems; was not different in 13 education systems; and was lower than in the United States in 31 education systems."}, {"section_title": "Science at grade 4", "text": "\u2022 In 2011, the average science score of U.S. 4thgraders (544) was higher than the international TIMSS scale average, which is set at 500. \u2022 At grade 4, the United States was among the top 10 education systems in science (6 education systems had higher averages and 3 were not measurably different) and scored higher, on average, than 47 education systems. \u2022 The 6 education systems with average science scores above the U.S. score were Korea, Singapore, Finland, Japan, the Russian Federation, and Chinese Taipei-CHN. \u2022 Among the U.S. states that participated in TIMSS at grade 4, both Florida and North Carolina scored above the TIMSS scale average but were not measurably different from the U.S. national average. \u2022 There was no measurable difference between the U.S. average science score at grade 4 in 1995 (542) and in 2011 (544). \u2022 There was no measurable difference between the U.S. average score in 2007 (539) and in 2011 (544). \u2022 The percentage of 4th-graders performing at or above the Advanced international science benchmark in 2011 was higher than in the United States in 3 education systems, was not different in 6 education systems, and was lower than in the United States in 47 education systems."}, {"section_title": "Science at grade 8", "text": "\u2022 In 2011, the average science score of U.S. 8th-graders (525) was higher than the TIMSS scale average, which is set at 500. \u2022 At grade 8, the United States was among the top"}, {"section_title": "CONTENTS", "text": "\n"}, {"section_title": "List of Tables", "text": "Page Table 1. Participation in the TIMSS assessment, by education system: 1995, 1999, 2003, 2007, and 2011 ................... Tables -Continued   Page   Table 23. Average mathematics scores in grade 8 for selected student groups in public schools in Minnesota: 2011 .............................................................................................................................................  xii"}, {"section_title": "List of", "text": ""}, {"section_title": "List of Figures", "text": "Page Figure 1. Change in average mathematics scores of 4th-grade students, by education system: 2007. Change in average mathematics scores of 8th-grade students, by education system: 2007-2011 and 1995-2011 ................................................................................................................................              "}, {"section_title": "INTRODUCTION Introduction", "text": ""}, {"section_title": "TIMSS in brief", "text": "The Trends in International Mathematics and Science Study (TIMSS) is an international comparative study of student achievement. TIMSS 2011 represents the fifth such study since TIMSS was first conducted in 1995. Developed and implemented at the international level by the International Association for the Evaluation of Educational Achievement (IEA), an international organization of national research institutions and governmental research agencies, TIMSS is used to measure the mathematics and science knowledge and skills of 4th-and 8th-graders over time. TIMSS is designed to align broadly with mathematics and science curricula in the participating countries and education systems. The results, therefore, suggest the degree to which students have learned mathematics and science concepts and skills likely to have been taught in school. TIMSS also collects background information on students, teachers, schools, curricula, and official education policies to allow cross-national comparison of educational contexts that may be related to student achievement. In 2011, there were 54 countries and 20 other education systems that participated in TIMSS, at the 4th-or 8th-grade level, or both. 1 For the purposes of this report, \"countries\" are complete, independent political entities, whereas \"other education systems\" represent a portion of a country, nation, kingdom, or emirate or are other non-national entities. Thus the category \"other education systems\" includes all U.S. states and Canadian provinces that participated as \"benchmarking participants\" 2 as well as Flemish Belgium, Chinese Taipei, England, Hong Kong Special Administrative Region, Northern Ireland, and the Palestinian National Authority. In this report these \"other education systems\" are designated as such by their national three-letter international abbreviation appended to their name (e.g., England-GBR, Ontario-CAN). This report presents the performance of U.S. students relative to their peers in other countries and other education systems, and reports on changes in mathematics and science achievement since 1995. Most of the findings in the report are based on the results presented in two international reports published by the IEA and available online at http://www.timss.org: \u2022 TIMSS 2011 International Results in Mathematics ; and \u2022 TIMSS 2011 International Results in Science ."}, {"section_title": "Countries or Education Systems?", "text": "The international bodies that coordinate international assessments vary in the labels they apply to participating entities. For example, the IEA, which coordinates TIMSS and the Progress in International Reading Literacy Study (PIRLS), differentiates between IEA members, which the IEA refers to as \"countries\" in all cases, and \"benchmarking participants.\" IEA members include countries such as the United States and Japan, as well as subnational entities, such as England and Scotland (which are both part of the United Kingdom), the Flemish community of Belgium and the French community of Belgium, and Hong Kong, which is a Special Administrative Region of China. IEA benchmarking participants are all subnational entities and include U.S. states, Dubai in the United Arab Emirates, and, in 2011, participating Canadian provinces. The Organization for Economic Cooperation and Development (OECD), which coordinates the Program for International Student Assessment (PISA), differentiates between OECD member countries and all other participating entities (called \"partner countries\" or \"partner economies\"), which include countries and subnational entities. In PISA, the United Kingdom and Belgium are reported as whole countries. Hong Kong is a PISA partner country, as are countries like Singapore, which is not an OECD member but is an IEA member. In an effort to increase the comparability of results across the international assessments in which the United States participates, this report uses a standard international classification of nation-states (see the U.S. State Department list of \"independent states\" at http://www.state.gov/s/inr/rls/4250.htm) to report out separately \"countries\" and \"other education systems,\" which include all other non-national entities that received a TIMSS score. This report's tables and figures, which are primarily adapted from the IEA's TIMSS 2011 report, follow the IEA TIMSS convention of placing members and nonmembers in separate parts of the tables and figures in order to facilitate readers moving between the international and U.S. national report. However, the text of this report refers to \"countries\" and \"other education systems,\" following the standard classification of nation-states."}, {"section_title": "INTRODUCTION", "text": "HIGHLIGHTS FROM TIMSS 2011 Table 1. Participation in the TIMSS assessment, by education system: 1995, 1999, 2003, 2007, and 2011 Year and grade   Education system  1995  1999  2003  2007  2011  Total count  52  54  51  65  77  Total IEA members count  44  37  47  57  It is important to note that comparisons in this report treat all participating education systems equally, as is done in the international reports. Thus, the United States is compared with some education systems that participated in the absence of a complete national sample (e.g., Northern Ireland-GBR partici\u00acpated but there was no national United Kingdom sample) as well as with some education systems that participated as part of a complete national sample (e.g., Alabama-USA participated as a separate state sample of public schools and as part of the United State national sample of all schools). For a number of countries and education systems, changes in achievement can be documented over the last 16 years, from 1995 to 2011. For those that began participating in TIMSS data collections after 1995, changes can only be documented over a shorter period of time. Table 1 shows the countries and other education systems that participated in TIMSS 2011 as well as their participation status in the earlier TIMSS data collections. The TIMSS 4th-grade assessment was implemented in 1995, 2003, 2007, and 2011, while the 8th-grade assessment was implemented in 1995,1999,2003,2007, and 2011. Table 1. Participation in the TIMSS assessment, by education system: 1995, 1999, 2003, 2007,  8 \u2021 Participated in assessment but results not reported. 1 Administered the TIMSS 4th-grade assessment to 6th-grade students and the 8th-grade assessment to 9th-grade students in 2011.\n2 Administered the TIMSS 8th-grade assessment to 9th-grade students in 2011. 3 Administered the TIMSS 4th-grade assessment to a national sample of 4th-grade students and a national sample of 6th-grade students in 2011. NOTE: Italics indicates participants identified and counted in this report as an education system and not as a separate country. The number in the table indicates the grade level of the assessment administered. TIMSS did not assess grade 4 in 1999. Only education systems that completed the necessary steps for their data to meet TIMSS standards and be eligible to appear in the reports from the International Study Center are listed. Unless otherwise noted, education systems sampled students enrolled in the grade corresponding, respectively, to the 4th and 8th year of formal schooling, counting the International Standard Classification of Education (ISCED) Level 1 as the first year of formal schooling, providing that the mean age at the time of testing was, respectively, at least 9.5 and 13.5 years. In the United States and most other countries this corresponds, respectively, to grade 4 and grade 8. Benchmarking education systems are subnational entities that are not members of the IEA but chose to participate in TIMSS to be able to compare themselves internationally. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 1995(TIMSS), , 1999(TIMSS), , 2003(TIMSS), , 2007(TIMSS), , and 2011 This report describes additional details about the achievement of U.S. students that are not available in the international reports, such as the achievement of students of different racial and ethnic and socioeconomic backgrounds. Results are presented in tables, figures, and text summaries of the tables and figures. In the interest of brevity, in most cases, the text reports only the names of countries and other education systems (including U.S. states) scoring higher than or not measurably different from the United States (not those scoring lower than the United States). In addition, because all TIMSS participants are treated equally, comparisons are made throughout this report between the United States (national sample) and the U.S. states that participated in TIMSS 2011 not only as part of the U.S. national sample of public and private schools but also individually with state-level public school samples. Summaries for each of these U.S. states are included in the section, \"Performance within the United States.\" Participating countries and education systems administered TIMSS to a probability sample of 4th-and 8th-grade students and schools, based on standardized definitions. TIMSS required participating countries and other education systems to draw samples of students who were nearing the end of their fourth or eighth year of formal schooling, counting from the first year of the International Standard Classification of Education (ISCED) Level 1. 4 In most education systems, including the United States, these students were in the 4th and 8th grades. Details on the average age at the time of testing in each education system are included in appendix A.\n"}, {"section_title": "Design and administration of TIMSS", "text": "In the United States, one sample was drawn to represent the nation at grade 4 and another at grade 8. In addition to these two national samples, several state public school samples 4 The ISCED was developed by the United Nations Educational, Scientific, and Cultural Organization (UNESCO) to assist countries in providing comparable, cross-national data. ISCED Level 1 is termed primary schooling, and in the United States is equivalent to the first through sixth grades (Matheson et al. 1996)."}, {"section_title": "HIGHLIGHTS FROM TIMSS 2011", "text": "were also drawn at both grades in order to benchmark those states' student performance internationally. Separate state public school samples were drawn, at grade 4, for Florida and North Carolina and, at grade 8, for Alabama, California, Colorado, Connecticut, Florida, Indiana, Massachusetts, Minnesota, and North Carolina. Some of these states chose to participate as benchmarking participants in order to compare their performance internationally, and others were invited to participate in TIMSS by the National Assessment of Educational Progress (NAEP), which is conducting a study to link TIMSS and NAEP (as explained in appendix A). The states invited to participate were selected based on state enrollment size and willingness to participate, as well as on their general NAEP performance (above or below the national average on NAEP), their previous experience in benchmarking to TIMSS, and their regional distribution. In the United States, TIMSS was administered between April and June 2011. The U.S. national sample included both public and private schools, randomly selected and weighted to be representative of the nation at grade 4 and at grade 8. 5 In total, the U.S. national sample consisted of 369 schools and 12,569 students at grade 4, and 501 schools and 10,477 students at grade 8. (For the participation rates for all the U.S. state samples, see table A-1 in appendix A.) The weighted school response rate for the United States was 79 percent at grade 4 before the use of substitute schools (schools substituted for originally sampled schools that refused to participate) and 84 percent with the inclusion of substitute schools. 6 At grade 8, the weighted school response rate before the use of substitute schools as well as with the inclusion of substitute schools was 87 percent. The weighted student response rate at grade 4 was 95 percent and at grade 8 was 94 percent. Student response rates are based on a combined total of students from both sampled and substitute schools. (For the response rates for each of the U.S. states that participated in TIMSS, see table A-1 in appendix A.) Detailed information on sampling, administration, response rates, and other technical issues are in appendix A. 5 The sample frame for public schools in the United States was based on the 2011 National Assessment of Educational Progress (NAEP) sampling frame. The 2011 NAEP sampling frame was based on the 2007-08 Common Core of Data (CCD). The data for private schools are from the 2007-08 Private School Universe Survey (PSS). Any school containing at least one grade 4 or one grade 8 class was included in the school sampling frame. For more information about the NAEP sampling frame, see http://nces.ed.gov/nationsreportcard/tdw/ sample_design/. 6 Two kinds of response rates are reported here in the interests of comparability with the TIMSS international reports, which report response rates before and after \"replacement.\" However, NCES standards advise that substitute schools should not be included in the calculation of response rates (Statistical Standard 1-3-8; National Center for Education Statistics 2002). Thus, response rates calculated before the use of substitute schools (\"before replacement\") are consistent with this standard, while response rates calculated with the inclusion of substitute schools (\"after replacement\") are not consistent with NCES standards.\n\nAverage scores of students attending public schools of various poverty levels In 2011, the average mathematics score of U.S. 4th-graders in the highest poverty public schools (at least 75 percent of students eligible for free or reduced-price lunch) was not measurably different from the TIMSS scale average; however, the average scores of 4th-graders in each of the other categories of school poverty were higher than the TIMSS scale average (figure 8). Fourth-graders in the highest poverty public schools, as well as those in public schools with at least 50 percent but less than 75 percent of students eligible for free or reduced-price lunch had average scores below the U.S. national average, while those in public schools with lower proportions of low-income students scored higher, on average, than the U.S. national average. At grade 8, students in the highest poverty public schools had a lower average score than the TIMSS scale average (468 vs. 500), while students in public schools with at least 50 percent but less than 75 percent of students eligible for free or reducedprice lunch had an average score not measurably different from the TIMSS scale average. U.S. 8th-graders attending public schools with less than 50 percent of students eligible for the free or reduced-price lunch program scored higher, on average, than the TIMSS scale average in mathematics. Eighth-graders in public schools with less than 50 percent of students eligible for free or reduced-price lunch scored, on average, above the U.S. national average, while those in public schools with 50 percent or more eligible scored, on average, below the U.S. national average. Percentage of public school students eligible for free or reduced-price lunch \nSchool population. The target population is all eligible schools 2 containing either one or more 4th-grade classrooms or one or more 8th-grade classrooms. Although participating education systems were expected to include all students in the International Target Population, sometimes it was not feasible to include all of these students because of geographic or linguistic constraints specific to the country or territory. Thus, each participating education system had its own \"national\" desired target population (also referred to as the National Target Population), which was the International Target Population reduced by the exclusions of those sections of the population that were not possible to assess. Working from the National Target Population, each participating education system had to operationalize the definition of its population for sampling purposes: i.e., define their \"national\" defined target population (referred to as the National Defined Population). While each education system's National Defined Population ideally coincides with its National Target Population, in reality, there may be additional exclusions (e.g., of regions or school types) due to constraints of operationalizing the assessment.\n\nIn almost all instances, the tests for significance used were standard t tests. 22 These fell into two categories according to the nature of the comparison being made: comparisons of independent samples and comparisons of nonindependent samples. Before describing the t tests used, some background on the two types of comparisons is provided below. The variance of a difference is equal to the sum of the variances of the two initial variables minus two times the covariance between the two initial variables. A sampling distribution has the same characteristics as any distribution, except that units consist of sample estimates and not observations. Therefore, The sampling variance of a difference is equal to the sum of the two initial sampling variances minus two times the covariance between the two sampling distributions on the estimates. If one wants to determine whether girls' performance differs from boys' performance, for example, then, as for all statistical analyses, a null hypothesis has to be tested. In this particular example, it consists of computing the difference between the boys' performance mean and the girls' performance mean (or the inverse). The null hypothesis is To test this null hypothesis, the standard error on this difference is computed and then compared to the observed difference. The respective standard errors on the mean estimate for boys and girls can be easily computed. The expected value of the covariance will be equal to 0 if the two sampled groups are independent. If the two groups are not independent, as is the case with girls and boys attending the same schools within an education system, or comparing an education system's mean with the international mean that includes that particular country, the expected value of the covariance might differ from 0. In TIMSS, participating education systems' samples are independent. Therefore, for any comparison between two education systems, the expected value of the covariance will be equal to 0, and thus the standard error on the estimate is with being a tested statistic. 22 Adjustments for multiple comparisons were not applied in any of the t-tests undertaken. Within a particular education system, any subsamples will be considered as independent only if the categorical variable used to define the subsamples was used as an explicit stratification variable. Therefore, as for any computation of a standard error in TIMSS, replication methods using the supplied replicate weights are used to estimate the standard error on a difference. Use of the replicate weights implicitly incorporates the covariance between the two estimates into the estimate of the standard error on the difference. Thus, in simple comparisons of independent averages, such as the U.S. average with other education systems' averages, the following formula was used to compute the t statistic: Est 1 and est 2 are the estimates being compared (e.g., average of education system A and the U.S. average), and se 1 and se 2 are the corresponding standard errors of these averages. The second type of comparison used in this report occurred when comparing differences of nonsubset, nonindependent groups (e.g., when comparing the average scores of boys versus girls within the United States). In such comparisons, the following formula was used to compute the t statistic: Est grp1 and est grp2 are the nonindependent group estimates being compared. Se(est grp1 -est grp2 ) is the standard error of the difference calculated using a JRR procedure, which accounts for any covariance between the estimates for the two nonindependent groups."}, {"section_title": "The mathematics assessment", "text": "The TIMSS mathematics assessment is organized around two dimensions: (1) a content dimension specifying the subject matter to be assessed and (2) a cognitive dimension specifying the cognitive or thinking processes to be assessed. At grade 4, TIMSS assesses student knowledge in three content domains: number, geometric shapes and measures, and data display. At grade 8, TIMSS assesses student knowledge in four content domains: number, algebra, geometry, and data and chance. At both grades (and across all content domains), TIMSS assesses students' mathematical thinking in three cognitive domains: knowing, applying, and reasoning. Example items from the TIMSS mathematics assessment are included in appendix B (see items B-1 through B-10). The proportion of item score points devoted to a content domain and, therefore, the contribution of the content domain to the overall mathematics scale score differ somewhat across grades (as shown in table 2). For example, in 2011 at grade 4, one-half or 50 percent of the TIMSS mathematics assessment focused on the number content domain, while the analogous percentage at grade 8 was 29 percent. The proportion of items devoted to each cognitive domain was similar across grades."}, {"section_title": "The science assessment", "text": "Similarly, the TIMSS science assessment is organized around two dimensions: (1) a content dimension specifying the subject matter to be assessed and (2) a cognitive dimension specifying the cognitive or thinking processes to be assessed. At grade 4, TIMSS assesses student knowledge in three content domains: life science, physical science, and Earth science. At grade 8, TIMSS assesses student knowledge in four content domains: biology, chemistry, physics, and Earth science. At both grades (and across all content domains), TIMSS assesses students' scientific thinking in three cognitive domains: knowing, applying, and reasoning. Example items from the TIMSS science assessment are included in appendix B (see items B-11 through B-18). The proportion of item score points devoted to a content domain and, therefore, the contribution of the content domain to the overall science scale score differ somewhat across grades (as shown in table 2). For example, in 2011 at grade 4, some 21 percent of the TIMSS science assessment focused on the Earth science domain, while the analogous percentage at grade 8 was 18 percent. The proportion of items also differed slightly across grades. For example, 41 percent of the TIMSS science assessment at grade 4 focused on the knowing cognitive domain, whereas at grade 8 it was 32 percent."}, {"section_title": "For more detailed information", "text": "In both the mathematics and science assessments, items vary in terms of difficulty and the form of knowledge and skills addressed; they also differ across grade levels to reflect the nature, difficulty, and emphasis of the subject matter INTRODUCTION encountered in school at each grade. For more detailed descriptions of the range of content and cognitive domains assessed in TIMSS, see the TIMSS 2011 Assessment Frameworks (Mullis et al. 2009). The development and validation of the mathematics cognitive domains is detailed in IEA's TIMSS 2003 International Report on Achievement in the Mathematics Cognitive Domains: Findings From a Developmental Project (Mullis, Martin, and Foy 2005)."}, {"section_title": "Reporting TIMSS results", "text": "TIMSS achievement results are reported on a scale from 0 to 1,000, with a TIMSS scale average of 500 and standard deviation of 100. TIMSS provides an overall mathematics scale score and an overall science scale score as well as content and cognitive domain scores for each subject at each grade level. The scaling of data is conducted separately for each subject and grade. Data are also scaled separately for each of the content and cognitive domains. NOTE: The percentages in this table are based on the number of score points and not the number of items. Some constructedresponse items are worth more than one score point. For the corresponding percentages based on the number of items, see table A-3 in appendix A. The content domains define the specific mathematics and science subject matter covered by the assessment, and the cognitive domains define the sets of thinking processes students are likely to use as they engage with the respective subject's content. Each of the subject content domains has several topic areas. Each topic area is presented as a list of objectives covered in a majority of participating education systems, at either grade 4 or 8. However, the cognitive domains of mathematics and science are defined by the same three sets of expected processing behaviors-knowing, applying, and reasoning. Detail may not sum to totals because of rounding. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 2011."}, {"section_title": "INTRODUCTION HIGHLIGHTS FROM TIMSS 2011", "text": "Although each scale was created to have a mean of 500 and a standard deviation of 100, the subject matter and the level of difficulty of items necessarily differ between subject, grade, and domains. Therefore, direct comparisons between scores across subjects, grades, and different domain types should not be made. (For details on why such comparisons are not warranted, see \"Weighting, scaling, and plausible values\" in appendix A.) However, scores within a subject, grade, and domain (e.g., grade 4 mathematics content domain) are comparable over time. The TIMSS scale was established originally to have a mean of 500 set as the average of all of the countries and education systems that participated in TIMSS 1995 at the 4th and 8th grades. Successive TIMSS assessments since then (TIMSS 1999(TIMSS , 2003(TIMSS , 2007(TIMSS , and 2011 have scaled the achievement data so that scores are equivalent from assessment to assessment. 7 Thus, for example, a score of 500 in 8th-grade mathematics in 2011 is equivalent to a score of 500 in 8th-grade mathematics in 2007, in 2003, in 1999, and in 1995. The same example would be true for 4th-grade mathematics scores as well as science scores at either grade. (For more information on how the TIMSS scale was created, see \"Weighting, scaling, and plausible values\" in appendix A.) In addition to scale scores, TIMSS has also developed international benchmarks for each subject and grade. The TIMSS international benchmarks provide a way to interpret the scale scores and to understand how students' proficiency in mathematics and science varies along the TIMSS scale. The TIMSS benchmarks describe four levels of student achievement (Advanced, High, Intermediate, and Low) for each subject and grade, based on the kinds of skills and knowledge students at each score cutpoint would need to successfully answer the mathematics and science items. The score cutpoints for the TIMSS benchmarks were set in 2003 based on the distribution of students along the TIMSS scale in previous administrations. 8 More information on the development of the benchmarks and the procedures used to set the score cutpoints can be found in the TIMSS and PIRLS Methods and Procedures (Martin and Mullis 2011). 7 Even though the number and composition of education systems participating in TIMSS have changed between 1995 and 2011, comparisons between the 2011 results and prior results are still possible because the achievement scores in each of the TIMSS assessments are placed on a scale which is not dependent on the list of participating countries in any particular year. A brief description of the assessment equating and scaling is presented in appendix A to this volume. A more detailed presentation can be found in the TIMSS and PIRLS Methods and Procedures . 8 For the TIMSS 1995 and 1999 assessments, the TIMSS scales were anchored using percentiles (90th, 75th, 50th, and 25th percentiles) instead of score cutpoints. By TIMSS 2003, however, it was clear that, with different education systems participating in each TIMSS cycle (and potentially different achievement for education systems in each cycles), TIMSS needed a set of points to serve as benchmarks that would not change in the future, that made sense, and that were similar to the points used in 1999. For these reasons, TIMSS selected the set of four score points (400, 475, 550, and 625) with equal intervals on the mathematics and science achievement scales that have been used ever since 2003 as the international benchmark cutpoints. All differences described in this report are statistically significant at the .05 level. No statistical adjustments to account for multiple comparisons were used. Differences that are statistically significant are discussed using comparative terms such as \"higher\" and \"lower.\" Differences that are not statistically significant are either not discussed or referred to as \"not measurably different\" or \"not statistically significant.\" In the latter case, failure to find a difference as statistically significant does not necessarily mean that there was no difference. It could be that a real difference cannot be detected by the significance test because of small sample size or imprecise measurement in the sample. If the statistical test is significant, this means that there is convincing evidence (though no guarantee) of a real difference in the population. However, it is important to remember that statistically significant results do not necessarily identify those findings that have policy significance or practical importance. Supplemental tables providing all estimates and standard errors discussed in this report are available online at http://nces.ed.gov/pubsearch/pubsinfo.asp?pubid=2013009. All data presented in this report are used to describe relationships between variables. These data are not intended, nor can they be used, to imply causality. Student performance can be affected by a complex mix of educational and other factors that are not examined here."}, {"section_title": "Nonresponse bias in the U.S. TIMSS samples", "text": "NCES Statistical Standards require a nonresponse bias analysis if school-level response rates fall below 85 percent, as they did for the 4th-grade school sample in TIMSS 2011. As a consequence, a nonresponse bias analysis was undertaken for the 4th-grade school sample similar to that used for TIMSS 2007 ). 9 Nonresponse bias analyses examined whether the participation status of schools (participant/non-participant) was related to seven school characteristics: region of the country in which the school was located (Northeast, Midwest, South, West); type of community served by the school (city, suburban, town, rural); whether the school was public or private; percentage of students eligible for free or reduced-price lunch; number of students enrolled in 4th-grade; total number of students; and percentage of students from minority backgrounds. (See appendix A for a detailed description of this analysis.) The findings indicate some potential for bias in the data arising from school control, enrollment, regional and community-type differences in participation, along with the fact that schools with higher percentages of minority students were less likely to participate. Specifically, public schools were much more INTRODUCTION likely to participate than private schools, grade 4 schools in the Midwest region were more likely to participate than schools in the other regions, and rural schools were more likely to participate than schools in central cities. However, with the inclusion of substitute schools and school nonresponse adjustments applied to the weights, 10 there were no measurable differences by school control, enrollment, and community type; only differences by region remained. Grade 4 schools with higher percentages of minority students were less likely to participate, but the measurable differences were small after substitution. Since TIMSS is conducted under a set of standard rules designed to facilitate international comparisons, the U.S. nonresponse bias analysis results were not used to adjust the U.S. data for this source of bias. While this may be possible at some later date, at present the variables identified above remain as potential sources of bias in the published estimates. See appendix A for additional details on the findings. The full text of the nonresponse bias analysis conducted for TIMSS 2011 will be included in the technical report released with the U.S. national dataset.\nNCES standards require a nonresponse bias analysis if the school-level response rate falls below 85 percent of the sampled schools (standard 2-2-2; NCES Education Statistics 2002), as it did for the 4th-grade sample. As a consequence, a nonresponse bias analysis was initiated and took a form similar to that adopted for TIMSS 2003 (Ferraro and Van de Kerckhove 2006). A full report of this study will be included in a technical report to be released with the U.S. national TIMSS dataset. The state samples were sufficient enough that none required a nonresponse bias analysis. Three methods were chosen to perform this analysis. The first method focused exclusively on the sampled schools and ignored substitute schools. The schools were weighted by their school base weights, excluding any nonresponse adjustment factor. The second method focused on sampled schools plus substitute schools, treating as nonrespondents those schools from which a final response was not received. Again, schools were weighted by their base weights, with the base weight for each substitute school set to the base weight of the original school that it replaced. The third method repeated the analyses from the second method using nonresponse adjusted weights. 13 In order to compare TIMSS respondents and nonrespondents, it was necessary to match the sample of schools back to the sample frame to identify as many characteristics as possible that might provide information about the presence of nonresponse bias. 14 The characteristics available for analysis in the sampling frame were taken from the CCD for public schools, and from the PSS for private schools. For categorical variables, the distribution of the characteristics for respondents was compared with the distribution for all schools. The hypothesis of independence between a given school characteristic and the response status (whether or not the school participated) was tested using a Rao-Scott modified chi-square statistic. For continuous variables, summary means were calculated and the difference between means was tested using a t test. Note that this procedure took account of the fact that the two samples in question were not independent samples, but in fact the responding sample was a subsample of the full sample. This effect was accounted for in calculating the standard error of the difference. Note also that in those cases where both samples were weighted using just the base weights, the test is exactly equivalent to testing that the mean of the respondents was equal to the mean of the nonrespondents. In addition, multivariate logistic regression models were set up to identify whether any of the school characteristics were significant in predicting response status when the effects of all potential influences available for modeling were considered simultaneously. Public and private schools were modeled together using the following variables: 15 community type (city, suburban, town, and rural); control of school (public or private); Census region (Northeast, Southeast, Central, and West); poverty level (percentage of students in school eligible for free or reduced-13 A detailed treatment of the meaning and calculation of sampling weights, including the nonresponse adjustment factors, is provided in TIMSS and PIRLS Methods and Procedures (Martin and Mullis 2011). 14 Comparing characteristics for respondents and nonrespondents is not always a good measure of nonresponse bias if the characteristics are either unrelated or weakly related to more substantive items in the survey. Nevertheless, this is often the only approach available. 15 NAEP region and community type were dummy coded for the purposes of these analyses. In the case of NAEP region, \"West\" was used as the reference group. For community type, \"urban fringe/large town\" was chosen as the reference group. APPENDIX A price lunch); 16 number of students enrolled in 4th-grade; total number of students; and, percentage minority students. 17 Results for the original sample of schools. In the analyses for the original sample of schools, all substituted schools were treated as nonresponding schools. The results of these analyses follow. Fourth grade. In the investigation into nonresponse bias at the school level for TIMSS 4th-grade schools, comparisons between schools in the eligible sample and participating schools showed that there was no relationship between response status and the majority of school characteristics available for analysis. In separate variable-by-variable bivariate analyses, six variables were found to be statistically significantly related to participation in the bivariate analysis: school control, community type, 4th-grade enrollment, the percentage of Black, non-Hispanic students, the percentage of Hispanic students, and the percentage of American Indian or Alaska Native students. Although each of these findings indicates some potential for nonresponse bias, when all of these factors were considered simultaneously in a regression analysis, private schools, the South (as a region), total school enrollment, 4th-grade enrollment, and the percentage of American Indian or Alaska Native students and Hispanics were significant predictors of participation. The second model showed that private schools, total school enrollment, and 4thgrade enrollment were significant predictors of participation. Results for the final sample of schools. In the analyses for the final sample of schools, all substitute schools were included with the original schools as responding schools, leaving nonresponding schools as those for which no assessment data were available. The results of these analyses follow and are somewhat more complicated than the analyses for the original sample of schools. Fourth grade. The bivariate results for the final sample of 4thgrade schools indicated that five of the six variables remained 16 The measure of school poverty is based on the proportion of students in a school eligible for the free or reduced-price lunch (FRPL) program, a federally assisted meal program that provides nutritionally balanced, low-cost or free lunches to eligible children each school day. For the purposes of the nonresponse bias analyses, schools were classified as \"low poverty\" if less than 50 percent of the students were eligible for FRPL and as \"high poverty\" if 50 percent or more of the students were eligible. Since the nonresponse bias analyses involve both participating and nonparticipating schools, they are based, out of necessity, on data from the sampling frame. TIMSS data are not available for nonparticipating schools. The school frame data are derived from the CCD and PSS. The CCD data provide information on the percentage of students in each school who are eligible for free or reduced-price lunch, but are limited to public schools. The PSS data do not provide the same information for private schools. In the interest of retaining all of the schools and students in these analyses, private schools were assumed to be low-poverty schools-that is, they were assumed to be schools in which less than 50 percent of students were eligible for FRPL. Separate analyses of the TIMSS data for participating private schools suggest the reasonableness of this assumption. Of the 21 grade 4 private schools, only one reports having 50 percent or more of students eligible for FRPL. 17 Two forms of this school attribute were used in the analyses. In the bivariate analyses the percentage of each racial/ethnic group was related separately to participation status. In the logistic regression analyses a single measure was used to characterize each school, namely, \"percentage of minority students.\" statistically significant in the bivariate analysis: school control, 4th-grade enrollment, the percentage of Black, non-Hispanic students, the percentage of Hispanic students, and the percentage of American Indian or Alaska Native students. When all of these factors were considered simultaneously in a regression analysis, private schools, total school enrollment, and the percentage of American Indian or Alaska Native students remained significant predictors of participation. For the final sample of schools in 4th grade, with school nonresponse adjustments applied to the weights, only two variables were statistically significant in the bivariate analysis: region and the percentage of Black, non-Hispanic students. 18 The multivariate regression analysis cannot be conducted after the school nonresponse adjustments are applied to the weights. These results suggest that there is some potential for nonresponse bias in the U.S. 4th-grade original sample based on the characteristics studied. It also suggests that, while there is no evidence that the use of substitute schools reduced the potential for bias, it has not added to it substantially and the inclusion of the replacement schools increased sample available for analyses. The application of school nonresponse adjustments substantially reduced the potential for bias."}, {"section_title": "Further information", "text": "To assist the reader in understanding how TIMSS relates to the National Assessment of Educational Progress (NAEP), the primary source of national-and state-level data on U.S. students' mathematics and science achievement, NCES compared the form and content of the TIMSS and NAEP mathematics and science assessments. A summary of the results of this comparison is included in appendix C. Appendix D includes a list of TIMSS publications and resources published by NCES and the IEA. Standard errors for the estimates discussed in this report are available online at http://nces.ed.gov/pubsearch/pubsinfo.asp?pubid=2013009. Detailed information on TIMSS can also be found on the NCES website at http://nces.ed.gov/timss and the international TIMSS website at http://www.timss.org. 10 The international weighting procedures created a nonresponse adjustment class for each explicit stratum; see the TIMSS and PIRLS Methods and Procedures  for details. In the case of the U.S. 4thgrade sample, 8 explicit strata were formed by poverty level, school control, and Census region. The procedures could not be varied for individual countries to account for any specific needs. Therefore, the U.S. nonresponse bias analyses could have no influence on the weighting procedures and were undertaken after the weighting process was complete."}, {"section_title": "MATHEMATICS Mathematics Performance in the United States and Internationally", "text": "Average scores in 2011 In mathematics, the U.S. national average score was 541 at grade 4 and 509 at grade 8 (tables 3 and 4). Both scores were higher than the TIMSS scale average, which is set at 500 for every administration of TIMSS at both grades. 11 Among the 45 countries that participated at grade 4, the U.S. average mathematics score was among the top 8 (3 countries had higher averages and 4 had averages not measurably different from the United States). Thirty-seven countries had a lower average score than the United States. Looking at all 57 education systems that participated at grade 4 (i.e., both countries and other education systems, including U.S. states that participated in TIMSS with individual state samples), the United States was among the top 15 education systems in average mathematics scores (8 education systems had higher averages and 6 were not measurably different). Singapore, Korea, Hong Kong-CHN, Chinese Taipei-CHN, Japan, Northern Ireland-GBR, North Carolina-USA, and Belgium (Flemish)-BEL had higher average scores than the United States; and Finland, Florida-USA, England-GBR, the Russian Federation, the Netherlands, and Denmark had average scores not measurably different from the U.S. average at grade 4. The United States outperformed 42 education systems. 11 A score of 500 represents the international average of participants in the first administration of TIMSS in 1995. The TIMSS scale is the same in each administration such that a value of 500 in 2011 equals 500 in 1995. At grade 8, among the 38 countries that participated in TIMSS, the U.S. average mathematics score was among the top 11 (4 countries had higher averages and 6 had averages not measurably different from the United States). Twenty-seven countries had lower average scores than the United States. Looking at all 56 education systems that participated at grade 8, the United States was among the top 24 education systems in average mathematics scores (11 had higher averages and 12 were not measurably different). Korea, Singapore, Chinese Taipei-CHN, Hong Kong-CHN, Japan, Massachusetts-USA, Minnesota-USA, the Russian Federation, North Carolina-USA, Quebec-CAN, and Indiana-USA had higher average scores than the United States; and Colorado-USA, Connecticut-USA, Israel, Finland, Florida-USA, Ontario-CAN, England-GBR, Alberta-CAN, Hungary, Australia, Slovenia, and Lithuania had average scores not measurably different from the U.S. average at grade 8. The United States had a higher average mathematics score than 32 education systems."}, {"section_title": "MATHEMATICS", "text": "HIGHLIGHTS FROM TIMSS 2011 \nAverage score is higher than U.S. average score. Average score is lower than U.S. average score. 1 National Defined Population covers 90 to 95 percent of National Target Population (see appendix A). 2 National Defined Population covers less than 90 percent, but at least 77 percent, of National Target Population (see appendix A). 3 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 4 National Target Population does not include all of the International Target Population (see appendix A). 5 Exclusion rates for Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. 7 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 25 percent. NOTE: Education systems are ordered by 2011 average score. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. All average scores reported as higher or lower than the U.S. average score are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one education system may be significant while a large difference between the United States and another education system may not be significant. \n\n\n\u2022 At grade 4, all racial/ethnic groups performed above the TIMSS scale average. At grade 8, White, Asian, and multiracial students' average scores were above the TIMSS scale average, while Black and Hispanic students' average scores were not measurably different from the TIMSS scale average (table 25). \u2022 In general, students at grade 4 scored higher, on average, than the TIMSS scale average. At grade 8 students in public schools with less than 50 percent of students eligible for free or reduced-price lunch scored higher, on average, than the TIMSS scale average, while average scores for students in public schools with 50 percent or more students eligible for free or reduced-price lunch were not measurably different from the TIMSS scale average. "}, {"section_title": "Benchmarking education systems", "text": "North Carolina-USA 1,3 554 Florida-USA 3,8 545 Average score is lower than U.S. average score. 1 National Defined Population covers 90 to 95 percent of National Target Population (see appendix A). 2 Met guidelines for sample participation rates only after replacement schools were included. 3 National Target Population does not include all of the International Target Population (see appendix A). 4 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 5 Exclusion rates for Azerbaijan and Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. 7 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 25 percent. 8 National Defined Population covers less than 90 percent, but at least 77 percent, of National Target Population (see appendix A). NOTE: Education systems are ordered by 2011 average score. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. All average scores reported as higher or lower than the U.S. average score are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one education system may be significant while a large difference between the United States and another education system may not be significant. The standard errors of the estimates are shown in table E-1 available at http://nces.ed.gov/pubsearch/pubsinfor.asp?pubid=2013009. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 2011. 545 North Carolina-USA 2,4 537 Quebec-CAN 532 Indiana-USA 1,4 522 Colorado-USA 4 518 Connecticut-USA 1,4 518 Florida-USA 1,4 513"}, {"section_title": "Change in scores", "text": "Several education systems that participated in TIMSS 2011 also participated in the last administration of TIMSS in 2007 or in the first administration of TIMSS in 1995. Some education systems participated in both of these previous administrations. Comparing scores between previous administrations of TIMSS and the most recent administration provides perspective on change over time. 12 12 Several participating countries that are reported with the 2011 results in other tables in this report are excluded from these comparisons over time based on the International Study Center (ISC) review of the assessment results. Kuwait, Morocco, and Yemen participated at grade 4 in both 2007 or 1995 and 2011, but had unreliable 2011 mathematics scores. Armenia, Kazakhstan, and Qatar also participated in 2007 and 2011 at grade 4, but their 2007 mathematics scores were not comparable to their 2011 scores. Kuwait, Italy, and Thailand participated in 1995 and 2011 at both grades 4 and 8, but their 1995 mathematics scores were not comparable to their 2011 scores. Ghana and Morocco participated in 2007 and 2011 at grade 8, but their 2011 mathematics scores were unreliable. Armenia, Qatar, Saudi Arabia, and Turkey participated in 2007 and 2011 at grade 8, but their 2007 mathematics scores were not comparable to their 2011 scores. Lastly, Indonesia and Israel participated in both 1995 and 2011 at grade 8, but their 1995 mathematics scores were not comparable to their 2011 scores.\nSeveral education systems that participated in TIMSS 2011 also participated in the last administration of TIMSS in 2007 or in the first administration of TIMSS in 1995. Some education systems participated in both of these previous administrations. Comparing scores between previous administrations of TIMSS and the most recent administration provides perspective on change over time. 17"}, {"section_title": "Change at grade 4 between 2007 and 2011", "text": "Among the 28 education systems that participated in both the 2007 and 2011 TIMSS mathematics assessments at grade 4, the average mathematics score increased in 12 education systems, including the United States. There was no measurable change in the other 16 education systems that participated in TIMSS in both these years, and in none did average scores decrease measurably (figure 1). The U.S. increase in average score at grade 4 between 2007 and 2011 was 12 score points (from 529 to 541). Five education systems had larger increases than the United States during this time: Tunisia (32 points), the Islamic Republic of Iran (28 points), the Czech Republic (24 points), Dubai-UAE (24 points), and Norway (22 points). Despite experiencing larger gains than the United States between the two time points, all five of these education systems had lower average scores than the United States in 2011. Thus, none of these increases changed these education systems' standing relative to the United States between 2007 and 2011.  \nAmong the 28 education systems that participated in both the 2007 and 2011 TIMSS science assessments at grade 4, the average science score increased in 9 education systems and decreased in 5 education systems (figure 9). In the rest, including the United States, there was no measurable change in the average grade 4 science scores between 2007 and 2011. The education systems in which 4th-graders' average scores increased between 2007 and 2011 were Georgia (37 points), Tunisia (27 points), the Czech Republic (21 points), Norway (17 points), the Islamic Republic of Iran (17 points), Denmark (11 points), Japan (11 points), Sweden (9 points), and the Netherlands (8 points). None of these increases changed these education systems' standing relative to the United States between 2007 and 2011. 18 Scores decreased at grade 4 during this time in Hong Kong-CHN (19 points), England-GBR (13 points), Australia (12 points), Italy (11 points), and New Zealand (7 points). As a result, U.S. average performance at grade 4 went from below the average of Hong Kong-CHN in 2007 to higher than that country's average in 2011, and from not measurably different from the averages of England-GBR and Italy in 2007 to higher than their averages in 2011 Average score is lower than U.S. average score. 1 National Defined Population covers 90 to 95 percent of National Target Population (see appendix A). 2 Met guidelines for sample participation rates only after replacement schools were included. 3 National Target Population does not include all of the International Target Population (see appendix A). 4 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 5 Exclusion rates for Azerbaijan and Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. 7 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 25 percent. 8 National Defined Population covers less than 90 percent, but at least 77 percent of National Target Population (see appendix A). NOTE: Education systems are ordered by 2011 average score. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. All average scores reported as higher or lower than the U.S. average score are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one education system may be significant while a large difference between the United States and another education system may not be significant. 542 Indiana-USA 1,4 533 Connecticut-USA 1,4 532 North Carolina-USA 3,4 532 Florida-USA 1,4 530 Average score is lower than U.S. average score. 1 National Defined Population covers 90 to 95 percent of National Target Population (see appendix A). 2 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 3 National Defined Population covers less than 90 percent, but at least 77 percent of National Target Population (see appendix A). 4 National Target Population does not include all of the International Target Population (see appendix A). 5 Exclusion rates for Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. NOTE: Education systems are ordered by 2011 average score. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. All average scores reported as higher or lower than the U.S. average score are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one education system may be significant while a large difference between the United States and another education system may not be significant. "}, {"section_title": "Change at grade 4 between 1995 and 2011", "text": "Among the 20 education systems that participated in both the 1995 and 2011 TIMSS mathematics assessments at grade 4, the average mathematics score increased in 13 education systems, including the United States, and decreased in 4 education systems (figure 1). In the other 3 education systems, there was no measurable change in the average grade 4 mathematics scores between 1995 and 2011. The U.S. increase in the average mathematics score at grade 4 between 1995 and 2011 was 23 score points (from 518 to 541). Five education systems had larger increases than the United States during this time: Portugal (90 points), England-GBR (58 points), Slovenia (51 points), Hong Kong-CHN (45 points), and the Islamic Republic of Iran (44 points). U.S. average performance at grade 4 went from above that of England-GBR in 1995 to being not measurably different in 2011. 13 None of the other education systems' increases changed their standing relative to the United States between 1995 and 2011. Average scores decreased during this time at grade 4 in the Czech Republic (30 points), Austria (22 points), Quebec-CAN (17 points), and the Netherlands (9 points). U.S. average performance at grade 4 went from below the averages in the Czech Republic, Austria, and Quebec-CAN in 1995 to higher than their averages in 2011, and from below the average in the Netherlands in 1995 to being not measurably different in 2011. 13 More than three-quarters of England's increase (47 points) occurred between 1995 and 2003.  1 The change in average score is calculated by subtracting the 2007 or 1995 estimate, respectively, from the 2011 estimate using unrounded numbers. 2 National Defined Population covers 90 to 95 percent of National Target Population for 2011 (see appendix A). 3 Met guidelines for sample participation rates only after replacement schools were included for 2011. 4 National Target Population does not include all of the International Target Population for 2011 (see appendix A). 5 Nearly satisfied guidelines for sample participation rates after replacement schools were included for 2011. 6 Exclusion rates for Azerbaijan and Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available for 2011. 7 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation in 2011 exceeds 15 percent, though it is less than 25 percent. NOTE: Education systems are ordered by 2011 average scores. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Data are not shown for some education systems because comparable data from previous cycles are not available. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. For 1995, Korea, Portugal, and Ontario-CAN had National Defined Population covering 90 to 95 percent of National Target Population; England-GBR had National Defined Population that covered less than 90 percent of National Target Population (but at least 77 percent) and met guidelines for sample participation rates only after replacement schools were included; Netherlands, Australia, and Austria did not satisfy guidelines for sample participation rates. For 2007, the United States, Quebec-CAN, Ontario-CAN, and Alberta-CAN had National Defined Population covering 90 to 95 percent of National Target Population; the United States and Denmark met guidelines for sample participation rates only after replacement schools were included; the Netherlands and Dubai-UAE nearly satisfied guidelines for sample participation rates after replacement schools were included; Georgia had a National Target Population that did not include all of the International Target Population; Dubai-UAE tested the same cohort of students as other countries, but later in the assessment year at the beginning of the next school year. All average scores reported as higher or lower than the U.S. average score are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one education system may be significant while a large difference between the United States and another education system may not be significant. Detail may not sum to totals because of rounding. The standard errors of the estimates are shown in table E-3 available at http://nces.ed.gov/pubsearch/pubsinfo.asp?pubid=2013009. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 1995, 2007, and 2011. \nAmong the 20 education systems that participated in both the 1995 and 2011 TIMSS science assessments at grade 4, the average science score increased in 9 education systems and decreased in 2 (figure 9). In the other 9 education systems that participated in TIMSS in both years, including the United States, there was no measurable change in the average grade 4 science scores between 1995 and 2011. The education systems in which 4th-graders' average scores increased between 1995 and 2011 were the Islamic Republic of Iran (73 points), Portugal (70 points), Singapore (60 points), Slovenia (56 points), Hong Kong-CHN (27 points), Hungary (27 points), Ontario-CAN (11 points), Korea (11 points), and Japan (5 points). The increase in the Singapore average meant that it moved from having a lower average score at grade 4 than the United States in 1995 to having a higher average score in 2011. 20 The increases in the other education systems did not change their standing relative to the United States. Scores decreased during this time for 4th-graders in Quebec-CAN (12 points) and Norway (10 points). These decreases did not change their standing relative to the United States.  See notes on next page.  *p<.05. Change in average scores is significant. 1 The change in average score is calculated by subtracting the 2007 or 1995 estimate, respectively, from the 2011 estimate using unrounded numbers. 2 National Defined Population covers 90 to 95 percent of National Target Population for 2011 (see appendix A). 3 Met guidelines for sample participation rates only after replacement schools were included for 2011. 4 National Target Population does not include all of the International Target Population for 2011 (see appendix A). 5 Nearly satisfied guidelines for sample participation rates after replacement schools were included for 2011. 6 Exclusion rates for Azerbaijan and Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available for 2011. 7 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation in 2011 exceeds 15 percent, though it is less than 25 percent. NOTE: Education systems are ordered by 2011 average scores. Italics indicate participants identified and counted in this report as an education system and not as a separate country. All education systems met international sampling and other guidelines in 2011, except as noted. Data are not shown for some education systems because comparable data from previous cycles are not available. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. For 1995, Korea, Portugal, and Ontario-CAN had National Defined Population covering 90 to 95 percent of National Target Population; England-GBR had National Defined Population that covered less than 90 percent of National Target Population (but at least 77 percent); England-GBR, Netherlands, Australia, and Austria did not satisfy guidelines for sample participation rates. For 2007, the United States, Quebec-CAN, Ontario-CAN, and Alberta-CAN had National Defined Population covering 90 to 95 percent of National Target Population; the United States and Denmark met guidelines for sample participation rates only after replacement schools were included; the Netherlands and Dubai-UAE nearly satisfied guidelines for sample participation rates after replacement schools were included; Georgia had a National Target Population that did not include all of the International Target Population; Dubai-UAE tested the same cohort of students as other countries, but later in the assessment year at the beginning of the next school year. All average scores reported as higher or lower than the U.S. average score are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between averages for one education system may be significant, while a large difference for another education system may not be significant. Detail may not sum to totals because of rounding. The standard errors of the estimates are shown in "}, {"section_title": "Change at grade 8 between 2007 and 2011", "text": "At grade 8, among the education systems that participated in both the 2007 and 2011 TIMSS mathematics assessments, the average mathematics score increased in 10 education systems and decreased in 6 education systems (figure 2). In the rest, including the United States, there was no measurable change. The education systems in which 8th-graders' average scores increased between 2007 and 2011 were the Palestinian National Authority (37 points), the Russian Federation (27 points), Georgia (22 points), Italy (19 points), Singapore (18 points), Ukraine (17 points), Dubai-UAE (17 points), Korea (16 points), Bahrain (11 points), and Chinese Taipei-CHN (11 points). The 27-point increase in the Russian Federation moved their 8th-graders from on a par with their U.S. peers in 2007 to higher than the U.S. national average in 2011. The increases in the other education systems did not change their standing relative to the United States. 14 Scores decreased during this time at grade 8 in Malaysia (34 points), Jordan (21 points), the Syrian Arab Republic (15 points), Thailand (14 points), Hungary (12 points), and Sweden (7 points). None of these decreases changed these education systems' standing relative to the United States between 2007 and 2011.    1 The change in average score is calculated by subtracting the 2007 or 1995 estimate, respectively, from the 2011 estimate using unrounded numbers. 2 National Defined Population covers 90 to 95 percent of National Target Population for 2011 (see appendix A). 3 Nearly satisfied guidelines for sample participation rates after replacement schools were included for 2011. 4 National Target Population does not include all of the International Target Population for 2011 (see appendix A). 5 Exclusion rates for Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available for 2011. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation in 2011 exceeds 15 percent, though it is less than 25 percent NOTE: Education systems are ordered by 2011 average scores. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Data are not shown for some education systems because comparable data from previous cycles are not available. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. For 1995, Lithuania's National Target Population did not include all of the International Target Population; the Russian Federation and Lithuania had a National Defined Population that covered 90 to 95 percent of National Target Population; England-GBR had a National Defined Population that covered less than 90 percent of National Target Population (but at least 77 percent); the United States, England-GBR, and Minnesota-USA met guidelines for sample participation rates only after replacement schools were included. For 2007, Lithuania, Georgia, and Indonesia had National Target Populations that did not include all of the International Target Population; Massachusetts-USA, Quebec-CAN, and Ontario-CAN had National Defined Population that covered 90 to 95 percent of National Target Population; Hong Kong-CHN, England-GBR, and Minnesota-USA met guidelines for sample participation rates only after replacement schools were included; Dubai-UAE nearly satisfied guidelines for sample participation rates after replacement schools were included. All average scores reported as higher or lower than the U.S. average score are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one education system may be significant while a large difference between the United States and another education system may not be significant. Detail may not sum to totals because of rounding. The standard errors of the estimates are shown in "}, {"section_title": "Content domain scores in 2011", "text": "In addition to overall average mathematics scores, TIMSS provides average scores by specific mathematics topics called content domains. At grade 4, TIMSS tested student knowledge in three content domains: number, geometric shapes and measures, and data display. At grade 8, TIMSS tested student knowledge in four content domains: number, algebra, geometry, and data and chance. At grade 4, the U.S. average was higher than the TIMSS scale average of 500 in all three content domains (table 5). In comparison with other education systems, U.S. 4th-graders performed better on average in number and data display than in geometric shapes and measures. That is, fewer education systems had higher average scores than the United States in these two domains than in geometric shapes and measures. In both number and data display, 8 education systems had higher average scores than the United States, whereas 12 education systems had a higher average score than the United States in the geometric shapes and measures. At grade 8, the U.S. average was higher than the TIMSS scale average of 500 in three of the four 8th-grade content domains and below the TIMSS scale average in the fourthgeometry (table 6). In comparison with other education systems, U.S. 8th-graders performed better on average in algebra than in the other three domains. That is, fewer education systems had higher average scores than the United States in algebra than in data and chance, number, or geometry. In algebra, 9 education systems had a higher average score than the United States, whereas in both number and data and chance 14 education systems had higher average scores, and in geometry 21 education systems had a higher average score. Average score is higher than U.S. score. Average score is lower than U.S. score. 1 National Defined Population covers 90 to 95 percent of National Target Population (see appendix A). 2 Met guidelines for sample participation rates only after replacement schools were included. 3 National Target Population does not include all of the International Target Population (see appendix A). 4 Exclusion rates for Azerbaijan and Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 5 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. 7 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 25 percent. 8 National Defined Population covers less than 90 percent, but at least 77 percent, of National Target Population (see appendix A). NOTE: Education systems are ordered by 2011 average score in number domain. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. All average scores reported as higher or lower than U.S. average score are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one education system may be significant while a large difference between the United States and another education system may not be significant. Average score is higher than U.S. average score. Average score is lower than U.S. average score. 1 National Defined Population covers 90 to 95 percent of National Target Population (see appendix A). 2 National Defined Population covers less than 90 percent, but at least 77 percent, of National Target Population (see appendix A). 3 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 4 National Target Population does not include all of the International Target Population (see appendix A). 5 Exclusion rates for Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. 7 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 25 percent. NOTE: Education systems are ordered by 2011 average score in number domain. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. All average scores reported as higher or lower than U.S. average score are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one education system may be significant while a large difference between the United States and another education system may not be significant. The standard errors of the estimates are shown in table E-6 available at http://nces.ed.gov/pubsearch/pubsinfor.asp?pubid=2013009. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 2011.\nIn addition to overall average science scores, TIMSS provides average scores by specific science topics called content domains. At grade 4, TIMSS tested student knowledge in three content domains: life science, physical science, and Earth science. At grade 8, TIMSS tested student knowledge in four content domains: biology, chemistry, physics, and Earth science. At grade 4, the U.S. average score was higher than the TIMSS scale average of 500 in all three content domains in 2011 (table 28). In comparison with other education systems, U.S. 4th-graders performed better on average in life science than in the other two domains. That is, fewer education systems outperformed the United States in life science than in physical science or Earth science. U.S. 4thgraders were outperformed on average by their peers in 4 Average score is higher than U.S. average score. Average score is lower than U.S. average score. 1 National Defined Population covers 90 to 95 percent of National Target Population (see appendix A). 2 Met guidelines for sample participation rates only after replacement schools were included. 3 National Target Population does not include all of the International Target Population (see appendix A). 4 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 5 Exclusion rates for Azerbaijan and Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. 7 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 25 percent. 8 National Defined Population covers less than 90 percent, but at least 77 percent of National Target Population (see appendix A). NOTE: Education systems are ordered by average score in life science domain. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. All average scores reported as higher or lower than U.S. average score are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one education system may be significant while a large difference between the United States and another education system may not be significant.  At grade 8, the U.S. average was also higher than the TIMSS scale average of 500 in all four content domains in 2011 (table  29). In comparison with other education systems, U.S. 8thgraders performed better on average in biology than in the other three domains. That is, fewer education systems had higher average scores than the United States in biology than in chemistry, physics, or Earth science. U.S. 8th-graders were outperformed on average by their peers in 9 education systems in biology, 10 education systems in Earth science, 10 education systems in chemistry, and 16 education systems in physics. Average score is higher than U.S. score. Average score is lower than U.S. score. 1 National Defined Population covers 90 to 95 percent of National Target Population (see appendix A). 2 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 3 National Defined Population covers less than 90 percent, but at least 77 percent of National Target Population (see appendix A). 4 National Target Population does not include all of the International Target Population (see appendix A). 5 Exclusion rates for Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. NOTE: Education systems are ordered by average score in biology domain. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. All average scores reported as higher or lower than U.S. average score are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one education system may be significant while a large difference between the United States and another education system may not be significant. The standard errors of the estimates are shown in table E-27 available at http://nces.ed.gov/pubsearch/pubsinfor.asp?pubid=2013009. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 2011."}, {"section_title": "MATHEMATICS Performance on the TIMSS international benchmarks", "text": "The TIMSS international benchmarks provide a way to understand how students' proficiency in mathematics varies along the TIMSS scale (table 7). TIMSS defines four levels of student achievement: Advanced, High, Intermediate, and Low. The benchmarks can then be used to describe the kinds of skills and knowledge students at each score cutpoint would need to successfully answer the mathematics items included in the assessment. The descriptions of the benchmarks differ between the two grade levels, as the mathematical skills and knowledge needed to respond to the assessment items reflect the nature, difficulty, and emphasis of the expectations at each grade. In 2011, higher percentages of U.S. 4th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. 16 For example, 13 percent of U.S. 4th-graders performed at or above the Advanced benchmark (625) compared to the international median of 4 percent. Students at the Advanced benchmark demonstrated an ability to apply their understanding and knowledge to a variety of relatively complex mathematical situations, explain 16 The international median is the median percentage for all IEA member countries (see the inset box on page 1 for IEA member countries). Thus, the international median at each benchmark represents the percentage at which half of the participating IEA member countries have that percentage of students at or above the median and half have that percentage of students below the median. For example, the Low international benchmark median of 90 percent at grade 4 indicates that half of the countries have 90 percent or more of their students who met the Low benchmark, and half have less than 90 percent of their students who met the Low benchmark."}, {"section_title": "Table 7. Description of TIMSS international mathematics benchmarks, by grade: 2011", "text": ""}, {"section_title": "Benchmark (score cutpoint)", "text": "Grade 4 Advanced 625Students can apply their understanding and knowledge in a variety of relatively complex situations and explain their reasoning. They can solve a variety of multi-step word problems involving whole numbers including proportions. Students at this level show an increasing understanding of fractions and decimals. Students can apply geometric knowledge of a range of two-and three-dimensional shapes in a variety of situations. They can draw a conclusion from data in a table and justify their conclusion.\nGrade 4 Advanced (625) Students apply knowledge and understanding of scientific processes and relationships and show some knowledge of the process of scientific inquiry. Students communicate their understanding of characteristics and life processes of organisms, reproduction and development, ecosystems and organisms' interactions with the environment, and factors relating to human health. They demonstrate understanding of properties of light and relationships among physical properties of materials, apply and communicate their understanding of electricity and energy in practical contexts, and demonstrate an understanding of magnetic and gravitational forces and motion. Students communicate their understanding of the solar system and of Earth's structure, physical characteristics, resources, processes, cycles, and history. They have a beginning ability to interpret results in the context of a simple experiment, reason and draw conclusions from descriptions and diagrams, and evaluate and support an argument."}, {"section_title": "High (550)", "text": "Students can apply their knowledge and understanding to solve problems. Students can solve word problems involving operations with whole numbers. They can use division in a variety of problem situations. They can use their understanding of place value to solve problems. Students can extend patterns to find a later specified term. Students demonstrate understanding of line symmetry and geometric properties. Students can interpret and use data in tables and graphs tosolve problems. They can use information in pictographs and tally charts to complete bar graphs.\nStudents can apply their understanding and knowledge in a variety of relatively complex situations. Students can use information from several sources to solve problems involving different types of numbers and operations. Students can relate fractions, decimals, and percents to each other. Students at this level show basic procedural knowledge related to algebraic expressions. They can use properties of lines, angles, triangles, rectangles, and rectangular prisms to solve problems. They can analyze data in a variety of graphs.\nStudents apply their knowledge and understanding of the sciences to explain phenomena in everyday and abstract contexts. Students demonstrate some understanding of plant and animal structure, life processes, life cycles and reproduction. They also demonstrate some understanding of ecosystems and organisms' interactions with their environment, including understanding of human responses to outside conditions and activities. Students demonstrate understanding of some properties of matter, electricity and energy, and magnetic and gravitational forces and motion. They show some knowledge of the solar system, and of Earth's physical characteristics, processes, and resources. Students demonstrate elementary knowledge and skills related to scientific inquiry. They compare, contrast, and make simple inferences, and provide brief descriptive responses combining knowledge of science concepts with information from both everyday and abstract contexts.\nStudents demonstrate understanding of concepts related to science cycles, systems, and principles. They demonstrate understanding of aspects of human biology, and of the characteristics, classification, and life processes of organisms. Students communicate understanding of processes and relationships in ecosystems. They show an understanding of the classification and compositions of matter and chemical and physical properties and changes. They apply knowledge to situations related to light and sound and demonstrate basic knowledge of heat and temperature, forces and motion, and electrical circuits and magnets. Students demonstrate an understanding of the solar system and of Earth's processes, physical features, and resources. They demonstrate some scientific inquiry skills. They also combine and interpret information from various types of diagrams, contour maps, graphs, and tables; select relevant information, analyze, and draw conclusions; and provide short explanations conveying scientific knowledge."}, {"section_title": "Intermediate (475)", "text": "Students can apply basic mathematical knowledge in straightforward situations. Students at this level demonstrate an understanding of whole numbers and some understanding of fractions. Students can visualize three-dimensional shapes from two-dimensional representations. They can interpret bar graphs, pictographs, and tables to solve simple problems.\nStudents can apply basic mathematical knowledge in straightforward situations. Students can solve problems involving decimals, fractions, proportions, and percentages. They understand simple algebraic relationships. Students can relate a two-dimensional drawing to a three-dimensional object. They can read, interpret, and construct graphs and tables. They recognize basic notions of likelihood.\nStudents have basic knowledge and understanding of practical situations in the sciences. Students recognize some basic information related to characteristics of living things, their reproduction and life cycles, and their interactions with the environment, and show some understanding of human biology and health. They also show some knowledge of properties of matter and light, electricity and energy, and forces and motion. Students know some basic facts about the solar system and show an initial understanding of Earth's physical characteristics and resources. They demonstrate ability to interpret information in pictorial diagrams and apply factual knowledge to practical situations.\nStudents recognize and apply their understanding of basic scientific knowledge in various contexts. Students apply knowledge and communicate an understanding of human health, life cycles, adaptation, and heredity, and analyze information about ecosystems. They have some knowledge of chemistry in everyday life and elementary knowledge of properties of solutions and the concept of concentration. They are acquainted with some aspects of force, motion, and energy. They demonstrate an understanding of Earth's processes and physical features, including the water cycle and atmosphere. Students interpret information from tables, graphs, and pictorial diagrams and draw conclusions. They apply knowledge to practical situations and communicate their understanding through brief descriptive responses."}, {"section_title": "Low (400)", "text": "Students have some basic mathematical knowledge. Students can add and subtract whole numbers. They have some recognition of parallel and perpendicular lines, familiar geometric shapes, and coordinate maps. They can read and complete simple bar graphs and tables.\nStudents have some knowledge of whole numbers and decimals, operations, and basic graphs. NOTE: Score cutpoints for the international benchmarks are determined through scale anchoring. Scale anchoring involves selecting benchmarks (scale points) on the achievement scales to be described in terms of student performance, and then identifying items that students scoring at the anchor points can answer correctly. The score cutpoints are set at equal intervals along the achievement scales. The score cutpoints were selected to be as close as possible to the standard percentile cutpoints (i.e., 90th, 75th, 50th, and 25th percentiles). More information on the setting of the score cutpoints can be found in appendix A and .   1 National Defined Population covers 90 to 95 percent of National Target Population (see appendix A). 2 Met guidelines for sample participation rates only after replacement schools were included. 3 National Target Population does not include all of the International Target Population (see appendix A). 4 Exclusion rates for Azerbaijan and Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 5 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. 7 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 25 percent. 8 National Defined Population covers less than 90 percent, but at least 77 percent of National Target Population (see appendix A). NOTE: Education systems are ordered by percentage at Advanced international benchmark. Italics indicate participants identified and counted in this report as an education system and not as a separate country. The TIMSS international median represents all participating TIMSS education systems, including the United States, shown in the main part of the figure; benchmarking education systems are not included in the median. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one education system may be significant while a large difference between the United States and another education system may not be significant. The standard errors of the estimates are shown in table E-7 available at http://nces.ed.gov/pubsearch/pubsinfor.asp?pubid=2013009. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 2011. their reasoning, and draw and justify conclusions from data (see description in table 7). The percentage of 4th-graders performing at or above the Advanced international mathematics benchmark was higher than in the United States in 7 education systems; was not different in 4 education systems; and was lower than in the United States in 45 education systems. Singapore, Korea, Hong Kong-CHN, Chinese Taipei-CHN, Japan, Northern Ireland-GBR, and England-GBR had a higher percentage of students performing at or above the Advanced international mathematics benchmark than the United States at grade 4; and North Carolina-USA, the Russian Federation, Florida-USA, and Finland had percentages not measurably different from the U.S. percentage. Similar to their 4th-grade counterparts, higher percentages of U.S. 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians (figure 4). For example, 7 percent of U.S. 8thgraders performed at or above the Advanced benchmark (625) compared to the international median of 3 percent. Students at the Advanced benchmark demonstrated an ability to reason with information, draw conclusions, make generalizations, and solve linear equations and multi-step problems (see description in table 7). The percentage of 8th-graders performing at or above the Advanced international mathematics benchmark was higher than the United States in 11 education systems; was not different in 13 education systems; and was lower than the United States in 31 education systems. Chinese Taipei   (625) (550) (475) (400) Massachusetts-USA 1,4 19 * 57 * 88 * 98 * North Carolina-USA 2,4 14 * * 78 * 95 * Minnesota-USA 4 13 * 49 * 83 * * Connecticut-USA 1,4 10 * 37 69 91 Florida-USA \nStudents have some elementary knowledge of life science and physical science. Students demostrate a knowledge of some simple facts related to human health, ecosystems, and the behavioral and physical characteristics of animals. They also demonstrate some basic knowledge of energy and the physical properties of matter. Students interpret simple diagrams, complete simple tables, and provide short written responses to questions requiring factual information.\nStudents can recognize some basic facts from the life and physical sciences. They have some knowledge of the human body, and demonstrate some familiarity with physical phenomena. Students interpret simple pictorial diagrams, complete simple tables, and apply basic knowledge to practical situations. NOTE: Score cutpoints for the international benchmarks are determined through scale anchoring. Scale anchoring involves selecting benchmarks (scale points) on the achievement scales to be described in terms of student performance, and then identifying items that students scoring at the anchor points can answer correctly. The score cutpoints are set at equal intervals along the achievement scales. The score cutpoints were selected to be as close as possible to the standard percentile cutpoints (i.e., 90th, 75th, 50th, and 25th percentiles). More information on the setting of the score cutpoints can be found in appendix A and in  1 National Defined Population covers 90 to 95 percent of National Target Population (see appendix A). 2 Met guidelines for sample participation rates only after replacement schools were included. 3 National Target Population does not include all of the International Target Population (see appendix A). 4 Exclusion rates for Azerbaijan and Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 5 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. 7 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 25 percent. 8 National Defined Population covers less than 90 percent, but at least 77 percent of National Target Population (see appendix A). NOTE: Education systems are ordered by percentage at Advanced international benchmark. Italics indicate participants identified and counted in this report as an education system and not as a separate country. The TIMSS international median represents all participating TIMSS education systems, including the United States, shown in the main part of the figure; benchmarking education systems are not included in the median. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one education system may be significant while a large difference between the United States and another education system may not be significant. The standard errors of the estimates are shown in table E-28 available at http://nces.   "}, {"section_title": "Grade 8 Advanced (625)", "text": "Students can reason with information, draw conclusions, make generalizations, and solve linear equations. Students can solve a variety of fraction, proportion, and percent problems and justify their conclusions. Students can express generalizations algebraically and model situations. They can solve a variety of problems involving equations, formulas, and functions. Students can reason with geometric figures to solve problems. Students can reason with data from several sources or unfamiliar representations to solve multi-step problems.\nStudents communicate an understanding of complex and abstract concepts in biology, chemistry, physics, and Earth science. Students demonstrate some conceptual knowledge about cells and the characteristics, classification, and life processes of organisms. They communicate an understanding of the complexity of ecosystems and adaptations of organisms, and apply an understanding of life cycles and heredity. Students also communicate an understanding of the structure of matter and physical and chemical properties and changes and apply knowledge of forces, pressure, motion, sound, and light. They reason about electrical circuits and properties of magnets. Students apply knowledge and communicate understanding of the solar system and Earth's processes, structures, and physical features. They understand basic features of scientific investigation. They also combine information from several sources to solve problems and draw conclusions, and they provide written explanations to communicate scientific knowledge."}, {"section_title": "Average scores of male and female students", "text": "In 2011, at grade 4, the U.S. average score in mathematics was 9 score points higher for males than for females (figure 5). Among all 57 education systems that participated in TIMSS at grade 4, there were 30 education systems that showed a significant difference in the average mathematics scores of males and females: in favor of males (including Florida-USA and North Carolina-USA, as well as the nation as a whole) and 5 in favor of females. The difference in average scores between males and females ranged from 35 score points in Kuwait in favor of females to 12 score points in North Carolina-USA in favor of males. In 27 education systems, there was no measurable difference between the average mathematics scores of males and females. At grade 8, there was no statistically significant difference between the average scores of U.S. males and females (figure 6). Among all 56 education systems that participated in TIMSS at grade 8, there were 21 education systems that showed a significant difference in the average mathematics scores of males and females: 8 in favor of males (including Indiana-USA) and 13 in favor of females. The difference in average scores between males and females ranged from 63 score points in Oman in favor of females to 23 score points in Ghana in favor of males. In 35 education systems, there was no statistical difference between the average mathematics scores of males and females (including the U.S. states of Alabama, California, Colorado, Connecticut, Florida, Massachusetts, Minnesota, and North Carolina, as well as the nation as a whole). Figure 5. Difference in average mathematics scores of 4th-grade students, by sex and education system: 2011 2 Met guidelines for sample participation rates only after replacement schools were included. 3 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 4 National Target Population does not include all of the International Target Population (see appendix A). 5 Exclusion rates for Azerbaijan and Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 25 percent. 7 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. 8 National Defined Population covers less than 90 percent, but at least 77 percent of National Target Population (see appendix A). NOTE: Education systems are ordered by male-female difference in average score. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. All differences in average scores reported as statistically significant are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference for one education system may be significant while a larger difference for another education system may not be significant. The standard errors of the estimates are shown in table E-9 available at http://nces.ed.gov/pubsearch/ pubsinfor.asp?pubid=2013009. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 2011. 9 # # # Figure 6. Difference in average mathematics scores of 8th-grade students, by sex and education system: 2011 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 25 percent. 2 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. 3 National Defined Population covers 90 to 95 percent of National Target Population (see appendix A). 4 National Target Population does not include all of the International Target Population (see appendix A). 5 Exclusion rates for Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 6 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 7 National Defined Population covers less than 90 percent, but at least 77 percent of National Target Population (see appendix A). NOTE: Education systems are ordered by male-female difference in average score. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. All differences in average scores reported as statistically significant are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference for one education system may be significant while a larger difference for another education system may not be significant. The standard errors of the estimates are shown in table E-10 available at http://nces.ed.gov/pubsearch/ pubsinfor.asp?pubid=2013009. \nIn 2011, the U.S. average score in science at grade 4 was 10 points higher for males than for females (figure 13). Among all 57 education systems that participated in TIMSS at grade 4, there were 32 education systems that showed a measurable difference in the average science scores of males and females: 20 in favor of males (including both participating U.S. states) and 12 in favor of females. The difference in average scores between males and females ranged from 53 score points in Kuwait (in favor of females) to 15 score points in the Czech Figure 13. Difference in average science scores of 4th-grade students, by sex and education system: 2011 3 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 4 National Target Population does not include all of the International Target Population (see appendix A). 5 Exclusion rates for Azerbaijan and Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 25 percent. 7 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. 8 National Defined Population covers less than 90 percent, but at least 77 percent of National Target Population (see appendix A). NOTE: Education systems are ordered by male-female difference in average score. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. All differences in average scores reported as statistically significant are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference for one education system may be significant while a larger difference for another education system may not be significant. "}, {"section_title": "MATHEMATICS Performance within the United States", "text": "In 2011, TIMSS was administered to enough students and in enough schools in the United States to provide separate average mathematics scores for students by race/ethnicity and schools serving varying percentages of low-income students as measured by the percentage of students eligible for free or reduced-price lunch.In addition, TIMSS was administered to enough students and in enough schools in nine U.S. states to provide each of the states its own separate TIMSS results for public school students at grade 8 and, in two of the states, at grade 4 as well. These state mathematics results are reported at the end of this section. As mentioned in the introduction (and explained in detail in appendix A), separate state public school samples were drawn, at grade 4, for Florida and North Carolina and, at grade 8, for Alabama, California, Colorado, Connecticut, Florida, Indiana, Massachusetts, Minnesota, and North Carolina. Some of these states chose to participate as benchmarking participants in order to compare their performance internationally, and others were invited to participate in TIMSS by the National Assessment of Educational Progress (NAEP), which is conducting a study to link TIMSS and NAEP (as explained in appendix A). The states invited to participate at grade 8 were selected based on state enrollment size and willingness to participate, as well as on their general NAEP performance (above or below the national average on NAEP), their previous experience in benchmarking to TIMSS, and their regional distribution."}, {"section_title": "Average scores of students of different races and ethnicities", "text": "In 2011, the average mathematics scores for U.S. White, Hispanic, Asian, and multiracial 4th-graders were higher than the TIMSS scale average, but for U.S. Black 4th-graders it was lower (figure 7). In comparison with the U.S. national average, U.S. White, Asian, and multiracial 4th-graders scored higher, on average, while U.S. Black and Hispanic 4th-graders scored lower, on average. At grade 8, the average mathematics scores for U.S. White and Asian students were higher than both the TIMSS scale average and the U.S. national average. However, U.S. Black and Hispanic 8th-graders scored lower, on average, than the TIMSS scale average and the U.S. national average. U.S. multiracial 8th-graders' mathematics score was higher, on average, than the TIMSS scale average but not measurably different from the U.S. national average. 4th-and 8th-grade students, by race/ ethnicity: 2011 Percentage of public school students eligible for free or reduced-price lunch \nIn 2011, the average science scores for U.S. White, Asian, Hispanic, and multiracial 4th-graders were higher than the TIMSS scale average, but for U.S. Black 4th-graders the average was lower (figure 15). In comparison with the U.S. national average, U.S. White, Asian, and multiracial 4thgraders scored higher, on average, while U.S. Black and Hispanic 4th-graders scored lower, on average. At grade 8, the average science scores for U.S. White, Asian, and multiracial students were higher than both the TIMSS scale average and the U.S. national average. However, U.S. Black and Hispanic 8th-graders scored lower, on average, than the TIMSS scale average and U.S. national average.   "}, {"section_title": "MATHEMATICS TIMSS 2011 results for Alabama", "text": "Mathematics -Grade 8 \u2022 Public school students' average score was 466 at grade 8. \u2022 The percentages of Alabama 8th-graders reaching each of the four TIMSS international benchmarks were not measurably different than the international medians (figure 4). \u2022 Both male and female students in Alabama scored lower, on average, in mathematics than the TIMSS scale average (table 9). \u2022 White, Asian, and multiracial students' average scores were not measurably different from the TIMSS scale average. However, Black and Hispanic students scored lower, on average, than the TIMSS scale average. \u2022 Students in public schools with 25 percent or more of students eligible for free or reduced-price lunch scored lower, on average, than the TIMSS scale average. Public school students' average score was 493 at grade 8. Higher percentages of California 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 5 percent of 8th-graders in California performed at or above the Advanced benchmark (625) compared to the international median of 3 percent at grade 8 (figure 4). White, Asian, and multiracial students' average scores were higher than the TIMSS scale average while Black and Hispanic students scored lower, on average, than the TIMSS scale average. Students in public schools with at least 10 percent but less than 50 percent of students eligible for free or reduced-price lunch scored higher, on average, than the TIMSS scale average, while students in public schools with 75 percent or more students eligible for free or reduced-price lunch scored lower, on average, than the TIMSS scale average.  \u2022 Public school students' average score was 518 at grade 8. \u2022 Higher percentages of Colorado 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 8 percent of 8th-graders in Colorado performed at or above the Advanced benchmark (625) compared to the international median of percent at grade 8 (figure 4). \u2022 Male and female students in Colorado scored higher, on average, in mathematics than the TIMSS scale average (table 13). \u2022 White and Asian students' average scores were higher than the TIMSS scale average, while Hispanic students scored lower, on average, than the TIMSS scale average. \u2022 Students in public schools with at least 10 percent but less than 50 percent of students eligible for free or reduced-price lunch scored higher, on average, than the TIMSS scale average, while students in schools with 75 percent or more students eligible for free or reduced-price lunch scored lower, on average, than the TIMSS scale average. \u2022 Public school students' average score was 518 at grade 8. \u2022 Higher percentages of Connecticut 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 10 percent of 8th-graders in Connecticut performed at or above the Advanced benchmark (625) compared to the international median of 3 percent at grade 8 (figure 4). \u2022 Male and female students in Connecticut scored higher, on average, in mathematics than the TIMSS scale average (table 15). \u2022 White and Asian students' average scores were higher than the TIMSS scale average, while Black and Hispanic students scored lower, on average, than the TIMSS scale average. \u2022 Students in public schools with less than 25 percent of students eligible for free or reduced-price lunch scored higher, on average, than the TIMSS scale average, while students in schools with 50 percent or more scored lower, on average, than the TIMSS scale average. "}, {"section_title": "MATHEMATICS TIMSS 2011 results for Florida", "text": "Mathematics -Grades 4 and 8 \u2022 Public school students' average score was 545 at grade 4 and 513 at grade 8. \u2022 Higher percentages of Florida 4th-and 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 14 percent of 4th-graders and 8 percent of 8th-graders in Florida performed at or above the Advanced benchmark (625) compared to the international median of 4 percent at grade 4 and 3 percent at grade 8 (figures 3 and 4). \u2022 Male and female students in Florida scored higher, on average, than the TIMSS scale average in mathematics at grade 4, and males scored higher, on average, at grade (table 17). Continued on next page \u2022 At grade 4, White, Hispanic, Asian, and multiracial students' average scores were higher than the TIMSS scale average. \u2022 At grade 8, White and Asian students' average scores were higher than the TIMSS scale average, while Black students' average scores were lower. \u2022 Students at grade 4 scored higher, on average, than the TIMSS scale average regardless of the level of poverty within public schools. At grade 8 students in public schools with at least 10 percent but less than 50 percent of students eligible for free or reduced-price lunch scored higher, on average, than the TIMSS scales average. "}, {"section_title": "MATHEMATICS TIMSS 2011 results for Indiana", "text": "Mathematics -Grade 8 \u2022 Public school students' average score was 522 at grade 8. \u2022 Higher percentages of Indiana 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 7 percent of 8thgraders in Indiana performed at or above the Advanced benchmark (625) compared to the international median of 3 percent at grade 8 (figure 4). \u2022 Male and female students in Indiana scored higher in mathematics, on average, than the TIMSS scale average (table 19). \u2022 White and multiracial students' average scores were higher than the TIMSS scale average, while Black students scored lower, on average, than the TIMSS scale average. Hispanic and Asian students' average scores were not measurably different from the TIMSS scale average. \u2022 Students in schools with at least 10 percent but less than 50 percent of students eligible for free or reduced-price lunch scored higher, on average, than the TIMSS scale average, while students in schools with 75 percent or more students eligible for free or reduced-price lunch scored lower, on average, than the TIMSS scale average. HIGHLIGHTS FROM TIMSS 2011"}, {"section_title": "TIMSS 2011 results for Massachusetts", "text": "Mathematics -Grade 8 \u2022 Public school students' average score was 561 at grade 8. \u2022 Higher percentages of Massachusetts 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 19 percent of 8th-graders in Massachusetts performed at or above the Advanced benchmark (625) compared to the international median of 3 percent at grade 8 (figure 4). \u2022 Male and female students scored higher in mathematics, on average, than the TIMSS scale average (table 21). \u2022 White, Asian, and multiracial students' average scores were higher than the TIMSS scale average, while Black and Hispanic students' average scores were not measurably different from the TIMSS scale average. \u2022 Students in public schools with 75 percent or more of students eligible for free or reduced-price lunch did not score measurably different, on average, from the TIMSS scale average. All other groups scored, on average, above the TIMSS scale average. "}, {"section_title": "MATHEMATICS TIMSS 2011 results for Minnesota", "text": "Mathematics -Grade 8 \u2022 Public school students' average score was 545 at grade 8. \u2022 Higher percentages of Minnesota 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 13 percent of 8thgraders in Minnesota performed at or above the Advanced benchmark (625) compared to the international median of 3 percent at grade 8 (figure 4). \u2022 Male and female students scored higher in mathematics, on average, than the TIMSS scale average (table 23). \u2022 White, Asian, and multiracial students' average scores were higher than the TIMSS scale average, while Black and Hispanic students' average scores were not measurably different from the TIMSS scale average. \u2022 Students in public schools with 75 percent or more of students eligible for free or reduced-price lunch did not score measurably different, on average, from the TIMSS scale average. All other groups scored, on average, above the TIMSS scale average. "}, {"section_title": "TIMSS 2011 results for North Carolina", "text": "Mathematics -Grades 4 and \u2022 Public school students' average score was 554 at grade 4 and 537 at grade 8. \u2022 Higher percentages of North Carolina 4th-and 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 16 percent of 4th-graders and 14 percent of 8th-graders in North Carolina performed at or above the Advanced benchmark (625) compared to the international median of 4 percent at grade 4 and 3 percent at grade 8 (figures 3 and 4). \u2022 Males outperformed females by 12 score points, on average, at grade 4. At both grade 4 and 8, males and females scored higher in mathematics, on average, than the TIMSS scale average (table 25). "}, {"section_title": "SCIENCE Science Performance in the United States and Internationally", "text": "Average scores in 2011 In science, the U.S. national average score was 544 at grade 4 and 525 at grade 8 (tables 26 and 27). Both scores were higher than the TIMSS scale average of 500 at both grades. Among the 45 countries that participated at grade 4, the U.S. average science score was among the top 6 (5 countries had higher average scores than the United States). Thirty-nine countries had lower average scores than the United States. Among all 57 education systems that participated at grade 4 (i.e., both countries and other education systems, including U.S. states that participated in TIMSS with individual state samples), the United States was among the top 10 in average science scores (6 education systems had higher averages and 3 were not measurably different). Korea, Singapore, Finland, Japan, the Russian Federation, and Chinese Taipei-CHN had higher average scores than the United States; and Florida-USA, Alberta-CAN, and North Carolina-USA had average scores not measurably different from the U.S. average at grade 4. The United States outperformed 47 education systems. At grade 8, among the 38 countries that participated in TIMSS, the U.S. average science score was among the top 10 (6 countries had higher averages and 3 had averages not measurably different from the United States). Twenty-eight countries had lower average scores than the United States. Among all 56 education systems that participated at grade 8, the United States was among the top 23 education systems in average science scores (12 education systems had higher averages and 10 were not measurably different). Singapore, Massachusetts-USA, Chinese Taipei-CHN, Korea, Japan, Minnesota-USA, Finland, Alberta-CAN, Slovenia, the Russian Federation, Colorado-USA, and Hong Kong-CHN had higher average scores than the United States; and England-GBR, Indiana-USA, Connecticut-USA, North Carolina-USA, Florida-USA, Hungary, Ontario-CAN, Quebec-CAN, Australia, and Israel had average scores not measurably different from the U.S. average at grade 8. The United States had higher average science scores than 33 education systems."}, {"section_title": "SCIENCE HIGHLIGHTS FROM TIMSS 2011", "text": "\n\nand females) and 16 in favor of females. The difference in average scores between males and females ranged from 78 score points in Oman (in favor of females) to 30 score points in Ghana (in favor of males). In 23 education systems, there was no measurable difference between the average science scores of males and females."}, {"section_title": "Change at grade 8 between 1995 and 2011", "text": "At grade 8, among the 20 education systems that participated in both the and 2011 TIMSS science assessments, the average science score increased in 8 education systems, including the United States, and decreased in 3 education systems (figure 10). In the rest, there was no measurable change in the average grade 8 science scores between 1995 and 2011. The U.S. increase in average science score at grade 8 between 1995 and 2011 was 12 score points (from 513 to 525). Two education systems had larger increases than See notes on next page. "}, {"section_title": "SCIENCE", "text": "the United States during this time: Lithuania (50 points) and Slovenia (29 points). However, U.S. average performance at grade 8 went from being not measurably different than Slovenia, Hong Kong-CHN, and the Russian Federation in 1995 to having a lower average score in 2011; and from being above that of Ontario-CAN in 1995 to being not measurably different in 2011. The increases in the other education systems did not change their standing relative to the United States. Scores decreased during this time at grade 8 in Sweden (43 points), Norway (20 points), and Hungary (14 points). As a result, Sweden moved from having a higher average score than the United States at grade 8 in 1995 to having a lower average score in 2011, Hungary moved from having a higher average score than the United States in 1995 to being not measurably different in 2011, and Norway moved from being not measurably different from the United States in 1995 to having a lower average score in 2011. 24 Although the average score of England-GBR and New Zealand did not decrease measurably, England-GBR's standing relative to the United States moved from above the United States in 1995 to being not measurably different in 2011, and New Zealand's standing relative to the United States moved from being not measurably different in 1995 to scoring below the United States in 2011. *p<.05. Change in average scores is significant. 1 The change in average score is calculated by subtracting the 2007 or 1995 estimate, respectively, from the 2011 estimate using unrounded numbers. 2 National Defined Population covers 90 to 95 percent of National Target Population for 2011 (see appendix A). 3 Nearly satisfied guidelines for sample participation rates after replacement schools were included for 2011. 4 National Target Population does not include all of the International Target Population for 2011 (see appendix A). 5 Exclusion rates for Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available for 2011. 6 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation in 2011 exceeds 15 percent, though it is less than 25 percent. NOTE: Education systems are ordered by 2011 average scores. Italics indicate participants identified and counted in this report as an education system and not as a separate country. All education systems met international sampling and other guidelines in 2011, except as noted. Data are not shown for some education systems because comparable data from previous cycles are not available. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. For 1995, Lithuania's National Target Population did not include all of the International Target Population; the Russian Federation and Lithuania had a National Defined Population that covered 90 to 95 percent of National Target Population; England-GBR, and Ontario-CAN had a National Defined Population that covered less than 90 percent of National Target Population (but at least 77 percent); the United States, England-GBR, and Minnesota-USA met guidelines for sample participation rates only after replacement schools were included. For 2007, Lithuania, Georgia, and Indonesia had National Target Populations that did not include all of the International Target Population; the United States, Massachusetts-USA, Minnesota-USA, and Ontario-CAN had National Defined Population that covered 90 to 95 percent of National Target Population; Hong Kong-CHN, England-GBR, and Minnesota-USA met guidelines for sample participation rates only after replacement schools were included; Dubai-UAE nearly satisfied guidelines for sample participation rates after replacement schools were included. All average scores reported as higher or lower than the U.S. average score are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between averages for one education system may be significant, while a large difference for another education system may not be significant. Detail may not sum to totals because of rounding. The standard errors of the estimates are shown in table E-25 available at http://nces.ed.gov/pubsearch/pubsinfo.asp?pubid=2013009. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 1995, 2007, and 2011. \nRepublic (in favor of males). In 25 education systems, there was no measurable difference between the average science scores of males and females. At grade 8, the U.S. average score in science was 11 points higher for males than for females ( figure 14). Among all 56 education systems that participated in TIMSS at grade 8, there were 33 education systems that showed a significant difference in the average science scores of males and females: 17 in favor of males (including all the participating U.S. states except Alabama, Connecticut, and Massachusetts, which had no measurable difference in average scores between males Figure 14. Difference in average science scores of 8th-grade students, by sex and education system: 2011 Difference in average science scores Male-female difference in average science scores is statistically significant. Male-female difference in average science scores is not measurably different. # Rounds to zero. 1 The TIMSS International Study Center has reservations about the reliability of the average achievement score because the percentage of students with achievement too low for estimation exceeds 15 percent, though it is less than 25 percent. 2 National Defined Population covers 90 to 95 percent of National Target Population (see appendix A). 3 Nearly satisfied guidelines for sample participation rates after replacement schools were included. 4 National Defined Population covers less than 90 percent, but at least 77 percent of National Target Population (see appendix A). 5 National Target Population does not include all of the International Target Population (see appendix A). 6 Exclusion rates for Georgia are slightly underestimated as some conflict zones were not covered and no official statistics were available. NOTE: Education systems are ordered by male-female difference in average score. Italics indicate participants identified and counted in this report as an education system and not as a separate country. Participants that did not administer TIMSS at the target grade are not shown; see the international report for their results. All U.S. state data are based on public school students only. All differences in average scores reported as statistically significant are different at the .05 level of statistical significance. The tests for significance take into account the standard error for the reported difference. Thus, a small difference for one education system may be significant while a larger difference for another education system may not be significant. \n\u2022 At grade 4 and grade 8, White, Hispanic, Asian, and multiracial students' average scores were higher than the TIMSS scale average, while Black students' average scores were not measurably different from the TIMSS scale average (table 40). \u2022 Students at grade 4 scored higher, on average, than the TIMSS scale average regardless of the level of poverty within public schools. At grade 8, students in public schools with at least 10 percent but less than 75 percent of students eligible for free or reduced-price lunch scored, on average, higher than the TIMSS scale average, while the average score for students in public schools with 75 percent or more of students eligible for free or reduced-price lunch was not measurably different from the TIMSS scale average. \u2022 Public school students' average score was 533 at grade 8. \u2022 Higher percentages of Indiana 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 10 percent of 8th-graders in Indiana performed at or above the Advanced benchmark (625) compared to the international median of 4 percent at grade 8 (figure 12). \u2022 In Indiana, males outperformed females by 15 score points on average in science at grade 8 (figure 14). Male and female students in Indiana scored higher, on average, in science than the TIMSS scale average (table 42). \u2022 White students' average scores were higher than the TIMSS scale average, while Black and Asian students scored lower, on average, than the TIMSS scale average. Hispanic and multiracial students' average scores were not measurably different from the TIMSS scale average. \u2022 Students in public schools with at least 10 percent but less than 75 percent of students eligible for free or reduced-price lunch scored higher, on average, than the TIMSS scale average, while the average score of students in schools with 75 percent or more of students eligible for free or reducedprice lunch was not measurably different from the TIMSS scale average.  \nHIGHLIGHTS FROM TIMSS 2011 \u2022 At grade 8, White and Asian students scored, on average, above the TIMSS scale average while Black students scored lower, on average. Hispanic and multiracial students' average scores were not measurably different from the TIMSS scale average. \u2022 In general, at grade 4 students in public schools with less than 75 percent of students eligible for free or reduced-price lunch scored higher, on average, than the TIMSS scale average. Average scores among students in public schools with 75 percent or more of students eligible for free or reduced-price lunch were not measurably different from the TIMSS scale average. At grade 8, students in public schools with less than 50 percent of students eligible for free or reduced-price lunch scored higher, on average, than the TIMSS scale average, while average scores for students in schools with 50 percent or more students eligible for free or reduced-price lunch were not measurably different from the TIMSS scale average. "}, {"section_title": "Performance on the TIMSS international benchmarks", "text": "The TIMSS international benchmarks provide a way to understand how students' proficiency in science varies along the TIMSS scale (table 30). TIMSS defines four levels of student achievement: Advanced, High, Intermediate, and Low. The benchmarks can then be used to describe the kinds of skills and knowledge students at each score cutpoint would need to successfully answer the science items included in the assessment. The descriptions of the benchmarks differ between the two grade levels, as the scientific skills and knowledge needed to respond to the assessment items reflect the nature, difficulty, and emphasis at each grade. In 2011, higher percentages of U.S. 4th-graders performed at or above each of the four TIMSS international benchmarks than the international medians (figure 11). For example, 15 percent of U.S. 4th-graders performed at or above the Advanced benchmark (625) compared to the international median of 5 percent. Students at the Advanced benchmark demonstrated an ability to apply their knowledge and understanding of scientific processes and relationships and show some knowledge of the process of scientific inquiry (see description in table 30). The percentage of students performing at or above the Advanced international science benchmark was higher than in the United States in 3 education systems; was not different in 6 education systems; and was lower than the United States in 47 education systems. Singapore, Korea, and Finland had a higher percentage of students performing at or above the Advanced international science benchmark than the United States at grade 4; and the Russian Federation, Chinese Taipei-CHN, Japan, Florida-USA, Hungary, and North Carolina-USA had a percentage that was not measurably different from the U.S. percentage. Similar to their 4th-grade counterparts, higher percentages of U.S. 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians (figure 12). For example, 10 percent of U.S. 8thgraders performed at or above the Advanced benchmark (625) compared to the international median of 4 percent. Students at the Advanced benchmark demonstrated an ability to communicate an understanding of complex and abstract concepts in biology, chemistry, physics, and Earth science (see description in table 30). The percentage of 8th-grade students performing at or above the Advanced international science benchmark was higher than in the United States in 12 education systems; was not different in 10 education systems; and was lower than in the United States in 33 education systems. Singapore, Massachusetts-USA, Chinese Taipei-CHN, Korea, Japan, Minnesota-USA, Colorado-USA, Connecticut-USA, the Russian Federation, England-GBR, Slovenia, and Finland had a higher percentage of students performing at or above the Advanced international science benchmark than the United States at grade 8; and Florida-USA, North Carolina-USA, Alberta-CAN, Israel, Australia, Indiana-USA, Hong Kong-CHN, New Zealand, Hungary, and Turkey had a percentage that was not measurably different from the U.S. percentage."}, {"section_title": "SCIENCE Table 30. Description of TIMSS international science benchmarks, by grade: 2011", "text": ""}, {"section_title": "Performance within the United States", "text": "In 2011, TIMSS was administered to enough students and in enough schools in the United States to provide separate average science scores for students by race/ethnicity and schools serving varying percentages of low-income students as measured by the percentage of students eligible for free or reduced-price lunch. In addition, TIMSS was administered to enough students and in enough schools in nine U.S. states to provide each of the states its own separate TIMSS results for public school students at grade 8 and, in two of the states, at grade 4 as well. These state science results are reported at the end of this section. As mentioned in the introduction (and explained in detail in appendix A), separate state public school samples were drawn, at grade 4, for Florida and North Carolina and, at grade 8, for Alabama, California, Colorado, Connecticut, Florida, Indiana, Massachusetts, Minnesota, and North Carolina. Some of these states chose to participate as benchmarking participants in order to compare their performance internationally, and others were invited to participate in TIMSS by the National Assessment of Educational Progress (NAEP), which is conducting a study to link TIMSS and NAEP (as explained in appendix A). The states invited to participate at grade 8 were selected based on state enrollment size and willingness to participate, as well as on their general NAEP performance (above or below the national average on NAEP), their previous experience in benchmarking to TIMSS, and their regional distribution."}, {"section_title": "SCIENCE Average scores of students attending public schools of various poverty levels", "text": "In 2011, the average science score of U.S. 4th-graders in the highest poverty public schools (at least 75 percent of students eligible for free or reduced-price lunch) was not measurably different from the TIMSS scale average. The average scores of 4th-graders in each of the other categories of school poverty were higher than the TIMSS scale average ( figure  16). Fourth-graders in the highest poverty public schools, as well as those in public schools with at least 50 percent but less than 75 percent of students eligible for free or reducedprice lunch, had average scores below the U.S. national average, while those in public schools with lower proportions of low-income students scored higher, on average, than the U.S. national average. At grade 8, students in the highest poverty public schools had a lower average score than the TIMSS scale average (476 vs. 500), while students in public schools with less than 75 percent of students eligible for free or reduced-price lunch had a higher score, on average, than the TIMSS scale average. Eighth-graders in public schools with less than 50 percent of students eligible for free or reduced-price lunch scored, on average, above the U.S. national average, while students in public schools with 50 percent or more of students eligible scored, on average, below. Percentage of public school students eligible for free or reduced-price lunch   \u2022 Public school students' average score was 542 at grade 8. \u2022 Higher percentages of Colorado 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 14 percent of 8thgraders in Colorado performed at or above the Advanced benchmark (625) compared to the international median of 4 percent at grade 8 (figure 12). \u2022 In Colorado males outperformed females by 11 score points, on average, in science (figure 14). Male and female students in Colorado scored higher, on average, in science than the TIMSS scale average (table 36). \u2022 White, Asian, and multiracial students' average scores were higher than the TIMSS scale average. Black and Hispanic students' average scores were not measurably different (table 36). \u2022 Students in public schools with at least 10 percent but less than 50 percent of students eligible for free or reduced-price lunch scored higher, on average, than the TIMSS scale average. Average scores of students in public schools in other categories of eligibility for free or reduced-price lunch were not measurably different from the TIMSS scale average, including those in schools where less than 10 percent were eligible.  "}, {"section_title": "SCIENCE TIMSS 2011 results for Connecticut", "text": "Science -Grade 8 \u2022 Public school students' average score was 532 at grade 8. \u2022 Higher percentages of Connecticut 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 14 percent of 8th-graders in Connecticut performed at or above the Advanced benchmark (625) compared to the international median of 4 percent at grade 8 (figure 12). \u2022 Male and female students in Connecticut scored higher, on average, in science than the TIMSS scale average. \u2022 White, Asian, and multiracial students' average scores were higher than the TIMSS scale average, while Black and Hispanic students scored lower, on average, than the TIMSS scale average (table 38). \u2022 Students in public schools with less than 25 percent of students eligible for free or reduced-price lunch scored higher, on average, than the TIMSS scale average, while students in schools with 50 percent or more of students eligible for free or reduced-price lunch scored lower, on average, than the TIMSS scale average. "}, {"section_title": "TIMSS 2011 results for Florida", "text": "Science -Grades 4 and 8 \u2022 Public school students' average science score was 545 at grade 4 and 530 at grade 8. \u2022 Higher percentages of Florida 4th-and 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 14 percent of 4th-graders and 13 percent of 8th-graders in Florida performed at or above the Advanced benchmark (625) compared to the international median of 5 percent at grade 4 and 4 percent at grade 8 (figures 11 and 12). \u2022 Males outperformed females by 9 score points on average in science at grade 4 (figure 13) and by 15 score points at grade 8 ( figure 14). In both grade 4 and grade 8, male and female students in Florida scored higher, on average, in science than the TIMSS scale average (table 40). Table 39. Average science scores of 4th-and 8th-grade students in Florida public schools compared with other participating education systems: 2011"}, {"section_title": "SCIENCE TIMSS 2011 results for Massachusetts", "text": "Science -Grade 8 \u2022 Public school students' average score was 567 at grade 8. \u2022 Higher percentages of Massachusetts 8th-graders performed at or above each of the four TIMSS international benchmarks than the international medians. For example, 24 percent of 8th-graders in Massachusetts performed at or above the Advanced benchmark (625) compared to the international median of 4 percent at grade 8 (figure 12). \u2022 Male and female students in Massachusetts scored higher, on average, in science than the TIMSS scale average (table 44). \u2022 White, Asian, and multiracial students' average scores were higher than the TIMSS scale average, while Black and Hispanic students' average scores were not measurably different from the TIMSS scale average (table 44). \u2022 Students in public schools with 75 percent or more of students eligible for free or reduced-price lunch had average scores not measurably different from the TIMSS scale average. All other groups scored, on average, above the TIMSS scale average.   "}, {"section_title": "Introduction", "text": "The Trends in International Mathematics and Science Study (TIMSS) is a cross-national comparative study of the performance and schooling contexts of 4th-and 8thgrade students in mathematics and science. In this fifth cycle of TIMSS, mathematics and science assessments and associated questionnaires were administered in 57 education systems (45 of which were countries) at the 4th-grade level and 56 education systems (38 of which were countries) at the 8th-grade level during fall 2010 (in the Southern hemisphere) and during spring 2011 (in the Northern hemisphere). TIMSS is coordinated by the International Association for the Evaluation of Educational Achievement (IEA), with governmental sponsors in each participating country or education system. In the United States, TIMSS is sponsored by the National Center for Education Statistics (NCES), in the Institute of Education Sciences of the U.S. Department of Education. As part of the 2011 administration of TIMSS in the United States, NCES conducted a study to link the National Assessment of Educational Progress (NAEP), a national U.S. student assessment in mathematics and science, with TIMSS so that states can measure their performance against international benchmarks. This NAEP-TIMSS Linking Study uses 8th-grade mathematics and science data from NAEP to project state-level scores onto the TIMSS scale. The goal of the study is to predict 2011 TIMSS mathematics and science scores at 8th-grade based on state NAEP performance without incurring the costs associated with every state participating separately in TIMSS. Results of the study are forthcoming as part of a separate report and technical notes are included here only as the linking study related to technical aspects of the main study, the focus of this report. This appendix provides an overview of technical aspects of TIMSS 2011, including \u2022 International requirements for sampling design, data collection, and response rates; \u2022 Sampling, data collection, and response rates in the United States and for all participants; \u2022 Test development; \u2022 Recruitment, test administration, and quality assurance; \u2022 Scoring and scoring reliability; \u2022 Weighting, scaling, and plausible values; \u2022 International benchmarks; \u2022 Data limitations; \u2022 Description of background variables; \u2022 Confidentiality and disclosure limitations; and \u2022 Statistical procedures. More detailed information can be found in the TIMSS and PIRLS Methods and Procedures ."}, {"section_title": "International requirements for sampling, data collection, and response rates", "text": "In order to ensure comparability of the data across countries, the IEA provided detailed international requirements on the various aspects of data collection described here and implemented quality control procedures. Participating countries were obliged to follow these requirements. These requirements regarding the target populations, sampling design, sample size, exclusions, and defining participation rates are described below."}, {"section_title": "Target populations", "text": "In order to identify comparable populations of students to be sampled, the IEA defined the target populations as follows (Martin and Mullis 2011): Fourth-grade student population. The international desired target population (also referred to as the International Target Population) is all students enrolled in the grade that represents 4 years of schooling, counting from the first year of the International Standard Classification of Education (ISCED) Level 1, 1 providing that the mean age at the time of testing is at least 9.5 years. Eighth-grade student population. The international desired target population is all students enrolled in the grade that represents 8 years of schooling, counting from the first year of ISCED Level 1, providing that the mean age at the time of testing is at least 13.5 years. Teacher population. The target population is all mathematics and science teachers linked to the selected students. Note that these teachers are not a representative sample of teachers within the country. Rather, they are the mathematics and science teachers who teach a representative sample of students in two grades within the country (grades 4 and 8 in the United States)."}, {"section_title": "Sampling design", "text": "It is not feasible to assess every 4th-and 8th-grade student in the United States. Thus, as is done in all participating countries and education systems, a representative sample of 4th-and 8th-grade students was selected. The sample design employed by the TIMSS 2011 assessment is generally referred to as a two-stage stratified cluster sample. The sampling units at each stage were defined as follows. First-stage sampling units. In the first stage of sampling, sampling statisticians selected individual schools with a probability-proportionate-to-size (PPS) approach, which means that each school's probability of selection is proportional to the estimated number of students enrolled in the target grade. Prior to sampling, statisticians assigned schools in the sampling frame to a predetermined number of explicit or implicit strata. Then, sampling staff identified sample schools using a PPS systematic sampling method. Statisticians also identified substitute schools (schools to replace original sampled school that refused to participate). The original and substitute schools were identified simultaneously. Second-stage sampling units. In the second stage of sampling, statisticians selected classrooms within sampled schools using sampling software provided by the International Study Center at Boston College. The software uses a sampling algorithm for selecting classes that standardized the class sampling across schools and assures uniformity in the class selection procedures across participants. The software identified at least one classroom from a list of eligible classrooms that sampling staff prepared for each target grade. In various countries and education systems, including the United States, more than one eligible classroom per target grade per school was selected when possible. All students in sampled classrooms were selected for assessment."}, {"section_title": "Sample size for the main survey", "text": "TIMSS guidelines call for a minimum of 150 schools to be sampled per grade, with a minimum of 4,000 students assessed per grade. The basic sample design of one classroom per target grade per school was designed to yield a total sample of approximately 4,500 students per population. Countries with small class sizes or less than 30 students per school were directed to consider sampling more schools, more classrooms per school, or both, to meet the minimum target of 4,000 tested students. In the United States, a sample of 450 schools was drawn at 4th-grade and 600 schools at 8th-grade. These were larger sample sizes than used in previous administrations of TIMSS. The reason for a larger sample than in the past at 4th grade was that in 2011 both TIMSS (administered every 4 years) and another study sponsored by the IEA, the Progress in International Reading Literacy Study (PIRLS) (administered every 5 years at 4th grade), happened to coincide. Because the United States was participating in both studies, and because both studies required a 4th-grade sample of schools and students, the decision was made to draw a larger sample of schools and to request that both studies be administered in the same schools (where feasible), albeit to separate classroom samples of students. 3 Thus, TIMSS (4th grade) and PIRLS in the United States were administered in the same schools but to separately sampled classrooms of students. The reason for a larger sample than in the past at 8th grade was that the NAEP-TIMSS Linking Study (described above) required that NAEP be administered in the same schools in which TIMSS was administered, with one intact mathematics classroom randomly assigned to TIMSS and another to the linking study. Given that in previous administrations of TIMSS, two 8th-grade classrooms were sampled per school to take TIMSS, this requirement led to a doubling of the previous TIMSS sample size (from 300 to 600 schools) so that the total number of sampled TIMSS classrooms remained the same. In addition to the national school samples at 4th and 8th grade, state samples were also drawn for two states at 4th grade and nine states at 8th grade. These additional school samples are described below."}, {"section_title": "Exclusions", "text": "The following discussion draws on the TIMSS 2011 School Sampling Manual (Foy, Joncas, and Zuhlke 2009). All schools and students excluded from the national defined target population are referred to as the excluded population. Exclusions could occur at the school level, with entire schools being excluded, or within schools, with specific students or entire classrooms excluded. TIMSS 2011 did not provide accommodations for students with disabilities or students who were unable to read or speak the language of the test. The IEA requirement with regard to exclusions is that they should not exceed more than 5 percent of the national desired target population (Foy, Joncas, and Zuhlke 2009). The specifications for school and student exclusions were applied equally to the U.S. national and state samples."}, {"section_title": "School exclusions. Education systems could exclude schools that", "text": "\u2022 are geographically inaccessible; \u2022 are of extremely small size; \u2022 offer a curriculum, or school structure, radically different from the mainstream educational system; or \u2022 provide instruction only to students in the excluded categories defined under \"within-school exclusions,\" such as schools for the blind. Within-school exclusions. Education systems were asked to adopt the following international within-school exclusion rules to define excluded students: \u2022 Students with intellectual disabilities-Students who, in the professional opinion of the school principal or other qualified staff members, are considered to have intellectual disabilities or who have been tested psychologically as such. This includes students who are emotionally or mentally unable to follow even the general instructions of the test. Students were not to be excluded solely because of poor academic performance or normal disciplinary problems. \u2022 Students with functional disabilities-Students who are permanently physically disabled in such a way that they cannot perform in the TIMSS testing situation. Students with functional disabilities who are able to respond were to be included in the testing. \u2022 Non-English-language speakers-Students who are unable to read or speak the language(s) of the test and would be unable to overcome the language barrier of the test. Typically, a student who had received less than one year of instruction in the language(s) of the test was to be excluded."}, {"section_title": "Defined participation rates", "text": "In order to minimize the potential for response biases, the IEA developed participation or response rate standards that apply to all participating education systems and govern both whether or not a participating education system's data are included in the TIMSS 2011 international dataset as well as the way in which national statistics are presented in the international reports. These standards were set using composites of response rates at the school, classroom, and student and teacher levels; moreover, response rates were calculated with and without the inclusion of substitute schools (selected to replace original sample schools refusing to participate). The response rate standards determine how a participant's data will be reported in the international reports. These standards take the following two forms, distinguished primarily by whether or not meeting the school response rate of 85 percent requires the counting of substitute schools. Category 1: Met requirements. Participants that meet all of the following conditions are considered to have fulfilled the IEA requirements: (a) a minimum school participation rate of 85 percent, based on original (sampled) schools only; (b) a minimum classroom participation rate of 95 percent, from both original and substitute schools; and (c) a minimum student participation rate of 85 percent, from both original and substitute schools. Category 2: Met requirements after substitutes. In the case of participants not meeting the category 1 requirements, provided that at least 50 percent of schools in the original sample participate, a participating education system's data are considered acceptable if the following requirements are met: a minimum combined school, classroom, and student participation rate of 75 percent, based on the product of the participation rates described above. That is, the product of (a), (b), and (c), as defined in the category 1 standard, must be greater than or equal to 75 percent. Participants satisfying the category 1 standard are included in the international tabular presentations without annotation. Those able to satisfy only the category 2 standard are included as well but are annotated to indicate their response rate status. The data from participants failing to meet either standard are presented separately in the international tabular presentations."}, {"section_title": "Sampling, data collection, and response rates in the United States and for all participants", "text": "The U.S. TIMSS national sample design In the United States and most other participating countries and education systems, the target populations of students corresponded to the 4th and 8th grades. In sampling these populations, TIMSS used a two-stage stratified cluster sampling design (as explained above under \"Sampling design\"). 4 The U.S. sampling frame was explicitly stratified by three categorical stratification variables: percentage of students eligible for free or reduced-price lunch, school control (public or private), and region of the country (Northeast, Central, West, Southeast). 5 The U.S. sampling frame was implicitly stratified (that is, sorted for sampling) by two categorical stratification variables: community type (four categories) 6 and minority status (i.e., above or below 15 percent of the student population). The first stage made use of a systematic PPS technique to select schools for the original sample from a sampling frame based on the 2010 National Assessment of Educational Progress (NAEP) school sampling frame. 7 Data for public schools were taken from the Common Core of Data (CCD), and data for private schools were taken from the Private School Universe Survey (PSS). In addition, for each original school selected, the two neighboring schools in the sampling frame were designated as substitute schools. The first school following the original sample school was the first substitute and the first school preceding it was the second substitute. If an original school refused to participate, the first substitute was contacted. If that school also refused to participate, the second substitute was contacted. There were several from each stratum. In the case of implicit stratification, the units of interest are simply sorted with respect to one or more variables known to have a high correlation with the variable of interest. In this way, implicit stratification guarantees that the sample of units selected will be spread across the categories of the stratification variables. 5 The Northeast region consists of Connecticut, Delaware, the District of Columbia, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. The Central region consists of Illinois,Indiana,Iowa,Kansas,Michigan,Minnesota,Missouri,Nebraska,North Dakota,Ohio,Wisconsin,and South Dakota. The West region consists of Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oklahoma,Oregon,Texas,Utah,Washington,and Wyoming. The Southeast region consists of Alabama,Arkansas,Florida,Georgia,Kentucky,Louisiana,Mississippi,North Carolina,South Carolina,Tennessee,Virginia,and West Virginia. 6 Four community types are distinguished: city of 250,000 or larger; suburb less than 250,000; town of 25,000 or more; rural metropolitan statistical area (MSA). 7 In order to maximize response rates from both districts and schools it was necessary to begin the recruitment of both prior to the end of the 2009-10 school year. Since the 2011 NAEP sampling frame was not available until March 2010, it was necessary to base the TIMSS samples on the 2010 NAEP sampling frame. constraints on the assignment of substitutes. One sampled school was not allowed to substitute for another, and a given school could not be assigned to substitute for more than one sampled school. Furthermore, substitutes were required to be in the same implicit stratum as the sampled school. The second stage consisted of selecting intact mathematics classes within each participating school. Schools provided lists of 4th-or 8th-grade classrooms. Within schools, classrooms with fewer than 15 students were collapsed into pseudo-classrooms, so that each classroom in the school's classroom sampling frame had at least 20 students. 8 An equal probability sample of two classrooms 9 was identified from the classroom frame for the school. In schools where there was only one classroom, this classroom was selected with certainty. At the 4th-grade level, 16 pseudo-classrooms were created prior to classroom sampling and 8 of these were selected in the final 4th-grade classroom sample. At the 8thgrade level, 503 pseudo-classrooms were created, of which 64 were included in the final classroom sample. All students in sampled classrooms and pseudo-classrooms were selected for assessment. In this way, the overall sample design for the United States results in an approximately self-weighting sample of students, with each 4th-or 8th-grade student having a roughly equal probability of selection. Note that in small schools, a higher proportion of the classes (and therefore of the students) is selected, but this higher rate of selecting students in small schools is offset by a lower selection rate of small schools, as schools are selected with probability proportional to size."}, {"section_title": "Additional sampling requirements for TIMSS 2011 in the United States: The NAEP-TIMSS Linking Study", "text": "The NAEP-TIMSS Linking Study administered NAEP and TIMSS booklets in both the NAEP and TIMSS administration windows under first NAEP and then TIMSS testing conditions. This was done to gather examinee-level correlations between NAEP and TIMSS scores that will be used to improve the accuracy of the predicted TIMSS state scores. Eight states were invited to participate in TIMSS separately from the nation at the 8th grade (Alabama, California, Colorado, Connecticut, Indiana, Massachusetts, Minnesota, and North Carolina) and one additional state, Florida, chose to participate itself. The actual TIMSS data from these nine states will be used to APPENDIX A validate the predicted TIMSS average scores based on the linking study. The states invited to participate were selected based on state enrollment size and willingness to participate, as well as on their general NAEP performance (above or below the national average on NAEP), their previous experience in benchmarking to TIMSS, and their regional distribution. To facilitate the linking study, NAEP added a science assessment at the national and state levels during the January-March 2011 collection period, with all states and the District of Columbia agreeing to participate. In addition, during the NAEP assessment window, a separate national sample of students was administered a set of \"braided\" booklets that included both NAEP and TIMSS blocks of items. During the NAEP assessment window, the braided booklets were designed to appear as similar as possible to a regular NAEP assessment booklet and were administered under the same conditions as NAEP. During the TIMSS data collection period in spring 2011, TIMSS added state-level assessments in the nine states noted above as well as a separate national sample of students who were also administered a set of braided booklets that included both NAEP and TIMSS blocks of items. During the TIMSS assessment window, the braided booklets were designed to appear as similar as possible to a regular TIMSS assessment booklet and were administered under the same conditions as TIMSS. The inclusion of two additional samples of students who were administered the braided booklets during the NAEP and TIMSS assessment windows is similar to the braided booklet design used for recent NAEP studies to maintain NAEP trends in 2009 reading and 12thgrade mathematics. Within each school, selected intact mathematics classes at 8th grade were randomly designated as a TIMSS class or a NAEP-TIMSS Linking Study class. The students in the linking study classes were administered the braided TIMSS-NAEP booklets. Those in the TIMSS classes were administered the regular TIMSS booklets. The national and state samples were selected to coordinate the main NAEP and TIMSS samples. The data were collected by the regular NAEP field staff in winter and the regular TIMSS field staff in spring. The responses to the assessment questions were scored using the same scoring staff with the same training and quality control procedures that NAEP and TIMSS normally use. The results of the effort to link NAEP and TIMSS are to be released in a separate report from NCES."}, {"section_title": "Additional state samples", "text": "In addition to the states that participated at 8th grade, North Carolina also participated in TIMSS at 4th grade. North Carolina, like Florida at 8th grade, elected to participate in TIMSS on its own. The process for selecting the North Carolina school sample was the same as that employed in selecting the other state school samples (described below)."}, {"section_title": "Selecting school samples for the states", "text": "The TIMSS state samples included only public schools. The school frame was identical to the national frame of public schools in those states. The state samples included the public schools in each state that were previously selected as part of the TIMSS national sample plus a supplement of schools. The sample target was 100 assessed classrooms. The target reference is classrooms because in the national design one class in each school is selected for TIMSS and one for the NAEP-TIMSS Linking Study. Thus, only one class from each of the national public schools was included in the state assessment. The supplemental sample of schools selected for each state followed the normal TIMSS procedure of selecting two classes per school. The additional number of schools needed in each state is then ([100 -# national public schools] / 2) plus an additional five schools per state to account for ineligible schools (schools with no students in the target grade). The state sample was selected using a version of the Keyfitz procedure. Chowdhury, Chu, and Kaufman (2001) have described the implementation of the procedure. The method is generally used to minimize overlap but it can also be used to maximize overlap by ordering the rows in descending order of the response load indicator. By following the process outlined in table 2 of the paper, the rows in the table can be thought of as a hierarchy of selection preference where the top row maximizes the probability and the bottom row minimizes it. This property allowed us to maximize the overlap with the TIMSS national sample (in fact, select all national public schools) and minimize the overlap with the NAEP state operational public school sample or \"Alpha sample.\" 10 This minimization was undertaken to reduce the burden for schools selected in the NAEP sample and to improve response rates. This was accomplished by partitioning the frame into the following three groups shown in order as in table 2 of the paper. The three groups are: 1. public schools selected for the TIMSS national sample (including schools also selected for the NAEP Alpha sample); 2. public schools not selected for either the TIMSS national or NAEP Alpha samples; and 3. public schools selected for the NAEP Alpha sample and not the TIMSS national sample. The method guarantees all schools in group 1 will be selected with certainty since the probability of being selected for the validation sample is always larger than that for being selected for the national sample, because more schools were selected HIGHLIGHTS FROM TIMSS 2011 in each state sample (the national public schools plus a state supplement of public schools) than in the national sample with the frames being identical. The method minimized the overlap with schools in group 3 (NAEP Alpha sample) and selected the majority of the state supplement from schools in group 2."}, {"section_title": "U.S. TIMSS 4th-grade sample", "text": "School sample. The 4th-grade national school sample consisted of 450 schools (both public and private). As described previously, the joint administration of TIMSS and PIRLS at 4th grade required a larger sample of schools to ensure an adequate number of participating classes and students in both studies. Twelve ineligible schools and one excluded school were identified on the basis that they served special student populations, or had closed or altered their grade makeup since the sampling frame was developed. This left 437 schools eligible to participate, and 347 agreed to do so. The unweighted school response rate before substitution then was 79 percent. The analogous weighted school response rate was also 79 percent (see table A-1) and is given by the following formula: weighted school response rate before replacement where Y denotes the set of responding original-sample schools; N denotes the set of eligible non-responding original sample schools; Wi denotes the base weight for school i; Wi = 1/Pi, where Pi denotes the school selection probability for school i; and Ei denotes the enrollment size of age-eligible students, as indicated in the sampling frame. In addition to the 347 participating schools from the original sample, 22 substitute schools participated for a total of 369 participating schools at the 4th grade in the United States (see table A-2). This gives a weighted (and unweighted) school participation rate after substitution of 84 percent (see table A-1). 11 Classroom sample. Schools agreeing to participate in TIMSS were asked to list their 4th-grade mathematics classes as the basis for sampling at the classroom level. At this time, schools were given the opportunity to identify special classes-classes in which all or most of the students had intellectual or functional disabilities or were non-English language speakers. While these classes were regarded as eligible, the students as a group were treated as \"excluded\" since, in the opinion of the school, their disabilities or language capabilities would 11 Substitute schools are matched pairs and do not have an independent probability of selection. NCES standards (Standard 1-3-8) indicate that, in these circumstances, response rates should be calculated without including substitute schools (National Center for Education Statistics 2002). TIMSS response rates denoted as \"before replacement\" conform to this standard. TIMSS response rates denoted as \"after replacement\" are not consistent with NCES standards since, in the calculation of these rates, substitute schools are treated as the equivalent of sampled schools. render meaningless their performance on the assessment. Fifty 4th-grade schools excluded classes and 681 students were excluded from participation in TIMSS as a result. Prior to sampling, classes with fewer than 15 students were collapsed with other classes into what are called pseudoclassrooms. Creating pseudo-classrooms in this way ensured that all eligible classrooms in a school had at least 20 students. Up to four eligible classrooms were selected with classes being randomly assigned to TIMSS or PIRLS. In schools with only one classroom, this classroom was selected with certainty and randomly assigned to TIMSS or PIRLS. Some 1,257 classrooms were selected as a result of this process. All selected classrooms participated in TIMSS, yielding a classroom response rate of 100 percent (Martin et al. 2012, exhibit C.8). Student sample. Schools were asked to list the students in each of the classrooms, along with the teachers who taught mathematics and science to these students. A total of 14,205 students were listed as a result. These students are identified by IEA as \"sampled students in participating schools\" (see table A-2). This pool of students is reduced by within-school exclusions and withdrawals. At the time schools listed the students in the sampled classrooms, they had the opportunity to identify particular students who were not suited to take the test because of physical or intellectual disabilities (i.e., students with disabilities who had been mainstreamed) or because they were non-English-language speakers. Schools identified a total of 839 students they wished to have excluded from the assessment. By the time of the assessment a further 185 of the listed students had withdrawn from the school or classroom. In total, then, the pool of 14,205 sampled students was reduced by 1,024 students (839 excluded and 185 withdrawn) to yield 13,181 \"eligible\" students. The number of eligible students is used as the base for calculating student response rates (Martin et al. 2012, exhibit C.6). The number of eligible students was further reduced on assessment day by 612 student absences, leaving 12,569 \"assessed students\" identified as having completed a TIMSS 2011 assessment booklet (see table A-2). IEA defines the student response rate as the number of students assessed as a percentage of the number of eligible students which, in this case, yields a weighted (and unweighted) student response rate of 95 percent (see table A-1). Note that the 681 students excluded because whole classes were excluded do not figure in the calculation of student response rates. They do, however, figure in the calculation of the coverage of the International Target Population. Together, these 681 students excluded prior to classroom sampling, plus the 839 within-class exclusions, resulted in an overall student exclusion rate of 7.0 percent (see  Martin et al. 2012, exhibit C.3). Combined participation rates. For the results for an education system to be included in the TIMSS international report without a response rate annotation, the IEA requires a \"combined\" or overall response rate-expressed as the product of (a) the (unrounded) weighted school response rate without substitute schools and (b) the (unrounded) weighted student response rate-of at least 75 percent (after rounding to the nearest whole percent). The overall response rate for the United States, 75.72 percent without substitute schools, meets this requirement. However, the United States did include substitute schools because its school-level response rate was less than 85 percent, and, absent advance knowledge of the student-level response rate, introducing substitute schools was a prudent approach to take. For the results of an education system to be included in the TIMSS international report without a student inclusion annotation, the IEA requires a student inclusion rate of at least 95 percent. Because 7 percent of the 4th-grade student population was excluded in the United States, the overall U.S. student inclusion rate was 93 percent. For this reason, the U.S. 4th-grade results in the TIMSS international report carry a coverage annotation indicating that coverage of the defined student population was less than the IEA standard of 95 percent. Tables A-1 and A-2 are extracts from the international report exhibits noted above and are designed to summarize information on school and student responses rates and coverage of the 4th-and 8th-grade target populations in each nation."}, {"section_title": "U.S. TIMSS 8th-grade sample", "text": "School sample. The 8th-grade national school sample consisted of 600 schools. Twenty-two ineligible original schools and four excluded schools were identified on the basis that they served special student populations, or had closed or altered their grade makeup since the sampling frame was developed. This left 574 schools eligible to participate and 499 agreed to do so. The unweighted original school response rate before substitution then was 87 percent. The analogous weighted school response rate was 87 percent (see table A-1). In addition to the 499 participating schools from the original sample, 2 substitute schools participated for a total of 501 participating schools at the 8th grade in the United States (see Classroom sample. Schools agreeing to participate were asked to list their 8th-grade mathematics classes as the basis for sampling at the classroom level. At this time, schools were given the opportunity to identify special classes-classes in which all or most of the students had intellectual or functional disabilities or were non-English-language speakers. While these classes were regarded as eligible, the students as a group were treated as \"excluded\" since, in the opinion of the school, their disabilities or language capabilities would render meaningless their performance on the assessment. A total of 223 schools excluded classrooms from participation in TIMSS. This resulted in the exclusion of 4,650 students. Classrooms with fewer than 15 students were collapsed into pseudo-classrooms prior to sampling so that each eligible classroom in a school had at least 20 students. Two eligible classrooms were selected per school where possible. In schools where there was only one eligible classroom, this classroom was selected with certainty and was randomly assigned to TIMSS or the TIMSS-NAEP Linking Study. All selected classrooms participated in TIMSS yielding a classroom response rate of 100 percent (Martin et al. 2012, exhibit C.9). Student sample. Schools were asked to list the students in each of the sampled mathematics classrooms, along with the teachers who taught mathematics and science to these students. A total of 11,864 students were listed as being in the selected classrooms. These students are identified by IEA as \"sampled students in participating schools\" (see table A-2). This pool of students is reduced by within-school exclusions and withdrawals. At the time schools listed the students in sampled classrooms, they had the opportunity to identify particular students who were not suited to take the test because of physical or intellectual disabilities (i.e., students with disabilities who had been mainstreamed) or because they were non-English language speakers. Schools identified a total of 398 students they wished to have excluded from the assessment; and, by the time of the assessment, a further 302 of the listed students had withdrawn from the school or classroom. In total then, the pool of 11,864 sampled students was reduced by 700 students (398 excluded and 302 withdrawn) to yield 11,164 \"eligible\" students. The number of eligible students is used as the base for calculating student response rates (Martin et al. 2012, exhibit C.7).  A-16"}, {"section_title": "APPENDIX A", "text": "\nand education systems to ensure that the content, as explicated in the frameworks, was covered adequately. Items were reviewed by an international Science and Mathematics Item Review Committee and field-tested in most of the participating countries. Results from the field test were used to evaluate item difficulty, how well items discriminated between high-and low-performing students, the effectiveness of distracters in multiple-choice items, scoring suitability and reliability for constructed-response items, and evidence of bias toward or against individual countries or in favor of boys or girls. As a 4th-grade result of this review, 72 new 4th-grade mathematics and 72 new 4th-grade science items were selected for inclusion in the international assessment. In total, 175 mathematics and 172 science items were included in the 4th-grade TIMSS assessment booklets. At the 8th grade, the review of the item statistics from the field test led to the inclusion of 91 new 8th-grade mathematics and 92 new 8th-grade science items in the assessment. In total, 217 mathematics and 217 science items were included in the 8th-grade TIMSS assessment booklets. More detail on the distribution of new and trend items is included in table A-3."}, {"section_title": "APPENDIX A HIGHLIGHTS FROM TIMSS 2011", "text": "The number of eligible students was further reduced on assessment day by 687 student absences, leaving 10,477 \"assessed students\" identified as having completed a TIMSS 2011 assessment booklet (see table A-2). The IEA defines the student response rate as the number of students assessed as a percentage of the number of eligible students which, in this case yields a weighted (and unweighted) student response rate of percent (see table A-1). Note that the 4,650 students excluded because whole classes were excluded do not figure in the calculation of student response rates. They do, however, figure in the calculation of the coverage of the International Target Population. Together, these 4,650 students excluded prior to classroom sampling, plus the 398 within-class exclusions resulted in an overall student exclusion rate of 7.2 percent (see table A-1 and Martin et al. 2012, exhibit C.3). The reported coverage of the International Target Population then is 100 percent (see Martin et al. 2012, exhibit C.3). Combined participation rates. TIMSS' combined school, classroom, and student weighted response rate standard of 75 percent was met: the weighted and unweighted product of the separate U.S. response rates (81 percent) exceeded this 75 percent standard (see table A-1). However, the United States did include substitute schools because its school-level response rate was less than 85 percent, and, absent advance knowledge of the student-level response rate, introducing substitute schools was a prudent approach to take. Because 7 percent of the 8th-grade student population was excluded in the United States, the overall U.S. student inclusion rate was 93 percent. For this reason, the U.S. 8th-grade results in the TIMSS international report carry a coverage annotation indicating that coverage of the defined student population was less than the IEA standard of 95 percent. Table A-2 summarizes information on the coverage of the 8th-grade target populations in each participating education system.\nThe 2011 8th-grade assessment followed the same pattern and consisted of 14 booklets, each requiring approximately 90 minutes of response time. The assessment was given in two 45-minute parts, with a 5-to 10-minute break in between. As in 4th grade, the student questionnaire was given after the second part of the assessment, and was allotted approximately 30 minutes of response time. The 14 booklets were rotated among students, with each participating student completing 1booklet only. The mathematics and science items were assembled into 14 blocks, or clusters, of items. Each block contained either mathematics items or science items only. For each subject, the secure, or trend, items used in prior assessments were included in 8 blocks, with the other 6 blocks containing new items. Each of the 14 TIMSS 2011 booklets contained 4 blocks in total. The TIMSS booklets administered in the state samples were exactly the same as those administered in the national sample. As part of the design process, it was necessary to ensure that the booklets showed a distribution across the mathematics and science content domains as specified in the frameworks. The number of mathematics and science items in the 4th-and 8th-grade TIMSS 2011 assessments is shown in table A-4.\nMissing background data on other than key variables are not included in the analyses for this report and are not imputed. 20 Item response rates for variables discussed in this report exceeded the NCES standard of 85 percent and so can be reported without notation. Of the three key variables identified in the TIMSS 2011 data for the United States-sex, race/ ethnicity and the percentage of students eligible for free or reduced-price lunch (FRPL)-as table A-5 indicates, sex has no missing responses and race/ethnicity has minimal missing responses at some 2 percent. The FRPL variable has some 5 percent missing responses at 4th grade and 3 percent missing responses at 8th grade among the public schools in the sample and these were imputed by substituting values taken from the CCD for the schools in question. Note, however, that the CCD provides this information only for public schools. The comparable database for private schools (PSS) does not include data on participation in the FRPL program."}, {"section_title": "Test development", "text": "TIMSS is a cooperative effort involving representatives from every country participating in the study. For TIMSS 2011, the test development effort began with a review and revision of the frameworks that are used to guide the construction of the assessment (Mullis et al. 2009). The frameworks were updated to reflect changes in the curriculum and instruction of participating countries and education systems. Extensive input from experts in mathematics and science education, assessment, and curriculum, and representatives from national educational centers around the world contributed to the final shape of the frameworks. Maintaining the ability to measure change over time was an important factor in revising the frameworks. As part of the TIMSS dissemination strategy, approximately one-half of the 2007 assessment items were released for public use. To replace assessment items that had been released, participants submitted items for review by subjectmatter specialists, and additional items were written by the IEA Science and Mathematics Review Committee in consultation with item-writing specialists in various countries 18 The international weighting procedures created a nonresponse adjustment class for each explicit stratum; see TIMSS and PIRLS Methods and Procedures  for details. In the case of the U.S. 4th-grade sample, there was no explicit stratification and thus a single adjustment class. The procedures could not be varied for individual countries to account for any specific needs. Therefore, the U.S. nonresponse bias analyses could have no influence on the weighting procedures and were undertaken after the weighting process was complete."}, {"section_title": "A-18", "text": "APPENDIX A HIGHLIGHTS FROM TIMSS 2011 "}, {"section_title": "Design of instruments", "text": "TIMSS 2011 included booklets containing assessment items as well as self-administered background questionnaires for principals, teachers, and students."}, {"section_title": "Assessment booklets", "text": "The assessment booklets were constructed such that not all of the students responded to all of the items. This is consistent with other large-scale assessments, such as NAEP. To keep the testing burden to a minimum, and to ensure broad subjectmatter coverage, TIMSS used a rotated block design that included both mathematics and science items. That is, students encountered both mathematics and science items during the assessment. In 2011, the 4th-grade assessment consisted of 14 booklets, each requiring approximately 72 minutes. The assessment was given in two 36-minute parts, with a 5-to 10-minute break in between. The student questionnaire was given after the second part of the assessment. Although it was untimed, it was allotted approximately 30 minutes for response time. The 14 booklets were rotated among students, with each participating student completing 1 booklet only. The mathematics and science items were each assembled separately into 14 blocks, or clusters, of items. Each of the 14 TIMSS 2011 booklets contained 4 blocks in total. Each block contained either mathematics items or science items only and each block occurred twice across the 14 books. For each subject, the secure, or trend, items used in prior assessments were included in 8 blocks, with the other 6 blocks containing new items. A-20"}, {"section_title": "Background questionnaires", "text": "As in prior administrations, TIMSS 2011 included selfadministered questionnaires for principals, teachers, and students. To create the questionnaires for 2011, the 2007 versions were reviewed extensively by the national research coordinators from the participating countries and education systems as well as a Questionnaire Item Review Committee (QIRC). The QIRC comprises 10-12 experienced National Research Coordinators (NRCs) from different participating countries and education systems who have analyzed TIMSS data and use it in their own countries or education system. The QIRC review resulted in items being deleted or revised, and the addition of several new ones. Like the assessment items, all questionnaire items were field-tested and the results reviewed carefully. As a result, some of the questionnaire items needed to be revised prior to their inclusion in the final questionnaires. The questionnaires requested information to help provide a context for the performance scores, focusing on such topics as students' attitudes and beliefs about learning, their habits and homework, and their lives both in and outside of school; teachers' attitudes and beliefs about teaching and learning, teaching assignments, class size and organization, instructional practices, and participation in professional development activities; and principals' viewpoints on policy and budget responsibilities, curriculum and instruction issues and student behavior, as well as descriptions of the organization of schools and courses. For 2011, online versions of the school and teacher questionnaires were offered to respondents as the primary mode of data collection. Detailed results from the student, teacher, and school surveys are not discussed in this report but are available in the two international reports: the TIMSS 2011 International Mathematics Report  and TIMSS 2011 International Science Report ."}, {"section_title": "Translation", "text": "Source versions of all instruments (assessment booklets, questionnaires, and manuals) were prepared in English and translated into the primary language or languages of instruction in each education system. In addition, it was sometimes necessary to adapt the instrument for cultural purposes, even in countries and education systems that use English as the primary language of instruction. All adaptations were reviewed and approved by the International Study Center to ensure they did not change the substance or intent of the question or answer choices. For example, proper names were sometimes changed to names that would be more familiar to students (e.g., Marja-leena to Maria). Each participant prepared translations of the instruments according to translation guidelines established by the International Study Center. Adaptations to the instruments were documented by each participant and submitted for review. The goal of the translation guidelines was to produce translated instruments of the highest quality that would provide comparable data across participants. Translated instruments were verified by an independent, professional translation agency prior to final approval and printing of the instruments. Participating education systems were required to submit copies of the final printed instruments to the International Study Center. Further details on the translation process can be found in the TIMSS and PIRLS Methods and Procedures ."}, {"section_title": "Recruitment, test administration, and quality assurance", "text": "TIMSS 2011 emphasized the use of standardized procedures for all participants. Each participating country and education system collected its own data, based on comprehensive manuals and training materials provided by the international project team to explain the survey's implementation, including precise instructions for the work of school coordinators and scripts for test administrators to use in testing sessions."}, {"section_title": "Recruitment of schools and students", "text": "With the exception of private schools, the recruitment of schools required several steps. Beginning with the sampled schools, the first step entailed obtaining permission from the school district to approach the sampled school(s) in that district. If a district refused permission, then the district of the first substitute school was approached and the procedure was repeated. With permission from the district, the school(s) was contacted in a second step. If a sampled school refused to participate, the district of the first substitute was approached APPENDIX A and the permission procedure repeated. During most of the recruitment period sampled schools and substitute schools were being recruited concurrently. Each participating school was asked to nominate a school coordinator as the main point of contact for the study. The school coordinator worked with project staff to arrange logistics and liaise with staff, students and parents as necessary. On the advice of the school, parental permission for students to participate was sought with one of three approaches to parents: a simple notification; a notification with a refusal form; and a notification with a consent form for parents to sign. In each approach, parents were informed that their students could opt out of participating."}, {"section_title": "Gifts to schools, school coordinators, and students", "text": "Schools, school coordinators, and students were provided with small gifts in appreciation for their willingness to participate. Schools were offered $200, school coordinators received $100, and students were given a clock-compass carabiner."}, {"section_title": "Test administration", "text": "Test administration in the United States was carried out by professional staff trained according to the international guidelines. School personnel were asked only to assist with listings of students, identifying space for testing in the school, and specifying any parental consent procedures needed for sampled students."}, {"section_title": "Calculator usage", "text": "Calculators were not permitted during the TIMSS 4th-grade assessment. However, the TIMSS policy on calculator use at the 8th grade was to give students the best opportunity to operate in settings that mirrored their classroom experiences. Calculators were permitted but not required for the 8th-grade assessment materials. In the United States, all students were allowed, but not required, to use a calculator."}, {"section_title": "Quality assurance", "text": "The International Study Center monitored compliance with the standardized procedures. National research coordinators were asked to nominate one or more persons unconnected with their national center, such as retired school teachers, to serve as quality control monitors for their country or education system. The International Study Center developed manuals for the monitors and briefed them in 2-day training sessions about TIMSS, the responsibilities of the national centers in conducting the study, and their own roles and responsibilities. Some 60 schools in the U.S. samples were visited by the monitors-30 schools in the 4th-grade sample, and 30 schools in the 8th-grade sample. These schools included those in both the national and state samples and were scattered geographically across the nation."}, {"section_title": "Scoring and scoring reliability", "text": "The TIMSS assessment items included both multiple-choice and constructed-response items. A scoring rubric (guide) was created for every constructed response item included in the TIMSS assessments. The rubrics were carefully written and reviewed by national research coordinators and other experts as part of the field test of items, and revised accordingly. The national research coordinator in each country or education system was responsible for the scoring and coding of data for that participant, following established guidelines. The national research coordinator and, sometimes, additional staff attended scoring training sessions held by the International Study Center. The training sessions focused on the scoring rubrics and coding system employed in TIMSS. Participants in these training sessions were provided extensive practice in scoring example items over several days. Information on within-country agreement among coders was collected and documented by the International Study Center. Information on scoring and coding reliability was also used to calculate cross-country agreement among coders. Information on scoring reliability for constructed-response scoring in TIMSS 2011 is provided in the international report."}, {"section_title": "Weighting, scaling, and plausible values", "text": "Before the data were analyzed, responses from the groups of students assessed were assigned sampling weights to ensure that their representation in the TIMSS 2011 results matched their actual percentage of the school population in the grade assessed. With these sampling weights in place, the analyses of TIMSS 2011 data proceeded in two phases: scaling and estimation. During the scaling phase, IRT procedures were used to estimate the measurement characteristics of each assessment question. During the estimation phase, the results of the scaling were used to produce estimates of student achievement. Subsequent analyses related these achievement results to the background variables collected by TIMSS 2011."}, {"section_title": "Weighting", "text": "Responses from the groups of students were assigned sampling weights to adjust for over-or under-representation during the sampling of a particular group. The use of sampling weights is necessary for the computation of sound, nationally representative estimates. The weight assigned to a student's responses is the inverse of the probability that the student is selected for the sample. When responses are weighted, none are discarded, and each contributes to the results for the total number of students represented by the individual student assessed. Weighting also adjusts for various situations (such as school and student nonresponse) because data cannot be assumed to be randomly missing. The internationally defined weighting specifications for TIMSS require that each assessed student's sampling weight should be the product of (1) the inverse of the school's probability of selection, (2) an adjustment for school-level nonresponse, (3) the inverse of the classroom's probability of selection, and (4) an adjustment for student-level nonresponse. 19 All TIMSS 1995, 2007, and 2011 analyses are conducted using sampling weights. A detailed description of this process is provided in the TIMSS and PIRLS Methods and Procedures . For 2011, though the national and state samples share schools, the samples are not identical school samples and, thus, weights are estimated separately for the national and state samples."}, {"section_title": "Scaling", "text": "In TIMSS, the propensity of students to answer questions correctly was estimated with a two-parameter IRT model for dichotomous constructed response items, a three-parameter IRT model for multiple choice response items, and a generalized partial credit IRT model for polytomous constructed response items. The scale scores assigned to each student were estimated using a procedure described 19 These adjustments are for overall response rates and did not include any of the characteristics associated with differential nonresponse as identified in the nonresponse bias analyses reported above. below in the \"Plausible values\" section, with input from the IRT results. With IRT, the difficulty of each item, or item category, is deduced using information about how likely it is for students to get some items correct (or to get a higher rating on a constructed response item) versus other items. Once the parameters of each item are determined, the ability of each student can be estimated even when different students have been administered different items. At this point in the estimation process achievement scores are expressed in a standardized logit scale that ranges from -4 to +4. In order to make the scores more meaningful and to facilitate their interpretation, the scores for the first year (1995) are transformed to a scale with a mean of 500 and a standard deviation of 100. Subsequent waves of assessment are linked to this metric (see below). To make scores from the second (1999) wave of data comparable to the first (1995) wave, two steps had to be taken. First, the 1995 and 1999 data for countries and education systems that participated in both years were scaled together to estimate item parameters. Ability estimates for all students (those assessed in 1995 and those assessed in 1999) based on the new item parameters were then estimated. To put these jointly calibrated 1995 and 1999 scores on the 1995 metric, a linear transformation was applied such that the jointly calibrated 1995 scores have the same mean and standard deviation as the original 1995 scores. Such a transformation also preserves any differences in average scores between the 1995 and 1999 waves of assessment. In order for scores resulting from subsequent waves of assessment (2003, 2007 and 2011) to be made comparable to 1995 scores (and to each other), the two steps above are applied sequentially for each pair of adjacent waves of data: two adjacent years of data are jointly scaled, then resulting ability estimates are linearly transformed so that the mean and standard deviation of the prior year is preserved. As a result, the transformed-2011 scores are comparable to all previous waves of the assessment and longitudinal comparisons between all waves of data are meaningful. To facilitate the joint calibration of scores from adjacent years of assessment, common test items are included in successive administrations. This also enables the comparison of item parameters (difficulty and discrimination) across administrations. If item parameters change dramatically across administrations, they are dropped from the current assessment so that scales can be more accurately linked across years. In this way even if the average ability levels of students in countries and education systems participating in TIMSS changes over time, the scales still can be linked across administrations."}, {"section_title": "APPENDIX A Plausible values", "text": "To keep student burden to a minimum, TIMSS administered a limited number of assessment items to each student-too few to produce accurate content-related scale scores for each student. To accommodate this situation, during the scaling process plausible values were estimated to characterize students participating in the assessment, given their background characteristics. Plausible values are imputed values and not test scores for individuals in the usual sense. In fact, they are biased estimates of the proficiencies of individual students. Plausible values do, however, provide unbiased estimates of population characteristics (e.g., means and variances of demographic subgroups). Plausible values represent what the performance of an individual on the entire assessment might have been, had it been observed. They are estimated as random draws (usually five) from an empirically derived distribution of score values based on the student's observed responses to assessment items and on background variables. Each random draw from the distribution is considered a representative value from the distribution of potential scale scores for all students in the sample who have similar characteristics and identical patterns of item responses. Differences between plausible values drawn for a single individual quantify the degree of error (the width of the spread) in the underlying distribution of possible scale scores that could have caused the observed performances. An accessible treatment of the derivation and use of plausible values can be found in Beaton and Gonz\u00e1lez (1995). A more technical treatment can be found in the TIMSS and PIRLS Methods and Procedures ."}, {"section_title": "International benchmarks", "text": "International benchmarks for achievement were developed in an attempt to provide a concrete interpretation of what the scores on the TIMSS mathematics and science achievement scales mean (for example, what it means to have a scale score of 513 or 426). To describe student performance at various points along the TIMSS mathematics and science achievement scales, TIMSS uses scale anchoring to summarize and describe student achievement at four points on the mathematics and science scales-Advanced (625), High (550), Intermediate (475), and Low (400) international benchmarks. Scale anchoring involves selecting benchmarks (scale points) on the TIMSS achievement scales to be described in terms of student performance. Once benchmark scores have been chosen, items are identified that students are likely to score highly on. The content of these items describes what students at each benchmark level of achievement know and can do. To interpret the content of anchored items, these items are grouped by content area within benchmarks and reviewed by mathematics and science experts. These experts focus on the content of each item and describe the kind of mathematics or science knowledge demonstrated by students answering the item correctly. The experts then provide a summary description of performance at each anchor point leading to a contentreferenced interpretation of the achievement results. Detailed information on the creation of the benchmarks is provided in the international TIMSS reports ."}, {"section_title": "Data limitations", "text": "As with any study, there are limitations to TIMSS 2011 that researchers should take into consideration. Estimates produced using data from TIMSS 2011 are subject to two types of error-nonsampling and sampling errors. Nonsampling errors can be due to errors made in collecting and processing data. Sampling errors can occur because the data were collected from a sample rather than a complete census of the population."}, {"section_title": "Nonsampling errors", "text": "Nonsampling error is a term used to describe variations in the estimates that may be caused by population coverage limitations, nonresponse bias, and measurement error, as well as data collection, processing, and reporting procedures. The sources of nonsampling errors are typically problems like unit and item nonresponse, differences in respondents' interpretations of the meaning of the survey questions, response differences related to the particular time the survey was conducted, and mistakes in data preparation. Missing data. Five kinds of missing data were identified by separate missing data codes: omitted, uninterpretable, not administered, not applicable, and not reached. An item was considered omitted if the respondent was expected to answer the item but no response was given (e.g., no box was checked in the item which asked \"Are you a girl or a boy?\"). Items with invalid responses (e.g., multiple responses to a question calling for a single response) were coded as uninterpretable. The not administered code was used to identify items not administered to the student, teacher, or principal (e.g., those items excluded from the student's test booklet because of the BIB-spiraling of the items). An item was coded as not applicable when it is not logical that the respondent answer the question (e.g., when the opportunity to make the response is dependent on a filter question). Finally, items that are not reached were identified by a string of consecutive items without responses continuing through to the end of the assessment or questionnaire. A-24"}, {"section_title": "Sampling errors", "text": "Sampling errors arise when a sample of the population, rather than the whole population, is used to estimate some statistic. Different samples from the same population would likely produce somewhat different estimates of the statistic in question. This fact means that there is a degree of uncertainty associated with statistics estimated from a sample. This uncertainty is referred to as sampling variance and is usually expressed as the standard error of a statistic estimated from sample data. The approach used for calculating standard errors in TIMSS was jackknife repeated replication (JRR). Standard errors can be used as a measure for the precision expected from a particular sample. Standard errors for all of the reported estimates are included in appendix E. Confidence intervals provide a way to make inferences about population statistics in a manner that reflects the sampling error associated with the statistic. Assuming a normal distribution, the population value of this statistic can be inferred to lie within the confidence interval in 95 out of 100 replications of the measurement on different samples drawn from the same population. For example, the average mathematics score for the U.S. 8th-grade students was 509 in 2011, and this statistic had a standard error of 2.6. Therefore, it can be stated with 95 percent confidence that the actual average of U.S. 8th-grade students in 2011 was between 504 and 514 (1.96 x 2.6 = 5.1; confidence interval = 509 +/-5.1). 20 Key variables include survey-specific items for which aggregate estimates are commonly published by NCES. They include, but are not restricted to, variables most commonly used in table row stubs. Key variables also include important analytic composites and other policy-relevant variables that are essential elements of the data collection. For example, the National Assessment of Educational Progress (NAEP) consistently uses sex, race/ethnicity, urbanicity, region, and school type (public/private) as key reporting variables."}, {"section_title": "Description of background variables", "text": "The international versions of the TIMSS 2011 student, teacher, and school questionnaires are available at http://timss.bc.edu. The U.S. versions of these questionnaires are available at http://nces.ed.gov/timss."}, {"section_title": "Race/ethnicity", "text": "Students' race/ethnicity was obtained through student responses to a two-part question. Students were asked first whether they were Hispanic or Latino, and then whether they were members of the following racial groups: American Indian or Alaska Native; Asian; Black or African American; Native Hawaiian or other Pacific Islander; or White. Multiple responses to the race classification question were allowed. Students who responded that they are Hispanic or Latino were categorized as Hispanic, regardless of their reported races. Results are shown separately for Blacks, Hispanics, Whites, Asians, and Multiracial as distinct groups. The small numbers of students indicating that they were American Indian or Alaska Native or Native Hawaiian or other Pacific Islander are included in the total but not reported separately. Poverty level in public schools (percentage of students eligible for free or reduced-price lunch) The poverty level in public schools was obtained from principals' responses to the school questionnaire. The question asked the principal to report, as of approximately the first of October 2010, the percentage of students at the school eligible to receive free or reduced-price lunch through the National School Lunch Program. The answers were grouped into five categories: less than 10 percent; 10 to 24.9 percent; 25 to 49.9 percent; 50 to 74.9 percent; and 75 percent or more. Analysis was limited to public schools only. Missing data on this variable were replaced with measures taken from the CCD. The effect of this replacement on the confidentiality of the data was examined as part of the confidentiality analyses described in the following section."}, {"section_title": "Confidentiality and disclosure limitations", "text": "In accord with NCES standard 4-2-6 (NCES Education Statistics 2002), confidentiality analyses for the United States were implemented to provide reasonable assurance that public-use data files issued by the IEA and NCES would not allow identification of individual U.S. schools or students when compared against publicly available data collections. Disclosure limitations included the identification and masking of potential disclosure risks for TIMSS schools and adding an additional measure of uncertainty of school, teacher, and student identification through random swapping of a small number of data elements within the student, teacher, and school files. 21 These procedures were applied to the national and state samples."}, {"section_title": "Statistical procedures", "text": ""}, {"section_title": "Tests of significance", "text": "Comparisons made in the text of this report were tested for statistical significance. For example, in the commonly made comparison of education systems averages against the average of the United States, tests of statistical significance were used to establish whether or not the observed differences from the U.S. average were statistically significant. The estimation of the standard errors that is required in order to undertake the tests of significance is complicated by the complex sample and assessment designs, both of which generate error variance. Together they mandate a set of statistically complex procedures in order to estimate the correct standard errors. As a consequence, the estimated standard errors contain a sampling variance component estimated by the jackknife repeated replication (JRR) procedure; and, where the assessments are concerned, an additional imputation variance component arising from the assessment design. Details on the procedures used can be found in the WesVar 5.0 User's Guide (Westat 2007)."}, {"section_title": "APPENDIX B Appendix B: Example Items", "text": "After each administration of TIMSS, the IEA releases to the public somewhat less than half of the TIMSS items in order to illustrate the content of the assessment. The remaining items are kept secure so they can be used again in a future administration of TIMSS to measure trends in performance. This appendix contains sample mathematics and science items used in the U.S. administration of TIMSS 2011. These items have been selected from the set of released items to provide examples from each of the international benchmark levels, each of the content and cognitive domains, and each of the response types. Exhibits B.1 and B.2 below provide a key to which items are examples of each of these dimensions, B.1 for mathematics and B.2 for science. Reading exhibit B.1, for example, one can see that two items illustrate the Number content domain at grade 4 (exhibits B.5 and B.6) but that each of these represents a different benchmark level, a different cognitive domain, and a different item response type. Each item is presented on a separate page in this appendix. For all multiple choice items, the test question and \"response options\" (possible answers) are reproduced on the page along with the \"item key\" (correct answer). For all constructedresponse items, the \"scoring rubric\" (the criteria for scoring) is reproduced along with the test question. All item pages also include the percentage of students who received full credit for their answer in each participating country or other education system. Note that although most constructed response items were worth 1 point, some were worth 2 points with 1 point awarded for partial credit. In this appendix, if an example item was worth 2 points, only the percentages of students with responses awarded 2 points (full credit) are shown. in the NAEP science framework. 2 Finally, both assessments use similar proportions of multiple-choice and constructedresponse item formats, though the proportion of multiple-choice items is greater in NAEP than in TIMSS at both the 4th and 8thgrades. In short, the purpose of the assessments, the content coverage, and the grade-level correspondence of the assessment items distinguish TIMSS from NAEP 2009/2011. The item differences are more noteworthy in the science assessments than in the mathematics assessments. Thus, it is important to bear in mind these differences when interpreting U.S. students' achievement, nationally and internationally, on NAEP and TIMSS. As with the mathematics assessments, the percentage of \"no fit\" items by grade level in the science assessments is smaller than the percentage of \"no fit\" items by content level. This is because some of the content-level \"no fit\" items were determined to be implied in the NAEP 2011 framework at a particular grade. www.ed.gov ies.ed.gov"}]