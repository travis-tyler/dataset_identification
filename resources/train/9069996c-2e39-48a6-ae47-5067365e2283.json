[{"section_title": "", "text": ""}, {"section_title": "4-76", "text": "5-1 \"Other, specify\" codes added during fifth grade: School year 2003-04..........\nThe first original school that each team completed was called to ascertain how well the preassessment and assessment activities went. If the feedback from the school was positive, the fifth school that each team completed was called. If any problems were indicated in the first validation call, immediate action was taken with the field supervisor. The validation feedback was discussed with the supervisor and remedial action was taken, including telephone training on areas needing improvement or in-person observation of the supervisor's next school, if necessary; no remedial action was warranted based on the school validation calls. Field managers used a standardized telephone script, the School Validation Script, to call the school principals. The script covered the following topics: An overall rating of how the assessments went; Feedback the principal had received about the study from children and teachers; Suggestions for improving procedures and making it easier for a school to participate; and General comments and suggestions. Field managers called schools throughout the field period as scheduled assessments were completed. By the end of the field period in June, field managers had called 162 original sample schools (100 percent of the schools to be validated) to ascertain whether the conduct of the child assessments in the school was satisfactory; one school administrator could not be reached before the end of the school year. Table 4-23 presents the results of the school validation calls. 5-1"}, {"section_title": "5-3", "text": "5-2 \"Other, specify\" items added to the coding system: School year 2003-04 ....."}, {"section_title": "5-4 5-3", "text": "Number of text strings by \"Other, specify\" item, fifth-grade parent interview: xiii   School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 1-2 ECLS-K conceptual model: School years 1998School years -2004 1-3 Instruments used in the ECLS-K, by round of data collection: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 1-4 Direct child assessments, by domain and round of data collection: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003    xxi xxii This page is intentionally left blank. 1-1"}, {"section_title": "INTRODUCTION", "text": "This methodology report provides technical information about the development, design, and conduct of the fifth-grade 1 data collection of the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). It begins with an overview of the ECLS-K study. Subsequent chapters provide information on the development of the instruments, sample design, data collection methods, data preparation and editing, response rates, and weighting and variance estimation. The ECLS-K focuses on children's early school experiences, beginning with kindergarten. It is a multisource, multimethod study that includes interviews with parents; the collection of data from principals, teachers, and student record abstracts; and direct child assessments. The ECLS-K follows a nationally representative cohort of children from kindergarten into high school. The base year data were collected in the fall and spring of the 1998-99 school year when the sampled children were in kindergarten. A total of 21,260 kindergartners throughout the nation participated. Two more waves of data were collected in the fall and spring of the 1999-2000 school year when most, but not all, of the base year children were in first grade. 2 The fall-first grade data collection was limited to a 30 percent subsample of schools. Approximately 27 percent of the base year students who were eligible to participate in year 2 attended the 30 percent subsample of schools (see exhibit 1-1). The fall-first grade data collection was a design enhancement to enable researchers to measure the extent of summer learning loss and the factors that contributed to such loss and to better disentangle school and home effects on children's learning. The spring-first grade data collection, including the full sample, was 1 The term \"fifth grade\" is used throughout this document to refer to the data collection that took place in the 2003-2004 school year, at which time most of the sampled children-but not all of them-were in fifth grade. 2 Though the majority of base year children were in first grade during the 1999-2000 school year, about 5 percent of the sampled children were retained in kindergarten and a handful of others were in second grade during the 1999-2000 school year. 1 Fall data collection consisted of a 30 percent sample of schools containing approximately 27 percent of the base year students eligible to participate in year 2. NOTE: See section 1.3 for a description of the study components. More information is provided in the Combined User's Manual for the ECLS-K Fifth-Grade Data Files and Electronic Codebooks (NCES 2006-032) (Tourangeau et al. forthcoming). SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99, spring 2004 part of the original study design and can be used to measure annual school progress and to describe the first grade learning environment of children in the study. All children assessed during the base year were eligible to be assessed in the spring-first grade data collection regardless of whether they repeated kindergarten, were promoted to first grade, or were promoted to second grade. In addition, children who were not in kindergarten in the United States during the 1998-99 school year and, therefore, did not have a chance to be selected to participate in the base year of the ECLS-K were added to the spring-first grade sample. 3 Such children included immigrants to the United States who arrived after fall 1998 sampling, children living abroad during the 1998-99 school year, children who were in first grade in 1998-99 and repeated it in 1999-2000, and children who did not attend kindergarten. Their addition allows researchers to make estimates for all first-graders in the United States rather than just for those who attended kindergarten in the United States in the previous year. A fifth wave of data was collected in the spring of the 2001-02 school year when most, but not all, of the sampled children were in third grade. Approximately 89 percent of the children interviewed were in third grade during the 2001-02 school year, 9 percent were in second grade, and less than 1 percent were in fourth grade or higher. In addition to the school, teacher, parent, and child assessment data collection components, children were asked to complete a short self-description questionnaire, which asked them how they thought and felt about themselves, both socially and academically. The spring-third 3 The addition of these children is referred to as \"freshening\" the sample. See chapter 3 for more detail on the freshening process."}, {"section_title": "1-3", "text": "grade data collection can be used to measure school progress and to describe the third-grade learning environment of children in the study. A sixth wave of data was collected in the spring of the 2003-04 school year when most, but not all, of the sampled children were in fifth grade. 4 In addition to the school, teacher, parent, and child assessment data collection components, children were asked to complete a short self-description questionnaire, which asked them how they thought and felt about themselves, both socially and academically. They were also asked about their food consumption at school and in the week prior to the interview. The spring-fifth grade data collection can be used to measure school progress and to describe the fifth-grade learning environment of children in the study. The sample of children in the fifth-grade round of data collection of the ECLS-K represents the cohort of children who were in kindergarten in 1998-99 or in first grade in 1999-2000. Since the sample was not freshened after the first-grade year with third-or fifth-graders who did not have a chance to be sampled in kindergarten or first grade (as was done in first grade), estimates from the ECLS-K thirdand fifth-grade data are representative of the population cohort rather than all third-graders in 2001-02 or all fifth-graders in 2003-04. The estimated number of third-graders from the third-grade ECLS-K data collection is approximately 86 percent of all third-graders. From the fifth-grade ECLS-K data collection, the estimated number of fifth-graders is approximately 83 percent of all fifth-graders. While the vast majority of children in third grade in the 2001-02 school year and in fifth grade in the 2003-04 school year are members of the cohort, third-graders who repeated second or third grade, fifth-graders who repeated third or fourth grade, and recent immigrants are not covered. Data were collected from teachers and schools to provide important contextual information about the school environment for the sampled children. The teachers and schools are not representative of fifth-grade teachers and schools in the country in 2003-04. For this reason, the only weights produced from the study are for making statements about children, including statements about the teachers and schools of those children. The ECLS-K has several major objectives and numerous potential applications. The ECLS-K combines (1) a study of achievement in the elementary years; (2) an assessment of the developmental status of children in the United States at the start of their formal schooling and at key points during the elementary school years; (3) cross-sectional studies of the nature and quality of kindergarten programs in the United States; and (4) a study of the relationship of family, preschool, and 1-4 school experiences to children's developmental status at school entry and their progress from kindergarten, through elementary school, and into high school. The ECLS-K is part of a longitudinal studies program comprising two cohorts-a kindergarten cohort and a birth cohort. The birth cohort (ECLS-B) is following a national sample of children born in the year 2001 from birth to kindergarten. The ECLS-B focuses on the characteristics of children and their families that influence children's first experiences with the demands of formal school, as well as children's early health care and in-and out-of-home experiences. Together these cohorts will provide the depth and breadth of data required to more fully describe and understand children's health and early learning, development, and education experiences. The ECLS-K has both descriptive and analytic purposes. It provides descriptive data on children's status at school entry, their transition into school, and their progress into high school. The ECLS-K also provides a rich data set that enables researchers to analyze how a wide range of family, school, community, and individual variables affect children's early success in school; explore school readiness and the relationship between the kindergarten experience and later elementary school performance; and record children's cognitive and academic growth as they move through secondary school."}, {"section_title": "Background", "text": "Efforts to expand and improve early education will benefit from insights gained through analyses drawn from the large scale, nationally representative ECLS-K data and the study's longitudinal design. The ECLS-K database contains information about the types of school programs in which children participated, the services they received, and repeated measures of the children's cognitive skills and knowledge. The ECLS-K database also contains measures of children's physical health and growth, social development, and emotional well-being, along with information on family background and the educational quality of their home environments. As a study of early achievement, the ECLS-K allows researchers to examine how children's progress is associated with such factors as placement in high or low ability groups, receipt of special services or remedial instruction, grade retention, and frequent changes in schools attended because of family moves. Data on these early school experiences are collected as they occur, with the exception of 1-5 their experiences before kindergarten, which were collected retrospectively. This produces a more accurate measurement of these antecedent factors and enables inferences to be made about their relationship to later academic progress. The longitudinal nature of the study enables researchers to study children's cognitive, social, and emotional growth and to relate trajectories of change to variations in children's experiences in kindergarten and the early grades to later grades. The spring-fifth grade data collection can be used to describe the diversity of children in the study and the classrooms and schools they attend. It can also be used to study children's academic gains in the years following kindergarten and first grade. The ECLS-K sample includes substantial numbers of children from various minority groups. Thus, the ECLS-K data present many possibilities for studying cultural and ethnic differences in the educational preferences and literacy practices of families, the developmental patterns and learning styles of children, and the educational resources and opportunities that different groups are afforded in the United States."}, {"section_title": "Conceptual Model", "text": "The design of the ECLS-K has been guided by a framework of children's development and schooling that emphasizes the interrelationships between the child and family, the child and school, the family and school, and the family, school, and community. The ECLS-K recognizes the importance of factors that represent the child's health status and socioemotional and intellectual development and incorporates factors from the child's family, community, and school-classroom environments. The ECLS-K conceptual model is depicted in exhibit 1-2. The study has paid particular attention to the role that parents and families play in helping children adjust to formal school and in supporting their education through the elementary grades. It has also gathered information on how schools prepare for and respond to the diverse backgrounds and experiences of the children and families they serve."}, {"section_title": "Study Components", "text": "The emphasis placed on measuring children's environments and development broadly has critical implications for the design of the ECLS-K. The design of the study includes the collection of data from the child, the child's parents/guardians, teachers, and schools."}, {"section_title": "1-8", "text": "data as it is receipted from the field. Chapter 6 provides information on unit and item response rates. Chapter 7 discusses weighting and variance information. Because both this report and the ECLS-K Psychometric Report for the Fifth Grade focus on the fifth-grade data collection, minimal information is provided about the base year, first-grade, or thirdgrade data. Users who wish to learn more about these data collections should refer to the ECLS-K Base Year Public-Use Data Files and Electronic Codebook: User's Manual (NCES 2001-029r) (Tourangeau, Burke, et al. 2004); the User's Manual for the ECLS-K First Grade Public-Use Data Files and Electronic Codebook (NCES 2002-135) (Tourangeau et al. 2002); or the User's Manual for the ECLS-K Third Grade Public-Use Data File and Electronic Code Book (NCES 2004-001) (Tourangeau, Brick, et al. 2004) Additional information about the ECLS program can be found on the World Wide Web at http://nces.ed.gov/ecls."}, {"section_title": "1-9", "text": "Exhibit 1-3. Instruments used in the ECLS-K, by round of data collection: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003School years -04 1998 School facilities checklist X X X X Salary and benefits questionnaire 7 X Head Start verification 8 X X Round that included the instrument. 1 The fall-first grade data collection consisted of a 30 percent subsample of the study schools. See the User's Manual for the ECLS-K First Grade Public-Use Data Files and Electronic Code Book (NCES 2002-135) (Tourangeau, Burke, et al. 2002) for information about the purposes and methods of the fall-first grade data collection."}, {"section_title": "1-10", "text": "Exhibit 1-4. Direct child assessments, by domain and round of data collection: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003School years -04 1998  General knowledge (science and social studies) X X X X Science 2 X X Psychomotor X Height and weight X X X X X X X Round that included the instrument. 1 OLDS (Oral Language Development Scale) was given to language-minority students new to the study in the spring, or who did not pass the cut score in the English version during the previous OLDS administration. The screener determined if the children understood English well enough to receive the direct child assessments in English. For further information on the language screener, please refer to the ECLS-K Base Year Public-Use Data Files and Electronic Code Book: User's Manual (NCES 2001-029r) Burke, et al. 2004). The screener was not used in third or fifth grade because the vast majority of children passed it by spring-first grade. 2 In spring-third grade, the general knowledge assessment was replaced with a science assessment. Children received a science assessment in third and fifth grade that measured their understanding of science concepts and scientific investigation skills. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. 2-1"}, {"section_title": "DEVELOPMENT OF SURVEY INSTRUMENTS", "text": "The Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) fifthgrade survey collected data on the achievement and home and school experiences of children who had attended kindergarten in 1998-99 to provide information on the children's progress in the middle elementary grades. In the design phase of the ECLS-K kindergarten, first-grade, third-grade, and fifthgrade waves of data collection, policymakers, teachers, and researchers were consulted, and relevant literature was reviewed to ascertain the specific areas within each of the topical components for which national data were needed. Information gathered from these activities guided the formulation of research questions deemed most important for the ECLS-K to address. Extant surveys were reviewed to identify surveys that had been fielded to answer similar questions. The ECLS-K data collection instruments were similar in content and form in all six waves of the study. The ECLS-K employed two modes of data collection, computer-assisted and self-administered hard-copy instruments. This chapter describes the development of the computer-assisted and hard-copy instruments for the fifth-grade data collection. The procedures for developing the child assessment battery and indirect rating forms are described in a separate psychometric report. More information on the assessment battery and indirect rating forms is found in ECLS-K Psychometric Report for Kindergarten Through First Grade (NCES 2002-05) (Rock and Pollack 2002) and in ECLS-K Psychometric Report for the Third Grade (NCES 2005-062)  . In spring-fifth grade, several modifications were made to the instruments. Many of the changes were based on advice given by the ECLS-K Technical Review Panel (TRP) and Content Review Panel (CRP) that was provided for both the spring-third grade and spring-fifth grade data collections simultaneously. Modifications were made to the fifth-grade parent interview to reduce its length and add other items of interest. A timing study was conducted to assess the effect of these changes. Several changes were also made to the child assessment, the teacher questionnaires, and the school administrator   (PPQ.100,PPQ.110,PPQ.120,PPQ.130,PPQ.140,PPQ.150,PPQ.160,PPQ.170,PPQ.180,PPQ.190,PPQ.200,PPQ.210,PPQ.230,PPQ.240)."}, {"section_title": "2-5", "text": "Several new construct areas were added for fifth grade. These were as follows: Discussions with child about school and friends (HEQ.420a and b); Discussions with child about smoking, sexual activity, drinking alcohol, and using other drugs (HEQ.421a-d); Identification of when a diagnosis of a particular disability was made, if applicable (CHQ.076,CHQ.077,CHQ.136,CHQ.137,CHQ.186,CHQ.187,CHQ.226,CHQ.227,CHQ.314,CHQ.315,CHQ.346,CHQ.347,CHQ.376,CHQ.377); Identification of when cochlear implants were implanted (CHQ.251-CHQ.253); Child's use of cochlear implants in school (CHQ.254); Identification of when child's use of therapy services or program for children with disabilities ended (CHQ.536, CHQ.537); Reason why the child no longer participated in services for children with special needs or special education (CHQ.546); Whether child takes prescription medication for Attention Deficit Disorder (ADD), Attention Deficit Hyperactivity Disorder (ADHD), or hyperactivity (CHQ.740); Medications taken for ADD, ADHD, or hyperactivity (CHQ.750a-g); Length of time medications have been taken for ADD, ADHD, or hyperactivity (CHQ.760); Receipt of family therapy (CHQ.770); Reason for family therapy (CHQ.780); Type of family therapist seen (CHQ.790); and Number of times family therapist seen (CHQ.800). Other questions were added that had been used previously in the ECLS-K for parent figures in the household, but in spring-fifth grade were asked about a new group of persons (nonresident biological parents). These were: Country of origin, age moved to U.S., and U.S. citizenship for non-resident biological parents (section COQ);"}, {"section_title": "2-6", "text": "In addition, one question was reintroduced from an earlier year of the study: Whether father or mother figure has a high school diploma or its equivalent (PEQ.021); Finally, some questions do not include new content but were modified from a previous round: In spring-fifth grade, interviewers were able to record the time set aside every day for children to do homework in hours (HEQ.092b) in addition to minutes (HEQ.093a). Questions about who usually helps the child with his/her reading, language arts, or spelling homework or helps with math homework allowed the respondent to volunteer who helped (HEQ.095b, HEQ.099). In spring-third grade, respondents were instead asked separate questions about each household member and whether he/she helped with homework."}, {"section_title": "Timing Study", "text": "As with any study instrument, questionnaire length and respondent burden were issues of concern. A timing study was conducted for the draft parent questionnaire. Three Westat staff members conducted nine interviews with respondents who had previously volunteered to participate in studies being conducted by Westat. No attempt was made to recruit respondents representative of either racial or economic groups as the objective was to obtain an estimate of the length of the questionnaire rather than to examine how individuals interpreted the questions. Westat did attempt to select people who would go through the various questionnaire paths (e.g., married couples, single parents). All of the respondents were parents of fifth-grade children. All interviews were conducted over the telephone using a paper version of the questionnaire. Interviewers used stopwatches to time the individual sections and to get an overall time for the interview. The interviewers stopped the watches for extended interruptions, such as a respondent having to take care of the needs of a family member. In most cases, the respondents were asked to answer questions in sections that required knowledge of data collected from an earlier wave of the data collection as if they had provided the information in a previous round of the survey. In only two interviews were respondents asked to complete such sections, as would be the case with a new respondent. The revised paper version of the questionnaire took an average of 39 minutes and 17 seconds to complete. Table 2-1 summarizes the overall and section timings for each interviewer and presents the 2-7 average time expended for each section. The initials denote the three interviewers. Each interviewer completed three interviews (e.g., interviewer SG completed SG1, SG2, and SG3). One interview was done with a parent of twins (NV2). The second twin's time was included in the overall average time. Mean time Total 0:35:53 0:41:18 0:45:10 0:41:15 0:34:23 0:53:39 0:14:34 0:35:14 0:36:33 0:30:07 \u2020 INQ 0:01:28 0:00:39 0:01:01 0:01:45 0:01:12 0:02:05 0:00:23 0:01:31 0:01:28 0:00:43 0:01:19 PIQ 0:02:50 0:02:20 0:03:28 0:03:38 0:03:41 0:03:58 0:01:34 0:03:21 0:03:04 0:03:02 0:03:16 FSQ 0:01:59 0:03:10 0:01:04 0:03:30 0:01:07 0:02:35 0:00:00 0:02:12 0:01:55 0:01:39 0:02:08 HEQ 0:08:00 0:14:05 0:09:02 0:07:39 0:09:18 0:09:32 0:04:45 0:06:43 0:06:16 0:07:29 0:08:40 CFQ 0:00:12 0:00:11 0:00:00 0:00:10 0:00:10 0:00:56 Overall, the parent interview required just under 40 minutes to complete. The timings for the first interview in the household ranged from a low of 30 minutes to a high of 53 minutes. The approaches used to capture the information (update versus obtain new data) and the characteristics of the child and household contributed to the variations in the length of interviews. The twin interview required an additional 14 minutes. No individual section was unduly long. The section that required the most time to administer was the Home Environment, Activities and Cognitive Stimulation (HEQ) section. In the first interview for respondents (i.e., for child 1), it took between 6 minutes and 14 minutes, depending upon the interviewer. Only two respondents completed the Nonresident Parent section of the questionnaire, which 2-8 included items about parents who did not live with their child, such as a question about the frequency of their contact with the child. The average time for this section was about 3 minutes. The results of the timing study suggested that the parent questionnaire could be administered within 40 minutes."}, {"section_title": "Child Food Consumption Questions", "text": "To measure children's food consumption the ECLS-K assessors administered the Food Consumption Questionnaire (FCQ), a questionnaire used to determine the kinds of food the children can buy at school and food they have eaten in the past week. The FCQ for children consisted of 19 questions. There were also food consumption questions for school administrators. Those are described in section 2.6 below. In the FCQ for children, the first set of questions was about foods that are high in fat, sodium, and/or added sugars (e.g., candy, salty snacks, soda pop). Children were asked if they could buy these foods at school, and if so, how often they bought the food in the past week and where they bought the food (vending machine, cafeteria, or somewhere else in school). In the second set of questions, children were asked about whether they ate particular key foods and beverages in the past 7 days, such as milk, sweetened beverages (e.g., soft drinks), fruits and vegetables, and fast food. They were asked to include food they ate at home, at school, at restaurants, or anywhere else. Items for the FCQ were taken mainly from existing surveys, although some were developed for the ECLS-K. Two main sources for questions were two surveys by the Center for Disease Control/Division of Adolescent and School Health Surveys: the Youth Risk Behavior Surveillance Survey (YRBSS) and the School Health Programs and Policies Survey (SHPPS). 1 The question on fastfood meals was taken from the California Children's Healthy Eating and Exercise Practices Survey (CalCheeps). Questions on soft drinks and children's at-school consumption of snack foods were developed at the U.S. Department of Agriculture (USDA), using YRBSS and CalCheeps questions as models. Assessors read each question of the FCQ to the child, along with the response categories, and the child circled his or her answer. The child was asked to tell the assessor what he or she circled so the assessor could enter the answer into the computer. At the beginning of the FCQ, there is an example 1 Information on these CDC surveys is available at http://www.cdc.gov/nccdphp/dash/ 2-9 question to show the child what kinds of questions would be asked. The example was also used to show the child how to circle a response and to practice telling the assessor what answer had been chosen. After the first few questions of the FCQ, if the child appeared to understand the response categories and was in one of the higher reading categories in the reading assessment, the child was allowed to read the response categories if he or she wanted to do so. For children who were homeschooled by their parents or another adult and did not attend school, questions about food that could be purchased at school did not apply. For these cases, assessors were told to skip questions 1 through 9 and enter \"Don't Know\" into CAPI for each of these questions and then begin with the statement after question 9."}, {"section_title": "Modifications to Teacher Questionnaires", "text": "The approach for administering teacher questionnaires in spring-fifth grade differed from that of previous rounds because many fifth-grade children were expected to have different teachers for different subject areas. In the prior rounds of data collection, general education teacher questionnaires were designed for a single classroom teacher. All questions pertaining to the core academic subjects were asked in a single questionnaire and distributed to one teacher for each sampled child. When children had different subject matter teachers, it was left to the child's main teacher to ask the other teachers to complete specific sections of the questionnaire for the subjects they taught. However, as children move through the elementary grades, the prevalence of children being taught core academic subjects (reading/language arts, mathematics, and science) by a single teacher decreased. Data from the 1999-2000 Schools and Staffing Survey (SASS) indicated that the proportion of children taught core academic subjects by different teachers (i.e., team teaching and departmentalized instruction) was about 41 percent in public schools and about 56 percent in private schools. Thus, data collection procedures for spring-fifth grade were designed to ensure that the teachers most knowledgeable of the child's performance in each of the core academic subjects (i.e., reading/language arts, mathematics, and science) provided the data germane to each child's classroom environment, instruction in each of the core academic subjects, and the core academic teacher's professional background. During the spring-fifth grade data collection, each child's reading and math or science teacher received a self-administered teacher-level questionnaire about a variety of topics, including instructional practices, classroom resources, views on teaching and the school, and teacher background."}, {"section_title": "2-10", "text": "Three additional questionnaires specifically about the focal child were also distributed for teachers in reading, math, and science. Each reading and math or science teacher received a teacher questionnaire in addition to at least one child-level questionnaire in reading, mathematics, or science. All students were assigned to have their reading teacher complete questionnaires. To reduce respondent burden for teachers, half of students were randomly assigned to have a mathematics teacher complete questionnaires, and half of students were randomly assigned to have a science teacher complete questionnaires. In some schools, the sampled children were taught reading, mathematics, and science by the same person in one classroom. In other schools, different teachers taught these subjects to the sampled children. During the fifth grade data collection, 53 percent of the students were taught in self-contained classrooms, 24 percent received departmentalized instruction, 18 percent were team taught, and 5 percent were pulled out of class for instruction. For more information about how questionnaires were distributed, see section 4.5.5. The reading teacher questionnaire had three different parts. The first part included questions from the Social Rating Scale (SRS) that collected data on five areas of children's social skills. The second part had questions from the Academic Rating Scale (ARS) and gathered data on each sampled child's skills in areas of language and literacy. The third part asked child-specific instructional information (for example, child's grade, additional tutoring or services the child received), asked the teacher to tell how this child behaved and performed in language and literacy relative to the other children in the class, and asked about the teacher's classroom and the characteristics of the students, instructional activities and curricular focus, and instructional practices in language arts. The mathematics teacher questionnaire included questions from the ARS gathering data on each sampled child's skills in mathematics, asked child-specific specific instructional information (for example, child's grade, additional tutoring or services the child received), asked the teacher to tell how this child behaved and performed in mathematics class relative to the other children in the class, and asked about the teacher's classroom and the characteristics of the students, instructional activities and curricular focus, and instructional practices in mathematics. The science teacher questionnaire was similar to the mathematics teacher questionnaire with the questions focusing on science rather than mathematics. Teachers responded to two of these questionnaires for each sampled child. Therefore, data were gathered on each sampled child's skills in the areas of language and literacy and mathematical thinking, or in the areas of language and literacy and science. For more information on data collection with teachers, see section 4.5.5."}, {"section_title": "2-11", "text": "In addition to the teacher questionnaires described above, the ECLS-K also included special education teacher questionnaires. These were similar to the ones given to special education teachers in previous rounds and had two parts, A and B. Part A of the special education teacher questionnaire was designed to collect information about the special education teacher's professional background and experience. Part B asked about the special education services provided to the child and the nature of the child's special education curriculum. Except for one change, the spring-fifth grade special education teacher questionnaires were identical to the ones used in spring-third grade. A question on the receipt of special education or related services due to an attention deficit/hyperactivity disorder (ADHD) was added to Part B in the spring-fifth grade questionnaire."}, {"section_title": "Modifications to School Administrator Questionnaire", "text": "The principal, administrator, or headmaster at the school attended by the sampled child was asked to complete the school administrator questionnaire in the spring of 2004. As in previous rounds of the study, this self-administered questionnaire was intended to gather information about the school, student body, teachers, school policies, and administrator characteristics. Two main changes were made to the questionnaire in spring-fifth grade. First, it included items that in third grade had been in a questionnaire called the school fact sheet (e.g., the grades taught in the school, school sector and focus, the length of the school year). Also, a new content area about student food consumption was added. The main purpose of these questions was to determine the availability at school of various foods, including those that are healthy and those that are high in fat, sodium, and/or added sugars. Questions were asked about whether students could purchase food or beverages from vending machines at the school or a school store, canteen, or snack bar. School administrators were also asked if the school offered a la carte lunch or breakfast items to students that were not sold as part of the National School Lunch or the School Breakfast Program. In addition, questions were asked about whether children could buy particular foods and beverages at school, such as milk, sweetened beverages (e.g., soft drinks), fruits and vegetables, candy, and salty snacks; where these foods could be obtained in the school (e.g., a school store, a vending machine); and how full the cafeteria was at peak meal times. Questions on the availability of foods not part of USDA meal programs and cafeteria crowding were taken from SHPPS. The sources for the other food consumption questions in the school administrator questionnaire are the same as those described in section 2.4 about the children's food consumption questions."}, {"section_title": "2-12", "text": "This page is intentionally left blank. 3-1"}, {"section_title": "SAMPLE DESIGN AND IMPLEMENTATION", "text": "This chapter describes the sample design of the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), and how it was modified and implemented for each round of data collection. An overview of the sample design is given here and described in more detail in the following sections. The ECLS-K employed a multistage probability sample design to select a nationally representative sample of children attending kindergarten in 1998-99. In the base year the primary sampling units (PSUs) were geographic areas consisting of counties or groups of counties. The secondstage units were schools within sampled PSUs. The third-and final-stage units were students within schools. During the base year, data were collected in both the fall and the spring. Base year respondents were eligible for the first-grade data collection and nonrespondents were not eligible. A case was considered responding for the base year if there was a completed child assessment or parent interview in fall-or spring-kindergarten. A child with a disability who could not be assessed was also considered a base year respondent whether or not this child had a complete parent interview. Background characteristics such as sex, race/ethnicity, age, height, and weight are available for children with disabilities who could not be assessed. While all base year respondents were eligible for the spring-first grade data collection, fall-first grade was limited to a 30 percent subsample. The spring-first grade student sample was freshened to include current first-graders who had not been enrolled in kindergarten in 1998-99 and, therefore, had had no chance of being included in the ECLS-K base year kindergarten sample. For both fall-and spring-first grade, approximately 50 percent of sampled students who had transferred from their kindergarten schools were followed. The third-grade data collection included base year respondents and children sampled in first grade through a freshening operation in which the spring-first grade sample was freshened to include first-graders who had not been enrolled in kindergarten in 1998-99 and therefore had no chance of being included in the ECLS-K base year kindergarten sample. As in the first-grade data collection, where only a subsample of students who had transferred from their kindergarten schools was followed, subsampling of the movers was also used in third grade. In third grade, however, the subsampling rate applied to movers was slightly higher: children whose home language was non-English (also known as children belonging to the language minority group) and who had transferred for the first time between kindergarten or first 3-2 grade to third grade were followed with certainty. In other words, 100 percent of the children belonging to the language minority group who had not moved between kindergarten and first grade but had moved between first grade and third grade were followed into their new third-grade schools. Language minority children who had moved between kindergarten and first grade and were not subsampled for followup in first grade did not re-enter the third-grade sample; those who were subsampled for followup in first grade were followed with certainty into their third-grade schools if they had moved again between first grade and third grade. The higher subsampling rate allowed for the preservation of this group in the sample for analytic reasons. Children not in the language minority group continued to be subsampled for followup at a 50 percent rate if they had moved out of the original sample schools. In fifth grade, the sample that was fielded was reduced by excluding certain special groups of children from data collection, and by setting differential sampling rates for movers in different categories. Specifically, children in four groups were not fielded for the fifth-grade survey, irrespective of other subsampling procedures that were implemented. These were children who had become ineligible in an earlier round because they had died or moved out of the country, children who were subsampled out in previous rounds because they were movers, children whose parents emphatically refused to cooperate (hard refusals), and children eligible for the third-grade data collection for whom there are neither firstgrade nor third-grade data. Of the remaining children, those who had moved from their original schools during fifth grade or earlier were subsampled for followup. Children whose home language was not English (language minority) continued to be a special domain of analytic interest, and were subsampled at higher rates. Children were subsampled at different rates depending on the longitudinal data available for those children. The precision requirements and achieved sample sizes for the different waves of data collection are discussed in section 3.1. The base year, fall-first grade, spring-first grade, and spring-third samples are discussed in section 3.2, 3.3, 3.4 and 3.5, respectively. Sampling issues that were considered prior to the fifth-grade data collection are discussed in section 3.6. Section 3.6.3 includes a discussion of the characteristics of the fifth-grade sample and those of the children excluded from the fifth-grade data collection. 3-3"}, {"section_title": "Precision Requirements and Achieved Sample Sizes", "text": "The ECLS-K is a nationally representative longitudinal survey of children who attended kindergarten in 1998-1999, supplemented with children who were in first grade in spring 2000, but were not in kindergarten the previous year. Data on these children were collected from a variety of sources at two points in the base year (kindergarten in 1998-1999), two points in the 1999-2000 school year (as noted earlier, the fall collection was limited to a subsample of children) when most of the children were in first grade, in spring of 2002 when most of the children were in third grade, and again in spring of 2004 when most of the children were in fifth grade. The overall design for the survey evolved over time. The initial design study recommended sampling 23,500 children in approximately 1,000 kindergarten programs sampled from 100 PSUs. The initial plans also called for sampling children in private schools at a higher rate than children in public schools, as well as sampling minorities (children of Black, Hispanic, or Asian or Pacific Island [API] race or ethnicity) at higher rates than nonminorities. The design study assumed that because of nonresponse and losses due to children moving, the final number of completed interviews at the end of the survey would be about 10,300. While the design study was useful in providing overall direction, the final framework for the sample design differed in many ways from its recommendations. The sample design implemented through the fifth grade in the ECLS-K is described in this chapter. The remainder of this section gives an overview of the sampling objectives and how the design was revised to accommodate changes in those objectives over the course of the study. Subsequent sections of the chapter give the details of the procedures used to implement the sample in the various rounds or waves of data collection, beginning with the base year in 1998-1999. Four precision requirements for the survey were identified and formed the basis for the base year sample design and plans for the followups in subsequent rounds. These requirements are the ability to do the following: Measure a relative change of 20 percent in proportions across waves; Measure a relative change of 5 percent in a mean assessment score across waves; 3-4 Estimate a proportion for each wave with a coefficient of variation (CV) of 10 percent or less; and Estimate a mean assessment score for each wave with a CV of 2.5 percent or less. The goals were interpreted as being objectives not only for all children, but for subgroups of analytic interest that include children attending public and private schools (Catholic, non-Catholic), and children from different race and ethnic groups (Hispanic, Black, Asian and Pacific Islander, all other races). After the spring-first grade data collection, language minority children were a newly identified subgroup of analytic interest for sample design purposes. A large number of assumptions had to be made to estimate sample sizes sufficient to meet the precision requirements. The key assumptions included projections of the losses due to nonresponse and attrition due to children moving, the design effects 1 associated with the sample design, the element mean and standard deviations of the assessment scores, and the correlation of the statistics across waves. Since the ECLS-K is the first study of this population using this methodology, many of the assumptions had to be based on judgments without much supporting empirical data. The precision requirements that drive the sample design (those demanding the largest sample size) have to do with estimating changes over time and estimating the precision of estimates in the fifthgrade data collection. Based on assumptions described above, it was determined that a sample in fifth grade of about 10,000 children would be adequate to meet the precision requirements overall and for most subgroups. A sample of about 800 to 1,000 children in a subgroup would be achieved for most of the subgroups with an overall sample of 10,000 children and these would approximately meet the precision goals. For example, with a sample size of 10,000, the number of Hispanic and Black children would exceed 1,000, as shown in section 3.6.3. Children in private schools and APIs were the two subgroups that were expected to fall short of the goals if higher sampling rates were not applied. As noted in the following sections, sampling procedures were implemented to increase the sample size for these two groups. After the spring-first grade data collection was completed, the assumptions were reviewed and the ability of the sample to meet the survey goals was re-examined. At that time, language minority children were included as a subgroup of analytic interest. The evaluation showed that the sample sizes were adequate for most subgroups, but special efforts were needed to retain language minority children in subsequent rounds. Table 3-11 in section 3.5 shows the outcome of the spring-first grade data collection 3-5 by type of children. Since funding was made available to support these efforts, sampling procedures for retaining movers were modified. In the first-grade data collection, half of the movers were subsampled and included for followup, without taking any characteristics of the children into account. To increase the sample of language minority children, the sampling procedures were revised for the third-grade followup to retain as many of these children as possible. The evaluation also showed that the assumed design effects for assessment scores (reading, math and general knowledge) were larger than originally expected, ranging from 4.5 to 9.5. The larger than expected design effects for scores were first identified after the base year. The design effects for percentages, ranging from 1.6 to 6.9 for proportions greater than 30 percent, were close to those originally anticipated (3.8 on average). 2 The evaluation showed that the correlation over time of the scores was higher than expected. The higher correlation makes estimates of change in scores over time more precise. Consequently, the only precision objective that is substantially affected by the higher than expected design effects is for the mean assessment scores for fifth grade. This partially offsets the loss in precision due to the higher design effect. Table 3-1 tracks the ECLS-K sample from the base year through fifth grade. The table shows that the large initial sample of children has been reduced over time due to subsampling movers and nonresponse, as expected. While the initial assumptions that drove the sample design were not always accurate separately, the overall effect of the losses has been very close to what was expected. For example, in several rounds of the ECLS-K, the assumed moving rate was lower than the actual moving rate, but this was offset by higher completion rates. The overall number of eligible children at the end of the fifth-grade wave was more than 12,000 children, and the final sample size for the fifth-grade sample exceeded the 10,000 children in the initial projections. Table 3-1. ECLS-K sample size from the base year through fifth grade: School years 1998-99, 1999-2000, 2001-02, and 2003- Only 30 percent of base year schools were included in the fall-first grade sample. 2 Including 1,426 students from refusal converted schools and excluding 41 students in schools that cooperated in Fall-Kindergarten and refused in Spring-Kindergarten. 3 Only students who have at least one of the four base year data points (fall-kindergarten assessment or parent data, or spring-kindergarten assessment or parent data, and the 165 students sampled in first grade through sample freshening. 4 Excluding students described in section 3.6.1. 5 Child-complete if the child had assessment data or was not assessed due to a disability. 6 Parent-complete if the child had parent interview data. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, spring 2000, spring 2002, and spring 2004. The details on the sample sizes for subgroups at the end of the fifth grade are provided later in this chapter (see tables 3-20, 3-21, and 3-22). Those tabulations show that the number of fifth-grade respondents for all of the specific subgroups of interest exceeds 1,000, except for children in non-Catholic private schools and API children. For most of the key analytic groups the numbers of respondents are much larger than 1,000. For API children, the number of respondents is 970, which exceeds the minimum target of 800 and is very close to 1,000. The number of respondents in non-Catholic private schools is 957."}, {"section_title": "Base Year Sample", "text": "In the base year, the ECLS-K selected a nationally representative sample of children attending kindergarten in 1998-99, using a dual-frame multistage probability sample design. Counties and groups of counties constituted the first-stage sampling units or PSUs, schools or kindergarten programs within PSUs were the second-stage units, and children were the third-and final-stage units. 3-7"}, {"section_title": "Selecting the Area Sample", "text": "The point of departure for the ECLS-K area sample frame development was an existing multipurpose frame of PSUs created, using 1990 county-level population data and 1988 per capita income data from the U.S. Department of Commerce, Bureau of Economic Analysis. This frame contained 1,404 PSUs that were counties or groups of contiguous counties. PSUs did not cut across census regional 3 boundaries, but were allowed to cross state boundaries. Each 1990 metropolitan statistical area (MSA) 4 constituted a single PSU except where an MSA crossed census regions, and it was split into two PSUs. The minimum size of a PSU in the multipurpose frame was 15,000 persons. Since the focus of the ECLS-K is kindergarten students, the existing PSU frame was updated with 1994 population estimates of 5-year-olds by race/ethnicity, the most up-to-date estimates available from the U.S. Bureau of the Census at the time. The counts of 5-year-olds by race/ethnicity were used to revise PSU definitions relative to a different minimum PSU size and to construct a measure of size (MOS) that facilitated the oversampling of APIs. Each PSU in the frame that did not have at least 320 5-year-olds was collapsed with an adjacent PSU. This minimum PSU size was developed based on assumptions concerning anticipated school response rates, the average number of schools that would be selected per PSU, and the target number of students to be sampled per school. After this collapsing, the final ECLS-K PSU frame contained 1,335 records. The MOS used for selecting PSUs took into account the amount of oversampling of APIs required to meet the ECLS-K precision goals. The weighted MOS was calculated as follows: where 2.5 is the oversampling rate for APIs, and n API and n other are the counts of 5-year-old APIs and all others, respectively. The oversampling rate for APIs was calculated as the target number of completed API cases divided by the expected number of completed API cases without oversampling. In all, 100 PSUs were selected for the ECLS-K. The 24 PSUs with the largest measures of size were designated as certainty selections or self-representing (SR) 5 and were set aside. They were included in the sample with certainty. Once the self-representing PSUs were removed, the remaining PSUs, called non-self-3-8 representing (non-SR) 6 , were partitioned into 38 strata of roughly equal MOS. The frame of non-selfrepresenting PSUs was first sorted into eight superstrata by crossing the two MSA categories (MSA and non-MSA) and the four census region (Northeast, Midwest, South, and West). Within the four MSA superstrata, the variables used for further stratification were race/ethnicity (high concentration of API, Black, or Hispanic), size class (MOS \u2265 13,000 and MOS < 13,000), and 1988 per capita income range (shown in table 3-2, each range was defined so as to have roughly equal population in each of the stratum, where applicable) Within the four non-MSA superstrata, the stratification variables were race/ethnicity and per capita income. The term \"superstrata\" is used here to distinguish between the larger strata created by crossing MSA categories and census regions and the smaller strata defined by race/ethnicity, size class and per capita income. Table 3-2 describes how the 38 non-self-representing strata were created. Two PSUs were selected from each non-self-representing stratum using Durbin's Method (Durbin 1967). This method selects two first-stage units per stratum without replacement, with probability proportional to size and a known joint probability of inclusion. The Durbin method was used because it has statistical properties that make it easier to compute variances. Table 3-3 summarizes the characteristics of the ECLS-K PSU sample. The Durbin method required two passes of the frame with a different selection probability at each pass to obtain the desired probabilities of inclusion and joint probabilities of inclusion. In the first pass, one PSU was selected in the stratum with probability p 1 . In the second pass, the selected PSU was excluded and another PSU was selected with probability proportional to where p 1 = M 1 /M and p 2 = M 2 /M, M 1 is the MOS of the first unit selected, M 2 the MOS of the second unit selected, and M the MOS of the stratum. The overall selection probability of non-self-representing unit i is 3-9  1 MSA is a geographic entity designated as one or more counties in a metropolitan area, except in New England, where MSA is defined in terms of county subdivisions. Non-MSA designates one or more counties not in a metropolitan area. MSA and non-MSA are as defined by the Bureau of the Census. 2 A census region is a geographic region defined by the U.S. Bureau of the Census. 3 Primary sampling unit. NOTE: In this table, \"Any\" means any value of the column variable. For example, stratum 1 includes PSUs that have MSA status, are located in the Northeast region, with a MOS greater than or equal to 13,000 and per capita income ranging between $22,062 and $25,424, and can have any value of the race/ethnicity percentage. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998 and spring 1999. The joint probability of inclusion of the first and second units is "}, {"section_title": "Selecting the School Sample", "text": "In the second stage of sampling, public and private schools offering kindergarten programs were selected. For each ECLS-K PSU, a frame of public and private schools offering kindergarten programs was constructed, using existing school universe files: the 1995-96 Common Core of Data (CCD) (U.S. Department of Education 1995-96) and the 1995-96 Private School Universe Survey (PSS) (Broughman and Colaciello 1998). The school frame was freshened in the spring of 1998 to include newly opened schools that were not included in the CCD and PSS and schools that were in the CCD and PSS but did not offer kindergarten, according to those sources. A school sample supplement was selected from the supplemental frame."}, {"section_title": "School Frame Construction", "text": "The 1995-96 CCD Public School Universe File was the primary source for the ECLS-K public school sampling frame. Most schools run by the U.S. Department of the Interior, Bureau of Indian Affairs (BIA) and the schools run by the U.S. Department of Defense (DOD) were not included on the 1995-96 CCD. The 1995-96 Office of Indian Education Programs Education Directory (U.S. Department of the Interior, Bureau of Indian Affairs unpublished document) was consulted, in order to complete the list of BIA schools in the CCD file. For the DOD schools, a 1996 list of schools obtained directly from the DOD was used. The 1995-96 PSS Universe File was used as the primary source of the private school sampling frame."}, {"section_title": "3-12", "text": "The first step in frame construction involved subsetting the file to schools located in counties that constituted the ECLS-K PSU sample. Further subsetting retained only those schools that offered transitional kindergarten, kindergarten, or transitional first grade, or which were strictly ungraded, as indicated by the school's grade span. The constructed ECLS-K school frame included 18,911 public-school records and 12,412 private-school records. This frame constituted the original frame. The original frame was supplemented in the spring of 1998 to include schools that would be operational in fall 1998 but had not been included in the original frame. The procedures used to construct the supplemental or freshened frame are given later in this section. Table 3-4 gives the estimated number of schools offering kindergarten programs and the number of kindergarten students from the ECLS-K school frame. These are the numbers of schools and students in the sampled PSUs in the frame weighted by the inverse of the PSU selection probabilities. "}, {"section_title": "School Measure of Size", "text": "Within each PSU, schools with fewer than a predetermined minimum number of kindergarten students were clustered together before sampling in order to obtain a sample that is closer to self-weighting. The minimum number of kindergartners was 24 for public schools and 12 for private schools. Schools were selected with probability proportional to size. As with the PSU sample, a weighted MOS was constructed taking into account the oversampling of APIs: where 2.5 is the oversampling rate for APIs, and n API,ij and n other,ij are the counts of API kindergarten students and all other kindergarten students, respectively, in school j of PSU i."}, {"section_title": "School Allocation", "text": "Schools were sampled at rates designed to result in an approximately self-weighting sample of students within public and private school strata. The target number of sampled schools per PSU was calculated separately for public schools and private schools, and for self-representing and non-selfrepresenting PSUs. The number of schools selected was the target number of schools adjusted upward by the estimated school response and eligibility rate."}, {"section_title": "Public Schools", "text": "The total MOS for public schools was partitioned into the self-representing and non-selfrepresenting strata. There are 100 PSUs in the ECLS-K sample, of which 24 are in the self-representing strata. The number of public schools selected from the self-representing strata was calculated as 24 1 100 where n is the total number of public schools to be selected, w i is the weight of PSU i, and The value for n is 800/.85 = 941 where .85 is the expected eligibility and response rate for public schools. The supplement of public schools was expected to add relatively few schools to the frame and thus the 85 percent rate was not modified. The distribution of sampled schools was approximately 291 for self-representing strata and 650 for non-self-representing strata. For self-representing and non-3-14 self-representing strata alike, the number of schools allocated to each PSU was proportional to the weighted MOS of the PSU (w i \u00d7PSUMOS i ). In the ECLS-K public school frame, 4 percent of public schools had fewer than 24 kindergarten students. These schools were combined with other schools in the same PSU to form clusters with at least 24 students prior to sampling. Schools with 24 students or more were not grouped, but were also referred to as clusters (of one school each). To sample approximately 941 public schools, around 915 clusters (single schools or groups of schools) have to be selected. As a general rule, if a sampled school or cluster of schools had 24 or more students, 24 students were selected. However, for practical reasons, all students in the sampled school or cluster were selected if there were fewer than 27 students. More details on the clustering of schools are found in the next section. The number of clusters was allocated to each PSU proportionally to the weighted MOS of the PSU (w i \u00d7PSUMOS i ). When the 915 clusters were allocated to PSUs, it was discovered that in 5 PSUs there were not enough clusters in the frame to select the required number of clusters. As a result, only 900 clusters were selected. Table 3-5 shows the expected distributions of clusters, schools, and students. \nPublic schools with fewer than 24 kindergarten students were clustered. Within each PSU, the list of small schools (i.e., schools with fewer than 24 kindergartners) was sorted in ascending order of kindergarten enrollment; it was then split in half, with the second half re-sorted in descending order. The two halves were then put together in an interleaving fashion. Beginning at the top of the list, clusters of 3-17 schools with at least 24 kindergarten students were formed. If the last cluster on the list still did not have the required 24 minimum, then it was put together with the next-to-last cluster on the list. This clustering scheme resulted in 18 clusters with 5 or more schools, which were considered problematic as far as fieldwork was concerned. The worst case was one cluster with 13 schools and only 41 students. In order to minimize the number of clusters having 5 or more schools, each problematic cluster was broken into groups of 2 or 3 schools, and each group was combined with the smallest of the \"large\" schools having 25 or more kindergarten students. Since enrollment in schools with missing kindergarten enrollment was imputed to be equal to 24, grouping any of these imputed schools with another school was avoided, lest they turn out not to have kindergarten students. In addition to the 18 problematic clusters above, there were 12 PSUs with only 1 small school (with fewer than 24 kindergarten students) and there were 2 PSUs with only 2 small schools that, when grouped together, still had fewer than 24 kindergarten students. These small schools or groups of small schools were manually combined with the smallest school in another PSU (not one with only 1 or 2 schools) having 25 or more students (see table 3-7). \nWithin each PSU, the clusters were sorted by the MOS and separated into three size classes of roughly equal size (high, medium, and low). Within each size class, clusters were sorted by the proportion of APIs in a serpentine manner (alternating the sort order from one size class to the next).\nEach public school district having one or more schools sampled was sent a sampling framebased list of all schools offering kindergarten. Districts were asked whether any school that was expected to offer kindergarten in academic year 1998-1999 was missing from the list. For each school identified by the district, school name, address, telephone number, grade span, and kindergarten enrollment were obtained. Districts were also contacted that fell within the boundaries of the ECLS-K PSUs, but for which the CCD file listed no schools offering kindergarten, unless it was clear from their name that they were strictly secondary school districts (e.g., Middlebury Union High School District). The information obtained from the school districts was checked against the ECLS-K public school frame to confirm that these schools were truly new or newly eligible. Bona fide new schools were given a chance of being sampled. A new school's chance of selection was conditioned on the school district's probability of selection. Overall, 252 new public schools were identified. Of these, 19 were selected, using systematic sampling with probability proportional to size where the MOS was the same as it was for schools sampled from the main sample. Thus, a total of 953 public schools were included in the sample (934 + 19). The supplemental frame contained 11,405 private schools. A sample of 279 schools was selected, using systematic sampling with a probability proportional to these imputed enrollments. Each sampled school was contacted by telephone and screened to ascertain whether the school was public or private, whether it would be open in academic year 1998-1999; and whether it would offer kindergarten."}, {"section_title": "Private Schools", "text": "The procedure used to determine the allocation of the public schools was also used for allocating the private schools. The private school target samples are labeled n SR \u2032 and n NSR \u2032 for selfrepresenting and non-self-representing PSUs respectively, and n\u2032 is the sum of n SR \u2032 and n NSR \u2032. The value of n\u2032 is 200/.60=333, where .60 is the expected eligibility and response rate. The supplement to the frame 3-15 was expected to add some private schools with kindergarten programs. The 60 percent rate was used because of the uncertainties associated with the estimate of the eligibility and response rate for private schools. The percentage of schools with fewer than 24 kindergarten students was large for private schools. Approximately 56 percent of private schools offered a kindergarten program that had fewer than 24 students, and 44 percent of these small schools have fewer than 12 students in their kindergarten program. Schools having fewer than 12 kindergarten students (according to the frame) were grouped into clusters of schools with at least 12 students in each cluster, following the clustering rules discussed in the next section. Schools with 12 students or more were not grouped. As a general rule, if a sampled school or cluster of schools had 24 or more students, 24 students were selected; if a sampled school or cluster had fewer than 24, all students were sampled. However, for practical reasons, all students in the sampled school or cluster were selected if there were fewer than 27 students. In order to sample approximately 333 private schools, 278 clusters were selected (single schools or groups of schools). Table 3-6 shows the expected distributions of clusters, schools, and students. The number of clusters was not allocated separately to each self-representing PSU, since sampling was done on the aggregated list of school clusters in the self-representing PSUs. This aggregated list of school clusters in the self-representing PSUs had been sorted prior to sampling by religious affiliation in order to have better control of the sample distribution by religious affiliation. For the non-self-representing PSUs, the sample was allocated to each PSU proportionally to the weighted MOS of the PSU (w i \u00d7PSUMOS i ), with a minimum of one cluster per PSU imposed if the PSU was so small that it was not allocated any clusters. \nPrivate schools with fewer than 12 kindergarten students were clustered. Within each PSU, the list of private schools was first sorted by religious and nonreligious affiliation. If the number of religious schools and nonreligious schools in the PSU differed by no more than a factor of 3, the smaller of the two lists (religious or nonreligious) was sorted in descending order while the larger of the two lists\nWithin each PSU, each cluster was identified as religious, mixed, or nonreligious. 7 The list of clusters was then sorted by these three categories. Within each category, the clusters were sorted in a serpentine manner by the MOS prior to selection. However, for the self-representing PSUs, all clusters were sorted as if they were from the same PSU, i.e., the aggregated list of clusters from the 24 selfrepresenting PSUs was sorted by religious affiliation (religious/mixed/nonreligious). This procedure provided better control of the sample distribution of religious/mixed/nonreligious clusters. Across nonself-representing PSUs, clusters were sorted by religious affiliation, and within each category of religious affiliation, by the MOS in a serpentine manner."}, {"section_title": "3-16", "text": ""}, {"section_title": "Clustering of Small Schools", "text": "As noted above, schools with fewer than 24 students (public) or 12 students (private) were clustered together in order to obtain a sample that was closer to self-weighting. For example, if a school with 12 students was not clustered, the students from that school would be sampled at about half the probability as students in larger schools. The goal of the clustering of small schools was to form school clusters with a small number of schools, each cluster having close to 24 students and including heterogeneous schools. This goal was set so that if a cluster was selected, it would not be necessary to recruit many small schools; furthermore, the heterogeneity of schools improves the reliability of the estimates. Heterogeneity was defined by school size for public schools, and by religious affiliation and school size for private schools. Within each PSU, schools with fewer than a predetermined minimum number of kindergarten students were separated from the frame and clustered together. A few exceptions to this general rule did occur and are discussed later. The procedures for clustering of schools are described below."}, {"section_title": "3-18", "text": "was sorted in ascending order of kindergarten enrollment. The two lists were then put together in an interleaving fashion, so that the records that were at the bottom of the longer list were records with larger kindergarten enrollment, and did not have to be grouped together. Beginning at the top of the entire list, clusters of schools of at least 12 kindergarten students were formed. If the last cluster on the list still did not have the required minimum size, it was put together with the next-to-last cluster on the list. If the number of religious schools and nonreligious schools in the PSU differed by a factor greater than 3, schools were not separated into religious and nonreligious lists. Instead, the entire list of schools was sorted in ascending order of kindergarten enrollment; it was then split in half, with the second half re-sorted in descending order. The two halves were then put together in an interleaving fashion. Clusters of schools were formed as above. There were 3 PSUs where the clustering of small schools as specified above did not work well. Two of the 3 PSUs had only 1 small school each and the third one had 2 small schools that, when grouped together, still had fewer than 12 kindergarten students. These small schools or groups of small schools were manually combined with other large schools in another PSU (table 3-8). "}, {"section_title": "Implicit Stratification of Schools/Clusters of Schools", "text": "Public schools with more than 24 kindergarten students and private schools with more than 12 kindergarten students were not clustered. However, they are referred to as clusters (of one school each) for simplicity."}, {"section_title": "3-19", "text": ""}, {"section_title": "School Selection", "text": "Selection of the clusters of schools was systematic, with probability proportional to the MOS. Sampling of public schools was done independently within PSU (i.e., each PSU forms a separate sampling stratum) after the clusters of schools were sorted by MOS and proportion of API. Sampling of private schools was done separately for self-representing PSUs and for non-self-representing PSUs. All self-representing PSUs were placed in one sampling stratum and all non-self-representing PSUs were placed in a second stratum. In the self-representing stratum, sampling was done with one random start after sorting clusters of schools by religious affiliation and MOS. In the non-self-representing stratum, sampling was done with one random start after sorting clusters of schools by PSU, religious affiliation, and MOS. 7 A private school cluster is \"religious\" if all schools in the cluster are Catholic schools or non-Catholic religious schools; \"nonreligious\" if all schools in the clusters have no religious affiliation; \"mixed\" if it has a combination of schools with or without religious affiliation. 3-20"}, {"section_title": "The ECLS-K Main School Sample", "text": "A total of 1,280 schools were selected from the main school frame for the ECLS-K, of which 934 were public and were private schools. The characteristics of the school sample are presented in table 3-9."}, {"section_title": "Supplemental School Sample", "text": "As mentioned earlier, the public and private school frames were supplemented in the spring of 1998. The procedures for supplementing the frames were different for public schools, Catholic schools and non-Catholic private schools. These procedures are discussed below separately."}, {"section_title": "3-21", "text": ""}, {"section_title": "3-23", "text": "If the school met all of these conditions and was not Catholic, the school was eligible and released for data collection. A second supplemental procedure involved contacting local education agencies (LEAs) and local government offices for information on non-Catholic private schools. This procedure was done only in the smallest ECLS-K PSUs, on the theory that if these PSUs had coverage problems their large weights were likely to introduce a larger bias in the estimates. All LEAs within these PSUs were contacted by telephone. For each city/town within the PSU, a list of local government offices was compiled using the blue pages. Successive government offices were called within a city or town until one was found that could provide information on private schools. As with the yellow pages, new schools identified by LEAs and local government offices were unduplicated against the PSS file before being added to the new school frame. Since kindergarten enrollment was unknown, it was imputed as described in the previous paragraph and sampling was performed using systematic sampling with probability proportional to size. The LEA search resulted in the identification of 30 new private schools after unduplication, of which 14 were sampled. The local government search yielded 19 new schools, of which 8 were sampled. Finally, three additional new private schools were reported by field staff based on personal knowledge. Of these, two schools were sampled. The same screening procedures to ascertain whether the school was public or private; whether it would be open in academic year 1998-1999; and whether it would offer kindergarten were then applied to these sampled schools. The total number of non-Catholic private schools that were sampled was 303. After the screening procedures were applied, only 109 of these schools were eligible. These 109 schools are referred to as the supplemental sample of non-Catholic private schools. The goal of the student sample design was to obtain an approximately self-weighting sample of students to the extent possible while achieving the minimum required sample size for APIs (the only subgroup that needed to be oversampled to meet the study's precision goals). Two independent sampling strata were formed within each school, one containing API students and the second all other students. Within each stratum, students were selected using equal probability systematic sampling, using a higher rate for the API stratum. In general, the target number of children sampled at any one school was 24. The actual sample size per school ranged from 1 to 28. If one twin was selected into the sample then both twins were included, raising the maximum number of children to sample from 24 to 28 in a small number of schools. Once the sampled children were identified, parent contact information was obtained from the school and was used to identify a parent or guardian for the parent interview. During the fall-kindergarten data collection, a census of kindergarten teachers was taken at each school. In spring-kindergarten, new teachers who had joined the schools and teachers in schools participating after the fall were added to the census of teachers. In the spring-first and spring-third grade data collections, the only teachers included were the teachers of the sampled children. For every data collection, each sampled child was linked to his or her teacher. A child could be linked to only one general education teacher. In cases where a child had more than one general education teacher, a 'primary' teacher was identified for the child. In addition, special education teachers and service providers were linked to sample cases who received such services. As with the general education teachers, a child would be linked to only one special education teacher or service provider. Details on the linking of teachers to the children are found in chapter 4."}, {"section_title": "Fall-First Grade Subsample", "text": "The fall data collection consisted of a 30 percent sample of schools containing approximately 25 percent of the base year students eligible to participate in the second year. The goal of this subsample was to measure the extent of summer learning loss and the factors that contribute to such loss and to better disentangle school and home effects on children's learning 3-25"}, {"section_title": "PSU Sample", "text": "A subsample of ECLS-K PSUs was selected for the fall-first grade data collection. All 24 of the self-representing PSUs were retained. Of the 76 non-self-representing PSUs, 38 were retained by sampling one PSU per stratum with equal probability."}, {"section_title": "School Sample", "text": "Base year schools in the 62 fall-first grade sampled PSUs were stratified by frame source (original public, original private, supplemental public, and supplemental private as described in section 3.2.2.8) and arranged in their original selection order. A 30 percent equal probability sample of schools was drawn in the 24 self-representing PSUs and a 60 percent sample of schools was drawn in the 38 non-self-representing PSUs. In total 311 schools that had cooperated in either fall-or springkindergarten were selected. The characteristics of the base year cooperating schools selected for fall-first grade are presented in table 3-10."}, {"section_title": "Child Sample", "text": "Fall-first grade data collection consisted of the direct child assessment and the parent interview. Data collection was attempted for every eligible child found still attending the school in which he or she had been sampled during kindergarten. \"Eligible\" was defined as a base year respondent (i.e., a child who had either a fall-or spring-kindergarten child assessment or parent interview or was excluded from assessment because of a disability or because the child belonged in the language minority (not Spanish) group. Base year nonrespondents were not sampled and were handled by adjusting the weights (see section 7.2.1.2.1 for details of adjustment for base year nonresponse). Because of the additional burden of school recruiting, the cost of collecting data for a child who had transferred from the school in which he or she was originally sampled exceeds that for a child who stayed enrolled. To contain these costs, a random 50 percent of children were subsampled to be followed for fall-first grade data collection in the event that they had transferred. Except for children who were repeating kindergarten, all base year children sampled in schools with a high grade of kindergarten are de facto movers. Since many of these movers might move en masse to the same first-grade school, steps were taken to follow these children at a higher rate. Using the information collected during spring-kindergarten, a list of destination schools was compiled for each such school. The destination school having the most movers was designated as primary, unless no such school had more than three movers. Children who moved en masse into a primary destination school in fall-first grade were treated as \"nonmovers\" and were not subsampled, that is, they continued to be followed and were part of the ECLS-K sample. All other movers were sampled at the rate of 50 percent."}, {"section_title": "3-26", "text": ""}, {"section_title": "3-27", "text": "As discussed above, a random 50 percent of children were subsampled to be followed if they moved out of the kindergarten school. Sampling was done with equal probability. Prior to sampling, children were stratified into groups of nonmovers, movers with information identifying their new schools, and movers without such identifying information. A flag was created for each child indicating whether the child had been sampled to be followed."}, {"section_title": "Spring-First Grade Sample", "text": "The ECLS-K spring-first grade data collection targeted all base year respondents and not just the fall-first grade subsample. Hence, the sample includes children who were assessed and whose parents were interviewed in fall-or spring-kindergarten, as well as the 70 children who could not be assessed in fall-or spring-kindergarten because of a disability or because they belonged in the language minority (not Spanish) group. In addition, the spring student sample was freshened to include current first-graders who had not been enrolled in kindergarten in 1998-99 and therefore had no chance of being included in the ECLS-K base year kindergarten sample. This group includes children who skipped kindergarten altogether in 1998-99, children who attended a kindergarten program outside of the U.S. in 1998-99, and children who were in first grade in 1998-99 and repeating it in 1999-2000. While all students still enrolled in their base year schools were recontacted, only a 50 percent subsample of base year sampled students who had transferred from their kindergarten school was followed for data collection."}, {"section_title": "Subsampling Movers", "text": "In spring-first grade all children in a random 50 percent subsample of base year schools were flagged to be followed for data collection if they transferred from their base year school. (This is in contrast to fall-first grade where a random 50 percent of children in each of the 30 percent of schools subsampled were flagged.) In order to maximize the amount of longitudinal data, care was taken during spring-first grade sampling to ensure that any child who had been flagged to be followed in fall-first grade would continue to be followed. In selecting the spring-first grade 50 percent subsample of schools where movers would be flagged for followup, the three primary strata were self-representing PSUs, non-self-representing PSUs that had been selected for fall-first grade, and non-self-representing PSUs that had not been selected for 3-28 fall-first grade. Within these major strata, schools were grouped by frame source (original public, original private, supplemental public, and supplemental private). Finally within each frame source, schools were stratified by whether the school participated in the base year study, and arranged in original selection order. Schools that had been part of the 30 percent fall-first grade sample were automatically retained. Then equal probability sampling methods were employed to augment the sample to the desired 50 percent of schools. The net result of these procedures was that every base year selected school had a 50 percent chance of having its ECLS-K movers followed during spring-first grade, and any mover who had been followed in fall-first grade would still be followed in spring-first grade."}, {"section_title": "Sample Freshening", "text": "As noted earlier, a sample freshening procedure was used to make it possible to produce estimates of all children enrolled in first grade in the spring of 2000. The spring-first grade student freshening used a half-open interval sampling procedure (Kish 1965). The procedure was implemented in the same 50 percent subsample of ECLS-K base year schools where movers were flagged for followup. Each of these schools was asked to prepare an alphabetic roster of students enrolled in first grade and the names of ECLS-K kindergarten-sampled children were identified on this list. Beginning with the name of the ECLS-K first kindergarten-sampled child, school records were checked to see whether the student directly below in the sorted list attended kindergarten in the United States in fall 1998. If not, (1) that child was considered to be part of the freshened sample and was linked to the base year sampled student (i.e., was assigned that student's probability of selection), and (2) the record search procedure was repeated for the next listed child, and so forth. When the record search revealed that a child had been enrolled in kindergarten the previous year, that child was not considered part of the freshened sample and the procedure was resumed with the second base year ECLS-K sampled student name, and so on. 8 Student freshening brought 165 first graders into the ECLS-K sample, which increased the weighted survey estimate of the number of first graders in the United States by about 2.6 percent. The student freshening procedure was not entirely free of bias. A first grader would have no chance of being in the ECLS-K first grade sample if he or she was enrolled in a school where neither the child nor any of his or her classmates had attended kindergarten in the United States in fall 1998. This would be a rare circumstance and is not thought to be an important source of bias. A more significant 3-29 source of potential bias is nonresponse. One source of nonresponse inherent to the freshening plan was that the procedure only involved students who had not transferred from the school in which they had been sampled during the base year. Another source of nonresponse that also affected the freshening procedure were schools that refused to provide or could not provide the necessary information, such as alphabetic roster of students enrolled in first grade or whether students had attended kindergarten the previous year. The school freshening completion rate is slightly higher for public schools than for private schools. Of the 494 schools eligible for freshening, 380 are public schools and 114 are private schools. Ninety four percent of the public schools and 93 percent of the private schools participated in the freshening process."}, {"section_title": "Spring-Third Grade Sample", "text": "The procedures used in spring-first grade to subsample movers reduced the loss in sample size and reduced data collection costs since movers cost considerably more to interview than nonmovers. These procedures were also used for the ECLS-K third-grade data collection with some modifications. One reason for modifying the procedures was that some children had already moved out of their original school, and some of the movers were sampled and some were not. In addition, there were concerns about special domains of interest and methods that might be used to increase the sample size for the children in these groups. Results from the first-grade collection were used to address these third-grade sample design issues.  Characteristics of the schools (school affiliation and type of locale) are from the original sample schools. 2 The total number of children excludes 68 children who responded in fall-kindergarten and became ineligible in spring-kindergarten, and includes 139 children sampled in first grade who responded. 3 A respondent is a child with assessment data or parent interview data, or a child who could not be assessed due to a disability. 4 Nonrespondents include those who did not participate fully and movers who could not be located. 5 The unweighted completion rate was computed as the number of respondents divided by the sum of respondents and nonrespondents. 6 Race/ethnicity 1 was the strict definition of API (RACE=5-Asian, or 6-Native Hawaiian or other Pacific Islander), while race/ethnicity 2 was the broader definition (RACE=5-Asian, or 6-Native Hawaiian or other Pacific Islander or WKASIAN=1-Child is Asian, or WKPACISL=1-Child is Native Hawaiian or other Pacific Islander). Variables are from the ECLS-K base year data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten (ECLS-K), fall 1999 and spring 2000."}, {"section_title": "Estimates from Spring-First Grade", "text": ""}, {"section_title": "3-31", "text": "97 percent (15,357 nonmover respondents). A child was considered a respondent in these computations if either the child assessment or the parent interview was completed for spring-first grade or the child was not assessed due to a disability. The completion rate in this table was computed as the number of respondents divided by the sum of respondents and nonrespondents. Nonrespondents include those who did not participate fully and movers who could not be located. For first grade, 269 of the movers who were sampled for follow-up could not be located, or about 11 percent of all movers eligible for the first grade data collection. The mover rates show the types of variation that had been expected, with higher mover rates for Black and Hispanic children, for example. A total of 39 percent of the children in non-Catholic private schools moved to other schools when they advanced from kindergarten to first grade (855 movers as shown in table 3.11). Seventy-six percent of children who moved from kindergarten in private schools to first grade in public schools attended non-Catholic private schools in kindergarten, about three times the number of children who moved from Catholic to public schools. One of the concerns in using the kindergarten to first-grade mover rates to make estimates for future transitions was whether the mover rates for the 1-year time period between kindergarten and first grade were reasonable when applied to the transition between first and third grade. One might argue that a 2-year period should result in a higher mover rate than the 1-year rate. However, parents may be more reluctant to change the school for a child between first and third grade than between kindergarten and first grade. Kindergarten is also special for other reasons. For example, the availability of full-and part-day classes may be an important factor in the choice of the kindergarten. There are no other data sources that could be used to examine differential mover rates between years. As a result, the 1-year moving rates in table 3-11 were applied to the 2-year period between first and third grade after adding another 5 percent to the rates to account for the 2-year period. An exception was made for children who attended non-Catholic private schools in the base year and had the highest rates of moving among all the domains examined. This was assumed to be a special case for kindergarten and the average mover percentage was applied to these children for the third grade. The other main concern was whether the extremely high completion rate for nonmovers (97 percent) could be duplicated in future years. To be more conservative and to account for the fact that nonrespondents from earlier rounds (i.e., base year respondents were included in the third-grade sample whether or not they responded in first grade) were included in subsequent rounds of data collection, it was assumed that a 95 percent completion rate would be achieved for nonmovers in third grade."}, {"section_title": "3-32", "text": ""}, {"section_title": "Third-Grade Sample Design", "text": "The basic plan for third grade was the plan implemented for first grade where only 50 percent of the children who moved from the original sample schools were followed into their new schools. This plan was modified for third grade as described below. To be eligible for the third-grade sample, a child had to have been a base year respondent or sampled in first grade. Children who moved out of the country or died were excluded (i.e., ineligible). The following children were fielded for third grade: All the children responding in the base year who remained in their original schools, where the original schools also included destination schools (described later). All the children who moved from an original school in a previous wave of data collection and were retained in the subsample of movers for that wave. For example, if a child moved between kindergarten and first grade and was part of the 50 percent subsample that was followed, then the child would be retained for future rounds without subsampling as long as the child remained eligible. A subsample of 50 percent of the children who moved from their original school at any time after the base year. For example, a child who moved between first grade and third grade would be subject to subsampling and had a 50 percent chance of being included in the third-grade followup. In alternatives discussed later, differential subsampling rates were introduced. To prevent an accumulation of nonresponse, the ECLS-K design does not use the approach of many longitudinal studies that exclude sampled units from future rounds if they did not respond in a particular wave. Instead, the basic plan was modified so that all eligible base-year respondents who were sampled in the first-grade followup would be eligible for the third-grade followup even if they did not respond in the first grade. Even though the participation rate for first-grade nonrespondents might be lower compared with first-grade respondents in the subsequent followups, the effort was an attempt to increase overall response rates by including first-grade nonrespondents in third grade. The approach is also consistent with the analytic use of the data for the ECLS-K, since many analyses may include less than complete wave responses. For example, a change in scores from kindergarten to third grade for subgroups is an important analytic objective, and it can be estimated without complete data at each wave. A second procedure that was part of the modification of the basic plan for the third-grade followup was an extension of a procedure that was used in the first-grade followup to deal with schools that ended with kindergarten (i.e., kindergarten was the highest grade offered). A school was called a destination school if at least 4 students from a school ending in kindergarten attended this school in first grade. For the third grade 28 original schools ended in second grade, and 3 of the destination schools 3-33 identified in first grade ended in second grade. In total, 3 percent of all eligible first graders in the ECLS-K sample attended schools ending in second grade. As was done for the first-grade sample, children in the destination schools were treated as nonmovers for the third-grade sample. As nonmovers, they were all followed into their new schools, resulting in a 2 percent increase of the third-grade sample size over that which would result if 50 percent of these children were subsampled out as movers.   The mover rates and the completion rates for movers are those used in the computation of expected sample size, and are differential by subgroups. Since no \"total\" rates were used in the computation, they are not available. 3 The sampling rate for movers is set at 47 percent (instead of 50 percent) to account for ineligibility of students in future rounds. 4 The design effects in this column are the results of sampling movers and nonmovers differentially. They do not include the effect of clustering. After reviewing the expected yields without oversampling, it was decided to increase only the sample size for children belonging to the language minority group. Beginning in third grade, these children would not be subsampled for followup if they moved from their original school. Instead, data collection would be attempted for all language minority children. Table 3-13 is analogous to table 3-12 but is adjusted for this approach of retaining all movers in the language minority group (in practice the subsampling rates are shown as 95 percent because some children became ineligible)."}, {"section_title": "Expected Sample Size", "text": "One consequence of protecting this subgroup is to increase the sample size and precision for the subgroup. The design effect due to subsampling is slightly lower under this plan because a smaller proportion of the movers were subsampled than under the basic plan (only the movers that were already subsampled in first grade are subsampled). Another consequence is that the number of schools that the sampled children attended increased. Because all language minority children were followed , table 3-13 shows an expected increase of 395 schools in third grade (1,918 -1,523 = 395). The mover rates and the completion rates for movers are those used in the computation of expected sample size, and are differential by subgroups. Since no \"total\" rates were used in the computation, they are not available. 2 The sampling rate for movers is set at 47 percent (instead of 50 percent) to account for ineligibility of students in future rounds. If the language minority group is preserved, it is set at 95 percent. 3 The design effects in this column are the results of sampling movers and nonmovers differentially. They do not include the effect of clustering. 4 The number of new third-grade schools is estimated as 1.5 schools per sampled new mover. 3-38"}, {"section_title": "Precision Requirements", "text": "When the precision estimates were computed from the kindergarten sample at the end of the base year, higher than expected design effects for assessment scores were observed. The design effects for most other statistics, such as proportions of children with a particular characteristic, were moderate and within the range expected (1.6 to 6.9 for proportions greater than 30 percent for an average of 4.0). The design effects for assessment scores (4.5 to 9.5 for an average of 6.9) were investigated and found to be correct and unrelated to data collection artifacts. For example, interviewer effects were found to be negligible and did not bias assessment scores. The design effects for test scores were much larger than the average of 3.8 that was expected at the design stage. For all students, the design effects for math and reading scores were about 6.5, while for general knowledge the design effects were even larger, at 7.7. For design effects from the base year, see chapter 4 of the ECLS-K Base Year Public-Use Data Files and Electronic Code Book: User's Manual (NCES 2001-029r) (Tourangeau, Burke, et al. 2004). These larger design effects are one component that affects the ability of the survey to meet the precision requirements as described in section 3.1. The spring-third grade estimates of design effects are similar to those in the earlier rounds and are larger than had been predicted prior to any data collection. The longitudinal estimates have design effects that are not as large as might be expected given the larger cross-sectional design effects. In fact, the correlations for mean test scores are as high as 0.8 to 0.9. The higher than expected correlations make estimates of changes in scores over time more precise, thus it is possible to meet the precision requirements for estimates of change with smaller sample sizes. Table 3-14 shows that the sample sizes for the key analytic subgroups (public, Catholic, non-Catholic, Hispanic, Black, Asian/Pacific Islander, other races together, and language minority) were expected to be at least 1,000. 10 Samples of this size were expected to be sufficient for estimating most characteristics. For example, test scores were expected to have a coefficient of variation of about 3 percent with samples of 1,000. More details on estimates of design effects can be found in chapter 4 of the ECLS-K User's Manual for the First Grade Public-Use Data Files and Electronic Code Book (NCES 2002-135) (Tourangeau et al. 2002) and the ECLS-K User's Manual for the Third Grade Public-Use Data File and Electronic Code Book (NCES 2004-001) (Tourangeau, Brick, et al. 2004). "}, {"section_title": "3-39", "text": ""}, {"section_title": "Spring-Third Grade Sampling Outcome", "text": "To summarize, the sample of children for spring-third grade consists of all children who were base year respondents and children who were brought into the sample in spring-first grade through 3-40 the sample freshening procedure. Sample freshening was not implemented in third grade; hence no new students entered the sample. While all students still enrolled in their base year schools were recontacted, slightly more than 50 percent of the base year sampled students who had transferred from their kindergarten school were followed for data collection. This subsample of students was the same 50 percent subsample of base year movers followed in spring-first grade, including the movers whose home language was not English (language minority students). Children who were followed in spring-first grade were retained in the sample (i.e., the mover followup still targeted the same 50 percent subsample of children in the base year schools). In addition, children whose home language was not English and who had moved between spring-first grade and spring-third grade were all retained rather than being subsampled at the 50 percent rate. If they had moved before first grade, they were not to be followed. This modification to the mover followup procedure provided a larger sample of children whose home language is not English for analytic purposes. The mover followup activities that originally targeted a 50 percent subsample of children in base year schools resulted in a 54 percent subsample with the addition of language minority children. Tables 3-14 (count) and 3-15 (percent) show the characteristics of the achieved third-grade sample compared with the expected third-grade sample. The total number of children in the language minority group is virtually the same as the expected number while the total number of children in the other group is about 5 percent larger than the expected number. In computing the expected sample size, the same mover rate was assumed for both groups of children. The third-grade sample shows that the nonlanguage minority children moved at a lower rate (42 percent) than the language minority children (44 percent) resulting in a slightly larger sample of non-language minority children. The agreement between the expected and achieved sample sizes is rather remarkable given the numerous assumptions required. The actual percent distribution of third-graders within each subgroup is as expected with the exception of the Catholic and non-Catholic private schools where the percent of children in Catholic schools is higher than that of children in non-Catholic private schools. This may be due to the lower completion rate of children in non-Catholic private schools compared with children in Catholic private schools (93 percent and 97 percent, respectively). Elsewhere among the children in the language minority group, the difference between the expected distribution and the actual distribution is less than 1 percent. Elsewhere among the children not in the language minority group, the difference is less than 3 percent. The rate of base year respondents who moved out of their original sample schools is 42 percent (compared with the expected overall moving rate of 47 percent). The achieved sample size shown in table"}, {"section_title": "3-41", "text": "3-14 is a function of both the completion rate and the mover rate. Even though the actual completion rate for movers is lower than expected, the actual mover rate is also lower than expected. Fewer movers resulted in a larger sample size. Note that in all tables in this chapter a respondent is defined as a child with completed assessment data or completed parent interview data or a child who could not be assessed due to a disability, so that the completion rate calculated here is not the same as the completion rate in chapter 6 of this report or chapter 5 of the ECLS-K User's Manual for the Third Grade Public-Use Data File and Electronic Code Book (NCES 2004-001) (Tourangeau, Brick, et al. 2004) which is instrumentspecific. Characteristics of the schools (school affiliation and type of locale) are from the original sample schools. 2 The total number of children excludes 68 children who responded in fall-kindergarten and became ineligible in spring-kindergarten, and includes 165 children sampled in first grade who were eligible. 3 A respondent is a child with assessment data or parent interview data, or a child who could not be assessed due to a disability. 4 Race/ethnicity 1 was the strict definition of API (RACE=5-Asian or 6-Native Hawaiian or other Pacific Islander), while race/ethnicity 2 was the broader definition (RACE=5-Asian or 6-Native Hawaiian or other Pacific Islander or WKASIAN=1-Child is Asian or WKPACISL=1-Child is Native Hawaiian or other Pacific Islander). Variables are from the ECLS-K base year data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), spring 2002."}, {"section_title": "3-43", "text": ""}, {"section_title": "3-44", "text": ""}, {"section_title": "Spring-Fifth Grade Sample", "text": "For the fifth-grade data collection, different options for subsampling movers were explored to reduce the sample size in order to contain the cost of data collection. The original plan would use the same procedures for third grade to subsample and follow 50 percent of children who moved in fifth grade or earlier and retain all language minority children who had not been subsampled out before fifth grade. Three alternative plans were developed to decrease the sample sizes by reducing the subsampling rates. One of the alternatives was adopted as the final plan. The final subsampling rates maximize the amount of longitudinal data available for key analytic groups. A new feature of the fifth-grade sample is the subsampling of children for the administration of the mathematics or science questionnaires. While all children retained for the fifth-grade data collection had child-level questionnaires filled out by their reading teachers, half were subsampled to have child-level questionnaires filled out by their mathematics teachers and the other half had child-level questionnaires filled out by their science teachers. This affects only the computation of the combined child-parent-teacher weights as discussed in section 7.2.4."}, {"section_title": "Options for Subsampling Movers", "text": "All sampling options considered for fifth grade were based on the beginning sample of 21,357 children: 21,192 base year respondents who were still eligible after the base year, and 165 children sampled in first grade as part of the freshening procedure (see section 3.4.2). The first decision regarding the fifth-grade sample was to exclude the following groups of children from the fifth-grade survey, irrespective of other subsampling procedures that might be implemented: (1) children who had become ineligible in an earlier round (because they had died or moved out of the country); (2) children who were subsampled out in previous rounds because they had moved out of the original schools and were not subsampled to be followed; (3) children whose parents emphatically refused to cooperate (hard refusals) in any of the data collection rounds since spring-kindergarten; and (4) children eligible for the third-grade sample for whom there are neither first-grade nor third-grade data (i.e., no direct assessment data and no parent interview data from first grade and third grade). The children who met any of these conditions were not eligible for sampling in the fifth grade for any of the sampling options considered. In total, 5,214 children were excluded from the fifth-grade survey; they are distributed as shown in table 3-17. In the original plan, 50 percent of children who moved in fifth grade or earlier would be subsampled to be followed, and all language minority children who had not been subsampled out before fifth grade would be retained. This plan protects the language minority sample, as had been done in third grade."}, {"section_title": "3-45", "text": "In the first alternative plan, language minority movers would be subsampled for followup depending on the amount of data that they had from previous rounds. If they had both spring-first grade and spring-third grade data, then 50 percent would be subsampled and followed if they had only one data point after the base year, 25 percent would be subsampled and followed. Similarly, 25 percent of other movers would be subsampled and followed if they had both first-and third-grade data, and 12.5 percent would be subsampled and followed if they had only one data point besides base year data."}, {"section_title": "3-46", "text": "In the second alternative plan, only children with complete longitudinal data would be fielded, i.e., base year respondents who had first-grade and third-grade data. Of these children, 50 percent of language minority children who moved in fifth grade (or earlier) would be subsampled for followup, and 25 percent of other movers (in fifth grade or earlier) would be subsampled for followup. Children who were sampled in first grade through the sample freshening procedure would not be retained in the sample. The third and last option, adopted for the fifth-grade study, called for using rates that are approximately equal to those given below for subsampling base year respondents who are movers in fifth grade (or earlier): percent for non-language minority (LM) movers with full longitudinal data; percent for non-LM movers with third-grade but not first-grade data; percent for non-LM movers with first-grade but not third-grade data; percent for LM movers with full longitudinal data; percent for LM movers with third-grade but not first-grade data; and percent for LM movers with first-grade but not third-grade data. For subsampling freshened children (i.e., children sampled in first grade) who are movers in fifth grade (or earlier) the rates proposed were the following: percent for non-LM movers with full longitudinal data; percent for non-LM movers with third-grade but not first-grade data; percent for non-LM movers with first-grade but not third-grade data; percent for LM movers with full longitudinal data; percent for LM movers with third-grade but not first-grade data; and percent for LM movers with first-grade but not third-grade data. Table 3-18 shows the expected fifth-grade sample size separately for language minority children and by mover status for the different subsampling plans, the estimated design effect due to the subsampling of base year children, the effective sample size, and the expected number of children with a 3-47 completed fifth-grade assessment, assuming that 90 percent of children would be assessed successfully. The completion rate of 90 percent took into account children who had moved and whether they would be located. According to the third-grade collection, the unweighted completion rate for child assessment was 95 percent for nonmovers and 63 percent for movers, with an overall unweighted completion rate of 86 percent. Since fewer movers would be included in fifth grade compared with third grade, the assumption was for a slightly higher completion rate for the child assessment.   "}, {"section_title": "Sample Outcome and Precision Requirements", "text": "The fifth-grade sample of 16,143 excludes the base year respondents identified in section 3.6.1 who were not subject to data collection in the fifth grade, i.e., children who had become ineligible in 3-49 an earlier round (because they had died or moved out of the country); children who were subsampled out in previous rounds because they had moved out of the original schools and were not subsampled to be followed; children whose parents emphatically refused to cooperate (hard refusals) in any of the data collection rounds since spring-kindergarten; and children eligible for the third-grade data collection for whom there are neither first-grade nor third-grade data (i.e., no direct assessment data and no parent interview data from first grade and third grade). As discussed in section 3.1, a sample in fifth grade of about 10,000 children would be adequate to meet the precision requirements overall and for most subgroups. A sample of about 800 to 1,000 children in a subgroup would be achieved for most of the subgroups with an overall sample of 10,000 children and these would approximately meet the precision goals described in section 3.1. Two thirds of the public schools attended by the fifth-grade sample (1,355) are transfer schools, and almost half of the private schools (166) are transfer schools. The large number of transfer schools corresponds to the heavy movement of the ECLS-K children between schools.  The achieved sample is about 4 percent larger than the expected sample, with the language minority sample about 4 percent smaller than the expected sample. As in third grade, language minority children moved at a higher rate (48 percent) than non-language minority children (37 percent) resulting in the larger sample of non-language minority children."}, {"section_title": "3-50", "text": "As shown in table 3-21, the agreement between the expected and achieved fifth-grade samples is as seen in third grade. Children in non-Catholic private schools responded at a lower rate than children in Catholic schools, causing the achieved sample size for children in non-Catholic schools to be lower than expected. For all other characteristics, the difference between expected and achieved sample sizes is less than 1 percent for language minority children and around 3 percent or less for non-language minority children.  due to a disability) or completed parent interview data, so that the completion rate here is not the same as the instrument-specific completion rate in chapter 6."}, {"section_title": "3-51", "text": "Based on the achieved sample, the sampling and data collection procedures developed in the initial stages and modified throughout the course of the study did produce samples that met or exceeded requirements for the vast majority of key analytic groups. However, the introduction of the more intensive subsampling of children who moved to achieve the desired cost savings did result in some increases in the design effects for the estimates, as was expected. See chapter 4 of the ECLS-K Combined User's Manual  Characteristics of the schools (school affiliation and type of locale) are from the original sample schools. 2 A respondent is a child with assessment data or parent interview data, or a child who could not be assessed due to a disability. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), spring 2004."}, {"section_title": "3-54", "text": "This page is intentionally left blank. 4-1"}, {"section_title": "DATA COLLECTION METHODS", "text": "The following sections discuss the data collection procedures in the fifth-grade data collection phase of the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). Section 4.1 gives an overview of the data collection methods. Detailed information is provided on roles and responsibilities in the study (section 4.2), study training procedures (section 4.3), fall preassessment school contacts (section 4.4), spring-fifth grade data collection (section 4.5), and data collection quality control procedures (section 4.6)."}, {"section_title": "Overview of Data Collection Methods", "text": "The ECLS-K fifth-grade data collection was conducted in the fall and spring of the 2003-04 school year. Fall data collection included contacting sampled schools to schedule appointments to conduct the child assessments in the spring of the school year, verify the parent consent procedures, link children to their teachers, identify children who had withdrawn from the school, and obtain locating information about their new schools. Spring data collection instruments included the administration of direct child assessments and parent interviews and the collection of teacher and school questionnaires, student record abstracts, and facilities checklists. The activities to locate children and gain cooperation of the schools into which they transferred began in the fall and continued during the spring data collection. The content and timeline of the fifth-grade data collection is shown in exhibit 4-1. The mode of data collection was computer-assisted personal interviewing (CAPI) for the child assessments; telephone and in-person computer-assisted interviewing (CAI) was the mode of data collection for the parent interview; and self-administered questionnaires were used to gather information from teachers, school administrators, and student records. Field staff completed the facilities checklist."}, {"section_title": "4-2", "text": "Exhibit 4-1. Timeline of fifth-grade data collection: School year 2003-04 "}, {"section_title": "Roles and Responsibilities in the ECLS-K Study", "text": ""}, {"section_title": "School's Role", "text": "During school recruitment, the schools were asked to designate a staff member to be the school coordinator to assist the ECLS-K staff with all school arrangements. Since the child assessments were administered at the schools, schools needed to provide appropriate space for conducting the assessments. 4-3"}, {"section_title": "School Coordinator's Role", "text": "A school coordinator was designated by the principal to facilitate the ECLS-K activities in the school. The school coordinator played a significant role in the smooth functioning and successful completion of the ECLS-K child assessments in each cooperating school. He or she knew the personality of the school, the most opportune times to schedule the assessments, the available locations where the one-on-one assessments could be conducted, and the best way to notify students, their parents, and their teachers of the assessment. The coordinator was asked to assist the ECLS-K in four ways: Notify selected students, their teachers, and their parents of the study; Arrange for suitable space for the assessment activities; Provide information on sampled children, such as their grade and teachers' names; and Distribute teacher and school questionnaires."}, {"section_title": "Supervisor's Role", "text": "There were a total of 81 supervisors during the fifth-grade data collection who oversaw field staff conducting both the parent interviews and child assessments. The supervisors' responsibilities were as follows: Contact each school assigned to them to Follow up and track receipt of parental consent forms, as necessary; Update the Field Management System (FMS) regularly and report to their field manager;"}, {"section_title": "4-4", "text": "Transmit updated FMS data to the home office; Pick up e-mail regularly; and Return all materials at the end of the field period."}, {"section_title": "Assessor's Role", "text": "A team of three to four assessors worked with each supervisor in a work area. The primary responsibilities of the ECLS-K assessors were to conduct the computer-assisted one-on-one child assessments and parent interviews. There were a total of 258 assessors, with 16 having conducted the parent telephone interview only (these 16 assessors are referred to as interviewers rather than assessors in the remainder of this chapter). The remaining 242 conducted both the parent interview and the child assessments. In addition to these responsibilities, some assessors were asked by their supervisor to assist with various other activities that took place in the school. These included, but were not limited to, preparing parental consent forms (if required), collecting teacher questionnaires, and assisting with various other recordkeeping tasks."}, {"section_title": "Field Manager's Role", "text": "Six experienced regional field managers were assigned to oversee the work of the 81 supervisors. The field managers held weekly telephone conference calls with each supervisor assigned to them. If a supervisor had an immediate problem, he or she was encouraged to call the field manager at any time. Depending on the stage of the field period, the telephone conference calls between supervisors and field managers reviewed those activities that were in the planning stage, in progress, or in the process of being completed. These discussions included the following topics: "}, {"section_title": "Field Staff Training", "text": "Several in-person training sessions were conducted to prepare staff for the fifth-grade data collection. In the fall of 2003, supervisors were trained to contact original schools and recruit transfer schools. In the spring of 2004, three training sessions were held: one to train trainers, one for field supervisors, and one for assessors. All training sessions were conducted using scripted training manuals to ensure that all trainees received the same information. Training sessions consisted of interactive lectures, scripted role plays, interactive exercises, and self-administered exercises. Interactive lectures were lectures with discussion and questions occurring periodically during the lecture. Scripted role plays usually consisted of pairs of trainees each pretending to be, for example, the assessor and the child or the 4-6 interviewer and the parent. Such role plays gave trainees a chance to become more familiar with their materials and duties. Interactive exercises were group exercises led by the trainer in which all trainees participated. Self-administered exercises were, as the name suggests, completed by trainees working independently. Because of the complexity of the ECLS-K, trainees were required to become familiar with the functionality of their laptop computers and with the programs installed on them. Trainees were also required to become familiar with the different child assessment materials. See chapter 2 of the ECLS-K Psychometric Report for the Third Grade (NCES 2005-062)  for a description of the child assessment materials. The following sections discuss the fall and spring trainings."}, {"section_title": "Advance Contact and Recruitment Training", "text": "During the fall 2003, advance contact was made with the schools in order to remind them about the study and to collect information that would be helpful in the spring 2004 national data collection. The advance effort not only reduced the burden on supervisors in the spring but also reacquainted schools with the study's procedures and gave supervisors a chance to encourage the schools' participation. The major fall tasks were to contact schools to set appointments for the child assessments in the spring, to verify the parent consent procedures, to link children to teachers and domains, and to identify children who had withdrawn from the school and obtain locating information about their new schools. Field supervisors were trained for 3 days in September 2003 to contact original sampled schools and transfer schools to set up the data collection in the spring. A total of 39 field supervisors and 2 field managers completed advance contact and recruitment training. Topics included an overview of study activities to date, verifying parent consent procedures, identifying and locating children who moved from the schools they attended in the first grade, identifying the teachers of ECLS-K children and linking them to those children, and exercises on scheduling schools efficiently within a work area. As in the third-grade training, advance contact and recruitment training was conducted using the automated FMS. The FMS is a database that is used throughout the data collection period to enter information about the sampled children, parents, teachers, and school and to monitor production on all data collection activities. The FMS contains information essential to conducting and monitoring the progress of the data collection. For example, it contains the names and addresses of each school, the principal's name and telephone number, the name and telephone number of the school coordinator, the first and last day of classes, the school hours, and the names of the sampled children in the school. For children, it contains information on their names and their parents' names, whether they have any 4-7 accommodations or need to use a hearing aid or glasses, as well as other information. Information of the same sort is also collected about the teachers and parents. For example, the FMS contains the name, address, and telephone number of each parent. For each parent, the name of the sampled child (or children) is also listed. For teachers, the FMS contains their names, schools, and the sampled children in their classrooms. It also indicates whether the teacher was a regular or a special education teacher, and what subject the regular teacher taught and in how many classrooms. To monitor production, the FMS contains case disposition codes and case assignment information. During training presentations, the field supervisors entered information into the FMS, thus acquiring hands-on experience with the FMS and all field procedures prior to beginning data collection. The field supervisors also completed role-play exercises that involved entering information into the FMS."}, {"section_title": "Spring-Fifth Grade Training", "text": "Field supervisors, interviewers, and assessors were trained for the spring-fifth grade data collection in two sessions in February 2004. The first session was trainers' training and certification. The second and largest training involved the training of the supervisors and assessors. Before the February inperson training session, supervisors and assessors completed 8 hours of home study training on the study design, field procedures, and computer keyboard skills. Staff conducting only the parent interviews did not have a pre-session home study to complete."}, {"section_title": "Trainers' Training and Certification", "text": "The purpose of trainers' training was to (1) introduce lead and co-trainers to the training materials; (2) evaluate the flow, language, exercises, and time allotment of the training sessions; and (3) certify trainers on the child assessment. Some, but not all, co-trainers and runners (staff who assisted trainees who experienced difficulties with the CAI application and helped with the management and distribution of training materials) were also certified on the child assessment. Experienced trainers with in-depth knowledge of the ECLS-K conducted the trainer training sessions. Not only had these trainers developed the CAI specs, but they had also worked with expert consultants to develop the child assessments and the assessment materials. In addition, they conducted nine assessments on nonsampled children in October 2003 in order to become certified on the assessments of the ECLS-K prior to the launch of the national data collection (see section 4.3.2.2)."}, {"section_title": "4-8", "text": "Approximately 16 lead trainers, 16 co-trainers, and 16 runners were trained at trainers' training in a single room. As noted earlier, experienced trainers conducted the parent interview and the child assessment training sessions. In addition, a data display person responsible for running the electronic data display and two runners assisted in the training. Trainers' training was conducted for 4 days in January 2004 in Rockville, Maryland. The trainers' training agenda covered many of the same topics that were presented during the assessment and parent interview training for the national data collection, as well as some additional topics on the trainers' tasks and responsibilities. Exhibit 4-3 is the trainers' training agenda. In addition, trainers were certified on the child assessment following the procedures described in section 4.3.2.2. The only difference between trainer certification and field staff certification was that trainers were certified on an entire child assessment and the field staff were certified on approximately half of a child assessment. Assessor Training. The assessor training sessions were conducted in Los Angeles, California. Assessor training lasted for 5 days; field supervisors were also trained to perform all assessor activities; bilingual assessors were trained for half a day to conduct the parent interview in Spanish. Two hundred forty-two assessors, 16 parent interviewers, and 81 field supervisors completed training. Assessor training included an overview of study activities to date, interactive lectures on the direct child assessments and the parent interview, role-play scripts to practice parent interviews and direct child assessments, direct child assessment precertification exercises on each form of the direct child domain assessments, techniques for parent refusal avoidance, and strategies for building rapport with children. A major goal of the assessor training was to train field staff in the proper procedures to conduct the direct child assessments. The sessions provided trainees with practical experience with all the direct child assessment materials and procedures and the CAI programs before data collection.  Training staff scored these exercises and feedback was given to the trainee on coding accuracy. Most trainees passed the written exercises on their first attempt. Less than a quarter of the trainees (77 trainees or 24 percent) did not pass at least one element of the reading certification exercises on the first attempt. The mathematics and science certification exercises were considerably easier for trainees; only two trainees were required to repeat any mathematics exercise and 55 trainees (17 percent) were required to repeat the science yellow certification exercise. This variability was due to the complexity of the fifthgrade reading scoring rubrics and the unfamiliarity of the exercises themselves (reading exercises were distributed first, with mathematics and science exercises on later days). The trainees who did not pass the written certification exercises completed specified remedial training steps prescribed by training staff. All of the trainees who had to re-take the exercises after the remedial evening session achieved a passing score. No field staff were released because of failure on the certification exercises. Training staff implemented a variety of remedial training steps for individual trainees and trainees collectively. The QxQs and interactive lectures were reviewed as necessary and the trainee(s) practiced the subdomain test again. The help laboratory was made available after the daily training session to provide additional instruction and help to trainees before the live portion of the child assessment certification. The culmination of the child assessments training was administering the cognitive assessment battery to children. This allowed the trainees to experience what it is like in the field and to put into practice all their new skills. Training staff who were already certified on the assessment observed trainees as they administered parts (e.g., a routing test and a level test) of the assessment to fifth gradeaged children. Feedback on trainee performance was given using the Assessment Certification Form. Supervisors and assessors were certified on the child assessment by administering approximately half of a cognitive assessment to a child while being observed by certified evaluators. As evaluators observed the 4-21 assessment, they completed the appropriate sections of the Assessment Certification Form. This form had both the trainee and the evaluator's names recorded on the cover as well as the date the evaluation was conducted. The Assessment Certification Form had two sections: section 1 (exhibit 4-6) rated the trainee on key skill areas, such as building rapport, using neutral praise, responding to behaviors presented by the child, appropriate pacing, and avoiding coaching. The evaluator marked each skill area that the trainee did not demonstrate appropriately. Section 2 listed specific questions from each routing and subdomain form. The instructions for completing section 2 are shown in exhibit 4-7. For each of the listed questions observed, the evaluator recorded both the child's response and noted if the trainee did not demonstrate the specified required administration skills for that question. The required administration skills included reading questions verbatim, using appropriate probes, and using appropriate hand motions (gesturing). For each question on which the evaluator observed that the trainee did not demonstrate the required administration skill(s), he or she checked a box, indicating which skill was not performed."}, {"section_title": "4-22", "text": "Exhibit 4-7. Instructions for Section 2 of the Assessment Certification Form: School year 2003-04 SECTION 2: Specific Assessment Activities Supervisor/Evaluator: Code the items as the assessor administers the assessment. Code the child's response as the item is administered. If the item requires probing, check the box if the assessor does not use the appropriate probe. Check the box in the \"Verbatim\" column if the assessor does not read the item exactly as worded on the screen. Check the box in the \"Gesturing\" column if the assessor does not use appropriate hand motions.  At the end of the \"live\" child assessment, after the child had been escorted from the room, the evaluator and the trainee reviewed each trainee's overall performance on the half of the cognitive assessment that he or she conducted. After discussing the ratings in section 1, the trainee accessed the Finally, the evaluator determined the proportion correct by using the Proportion Correct Chart displayed in exhibit 4-14. Continuing the example, the evaluator found the column on the Proportion Correct Chart that displayed the total possible points for the trainee based on the portions of the assessment observed (41) and the row on the Proportion Correct Chart that displayed the total check marks recorded for the trainee (1), and recorded the proportion from the corresponding box on the Proportion Correct Chart in the box under Step 6 (exhibit 4-15) on the scoring form (.98). Once the evaluator completed the scoring, the trainee was rated as Certified, Remedial Action, or Failed."}, {"section_title": "4-26", "text": "Exhibit 4-10. Certification Scoring Form: School year 2003-04"}, {"section_title": "Certification Scoring", "text": "Step 1: Record the number of check marks from Section 1: Rapport Building and Section 2: Specific Assessment Activities in the appropriate boxes of Form A. Step 2: Sum each row and record total in Row Totals column of Form A. Step Step 6: Use Proportion Correct Chart to determine the proportion correct and write that proportion in this box: "}, {"section_title": "4-31", "text": "Exhibit 4-15. Example of Proportion Correct Chart: School year 2003-04 Step 6: Use Proportion Correct Chart to determine the proportion correct and write that proportion in this box:   The majority of the trainees (99.0 percent) scored above 85 percent on the certification form, with only 1.0 percent (n=3) scoring between 70 and 84 percent. None of the trainees failed to meet the 70 percent threshold on the Assessment Certification Form. All trainees who needed remedial training were certified qualified to administer the child assessments after they conducted a second assessment on a fifth grade-aged child who was not part of the ECLS-K sample. The three trainees who required remedial action were assigned remedial training, exercises, and practice assessments to perform on an age-appropriate child in their area. Their supervisor observed the practice assessment and certified them. The scoring form specifies the type of remedial training activities that were assigned based on the type of problems observed by the evaluator (see exhibit 4-16). Once the remedial training activities were completed, the field supervisor or field manager was responsible for conducting an additional certification assessment with the trainee with a fifth grade-aged child not in the ECLS-K sample prior to being able to start work. "}, {"section_title": "Parent Interview Training and Certification", "text": "Training assessors to conduct the parent interview also included interactive lectures and role plays that were designed not only to review the intent of the questions but also to demonstrate the different ways that parents may answer questions. Ambiguous answers were included in both training interactive lectures and role-play scripts to provide practice in probing and handling a variety of responses. Parent interview QxQs were carefully reviewed throughout training so that assessors/interviewers would be prepared for a variety of responses from parents. The culmination of parent interviewer training was a final certification role play designed to test all the protocols and techniques reviewed during training. The final role play was conducted using a scripted parent interview and the final parent role play evaluation form. The final role-play script was designed to test the trainee's understanding of the interview content, proper interviewing techniques, including probing, and accurate response recording in CATI. Trainees were paired up and each completed half of a parent interview as the interviewer. As they conducted the interview as the respondent, trainees completed the evaluation form that consisted of a checklist to evaluate key areas, such as contacting and selecting the respondent, asking the questions verbatim, probing properly, and following the correct question path. Two points were given for each item performed correctly. Trainees had to score at least 30 out of a possible 36 points to pass. Table 4-2 presents the results of the parent certification. All trainees who expected to conduct parent interviews completed the parent certification procedures. The majority of trainees (95.5 percent or 294 trainees) were successfully certified on the parent interview. Parent certification scores were not reported for sixteen trainees. The 14 trainees who did not pass the parent certification at training worked with their supervisors to improve the areas in which they lacked skills. These staff completed another parent certification role play with their supervisor. All 14 staff were certified on the parent interview after the remedial training."}, {"section_title": "4-33", "text": ""}, {"section_title": "Fall Preassessment School Contact", "text": "Beginning in September 2003, all participating ECLS-K schools (i.e., schools that participated in previous rounds of data collection) were contacted by telephone to prepare for the spring data collection. When children were identified as having transferred to another school, the child's new school (and district, if necessary) was recruited. As noted in section 4.3.1, the advance contact served several purposes. It reminded schools about the study and reacquainted them with the study's procedures; it provided supervisors the opportunity to persuade the schools to participate; and it allowed the collection of information necessary for the spring 2004 national data collection. There were four primary tasks to be accomplished during the fall contact. These were to schedule appointments to conduct the child assessments in the spring; to verify parent consent procedures; to identify the children's teachers; and to identify children who had withdrawn from the school and obtain locating information about their new schools. The fall contact activities are described below. 4-34"}, {"section_title": "Advance Mailings", "text": "In September 2003, an advance package was mailed via Federal Express to all participating ECLS-K schools, i.e., schools that participated in third grade, asking them to prepare for the preassessment contact telephone call. The schools were asked to identify a school staff coordinator to serve as a liaison with the study (in original sampled schools, this person was usually the coordinator from the previous rounds of data collection). A package containing study materials was sent to the schools. The package contained the following materials and was customized by school type-original or transfer schools: Original school advance package: A letter printed on ECLS-K letterhead reminding school staff about the study, describing the fifth-grade data collection, and alerting the school coordinator of the advance contact in the fall; A School Summary Sheet (original schools) -a two-page document providing a brief review of the study to date and the fifth-grade data collection activities; A Study Findings Sheet (original schools)-a summary of findings about children from the previous rounds of data collection; and An ECLS-K Study Children Form and Instructions-a listing of all the sampled children and instructions for completing the form with specific information such as the children's continued attendance at the school, their grade, their teachers' names, classroom numbers, receipt of special education services, and receipt of assessment accommodations/exclusions before the preassessment call. Transfer schools: A letter printed on ECLS-K letterhead introducing school staff to the study, describing the fifth-grade data collection, and alerting the school coordinator of the advance contact in the fall; A School Summary Sheet (transfer schools)-a two-page document providing a brief overview of the study and the fifth-grade data collection activities; A Study Findings Sheet-a summary of findings about children from the previous rounds of data collection; and An ECLS-K Study Children Form and Instructions-a listing of all the sampled children and instructions for completing the form with specific information such as the children's continued attendance at the school, their grade, their teachers' names,"}, {"section_title": "4-35", "text": "classroom numbers, receipt of special education services, and receipt of assessment accommodations/exclusions before the preassessment call."}, {"section_title": "Fall Preassessment School Coordinator Contact", "text": "The preassessment contacts were made by telephone from September through December"}, {"section_title": "2003", "text": ". The preassessment school contacts were successful in meeting all four tasks described above. Contacting original sampled schools to set up the spring assessment and identifying children who withdrew from their spring-third grade school and moved into their fifth-grade transfer school, enabled the identification of schools that were ineligible for fifth-grade data collection. Schools were determined to be ineligible for fifth-grade data collection if no ECLS-K sampled children were currently enrolled. Original sampled schools became ineligible because fourth grade was the highest grade in the school or because the school had closed, that is, was no longer operational. More transfer schools were determined to be ineligible as children transferred out of them into other schools. During the preassessment contact, the field supervisor contacted the school coordinator to collect some basic information about the school and some detailed information about each ECLS-K sampled child. The field supervisor used the School Information Form to collect basic information about the school, such as school start and end dates, vacation and holiday schedules, and parking directions. The form was also used to determine if the school was a year-round school, taught fifth grade, or required new parent consent, and to obtain information on class organization. The supervisor used the Child Work Grid (exhibit 4-17) to collect basic information about the child such as his or her grade, the name and classroom number of the child's primary teacher to link the child to a teacher, and whether the child had an Individualized Education Program (IEP) or its equivalent. In addition, in original sample schools, the assessment date was scheduled; assessment dates for new transfer schools were scheduled in the spring. Reviewing parent consent with the school. Because parental consent was obtained in the base year and re-obtained in the third-grade year, field supervisors did not raise parental consent with the school coordinator unless the school district required it. However, if the school was a transfer school then the supervisor did ask the school coordinator whether it was necessary to obtain parental consent. If the schools required current consent forms or changed the type of consent that was required (e.g., from implicit to explicit), parent letters and consent forms were either mailed to the school for distribution to parents or sent directly to parents by Westat depending on the schools' preference. Parent cover letters  If a child was identified as having transferred out of the school, the field supervisor asked the school coordinator to provide the names, addresses, and telephone numbers of these transfer schools Of those children who transferred, only a subset was followed to their new school (see section 3.6.1 in chapter 3 for more detail on how mover children were subsampled). If the new school belonged to a district that was new to the study, the district was mailed a letter with the new school name. A field supervisor contacted the district by telephone and recruited the district into the study before any contact was made with the school. If the district was already cooperating, the district was notified by mail and the new school was contacted and recruited directly. Field supervisors also verified with the school that no child who had previously transferred had returned to the school. "}, {"section_title": "4-40", "text": "Contacting families of homeschooled children. As part of the fall preassessment contact, children in the ECLS-K sample who were homeschooled in previous rounds were identified. The status of home-schooled children who were identified in rounds 1 through 4 was verified with their parents and updated as necessary. In addition, during the preassesment contact some schools identified homeschooled children. Their status was also verified with their parents during data collection. Parents of these children were contacted by telephone in September through December 2003 to determine if the child was still homeschooled or had enrolled in a school. If the child had enrolled in a school, the new school was contacted and recruited into the study. Parents of children who were still schooled at home were notified about the next round of data collection in the spring. Identifying the key child in classrooms with multiple study children. In grade 5, the design of the child-level teacher questionnaire was changed to include collecting data about the child's reading class and mathematics or science class. In previous rounds, children were taught primarily in selfcontained classrooms and teachers only reported classroom level information once for the classroom. Due to the grade 5 design change, the teacher-child links were restructured to include the domain (reading, mathematics, or science) and a classroom identifier (e.g., room number and the time the class meets). A unique link number was created for each teacher/child/domain/class combination. In order to reduce data collection burden for teachers who were linked to multiple sample children in the same class, a \"Key Domain Child\" was identified for each separate subject and class that each teacher taught. The teachers were asked to report classroom level information only once in the questionnaire for the key domain child and child-level information for all sampled children in their class. Field supervisors collected the teacherchild-domain-classroom link information about each child and entered the information into the FMS. The information was used to generate the hardcopy teacher questionnaires (see section 5.4.3 for more information)."}, {"section_title": "Fall Preassessment Contact Results", "text": "The goals for the fall preassessment contact with schools were the following: (1) set appointments for the spring assessment in original sample schools, (2) identify schools that ended at fourth grade to determine the school to which the sample children transferred, (3) identify children who changed schools since third grade, (4) link children to teachers and domains (i.e., reading, mathematics, science) for the advance school and teacher questionnaire mailings, and (5) contact as many transfer schools as possible within the field period to ascertain whether the child was still there and recruit the 4-41 school into the study. It was not expected that every transfer school identified within the fall contact could be contacted within the fall field period because of the numbers of children that were expected to move. It was also expected that additional schools would be contacted during the spring round because children were expected to continue to move between fall and spring of the school year.  NOTE: Movers who had a status of \"Fielded for assessment in spring\" were treated like the nonmover cases during the spring data collection. Those with a status of \"Unlocatable\" could not be located during the fall preassessment contact; additional attempts to locate them continued in spring data collection. Cases identified as \"Moved to nonsampled PSU\" or \"Moved to outside of U.S.\" were not fielded for the spring data collection because they had moved out of the designated data collection area. Cases with a status of \"Subsample not followed\" were not fielded in the spring because the sampling plan called for collecting data from only a subsample of movers (see chapter 3, section 3.6.1). These cases were not in the subsample to be followed. Only 4 cases with a status of \"End of field period\" were not contacted in the fall because the schools were either new to the school sample frame with limited information available to contact them or in school districts new to the sample that required an additional contact at the district level before the schools could be contacted for spring data collection. These cases were closed out and rolled over for contact in the spring. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), spring 2004."}, {"section_title": "4-42", "text": "The fall preassessment contact protocol was completed for 100 percent of the original ECLS-K schools and 97 percent of the transfer schools (both those identified before and during the fall field period) within the fall field period. Twenty percent of schools were identified as out of scope, since they did not contain any sampled students (9 percent of original sampled schools and 25 percent of transfer schools). All of the schools that children transferred to as a result of the school ending at fourth grade, closing, or merging with another school were identified within the field period. Tables 4-4 through 4-6 present the production reports for the fall preassessment contact for original sample and transfer schools.   4-45"}, {"section_title": "4-43", "text": ""}, {"section_title": "4-44", "text": ""}, {"section_title": "Tracing Activities during Fifth-Grade Data Collection", "text": "In June 2003 an attempt was made to obtain a current address for all eligible households using Telematch, a company that searches for current address information through its many databases.  In order to ensure that as many of the sampled children as possible were contacted in the spring, locating efforts were conducted from June through early August 2003. Staff in Westat's Telephone Research Center (TRC) traced the 829 children who could not be located during previous rounds of data collection. TRC staff used the Internet, telephone directories, and other means to locate these children and their households. When children and/or households were found, the new school and contacting information was entered into the tracing system database for fielding in the spring. Table 4-8 presents the results of the summer tracing effort.  "}, {"section_title": "Spring-Fifth Grade Data Collection", "text": "All children who were assessed during the base year or for whom a parent interview was completed in the base year were eligible to be assessed in the spring-fifth grade data collection, with four exceptions: They were (1) children who became ineligible in an earlier round (because they died or moved out of the country), (2) children who were subsampled out in previous rounds because they moved out of the original schools and were not subsampled to be followed, (3) children whose parents emphatically refused to cooperate (hard refusals) in any of the data collection rounds since springkindergarten, and (4) children in the fifth-grade sample for whom there were neither first-grade nor thirdgrade data. Eligibility for the study was not dependent on the child's current grade, that is, children were eligible whether they were promoted to fifth grade or were retained in fourth grade. In spring-fifth grade, the children attended 2,008 public schools and 356 private schools. Two-thirds of the public schools (1,355) and almost half of the private schools (166) were transfer schools. As in previous rounds of data collection, the field staff were organized into work areas, each with a data collection team consisting of one field supervisor and two or more assessors. The data collection teams were responsible for all data collection activities in their work areas; they conducted the 4-47 direct child assessments and the parent interviews, collected all school and teacher questionnaire and completed checklists. The majority of field staff members in fifth grade were continuing from previous rounds of data collection; a few new staff were hired in areas where no experienced ECLS-K staff lived."}, {"section_title": "Spring Preassessment Activities", "text": "Based on the information collected in the fall of 2003, packets of hard-copy teacher and school administrator questionnaires were assembled and mailed to schools in January and February 2004, along with letters confirming the scheduled visits to the school. Teachers and school administrators were asked to complete the questionnaires for pickup by the field supervisor or lead assessor on assessment day. Letters were also mailed to parents at this time reminding them of the spring-fifth grade data collection activities. Most preassessment contact with schools was by telephone starting in March 2004. Field supervisors contacted the school to confirm the assessment date and the receipt of the hard-copy questionnaires and to arrange for space to conduct the assessments. This follow-up call to the schools was essentially to confirm the logistics for the assessments."}, {"section_title": "Conducting the Direct Child Assessments", "text": "The direct child assessments were conducted from March through June 2004, the same time of year as in prior spring data collections. The school coordinator set the assessment date with a supervisor based on the school's schedule. Over three-quarters of the assessments were completed by the end of April, with 20.5 percent completed in May and 1.3 percent completed in June. There was no evidence that certain types of children or schools were assessed early or late in the field period. Table 4-10 presents the weekly completion rates for the child assessments. In year-round schools, assessment teams made multiple visits to the school, visiting when each track was in session to assess the sampled children. There were 501 (2.3 percent) sampled children in year-round schools. The direct child assessments were usually conducted in a school classroom or library. Before conducting the assessments, field supervisors and assessors set up the room for the assessments. They followed procedures for meeting children that were agreed upon during the preassessment contact with the school. Each child was signed out of his or her classroom prior to the assessment and signed back into the classroom upon the conclusion of the assessment. During the scheduling of schools in the fall, an attempt was made to conduct the direct child assessments at about the same point in time from the beginning of school year and the end of the year to increase the chances that exposure to instruction was about the same for all children. The fifth-grade direct child assessments averaged 97 minutes in length. The term accommodation in this table is the field operational definition of accommodation, which includes the wearing of glasses and hearing aids. These types of aids were systematically tracked to ensure that every child had the same chance at a successful assessment. With this information, assessors could prompt a child (e.g., to get her glasses before being assessed). NOTE: This table reflects final production numbers prior to statistical adjustment. This table does not include children who were subsampled out in fall-and spring-first grade and spring-third grade (see section 5.5.4.) These numbers should not be used to estimate student mobility. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. 4-50 percent of the sample was assessed in transfer schools in spring-fifth grade. Assessments of homeschooled children, or children in nonparticipating transfer schools, were held at the parent's home or another location, such as a library, of the parent's choosing."}, {"section_title": "4-48", "text": "Less than 1 percent of participating children in fifth grade required accommodations or were excluded from the direct child assessments. Children were excluded from the direct assessments if they had a disability, e.g., blindness, or deafness, that could not be accommodated by the ECLS-K direct assessments, or their Individualized Education Program prevented their participation in assessments or required an accommodation not offered by the ECLS-K assessments. Accommodations offered by the ECLS-K assessments were as follows: alternative setting, scheduling, or timing; health care aide present; or the use of a personal assistive device. Table 4-12 presents the number of children excluded from or requiring an accommodation to the direct child assessment procedures in the spring of fifth grade. "}, {"section_title": "Conducting the Parent Interview", "text": "Parent interview procedures mirrored those of previous rounds of data collection. The parent interview was administered, primarily by telephone interview using CAI, between February and June 2004. Slightly over 50 percent of the parent interviews were completed in February and March, 43 percent were completed in April and May, and 6 percent were completed in June. The parent interview averaged 43 minutes. Table 4-13 presents the weekly completion of parent interviews. In spring-fifth grade data, 10 percent of the parent cases were classified as final nonresponse."}, {"section_title": "4-51", "text": "As in most field studies, the primary reasons for final nonresponse were parents who could not be located and parents who refused to complete the interview; 32.4 percent of the nonresponse parent cases were parents who could not be located, 43.9 percent were refusals and 23.8 percent were other nonresponse (e.g., language barrier). 4-53"}, {"section_title": "4-52", "text": ""}, {"section_title": "Conducting Data Collection on Children Who Withdrew From Their Previous Round", "text": ""}, {"section_title": "School", "text": "While contacting schools, field supervisors asked school coordinators to identify children who had withdrawn from the school since the spring of third grade. School staff were asked whether they knew the name and address of the school to which the child transferred, as well as any new information about the child's household address. For the children who had moved from their spring-third grade school and were not part of the sample to be followed, information was collected only from the school personnel and not from parents. For children who had withdrawn from their spring-third grade school and were identified to be followed (i.e., were part of the sample of movers), supervisors also consulted parents and other contacts for information on the children's new school. This information was entered into the FMS and processed at Westat for data collection. 12,717 children were identified as having transferred from the school in which they were enrolled during the spring of base year, first grade, or third grade. The movers described in this table are defined as \"operations movers\" rather than \"statistical movers\" since cooperation must be secured from the transfer schools in order for data collection to proceed (see section 3.6.1 for a discussion of statistical movers.). There are two flags that describe movers. One flag is for statistical purposes and the other for operations purposes. The definitions for the two flags necessarily differ. For example, children who move into a destination school are flagged as a mover for operations purposes, since cooperation must be secured from the school to follow the child there, but the statistical flag is not set since these children are not movers in the statistical definition. Table 4-15 describes total movers across all rounds, which is the only way they were tracked; the chapter 3 tables describe the grade 5 movers from a statistical perspective. Of the 12,717 mover children in spring-fifth grade, 4,187 (32.9 percent) were in scope (i.e., children selected to be followed) and followed. The remaining 8,530 mover children were out of scope and were not followed; no child assessment or parent interview was conducted for these children. 149 3.6 Not assessed/absent 5 63 1.5"}, {"section_title": "4-54", "text": "1 The movers described in this table are defined as \"operations movers\" rather than \"statistical movers\" since cooperation must be secured from the transfer schools in order for data collection to proceed. 2 Percent based on total movers. 3 Percent based on out-of-scope children. 4 In fifth grade, four groups of children were excluded, irrespective of other subsampling procedures that were implemented. They are (1) children who became ineligible in an earlier round (because they died or moved out of the country); (2) children who were subsampled out in previous rounds because they moved out of the original schools and were not subsampled to be followed; (3) children whose parents emphatically refused to cooperate (hard refusals) in any of the data collections rounds since spring-kindergarten; and (4) children in the third-grade sample for whom there are neither first-grade nor third-grade data. Parent interviews were attempted for all in-scope children. However, different school and assessment data collection strategies were followed for children who moved, depending on where they moved and the cooperation status of their new school. School and assessment data collection was attempted for children who moved and were flagged as \"follow\" in spring-fifth grade in the following ways: Data collected for children moving into cooperating base year sampled schools included the child assessments in the school, school administrator questionnaire, regular and/or special education teacher questionnaires, facilities checklist, and student record abstract forms. Data collected for children moving into nonsampled schools in base year cooperating districts included the child assessments in the school, school administrator questionnaires, regular and/or special education teacher questionnaires, and student record abstract forms, if school permission was obtained. If school permission was not 4-55 obtained, the assessments were conducted in the home and no school or teacher data were collected. For children moving into transfer schools that refused, schools in sampled districts that refused, or originally sampled schools that were ineligible when sampled because they did not have kindergarten classes, the direct child assessments were conducted in the home. No school or teacher data were collected. For children moving into schools in nonsampled districts or dioceses: -If the school was within the primary sampling unit (PSU), data collected included the child assessments in the school, school administrator questionnaire, regular and/or special education teacher questionnaires, facilities checklist, and student record abstract forms, if school permission was obtained. If school permission was not obtained, the assessments were conducted in the home and no school or teacher data were collected. -If the school was outside a sampled PSU, no child, school, or teacher data were collected. For children who were not enrolled in school in the spring (including children who were home schooled), data collected included the child assessments in the home if the child was in the sampled PSU. If the child was outside the sampled PSU, no child assessment or school or teacher data were collected. As previously mentioned, 4,039 movers were identified during the fall preassessment contact. During spring-fifth grade, an additional 366 movers were identified. The identification of 92 percent of the fifth-grade movers during the fall contact accomplished two important goals: (1) allowed additional time to trace movers; and (2) reduced the burden of tracing a large number of movers during the spring-fifth grade data collection. Table 4-16 presents the final status of the children who were identified as movers in fifth grade; a total of 12,717 children were identified as having transferred from the school in which they were enrolled when they were sampled in kindergarten. Of the 12,717 children identified as movers in spring-fifth grade, 4,187 children were selected to be followed and were followed (32.9 percent of total movers). The remaining 8,530 mover children were out-of-scope for this round of data collection because they moved out of the country, were deceased, excluded from fifth grade because they had no first-or third-grade data, or were subsampled out; no child assessments or parent interviews were conducted for these children. The movers described in this table are defined as \"operations movers\" rather than \"statistical movers\" since cooperation must be secured from the transfer schools in order for data collection to proceed. 2 Percent based on total movers. 3 Percent based on out-of-scope children. 4 In fifth grade, four groups of children were excluded, irrespective of other subsampling procedures that were implemented. They are (1) children who became ineligible in an earlier round (because they died or moved out of the country); (2) children who were subsampled out in previous rounds because they moved out of the original schools and were not subsampled to be followed; (3) children whose parents emphatically refused to cooperate (hard refusals) in any of the data collections rounds since spring-kindergarten; and (4) children in the third-grade sample for whom there are neither first-grade nor third-grade data. "}, {"section_title": "4-56", "text": ""}, {"section_title": "4-57", "text": "Of the children who moved in fifth grade and were selected to be followed, 66.9 percent moved into nonsampled schools, 5.3 percent moved into a school outside the PSU and 10.4 percent of the movers could not be located. Table 4-17 presents the fifth-grade movers by school and district status. "}, {"section_title": "Teacher and School Data Collection", "text": "Data were collected from school administrators, regular classroom teachers, and special education teachers from February through June 2004. The school and teacher questionnaires were mailed to the school coordinators in January and February 2004. This schedule allowed additional time for these respondents to complete and return the instruments to Westat. During the fall advance school contact, it became apparent that many fifth-grade teachers would be asked to report about many sampled children and multiple domains (e.g., reading and math or science) due to the modification to the fifth-grade teacher questionnaires (see section 2.5 for a detailed discussion). In fifth grade, there was still considerable clustering of students in classrooms and teachers would be repeating classroom level information across many questionnaires. Table 4-18 presents the number of child-level questionnaires and percent of teachers asked to complete them based on fall linkages of children to teachers and domains as well as the percent of teachers completing that number of questionnaires during spring data collection. A procedure to reduce burden on teachers was developed to identify a \"Key Child\" for each domain and class. Teachers were asked to complete all items in the reading, mathematics, and science questionnaires for the designated key child, designated by a blue dot on the questionnaire cover, for the appropriate domains/classes. They were asked to complete only the child-level items, and not the classroom-level items, for the remaining children in that domain/class. Teacher and school administrator packets were bundled together by school and mailed to the school coordinator for distribution. If the school and/or teacher and school administrator were not identified in the fall advance contact, then the supervisor gathered the relevant information during the preassessment call in the spring and mailed the packets at that time. During their visits to the schools, field supervisors also completed a facilities checklist for each sampled school."}, {"section_title": "4-58", "text": "Field supervisors began prompting for the return of questionnaires when they contacted schools to confirm the assessment schedule. During the field period, field supervisors followed up with school administrators and teachers by telephone and with visits to the schools to prompt for the return of the questionnaires. Field supervisors returned completed questionnaires to Westat. In April 2004, the field supervisors were instructed to conduct intensive followup for missing school administrator questionnaires for schools with high minority student populations to reduce potential bias. There were 334 such schools identified. As a result of the followup efforts by the field supervisors, 324 of the 334 schools (97 percent) completed and returned the school administrator questionnaires."}, {"section_title": "Hard-Copy Data Retrieval", "text": "Retrieval Procedure. Data retrieval involved collecting missing items for some questionnaires that were otherwise complete. Based on analyses of the success of the third-grade data retrieval, critical items were determined only for the school administrator questionnaire and the Key Data Retrieval Attempts. Field supervisors attempted to retrieve missing critical items and missing questionnaires in the schools in their assignments. They scheduled their retrieval efforts for the day the assessments were scheduled and attempted to find the respondents in person. Otherwise, they attempted these retrieval attempts by telephone. Field supervisors recorded any changes to missing critical items in blue pencil in the questionnaire. If the respondent did not know the answer, they recorded \"DK\" by the item; if the respondent refused to answer, they recorded \"RF\" by the item. Field supervisors 4-61 recorded the status of the questionnaire as one of the following: Questionnaire complete with no missing critical items: no data retrieval required. Questionnaire is missing one or more critical items: data retrieval required; one or more critical items collected. Questionnaire is missing one or more critical items: data retrieval required; no critical items were collected. Questionnaire refused: unit nonresponse. Table 4-19 presents the results of attempts to retrieve critical items."}, {"section_title": "Incentives in the ECLS-K", "text": "In order to gain respondent cooperation and ensure participation throughout the various data collection phases of the ECLS-K study, various incentives were offered. The type of incentive, monetary or nonmonetary, depended on whether the respondent was a sampled child, parent, teacher, or school. Exhibit 4-20 delineates the types of incentives used in the ECLS-K. Child Incentives. Children were given a small token at the end of the assessment to thank them for their cooperation in completing the assessment. In the spring-fifth grade, they were given a green lanyard with the ECLS-K sun log and study name. In addition, each month Westat mailed birthday cards to children whose birthdays fell within that month. Children were sent birthday cards throughout the calendar year, not just during the school year. By mailing these cards, children were not only thanked again for their participation, but parents were also reminded about the study. These periodic reminders are important in a longitudinal study, in which respondents may become apathetic toward the study during later rounds. Not only do these reminders encourage respondent participation, but they help the home office update addresses of families that have moved. Parent Incentives. In the fall of 2003, a newsletter about the study was published and mailed to parents. The newsletter served to update respondents on the initial findings from the first grade year and inform them about the future rounds of data collection. Parent involvement with homework, teacher views on homework and the average time spent on homework were a few of the topics discussed."}, {"section_title": "4-62", "text": "Not only did the newsletter update parents on the findings of the study and highlight its importance, but it also was an incentive for future rounds of participation. Respondents were able to see the results of their participation in the study. Parents received an incentive for participating in the parent interview. At the end of the field period, thank-you letters were generated for every respondent who completed a parent interview. The letters, along with an ECLS-K magnet with attached notepad, were mailed to the respondents. Letters translated into Spanish were mailed to those respondents who completed the interview in Spanish. Teacher Incentives. In order to maximize response rates, all teachers were mailed a $20 check with the questionnaire package. Teachers were also reimbursed $7 for each child for whom they completed a questionnaire, regardless of the number of questionnaires. Special education teachers were reimbursed in the same manner. School Incentives. Schools were also paid a monetary incentive for participating in the ECLS-K. Because school staff are often very busy and may not be aware of the benefits of cooperating, the cooperating original sample schools were remunerated $200 for participating, of this $50 was mailed with the school administrator questionnaire package and $150 was mailed once the school assessment 4-64 work was begun. Transfer schools received the $50 with the school administrator questionnaire package. School staff who completed the student records abstract received $7 for each questionnaire completed. All checks to schools, teachers, and school staff completing the SRAs were mailed weekly during the field period and were sent with thank-you letters. The checks to teachers and to staff completing the SRA were made out to the individual who completed the forms."}, {"section_title": "Data Collection Quality Control", "text": "The ECLS-K data are used by researchers to study children's school experience and its relation to student outcomes, and by educators and policymakers to inform policy decisions. It is important that the information used by these groups is based on sound research practice and that considerable attention be paid to identifying potential sources of error, quantifying this error, and designing techniques to either reduce the error or minimize its impact on survey estimates. The work carried out in support of the ECLS-K includes a variety of activities that are directed toward ensuring that the data are of high quality."}, {"section_title": "Maintaining Reliability on the Child Assessment", "text": "To ensure that assessors maintained the standard that they achieved at training, assessors were observed by their supervisor in the field at two different points in time. The first observation was to be conducted by the end of March and the second observation by the end of April. The supervisor completed the Assessment Observation Form, which rated the assessor on key areas of the assessment protocol. In the Assessment Observation Form, the supervisor simultaneously coded with the assessor those open-ended assessment items that required judgment by the assessor to determine whether the child's answer was correct. At the end of the assessment period, after the child was escorted from the room, the supervisor and the assessor reviewed the assessor's overall performance. The two also compared the way that they each handled the open-ended questions. If there were large discrepancies, they reviewed the QxQs for these items carefully. 4-65"}, {"section_title": "Assessment Observation Form", "text": "The Assessment Observation Form had the names of the assessor and the supervisor, the case ID observed, the observation number, as well as the date the observation was conducted. The form had two sections: section 1 (shown in exhibit 4-21) was used by supervisors to rate the assessor on key overall skill areas, such as building rapport, using neutral praise, responding to behaviors presented by the child, pacing appropriately, and coaching. In section 1 the supervisor checked \"No\" for each skill area that the assessor did not demonstrate appropriately. Section 2 listed specific questions from each routing and subdomain (e.g., mathematics) form. The instructions for completing section 2 are shown in exhibit 4-22. For each of the listed questions observed, the supervisor recorded both the child's response and if the assessor did not demonstrate the specified required administration skills for that question. The required administration skills included reading questions verbatim, using appropriate probes, and using appropriate hand motions. For each question in which the supervisor observed that the assessor did not demonstrate the required administration skill(s), he or she checked a box, indicating which skill was not performed. Code the child's response as the item is administered."}, {"section_title": "4-66", "text": "If the item requires probing, check the box if the assessor does not use the appropriate probe. Check the box in the \"Verbatim\" column if the assessor does not read the item exactly as worded on the screen. Check the box in the \"Gesturing\" column if the assessor does not use appropriate hand motions.  At the end of the child assessment, after the child was escorted from the room, the supervisor and the assessor reviewed the assessor's overall performance. After discussing the ratings on section 1, the assessor accessed the quality control screen. The assessor and supervisor reviewed their codes for each open-ended question asked in section 2 of the Assessment Observation Form. The supervisor then scored the assessment observation using the scoring form shown in exhibit 4-24. In the first part of the form (Form A), the supervisor counted the number of check marks recorded in section 1: Rapport Building and recorded that number in the appropriate row. The supervisor then recorded the number of check marks for each section of the assessment that was observed and entered those numbers in the appropriate box."}, {"section_title": "4-68", "text": "Exhibit 4-24. Observation Scoring Form: School year 2003-04"}, {"section_title": "Observation Scoring", "text": "Step 1: Record the number of check marks from Section 1: Rapport Building and Section 2: Specific Assessment Activities in the appropriate boxes of Form A. Step 2: Sum each row and record total in Row Totals column of Form A. Step Step 6: Use Proportion Correct Chart to determine the proportion correct and write that proportion in this box: Finally, the supervisor determined the proportion correct by using the Proportion Correct Chart displayed in exhibit 4-27. Continuing the example, the supervisor found the column on the Proportion Correct Chart displaying the total possible points for the assessor based on the portions of the assessment observed (74) and the row on the Proportion Correct Chart displaying the total check marks the supervisor recorded for the trainee 3, and recorded the proportion from the corresponding box on the Proportion Correct Chart in the box under step 6 (exhibit 4-28) on the scoring form (.96). Once the supervisor completed the scoring, the assessor was rated as Passed, Remedial Action, or Failed. The field supervisors recorded their observations on the form and then reviewed the form with the assessor. The most frequent problems observed were not reading the items verbatim and inappropriate gesturing. Feedback was provided to the assessors on the strengths and weaknesses of their performance and, when necessary, remedial training was provided in areas of weakness. "}, {"section_title": "4-72", "text": "Exhibit 4-28. Example of Proportion Correct Chart: School year 2003-04 Step 6: Use Proportion Correct Chart to determine the proportion correct and write that proportion in this box: .96  Two hundred and forty-two assessors were to be observed after training; assessors completing only parent interviews were not observed. 12 assessors had only one observation; 227 assessors had two observations. Only 1 assessor failed to pass the first observation and was released from the project by mutual consent. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), spring 2004."}, {"section_title": "Assessor Interrater Reliability", "text": "As part of the child assessment observations described in section 4.6.1, field supervisors completed an assessment certification form for each observation they conducted. An important element of this form was the \"validation items.\" With the exception of the reading routing test, all of the assessments included at least one item that both the observer and the assessor scored. These items had open-ended responses that called for interpretation on the part of the assessor to determine whether a child's response was correct. By comparing the extent to which assessors and observers agreed on scoring these validation items, a measure of interrater reliability was obtained. Interrater reliability provided a measure of the accuracy of the assessor's scoring compared with the standard, the observer's. relatively large number of observations (232) and also contained a relatively large number of validation items (5) compared with some of the other paths. Thus, there was greater opportunity for disagreement on this path compared with the others. The science blue level (the high science level) also had a relatively higher opportunity for disagreement (212 observations and 4 validation codes) and it, too, exhibited a somewhat lower interrater reliability (96.7 percent) compared with some of the other paths. The reliability, however, even on these more difficult paths, was high and demonstrated that the assessors accurately coded open-ended items. "}, {"section_title": "4-74", "text": ""}, {"section_title": "Validation of Parent Interviews", "text": "Approximately 10 percent of the respondents who completed parent interviews were called back by a field supervisor (i.e., validated). The first parent interview completed by an assessor was always validated. Over the course of the field period, a running count of an assessor's completed parent interviews was maintained, and each tenth completed parent interview was selected for validation, thus ensuring that 10 percent of each assessor's cases were selected for validation. The parent validation was approximately 5 minutes long and was conducted by telephone. No in-person cases were validated. Field supervisors used a standardized parent validation script, the Parent Interview Validation Form, to make validation calls to parents. The validation script included verification of the child's name, date of birth, and sex, and 7 questions from the parent interview. Parent Validation Interview Forms were generated throughout the field period. By the end of June, nearly 10 percent (1,023) of the total completed parent interviews had been validated. Validation results were as follows: (1) \"No changes\" meaning responses to the original interview and the validation interview were identical; (2) \"Minor changes\" meaning there was a minor discrepancy (e.g., the ZIP code was different) between the responses to the original interview and the validation interview; and (3) \"Major changes\" meaning there was a discrepancy between the response to the original interview and the response in the validation interview (e.g., bed times varied between the two contacts). Table 4-22 presents the results of the parent validations."}, {"section_title": "Validations of School Visits", "text": "To ensure that assessments proceeded smoothly, a validation call was completed with the school principal in at least two of each supervisor's assigned original schools in the spring-fifth grade data collection. Field managers conducted the school validations. No validation calls were made to transfer schools because the impact of the survey was so minimal due to most having only one child enrolled. "}, {"section_title": "4-75", "text": ""}, {"section_title": "DATA PREPARATION AND EDITING", "text": "As described in chapter 4, two types of data collection instruments were used for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) data collection in the springfifth grade: computer-assisted instruments and self-administered paper forms (hard copy). The data preparation approach differed with the mode of data collection. The direct child assessments and parent interview were conducted using computer-assisted interviewing (CAI) techniques. Editing specifications were built into the computer programs used by assessors to collect these data. The teacher, school administrator, and student records abstract forms were self-administered. When forms and questionnaires were received at Westat, coders reviewed them to ensure data readability for transfer into an electronic format and for the completion of critical items. The visual review included changing (upcoding) any \"Other, specify\" responses that actually fit within the available response categories of the question. For example, if the parent said \"skiing\" in response to the question on the types of exercise or physical activity the child participated in, this answer was upcoded to the existing category, \"individual sports.\" There were some items for which upcoding was conducted after the data were keyed due to the large volume of \"Other\" responses. Once they finished this review, the coders sent the instruments to data entry to be manually transferred to an electronic format and reviewed for range and logic consistency. The following sections describe the data preparation activities for both modes of data collection in more detail."}, {"section_title": "Coding and Editing Specifications for Computer-Assisted Interviews (CAI)", "text": "The very nature of designing a computer-assisted interview requires decisions about edit specifications to be made at the development stage. Both acceptable ranges and logical consistency checks were preprogrammed into the electronic questionnaires. The next few sections describe the coding and editing of the data that were conducted during and after the CAI parent interview."}, {"section_title": "Range Specifications", "text": "Within the CAI parent interview instruments, respondent answers were subjected to both \"hard\" and \"soft\" range edits during the interviewing process. A \"soft range\" is one that represents the \"Hard ranges\" have a finite set of parameters for the values that can be entered into the computer, for example, \"0-5 times\" for the number of times the child, in the previous 5 days, ate a breakfast that was not provided by the school. Out-of-range values for closed-ended questions were not accepted. If the respondent insisted that a response outside the hard range was correct, the interviewer could enter the response in a comments data file. Data preparation and project staff reviewed these comments. Out-of-range values were accepted and entered into the data file if the comments supported the response. The child assessments did not employ hard and soft ranges. Children's answers were recorded verbatim.\nHard-copy range specifications set the parameters for high and low acceptable values for a question. Where values were printed on the forms, these were used as the range parameters. For openended questions, such as, \"Counting this school year, how many years have you taught in your current school including part-time teaching?\" high and low ranges were established as acceptable values. Data frequencies were run on the range of values to identify any errors. Values outside the range were identified as errors and were printed for a data editor to review. Cases with range errors were identified, and the original response was updated. In some cases, range violations were retained in the data because the value was checked and found to be the value reported by the teacher or school. These were marked as KeepAsIs cases. Data frequencies were then rerun and reviewed. This iterative process was repeated until no further range errors were found."}, {"section_title": "Logical Consistency Checks (Logical Edits)", "text": "Logical consistency checks, or logical edits, examined the relationship between responses to ensure that they did not conflict with one another or that the response to one item did not make the response to another item unlikely. For example, in the household roster, one could not be recorded as a mother and male. A male mother or female father (or any other gender/relationship discrepancy) would cause the case to fail the parent edits and the case would be examined by hand. We certainly identified some same-sex partnership households this way, usually by looking at the combination of relationship, sex, and first name. Such households would be left as is. Only cases that were deemed in error were corrected. When a logical error such as this occurred during an interviewing session, the interviewer was presented with a message requesting verification of the last response and a resolution of the discrepancy. In some instances, if the verified response still resulted in a logical error, the interviewer recorded the problem either in a comment field or on a problem report. Consistency checks were not applicable to the child assessments. 5-3"}, {"section_title": "Coding", "text": "Additional coding was required for some of the items collected in the CAI instruments by data preparation and project staff after an interview was completed. These items included \"Other, specify\" text responses, occupation, and race/ethnicity. Interviewers entered verbatim responses to these items. Data preparation staff were trained to code these data using coding manuals designed by Westat and the National Center for Education Statistics (NCES) to support the coding process. This section describes the coding activities for the CAI instruments. Review of \"Other, Specify\" Items. There were twenty-five \"Other, specify\" open-ended responses in the parent interview. All of these items were reviewed to determine if they should be coded into one of the existing response categories. During data collection, when a respondent selected an \"other\" response in the parent interview, the interviewer entered the text into a \"specify\" overlay that appeared on the screen. The data preparation staff reviewed these text \"specify\" responses and, where appropriate, coded them into one of the existing response categories. If a response did not fit into one of the existing categories, it remained in \"other.\" If there were numerous responses that were essentially the same, approximately one hundred, then a new code was added to the item. Three new codes were added to parent interview items during fifth-grade data processing as presented in table 5-1. The parent \"Other, specify\" coding system was revised from previous rounds of data collection and in production testing in April 2004. The revisions consisted of adding new \"Other, specify\" items that had not been part of the previous rounds; six items in the parent interview included an \"Other, specify\" code and were added to the system (see table 5-2 for new items). A total of 3,601 \"Other, specify\" text strings were processed through the parent \"Other, specify\" coding system. All possible upcodes were applied to the 10,901 cases that had at least one \"Other, specify\" text string. As noted above, whenever appropriate, responses were upcoded to existing categories. There were no \"Other, 5-4 specify\" items in the child assessments. Table 5-3 presents the number of text strings for each \"Other, specify\" item including the new ones added in fifth grade.   Manual-1980(U.S. Department of Commerce 1980 were used. Both of these manuals use an expanded coding system and at the same time are directly related to the much more condensed NHES coding scheme. These manuals were used for reference in cases where the NHES manual did not adequately cover a particular situation. Exhibit 5-1 describes the aggregated categories that were used for coding occupation in the ECLS-K. Exhibit 5-1. Aggregated occupation coding categories in the ECLS-K: School years 1998School years -99, 2001School years -02, and 2003 1. Executive, Administrative, and Managerial Occupations This category includes senior-level and middle management occupations and occupations that directly support management. Senior-level managers are persons concerned with policymaking, planning, staffing, directing, and/or controlling activities. Middle managers include persons who plan, organize, or direct and/or control activities at the operational level. Workers in this category are not directly concerned with the fabrication of products or with the provision of services. Other officials and administrators include consultants, library directors, custom house builders, and location managers. Legislators are also included in this category.\nThe hard-copy questionnaires required a quick visual review of particular questions in each questionnaire, coding of race/ethnicity for teachers, and review of \"Other, specify\" text responses. The quick visual review was to ensure that the questionnaire values accurately reflected existing categories, were complete and consistent across variables, and that the numbers were converted to the appropriate unit of measurement prior to converting data to an electronic format. Once the hard-copy questionnaires had been visually reviewed, they were coded. The coding staff was trained on the procedures and had manuals to support the coding process. Senior coders verified coding. The verification rate was set at 100 percent for each coder until accuracy of less than 1 percent error rate was established. After that point, work was reviewed at a rate of 10 percent. The \"Other, specify\" text responses were reviewed by the data editing staff and, where appropriate, upcoded into one of the existing response categories. The specify responses that remained after upcoding were reviewed to evaluate whether the addition of any new response categories would be appropriate. There was no need for the addition of new response categories in fifth grade."}, {"section_title": "Engineers, Surveyors, and Architects", "text": "The category includes occupations concerned with applying principles of architecture and engineering in the design and construction of buildings, equipment and processing systems, highways and roads, and land utilization."}, {"section_title": "Natural Scientists and Mathematicians", "text": "This category includes those engaged primarily in the application of scientific principles to research and development. Natural scientists are those in the physical sciences (e.g., chemistry, physics) and the life sciences (e.g., biology, agriculture, medicine). In addition, this category includes those in computer science, mathematics (including statistics), and operations research."}, {"section_title": "5-6", "text": "Exhibit 5-1. Aggregated occupation coding categories in the ECLS-K: School years 1998-99, 2001-02, and 2003 This category includes occupations concerned with the social needs of people and in basic and applied research in the social sciences. This category includes occupations concerned with the maintenance of health, the prevention of illness and the care of the ill through the provision and supervision of nursing care; compounding drugs, planning food service or nutritional programs; providing assistance to physicians; and the provision of therapy and treatment as directed by physicians 9. Writers, Artists, Entertainers, and Athletes This category includes occupations concerned with creating and executing artistic works in a personally interpreted manner by painting, sculpturing, drawing, engraving, etching, and other methods; creating designs for products and interior decorations; designing and illustrating books, magazines, and other publications; writing; still, motion picture and television photography/filming; producing, directing, staging, acting, dancing, singing in entertainment; and participating in sports and athletics as a competitor or player and administering and directing athletic programs."}, {"section_title": "Health Technologists and Technicians", "text": "This category includes occupations concerned with providing technical assistance in the provision of health care. For example, clinical laboratory technologists and technicians, dental hygienists, radiologic technicians, licensed practical nurses (LPNs), and other health technologists are included here 11. Technologists and Technicians, except Health This category includes those providing technical assistance in engineering and scientific research, development, testing, and related activities, as well as operating and programming technical equipment and systems."}, {"section_title": "Marketing and Sales Occupations", "text": "This category includes occupations involving selling goods or services, purchasing commodities and property for resale, and conducting wholesale or retail business."}, {"section_title": "5-7", "text": "Exhibit 5-1. Aggregated occupation coding categories in the ECLS-K: School years 1998-99, 2001-02, and 2003 This category includes occupations involving preparing, transcribing, transferring, systematizing, and preserving written communications and records; collecting accounts; gathering and distributing information; operating office machines and data processing equipment; operating switchboards; distributing mail and messages; and other support and clerical duties such as bank teller, data entry keyer, etc 14. Service Occupations The category includes occupations providing personal and protective services to individuals, and current maintenance and cleaning for building and residences. Some examples include food service, health service (e.g., aides or assistants), cleaning services other than household, and personal services."}, {"section_title": "Agricultural, Forestry, and Fishing Occupations", "text": "This category is concerned with the production, propagation (breeding/growing), gathering, and catching of animals, animal products, and plant products (timber, crop, and ornamental); the provision of services associated with agricultural production; and game farms, fisheries, and wildlife conservation. \"Other agricultural and related occupations\" include occupations concerned with the production and propagation of animals, animal products, plants, and products (crops and ornamental)."}, {"section_title": "Mechanics and Repairers", "text": "Mechanics and repairers are persons who do adjustment, maintenance, part replacement, and repair of tools, equipment, and machines. Installation may be included if installation is usually done in conjunction with other duties of the repairers."}, {"section_title": "Construction and Extractive Occupations", "text": "This category includes occupations that normally are performed at a specific site, which will change over time, in contrast to production workers, where the work is usually at a fixed location. Construction workers include those in overall construction, brickmasons, stonemasons, carpenters, electricians, drywall installers, paperhangers and painters, etc. Extractive occupations include oil well drillers, mining machine operators, and so on."}, {"section_title": "Precision Production Occupations", "text": "Precision production includes occupations concerned with performing production tasks that require a high degree of precision or attainment of rigid specification and operating plants or large systems. Examples are tool and die makers, pattern and model makers, machinists, jewelers, engravers, and so on. Also included are some food-related occupations including butchers and bakers. Plant and system operators include water and sewage, gas, power, chemical, petroleum, and other plant or system operators."}, {"section_title": "Production Working Occupations", "text": "This category includes occupations concerned with setting up, operating, and tending of machines and hand production work usually in a factory or other fixed place of business."}, {"section_title": "Transportation and Material Moving Occupations", "text": "This category includes occupations concerned with operating and controlling equipment used to facilitate the movement of people or materials and the supervising of those workers."}, {"section_title": "5-8", "text": "Exhibit 5-1. Aggregated occupation coding categories in the ECLS-K: School years 1998School years -99, 2001School years -02, and 2003 Occupation coding began with an autocoding procedure using a computer string match program developed for the NHES. The program searched the responses for strings of text for each record/case and assigned an appropriate code. A little over a third of the cases were autocoded (39 percent). Cases that could not be coded using the autocoding system were coded manually using a customized coding utility program designed for coding occupations. The customized coding utility program brought up each case for coders to assign the most appropriate codes. In addition to the text strings, other information, such as main duties, highest level of education, and name of the employer, was available for the coders. The coders used this information to ensure that the occupation code assigned to each case was appropriate. After the cases were coded (either manually or via autocoding), they were reviewed and verified. One hundred percent of the cases were verified. Verification of coding is an important tool for ensuring quality control and extending coder training. As a verification step, a second coder independently assigned codes (i.e., double-blind coding) to industry and occupation cases that had been initially coded either by the autocoding system or manually by a coder. A coding supervisor adjudicated disagreements between the initial code and the verification code. In the early stages, 100 percent of each coder's work was reviewed. Once the coder's error rate had dropped to 1 percent or less, 10 percent of the coder's work was reviewed. Fifteen percent of the cases that were autocoded required adjudication because the verifier disagreed with the autocoding. Thirty percent of the manually coded cases required adjudication because the manual coder and the verifier disagreed."}, {"section_title": "5-9", "text": "Table 5-4 summarizes the results of the coding and verification process for occupation coding. In the table, manually coded indicates that occupation was initially coded by a coder as opposed to using the autocoding system. Discrepancies are the count of disagreements between the autocoder and the verifier or between the manual coder and the verifier: the discrepant cases required adjudication. The percentage of times in which the coding supervisor disagreed with the coder's (or the autocoding system's) initial coding is referred to as the coder error rate. The percentage of times in which the coding supervisor disagreed with the verifier's coding is referred to as the verifier error rate. The denominator used in calculating these error rates is the number of cases verified. The error rate for manually coded cases was higher for coders (20.8 percent) than for verifiers (12 percent). The autocoded cases had a lower error rates for both coders (10.8 percent) and verifiers (5.5 percent) compared with the manually coded rates. 5-10"}, {"section_title": "Editing the Household Roster in the Parent Interview", "text": "The parent data edit system was modified from spring-third grade in April and May 2004 to reflect changes to the fifth-grade parent interview. The parent interview data were edited in two batches as the interviews were completed (see table 5-5). This was done to make the process more efficient. The first batch consisted of all cases received from the beginning of the round through May 20, 2004. The second batch consisted of cases completed from May 21 through the end of data collection. The first step in the editing process was to extract the household roster data and run the data edits. The second step was to apply the programmatic updates to the cases failing the edits to correct any errors programmatically. The third step was for an expert reviewer to manually review the cases, conduct as-needed discussions with NCES for resolution, and resolve and correct data errors. Six cases were lost from the data file after the expert review and discussion with NCES because the children had no data and were removed from the data file. Several tests were run on the household roster to look for missing or inaccurate information. There were essentially three general types of roster tests performed to determine which cases required editing. First, the relationship of an individual to the focal child was compared to the individual's listed age and sex. Problems found were corrected on the basis of data from prior data collections wherever possible. Second, households with more than one mother or more than one father were scrutinized for errors. While it was possible to have more than one mother in a household, such cases warranted closer inspection. Corrections were made whenever clear errors and a clear resolution existed. Last, the relationship of an individual to both the focal child and the respondent was examined, as there were cases 5-11 in which the relationship of an individual to the focal child conflicted with his status as the spouse/partner of the reference person. For example, in a household containing a child's grandparents but not his or her parents, the grandmother may be designated the \"mother\" figure, and the grandfather thus becomes the \"father\" (for the purposes of some questions in the interview) by virtue of his marriage to the grandmother. These cases were examined but left unchanged. Both the original-and correct (grandfather)-relationship data and the new \"parent-figure\" designation (father) that had been constructed were kept. Updates to the household roster were required when one or more of the edit checks described above failed. The data for the case would be inspected, resolved, and, if necessary, corrected. There were 435 households, with 446 children linked to them, completed a parent interview that required editing in fifth grade (identified by P6EDIT=1). There were 76 cases in which the interviewer noted in the Family Structure section (FSQ) that they had entered a person incorrectly in the household roster. These cases can be identified by the flag \"P6ERRFLG\" on the final data files. These \"error\" cases may or may not have also been edit cases, depending on whether or not the error triggered one of the edit checks."}, {"section_title": "5.2", "text": "Coding and Editing Specifications for Hard-Copy Questionnaires"}, {"section_title": "Receipt Control", "text": "In order to monitor the more than 40,000 documents that were to be received in the fifthgrade year, the project-specific receipt and document control system developed in the base year was used, with some modifications. The electronic receipt and document control system was initially loaded or filled with identifying information, such as identification numbers for schools, teachers, and children; the identification numbers linking teachers, children, and domain; and the questionnaires that were expected from each school and teacher for each cooperating school in the sample. As data were collected in the field, field supervisors completed transmittal forms for each school to indicate which questionnaires were being mailed to the home office. Once data collection started, receipt control clerks reviewed the questionnaires sent in from the field for accuracy and completeness. The identification number on each form was matched against the identification numbers in the tracking system to verify that the appropriate number of forms for each school was returned. Questionnaires that matched identification numbers in the tracking system were 5-12 receipted in the system and assigned a batch number. All receipted questionnaires matched identification numbers in the tracking system. Each questionnaire type had a different batch number so that questionnaires of the same type would be processed together. Processing data in batches was more efficient because it allowed staff to process in volume one type of questionnaire rather than switching from one type to another. Questionnaires with the same batch numbers were then compiled into batches of 25 instruments. The batch sheets with all the questionnaire identification numbers were printed from the system; the questionnaires in the batches were verified against the batch sheets. Verified batches were then sent for data entry. Once the batches had completed data entry, that is, all the questionnaires in a given batch had been keyed, the batches, accompanied by the electronic data, were returned to the data preparation department. The electronic data were loaded into the editing system and edited. At each point in the process, a flag was set in the receipt and document control system, which indicated the status of the instrument and the batch to which it was assigned. These statuses were the following: Cases pending (in edit system)."}, {"section_title": "Scan Edit Procedures", "text": "Critical items were identified for the school administrator questionnaire and the child-level reading, mathematics, and science teacher questionnaires. Prior to mailing the school administrator or child-level teacher questionnaires to Westat, the field supervisors reviewed them to ensure that critical items had been completed. If the critical items were missing, field supervisors attempted to retrieve them and recorded the outcome, completed or refused, in green pencil in the questionnaire. Exhibit 5-2 presents the critical items for these questionnaires. Prior to receipting returned questionnaires, trained clerks scanned each instrument for completeness and assigned a status code of \"Complete, No Data Retrieval,\" \"Complete, Data Retrieval Required, Complete,\" or \"Complete, Data Retrieval Required, Refused\" based on the results of field supervisors efforts. Questionnaires were then logged into the receipt and document control system and batched for data entry. Once questionnaires were logged in, the data were first keypunched into electronic format and than coded and edited. Questionnaires that contained no data due to refusal by the respondents were logged into the receipt and document control system as \"refusal.\" Table 5-6 presents data on the number of questionnaires receipted by week. The following sections describe the data entry, coding, and editing processes for hard-copy questionnaires."}, {"section_title": "5-14", "text": ""}, {"section_title": "Data Entry", "text": "Data entry consisted of two steps: (1) entering the data and (2) verifying that the data had been entered accurately. Westat data entry staff keyed the forms in each batch. A set of keying rules was established for each questionnaire and followed by the data entry staff. To verify the accuracy of the data 5-15 entry, more senior data entry operators then rekeyed 100 percent of the data. The results of the two data entry sessions were compared and differences identified. When differences were found, the hard-copy form was pulled and examined to determine what corrections, if any, had to be made to the keyed data. These corrections were rekeyed, resulting in an accuracy rate exceeding 99 percent. The verified batches were then transmitted electronically to Westat's computer system for data editing."}, {"section_title": "Data Editing Management Process", "text": "The management of the data editing process involved the creation of several data files, including the Collection Database, Holding Database, Editing Database, and Delivery Database. Exhibit 5-3 provides a diagram of the process described below."}, {"section_title": "5-16", "text": "Exhibit 5-17"}, {"section_title": "Collection Database", "text": "This database contained the keyed records for hard-copy questionnaires. One Collection Database was created for each instrument and as additional data were keyed, the cases were added to the database. The Collection Databases were Blaise databases. The ASCII file resulting from the key entry process was converted to Blaise data in the Collection Database so that they could be merged with the parent interview data and undergo additional data review (see section 5.4) Records in the Collection Databases were assigned status codes reflecting their current status. All new records were assigned a status of CollectionNew. When cases were copied to the Holding Database, the status was updated to CollectionCopied. The data in the Collection Database were retained in their original form; that is, they were not modified based upon later steps."}, {"section_title": "Holding Database", "text": "Data were copied from the Collection Database to the Holding Database for the editing process. The Holding Database for each instrument was also a Blaise database. The copied cases were assigned a status code of New. Cases that had already been involved in a prior editing cycle and had been returned to the Holding Database were assigned a status of CheckEdit or KeepAsIs. As the data were copied from the Collection Database to the Holding Database, a number of processes were run. Code-all-that-apply (COTA) recoding and \"yes/no\" recoding were applied. COTA recoding involved changing the multiple-response values of 0/1, 0/2, 0/3, etc., to a series of yes/no (1/2) responses. Yes/no recoding provided a means to resolve questions left unanswered in a series of yes/no items. If all marked answers were \"Yes,\" then the unanswered items were converted to \"No.\" However, if any item was \"No,\" \"Don't know,\" or \"Refused,\" all unanswered items were converted to -9 (Not ascertained). All blanks were converted to -9 (Not ascertained) and don't know and refused responses were converted to -7 and -8 as appropriate. It was at this stage that skip patterns were enforced using the Blaise CheckRules function and legitimate skips were assigned the standard code of -1. Edit programs (range and logical checks) were run against all cases contained in the Holding Database. As the editing process continued, the Holding Database contained both new cases copied from the Collection Database and edited cases returned from the Editing Database (see section 5.3.3). Each"}, {"section_title": "5-18", "text": "case was assigned a status code that reflected its current status. For cases that were new to the Holding Database, the CheckRules function assigned one of two codes. The status CleanNew was assigned to new cases that contained no edit (range or logical) errors. The status DirtyNew was assigned to new cases that failed one or more edit checks. Those cases that had undergone edit updating were also subjected to edit checks to identify any errors that remained or were inadvertently introduced during edit updating. The CheckRules function assigned the status CleanEdit to cases with no remaining errors. The status DirtyEdit was assigned to cases returned from edit updating that had remaining or new errors. Those cases that were assigned a status of KeepAsIs in a previous editing round were considered clean. Cases that were found to have edit errors (DirtyNew and DirtyEdit) were copied to the Editing Database for review and updating. At that time, their status in the Holding Database was set to InEdit. A face sheet was generated for each case with editing errors, giving the batch number, case ID, and edit rules that had been violated."}, {"section_title": "Editing Database", "text": "Cases in the Holding Database that failed edit checks were copied to the Editing Database for the correction of errors. As cases were copied to the Editing Database, they were assigned a status of WaitingForEdit in the Editing Database. Editing staff worked from face sheets produced during the edit checks conducted on the Holding Database to retrieve and correct case records. Using the batch number and case ID number, editors retrieved and reviewed hard-copy instruments as necessary to resolve editing errors. Once the editor had reviewed and updated each case as necessary, he or she assigned one of two outcome codes. The status code of WasEdited was assigned when all edit errors had been corrected. A status of KeepAsIs was assigned when the editor's review indicated that data that violated an edit check should be retained, for example, when the hard-copy instrument indicated that an out-of-range value was correct. Cases with the statuses of WasEdited and KeepAsIs were moved back to the Holding Database. Cases that had a status of WasEdited in the Editing Database were assigned the status CheckEdit in the Holding Database. The edit rules were applied to these cases to ensure that they were clean. As noted earlier, cases assigned a status of KeepAsIs in the editing process were considered clean."}, {"section_title": "5-19", "text": ""}, {"section_title": "Delivery Database", "text": "The main purpose of the Delivery Database was to store the instrument data at the school, teacher, or child level in a \"rectangular\" format consistent with downstream activities in preparation for data delivery. Cases for which editing and coding activities were completed were copied from the Holding Database to the Delivery Database. These were cases with status codes of CleanNew, CleanEdit, or KeepAsIs. When the data were copied to the Delivery Database, the \"Other, specify\" upcodes and parent interview occupation codes were applied. See exhibit 5-4 for a summary of the status codes assigned for data management databases. 5-20"}, {"section_title": "Data Editing", "text": "The data editing process consisted of running range edits for soft and hard ranges, running consistency edits in Blaise, and then manually reviewing frequencies of the results."}, {"section_title": "Consistency Checks (Logical Edits)", "text": "By programming logical edits between variables, consistency between variables not involved in a skip pattern was confirmed. For example, in the school administrator questionnaire, the number of children eligible for free breakfast could not exceed the total number of children enrolled in the school. These logical edits were run on the whole database after all data entry and range edits were complete. The logical edits were run separately for each form. All batches of data were combined into one large data file, and data frequencies were produced. The frequencies were reviewed to ensure the data remained logically consistent within the form. When an inconsistency was found, the case was identified and the inconsistency was printed for an editor to review. The original value was corrected (or checked and \"kept as is\" if the data item was confirmed, and the case was again run through the consistency edits. Once the case passed the consistency edits, it was returned to the main data set. The frequencies were then rerun and reviewed. This was an iterative process; it was repeated until no further inconsistencies were found. Table 5-7 shows hard-copy questionnaire data preparation production. More than 87 percent of all 5-21 questionnaires passed all the edits. The student record abstract accounted for 83 percent of all cases failing edits and, for all but a handful of cases, the data remained as it was reported. "}, {"section_title": "Teacher Responses to Key Child Items", "text": "In fifth grade, teachers of sampled children were asked to respond to child-level questionnaires for the reading, mathematics, and science domains. In many cases, teachers had more than one sampled child in a class. The items in the child-level questionnaire that collected information about classroom characteristics were redundant under these circumstances. The key child approach was designed to minimize the burden on the teachers by designating one questionnaire in which the classroom characteristics items were to be completed. See section 4.4.2 for a description of the key child design and procedures."}, {"section_title": "5-22", "text": "Once the child-level questionnaires were keyed and loaded into the editing system, a review was conducted to identify cases in which teachers reported classroom characteristics on a different questionnaire than the one designated as the key child instrument for the given class. This process involved three steps: the review of missing data for classroom characteristics items within each domain (reading, mathematics, and science) for key child records, a detailed review of all data records in classes with multiple children and missing values for selected classroom characteristics items, and the updating of appropriate records. In the first step, data records for key children in all classrooms with more than one sampled child were selected. Frequency distributions of the classroom items were examined for the level of missing data within each domain. All classroom characteristics items were included in this review. The results of this initial review indicated that missingness was largely confined to the items concerning the race and sex composition of the classroom. In the second step, all returned instruments were selected for classrooms with multiple children that had missing data for the race and sex composition items. These cases were reviewed to ascertain whether the teacher had mistakenly reported the classroom characteristics items on a questionnaire other than that designated for the key child. In the third step, update specifications were prepared, directing data preparation staff to apply the classroom characteristics data to the key child record for the classroom. Updates were made to 10 reading records, 5 mathematics records, and 3 science records as a result of this review. A review was also conducted to identify classrooms with multiple sampled children for which no key child instrument was returned. There were 5 such cases for reading, 7 such cases for mathematics, and 3 such cases for science. Another child for whom an instrument was returned was designated as the key child in these classrooms."}, {"section_title": "Frequency and Cross-Tabulation Review", "text": "As a final review, frequencies and cross-tabulations were run to determine consistency and accuracy within the various forms. If discrepancies could not be explained, no changes were made to the data. For example, in teacher questionnaire A, an item asking about languages other than English spoken 5-23 in the classroom included a response option of \"No language other than English.\" If a respondent circled that response, but also answered (in subsequent items) that other languages besides English were spoken in the classroom, then the response was left as recorded by the respondent because the discrepancy could not be resolved."}, {"section_title": "Creation of the Socioeconomic Status (SES) Variable", "text": "Socioeconomic status (SES) was computed at the household level using data for the set of parents who completed the parent interview in fifth grade. The SES variable reflects the socioeconomic status of the household at the time of data collection for fifth grade (spring 2004). The components used to create the SES were as follows: 1. Father/male guardian's education; 2. Mother/female guardian's education; 3. Father/male guardian's occupation;"}, {"section_title": "4.", "text": "Mother/female guardian's occupation; and 5. Household income. Occupation was recoded to reflect the average of the 1989 General Social Survey (GSS) prestige score. This was computed as the average of the corresponding prestige scores for the 1980 Census occupational categories covered by the ECLS-K occupation.  (Tourangeau et al. forthcoming). Table 5-9 shows that only a small percentage of values for the education and occupation variables were missing; a much larger proportion of households had missing values for the detailed income range. The total number of households in the third-grade data file (identified by the parents in the households) is 10,895, of which 77.8 percent are households with both parents present. Of the households with single parents, 10.6 percent are headed by single fathers.\nDetailed income category was brought forward from the most recent round of data collection prior to fifth grade. Second, data still missing after this initial step were imputed using a hot deck methodology. In hot deck imputation, the value reported by a respondent for a particular item was assigned or \"donated\" to a \"similar\" person who failed to respond to that question. Auxiliary information known for both donors and nonrespondents was used to form groups of persons having similar characteristics. These groups of similar respondents and nonrespondents are called \"imputation cells.\" The imputed value for a case with a missing value is taken from a randomly selected donor among the respondents within the cell. Imputation cells were defined by respondent characteristics that were the best predictors of the variables to be imputed. These relationships had been determined previously by CHAID (Chi-squared Automatic Interaction Detector) analyses of the base year data, as shown in table 5-11. Missing values for the education, occupation, and detailed income range variables were imputed by the hot deck method for all households. Hot deck imputation was done in a sequential order, separately, by type of household 5-28  (female single parent, male single parent, and both parents present). For households with both parents present, the mother's and father's variables were imputed separately. Imputed as well as reported values were used to define imputation cells; missing values for donor characteristics were treated as a separate category. No imputed value was used as a donor. No donor was used more than once.\nFather's labor force status;"}, {"section_title": "5-24", "text": "Table 5-9 also includes labor force status that was used to determine whether missing occupation data would be imputed. Individuals were defined as \"in the labor force\" if they were working at a paid job, on vacation from a paid job, or looking for a job. Occupation was imputed only for parents in the labor force. A description of the levels of the SES components can be found in table 5-10.  2 These numbers include parents with missing labor force status."}, {"section_title": "5-25", "text": "3 Detailed income range: $5,000 or less, $5,001-$10,000, $10,001-$15,000, $15,001-$20,000, $20,001-$25,000, $25,001-$30,000, $30,001-$35,000, $35,001-$40,000, $40,001-$50,000, $50,001-$75,000, $75,001-$100,000, $100,001-$200,000, more than $200,000. 4 Broad income range: $25,000 or less, $25,001 or more. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), spring 2004. 2,500 7,500 12,500 17,500 22,500 27,500 32,500 37,500 45,000 62,500 87,500 150,000 300,000 $5,000 or less $5,001 to $10,000 $10,001 to $15,000 $15,001 to $20,000 $20,001 to $25,000 $25,001 to $30,000 $30,001 to $35,000 $35,001 to $40,000 $40,001 to $50,000 $50,001 to $75,000 $75,001 to $100,000 $100,001 to $200,000 More than $200,000"}, {"section_title": "5-26", "text": ""}, {"section_title": "5-27", "text": "A two-stage procedure was used to impute missing values for each component of the SES composite variable. First, if a parent had completed an interview in the kindergarten, first-grade, or thirdgrade year, missing values for the fifth-grade education, occupation, and detailed income range were filled in with values from the previous years. The rationale for this approach was that the best source of data for an individual or a household was the data from a previous year. This first imputation stage was implemented as follows: Education level was brought forward from the most recent round of data collection prior to fifth grade, but only if the fifth-grade parent was the same person for whom the education information was reported in that previous round."}, {"section_title": "2.", "text": "Labor force status (whether the parent was in the labor force) was brought forward from the most recent round of data collection prior to fifth grade, but only if the fifthgrade parent was the same person who reported the labor force status information from that previous round. Even though labor force status is not a direct component of the SES, it cannot be missing since it was used to determine how to impute missing occupation information."}, {"section_title": "3.", "text": "Occupation was brought forward from the most recent round of data collection prior to fifth grade, but only if the fifth-grade parent was in the labor force (i.e., he or she was working at a paid job, on vacation from a paid job, or looking for a job) and was the same person who reported the occupation information from that previous round."}, {"section_title": "5-29", "text": "Occupation imputation involved two steps. First, the labor force status of the parent was imputed (i.e., whether the parent was employed). Then the parent's occupation was imputed only for those parents whose status was identified as employed either through the parent interview or the first 5-30 imputation step. The detailed income range was imputed in two steps: first for cases where the broad income range was known, and second for cases where it was unknown. For households where both parents were present, the order of hot deck imputation was as follows: 1. Mother's education; 2. Father's education; 3. Mother's labor force status;"}, {"section_title": "5.", "text": "Mother's occupation; 6. Father's occupation;"}, {"section_title": "7.", "text": "Detailed income range, where the broad income range was known; and 8. Detailed income range, where the broad income range was unknown. Following imputation, all values of the SES components were nonmissing including the value \u22121 for \"not applicable.\" Examples of not applicable cases are children with no father in the household, in which case the father's education and occupation are not applicable. Table 5-12 summarizes the imputation results. Tables 5-13 to 5-19 summarize the distribution of the records before and after imputation. The percentage columns may not always add to 100 percent due to rounding. 1 Occupation was not imputed if parent was not in labor force (whether labor force was filled in with data from previous round or imputed by hot deck). SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), spring 2004.        The values of each SES component were then normalized so that the component has a mean of 0 and a standard deviation of 1. This is also known as the z-score. For the h-th SES component, a zscore z hi for the i-th household was computed as"}, {"section_title": "5-31", "text": ""}, {"section_title": "5-32", "text": ""}, {"section_title": "5-33", "text": ""}, {"section_title": "5-34", "text": ""}, {"section_title": "5-35", "text": ""}, {"section_title": "5-36", "text": "is the value of the h-th SES component for the i-th household; w i is the base weight for the i-th record; Note that where h is household income, hi x is the natural log of the midpoint of the detailed income range. The log of the detailed income range midpoint does not vary widely within the levels of the detailed income range, so the midpoint was a reasonable choice. It was used only for the purpose of computing the SES composite and was not retained in the data file."}, {"section_title": "5-37", "text": "Thus, each component was converted to a z-score with mean of 0 and a standard deviation of one. The SES variable for the i-th household was computed as where m i is the number of nonmissing SES components for the i-th household. Table 5-20 shows the distribution of the SES values. As described, the SES composite is the average of up to five measures, each of which was standardized to have a mean of 0 and a standard deviation of 1, hence the negative values. Note that for households with only one parent present, not all the components were defined. In these cases, the SES was computed averaging the available components. In the fifth-grade data file, the continuous SES variable is W5SESL. A categorical SES variable (W5SESQ5) was created that contains the quintile for the value of the composite SES for the child. Quintile 1 represents the lowest SES category and quintile 5 represents the highest SES category. The quintiles were computed at the child level using the fifth-grade child-level parent weights. Unweighted frequencies for this variable are given in table 5-21.  5-39"}, {"section_title": "5-38", "text": ""}, {"section_title": "Imputation of the School Lunch Composites", "text": "The school lunch composites were computed at the school level for the set of public schools that have at least one child or parent respondent (i.e., the child has nonzero child weight, C6CW0, or nonzero child-level parent weight, C6PW0) in spring-fifth grade. There are two school lunch composites as follows: (1) Percent of children eligible for free school lunch; and, (2) Percent of children eligible for reduced-price lunch. For a description of how the data were collected and how the composites were computed, see section 7.5.4.6 of the Combined User's Manual for the ECLS-K Fifth-Grade Data Files and Electronic Codebooks (NCES 2006-032) (Tourangeau et al. forthcoming). Not all schools completed the school administrator questionnaire, and among those who did, not all responded to all three questions needed to compute the school lunch composites. Therefore, there were missing values for some of the components of the school lunch composite variables. Prior to fifth grade, if the source variables have missing value, then the composites were filled in with values computed using the most recent Common Core of Data (CCD) where available, or left missing. In fifth grade, the composites were computed as in the past, but if they had missing values, they were imputed. The source variables, however, were not imputed. Table 5-22 shows the level of missing data for the school lunch composite variables among the 2,008 public schools that have 9,328 child or parent respondents in the fifth grade of the ECLS-K. Similar to the components for the SES composite, a two-stage procedure was used to impute missing values for each school lunch composite variable. First, if a school had nonmissing value of the school lunch composite in the kindergarten, first-grade, or third-grade year, missing values for the springfifth grade school lunch composites were filled in with values from the previous years. The rationale for this approach was that the best source of data for a school was the data from a previous year."}, {"section_title": "5-40", "text": "Second, data still missing after this initial step were imputed using a hot deck methodology. In hot deck imputation, the values computed for a school for both composites were assigned or \"donated\" to a \"similar\" school that had missing values of the composites. Imputation cells were created using the Title I status of the school (hard boundary) and the school latitude and longitude (soft boundaries). The imputed value for a school with a missing value was taken from the nearest neighbor defined by latitude and longitude within the hard-boundary cell (Title I status). The Title I status is a derived variable using the data on whether the school received Title I funds (S6TT1) and whether Title I funds were targeted or used school-wide (S6TT1TA), both collected in the school administrator questionnaire. If these two variables have missing values for fifth grade, then the most recent available data (from third grade or first grade or kindergarten) were used. If these data were missing from the school administrator questionnaire for all rounds, then the information from the most recent CCD (2002-03) was used. The resolution of cases having missing data is shown for each school lunch composite in table 5-23 (for schools) and table 5-24 (for child-parent respondents). Schools that were imputed by hot deck are generally transfer schools with few sample children in those schools. This is reflected in tables 5-23 and 5-24 where the percent of children with hot-deck values of the school composites is much smaller than the percent of schools with hot-deck values of the school composites. Since children were designated as eligible for either free lunch or reduced-price lunch but not for both services, the two school lunch composites should sum to no more than 100 percent. A very small number of schools (less than 2 percent) had imputed values of the two school lunch composites summing to more than 100 percent. These values came from two sources: (1) from values reported by the school in another year, or (2) from the hot-deck imputation. The reporting error has been present in all rounds of the ECLS-K, and the decision was to keep the reported values in the data file. If the erroneous values came from the hot-deck imputation, then they were corrected so that the two school lunch composites do not add to more than 100 percent. Correction was made by capping the hot-decked values of the two lunch composite variables. This was done by multiplying each value by 100 divided by the sum of the two variables before capping. This way, both values were reduced by the same amount so they sum to 100 percent. Tables 5-25 to 5-28 show the characteristics of the school lunch composites before and after imputation, at the school level and at the child level.      "}, {"section_title": "5-41", "text": ""}, {"section_title": "5-42", "text": ""}, {"section_title": "5-43", "text": ""}, {"section_title": "5-44", "text": ""}, {"section_title": "5-45", "text": ""}, {"section_title": "5-46", "text": "This page intentionally left blank. 6-1"}, {"section_title": "RESPONSE RATES", "text": "This chapter describes the computation of unit completion rates for the spring-fifth grade data collection of the ECLS-K, and unit overall response rates for the base year respondents. Weighted and unweighted unit completion rates are presented for three groups of children: (1) children sampled in kindergarten, (2) children sampled in first grade through the freshening procedure, and (3) both groups combined. Completion rates for the fifth-grade data collection were computed with the same procedures used for spring-first grade and spring-third grade to allow for comparisons of completion rates for the three years of data collection following the base year. Item response rates for selected items from the ECLS-K fifth-grade instruments are also presented. For spring-first grade and spring-third grade, the sample of children is the same: base-year respondents (i.e., children who had either a fall-or spring-kindergarten child assessment or parent interview) and children sampled in spring-first grade as part of sample freshening as described in section 3.4.2. For spring-fifth grade, the sample of children was reduced to exclude base-year respondents who belonged in the following special groups as described in section 3.6: (1) children who became ineligible in an earlier round (because they died or moved out of the country), (2) children who were subsampled out in previous rounds because they moved out of the original schools and were not subsampled to be followed, (3) children whose parents emphatically refused to cooperate (hard refusals) in any of the data collection rounds since spring-kindergarten, and (4) children eligible for the third-grade sample for whom there are neither first-grade nor third-grade data. Among the 21,357 children who were eligible for the study after the base year, 16,143 were part of the fifth-grade data collection."}, {"section_title": "Definition of Response and Completion Rates", "text": "Response rates and completion rates are two ways to describe the outcomes of data collection activities. A response rate is the ratio of the number of units with completed interviews (for example, the units could be children, parents, schools or teachers) to the number of units sampled and eligible for the interview. The response rate indicates the percentage of possible interviews completed, taking all survey stages into account. On the other hand, the completion rate measures the percentage of interviews completed for a specific stage of the survey. For example, in the base year of the ECLS-K children were identified for assessment in a two-stage process. The first stage involved the recruitment of 6-2 schools to participate in the study. Preassessment visits were made to schools that agreed to participate. During the preassessment visit, field supervisors met with the participating school's school coordinator to enumerate and sample the kindergartners. Assessments were then conducted for the sampled children whose parents consented. If the school refused to participate in the study, no children were sampled for assessment. Under this design, the completion rate for the child assessment is the percentage of sampled children who completed the assessment. The response rate is the product of the school participation or cooperation rate and the child assessment completion rate. Response and completion rates can be either unweighted or weighted. The unweighted rate, computed using the raw number of cases, provides a useful description of the success of the operational aspects of the survey. The weighted rate, computed by summing the weights (usually the reciprocals of the probability of selecting the units) for both the numerator and denominator, gives a better description of the success of the survey with respect to the population sampled since the weights allow for inference of the sample data (including response status) to the population. Both rates are usually not very different unless the probabilities of selection and the response rates in the categories with different selection probabilities vary considerably. For example, the weighted completion rate of the ECLS-K child assessment (CA) is computed as where W i is the weight (inverse of the probability of selection of the child) for child i, and ER CA denotes eligible child assessment respondent and ENR CA eligible child assessment nonrespondent. To compute the unweighted rates, W i is set to 1 for each child. The response rate of the child assessment can be computed as where r S is the school cooperation rate and r CA is the child assessment completion rate."}, {"section_title": "6-3", "text": "After the base year, only completion rates were computed for the different ECLS-K instruments, since the response rates of the schools where the children were sampled remained the same for each subsequent round. Data users can compute the fifth-grade response rate for each ECLS-K instrument by multiplying the school response rate from the base year and the fifth-grade completion rate for each instrument. Both unweighted and weighted rates are presented in the tables of completion rates in this chapter. While unweighted rates are useful for evaluating sample performance as mentioned earlier, only weighted rates are discussed in the text."}, {"section_title": "Completion Rates", "text": "For the ECLS-K fifth-grade data collection, there were 11 survey instruments: child assessment; parent interview; school administrator questionnaire; facilities checklist; student records abstract, teacher-level questionnaire, subject-specific child-level questionnaires (reading, mathematics, and science); and special education teacher questionnaire part A and part B. The mathematics teacher questionnaire was completed for about half of the children in the fifth-grade sample and the science teacher questionnaire was completed for the other half, so that each child would have data from no more than 10 instruments. Except for the child assessment and the parent interview, all other instruments were paper-and-pencil instruments. For each instrument, completion rates were computed separately for children who were sampled as part of the kindergarten cohort in the base year and for children who were sampled in first grade through the student sample freshening procedure. While the completion rate for children sampled in the base year has only one component (to account for nonresponse attrition during data collection), the completion rate for children sampled in first grade has two components (to account for nonresponse attrition during the freshening procedure and during data collection). Section 6.2.2 describes in detail the two components of the completion rates for the freshened children. The two sets of rates were combined to obtain the completion rates for all children in fifth grade. Movers who were sampled to be followed but could not be located are considered nonrespondents and included in the denominator of the computation of completion rates. 6-4"}, {"section_title": "Children Sampled in Kindergarten", "text": "For the ECLS-K, a completion rate is a response rate conditioned on the results of an earlier stage of data collection. In the case of the ECLS-K fifth-grade data collection, the condition is that children who were sampled in kindergarten were base year respondents since only base year respondents were eligible for subsequent data collection efforts. Children sampled in first grade were exempt from this condition in the computation of completion rates. They are discussed in section 6.2.2. For each instrument, the unweighted completion rate is the proportion of base year respondents with completed data for the instrument to the base-year respondents who remain eligible to have the fifth-grade instrument administered. Base-year respondents who were subsampled out because they moved from their base-year original sample schools and base-year respondents who died or moved out of the country were not included in the denominator. For the weighted completion rates, the weight used is the product of the school base weight, the within-school child weight, and the factor that was used to adjust for movers between base year and fifth grade who were subsampled out for data collection. For a description of these weights and adjustment factor, see chapter 7. Tables 6-1 to 6-4 present weighted and unweighted child-level completion rates for springfifth-grade data collection, broken out by school characteristics. 1 These rates pertain to children who were sampled as part of the kindergarten cohort in the base year. In general, completion rates for fifth grade are higher than they had been for the third grade. This is due to the exclusion of hard-to-field cases from the fifth-grade collection. Hard-to-field cases are the hard-refusal cases and cases that were nonrespondents in both first and third grades as described in section 3.6. If these cases had not been excluded from the fifth grade, they would most likely be nonrespondents and would bring down the completion rates. Table 6-1 shows that the completion rates for the child assessment are quite high and uniform across school characteristics. Excluding the \"unknown\" category, the rates range from 93.1 percent in non-Catholic private schools to 99.7 percent in schools in large town. Similarly, the completion rates for the parent interviews were uniform across school characteristics ranging from 87.2 percent for children in schools with total enrollment of 750 or more, and in schools where 50 to 89 percent of 1 Children in schools with unknown characteristics are movers who could not be located (and considered as nonrespondents in the completion rates). Their weights are large because of the mover adjustment where movers who were followed carry the weight of movers who were subsampled-out for follow-up. The categories of school affiliation in the tables in this chapter do not match categories of school affiliation in the tables in chapter 3. This is to allow users to compare completion rates in fifth grade with those in previous years. children belong to the minority groups, to 94.3 percent for children in small towns (excluding the \"unknown\" category). The \"unknown\" category includes children who were unlocatable as their whereabouts were unknown and those children who had moved into a nonsampled county. The category \"unknown\" also includes 35 children who were homeschooled and thus had no information concerning schools."}, {"section_title": "6-5", "text": "The \"unknown\" category aside, both the child assessment and the parent interview completion rates increased between third grade and fifth grade for all school characteristics. The completion rates by mover status are discussed later, but the rates of completing all the instruments are much lower for children who moved than for those who did not move. Table 6-2 shows that the overall weighted completion rate is 77.1 percent for the school administrator questionnaire, and 78.8 percent for the facilities checklist. The rate for school administrator questionnaires is 11 percentage points higher than the corresponding rate in third grade. The rate for facilities checklist is only about 2 percent higher. Excluding the \"unknown\" category, the completion rates for the school administrator questionnaire range from 87.4 percent for schools with 750 or more students to 100 percent for those in large towns. Rates for the facilities checklist range from 90.3 percent for schools in the urban fringe of mid-size cities to 100 percent for schools in large towns. It is worth noting that the completion rates for the school administrator questionnaire are lower for schools with higher percentages of minorities, a phenomenon also observed in previous rounds for the school administrator questionnaire. However, this disparity decreased considerably after the base year, reflecting the success of increased data collection efforts targeted toward these schools. 6-8  Table 6-3 shows that the rates for the student records abstract are the lowest of all the instruments, as they always were in previous years of the ECLS-K. For fifth grade, this rate is about 70 percent compared with 67 percent in third grade. The \"unknown\" category aside, the completion rates of the student records abstract range from 71.8 percent in the northeast region to 93.7 percent for children in large towns. All four of the teacher questionnaires were completed at an overall rate of 78 to 80 percent (tables 6-3 and 6-4), much higher than the 62 to 63 percent range achieved in third grade. Excluding the \"unknown\" category, the completion rates for the teacher-level questionnaire in table 6-3 are uniform across school characteristics, ranging from 86.9 percent for schools in the northeast and schools with 750 or more students to 99.7 percent for schools in large towns. The same uniform rates are found for the subject-specific child-level teacher questionnaires in table 6-4: 85.7 to 99.7 percent for reading, 81.7 to 99.4 percent for mathematics, and 86.8 to 100 percent for science. These rates are higher than in any previous years of the ECLS-K, a change most likely attributable to the higher incentives employed in fifth grade. For a discussion of the incentives used in the ECLS-K, see section 4.5.6. As noted above, the rate at which the survey instruments were completed varies markedly by mover status and, within movers, by whether the child was located and followed. As shown in table 6-5, the completion rate for the child assessment was 98.2 percent for children still enrolled in their base year school. For movers it dropped by about 6 points to 91.9 percent for those who were located and followed, and for those not located or followed due to a move to a non-ECLS-K PSU, it was zero. The parent interview completion rates varied from 91.6 percent for nonmovers to 87.1 percent for movers who were located and followed for the purposes of the child assessment, to 85.7 percent for movers who could either not be located or were not followed for the purposes of the child assessment. Even though children who had moved to a non-ECLS-K PSU were not administered the child assessment, a parent interview was conducted by telephone wherever possible, leading to the 86 percent response rate for this category. The school administrator questionnaire completion rate is 15 points lower for movers, even when the children were located and followed; for the facilities checklist, it is 14 points lower (table 6-6)."}, {"section_title": "6-11", "text": "There are several reasons for this difference: located movers were not always assessed in schools; new schools in which movers enrolled had a lower level of commitment to the ECLS-K and often refused to complete the school administrator questionnaire; and some of these schools were contacted too late in the school year for them to consider completing it. The completion rate for nonmovers was 97.1 percent for the school administrator questionnaire and 98.8 percent for the facilities checklist. For located and followed movers it was 82.4 and 84.8 percent for the school administrator questionnaire and for the facilities checklist, respectively. The rates for the student records abstract are 90.1 percent for nonmovers and 72.3 percent for movers who were located and followed (table 6-7). The teacher-level questionnaire completion rate, as shown in table 6-7, is about 14 points lower for movers who could be located and followed (82.2 percent) than for nonmovers (96.5 percent). Movers who could not be located were all nonrespondents for this instrument, pulling the overall completion rate for movers downward to 63.9 percent. Table 6-8 shows the completion rates for all three child-level teacher questionnaires. These rates are between 95 and 96 percent for nonmovers, and between 80 and 82 percent for movers who were located and followed. Children who could not be located were all nonsrespondents for the child-level teacher instruments. The reasons for lower completion rates from teachers if the child moved are similar to the reasons that affected the school administrator questionnaire and facilities checklist completion rates for movers.  Tables 6-9 to 6-12 present child-level weighted and unweighted completion rates for the spring-fifth grade data collection for children who were sampled as part of the kindergarten cohort in the base year, this time broken out by child characteristics. When the \"unknown\" categories are not included, the differences in completion rates by sex and by year of birth are inconsequential, but for race and ethnicity they are more substantial. Table 6-9 shows that the child assessment completion rate was highest for Asians (87.6 percent) and lowest for American Indians or Alaska Natives (78.3 percent). For the parent interview it is the opposite; the rate was highest for American Indians or Alaska Natives (95.2 percent) and lowest for Asian children (82.8 percent). Table 6-10 shows that, excluding the \"unknown\" categories, the highest completion rates for the school administrator questionnaire and for the facilities checklist are for Pacific Islanders (85.7 percent and 86.7 percent, respectively), and the lowest completion rates are for American Indians or Alaska Natives (65.8 percent and 71.1 percent, respectively). Table 6-11 shows that the completion rate for the student records abstract is highest for children with \"other\" race (72.8 percent) and the lowest is for Black (63.9), excluding the \"unknown\" categories."}, {"section_title": "6-17", "text": "For the teacher-level questionnaire (table 6- In addition to the child assessment, parent interview, school administrator questionnaire, facilities checklist, student records abstract, and teacher questionnaires, whose completion rates have been summarized in the preceding paragraphs, data were also collected in fifth grade from the special education teachers for children who had special education teachers. Table 6-13 presents counts of completes and weighted and unweighted completion rates at the overall student level for the special education questionnaires A and B. The number of special education teacher questionnaires is small but 6-21 their completion rates are higher, 92.2 percent for part A, which captures teacher information, and 93.7 percent for part B, which relates to children who receive individualized special education services. These rates are not broken down by school and child characteristics because of the small sample sizes."}, {"section_title": "Children Sampled in First Grade", "text": "In spring-first grade, the student sample was freshened to include first graders who had no chance of selection in the base year because they had not attended kindergarten in the United States or were in first grade in the fall of 1998. For a detailed description of the freshening procedure see chapter 3, section 3.4.2. This same group of children was followed into spring-fifth grade, unless they belonged in the excluded groups. Nonresponse in the freshened student sample could occur at two stages: during the procedure for sampling schools for freshening and identifying children to be used as freshening links in spring-first grade (first component) and then during data collection from the freshened children in springthird grade (second component). The first component of the completion rate is the proportion of children sampled from the base year and subsampled for freshening for which the study was able to do freshening. The numerator includes all children available for freshening (i.e., those who did not move and in schools that cooperated with the freshening in first grade); the denominator includes children sampled for freshening (excluding movers not subsampled). For the weighted first component of the completion rate, both numerator and denominator were weighted by the product of the school base weight, the within-school child weight, and the freshening adjustment factor. The school base weight and the within-school child level weight reflect the multi-stage sampling of the ECLS-K design, while the freshening adjustment factor is necessary because schools were subsampled for freshening in first grade as described in section 3.4.2. These weights and adjustment factor are discussed in more detail in chapter 7. The first component alone can 6-26 further be decomposed into two sources: attrition due to the refusal of entire schools to implement the freshening procedure (the school term), and attrition due to ECLS-K sampled children moving to other schools (the child term). To contain costs, children who transferred from schools targeted for freshening were not used as links to identify freshened children, even when they were otherwise followed for data collection. These movers were considered freshening nonrespondents in the child term. The second component is the proportion of freshened children with completed data for the instrument from within the population brought into the sample by freshening. The weight for this component is the product of the school base weight, the within-school child weight, the school freshening subsample adjustment factor and the fifth-grade mover subsampling adjustment factor. The final completion rate is the product of the two components. For example, the final completion rate for the child assessment is computed as follows: where A is the set of children who could be freshened from (as described above), B is the set of children sampled for freshening, W 1i is the weight for child i for the first component as described above, W 2i is the weight for child i for the second component as described above, ER CA denotes eligible child assessment respondent, and ENR CA eligible child assessment nonrespondent. To compute the unweighted rates, W 1i and W 2u are set to 1 for each child. prepare an alphabetic roster of students enrolled in first grade. These schools were also requested to identify which students did not attend kindergarten the previous year. Schools did not participate in the In the first component, this is the completion rate for freshening. In the second component, this is the completion rate for the survey instruments. The product of the two components is the overall completion rate for the survey instruments. The freshening completes and completion rates for children in schools targeted for freshening. The freshening completes and completion rates for children in schools that agreed to the freshening procedure. The number of children sampled via the freshening procedure was 165 but not all were eligible for the fifth-grade data collection. Some became ineligible by the time of the fifth-grade data collection, and some were movers not subsampled for followup. The denominator of the completion rate also varies according to the instrument. For example, children who were homeschooled were not eligible to have school and teacher questionnaires. The denominator for each instrument can be obtained by dividing the number of completes by the unweighted completion rate. Reading, mathematics, or science assessment was scorable, or child was disabled and could not be assessed. Family structure portion of parent interview was completed. A completed questionnaire was defined as one that was not completely left blank. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), spring 2004."}, {"section_title": "6-28", "text": "freshening process because they either refused or were unable to provide the requested information. Within the schools that agreed to freshen, the freshening completion rate is 98.2 percent, the slight loss due to children who transferred to other schools (the child term). Multiplying these two terms together gives a first component completion rate of 66.0 percent. Note that the first component rate for spring-fifth grade is not identical to the first component rate for spring-first grade and spring-third grade because of the exclusion of children in special groups as explained in section 3.6. The second component varies by survey instrument. The rates for the paper-and-pencil instruments range from 67.0 percent for the student records abstract to 100.0 percent for the special education questionnaire part A. The rate for the child assessment at 78.6 percent is about 6 points lower than for the kindergarten sample, and the parent interview, at 81.9 percent, is about 7 points lower. The rates for the school instruments and the student records abstract are also lower than for the kindergarten sample, but by a smaller amount. The rates for some of the teacher instruments are higher than for the kindergarten sample. The larger difference in the parent interview rates, as compared with the difference in the rates for the child assessment, is due to the different number of followups given the two groups of parents. For parents of children sampled in first grade, the fifth-grade data collection was the second followup, while most parents of children sampled in kindergarten had at least four followups (springkindergarten, spring-first grade, spring-third grade, and spring-fifth grade). The number of followups does not affect the completion rate of the child assessment in the same way since the child assessment was done in the school for most children and hence could be perceived as part of school activities. The final completion rate for each instrument is the product of the two components. Because of the low rates at the first stage, these range from a high of 53.6 percent for the teacher-level questionnaire to a low of 44.2 percent for the student records abstract."}, {"section_title": "Spring-Fifth Grade Completion Rates for All Children", "text": "To compute the fifth-grade completion rate for the combined set of children sampled in the base year and children sampled in first grade, the fifth-grade completion rate for each group is weighted by the proportion of all children in that group, and the two weighted fifth-grade completion rates were 6-29 added together. For example, the weighted fifth-grade completion rate for the child assessment (CA) was computed as where BY denotes base year, 1ST denotes first grade, r CA,BY is the child assessment completion rate for children sampled in the base year, r CA,1ST is the child assessment completion rate for children sampled in first grade, and W i is the final weight (C6CW0 for the child assessment) for child i. To get the weighted fifth-grade completion rate for the child assessment (which is 84.7 percent for children sampled in the base year and 51.9 percent for children sampled in first grade), the weighted proportion of children who were sampled in the base year was 0.9762; the weighted proportion of children who were sampled in first grade was 0.0238. The fifth-grade weighted completion rate for the child assessment was 0.847\u00d70.9762+0.519\u00d70.0238=0.839, or 83.9 percent. grade represent such a small fraction of the total population of children, their inclusion in the computation of the completion rate brings down the rates for all children by less than one percent relative to the rates for children sampled in kindergarten, even though the completion rates for children sampled in first grade are much lower than the kindergarten rates. The spring-fifth grade overall completion rates for the child assessment and the parent interview are 83.9 percent and 88.3 percent, respectively. These rates are higher than in third grade by about 4 percentage points for the child assessment and by about 11 percentage points for the parent interview. In all tables, the unweighted completion rates are almost always higher than the weighted completion rates, by as much as 23 percent at the overall level. Where there is a large difference, it is due to movers who have larger weights and higher nonresponse rates than nonmovers. The weights of the movers were increased to account for the subsampling of movers. They also responded at a much lower rate than nonmovers, as shown earlier in tables 6-5 to 6-8. This difference is larger than in previous years because movers in fifth grade have much larger weights than in previous years (many more movers were not included in fifth grade, necessitating larger adjustment factors). Note that the unweighted completion rates follow the traditional ECLS-K pattern, that is, rates for the child 6-30 6-31 assessment are higher than rates for the parent interview (93.4 percent for the child assessment and 90.5 percent for the parent interview). This is again due to movers with large weights and to the fact that there are more parent-responding movers than child-responding movers. Thus, the weighted completion rates are higher for the parent interview than for the child assessment. Table 6-16 shows the completion rates for the child assessment, the parent interviews, and the school and teacher instruments for children who have nonzero child weights (C6CW0>0). These are children whose spring-fifth grade reading, mathematics, or science assessments were scorable, or children who could not be assessed because of disabilities. These conditioned completion rates are useful to analysts who want to assess the relationship between the different instruments in term of participation. The completion rates from the different instruments are dependent in that if data from one instrument are missing (e.g., parent instrument) it is likely that data from other instruments are also missing. (e.g., school administrator questionnaire). The conditioned completion rate for the child assessment is by definition 100 percent. The rate slightly less than 100 percent, shown when children sampled in kindergarten are combined with children sampled in first grade, is due to the school freshening nonresponse for children sampled in first grade. When the completion rates are conditioned on the presence of the child weight, they are at least 13 points higher than the unconditional completion rates for all instruments but the parent interview and the special education questionnaires. For these last two instruments, the difference between the number of completes for the conditional and unconditional rates is very small; hence the conditional rates are not affected as much as for the other instruments. For the parent interview, the unconditional rate is fairly high for the reason explained earlier; that is, movers in fifth grade have much larger weights than in previous years and there are more parent-responding movers than child-responding movers (the weighted completion rates are higher for the parent interview than for the child assessment). This results in the smaller difference between conditional and unconditional of about 4 percent. For all the other instruments, the conditional completion rates are higher by 13.6 points for the child-level science teacher questionnaire and as high as 17.2 points for the facilities checklist. As explained in section 3.6, four groups of children were excluded from the fifth-grade data collection. These are (1) children who became ineligible in an earlier round (because they had died or moved out of the country), (2) children who were subsampled out in previous rounds because they had moved out of the original schools and were not subsampled to be followed, (3) children whose parents emphatically refused to cooperate (hard refusals) in any of the data collection rounds since springkindergarten, and (4) children eligible for the third-grade sample for whom there are neither first-grade nor third-grade data. Table 6-17 shows the completion rates for all instruments had children in the last two exclusion groups been counted as nonrespondents. These are children who would have been eligible for the fifth-grade collection but past experience showed that they would most likely be nonrespondents."}, {"section_title": "6-32", "text": "When compared to table 6-15, the completion rates for all instruments in table 6-17 are lower, as expected, but only by about 2 percent, with the smallest difference for the student records abstract and largest difference for the parent interview. Note that the rates for mathematics and science teacher appear to be unchanged. Recall that only about half of the children had a mathematics teacher questionnaire filled out for them and the other half a science teacher questionnaire. Since the mathematics/science sampling flags were not assigned to children not included in the sample, we were not able to compute a correct completion rate for these two instruments for these children. But the pattern of completion rates would be the same as for the other instruments and we would expect a drop of about 2 percent for the mathematics and science teacher questionnaire."}, {"section_title": "Unit Nonresponse Bias Analysis", "text": "Among all the instruments administered in the fifth-grade data collection, the student records abstract (SRA) is the only instrument with an unweighted unit completion rate lower than 85 percent. The unweighted completion rate for this instrument is 82.1 percent for children sampled in kindergarten, 57.6 percent for children sampled in first grade, for an overall unweighted response rate of 81.9 percent (see table 6-15). When these rates were computed for children with scorable reading, mathematics or science assessment or children not assessed due to disabilities, the unweighted completion rate is much higher (88.8 percent overall as shown in table 6-16). An analysis was undertaken for the SRA to examine the potential nonresponse bias that may exist for this instrument. Methods for nonresponse bias analysis in this situation include (1) comparisons of respondents and nonrespondents using available sample frame, (2) multivariate analysis to identify the characteristics of cases most likely to respond, and (3) comparison of respondents to known population characteristics from external sources. "}, {"section_title": "6-34", "text": ""}, {"section_title": "6-35", "text": "There are 45 items in the SRA and all but seven are suppressed in the public-release data file. The suppressed items are dates when the child entered and left school, whether school kept attendance record, presence of IEP records, and items on learning disabilities. The seven items that are not suppressed are items on absence and tardiness. Because there are no school frames that report this type of data and there are no external sources that include these data, the nonresponse bias analysis is limited to analyzing the relationship between the SRA response status and response indicators such as key school characteristics. The Chi-square Automatic Interaction Detector (CHAID) was used to look at the relationship on variables with known values for both respondents and nonrespondents. CHAID is a classification algorithm that uses chi-square tests to divide the sample into subgroups that are related to whether the unit responds. The analysis in CHAID begins by dividing the sample into two or more groups based on the categories of the best predictor of response. Each of these groups is divided into smaller subgroups based on the available predictors at each level. The splitting process continues until no statistically significant predictor remains. The CHAID software displays the final subgroups in the form of a tree diagram whose branches correspond to the groups, showing all potential response predictors. The resulting classification tree reveals the response cells, as defined by combination of variables, which identify cells with the lowest response rate. In other words, CHAID divides the sample into cells so that the response rate within cells is as constant as possible, while the response rate between cells is as different as possible. The response indicators used in CHAID are school characteristics that are known for the respondents and nonrespondents to the SRA: census region, school affiliation, type of locale, total enrollment, and percent non-White enrolled. Exhibit 6-1 shows the CHAID tree when the SRA response status is analyzed together with these response indicators. Each box represents a group of SRAs with specified characteristics, the weighted completion rate and the number of fifth graders with completed SRA (i.e., SRA respondents). The first branch of the tree shows that census region is the first indicator that was used to divide the SRAs into four groups with response patterns that are very different from each other (with significance level of 0.05). The response rates in the four groups are 71.5 in the Northeast, 89.7 percent in the Midwest, 84.4 percent in the South, and 80.7 percent in the West. In the next branch of the tree, SRAs in the Northeast could be further split into four groups with very different response pattern by percent of non-White enrolled in the school, while those in the Midwest were subdivided into two groups with differential response patterns based on the type of 6-36 locale. Thus, the tree continues and ends after the sample was divided into 49 cells with varying response patterns. All variables used in the analysis show up in the tree as significant predictors of response patterns. Some are more prevalent than others. For example, school affiliation is not a significant indicator of response in the South while the total school enrollment is a significant predictor of response for all instances in the South. School affiliation is a significant response indicator in two groups of SRAs in the Midwest (SRAs from schools in large and mid-size cities/suburb of large and mid-size cities/large towns where the percent of non-White enrolled is less than 50 percent, and where the total school enrollment is less than 750; and, SRAs from schools in small towns and rural areas where the percent of non-White enrolled is less than 50 percent), and in the West (where schools have 11 or more but less than 50 percent of non-White enrolled in large and mid-size cities, small towns and rural areas, and where schools have 50 or more but less than 90 percent of non-White enrolled). School affiliation is significant in only one group in the Northeast (where schools have 10 percent or less non-White enrolled). The other variables (total school enrollment, percent non-White enrolled, and type of locale) are all good predictors of response. The lower completion rates are in the Northeast, in schools that are nonreligious private and in schools with high enrollment Potential nonresponse bias that may exist is likely to have been lessened by the weighting adjustment procedures described in chapter 7. School affiliation, type of locale and region were used as raking dimensions in the last step of the weighting procedures for all weights including the child-level weight C6CW0, the most appropriate weight to use in the analysis of the SRA data."}, {"section_title": "Item Response Rates", "text": "In the ECLS-K, as in most surveys, the responses to some data items are not obtained for all interviews. There are numerous reasons for item nonresponse. Some respondents do not know the answer for the item or do not wish to respond for other reasons. Some item nonresponse arises when an interview is interrupted and not continued later, leaving items at the end of the interview blank. Item nonresponse may also be encountered because responses provided by the respondent are not internally consistent, and this inconsistency is not discovered until after the interview is completed. In these cases, the items that were not internally consistent were set to missing."}, {"section_title": "6-37", "text": "Exhibit 6-1. Relationship between the student record abstract response status and school characteristics: School year 2003-04 "}, {"section_title": "6-41", "text": "Every item in the ECLS-K data file has values that indicate whether the respondent did not know the answer to the item (-8), or refused to give an answer (-7). The value -9 is used in all other cases where the answer is left blank or set to missing due to reasons mentioned above (described in the data file as \"Not ascertained\"). However, where an item is left blank due to a valid skip pattern, this is indicated by the value \u22121. Chapter 7 of the ECLS-K Combined User's Manual for the ECLS-K Fifth Grade Public-Use Data Files and Electronic Codebooks. (NCES 2006-032) (Tourangeau et al. forthcoming) discusses in detail these special values. For each survey item, the response rate was computed as the unweighted number of responses not equal to any of the special values (\u22121, \u22127, \u22128, or \u22129) divided by the unweighted number of responses not equal to \u22121. Of all the ECLS-K instruments, only the child assessment and the parent interview have a sizable number of items with special values \u22127 (\"Refused\") or \u22128 (\"Don't know\"). Table 6-18 shows the unweighted distribution of the nonresponse values for each instrument.     "}, {"section_title": "6-43", "text": ""}, {"section_title": "6-48", "text": "This page is intentionally left blank. 7-1"}, {"section_title": "WEIGHTING AND VARIANCE ESTIMATION", "text": "The ECLS-K data were weighted to compensate for differential probabilities of selection at each sampling stage and to adjust for the effects of nonresponse. In the ECLS-K base year, weights were computed at the child, school and teacher levels. Estimates using the base year weights are representative of all kindergarten children, all schools with kindergarten programs and all kindergarten teachers. After the base year, only child-level weights were computed. The use of these weights was essential to produce estimates that are representative of the cohort of children who were in kindergarten in 1998-99 or in first grade in 1999-2000. In first grade, the sample was freshened with first-graders who had not been sampled in kindergarten or first grade. Similarly, home environment data were collected from parents. Data from these sources are not representative of all fifth-grade parents, teachers, and schools in 2003-04. For this reason, the only weights produced from the study are child-level weights for making statements about children, including statements about the parents, teachers, and schools of those children. The different types of weights are discussed in section 7.1, followed by a detailed description of the computation of the weights in section 7.2. Section 7.3 describes the variance estimation methods suitable for the ECLS-K."}, {"section_title": "Types of Weights", "text": "Two sets of weights were computed for fifth grade, cross-sectional and longitudinal. The use of these weights is essential to produce estimates that are representative of the cohort of children who were in kindergarten in 1998-99 or in first grade in 1999-2000. Since the ECLS-K sample was not freshened after the first-grade year with third-or fifth-graders who did not have a chance to be sampled in As in previous years, there are several survey instruments administered to sampled children and their parents, teachers and schools: cognitive and physical assessments for children; self-description child questionnaire (third and fifth grade only), parent instruments; several types of teacher instruments completed by reading, mathematics, science and special education teachers; and school instruments. The stages of base year sampling in conjunction with differential nonresponse at each stage and the diversity of survey instruments require that multiple fifth-grade cross-sectional sampling weights be computed for use in analyzing the fifth-grade ECLS-K data. Several combinations of kindergarten through fifth-grade longitudinal weights were also computed. Exhibit 7-1 summarizes the different types of cross-sectional weights."}, {"section_title": "7-3", "text": "Exhibit 7-1. ECLS-K fifth-grade cross-sectional weights: School year 2003-04"}, {"section_title": "Weight", "text": "To be used for analysis of ...\nTo be used for analysis of ..."}, {"section_title": "C6CW0", "text": "Fifth-grade direct child assessment data, alone or in conjunction with any combination of (a) a limited set of child characteristics (e.g., age, sex, race/ethnicity), (b) teacher-level data from any fifth-grade teacher questionnaire without child-level teacher data, or (c) data from the school administrator questionnaire or school facilities checklist."}, {"section_title": "C6PW0", "text": "Fifth-grade parent interview data alone or in combination with (a) fifth-grade child assessment data, (b) data from any fifth-grade teacher questionnaire (teacher-level or child-level), or (c) data from the school administrator questionnaire or school facilities checklist. Exception: If data from the parent AND child assessment AND teacher (child-and/or teacher-level) are used together, then either C6CPTR0, C6CPTM0 or C6CPTS0 should be used."}, {"section_title": "C6CPTR0", "text": "Fifth-grade direct child assessment data combined with fifth-grade parent interview data AND fifth-grade teacher-level data with or without child-level data from the reading teacher, alone or in conjunction with data from the school administrator or facilities checklist."}, {"section_title": "C6CPTM0", "text": "Fifth-grade direct child assessment data combined with fifth-grade parent interview data AND fifth-grade child data from mathematics teacher (with or without teacher-level data), alone or in conjunction with data from the school administrator or facilities checklist."}, {"section_title": "C6CPTS0", "text": "Fifth-grade direct child assessment data combined with fifth-grade parent interview data AND fifth-grade child data from science teacher (with or without teacher-level data), alone or in conjunction with data from the school administrator or facilities checklist. The ECLS-K longitudinal file is created by merging data from the base year, first grade, third grade, and fifth grade. Longitudinal weights were created to use in analyzing data in this longitudinal file. These weights are described in exhibit 7-2. All longitudinal weights are child-level weights."}, {"section_title": "7-4", "text": "Exhibit 7-2. ECLS-K: K-5 longitudinal weights, spring-fifth grade: School year 2003-04"}, {"section_title": "C56CW0", "text": "child direct assessment data from BOTH spring-third grade and spring-fifth grade, alone or in conjunction with any of the school or teacher data, or a limited set of child characteristics (e.g., age, sex, and race/ethnicity)."}, {"section_title": "C56PW0", "text": "parent interview data from BOTH spring-third grade and spring-fifth grade, alone or in conjunction with any of the child assessment, school, or teacher data."}, {"section_title": "C456CW0", "text": "child direct assessment data from spring-first grade AND spring-third grade AND springfifth grade, alone or in conjunction with any of the school or teacher data, or a limited set of child characteristics (e.g., age, sex, and race/ethnicity)."}, {"section_title": "C456PW0", "text": "parent interview data from spring-first grade AND spring-third grade AND spring-fifth grade, alone or in conjunction with any of the child assessment, school, or teacher data."}, {"section_title": "C2_6FC0", "text": "child direct assessment data from FOUR rounds of data collection involving the FULL sample of children (spring-kindergarten, spring-first grade, spring-third grade, and spring-fifth grade), alone or in conjunction with any of the school or teacher data, or a limited set of child characteristics (e.g., age, sex, and race/ethnicity)."}, {"section_title": "C2_6FP0", "text": "parent interview data from FOUR rounds of data collection involving the FULL sample of children (spring-kindergarten, spring-first grade, spring-third grade, and spring-fifth grade), alone or in conjunction with any of the child assessment, school, or teacher data."}, {"section_title": "C1_6FC0", "text": "child direct assessment data from FIVE rounds of data collections involving the FULL sample of children (fall-kindergarten, spring-kindergarten, spring-first grade, spring-third grade, and spring-fifth grade), alone or in conjunction with any of the school or teacher data, or a limited set of child characteristics (e.g., age, sex, and race/ethnicity)."}, {"section_title": "C1_6FP0", "text": "parent interview data from FIVE rounds of data collections involving the FULL sample of children (fall-kindergarten, spring-kindergarten, spring-first grade, spring-third grade, and spring-fifth grade) alone or in conjunction with any of the child assessment, school, or teacher data."}, {"section_title": "C1_6SC0", "text": "child direct assessment data from ALL SIX rounds of data collection (fall-kindergarten, spring-kindergarten, fall-first grade, spring-first grade, spring-third grade, and spring-fifth grade), alone or in conjunction with any of the school or teacher data, or a limited set of child characteristics (e.g., age, sex, and race/ethnicity)."}, {"section_title": "C1_6SP0", "text": "parent interview data from ALL SIX rounds of data collection (fall-kindergarten, springkindergarten, fall-first grade, spring-first grade, spring-third grade, and spring-fifth grade), alone or in conjunction with any of the child assessment, school, or teacher data. "}, {"section_title": "7-5", "text": "Each set of weights created to be used with the ECLS-K data consists of a full sample weight that is used in computing survey estimates and replicate weights that are used in variance estimation with a jackknife replication method. First-stage stratum and primary sampling unit (PSU) identifiers are also created so that variance estimation using the Taylor series approximation method can be produced using the full sample weights. See section 7.2.6 for a description of how replicate weights were created. Section 7.3 discusses variance estimation methods. The data file includes the final full sample weight (described in section 7.2.4) and the final replicate weights (described in section 7.2.6) but not the intermediate weights leading to the final weights. The names of the full sample weights in the file are as described in exhibits 7-1 and 7-2 (e.g., C6CW0). The names of the replicate weights have the same prefix as the full sample weight with the last digit indicating the replicate (e.g., C6CW1 to C6CW90 are the 90 replicate weights to be used with the full sample weight C6CW0)."}, {"section_title": "Computation of the Fifth-Grade Weights", "text": "Among the 21,357 children who were eligible for the study after the base year (21,292 base year respondents and 165 children sampled in first grade), the fifth-grade sample excluded 5,214 children as explained in section 3.6.1. As in third grade, only a subsample of children who moved from the schools they were attending when they were sampled originally was followed into their new schools. However, children who moved into a destination school 1 because they had completed the highest grade at the originally sampled school were all followed. The fifth-grade subsampling of movers continues to give more weight to children in the language minority group (i.e., movers in this group were subsampled at higher rates than non-language minority movers). Other smaller groups of movers were also subsampled at lower rates, such as selected groups of movers who were sampled in first grade (as compared with base year respondent movers), and movers who did not have full longitudinal data. Differential sampling rates of movers are presented in section 3.6.1. Another feature of the fifth-grade sample is the subsampling of children for the administration of the mathematics or science questionnaires as discussed in section 3.6. In the weighting procedures, children excluded from the fifth-grade data collection are considered ineligible if they became ineligible in an earlier round (because they had died or moved out of the country), as movers not subsampled for follow-up if they were subsampled out in previous rounds 1 A destination school is a school that received at least four students from the school where they had just completed the highest grade."}, {"section_title": "7-6", "text": "because they moved out of the original sample, or of unknown eligibility if they were hard refusal cases or if they had neither first-grade nor third-grade data. Excluded children are properly adjusted for in the weighting procedures. The weighting procedures for both cross-sectional and longitudinal weights are similar, although weighting cells were defined differently for each type of weight. For example, any longitudinal weight that contains data from fall-first grade may have used different cells due to sample size constraints. The weighting procedures for the fifth grade were divided into three main stages. These procedures were followed for creating each weight shown in exhibit 7-1 and exhibit 7-2. The change in the procedures pertains only to the change in the eligibility of children for whom the weight applies. For example, weight C6CW0 pertains to children with completed assessments in fifth grade; weight C56PW0 pertains to children with completed parent interview in both third grade and fifth grade. In the base year, children who were not assessed because of a disability or because they were language minority children had positive C1CW0 and C2CW0 weights because they had data such as age, sex, race/ethnicity, height and weight, and characteristics of parents, teachers, and classrooms. In subsequent rounds of data collection, they continued to be treated the same. Weights that include any fall-first grade data (such as C1_6SC0, which is the weight for children for whom child assessments were obtained in all six rounds) were computed using the same procedures, but the cells for the weighting adjustments were more restricted because only the fall-first grade subsample was included. The replication scheme for data that include the fall-first grade panel is also different as described in section 7.2.6. The first stage of weighting was to compute an initial child weight that reflects the following: Adjustment of the school base weight for base year school-level nonresponse; Adjustment of the child weights for base year child-level nonresponse; and Adjustment of the base year child weight for subsampling of schools for freshening in first grade (for children sampled in first-grade only)."}, {"section_title": "7-7", "text": "The procedures used in this first stage are the same as in all rounds of data collection after the base year because the same sample of children (base year respondents and children sampled in first grade) is eligible for subsequent rounds of data collection. The second stage of weighting was to adjust the initial child weight computed in the first stage for the following:"}, {"section_title": "Subsampling of movers; and", "text": "Child-level nonresponse. For the mathematics and science child-parent-teacher weights, an additional adjustment was necessary (before the second stage adjustment for the subsampling of movers and for nonresponse) to adjust for the subsampling of children for whom mathematics or science teacher data questionnaires were administered. This adjustment is described in section 7.2.5. The third and last stage was to rake the weights adjusted in the second stage to sample-based control totals. Raking is a multivariate poststratification of the weights, explained in section 7.2.4. In general, in each adjustment to the weight, the adjustment factor is multiplied by the weight in the prior step to get the adjusted weight. This fact is not repeated in the discussions of the weight adjustments in the following sections; only the computation of the adjustment factor is discussed."}, {"section_title": "Initial Child Weights", "text": "As mentioned earlier, the first stage of weighting was to compute an initial child weight that reflects: (1) the adjustment of the school base weight for base year school-level nonresponse (school-level weights), (2) the adjustment of the child weights for base year child-level nonresponse (child-level weights), and (3) the adjustment of the base year child weight for subsampling of schools for freshening in first grade (child-level weights, for children sampled in first grade only). These weights were already computed for spring-first grade. For completeness, they are described below, in section 7.2.1.1 for the school-level weights, and in section 7.2.1.2 for the child-level weights. where c BY_R denotes the set of base year child respondents in cell c, and BY_NR c denotes the set of base year child nonrespondents in cell c. The base year child weights were adjusted using weighting classes similar to those developed for the cross-sectional spring-kindergarten child weights. These classes were created with CHAID, using the school characteristics from the school nonresponse adjustments (i.e., school affiliation, locale, region, school enrollment classified into size category), and a set of child characteristics (i.e., year of birth, sex, and race/ethnicity). Data on year of birth were obtained from the parent interviews, while data on sex and race/ethnicity were from the child sampling information, which was provided by the schools. If year of birth was missing from the parent interview, then it was taken from the child sampling information. If sex or race/ethnicity was missing from the child sampling information, then they were obtained from the parent interview data. Any remaining missing data were imputed with the modal value from the school from which the child was sampled for this purpose."}, {"section_title": "Base Year Child Weights for Children Sampled in First Grade", "text": "In spring-first grade the student sample was freshened to include first-graders who had not been enrolled in kindergarten in 1998-99 and, therefore, had no chance of being included in the ECLS-K base year kindergarten sample. For this group of children who entered the sample in first grade, their weights need to have additional adjustments to account for the freshening procedure. See chapter 3 for a discussion of the student freshening in spring-first grade. Since each child sampled in first grade was directly linked to a child sampled in kindergarten, the first step was to compute a weight for the children who were sampled in kindergarten that reflects the school freshening subsampling and the school freshening nonresponse (some schools refused to provide the complete alphabetical roster of all students enrolled in first grade needed for freshening). This weight was then linked back to the child sampled in first grade and further adjusted for nonresponse because the data (e.g., assessment data, parent interview data) had not been obtained from 7-13 the sample of freshened children. The procedures for computing the base year child weights for children sampled in first grade are described next. , the adjustment factor, was computed as where c F denotes the set of schools subsampled for freshening, and c F denotes the set of schools not subsampled for freshening. This adjustment was done within cells defined by school affiliation (public, Catholic private, non-Catholic religious private, or nonreligious private) and census region (Northeast, Midwest, South, or West). Adjustment cells were created using CHAID. School Weight Adjusted for Freshening Nonresponse. The freshening procedure could not be applied in all designated schools because some schools did not provide the information needed for freshening (see chapter 3 for more details on the freshening procedures). These schools were considered nonrespondents. The school weight adjusted for freshening school-level nonresponse, In both the numerator and denominator of this factor, the school measure of size (i.e., the count of students in the school as described in section 3.2.2.2) was incorporated; the school measure of size is relevant because the weights will be used for child-level estimates, not school-level estimates. The nonresponse cells for this adjustment were created with CHAID using school affiliation (public, Catholic private, non-Catholic religious private, or nonreligious private) and type of locale (large city, midsize city, suburb of large city, suburb of midsize city, large town, small town, or rural area). Base Year Child Weight. Next, the school-adjusted weight was multiplied by the inverse of the probability of sampling the child in the base year to obtain a base year child weight for freshening. The base year child weight was is the within-school child selection probability. The base year child weight was then adjusted for base year child nonresponse because children who did not respond in the base year could not be linked to children in first grade in spring 2000. where c BY_R denotes the set of base year child respondents in cell c, and BY_NR c denotes the set of base year child nonrespondents in cell c. The nonresponse cells were created with CHAID using the school characteristics school affiliation, locale, region, and school enrollment size, and the child characteristics age, sex, and race/ethnicity. Base Year Child Weights Adjusted for Movers. Only children who did not move from their original school were designated as links to children in the freshening procedure. The children who moved and were followed into their new schools were not identified to participate in the freshening process in their new schools. As a result, all children who moved were considered nonrespondents for the freshening process. Additionally, nonmovers and movers who were not in first grade were not eligible for freshening (e.g., if the child was in kindergarten in spring 2000, he or she would be linked only to other kindergarten children and thus was not eligible for the freshening of first-graders). An adjustment was necessary to account for these two groups of children and was done in two steps. In the first step, an adjustment was made for movers whose grade was unknown. A portion of the movers was assumed to be in first grade. In the second step, the weights were adjusted for children who were in first grade and who were not identified to participate in the freshening process because they moved into a new school. For this two-step adjustment, each child was classified as in table 7-2. "}, {"section_title": "7-16", "text": ""}, {"section_title": "7-17", "text": "This two-step adjustment was done within cells defined by school affiliation and census region. The weights thus created for children sampled in kindergarten were then linked to the children that they brought into the sample in first grade through sample freshening. In other words, the weight of the child sampled in first grade was defined at this point to be the weight computed for the child sampled in kindergarten that was responsible for bringing the first-grader into the sample. For the next step in the computation of the fifth-grade child weights, the two groups of children-base year respondents and children sampled in first grade through sample freshening-were put together, and a common variable and label were used to designate the initial child weight. This is the base year child weight as computed above for each group of children: "}, {"section_title": "ABYCHLDW ICHLDW", "text": "The initial child weights i ICHLDW were adjusted for movers between the base year and fifth grade and for nonresponse in fifth grade, and raked to sampled-based control totals to obtain the final fifth-grade child weights. These adjustments and raking procedures are described below."}, {"section_title": "Adjustment for Movers Between the Base Year and Fifth Grade", "text": "First, the initial child weights were adjusted to reflect the subsampling of movers. In the ECLS-K, a child could move more than once and at different times. For example, a child could move out of his original sample school because the school did not have grades higher than kindergarten. Then he could move again between first and third grade, first and fifth grade, or third and fifth grade. Once a child was identified as a mover, he stayed a mover unless he moved back to the original sample school. For example, a child who moved between kindergarten and third grade, but stayed in that same school between third and fifth grade, was considered a mover for the fifth grade. Each mover in the fifth grade had a flag indicating whether he was followed into the new school. These flags were set according to the mover subsampling plan described in section 3.6.1. Children who were excluded from the fifth-grade data collection because they had moved out of the original 7-18 schools and were subsampled out for followup in previous rounds had their flag set to \"not followed.\" In fifth grade, children were fielded as described in exhibit 7-3.  1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. The initial child weight described in section 7.2.1 was adjusted to reflect this subsampling movers. The initial child weight adjusted for movers i CHILDW R  1  6 was computed as For the cross-sectional weights, the mover adjustment factor was computed within cells created with CHAID using the following characteristics: whether children were sampled in kindergarten 7-19 or first grade, and whether they were language minority children. 4 For the longitudinal weights, a longitudinal mover follow status was created that takes into account whether the child moved from his original school in fall-first grade, spring-first grade, or spring-third grade (for longitudinal weights involving the fall-first grade data) or whether the child moved from his or her original school in springfirst grade or spring-third grade (for the other longitudinal weights). If a child moved in either round, he was considered a mover. Appendix A gives the cell definitions for the mover adjustment for crosssectional and longitudinal weights. Twelve children with large weights had their weights trimmed by 40 percent. The trimming procedure was the same as in previous years. However, the weights were not redistributed because the total sum of weights was re-established in the raking procedure that came later."}, {"section_title": "Adjustment for Fifth-Grade Nonresponse", "text": "After the adjustment for subsampling movers, the child weights were adjusted for nonresponse. As in spring-first grade and spring-third grade, the nonresponse adjustment was done in two steps. In the first step, the adjustment was for children whose eligibility was not determined (unknown eligibility). A portion of children of unknown eligibility was assumed to be ineligible, equal to the proportion of children of known eligibility who are ineligible. In the second step, the adjustment was for eligible nonrespondents. To carry out these adjustments, each child was classified as (a) an eligible respondent, (b) an eligible nonrespondent, (c) ineligible (out of the country or deceased) or (d) of unknown eligibility (mover who could not be located), as shown in table 7-3.  4 Fewer characteristics were used than in previous years to create cells for mover adjustments. This is due to cells with a small number of records, requiring them to be collapsed in order to avoid large adjustment factors. This resulted in fewer cells, hence fewer characteristics being used. In both steps of the adjustment, separate nonresponse classes were created for movers and nonmovers with CHAID using various combinations of response status of child assessments and parent interviews in the base year as well as whether children belong to the language minority group, the type of household collected from the parent interviews (all cross-sectional weights except C6CW0), and the school affiliation including whether the child was homeschooled (C6CPTR0, C6CPTM0 and C6CPTS0 only). The adjustment cells for each type of weight are shown in appendix B. Where applicable, very large nonresponse adjusted weights were trimmed by 40 percent. As before, the weights were not redistributed in each case because the total sum of weights was re-established in the raking step that came next."}, {"section_title": "7-21", "text": ""}, {"section_title": "Raking to Sample-Based Control Totals", "text": "To reduce the variability due to the subsampling of schools and movers, the child weights were then raked (i.e., calibrated) to sample-based control totals computed using the initial child weights described in section 7.2.1. A file was created with the initial child weights and school and child characteristics collected in the base year or first-grade year (such as school affiliation, region, urbanicity, sex, age, race/ethnicity, SES, language minority status, whether sampled in kindergarten or first grade, and if sampled in kindergarten, mover status in spring-first grade) to be used in the computation of the This raking procedure is essentially a multivariate poststratification. Raking cells (also known as raking dimensions because they typically involve more than one variable, for example, sex by age) were created with CHAID using school and child characteristics collected in the base year or firstgrade year: school affiliation, region, type of locale, sex, age, race/ethnicity, socio-economic scale (SES), language minority status, whether sampled in kindergarten or first grade, and if sampled in kindergarten, 7-22 mover status. These characteristics come from the fifth-grade data collection or previous rounds of data collection if fifth-grade data are missing. Appendix C gives the raking dimensions used for fifth grade. There was no restriction set in the number of iterations during the raking procedure. The procedure was allowed to run until complete convergence was achieved within a control total. This occurred after 12 to 19 iterations."}, {"section_title": "Additional Adjustment for Child-Parent-Teacher Cross-Sectional Weights", "text": "A new feature of the fifth-grade sample is the subsampling of children for the administration of the mathematics and science teacher questionnaires. While all children had child-level questionnaires filled out by their reading teachers, half were subsampled to have child-level questionnaires filled out by their mathematics teachers and the other half had child-level questionnaires filled out by their science teachers. For this reason, there are three child-parent-teacher weights that will be used to analyze direct child assessment data combined with parent interview data and child data provided by teachers (in conjunction with school level or teacher level data). In all three weights, the presence of at least one completed teacher-level questionnaire determines whether a child would have a positive child-parentteacher weight in the two subjects to which they were assigned (i.e., reading and mathematics or reading and science.). A child could have one teacher who taught all subjects, in which case the teacher would be asked to fill out both the reading questionnaire and the mathematics questionnaire (if the child was selected for mathematics) or science questionnaire (if the child was selected for science). A child could also have different teachers teaching different subjects, in which case the child may have a reading teacher filling out the reading questionnaire and a mathematics teacher filling out the mathematics questionnaire, and both teachers could have filled out the teacher-level questionnaire. No children have both completed mathematics and science questionnaires because of the subsampling. Table 7-4 shows the distribution of children who have direct child assessment data, parent interview data and child-level data from the mathematics teacher by the number of teachers they had who filled out the teacher-level questionnaire. The first column in this table shows the number of teachers that each child had: only one teacher who taught both reading and mathematics, or two teachers, one teaching reading and the other teaching mathematics. The second column shows the type of teacher who filled out the teacher-level questionnaire. If the child had only one teacher, then it was this teacher-identified in the table as the reading teacher-who filled out the teacher-level questionnaire (3,142 cases out of 5,009 7-23 or 63 percent). If the child had two teachers, then in the majority of cases, both teachers filled out the teacher-level questionnaire (1,803 cases out of 5,009 or 36 percent). There are very few cases where only one of the two teachers filled out the teacher-level questionnaire. Table 7-5 shows the same information for science. Since C6CPTM0 and C6CPTS0 are used for the analysis of child and parent data with data from mathematics and science teachers, another option to define these weights is to use the presence of child-level data from the mathematics/science teachers. However, tables 7-4 and 7-5 show that by considering the presence of teacher-level data in constructing the child-parent-teacher weights, there are more records with positive weights for analysis (5,017 cases with nonzero C6CPTM0 compared with 5,009 in table 7-4; and 5,103 cases with nonzero C6CPTS0 compared with 5,088 in table 7-5). 5 Using teacher-level data to define the child-parent-teacher weights is also consistent with previous years' practice.  An additional adjustment is necessary to adjust for the subsampling of children for whom mathematics or science teacher data questionnaires were administered. 6 Since only half of the children in fifth grade were eligible to have a completed mathematics teacher questionnaire and the other half to have a completed science teacher questionnaire, the weights before adjustment for movers and nonresponse adjustments (described in sections 7.2.2 and 7.2.3, respectively) were adjusted to account for the subsampling of children. ) ."}, {"section_title": "Replicate Weights", "text": "For each set of cross-sectional and longitudinal weights included in the fifth-grade data file, a set of replicate weights was computed. All adjustments to the full sample weights were repeated for the replicate weights. The replication scheme used for the base year was used for all of the fifth-grade weights that did not contain any fall-first grade component. If a fall-first grade component was included in the definition of the respondents for the weight, then the replication scheme used for fall-first grade estimates was used. Replicate weights are needed to estimate the standard errors of survey estimates. A total of 90 replicate weights were computed using the paired jackknife method (denoted as JK2) for the fifthgrade weights if no fall-first grade component was included. These replicates take into account the Durbin method of PSU selection (Durbin 1967). A total of 40 replicates using the paired jackknife method were created for the weights that contain a fall-first grade component. The number of replicates is smaller because only 30 percent of the full sample of schools was included in the fall-first grade subsample. Only one of the two sampled PSUs in the non-self-representing strata was kept in the sample. Consequently, the fall-first grade weights do not account for the Durbin PSU sampling method, which required two PSUs per stratum."}, {"section_title": "7-26", "text": "The procedures used to compute the replicate weights took into account each step of the weighting process. One feature that is somewhat uncommon in practice is the use of sample-based raking as described in section 7.2.4. The control totals ( c CNT _ SMP ) used for raking are estimates calculated using the initial child weights ( i ICHLDW ). When population-based raking is used, these totals are assumed to be numbers that are known and without sampling error. To reflect the variability of the control totals in the sample-based raking, a set of replicate control totals was calculated rather than having a constant set of totals. Each replicate weight was then raked to the corresponding replicate-based control total. The result of this process was that each replicate retained the variability associated with the original sample estimates of the control totals. As with the full sample weight, the raking procedure was allowed to run until complete convergence. For fifth grade, full convergence was achieved after 12 to 19 iterations for each replicate weight."}, {"section_title": "Replicate Weights for Samples Not Involving Fall-First Grade", "text": "For the original ECLS-K design in the base year, replicate weights were created taking into account the Durbin method of PSU selection. The Durbin method selects two first-stage units per stratum without replacement, with probability proportional to size and a known joint probability of inclusion. In the ECLS-K PSU sample design, there were 24 self-representing (SR) strata and 38 nonself-representing (NSR) strata. Among the 38 NSR strata, 11 strata were identified as Durbin strata and were treated as SR strata for variance estimation. The purpose of the Durbin strata is to allow variances to be estimated as if the first-stage units were selected with replacement. This brings the number of SR PSUs to 46 (24 original SR PSUs and 22 Durbin PSUs from the 11 Durbin strata). The remaining 54 NSR PSUs are in 27 NSR strata; thus 27 replicates were formed, each corresponding to one NSR stratum. For the SR strata, 63 replicates were formed. The 90 replicates will yield about 76 degrees of freedom for calculating confidence intervals for many survey estimates. As stated earlier, the sample of PSUs was divided into 90 replicates or variance strata. The 27 NSR strata formed 27 variance strata of two PSUs each; each PSU formed a variance unit within a variance stratum. All schools within an NSR PSU were assigned to the same variance unit and variance stratum. Sampled schools in the 46 SR PSUs were grouped into 63 variance strata. In the SR PSUs, schools were directly sampled and constituted PSUs. Public schools were sampled from within PSU while private schools were pooled into one sampling stratum and selected systematically (except in the SR"}, {"section_title": "7-27", "text": "PSUs identified through the Durbin method where private schools were treated as if they were sampled from within PSU). Schools were sorted by sampling stratum, type of school (from the original sample or newly selected as part of freshening), type of frame (for new schools only), and their original order of selection (within stratum). From this sorted list, they were grouped into pairs within each sampling stratum; the last pair in the stratum may be a triplet if the number of schools in the stratum is odd. This operation resulted in a number of ordered preliminary variance strata of two or three units each. The first ordered 63 strata were then numbered sequentially from 1 to 63; the next ordered 63 strata were also numbered sequentially from 1 to 63, and so on until the list was exhausted, thus forming the desired 63 variance strata. In strata with two units, a unit being a PSU in the case of NSR PSUs and a school in the case of SR PSUs, the base weight of the first unit was doubled to form the replicate weight, while the base weight of the second unit was multiplied by zero. In strata with three units, two variance strata were created: in the first variance stratum, the base weight of two of the three units was multiplied by 1.5 to form the replicate weight, and the base weight of the last unit was multiplied by zero; in the second variance stratum, the base weight of a different group of two units was multiplied by 1.5, and the base weight of the third unit was multiplied by zero. Multiplying the base weight in a unit by zero is equivalent to dropping one unit as required by the jackknife method. All adjustments to the full sample weights were repeated for the replicate weights. For each full sample weight, there are 90 replicate weights with the same weight prefix. A child sampled in first grade through the freshening process was assigned to the same replicate as the originally sampled child to whom the child was linked. When the child sampled in first grade was assigned a full sample weight, he or she was assigned the replicate weights in the same manner."}, {"section_title": "Replicate Weights for Samples Involving Fall-First Grade", "text": "For the two longitudinal weights involving fall-first grade (C1_6SC0 and C1_6SP0), there are 40 replicate weights. The reason for the smaller number of replicates is that only a subsample of schools was included in the fall-first grade sample. The weights associated with the fall-first grade data do not account for the Durbin method of selecting PSUs, since it no longer applied. Rather, they reflect the fact that only one of the two sampled PSUs in the NSR strata was kept in the subsample. To account for this feature, pairs of similar NSR PSUs were collapsed into 19 variance strata. The SR PSUs account for 7-28 the remaining 21 variance strata. The 40 replicates will yield about 40 degrees of freedom for calculating confidence intervals for many survey estimates."}, {"section_title": "Variance Estimation", "text": ""}, {"section_title": "Jackknife Method", "text": "The final full sample and the adjusted replicate weights can be used to compute estimates of variance for survey estimates using WesVar, AM, SUDAAN, or other software that handles replicate weights. The estimate of variance is the sum of the squared deviations of the replicate estimates from the full-sample estimate: The Taylor series method relies on a simplified procedure for estimating the variance for a linear statistic even with a complex sample design and is valid in large samples in which the first stage units are sampled with replacement. For the ECLS-K, this simplified method does not capture the variance related to the Durbin sampling method, the effects of the adjustments of the weights for nonresponse, or the sample-based raking procedures. These effects are not captured in the Taylor series variance estimates mainly because each adjustment corresponds to a different estimator that the variance estimation software does not support. In some cases these adjustments may have only a minor effect on the variance estimates, but in other cases the effects could be more substantial. For software that uses the Taylor series method, the variance strata and PSUs must be defined. For the fifth-grade ECLS-K, the Taylor variance strata were assigned by sequentially numbering the sampling strata and collapsing any stratum with one PSU with an adjacent stratum. In theory, any variance stratum with fewer than two responding units would be combined with an adjacent stratum, but this did not happen in the ECLS-K. The variance units were assigned by sequentially numbering the firststage sampling units within sampling strata. For example, for C6CW0, Taylor variance strata were numbered sequentially from 1 to 90. Within each Taylor stratum, Taylor units were numbered sequentially from 1 to the total number of units in the stratum. This procedure was done separately for each cross-sectional and longitudinal weight.  Variables used to create unknown eligibility and nonresponse adjustment cells: HHTYPE = 1, in family of 2 parents plus siblings 2, in family of 2 parents and no siblings 3, in family of 1 parent plus siblings 4, in family of 1 parent and no siblings 5, in other type of family 9, in unknown type of family HOMESCH = 0, not homeschooled 1, in home school LFMOVST = 0, not a longitudinal (full sample) mover 1, longitudinal (full sample) mover LMSTATUS = 1, home language is not English 2, home language is English LSMOVST = 0, not a longitudinal (subsample) mover 1, longitudinal (subsample) mover R6MOVST = 0, not a spring-fifth grade mover 1, spring-fifth grade mover RC12P12 = 1, not assessed due to a disability or language minority (LM) or respondent to child assessment and parent interview in fall-kindergarten and spring-kindergarten (i.e., C1CW0>0 and C2CW0>0 and C1PW0>0 and C2PW0>0) 2, not a respondent to child assessment or parent interview in fall-kindergarten or spring-kindergarten (i.e., C1CW0=0 or C2CW0=0 or C1PW0=0 or C2PW0=0) RC1245P1245 = 1, not assessed due to a disability or language minority (LM) or respondent to child assessment and parent interview in fall-kindergarten and spring-kindergarten and spring-first grade and spring-third grade (i.e., C1CW0>0 and C2CW0>0 and C4CW0>0 and C5CW0>0 and C1PW0>0 and C2PW0>0 and C4PW0>0 and C5PW0>0) 2, not a respondent to child assessment or parent interview in fall-kindergarten or spring-kindergarten or spring-first grade or spring-third grade(i.e., C1CW0=0 or C2CW0=0 or C4CW0=0 or C5CW0=0 or C1PW0=0 or C2PW0=0 or C4PW0=0 or C5PW0=0) RC125P125 = 1, not assessed due to a disability or language minority (LM) or respondent to child assessment and parent interview in fall-kindergarten and spring-kindergarten and spring-third grade (i.e., C1CW0>0 and C2CW0>0 and C5CW0>0 and C1PW0>0 and C2PW0>0 and C5PW0>0) B-2 2, not a respondent to child assessment or parent interview in fall-kindergarten or spring-kindergarten or spring-third grade (i.e., C1CW0=0 or C2CW0=0 or C5CW0=0 or C1PW0=0 or C2PW0=0 or C5PW0=0) RC5 = 1, not assessed due to a disability or LM or respondent to spring-third grade child assessment (i.e., C5CW0>0) 2, not a respondent to spring-third grade child assessment (i.e., C5CW0=0)"}, {"section_title": "RP4", "text": "= 1, a respondent to parent interview in spring-first grade (i.e., C4PW0>0) 2, not a respondent to parent interview in spring-first grade (i.e., C4PW0=0) RP5 = 1, a respondent to parent interview in spring-third grade (i.e., C5PW0>0) 2, not a respondent to parent interview in spring-third grade (i.e., C5PW0=0) STYPE = 1, Catholic 2, Other religious private 3, Nonreligious private 4, public 9, unknown These variables are in the weighting working files. If they are included in the final data file, they do not have the same variable names.  "}]