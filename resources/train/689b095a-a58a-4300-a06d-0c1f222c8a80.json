[{"section_title": "Abstract", "text": "c o m p u t e r m e t h o d s a n d p r o g r a m s i n b i o m e d i c i n e 1 1 1 ( 2 0 1 3 ) 255-268"}, {"section_title": "Introduction", "text": "Functional images, such as Positron Emitted Tomography (PET) or Single Photon Emitted Computed Tomography (SPECT), are very important in the process of diagnosis of neurodegenerative diseases. They allow clinicians to correctly interpret the presence of different functional patterns -like blood flow or glucose concentration -in several key regions of the brain, caused by neurological diseases such as Alzheimer's Disease (AD), where early diagnosis is of particular importance in the development of new treatments. However, the evaluation of these scans often relies on manual reorientation, visual reading and semiquantitative analysis. Therefore, Computer Aided Diagnosis (CAD) tools based on medical imaging are a very valuable help for physicians in the AD detection, providing an objective, operator-independent, consistent analysis of the images.\nA CAD system is a computer method and program that assists physicians in the diagnosis task using biomedical signals. It encompasses a set of signal processing methods implemented in a high-level language, i.e. Matlab, resulting in complete systems, such as Statistical Parametric Mapping (SPM) [1] . This is the most extended tool in the neuroimaging community, and refers to the construction and assessment of spatially extended statistical processes used to test hypotheses about functional imaging data. The CAD paradigm has been successfully applied to medical imaging in many works [1] [2] [3] [4] [5] to perform an exhaustive quantitative and qualitative analysis of these medical images. Even in the simplest approach, which uses the raw voxels as an input to the classifier (Voxels as Features, VAF [5] ), a new strategy to avoid the small sample size problem [6] is desirable. This phenomenon occurs when the number of input features to the classifier is higher than the number of samples used to train this classifier. Techniques like image segmentation and definition of Regions of Interest (ROIs) [3, [7] [8] [9] allows a CAD system to consider only brain zones which are significant for AD detection. Other techniques involve statistical measures that compute significance values for each voxels. Then, voxels are ranked and the most significative ones are selected. Statistical measures can be computed by the widely used Student's t-Test [1, 2, 5] , Mann-Whitney-Wilcoxon U-Test [10, 11] or Relative Entropy [12, 13] . These methods have been proven very useful in applications ranging from cancer diagnosis [13] to segmentation of microscope images [12] or autoradiographic images [11] . Later, feature extraction algorithms like Independent Component Analysis (ICA) [14, 15] , Fisher Linear Discriminant Ratio (FDR) [16] , Principal Component Analysis (PCA) [17] [18] [19] or Factor Analysis [2, 20] have been used for extracting common features. In [15] , a method for AD early diagnosis based on ICA analysis is proposed. The method has tested in the feature extraction step with excellent results, but some improvements can be applied to it. One voxel selection step by means of statistical significance has been added in order to select the most significant voxels in the brain and use them to extract the independent components. The second improvement relies in the estimation of the mixing matrix in the ICA step. While the method in [15] relies on creating an average image of the individual samples of each class (AD and CTRL) and then compute the mixing matrix, our method uses all the sample images and their class information to compute this matrix. This is possible thanks to the feature selection step, which reduces the number of voxels from over 500,000 to less than 20,000, enabling a faster computation, and hence, allowing us to include all the samples in the calculation of the mixing matrix. This leads to a more consistent estimation of the Independent Components.\nIn this work, a new combination of algorithms for voxel selection and feature extraction is proposed. Firstly, the most significative voxels in the task of separating classes are selected by using the significance values, computed using Student's t-Test, Mann-Whitney-Wilcoxon U test and Relative Entropy. To reduce the number of features, we extract independent components from the selected voxels using ICA. Then, these features are classified using a Bayesian or Support Vector Machine (SVM) classifier.\nProcessing of perfusion images is performed in three stages:\n1 Voxel selection. Ranking of voxels by measures of difference between two classes and posterior selection of the first N, using Student's t-Test, Mann-Whitney-Wilcoxon (MWW) and Relative Entropy (RE) techniques. 2 Feature extraction. Extraction of K common features from the voxels selected in the previous step, using an ICA technique. These K features will be used as an input vector to the classifier. 3 Classification of the features vector in two different classes:\nNormal Controls (CTRL) and AD. We make use of two different classifiers: Naive Bayes and SVM.\nA baseline method is also defined, and will be used along with the VAF approximation for comparing purposes. This method uses the selected voxels (obtained by the algorithms used in the voxel selection stage) as input features to the classifier, without using any feature extraction technique, and so, is called Selected Voxels As Features (SVAF)."}, {"section_title": "2.", "text": "Materials and methods"}, {"section_title": "Datasets", "text": "To evaluate the performance of the proposed method, we use two different dataset of perfusion images: a database composed by 96 SPECT brain images from \"Virgen de las Nieves\" Hospital in Granada (Spain) and a set of 403 PET brain images from the ADNI initiative."}, {"section_title": "SPECT database", "text": "The database is built up of imaging studies of subjects following the protocol of an hospital-based service. First, the neurologist evaluated the cognitive function, and those patients with findings of memory loss or dementia were referred to the nuclear medicine department in the \"Virgen de las Nieves\" hospital (Granada, Spain), in order to acquire complementary screening information for diagnosis. 2 Experienced physicians evaluated the images visually. The images were assessed using 4 different labels: Control (CTRL) for subjects without scintigraphic abnormalities and mild perfusion deficit (AD1), moderate deficit (AD2) and severe deficit (AD3), to distinguish between different levels of presence of hypoperfusion patterns compatible with AD. In total, the database consists of n = 97 subjects: 41 CTRL, 30 AD1, 22 AD2 and 4 AD3 (see Table 1 for demographic details). Since the patients are not pathologically confirmed, the subject's labels possesses some degree of uncertainty, as the pattern of hypo-perfusion may not reflect the underlying pathology of AD, nor the different classification of scans necessarily reflect the severity of the patients symptoms. However, when pathological information is available, visual assessments by experts have been shown to be very sensitive and specific labelling methods, in contrast to neuropsychological tests [21, 22] . Given that this is an inherent limitation of 'in vivo' studies, our working-assumption is that the labels are true, considering the subject label positive when belonging to any of the AD classes, and negative otherwise. ADNI is the result of efforts of many coinvestigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 adults, ages 55-90, to participate in the research, approximately 200 cognitively normal older individuals to be followed for 3 years, 400 people with MCI to be followed for 3 years and 200 people with early AD to be followed for 2 years. For up-to-date information, see www.adni-info.org."}, {"section_title": "ADNI PET database", "text": "FDG PET scans were acquired according to a standardized protocol. A 30-min dynamic emission scan, consisting of 6 5-min frames, was acquired starting 30 min after the intravenous injection of 5.0-0.5 mCi of 18 F-FDG, as the subjects, who were instructed to fast for at least 4 h prior to the scan, lay quietly in a dimly lit room with their eyes open and minimal sensory stimulation. Data were corrected for radiation attenuation and scatter using transmission scans from Ge-68 rotating rod sources and reconstructed using measured attenuation correction and image reconstruction algorithms specified for each scanner (www.loni.ucla.edu/ADNI/Data/ADNIData.shtml). Following the scan, each image was reviewed for possible artefacts at the University of Michigan and all raw and processed study data was archived.\nEnrolled subjects were between 55 and 90 (inclusive) years of age. Baseline FDG PET data from 401 ADNI participants, acquired from Siemens, general electric (GE), Philips, Siemens HRRT and BioGraph HiRez PET scanners, were collected from the ADNI Laboratory on NeuroImaging (LONI, University of California, Los Angeles) website (http://www.loni.ucla.edu/ADNI/). Participant's enrolment was conditioned to some eligibility criteria. General inclusion/exclusion criteria were as follows:\n\u2022 Normal control subjects: MMSE scores between 24 and 30 (inclusive), a CDR of 0, non-depressed, nonMCI, and nondemented. The age range of normal subjects will be roughly matched to that of MCI and AD subjects. For posterior analysis, the data was arranged into only one group, considering the case AD vs CTRL. This group has n = 196 images in total: 101 CTRL and 95 AD. We will consider that each spatially and intensity normalized image from the database corresponds to a different observation of the brain."}, {"section_title": "Preprocessing", "text": "The complexity of brain structures and the differences between brains of different subjects makes difficult to use the registered images for all type of calculations that directly compares them with each other. However, the proposed system will assume the following two conditions to be met:\n1 the same position in the image coordinate system within different images corresponds to the same anatomical position. 2 the same intensity value, wherever it might be found (within an image or in other images) corresponds to the same physical value.\nWe met the first condition by performing a spacial normalization. This method assumes an affine generic model with 12 parameters [23] and a cost function which presents an extreme value when the image and a template that represents our 'brain space' correspond one with the other. The spacial normalization is achieved by optimizing the quadratic difference between the template and each image.\nAfter this spacial normalization, the resulting images (of size 95 \u00d7 79 \u00d7 69 voxels in both datasets) are filtered, yielding in a smoothing of the images in function of a parameter called \"Full Width at Half Maximum\" (FWHM). This filtering increases the Signal to Noise Ratio (SNR) and guarantees that the changes present in the images will be given in an anatomically significant scale.\nThe second condition is met when we apply a intensity normalization on the images. This is done by normalizing the intensity value of the images to a maximum value, I max , computed by averaging the 3% highest voxel intensities in the SPECT database. For the ADNI database, an average image is computed from the spatially normalized images, and then a specific mask is applied to each subject, so that the average of the voxels inside the mask is exactly one, as explained in [24] ."}, {"section_title": "Voxel selection", "text": "In this stage, we use the results of different statistical measures of significance, like the Student's t-Test [25] , the Mann-Whitney-Wilcoxon U-Test (MWW) [10, 11, 25] and the Relative Entropy (RE) [12, 26] , to rank voxels from the most to the least significative ones. These measures of significance will be higher when the coordinate tested in both classes is more different. Then, we proceed with the selection of the first N voxels."}, {"section_title": "Student's t-Test", "text": "Student's t-Test (t-Test) is a widely used statistical test which quantifies the differences between two different classes. It uses a common estimation of variance for both classes. The value of statistical t can be computed [25] as:\nwhere\nis an estimator of common standard deviation of both samples, \u02dd1 and \u02dd2 are the mean of each class, n 1 is the number of samples in class \u03c9 1 and n 2 is the number of samples in class \u03c9 2 ."}, {"section_title": "Mann-Whitney-Wilcoxon", "text": "Mann-Whitney-Wilcoxon U-Test (MWW) uses the absolute value of the statistical U to rank voxels. Calculation of U value is done by the following expression [25] :\nwhere n i is the sample size for sample i, and R i is the sum of the ranks in sample i (where i = 1, 2). Smaller U i value is taken as the final U value. This statistical test measures the dissimilarity between two groups of values, and, although is similar to Student's t-Test, is less likely than it to spuriously indicate significance because of the presence of outliers. As we make use of real data, and we have no knowledge about the images' statistical distribution, MWW test could be a very good choice [25] . "}, {"section_title": "Relative entropy", "text": "Relative Entropy (or Kullback-Leibler divergence) is a nonsymmetric measure of the difference between two probability distributions \u02dd1 and \u02dd2. Because of its non-symmetric property, we can make use of this to evaluate the difference between CTRL and AD images for each voxel. Relative Entropy can be calculated with Eq. (4) [26] .\nLet \u02dd1 and \u02dd2 be two discrete random variables. Relative Entropy is defined as:\nwhere is any measure of V, the set of all voxels that are placed on a certain brain coordinate, in which \u03c9 1 = "}, {"section_title": "Independent Component Analysis", "text": "Independent Component Analysis (ICA) [27] , is a statistical technique that represents a multidimensional random vector as a lineal combination of non-Gaussian random variables (the so-called \"independent components\") to be as independent as possible, and has been used widely on segmentation and clustering of medical images [28] [29] [30] [31] . It can be considered as a non-Gaussian version of Factor Analysis. Assume that we observe n lineal mixtures x 1 , x 2 , . . ., x n of length N that can be modelled as an expression of K independent components (IC). These independent components are defined as S = (s 1 , s 2 , . . ., s K ), where each s K vector has a length of N. So, each random vector x n can be described as a linear combination of K independent components:\nWithout loss of generality we can assume that both the observed vectors and the independent components are zero mean. If the previous conditions are not met, the x variables can be centred by subtracting the sample mean. To use a vector-matrix notation, more convenient in this case, we denote as matrix X the random vector whose elements are x 1 , . . ., x n . We also denote as A the matrix that contains all a Kn elements, the \"mixing matrix\" that projects each image into the space defined by the IC. Using this notation, the mixing model above remains as follows:\nThe starting point of ICA is the assumption that all components s K are statistically independent. To measure independence, we assume that all independent components have a non-Gaussian statistical distribution. It is assumed that a sum of independent signal trends to Gaussianity, so if nonGaussianity is maximized with any independence criteria F, for instance, the kurtosis or negentropy, we obtain signals that are more independent than the previous ones [32, 27] . After estimating the matrix A, we can compute its inverse, W and obtain the projection S of the images in the dataset into the IC space with:"}, {"section_title": "FastICA", "text": "Adaptive algorithms based on gradient descend can be problematic when they are used on an environment in which adaptation is not necessary, like this case. The convergence is often slow, and depends on the choice of convergence parameters. As a solution to this problem, block algorithms based on fixed-point iteration [33, 34] can be used. In [33] a fixed-point algorithm based on kurtosis is introduced. In [34] , this algorithm, known as FastICA, is generalized to general contrast functions. The single unit FastICA algorithm has the following form:\nwhere the loadings vector w is normalized to unit norm in each iteration, and the function g(x) is a derivative of the contrast function G defined in [32] . The expected values are estimated in practice by using the mean of a significantly high number of samples of the input data. The speed of convergence of the fixed-point algorithms is clearly superior to more neural algorithms. Improvements between 10 and 100 times the speed are observed frequently [35] ."}, {"section_title": "Application to the databases", "text": "In this stage, we have a matrix X that contains all x n mixture vectors, each one containing N values. Due to the preprocessing step (see Section 2.2) this vector represents the intensity values for each brain coordinate in each image. The application of the FastICA algorithm to this matrix first reduces the dimensionality by extracting the first K eigenvalues, when estimating the input matrices to the fixed-point algorithm: D, the matrix of eigenvalues, of size K \u00d7 K and the matrix of eigenvectors E, of size N \u00d7 K.\nThen we proceed to the whitening of the data, and the independent component is extracted using the algorithm described above. From this algorithm we obtain the mixing matrix A and its inverse W, of size K \u00d7 n, for the complete database X. Later, the inverse of the mixing matrix is applied individually to each vector of selected voxels x n , to project each vector into the IC space\nThe above expression shows the transformation applied to each vector of selected voxels, where s n is the projection of the data vector x n into the K-dimensional IC space, which corresponds to one image of the dataset. Finally we obtain the n representations of the images on K independent components. Each of these vectors will be used as input to the classifier. In [15] , the mixing matrix A is estimated from an average image of all images in each class. On the other hand, in the proposed system, A is estimated from the complete database X, what results in a different estimation. Fig. 2 shows the first four independent components obtained using this system and the SPECT database. These components are extracted from the W matrix, and has been spatially represented by assigning the brain coordinate of each voxel to each of the values in these IC. We can observe that each component highlights different zones that are usually related to Alzheimer's Disease, such as the posterior cingulate gyri and precunei, as well as the temporo-parietal region, both considered as typically affected by glucose hypometabolism or rCBF hypoperfusion in AD patients [36] ."}, {"section_title": "Classifiers", "text": ""}, {"section_title": "Naive Bayes Classifier", "text": "The Naive Bayes Classifier (from now on, Bayes classifier or simply Bayes) is based on the optimal decision rule that minimizes the error in a two-hypothesis test. Bayesian-based classifiers have been successfully used in patter recognition problems [37, 17] . The Bayes classifier evaluates the a posteriori probability function [38] . Let {\u03c9 1 , \u03c9 2 , . . ., \u03c9 c } denote the object classes and x a feature vector obtained by some feature extraction technique. The a posteriori probability function of x given \u03c9 i is defined as\nwhere P(\u03c9 i ) is a priori probability, P(\u03c9 i |x) the conditional probability density function of x given \u03c9 i and P(x) is the mixture density. The maximum a posteriori (MAP) decision rule for the Bayes classifier is defined as\nThe test vector data x is classified to \u03c9 i of which the a posteriori probability given x is the largest between the classes. This assumption leads to a quadratic discriminant function, which is referred to hereinafter as Quadratic Bayes Classifier (or Bayes Quadratic). If we assume that the covariances of the two classes \u03c9 1 and \u03c9 2 are identical, the quadratic terms in x of the exponents of Gaussian densities can be cancelled, resulting in a linear discriminant in function of x. This classifier is referred as Linear Bayes Classifier (or Bayes Linear).\nFor the classification problem that we are dealing with, the a priori probability P(\u03c9 i ) is initially set to 0.5, that is, AD and CTRL classes are equally probable. Regarding the conditional probability density function, usually there are not enough samples to estimate it for each class (within-class density). A compromise, therefore, is to make an assumption of a particular density form and convert the general density estimation problem into a parametric one. The within class densities are usually modelled as normal distributions."}, {"section_title": "Support Vector Machines", "text": "Support Vector Machines (SVM), introduced in the late 70s [39] , are a set of related supervised learning methods widely used in pattern recognition [40] , voice activity detection (VAD) [41] and classification [20] . SVM with linear discriminant functions define decision hypersurfaces or hyperplanes in a multidimensional feature space, that is:\nwhere w is known as the weight vector and \u03c9 0 as the threshold. The weight vector w is orthogonal to the decision hyperplane and the optimization task consists of finding the unknown parameters \u03c9 i , i = 1, . . ., n defining the decision hyperplane. Let x i , i = 1, 2, . . ., n be the feature vectors of the training set, X. These belong to either \u03c9 1 or \u03c9 2 , the two classes. If the classes were linearly separable, the objective would be to design a hyperplane that classifies correctly all the training vectors. Among the different design criteria, the maximal margin hyperplane is usually selected since it leaves the maximum margin of separation between the two classes. Since the distance from a point x to the hyperplane is given by z = |g(x)|/ w , scaling w and w 0 so that the value of g(x) is +1 for the nearest point in \u03c9 1 and \u22121 for the nearest points in \u03c9 2 , the optimization problem is reduced to minimizing a cost function J(\u03c9) = 1/2||\u03c9|| 2 subject to:\nwhere \u02dbi are the solution of a quadratic optimization problem that are usually determined by quadratic programming or the well-known sequential minimal optimization algorithm, and (s) or (x) denote the transformation of the feature vectors into the effective feature space. This basic SVM classifier produces a linear separation hyperplane. A more general expression for SVM can be defined with the addition of kernels, that substitute the dot product of the transformed feature vectors (s i ) \u00b7 (x) = K(s i , x). Thus, the expression of the hyperplane in Eq. (12) can be replaced by:\nThe kernel functions that we use in this work are the linear kernel K(s, x) = s \u00b7 x (on Eq. (12)) and a Gaussian radial basis function (RBF) kernel K(s, x) = exp{ \u2212 |s \u2212 x| 2 /(2 2 )}."}, {"section_title": "Evaluation", "text": "In this work, we will compare the two algorithms using accuracy, sensitivity and specificity measures, as long as values of Positive and Negative Likelihood (PL and NL) [42] for each algorithm and each dataset. These values are obtained using both k-fold and leave-one-out cross-validation methods [43] . k-Fold divides the entire dataset in k folds, then train the whole system with k \u2212 1 and finally test the system with the remaining fold. This is performed k times, leaving one different fold out at a time, and then, average values of the evaluation parameters are estimated. Leave-one-out is a particular case of k-fold, in which k = n. In order to compare the proposed methods, we obtain values of accuracy, sensitivity, specificity, positive likelihood and negative likelihood in function number of selected voxels (N) and number of features extracted (K).\nIn clinical medicine, values of PL greater than 5 or NL values less than 0.2 can be applied to the pre-test probability of a patient having the disease tested for to estimate a post-test probability of the disease state existing [42] . A positive result for a test with a PL of 8 adds approximately 40% to the pre-test probability that a patient has a specific diagnosis."}, {"section_title": "Results", "text": "In this section the combination of different selection criteria and classifiers along with the feature extraction based on Independent Component Analysis will be analysed. The following classifiers have been tested in order to compare the performance results: Linear and Quadratic Normal Multivariate Classifier (MV Linear and MV Quadratic) [44] , Naive Bayes Classifiers with either linear and quadratic discriminant function (introduced above), a classifier that makes use of the Mahalanobis distance to compute the class membership [45] and SVM classifiers [46] with linear, quadratic, polynomial and Radial Basis Function kernels (SVM Linear, Quadratic, Polynomial and RBF).\nTo obtain a general view of the results that these different combinations yield, values of accuracy obtained for each combination of classifier, selection criteria and database are shown in Table 2 . Peak values of accuracy could have been displayed in this table, but we have opted for averaging the 10% higher accuracy results obtained using each system, in order to provide us with an idea of both performance and stability, and thus, help us in the task of choosing the best system.\nRegarding the SPECT database, best accuracy results are obtained by using Relative Entropy selection criteria, with values up to 96.9%. With this database, the best classifiers are the Multivariate and Bayesian Quadratic, and SVM-RBF. On the other hand, the ADNI database achieves its best accuracy results when using linear classifiers, along with the Wilcoxon and Relative Entropy selection criteria. Since the variance of parameters estimated using leave-one-out method might be high, a k-fold estimation with k = 10 may be the best option when choosing an statistical procedure [47] . Table 3 shows the average values obtained by k-fold using also the 10% higher accuracy scores for each combination.\nThe combinations that we had highlighted previously are confirmed by examining the above table, as the best results are obtained with the Relative Entropy criteria in both databases, and non-linear classifiers (for SPECT database) and linear classifiers (for ADNI database). With all these measures, we consider the combination of entropy selection criteria and Bayesian Quadratic classifier the Higher accuracy values are highlighted in bold for better understanding in the subsequent analysis. most robust system for the SPECT database. For the ADNI database, multivariate Lineal and SVM Linear classifier obtain similar results. Since the highest accuracy value is achieved with SVM linear, we finally decide on this. Fig. 3 depicts the accuracy, sensitivity and specificity results by means of the number of selected voxels as input vector to the feature extraction step and by means of the number of components extracted by ICA. In these figures, the measures are presented along with the accuracy, sensitivity and specificity values obtained for the baseline method based on a Relative Entropy voxel selection. Fig. 3(a) shows an interesting feature of this classifier. The final accuracy obtained is almost independent from the number of selected voxels. The obtained results are always better than the proposed baseline (SVAF) computed for the Relative Entropy criteria, and are always between 0.92 and 0.97. On the other hand, Fig. 3(b) depicts the behaviour of the cited "}, {"section_title": "SPECT", "text": ""}, {"section_title": "ADNI", "text": "For ADNI database, the behaviour is similar, but of lower values in evaluation parameters. Fig. 4 depicts this behaviour, and compares it to the values obtained when evaluating the Relative Entropy baseline method. The proposed method shows values that are similar to those obtained with the baseline method, and shows, in general, an ascending trend by increasing the number of selected voxels. As commented above, accuracy and sensitivity values trends to higher values when increasing the number of voxel selected N, as shown in Fig. 4(a) . Fig. 4 (b) depicts these values in function of the number of features extracted K, and shows similar and occasionally lower values to those obtained for the baseline method. There are some increments on the evaluation parameters when using a K value between 12 and 15, and K = 4, as happened with the SPECT database. Despite this, the benefits of using a feature extraction with the ADNI database are not significant, or at least not in terms of these evaluation parameters. In terms of computational efficiency, the method proposed reduces the number of parameters used as input to the classifier, although it takes a longer time to estimate the independent components, which results also in small performance improvements over the baseline method."}, {"section_title": "Discussion", "text": "It is interesting to note, before starting with this discussion, that the CAD systems proposed are trained to reproduce the actual medical knowledge, since they have been trained with images labelled by physicians. Images in SPECT database have been labelled by visual assessment and posterior agreement, while images in ADNI database have been labelled using the scores obtained by patients in MMSE cognitive test, and not taking into account the visual information provided by PET images. Thus, these labels will be defined as our \"Gold Standard\" [48] , and the statistical measures are an estimation of how a supervised learning system is able to reproduce a medical diagnosis carried out by experts. Also, the use of evaluation parameters that are sample prevalence dependent can be inappropriate. As our databases are not symmetric, other statistical values like Positive and Negative Likelihood Ratio (PL and NL) can be more reliable. Fig. 5 depicts the average behaviour of the proposed system in function of the number of selected voxels and the number of independent component extracted. As commented above, the accuracy obtained when referring to the SPECT database is significantly constant in function of N. However, the accuracy obtained for the ADNI database trends to higher scores when increasing the number of selected voxels, which is logical, due to the higher amount of information contained in these voxels. For SPECT images, this behaviour can be caused by the higher level of noise contained [49] . As the number of selected voxels from the SPECT images increases, the level of noise included in the estimation of the independent components increases as well, so that the effect of noise counteract the theoretic benefits of increasing the number of voxels.\nConcerning the value of K, Fig. 5(b) shows the values obtained for the SPECT and PET databases. This figure also shows an average common pattern that is followed by both databases: the system obtains the best values when K ranges from 4 to 15, with a significant peak in K = 4. This peak can be caused because the meaningful estimation performed by ICA [32] , as seen in Fig. 2 , which show the 4 first components. These 4 components are related to typical AD patterns affecting the temporo-parietal region and the posterior cingulate gyri and precunei, considered as typically affected by glucose hypometabolism in PET images [50] or rCBF hypoperfusion in SPECT [36] . For these reasons, we will consider an operation point of K = 4 for both databases, and N = 13 for SPECT database and N = 18 for ADNI database. Table 4 compares the results obtained by the proposed system at the operation point with those obtained by the baseline approximation at its highest accuracy point for both databases. The improvements of the AD detection in SPECT images when using the Bayes classifier are patent, although the SVM linear classifier does not achieve these results. It maintains a good PL, but also the NL substantially increases, and though, the final accuracy slightly outperforms the baseline for only 0.02. Something similar holds for the ADNI database. Although high accuracy rates are obtained with the proposed system, the improvements of using a feature extraction step over the baseline SVAF are not as significant, only a 0.02 higher, and the use of a Bayesian classifier even worsens the outcomes. Probably, quadratic classifiers perform better in SPECT database due to the variability of classes included under the AD label (AD-1, which often corresponds to the Mild Cognitive Impairment (MCI), AD-2 and AD-3), while linear classifiers perform better in ADNI database, which only considers the Normal and AD classes, as the MCI class exposed in Section 2.1.2 has not been included in the test. To illustrate this, a spatial view of the first three projections of each of the images in the space and the resulting decision surfaces of the classifiers are shown in Fig. 6 . This higher separation between classes in ADNI database should lead to better results, but this does not hold. This is mainly due to our use of the \"Gold Standard\", in which our final scores are computed using the labels of the databases. As the labelling in ADNI database does not make use of functional imaging, but of psychological tests, this can lead to worse results when analysing functional patterns. Table 5 compares the proposed system with the defined baseline model and other methods proposed in the bibliography, such as the Voxels-as-Features (VAF) method, which considers all the voxels in an image as input features to the classifier [5, 51] , Principal Component Analysis (PCA) [17] , Factor Analysis (FA) [20] or Gaussian Mixture Models (GMM) [52, 51] in terms of accuracy, sensitivity, specificity, PL and NL.\nVAF is considered as a baseline in many works, as different studies have concluded that this method is, at least, comparable with the visual exams performed by experts [5] . The proposed system outperforms in every evaluation parameter this baseline approximation, so improvements of our CAD system over the VAF approximation are present when applying to both databases.\nWhen comparing to other methods in bibliography, the ICA based system is the best when applied to the SPECT database in terms of accuracy, sensitivity, specificity, PL and NL. The accuracy FA based system is 3% less than the accuracy obtained with the proposed system, which is equivalent to 3 more well-classified patients, and outperforms all the other methods proposed in the bibliography. The values of PL are more than three times the value considered to be a good classdiscriminant, and the NL are far less than 0.1, considered also a good value. Regarding the ADNI database, the performance values obtained by the CAD that uses ICA outperforms most methods proposed in the bibliography, like the GMM, the ICA method in [15] and PCA. Nevertheless, the method based on Factor Analysis obtains similar values, with a slightly higher accuracy results, but a lower sensitivity in the AD pattern detection task. Fig. 7 shows the ROC curves computed for each database when varying the distance from the points to the separation hyperplane of the classifier, and comparing the proposed methods with other methods in the bibliography, such as the FA and PCA based methods, and the VAF approximation. It compares also the proposed method with the baseline proposed. Fig. 7(a) shows the ROC curve for the SPECT database, and confirms the advantages of the proposed system over those proposed in the bibliography, yielding an Area Under the ROC Curve (AUC) of approximately 0.98. Fig. 7(b) shows that the proposed system and the methods in the bibliography, including the SVAF approximation, offer a similar performance when applied to the ADNI database. However, the system yet outperforms the VAF approximation, often considered a standard baseline for CAD systems, as indicated previously."}, {"section_title": "Conclusions", "text": "This works presents a new Computer Aided Diagnosis (CAD) system, which makes use of a combination of voxels selection based on statistical significance measures and feature extraction using Independent Component Analysis (ICA). The use of only a voxel selection step already outperforms the Voxels As Features (VAF) approximation, considered as a reference for many works. The feature extraction using Independent Component Analysis allow us to extract highly representative features, which are closely related to typical Alzheimer's Disease (AD) patterns, and yielding on a high-accuracy classification. The resulting system performs specially well with SPECT database, probably due of the meaningful interpretation that ICA plays on the typical AD patterns when extracting the Independent Components, that also acts as a noise reduction system. When applied to the ADNI database, the resulting parameters were also high, but not significantly better than the proposed baseline, which uses the Selected Voxels As Features (SVAF) approximation.\nThe proposed system shows excellent accuracy values in the classification task with AD compared to Normal Control (CTRL) images, and displays a high robustness to variations in the input parameters, specially regarding the number of voxels selected. Even the proposed SVAF baseline method outperforms some of the methods proposed in the bibliography. In particular, the baseline Voxel As Features (VAF) approximation, took as a reference in numerous works and indicative of the effectiveness of visual examination of the images, was outperformed in every case by our baseline SVAF method and the proposed CAD system."}, {"section_title": "Conflict of interest", "text": "There is no conflict of interest."}]