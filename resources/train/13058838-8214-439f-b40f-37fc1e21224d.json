[{"section_title": "Abstract", "text": "Patients with Alzheimer's disease and other brain disorders often show a similar spatial distribution of volume change throughout the brain over time, 1, 2 but this information is not yet used in registration algorithms to refine the quantification of change. Here, we develop a mathematical basis to incorporate that prior information into a longitudinal structural neuroimaging study. We modify the canonical minimization problem for non-linear registration to include a term that couples a collection of registrations together to enforce group similarity. More specifically, throughout the computation we maintain a group-level representation of the transformations and constrain updates to individual transformations to be similar to this representation. The derivations necessary to produce the Euler-Lagrange equations for the coupling term are presented and a gradient descent algorithm based on the formulation was implemented. We demonstrate using 57 longitudinal image pairs from the Alzheimer's Disease Neuroimaging Initiative (ADNI) that longitudinal registration with such a groupwise coupling prior is more robust to noise in estimating change, suggesting such change maps may have several important applications."}, {"section_title": "INTRODUCTION", "text": "In longitudinal neuroimaging studies, non-linear image registration is often used to quantify the spatial distribution of volume change experienced by the brain over time. 3 In such a setting, we are given two images of the same brain acquired at different times and we wish to compute a mapping between them that reveals where tissue has contracted or expanded. To make this notion mathematically precise, take R(x), T (x) \u2208 L 2 (\u2126, R) to be finite scalar valued images over the domain \u2126 \u2282 R d ; further suppose R(x) and T (x) are images of the same brain acquired at different times. The problem of non-linear image registration is then typically formulated as follows: find a transformation \u03c6 : \u2126 \u2192 \u2126 on the coordinate system x such that * : (\u2126, R) \u2192 R is an appropriate image similarity function that measures how well matched two images are, and hence assesses the likelihood that the transformation \u03c6 is a suitable one. Although there are some specific biological situations where the assumption may be violated (e.g., neoplastic growth), for our current application we assume that transformations that reflect actual biological change are bijective and smooth, so the function S : \u03c6 \u2192 R is a regularizer/smoother on an appropriate set of transformations \u03c6. Finally \u03b2 is a scalar that regulates the relative importance between image matching and the smoothness of \u03c6 as regulated by S. Given an approximate solution to (1) , the spatial distribution of tissue change can be estimated from the Jacobian determinant of the transformation, or (less commonly) other metrics derived from the local deformation or strain tensor. This method is often termed tensor-based morphometry (TBM). 4 In a longitudinal study with n individuals, we are given a collection of such image pairs: R 0 , T 0 , ..., R n\u22121 , T n\u22121 and we wish to find transformations \u03c6 0 , ..., \u03c6 n\u22121 such that:"}, {"section_title": "D[R, T (\u03c6)", "text": "Each term in this sum can be minimized independently and the task is simply a concatenation of n separate registrations presenting no new challenges. However, consider the addition of a term P : \u03c6 n \u2192 R to measure some aspect of the collection of transformations \u03c6 0 , ..., \u03c6 n\u22121 as a whole, producing the objective:\nfor some scalar \u03b1. The construction of each \u03c6 i is no longer independent of the others, and the individual registrations must share information to acheive a minimum.\nSuppose the images R 0 , ..., R n\u22121 were acquired before the same event or process occurred in the n different subjects and images T 0 , ..., T n\u22121 were acquired after. It is reasonable to expect the transformations \u03c6 0 , ..., \u03c6 n\u22121 to share some characteristics. Performing the registrations independently does not take advantage of knowledge that can be learned by observing similarity amongst the group. We incorporate P in this work to account for our prior knowledge that the images have undergone transformations generated from the same or similar processes.\nTo further motivate the use of such a prior, consider the example presented in Figure 1 . In Figure 1A , the two red dots represent the unobserved ground truth values of two points in a 2 dimensional independent/dependent variable system. If we assume the model is linear, the optimal linear regression between the two points is the red line passing through both points. The blue dots correspond to noisy measurements of these variables. We attempt to estimate the parameters of the model from the noisy measurements and find the blue line, which clearly differs from the unobserved ground truth. For values of t (the independent variable) close to the observations, the difference dy1 between the ground truth model and the estimated model may be small. However, for values of t further from the observations, the difference dy2 will grow substantially. Unfortunately, this is often the situation in longitudinal neuroimaging: we wish to predict the trajectory of a brain's growth and shape change from only two noisy observations acquired temporally close together. In clinical trial design, or to assess treatment effects in an individual, there is often a desire to begin to estimate brain changes at the soonest possible follow-up interval, when changes are harder to estimate. Figure 1B demonstrates the use of a groupwise similarity prior. Again, the red dots represent the unobserved ground truth values of two points in a linear system, and the dotted red line represents the optimal linear prediction. We make five separate noisy observations of the system (the blue dots), and wish to interpolate between each of the five pairs separately (the dotted blue lines). If we take the average of these models (the green dots and line), and shrink each model toward it, in this case, all the estimated models will approach the ground truth model. Predictions made from these improved individual models will be closer to the ground truth values at distant time points than predictions made from the original estimates. Here, the average estimated model (in green) serves as the groupwise representation, and we couple the estimation of the individual models by enforcing they be similar to their average.\nOther works have incorporated cross-sectional or group level information into individual computations to improve sensitivity and group level consistency. In Pennec et al.,\n5 the authors suggest computing statistics of the Hencky strain tensor of the transformation from a training data set of multiple registration pairs. In a second round of registrations, the strain tensor is constrained by the previously computed statistics. Our method differs in that the group level information is determined from the data itself during registration in an empirical Bayes fashion, eliminating the need for a training set and multiple rounds of registration.\nA groupwise similarity prior was also used more recently in Liu et al. 6 for the determination of resting state networks in fMRI analysis. In this work, the authors propose using a hierarchical Markov Random Field (hMRF) to cluster voxels into functional networks in fMRI analysis. The upper level of the hierarchy is an average representation of the resting state network across the population. Edges connecting this upper level to the individual fields represent a constraint that individual networks should be similar to the group level representation. Similar to our work, the group level representation and individual networks are simultaneously estimated in an iterative fashion. However, their work is formulated in a discrete setting and for a different data modality."}, {"section_title": "METHODS", "text": ""}, {"section_title": "Mathematical Formulation", "text": "Here, we propose P in (3) be a measurement of the disagreement among the patterns of tissue change determined by the transformations \u03c6 0 , ..., \u03c6 n\u22121 . If we want to compare volumetric change maps, we have to do it in a common coordinate system. So along with the images R i and T i , we assume we're given transformations \u03c8 i such that\n. This is achieved by computing a Minimal Deformation Template (MDT) for the initial time point images T i prior to longitudinal registration using established techniques.\nFirst, let J mdt \u03c6i (x) = J \u03c6i (\u03c8 i (x)) be the Jacobian matrix of transformation \u03c6 i at position x in the common coordinate system, and let log( det[J mdt \u03c6i (x)] ) be its log determinant. We take the sample mean of these to be \u00b5(\n, and use it to center the log Jacobian determinants:\ncen be a column vector of the mean centered log Jacobian determinant in the common coordinate system evaluated at all spatial positions in X. Finally, we define the mean centered design matrix of the log Jacobian determinant in the common coordinate system to be:\nand choose for P in (3):\nNote that (1/n)A T A is the sample covariance matrix 7 for the log( det[J mdt \u03c6i (X)] ) cen . Sample covariance matrices are symmetric positive semi-definite, and thus have a canonical representation A T A = V \u039bV T for unitary/rotation matrix V and diagonal matrix \u039b. 7 The columns of V are the eigenvectors, or principal axes, of the covariance and the elements of \u039b are the corresponding eigenvalues, \u03bb i . Because A T A is symmetric positive semi-definite, the \u03bb i are real and non-negative. Each \u03bb i is a measure of the magnitude of the corresponding principal axis. Finally, the inner-product matrix AA T has the same non-zero eigenvalues as A T A."}, {"section_title": "7", "text": "The trace of a matrix is invariant to rotation, hence Trace(AA T ) = Trace(\u039b) = n\u22121 i=0 \u03bb i . From this point of view, we see that minimizing (5) minimizes the magnitude of the principal axes of the covariance. That is, (5) is a scalar measure of the spread of the a i , and minimizing it compresses the log Jacobian determinant images about their mean. We also have Trace(AA\nFrom this point of view, we can say that throughout the computation of the \u03c6 i , we maintain a group representation of the volumetric change in the brain, \u00b5(X), and constrain individual volumetric change maps to be similar to it.\nTo simplify notation, we return to the continuous setting and omit positional dependence of transformations and images. To optimize (3) using (5), we make use of the gradient:\nin a gradient descent strategy detailed below. The derivation of (6) from (5) is presented in the appendix. This gradient is proportional to the distance between the log Jacobian determinant for the transformation we wish to update and the mean log Jacobian determinant across the data set. Hence, when this distance is large, this term more forcefully pushes the transformation in the direction of the average log Jacobian determinant image. Note also that this gradient is in the MDT coordinate system and must be composed with \u03c8 \u22121 k before it can be used to update \u03c6 k ."}, {"section_title": "Algorithm Formulation", "text": "Denote the Euler-Lagrange equations corresponding to P, D, and S as p, d, and s respectively. Optimization of (3) with respect to the \u03c6 i is done iteratively by gradient descent. One approach to this optimization is to solve:\nfor \u03c6 l+1 k . Here, d and p apply forces to the template image: d in a direction that registers the template to the reference image and p such that the log Jacobian determinant of the transformation is pushed toward its average. \u03b1 and \u03b2 control the compromise between these two forces, representing individual matching accuracy and group level consistency. Finally, s regularizes or smooths this force field.\nIn (7), the \u03c6 i are updated one at a time, taking advantage of the maximum amount of information available at all times. This approach is exceedingly time consuming. A more tractable approach is to solve:\nin parallel for all n image pairs holding the average \u00b5 fixed, then update \u00b5 using all of the \u03c6 l+1 i\nfor use in the next iteration. We used this method in our experiments.\nFinally, for S we use the elastic potential energy, the EL equations for which are the Navier-Lame equations for linear elasticity:\nfor lame constants \u03bb and \u00b5, which control the relative contributions of the Laplacian and gradient of divergence terms. u is the displacement vector component of \u03c6(x) = x + u(x).\nFor D we used the cross correlation (CC). Let \u00b5(I) be the intensity average for image I.\n2 dx is the intensity variance of T,\n2 dx is the intensity variance of R, and\ndx is the intensity covariance of images R and T, assuming \u2126 is the unit cube. The CC is then:\nThis formula ranges from 0 to 1 and equals 1 if the statistically normalized images are perfectly aligned, hence we attempt to maximize rather than minimize it. The EL equations of the CC are:"}, {"section_title": "Experimental Formulation", "text": "We downloaded screening and 1 year follow up 1.5 Tesla T1-weighted images for 57 participants in the Alzheimer's Disease Neuroimaging Initiative (ADNI). All 57 participants had been diagnosed with Alzheimer's Disease (AD) prior to the acquisition of their screening image. The population consisted of 32 men of mean age 75.91 +/-7.85 years and 25 women, with mean age 75.08 +/-8.15 years. This was the maximum number of individuals we could download from the ADNI 1 cohort who were in the AD group and had screening, year 1, and year 2 follow up images available. All images were corrected for geometric distortion and bias in the static field with GradWarp and N3 before downloading as part of the ADNI preprocessing protocol. 9 Subsequent to downloading, the images were linearly registered to the ICBM template and skull stripped using ROBEX. 10 Transformations \u03c8 i mapping the template images T i into a MDT coordinate system were computed using a preexisting implementation of the registration method in Yanovsky et al."}, {"section_title": "11", "text": "A second set of images was created by adding isotropic Gaussian noise to every voxel within the skull stripped mask of the initial 57 image pairs. The average standard deviation in intensity of the original images was 0.164. We used 0.082 for the standard deviation of the Gaussian noise, which made for an artificial SNR of 4 for the noisy image set. Both the initial and noisy image sets were registered for 100 iterations using (8) with \u03b2 = 1 and \u03b1 \u2208 {0, 25, 50, 75, 100, 150, 200}. If J \u03b1 is the Jacobian determinant map for the transformation of a given image pair from the initial no noise added images at a given value of \u03b1, then let J nz \u03b1 be the corresponding Jacobian determinant map for the images with noise added. We computed:\nfor all image pairs and values of \u03b1. We hypothesized that as \u03b1 increased, the SSD would decrease. That is, as the 57 image pairs shared more information during registration, their sensitivity to noise in the estimation of change would decrease."}, {"section_title": "RESULTS", "text": "The total cross-correlation (CC) throughout optimization summed over all 57 image pairs for all tested values of \u03b1 are presented in Figure 2 . Figure 2A shows the values for the initial image set, and figure 2B shows the values for the noisy image set. The algorithm succeeds in improving the CC for all values of \u03b1. As \u03b1 increases the final CC value decreases. This is entirely consistent with expectations as we have compromised exact matching for groupwise consistency. The final CC values are lower for the noisy image set, indicating that exact matching is harder in the presence of this substantial noise.\nA coronal cross-section of the mean and variance of the Jacobian determinant images in MDT coordinates for all values of \u03b1 are shown in Figure 3 . Figure 3A shows the statistics for the initial image set, and Figure 3B shows the statistics for the noisy image set. The mean images are in the top rows of A and B, and the variance images are in the bottom rows. \u03b1 increases from left to right, the first column corresponding to \u03b1 = 0 and subsequent columns to \u03b1 equaling 25, 50, 75, 100, 150, and 200 respectively. The left colorbars correspond to the mean images, the right colorbars correspond to the variance images. The algorithm successfully minimizes the variance of the Jacobian determinant maps at every voxel as \u03b1 increases. We also see that the variance at many voxels increases somewhat under noisy conditions relative to the original data set, as perfect matching is more challenging in the presence of noise. The averages are consistent across \u03b1 values, however have slightly more defined regions and smaller magnitude as \u03b1 increases. To test if the reduction in SSD values with increasing \u03b1 was due to chance, a one-sided Student's t-test was performed between the \u03b1 = 0 data and the data sets for all other \u03b1 values respectively. The results are presented in Table 1 . The improvement in Jacobian determinant map estimation in the presence of noise was statistically significant for all values of \u03b1 with p-values less than 1e-8. Finally, the results of calculating (12) for all image pairs at all \u03b1 levels are shown in"}, {"section_title": "DISCUSSION", "text": "First we address the reduction in final cross-correlation value with increasing \u03b1. This is expected, as we compromise the exact matching of the template to the reference image in favor of some level of groupwise consistency among the log Jacobian determinant maps. In our model, this is the appropriate thing to do, as an exact matching between template and reference images would have registered not only the anatomy, but whatever noise was corrupting it as well. We hypothesized that most of the CC lost due to the groupwise consistency term was overfitting to the noise in the images. Figure 4 and Table 1 suggest this hypothesis was reasonable and warrant its further exploration.\nConsidering the results in Figure 4 and Table 1 , we interpret the change in the mean Jacobian determinant images with \u03b1, presented in Figure 3 , as an enrichment for change caused by actual biological processes present in most of the 57 image pairs. The slight reduction in magnitude may be due to the loss of components of the transformation that were solely due to noise. Also, the boundaries of anatomy are somewhat more clear in the mean images corresponding to increased \u03b1, so those images may be a better measure of biological change.\nIt is very important to mention that Jacobian determinant maps estimated using this method should only be used for voxel-based analysis studies with extreme care. Equation (5) intentionally minimizes the variance in the log Jacobian determinant values at every voxel, and statistics dependent on that variance are biased. 12 It is still possible to use Jacobian determinant maps learned in this way, if one empirically determines the null distribution of the variance values, and uses it to correct any statistics for the bias. However, the most obvious applications for registration via this method lie elsewhere.\nThe most immediate extension of the work is to put the registration framework into the diffeomorphic setting. 13, 14 In that case, for each image pair we truly estimate a geodesic on a manifold of diffeomorphisms (specified by the choice of metric) that ideally passes trough the identity and a transformation that perfectly matches the template to the reference image. In that case, our example presented in figure 1 is even more applicable, as we are estimating the \"slope\" of the geodesic, given by the initial momentum. In that case, predictions for future unobserved time points can be made by shooting the geodesic given by the estimated momentum further in time. For predictions far in time from the initial data, the accuracy of the momentum estimation is very important, as shown in Figure 1 . This method may make such predictions more accurate and clinically useful, and ease the inference of clinically relevant information from data collected sooner or closer together in time.\nFinally, there is a close connection between the proposed method and empirical Bayes (EB) methods. 15 In EB methods, we know the form of a prior distribution over our model parameters, but we do not know the parameters for the prior itself. Those parameters are instead estimated from the data, and the estimated prior is then used to constrain the fitting of individual models. In such methods, the prior estimation is independent of the model fitting. In our case, we have assumed there is a prior distribution for log Jacobian determinant maps (or momenta in the diffeomorphic setting) for the images we are working with. We would like to constrain the estimation of our Jacobian determinant maps with this prior, but we do not have an explicit representation of it. In lieu of having a given prior, we use the average log Jacobian determinant map, and constrain the estimation of individual log Jacobian maps to be similar to it. Future work will explore the connection between the proposed method and the EB framework."}, {"section_title": "CONCLUSIONS", "text": "We demonstrated a mathematical framework to couple the registration of N longitudinal image pairs. We hypothesized that by sharing information, the estimation of Jacobian determinant maps would be less prone to over fitting due to noise. We conducted an experiment, the results of which support that hypothesis and suggest further investigation into groupwise methods and priors for longitudinal registration."}]