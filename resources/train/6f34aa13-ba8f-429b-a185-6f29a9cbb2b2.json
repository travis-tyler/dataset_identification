[{"section_title": "Abstract", "text": "We propose a two-sample testing procedure based on learned deep neural network representations. To this end, we define two test statistics that perform an asymptotic location test on data samples mapped onto a hidden layer. The tests are consistent and asymptotically control the type-1 error rate. Their test statistics can be evaluated in linear time (in the sample size). Suitable data representations are obtained in a data-driven way, by solving a supervised or unsupervised transfer-learning task on an auxiliary (potentially distinct) data set. If no auxiliary data is available, we split the data into two chunks: one for learning representations and one for computing the test statistic. In experiments on audio samples, natural images and three-dimensional neuroimaging data our tests yield significant decreases in type-2 error rate (up to 35 percentage points) compared to state-of-the-art two-sample tests such as kernel-methods and classifier two-sample tests. Vidmantas Bentkus. A lyapunov-type bound in rd. Theory of Probability & Its Applications, 49(2):311-323, 2005. Miko\u0142aj Bi\u0144kowski, Dougal J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018. Kacper P Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton. Fast two-sample testing with analytic representations of probability measures. In Advances in Neural Information Processing Systems, pages 1981-1989, 2015. EH Corder, AM Saunders, WJ Strittmatter, DE Schmechel, PC Gaskell, GW Small, AD Roses, JL Haines, and MA Pericak-Vance. Gene dose of apolipoprotein e type 4 allele and the risk of alzheimer's disease in late onset families. Science, 261(5):921-923, 1993."}, {"section_title": "INTRODUCTION", "text": "For almost a century, statistical hypothesis testing has been one of the main methodologies in statistical inference (Neyman and Pearson, 1933) . A classic problem is to validate whether two sets of observations are drawn from the same distribution (null hypothesis) or not (alternative hypothesis). This procedure is called two-sample test.\nTwo-sample tests are a pillar of applied statistics and a standard method for analyzing empirical data in the sciences, e.g., medicine, biology, psychology, and social sciences. In machine learning, two-sample tests have been used to evaluate generative adversarial networks (Bi\u0144kowski et al., 2018) , to test for covariate shift in data (Zhou et al., 2016) , and to infer causal relationships (Lopez-Paz and Oquab, 2016) .\nThere are two main types of two-sample tests: parametric and non-parametric ones. Parametric two-sample tests, such as the Student's t-test, make strong assumptions on the distribution of the data (e.g. Gaussian). This allows us to compute p-values in closed form. However, parametric tests may fail when their assumptions on the data distribution are invalid. Non-parametric tests, on the other hand, make no distributional assumptions and thus could potentially be applied in a wider range of application scenarios. Computing non-parametric test statistics, however, can be costly as it may require applying re-sampling schemes or computing higher-order statistics.\nA non-parametric test that gained a lot of attention in the machine-learning community is the kernel two-sample test and its test statistic: the maximum mean discrepancy (MMD). MMD computes the average distance of the two samples mapped into the reproducing kernel Hilbert space (RKHS) of a universal kernel (e.g., Gaussian kernel). MMD critically relies on the choice of the feature representation (i.e., the kernel function) and thus might fail for complex, structured data such as sequences or images, and other data where deep learning excels.\nAnother non-parametric two-sample test is the classifier twosample test (C2ST). C2ST splits the data into two chunks, training a classifier on one part and evaluating it on the remaining data. If the classifier predicts significantly better than chance, the test rejects the null hypothesis. Since a part of the data set needs to be put aside for training, not the full data set is used for computing the test statistic, which limits the power of the method. Furthermore, the performance of the method depends on the selection of the train-test split.\nIn this work, we propose a two-sample testing procedure that uses deep learning to obtain a suitable data representation. It first maps the data onto a hidden-layer of a deep neural network that was trained (in an unsupervised or supervised fashion) on an independent, auxiliary data set, and then it performs a location test. Thus we are able to work on any kind of data that neural networks can work on, such as audio, images, videos, time-series, graphs, and natural language. We propose two test statistics that can be evaluated in linear time (in the number of observations), based on MMD and Fisher discriminant analysis, respectively. We derive asymptotic distributions of both test statistics. Our theoretical analysis proves that the two-sample test procedure asymptotically controls the type-1 error rate, has asymptotically vanishing type-2 error rate and is robust both with respect to transfer learning and approximate training.\nWe empirically evaluate the proposed methodology in a variety of applications from the domains of computational musicology, computer vision, and neuroimaging. In these experiments, the proposed deep two-sample tests consistently outperform the closest competing method (including deep kernel methods and C2STs) by up to 35 percentage points in terms of the type-2 error rate, while properly controlling the type-1 error rate. We consider non-parametric two-sample statistical testing, that is, to answer the question whether two samples are drawn from the same (unknown) distribution or not. We distinguish between the case that the two samples are drawn from the same distribution (the null hypothesis, denoted by H 0 ) and the case that the samples are drawn from different distributions (the alternative hypothesis H 1 ).\nWe differentiate between type-1 errors (i.e,rejecting the null hypothesis although it holds) and type-2 errors (i.e., not rejecting H 0 although it does not hold). We strive for both the type-1 error rate to be upper bounded by some significance level \u03b1, and the type-2 error rate to converge to 0 for unlimited data. The latter property is called consistency and means that with sufficient data, the test can reliably distinguish between any pair of probability distributions.\nLet p, q, p and q be probability distributions on R d with common dominating Borel measure \u00b5. We abuse notation somewhat and denote the densities with respect to \u00b5 also by p, q, p and q . We want to perform a two-sample test on data drawn from p and q, i.e. we test the null hypothesis H 0 : p = q against the alternative H 1 : p q. p and q are assumed to be in some sense similar to p and q, respectively, and act as auxiliary task for tuning the test (the case of p = p and q = q is perfectly valid, in which case this is equivalent to a data splitting technique).\nWe have access to four (independent) sets X n , Y n , X n , and Y n of observations drawn from p, q, p , and q , respectively. Here X n = {X 1 , . . . , X n } \u2282 R d and X i \u223c p for all i (analogue definitions hold for Y n , X n , and Y n ). Empirical averages with respect to a function f are denoted by f (X n ) := 1 n n i=1 f (X i ). We investigate function classes of deep ReLU networks with a final tanh activation function:\nHere, the activation functions tanh and \u03c3(z) := ReLU(z) = max(0, z) are applied elementwise, || \u00b7 || Fro is the Frobenius norm, H = d + 1 is the width and D N and \u03b2 N are depth and weight restrictions onto the networks. This can be understood as the mapping onto the last hidden layer of a neural network concatenated with a tanh activation."}, {"section_title": "DEEP TWO-SAMPLE TESTING", "text": "In this section, we propose two-sample testing based on two novel test statistics, the Deep Maximum Mean Discrepancy (DMMD) and the Deep Fisher Discriminant Analysis (DFDA). The test asymptotically controls the type-1 error rate, and it is consistent (i.e., the type-2 error rate converges to 0). Furthermore, we will show that consistency is preserved under both transfer learning on a related task, as well as only approximately solving the training step."}, {"section_title": "Proposed Two-sample Test", "text": "Our proposed test consists of the following two steps. 1. We train a neural network over an auxiliary training data set. 2. We then evaluate the maximum mean discrepancy test statistic (Gretton et al., 2012a) (or a variant of it) using as kernel the mapping from the input domain onto the network's last hidden layer."}, {"section_title": "Training Step", "text": "Let the training data be X n and Y m . Denote N = n + m . We run a (potentially inexact) training algorithm to find \u03c6 N \u2208 T F N with:\nHere, \u03b7 \u2265 0 is a fixed leniency parameter (independent of N); finding true global optima in neural networks is a hard problem, and an \u03b7 > 0 allows us to settle with good-enough, local solutions. This procedure is also related to the early-stopping regularization technique, which is commonly used in training deep neural networks (Prechelt, 1998) ."}, {"section_title": "Test Statistic", "text": "We define the mean distance of the two test populations X n , Y m measured on the hidden layer of a network \u03c6 as\nUsing \u03c6 N from the training step, we define the Deep Maximum Mean Discrepancy (DMMD) test statistic as\nWe can normalize this test statistic by the (inverse) empirical covariance matrix:\n. This leads to a test statistic (which we call Deep Fisher Discriminant Analysis-DFDA) with an asymptotic distribution that is easier to evaluate. Note that the empirical covariance matrix is defined as:\nwhere \u03c1 n,m > 0 is a factor guaranteeing numerical stability and invertibility of the covariance matrix, and Z = {Z 1 , . . . , Z m+n } = {X 1 , . . . , X n , Y 1 , . . . , Y m }."}, {"section_title": "Discussion", "text": "Intuitively, we map the data onto the last hidden layer of the neural network and perform a multivariate location test on whether both map to the same location. If the distance D n,m between the two means is too large, we reject the hypothesis that both samples are drawn from the same distribution. Consistency of this procedure is guaranteed by the training step.\nInterpretation as Empirical Risk Minimization If we identify X i with (Z i , 1) and Y i with (Z n +i , \u22121) in a regression setting, this is equivalent to an (inexact) empirical risk minimization with loss function L(t,t) = 1 \u2212 tt:\nwhere we denote by R N the empirical risk; the corresponding expected risk is\nAssuming that Pr(t = 1) = Pr(t = \u22121) = 1 2 , we have for the Bayes risk\nand only if p q . As long as p and q are selected close enough to p and q, respectively, the corresponding test will be able to distinguish between the two distributions.\nSince we discard w after optimization and use the norm of the hidden layer on the test set again, this implies some finetuning on the test data, without compromising the test statistic (see Theorem 3.1 below). This property is especially helpful in neural networks, since for practical transfer learning, only fine-tuning the last layer can be extremely efficient, even if the transfer and actual task are relatively different (Lu et al., 2015) .\nRelation to kernel-based tests The test statistic S n,m is a special case of the standard squared Maximum Mean Discrepancy (Gretton et al., 2012b) with the kernel k(z 1 , z 2 ) := \u03c6(z 1 ), \u03c6(z 2 ) (analogously for T n,m and the Kernel FDA Test (Harchaoui et al., 2008) ). For a fixed feature map \u03c6 this kernel is not characteristic, and hence the resulting test not necessarily consistent for arbitrary distributions p, q. However, by first choosing \u03c6 in a data-dependent way, we can still achieve consistency."}, {"section_title": "Control of Type-1 Error", "text": "Due to our choice of \u03c6 N , there need not be a unique, welldefined limiting distribution for the test statistics when n, m \u2192 \u221e. Instead, we will show that for each fixed \u03c6, the test statistic S n,m has a well-defined limiting distribution that can be well evaluated. If in addition the covariance matrix is invertible, then the same holds for T n,m .\nIn particular, the following theorem will show that D n,m (\u03c6) converges towards a multivariate normal distribution for n, m \u2192 \u221e. S n,m then is asymptotically distributed like a weighted sum of \u03c7 2 variables, and T n,m like a \u03c7 2 H (again, if well-defined). Theorem 3.1. Let p = q, \u03c6 \u2208 T F and \u03a3 := Cov(\u03c6(X 1 )) and assume that n n+m \u2192 r \u2208 (0, 1) as n, m \u2192 \u221e. (ii) As n, m \u2192 \u221e,\nwhere \u03be i iid \u223c N(0, 1) and \u03bb i are the eigenvalues of \u03a3.\n(iii) If additionally \u03a3 is invertible, and \u03c1 n,m \u2193 0 then as n, m \u2192 \u221e\nSketch of proof (full proof in Appendix A.1). (i) As under H 0 \u03c6(X i ) and \u03c6(Y j ) are identically distributed, D n,m (\u03c6) is centered and the result follows (with a few additional steps) from a Central Limit Theorem.\n(ii) and (iii) then follow from the continuous mapping theorem and properties of the multivariate normal distribution.\nUnder some additional assumptions we can also use a Berry-Esseen type of result to quantify the quality of the normal approximation of D n,m (\u03c6 N ) conditioned on the training. In particular, if we assume that n = m and \u03a3 = Cov p,q (\u03c6 N (X 1 ))|X n , Y n invertible, then Bentkus (2005) shows that the normal approximation on convex sets is O H 1/4 \u221a n . Computing p-values for both S n,n and T n,n only requires computation over convex sets, so the result is directly applicable."}, {"section_title": "Computational Aspects", "text": "Testing with S n,m As shown in Theorem 3.1, the null distribution of S n,m can be approximated as the weighted sum of independent \u03c7 2 -variables. There are several approaches to computing the cumulative distribution function of this distribution, see Bausch (2013) for an overview and Zhou and Guan (2018) for an implementation. However, computing p-values with this method can be rather costly.\nAlternatively, note that the test statistic S n,m is linear in the number of observations and dimensions. Hence, estimating the null distribution via Monte-Carlo permutation sampling (Ernst et al., 2004) is feasible. Note also that it suffices to evaluate the feature map \u03c6 on each data point only once and then permute the class labels, saving more time.\nIn practice we found that the resampling-based test performed considerably faster. Hence, in the remainder of this work, we will evaluate the null hypothesis of the DMMD via the resampling method.\nTesting with T n,m Since in many practical situations one wants to use standard neural network architectures (such as ResNets), the number of neurons in the last hidden layer H may be rather large, compared to n, m. Therefore, using the full, highdimensional hidden layer representation might lead to suboptimal normal approximations. Instead, we propose to use a principal component analysis on the feature representation (\u03c6(Z i )) n+m i=1 to reduce the dimensionality to\u0124 m + n. In fact, this does not break the asymptotic theory derived in Theorem 3.1, even though the PCA is both trained and evaluated on the test data; details can be found in Appendix C. Unfortunately, the O H 1/4 \u221a n rate of convergence is not valid anymore, due to the observations not being independent. We still need to grow\u0124 towards H with n, m in order for the consistency results in the next section to hold, however. Empirically we found\u0124 = min n+m 2 , H to perform well.\nThe cumulative distribution function of the \u03c7 2 H distribution can be evaluated very efficiently. Although for the DFDA it is also possible to estimate the null hypothesis via a Monte Carlo permutation scheme, doing so is more costly than for the DMMD, since it involves either a matrix inversion once or solving a linear system for each permutation draw. Hence, in this work we focus on using the asymptotic distribution."}, {"section_title": "Consistency", "text": "In this section we show that if (a), the restrictions \u03b2 N , D N on weights and depth of networks in T F N are carefully chosen, (b), the transfer task is not too far from the original task, and (c), the leniency parameter \u03b7 in the training step is small enough, then our proposed test is consistent, meaning the type-2 error rate converges to 0. Theorem 3.2. Let p q, n = n , m = m with n m \u2192 1, N = n+m, R * = 1 \u2212 the Bayes error for the transfer task with > 0, and assume that the following holds: Then, as N \u2192 \u221e both test test statistics S n,m (\u03c6 N , X n , Y m ) and T n,m (\u03c6 N , X n , Y m ) diverge in probability towards infinity, i.e. for any r > 0\nSketch of proof (full proof in Appendix A.2). The test statistics S n,m is lower-bounded by a rescaled version of\nThe finite-sample error R n,m (\u03c8 N ) approaches its population version R(\u03c8 N ) for large n, m, and the difference between R(\u03c8 N ) and R (\u03c8 N ) can be controlled over \u03b4. The rest of the proof is akin to standard consistency proofs in regression and classification. Namely, we can split R N (\u03c8 N ) \u2212 R * into approximation and estimation error and control these via a Universal Approximation Theorem (Hanin, 2017) , and Rademacher complexity bounds on the neural network function class (Golowich et al., 2017), respectively.\nThe main caveat of Theorem 3.2 is that it gives no explicit directions to choose the transfer task p and q . Whether the respective \u00b5-densities are L 1 -close to the testing densities in general cannot be answered, and similarly the Bayes error rate 1 \u2212 is not known beforehand. If abundant data for the testing task is at hand, then splitting the data is the safe way to go; if data is scarce, Theorem 3.2 gives justification that a reasonably close transfer task will have good power as well. * A similar Theorem holds also for the case of unbounded support, see Appendix B\nThe bounded support requirement (iv) on p and q can be circumvented as well -by choosing the support large enough one can always just truncate (X i ) and (Y i ) and will still satisfy requirements (ii) and (iii), especially also in the case of p = p and q = q with unbounded support. This procedure, however, requires knowledge of where to truncate the transfer distributions. Instead one can also grow the support of p and q with N; for more details, see Appendix B."}, {"section_title": "RELATED WORK", "text": "In this section, we give an overview over the state-of-the-art in non-parametric two-sample testing for high-dimensional data."}, {"section_title": "Kernel Methods", "text": "The methods most related to our method are the kernelized maximum mean discrepancy (MMD) (Gretton et al., 2012a) and the kernel Fisher discriminant analysis (KFDA) (Harchaoui et al., 2008) . Both methods effectively metricize the space of probability distributions by mapping distribution features onto mean embeddings in universal reproducing kernel Hilbert spaces (RKHS, (Steinwart and Christmann, 2008) ). Test statistics derived from these mean embeddings can be efficiently evaluated using the kernel trick (in quadratic time in the number of observations, although there are lower-powered linear-time variations). Mean Embeddings (ME) and 2018) use deep kernels in the context of relative goodness-of-fit testing without directly considering consistency aspects of this approach. Extensions from the GAN literature to two-sample testing is not straightforward since statistical consistency guarantees strongly depend on careful selection of the respective function classes. To the best of our knowledge, all previous works made simplifying assumptions on injectivity or even invertibility of the involved networks.\nIn this work we show that a linear kernel on top of transferlearned neural network feature maps (as has also been done by Xu et al. (2018) for GAN evaluation) is not only sufficient for consistency of the test, but also performs considerably better empirically in all settings we analyzed. In addition to that, our test statistics can be directly evaluated in linear instead of quadratic time (in the sample size) and the corresponding asymptotic null distributions can be exactly computed (in contrast to the MMD & KFDA).\nClassifier Two-Sample Tests (C2ST) First proposed by Friedman (2003) and then further analyzed by Lopez-Paz and Oquab (2016) , the idea of the C2ST is to utilize a generic classifier, such as a neural network or a k-nearest neighbor approach for the two-sample testing problem. In particular, they split the available data into training and test set, train a classifier on the training set and evaluate whether the performance on the test set exceeds random variation. The main drawback of this approach is that the data has to be split in two chunks, creating a trade-off: if the training set is too small, the classifier is unlikely to find a statistically relevant signal in the data; if the training set is large and thus the test set small, the C2ST test loses power.\nOur method circumvents the need to split the data in training and test set -Theorem 3.2 shows that training on a reasonably close transfer data set is sufficient. Even more, as shown in Section 3.1.3, our method can be interpreted as empirical risk minimization with additional fine-tuning of the last layer on the testing data, guaranteed to be as least as good as an equivalent method with fixed last layer."}, {"section_title": "EXPERIMENTS", "text": "In this section, we compare our proposed deep learning twosample tests with other state-of-the-art approaches."}, {"section_title": "Experimental setup", "text": "For the DFDA and DMMD tests we train a deep neural network on a related task; details will be deferred to the corresponding sections. We report both the performance of the deep MMD S n,m where we estimate the null hypothesis via a Monte Carlo permutation sample (Ernst et al., 2004) (we fix M = 1000 resam-pling permutations except otherwise noted), and the deep FDA statistic T n,m , for which we use the asymptotic \u03c7 2 H distribution. As explained in Section 3.2.1, for the DFDA we project the last hidden layer onto\u0124 < H dimensions using a PCA. We found the heuristic\u0124 := m+n 2 to perform well across a number of tasks (disjoint from the ones presented in this section). For the DMMD we do not need any dimensionality reduction. We calibrated parameters of both tests on data disjoint from the ones that we report results on in the subsequent sections.\nFor the C2ST, we train a standard logistic regression on top of the pretrained features extracted from the same neural network as for our methods.\nFor the kernel MMD we report two kernel bandwidth selection strategies for the Gaussian kernel. The first variant is the \"median distance\" heuristic (Gretton et al., 2012a) which selects the median of the euclidean distances of all data points (MMDmed). The second variant, reported by Gretton et al. (2012b) , splits the data in two disjoint sets and selects the bandwidth that maximizes power on the first set and evaluates the MMD on the second set (MMD-opt). We use the implementation provided by Jitkrittum et al. (2016) , which estimates the null hypothesis via a Monte Carlo permutation scheme (we again use M = 1000 permutations).\nFor the Smoothed Characteristic Functions (SCF) and Mean Embeddings (ME), we select the number of test locations based on the task and sample size. The locations are selected either randomly (as presented by Chwialkowski et al. (2015) ) or optimized on half of the data via the procedure described by Jitkrittum et al. (2016) . The kernel was either selected using the median heuristic, or via a grid search as by Chwialkowski et al. (2015) ; Jitkrittum et al. (2016) . In each case we report the kernel and location selection method that performed best on the given task, with details given in the corresponding paragraphs. Note that for very small sample sizes, both SCF and ME oftentimes do not control the type-1 error rate properly, since they were designed for larger sample sizes. This results in highly variable type-2 error rate for small m in the experiments. Again, we use the implementation provided by Jitkrittum et al. (2016) .\nIn addition to these published methods, we also compare our method against a deep kernel MMD test (k-DMMD), i.e. the MMD test where the output of a pretrained neural network gets fed into a Gaussian kernel (instead of a linear kernel as in our case). Jitkrittum et al. (2018) used this method for relative goodness-of-fit testing instead of two-sample testing. For image data, we select the bandwidth parameter for the Gaussian kernel via the median heuristic, and for audio data via the power maximization technique (in each case the other variant performs considerably worse); the pretrained networks are the same as for our tests and the C2ST.\nAll experiments were run over 1000 runs. Type-1 error rates are estimated by drawing both samples (without replacement) from the same class and computing the rate of rejections. Similarly, type-2 error rates are estimated as the rate of not rejecting the null hypothesis when sampling from two distinct classes. All figures of type-1 and type-2 error rates show the 95% confidence interval based on a Wilson Score interval (and a \"rule-of-three\" approximation in the case of 0-values (Eypasch et al., 1995) ). In all settings we fixed the significance level at \u03b1 = 0.05. In addition to that we show in Appendix D.3 empirically that also for smaller significance levels high power can be preserved. Preprocessing for image data is explained in Appendix D.2."}, {"section_title": "Control of Type-1 Error Rate", "text": "Since the presented test procedures are not exact tests it is important to verify that the type-1 error rate is controlled at the proper level. Figure 1a shows that the empirical type-1 error rate is well controlled for the amplitude modulated audio data introduced in the next section. For the other data sets, results are provided in Appendix D.4."}, {"section_title": "Power Analysis", "text": "Amplitude Modulated Audio Data Here we analyze the proposed test on the amplitude modulated audio example from (Gretton et al., 2012b) . The task in this setting is to distinguish snippets from two different songs after they have been amplitude modulated (AM) and mixed with noise. We use the same preprocessing and amplitude modulation as Gretton et al. (2012b) . We use the freely available music from Gramatik (2014); distribution p is sampled from track four, distribution q from track five and the remaining tracks on the album were used for training the network in a multi-class classification setting. As our neural network architecture we use a simple convolutional net-work, a variant from Dai et al. (2017) , called M5 therein; see Appendix D.6 for details. Figure 1b reports the results with varying number of observations under constant noise level \u03c3 2 = 1. Our method shows high power, even at low sample sizes, whereas kernel methods need large amounts of data to deal with the task. Note that these results are consistent with the original results in Gretton et al. (2012b) , where the authors fixed the sample size at m = 10, 000 and consequently only used the (significantly less powerful) linear-time MMD test.\nAircraft We investigate the Fine-Grained Visual Classification of Aircraft data set (Maji et al., 2013) . We select two visually similar aircraft families, namely Boeing 737 and Boeing 747 as populations p and q, respectively. The neural network embeddings are extracted from a ResNet-152 (He et al., 2016) trained on ILSVRC (Russakovsky et al., 2015) . Figure 1c shows that all neural network architectures perform considerably better than the kernel methods. Furthermore, our proposed tests can also outperform both the C2ST and the deep kernel MMD."}, {"section_title": "Facial Expressions", "text": "The Karolinska Directed Emotional Faces (KDEF) data set (Lundqvist et al., 1998) As test classes we select the dog breeds 'Irish wolfhound' and 'Scottish deerhound', two breeds that are visually extremely similar. Since the data set is a subset of the ILSVRC data, we cannot train the networks on the whole Ima-geNet data again. Instead, we train a small 6-layer convolutional neural network on the remaining 118 classes in a multi-class classification setting and use the embedding from the last hidden layer. To show that our tests can also work with unsupervised transfer-learning, we also train a convolutional autoencoder on this data; the encoder part is identical to the supervised CNN, see Appendix D.7 for details. Note that for this setting, the theoretical consistency guarantees from Theorem 3.2 do not hold, although the type-1 error rate is still asymptotically controlled. Figure 1e reports the results, with *-sup denoting the supervised, and *-unsup the unsupervised transfer-learning task. As expected, tests based on the supervised embedding approach outperform other tests by a large margin. However, the unsupervised DMMD and DFDA still outperform kernel-based tests. Interestingly, both the C2ST and the k-DMMD method seem to suffer more severely from the mediocre feature embedding than our tests. One potential explanation for this phenomenon is the ability of DMMD and DFDA to fine-tune on the test data without the need to perform a data split. The ADNI dataset consists of individuals diagnosed with Alzheimer's Disease (AD), with Mild Cognitive Impairment (MCI), or as cognitively normal (CN); Figure 2 shows exemplaric images of an AD and a CN subject. Table 1 shows that our test can detect statistically significant differences between MRI scans of individuals with a different diagnosis. Additionally, we evaluate whether our test can detect differences between individuals who have a known genetic risk factor for neurodegenerative diseases and individuals without that risk factor. In particular, we compare the two variants \u03b53 (the \"normal\" variant) and \u03b54 (the risk-factor variant) in the Apolipoprotein E (APOE) gene, which is related to AD and other diseases (Corder et al., 1993) . By grouping subjects according to which variant they exhibit we test for statistical dependence between a (binary) genetic mutation and (continuous) variation in 3D MRI scans. Table 1 shows that individuals with \u03b54 and \u03b53 APOE variants are significantly different, suggesting a statistical dependence between genetic variation and structural brain features. "}, {"section_title": "A PROOF OF THEOREMS", "text": "A.1 Control of type-1 error rate Proof of Theorem 3.1. (i) Under p = q, it holds that E[\u03c6(X 1 )] = E[\u03c6(Y 1 )] and \u03a3 = Cov(\u03c6(X 1 )) = Cov(\u03c6(Y 1 )). Then we have\nThen the first term in the last expression converges in distribution against N(0, r\u03a3) and the second term converges in distribution against N(0, (1 \u2212 r)\u03a3) by a multivariate Central Limit Theorem (note that \u03c6(X 1 ) lies within [\u22121, 1] H and hence all moments are finite). Since all X i and Y j are jointly independent, the limiting distributions are also independent, hence the whole term converges against N(0, r\u03a3) \u2212 N(0, (1 \u2212 r)\u03a3) = N(0, \u03a3). The rest follows again by the continuous mapping theorem."}, {"section_title": "A.2 Proof of Consistency", "text": "Before we begin the proof we start with some auxiliary definitions and preliminary results.\nAs in Section 3.1 we can use the regression framework with (Z i , t i ) i \u2282 R d \u00d7 {\u22121, 1}. Then Z i |t i = 1 \u223c p, Z i |t i = \u22121 \u223c q and similarly for (Z i , t i ) and all jointly independent. As we assume Pr(t = 1) = Pr(t = \u22121) = 1 2 , the distribution of (Z, t) is fully determined by specifying p and q and hence we write for the expected value e.g. E p,q [ f (Z, t)] for some function f .\nWe define the loss function\nwith corresponding empirical and expected risks\nThe Bayes risk under the transfer task will be denoted as\nThe following Lemma is based on Theorem 1 in (Golowich et al., 2017) and we will need it to bound the complexity of the neural network function class G N .\nLemma A.1. Let the data be a.s. be bounded by some B > 0 and\nThen, the empirical Rademacher complexity of G can be bounded as:\nProof of Lemma A.1. We define auxiliary function classes\nThen we can rewrite G as\nTherefore we can bound the Rademacher complexity a\u015d\nby standard learning theory arguments. For G 1 D\u22121 , we use the Rademacher bound found in (Golowich et al., 2017) Theorem 1 (we cannot use the Theorem directly on G since tanh is not positive homogeneous):\nThe original Theorem 1 in (Golowich et al., 2017) holds for depth D \u2212 1 networks, but we allowed networks of lower depth. However, one can fill up the networks to depth D\u22121 with identity weight matrices and identity activation functions; inspection of the proof of the Theorem then shows that the claim still holds.\nSince H = d + 1, the claim follows.\nWith these preliminary notions set up, we can proceed with the actual proof of consistency.\nProof of Theorem 3.2. We intend to show that R n,m (\u03c8 N ) is asymptotically strictly smaller than 1; the divergence of the test statistic then follows easily. We will proceed in 5 steps. First, we split R(\u03c8 N ) \u2212 R * into transfer error, estimation error (of the transfer task) and approximation error (of the transfer task). Second, we show that the approximation error converges to zero (due to a Universal Approximation Theorem for deep networks); third we show that the estimation error is asymptotically bounded by \u03b7, using a learning theory bound on the Rademacher complexity of the neural network function class. This together implies that R(\u03c8 N ) and R * are (\u03b4 + \u03b7)-close asymptotically. Fourth, we show that the R n,m (\u03c8 N ) \u2212 R(\u03c8 N ) p \u2192 0 and from this we finally deduce that the test statistics diverge to +\u221e."}, {"section_title": "Splitting the terms We have", "text": "The first term is bounded as follows:\ndue to boundedness of \u03c8 N and requirement (ii).\nThe second term can again be split:\n2. Convergence of min G N R (\u03c8) \u2212 R * : Let\u03bc be the Borel measure of Z (not conditioned on t), i.e. Pr(Z \u2208 A) =\u03bc(A) for any A \u2282 R d Borel. Following a similar argument as Lemma 30.2 in (Devroye et al., 2013) then yields the following. For any fixed > 0, select a measurable function h : R d \u2192 [\u22121, 1] such that |R(h) \u2212 R * | \u2264 4 , and a compact set K \u2282 R d with\u03bc(K) \u2265 1 \u2212 8 . Then, since compact-support continuous functions are dense in L 1 (\u00b5), there exists a continuous function f :\nFrom the universal approximation theorem for deep ReLU networks in (Hanin, 2017) , there exists N 0 \u2265 1 such that for all N \u2265 N 0 we can find a \u03c8 \u2208 G N with\nNote that the Theorem in (Hanin, 2017) holds for ReLUnetworks, but since tanh is invertible one can apply the universal approximation theorem on the first node in the last hidden layer, select the w N = [1, 0, . . . , 0] and still get the universal approximation property.\nCombining these yields, for N large enough,\nThen, since > 0 was arbitrary, and min \u03c8\u2208G N R (\u03c8) \u2265 R * we get min G N R (\u03c8) \u2192 R * as N \u2192 \u221e.\n3. Asymptotic closeness of R (\u03c8 N ) and min G N R (\u03c8): We can first bound by standard arguments:\n, t)|\u03c8 \u2208 G N } as the conjunction of neural networks with the loss function. The first term can be bound with high probability by two-sided Rademacher inequalities:\nfor any \u03b6 > 0. This complexity bound follows from Theorem 11.3 in (Mohri et al., 2018) if we insert the function class\u0124 := H N \u222a 2 \u2212 H N by noting that the loss function is 1-Lipschitz in both its arguments non-negative and bounded from above by 2.\nBut Lemma A.1 bounds the Rademacher complexity a\u015d\nfor some C > 0 and D N large enough. Then\nand the last term diverges to \u221e if \u03b2 2 N D N N \u2192 0. Hence, the righthand side in equation (2) converges to 0."}, {"section_title": "This shows that", "text": "Next we need to show that the empirical risk (over (Z, t), not (Z , t )) also is asymptotically smaller than 1.\nWe look at \u03be N,i := t i \u03c8 N (Z i ), which is a triangular array of random variables on [\u22121, 1]. We will use a weak law of large numbers for triangular arrays, see Theorem 2.2.11 in (Durrett, 2019) . Both requirements in the Theorem are satisfied since \u03be N,i is bounded, and hence we get\nBut as shown above, R(\u03c8 N ) is \u03b4-close to R (\u03c8 N ) and R (\u03c8 N ) is asymptotically \u03b7-close to R * ; hence we get Pr(R(\u03c8 N ) \u2212 R * \u2264 + \u03b4 + \u03b7) \u2192 1 for any > 0, and therefore Pr(R n,m (\u03c8 N ) \u2212 R * \u2264 + \u03b4 + \u03b7) \u2192 1 5. Divergence of test statistics Define M N = 1 \u2212 R n,m (\u03c8 N ), then Pr(M N \u2265 * \u2212 \u03b4 \u2212 \u03b7 \u2212 ) \u2192 1 for any > 0. Since \u03b4 + \u03b7 < * , we then have for any r > 0 :\ni.e. the version of S n,m where the last layer is still selected on the training data instead of the test data. Then it holds that\nsince all |\u03c8 N (X i )| \u2264 1 and m n \u2192 1. Hence, we also get Pr(\u015c n,m > r) \u2192 1 for any r > 0. But\nFor the DFDA test statistic, we have T n,m = nm n + m D n,m\u03a3 \u22121 n,m D n,m \u2265 nm n + m ||D n,m || 2 \u03bb min (\u03a3 \u22121 n,m ) = S n,m \u03bb max (\u03a3 n,m ) \u22121 . \u03bb max (\u03a3 n,m ) is always positive (due to the \u03c1 n,m > 0 summand), and also bounded from above by some C > 0 (due to the boundedness of all individual entries), therefore T n,m \u2265 C \u22121 S n,m .\nHence we also have S n,m , T n,m p \u2192 +\u221e."}, {"section_title": "B DISTRIBUTIONS WITH UNBOUNDED SUPPORT", "text": "Considering the case where p and q have unbounded support, but requirements (ii), (iii) and a variant of (i) in Theorem 3.2 are still satisfied, we can still prove a similar consistency result.\nIn particular, we can make p and q vary with N by replacing them with truncated, bounded-support versions that converge towards the true densities slowly enough. First, select p N and q N with support on [\u2212B N , B N ] d for some sequence B n \u2191 +\u221e, and ||p N \u2212 p || L 1 (\u00b5) \u2192 0 and ||q N \u2212 q || L 1 (\u00b5) \u2192 0. Then there exists a N 0 > 0 such that for all N \u2265 N 0 , requirements (i), (ii) and (iii) are satisfied for p N and q N . In practice these truncated variables can be achieved for example by rejection sampling from p and q .\nThe only part in the proof of Theorem 3.2 where we need the boundedness assumption on p and q is when bounding the Rademacher complexity of the class G N in equation 3. The modified Rademacher bound now i\u015d\nThe requirement for the exponent in equation (2) to diverge then is\nThe rest of the proof is as before. We can summarize this as follows: Theorem B.1. Let p q, n = n , m = m with n m \u2192 1, N = n+m, R * = 1\u2212 the Bayes error for the transfer task with > 0. Furthermore, let ||p N \u2212 p || L 1 (\u00b5) \u2192 0 and ||q N \u2212 q || L 1 (\u00b5) \u2192 0 for sequences of \u00b5-densities (p N ) N and (q N ) N , Assume that the following holds: Then, as N \u2192 \u221e both test test statistics S (\u03c6 N , X n , Y m ) and T (\u03c6 N , X n , Y m ) diverge in probability towards infinity, i.e. for any r > 0 Pr (S (\u03c6 N , X n , Y m ) > r) \u2192 1 and Pr (T (\u03c6 N , X n , Y m ) > r) \u2192 1."}, {"section_title": "C DIMENSIONALITY REDUCTION", "text": "In practice, we oftentimes first apply a PCA transformation on the data before computing the DFDA test statistic. Since we fit the PCA on the test data itself, however, the observations are not independent anymore and Theorem 3.1 is not directly applicable anymore. As an unsupervised linear transformation, however, we can show via a Slutsky-type argument that the normal approximation is still valid. Theorem C.1. Let (\u03be i ) i and (\u03be i ) i be all jointly independent and identically distributed on R d with bounded support and assume that n n+m \u2192 r \u2208 (0, 1) as n, m \u2192 \u221e. Let A N \u2208 R s\u00d7d be a PCA transform, fitted on \u03be 1 , . . . , \u03be n , \u03be 1 , . . . , \u03be m (N = m + n), for some s \u2208 {1, . . . , d}. Let \u03a3 = Cov(\u03be 1 ) with eigenvalues \u03bb 1 , . . . , \u03bb d sorted in descending order, and assume that \u03bb s \u03bb s+1 (if s < d).\nThen nm n + m\nas n, m \u2192 \u221e, where \u03a3 = diag(\u03bb 1 , . . . , \u03bb s ).\nNote that the \u03bb s \u03bb s+1 assumption is only necessary for uniqueness of the limiting distribution -in practice one can ignore this requirement.\nProof of Theorem C.1. Since A N is a PCA transformation, A N is the matrix with the normalized eigenvectors corresponding to the s largest eigenvalues of the empirical covariance matrix \u03a3 n,m . But, due to a weak law of large numbers,\u03a3 n,m p \u2192 \u03a3 and accordingly A N p \u2192 A with the population PCA A being the normalized eigenvectors corresponding to the s largest eigenvalues of \u03a3 (without loss of generality we can assume the row-wise signs to be determined by some deterministic procedure, and hence for large enough N, A N and A unique e.g. by requiring that the first non-zero entry in the vector be positive).\nDue to the same argument as in the proof of Theorem 3.1 (i),\nDue to a multivariate Slutsky theorem, then "}, {"section_title": "D.2 Image Preprocessing", "text": "For the deep learning-based methods (DFDA, DMMD & C2ST), before evaluation, all image data is rescaled to (224, 224) and normalized according to the requirements of the neural network.\nFor kernel-based tests we found different strategies to work differently well on each data set. Hence, for the Aircraft, Stanford Dogs and Birds data set, data is rescaled to (48, 48) dimensions and converted to grayscale. For the facial expression data, images were first cropped to the center (resulting in (462, 462) dimensions) and then rescaled to (96, 96) dimensions; no conversion to grayscale was performed. We found no increase in power for higher resolution (e.g. (224, 224))."}, {"section_title": "D.3 Sensitivity to Significance Level", "text": "Special care has to be taken if several hypotheses are tested at the same time, leading to a multiple testing problem. One simple approach to control the so-called familywise error rate (FWER, (Lehmann and Romano, 2006) ), i.e., the probability of at least one wrong rejection of a null hypothesis, is the Bonferroni correction (Lehmann and Romano, 2006) . The Bonferroni correction divides the original significance level \u03b1 by the number of tests to be performed. Therefore, in many practical settings the significance level for each test will be considerably lower than the \"standard\" values of 0.05 or 0.01. This represents a problem in practice, since approximating the distribution in the tails usually is more challenging. Here we only give results for the asymptotic DFDA distribution, since permutation-based methods do not scale well to very low significance levels. Figure 3a shows that our method controls type-1 error rate at significance levels 5 \u00b7 10 \u2212u for u = 2, 3, 4, 5; Figure 3b shows that even at small significance levels, the DFDA can still maintain relatively high power.\nD.4 Control of Type-1 Error Rate Figure 4 shows that both DMMD and DFDA properly control the type-1 error rate even at low sample sizes."}, {"section_title": "D.5 Birds Experiments", "text": "Here we report results on another fine-grained classification data set, the Caltech-UCSD Birds-200-2011, Caltech-UCSD Birds-200-2011 (Wah et al., 2011) . We selected two visually very similar species of birds, namely the \"Blue-winged Warbler\" and the \"Hooded Warbler\" for differentiation. Results are shown in Figure 5 ."}, {"section_title": "D.6 AM Audio Experiments", "text": "Data preprocessing consists of sampling the original audio signal at 8kHz, the resulting AM signal is sampled at 120kHz, and snippets of length 1000 are used for identification. Gaussian noise with standard deviation 1 is added to the samples after processing.\nThe model has four one-dimensional convolutional layers, each followed by Batch normalization, a ReLU activation and maxpooling. The last layer is fully connected, but only used for training the network, i.e., the feature extraction is fully convolutional. In contrast to the M5 network, we use an input layer with kernel size of 20 instead of 80 and the final global average pooling layer can be removed, to accommodate the significantly smaller input dimension of the audio snippets. We train the network to classify noisy AM snippets from the remaining songs on the album, with a multi-class cross-entropy loss and a L 2regularization of 10 \u22124 on all weights; we use the Adam optimizer for this task Kingma and Ba (2014). Table 2 shows the convolutional autoencoder architecture used in the experiments on the Stanford Dogs data set. The autoencoder was trained to optimize multi-scale structural similarity between input and output images."}, {"section_title": "D.7 Stanford Dogs Experiments", "text": "The supervised training was performed with a network with the same encoder as in Table 2 and a fully connected layer on top, to classify the remaining 118 dog breeds. Again, we use the multi-class cross-entropy loss.\nFor both the supervised and the unsupervised task we use the Adam optimizer and L 2 regularization of size 10 \u22124 ."}, {"section_title": "D.8 KDEF Experiments", "text": "Note that Jitkrittum et al. (2016) and Lopez-Paz and Oquab (2016) only compared tests that use train/test splits. Hence, results therein are reported for n te , which is the size of the test set of each sample, i.e. n te = 1 2 m in our case (n te = 201 corresponds to m = 402). Figure 3 : Results on the AM audio data for m = 50 with small significance levels \u03b1. We show average values over 10 6 tests, where we fixed the sample size m per population to be equal to 50. (a) Empirical type-1 error rates for small \u03b1 values consistently lie below the expected type-1 error rate (dotted line). (b) Empirical type-2 error rates."}, {"section_title": "D.9 Imagenet Training", "text": "For the aircraft, facial expression, and birds data set we use a ResNet-152, trained on the whole ILSVRC 2012 data set. Instead of training this network ourselves, we use the parameters and implementation provided in the PyTorch deep learning library Paszke et al. (2017) ."}, {"section_title": "D.10 MRI Scan Preprocessing and Experiments", "text": "The T1 MRI scans acquired through the MP-RAGE protocol were selected from GSP and ADNI. The scans were standardized to (256, 256, 256) and cropped to (96, 96, 96) dimensions with isotropic voxels of 1mm. Model architecture is shown in table 3. The model was trained for 400 epochs on 1413 MRI scans from GSP. The loss function was set to the mean squared error and the batch size was set to one. No MRI scans from ADNI was used for training."}, {"section_title": "E CODE AND DATA", "text": "We provide an implementation of our methods in the supplementary files.\nAll 2D imaging and audio data can be downloaded from the following sources: "}]