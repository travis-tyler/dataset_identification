[{"section_title": "Abstract", "text": "Abstract. Reconstruction of PET images is an ill-posed inverse problem and often requires iterative algorithms to achieve good image quality for reliable clinical use in practice, at huge computational costs. In this paper, we consider the PET reconstruction a dense prediction problem where the large scale contextual information is essential, and propose a novel architecture of multi-scale fully convolutional neural networks (msfCNN) for fast PET image reconstruction. The proposed msfCNN gains large receptive fields with both memory and computational efficiency, by using a downscaling-upscaling structure and dilated convolutions. Instead of pooling and deconvolution, we propose to use the periodic shuffling operation from sub-pixel convolution and its inverse to scale the size of feature maps without losing resolution. Residual connections were added to improve training. We trained the proposed msfCNN model with simulated data, and applied it to clinical PET data acquired on a Siemens mMR scanner. The results from real oncological and neurodegenerative cases show that the proposed msfCNN-based reconstruction outperforms the iterative approaches in terms of computational time while achieving comparable image quality for quantification. The proposed msfCNN model can be applied to other dense prediction tasks, and fast msfCNNbased PET reconstruction could facilitate the potential use of molecular imaging in interventional/surgical procedures, where cancer surgery can particularly benefit."}, {"section_title": "Introduction", "text": "Positron Emission Tomography (PET) provides non-invasive in vivo imaging of radioactive tracer that binds to a particular molecular target of interest. It is by far the most sensitive modality for non-invasive molecular assaying of the human body, and plays an important role in understanding and diagnosis of a wide range of diseases. A PET scanner collects photons emitted from radioactive tracer molecules serving as the imaging agent, and the reconstruction of PET images is the estimation of tracer distribution, which can be used for quantification of the molecular target of interest. PET reconstruction can be done by analytic methods to directly calculate the image in simple steps, such as filtered back projection (FBP), which has been commonly used in computed tomography (CT) reconstruction. However PET reconstruction with analytic methods is very challenging due to the fact that, in PET imaging, the emission of photons is a stochastic process and the noise statistics are hard to model by the analytic methods. For this reason, PET reconstruction algorithms based on a statistical model in the maximum likelihood (ML) framework have been developed with the advantages of having high flexibility in modelling the data acquisition process [?] . These methods usually require an iterative optimisation approach to keep updating the reconstructed image to fit the measured photon data, and can be very computationally expensive as the flexibility/complexity of the model grows for improving reconstruction image quality. The long reconstruction time required by the iterative methods also impedes the opportunities of incorporating PET imaging into interventional/surgical procedures, where cancer surgery can particularly benefit, for example, from in-operation tumour removal assessment [1] .\nRecently, with the development of deep learning, neural network based methods have shown promise in providing exciting solutions for medical/biological imaging problems [2] [3] [4] . The flexibility based on the universal approximation rule allows deep neural networks to learn complex nonlinear relationships in a task while being computationally simple.\nIn this work, we aimed to address the speed-quality trade-off in the existing PET reconstruction methods (analytic and iterative) by proposing an alternative approach for improvements in both accuracy and computational performance. We consider image reconstruction a dense prediction problem, for which solutions based on deep neural networks are rapidly developing, and we proposed a novel multi-scale fully convolutional neural network (msfCNN) for learning contextual features with both memory and computational efficiency. The model was trained using simulated data and then applied to real clinical data for fast reconstruction. The experimental results show that the msfCNN-based method can effectively reduce the reconstruction time while achieving comparable image quality for target density quantification and tumour detection."}, {"section_title": "Method", "text": ""}, {"section_title": "PET reconstruction as a dense prediction problem", "text": "Image reconstruction can be considered as a dense prediction problem, where for each voxel/pixel, a model predicts a value in the reconstructed image from the input data. In PET imaging, the raw data from the scanner are photon emission counts, which can be arranged as sinograms in the projection space. The mapping from the projection space to the image space can be achieved by back projection, which is a linear transform. In [4] the back projection was implemented by a fully connected layer in the neural network, however this can introduce millions of parameters, since the number of parameters n p = n v \u00d7 n d , where n v and n d are the numbers of voxels and photon detector pairs respectively. The back projection can also be calculated by inverse radon transformation with no need for parameters, where the measurement uncertainties are transferred into the image space, resulting in radial streak artefacts and high noise which are challenging to remove while preserving the original resolution. In this work, we firstly used inverse radon transformation to project backwards the photon counts data into the image space, and focused on reconstructing the PET images from there. The goal was to design a neural network with the capacity to compensate for the uncertainties in the photon emissions, which make PET reconstruction an ill-posed inverse problem, by using the large scale contextual information in the back projection image to accurately recognise and remove the artefacts and noise, hereby recovering the original image of the tracer distribution. We designed a novel multi-scale fully convolutional neural network (msfCNN) which achieves large receptive fields with both memory and computational efficiency, and does not suffer from spatial resolution loss while down-scaling and up-scaling, leading to less blurry reconstructed PET images."}, {"section_title": "Architecture of the multi-scale fully CNN (msfCNN)", "text": "Fig. 1. Architecture of the proposed multi-scale fully convolutional neural network (msfCNN) for 2D. For PET reconstruction the input is the back projection of photon emission data (in the image space) and the output is the reconstructed PET image. The numbers in the convolutional layers correspond to the size of kernels, the number of feature maps (n) and the dilation rate (d) for dilated convolution. The model has a downscaling-upscaling structure without resolution loss by using the periodical shuffling (PS) operations. The PS \u2191 operations have the same definition as in sub-pixel convolution to reshape the feature maps by a factor of 2\u00d72 in upscaling, and PS \u2193 is its inverse operation in downscaling, and the total feature map size (height\u00d7width\u00d7depth) does not change throughout all the layers. Arrows denote element-wise sum for residual connections.\nIn this work, we constructed the network in two dimensions. The proposed multi-scale fully convolutional neural network (msfCNN) consists of convolutional layers with the kernel size of 3 \u00d7 3. Efficient receptive field increase was obtained by incoporating dilated convolutions [5] which can aggregate multiscale contextual information with the same number of parameters. All dilated convolutions have the dilation factor of 2. To reduce the memory costs from having a constant size (height \u00d7 width) for all the feature maps throughout the network, previously pooling was used to downscale the spatial size of feature maps gradually as the network grows deeper with the increasing number of channels, however this can result in significant resolution loss which will be hard (or impossible) to recover by later layers. In [6] sub-pixel convolution with a periodic shuffling operator was proposed to jointly learn the feature extraction and up-sampling filter weights for super-resolution reconstruction, where the periodic shuffling operator P S \u2191 (T ) is defined as, for a tensor T of shape\nInspired by this idea we derived its inverse operator P S \u2193 (T ) x,y,d , for a tensor\nwhich allows the joint learning of the feature extraction and down-sampling filter weights when combined with the convolutional layers. As P S \u2193 changes the shape of the feature maps from r \u00b7 H \u00d7 r \u00b7 W \u00d7 D to H \u00d7 W \u00d7 r 2 \u00b7 D, no pooling is required, and all the information will be used by the following layers with no loss of spatial resolution. In addition, by using P S \u2191 and P S \u2193, the total size of the feature maps remains the same throughout all the layers which provides memory efficiency for layers with a larger number of feature maps. In this work we used the shuffling rate r = 2. Therefore the proposed msfCNN shares a similar encoder-decoder style structure as [2] but eliminates the problem of resolution loss. To facilitate training, residual connections were added, following the arrangement proposed in [7] ."}, {"section_title": "Implementation and training details", "text": "The training data were generated by simulations, where an image serving as the ground truth is forward projected into the sinogram space by radon transformation, and after Poisson is added, the sinogram is backward projected into the image space by the inverse radon transformation with ramp filter and linear interpolation to generate the input image. The ground truth images in this work were extracted from brain scans in the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The training data were generated from the axial slices from T1-weighted MR, FDG PET and AV45 PET data, and 45k images (MR:FDG:AV45 = 1:1:1) were used for training. The proposed model was implemented using Tensorflow [8] with Keras [9] , and trained with the initialisation proposed in [10] and Adam optimiser [11] with the settings learning rate = 0.001, \u03b2 1 = 0.9, \u03b2 2 = 0.999, = 1e \u2212 08, decay = 0.0, on a Tesla K40c GPU. Mean squared error was used as the training loss function."}, {"section_title": "Results", "text": ""}, {"section_title": "Reconstruction evaluation", "text": "We applied the model learned from simulated data to reconstruct PET images from clinical data acquired on a Siemens Biograph mMR scanner from patients. In the absence of ground truth, we compared the performance of the proposed msfCNN reconstruction with ordered subset expectation maximisation (OSEM), which is an iterative method, provided by the manufacturer with default settings which includes a post-reconstruction Gaussian filter of 3.5mm. For this PET-MR scanner, the PET reconstruction uses a pseudo CT image synthesised from T1-weighted and T2-weighted MR images acquired in the same imaging session to calculate a \u00b5 map [12] for attenuation/scatter correction. Attenuation, scatter, random, dead-time corrections and normalisation were applied to the PET data using the manufacturer's software before reconstruction. The scans were conducted in dynamic mode, and the reconstruction was performed to produce a series of time frames. PET data were acquired in list mode in 3D, and for the proposed msfCNN, which was implemented in 2D, the 3D projection data were firstly converted into 2D in the axial direction by using single-slice rebinning and then projected backwards by inverse Radon transformation with ramp filter and linear interpolation. Figure 2 shows the reconstruction of clinical ["}, {"section_title": "Amyloid imaging", "text": "18 F]florbetapir data of a patient with Alzheimer's disease (AD). [\n18 F]florbetapir is a PET radiotracer that binds to amyloid-\u03b2, which is considered to be a major target in the AD brain. Regional uptake of the tracer is of interest to assess the patient's amyloid-\u03b2 load, and the values were extracted from the reconstructed images based on the brain parcellation generated by using GIF [13] on the subject's T1-weighted MR image. The regional uptake values are shown in Figure 2 for 158 brain regions. Figure 3 shows the reconstruction of a ["}, {"section_title": "Tumour imaging", "text": "18 F]choline scan from a patient with a brain tumour. [\n18 F]choline is a PET radiotracer that gets absorbed by cancer cells and participates in cell proliferation. The proposed msfCNN-based reconstruction effectively recovered the tumour for detection."}, {"section_title": "Computational time", "text": "The proposed msfCNN method took \u223c15 milliseconds for a slice, \u223c1.9 seconds per frame and finished the reconstruction in 54 seconds for the whole [ 18 F]florbetapir scan (29 time frames), and 42 seconds for the whole [\n18 F]choline scan (22 time frames), using a Tesla K40c GPU. The OSEM method took \u223c1 minute per frame (perhaps with overheads of preprocessing), using a Tesla K20c GPU. "}, {"section_title": "Discussion and Conclusion", "text": "In this work, we propose a novel architecture of multi-scale fully convolutional neural networks (msfCNN) for fast PET image reconstruction. The proposed downscaling-upscaling structure using the periodic shuffling operation from subpixel convolution and its inverse has shown to be effective in preserving spatial resolution. The model applied to clinical data was only trained with simulated data without scanner modelling, which suggests the performance of the proposed model can be further improved by using more realistic training data. The input data for reconstruction was the back projection of sinograms. This avoids the introduction of a large number of parameters to map the back projection to a fully connected layer, and reduces the model's complexity. An end-to-end model that implements the back projection using convolutional layers will be explored in the future. In addition, the implementation of the proposed msfCNN model will be extended to three dimensions. 18 F]choline data showing a brain tumour (yellow arrows). The OSEM reconstructions were not used for training. For the proposed msfCNN method, the presence of the 'stacking' artefacts in the coronal and sagittal planes is due to the fact that msfCNN only operates in 2D on the axial slices."}]