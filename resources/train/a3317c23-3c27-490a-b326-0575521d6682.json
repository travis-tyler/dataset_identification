[{"section_title": "", "text": "I first engage in an empirical detection exercise using data on analyst forecasts and firm accounting statements. I apply regression discontinuity techniques to look for endogenous changes or discontinuities in the behavior of firms that just meet analyst earnings forecasts, exactly those firms which might be expected to have manipulated their behavior. I find that firms just meeting analyst forecasts exhibit lower longterm investment growth. Furthermore, CEOs and other executives just failing to meet targets receive lower overall compensation. I then build and structurally estimate a macro model of long-term growth. A distribution of heterogeneous firms makes R&D investments and engages in accounting manipulation subject to firm-level profitability shocks. Firm managers face firm-level earnings forecasts as well as pressure to meet such targets. I structurally estimate both the profitability shock processes governing fundamentals at firms as well as the amount of earnings pressure. Earnings targets lead to more volatile, and less e cient, R&D as firms react to short-term profitability shocks by changing their long-term investments. The result is lower macro growth (a reduction of around 6 basis points), lower aggregate welfare (by around 0.44% in consumption-equivalent terms), and lower average firm value (by around 1%). Compared to existing quantitative estimates of the welfare impact of business cycles or international trade, such costs are large. The second chapter, \"Alternative Methods for Solving Heterogeneous Firm Models,\" compares four algorithms for the solution of a canonical neoclassical heterogeneous firms model with fixed capital adjustment costs and firm-level productivity shocks. They include the Krusell-Smith method, the Explicit Aggregation method, the Parametrized Distributions method, and the Projection plus Perturbation method. These four methods, which have been evaluated in the context of models of consumer behavior, have not previously been studied in an environment of firm decisions with sticky or discrete choices. I find that the classic Krusell-Smith algorithm is comfortingly robust although quite computationally intensive. By contrast, the Explicit Aggregation and the Parametrized Distributions techniques o\u21b5er large speed gains at the loss of some internal accuracy. The Projection plus Perturbation technique o\u21b5ers an attractive scalability with a conceptually distinct approach. Trade between low-cost, low-wage countries like China and the developed world has increased substantially in recent decades. Recent empirical research suggests that firms exposed to this liberalization engage in more innovation. In a third chapter entitled \"Trapped Factors and China's Impact on Global Growth,\" which is joint work with Nick Bloom, Paul M. Romer, and John Van Reenen, we build a productcycle model of growth and trade which seeks to both rationalize this micro-level response of firm innovation to low-cost competition as well as quantify the dynamic or growth based gains from trade liberalization. Trade liberalization between a high-cost, innovating Northern economy and a low-cost Southern economy leads to an increase in long-term growth. However, we also embed adjustment costs at the firm level which \"trap\" factors of production in Northern firms which face import competition. With a surplus of inputs after a trade shock, such firms innovate more, rationalizing the micro evidence on innovation and liberalization. Quantitatively, the overall dynamic gains from trade as well as the corresponding increases in long-term growth are large. Our trapped factors mechanism as well as liberalization with China each contribute a substantial portion to the overall increase in growth rates within our model."}, {"section_title": "v", "text": ""}, {"section_title": "Chapter 1", "text": "The Macro Impact of Short-Termism"}, {"section_title": "Introduction", "text": "For over a century economists have been concerned that pressure from investors for short-term results can damage the long-run growth of companies. For example, Alfred Marshall famously wrote in 1890 that people act like \"children who pick the plums out of their pudding to eat them at once.\" This paper examines the macroeconomic growth implications of short-termism through the lens of analyst earnings targets, a particularly prevalent and observable manifestation of short-term pressure today. 1 The benefits from investments in research and development (R&D) or other intangible expenditures may either fail to materialize or appear only much later. Yet the costs of these long-term investments generally must be borne today. In a particularly important example of this tradeo\u21b5, R&D must be immediately subtracted or expensed from profits according to the Generally Accepted Accounting Principles (GAAP) governing public firms in the US. 2 If failure to deliver short-term results has adverse consequences, firms or managers may therefore be willing to sacrifice some long-term value to deliver higher profit now. Economic theory for decades has modeled overall growth and changes in aggregate productivity as the result of longterm investment at the microeconomic level. So a heavy dependence of long-term investment on short-term pressures can be crucial for understanding the process of economic growth in the macroeconomy. 3 Furthermore, factors impacting long-term growth disproportionately a\u21b5ect the economy in quantitative terms because of their compounding e\u21b5ects. This paper takes several concrete steps towards empirically documenting and structurally quantifying the costs of short-termism, through the lens of pressure to meet earnings forecasts, on innovative investments, long-term growth, and welfare. Research analysts at stock brokerages routinely forecast the earnings of public companies. Firms publicly release statements of earnings, an accounting measure of profitability also known as net income or simply profits, at the end of each fiscal quarter and year. The financial press as well as equity market participants follow these releases closely during earnings season. Around 90% of the respondents to a broad survey of managers in Dichev et al. (2013) report systematic pressure, internal and external to their firms, to meet earnings benchmarks. Consistent with these reports, Figure 1.1 displays the annual distribution of the di\u21b5erence between realized and forecast earnings (forecast errors), scaled by firm assets for a panel of US public firms. A disproportionate number of firm-years report zero or just positive earnings forecast errors, i.e. display profits that just meet or beat analyst forecasts, while the mass of forecast errors is hollowed out just below zero. 4 depreciation or amortization of R&D development costs (PricewaterhouseCoopers, 2013). 3 See, for example, work by Romer (1990), Aghion and Howitt (1992), Grossman and Helpman (1991), or Acemoglu et al. (2013). Also, note that a recent strand of papers models endogenous growth as the product of idea flows (Perla and Tonetti, 2014;Lucas and Moll, 2011;Alvarez et al., 2008;Sampson, 2014). Since exploiting idea flows remains costly for firms, the tradeo\u21b5 between growth and performance remains. 4 Marinovic et al. (2012) and Hong and Kacperczyk (2010) overview analyst earnings forecasts. Burgstahler and Chuk (2013) emphasizes that the discontinuity in Figure 1.1 is robust. The McCrary (2008) sorting test strongly rejects continuity of the distribution at the zero forecast error level. See Garicano et al. (2013), Gourio and Roys (2012), Chetty et al. (2011), Daly et al. (2012), and Allen and Dechow (2013), respectively for evidence of similar bunching in French firm sizes around regulatory thresholds, Danish income around tax kinks, nominal wage changes around zero, and even marathon Earnings benchmarks and potential concerns about short-termism are not exclusive to public firms. Interestingly, surveys of private and public firm managers reveal quite similar rates of reported earnings pressure. 5 However, the pressures and incentives surrounding innovation at large public firms, measured directly in this paper, are independently interesting for our broad understanding of innovation in the macroeconomy. Public firms undertake almost two-thirds (67%) of all non-government R&D expenditures in the United States. 6 A long history of research into apparent real earnings manipulation by public firms also suggests that R&D investments systematically and discontinuously change around earnings benchmarks or targets including analyst forecasts of profits. 7 More broadly, a literature on investments in technology notes that budgeting deadlines and agency frictions within firms can constrain the ability of organizations to invest in improved productivity, and other work in economic history suggests that the presence of sharp performance targets may contribute to overall ine ciency. 8 My paper first establishes two empirical facts using a merged database of analyst forecasts and firm accounting data. First, firms that just meet or beat analyst forecasts in a particular year exhibit discontinuously lower R&D and broader intangibles investment growth. There is a drop of around 30% in mean R&D and intangibles growth relative to firms that just fail to meet earnings forecasts. Such discontinuities, detected using flexible nonparametric regression discontinuity estimators, are consistent with systematic manipulation of long-term investment to meet analyst forecasts of earnings. Relatedly, a survey of executives at large US public firms in Graham finish times around focal points. 5 See Dichev et al. (2013), Table 13. 6 The R&D statistic reflects the aggregation of R&D expenditures in my baseline sample from Compustat (xrd) compared to aggregate research expenditures in 2000 from the NSF Survey of Industrial Research and Development (total private R&D). Note that recent empirical studies by Bernstein (2012), Aghion et al. (2013), or Asker et al. (2014) suggest that the quality and quantity of innovation and investment at publicly traded firms can either be lower than in their private counterparts or hinge crucially upon factors such as institutional ownership with long horizons. 7 See for example, Bhojraj et al. (2009), Baber et al. (1991), Roychowdhury (2006), or Gunny (2010). 8 See Liebman and Mahoney (2013) for a study in the US government and Atkin et al. (2014) for an experiment Pakistani manufacturing. See Meng et al. (Forthcoming) for evidence that food procurement quotas across regions may have contributed to the severity of the Great Famine in China. et al. (2005) corroborates the result: almost half of managers reveal that they would reject a positive net present value project if taking the project meant missing analyst forecasts of earnings. 9 In a second empirical contribution, the paper then applies the same techniques to inspect manager incentives and stock returns. CEOs just failing to deliver profits above consensus analyst expectations face drops of around 7% in total compensation. Earnings pressure is not limited to solely the CEO of a company. The several most highly paid executives in a company face around 5% less total compensation for just failing to meet analyst targets. Furthermore, stock returns are discontinuously lower for firms just failing to meet earnings targets. Firms that just fail to meet earnings targets see around 0.64% lower abnormal cumulative returns in a ten-day window to the earnings release date. The finding of apparent compensation incentives for managers to deliver short-term results, together with capital market pressures, helps to motivate real earnings manipulation but also concurs with a large literature on performance-based incentives and the relationship between earnings announcements and returns. 10 Building o\u21b5 of my empirical findings, the second part of the paper builds a theoretical model linking earnings targets and aggregate growth. The model features managers of heterogeneous firms making R&D investment decisions together with pure paper or accounting manipulation choices subject to both persistent and transitory profitability shocks. R&D expenditures by firms result in random innovation arrival according to a quality ladder structure that aggregates in general equilibrium. Earnings forecasts, endogenously produced by an outside sector of analysts, provide short-term pressure on managers who seek to avoid costs resulting from failure to meet earnings forecasts. The model is agnostic about the source of these costs for managers, but in practice earnings miss costs may be purely private to the manager due to reputational or career concerns (Dichev et al., 2013), borne by the firm due to 9 Firms of course can use paper or accounting rather than operational or real decisions to boost earnings above analyst forecasts. Studies such as Burgstahler and Eames (2006) document that discretionary accruals appear to be unusually high for firms just meeting earnings targets. 10 See, for example, Larkin (2014), Oyer (1998), Murphy (1999), Murphy (2001), Matsunaga and Park (2001), Edmans et al. (2013), Jenter and Lewellen (2010), Jenter and Kanaan (Forthcoming), Eisfeldt and Kuhnen (2013), and Asch (1990), Bhojraj et al. (2009), Bartov et al. (2002). higher external finance costs or disrupted communication costs with outsiders (Graham et al., 2005), or even the result of explicit manager compensation policies chosen by firms (Matsunaga and Park, 2001). 11 After laying out the model structure, the paper employs numerical solution methods and parametrizes the model using a combination of calibration and structural GMM estimation. Structural estimation here exploits the moments of R&D expenditures, sales, and forecast errors in a panel of firm-level data on thousands of large US public firms to produce a \"Baseline\" quantitative model economy. My main quantitative results in the paper compare the Baseline environment with a \"No Targets\" counterfactual economy in which there are no costs or incentives for managers to meet or beat earnings forecasts. The Baseline model with earnings targets for firms qualitatively reproduces the kinked earnings forecast error distribution in Figure 1.1, with a disproportionate mass of firm-years just above targets and a hollowing out the forecast error distribution below zero. The counterfactual No Targets economy, by contrast, fails to reproduce a kinked distribution. Furthermore, while the No Targets economy produces a smooth distribution of R&D growth across the zero forecast error threshold, the Baseline model leads to a cut in R&D growth for firms just meeting earnings targets, consistent with the empirical evidence from the first portion of the paper. Finally, the counterfactual exercise reveals insights into a body of research in corporate finance studying the release of information contained within earnings releases and surprises (Stein, 1989;MacKinlay, 1997). Firms in the Baseline model with mediocre profitability shocks are able to find the resources, either in long-term investment manipulation or paper obfuscation, to boost earnings above target. Therefore, firms that miss an earnings forecast in equilibrium are far less profitable than firms meeting or beating earnings forecasts, a quantitative di\u21b5erence which is positive but muted by contrast in the No Targets economy. Such considerations may help to explain why firms report increased pressure from outsiders to explain or divulge more information about the prospects of the firm if they miss analyst expectations. The information revelation upon missing an earnings target also naturally helps to motivate the change in stock returns I document empirically for firms just failing to meet earnings targets. Within the model, short-termism can be defined as responsiveness of forwardlooking R&D policies to purely transitory or short-term shocks to profitability, even when those shocks contain no information about the profitability of innovation in the long term. The Baseline economy features such short-termism, resulting in lower and more volatile R&D expenditures. 12 Increased volatility impacts the overall e ciency of R&D expenditures for firms even absent any e\u21b5ects on levels, since my theoretical structure includes curvature or diminishing returns to R&D. Just as Barlevy (2004) theoretically links business cycle volatility to reduced growth in the macroeconomy in the presence of diminishing returns to investments, my model implies that firms subject to transitory profitability shocks and choosing more volatile R&D expenditures have fewer innovation arrivals than would result from a smoother long-term investment path. 13 At the microeconomic level the result is lower firm value on average, an approximately 1% reduction in mean firm value in the Baseline economy relative to the No Targets case. By studying the distortion to long-term investments resulting from earnings targets, I contribute to a literature on structural estimation in dynamic corporate finance which outlines the costs to firms resulting from, for example, financial frictions, CEO firing costs, or agency frictions surrounding cash holding and investment. 14 At the macroeconomic level, the Baseline economy with earnings targets is characterized by lower aggregate growth of around 2.25% per year relative to a growth rate of around 2.31% per year in the No Targets environment. By exhibiting shorttermism and responding to purely transitory profitability shocks with their long-term R&D investments, manager earnings pressure causes a sort of research misallocation, 12 High R&D sensitivity to profitability echoes empirical work in corporate finance. See, for example, Borisova and Brown (2013), , and Mairesse et al. (1999). 13 For further macroeconomic work on the link between volatility and growth, see for example Ramey and Ramey (1995) or Imbs (2007). 14 See Hennessy and Whited (2007), Taylor (2010), and Nikolov and Whited (2010), respectively. Strebulaev and Whited (2012) provides an overview of structural estimation in dynamic corporate finance. whereby the e ciency of aggregate innovation declines. 15 Small changes in permanent growth rates translate into quantitatively large di\u21b5erences in welfare, because these changes are continuously compounded over time. In my model, the removal of earnings targets results in an overall increase in welfare of 0.44%, i.e. consumption in each period would need to be increased by almost half a percent in the Baseline to make the aggregate household as well o\u21b5 as in the No Targets balanced growth path. 16 For comparison, recent quantitative estimates of the welfare gains from the elimination of business cycles are around 0.1-1.8%, or the static gains from trade according to recent work could be approximately 2.0-2.5%. 17 Overall, short-termism from earnings targets results in a quantitatively large distortion to long-term growth and the macroeconomy. The main quantitative contribution of the paper should be interpreted as the delineation of the costs of earnings targets as outlined above. However, earnings targets may of course also provide benefits to firms or society, omitted from the baseline cost measurements. 18 For example, earnings forecasts may contribute to more accurate valuation of firms or alleviate financial frictions at otherwise healthy firms. In fact, a series of recent theoretical and empirical papers indicates that alleviation of financial frictions or the liberalization of equity markets can indeed reap e ciency gains from better allocation of resources throughout the macroeconomy. 19 A second source of potential benefits from earnings targets operates within firms through the 15 The misallocation from profitability volatility is distinct from but related to the broader literature on misallocation in Hsieh and Klenow (2009), Restuccia and Rogerson (2008), Peters (2013), Asker et al. (2013), or Yang (2014). 16 US consumption was around $11,500 billion in 2013 according to the BEA as of March 2014, so a 0.44% increase in consumption is equivalent to a permanent increase in consumption of around $51 billion each year. The overall welfare gains decompose into 1.32% dynamic gains from growth rate changes and a static loss of -0.86% due to higher initial investment in R&D. 17 See Krusell et al. (2009) for the welfare consequences of business cycles, Costinot and Rodr\u00edguez-Clare (2015) or Melitz and Redding (2013) for the welfare gains from trade. Also, see Hassan and Mertens (2011) for the social cost of \"near-rationality\" in investment, around 2.4% in consumptionequivalent terms. 18 An interpretation of this sort is the norm for cost calculations in macroeconomics, with the most prominent example being the literature on the costs of business cycles (Lucas, 2003;Barlevy, 2004;Krusell et al., 2009). Gains from the elimination of business cycles may not be achievable if their elimination is costly. 19 See, for example, Midrigan and Xu (2014), Moll (Forthcoming), Buera and Shin (2013), Buera et al. (2011), or Campello et al. (2014). provision of discipline to managers in the presence of agency conflicts. Compensation schemes which explicitly or implicitly punish managers for failure to meet publicly observable earnings forecasts may lead to firm or social gains. The final portion of the paper analyzes multiple sources of agency conflicts within the existing model of dynamic manager investment used initially to estimate the costs of earnings pressure. A corporate finance investment literature emphasizes that firms are riddled with agency frictions leading to a wedge between the interests of managers and firms as a whole (Stein, 2003). Two classic forms of agency conflict include unobservable shirking by managers (Edmans et al., 2009;Grossman and Hart, 1983) and empire building motivated by private manager benefits from size or investment (Nikolov and Whited, 2010;Jensen, 1986). When managers can provide low e\u21b5ort, I show that for strong enough shirking motives earnings targets within manager compensation contracts may improve value for firms as well as social welfare. The dynamic distortion to long-term investment, while costly, may be overwhelmed by the gains to levels obtained from increased discipline. In a second case, when firm agency conflicts are instead characterized by empire-building tendencies for managers, earnings targets may improve firm value by restraining R&D investments. If the social and private returns to R&D di\u21b5er, however, restraint of R&D through earnings pressure can lead to an exacerbation of underinvestment from the social perspective and an increase in social losses from earnings targets. So, the model's implications depend crucially on assumptions about the exact form of agency conflicts within firms. Motivated by this consideration, I focus on qualitative results within my discussion of the benefits of earnings targets, demonstrating possibilities over a broad range of model parametrizations. Section 1.2 describes my data and lays out the empirical results linking earnings forecasts, long-term investment, and CEO incentives. Section 1.3 describes the quantitative model of earnings pressure and growth, together with my numerical solution and estimation strategy. Section 1.4 performs the main quantitative analysis estimating the costs of short-termism for firms and the macroeconomy. Section 1.5 explores potential agency benefits from short-term pressure. Section 1.6 concludes. An appendix follows describing the data, theory, and numerical solution method."}, {"section_title": "Data and Empirical Discontinuities", "text": "This section empirically investigates the manipulation of long-term investment and incentives for executives surrounding earnings targets for firms. First, after joining analyst forecasts of earnings with the accounting releases of US public firms, my analysis reveals that firms just meeting earnings targets exhibit substantially lower long-term investment growth in R&D as well as broader intangibles. Similarly, CEOs and other executives at firms just failing to meet earnings targets receive discontinuously lower compensation. This paper draws on data from two main sources. First, I use millions of earnings forecasts at the firm-analyst level from the Institutional Brokers Estimates System (I/B/E/S) database. Actual or realized values of firm \"Street\" earnings per share accompany the analyst forecasts in I/B/E/S. 20 I also use Compustat data drawn from the annual accounting reports of public firms. Linking the I/B/E/S and Compustat datasets results in a panel of around 25,000 firm-fiscal year observations with consensus analyst forecasts, Street realizations, and basic accounting outcomes. Around 4,000 firms from 1983-2010 are available in the combined unbalanced panel. The sample primarily consists of larger firms, accounting for around 11% of US employment, 67% of all US private R&D expenditures, and total sales of around 31% of US GDP. 21 See Data Appendix Table A.1 for descriptive statistics on the sample. I also incorporate Execucomp data on CEO and executive compensation where possible, as well as Center for Research in Security Prices (CRSP) 20 So-called Street earnings, over which firms possess moderate discretion, are the appropriate measure of earnings for my purposes, since Street earnings are more widely followed by financial market participants and observers than the net income measures reported in Compustat (Bradshaw and Sloan, 2002). 21 For these comparisons, US employment is total nonfarm payrolls according to the BLS in 2000 (St. Louis FRED variable PAYEMS), while Compustat employment is the variable emp. US R&D expenditures are drawn from the National Science Foundation Survey of Industrial Research and Development in 2000, with R&D for the corresponding year from Compustat variable xrd. US nominal GDP in 2000 comes from the BEA (St. Louis FRED variable GDPA), with Compustat gross sales in variable sale. data on stock returns. Data Appendix A provides further information on the datasets, the sample restrictions imposed, and the construction of individual variables. If managers face incentives to meet earnings forecasts, then firms should avoid reporting profits just below analyst forecasts if possible, instead taking actions throughout their fiscal year to satisfy expectations. In this section I employ a flexible empirical tool, nonparametric regression discontinuity techniques, to identify exactly this type of earnings manipulation through changes in the distribution of long-term investment as firms just meet forecasts. By the first application of regression discontinuity estimators to my knowledge in this context, I contribute to a literature which treats similar results as prima facie evidence of earnings manipulation by firms. 22 Throughout the empirical analysis, my preferred measure of the forecast error for a particular firm j in year t is the realized value of Street earnings Street jt minus the median analyst forecast of firm earnings made from the middle of the same fiscal year Street f jt scaled by firm assets. This measure of earnings forecast errors is a standard one used in accounting studies (Burgstahler and Eames, 2006;Burgstahler and Chuk, 2013). Forecast errors serve as the running variable in my regression discontinuity estimation with a cutpoint of zero. The first measure of investment I consider at firms is the tangible investment rate. Since tangible capital expenditures are depreciated from earnings over time rather than immediately expensed as incurred, their impact on current earnings and hence usefulness as a tool for earnings manipulation is diluted. Ex-ante, therefore, I expect little change in tangible investment rates for firms just meeting earnings targets. By contrast, two measures of long-term investment, R&D expenditures and broader \"Intangibles\" expenditures are both immediately expensed from earnings in the period incurred. 23 A growing empirical literature within economics and finance concludes that long-term investment expenditures contribute 22 See, for example, Roychowdhury (2006), Gunny (2010), Baber et al. (1991), or Burgstahler and Eames (2006). 23 Intangibles expenditures are equal to selling, general, and administrative (SG&A) expenditures. SG&A, a basic accounting item, enjoys extensive coverage within the Compustat database and include not only R&D expenses but also a range of other nonproduction expenses such as management labor compensation, training expenditures, and advertising costs. to long-term profitability for firms, to aggregate productivity over the business cycle, and to an improved explanation of stock returns in the cross section of firms. 24 The first three columns of Table 1.1 above report regression discontinuity estimates of the predicted di\u21b5erence in the tangible investment rate, overall intangible expenditures growth, and R&D growth for firms just meeting their earnings targets in a given year relative to firms failing to meet an earnings target. I compute results using outcomes demeaned by firm then year, controlling for both permanent trend heterogeneity across firms in long-term investments as well as business cycle e\u21b5ects. 25 I detect no discontinuity in tangible investment rates. By contrast, R&D growth and overall intangibles growth are both approximately 2.5% lower for firms just meeting an earnings target. The discontinuities are statistically significant and economically meaningful, each representing a cut of around 30% of average annual long-term investment growth. 26 Note that I make no direct causal claims from my regression discontinuity results of the form that is typically relied upon in applied microeconomics (Lee and Lemieux, 2010). By contrast, the apparent endogenous \"sorting\" of firms to the right of the zero forecast error cutpoint, which would typically be considered a threat to identification, lies at the very core of my argument for the economic impacts of earnings targets. In a later section, I build and estimate a quantitative model of R&D investment and earnings forecasts. The model demonstrates that reduced R&D growth around the zero forecast error threshold should be expected in the presence of incentives for managers to meet earnings targets but would otherwise be absent. Such results 24 See for example Papanikolaou (2013, 2014), Gourio and Rudanko (2014), McGrattan and Prescott (2014), Hall (2004), Corrado et al. (2009), Corrado et al. (2013), or Corrado et al. (2012). 25 Of course this implies that the results from Table 1.1 are based upon a two-stage procedure. Table 1.1 follows the literature by relying upon straightforward clustering at the firm level in the calculation of standard errors. For robustness, however, Table A.3 in Data Appendix A reports the results with no qualitative changes from a block bootstrap procedure taking into account within-firm correlation as well as uncertainty associated with the first-stage demeaning of outcome variables. 26 For all the regression discontinuity tests in this section, see Table A.2 in Data Appendix A for placebo checks. Data Appendix Figure A.1 plots a range of robustness checks with bandwidth choice alternatives to the optimal Imbens and Kalyanaraman (2011) value used in Table 1.1. Placebo checks reveal no significant breaks at alternative cutpoints away from 0, and bandwidth plots reveal robustness. structurally support the convenient use of regression discontinuity methodology as a detection mechanism in this context. Pressure to meet earnings targets can represent the product of explicit e\u21b5orts of the distributed shareholders or boards of firms to provide discipline to CEOs and managers with interests divergent from their own. Therefore, discipline for managers may be evident in observed compensation for the CEO or for the several most highly paid executives in a firm. 27 Table 1.1 displays estimated discontinuities in pay. CEOs that generate earnings just below analyst forecasts earn approximately 7% less, while the top managers as a whole receive around 5% lower compensation. Discontinuous manager incentives in my sample link to a literature in corporate finance and accounting that documents either discontinuities in manager compensation at earnings benchmarks or interaction between investment responses to earnings targets and CEO equity incentives (Matsunaga and Park, 2001;Edmans et al., 2013). However, just as in the case of long-term investments, my results represent to my knowledge the first application of regression discontinuity methodology to the study of earnings target incentives. Finally, Table 1.1 also documents a discontinuity in abnormal returns. Firms just failing to meet targets have approximately 0.64% lower cumulative abnormal returns in a ten-day window to the earnings release date. This result corroborates literature on the information content within earnings releases as well as on a capital market premium to meeting or beating analyst expectations. 28 However, also note that horizon matters for the interaction between earnings targets and outcomes. Changes in long-term investments such as R&D expenditures over the course of the year naturally take time to implement. The results in this paper therefore reflect earnings forecasts made for the full fiscal year from the perspective of the middle of the year, i.e. from a two-quarter horizon. The single exception to this rule is the discontinuity in abnormal returns, which I document using a forecast horizon of one quarter. Following the related discussion of forecast horizons in Bhojraj et al. (2009), I feel that these timing choices strike the appropriate balance between allowing for R&D and investment choices to be implemented, on the one hand, and incorporating a fuller range of information available to capital market participants when examining return patterns on the other hand.  1983-2010 1983-2010 1983-2010 1992-2010 1992-2010 1983-2010 Firms  3969  3969  3969  2349  2382  7794  Observations  23084  23084  23084  17661  114296  48297  Relative to Mean 1.4% -27.0% -32.9% 6.78% a 4.88% a 0.64% a Note: *,**,*** denote 10, 5, 1% significance. The regression discontinuity estimation relies on local linear regressions and a triangular kernel, with bandwidth chosen via the optimal Imbens and Kalyanaraman (2011) approach. Standard errors are clustered at the firm level. The estimates represent the mean predicted di\u21b5erences for firms just meeting earnings forecasts relative to firms just failing to meet forecasts. Earnings forecast errors are Street earnings minus median analyst forecasts from a 2-quarter horizon, scaled by firm assets as a percentage. Investment Rate is the percentage tangible annual investment rate. Intangibles growth is annual percent selling, general, and administrative expenditures growth. R&D growth is annual percent research and development expenditure growth. CEO Pay, Executive Pay are the log of total compensation for the CEO and several most highly compensated executives at a firm, respectively. Abnormal Returns are the cumulative abnormal returns for a firm in a ten-day window to the announcement date, market adjusting using the returns of the S&P 500. For returns analyst forecasts are drawn from a 1-quarter horizon. a Executive pay and stock returns are already in normalized form, and these values duplicate discontinuity estimates."}, {"section_title": "Model of Earnings Pressure and Growth", "text": "In this section I build a quantitative model of endogenous growth and earnings targets, followed by a discussion of the equilibrium concept and numerical solution method. Finally, I explain my parametrization of the model based on GMM structural estimation using firm-level moments from my Compustat and I/B/E/S sample. "}, {"section_title": "Baseline Model Structure", "text": "The household also makes a savings choice B t+1 in a one-period bond with interest rate R t+1 . As is standard, in general equilibrium household Euler equations will link growth rates and firm policies to this interest rate. Furthermore, on the balanced growth path which I will consider in this paper, interest rates will be fixed at a value R. The numeraire final good is produced by a competitive, constant returns to scale, and price-taking sector which combines intermediate goods X jt from each firm j, and demands labor in the amount L D t to produce output Y t in each period. The labor share is , and the final goods technology is As will be discussed in more detail below, each intermediate goods firm at time t possesses both a quality level Q jt , together with an exogenous profitability shock a jt + \" jt . 29 Together, these quantities determine the marginal product of intermediate input X jt in final goods production. The final goods problem is The form of the final goods sector optimization problem above, which follows Acemoglu and Cao (Forthcoming), yields a standard isoelastic downward-sloping demand curve for variety j, given by Each member of the fixed continuum of intermediate goods firms j 2 [0, 1] faces idiosyncratic uncertainty. 30 Firm j is associated with a manager who in each period determines its monopoly price p jt , R&D investment z jt , and paper or accounting manipulation m jt . Firm j's long-term quality level Q jt is nonstationary and grows from R&D investments according to a quality ladder structure. Simultaneously, stationary exogenous profitability shocks a jt and \" jt satisfy The transitory shock process \" jt bu\u21b5ets firm profitability in each period, while the AR(1) process a jt persists. A number of recent papers apply a similar basic structure, decomposing volatility a\u21b5ecting firm or economy-wide investment choices into persistent and transitory components, and of course transitory-persistent shock breakdowns have a long history in labor economics. 31 Variable profits The intermediate goods firm profitability shock is the sum of persistent a jt and transitory \" jt . 30 Throughout the paper, I abstract from entry and exit with a fixed set of intermediate goods firms. This assumption is made more palatable by my structural estimation of the model with data from large public firms with lower exit hazards. However, I abstract from the Schumpeterian interactions between entry and innovation studied in endogenous growth models starting with Aghion and Howitt (1992). 31 Such papers for firm investments include Aguiar and Gopinath (2007)  The isoelastic form of the final goods sector's demand for input j implies an optimal constant markup pricing rule for p jt over marginal cost , 32 so that eventually variable profits take the following homogenous form in Q jt : Firm j's scaled R&D choice z jt leads to a total expenditure of z jt Q jt and results in an innovation with probability (z jt ) = Az \u21b5 jt . The parameter \u21b5 2 (0, 1) governs the elasticity of innovation arrival with respect to R&D. Innovations embody a proportional improvement up a quality ladder by amount > 1, so that the level of long-term quality Q jt+1 for firm j in period t + 1 is Eventually, if firm j lags and does not innovate for long enough, the firm will receive a di\u21b5usion of some small fraction ! of the average quality level Q t+1 of the economy as a whole. 33 Managers also make discretionary accounting choices which a\u21b5ect reported earnings. Empirically, paper manipulation by public firms can be accomplished through judicious use of tools such as heavy revenue accrual or recognition into earnings within a fiscal period. Through their accounting discretion, managers may also shift their reported Street earnings from a value which would be determined by strict application of (2007), Roys (2011), and Gourio (2008), while Blundell et al. (2008) and many others consider household persistent and transitory shock decompositions in the presence of a consumption/savings choice. 32 For notational convenience, following Acemoglu and Cao (Forthcoming), I make the assumption that = 1 , leading to a monopoly price of p jt = 1 1 = 1. 33 The di\u21b5usion structure follows Acemoglu and Cao (Forthcoming) and is useful to deliver existence of a stationary distribution of normalized firm-level quality levels Q jt on a balanced growth path for the economy. GAAP principles to the more flexible value reported in the financial press. However, activities such as accruals manipulation bear costs for at least two reasons. First, by recognizing revenues now firms constrain their ability to count those revenues towards earnings in future. Second, more discretionary accounting manipulation involves more disruption of manager time, higher auditor expenses, or even higher probability of fraud detection and prosecution. 34 In the model, by choosing manipulation level m jt firm j can induce a net paper shift of its reported earnings by m jt Q jt subject to a quadratic cost m m 2 jt Q jt . Overall earnings \u21e7 Street jt reported in the model are defined as variable profits net of R&D expenditures and paper manipulation: For individual firms, forecasts of earnings evolve over time based on the rational projections of an outside sector of identical equity analysts. Since earnings \u21e7 Street jt are homogeneous in long-term quality Q jt , analysts forecast normalized earnings Analysts understand the structure of the economy, including the exogenous shock processes and the potential for earnings manipulation by firms. Forecasters possess an information set at time t consisting of lagged normalized earnings \u21e1 jt 1 , consistent with survey evidence in Brown et al. (2014) revealing that large fractions of equity analysts incorporate recent earnings performance into the production of their earnings forecasts. 35 Further motivated by empirical evidence suggesting that analysts face career concerns and pressure to produce accurate forecasts of earnings, 36 in the model forecasts \u21e1 f jt (\u21e1 jt 1 ) minimize the following ex-ante expected quadratic 34 See Dichev et al. (2013) for a survey-based discussion of the costs of earnings manipulation perceived by managers at large firms in the United States. See Bradshaw and Sloan (2002) for a further discussion of the distinction between Street and GAAP earnings in practice. See Zakolyukina (2013) for a structural model of paper accounting manipulation. See Druz et al. (2015) for a discussion of manipulation of earnings conference calls through manager tone. 35 In practice, analysts may of course use more information for forecasts than lagged earnings alone. However, Numerical Appendix Table A.7 demonstrates that within the model a lagged earnings information set results in high forecast accuracy. In particular, only marginal improvements in forecasting performance would result from broader information sets including lagged forecast errors or R&D expenditures. 36 See, for example, Hong et al. (2000), Hong and Kubik (2003), or Hong and Kacperczyk (2010). Theoretical frameworks using analyst objective function based on squared-error loss include Marinovic et al. (2012) and Beyer and Guttman (2011). loss function: Optimally forecasts in the model therefore satisfy \u21e1 f jt (\u21e1 jt 1 ) = E(\u21e1 jt |\u21e1 jt 1 ), and firm j is aware of its earnings forecast \u21e1 f jt when making R&D investment and paper manipulation choices in period t. The manager of firm j maximizes the expected discounted flow of their personal utility. 37 Their decisions solve . Manager compensation depends on a constant, exogenous share \u2713 d of ownership in their firm. Given manager choices for R&D, accounting manipulation, and pricing, firm dividends in period t equal variable profits minus R&D expenditures and resource costs of paper manipulation: Manager flow utility, linear in consumption and other payo\u21b5s, is given by The first term in D M jt simply represents the manager's dividend share. The second term contains the impact of firm earnings forecasts on the manager objective and hence firm policies. A manager who fails to deliver earnings which meet or beat analyst expectations su\u21b5ers a fixed loss governed by the parameter \u21e0 0. In particular, however, when \u21e0 = 0 the manager problem results in firm profit maximization. Although in Section 1.5 I will explicitly examine the potential for other agency frictions such as a manager taste for shirking or empire-building, note that these channels are shut down in my initial framework. 37 Managers discount their flow utility using the interest rate R implied by household decisions. In Theory Appendix B, I provide details of a microfoundation of this discounting structure with overlapping generations of one period-lived managers selling a manager franchise onwards to the next period's manager after choosing firm policies. The discontinuous, fixed nature of the miss cost is a natural choice given the kinked forecast error distribution in Figure 1.1 as well as the evidence for discontinuous manager incentives from Section 1.2. In principle, earnings miss costs can represent three separate sources of loss for managers The first potential component of miss costs for managers, \u21e0 manager , is purely private and could include career or reputational concerns for managers. Surveyed managers report that such reputational concerns loom large (Dichev et al., 2013). Alternatively, managers may su\u21b5er increased e\u21b5ort costs due to higher rates of more negatively focused communications with outsiders upon an earnings miss (Yermack and Li, 2014). 38 The second potential component of miss costs, \u21e0 firm , reflects any resource, disruption, or other costs borne by firms rather than directly by managers for failure to meet analyst expectations. Such firm-borne costs still a\u21b5ect managers through their ownership shares \u2713 d . Surveyed managers report that e\u21b5orts to avoiding earnings misses are important to maintain a low cost of external finance, to avoid triggering debt covenants, and even to avoid higher likelihood of lawsuits from shareholders (Graham et al., 2005). The third and final component of miss costs, \u21e0 pay , represents the potential for a firm to explicitly condition manager compensation on meeting earnings targets. In particular, if exogenous compensation includes not only a dividend share but also an amount \u21e0 pay Q jt clawed back conditional upon a miss, then the net loss to a manager from this channel is given by (1 \u2713 d )\u21e0 pay . Empirically, managers failing to meet earnings benchmarks su\u21b5er reduced bonuses (Matsunaga and Park, 2001), and the empirical evidence from Section 1.2 suggests that total compensation is discontinuously lower for managers just failing to meet analyst forecasts. My structural estimation approach for quantifying a manager's cost of missing an earnings target, will identify only the combined cost \u21e0 rather than the three individual components \u21e0 manager , \u21e0 firm , and \u21e0 pay . When making quantitative statements about the overall cost of earnings targets in Section 1.4, I conservatively assume that the entirety of the term \u21e0 represents personal costs \u21e0 manager . Any changes in firm value or household welfare due to distorted manager policies are therefore due to the policies themselves rather than a direct mechanical contribution from resource costs \u21e0 firm ."}, {"section_title": "Balanced Growth Path Equilibrium and Numerical Solution", "text": "The model outlined above admits a balanced growth path equilibrium at which all model aggregates, including average quality Q t = R 1 0 Q jt dj, grow at constant rate g. Theory Appendix B outlines the full equilibrium definition, which involves four major optimizing components: 1) optimal household consumption and savings decisions C t , S jt , and B t+1 given the budget constraint, 2) competitive final goods firm optimization over intermediate goods X jt and labor demand L D t , 3) intermediate goods firm manager optimization over monopoly pricing p jt , R&D investment z jt , and paper earnings manipulation m jt , and 4) rational analyst forecasts of earnings \u21e1 f jt for each firm. An economy-wide resource constraint, labor market clearing, asset market clearing, and aggregation consistency conditions complete the characterization of general equilibrium for the model. I use numerical techniques to solve the model. Given homogeneity of manager returns in long-term quality, I first normalize their dynamic problem by the average quality level in the economy Q t . This normalization yields a stationary recursive formulation reported in Theory Appendix B as a function of four state variables: q (normalized endogenous long-term quality), a (exogenous persistent profitability), \" (exogenous transitory profitability), and \u21e1 f (endogenous analyst forecasts of earnings). I notationally omit dependence on j or t for clarity, indicating future and lagged values by 0 and 1 , respectively. I solve the manager problem using standard numerical dynamic programming techniques (Judd, 1998). I also rely upon a polynomial approximation to the analyst expectation \u21e1 f = E(\u21e1|\u21e1 1 ). 39 For a given 39 Table A.7 in the Numerical Appendix C records forecast accuracy or robustness statistics to alternative forecast systems with higher order approximations in \u21e1 1 above the baseline implementation (a linear rule), as well as to di\u21b5erent information sets including forecast errors and R&D expenditures. In all cases, the higher-order approximations and extended information sets yield parametrization of the model and solution to the manager's problem, I compute a stationary distribution \u00b5(q, a, \", \u21e1 f ) of firm states. Model aggregates are a function of the stationary distribution \u00b5. My algorithm for full general equilibrium solution of the model along a balanced growth path, explained in more detail in Numerical Appendix C, involves a hybrid dampened fixed-point and bisection algorithm iterating over the growth rate g, interest rate R, and forecast function \u21e1 f (\u21e1 1 ) such that the following three fixed points are satisfied: 1. The constant interest rate R and growth rate g satisfy the household Euler equation: 2. An economy-wide growth rate equal to g results from the aggregation of intermediate goods firm R&D investment policies z and the innovation arrival function (z): 3. Analyst expectations of earnings are consistent with the equilibrium distribution \u00b5:"}, {"section_title": "Estimation with Firm-Level Data", "text": "Numerical analysis of the baseline model laid out above requires fixing the values of many parameters. For the most part I follow a structural estimation strategy based on GMM using firm-level moments from my joint sample of Compustat and I/B/E/S data. However, before estimating the model I externally calibrate some of the parameters involving common quantities from the macroeconomics or innovation literature. Table 1.2 reports the values of these parameters. little quantitative gain in forecast accuracy. Manager equity share Nikolov and Whited (2013), 5.1% Note: The table displays the notation (first column) as well as an explanation (second column) of each model parameter fixed by outside calibration. The third column lists the source and value of each common parameter. The model period is one year. Together, an intertemporal elasticity of substitution of 0.5 or = 2, a subjective discount rate of \u21e2 = 1/1.02 \u21e1 0.98, and a targeted growth rate of near 2% yield annual interest rates of around 6%. A labor share of = 2/3 matches standard values in the quantitative macroeconomics literature, and a value of = 1.25 implies long-term quality increases of 25% upon innovation arrival. 40 I normalize labor supply to L = 1, and choose a manager equity share My GMM approach requires selection of informative moments to use for identification of the remaining six parameters of the model, including the persistence and volatilities of profitability shocks (parameters \u21e2 a , a , and \" ), as well as the magnitude of manager miss costs associated with earnings targets \u21e0, the productivity level for innovation A, and the costs of paper accounting manipulation m . Note: The data moments from the covariance matrix of sales growth, R&D growth, and earnings forecast errors above are computed from the estimation sample composing a panel of income statement and earnings forecast data from US firms in Compustat and I/B/E/S, with 4,839 firms and 32,597 firm-years from 1982-2010. implies standard deviation, \"Corr\" implies correlation. The aggregate growth rate is the mean US per capita real GDP annual growth rate. The Baseline moments are computed from the stationary distribution of the estimated baseline model, while the No Targets figures are computed from the counterfactual model stationary distribution with no manager miss cost of missing an earnings target, i.e. \u21e0 = 0, holding all other parameters fixed at Baseline levels. lists the seven selected moments together with their values in the data and model. 41 At the macroeconomic level I target the aggregate growth rate, while the covariance matrix between sales growth, R&D growth, and forecast errors provides a useful corresponding set of microeconomic moments. First, I lay out the structure of the GMM estimation algorithm for the vector of parameters \u2713 = (\u21e2 a , a , \" , A, m , \u21e0) 0 , with further details available in the Data Appendix A. After choosing a weighting matrix W , I estimate\u2713 through numerical minimization of the objectiv\u00ea where m(X) and m(\u2713) are the vector of moments from the data X and model with 41 Note that the table presents, for ease of reference, the covariance matrix transformed into percentage standard deviations and unit-free correlations, although the underlying GMM estimation is performed using the untransformed covariance matrix. Forecast errors are equal to the percentage di\u21b5erence between the realized Street earnings value and analyst forecasts of earnings within a period. Also, the model and data definitions of each growth rate are Davis and Haltiwanger (1992)style robust and bounded percent growth rates. A percentage growth rate for variable x equals % x jt = 2 parameters \u2713, respectively. 42 The moment weighting matrix W I use results in a sum of squared percentage deviations objective, but also, given its importance for an investigation of growth, places additional weight on the aggregate growth rate in the data. With point estimates\u2713 from the numerical minimization in hand, the asymptotic covariance matrix of the estimates follows standard GMM formulas. 43 With seven moments and six parameters, the estimation algorithm is overidentified GMM. The mapping between moments and estimated parameters in the model is joint and not one-to-one. However, certain moments are particularly influential for the identification of a given parameter. In the absence of a proof of identification, I conduct a formal investigation of this mapping by computing Gentzkow and Shapiro (2014)  I also broadly discuss the estimation mapping here, giving economic intuition. First, the estimate of the innovation productivity parameter A in the model depends heavily upon the data's aggregate growth rate, because higher innovation arrival rates in the model imply higher growth. Identification of the remaining parameters typically places significant weight on the covariance matrix of R&D growth, sales growth, and forecast errors. Forward-looking R&D investments feed into realized forecast errors, and this link implies that the structural estimates of persistent profitability volatility a load particularly upon R&D growth as well as forecast error volatility moments. By contrast, the estimated level of transitory profitability volatility, \" , depends relatively more upon overall sales growth volatility in the data. Estimation of the persistence \u21e2 a of profitability shocks again links to the behavior of forward looking R&D, placing weight both upon each of the volatility moments in the data but also crucially on the correlation or covariance between sales and R&D growth. Since easier paper manipulation in the model can obscure the passthrough of sales growth to earnings, the cost parameter m for paper manipulation is determined in large part by the 42 The numerical minimization is carried out using a standard global stochastic numerical optimization routine called particle swarm optimization which is simple, robust, and broadly comparable to other global stochastic optimization routines such as simulated annealing. 43 See for example, the derivations and formulas in Gourieroux and Monfort (1997), but note that the stationary distribution of model state variables is directly computable and doesn't require simulation. Therefore, my approach can be classified as GMM and avoids the need for the variance inflation factor of SMM estimation. Note: The parameter estimates above are computed from an unbalanced panel of income statement and earnings forecast data, with 4,839 firms and 32,597 firm-years from 1982-2010 in the US, together with data on the US per capita real GDP growth rate. The estimation procedure is standard overidentified GMM, with a moment covariance matrix reflecting time series correlation of the aggregate growth rate using a stationary bootstrap and arbitrary time series correlations within firm-level clusters for all microeconomic moments. Optimization was performed using particle swarm optimization, a stochastic global minimization routine. The weighting matrix is chosen so that the GMM objective equals the sum of squared percentage deviations, with 10 times extra weight placed on the aggregate moment. Asymptotics are computed in the number of firms while assuming independence between aggregate and microeconomic moments in the data. observed correlation between sales growth and forecast errors. Finally, we will see below that the primary model manifestation of earnings pressure from miss costs \u21e0 is higher R&D volatility, as firms sometimes react to profitability shocks by cutting R&D to boost earnings above targets. Naturally, therefore the estimated level of \u21e0 depends crucially upon both R&D growth volatility in the data as well as the correlation between forecast errors and R&D growth. Table 1.4 records the estimated parameters and standard errors based on my combination of aggregate and firm-level data in the baseline model. The persistent portion of profitability is highly autocorrelated, with \u21e2 a approximately equal to 0.9, and the combination of persistent and transitory volatility, p 2 a + 2 \" , is moderate at around 12% annually. 44 Profitability parameters are quite precisely estimated. Comfortingly, the persistence and volatility estimates are comparable to the structural estimates of the parameters of firm-level productivity or profitability processes found in both Gourio and Rudanko (2014) as well as Hennessy and Whited (2007), which are also based on dynamic firm-level problems and Compustat data. Naturally, however, I estimate higher persistence in the profitability shock a jt than found in those studies 44 Although the profitability processes are defined in levels, a profitability mean of 1 allows us to apply the log approximation to these deviations and interpret them approximately as percentage deviations. because I also allow for purely transitory variation \" jt . The accounting manipulation and R&D productivity parameters m and A are in model units di cult to map directly to observable quantities. However, I can naturally examine the plausibility of the estimated costs\u21e0 by expressing them in terms of observables. In the baseline model, managers are indi\u21b5erent between missing an earnings targets and a loss of around 3.6% of firm revenues on average, with the miss cost statistically distinguishable from zero at the 5% level. Benchmarks for direct comparison with the 3.6% figure are scarce. However, Taylor (2010) structurally estimates a perceived cost to firms of CEO turnover of around 5.9% of firm assets, equal to 8.9% of firm revenues given the mean assets to revenues ratio in my estimation sample. CEO firing events are rare at a 2% rate (Taylor, 2010) relative to earnings misses, which occur at a 44% rate in my GMM estimation sample. My estimated earnings miss costs should therefore naturally be smaller. Additionally, macroeconomics has for decades devoted considerable energy to quantifying the costs of price adjustment at firms, another fixed cost crucial to firm decision making. Zbaracki et al. (2004) provides estimates of the costs at a large firm associated with a price change and dominated by costs of customer negotiation and communication. These total expenses sum to about 1.2% of firm revenues in each annual price-changing cycle. Given that price changes predictably occur each year within firms, a lower direct estimate of price change costs relative to my structurally estimated costs perceived by managers from earnings misses is reassuring. Given the overidentified and highly nonlinear structure of the model, I can not in general expect an exact match between model and data moments. However, Table   1.3 demonstrates that the Baseline model with incentives to meet earnings forecasts leads to a broadly successful fit to the data moments. 45 In particular, the Baseline delivers an aggregate growth rate around the 2% level seen in the data, together with substantial variation in sales growth rates as in the data. Note that the Baseline model delivers somewhat less volatile forecast errors than observed in the data, but higher volatility than a model without earnings pressure (moments also reported in Table 1.3). Furthermore, in both the Baseline and the data, forecast errors negatively 45 Note, however, that the amount of data used for GMM estimation of the model implies that the J-test of overidentifying restrictions for the model is quite stringent, producing a strong rejection of the model. covary very slightly with R&D growth. In other words, the presence of earnings targets implies that cuts to R&D growth can be driven in the model by a desire to meet or beat earnings forecasts and therefore be correlated with higher forecast errors. By contrast, the model without earnings targets, in which R&D innovations are exclusively motivated by persistent profitability innovations, naturally produces a positive correlation of forecast errors with R&D growth. Furthermore, the presence of earnings targets in the Baseline causes dependence of R&D policies on transitory shocks to profitability, increasing the volatility of R&D growth substantially, while a model without a motive to meet earnings forecasts underpredicts the R&D volatility seen in the data by a large margin. Finally, the paper obfuscation in a model with earnings targets leads to lower correlation between sales growth and forecast errors, closer in line with the data, while a model without earnings targets overpredicts this correlation by a large amount. then move to a discussion of the economic costs of earnings targets. Earnings benchmarks force a distortion to the dynamic long-term investment decisions of firms in the Baseline model. Because of this e\u21b5ect, I find that the Baseline economy exhibits quantitatively meaningful decreases in aggregate growth rates and household welfare, lower and more volatile firm R&D expenditures, and lower firm value on average."}, {"section_title": "Estimated Costs of Short-Termism", "text": ""}, {"section_title": "Earnings Manipulation in the Baseline Model", "text": "Within my model, Figure 1  A strand of literature within both corporate finance and accounting seeks to understand the determinants of and information content within earnings releases. 47 In a theoretical contribution, Stein (1989) suggests that myopic distortions of investment by firms endogenously arise in order to boost short-term earnings of profitability in a signal-jamming equilibrium. An imperfectly informed market expects manipulation and therefore updates its inferences about firms with poor earnings reports particularly harshly. Anecdotally, this intuition is consistent the survey of large US firm managers in Graham et al. (2005), where one participant reported that \"if you see one cockroach, you immediately assume that there are hundreds behind the walls, even though you may have no proof that this is the case.\" In the context of my model, Figure 1.4 shows quantitative evidence of exactly this 46 The horizontal axis, based on normalization by long-term firm quality Q jt rather than a notion of firm assets, is not directly comparable to the earnings forecast error distribution displayed in Figure 1.1. However, the long-term value and scale of a firm in the model depends heavily upon quality, in a similar fashion to the heavy dependence of scale upon assets in the estimation sample. 47 See, for example, work by MacKinlay (1997), McNichols (1989), Kasznik and McNichols (2002), or Liu et al. (2009). type of selection into meeting earnings targets, with firms that meet forecasts in the Baseline model 146% more profitable on average (as measured by the persistent shock a jt ) than firms that miss a target. By contrast, firms in the No Targets economy that miss an earnings target are only 11% less profitable on average than firms that meet their targets. Clearly, in the Baseline model, observers of firms would be justified in inferring quite poor profitability prospects for firms failing to meet an earnings forecast. I view the results in Figure 1.4 as potentially suggestive as to the means by which disruptions to firms or managers from earnings misses could arise. Imperfectly informed analysts, the financial media, or the distributed owners of public firms may react particularly negatively to an earnings miss and demand manager time, attention, or even litigation as they seek to gain more information about the underlying profitability prospects of the firm in question. The results in Figures 1.2-1.4 above incorporate measurement error for earnings targets within the model for the purposes of plotting model outcomes against model forecast errors. Why is this useful? Quantitative models with fixed costs and heterogeneity routinely yield a stark sorting of agents across a threshold or into adjustment vs. inaction (Khan and Thomas, 2008;Berger and Vavra, 2014), and my model is no exception. In fact bunching is strong, and a range of forecast errors just below zero never occur in equilibrium if measurement error is ignored. The literature routinely incorporates some some quantitative addition, such as measurement error or maintenance investment depending on the context, in order to allow for a looser sorting of model stationary distributions. Motivated by these concerns, Theory Appendix B lays out my extended model of manager decisions with a decomposition of transitory profitability shocks into two separate components: \" jt (known to managers when policies are decided) and another component \u232b jt (unknown to managers when policies are decided). In practice \u232b jt serves as \"target measurement error,\" since the exact earnings threshold for meeting forecasts is ex-ante uncertain. However, throughout the rest of the paper in which direct comparison of firm outcomes to forecast errors is not the object of interest, I conservatively discuss results generated by the Baseline model without measurement error, since the impact of earnings pressure on growth and welfare turns out to be slightly lower in this case. 48"}, {"section_title": "Costs of Earnings Targets", "text": "Earnings pressure systematically changes real or economic outcomes for firms and the economy as a whole. More R&D volatility resulting from sensitivity to short-term shocks damages the overall e ciency of the innovation process. Table 1.5 reports the aggregate growth rate in the Baseline model, g\u21e0 = 2.25%, which is around 6 basis points lower than growth in the No Targets economy, g \u21e0=0 = 2.31%. These di\u21b5erences are quantitatively larger than they may at first appear, since growth changes lead to compounded di\u21b5erences across economies far into the future. The driving force behind diminished growth in my economy with earnings targets is curvature in the model link (z) between R&D expenditures and innovation. Given concavity or diminishing returns to R&D in (z), randomly more volatile R&D causes fewer innovation arrivals on generating Figures1.2-1.4, I calibrate the decomposition of known and unknown transitory shock volatilities to attribute approximately half of the total estimated transitory volatility to each source. 49 In both the Baseline and No Targets economies, of course, higher values of the persistent shock a induce higher levels of R&D expenditures z on average. 50 See also Schmitz (2014) for a related model of business cycles and R&D in which di\u21b5erences in firm size interact with R&D sensitivity to shocks to generate persistent business cycles. Note: The entries above represent the percent di\u21b5erence in the indicated aggregate quantity between the counterfactual \u21e0 = 0 and estimated benchmark\u21e0 cases. The moments are computed from the stationary distributions \u00b5 of the respective economies, and comparisons are across balanced growth paths without a transition path calculation. Welfare represents the percentage consumption equivalent variation of \u21e0 = 0 relative to the baseline economy. The change in firm value is the mean partial equilibrium percent change in firm value when \u21e0 =\u21e0 ! \u21e0 = 0 for an individual firm, averaged over the stationary distribution of\u21e0. The 100-yr change in Y is the percentage di\u21b5erence in output after 100 years from a no targets growth rate rather than a targets growth rate, using identical initial conditions. average, even ignoring level changes in R&D expenditures. 51 A negative relationship between volatility and growth, crucial to the quantitative results here, is familiar from the macroeconomics literature. For example, Barlevy (2004) describes the manner in which at the aggregate level, higher volatility of tangible investment caused by business cycles can increase the theoretical cost of business cycles by reducing the aggregate growth rate. The same mechanism appears at the microeconomic level in my Baseline model. Since macroeconomists routinely infer much higher volatility or variation in profitability at the disaggregated level than on aggregate (Bloom et al., 2012), my firm-level link between profit shocks and R&D naturally provides a potent source of changes in the innovative e ciency of the economy as a whole. To gain a better sense of the quantitative magnitude of the distortion to aggregate growth rates, I also engage in an accounting exercise for aggregate welfare. The second column of Overall, consumption equivalent gains on the order of 0.44% are large. For perspective, recent quantitative estimates of the welfare costs of business cycles range from 0.1-1.8% (Krusell et al., 2009), and measures of the static gains from trade range from 2.0-2.5% (Costinot and Rodr\u00edguez-Clare, 2015;Melitz and Redding, 2013). I conclude from the sizable changes in growth and welfare in my model that the dynamic distortion to R&D investment induced at firms by the presence of earnings targets matters for the macroeconomy. A final quantitative perspective in Table 1.5 on earnings target costs comes from the firm level. In the Baseline economy, the change in average firm value which would result in partial equilibrium from the removal of pressure to meet earnings forecasts is around 1%. 54 For perspective on the size of this firm-level distortion we can turn to work in structural dynamic corporate finance that quantifies the loss in shareholder value from CEO turnover frictions at around 3% (Taylor, 2010), or from manager agency frictions a\u21b5ecting cash holding at around 6% (Nikolov and Whited, 2013). Distortions to manager R&D policies and the resulting losses in firm value, growth rates, and welfare laid out above represent the costs of pressure to meeting earnings targets. However, earnings targets may provide a range of benefits as well. In principle the benefits could be external to the firm, functioning for example through more precise valuations of companies on the equity market and more socially e cient allocations of capital across firms. Or, by contrast, earnings pressure could yield benefits for firms themselves if agency considerations drive a wedge between the interests of managers and firm owners which is alleviated by earnings discipline. to the household in aggregate consumption. If the e\u21b5ect of the miss costs is included as entirely due to firm resource costs or disruptions \u21e0 miss , overall household gains are 0.48% instead. 53 The change in growth rates due to the removal of earnings targets is around 0.1% in the version of the model with calibrated target measurement error for firms, with a total change in aggregate welfare of 1.39%. The increase in the e\u21b5ects of targets is due to the increased mass of managers who perceive a possibility of missing an earnings target, given the addition of uncertainty surrounding the target itself. Overall, the dynamic distortion to long-term investment in strengthened, and the Baseline results in the main text should therefore be considered conservative. 54 This 1% figure is derived from computing average firm value from optimal policies in an environment with \u21e0 = 0, or no manager cost from missing earnings targets, with the aggregate growth rates, interest rates, and forecasting in the Baseline economy held fixed. The average is taken with respect to the equilibrium stationary distribution of state variables \u00b5 in the Baseline model. For conservatism I assume such costs are private to the manager. If disruption costs are borne as resource costs to the firm, the change in firm value is 1.3%. Consideration of such benefits from earnings targets seems important for two reasons. First, and crucially, the gains from removal of earnings pressure embodied in the No Targets counterfactual considered so far could fail to materialize in practice if policymakers were to take action preventing earnings targets and simultaneously extinguish the benefits from earnings pressure. Second, a significant portion of earnings targets could represent explicit or implicit compensation contracts with some portion of overall pay for managers dependent upon meetings earnings forecasts. In this case, a countervailing benefit from earnings targets for firms might in principle provide a microfoundation for the existence of pressure on managers to meet short-term benchmarks, even if such benchmarks lead to a loss in investment e ciency. In the next section, I build o\u21b5 of the baseline model structure and include two sources of agency frictions in the firm-manager relationship."}, {"section_title": "Agency Benefits from Short-Term Pressure", "text": "Empirically, Section 1.2 demonstrates that CEOs and other executives at large US public firms which just fail to meet an earnings target or forecast receive lower total compensation on average. These results open up a suggestive possibility: pressure to meet earnings forecasts or targets may arise from the e\u21b5orts of the boards or distributed shareholders of firms to exert discipline on managers. In this section, I consider two forms of agency frictions which may serve to both microfound and to provide countervailing benefits from the distortions to long-term investment induced by earnings targets."}, {"section_title": "Manager Shirking and E\u21b5ort Provision", "text": "I first consider the possibility of shirking by managers. In particular, firm owners may be unable to observe whether managers continually exert e\u21b5ort, and compensation contracts can serve as a tool to mitigate the possibility of low e\u21b5ort provision (Grossman and Hart, 1983;Edmans et al., 2009;Beyer et al., 2014). In this framework, managers can in each period choose either to shirk (s jt = 1) or to exert normal e\u21b5ort (s jt = 0). If managers shirk, then they receive some private benefit governed by the parameter s 0. Shirking comes at the cost of firm variable profits, with a proportional disruption to firm quality equal to some fraction s 2 (0, 1). The remainder of the economic environment is similar to before, where managers now solve Manager payo\u21b5s are given by D M jt where Manager flow returns reflect two components. First, D M jt includes an exogenous compensation contract with a fixed dividend share \u2713 d 2 (0, 1) and clawback by the firm of jt )Q jt conditional upon missing an earnings target. 55 Endogenization of such a contract is beyond the scope of the paper, but I will demonstrate in a quantitative exploration of the extended framework that earnings-target conditional compensation can be value-improving for firms on average. The second component of manager flow returns is a private benefit s s jt Q jt accruing to the manager when shirking. In practice shirking might represent increased leisure at the firm's expense or some sort of resource diversion towards managers. The countervailing costs of shirking for the firm enter the expression for dividends, written here net of manager clawback compensation: At this point it is worth exploring the tradeo\u21b5s involved for managers considering shirking. In the absence of earnings targets managers weigh a private gain from low e\u21b5ort equal to s Q jt versus a firm-wide loss diluted through their equity share of  For those firm-years in which shirking would lead to an earnings miss, i.e. in which earnings prospects are near earnings targets ex-ante, the prospect of lost compensation can induce e\u21b5ort provision by managers. Therefore, in an environment with manager shirking, the owners of firms face a fundamental tradeo\u21b5 between the dynamic distortion to long-term R&D investments from earnings pressure and the level gains which may accrue to the firm from disciplining managers. I more rigorously investigate the tradeo\u21b5s involved through the following experiment. First, I fix \u21e0 pay so that managers perceive a cost to missing equivalent in magnitude to the estimated miss costs\u21e0 from the structural estimation exercise above. Then, I vary the strength of the private motive for shirking s . fix the proportional loss to variable earnings at the firm from shirking at the round figure of s = 7.5%. The horizontal axis represents the ratio between the private returns perceived by managers and the average loss from shirking for a given value of s , both normalized by firm quality q. 56 As the shirking motive grows, managers unsurprisingly shirk more on average. However, there is a hump-shaped pattern to the increase in shirking seen if earnings discipline were to be removed. For very low levels of shirking motive, managers already provide e\u21b5ort almost always, so the presence of pressure only prevents a small increase in shirking. By contrast, for intermediate levels of private value to shirking, managers are close to indi\u21b5erent between shirking or not, and the presence of earnings pressure can shift a relatively larger portion of managers to provide e\u21b5ort. Finally, if managers have very high private returns to shirking, the lost compensation for managers upon an earnings miss does not dissuade shirking much at all. The hump-shaped pattern to the prevention of shirking feeds into counterfactual changes in firm value and social welfare: for intermediate levels of private shirking benefits, firm value would decrease on average if a firm removed miss costs for managers even though a dynamic investment distortion remains. The reduction in quality levels and hence production induced by shirking if earnings pressure were to be removed also leads to a static loss for the aggregate household, which crucially implies that the presence of earnings targets can be welfare-enhancing if they prevent a substantial amount of costly shirking by managers. The patterns in Figure 1.6 suggest that discontinuous and short-term incentives for managers may play a useful disciplinary role. Firms as well as the economy as a whole can benefit from imposing earnings miss costs if managers subsequently shirk less overall, even while targets distort the R&D decisions of firms. A disciplinary role may therefore provide a rationale for the existence of earnings targets within firms, since the use of a readily observable performance benchmark can improve firm value on average. Importantly, however, the next section considers an alternative agency justification for earnings targets and demonstrates that the exact nature of the conflict between managers and firms matters for the relationship between the firm and social implications of earnings benchmarks."}, {"section_title": "Manager Empire Building", "text": "If managers obtain some private benefit from the size or scale of the firm under their control, then overinvestment or empire building may occur. This type of empire building mechanism plays a role in a range of recent quantitative corporate finance studies (Nikolov and Whited, 2010;Glover and Levine, 2014) and conceptually dates to early work in Jensen (1986). If the owners of a firm do not perfectly observe the ex-ante expected profitability of R&D investments, then such overinvestment may be di cult to curtail in practice. However, earnings targets provide a convenient and readily observable benchmark against which manager decisions may be measured. This subsection analyzes a structure in which manager decisions are influenced by empire building and solve . By contrast with the previous subsection, managers are not motivated by a shirking possibility but instead face personal returns given by Again, there remains a fixed equity share \u2713 d 2 (0, 1) of firm dividends, as well as the potential for compensation contracts to impose an earnings miss cost on managers through \u21e0 pay > 0. However, the final term e Q jt indexes the strength of the empirebuilding motive for managers, where higher values of e give managers a more potent intrinsic taste for firm scale as determined by long-term quality Q jt . Here, dividends are standard and can be written net of manager clawback compensation as Exactly as in the analysis of manager shirking behavior above, I broadly explore the potential impact of earnings targets in the presence of empire building by first fixing \u21e0 pay to duplicate the magnitude of the earnings miss costs\u21e0. Subsequently varying the strength e of the empire motive yields the four panels of results in Figure 1.7, each of which plots against a horizontal axis equal to the ratio between the manager's private return to size and their average return to variable profits, both normalized by firm quality q. The top left panel plots the average R&D to sales ratio for the economy with earnings targets. As the private return to empire building grows, the R&D to sales ratio unsurprisingly increases. The top right panel plots the increase in the R&D to sales ratio which would be observed in an economy with target removal, i.e. with \u21e0 pay set to 0. As overinvestment from a firm's perspective becomes more severe, the scope for earnings targets to provide discipline on managers grows as well. Recall that by imposing miss costs through manager compensation, firms induce a distortion to R&D investment through excess sensitivity to short-term or transitory profitability shocks. For relatively low empire motives, the bottom left panel reveals that firms would in fact gain overall in partial equilibrium from the removal of targets, since they would remove the ine ciency associated with induced short-termism. However, for higher empire building pressures at firms the tradeo\u21b5 shifts in favor of targets, and firms would lose value on average from their removal because of the resulting overinvestment by managers. The bottom right panel of Figure 1.7 reveals that for the parametrizations considered here the aggregate household always experiences higher welfare if targets are removed. Intermediate goods firms realize profits and producer surplus from sales to a final goods sector, but managers ignore the consumer surplus accruing to the final goods sector and eventually to the aggregate household. In general this \"surplus appropriability problem,\" as coined by Jones and Williams (2000), causes firms to undervalue innovation and R&D relative to their social value, leading to ine ciently low growth rates. 57 Therefore, as shown in Figure 1.7 the presence of earnings targets discipline may increase intermediate goods firm value if it induces lower R&D on average. Simultaneously, however, earnings discipline may lead to a social loss. Just as in the analysis of manager shirking, the potential benefits of earnings targets in the presence of empire building recorded here should be taken as suggestive. The quantitative strength of the surplus appropriability problem and hence the divergence between firm and social returns to R&D depends on a markup structure which, while computationally convenient, links to the inverse capital share. Also, other potentially important factors such as the interplay between firm valuations, financial frictions, and earnings forecasts are omitted from the current structure. Therefore, I view a precise quantification of the benefits to firms or society from earnings targets as beyond the scope of the paper, preferring to instead outline that such benefits do exist."}, {"section_title": "Conclusion", "text": "Empirically, earnings realizations bunch directly above analyst forecasts. Firms that just meet or beat analyst targets of earnings display discontinuously lower long-term investment growth, while CEOs and other executives just failing to meet benchmarks face lower compensation. Together, these results suggest a pervasive tradeo\u21b5 in the data between the short-term prospects of firms versus their long-term investments.   \nIn this paper we present a new general equilibrium model of trade with endogenous growth that allows factors of production to be temporarily \"trapped\" in firms due, for example, to specific capital. This trapped factors model allows us to rationalize why in the face of an import shock from a low-wage country like China, incumbent firms in the a\u21b5ected industry may innovate more, as the firm-level micro-data suggest (Bloom et al., Forthcoming;Freeman and Kleiner, 2005). The mechanism behind this e\u21b5ect is a fall in the opportunity cost of R&D caused by a fall in the shadow cost of these trapped factors. The model also contains the more standard theoretical mechanism from the literature on trade and growth, whereby integration increases the profits from innovation. We calibrate a model and quantify the e\u21b5ects of a trade liberalization of the magnitude we observed in the decade around China's accession to the WTO 1997WTO -2006 Capital and labor adjustment costs are typically estimated at between 10% to 50% of the lifetime cost of the assets (Bloom, 2009) making these long-term investment similar to intellectual property protection. Also, while patent lengths vary between 15 to 20 years, e\u21b5ective patent lengths are typically shorter due to imitation, processing lags, and imperfections in enforcement. As such, a 10 year patent life is quite reasonable in our theoretical structure. Empirically, we find a substantial increase in welfare from such trade integration: a consumption equivalent increase of the order of 16% and a permanent increase in growth of around 0.4%. This leads to welfare e\u21b5ects that are much larger than conventional calibrations of static trade models which ignore the dynamic e\u21b5ects of trade on growth because they do not allow for the possibility that more innovation by firms can lead to more productivity growth for the economy as a whole. About a tenth (2% out of a 16% consumption equivalent increase) of the overall welfare gains are due to our trapped factor mechanism, a small but non-trivial proportion. Moreover, these trapped factor gains from growth come in the immediate aftermath of the a trade-liberalization, and so will be important to policymakers. These large dynamic gains from trade depend on increased profits that innovators in the North can earn from sales in the South. In this sense, the model ratifies the increasing attention that trade negotiators are devoting to non-tari\u21b5 barriers that might limit a foreign firm's ability to earn profits from a newly developed good. We have seen this already in the TRIPS agreement under the WTO, and better protection of intellectual property rights is also reported to be a central goal in the US approach to the negotiations leading up to the Trans Pacific Partnership. If this is where the largest welfare gains lie, this is where trade agreements can have their biggest e\u21b5ects. As noted in the Introduction, there are many ways in which the modeling framework could be extended and made more realistic. First, we have abstracted from \"catch-up\" in which growth rates in the South are higher than in North due to imitation. We did this in order to focus on welfare benefits in the North from a faster opening up of trade restrictions with the South. Second, we focus on the impact of North-South integration rather than North-North integration. This was motivated by evidence that the pro-innovation e\u21b5ects in the North were far stronger when trade barriers against the South were relaxed compared to richer countries, but an extended framework along say the lines of Aghion et al. (2005) could allow for Schumpeterian and \"escape competition\" e\u21b5ects. Third, a more careful analysis of the labor market and uninsured risk could o\u21b5er an important o\u21b5set to the e\u21b5ects that we identify. Although we have gone beyond steady states to look at transition dynamics we have, as is standard, abstracted away from the distributional changes as workers may suffer wage losses and unemployment when we introduce frictions in the labor market. These do seem to matter empirically so more work needs to be done to also incorporate such e\u21b5ects in quantitative theory models (e.g. Harrison et al. (2011)). The main message of our paper is that liberalized trade with the South can have substantial benefits in for the North and the entire world because it induces more innovation. This increase arises mainly through long-run increases in the profits that a new firm can earn from a newly developed good, but also because of a temporary contribution from trapped factors that reduces the opportunity cost of innovation. China alone accounts for almost half of the increase in welfare we identify. Because these benefits are less visible than the losses that firms and workers can face from an unexpected increase in trade, and because these e\u21b5ects can take decades to be realized, it is as important as ever for economists to understand why it may be so important to pursue and protect the gains from trade.   Note: The figure displays the benchmark transition path in response to a permanent, unanticipated trade liberalization from policy parameter to 0 > , which is announced in period 0 to become e\u21b5ective in period 1. The plotted transition is computed in the fully mobile economy, in which intermediate goods firms may respond to the information about trade liberalization without shortterm adjustment costs. The solid black line is the transition path, the upper horizontal solid blue line is the post-shock balanced growth path, and the lower horizontal dashed red line is the pre-shock balanced growth path.  "}, {"section_title": "3: R&D Growth versus Forecast Errors", "text": "Note: The figure plots the average R&D growth in the estimated benchmark model with miss cost \u21e0 (in red) and no miss costs (in black) conditional upon bins of the forecast error \u21e1 \u21e1 f , computed from the stationary distribution of the balanced growth path. The model is a calibrated version of the Baseline including ex-ante measurement error of targets on the part of firms. The model was solved via discretization, policy iteration, and nonstochastic simulation. Note: The figure above represents the conditional mean of profitability a for firms missing their forecasts (\u21e1 < \u21e1 f ), and firms meeting their forecasts (\u21e1 \u21e1 f ), computed from the stationary distribution of the balanced growth path associated with both the estimated earnings miss cost\u21e0 (in red) and \u21e0 = 0 (in black). The di\u21b5erence in mean profitability from missing is 146% in the estimated baseline, compared to 11% for \u21e0 = 0. The model is a calibrated version of the Baseline including ex-ante measurement error of targets on the part of firms. The model was solved via discretization, policy iteration, and nonstochastic simulation.  "}, {"section_title": "The top left panel plots the average shirking level 100E", "text": "\u00b5 s with earnings targets, the top right panel plots the percent di\u21b5erence in shirking from target removal, the bottom left panel plots the average PE percent change in firm value from target removal, and the bottom right panel plots the GE total consumption equivalent percent change in social welfare from target removal. Numerical comparative statics have been smoothed using a polynomial approximation.  The top left panel plots the average R&D to sales ratio with earnings targets, the top right panel plots the percent di\u21b5erence in the R&D to sales ratio from target removal, the bottom left panel plots the average PE percent change in firm value from target removal, and the bottom right panel plots the GE total consumption equivalent percent change in social welfare from target removal. Numerical comparative statics have been smoothed using a polynomial approximation."}, {"section_title": "Alternative Methods for Solving Heterogeneous Firm Models 2.1 Introduction", "text": "Heterogeneous agent business cycle models o\u21b5er the attractive possibility of combining a fully fledged business cycle structure with rich, testable implications for the cross-section of consumer or firm behavior. However, such frameworks, with seminal examples given by the incomplete markets model of Krusell and Smith (1998) and the heterogeneous firms model of Khan and Thomas (2008), pose several practical challenges for researchers. First, their solution and simulation are computationally intensive. Second, the traditional solution techniques used for these models, such as the Krusell Smith (KS) algorithm, rely on bounded rationality and aggregation assumptions and must be evaluated ex-post for the internal consistency of these assumptions. To help guide researchers around these issues in the practical solution of the incomplete markets model, many papers provide alternative solution techniques and computational strategies. 1 These advances profitably improve the speed and accuracy of solutions of the incomplete markets model, but the literature lacks a comprehensive analysis of their applicability to the heterogeneous firms context, which encompasses a fundamentally di\u21b5erent economic and computational environment. 2 In particular, the heterogeneous firms model requires a discrete investment choice by agents and use of Bellman equations rather than Euler equations (as in the incomplete markets model) to characterize optimal policies. The incomplete markets model can also be solved very e ciently at the microeconomic level through use of the Endogenous Grid Points method of Carroll (2006), while such a technique is unavailable for the lumpy investment problem of firms in the heterogeneous firms This paper seeks to provide such a comparison of solution techniques specifically targeted towards the solution of the heterogeneous firms model. The Khan and Thomas (2008) model is a natural framework on which to base such a comparison because of the large number of papers using a similar underlying structure. 3 The heterogeneous firms framework here combines aggregate uncertainty in the form of aggregate productivity shocks together with lumpy capital adjustment costs and a rich cross-sectional distribution of idiosyncratic productivity shocks and capital holdings. I adapt four existing algorithms to the heterogeneous firms structure, implementing each solution technique and comparing them along multiple dimensions: their simulated business cycle moments, cross-sectional investment rate distributions, impulse response functions, internal accuracy, as well as the computational burden posed by each algorithm. I implement the following four algorithms: 1. the traditional KS approach as first adapted by Khan and Thomas (2008) A quick word about the choice of these four techniques for comparison is in order. The KS approach is a natural and important choice because of its wide use in the heterogeneous firms literature to date. The PARAM algorithm is attractive both because it has been studied comprehensively in the context of the incomplete markets model but also because it bears conceptual similarity to another approach, the Backward Induction algorithm of Reiter (2010c). The XPA approach has been studied previously as a solution method for the heterogeneous firms model in Sunakawa (2012), model. and for comparability I rely on that paper's adaptation of the original Den Haan and Rendahl (2010) technique. 4 Finally, the REITER approach is an important addition to the algorithms considered here because it is conceptually distinct. The REITER method relies upon a linear perturbation approach in aggregates together with a rich, discretized projection problem at the microeconomic level to solve an approximation to the rational expectations equilibrium. For each solution method, code is readily available online. 5 Three main conclusions can be drawn from the comparisons in this paper. First, the KS algorithm compares very favorably with the other solution techniques considered here. The KS routine o\u21b5ers a high degree of internal accuracy and delivers economic implications well in line with those of the other bounded rationality projection-based solution methods. This result should be interpreted as a favorable robustness check to the large number of papers relying on the KS algorithm in the heterogeneous firms context, including the original work by Khan and Thomas (2008). However, these advantages do come at the cost of fairly high computational intensity within both model solution and simulation steps. Second, the other two bounded rationality solution techniques based on an approximation of the aggregate state space and projection methods, PARAM and XPA, both a\u21b5ord important speed gains relative to KS by avoiding simulation within the solution routine itself. As pointed out by Sunakawa (2012), these speed gains can open up the possibility of new applications for heterogeneous firms models, and the business cycle moments, micro-level investment rate distribution, and simulated impulse responses to productivity shocks are closely in line with those delivered by the KS algorithm. However, reduced computational burden comes at the cost of less internal accuracy for both the PARAM and XPA approaches. Furthermore, although model solution time is reduced by use of these algorithms, simulation for calculation of business cycle moments or impulse response analysis remains costly. 4 There are several di\u21b5erences between my work and Sunakawa (2012), however. While that paper usefully draws out a potential application of speed gains from the XPA algorithm in structural estimation of the aggregate technology process, I focus here instead on a comparison of the solution techniques themselves. Also, my implementation of the XPA approach uses a bias-correction step within the forecasting structure based on a steady-state solution of the model which is di\u21b5erent from the approach in Sunakawa (2012). 5 See https://sites.google.com/site/stephenjamesterry/. Third, the REITER approach stands apart from the KS, PARAM, and XPA algorithms for several reasons. The method is based on an alternative equilibrium concept, i.e. a perturbation approximation to the full rational expectations equilibrium. 6 The resulting solution to the heterogeneous firms model delivers qualitatively similar economic results as the traditional KS algorithm, although the aggregate dynamics do systematically di\u21b5er in some ways discussed in more detail below. In practical terms, the use of perturbation methods delivers speed gains far above those achieved by either the PARAM or XPA algorithms. Perhaps more significantly, however, the RE-ITER approach o\u21b5ers scalability of the macroeconomic complexity of heterogeneous agents models by providing a means for inclusion of a richer aggregate state space than would be currently tractable using a projection-based approach subject to the curse of dimensionality. Section 2.2 lays out the model and calibration, a direct simplification of Khan and Thomas (2008). Section 2.3 provides a brief overview of each of the four solution techniques implemented in the paper. In Section 2.4, I compare the resulting simulations, impulse responses, accuracy, and time requirements of each solution method. In an example in Section 2.5, I extend the baseline model to include two additional aggregate state variables using the REITER method. Section 2.6 concludes. Appendices follow containing detailed explanations of the solution algorithms as applied to the heterogeneous firms model, some practical details on their numerical implementation as well as additional accuracy checks and figures, a discussion of the simulation technique used to generate nonlinear impulse responses, and a discussion of an extension to the aggregate state space of the model solved using the REITER approach. 6 Interestingly, in a projection-based solution technique distinct from the projection plus perturbation approach by Reiter (2009Reiter ( , 2010a, Gordon (2011) also discusses a means of solving an approximation to the full rational expectations equilibrium of the incomplete markets model with the cross-sectional distribution as a state variable, using a dramatically reduced Smolyak grid for the projection routine. Note also that although Reiter (2009) and this paper's analysis use a linear perturbation, higher-order approximations of the same equilibrium system are possible, as suggested by Reiter (2010b)."}, {"section_title": "Model and Calibration", "text": "The model solved in this code is a simplified version of the model presented and solved in Khan and Thomas (2008). The simplification involves only the removal of maintenance investment and trend investment growth, but crucially maintains aggregate uncertainty, the discrete nature of the investment decision, and idiosyncratic productivity shocks at the microeconomic level. Interested readers can find much more detail on the assumptions underlying this economic structure in Khan and Thomas (2008). "}, {"section_title": "Households", "text": "Above, prices and wages are written in terms of an aggregate state (A, \u00b5) including aggregate productivity A and a cross-sectional distribution \u00b5 of capital and productivity, both of which are discussed in more detail below."}, {"section_title": "Firms", "text": "In each period there is a distribution of firms \u00b5(z, k) over idiosyncratic productivity and capital levels z and k. 7 Individual firms are subject to both idiosyncratic and aggregate productivity shocks, which are exogenous and are assumed to follow independent AR(1) processes in logs: where innovations to both processes are iid N (0, 1). The state vector for an individual firm is given by (z, k; A, \u00b5), which contains both the idiosyncratic states for that firm as well as an aggregate state including productivity and all distributional information. Firms also receive a random draw of fixed capital adjustment costs in each period, discussed below. Conditional upon idiosyncratic productivity and capital (z, k), a firm that chooses labor input n produces output given by the decreasing returns to scale technology y(z, k, n, A) = zAk \u21b5 n \u232b , where \u21b5 + \u232b < 1. In a rational expectations equilibrium there is a known transition mapping \u00b5 tracking the evolution of the cross-sectional distribution, as well as a mapping p from the aggregate state to the marginal utility of the representative household-owner p: Recall that the wage is a function of the household marginal utility given linear labor disutility, so that these two aggregate mappings fully characterize the aggregate structure of the economy from the perspective of an individual firm. Then, in each period, a firm receives a stochastic draw of a fixed capital adjustment cost \u21e0, given in units of labor. The firm value function V , adjusted by the marginal utility of the representative households, is therefore given by Once a firm receives a draw of a stochastic adjustment cost \u21e0 \u21e0 G(\u21e0), the firm faces a choice between paying the capital adjustment cost or not adjusting the capital stock where the value upon adjustment V A is given by optimization over investment and If a firm chooses not to adjust its capital stock, then it must face a dynamic return V NA which involves optimization of only the labor input n holding capital levels fixed at the (depreciated) level from last period: The trivial nature of the discrete choice problem leads to a cuto\u21b5 rule for capital investment, such that firms adjust their capital stock if and only if the adjustment cost draw \u21e0 is less than a cuto\u21b5 level where the numerator reflects the gains from capital adjustment relative to inaction and the denominator's adjustment by labor disutility is required to convert from marginal-utility to labor units. Further the distribution of lumpy capital adjustment costs is assumed to be given by G(\u21e0) = U (0,\u21e0), where\u21e0 > 0 indexes the level of the adjustment friction in the economy."}, {"section_title": "Equilibrium", "text": "An equilibrium represents a set of firm value functions\u1e7c , V, V A , V NA , firm policies and adjustment thresholds k 0 , n, \u21e0 \u21e4 , prices p(A, \u00b5), w(A, \u00b5), and mappings \u00b5 , p such that \u2022 Firm capital adjustment choices and policies conditional upon adjustment satisfy the Bellman equations defining V, V A , V NA above, and therefore firm capital transitions are given by \u2022 The distributional transition rule used in the calculation of expectations above by firms is consistent with the aggregate evolution of the distributional state \u2022 Aggregate output, investment, and labor are consistent with the current distribution \u00b5 and firm policies: \u2022 Aggregate consumption satisfies the resource constraint \u2022 The households are on their optimality schedules for savings and labor supply decisions, i.e. the first order conditions defining marginal utility and wages hold, and the price mapping is consistent \u2022 Aggregate productivity follows the assumed AR(1) process in logs.\nTo characterize the equilibrium in this closed economy, we can assume that final goods are produced by a single competitive constant returns to scale firm which demands as inputs intermediate goods and human capital. We also assume that the labor market is competitive. be assigned to any one of the N firms. 12 Finally, we assume that there is a set of potential entrants, that we refer to as \"fast copiers\", who act as a competitive fringe and force the firms that produce o\u21b5-patent goods to price them at marginal cost. 13 The equilibrium in this model takes a familiar form, with perfect competition in markets other than for the goods that are protected by patents, and by monopolistic competition with a zero marginal profit condition for firms that develop new designs that will be protected by patents. The full definition of the equilibrium for this model is given in the appendix. The fundamental equation for the dynamics of the model balances the cost of developing a new patented design against the profit that can be earned from the temporary ex post monopoly that it confers. This profit can be calculated as follows. In period t+1, the inverse demand for any input will be the derivative of the aggregate production function, which implies the inverse demand curve The usual markup rule for a constant elasticity demand curve implies that the monopoly price p M will be marked up by a factor 1/(1 \u21b5) above its marginal cost. One unit of output today can be converted into one unit of the intermediate that is available for sale tomorrow, so marginal cost in units of output tomorrow, is (1 + r) and the monopoly price tomorrow can be written as Together, these two equations imply monopoly output Because profit takes the form where . One easy way to see why profit increases linearly in H is to note that the price the monopolist sets is a fixed markup over marginal cost. This means that profit increases linearly with the quantity the monopolist sells. As in any constant returns to scale production function, at constant prices, an increase in the use of one input such as H will lead to an increase by the same factor in the quantity demanded of all The zero marginal profit condition for developing new goods implies that this expression for \u21e1 must be equal to the marginal cost of producing the last innovation at each firm. To express this cost, it helps to define a \"pseudo-growth rate\" 14 for an At . We denote the economy-wide growth rate of varieties At and note that g t+1 = Di\u21b5erentiation of the cost function for innovation yields On a balanced growth path, g t+1 will be equal to a constant g, which will also be equal to the rate of growth of output and of consumption. By symmetry among the N firms, we also have that g f t+1 = 1 N g. As a result, the cost of a new design reduces to If we define \u232b so that \u232b N (1 ) = 1 the cost of a new patent reduces to g 1 . Equating this marginal cost with the marginal benefit (ex post profit) yields: where Finally, using the fact that in a balanced growth equilibrium, consumption, patents, and total output will all grow at the same rate g, we can substitute in the expression for the interest rate equation (33) into equation (3.5) to generate the basic equation relating g and H: Proposition 1. Closed-Economy Balanced Growth Path The closed economy has a unique balanced growth path with a common constant growth rate g for varieties, output, and consumption, that satisfies the innovation optimality condition Proof in appendix. In the closed economy, this proposition says that the marginal cost of a patent must be equal to the appropriately discounted ex post profit that it will generate, and that this profit is proportional to the stock of human capital, H. When we extend this to the open economy setting, the same kind of expression in which g is an increasing function of H will still hold except that H will be replaced by an expression that depends on both H in the North, H \u21e4 in the South, and the extent of restrictions that limit trade between the two regions."}, {"section_title": "Calibration", "text": "The parameter choices used in the solution method comparison below are those chosen by Khan and Thomas (2008). The parameter choices reflect an annual frequency and positive levels of capital adjustment costs at the firm level, as summarized in Table   2.1. Given that this paper is concerned with the comparison of numerical solution techniques, and that the model is a simplified version of the original structure, these parameter choices should be taken as purely illustrative.\nWe started by specifying the basic parameters about which we have some prior information. Following Jones (1995b) and King and Rebelo (1999) we consider the case of log utility with = 1 and a labor share in production of \u21b5 = 2 3 . 17 Balanced growth path real interest rates of approximately 4% require = (0.98) 10 . We also estimated the ratio H \u21e4 H \u21e1 3 from international schooling data on educational attainment in the OECD and non-OECD countries in the year 2000. Therefore, we identify the OECD 17 In our model the price of M goods is equal to 1 1 \u21b5 = 3 times cost given \u21b5 = 2 3 , so the markup on new varieties is substantial. Importantly, however, the average markup in our calibration is much lower, since all o\u21b5-patent varieties of I and R goods are perfectly competitive. In our pre-trade shock baseline balanced growth path calibration described below, the average markup is approximately 40%. nations in our sample with the North and non-OECD nations with the South. We fix the parameter \u21e2 to the baseline value of \u21e2 = 0.5 based on Bloom et al. (2013b). The appendix contains more information over the calculation of H/H \u21e4 , and a later section checks robustness to di\u21b5erent values of most of the parameters above. We must also choose the final three parameters H, , and 0 which jointly govern the model's long-run balanced growth path growth rates and imports to output ratios. We compute the ratio of non-OECD imports to OECD GDP in 1997 (3.9%) and 2006 (7.0%), requiring that the model reproduce these import ratios in the pre-and postshock balanced growth paths, respectively. In other words, we require that the model reproduce the endpoints of the non-OECD imports series plotted in Appendix Figure   C.2. These import ratios are heavily influenced by our choice of and 0 , leaving us still to determine the model's scale through the choice of H to match growth rates from the data. We note that the model's concept of growth is most closely aligned with frontier per capita GDP growth. We therefore prefer to calibrate long-run frontier growth to the per capita GDP growth of the United States rather than the entire OECD. We choose a wide sample window of 1960-2010, yielding a calibration of H to match a pre-shock average annual growth rate of 2.0%. 18"}, {"section_title": "Solution Methods Overview", "text": "2.3.1 Krusell Smith Algorithm Khan and Thomas (2008) in the original exposition of the heterogeneous firms model use the first algorithm considered here, the KS approach. Their algorithm extends the one proposed in Krusell and Smith (1998) for use in the incomplete markets model and bases the general equilibrium components of the solution on a bounded rationality approach. When solving their dynamic problem, firms approximate the intractable distribution \u00b5(z, k) over idiosyncratic productivity and capital with some moments m. In practice, m is chosen to simply be the mean aggregate level of capital K. Given this approximation, two sets of forecast rules provide expectations for firms of both the aggregate level of consumption and the evolution of aggregate capital itself. Therefore, the intractable state vector (z, k; A, \u00b5) for the firm problem discussed above is replaced by (z, k; A, m), and the transition and price mappings are replaced by forecast rulesm 0 =\u02c6 m andp =\u02c6 p . In practice, the forecast rules are assumed to take a loglinear form conditional upon aggregate productivity, although the algorithm allows for more flexibility in theory. Solution of the model involves repeated simulation to obtain a fixed point on the forecast mappings for firms. First, a particular set forecast rules is assumed, allowing for the creation of value functions for the idiosyncratic firm problems using the simplified state space (z, k; A, m). Then, given the idiosyncratic firm value functions, the model is simulated. Throughout this paper unless otherwise noted, aggregate and productivity shocks in the KS method, as well as the PARAM and XPA techniques, are discretized using the Markov chain approximation process of Tauchen (1986). "}, {"section_title": "Parametrized Distributions Algorithm", "text": "The PARAM algorithm is based on the work of Algan et al. (2008Algan et al. ( , 2010a, which was done in the context of the incomplete markets model, and the solution technique implied aggregates can be obtained by integrating over the cross-sectional distribution of firm-level productivity and capital (z, k). Such integration is the key step within the PARAM algorithm and is performed numerically using flexible exponential functional forms for the density of the model which exactly match the aggregate moments m together with the higher-order reference moments in the cross-section. Iteration on prices and next-period moments continues until a fixed point is achieved, at which point the next value function iteration step is taken. Once the value function converges, the model is solved. Note that crucially the PARAM approach does not require simulation and therefore leads to large time savings relative to the KS algorithm's solution. However, if desired, new values for reference moments can be computed from simulation and updated until an outside fixed point is achieved, similar to the KS technique. In either case, however, simulation in each period requires a fixed-point iteration routine over market-clearing prices and next-period moments, similar to the process within the model solution step and involving integration over parametrized cross-sectional densities. See Appendix A for further details on the PARAM algorithm, as well as the functional forms used for the assumed cross-sectional densities. 8"}, {"section_title": "Explicit Aggregation Algorithm", "text": "The XPA solution method relies upon the techniques suggested by Den Haan and Rendahl (2010), as first adapted and applied to the heterogeneous firms model by Sunakawa (2012). The algorithm is essentially identical to the KS method, also making use of a bounded rationality assumption replacing the aggregate state space (A, \u00b5) with an approximation based on moments (A, m) and using forecasting rule\u015d m and\u02c6 p for the aggregate moments and prices. However, there is one main di\u21b5erence between the two techniques. XPA replaces the simulation step of the KS routine with an aggregation across a fixed cross-sectional distribution which is made feasible through the substitution of aggregate states into idiosyncratic policies. In other words, once value functions and policies are obtained based on a simplified state space of (z, k; A, m) and the posited forecast rules, marketclearing prices are obtained by integrating policies over the constant exogenous ergodic distribution of z and ignoring heterogeneity in idiosyncratic capital k. Afterwards, the forecast rules can be updated from the moments and prices generated in this manner until a fixed point is achieved. As the original work by Den Haan and Rendahl (2010) noted, substitution of aggregate states into idiosyncratic policies creates a Jensen's inequality-type bias in the forecast system (and hence the resulting aggregate solution) which can be ameliorated in a straightforward way by use of information from the steady-state solution. 9 Importantly, however, avoiding simulation within the solution step also allows for large time savings, as emphasized and put to interesting practical use for structural estimation of technology shocks by Sunakawa (2012)."}, {"section_title": "Projection plus Perturbation Algorithm", "text": "The REITER algorithm is based on the work of Reiter (2009) in the context of the incomplete markets model. The REITER approach has gained traction recently in the analysis of models with nonconvex costs and heterogeneity, being used in Costain and Nakov (2011) and Reiter et al. (2009). To my knowledge, this paper is the first application of the approach to a version of the Khan and Thomas (2008) model by itself, although it should be noted that Reiter et al. (2009) analyzes a similar stickycapital environment with the addition of a New Keynesian sticky-price structure. The REITER solution method departs in two important ways from the KS, PARAM, and XPA approaches. First, and importantly, the algorithm solves an approximation to the full, rational expectations equilibrium of the model rather than relying upon a bounded rationality assumption to reduce the state space using a set of moments. Therefore, the REITER method can be used as a check on the bounded rationality assumptions of the KS, PARAM, and XPA algorithms. Second, the RE- The REITER approach relies upon three steps. The first step imposes almost trivial computational cost: the solution of a steady-state model with no aggregate uncertainty but maintaining micro-level nonlinearity, using a discretization or histogram for idiosyncratic states (z, k). Then, the second step writes the full, discretized rational expectations equilibrium as the solution to a system of nonlinear equations F . The system is a function of current and lagged values of a large endogenous vector X t , as well as some exogenous aggregate shocks \u270f t . In the application to the heterogeneous firms model, the endogenous vector includes aggregate productivity, the cross-sectional histogram weights on each idiosyncratic point, firm values at a set of discrete points, as well as some implied model aggregates including price, output, investment, and labor. Therefore, the system F must take into account Bellman equations, distributional transitions, and equilibrium conditions. The third step involves the application of standard techniques for the solution of dynamic linear rational expectations systems, such as the method of Sims (2002) or Christiano (2002), to the solution of the heterogeneous firms model. Through numerical di\u21b5erentiation, the system F can be written as a linear approximation around the steady-state solution of the model, and then the standard methods for the solution of linear models may be applied. Further discussions of the details of the REITER solution method can be found in Appendix A. "}, {"section_title": "Comparing Solutions", "text": ""}, {"section_title": "Unconditional Simulation: Business Cycle Moments and Micro Investment", "text": "To begin the comparison, Figure 2.1 plots a representative 100-period portion of larger 2000-period simulation of the solutions from each technique, displaying log aggregate output, investment, labor input, exogenous productivity, and market-clearing price. For strict comparability we must maintain identical driving processes across the discretized aggregate productivity series of the KS, PARAM, and XPA methods and the REITER linearized solution, which by contrast takes continuous local shocks to aggregate productivity. To accomplish this, a set of continuous productivity shocks duplicating the discretized aggregate productivity process were trivially computed and input into the REITER solution to produce Although the simulated fluctuations are quite similar across solution methods in Figure 2.1, a few patterns are immediately visible to the naked eye. First, mean di\u21b5erences between the simulated aggregates appear small. This result is confirmed by Table 2.2, which reports average di\u21b5erences between each series and a KS method )) for each solution method and series X t . The exogenous aggregate productivity process reflects a Markov chain discretized using the Tauchen (1986) procedure and is held constant across solution methods during the simulation. To achieve this, an identical simulated discretized productivity process is input directly into the KS, PARAM, and XPA solutions, while a series of continuous aggregate shocks exactly replicating the discretized productivity process is input into the REITER solution. For each method, the full 2000 period simulation for each solution method begins after 500 periods, with an initial burn-in period discarded to avoid the influence of initial conditions on the simulated aggregates. baseline of well under 0.7% in magnitude throughout the simulation. However, and especially for the price and labor series, there appear to be some di\u21b5erences in volatility between the projection-based solutions KS, PARAM, and XPA and the linearized RE-ITER method. In particular, the REITER-simulated price (labor) series are slightly less (more) volatile than the other three simulations, a fact that is reflected in the business cycle moments available in Tables 2.3 and 2.4. The standard set of HP-filtered business cycle volatilities and ratios in Table 2.3 reveals broadly similar and familiar variability of aggregate output, as well as relative volatilities of productivity, across all four solution methods. 11 Furthermore, for all series, flow investment is at least several times more volatile than output, with the volatility of consumption (equal to that of price, given the log transformation of p = 1 C used here) smallest due to the smoothing e\u21b5ects of general equilibrium. Importantly, however, as noted above, the REITER solution delivers higher variability of labor as well as lower variability of prices or consumption than the projection-based techniques. This phenomenon is not due to the use of discrete shocks with the linearized 11 Comparison of Tables 2.3 and 2.4 here to Table IV of Khan and Thomas (2008) will reveal some moderate quantitative di\u21b5erences between the business cycle moments of this paper's KS simulation and their \"GE lumpy\" model. Given the removal of maintenance investment for expositional clarity, and the distinct discretizations of aggregate productivity, we would not expect identical results. Therefore, it should be emphasized that the correct analysis of the results in this paper is based on relative comparisons across methods, rather than direct comparison with Khan and Thomas (2008). The second through fifth columns report the ratio of the standard deviation of the aggregate HP-filtered log investment, labor, exogenous productivity, and price series to the output standard deviation in the first row. Following Khan and Thomas (2008), the HP-filter uses smoothing parameter 100. In the first four rows, the exogenous aggregate productivity process reflects a Markov chain discretized using the Tauchen (1986) procedure and is held constant across solution methods during the simulation. To achieve this, an identical simulated discretized productivity process is input directly into the KS, PARAM, and XPA solutions, while a series of continuous aggregate shocks exactly replicating the discretized productivity process is input into the REITER solution. In the fifth row, analogous moments from a continuous shock simulation for the REITER solution is reported, labelled \"REITER-OWN.\" For each case, the full 2000 period simulation for each solution method begins after 500 periods, with an initial burn-in period discarded to avoid the influence of initial conditions on the simulated aggregates. REITER solution, as the same conclusion can be drawn from the moments of the REITER-OWN simulation based on continuous shocks. Rather, the moderate pricedampening appears to be an implication of the use of local perturbation techniques rather than projection in the aggregate state space. Business cycle correlations with output reported in Table 2.4 reveal a similar message: broad similarities among the KS, PARAM, and XPA methods, with a bit less (more) comovement of price (labor) with aggregate output in the REITER simulations. As I will argue below, the perturbation approach of the REITER technique o\u21b5ers attractive scalability of the complexity of the aggregate state space, but the potential for di\u21b5erences in macroeconomic volatilities relative to the KS method in the heterogeneous firms context should be kept in mind by researchers using the technique. One important advantage of the heterogeneous agents approach is the possibility of disciplining the parametrization of a model with evidence from the micro-level distribution of investment rates. A number of cross-sectional moments can be computed for each period of the unconditional simulation of the model within each solution Note: The business cycle correlations for the KS, PARAM, XPA, and REITER solutions over a 2000-period unconditional simulation are reported in this table in the first four rows. The first column reports the correlation of the HP-filtered log aggregate output series with itself, 1. The second through fifth columns report the correlation of the aggregate HP-filtered log investment, labor, exogenous productivity, and price series with the filtered output series. Following Khan and Thomas (2008), the HP-filter is performed with smoothing parameter 100. In the first four rows, the exogenous aggregate productivity process reflects the Markov chain discretized using the Tauchen (1986) procedure and is held constant across solution methods during the simulation. To achieve this, an identical simulated discretized productivity process is input directly into the KS, PARAM, and XPA solutions, while a series of continuous aggregate shocks exactly replicating the discretized productivity process is input into the REITER solution. In the fifth row, analogous moments from a continuous shock simulation for the REITER solution is reported, labelled \"REITER-OWN.\" For each case, the full 2000 period simulation for each solution method begins after 500 periods, with an initial burn-in period discarded to avoid the influence of initial conditions on the simulated aggregates. method, based directly on the Young (2010)-style simulated histograms for the KS and XPA solutions, the parametrized cross-sectional densities and coe cients of the PARAM method, and the endogenous discretized vector of histogram weights available directly from the REITER simulation. Here, we focus on the moments analyzed by Khan and Thomas (2008), including the prevalence of inaction, large positive or negative investment \"spikes,\" and the overall likelihood of positive or negative investment. Table 2.5 reports the mean levels of these statistics, together with the first and second moments of the investment-rate distribution. Comfortingly, the methods deliver broadly similar implications for the cross-section of investment, although the cross-sectional dispersion of investment rates is lower for the REITER solution. 12 Around three-quarters of firms are inactive in each period, with around one-fifth of firms exhibiting both positive investment spikes or positive investment overall. Much smaller proportions of observations see negative investment spikes or rates. 13 Note: The rows of the table above report the mean value, across periods, of the indicated microeconomic moment of the cross-sectional distribution of investment rates i k from a 2000-period unconditional simulation of the KS, PARAM, XPA, and REITER methods. The first row reports the level of investment rates, the second row the cross-sectional standard deviation of investment rates, the third column the probability of investment inaction, the fourth (fifth) columns the probability of positive (negative) investment spikes larger in magnitude than 20%, and the sixth (seventh) columns the probability of strictly positive (negative) investment rates. The first four columns report values from simulations based on identical discretized aggregate productivity processes, while the fifth column, labelled REITER-OWN reports a distinct simulation with continuous aggregate productivity shocks input into the REITER solution. In all columns, the full 2000 period simulation for each solution method begins after 500 periods, with an initial burn-in period discarded to avoid the influence of initial conditions on the simulated aggregates."}, {"section_title": "Impulse Response Functions", "text": "Now we turn to a comparison of the heterogeneous firms solutions based on conditional responses, or impulse response analysis, rather than unconditional simulations. At this point, some concrete decisions must inevitably be made about the manner in which to simulate the underlying object of interest, i.e. the average change in the forecast of a given series in response to a shock to aggregate productivity of a certain size. Two considerations will always face a researcher working with nonlinear discretized models like those considered here. First, given the nonlinear structure of the KS, PARAM, and XPA solutions, the average conditional response to a shock will depend both upon initial conditions and upon the size of the shock. Second, we may wish to consider a shock scaled to a certain average size, such as the calibrated standard deviation of the underlying true aggregate productivity process, but a discrete Markov costless maintenance investment, the simplified structure here results in higher levels of investment inaction and lower levels of negative investment, because firms do not have a low-cost option for engaging in small adjustments to their capital stock. Consistently across solution methods, the environment here therefore delivers moments broadly similar to those of the \"Traditional Model\" of Table II in Khan and Thomas (2008). for each solution method method and impulse responsex method t , averaged over the post-shock period. The exogenous aggregate productivity impulse response is simulated as suggested by Koop et al. (1996), discussed in more detail in Appendix C, and involves the computing the average log di\u21b5erence between 2000 independent simulations of 50-period length, with and without positive productivity shocks. chain only admits discrete innovations in the aggregate productivity series. Neither challenge is present with a linearized solution such as that available for the REITER method, since in that case a classical impulse response is computable directly from the coe cients defining the model solution, and the local linearity guarantees that for small perturbations the impulse response scales directly with shock size. To create an approximation to the average conditional response in our context, we therefore simulate the \"generalized nonlinear impulse responses\" of Koop et al. (1996), although for simplicity we refer to these simply as \"impulse responses.\" The details of this approach are contained in Appendix C, but a brief overview is in order here. The approach relies upon a large number of pairs of simulations, with one \"shock\" simulation and one \"no shock simulation.\" Within each pair, the two simulations are run under identical exogenous shock processes, with one di\u21b5erence. At a designated period after some initialization range, we impose a positive shock to aggregate productivity in the shock simulation, allowing the aggregates to evolve as normal afterwards. The average percentage di\u21b5erence, across simulation pairs, between the shocked and no shock simulations provides a reasonable approximation to the average innovation to a given series in response to a productivity shock. To generate a flexibly-sized aggregate shock using discretized productivity, we simply convexify the shock arrival within each simulation pair described above, imposing a shock only with a probability calculated to generate any desired average change in aggregate productivity. The details of this additional modification, as well as Figure   B.3 comparing the virtually identical linearized and simulated impulse responses for the REITER method are again available in Appendix C. Figure 2.3 plots the impulse response to a one-standard deviation (1.4%) average positive aggregate productivity shock for aggregate output, investment, labor, and price. The responses are qualitatively identical: an increase in aggregate productivity leads immediately to a jump in output, labor, and investment, together with an increase in consumption (reduction in price) that dissipates more gradually than the other series. Unsurprisingly, investment responds most strongly, with more moderate responses from other aggregates. Table 2.6 reports mean percentage di\u21b5erences between the conditional responses in the KS case and the PARAM, XPA, and REITER simulations, revealing little di\u21b5erence on average, all under a third of a percent in magnitude. Unsurprisingly, given the unconditional business cycle patterns considered above, the price (labor) response is smaller (stronger) for the REITER method than for the projection based KS, PARAM, and XPA methods."}, {"section_title": "Accuracy Statistics", "text": "As described above, firms making investment choices in the bounded rationality KS, PARAM, and XPA solution techniques must rely upon the simplified aggregate state space (A, K) to form expectations both about market-clearing prices today p, as well as the aggregate capital level in the next period K 0 . In the case of the KS and XPA solutions, firms make use of explicit log linear forecast rules for both quantities, conditional upon the discretized aggregate productivity state today. The PARAM method does not make available an explicit forecast rule, but PARAM does endogenously generate within the solution step a mapping over some projection grid on (A, K) to clearing levels of (p, K 0 ), as discussed in more detail in Appendix A. By linearly interpolating this mapping we can generate a forecast system for price and aggregate capital from the PARAM solution. There are several metrics commonly used in the heterogeneous agents literature to evaluate the internal accuracy of the expectations embedded in bounded rationality forecasting rules. For each forecast system, it is natural to compare several horizons of forecasts to gauge their internal accuracy, and the one-step ahead forecast accuracy is commonly evaluated using the R 2 of the regressions, as in Khan and Thomas (2008) or the original Krusell and Smith (1998) analysis. Similarly, the root mean  ) to realized next-period capital K 0 (the first two columns) and to market-clearing prices p (the final two columns). The first two columns report the maximum and mean Den Haan (2010a) statistic for aggregate capital K 0 , i.e. the maximum and mean values, in 100 times log di\u21b5erences, between a capital series based purely on iteration forward of the forecast system and the realized simulated values. The third and fourth columns reflect analogous statistics for the market clearing price p. The exogenous aggregate productivity process reflects a Markov chain discretized using the Tauchen (1986) procedure and is held constant across solution methods during the simulation. To achieve this, an identical simulated discretized productivity process is input directly into the KS, PARAM, and XPA solutions, while a series of continuous aggregate shocks exactly replicating the discretized productivity process is input into the REITER solution. For each method, the full 2000 period simulation for each solution method begins after 500 periods, with an initial burn-in period discarded to avoid the influence of initial conditions on the simulated aggregates. The exogenous productivity simulation used for calculation of the DH statistics are distinct from the simulation used within the model-solution step of any algorithm. squared error (RMSE) in percentage terms, can be computed from the one-step ahead capital and current-period price forecast rules. Both the R 2 and RM SE statistics are reported in Appendix B in Table B The R 2 and RM SE measures are widespread and therefore important forecast accuracy statistics. However, Den Haan (2010a) emphasizes that forecast series which \"correct\" errors within the forecast system by substitution of realized aggregate capital series into the regression each period can be a weak tool for identification of the accumulation of forecast errors over time. Therefore, Table 2.7 reports the maximum Den Haan (DH) statistic and mean DH statistic over the simulations. These statistics rely after initialization only on forward iteration of the forecasts themselves through the forecasting system and are equal to the maximum and mean di\u21b5erences between realizations and the iterated forecast values, in percentage terms. Two messages clearly emerge from the results in Table 2.7. First, the KS method delivers the most consistently accurate results by any metric, with maximum DH errors around half a percent or lower for both capital and price at an extended horizon, as well as even smaller average DH statistics for both series. As can be seen in Appendix Table B.2, R 2 measures very near 1 also result from the KS solution, a result which was emphasized by Khan and Thomas (2008). Clearly, the absolute levels of these errors are dependent upon the exact density of the aggregate state space projection grid chosen in this paper (see details on that in Appendix B), and we would like internal accuracy to deliver errors far smaller than these KS results. However, given a fixed projection grid, as considered here, we can conclude that the relative accuracy of the KS method is substantially higher than that of the PARAM or XPA solutions. A second result also becomes apparent from Table 2.7: overall, although the accuracy of both the PARAM and XPA methods is comparable, the PARAM method appears to deliver slightly more accurate internal expectations. Maximum and mean DH statistics reflect generally superior capital forecasting performance from PARAM, with about the same level of accuracy for price forecasts. As can be seen in Appendix Table B.2, the RMSE and R 2 metrics do also indicate reduced forecast accuracy for the XPA approach relative to the PARAM solution, especially for extreme values of aggregate productivity. A final note is in order concerning the REITER solution method. Since it is based upon an approximation to the rational expectations equilibrium of the model, there is no directly comparable notion of forecast accuracy for that solution technique. However, as suggested by Reiter (2009), we can increase the density of the underlying discretization of the cross-sectional distribution substantially (by one-third in the case considered here), and compare the maximum and mean simulated di\u21b5erence between market-clearing aggregate price series in the baseline REITER discretization and the higher density approximation. Those statistics, only very roughly analogous to the price DH statistics reported in Table 2.7, are 0.1830(0.0466%) for the max (mean) di\u21b5erences. The baseline REITER price simulation, together with the same series generated using the denser grid, are plotted in Appendix Figure B.1. Note: The quantities above refer to the runtime on a six-core 3.33GHz Dell Precision T3500 with 24 GB of RAM. of model solution and simulation code in parallelized Fortran (KS, PARAM, and XPA methods) as well as Fortran and MATLAB (REITER method). All of the code used to produced these results can be found on Stephen Terry's website. \"Solution\" refers to the time required for calculation of a forecast rule fixed point (KS and XPA), completion of value function iteration (PARAM), or calculation of the linearized model solution representation (REITER). Unconditional simulation refers to the time required for unconditional simulation of a 2000-period economy with identical exogenous aggregate shocks across solution methods and an initial simulation range before 500 periods discarded. IRF simulation refers to the time required for simulation of a conditional impulse response function to an aggregate productivity shock using the method of Koop et al. (1996), with a simulation length of 50 years, 2000 replications, and shocks held constant across solution methods. All models are solved using comparable idiosyncratic and aggregate grids, identical Bellman equation or policy iteration tolerances, and identical forecast rule initial conditions, with the exception of the REITER method, which is solved using a denser cross-sectional grid. See Appendix B for details."}, {"section_title": "Computational Time", "text": "A final explicit comparison of the solution techniques involves computational time. Although runtime comparisons inevitably depend upon the e ciency and choices made when coding the solution methods, as well as the specific language or software used and the details of the numerical approach, a few considerations help to allay those concerns in this case. The projection-based solution KS, PARAM, and XPA solutions techniques used here, as well as the code solving the model with no aggregate uncertainty, were all written in Fortran by the same researcher, liberally parallelizing when possible and executing the programs on the same hardware. Then, using the model with no aggregate uncertainty from Fortran as an input, the REITER method was implemented by numerically di\u21b5erentiating the equilibrium system described in Appendix A around the steady-state and solving the resulting linear system in MATLAB using the standard method of Sims (2002 "}, {"section_title": "Figure 2.1: Unconditional Simulation Comparison", "text": "Note: The figure above plots a representative 100-period portion of a 2000-period unconditional simulation of the KS, PARAM, XPA, and REITER solutions. The KS solution is in black, the XPA in red, the PARAM in green, and the REITER in blue. The exogenous aggregate productivity process reflects a Markov chain discretized using the Tauchen (1986) procedure and is held constant across solution methods during the simulation. To achieve this, an identical simulated discretized productivity process is input directly into the KS, PARAM, and XPA solutions, while a series of continuous aggregate shocks exactly replicating the discretized productivity process is input into the REITER solution. The full 2000 period simulation for each solution method begins after 500 periods, with an initial burn-in period discarded to avoid the influence of initial conditions on the simulated aggregates. A ), and because of the continuity of the aggregate productivity process the simulation di\u21b5ers from the discretized simulations possible for the KS, PARAM, and XPA solutions. The full 2000 period simulation for each solution method begins after 500 periods, with an initial burn-in period discarded to avoid the influence of initial conditions on the simulated aggregates. Note: Simulated impulse responses to an aggregate productivity shocks for the KS, PARAM, XPA, and REITER series are plotted above, in percentages. The KS solution is in black, the XPA in red, the PARAM in green, and the REITER in blue. The exogenous aggregate productivity impulse response is simulated as suggested by Koop et al. (1996) and discussed in more detail in Appendix C. The simulation consists of 2000 independent simulations of 50-period length, with and without imposed productivity shocks which occur during a period labelled 1 above. In one simulation, a positive shock to aggregate productivity with mean equal to one-standard deviation of the aggregate productivity shock process occurs, while in another simulation with otherwise identical exogenous series, no aggregate productivity innovation occurs. The reported seriesx method t is the mean of 100(log(X shock Note: The figures above plot a representative 100-period portion of a larger 2000-period unconditional simulation of the three solution methods with explicit forecast mappings from the approximate aggregate state space (A, K) to realized prices and next-period productivity (p, K 0 ), i.e. the KS (in black), PARAM (in green), and XPA (in red) methods. The first column plots the realized marketclearing price p (solid line) and the forecast value of price (dotted line), computed by iterating forward the forecasting system as suggested by Den Haan (2010a) rather than substituting realizations of aggregate capital at each point. Similarly, the second column plots the realized capital series against the iterated forecasts for aggregate capital made on the basis of the forecast system alone. During the simulation, the exogenous aggregate productivity process reflects a Markov chain discretized using the Tauchen (1986) procedure and is held constant across solution methods during the simulation. To achieve this, an identical simulated discretized productivity process is input directly into the KS, PARAM, and XPA solutions, while a series of continuous aggregate shocks exactly replicating the discretized productivity process is input into the REITER solution. For each method, the full 2000 period simulation for each solution method begins after 500 periods, with an initial burn-in period discarded to avoid the influence of initial conditions on the simulated aggregates. The exogenous productivity simulation used for calculation of the DH forecast series is distinct from the simulation used within the model-solution step of any algorithm. Note: The figure above plots the linearized impulse response function to a positive shock to the aggregate demand series D t in the extension of the heterogeneous firms model augmented with aggregate demand and labor preference shocks. These impulse responses are computed after solving the extended model using the REITER technique and are therefore labelled REITER-EXTENDED. Given the endogenous variables X t of the linearized system describing the economy, a solution X t X SS = A(X t 1 X SS )+B\u270f t of the model trivially yields impulse responses A t 1 B representing the economy's local response to each aggregate shock in the vector \u270f t . The impulse response here is scaled by the assumed standard deviation of innovations to the aggregate demand series, with the exogenous shocked series itself plotted in the right hand side of the third row. In this paper, we craft a model to match recent evidence showing that the firms in Europe that faced more direct competition from China's low-wage exports undertook bigger increases in product innovation. 5 To match this response, the model lets firms choose how much to invest in developing new products and processes. In the spirit of models of endogenous growth, 6 the model requires that all increases in productivity come from these firm-level investments in innovation. As a result, it makes it possible to trace the e\u21b5ects that a modest change in trade policy has on innovation through to the implied change in the aggregate rate of growth, taking full account of general equilibrium interactions. The model confirms the intuition that the dynamic gains from trade can be large, substantially larger than other comparable exercises suggest. The challenge in capturing the micro-evidence is to explain why a firm that is more exposed to competition from imports from China has a bigger incentive to develop new goods when imports are liberalized. The model shows that this is precisely what one would expect if factors of production are temporarily \"trapped\" within firms due to moving costs. If, for example, the skilled engineers at a firm are expensive to train and then lay-o\u21b5, a negative demand shock to a good they helped produce leaves them in the firm but reduces their opportunity cost. Under this scenario, the firm innovates after the trade shock not just because the value of a newly designed product has gone up, but also because the opportunity cost of designing and producing it have gone down. This interpretation is consistent with the evidence that firms shift resources out of activities that compete with imports from low wage countries. 7 The idea is also born out in many case studies of international trade in which firms respond to import competition from a low-wage nation by developing an entirely new type of good that will be less vulnerable to this type of competition. 8 trade elasticity combined with the import share of GDP (see also Arkolakis et al. (2012)). Melitz and Redding (2013) show that in a heterogeneous firm model, these are not su cient statistics for calculating welfare gains. They find that reducing trade costs from current US levels, has larger welfare e\u21b5ects in their more structural approach. 4 Larger welfare e\u21b5ects of trade can be generated by allowing for traded intermediates or by focusing on more open economies than the US. 5 Bloom et al. (Forthcoming). 6 See for example, Romer (1990), Aghion and Howitt (1992), and Grossman and Helpman (1991). 7 See for example, Bernard et al. (2006). 8 For example, Freeman and Kleiner (2005) report that when a large American shoe firm was faced In addition to this trapped-factor e\u21b5ect of trade on innovation, the model allows for the independent e\u21b5ect that a more integrated world market has on the steadystate growth rate (a \"market size\" e\u21b5ect). A reduction in trade barriers increases purchasing power in the South, which increases the profit that a Northern firm can earn from sales there. In contrast to the e\u21b5ect of trapped-factors on innovation, which arises only at firms that face direct competition from low-wage imports, this increase in potential profits causes an increase in the rate of innovation at all Northern firms, and is therefore harder to identify from micro-data. It is an incremental version of the scale e\u21b5ect on growth that has been examined in models of trade with endogenous growth by comparing two isolated economies with a single fully integrated economy. 9 This mechanism has not, to our knowledge, been investigated in a model that is rich enough to be used in a calibration. At a minimum, such a model must allow a comparison across equilibria with a continuum of degrees of openness. In a sensitivity analysis, we also verify that over a time horizon of roughly a century, the conclusions from our endogenous growth model are very similar to alternative calibrations based on a model of semi-endogenous growth (like that of Jones (1995b,a)) in which a policy change can have a prolonged e\u21b5ect on the growth rate that eventually converges back to zero. In our product-cycle model, innovation in the North produces new intermediate inputs that are used by firms in both the North and the South. In a balanced growth equilibrium, trade barriers prevent factor-price equalization, so goods produced in the South have an absolute cost advantage. We calibrate the model to match both the baseline rate of growth and the firm-level decisions about innovation from the micro literature. We find that the increased global integration of the OECD with all low-wage countries that took place during the decade around China's accession to the WTO increases the long-run rate of growth in the OECD from 2.0% per year to almost 2.4% per year. Of this increase, approximately one half, or 0.2%, can be with rising imports of cheap shoes from China it abandoned production of mass-market mens' shoes. But, rather than simply close its factory the firm introduced new types of shoes for smaller niche markets, using its newly idle engineers to help develop these and its idle production line to produce these. For example, one new product was a batch of boots with metal hoops in the soles that made it easier for workers to rapidly climb ladders, ordered by a local construction firm. The design for these boots earned a patent. All of this occurred because the abandonment of the production line for mass market shoes in response to Chinese competition, which left its engineers temporarily free to innovate new shoes. 9 See for example, Grossman and Helpman (1990) and Rivera-Batiz and Romer (1991). attributed to China by alone. Of course, small increases in growth can generate substantial improvements in welfare. This increase in the rate of growth from trade with the South has a welfare e\u21b5ect that would be equivalent to increasing consumption by 16%. Of this increase in consumption, 14% is from the increased profitability of innovation and 2% is from the trapped-factor e\u21b5ect. But although the trapped-factors mechanism has a smaller long-run welfare e\u21b5ect it is front-loaded, so over the first decade after the trade shock its e\u21b5ect on the rate of growth is similar in magnitude to the market-size e\u21b5ect and might therefore be of comparable interest in policy debates. For trade with all low wage countries, the trapped-factor e\u21b5ect increases the rate of growth by an additional 0.3% per year (i.e. the combined e\u21b5ect of the market size and trapped factors e\u21b5ects raises the growth rate from 2.0% to 2.7% per year) in the first decade after the liberalization, with about one third being due to China alone. Our results connect to several other lines of work. To simplify the analysis, the model allows for heterogeneity among firms only in the degree of import competition that they face. One natural extension would allow for other dimensions of heterogeneity (e.g. Melitz and Redding (2014)). 10 We also assume that firms operate in only one region, so another natural extension would allow for multinational firms that manage R&D and production in both the North and South (see Antr\u00e0s and Yeaple (2014) for an overview of the evidence and theory in this area). In a model of growth based on di\u21b5usion of heterogeneous stocks of existing knowledge that is complementary to our model based on innovation, Waugh et al. (2013) find that trade liberalization can encourage more firms with low productivity to seek out interaction with high productivity firms from whom they can learn. Because the gains from di\u21b5usion are never exhausted, faster di\u21b5usion in this setting can also, at least in some cases, lead to a permanently faster rate of growth. Recent papers have also considered the interaction between di\u21b5usion and heterogeneity. 11 Our estimates are conservative in the sense that all these extensions are likely to generate additional gains from trade. 10 Atkeson and Burstein (2010) consider a heterogeneous firm trade model with endogenous process innovation. They find that reductions in trade costs lead to no greater increases in welfare in such models compared to homogeneous firm models. Like Atkeson and Burstein (2010) we also find that the steady state increase in welfare determined by the insight from the older literature that larger market size increases the returns to product innovation. Unlike them, however, we generate more growth in the transition to the new steady state due to our new \"trapped factor\" e\u21b5ect. 11 See for example Sampson (2014) or Costantini and Melitz (2007). Our paper connects not just to the general literature cited above on the welfare e\u21b5ects of trade, but also on those papers that look specifically at the impact of trade with China (e.g. Ossa and Hsieh (2011)). Because of concern about increased inequality, an older literature on the distributional e\u21b5ects of trade that arise when labor is industry-specific (e.g. Mussa (1974)) is generating renewed interest . In such models, the gains from trade for some groups are o\u21b5set by welfare losses for others. As we note below, the optimistic conclusions from our analysis of the gains from trade need to be tempered if such e\u21b5ects are large. In contrast to this literature, where specificity has no social benefits, in our second-best model, trapped factors generate additional welfare gains when there are unexpected increases in trade. The model of innovation spurred by a reduction in the opportunity cost of the inputs used in innovation is reminiscent of the old idea that trade competition can e\u21b5ect X-ine ciency without following the type of principal-agent structure (e.g. Schmidt "}, {"section_title": "Closed Economy Model", "text": "We introduce the basic structure of the model for a closed economy. This lets us describe the technology and highlight the key equation in the model. It characterizes the rate of growth of the variety of inputs, which can also be interpreted as the rate of growth of patents or new designs. Note that in this initial closed-economy equilibrium derivation, we omit discussion of the costs that can trap factors in firms. We bring them in after introducing trade."}, {"section_title": "Technology", "text": "There are two types of inputs in all types of production, human capital and a variety of produced intermediate inputs. At any date, these inputs can be used in three di\u21b5erent productive activities: producing final consumption goods, producing new physical units of the intermediate inputs that will be used in production in the next period, and producing new designs or patents. We assume that the two types of inputs are used with the same factor intensities in these three activities, so we can use the simplifying device of speaking of the production first of final output, and then the allocation of final output to the production of consumption goods, intermediate  Using the convention noted above, we can speak of firms in period t devoting a total quantity Z t of final output to the production of new patented designs that will increase the existing stock of designs A t to the value that will be available next period, A t+1 . If we let C t denote final consumption goods, final output is divided as follows: The intermediate inputs are like capital that fully depreciates after one period of use, an assumption that is made more palatable by our choice of period that is 10 years long. The key equation for the dynamics of the model describes the conversion of foregone output or R&D expenditures Z t into new patents. In period t, each of a larger number N of intermediate goods firms indexed by f can use final goods (or more explicitly, the inputs that could produce final output) to discover new types of intermediate goods that can then be produced for use in t + 1. Let M t+1 denote the aggregate measure of new goods discovered in period t, and let M ft+1 be the measure of these new goods produced at firm f . Here, the letter M is a mnemonic for \"monopoly\" because goods patented in period t will be subject to monopoly pricing in period t + 1. Because our patents, like our capital, last for only one period, only the new designs produced in period t will be subject to monopoly pricing in period t + 1. At a formal level, it simplifies the analysis considerably to assume that capital lasts for only one period and that innovators need to look ahead only one period to calculate their monopoly profits. In particular, these assumptions imply that the model converges in very few periods to a new steady state growth rate after any policy change, as we shall see. To allow for the problem that firms face in coordinating search and innovation in larger teams, we allow for a form of diminishing marginal productivity for the inputs to innovation in any given period. Let Z ft denote the resources devoted to R&D or innovation at firm f at time t. We assume that the output of new designs will also depend on the availability of all the ideas represented by the entire stock of existing innovations, A t . Hence we can write the number of new designs at firm f as: where 0 < \u21e2 < 1. The exponent on A t is crucial to the long-term dynamics of the model. The choice here, 1 \u21e2, makes it possible for an economy with a fixed quantity of human capital H to grow at a constant rate that will depend on other parameters in the model. As an alternative, we could follow the suggestion in Jones (1995a) and use a smaller value for this exponent, in which case we could generate a steady state by allowing for growth in the quantity of H. In either approach, the model has to match the baseline rate of growth that prevails prior to the trade shock. For a given value of \u21e2, they will respond in qualitatively similar ways to a trade shock. As a result, the two types of model o\u21b5er di\u21b5erent very long-run (100+ year) predictions about the e\u21b5ect that the trade shock on growth, but are similar for the first \u21e1 100 years, which because of discounting is e\u21b5ectively all that matters for our results. We formally detail and calibrate an extension of our model with semi-endogenous growth and show the results are very similar (see appendix). Another way to characterize the production process for new designs is to convert the innovation production function in equation (3.1) to a cost function that exhibits increasing marginal costs of innovation in period t,  In most cases, symmetry will allow for substantial simplification of this expression."}, {"section_title": "Preferences", "text": "A representative household in this economy consumes the final good in the amount C t each period, inelastically supplies labor input H, and has preferences over consumption streams given by . The representative household receives labor income, owns all the firms, and trades a one-period bond with zero net supply. As usual, if consumption grows at a constant Ct Ct , and if r denotes the one period interest rate on loans of consumption goods, these preferences imply the result 1 + r = 1 (1 + g) . (3.3) Because the price of consumption goods is always one unit of the numeraire good, r is also the one period interest rate on loans denominated in the numeraire."}, {"section_title": "Open Economy", "text": "Suppose next that there are two regions or countries, North and South. We treat To allow for a continuum of possible levels of trade restriction, we assume that the government in the North imposes a trade restriction which allows only a proportion of o\u21b5-patent intermediate goods varieties produced in the South to be imported into the North. If we make the simplifying assumption that the goods with the lowest index values are the ones that are allowed to trade, Figure 3.1 describes the goods that are used in production in the North and the South. The goods with the lowest index values are called I goods to signal that they are imported into the North. In terms of production in period t, the range of the I goods is from 0 to A t 1 . These goods are produced in the South for use in the South and also produced in the South and imported into the North. Next come the R (for restricted) goods. These are produced in the North for use in the North and produced in the South for use in the South. Finally, we have the M (for monopoly) goods, which are produced in the North and used in production in both the North and the South. Hence, M t represents the new goods developed in period t 1 for sale in period t; R t represents the trade-restricted but o\u21b5-patent goods available for use in production in period t; I t represents the o\u21b5 patent goods that can be imported into the North for use in period t. In a small abuse of the notation, we will use the symbols I, R, and M to denote both the set of goods and its measure. In this two economy model, we can consider a unit of final output (or equivalently the bundle of inputs that produces it) in both the South and the North. We will use output in the North as the numeraire and define the Southern terms of trade q t as the price in units of final output in the North of one unit of final output produced in the South. We impose trade balance in each period so there is no borrowing between North and South. Along any balanced growth path, the interest rates in the North and South will be the same, but the restriction on borrowing is binding during the short transition to the new balanced growth rate that follows a policy change. The terms of trade q adjust to achieve trade balance in each period, which requires that the value of imports into the North, q t p \u21e4 It I t x I , is equal to the value of the goods that the North sells to the South, M . As in the usual product cycle model, we are interested only in the case in which the South has a cost advantage in producing goods that it can export, due to its lower wages. On the balanced growth path, this is equivalent to having q t < 1. In our analysis, we restrict attention to the case of values of the trade policy parameter that are low enough to ensure that this restriction holds. It is important for the operation of the model that in this case, trade balance does not lead to factor price equalization. Identical workers in the North and the South earn wages that when converted at the terms of trade q are higher in the North and lower in the South. Restricted intermediate inputs that are produced and used only in the South are less expensive there than the same goods produced and used in the North. However, because consumption goods in the South are also less expensive, the di\u21b5erence in the wages is much smaller after a PPP correction. Although the formal assumptions imply that all intermediate goods could be tradeable if all trade restrictions were removed, the model can easily accommodate the possibility that a portion of them are intrinsically non-tradeable. All that matters is that the restriction imposed by is binding in the sense that it artificially forces some goods that could be tradeable to be non-traded. To describe the equilibrium for the open economy, it helps to define a second (irrelevant) constant = (1 \u21b5) \u21b5 1 2 \u21b5 that is analogous to the constant \u2326 = \u21b5(1 \u21b5) 2 \u21b5 \u21b5 for the closed economy. For any given value of the trade parameter , a straightforward extension of the analysis for the closed economy yields a two equation characterization of the balanced growth rate and the associated terms of trade: Proposition 2. Open-Economy Balanced Growth Path For low enough values of the trade parameter , the world economy follows a balanced growth path with a common, constant growth rate of varieties, worldwide output, and consumption in each region. The growth rate g( ) and the terms of trade q( ) are determined by the zero marginal profit condition for innovation (3.6) and the balanced trade condition and q( ) < 1."}, {"section_title": "Proof in appendix.", "text": "After substitution of equation (3.7) into equation (3.6), the growth rate g( ) can be seen to be determined by the intersection of a downward sloping innovation marginal profit curve with an upward sloping innovation marginal cost curve. For clarity, see Figure 3.2 which plots a stylized version of the equilibrium innovation optimality condition and the result in Proposition 2. The marginal profit of innovation is strictly increasing in the trade openness parameter , so the the open economy balanced growth rate is strictly increasing in . This implies that the terms of trade q( ) is also strictly increasing in . Proposition 2 is an important result as it establishes that trade liberalization will increase growth rates as it increases the incentive to invest in innovation. Essentially this is because the e\u21b5ective size of the market has expanded and this increases the profitability for new goods. R&D investments increase until at the margin ex ante expected profits are again zero, but this will be at a higher growth rate. Revealingly, the innovation optimality condition (3.6) is quite similar to the one in the closed-economy version of Proposition 1. Except in place of H, the term H + q( ) 1 \u21b5 H \u21e4 now determines the extent of the demand for any input and the profit that it will generate. The reason is all innovation takes place in the North. And the worldwide demand for newly invented goods in the North depends on demand in the North, which is proportional to H and on demand from the South, which is proportional to H \u21e4 but with a downward adjustment induced by the terms of trade. A trade liberalization caused by an increase in leads to an increased flow of imported I goods from North to South. The elasticity of demand for all inputs is 1 \u21b5 > 1, so revenue increases when prices fall. This means that in response to the increase in imports into the North, the prices of the goods that the North imports must go up and the prices that it receives for goods that it sells to the South must go down. Both changes imply an increase in q. Lower prices in the South for the exported monopoly goods increase the returns to innovation. In equilibrium, the rate of innovation, hence the rate of growth must increase, which increases the marginal cost of innovation and re-establishes the zero profit condition at the margin."}, {"section_title": "Trade Shocks", "text": "The open economy analysis in the last section calculated the constant perfect foresight growth rate and interest rate associated with a constant value of the parameter . Next, we start from a balanced growth path trade at an initial level and consider the e\u21b5ects of an unanticipated and permanent trade shock to a more liberal trade regime with 0 > . To carry this exercise out, we must be more explicit about the timing of decisions relative to the announcement of the change in ."}, {"section_title": "Timing with a Trade Shock", "text": "To specify the timing, it helps to return to the underlying model with three di\u21b5erent productive activities. Recall that there is a group of consumption good producing firms that acquire H and intermediate inputs x j and use them to produce final consumption goods using the basic production function We now want to be more precise about a second group of intermediate input producing firms that demand types of inputs in the same factor proportions and that devote these inputs either to the production of new designs or to the production of physical quantities of its intermediate inputs. For a specific firm f , let inputs with an overbar denote the inputs of a particular intermediate input producing firm f that are allocated to the production of new designs. In this case, the number of new patents that result can be written as In parallel, denote the inputs that firm f devotes to the production of physical units of intermediate inputs with a tilde. Let A f denote the set of goods that firm f can produce at time t at a positive profit (if it is still under patent) or at zero profit. Note that there may be some goods that the firm used to produce that are no longer in A f because they are now imported from the South and would be unprofitable to produce. Then we can characterize the production of the intermediate goods that this firm will produce in this period that will be sold in the next period as The total quantity of an input such as H that is controlled by firm f is the sum of H, which it devotes to innovation, and e H, which it devotes to production of physical units of intermediate goods for sale in the next period. In the same way, the total quantity of any intermediate input that it has available for production can also be split between these same two activities. This means that we can think of inputs being allocated between the three productive activities in two steps. First, inputs are allocated between the consumption good producing firms and the intermediate input producing firms. Next, the intermediate input producing firms make an internal allocation decision, dividing up their inputs between innovation and the production of physical units of the intermediate inputs that will be for sale in the next period. When is constant, a constant fraction of the o\u21b5-patent goods that each intermediate input firm in the North had previously produced under trade protection are now exposed to import competition. In the aggregate, the total stock of goods that are available as imports in period t is equal to times the o\u21b5-patent goods in period t, or A t 1 . For simplicity, we assume that this process of exposure is evenly distributed across all intermediate input producing firms. For firm f, this means that if it had a set of goods A f that it produced last period with measure m(A f ),in the current period, it will produce a measure of goods equal to (1 )m where M f is the set of new goods that it invents. Firms can take account of the predictable shrinkage in the goods that they can produce when they make their decisions about how much of each type of input to acquire. In contrast, if a government mandates in period 1 an unanticipated increase in to 0 , there will be a jump in the number of goods that are subject to import competition. At the aggregate level, the measure of goods in the goods unexpectedly become unprofitable for Northern firms is A 0 ( 0 ). To match the micro data, which has some firms that are exposed to larger trade shocks than others, we want to allow for the possibility that this range of goods A 0 ( 0 ) is not equally distributed among all firms. To do this, we split the set of intermediate input producing firms in the North into two groups of equal size. We refer to these as the \"Shocked\" and \"No Shock\" firms. We assume that all the goods that are unexpectedly exposed to competition from imports are goods that were previously manufactured by the Shocked firms."}, {"section_title": "Timing with Fully Mobile Inputs", "text": "With these definitions in mind, we can describe two di\u21b5erent assumptions about the mobility of factors. Consider first the case that we refer to as \"Fully Mobile\" because all allocation decisions are made after the shock is announced. 1. Intermediate goods firms enter period 0 with completed intermediate goods. 2. The government in the North announces a new level of the trade restriction 0 that will be in e\u21b5ect in period 1, together with the specific goods that will no longer be protected, which thereby determines which firms are in the shock group and which are in the no shock group. North stands ready to enter if these quantities are too low to yield a market price equal to marginal cost for the R goods that will be sold in the North. In this case, the trade shock of an increase from to 0 will be public information before any inputs are allocated to any firms. In particular, any intermediate input producing firm knows about the trade shock and knows if it is a shock firm or a noshock firm. If the intermediate input producing firms as a group want to reduce their input demands, inputs can freely move into the production of consumption goods."}, {"section_title": "Trapped Factors Case", "text": "We also consider a \"Trapped Factors\" case with less mobility in response to a trade shock. We do so because of evidence on the impact of Chinese trade on European firms from Bloom et al. (Forthcoming). They find that exposure to Chinese import competition leads to increases in innovative activity, but much of the increase in innovation occurs within incumbent firms. Even though the prices and profits of a\u21b5ected firms were falling, those that did not exit increased their rates of innovation. Relatedly, Freeman and Kleiner (2005) showed in a case study that US shoe manufacturers switched from making low-cost mass market shoes to innovative niche products when faced with rising Chinese competition. In Italy Bugamelli et al. (2010) show a range of manufacturers from ceramic tiles to women's clothing switched to more innovative high-end products in response to rising low-wage competition. With "}, {"section_title": "30.", "text": "The new level of the trade restriction 0 that will be in e\u21b5ect in period 1 is announced, together with the specific goods that will no longer be protected, which thereby determines which firms are in the shocked group and which are in the no shock group. The announcement of the increase in induces three types of adjustments that can influence the demand that an intermediate input producing firm has for inputs. First, faster growth changes equilibrium interest rates and the desired split between saving and consumption, which has to be mirrored by a split of inputs between firms that produce consumption goods and firms that produce intermediate inputs. Second,16 Note also that to fit this evidence, and the evidence from Bernard et al. (2006) that firms shift toward the production of new goods, sometimes even goods in new industries, we have developed a model in which innovation leads to new goods (Romer, 1987(Romer, , 1990) rather than quality improvements (Aghion and Howitt, 1992). A model of quality improvement based on \"escape competition\" as in Aghion et al. (2005) would be more applicable to trade liberalization between countries with similar factor prices than the case we consider of liberalization with a South that has a pure cost advantage in any goods it can export production. an intermediate input producing firm will want to allocate relatively more resources to innovation. Third, an intermediate input producing firm in the Shock group that has lost some of its potential output markets will want to release inputs that can be taken up and used by other firms. To calculate the full general equilibrium e\u21b5ects of the shock, we must take into account not only these impact e\u21b5ects on input demands but also any induced changes in interest rates and the terms of trade. The full equilibrium definitions for the closed economy, the open economy, and the trapped-factors trade shock economy can be found in appendix. Factors will be trapped in a firm in the Shock group if ex post it wishes that had not taken on so many inputs. In this case, the shadow value of its inputs will be lower that it was before the shock hit. Before moving to the quantitative analysis, we note one final technical detail. For factors of production to be trapped in a firm that has lost lines of production, we have to make an assumption which ensures that it can not simply steal lines of production from other firms. If it could, then there would be no net e\u21b5ect of having trapped factors. Instead of having factors move to the production opportunities, production opportunities can move to the factors. To give the assumption of trapped factors some bite, we make the additional assumption that the cost of producing a unit of any intermediate good is substantially lower for a firm that developed the good and produced it in the past than it would be for an intermediate input producing firm that does not have this kind of experience. Having made this assumption, we then have to make a further simplifying assumption to ensure that incumbent producers of the protected goods do not have market power. This is where we rely on the existence of a second potential type of intermediate input producing firm, a \"fast copier,\" which has been mentioned before. This means that there are two distinct types of intermediate input producing firms, innovators or fast copiers. Innovators, the type of firm we have been describing so far, can develop new goods but they can't copy the goods developed by other firms. In contrast, fast copiers can produce intermediate inputs developed by other firms at the same cost as the other firm but they are not capable of innovation. We can now be more precise and say that all the intermediate input producing firms in the South are fast copiers. All the intermediate input producing firms that are active in the North are innovators. In equilibrium, fast copiers never produce anything in the North. Nevertheless, their presence limits the pricing decisions of the intermediate input producing firms in the North and forces them to sell o\u21b5-patent goods at marginal cost."}, {"section_title": "Quantitative Analysis: OECD Trade Liberalization with Non-OECD Countries", "text": "We can now calibrate and perform a quantitative exercise with the model, considering the impact of a trade shock over a full transition path. Appendices B and C give more details on the calibration and solution. First, we assume a model period of 10 years. Then, we calibrate the model economy to match long-run growth rates, and movements in trade flows between the OECD and non-OECD countries from 1997-2006, the ten-year window surrounding Chinese WTO accession in 2001. As plotted in Appendix Figure C.2, imports from non-OECD countries into the OECD almost doubled as a proportion of GDP over this period. China in particular accounts for almost half of the increase in low-wage imports. To match this pattern from the data, the model experiment we consider is an unanticipated, permanent trade shock moving from the balanced growth path from trade policy to a new liberalized policy 0 , as detailed in the theory section above."}, {"section_title": "The Long-Term Impact of a Trade Shock", "text": "We summarize the long-term impacts of trade liberalization in our model in Table   3.1. To reproduce the changes in the OECD imports to GDP ratio observed in the data requires an exogenous increase in trade policy from 9.7% to 20.7%, and this exogenous change produces, through the e\u21b5ective market size e\u21b5ect discussed in Section 3.3, a movement in the long-term growth rate from its pre-shock calibrated value of 2.0% to a new value of 2.37%. 18 We take two steps to examine the robustness of our results to this calibration strategy. First, in a robustness check discussed further in a later section we consider an alternative calibration window ending at Chinese WTO accession in 2001 for the pre-shock growth rate, and second we also solve a version of the model with \"semi-endogenous growth\" and therefore only level rather than growth e\u21b5ects on output from changes in trade policy. Details on the semi-endogenous growth version of the model can be found in the appendix. "}, {"section_title": "Transition Dynamics in the Fully Mobile Economy", "text": "Next we consider the transition dynamics of the fully mobile economy, starting from the balanced growth path associated with trade policy and allowing an unanticipated and permanent trade policy shock ! 0 that is announced in period 0, to become e\u21b5ective in period 1. In Figure 3.3, we plot the aggregate transition dynamics of the fully mobile economy for aggregate variety growth, the terms of trade, and output growth in the North and South. Consumption growth, and interest rates, follow the pattern for output growth. The full transition to the new balanced growth path is complete in approximately 6 periods (60 years). Given the trade liberalization shock, the Southern terms of trade increases rapidly to maintain balanced trade, leading to an associated increase in the returns to innovation and hence the aggregate variety growth rate. Consumption smoothing dictates a slower, smooth transition of consumption growth rates, output growth rates, and interest rates in both economies to their long-run values. 19 We can also compute the long-run welfare gains from trade in the fully mobile environment, taking the transition path into account (see Table 3. "}, {"section_title": "Transition Dynamics in the Trapped Factors Economy", "text": "In Figure 3.4 we plot the path of some selected aggregates over a trapped factors transition path. Comparing the trapped factors transition with the fully mobile transition in Figure 3.3 above, we immediately note that the variety growth rate is higher upon impact of the trade shock. Instead of a growth rate of about 2.4% in the shock period as seen in the fully mobile transition, the trapped factors variety growth rate on impact is 2.7%. The increased Northern innovation and flow of M goods from North to South in the shock period slows the appreciation in the Southern terms of trade, and output growth in the North and South both overshoot their longrun levels after the trade shock. Although the transition path is again complete in approximately 6 periods (60 years), the path of innovation is clearly significantly higher in the presence of short-run adjustment costs and trapped factors. Recall that we assume that there are two industries with half of the firms each. One of these industries (Shocked) contains all the shocked firms and bears the brunt of the direct e\u21b5ects of liberalization in that all of the liberalized R goods varieties which lose protection are in this industry. The other industry (No Shock) has no liberalized R goods. In Figure 3.5 we plot three separate patent flows. In the solid black bar on the left labeled \"Pre-Shock,\" we present period 0 or pre-shock patent flows for the \"Shocked\" and \"No-Shock\" industries, which are ex-ante identical. These patent flows are arbitrarily normalized to 1, 000 for ease of reference. The blue middle bar with upward-sloping lines and the red right bar with downward sloping lines, by contrast, plot the patent flows for industry \"No-Shock\" and for industry \"Shocked\" during period 1, the period in which policy liberalization becomes e\u21b5ective. Although both industries increase patenting during the shock period due to terms of trade movements, industry \"Shocked\" patents approximately 28.8% more in the period after the shock. The cross-sectional link between industry patenting and exposure to low-cost import competition in the model is consistent with the increased innovation documented by Bloom et al. (Forthcoming) in European firms exposed to Chinese import competition relative to their una\u21b5ected peers. 21 The stark increase in innovation or patenting at firms in the shocked industry is directly linked to a surplus of resources useful for R&D at those firms, which unexpectedly lose 24% of their R goods varieties to import competition. In Figure   3.6 we expand the set of variables included in the trapped factors transition path. In the top two panels we can see the shadow value of resources in each industry, which in normal times without trade shocks is normalized to 100%. Since the lost R goods opportunities imply a surplus of inputs which must be allocated to the unanticipated use of innovation, on the top left panel we see an opportunity cost or resource shadow value decline of 25.5% in period 1 for firms in the shocked industry. In the upper right panel of Figure 3.6 we also see a much more moderate decline in opportunity costs by around 9.8% at firms in the no-shock industry. This is less intuitive and operates entirely through general equilibrium channels. To understand this, we must examine the movements in interest rates also recorded in Figure 3.6. The sudden increase in variety growth in the Northern economy in the shock period induces an increase in consumption growth rates and hence interest rates. Therefore, even though this does not represent an increases in resources within the no-shock firms, the higher interest rates and hence changed marginal valuations of their Northern owners require a fall in these firms' shadow values to deliver consistency with their value-maximization problem. Turning again to welfare measures, the total consumption equivalent welfare increases from the trade shock with trapped factors are 16.3% for the North, compared to the 14.2% dynamic gains in the fully mobile case discussed above. To understand this larger welfare gain from trapped factors, note that the externalities in the innovation process through which previous ideas at one firm assist later innovation by all firms are not taken into account in the firm's innovation optimality conditions. Hence, there is \"too little\" R&D from a social welfare perspective, as is typical in endogenous growth models. The initial increase in variety growth due to the trapped factors mechanism helps to moderate this social ine ciency and leads to a welfare increase from our model. Compared to the aggregate welfare gains of 14.2% from trade liberalization in the fully mobile case, the marginal impact of the trapped factors mechanism is approximately a tenth of the total gains from trade liberalization (2.1%). However, while this is small overall, in the first period (10 years in our simulation) trapped factors roughly doubles the impact of the trade shock on innovation. So the short-run impact of this mechanism is potentially large, and will thus be important for policy (for which a 10 year time frame is the long-term) and empirical work."}, {"section_title": "What is the Contribution of China to OECD Growth?", "text": "Our model suggests that there was a large scale e\u21b5ect and smaller trapped factors   We conclude that understanding the OECD and Chinese policies which contributed to the increased trade with China is a crucial avenue to consider when quantifying dynamic gains from liberalization over this period. A caveat to this strategy is that it assumes a counterfactual world in which policymakers do not \"make up the gap\" by relaxing restrictions on non-Chinese low wage imports. If such a relaxation did take place this would reduce the marginal contribution of China to welfare. In a robustness check in the appendix, we compute the marginal impact of China with half of all Chinese import growth allowed in as imports from the non-OECD non-Chinese countries. As expected, these results essentially halve the Chinese contribution to innovation and welfare."}, {"section_title": "Price and Variety E\u21b5ects", "text": "A useful exercise is to decompose the impact of liberalization on output into dynamic variety e\u21b5ects, which require an endogenous growth structure, and price e\u21b5ects that are the focus of more traditional quantitative trade models. The relative contributions of price and variety e\u21b5ects may di\u21b5er in the short run and the long run, so we will consider both. We find that price e\u21b5ects are responsible for about one-third of  . 23 Since imported goods are cheaper than domestic goods, the di\u21b5erence in output relative to the baseline level Y 1 can be interpreted as due to a traditional price e\u21b5ect. We then compute the value of Northern output if we shut down not just the unexpected price e\u21b5ects but also hold back the creation of additional newly innovated M goods varieties at its pre-shock level (Y novariety 1 ). The ratio of price to total output gains in the shock period is = 0.37, so slightly more than one third of the output gains from liberalization in the shock period arise from the price e\u21b5ect. 24 The price e\u21b5ects are substantial in the shock period when a large number of cheaper goods are unexpectedly allowed into the North as new imports. By contrast, the contribution of price e\u21b5ects decreases to a miniscule level in the long run. In particular, let Northern output along the pre-shock balanced growth path in an arbitrary period be Y ss . Then, compute output if the ongoing conversion of goods from domestic to lower-price imported varieties is shut down for one period (counterfactual level "}, {"section_title": "Extensions and Robustness", "text": "In this section we discuss some extensions and the robustness of our results."}, {"section_title": "Robustness of Calibration", "text": "The qualitative e\u21b5ect of trade liberalization on growth, and the boost of innovation from the trapped factors mechanism, are quite robust to alternative parametrizations. To demonstrate this we vary parameter values and consider the impact upon the variety growth rate in the trapped factors transition in Figure 3.8. In none of these cases is the pattern or magnitude qualitatively changed. Finally, in an unreported robustness check, we used an alternative calibration window of the pre-shock growth rate to the US per capita real GDP growth rate from 1960-2001, with qualitatively similar dynamics starting from a di\u21b5erent growth level."}, {"section_title": "Semi-endogenous growth", "text": "As discussed above, Jones (1995a) argues for an alternative innovation production function. We have been using M ft+1 = (Z ft ) \u21e2 A 1 \u21e2 t , but an alternative is to use an exponent less than 1 \u21e2 on A 1 \u21e2 t following Jones' \"semi-endogenous\" approach. In such models steady state growth no longer depends on the level of human capital but on the growth of human capital. In the appendix, we fully re-derive all the implications for long-term growth from such a model and numerically compute transition paths in this case, allowing for growth in human capital. Reasonably calibrated transition dynamics are extremely persistent, and long-run di\u21b5erences between our baseline model and the semi-endogenous growth model are heavily discounted into the future. The two model assumptions therefore deliver remarkably similar quantitative welfare results."}, {"section_title": "R&D congestion e\u21b5ects", "text": "Another concern with our baseline model is that R&D could have cross-firm congestion e\u21b5ects -from research duplication or patent race type e\u21b5ects. In an extension discussed in detail in the appendix, we also introduce a model parameter \u2318 which allows for R&D congestion externalities. \u2318 flexibly nests our baseline case of no congestion externalities, (\u2318 = 1), but also allows for intermediate degrees of congestion all the way to the extreme case of full externalization of R&D costs (\u2318 = 0). Empirical evidence suggests that these congestion e\u21b5ects are not large in the economy as a whole. Bloom et al. (2013b) estimate congestion e\u21b5ects from a large sample of US firms and find them to be statistically insignificant (i.e. \u2318 = 1). 25 Consequently, we have chosen to omit R&D cost externalities from the baseline model. For completeness, however, we also consider the intermediate case of \u2318 = 0.5 in Figure   3.8. In this case, congestion externalities dampen the magnitude of the short-term growth boost from trade. The dampening e\u21b5ect is not quantitatively large, however, and the long-run growth e\u21b5ect remains the same."}, {"section_title": "Other channels through which China influences welfare", "text": "Trade between OECD countries and low-wage countries like China can have a large number of e\u21b5ects in addition to the ones considered in this paper. We focus on its impact on the incentives for developing new goods because of the sheer potential scale of the dynamic gains from trade that it o\u21b5ers. The most important potential o\u21b5set to these gains, however, might come from the labor market. Our model, like many others in trade, abstracts away from the unemployment and wage losses that may arise as workers are reallocated. Recent work by Pierce and Schott (2012),  and  suggests that these dislocation e\u21b5ects can be substantial. There may be long-run e\u21b5ects on inequality through Heckscher-Ohlin factor price equalization e\u21b5ects or imperfect labor markets. Helpman et al. (2010) show how trade may increase steady state unemployment and wage inequality by making the exporting sector more attractive in a search theoretic context, with some evidence for the theory in Helpman et al. (2012). When our single adjustment cost traps factors of production inside a firm, an unexpected increase in low-wage imports will cause losses that must be shared between the workers and the equity holders of an a\u21b5ected firm. We do not model how these losses are shared, so that in e\u21b5ect our approach is equivalent to making the assumption that there is a perfect insurance market among all residents in the North. To be sure, other types of adjustment costs could reduce welfare by making unemployment worse or exposing people to new uninsured risks. But as our analysis shows, in endogenous models of growth, it does not immediately follow that adjustment costs necessarily reduces gain from trade."}, {"section_title": "Anticipation E\u21b5ects", "text": "We have modeled the trade shock as being unexpected to firms. Although events such as China's WTO accession were of course partially anticipated, there was some surprise as negotiations were fraught. Moreover, in the entire European Union the liberalizations with China were temporarily reversed due to a political backlash. To the extent that a shift from to 0 is announced in anticipated, agents will change their behavior. 26 In particular there will be a disincentive to invest in trapped factors because the firm anticipates the liberalization. Hence, Northern firms will start shifting into more innovation activities prior to the liberalization. The transition dynamics will change even though the long-run post-transition growth rates will remain the same. These considerations also demonstrate why a policy maker cannot engineer a larger short-run e\u21b5ect from trade by increasing adjustment costs. Increasing firing costs, for example, will certainly made factors more trapped, but it would itself signal impending liberalization and undue the desired innovative e\u21b5ect."}, {"section_title": "Patent Length vs. Adjustment Cost Horizon", "text": "Embedded within our analysis is an assumption that the model period, 10 years in our calibration, represents both monopoly protection period and the period over which factors are trapped. While this is not an unreasonable assumption given large empirical estimates of adjustment costs 27 -it is clearly very stark and worth exploring. Allowing asset and monopoly lengths to di\u21b5er would considerably complicate our analysis. However, we can consider the impacts qualitatively by examining the two potential cases arising from delinking the monopoly horizon (T M ) years, from the adjustment cost horizon (T A ). First, if T A > T M , then adjustment-cost induced periods of immobility are longer than monopoly protection. Trapped inputs would be used for the innovation of multiple cohorts of new varieties, which would likely not change the results qualitatively. In the alternative case of T A < T M , preexisting cohorts of on-patent varieties may exist within firms at the time of a trade shock. These preexisting monopoly varieties would o\u21b5er an alternative substitution possibility into which trapped resources could be directed instead of innovation. This would reduce the innovation boost induced by our trapped factors mechanism, but on the other hand it would also reduce the welfare loss from monopoly mark-ups. Hence, the net impact on welfare is ambiguous."}, {"section_title": "\uf036 \uf037 \uf034 \uf038 \uf034", "text": ""}, {"section_title": "28.8%", "text": "Post\u2212Shock Period Figure 3.5: A Shocked Industry Patents More Note: The solid black bar on the left displays the level of industry patenting in the period before a permanent and unanticipated trade liberalization from policy parameter to 0 > . Patent flows in the pre-shock period are normalized to equal 1000. The middle blue bar with downward sloping lines and right red bar with upward sloping lines represent the response of the unshocked and shocked industries, respectively, to the trade liberalization in an economy with trapped factors. The shocked industry loses 24.2% of its previously protected R goods production opportunities when these are converted to imported I goods from the South, and the no shock industry does not lose any unanticipated R goods to Southern competition.  Note: The figure displays the trapped-factors transition path in response to a permanent, unanticipated trade liberalization from policy parameter to 0 > , which is announced in period 0 to become e\u21b5ective in period 1. Since the plotted transition is computed in the trapped-factors economy, adjustment costs prevent the movement of resources outside of intermediate goods firms within the period of the shock. The solid black line is the transition path, the upper horizontal solid blue line is the post-shock balanced growth path, and the lower horizontal dashed red line is the pre-shock balanced growth path. For the two shadow value figures, shadow values are normalized to equal 100% in non-shock periods. The first transition path, in solid black, \"Baseline,\" replicates the trapped factors transition path displayed in Figure 4 above. A permanent and unanticipated trade liberalization from to 0 > is announced in period 0 to become e\u21b5ective in period 1. The second transition path in green with triangle symbols, \"No China,\" plots the trapped factors transition path, starting with the same initial conditions as \"Baseline,\" but instead considering a counterfactual increase of to a level between and 0 which matches post-liberalization imports to GDP ratios, assuming no growth in Chinese imports into the OECD. The upper horizontal solid blue line is the post-shock balanced growth path, and the lower horizontal dashed red line is the pre-shock balanced growth path.  Note: The figure displays the trapped-factors transition path in response to a permanent, unanticipated trade liberalization from policy parameter to 0 > , which is announced in period 0 to become e\u21b5ective in period 1. All plotted parametrizations of the model vary only the parameter indicated in the legend, starting from the baseline trapped factors calibration described in the text."}, {"section_title": "A.1.2 I/B/E/S Data", "text": "I downloaded I/B/E/S earnings forecasts and realized earnings data from WRDS in January 2014. My data construction requires files for (stock-split) adjusted detail history, unadjusted detail history, adjusted detail actuals, unadjusted detail actuals, currency headers, and identification headers. I made the following sample restrictions where applicable: \u2022 Nonmissing I/B/E/S permanent ticker ticker, earnings per share (EPS) value of forecast or realization value, nonmissing fiscal period end date pends or fpedats, nonmissing announcement date anndats, nonmissing analyst and estimator codes analys, estimator \u2022 Only US firms, as indicated in all files by usfirm = 1 \u2022 Only firms reporting in US dollars, with available primary/diluted reporting basis flag and historical CUSIP number, as indicated by the currency and identification header files by curr, pdi, cusip I/B/E/S makes available forecasts for earnings per share as well as realized \"Street\" earnings per share on two reporting bases: \"adjusted,\" in which the entire time series for a security is continuously adjusted for both stock splits and primary/dilution factors, as well as \"unadjusted,\" in which the originally reported forecasts and actuals are stored. Information is also available as \"summary\" or \"detail\" data, with summary files containing consensus forecasts for a firm as well as reported actuals, rounded to 2 digits (i.e. cents of earnings per share) and detail files containing the history of analyst forecast rounded to 4 digits. As Payne and Thomas (2003) note, the joint presence of stock splits and rounding in the adjusted summary files can lead to a severe loss of information as some earnings hits or misses are misclassified as zeros due to the ex-post adjustments made by I/B/E/S. Because accurate classification of earnings hits or misses is crucial to my research agenda, I base my analysis on the unadjusted detail files. However, this requires that all analyst forecasts from the unadjusted files be readjusted to the reporting basis as of the earnings announcement date, since reporting conventions for some securities may change in between a given analyst forecast and the earnings announcement. To readjust analyst forecasts to the same basis as announced unadjusted actuals requires the following process: Since they are on the same reporting basis, the analyst forecasts\u1ebc PS ijt , which have 4 digit precision, can be directly compared to the unadjusted actuals series EPS unadj jt . All forecast statistics are computed from these underlying series. Note that forecasts are made throughout the fiscal year for a given end of year financial release. Therefore, I must make a choice of horizon at which to compute earnings forecasts. In the baseline analysis, I consider forecasts made from a two-quarter horizon, i.e. from 91 to 180 days before the data release. Given a horizon, I construct, for a given firm and fiscal year combination (ticker and pends in I/B/E/S), a dataset with realized Street actuals as well as median analyst forecasts of earnings per share within that horizon window . More precisely, my forecast for a particular firm-fiscal year of earnings per share with horizon window"}, {"section_title": "A.1.3 Linking Compustat and I/B/E/S", "text": "Linking the Compustat and I/B/E/S data requires all observations from the underlying Compustat data, which are uniquely identified by a combination of permanent security identifier gvkey and datadate, with I/B/E/S observations of realized EPS and forecast EPS, which are uniquely identified by the permanent ticker ticker and forecast period end date variables pends and fpedats. Following the WRDS recommendation for linking in Moussawi (2006), these sets of identifiers can be linked through the CRSP dataset as follows. \u2022 Download the CRSP linking information with the permanent CRSP identifier permno together with historical CUSIP security identifiers cusip and first date of use date \u2022 For each observation in the Compustat dataset which, as a member of the Compustat/CRSP merged database already contains the CRSP identifying PERMNO value, use the date range in the CRSP linking table to assign an historical CUSIP value \u2022 Match a Compustat accounting observation to an I/B/E/S forecast information and realized earnings observation if they have identical CUSIP, PERMNO, as well as fiscal year end date (defined by year and month)"}, {"section_title": "A.1.4 Execucomp Data", "text": "Data from Execucomp from fiscal years 1992-2010 is integrated with the Compustat panel using the common firm identifier gvkey together with the date variable datadate. CEO compensation and compensation for other executives are considered, with observations requiring pceo equal to \"CEO\" for the CEO subsample. Total compensation for a given fiscal year is measured as the log total pay tdc2 from Execucomp."}, {"section_title": "A.1.5 CRSP Data", "text": "Stock returns data from the Center for Research in Security Prices (CRSP) from fiscal years 1983-2010 is integrated with the Compustat panel using the common firm identifier permno. Abnormal returns are equal to the cumulative abnormal return over a ten-day window up to the earnings release date for a particular firm fiscal year, market-adjusting daily returns using the S&P 500 index return and within-firm regressions. Note that in the discontinuity detection exercises, I wish to focus on behavior near the earnings forecast targets and to remove the influence of observations with an unusually high number of analyst forecast records (and potentially dramatic changes in firm news between forecast generation and earnings releases). Therefore, before estimating the regression discontinuities reported in the main text I further restrict the sample to remove observations with forecast errors greater than 1% of firm assets in absolute value or with higher than the 99.5 percentile of forecast frequency in the aggregation period. for estimation of the investment regression discontinuities in Section 2 of the paper.  This measure of growth rates as the di\u21b5erence relative to the average follows Davis and Haltiwanger (1992) and has the advantage of being bounded within [ 2,2]. Selection out of R&D with zero spending for a particular year results in a bounded rather than missing growth value. Following the construction of growth rates and real series from Compustat data, I use the linking process described above to I/B/E/S to obtain a dataset with merged accounting (from Compustat) and earnings forecast (from I/B/E/S) data. After the link, unscaled values of Street earnings (Street jt ) and forecasts (Street f t ) can be computed by multiplying either the primary or diluted share count as of the fiscal year end date from Compustat (cshpri or cshfd, respectively, with choice determined by I/B/E/S dilution flag pdi) by the unadjusted earnings per share actual value Note: *,**,*** denote 10, 5, 1% significance. The results reflect a block bootstrap procedure. Draws of data blocks were sampled with replacement from the distribution of firms, taking into account within-firm correlation as well as uncertainty surrounding variable demeaning by firm and year and the estimation of the regression discontinuity itself. The point estimates are the mean, and the standard errors are the standard deviation, over 250 bootstrap replications. The regression discontinuity estimation relies on local linear regressions and a triangular kernel, with bandwidth chosen via the optimal Imbens and Kalyanaraman (2011) approach. The estimates represent the mean predicted di\u21b5erences for firms just meeting earnings forecasts relative to firms just failing to meet forecasts. Earnings forecast errors are Street earnings minus median analyst forecasts from a 2-quarter horizon, scaled by firm assets as a percentage. Investment Rate is the percentage tangible annual investment rate. Intangibles growth is annual percent selling, general, and administrative expenditures growth. R&D growth is annual percent research and development expenditure growth. CEO Pay, Executive Pay are the log of total compensation for the CEO and several most highly compensated executives at a firm, respectively. Abnormal Returns are the cumulative abnormal returns for a firm in a ten-day window to the announcement date, market adjusting using the returns of the S&P 500. For returns analyst forecasts are drawn from a 1-quarter horizon. a Executive pay and stock returns are already in normalized form, and these values duplicate discontinuity estimates. Note: The moments sample is as described in the text above, with 32,597 firm-fiscal year observations in an unbalanced panel with Davis and Haltiwanger (1992) growth rate and forecast error transformations applied to real sales, real R&D expenditures, and Street forecast error series in a merged Compustat and I/B/E/S dataset from 1982-2010. For correspondence with model moments, I compute percentage forecast errors defined asf This measure of forecast error relative to the average absolute value of actual and forecasted Street earnings has several advantages. First,fe jt is bounded in [ 2,2] and can flexibly accommodate zeros in forecast or actual earnings series together with di\u21b5erences in sign. I construct estimation moments from data which includes the following series: sales growth, R&D growth, and percentage forecast errors. To avoid the influence of outliers, I further remove observations more extreme than the 0.5% or 99.5% quantiles for accounting series and observations exactly equal to 2 or 2 for percentage forecast error. As noted in the main text, I consider a total of six micro moments for estimation. The values of the moment covariance matrix, in raw form as utilized in the GMM estimation procedure itself, are reported in Table A.4. I now turn to the details of the overidentified GMM structural estimation of\u2713 in the baseline model based on the vector of moments m(X). Recall that the aggregate growth rate is used as a targeted moment in the estimation, together with the microlevel covariance matrix of sales growth, R&D growth, and forecast errors. The growth rate is the average annual growth rate of US per capita GDP from 1961-2010, FRED series USARGDPC. Under an assumption of independence between micro and macro data samples, the covariance matrix of the joint set of moments m(X) is computed in a two-stage process. First, I compute the variance of the aggregate growth rate g,\u02c6 2 g , taking into account arbitrary stationary time series correlation in my sample of length T using the stationary bootstrap of Politis and Romano (1994). Second, note that the vector of micro moments can be written as a smooth function of unscaled first and second moments, say\u03bc, of sales growth, R&D growth, and forecast errors, so that the estimated covariance matrix of the micro moments,V , is immediately implied by an estimate of the covariance matrix of the raw moments, \u2326, and the Delta method. I compute\u2326 with asymptotics in the number of firms N allowing for arbitrary clustering within firms. If each firm j has T j observations in the sample and the average number of observations is\u2327 = where x jt is the stacked vector of levels and cross-products of R&D growth, sales growth, and forecast errors for firm j in period t. Under an assumption that\u02c6 = q T N ! asymptotically as N ! 1, which adjusts for relative sample sizes, together with the assumption of independence between the micro and macro samples, I can write the joint asymptotic covariance matrix of the vector m(X) of the aggregate growth rate and micro moments together as Given the asymptotic distribution of the moments used in the estimation of the underlying structural parameters \u2713, the definition of\u2713 as the minimizer of the GMM objective function and standard GMM arguments yield the result that where the covariance matrix of the estimated parameters is given by Here, the weighting matrix W is chosen so that the GMM objective is equal to the sum of the squared percentage deviations of model from data moments, with one modification. The aggregate growth rate, of crucial importance economically given my endogenous growth framework, is assigned 10 times more weight than the micro moments. Estimates of\u2713 are computed using particle swarm optimization, a robust and standard global stochastic optimization routine. Given\u2713 and W in hand, an estimate of the Jacobian @m(\u2713) @\u2713 0 of model moments with respect to parameters is computed using straightforward numerical di\u21b5erentiation averaging over relative step sizes of 0.75%, 1%, and 1.25%."}, {"section_title": "A.2 Theory", "text": ""}, {"section_title": "A.2.1 Model Equilibrium", "text": "An equilibrium of the model consists of household consumption and savings policies , and lump-sum transfers t , together with prices R t+1 , {P jt }, and w t such that the following conditions hold."}, {"section_title": "Household Optimizes", "text": "Taking as given wages w t , share prices and dividends {P jt } j , {D jt } j , and lumpsum transfers T HH t , the values for household consumption C t , one-period risk free bond savings B t+1 , and share purchases in intermediate goods firms {S jt } maximize household utility: Final Goods Sector Optimizes Taking as given wages w t and intermediate input prices p jt , the competitive and constant returns to scale final goods sector labor and intermediate input demands"}, {"section_title": "Managers Optimize", "text": "Taking as given an exogenous endowment of consumption goodsC M Q t , exogenous persistent and transitory profitability shocks a jt 1 , \" jt 1 , long-term quality level Q jt 1 , previous manager R&D and paper manipulation choices z jt 1 , m jt 1 , next-period earnings forecasts \u21e7 f jt , and the previous manager's take-it-or-leave it price M jt 1 for the managerial franchise, each manager j 2 [0, 1] born in period t 1 must make the end of period t 1 choice r jt 1 2 {0, 1} to reject (r jt 1 = 1) or accept (r jt 1 = 0) the o\u21b5er of the managerial franchise when seeking to maximize period t expected utility, i.e. Conditional upon accepting the previous manager's franchise o\u21b5er (r jt 1 = 0), in their second period of life in period t each manager j 2 [0, 1] born in period t 1 must make R&D investment, paper manipulation, monopoly pricing, and managerial franchise pricing o\u21b5er choices z jt , m jt , p jt , and M jt . These decisions take as given the realization of exogenous persistent and transitory profitability shock a jt , \" jt , long-term quality Q jt , current profit forecast \u21e7 f jt , as well as the optimal choice r jt of acceptance or rejection of the managerial franchise by the next-period manager born in period t. The manager seeks to maximize their period t utility, i.e. they solve the problem From the perspective of the manager, perceived miss costs are a combination \u21e0 = \u21e0 manager +\u2713 d \u21e0 firm +(1 \u2713 d )\u21e0 pay , and dividends net of manager clawback compensation and firm-borne miss costs are"}, {"section_title": "Intermediate Goods Firm Values", "text": "Given exogenous persistent and transitory profitability shocks a jt , \" jt , long-term quality level Q jt , and analyst forecasts \u21e7 f jt , as well as manager-determined intermediate goods firm R&D investments z jt , monopoly prices p jt , shirking decisions s jt , and accounting manipulation choices m jt , the value of intermediate goods firms j at time t is given by the present-discounted value of firm dividends"}, {"section_title": "Analyst Sector Optimizes", "text": "Taking as given normalized Street earnings last period jt 1 , an outside analyst sector forecasts normalized Street earnings \u21e1 jt today, where the forecast jt must minimize analyst loss as follows \u21e1 f jt = arg min Labor and Asset Markets Clear (1 r jt 1 )dj (Borrowing for Franchise Purchases Only) Government Budget Balances Managers Consume Their Endowments Resource Constraint and Aggregation Conditions are Satisfied"}, {"section_title": "A.2.2 Normalization and Recursive Firm Problem", "text": "Consider a stationary balanced growth bath equilibrium where average quality in the economy Q t = R Q jt dj grows at a constant rate g and there exists an invariant distribution \u00b5(a jt , \" jt , q jt , \u21e1 f jt ) of intermediate goods firm manager state variables with q jt = Q jt Qt and \u21e1 f jt defined above. Then, immediately, all of the aggregates in the economy grow at the rate g as well, since Therefore, the household intertemporal Euler equation for savings in one-period bonds yields the standard result of a constant interest rate R t+1 = 1 \u21e2 (1 + g) = R. Note, as will be shown below, that manager value maximization solves The above trivially omits the monopoly pricing decision p jt = 1 from the firm problem. Also, if 1 then 1+g , the manager flow return written in full in the equilibrium above, is homogenous in Q jt and hence stationary since Qt is stationary. Therefore, the intermediate goods firm manager's objective exists in stationary, normalized form. Manager policies can be obtained as the result of maximization of manager flow returns discounted by the market interest rate, the objective written and analyzed above. To justify this, first note that manager j born in time t 1 will accept the o\u21b5er of a managerial franchise (i.e. set r jt 1 = 0) for the following period t at price M jt 1 if and only if Via backward induction, since M jt 1 is a take-it-or-leave it price from the previous manager and since the previous manager's utility is strictly increasing in M jt , it must always be the case that market clearing for managerial franchises pins down the price M jt 1 : Repeated forward substitution into the expression for manager consumption in period t therefore implies that in period t the manager born in t 1 maximizes the present discounted stream of manager utilities from period t onwards, exactly the objective stated in the text. Note that because they are exogenous to the manager's linear payo\u21b5s, the manager consumption endowmentsC M Q t and transfers T M t do not impact manager policies or intermediate goods firm values. However, both terms are useful technically. A high enough value ofC M ensures that potentially negative dividends and clawbacks \u21e0 pay do not result in negative manager consumption levels. Meanwhile an appropriate and maintained choice of Q jt dj ensures that manager consumption on aggregate is equal to exogenous endowment levelsC M Q t exactly. Hence, household consumption can be backed out via the resource constraint, i.e. which is the expression used to argue for R t = R above. Also, trivially note that the analyst problem yields \u21e1 f jt = E \u00b5 (\u21e1 jt |\u21e1 jt 1 ) given the mean squared error loss function for analysts. Omitting t and j subscripts for clarity, using 0 to denote future periods, and writing the manager problem recursively yields The stationary, recursive, normalized intermediate goods firm manager problem above features discounting at rate (1 + g)/R rather than 1/R, and sees \"depreciation\" of normalized relative long-term quality levels q by the rate g each period. The manager problem also allows for the influence of corporate taxes, through the \u2327 c marginal rate, on firm decisions. 3 In this form, the problem can be solved using standard numerical dynamic programming techniques, as discussed in the Numerical Appendix C below. Also, once optimal policies are obtained, a similar recursive structure obtains for intermediate goods firm values themselves through direct substitution of manager optimal policies. Now I explicitly define the notion of stationarity which must be satisfied by the distribution of normalized state variables \u00b5(a, \", q, \u21e1 f ). The distribution must be invariant to forward iteration on both the exogenous driving profitability processes a and \" as well as the endogenous forecast and long-term quality transitions. Let z(a, \", q, \u21e1 f ), m(a, \", q, \u21e1 f ), and \u21e1(a, \", q, \u21e1 f ) be the optimal R&D policy, optimal accounting manipulation policy, and induced normalized Street earnings functions, and let f a (a 0 |a) and f \" (\") be the transition and density functions for the exogenous processes. Formally, the stationary distribution \u00b5 satisfies the following condition: The aggregation condition which must further be satisfied on a stationary balanced growth path, which guarantees that the aggregate growth rate of long-term quality is generated by firm policies and the stationary distribution \u00b5, repeats here from the main text. The first term represents quality growth generated by quality ladder innovation arrivals, the second term represents quality growth from lagging-quality firms away from the di\u21b5usion bound !, and the final term represents quality growth from lagging quality firms at the di\u21b5usion boundary. Note that the model used for cost estimation in Section 4 imposes e = s = s jt = 0 and \u21e0 = \u21e0 manager , i.e. the model assumes away agency conflicts and mechanical resource costs of earnings misses, while the shirking model in Section 5 assumes e = 0 and the empire building case in Section 5 assumes s = s jt = 0. Both models of Section 5 assume \u21e0 = (1 \u2713 d )\u21e0 pay , i.e. that the costs of earnings misses represent explicit manager compensation policies."}, {"section_title": "A.2.3 Welfare and Firm Value Change Formulas", "text": "The total consumption equivalent welfare gains from the removal of earnings targets, i.e. moving from \u21e0 > 0 ! \u21e0 = 0 comparing balanced growth paths only, can be written as 100 where satisfies the following equation: All \"targets\" subscripts refer to cases with \u21e0 > 0 and \"notargets\" subscripts refer to cases with \u21e0 = 0. Trivially, this yields the following formula and decomposition of the welfare gains from removal of the earnings target friction: The above welfare calculations are general equilibrium, in that they take into account all aggregate changes in growth rates, forecasting systems, and the stationary distribution of the economy when targets are removed. By contrast, the partial equilibrium change in firm value resulting from the removal of earnings targets is computed leaving these quantities unchanged, since from the perspective of the firm such aggregates are fixed. The resulting formula for the average change in firm value used in the text is Note that the text reports in the cost estimation of Section 4 a conservative version of the measures above which omit the direct e\u21b5ect of the removal of earnings targets costs on the aggregate consumption level and firm value by assuming costs are private to the manager, \u21e0 = \u21e0 manager . Therefore, there is no mechanical e\u21b5ects of the target removal on aggregate household consumption or firm dividends through a resource channel. By contrast Section 5, which assumes that miss costs are based on manager compensation, prevents a mechanical impact of miss costs on aggregate consumption through the lump-sum transfers away from managers but does allow for clawback to increase firm flow dividends for valuation purposes."}, {"section_title": "A.2.4 Adding Measurement Error", "text": "The main text shows results for a version of the baseline model with \"target measurement error\" \u232b jt for firms. \u232b jt is a transitory white noise disturbance with variance 2 \u232b for firm j in period t which is unknown at the time manager policies are determined but shifts the realized profits for firms and hence the relevant earnings target. More precisely, this involves replacement of the standard intermediate goods firm manager optimization problem from the equilibrium definition above with one that incorporates \u232b jt : In practice, since \u232b isn't a state variable for the firm at the time policies are determined, the recursive normalized problem can be modified from the statement above to the following form:"}, {"section_title": "\u21e2\u2713", "text": "(1 \u2327 c ) ( (a + \")qL zq) , with prob. 1 (z) Note that since the measurement error version is only discussed in the context of the cost estimation model with e = s = s = 0, I omit those terms from the dividend flows above and write the earnings miss costs as\u21e0, which is simply equal to \u21e0/\u2713 d in previous notation. Note: The table above describes some practical numerical choices made to solve the normalized recursive model described in the Theory Appendix B. The model is solved with discretization, and the grid boundaries as well as densities are displayed above together with tolerances for the various fixed-points required by the model and described in the numerical solution overview. Note: The entries above represent percent di\u21b5erences between the counterfactual \u21e0 = 0 and estimated benchmark\u21e0 cases. The moments are computed from the stationary distributions \u00b5 of the respective economies.  Note: All statistics are computed using the stationary distribution \u00b5 of the Baseline model, based on a forecast system of \u21e1 f = \u2318 0 + \u2318 1 \u21e1 1 . RMSE is the root mean squared error of a given forecasting rule, i.e. for system i, , where \u21e1 f i is the forecast from system i and \u21e1 is model Street earnings from the Baseline. Each column reports the scaled value of RMSE i /RMSE 1 , where RMSE 1 is the RMSE implied by the forecast rule with only a constant or mean prediction. Movement down rows within each column tracks forecast accuracy improvement when sequentially adding terms to the mean only forecast rule. the model. Also, Figure A.3 displays the ergodic or stationary marginal distributions of model state and policy variables in the Baseline and No Targets economies. Note: Standard errors are clustered at the firm level. The baseline regression discontinuity estimation relies on local linear regressions and a triangular kernel, with bandwidth chosen via the optimal Imbens and Kalyanaraman (2011) approach. The figures above plot, for each outcome variable, regression discontinuity estimates and 90% confidence intervals for a range from one half to twice the optimal bandwidth amount. The optimal bandwidth choice is indicated by red vertical lines. The estimates represent the mean predicted di\u21b5erences for firms just meeting earnings forecasts relative to firms just failing to meet forecasts. Earnings forecast errors are Street earnings minus median analyst forecasts from a 2-quarter horizon, scaled by firm assets as a percentage. Investment Rate is the percentage tangible annual investment rate. Intangibles growth is annual percent selling, general, and administrative expenditures growth. R&D growth is annual percent research and development expenditure growth. CEO Pay, Executive Pay are the log of total compensation for the CEO and several most highly compensated executives at a firm, respectively. Abnormal Returns are the cumulative abnormal returns for a firm in a ten-day window to the announcement date, market adjusting using the returns of the S&P 500. For returns analyst forecasts are drawn from a 1-quarter horizon.    Note: The figure above represents the conditional mean of profitability a for firms missing their forecasts (\u21e1 < \u21e1 f ), and firms meeting their forecasts (\u21e1 \u21e1 f ), computed from the stationary distribution of the balanced growth path associated with both the estimated earnings miss cost\u21e0 (in red) and \u21e0 = 0 (in black). The di\u21b5erence in mean profitability from missing is 193% in the estimated baseline, compared to 25% for \u21e0 = 0. Note: The figure plots Gentzkow and Shapiro (2014) sensitivity estimates of each of the estimated model parameters to the seven moments used in GMM estimation of the baseline model. The sensitivity estimates represent the coe cients of a theoretical regression of the estimated parameters on data moments over their joint asymptotic distribution. For ease of reference, the sensitivity parameters are reported as elasticities of the parameter to the relevant data moment. The label g represents the aggregate growth rate, while microeconomic moment labels V and C are variance and covariance, respectively, for sales growth s, R&D growth z, and forecast errors FE.  where s = 0.025. The top left panel plots the average shirking level 100E \u00b5 s with earnings targets, the top right panel plots the percent di\u21b5erence in shirking from target removal, the bottom left panel plots the average PE percent change in firm value from target removal, and the bottom right panel plots the GE total consumption equivalent percent change in social welfare from target removal. Numerical comparative statics have been smoothed using a polynomial approximation. where we have that policies for labor have a closed-form and capital policies follow a threshold rule, given optimal policies k 0 \u21e4 (z, k) from V A (z, k): and that \u00b5(z, k) is the ergodic distribution implied by a discretization as in Young (2010) together with application of the capital policies, adjustment cost thresholds, and idiosyncratic productivity transitions. Once the equilibrium is obtained, it is trivial to compute aggregate capital as K = R kd\u00b5(z, k)."}, {"section_title": "B.1.2 Krusell Smith (KS)", "text": "The KS solution method used in the paper, following Khan and Thomas (2008) assumes that the set of approximating moments m in the aggregate state space of the model is equal to the mean or aggregate capital level K. Further, the solution method discretizes idiosyncratic and aggregate productivity processes following Tauchen (1986). Conditional upon a discretized level of aggregate productivity A, the forecast rules for price and next period's aggregate capital level take a loglinear form: Recall that from the household problem this yields a forecast wage level\u0175 = p . Given these choices, the solution algorithm works as follows. First, guess an initial forecast rule system\u02c6 (1) = (\u21b5 K ). Then, before the solution iteration takes place, draw a large number T of exogenous aggregate productivity values based on the discretized Markov chain for A. Finally, in iteration s = 1, 2, ... do the following 1. Given forecast rule system\u02c6 (s) , solve the following system of equations via projection on some grid of states (z, k; A, K) (s) (z, k; A, K). Note that expectations are computed via summation over the appropriate portion of the discretized transition matrices \u21e7 z and \u21e7 A for idiosyncratic and aggregate productivity, respectively, and that the solution to the Bellman equations can be achieved with policy iteration, Howard acceleration of the Bellman equations, and continuous univariate optimization techniques in next period's capital level k 0 , such as Brent optimization or golden section search. 2. Given the value function solution from step 1, simulate the model. Simulation follows the nonstochastic approach of Young (2010), and requires initialization of the cross-sectional distribution of capital and productivity \u00b5(z, k) over a dense, discrete grid of (z i , k j ) with n d points for discretized capital. In each period t, market-clearing prices p t must be determined without reference to the forecast price levelp (s) (which appear only through embedded expectations in the continuation value). The price clearing algorithm, for a given guessed price valuep requires calculation of the excess demand or error function, which is done as follows: \u2022 Compute new capital and adjustment cuto\u21b5 thresholds at each point on the dense simulation grid (z i , k j ) through where the optimization over labor is a static problem which yields an analytical reduced-form for the right hand side expressions which can be optimized in capital k 0 alone. Let optimal labor and capital policies conditional upon adjustment be given by n \u21e4 (z i , k j ) and k 0 \u21e4 (z i , k j ). We have suppressed dependence on the aggregate states (A, K) for the moment, which is constant through the market-clearing process. \u2022 Compute implied output, investment, consumption, and labor from \u2022 Define the excess demand function or clearing error as 1 p C \u2022 If excess demand is suitably small, the clearing algorithm stops. If not, repeat with an updated value ofp, using any preferred method such as bisection. \u2022 With market clearing price p t in hand (as well as Y t , I t , C t , N t ), update the discretized distribution \u00b5 t+1 (z i , k j ) for the next period. Based on Young (2010), distributional transitions are calculated through linear interpolation of capital policies and use of the idiosyncratic productivity transition matrix where abovek j = max((1 )k j , k 1 ) is the non-adjustment next-period capital stock. Also, with \u00b5 t+1 in hand, next period's simulated capital stock is computable as \u2022 Continue to simulate period t + 1. 3. With the simulation from periods 1 to T completed given forecast rules\u02c6 (s) , discard some number T erg of periods as initialization, and update the forecast rules based on OLS regressions. Run the loglinear OLS regressions of realized prices p t on aggregate capital stocks K t and realized next-period capital stocks K t+1 on current stocks K t , where new coe cients (\u21b5 p ,\u02c6 p ,\u21b5 K ,\u02c6 K ) are obtained separately for each level of aggregate productivity, and compute forecast rule errors as the maximum absolute di\u21b5erence between assumed coe cients\u02c6 (s) and the new estimated values. If the coe cients have converged to an acceptable tolerance, the model is solved. If not, then update\u02c6 (s+1) using dampened fixedpoint iteration (i.e. set\u02c6 (s+1) equal to a weighted average of\u02c6 (s) and the newly estimated coe cients), then continue to solution iteration s + 1."}, {"section_title": "B.1.3 Parametrized Distributions (PARAM)", "text": "This is a discussion of the computational algorithm due to Algan et al. (2008Algan et al. ( , 2010a (henceforth AADH) which relies upon higher-order reference moments, as well as assumed functional forms for the cross-sectional distribution of idiosyncratic capital and productivity \u00b5(z, k) in the solution of the model. Just as in the KS algorithm, we first must discretize the aggregate and idiosyncratic productivity processes following Tauchen (1986). Let the number of idiosyncratic (aggregate) productivity points be given by n z (n A ). Then, prior to the solution of the model, we first determine a set of aggregate moments m to be included in the aggregate state space. Here, this will be the singleton of aggregate capital, the cross-sectional mean K. Together with aggregate productivity, (A, K) therefore forms the aggregate state. Then, determine a set of reference moments m ref used to help pin down the shape of the cross-sectional distribution of idiosyncratic capital and productivity. Here, this will be the first n M n z centered moments of the capital distribution conditional upon each value of idiosyncratic productivity. Also, compute the exogenous ergodic distribution of idiosyncratic productivity\u21e1 z for future use. The reference moments are needed in this algorithm because, together with the aggregate capital state, they jointly determine the coe cients of the flexible exponential function form for the approximation to \u00b5(z i , k) at discretized levels of z i : As discussed in AADH, a well-behaved convex minimization problem which is laid out below yields a mapping between the n z n M moments of the cross-sectional distribution and coe cient vectors \u21e2 z i , i = 1, ..., n z , because the first-order conditions of this problem are identical to the moment conditions setting distributional and reference moments equal to each other. We assume when manipulating or integrating this distribution that idiosyncratic capital lies in the bounds [k,k]. To solve the model, start with an initial constant guess for these reference moments, from the distribution in the steady-state model with no aggregate uncertainty. If desired, simulation can later provide a set of reference moments which vary with the level of aggregate productivity, or these steady-state reference moments themselves can be held fixed. Although we will outline both the repeated simulation and simulation-free approaches below, the results reported in the paper hold the reference moments at their steady-state levels. Finally, another option is also to use a regression forecast system from (A, K) ! m ref , although following AADH we forego that approach here. 1. Iterate over the reference moments or the reference moment forecasting system, which yields a constant or variable mapping (A, K) ! m ref . (a) Loop over the aggregate states (A, K), on the discretized grid for A and some projection grid for K. \u2022 For each (A, K) and implied m ref (A, K), choose distributional coecients noting that the constants \u21e2 z i a,K,0 are irrelevant for the minimization and are simply chosen to ensure integration to 1. This is a well-behaved convex minimization problem and the integral can be computed via any standard quadrature rule. Here and throughout, integrals are computed via Simpson's rule. It is important to note that any rule with fixed nodes and weights is preferable to adaptive methods because fixed rules contribute to stability in the minimization problem above. The minimization is implemented in this paper using a robust and quick quasi-Newton routine with symmetric rank-one (SR1) updating of the approximation to the inverse Hessian. The method used in this paper is dampened fixed-point iteration in the pair p, K 0 . -For each value of p, K 0 , evaluate on the discretized grid for z and some spline projection grid for k the following equations: -Then, compute the errors to the system of equations in p and K 0 given by where above the calculation requires the ability to compute n(z i , k; A, K), k 0 (z i , k; A, K), and \u21e0 \u21e4 (z i , k; A, K) at a set of Simpson integration nodes in k. These can be computed by recalculation of the right hand sides of the Bellman equations above at the quadrature nodes in k, and this task is simplified by noting that capital policies depend only on idiosyncratic productivity and that the labor demand optimization is static and can be obtained in closed-form. Also, the densities P (k, \u21e2 z i A,K ) must be evaluated using moments which reflect both the reference moments for higher order terms but also the current mean level of aggregate capital. Therefore, the first moments of capital used in the construction of P (k, \u21e2 z i A,K ) are linearly shifted to deliver consistency with K as the current aggregate shape. \u2022 Error on the value function iteration is given by ||V (s) ||, which can be defined as desired. The solution used in this paper is based on the max absolute percentage di\u21b5erence. If the value function has converged, exit the value function iteration process. If not, go iteration s + 1. (c) At this point, a decision must be made. If the reference moments used in the solution are to be held fixed at their steady-state values, the model is now solved. However, if the reference moments are to be updated with simulation, then simulation must be performed as part of the solution itself. In either case, simulation proceeds as follows, noting that a set of exogenous productivity draws A t for T periods have been made outside of the solution loop. Given converged values V , simulate the economy for a large number of periods, in each period imposing market clearing to obtain p, K 0 . For each period, do the following, taking advantage of the functional forms assumed for the cross-sectional density. \u2022 Start period t with a set of coe cients \u21e2 z i t and moments m t (the first n z n M moments of the idiosyncratic capital density conditional upon idiosyncratic productivity) which jointly pin down the cross-sectional density in (z, K) for the period. Then using the value function from above and the same fixed point iteration approach, compute the equilibrium p, K 0 consistent with both the value function and the crosssectional distributions for the current period. \u2022 Compute the value of all reference moments for the next period t + 1. These are higher-order centered moments of the cross-sectional distribution of capital next period, and can be computed directly via quadrature, given the policies and cross-sectional distribution coecients of period t. \u2022 Now that the reference moments are set for period t+1, along with the aggregate capital state, compute the coe cients of the cross-sectional distribution associated with period t + 1 using the exact same minimization step as above. (d) After simulation is completed for all T periods, and a certain number T erg of initial periods are discarded, you have two options. If the reference moments are held constant at their steady-state values, you simply have an unconditional simulation of the model. If a fixed-point on the reference moments is desired, then update the reference moments in the outer loop now. The appropriate method depends on your assumptions for the reference moments. If you have assumed one unconditional constant set of reference moments not varying with aggregates, compute the unconditional average of each reference moment over the simulation. If you have assumed reference moments which vary with aggregate productivity, then compute the conditional average of each reference moment on the value of aggregate productivity. If you have assumed a reference moment forecasting system, update this system with OLS. Return to step (a) if the reference moments have not converged based on some criterion, say max absolute di\u21b5erence."}, {"section_title": "B.1.4 Explicit Aggregation (XPA)", "text": "First discretize the aggregate productivity process A and solve the steady-state model for each aggregate productivity value A k , k = 1, ..., n A . For each level, save values of equilibrium price and capital stocks p SS (A k ), K SS (A k ). Also, discretize the idiosyncratic productivity process and store the exogenous ergodic distribution\u21e1 z of discretized idiosyncratic productivity for future use. Set up the aggregate state space to include (A, K) and posit forecast rules for market-clearing prices and next-period capital identical to the KS case. Then, solve the model exactly as in the case of the KS algorithm, with two modifications: (2') Replace KS simulation step (2) with an \"explicit aggregation\" step. In particular, loop over aggregate states (A, K), where A varies over its discretized grid and K varies over the same projection grid used to compute the value functions. steady-state model prices and capital stocks do incorporate cross-sectional integration over a distribution of idiosyncratic capital presumably similar to the distributions within the model with aggregate uncertainty. The modification by the term x Bias (A) requires that the estimated forecast system be able to exactly reproduce as a fixed point the steady-state prices and aggregate capital stocks p SS (A) and K SS (A), conditional upon aggregate productivity. Note also that after the model is solved, simulation is completed exactly as in the KS algorithm, using the Young (2010) nonstochastic or histogram-based approach, and requiring market-clearing in each period with integration over the full crosssectional distribution of idiosyncratic capital."}, {"section_title": "B.1.5 Projection Plus Perturbation (REITER)", "text": "The REITER solution method is based on three steps, and provides a perturbation approximation to the full rational expectations equilibrium. The first step is to solve the steady-state version of the model, with no aggregate uncertainty and aggregate productivity held fixed at a value of A = 1. The steady-state solution is identical to the one used, for example, as an input into the PARAM solution. The second step is to set up a system of nonlinear equations defining the model's equilibrium, which is covered in the first subsection below. The final step is to linearize and solve the system using standard numerical di\u21b5erentiation and solution techniques, covered in the second subsection below."}, {"section_title": "Nonlinear System of Equations in the Discretized Model", "text": "We first establish a grid of n z idiosyncratic productivity points and a Markov transition matrix \u21e7 zij = P(z t+1 = z j |z t = z i ) following Tauchen (1986). Then, we establish a grid of n k idiosyncratic capital stock nodes k i , which will function as knot points for cubic spline interpolation of the value functions. Finally, a denser simulation grid of n d idiosyncratic points will be used to store the distribution. We then have the following equations, which are to be linearized around the zero-aggregate uncertainty steady-state of the model. Together, the above system represents a total of n s = 3n z n k + n z n d + 5 equations in the n s \u21e5 1 vector of endogenous variables together with the exogenous shock process \" t . We write this nonlinear rational expectations system corresponding to the discretized model as F (X t , X t 1 , \u2318 t , \" t ) = 0, where F (\u2022) is the left hand side minus the right hand side of each of the n s equations above. Note that we know this equation is satisfied exactly at the steady-state value of X t = X t 1 = X SS and \u2318 t = 0, \" t = 0 corresponding to no aggregate uncertainty and A t = 1. The vectors X t are n s \u21e5 1 while the vector \u2318 t is n \u2318 \u21e5 1, where n \u2318 = 2n z n k . A few practical comments are in order. First, in general the approximation nodes used for interpolation of the value function in k will be di\u21b5erent (and less dense) than the discrete values of k used to store the cross-sectional distribution \u00b5 t above. The value \" t is the exogenous shock to aggregate productivity. The vector \u2318 t is the stacked set of expectational errors using Sims (2002) notation which must be applied to the expectations in the Bellman equations above, and these expectational errors depend upon aggregates only as the idiosyncratic uncertainty reflected in the discretization of idiosyncratic productivity is already taken into account through the summation with respect to the transition matrix \u21e7 z . Also, note that when the system above is evaluated, policies must be reoptimized to obtain the values of k 0 (z i , k j ) used in the distributional transitions above. 1 In practice, this reoptimization to compute F is not particularly burdensome because the optimization must only be performed once, and because the capital policies conditional upon adjustment can be shown to depend only on the value of idiosyncratic productivity z. The functions f , y, and n above represent the reduced-form expressions for revenue net of labor costs, output, and labor input at firms, conditional upon the analytic solution to the static labor optimization problem."}, {"section_title": "Linearizing the System", "text": "We then numerically di\u21b5erentiate F with respect to each of its arguments, at the steady-state, to obtain the system of n s equations below , and F 4 = @F @\"t (n s \u21e5 1) are the approximated derivative matrices evaluated at the steady-state of the model. This system of equations can be solved using a large variety of solution methods for linear rational expectations models, including those in Christiano (2002) or Sims (2002). In the calculations performed in this paper, the Sims (2002) algorithm as applied by the gensys software in MATLAB available on Chris Sim's website is used to solve the linear model, and the numerical di\u21b5erentiation is carried out by forward di\u21b5erentiation from the steady-state with relative step size 1e 6. For concreteness, note that the linear solution to the model is simply a set of coe cient matrices A (n s \u21e5 n s ) and B (n s \u21e5 1) such that locally around the steadystate the model satisfies Immediately, the traditional local impulse responses IRF t , t = 1, ..., T IRF (n s \u21e5 1 vectors) to a shock to aggregate productivity can be computed as and the model can be simulated by drawing N (0, 2 A ) shocks and substituting directly into the solution equation above. At this point, it's important to note that by numerically di\u21b5erentiating the system F we are assuming that although nonlinearity and threshold decision rules exist and are preserved at the microeconomic level, the dependence of these micro-level decisions, as embedded in the value functions and distributional transition weights above, on aggregate shocks to productivity is smooth. In the context of the Khan and Thomas (2008) model with stochastic adjustment costs, the resultingly smooth value functions and policies make such an assumption sensible. However, models with discrete choices and fixed, nonstochastic adjustment costs, such as those in Bloom et al. (2012), which can not be expected to see policies vary smoothly with aggregate shocks would not allow for a reasonable application of the REITER approach. Finally, for the levels of discreteness chosen in this paper's solution, the linear system has approximately 900 equations, but for a denser discretization model reduction techniques can be applied which still allow for the model's solution. These reduction techniques are laid out in Reiter (2010a) and recently applied in McKay and Reis (2013)."}, {"section_title": "B.2 General Numerical Choices and Additional Accuracy Statistics", "text": "To complement the general discussion of each solution algorithm, it is also useful to list some practical choices made in the projection, optimization, and discretization of the model, with further information available in Table B.1. \u2022 The mean or aggregate level of capital K is used as the approximating moment m for the cross-sectional distribution \u00b5(z, k) for the three solution methods requiring such an approximation (KS, PARAM, and XPA). \u2022 In all solution techniques, idiosyncratic and aggregate productivity processes z and A are discretized according to Tauchen (1986) and along grids spanning two standard deviations of the unconditional process standard deviation around the process steady-state. 2 \u2022 Value functions are approximated as cubic splines in idiosyncratic capital k with a natural spline endpoint condition, and using linear interpolation in aggregate capital K. For the KS, XPA, and no-aggregate uncertainty models, the firm problem is solved using policy iteration with Howard acceleration of the value function, while in the PARAM method value function iteration is performed. \u2022 The idiosyncratic capital policies within the Bellman equations are determined using Brent optimization (KS, XPA, PARAM, and no aggregate uncertainty), or golden section search (REITER di\u21b5erentiation). \u2022 Price-clearing is performed during simulation using bisection (XPA, no aggregate uncertainty) or hybrid bisection/inverse quadratic interpolation (KS). Within the model solution step, joint search for clearing price and next period aggregate capital is performed using dampened fixed point iteration (PARAM). \u2022 Minimization of the density-based objective determining distributional coecients during simulation and solution of the PARAM model is performed using  K) to realized next-period capital K 0 (the first three columns) and to market-clearing prices p (the final three columns). The first five rows represent the root mean squared error, in 100 times logs, between the forecasts of one-period ahead capital and current price and realized values, conditional upon the current discretized value of aggregate productivity A 1 , ..., A 5 . The final five rows represent the R 2 of each forecast regression, conditional upon the current value of aggregate productivity. The exogenous aggregate productivity process reflects a Markov chain discretized using the Tauchen (1986) procedure and is held constant across solution methods during the simulation. To achieve this, an identical simulated discretized productivity process is input directly into the KS, PARAM, and XPA solutions, while a series of continuous aggregate shocks exactly replicating the discretized productivity process is input into the REITER solution. For each method, the full 2000 period simulation for each solution method begins after 500 periods, with an initial burn-in period discarded to avoid the influence of initial conditions on the simulated aggregates. The exogenous productivity simulation used for calculation of the accuracy statistics are distinct from the simulation used within the model-solution step of any algorithm. a standard quasi-Newton algorithm with symmetric rank-one (SR1) updates to the inverse Hessian approximation. \u2022 Within the PARAM solution, integration over the cross-sectional densities is performed using standard Simpson quadrature rules."}, {"section_title": "B.3 Simulated Impulse Responses with Nonlinear Models", "text": "As noted in the main text, nonlinear impulse response analysis must take into account variation in the initial conditions and size of the imposed shocks, and following Koop et al. (1996) we take the following approach to compute the average conditional response to a one-standard deviation aggregate productivity shock: 1. Fix a large number N of simulations, a per-simulation length T IRF , and a shockperiod T shock . 2. Independently draw exogenous u it \u21e0 U (0, 1) shocks for each period t of each simulation i, as well as one \"shock occurrence\" s i \u21e0 U (0, 1) draw for each simulation i. 3. For each simulation i, based on comparison of the shocks u it with the entries of the cumulated transition matrix of discretized aggregate productivity, create two versions of simulated aggregate productivity series A shock it and A noshocks it , which are identical until the shock-period for t < T shock . In the shock period T shock for each simulation, compare the shock occurrence draw s i for simulation i with the cuto\u21b5 thresholdss defined below. If s i \uf8ffs, set A shock iT shock = A nA , where n A is the highest level of the discretized aggregate productivity process, and then allow A shock it to transit normally based on u it for t > T shock . If s i >s, allow A shock it to transit normally based on u it for all periods t T shock . In either case, allow A noshocks it to transit normally based on u it for all t T shock . 4. Simulate any aggregate series of interest X twice, using both A shock it and A noshock it exogenous processes. The impulse responsesx t of series X in period t to an aggregate productivity shock in period T shock is given b\u0177 To obtain an average percentage innovation in aggregate productivity which equals A exactly, we choose the shock thresholds to solve where\u21e1 A is the ergodic distribution of the discretized aggregate productivity process A. As noted in Appendix B, to compute the impulse responses plotted in the main text, we set T IRF = 50, T shock = 25, and N = 2000, and we hold exogenous draws u it , s i constant across simulation methods. We also set n A = 5. One final comment is in order regarding the REITER solution method. Because the REITER approach yields a linearized solution, the simulation-based analysis of Koop et al. (1996) is unnecessary. Although for completeness and comparability we perform the simulation-based impulse response with the REITER method, a much simpler alternative, invariant to shock scaling or initial conditions, is available. In particular, when writing the REITER solution as X t = AX t 1 +B\" At , where X t is the endogenous vector defined in Appendix A and \" At is a continuous shock to aggregate productivity \" At \u21e0 N (0, 2 A ), the linear impulse response is given b\u0177 which is a vector of responses of each endogenous variable to an innovation in \" At . Figure B.3 plots the simulated impulse response and the linearized version immediately computable from the solution matrices, unsurprisingly confirming that they are virtually identical."}, {"section_title": "B.4 Adding Aggregate Complexity with the RE-ITER Method", "text": "To add a persistent demand or preference shock D t as well as a labor disutility process D N t to the baseline heterogeneous firms context considered here, replace the household objective based on maximization of Nt . First order conditions for the modified problem immediately reveal that firms owned by households subject to the shocks D t and D N t will value dividend flows at the rate p t D t where p t = 1 Ct , and equilibrium wages are given by w t = pt . By replacing price p t and wage w t with the formulas above in the system F from the REITER discussion of Appendix A, and adding the evolution equations for D t and D N t , we immediately obtain a system with two more endogenous variables and two more exogenous shock processes, which can be linearized and solved using numerical di\u21b5erentiation and the solution technique of Sims (2002), exactly as in the baseline REITER method. The impulse responses from this model are computed using the somewhat arbitrary choices of \u21e2 D = \u21e2 N = 0.859, identical to the persistence of aggregate productivity, scaled by the standard deviations D = N = 0.014, also equal to baseline aggregate productivity volatility. The main text displays the response to a demand shock D t , while Appendix Figures B.4 and B.5 plot local impulse responses to aggregate productivity and labor disutility shocks, respectively.   Figure B.2: Realizations vs. One-Period Ahead Forecasts Note: The figures above plot a representative 100-period portion of a larger 2000-period unconditional simulation of the three solution methods with explicit forecast mappings from the approximate aggregate state space (A, K) to realized prices and next-period productivity (p, K 0 ), i.e. the KS (in black), PARAM (in green), and XPA (in red) methods. The first column plots the realized marketclearing price p (solid line) and the forecast value of price (dotted line), given the currently realized value of (A, K). The second column plots the realized capital series (solid line) against the one-period ahead forecasts (dotted line) made on the basis of (A, K). During the simulation, the exogenous aggregate productivity process reflects a Markov chain discretized using the Tauchen (1986) procedure and is held constant across solution methods during the simulation. To achieve this, an identical simulated discretized productivity process is input directly into the KS, PARAM, and XPA solutions, while a series of continuous aggregate shocks exactly replicating the discretized productivity process is input into the REITER solution. For each method, the full 2000 period simulation for each solution method begins after 500 periods, with an initial burn-in period discarded to avoid the influence of initial conditions on the simulated aggregates.  Figure B.3: Impulse Response, Linear vs. Simulation-Based Note: The figure above plots the linearized impulse response function (in dotted lines, labelled REITER-LIN) to a positive shock to the aggregate productivity series A t in the baseline REITER solution of the heterogeneous firms model. Given the endogenous variables X t of the linearized system describing the economy, a solution X At of the model trivially yields impulse responses A t 1 B representing the economy's local response to the aggregate productivity shock \" At . The impulse response here is scaled by the assumed standard deviation of innovations to the aggregate productivity series, with the exogenous shocked series itself plotted in the right hand side of the second row. In solid lines labelled REITER, the figure plots analogous impulse responses computed as suggested by Koop et al. (1996), based on 2000 repeated simulations of a discretized aggregate productivity process of 50-period length each, with and without imposed productivity shocks at the period labelled 1 above. The REITER-LIN responses should be interpreted as 100 times log deviations from steady-states, while the REITER responses are equal to 100 times average log di\u21b5erences between the shocked and unshocked simulations.  Figure B.4: Impulse Response, Productivity Shock in Extended Model Note: The figure above plots the linearized impulse response function to a positive shock to the aggregate productivity series A t in the extension of the heterogeneous firms model augmented with aggregate demand and labor preference shocks. These impulse responses are computed after solving the extended model using the REITER technique and are therefore labelled REITER-EXTENDED. Given the endogenous variables X t of the linearized system describing the economy, a solution X t X SS = A(X t 1 X SS )+B\u270f t of the model trivially yields impulse responses A t 1 B representing the economy's local response to each aggregate shock in the vector \u270f t . The impulse response here is scaled by the assumed standard deviation of innovations to the aggregate productivity series, with the exogenous shocked series itself plotted in the right hand side of the second row. Note: The figure above plots the linearized impulse response function to a positive shock to the labor disutility series D N t in the extension of the heterogeneous firms model augmented with aggregate demand and labor preference shocks. These impulse responses are computed after solving the extended model using the REITER technique and are therefore labelled REITER-EXTENDED. Given the endogenous variables X t of the linearized system describing the economy, a solution X t of the model trivially yields impulse responses A t 1 B representing the economy's local response to each aggregate shock in the vector \u270f t . The impulse response here is scaled by the assumed standard deviation of innovations to the labor disutility series, with the exogenous shocked series itself plotted in the left hand side of the fourth row. purchases s ft , i.e. these decisions solve Final  Intermediate Goods Firms Optimize: Taking marginal utilities m t , perfectly competitive o\u21b5-patent intermediate goods prices p jt , j \uf8ff A t 1 , and aggregate variety and innovation levels A t , M t+1 as given, intermediate goods firms maximize firm value, the discounted stream of dividends, by choosing the measure of newly innovated goods M ft+1 to add to the existing measure of varieties A ft in their portfolios, the supply of all intermediate goods for use next period x S jt+1 , and the price of on-patent intermediate goods p jt , j 2 (A t 1 , A t ], i.e. these quantities solve Labor, Bond, Stock, and Intermediate Goods Markets Clear: Final Goods Market Clears: Innovation and Variety Consistency Conditions Hold: A ft."}, {"section_title": "Final Goods Markets Clear", "text": "No Arbitrage Pricing Condition Holds Innovation and Variety Consistency Conditions Hold: Southern Cost Advantage Condition Holds: O\u21b5-restriction goods are always produced in the Southern economy only. Although the fully mobile economy with a trade shock has essentially the same equilibrium concept as laid out in the previous section initially discussing the open economy, we must be more explicit about the trapped factors environment. In the trapped factors equilibrium, Northern intermediate goods firms face an additional constraint due to the adjustment costs preventing them from immediately responding in their input usage to the new trade shock. Formally, they must solve the modified problem where is the optimal input demand for period t , given expectations of the trade restriction E t,t+1 for the next period. X ft is also indexed by f and depends both upon the number of M goods that the firm plans to produce for next period, as well as the number of R goods that the firm has in its portfolio and plans to produce for the next period. Therefore, although these portfolio shares are only allocative in a period in which a trade shock occurs, we must be explicit about the structure we assume for the pre-shock portfolios of R goods held by each firm f , as well as the actual allocation of the trade shock liberalization among existing firms' measures of R goods. We now define some additional notation. Let e s f be the share of o\u21b5-patent R goods production firm f anticipates doing before the trade shock, where Then, let the trade shock allocate destruction of R goods production opportunities across firms so that only the proportion f of R goods varieties can still be produced in each firm. As long as we have the consistency condition an arbitrary choice of f will be consistent with the trade shock ! 0 . We will henceforth make the assumption that e s f = 1 N for all firms, i.e. that pre-shock allocations of R goods production is uniform across firms. This assumption grows naturally out of our structure in which we assume that firms continue to be the producers of goods which they invented, even after these goods fall o\u21b5-patent and become perfectly competitive. We also will now assume that N is even, and that half of the firms in the economy are in the \"No Shock\" industry, industry 1 . The other half of firms in the economy, those in the \"Shocked\" industry 2 , experience a loss of R goods production opportunities during the trade shock with only a fixed proportion 2 of R goods remaining. This framework is a rough approximation of the heterogeneity in the direct e\u21b5ects on firms in developed countries during the trade liberalizations of the early 2000s. Seen in this light, industries such as textiles which experienced a substantial loss of protection against manufacturers in low-wage economies such as China, can be identified with industry 2 , while other industries would be represented by firms in group 1 in our environment. We now define a trapped factors equilibrium formally. aggregate innovation quantities, imported variety measures, restricted variety measures, and aggregate variety quantities t , R t , and A t such that Northern Household Optimizes: Taking wages w t , interest rates r t , and stock prices q ft as given, the representative household in the North maximizes the present discounted value of its consumption stream by choosing period consumption C t , debt b t+1 , and share purchases s ft , i.e. these decisions solve Southern Household Optimizes: Taking wages w \u21e4 t , interest rates r \u21e4 t , and stock prices q \u21e4 ft as given, the representative household in the South maximizes the present discounted value of its consumption stream by choosing period consumption C \u21e4 t , debt b \u21e4 t+1 , and share purchases s \u21e4 ft , i.e. these decisions solve max Northern Final Goods Firm Optimizes: Taking    given their expectations of trade policy next period, then choosing the measure of newly innovated goods M ft+1 to add to the existing measure of varieties A ft in their portfolios, the supply of all intermediate goods in their portfolio for use next period x S jt+1 , x \u21e4S jt+1 , and the price of on-patent intermediate goods p jt , j 2 (A t 1 , A t ], i.e. these quantities solve Southern Intermediate Goods Firm Optimizes: Taking marginal utilities m \u21e4 t and perfectly competitive o\u21b5-patent intermediate goods prices p \u21e4 jt , j \uf8ff A t 1 as given, intermediate goods firms in the South maximize firm value, the discounted stream of dividends, by choosing the supply of all intermediate goods in their portfolios A \u21e4 ft for use next period x S jt+1 ,x \u21e4S jt+1 , i.e. these quantities solve Labor, Bond, Stock, and Intermediate Goods Markets Clear Final Goods Markets Clear: No Arbitrage Pricing Condition Holds Innovation and Variety Consistency Conditions Hold: Southern Cost Advantage Condition Holds: O\u21b5-restriction goods are always produced in the Southern economy only. Proof of Proposition 1: Closed Economy Balanced Growth Path To complete the proof of Proposition 1, we need to show that the rates of growth of output, consumption, and varieties are equal on the balanced growth path. The final goods market clearing condition is where we note that since it is the measure of o\u21b5-patent varieties, R t = A t 1 , and the measure of innovated varieties M t = gA t 1 . Now, recall the assumption of balanced growth. If we define the growth rate of consumption by g C , and note that the by symmetry the individual firm patenting ratios g f = g n , we can use the intermediate goods firm pricing rules to rewrite the final goods market clearing condition as Since Ct At is constant, we conclude that g = g C , so that the innovation optimality condition reads \u232b This expression motivates the choice of the scaling constant so that the balanced growth path growth rates are invariant to the number of firms or the degree of cost externalities across firms as well as the number of firms N . We obtain the balanced growth path innovation optimality condition The left-hand side, the marginal cost of innovation, is strictly increasing in g , is equal to 0 when g = 0 , and limits to 1 as g ! 1 . The right-hand side, the discounted monopoly profits from innovation, is strictly decreasing in g , is equal to \u2326 1 a H > 0 when g = 0, and limits to 0 as g ! 1 . We conclude that a balanced growth path equilibrium exists and is uniquely determined by the value of g which satisfies the innovation optimality condition. This completes the proof. Proof of Proposition 2: Open Economy Balanced Growth Path The demand schedules for intermediate goods, based on the Northern and Southern final goods firms' technologies, are given by where p jt and p \u21e4 jt are the prices of intermediate good variety j in Northern and Southern output units, respectively, and The optimality conditions for the Northern intermediate goods firm, combined with the Euler equations of the Northern representative household for debt and equity, are given by Di\u21b5erentiating the cost function and substituting in the optimal pricing rules we have that the third condition, the innovation optimality condition, is given by Now the balanced trade condition can be written Now, applying the assumption of balanced growth, we immediately obtain from the Euler equations of both representative households that interest rates in the Northern and Southern economies, as well as the terms of trade, are constant. Also, exactly as in the proof of Proposition 1 , the final goods market clearing conditions for each economy, together with the assumption of balanced growth, imply that the ratios are constant, so that we conclude that (1 + r) = (1 + r \u21e4 ) = 1 (1 + g) . Using this, we conclude that Now the innovation optimality condition can be rewritten as Also, substituting the terms of trade formula/balanced trade condition into the innovation optimality condition yields As a function of g, the marginal cost of innovation on the left-hand side is strictly increasing in g , starting at 0 and growing exponentially to 1 as g ! 1 . The right-hand side, the discounted monopoly profits from sale of newly patented goods in the North and the South, is strictly decreasing in g, asymptoting to 1 as g ! 0 and to 0 as g ! 1 . We conclude both that there exists a balanced growth path equilibrium for this economy, and that it is the unique balanced growth path growth rate. For any given fixed value of , we denote this growth rate, and the associated terms of trade, by g( ) and q( ) . This completes the proof."}, {"section_title": "C.2 Parameter Values and Robustness Checks", "text": "Calculating the ratio of Hto H \u21e4 To calculate the ratio of H to H \u21e4 , we follow the human capital accounting approach in Hall and Jones (1999) and compute the human capital endowment in country c from the Barro and Lee (2013) data as H c = e \u00b5cSc P c , where S c is the average number of years of schooling completed in the adult population above age 25, and P c is the size of the population of the country c in 2000. We take into account the di\u21b5erences in educational quality and the returns to schooling across countries by using the Mincerian returns to education of immigrants in the United States from country c , \u00b5 c , from Table 4 in Schoellman (2012). If Mincerian returns for a country c is not available in Schoellman (2012), we take \u00b5 c = 7% for non-OECD countries and \u00b5 c = 9% for OECD countries. These are the averages of returns to schooling for the two categories in Schoellman's sample. We finally define H c , where the ratio 2.1 corrects for the fact that not all non-OECD countries are represented in the Barro and Lee data. In particular 2.1 is equal to the ratio of the non-OECD to OECD population ratio in 2000 in the Wolfram Alpha database (with full global coverage) to the non-OECD to OECD population ratio in 2000 in the Barro and Lee data. Such a procedure relies on the implicit assumption that the schooling rates and returns to education in countries not represented in the Barro and Lee data are similar to those with data present. From the procedure above we obtain H \u21e4 H \u21e1 2.96, which we round to 3.0 in the text discussion."}, {"section_title": "Computing Patent Ratios", "text": "United States Patent and Trademark O ce data on patents granted from 1977-2006, by application year and nationality of assignee, are downloaded from the NBER website for the Patent Data Project, as of early 2013. This website represents an update of the data which was originally collected and documented in Hall et al. (2001). Patents granted to multiple assignees are counted only once, and the nationality of the patent is determined by the first assignee. OECD status is as of application year. Total foreign, non-OECD, and Chinese patent ratios are equal to the number of granted patents with a particular application year, normalized by the total number of granted patents in the same application year. This normalization incorporates the reduction in grant numbers as the application year approaches the end of the sample, the well known application lag/truncation problem with patent data of this form. Figure C.1 plots the proportion of all US patents granted by application year from any foreign nation, from non-OECD countries, and from Chinese assignees, for the years 1977-2006."}, {"section_title": "Calculating the Trade Shares", "text": "The real per capita output growth rate is from the US NIPA tables, computed as the average annual real GDP per capita growth rate from 1960-2010. Trade data was downloaded from the OECD-STAN database, and OECD GDP data comes from the Penn World Tables, Version 7.1. The non-OECD country to OECD imports to OECD output ratios were computed over the years [1997][1998][1999][2000][2001][2002][2003][2004][2005][2006]. The period was chosen to incorporate the accession of China to the WTO in 2001, and the 10-year window accords with the model calibration of a period to 10 years. All of the data and simple calculations performed in the calibration procedure are available on Nicholas Bloom's website: http://www.stanford.edu/nbloom. Figure C.2 plots the non-OECD imports to OECD GDP ratio over this period, together with Chinese imports into the OECD. Trade policy substitution in the counterfactual away from China towards the rest of the non-OECD Total observed low-wage import growth into the OECD as a share of GDP from 1997-2006 is equal to 3.1%. Growth in Chinese import shares was equal to 1.61%, implying that non-China/non-OECD countries saw their import shares into the OECD increase by 1.49%. The no China counterfactual in the main text assumed that the growth in Chinese import shares was completely removed from liberalization over this period. If, however, policy-makers partially substituted towards other non-OECD imports in lieu of Chinese imports, we would still see import share growth in the counterfactual. To analyze the quantitative magnitude of this substitution e\u21b5ect, we consider a case where exactly one half of Chinese import growth is realized in the no China counterfactual, via substitution towards other non-OECD countries. Starting with a low-wage import share of 3.9%, this \"half substitution\" case exhibits import share growth of 0.5*1.61+1.49 = 2.295%, so that the resulting target import to output ratio post-liberalization in the counterfactual is 3.9+2.295 = 6.195%. Figure C.3 plots the resulting two trapped factors transition paths, analogous to Figure 3.7, in the total observed import liberalization and \"Half China\" cases. As can be seen immediately, the transition paths di\u21b5er by less than the case in which all Chinese import growth is removed-, which works to reduce the marginal contribution of China to welfare to a total of 3.3% (North) and 3.2% (South). In this alternative counterfactual, the impact of China is equal to 20% (North) and 21% (South) of the overall welfare gains from trade observed in the data."}, {"section_title": "Other Robustness Checks", "text": "In this section we provide the main numbers underlying the robustness checks underlying Figure 3.8 in the main text. In particular, beginning from our baseline calibration, in Table C.1 we list the post-shock balanced growth path growth rate, as well as the maximum growth rate along the trapped-factors transition path, for a number of alternative parameter choices. Also, note that in the text we mention an alternative calibration strategy for the pre-shock balanced growth path growth rate. If we compute the United States per capita real GDP growth rate over the period 1960 2001 rather than the baseline calibration window of 1960 2010 , we obtain a pre-shock balanced growth rate of Note: The first column records the parameter varied from our baseline calibration. The second column represents the maximum annualized percentage variety growth rate over the trapped factors transition path in the alternative calibrations. The third column represents the post-shock balanced growth path annualized percentage growth rate associated with the alternative calibration. The baseline calibration features parameter choices of \u21e2 = 0.5, \u21b5 = 0.667, = 1/1.02, = 1.0, and \u2318 = 1.0. 2.3% rather than the baseline 2.0% . However, in this case, the peak transition path growth rate is 3.09% , and the post-shock balanced growth rate is 2.70% . Given the higher initial condition, this is almost a direct translation upwards of the baseline transition path. Given the nonlinearity of the model, such a result is not automatic. Note that a previous version of our calibration strategy, with results published in Bloom et al. (2013a) yielded smaller dynamic impacts of trade liberalization. Our improved calibration strategy here di\u21b5ers from that earlier work in three respects. First, we consider a model period of ten years rather than one year to match a more plausible e\u21b5ective monopoly length. Second, we base the calibration on imports to value added ratios rather than imports to gross output ratios, since data availability for China is better for value added. Third, instead of calibrating the post-liberalization trade openness via a \"limiting\" highest 0 which still maintained product-cycle trade (i.e. q( 0 ) < 1), the first two calibration changes allow us to now directly match observed pre-and post-liberalization trade ratios in 1997 and 2006, which results in larger growth impacts more aligned with observed trade liberalization."}, {"section_title": "C.3 Solution Technique and Equilibrium Conditions for the Calibration", "text": "Please find both replication data files for the calibration exercise, as well as code to duplicate all of the quantitative results in the paper, on Nicholas Bloom's website at http://www.stanford.edu/nbloom/. We solve each of the systems of nonlinear equations laid out below using particle swarm optimization as implemented in R . This is an extremely robust global nonlinear optimization technique, and all solutions are computed with a summed squared percentage error across all equations of less than 10 7 ."}, {"section_title": "Balanced Growth Path", "text": "As documented in the proof of Proposition 2 , the balanced growth path growth rate g( ) of the open economy given trade restriction is fully characterized by the equilibrium innovation optimality condition All other long-run quantities, in particular the interest rates and exchange rate, are direct functions of this balanced growth path growth rate through the Euler equations and balanced trade condition (1 + r( )) = (1 + r \u21e4 ( )) = 1 (1 + g( ))"}, {"section_title": "Fully Mobile Transition Dynamics", "text": "To compute the transition dynamics of the fully mobile model in response to a trade shock in period 0 , starting from the balanced growth path associated with trade restriction , we first pick a horizon T . We also normalize A 0 = 1 . Then, we assume that the model has converged to the balanced growth path associated with 0 by period T . This structure requires that we solve for 3(T 1) prices, {q t , r t , r \u21e4 t } T"}, {"section_title": "t=2", "text": ". These 3(T 1) prices are pinned down by 3(T 1) equations: the balanced trade condition, the Northern Euler equation, and the Southern Euler equation, in periods 1, ..., T 1 . These equations are given by We note that all allocations in the transition path are a function of these three prices. Intermediate goods prices follow the monopoly markup or competitive pricing Since all allocations in this economy are therefore a function of the 3(T 1) prices, we can construct the errors in 3(T 1) equations above given any input sequence of prices. The percentage squared errors of this system of equation are minimized using particle swarm optimization. After solving for the transition path price paths, we check to see if the cost advantage for I goods production is maintained by the South, justifying our M, R, I goods partitioning. This is equivalent to checking that, for each period (1 + r \u21e4 t )q t \uf8ff (1 + r t ). In the baseline results shown in Section 3.5 , we choose T = 7 ."}, {"section_title": "Trapped Factors Transition Dynamics", "text": "The equilibrium conditions which we must solve to compute the transition dynamics for the trapped factors model are identical to those in the fully mobile economy, for period 2, ..., T 1 . There are, however, di\u21b5erences in the equilibrium conditions in the period of the shock. In particular, there is heterogeneity in the response of the a\u21b5ected and una\u21b5ected industries to the shock, and instead of solving for simply the 3(T 1) prices {q t , r t , r \u21e4 t } T t=2 as in the fully mobile case, we must solve for these prices and the four additional variables {g 1 2 , g 2 2 , \u00b5 1 , \u00b5 2 } . These variables are patenting rates and shadow values of inputs within Northern firms in the una\u21b5ected industry (1 ) and the a\u21b5ected industry (2 ). Therefore, we must pin down 3(T 1)+4 quantities, which we do with 3(T 1) + 4 equations: 1 H \u21e4 ). The first 3(T 1) equations are simply the balanced trade and Euler equations for the Northern and Southern households in periods 1, ..., T 1 . The balanced trade condition must be modified in period 1 to reflect the fact that flows of M goods from North to South come from both industry 1 and industry 2 , with di\u21b5erent prices and quantities for each. The final four equations represent the innovation optimality conditions for firms in industry 1 and industry 2 , as well as the trapped factors constraints for firms in each industry. The innovation optimality conditions are simply the first-order conditions of firms with respect to the mass of new varieties to be innovated in period 0 for use in period 1 . Note that we are defining \u00b5 1 = 1 1 and \u00b5 2 = 1 2 , where m 1 1 and m 1 2 are the multipliers on the trapped factors input constraints in the optimization problem for Northern intermediate goods firms in period 1 . A fall in \u00b5 below 1 represents a fall in the shadow value of inputs for an intermediate goods firm. Also, if M f 1 is the number of new patents innovated by a firm in industry f in period 0 for use in period 1 , we are following the conventions A 0 , and imposing the consistency condition The trapped factors constraints are simply the input demands for R goods production and M goods innovation and production expenditure pre-shock (left hand side) and post-shock (right hand side). The input constraints di\u21b5er across industries because the R goods available in the post-shock period in industry 2 for production are reduced by the factor 2 , where 2 satisfies 1 + 2 2 = 1 0 1 , which is the consistency condition discussed in the equilibrium definition. Also, the right-hand side on the trapped factors constraints take into account the following with the no variety case given by Trapped factors versions reported in the text require generalization to the case of two separate industries' M goods varieties but are straightforward versions of the above. Along a balanced growth path with constant trade restriction , we have that the baseline Northern output level in a given period, with the (arbitrary) level of varieties in that particular period given by A ss and considering balanced growth path intensive margins, is equal to Y ss = H \u21b5 (M ss x 1 \u21b5 Mss + R ss x 1 \u21b5 Rss + I ss x 1 \u21b5 Iss ) M ss = A ss g ss , R ss = A ss (1 ), I 1 = A ss , and the output level in the no price with no R to I conversion in that period is given by Y noprice ss = H \u21b5 (M ss x 1 \u21b5 Mss + (A ss I ss 1 )x 1 \u21b5 Rss + I ss 1 x 1 \u21b5 Iss ) I ss 1 = I ss /(1 + g ss ), with the no variety case given by Mss ."}, {"section_title": "C.4 Semi-endogenous Growth Model", "text": "In this appendix we consider the semi-endogenous growth model approach to show that it delivers quantitatively similar results to our fully endogenous growth model. As documented in Jones (1995b,a) the implication of a model like that considered in the main text, with \"strong scale e\u21b5ects\" implying that the long-term growth rate is dependent upon the level of human capital, is rejected by the time series evidence which documents the concurrence of rising populations and researcher numbers with constant growth rates. Jones proposes a small modification to the production function for new varieties, or alternatively, to the cost function for innovation, which implies smaller returns from the existing stock of varieties in the production of new patents. This change to the model converts the structure into a \"semi-endogenous\" growth model with \"weak scale e\u21b5ects,\" since the long-term growth rate is now proportional to the growth rate of human capital rather than the level of human capital. Analogously, in our context with product-cycle trade, such a modification of the model leads to long-term growth rates proportional to human capital growth rates and, crucially, independent of the trade liberalization policy . As we will see, however, a reasonable calibration of a semi-endogenous growth model consistent with the data on both per-capita growth rates and population growth displays extremely long transition dynamics and considerable temporary e\u21b5ects on variety growth rates from trade liberalization. Therefore, the temporary growth e\u21b5ects of liberalization (and the permanent level e\u21b5ects), imply similar results for welfare regardless of whether one considers a strong or weak scale e\u21b5ects model. Given that the model with strong scale e\u21b5ects delivers closed-form expressions for the balanced growth path growth rates dependent upon the trade policy parameter , and given that the transition dynamics for the strong scale e\u21b5ects model are of a more reasonable length, we prefer to work with the strong scale e\u21b5ects model as our baseline version."}, {"section_title": "Model", "text": "We now lay out the model structure and equilibrium concept in the semi-endogenous growth framework, for the fully mobile environment only. Population and Human Capital We assume that in the North and in the South there is a continuum of identical households of measure 1 , each with an expanding set of members [0, L t ] and [0, L \u21e4 t ] , respectively. We further assume that there is an constant level of human capital per member of the population, i.e. H t = hL t and H \u21e4 t = hL \u21e4 t , respectively. This assumption implies that preferences of the CRRA form defined over per-capita consumption or over consumption expressed relative to human capital di\u21b5er only by a constant, and for convenience we express preferences as per unit of human capital. 1 Northern Households Given a sequence of wages w t , firm stock prices q ft , firm dividends D ft , and interest rates r t , a Northern household supplies labor inelastically and chooses consumption C t , portfolio positions S ft , and bond purchases B t+1 to solve the problem Southern Households Given a sequence of wages w \u21e4 t , firm stock prices q \u21e4 ft , firm dividends D \u21e4 ft , and interest rates r \u21e4 t , a Southern household supplies labor inelastically and chooses consumption C \u21e4 t , portfolio positions S \u21e4 ft , and bond purchases B \u21e4 t+1 to solve the problem max 1 Note that we omit below a term multiplying per capita preferences by the size of the population, which would be proportional to H \u21e4 t given our assumptions. Such an assumption, as will be seen below, results in a level shift in interest rates. However, and importantly, our assumption prevents the mechanical inflation of the welfare gains from trade liberalization (relative to our baseline strong scale e\u21b5ects model with no population growth) simply because liberalization gains occur in the future with a larger population. In unreported results, however, we also solved an alternative model with per-capita preferences weighted by population size. Predictably, this resulted in larger welfare gains from trade liberalization.  . This is equivalent to stock price or value maximization as can be seen from iteration on the Southern Household's first order condition for S ft and insertion of the Southern Household's first order condition for B \u21e4 t+1 ."}, {"section_title": "Terms of Trade Notation/No Arbitrage Condition", "text": "Trade Restrictions and Monopoly Structure There is one-period monopoly protection for any newly innovated M goods, trade restriction for an exogenously set proportion 1 t of o\u21b5-patent goods labeled R goods, and imports from South to North of the exogenously set proportion t of o\u21b5-patent goods labeled I goods."}, {"section_title": "Equilibrium Definition", "text": "\u2022 Some sequence of t is exogenously set by the Northern government \u2022 Northern households optimize consumption, savings, and equity purchase decisions \u2022 Southern households optimize consumption, savings, and equity purchase decisions \u2022 Perfectly competitive Northern final goods sector optimizes human capital and intermediate goods demand \u2022 Perfectly competitive Southern final goods sector optimizes human capital and intermediate goods demand \u2022 Northern intermediate goods firms optimizes M goods innovation, M goods monopoly production, and fast-copier-constrained de facto perfect competition R goods production decisions \u2022 Southern intermediate goods firms or fast copier optimize perfectly competitive R and I goods production decisions \u2022 Trade is balanced: , imposes symmetry g Af t+1 = (1/N )g At+1 Southern Cost Dominance for I Goods Proposition 3. A balanced growth path with constant exists and is unique. On this balanced growth path the growth rate g A of varieties satisfies (1 + g A ) interest rates satisfy 1 + r = 1 + r \u21e4 = 1 (1 + g H )(1 + g A ) , and the terms of trade satisfies On this unique balanced growth path, output and consumption grow as the factor (1 + g H )(1 + g A ) and per capita consumption has growth rate equal to the number of varieties g A . Proof of BGP Formulas Assume constant growth rates of quantities and a constant . Then the HH Euler equations yield 1 + r = 1 (1 + g H )(1 + g c ) 1 + r \u21e4 = 1 (1 + g H )(1 + g c \u21e4 ) , which implies that interest rates are constant. But the BT condition is then which implies that the terms of trade are constant. But the innovation FOC is 1 \u21e2 = (1 + g H ) on any BGP Now note that prices of all goods are constant because they are functions of interest and terms of trade, so the intensive demand margins are also constant multiples of human capital. In particular, Note also that by the consistency conditions M t = g A A t 1 , R t = (1 )A t 1 , I t = A t 1 are all constant multiples of A t (given the fact that A t 1 = 1 1+g Now from the uses identity we also have But since 1 + g H = (1 + g A ) 1 \u21e2 on any BGP by the innovation FOC, we have Z t / ((1 + g H )(1 + g A )) t Therefore, we have C t / ((1 + g H )(1 + g A )) t , c t / (1 + g A ) t implying that g c = g A , so that 1 + r = 1 (1 + g H )(1 + g A ) . Now similar reasoning shows that Note that this final expression implies that for su ciently small , q < 1 , which is equivalent along the BGP to Southern cost dominance in I goods. Finally, uniqueness follows from the innovation FOC . After dividing both sides by (1 + g H ) t , we have that Since > 1 , the LHS is increasing in g A . Since r is increasing in g A and q is decreasing in g A , there is at most one solution for g A . Since all other prices are functions of g A , they are unique as well. Existence is shown by noting that the increasing LHS asymptotes to 1 as g A ! 1 and to 0 as g A ! 0 . The decreasing RHS asymptotes to 1 as g A ! 0 (see the formula for q ) and to 0 as g A ! 1 (see the formulas for r and q ). By the continuity and monotonicity of everything involved, as well as the intermediate value theorem, g A exists uniquely. End of Proof"}, {"section_title": "Calibration Strategy", "text": "We would like to consider, as in the fully mobile environment described above, the transition path associated with a shock from the balanced growth path associated with trade policy parameter to the balanced growth path associated with trade policy parameter 0 . As before, we will consider the impact of a permanent and unanticipated shock moving the policy parameter from to 0 . The timing conventions are identical to those discussed in the fully mobile trade shock timing section in the main text. According to the OECD National Accounts Main Aggregates dataset and Population dataset, as current in early May 2013, the average total OECD real GDP per-capita growth rate from 1984 2000 is equal to approximately 2.37% per year. The average OECD population growth rates over this same period is approximately equal to 0.78% per year. Now note that the balanced growth path relationship above between g H and g A is a logarithmic equation whose solution yields = 1 \u21e2 log(1 + g H ) log(1 + g A ) . Above, note that g A and g H are 10-year versions of the annual growth rates taken from OECD data. Now, with the calibration \u21e2 = 0.5 from above, we have that = 0.83 . The remaining parameters to calibrate in the model are , , \u21b5 ,H \u21e4 H , H 1 , , and 0 . The values for \u21b5 = 2/3, = 1 , = 1/1.02 , and Ht = 2.96 are unchanged from before. The final three parameters which must be calibrated are , 0 , and H 1 . We jointly pick these three parameters so that the following three conditions hold: I Y ,BGP = 3.9% , I Y ,BGP = 7.0% , and the innovation first order condition for the pre-shock balanced growth path is satisfied. The first two conditions require that the model match the non-OECD to OECD trade shares which the strong scale e\u21b5ects model is calibrated to match. The final condition requires that the scaling of varieties to human capital at the initial condition of the transition path is consistent with the equilibrium conditions. Given the calibration, the transition path in response to a fully mobile shock moving the economy from to 0 can be written as a minimization problem in r t , r \u21e4 t , and q t , as in the strong scale e\u21b5ects case. The endpoints of each series are known, because they reflect balanced growth path values. Figure C.4 plots the transition path for the semi-endogenous economy in response to the trade liberalization, for variety growth, the Southern terms of trade, and Northern and Southern per-capita output growth. In fact, the transition is not complete 25 periods. Recall that a period in this calibration is one decade, so this represents a transition path which is not complete 250 years after the initial shock. However, the broad pattern of the transition path is similar to that observed in the strong scale e\u21b5ects model. In particular, we have that in response to trade liberalization, the appreciation of the Southern terms of trade due to the increased flow of I goods from South to North causes an increase in the variety growth rate, as well as Northern and Southern output growth rates. Variety growth rates immediately begin to fall, however, as the gains from increased variety levels fade in the semi-endogenous innovation cost function. This process is incredibly persistent, however, because the level of implied by OECD evidence on per capita GDP and population growth rates is quite close to 1 , yielding something quantitatively similar to the strong scale e\u21b5ects model. Because of consumption smoothing and the implied movements in interest rates, Northern and Southern output growth rates are smoother than variety growth, yet just as persistent. Finally, as the variety growth rate and interest rates begin to return to their normal long-run levels, the Southern terms of trade q slowly converges to its new long-run value associated with 0 ."}, {"section_title": "Results", "text": "More precisely, in Table C.2 we present the detailed statistics associated with trade liberalization in the semi-endogenous model. In particular, note that the halflife of the shock to the variety growth rate is 16 periods, or 160 years. Also, note that the welfare gains to the North and to the South from liberalization, 16.5% and 15.4%, which are permanent consumption equivalent welfare gains defined analogously to before, are qualitatively similar to those obtained from the strong scale e\u21b5ects model."}, {"section_title": "C.5 R&D Cost Externalities within Strong Scale E\u21b5ects Model", "text": "As noted in the main text, to allow for the problem that firms face in coordinating 15.4% Note: The table above displays a summary of the quantitative exercise performed for the semiendogenous model given a calibrated trade liberalization. The long-run annualized value of the interest rate is given as r , and all other quantities are computed from a transition path in response to an unanticipated, permanent movement of trade policy to 0 > , where and 0 are chosen to match the movement in low-cost imports to OECD GDP observed in the data from 1997-2006 and also displayed in the table. The pre-and post-shock Southern terms of trade q( ) and q( 0 ) vary permanently with the trade policy parameter and reflect the balanced growth path for the indicated policy. The maximum level of variety growth max g At and the maximum di\u21b5erence in variety growth from its long-run level over the transition path are displayed in the first two rows, while the half life of the shock to variety growth induced by trade liberalization is indicated in the third row. The model calibration of a period is one decade. W and W \u21e4 refer to the permanent consumption equivalent of trade liberalization for a Northern and Southern household, respectively. In particular, this percentage is the permanent fraction by which consumption for a household must increase in each period without the trade shock to make the household indi\u21b5erent to the allocation with trade liberalization. conditions and resource constraints. For the fully mobile environment, the symmetry across firms causes invariance of the aggregate allocation to the level of \u2318 . Only the trapped factors transition dynamics are modified. For completeness, we reproduce below the modified system of equations solved numerically to compute the transition path in the trapped factors case with an arbitrary level of \u2318 . These equations are the direct analogy of those in appendix above. Note: Non-OECD and Chinese imports into OECD countries are from the OECD-STAN database as available in April 2013. Chinese import data is directly available, and non-OECD imports are imputed as the di\u21b5erence between world imports and imports from other OECD members in a given year. The normalizing GDP measure for the OECD is computed from the Penn World Tables version  7.1 and equals the sum of GDP for all OECD members in a given year. The Chinese imports to OECD GDP ratio in 1997 is 0.79% and in 2006 is 2.4%. The total non-OECD imports to OECD GDP ratio in 1997 is 3.9% and in 2006 is 7.0%. Note: The figure displays the transition path in response to trade liberalization in two scenarios. The first transition path, in solid black, \"Baseline,\" replicates the trapped factors transition path displayed in Figure 4 above. A permanent and unanticipated trade liberalization from to 0 > is announced in period 0 to become e\u21b5ective in period 1. The second transition path in green with triangle symbols, \"Half China,\" plots the trapped factors transition path, starting with the same initial conditions as \"Baseline\" but instead considering a counterfactual increase of to a level between and 0 which matches post-liberalization imports to GDP ratios assuming that half the growth in Chinese imports into the OECD occurs through policy substitution to non-China, non-OECD countries. The upper horizontal solid blue line is the post-shock balanced growth path, and the lower horizontal dashed red line is the pre-shock balanced growth path. Note: The figure displays the fully mobile transition path in the semiendogenous growth model in response to a permanent, unanticipated trade liberalization from policy parameter to 0 > , which is announced in period 0 to become e\u21b5ective in period 1. Intermediate goods firms may respond to the information about trade liberalization without short-term adjustment costs. The solid black line is the transition path, the upper horizontal solid blue line is the post-shock balanced growth path, and the lower horizontal dashed red line is the pre-shock balanced growth path. Note that since the semiendogenous growth model's value for variety growth and output growth in the long run does not vary with trade policy, there is only one balanced growth marker for these series."}]