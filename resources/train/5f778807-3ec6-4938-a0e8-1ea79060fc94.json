[{"section_title": "Introduction", "text": "The National Science Foundation's Survey of Doctorate Recipients is conducted every two or three years and collects detailed information on individuals receiving PhDs in science and engineering in the U.S. and some others with PhDs from abroad in these areas. Survey weights adjust for oversampling and nonresponse on a cross-sectional basis. The survey is used as the basis for reports such as NSF (2008,2011). Every survey year the target population is a little bit different because people enter (e.g., new Ph.D. recipients in the U.S.) or leave (e.g., deaths) the population. Numerous variables are included in the data set. Variables cover labor force status, academic rank and tenure, salary, field and institution of degree and employment, age, sex, race/ethnicity, marital status, spouse employment, whether children are at home and their ages, U.S. citizenship, work responsibilities, management position, professional memberships, reasons for taking a post doctoral position, and questions about a career path job. Survey weights adjust for oversampling and nonresponse on a cross-sectional basis. That means that analysis using the survey data with the survey weights in a given year is representative of a corresponding population. The survey weights are not designed explicitly for longitudinal analysis of data sampled in different survey years. This fact does not mean, however, that no longitudinal analysis is possible. Indeed, a significant portion of the sample (e.g., 60% on 3 or more surveys from 1993-2006) appears in multiple survey years and can be linked across time. Despite this fact, there are no longitudinal weights for the survey that would enable estimation of statistical models or comparison of finite population characteristics using data from multiple survey waves together."}, {"section_title": "Longitunidal analysis and the SDR", "text": "The type of analysis of change over time that can be accomplished with the Survey of Doctorate Recipients is focused on cohorts defined by survey years. If one wants to estimate rates of progression or factors associated with advancement in employment within a field of study, then one can do so using a particular cohort or survey year. For example, if one wants to estimate the probability of proceeding from PhD in 1991-1992 to postdoc to tenured faculty member within ten years in the biological, agricultural and environmental life sciences, one can examine the recent PhD graduates in this area sampled in the 1993 survey who can be linked over time in the 1995, 1997, 1999, 2001, and 2003 surveys. One then could look at the same question for other years, such as the 1993-1994 PhD graduates appearing first in the 1995 survey and linked through the 2006 survey. A consequence of conducting cross-sectional analyses is that sample sizes are more limited than they would be if longitudinal analysis was planned into the design. For example, there could be graduates in from 1991-1992 who did not enter the survey until a survey year after 1993, such as 1995. Individuals such as these cannot be readily combined with the 1993 survey data, because their 1995 survey weights are designed only for cross-sectional estimation. Another limitation occurs when estimating statistical models of change over time. Imagine estimating change in salaries over time (years 1991 to 2002, surveys 1993 to 2003) by field of study and demographic characteristics, such as sex, rank, Carnegie ranking of institution, and U.S. citizenship. Ideally one would use all respondents from all survey years. What should one do with the cross-sectional survey weights that each respondent has for each survey in which they participate? If there were one longitudinal survey weight for each unique respondent, then combining respondents from different survey years would be more readily doable."}, {"section_title": "Surveys designed for longitudinal analysis", "text": "Before proceeding to describe calibration weighting to create longitudinal survey weights, it should be noted that some surveys directly plan for longitudinal, panel, or time series analysis. The American Community Survey (ACS; http://www.census.gov/acs/www/) selects five years of household sample cases at once (U. S. Census Bureau 2009;chapter 4). Within each county, the sample for five years is selected all together and then split into five parts. Doing so produces consistent weights for combining sample respondents together. This is particularly important for estimation of characteristics in small places. The ACS is not longitudinal, however, because individuals are included in only one survey year of data collection. The Current Population Survey (CPS; http://www.bls.gov/cps/; http://www.census.gov/cps/) is designed to measure the level of and changes in employment, unemployment, and labor force participation. The CPS is longitudinal in that individuals are measured for four months initially and then for another four months after an eight month break (U. S. Census Bureau 2006). Longitudinal weights are discussed in chapter 10 (starting on page 10-14). These weights are constructed for flows based on population controls from the U. S. Census Bureau. As discussed below, such information is not available to researchers utilizing the NSF SDR data. There are many other surveys -longitudinal surveys and panel surveys -that are designed to measure change over time. Several of these surveys plan survey estimation and weighting with this goal in mind. Examples of these surveys include the Survey of Income and Program Participation (SIPP), the National Longitudinal Surveys (http://www.bls.gov/nls/), the Panel Study of Income Dynamics (http://psidonline.isr.umich.edu/), the 2009 Panel Survey of Consumer Finances (http://www.federalreserve.gov/pubs/oss/oss2/scfindex.html), and the Medical Expenditure Panel Survey (http://www.meps.ahrq.gov/mepsweb/). it is beyond the scope of this article to review the methodology utilized in these and other studies. Surveys in the area of environmental monitoring are intended to enable estimation over time. One development in this area is generalized least squares estimation as in Breidt and Fuller (1999). Within each survey year, one could estimate the outcome for a variable of interest conditional on certain covariate variables. For each year, one then estimates variance based on sample and weights in a given year. One would then estimate the covariance between estimates in pairs of years. There is covariance that depends on the overlap of samples across time. An estimate of change is then computed as a function of these totals. The variance of the change estimate is then a function of the estimated variances and covariances. The USDA's National Resources Inventory is a survey that utilizes this methodology in estimation. Panel surveys and surveys over time are considered by Duncan and Kalton (1987), Fuller (1999), andMcDonald (2003) and references therein. Comparison to these and other survey designs will be considered at a later time."}, {"section_title": "Outline", "text": "This paper explores calibration estimation (Deville and S\u00e4rndal 1992 and references given below) for construction of longitudinal weights for cross-sectional sample surveys. Section 2 discusses calibration and formation of longitudinal survey weights from cross-sectional weights. Section 3 outlines a simulation study plan. Section 4 gives preliminary results. Section 5 discusses findings, limitations, and future work. The paper ends with references and acknowledgments, which include a disclaimer."}, {"section_title": "Calibration for Longitudinal Weighting", "text": ""}, {"section_title": "Calibration Weighting", "text": "Calibration estimation and calibration weighting methods were described by Deville and S\u00e4rndal (1992). The connection to raking adjustment was demonstrated in Deville, S\u00e4rndal, and Sautory (1993). Reviews of the literature and methods for calibration in sample surveys can be found in Kim and Park (2010) and S\u00e4rndal (2007). Calibration methods in survey sampling allow one to adjust survey weights so that they are close to initial weights, such as the sampling design weights, but satisfy certain constraints. The closeness of the weights is described by a distance function. For example, if is a value for a variable on subject in the sample and the total for variable in the population is known to be , then a constraint could be that the weighted total of the -values in the sample equal : Let { } be original survey (design) weights. Let = \u2211 is a known total in the population with indices ; can be a vector. The calibrated weights { } are \"close\" to { } but satisfy a set of calibration equations: There are various ways to compute the weights, including in the R survey package (Lumley 2011). Calibration weighting can match (published) control totals and reduce mean squared error. A reduction in mean squared error might occur when the variable is sufficiently correlated with an outcome variable. Calibration can be implemented in a way to control the minimum and maximum value of weights and to match one or more control totals. It is therefore a very flexible methodology. Indeed, Zhang (2000) describes how calibration can produce adjusted weights equivalent to those produced with post stratification. In the context of nonresponse weighting, one can specify the desired post stratification adjustments in terms of control totals for calibration weighting. For example, the goal could be to have the sum of weights for respondents in a weighting class or post stratification cell match the sum of weights of sampled units in that cell. One might also want to place an upper bound on the largest weight in the cell. Then the survey calibration algorithm provides a procedure for adjusting the current weights. The Research Triangle Institute (RTI 2008) implements a general methodology that enables this form of calibration. Inherent in the use of calibration, cell-based adjustment, and raking is the need to select variables and subgroups to define the control targets. These methods will be more successful in removing non-response bias if cells and control variables are related to probabilities of non-response and to variables used for analyses. Mirel et al. (2010) used the RTI SUDAAN program to compare weighting class and more general calibration adjustments for weights in the NHANES (2003)(2004). In some survey settings, researchers have used calibration to adjust weights to match estimated control totals. Estimated control totals have their own degrees of uncertainty associated with them. Variance estimation with calibrated estimators when the calibration is based on estimated totals receives further comment in the discussion section below."}, {"section_title": "Longitudinal Calibration", "text": "The principle motivation for creating longitudinal weights is a desire to be able to take multiple survey years together. Combining data from survey years increase sample size versus a single cohort. Although the NSF SDR survey is large by most standards, the number of individuals in certain discipline by rank by demographic group combinations in a single survey year can be small. One complication with combining data from different survey years is that each individual in each year has survey weight for that year. Calibration weights for estimation with longitudinal data in the National Long Term Care Survey (NLTCS; http://www.nltcs.aas.duke.edu/) has been considered by Ash (2005). Cross-sectional weights for this survey are computed so that weights sum to population totals. This is an example of classical post stratification. When the interest is the difference between totals at two time points, there are two sets of population totals (earlier totals, later totals) that are available. Ash (2005) uses calibration estimation to adjust weights for both sets of known total controls. The author investigated one-and two-step calibration approaches, which differ in whether the various calibration totals are used simultaneously or one after another in weight adjustment. The NLTCS uses repeated replications in variance estimation. The interest in the current paper differs from the interest of Ash (2005) in a few important ways. First, the goal here is to use several survey years together, not only two. Second, the known population totals are not available; rather, estimated totals can be produced in each survey year. Third, a broader set of estimands is being considered; these are describe further below. Otherwise, the current paper shares much of the same interest as the paper by Ash (2005). Three requirements are considered when producing longitudinal weights. First, the weight needs to be calculable from existing data, which means either the public use data sets or the restricted use versions that NSF releases under strict licensing. The exact population totals and the exact definition of post stratification cells are not known to the researchers outside of the organization that produced the data. Second, the weight needs to be useful for reproducing key cross-sectional analyses. This is both a requirement for consistency and an attempt to produce advantages in estimation via correlations. If a calibrated set of weights could not reliably reproduce analyses of interest (not with exact correspondence necessarily but with reasonable proximity in some metric), then users would be unlikely to utilize the new weight set. Third, the weight should be low in variability, because high variability weights are associated with low precision in estimation. The third requirement potentially affects all weight adjustment procedures and applications. In the area of nonresponse adjustment, fine adjustments to weights often have the potential to remove more nonresponse bias than coarse adjustments, but the resulting weights are often more variable, which can negatively affect the standard errors for some estimators. The process of calibrating cross-sectional weights to produce a set of longitudinal weights for analysis of data from combined survey years can be divided into five steps. 1. Selection of initial weights for each subject that appears in at least one survey year. 2. Selection and computation/estimation of control targets from one or more survey years. 3. Selection of a calibration method from the available options. Some calibration methods require making choices such as minimum and maximum allowable weight. 4. Computation of calibrated weights. 5. Evaluation of the calibrated weights in terms of analyses of interest. The evaluation includes computation of point estimates as well as standard errors. Section 3 presents the prototype scenario that is used in simulations and discusses the steps listed above in this context. 3. Simulated population, simulation parameters, calibration options, and estimands 3.1 Simulated population Table 1 illustrates a prototype scenario for a cross-sectional survey. The populations in years 1, 2, and 3 are 1 , 2 , and 3 , respectively. Within each population is a  x 1 = 8000 2 = 8000 3 = 8000 domain or subpopulation of interest, \u2282 , such as female doctorate recipients, recent graduates, minority doctorate recipients, or graduates with a degree is a specific field of study. Variables measured in the population can be numerous, but for estimation and calibration work they will be divided into two sets in survey year : are variables used as covariates or control variables, are outcome variables of interest to the study. Within each population, a sample is selected: \u2282 in survey year . The populations overlap as depicted in left portion of Table 2. The rows are not intended to be proportional to population size. Rows 1-4 denote the population in survey year 1. Rows 2-6 denote the population in survey year 2. Rows 3-4 and 6-7 denote the population in survey year 3. Some elements in the three populations appear in only one survey year: row 1 in year 1, row 5 in year 2, and row 7 in year 3. Other elements appear in two of the three populations: row 2 in years 1 and 2 and row 6 in years 2 and 3. In some applications, such as labor force surveys, elements could appear in years 1 and 3, but not in year 2. Such a scenario is not considered in this work, but should fit within the general framework proposed below. Other elements, represented by rows 3 and 4, exist in all three populations. In the simulation, the population size in each year is taken to be 1 = 2 = 3 = 8000. It is assumed that each year 1000 individuals enter and each year 1000 leave the population. The right portion of Table 2 gives population sizes illustrating the sizes of overlaps across years. Note that the rows do not necessarily correspond to rows in previous tables. The sampling design for the Survey of Doctorate Recipients is described on the National Science Foundation NCSES (2011) website. The prototype sampling design is depicted in Table tab3. The rows are not intended to be proportional to sample size. The sample in survey year 1 is 1 \u2282 1 , which is represented in rows 1-4. The sample in survey year 2 is 2 = { 21 , 22 } \u2282 2 and is represented in rows 3-6. Elements in rows 3 and 4 that were selected in 1 are included again in 2 . Together they are denoted 21 =\u2282 2 . Other elements in 2 are selected for the survey year 2 sample from elements in the population in 2 that were not in the population in year 1 . The subset 22 \u2282 2 with 22 \u2282 2 \u2216 1 is in rows 5 and 6. These elements correspond to new PhD's in the Survey of Doctorate Recipients; they received their degrees and entered the survey target population after the years included in survey year 1. The 's in the table indicate that the population in the given column (survey year) did not include the elements covered by the rows. For example, rows 5-7 represent elements that were not members of population 1 , rows 1 and 7 were not in population 2 , and rows 1, 3, and 5 were not in population 3 . Not depicted in the table are members of the population there were not sampled. For example, the elements not sampled in survey year 1 are 1 \u2216 1 . The sample in survey year 3 can be found in rows 2, 4, 6, and 7. Elements in row 2 are selected from those that were selected in years 1 and 2 ( 31 \u2282 21 \u2282 1 ). Units in row 6 ( 32 ) are selected from the elements that were new to the population in survey year 2 and selected in 22 \u2282 2 . Units in row 7 ( 33 ) are selected from the new members of population 3 . Additional units (row 2, 34 ) are selected from 1 \u2229 3 that were selected in year 1, but not in year 2. The set 1 is sampled from stratum 1, which is 1 . The set 22 is sampled from stratum 2, which is 2 \u2216 1 . The set 33 is sampled from stratum 3, which is 3 \u2216 ( 1 \u222a 2 ). Note that 21 \u2282 1 and 31 \u2282 21 are taken from stratum 1, 32 is taken from stratum 2 ( 2 \u2216 1 ; 32 \u2282 3 \u2229 2 \u2216 1 ), and 34 is drawn from stratum 1 ( 1; 34 \u2282 1 , 34 \u2229 31 = \u2205, 34 \u2282 1 \u2229 3 ). Sampling rates for the simulation will be determined within strata. Table 4 presents cross-sectional weights that would be determined for each survey year. Weighting formulas can differ by strata. Each year a subject is included in the sample it receives a weight. The final column of Table 4 illustrates the goal of a composite or single weight for each subject included in one or more of the samples in survey years 1, 2, and 3. In the simulation, 1 = 600 subjects are randomly sampled (simple random sampling without replacement) in year 1. In year 2, 21 = 400 are randomly sampled from those in 1 and 22 = 200 are selected from the 1000 new members of population 2 . So 2 = 600 as well. In year 3, 31 = 300 are randomly sampled  1 \u223c (97.5 + 10 , 30 2 ) 1 \u223c (8000 + 10 1 + 600 , 140 2 ) 2 \u223c ( 1 + 10 , 30 2 ) 2 \u223c (9000 + 10 2 + 800 , 140 2 ) 3 \u223c ( 2 + 10 , 30 2 ) 3 \u223c (10000 + 10 3 + 1000 , 140 2 ) from those in 21 , 32 = 100 are randomly sampled from those in 22 , 33 = 150 are randomly sampled from the 1000 new members of population 3 , and 34 = 50 are selected from the 200 units selected originally in year 1, but not picked in year 2 ( 34 \u2282 3"}, {"section_title": "\u2229", "text": "( 1 \u2216 21 )). Table 6 illustrates the sample sizes in the simulation study. Survey weights in the simulation are computed as the inverse of the probability of selection within strata. That is, the weight is / where is a relevant population size and is the sample size. Table 7 gives initial survey weights. The lower weights among new additions to the populations in years 2 and 3 reflect oversampling. It remains to give details of how the population variables are to be simulated. A domain of interest will be determined by a variable with = 1 meaning that the subject is a member of the domain and = 0 indicating non membership. Variable is generated as a Bernoulli random variable with probability = 0.25. Auxiliary variables 1 , 2 , and 3 and outcome variables 1 , 2 , and 3 are generated from univariate normal distributions as given in Table 5. Future work simulations could consider a number of modifications. Of interest would be a smaller domain ( < 0.25), correlations between 's, between 's, and between 's and 's that are weaker or stronger, and non normal data."}, {"section_title": "Calibration options", "text": "Step 1 in the calibration procedure is to choose initial weights. For initial weights, four options are being considered: (1) Equal weighting ( / = 10000/800) for elements in = 1 \u222a 22 \u222a 33 . (2) The earliest available weight ( 1 for 1 , 2 for 22 , 3 for 33 ). (3) The average of available weights for each case. (4) The latest available weight ( 3 for 3 , 2 for 2 excluding 3 , 1 for the rest). Step 2 Table 6: Prototype sampling design for prototype scenario for longitudinal weighting: sample sizes for simulation study. Samples in years 1 and 2 are listed multiple times in order to illustrate the relationship to later years."}, {"section_title": "Year", "text": "Year 1 Year 2 Year 3 Population U1 U2 U3 stratum 1 32 , 32 =100 stratum 3 0 0 33 , 33 =150 Total 1 = 600 2 = 600 3 = 600 in the process of calibrating cross-sectional weights to produce a set of longitudinal weights for analysis of data from combined survey years is to identify targets for calibration. Potential targets that could be used singly or in combination include: total estimates in the domain (\u02c6 1 ,\u02c6 2 ,\u02c6 3 ). In the simulation, some combinations of calibration control totals are used. The sets of control totals are (1) A, (2) A and B, (3) A and C, (4) A, B, and C, and 5A through D. Some are known values, such as population sizes, whereas others are estimates themselves. Others, including second moments and interactions among variables, could be possible. A difference between this simulation and application to the actual NSF Survey of Doctorate Recipients, or to any other survey for that matter, is that there could potentially be several domains and auxiliary variables to consider. It is an open question as to how many variables can or should be used in survey weight calibration. In general, calibrating on many variables has the potential to increase variability of resulting weights, which could dramatically increase standard errors for some estimates. Step 3 is to select a calibration method. Only two are being considered in this work: raking and linear regression calibration. Both are implemented in the R package survey, which addresses Step 4. Ash (2005) considered one-step or twostep calibration. Here we consider calibration in a single step. A further option that could be considered in later work is calibration after a logarithmic transformation of the weights. Such as transformation ensures that all weights are positive. Calibration methods receive further comment in the discussion. One of the requirements of the calibrated weights is that the the weight needs to be useful for reproducing key cross-sectional analyses. This is given as both a requirement for consistency and an attempt to produce advantages in estimation via correlations. In addition, it is of interest to examine the impact of weighting on a longitudinal analysis. Estimands and corresponding estimators considered for evaluation are listed below. The last option that is listed below is discussed further in the next section. 1. Means in year : estimation using sample and new weights , = 1, 2, 3. Comparison is made to estimation using sample and weights for sample year , . 2. Domain means in year : estimation using sample \u2229 and new weights , = 1, 2, 3. Comparison is made to estimation using sample \u2229 and weights for sample year , . 3. Change in means: estimation using cases sampled in both years. 4. Change in domain means: estimation using cases sampled in both years. 5. Linear mixed effects model estimate of slope in population : estimation of regression slope using single stage cluster sample."}, {"section_title": "A focal analysis and an associated question about modeling in finite populations", "text": "What analysis would benefit from considering population = 1 \u222a 2 \u222a 3 ? One analysis that would clearly benefit from using subjects sampled in all three years would be a regression of on over the three time periods. The composite population sample should have larger sample size and more observations than any one year sample. If the data were generated from a longitudinal study without reference to a finite population, one likely would use a linear mixed effects model with random effects for individuals. Each individual could have up to three measurements over time. How should such a modeling endeavor be presented in the context of a finite population framework? One possibility is to consider a generalized least squares estimation as in Breidt and Fuller (1999). Within each stratum or subsample, one could estimate the average salary in a given year conditional on certain variables, such as demographics and field of degree. For each estimated average, one then estimates variance based on sample and weights in a given year. One would then estimate the covariance between estimates in pairs of years. An estimate of change is then computed as a function of these totals. The variance of the change estimate is then a function of the estimated variances and covariances. See also Duncan and Kalton (1987), Fuller (1999), andMcDonald (2003) and references therein. Future work will explore recent references and this approach. Another option would be to consider each subject to be a cluster and each cluster to consist of measurements over time. One then could use a cluster analysis variance formula with the svyglm function in the R survey package to estimate a linear model with time and other variables as predictors. Is a linear mixed effects growth model with random effect for subject really equivalent to a single stage cluster sample with each subject being a cluster and the model including a time predictor? In other words, how should one implement repeated measures or growth curve models in a finite population survey context? Future work will examine the correspondence and possible lack thereof between linear mixed effects growth models and cluster sampling with a linear model with time covariates."}, {"section_title": "Simulation Results", "text": ""}, {"section_title": "Simulation Study Implementation", "text": "The simulation study was implemented as follows. The population, sample, weighting, and variable details describe above were utilized. Conduct the following steps = 1, . . . , = 1000 times: 1. Generate a population in years 1, 2, and 3 from the models given above. 2. Select a sample in years 1, 2 and 3 according to the stated sampling scheme. 3. Compute and estimate control totals. 4. For each combination of starting weights and groups of control totals, compute calibration weights using raking. Raking cannot be used when methods A through D are used together due to the interaction between domain size and domain total. 5. For each combination of starting weights and groups of control totals, compute calibration weights using linear regression calibration. All groups of controls can be used with linear regression calibration. 6. Estimate each estimand and its standard error using each set of calibrated weights. The results are summarized by computing the average of estimates, the standard deviation of estimates, the average of estimated standard errors, the standard deviation of estimated standard deviations, and the percent of simulations in which a confidence interval for the estimand covers the true value in the composite population."}, {"section_title": "Summary of Results", "text": "Results are given in summary fashion only in this paper. Future presentations will present numerical and graphical summaries as well as extensive tables. Estimation in general seems to work well, but there is one dominant issue that is being addressed in ongoing work. In short, it appears that it is very important for variance estimation to take into account the fact that some control totals are estimated from survey data. Estimated control totals have their own uncertainty, which needs to be propagated in analyses. Some literature on this topic is reviewed in the discussion section below. It will make more sense to present more extensive results once methods for properly accounting for uncertainty in estimation with calibrated weights is incorporated into analyses. Propagation of uncertainty in another scenario was considered by Lahiri and Larsen (2005). For the calibration methods (raking and linear regression) and the initial weight options (the four listed above) considered, very similar results were obtained. That is, estimates and estimated standard errors differed in a minor amount across the method-weight combinations. There are two differences to mention in comparing raking and linear regression. Raking does not produce negative weights, but it was possible for linear regression calibration to produce negative weights. Negative weights can be used in estimation, but in general they are not desirable. One cannot interpret calibration weights in the same way as one tends to interpret survey design weights or nonresponse adjusted weights; namely, as indicating the portion of the population that the observation associated with the weight represents. Calibration weights are supposed to be close to the initial weights but also satisfy the calibration constraints. The raking option, however, could not handle the full combination of options A-D; the R program ran into problems with the implied interaction among the and domain indicator variables. Negative weights and choices for weight restrictions are mentioned in the discussion. After calibration using data and targets from three years, estimates in a given survey year using the new weights accurately reproduce estimates from a single survey year using the original sampling weights for that year. Estimates of change (1 versus 2, 1 versus 3, or 2 versus 3) also are preserved. When population size and domain size are used as calibration targets, estimated standard errors for yearly totals and change between years are approximately the same as before. When calibration targets include the -variable total or both the overall and the domain -variable totals, standard errors are estimated to be much smaller (e.g., 60-80% of the value) than the original estimated standard errors. In general smaller standard errors is desirable. In this case, however, coverage of the known population values by confidence intervals based on the calibrated standard error estimates is lower than the nominal 95% level (e.g., 70-85% coverage). A reduction in coverage below the nominal level is not desirable. This drop in coverage is discussed in the next section."}, {"section_title": "Section on Survey Research Methods -JSM 2011", "text": "For the focal longitudinal analysis, a mixed effects model was fit to the population data. The fixed effects slope parameter estimates were compared to estimates from the sample data, where estimation was implemented as described above in Section 3.3. The model being estimated has a slope which is multiplied by year (1, 2, or 3). There is a clear benefit to using data from all three survey years instead of data only from year one. Data for subjects collected first in survey year 2 have data in years 2 and 3 or in year 2 only. If they have data in years 2 and 3, then their data is informative about the slope. If they are collected only in one year, then their data is informative about the intercept. The estimate of the slope is nearly the same as with only year one data with a much lower standard error. There is adequate confidence interval coverage when calibrating on population or domain size. Calibration on totals reduces standard error further and reduces coverage a little bit."}, {"section_title": "Discussion", "text": "A critical question is, why was there a drop in confidence interval coverage with calibration on totals? The likely reason is that calibration is being implemented to control weights to a survey estimate rather than to a population total. As mentioned, the survey estimates have their own uncertainty that should be propagated into the standard errors. It is hypothesized that variance estimation with longitudinally calibrated survey weights must take into account the fact that some of the target control values are estimated from the separate surveys rather than based on a known population value. Resampling methods have been considered by some authors in similar contexts. There are replicate weights for the restricted use NSF SDR data; the replicate weights must be requested separately from the usual restricted use data. Dever and Valliant (2010) cite examples of surveys in which researchers have estimated control totals and then used post stratification. Dever and Valliant (2010) then study the estimated-control post stratified estimatior (ECPE). The authors present a linearization variance estimator and three delete-one jackknife variance estimators. The results of their work support the need for development of theory and methods in this area. Elliott et al. (2010) combine samples from two sources in order to improve estimation. In order to combine samples, the authors estimate weights that they refer to as pseudo-weights. In order to incorporate uncertainty due to weight estimation, the authors use a jackknife approach. For each jackknife sample, the authors reestimate the pseudo-weights. In their simulation, the jackknife approach reduces or eliminates undercoverage of 95% confidence intervals. Breidt and Opsomer (2008) study post stratification where the post strata are formed based on an estimated classification function. They call this endogenous post stratification (ESP). Under certain conditions including a fixed number of parameters in the classification model, the authors demonstrate consistency of estimation and a central limit theorem. In simulations, they show scenarios in which the estimated classification aspect of ESP estimation (EPSE) has a small effect. The authors simulate mean squared errors (MSE) under three methods, but do not discuss variance estimation or confidence interval coverage. The calibration ideas were applied to a few variables for three years (1993, 1995, and 1997) from the NSF SDR public use data files. Initial evidence suggests that calibration can create useful longitudinal weights. Weights preserve means and group sizes by year without inflating standard errors much in these preliminary applications. It is anticipated that as more control totals, especially estimated control totals, are added to the calibration targets that methods to properly account for variance will make a bigger difference from naive variance estimation methods. Three further issues can be mentioned for consideration. First, although the populations from different survey years are being combined, there are some underlying true population exclusions. For example, some individuals, such as recent Ph.D. recipients, are not members of the population until they obtain their Ph.D. Analyses might logically exclude some individuals for some relationships due to this fact or due to their leaving the population. Second, a more serious complication is variance estimation. The NSF SDR utilizes Generalized Variance Functions (GVFs) for variance estimation (Jang 2001). GVFs are functional relationships, which, in this case, are specific to the given survey year (by gender and major field). A multi-year analysis will need to consider what to do with the existence of multiple GVFs. An alternative is to use the NSF SDR replicate weights. One then must determine whether it is necessary to calibrate separately each set of replicate weights. This can be considered in the context of variance estimation methodologies such as Dever and Valliant (2010). GVFs were not an issue in the simulation. Only two calibration methods were considered in this work: raking and linear regression calibration. Both were implemented in a single step as opposed to two or more steps (Ash 2005). It is sometimes possible with calibration to also restrict the range of weights. With certain distance functions, calibration equations, and weight restrictions, it is possible that there is no exact solution to the calibration problem. Some programs then seek the solution that minimizes discrepancies from the target controls. Presumably these solutions encounter one or more of the restrictions on weights. Another option that could be considered in later work is calibration after a logarithmic transformation of the weights. Such as transformation is used to ensure all positive weights. It is possible to place bounds on the weights after log transformation as well. Methods for choosing a distance function, a transformation, and weight bounds need to be developed in general and in the problem of longitudinal weighting in particular. Some distance functions and weight bounds were considered in Deville and S\u00e4rndal's (1992) paper. The Research Triangle Institute (RTI 2008) implements a general methodology that enables this form of calibration. Of course, inherent in the use of calibration is the challenge of selecting variables and subgroups to define the control targets."}]