[{"section_title": "Abstract", "text": "Testing the independence between two random variables x and y is an important problem in statistics and machine learning, where the kernel-based tests of independence is focused to address the study of dependence recently. The advantage of the kernel framework rests on its flexibility in choice of kernel. The Hilbert-Schmidt Independence Criterion (HSIC) was shown to be equivalent to a class of tests, where the tests are based on different distance-induced kernel pairs. In this work, we propose to select the optimal kernel pair by considering local alternatives, and evaluate the efficiency using the quadratic time estimator of HSIC. The local alternative offers the advantage that the measure of efficiency do not depend on a particular alternative, and only requires the knowledge of the asymptotic null distribution of the test. We show in our experiments that the proposed strategy results in higher power than other existing kernel selection approaches."}, {"section_title": "Introduction", "text": "Tests of independence have been widely studied in the field of statistics (Bakirov and Szekelyo, 2008; Szekely et al., 2007) , and machine learning (Gretton et al., , 2008 Smola et al., 2007) . In particular, the Hilbert-Schmidt Independence Criterion (HSIC) Gretton et al. ( , 2008 ; Smola et al. (2007) is a kernel-based independence measure, and it is defined as the maximum mean discrepancy (MMD) between the joint mean embedding and the product of marginal embeddings of x and y (Smola et al., 2007) . The advantages of HSIC include consistency against all alternatives (linear or non-linear effects), allowing for variable dimensions of the two input variables, and having the option of using different kernels to represent different assumptions of the underlying variable structures. Recently, HSIC has been shown to be a class of kernel tests (Sejdinovic et al., , 2013 , where the tests are based on different combinations of distance-induced kernel pairs. Based on this finding, a natural extension is to design a strategy that selects the optimal kernel pair from a class of distance-induced kernels.\nOne intuitive method is to select the kernel combination that maximizes the test statistic (Sriperumbudur et al., 2009) , which is equivalent to minimizing the p-value; while this approach achieves the highest power in the case of linear correlation among bivariate samples, the maximum statistic is inflated by the polynomial kernel when the dimensions of the variables are large (Section 5.2). Another method for optimal kernel selection is to compute the test statistic that incorporates all the candidate kernels by a weighted average (Wu et al., 2013) , but it is difficult to interpret the underlying effects of the variables from the presence of all the kernels. In contrast to these methods that are based on empirical or heuristics results, proposed to study the optimality in the scenario of hypothesis testing, where the quadratic time estimator of the HSIC test is reformatted into a linear time estimator that was proved to be Normal distributed asymptotically. Furthemore, formulated the optimality in terms of Type II error prob. under the alternative, given the prob. of Type I error (size) \u03b1 under the null. This approach is elegant but requires a large sample size to achieve the proper power performance.\nIn this work, we propose to use the efficiency as the criteria for the optimal kernel pair selection. For example, if test A requires 200 samples to achieve a certain power while test B only needs 100 samples for the same power, then test B is twice as efficient. According to this, the classical Neyman-Pearson lemma (the uniformly most powerful tests from Ch. 8 in Casella and Berger (1990) ) is considered optimal in the field of parametric tests. For nonparametric approaches such as the test based on HSIC, HodgeLehmann (1956) (Ch. 10 in Serfling (1980) ) studied the asymptotic efficiency that at a fixed alternative, the optimal one has the faster rate of Type II error prob. approaching to zero than any other tests when size is held at a certain level. However, the asymptotic argument is not trivial, since the Type II error prob. of any test approaches to zero as the sample approaches to infinity.\nTherefore, we propose to discuss the efficiency against local alternatives (Pitman efficiency (1949) Ch. 10 in Serfling (1980) ), such that as the sample size grows, the alternatives ever closer to the null can be detected by the test. In other words, the Type II error prob. tends to a positive constant given an size \u03b1. The advantage of local alternatives is that the measure of efficiency obtained does not depend on a particular alternative, it only requires the knowledge of the asymptotic null distribution of the test. Based on the Pitman approach, the optimal choice is the kernel pair that maximizes the power under the local alternative at a given size. We emphasize from the practical point of view that higher powers under \"closer\" alternatives are more important for the optimality. Our proposed method makes three contributions. First, the test statistic is computed based on the quadratic time estimator of HSIC, which requires fewer samples than the linear time estimator of HSIC to achieve the same power. Second, the optimal kernel pair can be used for the interpretation of input variables. Third, our proposed method results in higher power than other existing methods in our experiments."}, {"section_title": "Preliminaries", "text": "In this work, we mainly focus on kernel pair selection in the test of independence using kernels in Hilbert spaces, and distance measures in Euclidean spaces. Therefore, we briefly review some background and definitions of kernels and distance measures. Definition 1. Inner product Let H be a vector space over R. A function \u00b7, \u00b7 : H \u00d7 H \u2192 R is said to be an inner product H if\n. f, f \u2265 0, and f, f = 0 if and only if f = 0 A complete inner product space is called a Hilbert space.\nDefinition 2. Kernels Let Z be a non-empty set. A function k : Z \u00d7 Z \u2192 R is a kernel if there exists an R-Hilbert space and a mapping \u03c6 : Z \u2192 H such that\nIf we have a function with two arguments, how can we determine if it is a valid kernel? We can find a feature mapping and check the conditions defined in (1); however, the feature mapping is not unique. Therefore, the direct property of kernel function is to check the positive definiteness.\n, then the function k is strictly positive definite. From definition 3, we know that every kernel is a positive definite function. However if a function of two arguments is positive definite, can we conclude that it is a valid kernel? We need to have the following property.\nIf H has a reproducing kernel, it is called a reproducing kernel Hilbert space (RKHS).\nTherefore, if a function k(z, z \u2032 ) of two arguments is symmetric and positive definite, then it is a valid unique reproducing kernel defined (Moore-Aronszaji theorem).\nWe then provide some reviews of distance measure defined on semi-metric spaces of negative type.\nDefinition 5. Semi-metric space Let Z be a non-empty set and let \u03c1 :\nThen (Z, \u03c1) is a semimetric space and \u03c1 is a semimetric on Z( not enforced triangle inequality).\nIf triangle inequality is held, then (Z, \u03c1) is a metric space.\nDefinition 6. Negative type The semimetric space (Z, \u03c1) is said to have negative type if \u2200n 2, z 1 , ..., z n \u2208 Z, and \u03b1 1 , ..., \u03b1 n \u2208 R, with\nNotice that all Euclidean spaces are of negative type.\n3 Hilbert Schmidt Independence Criterion 3.1 HSIC for a class of distance-induced kernels\nThe goal of our work is to provide a kernel combination selection strategy on an unifying class for test of independence, where the class members are and HSIC Gretton et al. ( ,?, 2008 ; Smola et al. (2007) and distance covariance Bakirov and Szekelyo (2008) ; Szekely et al. (2007) . It has been shown that distance covariance is actually a member of the family of HSIC methods when it is discussed under the semimetric space of negative type Sejdinovic et al. (2012 Sejdinovic et al. ( , 2013 . We now introduce HSIC on a family of distanceinduced kernels K \u03c1 based on Sejdinovic et al. (2012 Sejdinovic et al. ( , 2013 :\nwhere Z is a nonempty set with z 0 \u2208 Z and \u03c1 is a measure of negative type on a semietric space Z, such that the distance-induced kernel k \u2208 K \u03c1 is generated by a semimetric \u03c1 and centered at z 0 . Let (X \u2286 R p , \u03c1 x ) and (Y \u2286 R q , \u03c1 y ) be semimetric spaces of negative type, and\n\u2208 K \u03c1x on X with respect to a reproducing kernel Hilbert space (RKHS) F and an associated feature mapping \u03c6(x) \u2208 F . Likewise, l(y, y \u2032 ) = \u03c8(y), \u03c8(y \u2032 ) is another distance-induced kernel that generates \u03c1 y on Y with an associated RKHS G and a feature mapping \u03c8(y) \u2208 G. The HSIC can be measured as:\nwhere x \u2032 and y \u2032 are the independent copies of x and y respectively. The first equation is the MMD between the joint mean embedding \u00b5 Pxy and the product of marginal embeddings \u00b5 PxPy that determines whether two random variables x and y are independent Smola et al. (2007) ."}, {"section_title": "Empirical estimate of HSIC and asymptotic distributions", "text": "For the test of independence, the empirical estimate of HSIC(P xy , F , G) for the random samples (x, y) = (x 1 , y 1 ), ..., (x n , y n ) is as follows. Define\nThen,\nwhere K is a n \u00d7 n matrix with entries k ij = k(x i , x j ), L is another n \u00d7 n matrix with entries l ij = l(y i , y j ), and H = I \u2212 1 n 11 T . Notice that Gretton et al. (2008) ; Szekely et al. (2007) showed the asymptotic distribution of HSIC test under H 0 : P xy = P x P y :\nwhere S 2 is defined in (4), and Q follows a weighted sum of independent Chi-square variables with E(Q) = 1 (Szekely et al., 2007) . Gretton et al. (2008) also discussed that the empirical estimator of HSIC follows an asymptotic Gaussian distribution under the alternative hypothesis (P xy = P x P y ). As a result, the asymptotic distribution between the null and the alternative are different, it is hard to evaluate the power under these difference. Therefore, we propose to look at the local alternative surrounding the null hypothesis.\n1 As defined in Sejdinovic et al. (2013) , let \u03bd \u2208 M(Z) be a finite \u03b8-moment w.r.t. a semimetric \u03c1 on Z of negative type:\n4 Hypothesis testing, local alternatives and optimal kernel pair selection\nWith the representation of HSIC in MMD format for test of independence, one can convert the hypothesis from H 0 : P xy = P x P y versus H 1 : P xy = P x P y into H 0 : \u00b5 Pxy = \u00b5 PxPy versus H 1 : \u00b5 Pxy = \u00b5 PxPy + \u2206, where \u2206 = 0 is defined in Hilbert space such that \u00b5 PxPy + \u2206 is still a valid mean embedding measure. Given the observation data (x, y), hypothesis testing assesses the compatibility of the data with the null hypothesis. Since there are exactly two possible outcomes (reject, or fail to reject H 0 ), the false inferences can therefore only be made in two way. Type I errors happen when the null hypothesis is incorrectly rejected, and Type II errors occur when it is incorrectly accepted. For any test, we call \u03b1 the size of the test which is the prob. of Type I error, and denote \u03b2 as the prob. of Type II error. The power of a test is the prob. of rejecting H 0 when it is false, which is 1 \u2212 \u03b2. In practice, the exact size and power are not computable through the tests because the test distributions are often unknown. In the case of HSIC test, we consider the asymptotic performances instead; let r n be a sequence of critical values associated to the HSIC test (5) with sample size n, , and the limiting size and power of the HSIC test are:\nA kernel pair of HSIC test can be treated as an individual test of independence, while the different kernel pairs are the independence tests for the same H 0 . A general way to compare the tests is to select the most efficient test, where the test with the highest power given an upper bound of \u03b1 is desired. In this work, we consider the local alternatives that surround the null, i.e.,\nfor measuring efficiency. The claim of the distribution of the HSIC test under local alternatives is provided by the following statement:\nTheorem 1. If \u2206 n can be expressed as cn \u22121/2 , where c is an arbitrary constant, then the power function of the HSIC test (5) is independent of sample size n, but proportional to c 2 .\nProof: We first discuss the case that under the null hypothesis. Let r n are sequence critical values and given an asymptotic size \u03b1, the limiting size:\nHere, lim n r n = t \u03b1 , where t \u03b1 is a threshold associated to distribution Q at level \u03b1, where Q is defined in (5). The power function of HSIC test under local alternatives is computed based on B2 in that the empirical HSIC is as follows:\nThe first, second, and third terms of (7) are centered by their mean embeddings, which are:\nTherefore, the empirical centered HSIC, denoted as cHSIC n , is then:\nWith the statement of H local , \u00b5 Pxy = \u00b5 PxPy + \u2206 n ; the difference between HSIC n and cHSIC n is\nTherefore, as n goes to infinity, n \u00d7 cHSIC n (x, y)S \u22121 2 \u2212\u2192 D Q under the null hypothesis. Also, by SLLN, S 2 \u2192 E(S 2 ) almost surely, and given \u2206 n = cn \u22121/2 , we obtain nS\n2 \u2243 c 2 , therefore, \u2206 n 2 dominates the second and third terms of (8) in terms of c 2 . So, the local limiting power function given the threshold t \u03b1 0 at a size \u03b1:\nwhere (9) is independent of n, and increases as c increased.\nNotice that the local limiting power function \u03c0(c) = P (Q + c 2 > t \u03b1 ) of HSIC test is independent of the sample size n but proportional to c. Therefore, we are able to select the optimal kernel combination according to the maximum local limiting power \u03c0(c), by maximizing the offset c 2 . Furthermore, c 2 can be decomposed into sample size n and the difference \u03b4 2 :\nSince n is fixed when we evaluate the power among the tests, the optimal kernel combination can be determined by maximizing \u03b4 2 :\nwhere \u03b4 2 k,l is defined in (10) given a kernel pair (k, l). In practice, we do not know the true difference \u03b4 2 , we therefore use the empirical estimates for \u03b4\nand the empirical\u03b4 2 k,l is defined a\u015d\n2 , where HSIC n (x, y|H local )S \u22121 2 and HSIC n (x, y|H 0 )S \u22121 2 are the empirical estimates of \u00b5 Pxy \u2212 \u00b5 PxPy \u2212 cn \u22121/2 2 \u00d7 \u00b5 PxPy \u22122 and \u00b5 Pxy \u2212 \u00b5 PxPy 2 \u00d7 \u00b5 PxPy \u22122 , respectively. The following theorem shows the convergence of (k\nTheorem 2. Consider the finite kernel combinations (k, l)'s that are in a class of distance-induced kernels K in (2), both k and l are therefore bounded. If E(S 2 ) = \u00b5 PxPy 2 is bounded away from zero, then sup k,l\u2208K\u03b4\nProof: We start with the bounded difference:\nWe first focus on the bounded difference under H local , and the bounded difference can be expanded as:\nThe first equation above is based on (8), and the second equation is using triangle inequality. The proof of uniform convergence of means involves three steps: a concentration inequality, symmetrization and simple restrictions. In (12), the difference of (a) is first concentrated by McDiarmid inequality McDiarmid (1989) , where an (x i , y i ) pair changes the quality of (a) by no more thanDn \u22122 , therefore, with probability 1 \u2212 \u03be:\nSecond, we symmetrize by replace \u00b5 Pxy \u2212 \u00b5 PxPy 2 by cHSIC n (x \u2032 , y \u2032 ), where (x \u2032 , y \u2032 ) are the i.i.d. copies of (x, y), and cHSIC n (x, y) = n ijk (x i , x j )l(y i , y j )/n 2 . In particular,\ngiven the observed sample (x, y)\n, we have\nis defined as Rademacher process, and E R n K is a Rademacher complexity of K, which leads us to consider the simple restrictions over data. Consider,\nTherefore, E R n K = E(R n ) \u2264 4Dn \u22121 . Hence, it results in our main conclusion of part (a) that with the probability at least 1 \u2212 \u03be,\nThe McDiarmid inequality is first applied on part (b) in (12), then the supremum of the empirical process of (b) is concentrated to its expectation, and the expectation has the same rate as the expected supremum of of the Rademacher processes with the probability of at least 1 \u2212 \u03be:\nThe last equation is based on Lemma 22 Bartlett and Mendelson (2003) , where \u2206 n = cn \u22121/2 , c \u2208 R is a constant. Similarly, part (c) in (12) is bounded by\nln 2 \u03be with probability 1 \u2212 \u03be. Therefore, (b) and (c) in (12) are both bounded in probability with the rates n \u22121 . Putting them all together, sup k,l\u2208K\nThe bounded difference between HSIC n (x, y|H 0 ) and its expectation is similar to (13), because the \u2206 n under the null is zero, therefore, we can focus on part (a) in (12), which is (13). The only difference here is that under the null hypothesis, the expectation of HSIC n (x, y|H 0 ) equals to zero. Therefore, with the probability of at least 1 \u2212 \u03be, we have:\nHence,\nIn summary, according to SLLN, S 2 converges to E(S 2 ) almost surely, therefore, S 2 is bounded. \u03be 1 and \u03be 2 are the constants for the boundedness of E(S 2 )/S 2 E(S 2 ) and (E(HSIC n (x, y|H local ) \u2212 HSIC n (x, y|H 0 )))/S 2 E(S 2 ). In addition, (iii) is one of the component of (i) and (ii), and (i) and (ii) are both bounded in probability with the rate n \u22121 by McDiarmid's inequality McDiarmid (1989) , and Rademacher complexity bound in Bartlett and Mendelson (2003) . Therefore, sup k,l\u2208K\u03b4\n, and (k * ,l * ) converges to (k * , l * ) in probability, where (k * ,l * ) and (k * , l * ) are the corresponding optimizers of sup k,l\u2208K\u03b4 2 k,l and sup k,l\u2208K \u03b4 2 k,l ."}, {"section_title": "Local-alternative Power Maximization (LaPM) Algorithm", "text": "We have so far described maximizing power for local alternatives as a strategy for the optimal kernel combination (k * , l * ), which is to consider the maximum difference \u03b4 2 k,l among different input kernel pairs. Here, we adopt permutation to estimate the difference, by subtracting the average T n 's in (5) over the permuted samples from the T n calculated on original samples, where we assume that the T n computed on the original data is from the alternative. We denote our proposed algorithm as Local-alternative Power Maximization (LaPM ), and is detailed in the following: Initialize: Consider a class of distance-induced kernels K in (2), we evenly split the 2n data into training and test points, where the training samples are independent of the test samples.\n1. Compute T n (x, y|k, l) in (5) for each kernel pair (k, l \u2208 K) using the training set. 2. The indices of y are randomly permuted b times, denoted as\u1ef9 i ; i = 1, .., b, for a given pair of kernel (k, l), i.e., 3. (11) is then estimated by T n (x, y|k, l) (in step 1) minus "}, {"section_title": "Two modified existing methods", "text": "For our experiment in Section 4, two existing methods are included for the performance evaluation Sriperumbudur et al. (2009); , where their original discussions were focused on two samples test using MMD. For our evaluations, we modify their algorithms into the settings for test of independence.\nFirst, we present the work of Sriperumbudur et al. (2009) for kernel combination choice for test of independence, where Sriperumbudur et al. (2009) selects the maximum statistic (minimum p-value) as the optimal kernel pair, and we denote the modified version as the max-HSIC algorithm: Initialize: Consider a class of distance-induced kernels K. We evenly split the 2n data into the training samples and test samples.\n1. Compute T n (x, y|k, l) in (5) for each kernel pair (k, l \u2208 K) using the training samples. 2. Select the (k * ,l * ) \u2208 K that maximizes T n (x, y|k, l) from training samples. 3. Apply the kernelsk * , andl * on the test samples, compute\u0164 n (x, y|k * ,l * ) and then permute y's indices B times over the test samples for evaluating test significance. The second method to be evaluated in our experiments is the work of . In order to modify it for test of independence, we first present the linear time estimate of HSIC, and then discuss its asymptotic distribution for hypothesis testing.\nis a linear unbiased estimator for HSIC(P x,y , F , G), where\n4 ), and f 2 (y 1 , y 2 , y 3 , y 4 ) := l(y 1 , y 2 ) \u2212 l(y 1 , y 3 ) \u2212 l(y 2 , y 4 ) + l(y 3 , y 4 )."}, {"section_title": "Proof:", "text": "The proof is according to Lyons et al. (2013) . Let (X , \u03c1 x ) be a semimetric space of negative type, and (Y, \u03c1 y ) is another semimetric space of negative type, and let x \u223c P x \u2208 M 2 \u03c1x (X ) and y \u223c P y \u2208 M 2 \u03c1y (Y) with a joint distribution P xy \u2208 M 2 \u03c1x\u03c1y (X \u00d7 Y). k and l are two kernels on X and Y that are induced by \u03c1 x and \u03c1 y . Now, consider (x i , y i ); i = 1, .., 4 are the random samples of P x,y . By triangle inequality,\n|f 2 (y 1 , y 2 , y 3 , y 4 )| \u2264 g 2 (y 2 , y 4 , y 3 ) := 2 max{l(y 2 , y 4 ), l(y 4 , y 3 )}, where g 1 (x 1 , x 3 , x 4 ) and g 2 (y 2 , y 4 , y 3 ) are integrable. By Fubini's theorem, the expectation of h equals to HSIC(P x,y , F , G), i.e., E(h) = HSIC(P x,y , F , G) . Therefore, lHSIC n (x, y|k, l) is an unbiased estimator and can be computed in linear time.\nSimilar to , the asymptotic distribution of lHSIC n can be derived by Central Limit Theorem, under both null and alternative hypotheses:\n, under the assumption of 0 < E(h 2 ) < \u221e. Therefore, the test based on the linear estimator (14) of asymptotic level \u03b1 has the threshold t \u03b1|k,l = n \u22121/2 \u03c3 k,l \u03a6 \u22121 (1 \u2212 \u03b1), where \u03a6 \u22121 is the inverse CDF of standard normal. The limiting power under H 1 : HSIC(P x,y , F , G) > 0 is\n(15) decreases as the ratio HSIC(P x,y , F , G)\u03c3\nk,l increases, since \u03a6 is a monotone function. Consequently, the optimal kernel combination (k * , l * ) can be determined by maximizing the ratio HSIC(P x,y , F , G)\u03c3 \u22121 k,l over the class K given t \u03b1|k * ,l * . However, the true parameters are not known with finite samples, therefore, we use the sample empirical estimators given t \u03b1|k * ,l * for the optimality, such that\nFinally, we denote the modified algorithm based on as the maxratio algorithm: Initialize: Consider a class of distance-induced kernels K. Evenly split the 2n data into the training samples and test samples. 1. Compute the empirical linear estimator lHSIC n , and\u03c3 under the training samples. 2. Plug lHSIC and\u03c3 into (16) to select the (k * ,l * ), and t \u03b1|k * ,l * . 3. Apply the kernel (k * ,l * ) on the test samples and compute\u013eHSIC n , and\u03c3. 4. Evaluate the significance of ( \u221a n \u00d7\u013eHSIC n )/(2\u03c3) based on standard normal."}, {"section_title": "Experiments and results", "text": "We used b = 100 for LaPM, B = 10"}, {"section_title": "Toy example simulation", "text": "The first simulation was designed based on the correlations between bivariate samples under the local alternatives Anderson et al. (1994) , such that the (x, y) samples were generated from a Bivariate normal (BVN) with mean zero and a 2\u00d72 covariance matrix \u03a3, where the diagonals are \u03c3 2 and the off-diagonals are cn \u22121/2 . We compared our proposed LaPM method with max-HSIC, and max-ratio with distance-induced kernels of class K that included L 2 distance, linear, quadratic, and Gaussian RBF kernels with \u03c1 = 0.1, 0.5, 1, 5, 10 for both k and l, therefore making the cardinality |K| = 8. We also included a baseline method of Pearson's correlation coefficient test (\u03c1-test) in this evaluation. Figure 1 : the left panel displays the scatter plots of 100 (upper 4) and 500 (lower 4) test samples for c = 0, 2, 4, 6 and \u03c3 2 = 1. The plots to the right show the empirical sizes and powers under n = 100 (top row), and n = 500 (bottom row) for c = 0, 2, 4, 6, and \u03c3 2 = 1, 2, 4. Figure 1 displays the simulation results. The left panel shows the scatter plots of four different strength c \u2032 s, and \u03c3 2 = 1 for 100 and 500 test samples, respectively. We can observe the positive correlations between x \u2032 s and y \u2032 s when n = 100, c = 6, and \u03c3 2 = 1, but the correlation became weaker when n = 500, c = 6, and \u03c3 2 = 1, this is because of the design of simulation is based on local alternatives. The right part of Figure 1 shows that the empirical size of all approaches were all close to size 0.05 when c = 0, which suggests all methods are able to control Type I errors, and the empirical powers (c > 0) all decreased when the \u03c3 2 increased. In addition, the increase in sample size did not affect the empirical powers much due to the local alternative effects of our simulation setting. Of the evaluated approaches, both max-HSIC and LaPM were very close to the baseline approach of \u03c1-test; but, the max-ratio that was based on the linear estimator strategy showed less power on the correlation detection. This reflected the fact that max-HSIC and LaPM are computed based on n samples, while max-ratio is calculated using n/4 samples."}, {"section_title": "Simulation based on Alzheimer's Disease Neuroimaging", "text": "Initiative study\nIn this experiment, we consider associations between x and y in a more higher-dimensional setting. The simulation design was based on the Alzheimer's Disease Neuroimaging Initiative (ADNI) study dataset ADNI (2003) . ADNI dataset contains brain magnetic resonance imaging (MRI) scans of the enrolled subjects, where the goal of the ADNI study is to find the correlation between the genetic variants (in terms of single nucleotide polymorphisms (SNPs)) and the changes in brain volume (in terms of brain MRI scans). We utilized the same simulation design as the work in Hua and Ghosh (2014) , where a linear model y r = h(x) + \u01eb r with r = 1, ..., q was used to associate the phenotypes (y = (y 1 , ..., y q )) and genotypes (x = (x 1 , ..., x p )). The structure of the responses (y 1 , ..., y q ) was modelled using a covariance matrix\u03a3 based on the eight (q = 8) positive correlated frontal cortex regions using the 358 mild cognitive impairment (MCI) subjects (Figure 2a ), since the MRI scans of the MCI samples are relatively more uniform than both the healthy and disease groups Vounou et al. (2010) . Therefore, the multivariate responses were according to all \u01eb's that were generated from multivariate normal MVN(0,\u03a3). For generating the predictors (i.e., the genotype effects), 141 SNPs on gene FLJ16124 were used for the genotype elements, i.e. x = (x 1 , ..., x 141 ), from Hua and Ghosh (2014) . The effect of h(x) = k(x, x \u2032 ) was defined as h(x 1 , ..., x 141 ) = c \u00d7 h 1 , while only the first 5 SNPs, (x 1 , ..., x 5 ) of 141 x's were the causative SNPs, such that h 1 (x 1 , ..., x 5 ) = 2 cos(x 1 ) \u2212 3x 2 2 + 2 exp(\u2212x 3 )x 4 \u2212 1.6 sin(x 5 ) cos(x 3 ) + 4x 1 x 5 , and c was the association strength, such that c = 0, 2, 4, 6.\n200 and 500 (2n) samples were generated, and the empirical size (c = 0) and powers (c = 0.05, 0.1) were computed based on 1000 runs and the significance level of 0.05. The kernel choice for k included L 2 distance, linear, quadratic and identical by state (IBS) Wessel and Schork (2006) ; the kernel choice for l were L 2 distance, linear, quadratic, and Gaussian RBF with \u03c1 \u2208 0.1, 0.5, 1, 5, 10, where |K| = 22. In addition, we added another kernel selection method called the perturbation method from Wu et al. (2013) , denoted as the avg method in this experiment, which analyzes the association based the single phenotype kernel machine regression (KMR) model Liu et al. (2007) , and uses a weighted average among the multiple candidate kernels for the optimal selection. The advantage of the avg method lies in its ability to incorporate all samples for kernel selection, while LaPM, max-HSIC, and max-ratio all require the data to be split into training and test sets. To evaluate the avg method under this simulation, we used the first principal component of the eight phenotypes as the single phenotype for KMR, and included all four kernels for the genotype effects. Furthermore, we evaluated the avg method using both the entire sample size, as well as the second half only setting (corresponding to test samples), denoted as avg w and avg h . Figure 2b shows the frequency counts of the ADNI simulation for K, where the max-HSIC only chose the kernel pairs (k, l)=(quadratic,linear) or (quadratic,RBF with \u03c1 = 10) for all strengthes of associations c = 0.0, 0.05, 0.1, which was due to the quadratic effects of high dimensional variables (141 SNPs) from the quadratic kernel; furthermore, since max-HSIC only selects the largest statistic among the candidate kernel pairs, therefore (quadratic,linear) and (quadratic,RBF with \u03c1 = 10) were the two combinations se- lected based on such largest-statistic strategy (max-HSIC). In the contrary, our proposed method (LaPM) demonstrated more uniformity in the kernel selection when there was no association, and selecting the appropriate kernel pairs in (k * , l * ) = (quadratic,linear) as the associations strength increased. Due to the lack of empirical power, we skipped the frequency count of max-ratio, and in fact the frequency distributions of max-ratio are relatively uniform among all kernel pairs in the class K.\nIn summary, the (quadratic,linear) kernel combination from our proposed method achieved a higher power than max-HSIC. This suggest that there exists strong pairwise interaction effects among the SNPs in FLJ16124 that associate with eight brain regions. Table 1 shows the empirical size and powers for the max-HSIC, LaPM, max-ratio, and avgs results, and the values of empirical size of max-HSIC, LaPM and max-ratio were all close to \u03b1 = 0.05, which suggests that all approaches were able to control Type I error \u03b1. However, the values of empirical size of avg h and avg w were less than 0.05, which suggests the test of avg approach is on the conservative side. For the empirical powers on the test samples, LaPM achieved the highest power with avg h coming in second, followed by max-HSIC and then max-ratio: the performance of the avg h method can be affected by bad kernels due to its averaging process; the max-HSIC's largest-statistic strategy were over-influenced by the quadratic kernel's inflation effect; and the maxratio required more samples in the linear estimator. Finally, avg w achieved the highest empirical power but required more samples (the entire sample size). "}, {"section_title": "Real ADNI study", "text": "We also applied the proposed LaPM approach on the real ADNI samples to find the optimal kernel pair and the associations between the genetic variants and multivariate brain MRI scans. In contrast to the previous simulation setup, we utilized all 741 subjects of the ADNI study, using 141 SNPs within gene FLJ16124 as the predictors, and 119 ROIs as the responses. To select the optimal kernel, we randomly split the 741 subjects into 400 training samples and 341 test samples, and we set b = 100, B = 10 4 for computing p-value where the significance level was 0.05. The same distance-induced kernels class K (22 kernel pairs) discussed in the simulation was again considered here. Table 2 displays the top five kernel pairs selected by the LaPM method, the optimal kernel pair was the (L 2 distance,L 2 distance), and the p-value was 0.0239. The result is consistent to the findings from Hua and Ghosh (2014) , that the SNP located on gene FLJ16124 shows the strongest association to brain MRI regions in the neuroimaging genomewide association study. In addition, the second to fifth kernel pairs in Table 2 were insignificant because their p-values were all greater than 0.05, one possible reason would be the number of test samples are too small. Furthermore, the p-value of avg w was 0.125, where we used the entire 741 subjects, applied first PC for the phenotype, and considered the linear, quadratic and IBS kernels for 141 genotype effects. "}, {"section_title": "Conclusion and discussion", "text": "In this work, we have introduced a strategy to select the optimal kernel combination for test of independence against local alternatives. Our proposed method using the quadratic time estimator of HSIC achieved the highest power when compared to other existing methods in our experiments, and the optimal kernel pair selected from our proposed LaPM approach could potentially explain the underlying structure of input variables. Furthermore, we adopted permutations to evaluate the test power when the asymptotic null distribution is not a Normal or Chi-Square distribution. The permutation-based approach was utilized to approximate the asymptotic null distribution in this work, and while the study of asymptotic distribution is worthy of further studies, we leave it for future work as it is beyond the scope of the current article. Finally, based on our simulations, the simple kernel pair (i.e., (k * , l * )=(linear,linear) and (quadratic,linear)) outperformed other candidate kernel combinations, which suggests the importance of further explorations in the kernel structures by examining an objective function combined with kernel complexity penalization."}]