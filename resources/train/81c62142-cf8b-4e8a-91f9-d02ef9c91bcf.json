[{"section_title": "Abstract", "text": "Abstract-In this paper, a new feature-based approach to automated image-to-image registration is presented. The characteristic of this approach is that it combines an invariantmoment shape descriptor with improved chain-code matching to establish correspondences between the potentially matched regions detected from the two images. It is robust in that it overcomes the difficulties of control-point correspondence by matching the images both in the feature space, using the principle of minimum distance classifier (based on the combined criteria), and sequentially in the image space, using the rule of root meansquare error (RMSE). In image segmentation, the performance of the Laplacian of Gaussian operators is improved by introducing a new algorithm called thin and robust zero crossing. After the detected edge points are refined and sorted, regions are defined. Region correspondences are then performed by an image-matching algorithm developed in this research. The centers of gravity are then extracted from the matched regions and are used as control points. Transformation parameters are estimated based on the final matched control-point pairs. The algorithm proposed is automated, robust, and of significant value in an operational context. Experimental results using multitemporal Landsat TM imagery are presented."}, {"section_title": "I. INTRODUCTION", "text": ""}, {"section_title": "I", "text": "MAGE registration is the process of spatially matching two images so that corresponding pixels in the two images correspond to the same physical region of the scene being imaged. Image registration is an inevitable problem arising in many image-processing applications whenever two or more images of the same scene have to be compared pixel by pixel. These applications include the following.\nMultisource Data Fusion: Fusion of multisource data usually requires establishing pixel-to-pixel correspondence [1] .\nTime-Varying Image Analysis for Changes: Change analysis requires reliable and accurate multitemporal image registration. Most existing change detection algorithms depend considerably on reasonably accurate image registration to be workable [2] .\nManuscript received March 25, 1998 ; revised December 11, 1998 . This work was supported in part by the Coastal Change Analysis Program (C-CAP) of the National Oceanic and Atmospheric Administration (NOAA), funded through the North Carolina Sea Grant Program and by SGI/Cray Research, Inc., St. Louis, MO.\nThe authors are with the Center for Earth Observation, North Carolina State University, Raleigh, NC 27695-7106 USA.\nPublisher Item Identifier S 0196-2892(99)06150-1.\nImage Mosaicking: In many regional and/or global remote sensing applications involving multiple scenes, a single coverage is needed, and individual satellite scenes must be mosaicked for further processing [3] .\nMotion Detection and Object Recognition: Image registration also is a required process in other examples such as virtual reality construction, stereo matching and generation, image composite generation, and object height estimation [4] .\nExisting image registration techniques fall into two broad categories: manual registration and automated registration. Manual registration techniques commonly have been used in practical applications. In these methods, ground control points (GCP's) are located visually on both the sensed image and the reference image by choosing special and easily recognizable points such as road intersections. By doing so, the steps of feature identification and matching are done simultaneously by human operators. In order to get reasonably good registration results, a large number of control points must be selected across the whole image [4] . This is very tedious, laborintensive, and repetitive work, especially when multiple widearea scenes such as thematic mapper (TM) scenes (over 6000 6000 pixels for each TM scene), are involved in a regional and/or global change-detection implementation. These techniques also are subject to the problems of inconsistency, limited accuracy, and, in many instances, lack of availability of reference data for locating GCP's. Therefore, there is a critical need to develop an automated technique that requires little or no operator supervision to coregister multitemporal and/or multisensor images when higher accuracy is desired and image analysis is subject to time constraints [5] .\nThe current automated registration techniques can be classified into two broad categories: area-based and feature-based techniques. In the area-based algorithms, a small window of points in the sensed image is compared statistically with windows of the same size in the reference image [6] . Window correspondence is based on the similarity measure between two given windows. The measure of similarity is usually the normalized cross correlation. Other useful similarity measures are the correlation coefficient and the sequential-similarity detection [7] . Area-based techniques can be implemented by the Fourier transform using the fast Fourier transform (FFT) [8] . The centers of the matched windows then are used as control points to solve for the transformation parameters between the two images. Although a novel approach has been proposed by [9] to register images with large misalignment by correcting the image rotation first and then establishing correspondences, a majority of the area-based methods have the limitation of registering only images with small misalignment, and therefore, the images must be roughly aligned with each other initially. The correlation measures become unreliable when the images have multiple modalities and the gray-level characteristics vary (e.g., TM and synthetic aperture radar (SAR) data).\nIn contrast, the feature-based methods are more robust and more suitable in these cases. There are two critical procedures generally involved in the feature-based techniques: feature extraction and feature correspondence. A variety of imagesegmentation techniques have been used for extraction of edge and boundary features, such as the Canny operator in [10] and [11] , the Laplacian of Gaussian (LoG) operator in [12] and [13] , the thresholding technique in [14] , the classification method in [15] , and the region growing in [16] . Images are represented by a set of features after extraction either in the spatial domain [17] , [18] or in the transform domain [20] - [22] . Spatial features usually include edges, boundaries, road intersections, and special structures. In the transform domain, a set of transform coefficients represent the images. Feature representations, which are invariant to scaling, rotation, and translation, are desirable in the matching process. The general feature representations are chain code, moment invariants, Fourier descriptors, shape signatures, centroidal profiles, and shape matrices [23] . Centers of gravity of closed boundaries generally have been used as control points [14] , [16] , [17] . Other points, such as salient points in [6] and locations of maximum curvature in [12] , also have been used as control points. Feature correspondence is performed based on the characteristics of the features detected. A robust algorithm to establish control-point correspondences is one of the most important tasks in automated image registration. Existing feature-matching algorithms include binary correlation, distance transform and Chamfer matching, dynamic programming, structural matching, chain-code correlation, distance of invariant moments, and relaxation algorithms [16] , [23] . In most existing feature-based techniques, feature correspondence is still the most challenging problem. This paper proposes a new feature-based approach to automated registration of remotely sensed images of the same scene using combined criteria of invariant-moment distance and chain-code matching, as shown in Fig. 1 . The critical elements for an automated image registration procedure are explored. These elements include feature identification, feature matching, and transformation parameter estimation. Special attention has been paid to feature extraction by image segmentation and control-point correspondence by region matching. In image segmentation, the performance of the LoG zerocrossing edge detector is improved by developing a selective zero-crossing scheme and defining an edge strength (ES) array. Images are segmented by the improved LoG edgeextraction technique. In region matching, a new method for the determination of corresponding regions is proposed using combined criteria of invariant-moment distance and chaincode matching between the detected regions. Each region is represented by affine invariant moments and improved chain codes that are affine-invariant features describing the shape of a region. Region matching then is implemented in the feature space and is implemented sequentially in the image space. In the feature space, minimum distance classification is used to identify the most robust control points for initial image transformation. In the image space, region-to-region correspondence is established by the root mean-square error (RMSE) rule.\nThe rest of the paper is organized as follows. In Section II, the image segmentation technique developed in this work is described. In Section III, we focus on the image-matching algorithm proposed to achieve a robust scheme for controlpoint correspondence. The experimental results are presented in Section IV. The final conclusions are included in Section V."}, {"section_title": "II. FEATURE EXTRACTION: IMAGE SEGMENTATION", "text": "Image segmentation for the purpose of obtaining boundaries is an important step in a feature-based registration system. There are many segmentation techniques available that could be used. However, there is no unique segmentation technique that can perform best on all types of images, and most segmentation techniques are image-dependent [14] . In this paper, we use a technique that can produce a number of desirable boundaries to be used in the control-point finding process. The Laplacian of Gaussian (LoG) operator combines smoothing and second-derivative operations and produces closed-edge contours. Based on the image obtained by convolving the original image with the LoG, edges are found at zero-crossing points. Therefore, the LoG zero-crossing operator is deemed appropriate in this study. The LoG filter is well understood, and there are abundant references on the technique itself and its implementations (see [6] , [10] , and [23] ). However, determination of the zero crossings is not without problems.\nConventionally, the resultant convolved image is scanned to detect pixels that have a zero value or pixels at which a change of sign has occurred. These pixels are marked as edge pixels. To avoid noisy pixels, a threshold is used to cut the edge pixels that have small convolution values [23] . However, this method causes discontinuity at the weak edge pixels. Simple zero-crossing patterns, which are composed of signs of pixel values of the filtered image, are detected along both vertical and horizontal directions in [6] . The authors then used a twothreshold method based on the ES at each zero-crossing point. This was used to refine the edge points and assure continuities at weak edge points. A method that conceptually corresponds to intersecting the convolution surface with a horizontal plane at convolution value zero was introduced in [12] . This assured not only closed contours but also correct connections atjunctions. Starting with a seed pixel, its neighboring pixels were expanded until a sign change occurred. Pixels on the boundary were flagged as zero crossings. These methods have two general drawbacks when we use the edge points for the purpose of image registration: thick edges and discontinuity at weak edge points.\nTo overcome these drawbacks, we propose a method, called Thin and Robust Zero-Crossing, which produces thin edges and assures continuities at weak edge points as well. This technique can be performed in two steps: selective zero-crossing point marking and edge refinement and sorting. At the first stage, we mark as an edge point every pixel that satisfies the following three conditions.\n1) The pixel is a zero-crossing point (sign change on the convolved image).\n2) The pixel lies in the direction of the steepest gradient change.\n3) The pixel is the closest pixel to the virtual zero plane of the LoG image among its eight neighbors. Equivalently, if a pixel is a zero-crossing point and its LoG value satisfies the following condition:\nwhere LoG is the LoG value of the pixel neighborhood , and sign is the signum function, it is flagged as an edge point. This condition usually preserves the continuities at the -junctions and assures the detected edge is only one-pixel thick, which indicates the best locational approximation of edge points. For every marked edge point, ES is defined as the steepest gradient slope change along the eight directions in its 8-neighborhood, based on the LoG values of itself and its eight neighbors [11] ES LoG sign LoG (\nThe results from this stage may be \"noisy,\" as shown in Fig. 2 (a) and (b), but the edges are continuous and thin. More importantly, the noisy edge points can be discarded easily in the edge sorting and refinement processing using the ES array defined above.\nAt the edge refinement and sorting stage, we use the twothreshold method [6] to search and clean the marked edge points to detect contours. This scheme applies the following two conditions to the detected zero-crossing points.\n1) The ES at each point along the edges is greater than .\n2) At least one point on the contour has an ES greater than . Here, and are the preset thresholds, and . preserves the whole contour around the region boundary without incurring discontinuity at weak edge points. Fig. 2(c) and (d) show the results after the threshold was applied, where was set to ten. is set large enough to avoid spurious edges and initiates a new contour search. This twothreshold scheme is implemented by the contour search and sorting program (Fig. 1) . A new contour search is initiated whenever one point with a value greater than is detected. When a new search for another contour is initiated, the neighboring pixels with a value greater than are accepted as contour points until no neighboring pixels can satisfy this condition. Then, all the ES values along the detected contour are disabled by setting them to zeros such that these points will not be visited again. The search operation continues until the whole ES array has been scanned. The detected edge points are shown in Fig. 2(e) and (f) . The detected closed-contour points for each contour are then sorted and stored in order in an array for further processing. Sorted closed contours are then marked as region boundaries, as shown in Fig. 2(g ) and (h)."}, {"section_title": "III. CONTROL-POINT CORRESPONDENCE: IMAGE MATCHING", "text": "After the closed contours are detected from two images, a correspondence mechanism between the regions must be established to match these two images. Region similarity is obtained by comparing the shapes of the segmented regions. Since the images may have translational, rotational, and scaling differences, the shape measures should be invariant with respect to translation, rotation, and scaling. Some of the techniques that measure shape similarity in this fashion are Fourier descriptor, chain-code matching, shape signatures, centroidal profiles, invariant momentum, and shape matrices. In practice, these descriptors have been applied individually or sequentially in the process of image matching. A comparison between these similarity measures can be found in [23] . To achieve better discrimination capability, the modified chaincode matching and the invariant moment are combined to build the correspondence between regions in the two corresponding images."}, {"section_title": "A. Moment Representation of Regions: Invariant Moments", "text": "One set of the useful shape descriptors is based on the theory of moments. The moment invariants are moment-based descriptors of planar shapes, which are invariant under general translational, rotational, scaling, and reflection transformation [24] . The moments of a binary image are given by (3) where and define the order of moment. The center of gravity of the object can be given by using object moments\nThe central moments then are defined as\nZero-order moment represents the binary object area. Second-order moments express the distribution of matter around the center of gravity. In the case of objects with mass, they are called moments of inertia. Third-order moments express the basic properties of symmetry of object. They are equal to zero for objects with central symmetry. Moments of higher order describe more slight variations in shape, but they are more sensitive to noise. Central moments are translational invariants. Under a scale change and , the moments of change to . The normalized moments are then defined as where (6) Central moments and scaling invariant-moment representation can be employed to produce a set of invariant moments that are further invariant to rotational and reflectional differences. The lower-order invariants, which are composed of the secondand third-order central moments, are usually sufficient for most registration tasks on remotely sensed image matching. Detailed definitions of these invariants can be found in literature (see [17] and [25] ). The following seven invariants are used in this research:\nAll moments are scale, rotation, and translation invariant. Moments are reflection invariant as well. The magnitude of is reflection invariant. However, its sign changes under reflection."}, {"section_title": "B. Improved Chain-Code Representation of Regions", "text": "Another efficient representation of digital curves is the modified Freeman chain code. The standard Freeman chain code has certain drawbacks for correlation between two potentially matched contours, such as being noisy and variant to rotational, scaling, and translational differences. Therefore, the standard chain-code representation is improved by the following four operations, as described in [6] ."}, {"section_title": "1) Shift Operation:", "text": "A modified version of the standard chain code is produced by a shifting operation defined recursively by integers mod while is minimized.\nThe shift operation eliminates or reduces the problem of wraparound.\n2) Smoothing Operation: Curve smoothing is obtained by a smoothing filter, such as a Gaussian.\n3) Normalization Operation: The difference between the means of the corresponding chain-code representations measures the rotational difference between two curves. This rotational difference can be removed by subtracting the means."}, {"section_title": "4) Resampling Operation:", "text": "To be scale invariant, the chain code is resampled to the same length as that of its corresponding region.\nThe improvement in similarity measure gained by introducing these operations can be quantified easily with matching coefficients for a pair of chain codes. For a visual demonstration, a graphical comparison is presented in Fig. 3(a) -(h). The standard chain codes, the shifted chain codes, the shifted and smoothed chain codes, and the modified chain codes are computed by all four operations for a pair of example-matched contours and displayed for comparison. The improvement achieved by these four operations easily can be seen from the plots in terms of the accuracy with the similarity measure of matched contours."}, {"section_title": "C. Control-Point Correspondence by Region Matching", "text": "Control-point correspondence is performed in two steps. The first step includes region matching in the feature space based on combined criteria of minimum distance of seven invariant moments and maximum chain-code matching coefficients between a pair of regions. In this step, two similarity matrices between detected regions in two images are first computed. The first similarity matrix is the Euclidean distance matrix in seven-dimensional (7-D) invariant-moment space. The second similarity matrix is the chain-code matching matrix. Based on these two similarity matrices, the minimum distance classifier is used to identify the most robust matches. For example, a minimum of three GCP's are required to fit a general affinebivariate polynomial transformation. The centers of gravity for these three matched regions are used as the GCP's. The second step is performed in image space based on the image registration result of the first step. Given the three most robust GCP's from the result of the first step, the parameters of the first-order bivariate polynomial can be calculated simply. Based on this transformation, we can map the other potential regions in the sensed image into the reference image. The distance between the center of gravity of the mapped region in the sensed image and the actual center of gravity in the reference image is computed. This distance is actually the RMSE at this pair of corresponding points. A threshold easily can be set to avoid false matches. The final image transformation parameters are based on the matched points resulting from the second step of the image match algorithm."}, {"section_title": "1) Invariant-Moment Distance Matrix:", "text": "Let and denote the values of the seven invariant moments of the regions detected in the reference image and the regions detected in the sensed image, respectively. For every region in the reference image and region in the sensed image, we compute the invariant-moment distance between regions and (15) The result is an distance matrix in 7-D feature space, which represents the similarity relationship between the regions in two images. To simplify the search process, the impossible matches between region pairs that have more than a certain amount of difference in their lengths (say 35%) are left out by setting the distance values to one (a special flag for ignoring these region pairs) in the distance matrix. Based on the principle of minimum distance classifier, we conclude that the smaller the distance, the more similar the shapes of two regions.\n2) Chain-Code Matching Matrix: A correlation measure of a region pair proposed in [6] can be used to derive the matching matrix. \nmod mod (18) The chain-code curve of closed contour is periodic and can be represented by a modulus operation, as shown in (17) . The sliding process between contours is achieved by the maximum operation in (16) to find the best fit between the contour pair. The matching measure is similar to the MSE of two signals.\nThe cosine function guarantees the matching measure to be less than one. When , there is a perfect match. The result is also an matrix in which each element represents another reliable similarity measure between the regions in two images. By the same token, the impossible matches between the region pair which have more than a certain amount of difference (say 35%) are disabled by setting the matching coefficient value to zero in the matching matrix. We can notice that the greater the chain-code matching coefficient, the more the contours resemble each other in shape.\n3) The Algorithm of Image Matching: Based on the two similarity matrices defined above, the following matching algorithm, \"ImageMatch,\" is developed.\nAlgorithm ImageMatch:\nStep 1: Based on the output of the contour search and sorting program, the regions detected in the reference image and the regions detected in the sensed image are identified, ordered, and denoted as and , respectively.\nStep 2: Compute two feature matrices: the invariantmoment distance matrix [defined in (15) ] and the chain-code matching matrix , based on (16). In the invariant-moment computation, we also build two matrices containing the coordinates of the centers of gravity. One is an matrix, denoted as , for the regions in the reference image. The other is an matrix, denoted as , for the regions in the sensed image.\nStep 3: Searching for potential matches is based on the following condition. For any pair of regions , if and , then the pair is accepted as a potential match, where and are preset thresholds. Let denote a set of the potential matches detected by the condition above. This step will dramatically reduce search complexity and limit the following search to the matches with highest probabilities.\nStep 4: Working with the reduced-potential match set , minimum distance classification is performed based on (19) and the three pairs of regions with minimum values in this classification are then identified as the initial matched regions.\nStep 5: Based on the three initial matched pairs of regions, their corresponding centers of gravity are used as the GCP's to fit a first-order bivariate polynomial transformation and solve the following two equations in a least-square sense for the transformation parameters and (20) where and are two sets of coordinates of the corresponding matched points in the reference image and the sensed image, respectively. At the same time, these three pairs of the potential GCP's are removed from and the remainder of potential matches are denoted as .\nStep 6: Based on the transformed image in the image space and working with , any additional matches can be detected by examining the following condition:\nwhere , the th potentially matched pair with coordinates and in the reference image and sensed image, respectively, and and is a preset threshold that is actually the maximum RMSE allowed at the GCP's for the final image registration. If this condition is satisfied, the pair of points is considered a GCP pair. If not, this pair of points is removed from the potential matches. The output from Step 6 of the algorithm described above is a set of matched regions that are considered the final matches between the detected regions in the two images. Their centers of gravity then are used as the ground control points for use in the estimation of transformation parameters."}, {"section_title": "IV. EXPERIMENTAL RESULTS", "text": "In this section, we provide the experimental results using the proposed algorithms for automated image registration. In the experiments, two multitemporal images of the coastal plain of North Carolina were used. The reference image (image 1) was taken by Landsat 5 TM in November 1988. The sensed image (image 2, the image to be transformed) also was taken by Landsat 5 TM, but in December 1994. Subscenes of 512 512 pixels from the original images were used. Water boundaries usually are defined better in the infrared band images than in the visible band images. Therefore, TM band 5 images were used in the registration experiments, as shown in Fig. 4(a) and (c), respectively."}, {"section_title": "A. Image Segmentation and Region Extraction", "text": "In the image segmentation, the standard deviation of the LoG operator was set to 1.5 and the kernel size was 15 15 pixels. The minimum value of ES for edge points was set to ten, and the value of ES to initiate a contour search was set to 40. The minimum region boundary length was set to 40 pixels for a region to be qualified in region matching. With these parameters set, there are 22 regions in the reference image and 29 regions in the sensed image that were detected by the contour search and sorting program. These detected features are shown in Fig. 4(b) and (d) , respectively. The main region boundaries of interest are clearly recognizable in the stretched gray-scale images. The rest of the feature boundaries, which are short in length and embedded in the image features, are detected falsely for the most part. These features were to be examined for correspondence and removed by the ImageMatch algorithm."}, {"section_title": "B. Feature Extraction and Control-Point Correspondence", "text": "From the detected regions in two images, we constructed the invariant-moment distance matrix and chain-code matching matrix.\nStep 3 in the ImageMatch algorithm identified 21 potential matches out of 22 29 possibilities, where the invariant-moment threshold was set to 0.1, and the chaincode matching threshold was set to 0.8. Three pairs of GCP's with minimum distance and highest correlation were detected and used initially to estimate the transformation parameters. In the transformed image space, four additional matches were detected by Step 6 in the ImageMatch algorithm. The centers of gravity of the corresponding regions were used as control points. The complete matching results determined after the image matching algorithm are summarized in Table I ."}, {"section_title": "C. Estimation of Registration Parameters", "text": "Given a number of corresponding ground-control points from two images, the transformation parameters can be estimated for the images. The registration of images with rotational, translational, scaling, and image-skew differences can be approximated by the following general affine relationship [16] ( 22) where and are two corresponding points in the reference image and the sensed image, respectively and are the transformation parameters and express scaling, rotational, translational, and skew differences between two images. The affine mapping function is appropriate for data with flat topography. For some data with nonlinear, local geometric distortions (such as hilly areas with terrain changes), more complex transformation functions may be needed to produce better interpolation results. These mapping functions include thin-plate spline interpolation [26] , [27] and adaptive mapping function [28] . For general affine TABLE II  ROOT MEAN-SQUARE ERRORS AT CONTROL POINTS (IN PIXELS) transformation, a least-square approach is used to determine these six transformation parameters: from matched control points (23) where (24) and (25) The RMSE at the GCP's commonly is used to evaluate the performance of the least-square fitting and the accuracy of the image registration, which is usually defined as shown in (26) ."}, {"section_title": "D. Image Resampling and Performance Comparison", "text": "Since the determination of parameters of the general affine transformation (22) requires knowledge of only three control points, seven point pairs were sufficient. Given a number of corresponding control points in two images, the transformation parameters can be estimated using the least-square technique in (23) . The transformation equations were obtained as follows:\nKnowing the mapping function, the sensed image was transformed and resampled using cubic convolution interpolation. The resampled image is shown in Fig. 5(a) . The resampled image was then mosaicked with the reference image, as shown in Fig. 5(b) . The registration accuracy was estimated using (31) at every control point, as shown in Table II .\nTo do a fair comparison with the result of manual registration, we visually located and digitized seven best-control points using a magnifier on the standard false-color composite image (TM bands 4, 3, 2) displayed on a 24-bit screen. These seven points were then used in image transformation parameter estimation. The RMSE in both and directions is below 0.7 pixel, which is about the best accuracy that manual registration method can achieve. It seems that manual registration results can be improved by changing control points and other methods. But in practice, this improvement or refinement is limited. These limiting factors mainly include inaccuracy of reference data for image-to-map registration and inevitable positional error or deviation during visual correspondence. These factors are the sources of system errors in the manual registration method. The manual registration result was then compared to that of the proposed automated algorithm. The result of this comparison is shown in Table III . The results show that the proposed automated algorithm has better performance than manual registration, with an improvement in the registration accuracy of more than half a pixel on the average."}, {"section_title": "V. CONCLUSIONS", "text": "This paper has dealt with automatic registration of remotely sensed imagery. This procedure appears often in the processing of multitemporal and/or multisensor satellite image analyses, such as multisource data fusion, multitemporal change detection, and image mosaicking. In this work, we explored the critical elements for an automated image registration system using Landsat TM imagery. These elements included image segmentation, control-point selection and correspondence, and transformation parameter estimation. Attention has been paid to the region-feature extraction and automated controlpoint correspondence. In image segmentation, we developed a method called thin and robust zero-crossing based on the conventional LoG operator, and we improved the performance of the LoG zero-crossing edge detector by selectively picking zero-crossing points and defining an ES array. The images (both the reference image and the sensed image) were then segmented by the thin and robust zero-crossing-edge extraction technique, and the regions with closed boundaries were extracted. Each region was then represented by modified chain codes and invariant moments, which are affine-invariant descriptors describing the shape of a region. The correspondence RMSE between regions was established using the combined criteria of invariant-moment distance and improved chain-code matching.\nRegion matching was first implemented in the feature space and was then implemented sequentially in the image space. In the feature space, minimum distance classification was used to identify the most robust control points for initial image transformation. In the image space, region-to-region correspondence was established by the RMSE rule. After the correspondence between the regions had been established, the centers of gravity of the corresponding regions were used as control points. The parameters of transformation were computed by the least-square rule based on general affine transformation.\nThe performance of the proposed algorithm has been demonstrated by registering two multitemporal Landsat TM images taken in different years. Registration accuracy of less than one-third of a pixel has been achieved in the experiments. The proposed automated algorithm outperforms manual registration by over half a pixel on the average (in terms of the RMSE at the GCP's). In summary, the technique of automated image registration developed in this work is potentially powerful in terms of its registration accuracy, the degree of automation, and its significant value in an operational context. The approach is also robust, since it overcomes the difficulties of control-point correspondence caused by the problem of feature inconsistency. It has numerous applications in automated change detection, image fusion, motion detection, and stereo matching."}]