[{"section_title": "Abstract", "text": "Alzheimer's Disease (AD) is a progressive neurodegenerative disease. Amnestic mild cognitive impairment (MCI) is a common first symptom before the conversion to clinical impairment where the individual becomes unable to perform activities of daily living independently. Although there is currently no treatment available, the earlier a conclusive diagnosis is made, the earlier the potential for interventions to delay or perhaps even prevent progression to full-blown AD. Neuroimaging scans acquired from MRI and metabolism images obtained by FDG-PET provide in-vivo view into the structure and function (glucose metabolism) of the living brain. It is hypothesized that combining different image modalities could better characterize the change of human brain and result in a more accuracy early diagnosis of AD. In this paper, we proposed a novel framework to discriminate normal control(NC) subjects from subjects with AD pathology (AD and NC, MCI subjects convert to AD in future). Our novel approach utilizing a multimodal and multiscale deep neural network was found to deliver a 85.68% accuracy in the prediction of subjects within 3 years to conversion. Cross validation experiments proved that it has better discrimination ability compared with results in existing published literature."}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD), the most common dementia, affecting 1 out of 9 people over the age of 65 years 1 . Alzheimer's diseases involves progressive cognitive impairment, commonly associated with early memory loss, requiring assistance for activities of self care during advanced stages. Alzheimer's is posited to evolve through a prodromal stage which is commonly referred to as the mild cognitive impairment(MCI) stage and 10 -15% of individuals with MCI, progress to AD 2 each year. With improved life expectancy, it is estimated that about 1.2% of global population will develop Alzheimer's disease by 2046 3 thereby affecting millions of individuals directly, as well as many more indirectly through the effects on their families and caregivers. Current Alzheimer's research targets reliable prodromal identification of patients harbouring Alzheimer's pathology, for reasons that early intervention could potentially change the course of illness. Clinical diagnosis involves rigorous evaluation to rule out non-Alzheimer's causes for cognitive decline but this is however limited by specificity of identifying prodromal AD. Hence, we see the necessity of a tool to reliably detect and identify prodromal Alzheimer's disease.\nEfforts to understand AD pathology in the past resulted in identifying neuroimaging as one of the promising tool for prodromal diagnosis 4 . Neuroimaging involving magnetic resonance imaging (MRI) 5 and fluorodeoxyglucose positron emission tomography (FDG-PET) 6 were the unique imaging modalities recognized as useful tools to identify individuals with prodromal AD. MRI offers structural details such as texture, thickness, density and shape of various brain regions, while FDG-PET measures the resting state glucose metabolism 7 , reflecting the functional activity of the underlying tissue 6 . FDG-PET and MRI are frequently employed neuroimaging techniques in computer-aided diagnosis of neurodegenerative diseases. There has been considerable efforts to use structural MRI [8] [9] [10] or FDG-PET 11, 12 or a combination with other biomarkers 13, 14 to develop automated computer aided tools for prodromal diagnosis of Alzheimer's disease.\nDeep neural networks have been studied extensively and proven to have the best performance for many recognition tasks 15 . The application of deep neural networks in recognition of AD-related patterns has also attracted interests in its application for"}, {"section_title": "Methods", "text": "There are two major steps in the proposed framework: 1)image preprocessing: segment both MRI scans and FDG-PET images into patches, and extract features from each patch; and, 2)classification: train a deep neural network to learn the patterns that discriminate AD pathology, and then use it to classify individuals with AD pathology."}, {"section_title": "Data", "text": "Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial MRI, PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD).\nFor a comprehensive validation of the proposed method, it is emphasized that all the available ADNI subjects(N = 1242) with both a T1-weighted MRI scan and FDG-PET image at the time of preparation of this manuscript were used in this study. These subjects were categorized into 5 groups: 1) stable Normal controls (sNC): 360 subjects diagnosed to be NC at baseline and remained the same at the time of preparation of this manuscript; 2) stable MCI (sMCI): 409 subjects diagnosed to be MCI at all time points(at least for 2 years); 3) progressive NC (pNC): 18 subjects evaluated to be NC at baseline but have progressed to probable AD at the time of preparation of this manuscript; 4) progressive MCI (pMCI): 217 subjects evaluated to be MCI at baseline and progressed to probable AD; 5) stable Alzheimer's disease (sAD): 238 subjects diagnosed to be AD for all available time points. Demographic and clinical information of the subjects are shown in Table 1 . Numbers in brackets are the number of male and female subjects in the second row,, while in the rest 3 rows the two number represent the minimum and maximum value of age, education year and MMSE(Mini-Mental State Examination) score. It was worth mentioning that each subject could have multiple scans at different time points. In total there were 2402 FDG-PET scans and 2402 MRI images used in this study. Detailed descriptions of the ADNI subject cohorts, image acquisition protocols procedures and post-acquisition preprocessing procedures can be found at http://www.adni-info.org. Table 1 . Subject Demographics. In the second row, the first number in each cell is the total number of subjects and the numbers in brackets represents the number of male and female. In the last 3 rows, the first number in each cell is the mean and the two numbers in brackets are the minimum and maximum value of age, education year and MMSE score, respectively."}, {"section_title": "Mean", "text": ""}, {"section_title": "Image Processing", "text": "Unlike typical image recognition problems where deep learning has shown to be effective, our data set is relatively small. Hence directly using this smaller database of images to train the deep neural network is not likely to deliver high classification accuracy."}, {"section_title": "2/12", "text": "However, contrary to typical image recognition tasks, where the images contain large heterogeneity, the images in this database are all human brain images acquired with similar pose and scale which show relatively much less heterogeneity in comparison. Therefore we applied following processing steps to extract patch-wise features as shown in Figure 1 : FreeSurfer 5.1 22 was used to segment each T1 structural MRI image into gray matter and white matter followed by subdivision of the gray matter into 87 anatomical regions of interest (ROI). The Freesurfer segmentation were quality controlled by an expert neuroanatomist and any errors noted were manually corrected. Then, for a standard T1 MRI image, a voxel-wise k-means clustering based on spatial coordinates was performed to segment each ROI into patches based on its spatial information 23 . The size of patch was predefined as 500, 1000 and 2000 voxels in this study and resulted into 1488, 705 and 343 patches, respectively. It was designed to keep enough detailed information as well as avoiding too large feature dimension considering the limited number of data samples were available in this study. Subsequently, each ROI of the standard template MRI was registered to the same ROI of every target image via a high-dimensional non-rigid registration method (LDDMM 24 ). The registration maps were then applied to the patch-wise segmentation of the standard template. This transformed the template segmentation into each target MRI space so the target images were subdivided into the same number of patches. It worth mentioning that after the transformation, the size of a template patch in different images is not the same due to non-rigid registration encoding local expansion/contraction and hence is one of the features used to represent the regional information of a given structural brain scan. Then, for each target subject, the FDG-PET image of the subject was co-registered to its skull-stripped T1 MRI scan with a rigid transformation using FSL-FLIRT program 25 based on normalized mutual information. The degrees of freedom (DOF) was set as 12 and Normalized correlation was used as cost function. The mean intensity in the brainstem region of the FDG-PET image was the chosen reference to normalize the voxel intensities in that individual brain metabolism image, because brainstem region was most unlikely to be affected by AD. The mean intensity of each patch was used as an element to form the feature vector to represent the metabolism activity, and the volume of each patch was used to represent the brain atrophy. "}, {"section_title": "Multiscale Deep Neural Network", "text": "With the features extracted from MRI and FDG-PET images, we trained a Multimodal Multiscale Deep Neural Network (MMDNN) to perform the classification. As shown in Fig 2 , the network consists of two parts. The first parts was 6 independent deep neural network (DNN) corresponding to each scale of a single modality. The second part was another DNN used to fuse the features extracted from these 6 DNNs. The input data of this DNN was the concatenated latent representation learned from each single DNN. The DNNs in two parts shared the same structure. For each DNN, the number of nodes for each hidden layer were set as 3N, 3 4 N and 100 respectively, where N denotes the dimension of input feature vector. The number of nodes was chosen to explore all possible hidden correlation across features from different patches in the first layer and gradually reduce the number of features in the following layers to avoid over-fitting. We trained each DNN with two steps, unsupervised"}, {"section_title": "3/12", "text": "pre-training and supervised fine-tuning, respectively. Then all the parameters of MMDNN was tuned together. \u2022 Unsupervised Pre-training For unsupervised pre-training step, each DNN was trained as a stacked-autoencoder(SAE).\nAutoencoder is an artificial neural network used for unsupervised learning of non-linear hidden patterns from input data. It consists of three layers, input layer, hidden layer and output layer, for which two nearby layers are fully-connected. Three functions are used to define an autoencoder, encoding function, decoding function and loss function. In this study, encoding function is defined as:\n, where x is the input data, y is the latent representation, W 1 is the weight matrix, b 1 is the bias term and s is the activation function for which we used rectified linear function max(0, x). Similarly, decoding function can be represented as: z = s(W 2 y + b 2 ), where we constrained it with tied weight W 1 = W T and z is the reconstructed data which is supposed to be close to input x. Squared error 1 2 ||x \u2212 z|| 2 is applied as loss function to optimize the network. The hypothesis is that the latent representation can capture the main factors of variation in the data. Comparing with another popular unsupervised feature learning method principle component analysis (PCA), the activation function enables the network to capture non-linear factors of data variation, especially when multiple encoders and decoders are stacked to form a SAE. To fully train the network, we applied greedy layer wise training 26 approach where every hidden layer was trained separately.\n\u2022 Supervised Fine-tuning After pre-training, the first three layers of a DNN were initialized with the parameters of encoders from pre-trained SAE followed by a softmax output layer. At first we trained the output layer independently while fixing the parameters of first 3 layers. Then we fine-tuned the whole network as Multilayer Perceptron (MLP) with subject labels for criterion. The network outputs the probabilities of a subject belongs to each class and the class with highest probability determines the output label of the subject. If we use x i , y i to represent the input feature vector and label of the i th sample, respectively, the loss function based on cross entropy can be displayed as:\nwhere N is the number of input samples, j represents the class of samples, and h represents the network function.\n\u2022 Optimization of Network Every training step of the networks were performed via back propagation with Adam algorithm 27 . It is a first-order gradient-based optimization algorithm which has been proven to be computational efficient and appropriate for training deep neural network. During training stage, the training set was randomly split into mini"}, {"section_title": "4/12", "text": "batches 28 where each of them contains 50 samples in this study. At every iteration, only a single mini batch was used for optimization. After every batch has been used once, the training set was reordered and randomly divided again so that each batch would have different samples in different epoch.\n\u2022 Dropout In order to prevent deep neural network from overfitting, regularization is necessary to reduce its generalization error. In this study, we used dropout 29 to learn more robust features and prevent overfitting. In the dropout layer, some units were randomly dropped, providing a way to combine exponentially many different neural networks. In this study, we inserted dropout layers after every hidden layer. In each iteration of training stage, only half of hidden units were randomly selected to feed the results to the next layer, while in the testing stage all hidden units were kept to perform the classification. By avoiding training all hidden units on every training sample, this regularization technique not only prevented complex co-adaptations on training data and decrease overfitting, but also reduced the amount of computation and improved training speed.\n\u2022 Early Stopping Another approach we used to prevent overfitting is early stopping. Because deep architecture were trained with iterative back propagation, the network were prone to be more adaptive to training data after every epoch. At a certain point, improving the network's fit to the training set will start to decrease generalization accuracy. In order to terminate the optimization algorithm before over-fitting, early stopping was used to provide guidance for how many iterations are needed. In the cross validation experiment, after dividing the data set into training and testing, we further split the training samples into a training set and a validation set. The networks were trained only with data in the former set, while samples in the latter set was used to determine when to stop the algorithm: while the network has the highest generalization accuracy for validation set. In actual training, we stopped the optimization if the validation accuracy had ceased to increase for 50 epochs.\n\u2022 Ensemble Classifiers Although early stopping has proven to be useful in most deep learning problems, relatively small data set limited the number of samples we could use for validation. And a small validation set may not able to represent the whole data set resulting in a biased network. Therefore, we resorted to ensemble multiple classifiers to perform more stable and robust classification. Instead of selecting a single validation set, we randomly divided the training set into 10 sets and used them to train 10 different networks to 'vote' for the classification. At the training stage, for network i, set i would be used for validation while the rest 9 sets were used for training. At the testing stage, the test samples were feed into all these networks resulting in 10 sets of probabilities. For each sample, the probabilities from 10 networks was added and the class with highest probability was the classification result of this sample. Although the performance of ensemble classifiers may not be better than single network in every occasion, this strategy can statistically improve the classification accuracy as well as the robustness and stability of the classifier."}, {"section_title": "Results and Discussion", "text": ""}, {"section_title": "Experiments Setup", "text": "The proposed deep neural network was built with Tensorflow 30 , an open source deep learning toolbox provided by Google. First, to compare the discriminant ability with state-of-the-art methods, 10-fold cross validation experiments were applied to classify sMCI and pMCI subjects. Then we performed three experiments with different training sets to test whether the images of pNC and pMCI contain AD pathology or not. For these experiments, 4 data sets: sNC, sAD, pNC and pMCI, were divided into two groups in 3 different ways: 1) subjects of sNC and sAD were considered as group 1, and subjects of pNC and pMCI belonged to group 2; 2) subjects of pMCI, sNC and sAD belonged to group1, and pNC were considered as group2; 3) all subjects were considered as group1. For each experiment, we applied a 10-fold cross validation on group1. The subjects of group 1 were randomly divided into 10 subsets, with 9 sets used for training and the rest set combined with subjects of group 2 used for testing. As detailed in the Methods Section, 10% of training subjects were randomly selected as validation set for early stopping to prevent overfitting. 10 networks with different validation set were trained to 'vote' for the final classification result. Noticing it was not images but subjects we were splitting, so images from different time point of the same subject won't be used for both training and testing."}, {"section_title": "Compare with State-of-the-Art Methods", "text": "Researchers in the past have worked on classification of subjects with progressive cognitive decline and those with stable cognitive impairment. pMCI were recognized as individuals with high risk of AD, while the sMCI were considered as those with no risk or low risk of AD in these studies. To evaluate the performance of our approach, we compared the classification accuracy of pMCI vs. sMCI with previous methods using the same data modality, i.e. T1-weighted MRI and FDG-PET 13, 18, 31, 32 . The proposed network outperformed the state-of-the-art method in classifying pMCI and sMCI individuals, irrespective of using single or multimodal imaging, as shown in Table 2 . It is worth to mention that in the study of Chen et.al 32 , they performed"}, {"section_title": "5/12", "text": "domain transfer learning to exploit the auxiliary domain data(AD/NC subjects) to improve the classification. Even though, the acuccracy of our methods without auxiliary knowledge was 3.5% accuracy than theirs. Table 2 . Accuracy(%), Sensitivity(%), and Specificity(%) of the proposed network comparing with state-of-the-art methods."}, {"section_title": "Method", "text": "The third row is the number of subjects used in the experiments"}, {"section_title": "AD Pathology Classification", "text": "One problem of sMCI subjects was that we only know they remained stable at the time of preparation this manuscript, but they could still progress to AD or other mental disease in the future. Although the the sMCI vs. pMCI experiment were commonly used to test the discriminate ability of classifiers in recent studies, the classification result of sMCI subjects may not be very accurate. Therefore, we performed the second experiment involved classifying individuals with only known Alzheimer's progression (pNC, pMCI and sAD) and normal controls (sNC). We investigated the performance of the classifier by using various combinations of samples during training phase. At a very basic level, we trained the classifier by discriminating sAD and sNC, at the next level pMCI and sAD were combined to represent the Alzheimer's group and trained to discriminate them from the sNC group. In the last level we combined pNC, pMCI and sAD to represent the Alzheimer's group to discriminate from the sNC group. The features extracted by the deep neural network are displayed in Fig 3. We observed the accuracy and sensitivity of the classifier progressively improved by additionally training with pMCI and pNC, while the specificity decreased, as displayed in Table 4 . Further, the classifier performance was marginally better with the combination of FDG-PET and structural measurements compared to the performances with individual modalities. Interestingly, the classifier performance of structural imaging measurements were inferior to that of FDG-PET measurements. Supporting, the fact that FDG-PET, a measure of neuronal activity is a better tool to identify prodromal Alzheimer's as compared to structural images 33, 34 ."}, {"section_title": "Multiscale Classification", "text": "The classification accuracy of features extracted with different scales are listed in Table 3 . We could not recognize any trend of increasing or decreasing classification performance with the changes in patch size. Therefore, features with higher resolution do not necessarily cover the discriminative information of lower resolution features. However, fusing multiscale features yielded superior accuracy as compared to uniscale, suggesting the network has the ability to capture discriminative information across the coarse to fine resolutions."}, {"section_title": "Early Diagnosis", "text": "We also investigated the classifier's ability to identify individuals with high risk of acquiring Alzheimer's, prior to disease onset. The classifier trained with the combined sample of Alzheimer's trajectory (pNC, pMCI and sAD) was superior, as compared to the classifier trained with sAD alone. As the network classifier was trained with patterns of AD trajectory using pNC and pMCI, the network was able to achieve exceptional classification performances in identifying individuals with AD risk, i.e the classifier recognized individuals with AD risk with a of 90.08%, 85.61% and 81.20%, approximately at 1, 2, and 3 years prior to disease onset respectively. Studies in the past have predicted AD onset, using unimodal or multimodal investigation. Few studies have used PET as a single modality or in combination with structural MRI, CSF or cognitive measures to predict Table 3 . Accuracy(%) using features at different scales of different modality.\nAD onset 13, 32, [35] [36] [37] [38] . The accuracy of 3 year prediction in the present network analysis was superior than those reported in the quoted studies. Studies predicting the illness onset, using structural MRI as a standalone tool or in addition to other clinical variables, have reported accuracy values inferior than to those obtained using PET 5, [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] . Deep neural network is a strong tool for accurate recognition of objects, by a-priori training of images with well characterized objects. Hence the basic requirement for accurate classification using DNN tool are providing large number of images (usually in millions) and well characterized objects during supervised training phase 15 . Therefore a compromise in the a-priori knowledge of the objects (features of Alzheimer's) provided during training would limit the accuracy of the subsequent classification. As our current understanding of AD pathogenesis and its precise characteristics in FDG-PET and structural MRI images are limited, DNN suffered jeopardy in achieving 100% accurate classification. The clinical criteria for the diagnosis of AD involves a series of evaluations to provide near precise diagnosis. Despite rigorous evaluations, clinically diagnosed individuals with AD are not 100% accurate and hence the FDG-PET and structural MRI characters can overlap with conditions other than AD, Table 4 . Accuracy(%), sensitivity(%) and specificity(%) of different modality using different training sets.\nincluding NC. Therefore the DNN trained with less accurately characterized images (FDG-PET and structural MRI) was unable to achieve 100% accurate classification. We propose an improvement in characterization of AD features by either upgrading FDG-PET and structural imaging methods or an increment in the understanding of AD specific pathogenesis, would positively impact the classification accuracy of the DNN classifier tool in future studies."}, {"section_title": "Conclusion", "text": "In summary, we proposed a deep neural network to identify individuals at risk of developing Alzheimer's disease. We trained the classifier with patterns hidden in different resolutions and different modalities to distinguish subjects with Alzheimer's trajectory (pNC, pMCI and sAD) and those without cognitive deficits (sNC). Our results show the classifier's ability to successfully distinguish individuals with AD pathology from sNC with a remarkable accuracy of 82.93% using cross validation experiments. We observed the performance of network classifier built by the combination of FDG-PET and structural MRI images was better than those built with either structural MRI or FDG-PET alone. Further the classifier trained with the combined sample of pNC, pMCI and sAD (Alzheimer's trajectory) was found to yield the highest classification accuracy. Lastly, our experiment to recognize individuals with AD pathology, prior to illness onset demonstrated a sensitivity of 85.68% in 3 years earlier to illness onset. Hence, the proposed deep neural network classifier can be a potential tool of choice in the future for early prediction of AD pathology. The number of pNC subjects was limited in this study resulting in a relatively low accuracy for pNC, as more data is accumulated in the future, we expect better accuracy in the prediction of NC subjects with AD pathology."}]