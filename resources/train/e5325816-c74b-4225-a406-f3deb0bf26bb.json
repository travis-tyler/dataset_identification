[{"section_title": "", "text": "Correlation between student and parent responses to family background characteristics, by sex and raceethnicity Percent of cases matched between student and parent responses to family background characteristics, by sex and raceethnicity Table 3.5 Relative bias between student and parent responses to family background characteristics, by sex and raceethnicity  Table 3.6 Correlation between student and parent responses to family background characteristics, by socioeconomic status and reading ability 21 Table 3.7 Percent of cases matched between student and parent responses to family background characteristics, by socioeconomic status and reading ability Table 3.8 Relative bias between student and parent responses to family background characteristics, by socioeconomic status and reading ability Table 3.9 Validity coefficients and percentage of cases with matched values on selected school variables.   Percent of cases matched between student and parent responses to school-related variables, by sex and raceethnicity. school-related variables, by sex and raceethnicity Correlation between student and parent responses to school characteristics, by socioeconomic status and reading ability Table 3.15 Percent of cases matched between student and parent responses to school characteristics, by socioeconomic status and reading ability Table 3.16 Relative bias between student and parent responses to school characteristics, by socioeconomic status and reading ability Table 4.1 Percent of students responding that they attend remedial mathematics, regular mathematics, an i Algebra classes 30 Percent of students participating in extracurricular activities. 31 Table 3.13 Relative bias between student and parent responses to Table 3.14   Table 4.2   Table 4.3   Table 4.4 Table 5.1 Table 5.2   Table 5.3 Table 5.4 Tabie 5.5 Table 5.6 Table 5.7 Table 5.8 Number and percent of schools and number of students with mismatches on academic honors society item 34 Administrator responses to item on availability of academic honors societies, by student self-reported participation and reading quartile Description of student-level scales Reliability analysis (Cronbach's Alpha) of student-level scales in NELS:88. Table 5.9 Table 5.10 Table 5.11 Table 5.12 Table 5.13 Correlations, varimax rotated factor matrix, and factor statistics for locus of control items 46 Results of confirmatory factor analyses, locus of control items."}, {"section_title": "47", "text": "Variable names and abbleviated text for items comprising the self-concept scale. Correlations, varimax rotated factor matrix, and factor statistics for locus of control items Results of confirmatory factor analyses, self-concept items. "}, {"section_title": "LIST OF FIGURES", "text": "Comparison of correlation between patent and student responses for selected items from :I44.S1 :88 and High School and Beyond INTRODUCTION The base year of the National Education Longitudinal Study of 1988 (NELS:88) is the first stage of the current major educational longitudinal data collection effort by NCES. Students, parents, teachers, and school administrators were selected to participate in the survey. About 25,000 students participated in the base-year survey (a response rate of 93 percent of those selected). Over 24,000 parents responded to the survey (a respense rate of over 92 percent of those selected) and data from at least one teacher was obtained for over 92 percent of the students who participated in the study. Because the NELS:88 research protocol is focused on determining the effects of students' home and learning environments on their educational achievement, it is essential that the data accurately reflect these environments. The study design developed for NELS:88 avoids many of the reporting errors likely to be committed by eighth-grade students. This was accomplished by gathering information from parents on items that typically have been inaccurately reported by students (such as family income). However, the base-year study relies on student self-reported data in a variety of critical areas. The NELS:88 dataset enables researchers to examine the relationship of various student, family, and school characteristics to students' success in school. Accurate background information on the student is essential to achieving these analytical goals. This report presents the results from an analysis of the quality of the data from the NELS:88 base-year survey. Specifically, this study assesses the student data in NELS:88 by examining some of the student responses to see how they correspond with parent or teacher responses or according to their consistency with other student items. In developing the NELS:88 database, NCES quite consciously used other NCES longitudinal studies as a foundation, so that the results from NELS:88 could be compared with those from other databases such as HS&B. Therefore, wherever possible, this report compares the quality of the NELS:88 data with that of the HS&B data reported in Fetters, Stowe, and Owings.' Furthermore, this analysis was conducted without the use of the weights associated with the NELS:88 database. Errors in responses to questionnaire items are, by their nature, directly linked to the wording of particular items, the placement of particular items in the questionnaire, and the conditions under which the questions are administered. To study the errors in responses, researchers in the field of measurement focus their attention on whether the questionnaire items can obtain accurate information. That is, they cxamine how the particular sample of persons responded to the particular survey instrument. Since they are only intemsted in the persons who actually responded to the survey, they commonly use unweighted analyses to gain knowledge on psychometric properties (such as validity or reliability) of these items. On the other hand, researchers in the field of survey sampling are more interested in making inferences about the population of survey respondents. That is, instead of examining how a particular sample of persons responded to the survey, they study how the population of persons might have responded to the survey. They focus their attention on investigating the item response errors as part of the nonsampling error of population estimates. They generally use weighted analyses for their studies. One of the major objectives of this report is to compare the NELS:88 data quality with High School mid Beyond (HS&B), the last longitudinal study conducted by NCES. Because the report on the quality of responses for HS&B was conducted with unweighted data, this analysis also reports the results using unweighted data. However, in producing this report, both weighted and unweighted analyses were conducted for a sample of survey items. The results indicated that using weighted rather than unweighted data produced few differences in the indicators of data quality used in this analysis. However, for readers who are interested in the results of the weighted analyses, a comparison of the weighted and unweighted results for some selected items are included in appendix C of this report. This report is organized into six chapters. This introductory chapter reviews earlier findings on the accuracy of self-reported data collected with survey questionnaires. Following this introductory chapter, the second chapter describes the methodology used for assessing the quality of the NELS:88 data. A third chapter explores the consistency of student and parent responses to similar items. The next chapter looks at the consistency of student responses to similar items by examining the reliability of student responses from one item to the next. A fifth chapter looks at the internal reliability of several scales created from the NELS:88 base-year survey. The report concludes with a discussion of the results of this analysis and the implications for future analytical use of NELS:88 data."}, {"section_title": "Accuracy of Student Self-Report Data", "text": "Social science researchers often depend on survey data to analyze social phenomena. However, the use of such data has raised numerous questions about the accuracy of self-reports, which has generated a substantial literature on the validity and reliability of data collected from survey respondents. Within this larger class of studies are those that analyze the accuracy of reports of socioeconomic class and other family characteristics by children. Most of these reports focus on two measures of quality with respect to the data provided: the validity of reported data in relation to the true value and the reliability of these reports over time. The validity coefficient is generally defined as the correlation between the evaluated response and the true value for the response, whereas the reliability coefficient is defmed as the correlation between responses of the individual to the same item in a test-retest situation. 2 Prior longitudinal studiesby the National Center for Education Statistics (NCES), the National Longitudinal Study of 1972 (NLS-72), and High School and Beyond (HS&B)relied on the student as the primary source of information about all aspects of home and school life. In a report using HS&B questionnaire and transcript data, Fetters, Stowe, and Owings found that students tended to provide relatively accurate information on a large number of issues.3 Due to the richness of the HS&B data, this study was considerably more comprehensive than most examining the quality of student-reported data. While many of these studies examined the quality of data reported in questionnaires using either validity or reliability measures, Fetters, et al. were able to examine both measures and the level of agreement between students and their parents on subjective or opinion-oriented items. In general, they found that students were accurate reporters of factual information, such as raceethnicity or parents' educational level. On the other hand, they were not as accurate in reporting information about opinions or attitudes, such as mother's expectations for the student's educational achievement. One exception to this general rule was family income, which, although a factual item, was a matter of speculation for many students and thus inaccurately reported. These results confirmed findings from several other studies concerning the validity of student reports of family characteristics. For example, Cohen and Orum found that students accurately reported their parents' education and occupation when allowed to fill in a response blank.4 Borus and Nestel concluded that, on average, the son's estimate of his father's educational attainment was very close to that of his father's.5 Similarly, Kayser and Summers found that students were relatively accurate reporters of parental education, but like Fetters, et al., they concluded that students were not good reporters of their father's income.6 These studies an the accuracy of student self-reported data generally conclude that students are relatively good sources of information about family background variables, but that the accuracy of their reporting is systematically affected by the way in which questions are asked, the specific information sought, and the characteristics of the student. For example, Fetters, Stowe, and Owings found that HS&B seniors were more dependable reporters than HS&B sophomores? These results are consistent with an earlier study by Kerckhoff, Mason, and Poss, which concluded that older children are more accurate reporters than younger ones, and that the validity of reports by children increases as they get older.8 These findings have important implicadons for NELS:88, because this study relies on the reports of eighth-graders. Other student characteristics identified by Fetters, et al., that were related to the accuracy of self-reported data included sex (females were slightly more accurate reporters than males), race ethnicity (whites were more accurate reporters than Hispanics or blacks), and ability (high-ability students were more accurate reporters than low-ability students). Likewise, Kerckhoff, et al., found that among boys in the sixth and ninth grades, whites tended to be more accurate reporters of family social status than blat.ls, although this discrepancy largely disappeared by the time the boys reached the 12th grade. Furthermore, these differences in accuracy were due to the different distributions of blacks and whites on the specific characteristic studied.9 Moreover, Borus and Nestel found slight evidence that white young men reported their father's education and socioeconomic status with greater accuracy than did black young men, although the only statistically significant difference in accuracy was between rural, poorly educated blacks from large families and everybody else,10 Another factor associated with the validity of student reports about family background variables is the way in which the question is asked. In particular, Cohen and Orum discovered that children reported their father's occupation more accurately when they were asked to answer open-ended questions than when they were asked to specify the broad occupational category in which their father's occupation belonged.\" The number of response categories also has an impact on the validity of student responses: variables with few response categories may produce artificially high levels of agreement, whereas variables with many categories may produce artificially low levels. In an analysis of ungrouped data for an onlinal variable, Cohen and Orum found that a majority of incorrect responses were found in categories adjacent to the correct response and did not result in serious misplacement of children on the ordinal scale.12 Furthermore, when students had the option of responding \"I don't know\" to questionnaire items, the overall accuracy of their responses declined.13 The type of information sought also affects the accuracy of student reports. Validity and reliability studies of student-reported data have consistently found that factual, current items are more accurately reported than subjective or ambiguous ones. Likewise, items that are personally sensitive tend to be less accurately rvorted. From all of the studies cited here that analyzed student reports of family income, it was found that students are not good sources for this type of information. Perhaps an explanation is that income could be a sensitive item, or it might simply be something that few students actually know much about. Finally, most studies have found that family life is an area in which student and parent reports tend to In inconsistent. Fetters, et al., found only moderate agreement between students and parents on the mother's educational aspirations for the student, while they discovered low agreement on items such as the influence of parents on the student's post-high school plans and on sex role attitudes.14 Jessop concluded that relative to other topics, agreement on the nature of parent-adolescent relationships was low. Further, the results indicate that biases by students. tended to enlarge the area of power and influence they had on family life.15 In addition, Jessop suggested that the responses of both students and parents may be biased by beliefs about what is considered socially desirable, that parents were more biased reporters of family life issues than students. Thus, students may be better reporters of family life issues than parents.16"}, {"section_title": "CHAPTER 2 DATA AND METHODOLOGY", "text": "There are many different ways in which analysts can judge the quality of the data from any survey. In this study three types of analyses were used to assess the quality of the NELS:88 data: 1) the correspondence between the student and the parent iesponses to similar itemsincluding the bias in the student-reported data; 2) the consistency of students' responses to related items; and 3) the internal reliability of scales created from the NELS:88 dataset. These analyses are presented separately in the next three chapters of this report. Beginning with a brief description of the NELS:88 dataset, this chapter presents a detailed description of the methodology used in each of the three analyses."}, {"section_title": "NELS:88 Data", "text": "The NELS:88 base-year study used a two-stage stratified probability sample design to select a nationally representative sample of schools and students. Nearly 25,000 students are included in the fmal realized sample.17 The student file includes respondents in the main sample and supplemental samples of Asians and Hispanics. In addition, one parent and two teachers of each student in the student sample were also selected to participate in the parent and teacher data collection efforts. More than 22,000 parents responded to the survey (a response rate of more than 92 percent of those selected), and data from at least one teacher was obtained for more than 92 percent of the students who participated in the study. This analysis is based on unweighted data from the public release files for NELS:88.18 These data have been machine edited to enforce certain kinds of consistency. Consequently, in comparing responses to particular variables, one could be comparing responses that have been changed to be consistent with other independent filters, and not the iespondent's actual answer to the item.19 Fortunately, the machine editing on the base year of NELS:88 was light and quite conservative. The analysis could just as easily have been run on the original unedited versions of the files. However, since the purpose of this report is to provide researchers with information on the quality of the NELS:88 data, the analysis was conducted on the data iesearchers will actually usethe public release data."}, {"section_title": "Methodology", "text": "Correspondence between student and parent responses. V alidity is generally defined as the correspondence between an item and some standard assumed to be the true value. In most cases throughout this report, the parent response is the  1.7 standard upon which the validity of the student response is measured.\" It is important for the reader to keep in mind that this section examines only those items on the student questionnaire that had a corresponding item (or similar item) on the parent questionnaire. Furthermore, the sample size for the analysis of each item was based the number of logical studentparent pairs for each item. The number of logical studentparent pairs depended primarily on the skip pattern of previous items and whether the mother or the father responded to the parent questionnaire. Therefore, all 22,651 studentparent pairs were used in the analysis of some items, while other items (e.g., father's educational expectations for the student) were based on much smaller logical sample sizes. Three types of statistics were used to assess the correspondence between student and parent responses: 1) the item's validity coefficient, or the correlation of student and parent responses to similar items21; 2) the percentage of students whose response identically matched their parent's response; and 3) the relative bias in the student responsesor the difference between the mean of the parent response and the mean of the student response divided by the mean of the parent response. Validity coefficients. For the family background variables, validity coefficients were calculated for each variable representing a factual item. For variables measured on an interval or ordinal scale (e.g., number of siblings and father's education), Pearson's product moment correlation coefficient (r) was employed. For variables measured on a nominal scale (e.g., race ethnicity), the statistic called Cramer's V was employed.22 Like r, V can reach a maximum value of 1; for dichotomous variables, V equals r. Some of the comparisons in this report do not involve measuring the validity of student responses as much as measuring the consistency between student and parent responses to less factual items. On these items there is no \"right\" answer, so when a summary statistic like Pearson's r c Cramer's V are used, we are actually examining the consistency of responses rather than the validity of responses. Percentage of matched responses. While validity coefficients represent a well-known means for assessing data quality, looking at the correlation between parent and student responses alone can be misleading. Another method of assessing data quality that can be used in conjunction with the validity coefficient is the percentage of cases in which the students matched their parent's response for an item. For example, tables 2.1 and 2.2 present two sets of simulated data, 20 In all cases the student's responses are compared to one of the student's parentsthe one that ahswered the questionnaire. It is not known how much one of the student's parents wo-ld agree with the other parent. Thus, while student-parent responses in this study disagree, the student may well be in agreement with the other parent. 21 Validity in the most strict sense is the correlation of a response to an item with the \"true\" value for that item. This analysis uses the parent response as the standard to judge the validity of the student response. Given that there may be error in the parent response, the \"true\" value for the student response is unknown, and therefore the validity of the student response is unknown. However, to be consistent with the previous report on the quality of the data in High School and Beyond, we use the term \"validity coefficient\" here. 22 M. G. Kendall and A. Stuart, The Advanced Theory of Statistics, vol. 2, (New York: MacMillan, 1979). 6 1 8 Table 2.1   Data exam le   Student  response   Eilmat_____   0   1   Total   0   30  20  50   1   20  30  50   Total  50  50  100   Table 2.2  Data exam le 2   Student  Parentresponse  response   0   1   Total   0   2  10  12   1   8   80  88   Total   10   90   100 In the first table the correlation between student and parent responses is about 0.20, and the percentage of cases matched is 60 percent. In the second table the percentage of cases matched is much higher (82 percent), but the correlation is only 0.08. (Notice that in example 2 the marginal distributions are quite skewedthat is, there are many more 1 s than Os.) Under these circumstances when dealing with binomial variables, the measure of association can suffer from restriction in range. Clearly, using the validity coefficient alone to assess the relative quality of these data would be misleading. In fact, in this example, the percentage of cases identically matched seems to be a better indicator of data quality dr a the validity coefficient. However, reporting and relying only on the percentage of cases identically matched also has its limitations. One such limitation is that the quality of the mismatched cases is not measured. For example, in tables 2.3 and 2.4 the percentage of cases identically matched for both examples is 60 percent. However, the distribution in table 2.3 is more clustered around the diagonal than the distribution in table 2.4. For ordinal or quantitative scales, the quality of the data in table 2.3 is better than in table 2.4.23 In other words, the total amount of discrepancy between student and parent responses is much lower in table 2.3. The size of this mismatch is measured by the validity coefficient. In tables where the responses are more \"clustered\" around the diagonal of the table the correlation will be larger than in tables where there is more spread. For example, the correlation for table 2.3 is 0.58, whereas the correlation for table 2.4 is 0.31.   2   3   4   Total   response   1   15   1   4   5  25   2  2   15  3  5  25   3   5   2   15  3   25   4   6   4  0   15   25   Total  28   22  22  28  100 From the examples presented here, it is clear that none of these statistics can unambiguously assess the quality of student responses because each provides a slightly different piece of information about the data. Table 2.5 presents a matrix that uses the information derived from both the validity coefficient and the percentage of cases matched. This matrix can be used to create some simple decision rules about the data. For example, if the correlation is high and the percentage of cases matched is high, then the analyst can be fairly confident in the quality of the student responses (as judged by how they correspond to the parent responses). In contrast, if the correlation is low and the percentage of cases matched is also low, then the analyst should be wary of the student responses because they correspond poorly with the parent responses. However, in cases where the information from the correlation does not correspond with the information from the percentage of cases matched, either the data are biased or the marginal distributions of the variables are skewed. Skewness in parent and student data. If the correlation for a studentparent comparison is low, but the percentage of cases matched is high, then the analyst should investigate the dislibutional properties of the two variables. For example, in table 2.2 the percentage of cases matched was quite large (82 percent), but the correlation was quite low (0.08). However, in this example the distributions of the two variables were quite skewed. Clearly, in this case the analyst should investigate the shape of these distributions before making any judgements about their suitability for the purpose at hand. To facilitate this process, appendix B provides the bivariate distributions for all of the studentparent comparisons shown in this report. Bias in student-reported data. On the other hand, if the correlation is high and the percentage of cases matched is low, then the student responses are almost certainly biased to some degree. That is, student responses may correlate with parent responses, yet may systematically underestimate or overestimate the value for a specific item. For example, there may be a strong correlation between students' and parents' estimates of the parents' expectations for the students' education, yet students may systematically report lower expectations than do their parents. In the Fetters et al. report on the quality of responses in HS&B, bias was simply defined as the difference in response means of parents and students: where yi = the student response, xi = the parent response, and n = the number of studentparent pairs. A positive bias was associated with over-reporting by the student, while a negative bias was associated with under-reporting. A weakness of this statistic is that the size of the bias is dependent in part on the units of the original items. For example, the bias in the students' estimate of father's education will be larger if father's education is measured on a 10-point scale, rather than a 5-point scale. In order to avoid this problem, this report uses a relative measute of bias. Relative bias is defined as:"}, {"section_title": "RELBIAS", "text": "xl or the amount of bias in the student response relative to the mean of the parent response. These statistics are on the same scale and are therefore comparable across items with different original scales. However, they are appropriate only for items measured on an ordinal or interval scale. Subgroup comparisons. The literature on the quality of data derived from questionnaires administered to students or children suggests that certain characteristics of respondents are related to the quality of their responses. Therefore, validity coefficients, percentages of cases matched, and relative bias statistics were generated for the whole sample of students, as well as for the various student subgroups. This enabled us to assess whether data quality was constant across all students or whether it varied systematically in relation to the student characteristics. Table 2.6 shows a listing of student characteristics that ait used to disaggregate the sample of students. Inter-item consistency of student responses. Inter-item consistency is a measure of the reliability of student responses from one item to the next. For example, if the student claims to be in the high-ability category in math but then claims to be enrolled in a remedial math class, we could conclude that the student is not a particularly reliable reporter. (However, even if the student gave reliable answers to these items, this would indicate nothing about the validity of those answers.) This situation could be interpreted as a modification of the test-retest scenario and hence an alternative to the classic form of the reliability coefficient. The inter-item consistency of student responses was examined in two ways: in relation to the reliability of the student's reporting of similar factual items; and in reladon to the consistency of the student's perceptions of less factual or subjective items."}, {"section_title": "Reliability of scales.", "text": "Finally, the reliability of several scales previously created from student, teacher, and school administrator data files was assessed. Many of these scales were created by the NELS:88 data collection contractor and are included in the public use data files. Other scales such as the teacher engagement, academic press, discipline climate, and student behavior scales, were created by MPR Associates for special analyses of the NELS:88 data. The inter-item reliability of these items was explored using the criteria of Cronbach's Alpha. In addition, the reliability and the dimensionality of these scales for different groups of students was also exploied. Comparisons with High School and Beyond. Many of the items in the NELS:88 questionnaire are similar (and in some cases identical) to the items used by Fetters et al. to evaluate the quality of student responses to the HS&B questionnaires. Therefore, in some instances the validity of the NELS:88 data was compared with the validity of the HS&B data. However, caution should be used in interpreting these comparisons. There are several differences in the context, population coverage, and pattern of nonresponse between the two datasets that preclude strict comparisons of NELS:88 and HS&B. For example, the 8th-grade population surveyed by NELS:88 is somewhat more heterogeneous than the 10th-grade population surveyed by HS&Bjust as the 10th-grade population is more heterogeneous than the 12th-grade population. Dropoutsthose persons lost between the 8th, 10th, and 12th gradesare disproportionately the least reliable reporters. Hence, the HS&B data should be more reliable because more of these less reliable students have dropped out by the 10th grade. Furthermore, the response rate for the NELS:88 base-year survey was much higher (93 percent) than for the base-year HS&B sophomore or senior cohorts (81 and 84 percent respectively). In addition, the last sections of the HS&B questionnaire had a nonresponse rate of more than 20 percent, while the nonresponse rates in the last portion of NELS:88 was 7.5 percent.24 Since the least reliable respondents tend to be less likely to participate and less likely to finish the questionnaire, the principal contributors of poor data quality are more likely to have been filtered out of HS&B."}, {"section_title": "CHAFFER 3 CONSISTENCY AND CORRESPONDENCE BETWEEN STUDENT AND PARENT RESPONSES", "text": "This chapter examines the correspondence between parent and student responses to similar items in NELS:88. Table 3.1 lists the items used in this analysis, along with their sources in the base-year questionnaire. The items wem divided into two groups: 1) family background items; and 2) school experiences. (Bivariate distributions of the parent and student responses to all of these items are provided in appendix B.) The parent item inquires about the parent's race while the student item inquires about the student's race. Table 3.2 displays the correlation, the percentage of cases identically matched, and the relative bias for parentstudent responses to family background items. Also included in table 3.2 are the number of studentparent pairs for each item and the percent of missing cases for each studentparent pair for each item. The percent missing includes those instances where the student or the parent responded \"don't know\" to an item.25 For most of the items the percentage of missing data was not excessive-ranging from about 2 percent to about 15 percent. The high percentage of missing data in the items on mother's and father's education and their educational expectations for the student was primarily caused by a high percentage of \"don't knows\" in the student responses."}, {"section_title": "Validity of Family Background Items", "text": "The correlations range from a low of 0.41 for father's expectations for the student's education to a high of 0.85 for the number of older siblings. However, as was discussed in the previous chapter, the marginal distributions of a pair of variables can have a dramatic impact on the size of the correlation between them. Therefore, the percentage of student responses that identically matched parent responses and the relative bias for each variable are also presented in table 3.2. The percentage of cases matched ranged from a high of 91.6 percent for race-ethnicity to a low of 43.1 percent for mother's expectations for the student's education. Judging by the validity coefficients combined with the percentage of responses matched, students were fairly good informants of their race (r4.77, percentage matched=91.6), number of siblings (r=0.83, percentage matched=82.2), and number of older siblings (r=0.85, percentage matched=86.4).26 Students and their parents were less likely to agree on whether their parents were at home when they came home from school (father, r=0.61, nercentage matched=55.0; mother, r=0.70, percentage matched=64.9).27 Students were not good informants of their parents' occupations (father, r=0.53, percentage matched=51.8; mother, r-4.42, percentage matched=47.8)28 and their parents' expectations for their education (father, r4.41, percentage matched=47.5; mother, r4.43, percentage matched=43.1).29 While the validity coefficients for mother's and father's education were quite high (father-4.82, mother4.76), the percentars of matched cases were only moderate (father=61.0, mother=62.5). As discussed above, this indicates that there is some kind of bias in the student responses. Indeed, table 3.2 shows that the students systematically overestimated the level of their father's education by about 7 percent of the parent's response and undemstimated their mother's educational level by about 8 percent of the parent's response. In contrast, the validity coefficient for the primary language spoken in the home was moderate (r2:0.62), while the percentage of cases matched was relatively high (percentage matched=72.3), i:Idicating some sort of skewness in the marginal distributions of these variables.30 In this instance, both students and parents were most likely to respond that English was the language usually spoken in the home (see table B.13, page B-23). In fact, 53 percent of parents and 37 percent of students responded that English was the primary language spoken in the home.31 Therefore, it is somewhat misleading to use only the validity coefficient to assess the quality of this variable. Comparison of the quality of NELS:88 data to student responses in High School and Beyond. Many of the items in the NELS:88 questionnaire are similar (and in some cases identical) to the items used by Fetters et al. to evaluate the quality of student responses to the HS&B questionnaires. Figure 3.1 shows the validity coefficients for those family background variables that are directly comparable in HS&B and NELS:88. Generally, the quality of NELS:88 responses compares very favorably with those from HS&B sophomores and seniors. In all instances, however, the validity of student responses to these items was somewhat lower than in HS&B. In almost all cases there is a gradual increase in validity from younger to older students' responses. For example, the correlations between the student and the parent msponses to father's education level was 0.82 for eighth graders, 0.87 for tenth graders, and 0.89 for twelfth graders. student's parents am r-uf the same race-ethnicity and/or the parent responding to the questionnaire is a step-r7 ent It is unclear whew-. dent or the parent is the best informant on this item. Parents may like to think that they are at home who jleiz )1ildren return, and social desirability says very strongly that mothers (the primary respondegns to the parent ionnaire) should be at home when the child returns. That is, parents may overstate their presence at home becato 'he prevailing social norm dictates their presence. 28 There are several explanations for the low validity of the student reports of their parents' occupation. For example, the mismatch in these items may be due as much to arm in the parents' response as to error in the students' response. Adults also have difficulty responding to these kinds of items. Experience has shown that response errors are seen in occupational items, reganlless of the age of the respondents. Furthermore, occupational items are difficult for survey workers to code. Coding errors by survey workers could result in some amount of mismatch between students and parents. 29 For descriptive purposes, we have designated the range of r=0.75 to 1.00 to indicate high validity, 0.55 to 0.74 to describe moderate, and 0.0 to 0.54 to signify low validity. However, the defmition of high, moderate, and low validity used in this report is somewhat arbitrary. In some sense, either a measurement is valid or it is not and researchers should use some sort of dichotomy to make this judgement. 39 Weighted the validity coefficient was x, while the percent of cases matched was y. 31 These digributions are based on gnly those cases where both the parent and student had valid responses to the item (e.g., cases with invalid or missing responses were eliminated from the analysis). "}, {"section_title": "27", "text": "HS&B students were better informants concerning their mother's expectations for their education than were NELS:88 students. NELS:88 eighth grader's validity coefficient was 0.43 compared with HS&B 's coefficients of 0.57 for the sophomores and 0.59 for the seniors. There are several possible explanations for this phenomenon.. Perhaps parents have not discussed higher educadon with their children and the eighth graders themselves may have not yet given higher education much thought. (For example, almost 13 percent of parents in the NELS:88 sample said they either never talked or rarely talked to their child about the child's educational plans after high school.) Furthermore, many parents may not have clearly formed expectations for their child at this point. As their child enters high school, parents may, for the fffst time, begin thinking about the next level of education for their children. Perhaps at this point parents first communicate these expectations to their children. Validity offamily background data by student characteristics. Tables 3.3, 3.4, and 3.5 show the validity coefficients, the percentage of cases matched, and the relative bias for students' responses to family background variables separately for males and females and for students from different racial-ethnic backgrounds. There were no practical differences between males and females in the validity coefficients and the percentage of cases matched for these family background items. Females' average validity coefficient was 0.65 compared with 0.62 for males, while the percentage of cases matched was 62.4 percent for males and 64.7 percent for females. The relative bias in the student responses was also essentially the same for males and females. In contrast, differences between the validity of male and female responses in HS&B were generally greater than in NELS:88 with females furnishing consistently more valid responses than males (data not shown).   The validity coefficients and the percentage of cases matched for most items in NELS:88 was also generally lower for blacks and Hispanics than for whites or Asians. The mean validity coefficient for whites was 0.64, for Asians 0.65, for blacks 0.53, and for Hispanics 0.59. Except for the item on race-ethnicity, Asian and white students were more likely to match their parent's response than were blacks and Hispanics.32 The mean percentage of cases matched for all items was 63.1 percent for Asians, 59.8 percent for Hispanics, 56.5 percent for blacks, and 65.5 percent for whites. Black and Hispanic students in HS&B also had consistently lower validity coefficients than did other students. In addition, black students in HS&B generally provided less valid answers than did Hispanic students.33 The relative bias in student responses varied somewhat by racial-ethnic characteristics (table 3.5). Black students tended to underestimate their father's educational attainment and to overestimate their mother's, whereas Asian, Hispanic, and white students tended to do the opposite.34 On average, Asians and white students overestimated their father's educational attainment to a greater degree than did black and Hispanic students. Compared with white or Asian students, black and Hispanic students also overestimated to a greater degree their mother's educational expectations.35 As one might expect, students of lower socioeconomic status and those with lower reading abilities tended to give less valid tesponses to these family background items than did students from higher socioeconomic backgrounds and those with better reading skills (tables 3.6, 3.7, and 3.8). For example, the average validity coefficient for students in the lowest socioeconomic quartile was 0.53. In contrast, the average validity coefficient for students in the highest quartile was 0.60. However, in some instances these validity coefficients are somewhat misleading. For example, the validity coefficient for mother's education was 0.46 for the lowest socioeconomic quartile, while the same validity coefficient was 0.69 for the highest socioeconomic quartile. Nevertheless, the percentage of students who matched their pare xt's response to this item was 69.8 percent for low-SES students, but only 60.9 percent for high-SES students. The relative bias for low-SES students on this item was also lower (and in a different direction) than the relative bias for students from high-SES backgrounds (0.065 and -0.123, respectively). Lower SES students overestimated while higher SES students underestimated their mother's education. Furthermore, although the validity coefficient for mother's and father's educational expectations weir similar for students with different SES levels and trading ability quartiles, the percentage of cases matched and the relative bias were quite different for students with these various characteristics. For example, the validity coefficient for mother's educational expectations was 0.31 for students in the lowest SES quartile and 0.33 in the highest SES quartile. However, only 28 percent of low-SES students matched their parent's response to this item, while 58 percent of high-SES students did so. Furthermore, the relative bias for low-SES students was 17 percent (0.170), while the relative bias for high-SES students was about 1 percent (0.014).36 35 It is difficult to know for certain whether the parent and the student are referring to the same \"father\" or \"mother.\" For example, in some cases the parent may have defined the father as the step-father or male guardian, whereas the student may have defined \"father\" as his or her biological father. 36 Race-ethnicity, socioeconomic status, and reading ability are highly correlated with one another. Therefore, it is of no surprise that minority students, low-SES students, and students with lower reading abilities have common response patterns.  0.033 0.008 -0.004 0.001 0.046 0.008 0.002 -0.017 Number of older siblings 0.049 0.101 0.041 0.029 0.010 0.113 0.043 0.029 0.000 Father's education 0.066 0.053 -0.045 0.009 0.150 0.009 0.034 0.062 0.117 Mother's education -0.082 0.065 -0.063 -0.067 -0.123 -0.039 -0.074 -0.085 -0.103 Father's occupation Mother's ozcupation Mother home 0.085 0.085 0.074 0.073 0.106 0.085 0.086 0.083 0.087 Father home 0.009 -0.015 0.030 0.021 0.001 0.002 0.012 0.017 0.007 Other adult home -0.029 -0.062 -0.031 -0.013 -0.018 -0.053 -0.028 -0.024 -0.014 Father's expectations for student's education 0.062 0.105 0.094 0.086 0.020 0.120 0.091 0.058 0.020 Mother's expectations for student's education 0.078 0.170 0.105 0.068 0.014 0.145 0.104 0.072 0.022 Language usually spoken at home Mean of all items (absolute value) 0.052 0.077 0.055 0.041 0.049 0.068 0.053 0.048 0.043 NOTE: -signifies a variable not measured on an ordinal or interval scale and thus a bias could not be calculated. :3 3"}, {"section_title": "Validity of School-Related Variables", "text": "Six items were used to assess the validity of student responses to school-related questions (table 3.9). Included in table 3.9 are! 1) the validity coefficient; 2) the percentage of cases matched; 3) the relative bias; 4) the number of studentparent pairs for each item and; 5) the percent of missing cases for each studentparent pair for each item. For all of the items the percentage of missing data was not excessiveranging from about 3 percent to about 10 percent. Given the set of variables available for this analysis in the NELS:88 database, the assumption that parents are the most accurate morters of school-related information may not be valid. It is unclear whether the parent or the student is best informed about the student's school life. Hence, it is difficult to discuss these variables in terms of the validity of student responses. Furthermore, it is unclear to what extent parents are responding in a socially desirable manner to items such as the3;.filount of time they spend discussing school with their childien. Furthermore, several of these comparisons are more appropriately thought of as measures of the consistency of parent and student responses, rather than validity measuresthat is, they are responses to subjective, rather than factual questions. \"Schoo1 is safe\" and \"Discuss school with parents\" are questionnaire items that are related to opinion as opposed to fact.37 The time periods examined in each are also slightly different. It is interesting, nevertheless, to observe the correspondence between students and parents on all of these items. Generally, the validity coefficients for these school-related items aie lower than the validity coefficients of the family-background items shown in the previous section. While the validity coefficients for the items, \"Have the parents been warned about behavior?\" and \"Is student enrolled in a gifted class?\" are low (r=0.44 and r=0.51, respectively), the coefficients for the other variables in this list are very low. However, because the overwhelming majority of parents and students responded \"no\" to these items (i.e., the marginal distributions are skewed), the validity coefficients for these variables are somewhat misleading. The percentage of cases matched was generally quite highranging from 47 percent to 93 percent. The question on bilingual education produced a very low validity coefficient. However, while the correlation between student and parent responses to this item was especially low, the percentage of cases matched was quite high. Tal)le 3.10 shows the amount of agreement on this item between students and their parents.38 Overall 93 percent of the students and parents (or 19,018 out of 20,477 valid studentparent pairs) responded identically to this item. Almost all of the students and parents agreed that the student was not enrolled in a bilingual class. For example, 97 percent of students who said they did not attend a bilingual class had a parent who also said that their child did not attend a bilingual class. However, among the parents who said that their children attended a bilingual class, 86 percent of their children said that they did not attend such a class. Furthermore, among the students who claimed to attend a bilingual class, 91 percent of their parents said that their child did not attend this type of class. Clearly, there was little agreement between parents and students when either the child or the parent indicated that the child attended a bilingual class. However, it is less clear why this amount of disagreement should have occurred. One explanation may be in the way the items were written for the parents and the students. The patent item asked if the student was currently enrolled in a bilingual or bicultural program, while the student item asked if the student was enrolled in a program of special instruction for those whose language is not Englishfor example, bilingual education or English as a second language. These differences in wording may have affected the match between student and patent responses. Furthermore, the patents or the student may not know what a bilingual or bicultural class iseven when the student is enrolled in one. Nevertheless, given the results from this analysis analysts should use this item with extreme caution. 38 These tables were computed only on that subset of parents that said they knew whether or not their child attended a bilingual class. Student and parent responses to school-related items did not seem to correspond as well a, they did for the family background items. However, the assessment is not straightforward. The mean validity coefficient for school-related items is a low 0.26. However, the percentage of cases matched is somewhat higher for these school-related variables than for the family background variables (66 percent compared with 63 percent). To further complicate matters, the relative bias was greater for the school-related variables than for the family background variables. On the specific school-related items examined here, students were more likely to underestimate whether their parents were warned by the school about the student's grades or school behavior. Students were more likely than their parents to think of their school as a safe place. Of particular interest was the mean difference between the parent and student responses to the item \"Discuss school with parents.\" On average, parents respond that they often talk to their child about school affairs. Students, on the other hand, respond that they seldom talk to their parents about schoolrelated matters.39 39 One of the possible explanations for the discrepancy between parent and student responses to these items involves differences in the social desirability of the parent and student responses. While parents may see talking to their children as a socially desirable thing to do, students, striving for independence, may see this as less desirable."}, {"section_title": "31;", "text": "Validity of school-related data by student characteristics. There were differences between males and females in the validity of the school-related variables (tables 3.11, 3.12, and 3.13). Based on the validity coefficients, females tended more often to agree with their parents than did males on three items: \". . . enrolled in a gifted class,\" . school is safe,\" and \"discuss school with parent.\" However, pdging by the percentage of cases matched, females were better informants on all of the followin* parent-student interaction variables: \"Discuss school with parent\" (54.3 percent compared with 46.4 percent); \"Parents warned about grades\" (50.5 percent compared with 45.2 percent); and \"Parents warned about behavior\" (78.6 percent compared with 65.1 percent). In addition, the discrepancy between the parents' and the students' rating of the safety of the school was greater for parents of females than it was for parents of males (relative bias=0.037 for females and relative bias=0.005 for males). The correspondence between parent and student responses to these school-related variables also varirnd according to students' racial-ethnic backgrounds. Of particular importance is the item on bilingual education, where interestingly the correspondence between parents and students on this item (judged by the validity coefficient) was even low among those for whom bilingual education is most relevant-Hispanic and Asian students-although it was higher than for whites and blacks. The percentage of cases matched for Asians and Hispanics was also somewhat lower than for whites and blacks.   Table 3.13 -Relative bias between student and parent responses to schoolrelated variables, la sex and race-ethnicity The correspondence between parent and student responses also varied according to the student's SES background and reading ability (tables 3.14, 3.15, and 3.16). In general, the effect of these variables on the validity of the school-related variables was similar to their effect on the validity of the family background variables. For example, while the mean validity coefficient for all school-related variables was 13.6 percent higher for high-SES students than for low-SES students, the mean validity coefficient for all family background variables was 13.2 percent higher for high-SES students. Furthermore, the validity coefficients and the percentage of cases matched between parent and student responses regarding the frequency of parent-student and parent-school interactions was greater for students with higher SES levels and reading abilities.  There are several items in the NELS:88 student questionnaire that can be used to test the consistency of student responses to questions about their school life, thus providing some measure of the validity of their responses to these items. Although these variables cannot be assessed in terms of classical validity or reliability, the pattern of responses to these items can shed some light on the reliability of average student responses. Although there is no way to know which patterns are \"correct,\" judgments can be made about the most probable patterns of responses. As an illustration of this kind of analysis, three items in the questionnaire that examine the nature of the mathematics classes attended by NELS:88 students are presented below. Specifically the items ask students if they attend at least once a week: 1) zn Algebra or other advanced mathematics class; 2) a regular math class; or 3) a remedial math class. One would expect that these classes would generally be mutually exclusive.40 That is, if students attended a remedial mathematics class, they would not attend an Algebra or advanced mathematics class. Table 4.1 shows that this is indeed the most common pattern. About 29 percent of NELS:88 students responded that they attended Algebra or advanced mathematics . only, about 53 percent responded that they attended regular mathematics only, and about three percent responded that they attended remedial mathematics only (these figures are underlined in the table). Therefore, more than 85 percent of students responded in a pattern that seems reasonable. However, the other patterns of attendance may also be valid, students may indeed take regular mathematics and Algebra concurrently (perhaps in the same class). The only unreasonable patterns appear to occur among those students who report attending a remedial class and an Algebra class, those who report attending a remedial, a regular, and an Algebra class, and those who report attending none of these math classes.c However, the proportion of students who report the combination of remedial and Algebra classes was quite smallless than 2 percent. Table 4.1 also shows mathematics teachers' ratings of the ability levels of the classes attended by the student. These ability ratings correspond well with the class type reported by the student. Among students who reported attending only Algebra, 69 percent of their mathematics teachers reported that the math class these students attend was for high-ability students. Among students who reported attending a regular mathematics class only, 49 percent of their mathematics teachers reported that their class was of average ability. Finally, among students reporting attending remedial mathematics only, about 67 percent of their mathematics teachers reported that their class was of low mathematics ability. Similarly, there are a series of questions in NELS:88 exploring the students' participation in extracurricular activities. To examine the internal consistency of these variables, table 4.2 shows the number of extracurricular activities reported by the students. Clearly most students are reporting participating in a reasonable number of activities. However, a few students (about 3 percent) report participation in 10 or more activities-more than one would reasonably expect students to have sufficient time to undertake. Of course, a small group of students may indeed participate in this many extracurricular activities, and depending on the specific clubs or organizations, these activities may overlap a great deal. However, 58 students reported participating in all 21 of these activities. To take this exploration one step further, figure 4.1 compares the percentage of administrators who reported their schools offered academic honors societies, computer club, drama club, science club, and math club with the student's report of whether they participated in these activities.42 Clearly there is some discrepancy between the activities that administrators claim are offered at the school and those in which the students claim to be participating. For example, among students who said they had participated as an officer in an academic honors society, 25 percent of their principals (or school administrators) said that such honor societies were not offered at their school."}, {"section_title": "38", "text": "42 Whik the student item on extracurricular activities does not explicitly ask about school-sponsored activities, it is difficult to imagine that the activities listed in figure 4.1 are often offered outside of school.  It also seems clear from the wording of the two items that there should not have been any confusion over whether the item refernd to just school activities or to activities outside of school. The student question was: Have you or will you have participated in any of the following school activities during the current school year, either as a member, or as an officer (for example, vice-president, coordinator, team captain)? (Emphasis added) While the administrator question was: Are the following activities available to eighth-grade students in your school? (Emphasis added) Clearly both the administrator and student items ask specifically about these activities at the school. There are several plausible explanations for the apparent mismatch between student and administrator responses to these items. One reason might be that a few administrators misread the item and/or were not informed about the availability of these activities at their school. Because there is only one administrator per school, but up to 26 students in the school, an error by a handful of administrators can have a large effect on the overall number of mismatches. However, this does not appear to be the case here. Using the \"Academic Honors Society\" activity as an example, table 4.3 shows that the mismatch problem is spread over a wide range of schools and is not limited to a small number of school administrators. There were 911 students who said they were either participants or officers in academic honors societies but whose school administrator said this activity was not offered. These students were located in 337 schools (representing about one-third of all the schools in the sample). About one-third of the schools with a mismatch (32.6 percent) had only one student with a mismatch. (A similar pattern is exhibited with other extracurricular activities variables.)  Another explanation might be that the mismatches are due to a limited number of low-ability students either misreading the question or not giving the item proper attention. (This item is the second to last one on the student questionnaire.) However, the data reported in table 4.4 does not support this explanation. While about 31 percent of the self-reported student participants in the low-reading ability quartile had a mismatch with their administrator on this item, about 31 percent of self-reported participants in the high-ability quartile also had a mismatch with their administrators. Furthermore this represented a substantial number of students in the high-ability quartilemore than 1,200 students. Another explanation for the over-reporting of extracurricular activities might be a response set induced by fatigue. These items are at the end of the questionnaire and students might have become tired answering questions.43 Yet another explanation is that these items are seen as socially desirable activities and students are reluctant to report not participating in any activities. Still another explanation is that some of these activities take place in the individual classrooms and are not offered at an school-wide level."}, {"section_title": "CHAPTER 5 RELIABILITY OF SCALES", "text": "Several scales were created from the NELS:88 base-year student, teacher, and school administrator data files. Some of these scales, such as the Self-Concept and Locus of Control scales, had been created by the National Opinion Research Center (NORC) and me included as a part of the public release file. Other scales, such as teacher engagement, academic press, discipline climate, and student behavior, were created by MPR Associates for use in special analyses of NELS:88 data. This chapter explores the inter-item reliability of these scales using the criteria of Cronbach's Alpha. Alpha is a measure of the internal consistency of a scale, that is, how well the items in the scale correlate with one another. Six of these scales were created from the student file, and five were created from the school administrator file. Appendix A provides a detailed discussion of the methodology used to create these scales and provides a list of the component survey items that make up each of these variables."}, {"section_title": "Student-Level Scales", "text": "The scales constructed from the NELS:88 student file that were analyzed in this report are shown in table 5.1. Notice that there are two Locus of Control and two Self-Concept scales. The first version of each of these scales (labeled 1) is most directly comparable with the Locus of Control and Self-Concept scales in HS&B. However, the second version of these scales takes advantage of the increased number of relevant items in NELS:88 to create measuies designed to be more stable. \" if the items in the scale are standardized to have the same variance, alpha can be computed using the following formula: where k is the number of items in the scale and is the average correlation between items. This version of the Locus of Control scale was created from an expanded list of items in the NELS:88 database."}, {"section_title": "Self-concept 1 4", "text": "This scale measures the students' perceptions about their own self-worth. This version of the scale most closely matches the scale from HS&B."}, {"section_title": "Self-concept 2", "text": "Teacher quality 7   5This version of the Self-Concept scale was created from an expanded list of items in the NELS:88 database. This scale measures the student's perception of the quality of the teaching staff at their school. Table 5.2 shows the reliability analysis of these scales. The reliabilities of the Locus of Control (1) and Self-Concept (1) scales for NELS:88 compare well with the reliabilities of similar scales for the HS&B sophomores and seniors. Both the new Locus of Control and Self-Concept scales have greater reliabilities than their HS&B equivalents. The reliability of the new Locus of Control variable is 0.678 (compared with 0.572) and the reliability of the new Self-Concept scale is 0.785 (compared with 0.734). The higher reliability of the new scales is direztly related to the increase in the number of items in the new scales. The student version of the school problems scale is also highly reliable (0.920). The teliability of these scales for students with differing characteristics is presented in table 5.3. Student characteristics were related to the reliability of some scales more than others. For e-ample, the reliability of the school problems scale is quite consistent (and quite high) across all groups regardless of the sex, raceethnicity, SES, or reading ability. However, the mliability of both the old and new Self-Concept scales was substantially lower for black students and for students with low reading abilities than it was for other students. The reliability for the old Self-Concept scale was 0.734 for all students, but only 0.569 for black students and 0.686 for students with low reading abilities. Similarly, the reliability of the new Locus of Control scale was 0.678 overall, but only 0.590 for students with low reading ability.  Table 5.4 shows the scales from the school administrator file of NELS:88 that were analyzed in this report. Appendix A pmvides a detailed description of the manner in which these scales and the other scales were constructed as well as a listing of their component items. The reliabilities of all administrator variables are all fairly high (table 5.5). The reliability of these variables (school problems, teacher involvement, academic press, school security, and discipline climate) ranges from a low of 0.708 for academic press to a high of 0.881 for the school problems scale."}, {"section_title": "48", "text": "5 1 .708 .747 .820 Table 5.6 presents the reliability of these scales for different types of schools. The reliability of most of the scales seemed unrelated to a set of selected school characteristics. However, the academic press scale, though fairly reliable overall, was less reliable for schools in the north central region of the country. The academic press scale was not reliable for Catholic schools. Academic press was more reliable in schools that had a departmentalized school structure than it was in other schools. The scale measuring the discipline climate of the school was less reliable for Catholic schools and for those in the north central region than for other schools.45 45 There seems to be little relationship between the poor performance of the north central and in Catholic schools on the administrator composites. While Catholic schools make up c large proportion of sampled schooLs in the north central region (11.7 percent of sampled schools in the region), they make up an even greater share of schools in the northeast region (21.0 percent of sampled schools in the region). ti r 9 4., Tne low reliability of the academic press variable for the Catholic schools can be partially explained by the low variance for the individual component variables that make up the academic press scale for the Catholic school sample. The academic press scale was constructed from four variables: \"Students place a high priority on learning at this school\" (BYSC47C); \"Teachers encourage students to do their best\" (BYSC47E); \"Students are expected to do homework\" (BYSC47F); and \"Students face competition for grades\" (BYSC470). Response categories for these variables ranged from never (1) tc, always (5). The variances for all of these variables were substantially smaller for the Catholic schools than they were for other schools (table 5.7). There was essentially no variance for the Catholic schools on BYSC47F (\"Students are expected to do homework\"), because almost all Catholic school administrators responded with \"always\" to this item. (Hence the 4.94 mean for Catholic schools on this item.) For Catholic schools, the small variance for this item resulted in a low inter-item correlation and a low reliability for this scale. "}, {"section_title": "Dimensionality of Locus of Control and Self-Concept Scales", "text": "The examination of scale reliabilities presented thus far was meant to give some indication of item construct validity, a desirable property of items that enhances their quality. That is, reliability is a necessary, but not sufficient condition for validity. One can take the analysis of construct validity further by examining the factor structure of the items. While the reliability analysis signals the presence of at least one dimension underlying the set of items in question, exploratory factor analysis can more precisely indicate the level of dimensionality. It is important to explore the dimensionality of a scale to assure that the scales are measuring the same thing when they are applied to different subpopulations. Confirmatory factor analysis can then be used to make an assessment of the appropriateness of the dimensional solution for the entire sample and for different subgroups of respondents. In the analyses that follow, first, exploratory factor analyses were conducted on the items making up the new Locus of Control and Self-Concept scales.\" Next, confirmatory factor analyses were conducted for the entire group of respondents and for selected subgroups. Factor analysis of the Locus of Control scale. The first set of factor analyses were conducted on the six items comprising the measure of Locus of Control. The variable names and abbreviated text for these six items are listed in table 5.8. Before analyzing the items, values for BYS44K were reversed to make its di xtion comparable to the other items in the scale. Possible responses to these items were! 1) strongly agree; 2) agree; 3) disagree; and 4) strongly disagree. "}, {"section_title": "BYS44M", "text": "Chance and luck are important in my life Table 5.9 shows the correlations among the items, the varimax rotated factor matrix, and some factor statistics. Two factors, explaining 37.0 percent of the variance of the six items, were obtained. The first factor, explaining 28.8 percent of tly item variances, includes items that emphasize obstacles to attempts to control one's life, r one's personal efficacy (e.g., \"Every time I try to get ahead something or someone stops me\"). The second factor, explaining 8.2 percent of the item variances, includes items that emphasize the role of chance and luck in one's life. The two factor solution is similar to a previously found distinction in the literature on Locus of Control, that individuals distinguish between control by other people and control by impersonal forces.47 Table 5.9 -Correlations, varimax rotated factor matrix, and factor statistics for locus of control items Maximum likelihood confirmatory factor analyses were conducted to statistically examine the degree to which a two factor model better explains responses to the six items than a one factor model. Results were first obtained for the entire sample and then for subsamples for which reliability coefficients showed substantial variation. These were black versus nonblack respondents and respondents at the various reading quaitile levels. Results are shown in table 5.10.  (1988). PC -USREL 7.12. Mooresville, Indiana: Scientific Software, Inc. bModels 1 and 2 represent the one and two factor models. CDegrees of freedom me the number of covariances among the variables minus the number of parameters estimated. For the one factor model the number of parameters is equal to the number of factor loadings (6). For the two factor model the number of rerameters is equal to the number of factor loadings (6) plus the number of correlations between factors (1). dMaximum likelihood chi-square goodness-of-fit test statistic. ellie difference between chi-squares for related models is distributed as chi-squaie with degrees of freedom equal to the difference in degrees of freedom between the two models. fThe chi-square for model 1 divided by the chi-square for model 2. The results reported in table 5.10 indicate that for the entire sample and for the various subsamples under consideration the two factor model fits the data substantially better than a one factor model, confirming that the data are not unidimensional. It should be noted that neither model fits the data, as indicated by the large maximum likelihood chi-square statistic:18 This statistic is extmmely sensitive to model deviations in large samples such as the ones used in this analysis. For present purposes, it is not important that either model fits so much as that one model, for example, the two factor model, fits the data better than an alternative model, for example, the one factor model:19 The large difference in chi-square values between the models for the entire sample and for the subpoups indicate the significant improvement in fit of the two factor model over the one factor model. As an indication of relative improvement of the fit across the subgroups, the ratio of chisquares for the two models (model 1 divided by model 2) is presented in the last column. For the entire sample and the two race-ethnicity subsamples this ratio is similar in magnitude, suggesting that the improvement in fit for the two factor model compared with the one factor model is similar for these groups of respondents. This does not appear to be the case when the different reading quartile subgroups are considered. Here the improvement in fit is substantially greater for respondents in the highest reading quartile subgroup than for respondents in the lowest reading quartile subgroup, with improvement increasing steadily for the two middle groups. This suggests that, as reading ability increases, the two factor model becomes a more viable explanation of response: to the Locus of Control items. This is also seen in the bigger differences between the chi-square statistics for the two models for the higher versus lower reading groups (these differences are roughly comparable because the subgroups are made up of nearly equal numbers of respondents). Substantively, these results suggest that respondents with greater reading ability are better able to make the distinction between obstacles to achieving their goals and the role of chance and luck in their lives. Factor analysis of the Self-Concept scale. The second set of factor analyses were conducted on the seven items comprising the Self-Concept scale. The variable names and abbreviated text for these items are listed in table 5.11. Before analyzing the items, values for BYS44A, BYS44D, BYS44E, AND BYS44H were reversed to make the directions of all the items in the scale comparable. WeS44L I feel I do not have much to be proud of Table 5.12 shows the correlations among the items, the varimax rotated factor matrix, and some factor statistics. Two factors, explaining 46.2 percent of the variance of the six items, were obtained. The first factor, explaining 36.5 percent of the item variances, includes items that ask about general evaluations of oneself (e.g., \"I am a person of worth\"). The second factor, explaining 9.7 percent of the item variances, includes items referring to transient self-evaluations or evaluations occurring during specific instances (e.g., \"At times I feel I am no good at all\"). It is no contradiction for respondents to report, for example, that they are generally positive about themselves, but that at times they feel negative.50 However, discerning the dfference is a relatively subtle distinction and, as with the distinction made for the Locus of Control items, should be more pronounced among the students with greater verbal ability. As with Locus of Control, maximum likelihood confirmatory factor analyses were conducted to examine the degree to which a two factor model better fit the Self-Concept data than a one factor model. As for the Locus of Control analysis, results were first obtained for the entire sample and then for subsainples for which reliability coefficients showed substantial variation. Again, these were black versus nonblack respondents, and respondents at various reading quartile levels. Results are shown in table 5.13. The results indicate that for the entire sample and for the various subsamples the two factor model fits the data substantially better than a one factor model, once again confirming that the responses are not unidimensional. As with the Locus of Control data, it should be noted that neither model fits the data. However, the large difference in chi-square values between the models for the entire sample and for the subgoups indicate the significant improvement in fit of the two factor model over the one factor model.  (1988). PC -I.JSREL 7.12. Mooresville, Indiana: Scientific Software, Inc. bModels 1 and 2 represent the one and two factor models. CDegrees of freedom are the number of covariances among the variables minus the number of parameters estimated. For the one factor model the number of parameters is equal to the number of factor loadings (6). For the two factor model the number of parameters is equal to the number of factor loadings (6) plus the number of correlations between factors (1), dMaximum likelihood chi-square goodness-of-fit test statistic. eThe difference between chi-squares for related models is distributed as chi-square with degrees of freedom equal to the difference in degrees of freedom between the two models. file chi-square for model 1 divided by the chi-square for model 2."}, {"section_title": "f CHAIYIER CONCLUSIONS", "text": "Base-year data collected by the National Education Longitudinal Study of 1988 provides the foundation upon which the rest of the study will be built. Thus, it is important to carefully assess the quality of the data. Furthermote, because the initial phase of the study relies in part on self-reported data from eighth-grade studentsand younger reporters are assumed to be more unreliable reportersit is important to document the extent to which analysts can rely on the accuracy of the self-reported data for this age group. Generally, the student self-reported data on family background items were reliable and accurate. In fact, the validity coefficients of family background characteristics in NELS:88 rival those of the older cohorts in HS&B. Generally, the correspondence between the parent and student on these items, although lower than those in HS&B, was well within conventional standards of validity. These results are consistent with an earlier study by Kerckhoff et al., which concluded that older children are more accurate reporters than younger children, and that the validity of reports by children increases with age.51 Unfortunately, the validity of most of the school-related items was not as high as those for the family background items. This also parallels the findings from other national surveys. For instance, Fetters et al. found in HS&B that the 10th and 12th graders were not always the best informants about their own school experiences. When judged against the standard of transcripts, students from HS&B consistently misreported their grades, their coursework, and even theit field of study. Thus, the 8th graders in NELS:88 were no different from their 10th-and 12thgrade counterparts. However, it was also demonstrated that the low validity coefficients for the school-related items were somewhat misleading. When the percentage of cases matched was examined, the correspondence between the student and parent responses to these items appeamd to be much better. In an attempt to provide a thorough assessment of the quality of the NELS:88, we have presented three indicators in this report: validity coefficients, percentage of cases matched, and relative bias. Even with these three indicators, there were times when the information they provided was not sufficient for our assessment, so we also examined the actual bivariate distributions of the items in question. As demonstrated above, judgments on the quality of the data may vary depending on what indicator is used. This demonstrates the importance for analysts to use more than one indicator of \"data quality\" before judging the suitability of certain data elements for analyses. Furthermore, despite the difficulties students may have had in responding to some of the school-related items, our analysis of the internal consistency of many of the school items (such as the math course-taking pattern items) showed that the vast majority of students were answering the items consistently. Although we could not directly check the validity of these items, our exploratory analysis did not uncover any reasons to doubt the average student response to these questions taken as a whole. In addition, this analysis reinforces how important it is to create scales and/or multiple indicators of analytical concepts. That is, it is vital for analysts to use all of the available data to cross-check student responses before proceeding with their analyses. Indicators or scales built on this practice will be more reliable and accurate than individual items taken in isolation. We have also shown that tho validity of student responses was dependent on the characteristics of the students tht mselves. In fact, students from high socioeconomic backgrounds, those with higher abilities in reading, white or Asian students, and female students were mole likely to give valid answers than were their peers. These differences, however, were generally quite small. Furthermore, differences in the validity and consistency of responses due to raceethnicity, sex, SES, or reading level were less pronounced than what was observed with older cohorts in HS&B. Neve:Theless, analysts may want to consider using these validity coefficients as adjustment factors in models that incorporate a provision for measurement error. In so doing, it will be important for them to ensure that these adjustment factors correspond to the subgroup of students that they are investigating. The scale variables provided in the NELS:88 database and those that NORC and MPR have created for other analyses proved to be reliable. The school level scales were particularly consistent among respondents. It seems clear from these results that analysts should make full use of these and other scales when conductinf their own research. In most cases these scales tend to be reliable. However, the lower reliability of some of the scales for certain subgroups (e.g., blacks and those in the lower reading group) should cause researchers to use care when analyzing these subgroups in isolation. Furthermore, the results of the confirmatory factor analysis suggest that the Locus of Control and Self-Concept scales might have slightly different interpretations for respondents in these subgroups. Finally, given some of the inconsistencies in selected school-related items discovered in this analysis (e.g., enrollment in bilingual education classes), it is important that sufficient resources be allocated in future waves of NELS:88 to the collection and processing of transcript data. (Transcript data will be collected as part of the NELS:88 second follow-up.) As with HS&B, student self-reported data on school experiences should be used with caution. Furthermore, it will also be important to continue to collect data from students' teachers on individual classroom behavior. The teacher file in NELS:88 already has much more extensive information about classroom programs and practices than in other databases. It would be prudent to continue to collect context data from the teachers of survey rospondents. This appendix has three purposes. First, it briefly describes the NELS:88 database. Second, this section discusses the methods used in recoding some of the van'ables and in creating new variables to ensure comparability across parent and student responses and so comparable response categories from the different questionnaires could be compared. For example, in some cases an item asked similar questions of both the parent and the student, but provided different response categories. The third purpose of this section is to describe the procedures by which the composite variables examined in chapter 5 were created The longitudinal studies program provides statistics on the education, work, and family experiences of young adults during the pivotal transitions from eighth grade to high school and from high school to postsecondary education and the world of work. Since NLS-72, each successive longitudinal study conducted by NCES has grown substantially in complexity with respect to sample specifications, sources of information, and instrument sophistication. The current NELS:88 design reflects two decades of succerisful experiences with longitudinal education studies. NELS:88 differs from both NLS-72 and HS&B in that the first data collection phase begins in the eighth grade rather than high school. The decision to begin the study in eighth grade was made to provide pre-high school baseline data and construct a national database with the capacity to systematically examine the critical tar -'tion students undergo moving from eighth ga& in elementary, middle, or junior high school to tenth grade in secondary school. This period of transition is important for exploring broader policy issues such as how students are counseled into specific high school programs and courses and what impact program choice has on their tenth grade experiences. Information will be available to policy makers about the effects of this transiticn on student attitudes, aspirations, self-esteem, and academic performances. Base-year design. The Base-Year Survey was conducted in spring 1988. The study design includes a clustered, stratified national probability sample of approximately 1,000 schools (800 public schools and 200 private schools, including parochial institutions) in the U.S. that enroll eighth-grade students. Over 26,000 students across the U.S. participated in the Base-Year Study. The sample is representative of the nation's eighth-grade population, totalling about 3 million eighth graders in more than 37,000 schools in spring 1988. Questionnaires and a cognitive test were administered to each student in the NELS:88 sample. The student questionnaire covered school experiences, activities, attitudes, plans, selected background characteristics, and language proficiency. Other groups of respondents provided additional types of information. An administrator such as the principal filled out a questionnaire about the school; two teachers of each student were asked to answer qiestions about the student, about themselves, and about their school; and a sample of students parents was surveyed regarding family characteristics and student activities. The total survey effort thus provides a comprehensive database for analyses."}, {"section_title": "Note on Weighting", "text": "All of the analyses presented in this report have been conducted on unweighted data. Errors in responses to questionnaire items are, by their very nature, directly linked to the wording of a particular item, the placement of the particular item in the questionnaire, and the conditions uncler which the questionnaire was administered. Therefore, the conclusions of a report on the quality of responses of a specific questionnaire must limit itself to the respondents who in reality answered the questionnaire and cannot legitimately generalize to a larger population of respondents. Furthermore, since NELS:88 oversampled black and Hispanic students and black and Hispanic students *enerally gave less reliable responses, weighting the data would artificially improve the reliability of the data by assigning smaller weights to the least reliable reporters. Because inferences from the data were limited to the specific sample of students who actually participated in the survey, inferences were not made about some other hypothetical population of students that the sample represented. Therefore, there were no sampling error issues involved in this analysis and hence no tests of statistical tests were run on differences in reliability or validity between groups."}, {"section_title": "Notes on Recoding of Parent Responses", "text": "There were six questions in the student questionnaire that referral separately to mother's and father's occupations, their education, and their educational expectations. The parent questionnaire did not refer to father or wother, but instead to respondent and spouse. Furthermore, the respondent to the parent questionnaire could be a step-parent, a guardian, or a grandparent. When students answered itemt about their mother, father, or male or female guardian, it is unclear to whom students were referring in the case where the respondent to the parent questionnaire was not the mother or the, father. Were they referring to their father living outside the home, or to their stepfather or male guardian inside the home? Fortunately, this problem should have had a minimal impact on the results of the analysis in this report since approximately 95 percent of the respondents to the parent questionnaire were either the mother or father. Consequently, in this analysis all fmale respondents were classified as \"mother,\" and all male respondents were classified as \"father.\" For this analysis we created six new parent variables (table Al N.  1,3,5,7,9 2BYP1A1 = 2,4,6,8,10 When creating MOTI-IED and FATHED, the variables BYP30 and BYP31 were also recoded to match the student responses (the student variables also had to be recoded)."}, {"section_title": "G", "text": "If BYP30/P31 = set to 1 or 2 1 (LT HS) 3 or 4 2 (HS only) 5 through 10 3 (Some college) Several other variables had to be recoded so that the parent and student responses were comparable (table A2)."}, {"section_title": "Table A2", "text": "Notes on mitts of variables in the anal sis \\ . "}, {"section_title": "Discuss school with parents", "text": ""}, {"section_title": "Creation of Composite Variables", "text": "This section describes how the school problems, teacher engagement, academic press, school security, and discipline climate scales were constructed. The construction of the Locus of Control and Self Concept scales are described in chapter 5. These composite variables were constructed in the following manner. First, items were selected that seemed on face value to represent aspects of the desired concept. (For example, there were several variablPs in the student questionnaire that probed the student's attitudes about the quality of the teaching at the school. These were combined into the teacher quality scale.) Second, the dimensionality of these scales was examined by principal components analysis. Third, if the scale was judged 'o be reasonably unidimensional, the internal reliability of the scale was assessed with Cronbach's Alpha. Each item whose deletion would raise the scale's alpha was deleted from the scale and the scale's reliability was recalculated, until deletion of any variable in the scale would decrease the scale's reliability (as measured by Cronbach's Alpha). Table A3 displays the component vIriable names and abbreviated text for the items included in the composite scales constructed tbr this study.               1 1 8"}]