[{"section_title": "Abstract", "text": "Abstract-Discovering bi-multivariate associations between genetic markers and neuroimaging quantitative traits is a major task in brain imaging genetics. Sparse Canonical Correlation Analysis (SCCA) is a popular technique in this area for its powerful capability in identifying bi-multivariate relationships coupled with feature selection. The existing SCCA methods impose either the 1-norm or its variants. The 0-norm is more desirable, which however remains unexplored since the 0-norm minimization is NP-hard. In this paper, we impose the truncated 1-norm to improve the performance of the 1-norm based SCCA methods. Besides, we propose two efficient optimization algorithms and prove their convergence. The experimental results, compared with two benchmark methods, show that our method identifies better and meaningful canonical loading patterns in both simulated and real imaging genetic analyse."}, {"section_title": "I. INTRODUCTION", "text": "Brain imaging genetics has gained more and more attentions recently [1] , [2] . A major task of imaging genetics is to identify bi-multivariate associations between single nucleotide polymorphisms (SNPs) and imaging quantitative traits (QTs). Sparse canonical correlation analysis (SCCA), which is powerful in bi-multivariate relationship discovery coupled with feature selection, has become a popular technique in imaging genetic studies [3] , [4] , [5] , [6] , [7] .\nWitten et al. [3] introduced the 1 -norm (Lasso) to assure sparsity which only selects a small proportion of the features. Since then, many SCCA methods using the 1 -norm or its variants are proposed [8] . There are two major concerns regarding them. First, the 0 -norm, which only penalizes those nonzero features, is the most ideal constraint. But it is neither non-convex nor discontinuous [9] . Second, the 1 -norm constraint is not a stable feature selector and thus could incur estimation bias [10] .\nTo overcome the problem above, the truncated 1 -norm penalty (TLP) [10] , [11] is proposed. The TLP is defined as J \u03c4 (|x|) = min( |x| \u03c4 , 1) with \u03c4 being a positive tuning parameter. It approximates 0 -norm and permits desirable sparsity. In addition, TLP can be equivalently transferred to a piecewise linear function, and thus is easy to handle.\nIn this paper, we propose the TLP based SCCA (TLP-SCCA) which embraces the TLP into the CCA model. The TLP-SCCA has the following advantages [10] . First, the TLP performs as a tradeoff between the 0 and 1 functions. This means that it not only has improved feature selection, but also can be solved effectively. Second, it is an adaptive shrinkage method if \u03c4 is tuned appropriately. We propose two effective optimization algorithms, both using the alternating direction method of multipliers (ADMM) technique [12] , and they are guaranteed to converge. The experimental results, compared with two popular 1 -norm based SCCA [3] , [6] , show that both TLP-SCCA exhibit cleaner canonical loading patterns than the 1 -SCCA."}, {"section_title": "II. THE TRUNCATED 1 -NORM PENALTY", "text": "In this paper, a boldface lowercase letter denotes a vector, and a boldface uppercase letter denotes a matrix. X \u2208 R n\u00d7p denotes the SNP data, and Y \u2208 R n\u00d7q is the QT data. The truncated 1 -norm is defined as follows [13] : and \u03c4 = (middle), and 1 -norm ball (right).\nThe parameter \u03c4 is a threshold. Given an appropriate \u03c4 , TLP balances between the 0 -norm and 1 -norm according to the magnitude of the coefficients. Fig. 1 presents the norm ball of 0 -norm, 1 -norm, and TLP with different \u03c4 's. The TLP ball is more pointy than the 1 ball, and thus it is more desirable in sparse learning.\nSince TLP is non-convex, we employ the Difference of Convex functions (DC) programming technique [14] , [11] . Let f 1 (u i ) = |u i | and f 2 (u i ) = max(|u i | \u2212 \u03c4, 0), we have:\nwhere both f 1 (u i ) and f 2 (u i ) are convex. The affine minorization of f 2 (\u00b7) can be denoted as:\nwhere\nis the sub-gradient of f 2 at u i with the I(\u00b7) being an indicator function. Then the minimization of P T LP can be transferred to its subproblem,\nwhere D is a diagonal matrix with the i-th element as d i .\nIII. THE NOVEL SCCA VIA TRUNCATED 1 -NORM"}, {"section_title": "A. The TLP-SCCA Model", "text": "The TLP-SCCA can be formally defined as follows:\nare the TLP penalties. The merits are three-fold. Firstly, it has lower estimation bias than a traditional 1 -SCCA since it approximates the optimal 0 -SCCA if given an appropriate \u03c4 . Secondly, it has better sparsity-inducing ability than the 1 -SCCA. Thirdly, it is easier to solve than the 0 -SCCA."}, {"section_title": "B. The Optimization Algorithm", "text": "The TLP-SCCA is biconvex in u and v, and we solve it based on the Alternate Convex Search (ACS) strategy [15] . We show how to employ the ADMM algorithm to updating u with v fixed. 1 The ADMM combines the advantages of dual decomposition and augmented Lagrangian method [12] , which divides a large global problem into a series of small local subproblems [11] . Introducing a slack variable z, TLP-SCCA with respect to u conveys to:\nwhere \u03b2 and \u03bb are tuning parameters, \u03c4 is the parameter of the TLP, and D is a diagonal matrix with the i-th element as the sub-gradient sign(u i )I(|u i | > \u03c4 1 ) (Recall Section II)."}, {"section_title": "Algorithm 1 The Truncated 1 -norm Penalty based SCCA (TLP-SCCA)", "text": "Require:\nCanonical loadings u and v."}, {"section_title": "1: Initialize", "text": "2: Compute Cholesky factorization Fu and Fv; 3: while not converged do 4:\nSolve u according to Eq. (10);"}, {"section_title": "5:", "text": "Solve z1 according to Eq. (13);"}, {"section_title": "6:", "text": "Solve \u03bc1 according to Eq. (14);"}, {"section_title": "7:", "text": "Solve v in accordance with Step 4 \u223c 6 by swapping u and v;"}, {"section_title": "8:", "text": "Scale u so that ||Xu|| 2 = 1, and v so that ||Yv|| 2 = 1."}, {"section_title": "9: end while", "text": "Then we construct the augmented Lagrangian function:\nwhere \u03bc is the augmented Lagrangian multipliers, and \u03c1 \u2265 0 is the dual update step length. According to ADMM, this problem can be solved by sequentially updating u, z and \u03bc."}, {"section_title": "1) u-update:", "text": "In each iteration, u can be obtained from:\nwhere z, \u03bc and v are all fixed as constant. Then we can solve u by u = F \u22121 b, where\nObviously, F remains unchanged during the iteration, and thus we compute it at the beginning of the algorithm. The Cholesky factorization of the positive definite F is F = R T R, where R is an upper triangular matrix. Then u can be obtained by solving two linear system equations:\n2) z-update: Likewise, z can be obtained from:\nWe equivalently reformulate the equation as follows.\nThen the soft-thresholding method can be used:\n3) \u03bc-update: The dual update of \u03bc is shown as follows.\nAccording to the ACS strategy, v can be solved similarly by fixing u. The key steps are exhibited in Algorithm 1."}, {"section_title": "Algorithm 2 The TLP-SCCA with Restart (rTLP-SCCA)", "text": "Require:\nCanonical loadings u and v.\n1: while not converged do 2:\nCompute u, z1 and \u03bc1 using Algorithm 1;"}, {"section_title": "3:", "text": "if r k c < \u03b7r\n5:\n6:\nSolve v in accordance with Step 4 \u223c 8 by swapping u and v;\n10:\nScale u so that ||Xu|| 2 = 1, and v so that ||Yv|| 2 = 1."}, {"section_title": "11: end while", "text": ""}, {"section_title": "C. The Optimization Algorithm with Restart", "text": "We now propose an optimization algorithm with more stability. The algorithm uses a restart rule depending on the summation of the running primal and dual error [16] .\nWe define the primal residual at the (k + 1)\n, and the dual residual as r\nThe key steps of the rTLP-SCCA (TLP-SCCA with restart) are shown in Algorithm 2. If the residual r c decreases by a factor of at least \u03b7 (Step 3), we use the Nesterov's accelerated gradient descent method to accelerate [12] (Steps 4-5). Otherwise, we restart the iteration (Step 7)."}, {"section_title": "D. Computational Analysis and Convergence", "text": "In Algorithm 1, the initialization step is simple.\nStep 2 of Cholesky factorization only needs to be done once. According to Eq. (10), Steps 4 involves solving two linear systems. Thus its time complexity is O(pq).\nStep 5 is the soft-thresholding operator whose time consumption is O(p).\nStep 6 is quite simple and its time complexity is O(p). Thus the time complexity of Steps 4-6 is O(pq). Similarly, the time complexity of solving v is O(pq). In summary, the complexity of each iteration is O(pq).\nThe difference between TLP-SCCA and rTLP-SCCA is the restart steps which are easy to calculate. Thus the time complexity of each iteration of rTLP-SCCA is also O(pq).\nThe TLP-SCCA problem is biconvex in terms of u and v. According to Theorem 4.5 in [15] , the TLP-SCCA converges monotonically depending on two conditions: (1) The set B = (u, v) \u2208 R p \u00d7 R q is bounded. (2) Both u-update and v-update are solvable.\nWe note that u is bounded by ||Xu|| 2 = 1, and v is bounded by ||Yv|| 2 = 1. Therefore, B is bounded because that u and v are bounded. Obviously, the u-update and v-update are solvable guaranteed by ADMM method. Therefore, the TLP-SCCA converges during iteration. [15] ."}, {"section_title": "IV. EXPERIMENTAL STUDY", "text": ""}, {"section_title": "A. Benchmark Methods", "text": "In this paper, we mainly focus on the improved performance of SCCA using the TLP regularization. Thus we compare our method with the 1 -SCCA, other than those structured SCCA methods [5] , [17] , [7] , [18] , [19] . We use two different implementations of 1 -SCCA as benchmark. The first one is named as L1-SCCA which is contained in the Penalized Multivariate Analysis software toolkit [3] , and the second one is named as L1-S2CCA by considering each group consists of only one covariate [6] ."}, {"section_title": "B. Parameter Tuning", "text": "Take the u-update as an example, we have four parameters \u03bb, \u03b2, \u03c4 , and \u03c1, and two unknown variables z, \u03bc to be decided in advance. z is a slack variable which is split from u. We simply set z = 0. \u03bc is the dual variable and is set to 0 too. For the remaining parameters, we further reduce the computational burden based on some heuristic tricks. First, we set the dual update step size \u03c1 to a conservative value to permit a good accuracy. Second, \u03c4 is a key parameter in TLP-SCCA, thus it is reasonable to use an u-related \u03c4 . We set \u03c4 = E(|u|)+T * \u03c3(|u|) and update it during the iteration, where E(|u|) and \u03c3(|u|) are the mean and deviation of |u|. T is a constant which controls \u03c4 , and further controls how many u i 's are penalized by 0 -norm. According to normal distribution, T = 1.96 indicating that 95% of |u| are smaller than \u03c4 . This will yield a reasonable sparse result. At last, we only have two parameters, i.e. \u03bb 1 and \u03b2 1 , left. Likewise, this is the same situation for v-update. Thus we have four parameters to be tuned and utilize the nested 3-fold crossvalidation to tune them. In this study, we tune the parameters from the range of 10 \u03b8 with \u03b8 \u2208 [\u22123, \u22122, \u22121, 0, 1, 2, 3]. All methods utilize the same partition and run on the same platform to ensure a fair comparison. The stopping criteria is |obj (u) k \u2212 obj(u) k+1 |/obj(u) k \u2264 , where = 10 \u22125 is a desirable error bound in this paper."}, {"section_title": "C. Results on Synthetic Data", "text": "Two synthetic datasets are used in the experiment. Both datasets are generated by the following steps. 1) We first set up two vectors u with length p and v with length q separately. We then let most of elements of both u and v be zero. 2) We introduce a latent variable z with z \u223c N (0, I n\u00d7n ). 3) We generate X with each element x i \u223c N (z i u, \u03c3), where \u03c3 stands for the standard deviation of noise. We also create Y with y i \u223c N (z i v, \u03c3) . Fig. 2 shows the estimated canonical loadings. Both TLP-SCCA and rTLP-SCCA obtain cleaner canonical loading patterns than the competing methods. Our methods have smaller estimation bias regarding the canonical loadings. Table I shows results of the estimated correlation coefficients. We observe that three methods, i.e. the L1-SCCA, TLP-SCCA and rTLP-SCCA, perform similarly. The results on synthetic datasets reveal that although TLP-SCCA and rTLP-SCCA do not hold the highest correlation coefficients, they obtain the most unbiased canonical loadings. "}, {"section_title": "D. Results on Real Brain Imaging Genetics Data", "text": "We obtained the real imaging genetics data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The primary goal of conducting ADNI was to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to help detect the mild cognitive impairment (MCI) and early Alzheimer's disease (AD). Please see www.adni-info.org for up-to-date information.\nThe genotyping and baseline amyloid imaging data of 283 non-Hispanic Caucasian participants were downloaded from the ADNI website. The imaging data were preprocessed in accordance with [7] , and then the effects of the baseline age, gender, education and handedness were pre-adjusted by regression. After that we extracted 191 ROI level mean amyloid measurements. The genotyping data contain 58 SNP markers from the AD-related genes. Our goal is to verify if TLP-SCCA can find out the meaningful associations between the amyloid data and the SNP data. rs2927478  rs2968180  rs1135062  rs7026  rs1871045  rs11668536  rs10402271  rs4803760  rs34374273  rs10426423  rs1871047  rs440277  rs3852856  rs377702  rs519825  rs8105340  rs12610605  rs4803766  rs8104483  rs11879589  rs395908  rs2075642  rs387976  rs11667640  rs3729640  rs6859  rs283814  rs157580  rs2075650  rs157582  rs8106922  rs1160985  rs405697  rs405509  rs769449  rs429358  rs439401  rs445925  rs584007  rs4420638  rs5157  rs1132899  rs5167  rs2288911  rs10413089  rs3760627  rs204905  rs760114  rs11668758  rs16979595  rs7253458  rs7257916  rs8111069  rs16979600  rs7257610  rs3786505  rs204468 We first present the heat map regarding the estimated canonical loadings in Fig. 3 . On the genotyping data (left penal), we clearly see that the L1-SCCA, TLP-SCCA and rTLP-SCCA find out 5 out of 58 SNPs. The five SNPs are rs429358 and rs769449 from APOE, rs157582 and rs2075650 from TOMM40, rs4420638 from APOC1. Note that all the five SNPs have been demonstrated to be highly correlated with AD. The L1-S2CCA only discovers the SNP rs429358, which means it loses the useful information carried by other four SNPs. On the imaging data (right panel), both TLP-SCCA and rTLP-SCCA identify only one strong signal, showing very clear and clean canonical loading pattern. The identified frontal region in the brain is known to be related with AD [20] . The results indicate that the TLP-SCCA can identify meaningful associations between genetic data and imaging data.\nWe also show the estimated correlation coefficients in Table II . There is no significant difference among various methods. This phenomenon reveals that generating a clean canonical pattern does not always correspond to a higher correlation coefficient [10] . Nevertheless, the TLP-SCCA and rTLP-SCCA have the potential to balance between the overpenalization and the overfitting, indicating their promising power in imaging genetic studies."}, {"section_title": "V. CONCLUSION", "text": "In this paper, we proposed the TLP-SCCA coupled with two effective optimization algorithms, i.e. TLP-SCCA and rTLP-SCCA. They identified cleaner canonical loading patterns which reduced the estimation bias of 1 -SCCA. We analyzed their convergence and computation complexity.\nWe evaluated our methods and two 1 -SCCA methods on both synthetic and real data. Both TLP-SCCA and rTLP-SCCA identified cleaner canonical loadings, indicating they successfully and accurately recognized the signals which were the closest to the ground truth. The results on the real imaging genetic data showed that TLP-SCCA and rTLP-SCCA yielded more meaningful canonical loading patterns than the benchmarks. Possible future directions include (1) investigating the performance trajectory under different \u03c4 's, and (2) using relevant structure information to guide the bimultivariate association identification."}]