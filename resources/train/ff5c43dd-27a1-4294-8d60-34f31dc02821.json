[{"section_title": "Abstract", "text": "In this paper, we aim to predict conversion and time-to-conversion of mild cognitive impairment (MCI) patients using multi-modal neuroimaging data and clinical data, via cross-sectional and longitudinal studies. However, such data are often heterogeneous, high-dimensional, noisy, and incomplete. We thus propose a framework that includes sparse feature selection, low-rank affinity pursuit denoising (LRAD), and low-rank matrix completion (LRMC) in this study. Specifically, we first use sparse linear regressions to remove unrelated features. Then, considering the heterogeneity of the MCI data, which can be assumed as a union of multiple subspaces, we propose to use a low rank subspace method (i.e., LRAD) to denoise the data. Finally, we employ LRMC algorithm with three data fitting terms and one inequality constraint for joint conversion and time-to-conversion predictions. Our framework aims to answer a very important but yet rarely explored question in AD study, i.e., when will the MCI convert to AD? This is different from survival analysis, which provides the probabilities of conversion at different time points that are mainly used for global analysis, while our time-to-conversion prediction is for each individual subject. Evaluations using the ADNI dataset indicate that our method outperforms conventional LRMC and other state-of-the-art methods. Our method achieves a maximal pMCI classification accuracy of 84% and time prediction correlation of 0.665."}, {"section_title": "", "text": "Conversion and time-to-conversion predictions of mild cognitive impairment using low-rank affinity pursuit denoising and matrix completion "}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD) ( Association et al., 2016 ( Association et al., , 2017 is the most prevalent dementia and is commonly associated with progressive memory loss and cognitive decline. It is incurable and requires attentive care, thus imposing significant socio-economic burden on many nations. It is thus vital to detect AD in its earliest stage before its onset for possible therapeutic treatment. The prodromal stage of AD, called mild cognitive impairment (MCI), is characterized by mild but measurable decline of memory and cognition. Studies show that some MCI patients will recover over time, but more than half will progress to dementia within five years ( Gauthier et al., 2006 ) . MCI patients that will progress to AD are retrospectively categorized as progressive MCI (pMCI) patients, whereas those who remain stable as MCI are categorized as stable MCI (sMCI). In this paper, we focus on differentiating pMCI from sMCI patients and predicting the time to the event of AD conversion.\nBiomarkers based on different modalities, such as magnetic resonance imaging (MRI), positron emission topography (PET), and cerebrospinal fluid (CSF), have been widely studied for the prediction of AD progression ( Zhang et al., 2012; Li et al., 2015; Weiner et al., 2013; Zhan et al., 2015; Li et al., 2014; Adeli-Mosabbeb et al., 2015; Huang et al., 2015; Zhu et al., 2015; 2016; Zhou et al., 2017; Zhu et al., 2017; Thung et al., , 2017 . The Alzheimer's disease neuroimaging initiative (ADNI) collects these data longitudinally from subjects ranging from cognitively normal elderly subjects to AD patients in an effort to improve prediction of AD progression. However, these data are incomplete due to subject dropouts and unacquired modalities associated with factors such as study design and cost constraints. The easiest and most popular way to deal with missing data is by discarding incomplete samples ( Zhang et al., 2012 ) , which will however decrease sample size and statistical power. An alternative is to impute the missing data, via methods such as k -nearest neighbor (KNN), expectation maximization (EM), or low-rank matrix completion (LRMC) ( Troyanskaya et al., 2001; Zhu et al., 2011; Cand\u00e8s and Recht, 2009; Sanroma et al., 2014 ) . These imputation methods, however, do not perform well on data with blocks of missing values Yuan et al., 2012; Yu et al., 2014 ) , causing erroneous prediction outcomes.\nTo avoid the need for imputation, Yuan et al. (2012) proposed a method, called incomplete multiple source feature learning (iMSF) , to first divide the data into disjoint subsets of complete data, and then jointly learn the classification or prediction models for these subsets. Through joint feature learning, iMSF enforces all subset classifiers to use a common set of features for each modality. However, this will cause samples with less number of modalities to have limited number of features when making prediction. In addition, using disjoint subsets of data will also cause small sample size issue for each prediction model ( Xiang et al., 2014 ) .\nOn the other hand, the method proposed by Goldberg et al. (2010) imputes the missing feature values and target values (e.g., diagnostic status and clinical scores) simultaneously using a low-rank assumption. All samples, including those with missing feature values, and their corresponding targets are concatenated into a matrix and the unknown values are then imputed via LRMC. This approach is able to make use of the incomplete samples more effectively. Thung et al. (2014) improved the efficiency and effectiveness of this method by performing feature and sample selection before matrix completion.\nHowever, all these methods do not explicitly take into account the heterogeneous nature of the data. Recent studies ( Markesbery, 2010; Nettiksimmons et al., 2013 ) show that there is significant biological heterogeneity among ADNI amnestic MCI patients. Some MCI subjects are biologically similar to normal aging subjects, while some have the characteristic AD's pathologies, and some have other various late-life neurodegenerative pathologies ( Nettiksimmons et al., 2013; Rahimi and Kovacs, 2014 ) . Postmortem brain studies ( Markesbery, 2010; Petersen et al., 2006; Jicha et al., 2006; Cairns et al., 2015 ) on deceased MCI and AD subjects also confirm that most of them developed a mixture of neurodegenerative diseases. The comorbidities (other than AD) include argyrophilic grain dementia, Lewy body dementia, Parkinson disease, hippocampal sclerosis, and frontotemporal dementia. These studies imply that not all MCI subjects are affected by the same AD pathologies.\nIn this study, we utilize longitudinal multi-modality data to capture the complexity and heterogeneity of AD pathology. The data are heterogeneous, prone to noise, and incomplete. To deal with these problems, we recently proposed an approach ( Thung et al., 2015b ) to cluster the data into subsets using lowrank representation (LRR) and perform LRMC on the samples on each of these subsets separately, to improve the overall classification performance. This approach assumes that the data resides in a union of several low-dimensional subspaces, each spanned by a data subset, and tries to recover these subspaces through LRR. Each sample is assumed to reside in one of the subspaces. However, in reality, the samples can potentially reside across multiple subspaces ( Markesbery, 2010 ) . In addition, data clustering also reduces the number of samples associated with each subspace and hence may reduce the effectiveness of the prediction model. We have also demonstrated in ( Thung et al., 2015b ) that the prediction performance of the LRMC algorithm can also be improved by using a denoised version of the data, which can be obtained via LRR.\nIn this paper, we propose to use low-rank affinity pursuit denoising (LRAD) in combination with the sparse feature selection (FS) to improve the prediction power of LRMC for incomplete, noisy, and heterogeneous multi-modal data. More specifically, we use incomplete low-rank representation (ILRR) Shi et al., 2014 ) for LRAD, where the samples are denoised by representing them using their neighboring points. In addition, we use lasso ( Tibshirani, 1996; Liu et al., 20 09a, 20 09b; Liu and Ye, 20 09 ) to select the most discriminative features for use in prediction. Lastly, we utilize LRMC to predict the output targets, which consist of diagnostic labels (i.e., pMCI/sMCI) and conversion times. We tested our framework using longitudinal and cross-sectional multimodality MRI data and confirm that the proposed method outperforms the conventional LRMC method and other state-of-the-art methods. It is also important to note that there are many hyperparameters associated with LRMC. In this paper, we propose to use a Bayesian optimization framework to automatically select the best set of hyper-parameters. The contributions of this paper are threefold:\n1. We propose a framework for pMCI diagnosis and conversion time prediction using longitudinal multi-modal data, which can be incomplete and noisy. In comparison, previous studies in the literature ( Section 2.1 ) were often focusing on using either multi-modal or longitudinal data for pMCI diagnosis. Moreover, unlike our method which is applicable to incomplete datasets, most of the previous methods are only applicable to datasets without missing data. More importantly, timeto-conversion predictions in the literature are mostly used for global analysis based on statistical methods, while our study is one of the few non-statistical methods that addresses this issue at individual level. To the best of our knowledge, our study is the first to predict both the pMCI diagnosis and time-toconversion jointly. To this end, we propose to employ sparse feature selection to remove outlier features, ILRR to denoise the data, and finally LRMC to predict the target outputs. 2. We propose a matrix completion algorithm that is able to predict the conversion time even when some of the data are missing and censored. The missing data issue is due to missing modalities at certain time points for some subjects. In addition, our sMCI data is censored, i.e., we are unsure whether the sMCI subject will progress to AD if we increase the monitoring period indefinitely. Conventional linear regression models are not applicable to censored data, while the conventional methods that work on these data ( Section 2.2 ) only provide the \"probability\" of conversion. To this end, we design an LRMC algorithm with three data fitting terms, one for the input features, one for the diagnostic labels (binary targets), and one for the conversion time (continuous-valued targets), along with an additional inequality constraint. Our modified matrix completion algorithm enables us to predict the conversion time for the censored data (i.e., sMCI), by constraining their predicted values to be at least more than a specific value. 3. We employ a Bayesian optimization scheme to automatically select the optimal hyper-parameters for LRMC."}, {"section_title": "Related works", "text": "In this section, we briefly discuss the related previous research works."}, {"section_title": "MCI-to-AD conversion prediction", "text": "Many works ( Wei et al., 2016; Stoub et al., 2004 ) use MRI data for MCI-to-AD conversion predictions. For example, Stoub et al. (2004) used MRI-derived entorhinal volume for prediction. Wei et al. (2016) used MRI and structural network features to predict MCI-to-AD conversion. They employed sparse linear regression with stability selection to select features and then used support vector machine (SVM) for classification. They used data at baseline, and 6, 12, and 18 months before diagnosis of probable AD for prediction. The best classification accuracy they obtained was 76% using the data 6 months prior to AD diagnosis. Misra et al. (2009) used longitudinal MRI data to extract brain temporal changes for detecting MCI-to-AD conversion. However, this study used follow-up data of very short period (i.e., up-to 15 months) with unbalanced data at each cohort (i.e., pMCI and sMCI).\nSome works used multimodal data (e.g., MRI, PET, CSF, demographics, genetic data) for conversion prediction ( Davatzikos et al., 2011; Cheng et al., 2015b Cheng et al., , 2015a Dukart et al., 2016; Moradi et al., 2015 ) . Cheng et al. (2015b ) , for example, used MRI, PET, and CSF data in their studies. They employed transfer learning to borrow information from other related cohorts, i.e., AD and NC, to help select the features from MCI cohorts for MCI-to-AD conversion prediction, achieving 79% prediction accuracy. In another similar work, Cheng et al. (2015a ) employed multimodal manifold-regularized transfer learning for feature selection, and achieved 80% accuracy in conversion prediction. Xu et al. (2016) used modalityweighted sparse representation-based classification method to combine data from MRI, fluorodeoxyglucose PET, and florbetapir PET, and achieved 82.5% prediction accuracy. They defined pMCI as MCI subjects that progressed to MCI within 36 months, and defined the remaining MCI subjects as sMCI. However, such definition results in highly unbalanced cohorts (i.e., 27 pMCI and 83 sMCI). Korolev et al. (2016) used MRI, plasma, and clinical biomarkers to predict MCI-to-AD conversion via probabilistic pattern classification, and achieved 80% accuracy. Moradi et al. (2015) used MRI and clinical biomarkers for MCI-to-AD conversion prediction, and achieved an AUC of 0.90 using regularized logistic regression to select features and then using low density separation (LDS) as the classifier.\nMost of these methods are only applicable for datasets without missing data. In contrast, our study uses longitudinal multimodal data that can be incomplete. In addition, all of the previous studies mentioned above are focused on MCI-to-AD conversion prediction, which only answer the question on \"who\" will progress to AD. AD studies that predicted time to conversion, which answer the question on \"when\" the conversion will occur, are relatively rare. Conversion time prediction is important, as it gives us useful information about the disease progression rate and the severity of the disease, which may affect the individual treatment plan. In addition, knowing when the patient will progress to AD is also much more meaningful and clinically relevant (also more challenging) than just predicting whether the patient will progress to AD. Our work explores both problems."}, {"section_title": "Survival analysis", "text": "Conversion time prediction in this study is similar to survival analysis ( Miller Jr, 2011; Liu et al., 2017; Oulhaj et al., 2009 ) . Survival analysis computes the probability of event occurrence (e.g., disease status conversion) at future time points. For example, Oulhaj et al. (2009) used interval-censored survival analysis statistical methods to identify baseline cognitive tests that can best predict the time of conversion to MCI (from NC). Liu et al. (2017) used independent analysis and Cox model for their MCI-to-AD survival analysis study. Michaud et al. (2017) , on the other hand, employed competing-risks survival regression models and Cox proportional hazards models to investigate how the demographics and clinical characteristics are related to AD conversion time.\nDespite the similarity between conversion time prediction (as in our work) and survival analysis (as in many previous works), they actually address different questions. First, survival analysis aims to predict the probability of AD conversion at different future time points, mainly used for global analysis (e.g., comparing survival times of two groups); conversion time prediction in the current work predicts \"when\" the conversion will occur. As a result, survival analysis is generally based on a probability model (e.g., Cox regression model), whereas conversion time prediction is generally based on conventional regression models (e.g., least-squares regression model). Second, both analysis methods are designed for different types of data. Specifically, survival analysis is designed for censored data (where the survival times are unknown or incomplete) and uncensored data, whereas conversion time prediction is generally only suitable for non-censored data. For our study, the time-to-conversion data is censored for sMCI subjects, i.e., we do not know whether or when the sMCI subject will progress to AD if the monitoring time is extended indefinitely. Conventional linear regression models are unable to address this censored data issue, and thus unable to perform conversion time prediction. However, our improved matrix completion algorithm is able to address the censored data issue for the sMCI subjects, by treating the conversion time of sMCI subject as unknown and limiting its time-toconversion prediction to be at least a specific value (e.g., maximum monitoring period). We will discuss our method in greater detail in Section 4 ."}, {"section_title": "Low rank subspaces", "text": "It has been investigated in several previous studies that the data coming from different classes often lie in multiple lowdimensional subspaces ( Lin et al., 2015b; Vidal, 2011, 2013; Lin et al., 2015a; She et al., 2016 ) . Intuitively, this is because data from each class are often more related with each other than the data coming from other classes, and hence, the data is assumed to reside in a union of a number of lower-dimensional subspaces. For instance, the following sentence is directly quoted from Elhamifar and Vidal (2011) which is also extended and published in Elhamifar and Vidal (2013) :\n\"In many problems in signal/image processing, machine learning and computer vision, data in multiple classes lie in multiple lowdimensional subspaces of a high-dimensional ambient space.\"\nFor our application, where the data are from the MCI cohort of the ADNI dataset, there are samples with different survival times (time to convert to AD). This is similar to data with different \"classes\" and thus it is intuitive to assume that the data is a union of low-rank subspace. In this study, however, we are not using low rank subspace algorithm for clustering, but we take advantage of this concept for denoising the data."}, {"section_title": "Materials and preprocessing", "text": ""}, {"section_title": "Materials", "text": "In this study, we are interested in predicting two target outputs, i.e., the pMCI/sMCI class labels and the conversion times in months, using multi-modal data from the ADNI 2 dataset. The multi-modal data used in this study include T1 weighted MR scans, fluorodeoxyglucose PET (FDG-PET, PET for short for the rest of the manuscript) scans, and cognitive clinical scores (e.g., MiniMental State Exam (MMSE), Clinical Dementia Rating (CDR), and Alzheimer's Disease Assessment Scale (ADAS)). Using these multimodal data at a single time point and multiple time points, we performed cross-sectional and longitudinal study, respectively, in this paper. For cross-sectional study, we used different combinations of modalities at 18th month for pMCI and conversion time predictions. For longitudinal study, we examined our prediction model using different combinations of modalities at 18th month and one additional time point (i.e., baseline, 6th month or 12th month). For both the cross-sectional and longitudinal studies, we used the same set of subjects with the same assignment of disease status labels (i.e., pMCI and sMCI), for easier comparison of results between these two studies. More specifically, we define pMCI subjects as MCI subjects who progressed to AD within the monitoring period from 18th to 60th month, while MCI subjects who remained stable for upto 60th month were labeled as sMCI. We also excluded MCI subjects who progressed to AD on and before 18th month in this study since it is meaningless to use (longitudinal) data that were labeled as AD for pMCI/sMCI prediction. Based on the definition and exclusion criteria mentioned above, we have 65 pMCI and 53 sMCI subjects for this study, with their demographics summarized in Table 1 , and their Roster IDs (RIDs) given in the supplementary file. As can be seen from the table, there is no significant difference in term of education, age and gender distribution between these two cohorts of data. In addition to pMCI and sMCI labels, we also used conversion time as another target in our study. However, it is difficult, if not impossible, to obtain the \"ground truth\" of conversion time, as the conversion, in itself, is a process that does not occur at one single time point. In addition, ADNI only scans and evaluates the MCI patients at specific time points after the baseline scan (e.g., 12th, 18th, 24th, 36th month, etc.), where the conversion can occur at any time between two scan times. In this work, we estimate the \"ground truth\" of conversion time as the time period between the date of 18th month scan (we used 18th month as the reference) and the date of the nearest scan after the conversion had occurred. Though this is currently the best estimate we can get, this value is actually the upper bound of the real conversion time. As the exact scanning dates are used to estimate the \"ground truth\" of conversion time, the estimated values are not discrete (e.g., 6 months, 12 months, etc., if we use the scanning plan to obtain the conversion time), but rather real continuous values (e.g., 5.7 months, 10.4 months, etc.). Thus, for this target, we treat the conversion time prediction as a regression problem. During model evaluation, we choose performance measures that are less sensitive to the uncertainty of the noisy \"ground truth\"."}, {"section_title": "Preprocessing and feature extraction", "text": "We use region-of-interest (ROI)-based features from the MRI and PET images. Each MRI image was Anterior Commissure -Posterior Commissure (AC-PC) aligned using MIPAV 3 , corrected for intensity inhomogeneity using the N3 algorithm ( Sled et al., 1998 ) , skull stripped ( Wang et al., 2011 ) , tissue segmented ( Zhang et al., 2001 ) , and registered to a template ( Kabani, 1998; Shen and Davatzikos, 2002; Thung et al., 2014; Xue et al., 20 04, 20 06b, 20 06a ) . Gray matter (GM) volumes, normalized by the total intracranial"}, {"section_title": "The proposed methods", "text": "In this study, we use multi-modal (i.e., MRI, PET, clinical scores) and longitudinal data (i.e., data collected at multiple time points) for classification and regression analysis. These data are heterogeneous, high dimensional, possibly incomplete, and could be corrupted with noise. To address these issues, we propose a prediction framework that consists of three main components: 1) sparse feature selection (FS), which removes features that are unrelated to the targets via sparse linear regressions, 2) low-rank affinity pursuit denoising (LRAD), which utilizes low-rank representation (LRR) to denoise the data using neighboring samples in low-rank subspace, and 3) low-rank matrix completion (LRMC), which predicts the unknown targets (i.e., diagnostic labels and conversion times). Fig. 1 shows an overview of our proposed framework. The operation details involved in these three components are described in the following subsections."}, {"section_title": "Notation", "text": "We first introduce the notations that will be used to describe the formulation of the proposed method. We use X \u2208 R n \u00d7m to denote the feature matrix with n samples of m features. Here, n depends on the number of time points and the number of modalities used. Each sample (i.e., row) in X is a concatenation of features from different time points and different modalities (e.g., MRI, PET and clinical scores). Note that X can be incomplete because of missing data, due to various reasons described in the introduction ( Thung et al., 2013 ( Thung et al., , 2015a ( Thung et al., , 2015b . The corresponding target matrix is denoted as Y \u2208 R n \u00d72 , where the first column is a vector of labels (1 for pMCI, and \u22121 for sMCI), and the second column is a vector of conversion times (e.g., the number of months to convert to AD). The conversion times associated with the sMCI samples are unknown, but at least larger than the last monitored time. For any matrix M , M j, k denotes its element indexed by ( j, k ), whereas M j , : and M :, k denote their j th row and k th column, respectively. We denote M * = \u03c3 i (M ) as the nuclear norm (i.e., sum of the singular values\n) 1 / 2 as the l 2 -norm, and M T as the transpose of M. I is the identity matrix."}, {"section_title": "Feature selection using sparse regression", "text": "Not all the features are related to the disease progression Yuan et al., 2012 ) . We perform feature selection to remove features which are unrelated to our prediction tasks. We use lasso with logistic and least square loss functions ( Tibshirani, 1996; Liu et al., 2009b; ) to select features that are related to the target outputs. As the data, which is the concatenation of multiple modalities and time points, is possibly incomplete, we can not perform the feature selection using Eqs. (1) and (2) on the whole dataset directly. We can either use an advanced feature selection method that works with incomplete data, like ( Yuan et al., 2012 ) , or perform feature selection on each group of complete data separately. We choose the latter as methods like ( Yuan et al., 2012 ) do not work well when there are too many groups of data, as in our case. Specifically, we split the incomplete data into groups with complete data according to modalities and time points ( Thung et al., 2015b; 2015a ) , so that lasso can Fig. 1 . Overview of the proposed framework, which consists of sparse feature selection, low-rank affinity pursuit denoising (LRAD), and low-rank matrix completion.\nbe applied independently to each group. The two lasso algorithms used are given as min \u03b2\nwhere X ( i ) is the data matrix of the i th group, and \u03b2 (i ) 1 is the sparse weight vector. y is the target label (first column of Y ), as we are more interested in the classification task, while y j is the target label for the j th sample. We use two types of linear regressions to select features, as our previous study ( Thung et al., 2015a ) showed that the prediction model that uses two linear regressions is better than the model that uses one linear regression. The combined non-zero values (OR operation) in vectors \u03b2 (i ) 1 and \u03b2 (i ) 2 are used to select corresponding features in X ( i ) . The regularizing parameters \u03b3 1 and \u03b3 2 are determined through cross-validation using the training data."}, {"section_title": "Low-rank affinity pursuit denoising (LRAD)", "text": "ROI-based MRI and PET features can be noisy. In addition, when features from multiple time points are stacked together, the dimensionality of the features is high. Nevertheless, as these features are highly correlated, the true rank of the data matrix (i.e., a 2D matrix X , where each row denotes feature vector of a sample) is low if the noise is removed. Thus, we can use, e.g., a robust principal component analysis (RPCA) algorithm Cand\u00e8s et al., 2011; Wright et al., 2009 ) , to denoise the data by decomposing the data into two components -the low-rank component and the sparse noise component. However, as criticized by Vidal (2010) , RPCA algorithm denoises the data with the assumption that there is only one low-rank dimensional subspace in the data, which may not produce satisfactory results if the data is actually a union of low-rank subspaces, as could be the case of our data, where the data is heterogeneous. Following the work in ( Vidal, 2010; Thung et al., 2015b ) , we introduce low-rank affinity pursuit denoising (LRAD) to denoise data by representing each sample, with possible missing feature values, using its neighboring samples in the lowrank subspace, via an incomplete version of low-rank representation (LRR). LRR has been previously used in various applications, such as subspace clustering , subspace segmentation Liu et al. (2010) , etc ( Zhou et al., 2013; Liu and Yan, 2011 ) . In this work, we introduce a procedure to utilize it for denoising.\nIn LRR, the data is decomposed into two components -the low-rank self-representation data component and the error (or noise) component. As there are missing feature values in X , we use incomplete data version of LRR (ILRR) ( Shi et al., 2014 ) , which is given as:\nwhere X is the completed version of X , which is self-represented by A X , A \u2208 R n \u00d7n is the low-rank affinity matrix, E is the error matrix, and \u03b1 is the regularizing parameter. Each element of A indexed by ( i, j ) is an indicator of the similarity between the i th sample and the j th sample, which are represented by the i th row and the j th row of X , respectively. Thus, the i th row of A denotes the similarity of the i th sample with all other samples in X . A X is thus a reconstruction of X , where each row is a linear combination of neighboring rows determined by the A . By imposing low-rank constraint on A , A X is a low-rank recovery of X , which is called the \"lowest-rank representation\" of X . In brief, ILRR gives us a locally compact (low-rank) representation and denoised version of the raw data, given as D = A X . Problem in Eq. (3) is solved using inexact augmented Lagrangian multiplier (ALM), as described in ( Shi et al., 2014 ) . Note also that we regularized the error matrix E using the l 1 -norm, as we expect that the noise is sparse (e.g., the segmentation and registration errors could have happened at certain brain regions, causing sparse noise in ROI-based features). In addition to E 1 , we also test our framework using the l 2 -norm, E 2 , which assumes that the data matrix X is corrupted by Gaussian noise."}, {"section_title": "Predictions using low-rank matrix completion (LRMC)", "text": "Assuming a linear relationship between X and Y , the k th target of Y is given by Y :\n1 is a column vector of 1's, a k is the weight vector, and b k is the offset. Assuming that X is low-rank (i.e., each column of X could be represented by some other columns in X ), then the concatenated matrix M = [ X 1 Y ] is also low-rank ( Goldberg et al., 2010 ) , i.e., each column of M can be linearly represented by other columns, or each row of M can be linearly represented by other rows. Based on this assumption, low-rank matrix completion (LRMC) ( Goldberg et al., 2010; Sanroma et al., 2014; Thung et al., 2014; Chen et al., 2017 ) can be applied to M to impute the missing feature values and the target outputs simultaneously by solving min Z { Z * | M = Z } , where is the index set of known values in M , and Z is the completed matrix version of M . In the presence of noise, the problem can be relaxed as ( Goldberg et al., 2010 ) min\nwhere yl and x are the index sets of the known target labels and feature values, respectively, while\nare the logistic loss function and mean square loss function, respectively. The nuclear norm \u00b7 * in (4) is used as a convex surrogate for matrix rank. Parameters \u03bc and \u03bb 1 are the trade-off hyper-parameters that control the effect of each term. In our application, there are two targets, i.e., the pMCI label and the conversion time, which are binary and continuous, respectively. Thus, we use two separate hyper-parameters and data fitting terms, based on these two targets. The LRMC with three data fitting terms and one inequality constraint is given as:\nwhere yr is the index set of know regression targets for conversion time, and \u03bc, \u03bb 1 and \u03bb 2 are the hyper-parameters. The conversion times of sMCI samples are considered unknown, except we know that they are at least larger than the last monitored time point. Thus, we use the inequality constraint to make sure that the conversion times of the sMCI samples in the training set are always larger than a threshold time point, which we set as 12 months in addition to the maximum conversion time. When the data are z -normalized, this threshold is normalized accordingly. We solve Eq. (5) using fixed point continuation (FPC) ( Algorithm 1 ) Thung et al., 2014 ) , which consists of 2 alternating steps for each iteration. The alternating steps of k th iteration are given as:\nAlgorithm 1: Low-rank matrix completion. Evaluate gradient step:\nEvaluate shrinkage step:\nEvaluate projection based on inequality constraint: \nwhere \u03c4 is the step size and g ( Z k ) is the matrix gradient which is defined as\n2. Shrinkage step ( Cai et al., 2010 ) :\nwhere S ( \u00b7) is the matrix shrinkage operator, U V T is the SVD of G k , and max( \u00b7) is the elementwise maximum operator.\nThe value of \u03c4 is determined from the data. A minor modification of the argument in Goldberg et al., 2010 ) would reveal that, as long as we choose a non-negative step size satisfying \u03c4 < min (4| yr |/ \u03bb 2 , 4| yl |/ \u03bb 1 , | x |), the algorithm above is guaranteed to converge to a global minimum."}, {"section_title": "Bayesian hyper-parameter optimization", "text": "The problem in Eq. (5) involves multiple hyper-parameters (e.g., \u03bc, \u03bb 1 , \u03bb 2 ). The values of these hyper-parameters can be obtained by cross-validation and grid search. This is, however, time consuming. For example, if we test 6 candidate values for each hyperparameter, there would be a total of 6 3 = 216 combinations. If we test these combinations using 5 fold cross-validation, we will need to solve Eq. (5) more than 10 0 0 times. It is therefore desirable to have a more efficient strategy for the hyper-parameter optimization. In this work, we use a Bayesian optimization algorithm ( Bergstra et al., 2011; Thornton et al., 2013; Yogatama and Mann, 2014 ) to obtain the best hyper-parameters. In this approach, not all the combination of hyper-parameters are tested. Instead, only hyper-parameters that have higher probability of improving the cross-validation accuracy are evaluated. Specifically, Bayesian optimization first builds a prediction model based on previous records of hyper-parameters and their corresponding cross-validation accuracies. Using the prediction model, we obtain the posterior predictive distribution map, which predicts the accuracy distribution for each point in the hyper-parameters search range. Each point in the predictive distribution map can be characterized by a mean and a standard deviation, which are used to denote the prediction accuracy and information gain (the larger the standard deviation, the less certain of the prediction, and the higher of information gain) of this point, respectively. Balancing the information gain and the exploitation of the prediction accuracy, Bayesian optimization arrives at a value via an evaluation function (which is commonly called as acquisition function). Finally, the highest point of the acquisition function is used to choose the hyper-parameter point to be evaluated next. Then the whole process of selecting hyper-parameters is repeated until a stopping criterion is fulfilled.\nAlgorithm 2: Bayesian hyper-parameter optimization."}, {"section_title": "Data : X tr , y tr", "text": "Result : \u03b8 * with greatest \u03c8 * 1 Initialization : Randomly select n hyper-parameters and evaluate their 5-fold cross validation accuracy values:\nFind \u03b8 i by optimizing the acquisition function over GP: ( Bergstra et al., 2011 ) . Let \u03b8 denotes a hyper-parameter point, which consists of the hyper-parameters (i.e., \u03bc, \u03bb 1 , \u03bb 2 in (5) ) that we need to optimize, \u03c8 denote the corresponding cross validation accuracy using the training data ( X tr , y tr ), and H = { ( \u03b8, \u03c8 ) } denotes the historical observation of the hyper-parameters and their corresponding accuracy values. SMBO performs the following steps iteratively: 1) Build a model that captures the relationship of \u03b8 and \u03c8 using a Gaussian process; 2) Determine the next promising \u03b8 candidate; 3) Compute \u03c8 based on the selected \u03b8; and 4) Update H with a new pair of ( \u03b8, \u03c8) as well as the Gaussian process prediction model. We solve the problem in line 3 of Algorithm 2 by using a Gaussian Process (GP) prior ( Algorithm 3 ) ( Rasmussen, 2004; Rasmussen and Williams, 2006; Bergstra et al., 2011; Thornton et al., 2013; Snoek et al., 2012 ) . GP is an extension of a multivariate Gaussian distribution to an infinite dimensional stochastic process ( Brochu et al., 2010 ) . For each \u03b8, \u03c8( \u03b8) is assumed to be a sample from a multivariate Gaussian distribution, which is completely specified by mean m ( \u03b8) and covariance k ( \u03b8, \u03b8 ) :\nThere are many choices of covariance function ( Rasmussen and Williams, 2006; Brochu et al., 2010; Snoek et al., 2012 ) . In this paper, we use the squared exponential covariance function with isotropic distance measure:\nwhere s 1 and s 2 are the parameters of the covariance function."}, {"section_title": "Assuming that we have historical observation H", "text": ". . , t} from previous iterations, we want to determine the next plausible hyper-parameter point, \u03b8 t+1 . Let \u03c8 t+1 = \u03c8 ( \u03b8 t+1 ) denotes the function value at \u03b8 t+1 , and \u03c8 1: t = \u03c8 denotes the column vector of cross validation accuracy values using \u03b8 1: t . Then, by the properties of GP, \u03c8 and \u03c8 t+1 are jointly Gaussian ( Brochu et al., 2010 ) :\nwhere\nThe parameters s 1 and s 2 of the covariance function in (10) can be solved by maximizing the probability of \u03c8 given \u03b8 ( Rasmussen and Williams, 2006 ) :\nBased on (11) , the posterior predictive distribution is given as ( Brochu et al., 2010; Rasmussen and Williams, 2006 ) p\nwhere\nBased on the computed mean and covariance function, we evaluate the acquisition function which controls the balance between exploitation (favors \u03b8 with higher m ) and exploration (favors \u03b8 with higher \u03c3 2 ). We use expected improvement (EI) as acquisition function in this study, which is given as ( Brochu et al., 2010 ) :\nwhere\n, and ( \u00b7) and \u03c6( \u00b7) are the probability distribution function (PDF) and cumulative distribution function (CDF) of the standard normal distribution, respectively. The hyperparameter point corresponding to the highest value of the acquisition function is chosen for the next round of hyper-parameter test."}, {"section_title": "Results", "text": "We evaluated our proposed framework using both the longitudinal and the multi-modal data. We tested different variations of our proposed framework, and compared them with two baseline methods, as well as two state-of-the-art classification methods that also work on incomplete data. In the following, we describe the baseline methods, the variations of our proposed framework, the state-of-the-art methods, the parameter settings, the performance metrics, and the experimental results."}, {"section_title": "The baseline and the proposed methods", "text": "One of the differences of our proposed framework with the previous LRMC-based prediction model is the inclusion of LRAD denoising component, which improves the prediction performance significantly. Fig. 2 shows the flowchart of the comparison baseline methods and the proposed methods (i.e., three variations of the proposed framework). For simplicity, we use abbreviations to denote the baseline methods and our proposed methods. The top two rows in Fig. 2 , denoted as MC and FMC in the figure, are the baseline methods that do not use LRAD, i.e., LRMC and FS-LRMC (FS-based LRMC), respectively. The following three rows in Fig. 2 , denoted as DMC, FDMC and DFMC in the figure, are the proposed methods that utilize LRAD, i.e., LRAD-MC (no feature selection), FS-LRAD-MC (sequentially performing FS, LRAD and LRMC), and LRAD-FS-MC (sequentially performing LRAD, FS and LRMC), respectively. Note that the sequence of applying the feature selection and denoising algorithms will affect the final prediction result. In FS-LRAD-MC, we select features before data denoising, while, in LRAD-FS-MC, we select features after data denoising. While the feature selection algorithm works better if the data is denoised, the denoising algorithm also works better if the data is lower in dimension and discriminative to the prediction task. Therefore, there are pros and cons for both approaches, and we include both models in our study. In the experimental result section, we will discuss a simple guiding principle to help us in deciding which approach to be used in practice."}, {"section_title": "The comparison methods", "text": "We compared our method with two state-of-the-art methodsiMSF ( Yuan et al., 2012 ) and Ingalhalikar's ensemble method (Ingal) ( Ingalhalikar et al., 2012 ) . We made some modifications to both algorithms so that they can be applied to our dataset.\n1. iMSF : iMSF is a multi-task learning algorithm where each task is dedicated to the mapping of one data subset to its corresponding target vector. The incomplete dataset is first divided into several disjoint data subsets, each of which is the input for one learning task. The mappings of the subsets to their targets are learned jointly. One of the limitations of this algorithm is the limited number of samples in each disjoint subset. Therefore, we make some modifications to iMSF to use overlapped data subsets for each learning task. This modification greatly increases the number of samples in each data subset, and thus improves the performance of iMSF. 2. Ingalhalikar's ensemble model ( Ingalhalikar et al., 2012 ) : This algorithm uses an ensemble classification technique to fuse decisions from multiple classifiers constructed using data subsets, obtained similarly as ( Thung et al., 2013 ) . The algorithm groups the data into subsets, selects features using signal-to-noise ratio coefficient filter ( Guyon and Elisseeff, 2003 ) , performs classification using each data subset based on linear discriminant analysis (LDA), and fuses all classification results into a single result. The decisions are fused using weighted averaging by assigning a weight to the decision of each classifier based on its training classification error. We also implemented a regression ensemble model, where we build a sparse regression model for each data subset and fuse the regression outputs using weighted averaging."}, {"section_title": "Hyper-parameters and performance metrics", "text": "For our method, we use a small value \u03b1 = 0 . 005 for ILRR in (3) . The hyper-parameters \u03b3 1 and \u03b3 2 in feature selection are determined through 5-fold cross validation using only the training data of each fold. The parameters \u03bc, \u03bb 1 , and \u03bb 2 of LRMC are determined using Bayesian optimization as LRMC is more time consuming due to the computation of singular value thresholding. The hyper-parameters of iMSF and Ingalhalikar's fusion methods are determined using 5-fold cross-validation, since they both involve only one hyper-parameter.\nFor the classification task involving prediction of diagnostic labels, we use accuracy (ACC) and Area Under the Receiver operator curve (AUC) as the performance metrics. For the regression task involving prediction of MCI conversion time, we choose performance metrics that are less sensitive to the uncertainty or noise in the \"ground truth\" of conversion time (please refer to Section 3.1 ), i.e., Pearson correlation coefficient (PCC) and Spearman rank-order correlation coefficient (SROCC). PCC measures the prediction accuracy and SROCC measures the prediction monotonicity. In addition, we also include coefficient determination to measure how well future samples are likely to be predicted by the model. For all the performance metrics, higher values correspond to better predictions."}, {"section_title": "Cross-sectional study: prediction of diagnostic labels using multi-modal data and single time point data", "text": "Figs. 3 and 4 show respectively the pMCI classification accuracies and AUCs using different combinations of multi-modal data of time point T 4 = 18 th month. To show the efficacy of each component in the proposed framework, we report the results given by different combinations of the components, i.e., DMC, DFMC and FDMC in Fig. 2 , which respectively represents LRAD-MC, FS-LRAD-MC, and LRAD-FS-MC. LRMC and FS-LRMC, represented by MC and FMC for convenience, are the baseline LRMC methods without LRAD components. More specifically, LRMC and FS-LRMC are the matrix completion algorithms using the original and feature reduced matrices, respectively. Their results are denoted by the blue Fig. 3 . On the other hand, the red boxes in Fig. 3 are used to denote the results of the proposed methods that contain LRAD, i.e. ,LRAD-MC, FS-LRAD-MC, and LRAD-FS-MC, represented by DMC, FDMC, and DFMC, respectively. It can be observed from Fig. 3 that the LRAD improves the diagnostic accuracies (i.e., the red boxes are generally higher than the blue boxes). Generally, when LRAD is employed after feature selection, we observe some improvements (comparing FMC with FDMC), especially for MRI+PET, MRI+Cli, and MRI+PET+Cli. In contrast, when feature selection is employed after LRAD, the improvement is not obvious (comparing DMC with DFMC), since using LRAD alone has already significantly improved the accuracy (compare MC with DMC). However, performing feature selection after LRAD can reduce the computation cost because LRMC is applied on a smaller matrix. Similar conclusions can be drawn based on AUC (see Fig. 4 )."}, {"section_title": "Cross-sectional study: influence of regularization", "text": "We evaluated the effects of two types of regularization, i.e., the l 1 -norm and the l 2 -norm, which make different assumptions about the data noise. For the l 1 -norm, the data are assumed to be corrupted by sparse noise, which could be caused by any of the preprocessing steps, e.g. , segmentation or ROI alignment errors. For the l 2 -norm, the data are assumed to be corrupted by Gaussian noise. Tables 2 and 3 show the pMCI/sMCI classification results using multi-modal data of time point T 4 , with LRAD using an l 1 -norm ( E 1 ) or an l 2 -norm ( E 2 ) error term. Both tables show that the prediction of LRMC improves with LRAD. We further perform paired t -test between the best result and the other results in each category, and mark the statistically significant results ( p < 0.05) with asterisks ( * ). Comparing the results from both tables, the l 1 -norm gives greater improvement than the l 2 -norm, implying that the former gives a better denoising outcome. Table 4 shows the results using multi-modal and longitudinal data, when the l 1 -norm error term is used in LRR. Four time points are used in this experiment, namely time point 1, 2, 3 and 4, corresponding to the data acquired at baseline, 6th month, 12th month, and 18th month, respectively. Time point 4 ( T 4 ) is used as our reference time point since it is the latest time point and gives us the most current state of the subject. As shown in our previous work ( Thung et al., 2015a ( Thung et al., , 2015b , predictions using longitudinal data with 2 time points are generally better than using one time point. Hence, we test our method using 2 time points, i.e., the reference time point ( T 4 ) plus an additional historical time point data."}, {"section_title": "Longitudinal study: prediction of diagnostic labels using multi-modal and longitudinal data", "text": "For example, in Table 4 , T 4, 1 indicates that the data of T 4 and T 1 are used. From the table, it can be seen that LRAD improves prediction performance, for almost all combinations of modalities and time-points. The only case where the proposed method performs slightly worse than the baseline is MRI+PET+Cli-T 4, 3 . The difference is, however, not statistically significant. The highest accuracy achieved by the proposed method is 84.0% for the case of MRI+Cli- T 4, 1 . Similar observations can be made when the l 2 -norm error term is used in LRAD (See Table 5 ), even though the l 1 -norm is generally better than the l 2 -norm in this application."}, {"section_title": "Cross-sectional study: prediction of conversion time using multi-modal single time point data", "text": "Figs. 5 and 6 show respectively the PCC and SROCC results computed between the predicted conversion time and the groundtruth conversion time, using different combinations of multi-modal data of the reference time point. As shown in both figures, the performance of LRMC has been significantly improved with LRAD and feature selection. The best PCC of 0.665, which is about 10% higher than the original LRMC method, is achieved when using MRI data and clinical scores with the proposed framework LRAD-FS-LRMC. Similar results can be observed for coefficient determination (or R 2 scores), as shown in Fig. 7 . Table 6 shows the PCC values of the predicted conversion times using different combinations of longitudinal and multi-modal data. As can be seen from the table, the proposed methods (last 2 columns) perform best in all settings. Particularly, for a smaller feature dimension, LRAD-FS-MC (column DFMC) performs better (e.g., MRI, MRI+Cli at T 4 ). For a larger feature dimension, FS-LRAD-MC (column FDMC) performs better (e.g., MRI+PET, MRI+PET+Cli). The best performance is obtained when using MRI+Cli at T 4 , which gives us an average PCC value of 0.665. Similar observations can be obtained for SROCC, as shown in Table 7 , and R 2 scores, as shown in Table 8 . Thus, the rule of thumb is to choose LRAD-FS-MC when the feature dimension is smaller and less noisy, and choose FS-LRAD-MC when the feature dimension is bigger and noisier."}, {"section_title": "Longitudinal study: prediction of conversion time using multi-modal and longitudinal data", "text": ""}, {"section_title": "Discussions", "text": "Comparing the results of MRI+PET+Cli and MRI+Cli, especially referring to Table 4 , it seems that there is a drop in performance when additional PET data is used. There could be several possible reasons behind this observation, including the small sample size of the data. This is because the number of samples being used is much less than the number of features. The number of samples used in this study is 118, which is relatively small compared to the number of features (93 for each modality at each time point). During training, cross-validation uses an even smaller data subset for feature selection, resulting in instability especially in the presence of outliers and missing data. For the ADNI dataset we used in this study, PET data are not available for half of the samples, whereas clinical cognitive scores and MRI are relatively complete. The relatively smaller number of samples with PET data makes prediction using PET less reliable. We use the results in Table 4 as an example, where columns (c) and (d) refer respectively to our proposed method without and with feature selection. It can be seen that, with feature selection, MRI+PET and MRI+PET+Cli are better than the methods without feature selection, which to some extent verifies our expectation that removing outlier features in the PET data would improve prediction performance."}, {"section_title": "Comparison with other methods", "text": "In addition, we also compared our method with the methods proposed in ( Yuan et al., 2012; Ingalhalikar et al., 2012 ) . Some modifications were made to the method in ( Yuan et al., 2012 ) , so that it can be applied to our multi-modal and longitudinal dataset, as described in Section 5.2 . The results in Table 9 indicate that the proposed method outperforms these state-of-the-art methods for MRI, MRI+PET and MRI+Cli longitudinal data. For MRI+PET+Cli, the proposed method is still the best when data from a single time point is used, but does not perform as well as iMSF when more time points are used. It is worth noting that this iMSF result is obtained after our improvement modifications, the original iMSF algorithm can not handle so many missing patterns in the longitudinal multi-modal data. Nevertheless, this also likely indicates that a better feature selection method is needed for the proposed framework to further improve performance. As we are focusing on LRAD in this work, we left this as our future work. Similar obser- vation can be obtained for the PCC metric, as shown in Table 10 . The best classification and conversion time prediction accuracy for these two tables are still achieved by the proposed LRAD-FS-MC, using MRI and clinical data, at the value of 0.839 and 0.665, respectively."}, {"section_title": "Conclusion", "text": "In this study, we have proposed a series of algorithms based on subspace methods to address two very important questions on AD study -which MCI subject will progress to AD and when it will occur. Our framework is one of the few studies that addresses these queries jointly using incomplete multi-modal and longitudinal neuroimaging and clinical data. Our framework consists of three main components, i.e., sparse feature selection, lowrank affinity pursuit denoising (LRAD), and low-rank matrix completion (LRMC), in addition to efficient Bayesian hyper-parameter optimization. We have demonstrated that the LRAD is able to improve the LRMC-based predictions, either in terms of the diagnostic labels or the conversion time predictions using MCI data. We use LRAD to denoise heterogeneous multi-modal neuroimaging and clinical data by self-representing the data with the neighboring data. The LRAD with the l 1 -norm regularization performs better than the LRAD with the l 2 -norm regularization, indicating that the data we used contain more likely sparse noise rather than Gaussian noise. On the other hand, we have modified the original matrix completion algorithm by introducing three data fitting terms and one inequality constraint to predict conversion and time-toconversion jointly. The added inequality constraint has made the conversion time prediction of the censored sMCI data possible. In addition, we used Bayesian optimization to efficiently search for the optimal set of hyper-parameters for our proposed framework. Extensive evaluations also indicate that the proposed method outperforms the conventional LRMC in various settings, as well as a number of state-of-the-art methods."}]