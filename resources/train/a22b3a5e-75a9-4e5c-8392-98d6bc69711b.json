[{"section_title": "Abstract", "text": "We study the importance of teacher subject knowledge for student performance in Sub-Saharan Africa using unique international assessment data for sixth-grade students and their teachers. To circumvent bias due to unobserved student heterogeneity, we exploit variation within students across math and reading. Teacher subject knowledge has a modest impact on student performance. Exploiting vast cross-country differences in economic development, we find that teacher knowledge is effective only in more developed African countries. Results are robust to adding teacher fixed effects and accounting for potential sorting based on subject-specific"}, {"section_title": "", "text": "The remainder of the paper is structured as follows. Section II describes the data. Section III lays out the estimation strategy. Section IV presents the results regarding the effect of teacher subject knowledge on student learning, and Section V explores the robustness of these findings. Section VI presents results on effect heterogeneity. Section VII compares the magnitude of the teacher subject knowledge impact to other settings and to other education inputs, including teacher incentives. Section VIII concludes."}, {"section_title": "II. Data and Descriptive Statistics", "text": ""}, {"section_title": "A. The SACMEQ Assessments", "text": "The empirical analysis draws on data from the Southern and Eastern Africa Consortium for Monitoring Educational Quality (SACMEQ), a collaborative network of 15 SubSaharan African ministries of education and the UNESCO International Institute for Educational Planning (IIEP). The network periodically conducts international assessments of the math and reading knowledge of sixth-grade primary-school students and their teachers. By means of student, teacher, and principal questionnaires, it also collects detailed background information on student and teacher characteristics, as well as on classroom and school resources. The first wave of this assessment took place in 1995 and covered seven countries; the second wave, in 2000, covered 14 countries; and the third wave, in 2007, covered 15 countries. In this paper, we use data from the last two waves because teachers were not tested in the first wave.\nSACMEQ employs a two-stage clustered sampling design to draw nationally representative samples of sixth-grade students for each participating country. Schools are sampled within predefined geographical strata in the first stage, and a simple random sample of students is drawn from each selected school in the second stage. In the second wave, 20 students per school were sampled randomly, and the teachers who taught math and reading to these students were tested. In the third wave, 25 students per school were sampled randomly, and the math and reading teachers of the three largest classes in each school were tested.\n7 While all students are tested in both math and reading, teachers are tested only in the subject they teach. However, both math and reading scores are available for a subsample of teachers who teach sampled students in both subjects.\nThe SACMEQ student assessments are designed to reflect the elements common to the math and language curricula in the participating countries. The multiple-choice tests contain items developed by SACMEQ itself, as well as items from other international student assessment such as the Trends in International Mathematics and Science Study (TIMSS) . Students in all participating countries are administered the same tests at the end of sixth grade, with tests translated into the local language of instruction if it is different from English. The teacher assessments include items from the student tests and additional, more difficult questions.\nattention to a single country (South Africa), and Altinok (2013) uses a simple OLS model without student fixed effects. Hein and Allen (2013) focus primarily on other teacher characteristics such as experience. 7. The sampling design of the third wave implies that teacher test scores are missing for students who did not attend any of the three largest classes. As explained in Section II.B, all students with missing teacher test scores are excluded from the sample.\nBoth student and teacher tests are graded centrally in each country under the auspices of the IIEP. Using Item Response Theory, all test scores are placed on a common scale with a mean of 500 and a standard deviation of 100 across students participating in the second SACMEQ wave. Because of the overlapping items, test scores are directly comparable between students and teachers, as well as between the two assessment waves. The similarity between student and teacher tests also means that teacher test scores in SACMEQ reflect knowledge that is likely highly relevant for teaching math and reading. Therefore, these curriculum-based measures of teacher knowledge are noticeably different from other teacher test scores, for instance, SAT and ACT scores in the United States, which reflect teachers' general cognitive ability."}, {"section_title": "B. Sample Selection and Descriptive Statistics", "text": "We pool the data from the second and third waves of the SACMEQ assessment. We exclude Mauritius from the original set of 15 countries because it did not test teachers. Teachers in South Africa were not tested in the second wave and could opt out in the third wave, which 18 percent of the sampled teachers did, so we also exclude South Africa from the analysis. 8 We further exclude 5,428 students who could not be linked to a teacher in any subject, 4,018 students who had at least one teacher with missing test scores, and 225 students with missing test scores. 9 The final sample consists of 74,708 students with 8,742 teachers in 3,939 schools in 13 countries: Botswana, Kenya, Lesotho, Malawi, Mozambique, Namibia, Seychelles, Swaziland, Tanzania (mainland), Uganda, Zambia, Zanzibar (semiautonomous region of Tanzania), and Zimbabwe (participated only in the third wave). Table A1 in the Appendix reports descriptive statistics of student performance and teacher subject knowledge for the pooled sample and separately for each country. There are striking differences in student performance between countries. For example, in math, students in Kenya score on average more than 1.4 international SD higher than students in Zambia. Similarly, in reading, students in the Seychelles score more than 1.5 international SD higher than their peers in Malawi. Interestingly, the cross-country differences in teacher subject knowledge are even larger. Teachers in Kenya, for example, outperform teachers in Zanzibar by 2.2 international SD in math; the variation in teacher reading knowledge is of a similar magnitude.\n10 Figures A1 and A2 illustrate these large cross-country differences by plotting each country's distribution of teacher test scores in math and reading and, as a benchmark, the average test score of teachers in the best-performing country.\nTo put the observed variation in teacher subject knowledge into perspective, we compare it to the subject-knowledge variation between teachers with different levels of 8. Opting out (by either students or teachers) was not possible in any other country. Results are robust to retaining South Africa in the sample. 9. Some background variables have missing values. Since we consider a large set of explanatory variables and since a portion of these variables are missing for a relatively large fraction of students, dropping all student observations with any missing value would result in substantial sample reduction. We therefore imputed missing values for control variables by using the country-by-wave means. To ensure that imputed data are not driving our results, all our regressions include an indicator for each variable with missing data that equals one for imputed values and zero otherwise. 10. In each country, the average teacher significantly outperforms the average student in both math and reading. However, in all countries, the best students outperform the worst teachers.\neducation. For instance, in the pooled sample, the average math test score is 734 points for teachers with only primary education and 822 points for teachers with tertiary education. This difference is equivalent to 0.8 international SD in teacher subject knowledge in math. In other words, the difference in teacher math knowledge between the country with the best-performing teachers and the country with the worst-performing teachers is almost three times as large as the difference between teachers with tertiary education and teachers with primary education (in reading, this ratio is about 2). Another way to illustrate the substantial differences in teacher subject knowledge across countries is to consider individual test items. For instance, teachers participating in SACMEQ were asked to answer the following math question: \"x/ 2 < 7 is equivalent to (a) x > 14, (b) x < 14, (c) x > 5, or (d) x < 7/2?\" Eighty-three percent of teachers in Kenya answered this question correctly, but only 43 percent of teachers in Lesotho did so."}, {"section_title": "11", "text": "These large cross-country differences notwithstanding, teachers in Sub-Saharan Africa have much less knowledge than teachers in developed countries. While there is no dataset that would allow a direct comparison between African teachers and teachers in developed countries, we can compare the math knowledge of teachers in Sub-Saharan Africa to that of eighth-grade students in developed countries. In the TIMSS 1995 assessment, eighth-grade students were asked to solve the same math question described above (\"x/ 2 < 7 is equivalent to\"). In 19 out of 39 mostly developed countries, eighthgrade students did as well or even better than teachers in the worst-performing SubSaharan country (Lesotho), and in four countries they did even better than the average teacher in Sub-Saharan Africa. Moreover, 47 percent of eighth-grade students in the United States could solve this math problem, and-judging by this item alone-are therefore at the level of teachers in Botswana and Namibia. To get a first sense of the importance of teacher subject knowledge for student performance, we plot average student test scores against average teacher test scores at the country level. The upper panel of Figure 1 reveals positive associations for both math and reading: students in countries with highly knowledgeable teachers tend to perform better than their peers in countries with teachers who have less of a command of the material they are teaching.\nThe availability of both student and teacher performance measures is a unique feature of the SACMEQ assessments. Other international student assessments contain at best coarse measures of teacher quality, for example, teachers' educational attainment. To help us understand if teacher subject knowledge is a better predictor of student performance than their credentials, the bottom panel of Figure 1 plots a country's average student performance against the share of teachers with a college degree. Unlike subject 11. There are even bigger cross-country differences for the following item: \"If the height of a fence is raised from 60 cm to 75 cm, what is the percentage increase in height: (a) 15 percent, (b) 20 percent, (c) 25 percent, or (d) 30 percent?\" Correct answer rates vary between 18 percent in Zanzibar and 88 percent in Kenya. 12. These comparisons actually overestimate the relative performance of teachers in Sub-Saharan Africa because, in the SACMEQ assessment, the teachers had only four different answers from which to choose, whereas the eighth-grade students in TIMSS had to choose among five possible answers. knowledge, educational credentials appear to explain little if any of the cross-country variation in student performance."}, {"section_title": "13", "text": ""}, {"section_title": "III. Estimation Strategy", "text": "In the baseline ordinary least squares (OLS) model, we estimate the following education production function:\n(1) y ikcs = a + b T kcs + c 1 X ics + c 2 X cs + c 3 X s + d Z kcs + n c + e ikcs where y ikcs is the test score of student i in subject k (math or reading) in classroom c in school s, T kcs is the test score of student i's teacher in subject k, X ics is a vector of student-level subject-invariant controls measuring student and family background, X cs Figure 1 Potential Determinants of Cross-Country Differences in Student Performance Source: SACMEQ. Notes: Solid lines fit a linear relationship between student performance and teacher subject knowledge in the top panels and between student performance and the share of college-educated teachers in the bottom panels. Share of college-educated teachers is the share of sixth-grade teachers with a college degree (based on SACMEQ data). Country abbreviations: BOT = Botswana, KEN = Kenya, LES = Lesotho, MAL = Malawi, MOZ = Mozambique, NAM = Namibia, SEY = Seychelles, SWA = Swaziland, TAN = Tanzania, UGA = Uganda, ZAM = Zambia, ZAN = Zanzibar, ZIM = Zimbabwe.\n13. A qualitatively similar picture emerges if we instead use the share of teachers who completed at least secondary school. Bietenbeck, Piopiunik, and Wiederhold 559 is a vector of subject-invariant classroom and teacher characteristics, X s is a vector of subject-invariant school characteristics, and Z kcs contains classroom and teacher characteristics that vary across subjects (for example, the availability of teacher guides in math or reading).\n14 x c is a vector of country fixed effects, which absorb any countryspecific differences in student performance.\n15 e ikcs is the error term."}, {"section_title": "16", "text": "Interpreting the OLS estimate of b as the causal effect of teacher subject knowledge on student performance is problematic because of omitted variables that might be correlated with both student and teacher test scores. For instance,b will be biased upward if highly educated parents select schools or classrooms with better teachers and also foster their children's learning in other ways. Similarly, student sorting across or within schools will lead to biased estimates if students with high (unobserved) academic ability are more likely to attend schools or classrooms with highly knowledgeable teachers.\nTo address these sources of bias, we exploit the fact that students were tested in two subjects and ask whether differences in teacher knowledge between math and reading are systematically related to differences in student performance between the same two subjects. This implies that we identify the effect of teacher subject knowledge based only on variation between teacher math and reading knowledge within the same student. 17 We thus estimate the following first-differenced model, which we implement by pooling the two subjects, math and reading, and adding student fixed effects to Equation 1:\nThis model controls for the influence of any student-level performance determinants that are not subject-specific, such as family background, overall academic ability, or general motivation. It also eliminates the impact of school resources that do not differ across subjects, such as availability of blackboards, chairs, and computers. Therefore, estimates from the student fixed-effects model are not biased by student sorting across or within schools, as long as such sorting is not subject specific. 18 In robustness checks, we provide evidence that our estimates are also unlikely to be biased by subject-specific sorting.\nThe within-student model of Equation 2 ensures that the estimates are not confounded by any subject-invariant student characteristics; however, unobserved teacher traits could still bias the coefficient on teacher subject knowledge. For example, if teachers with high subject knowledge are also more motivated (not observed in the data), a positive estimate of b might partly reflect the impact of high motivation. The fact that about one-third of the students in our sample are taught both math and reading by the same teacher allows us to address this issue in a robustness check. Specifically, by restricting the sample to students taught both subjects by the same teacher (same-teacher sample), we can control for any teacher traits that affect students' math and reading 14 . See the notes to Table 1 for a complete list of control variables. 15. The country fixed effects also control for potential cross-country differences in school curricula or in the timing of national examinations. 16. Additionally, we include a wave dummy in all specifications. To simplify notation, we omit the wave dummy and the wave subscripts in all equations. 17. Within-student across-subject variation has been exploited in previous studies (for example, Dee 2005 Dee , 2007 Clotfelter, Ladd, and Vigdor 2010; Lavy 2015) . 18. In contrast to the OLS model, the impact of teacher subject knowledge in the fixed-effects model is \"net\" of teacher knowledge spillovers across subjects. performance in the same way. 19 The results suggest that our student fixed-effects estimates are not confounded by correlated teacher traits. Table 1 reports estimates of the association between student performance and teacher subject knowledge in math (Panel A) and in reading (Panel B) based on the model in Equation 1. In addition to an increasing number of control variables at the student, classroom, school, and teacher level, all specifications include country fixed effects."}, {"section_title": "IV. Results", "text": ""}, {"section_title": "A. Ordinary Least Squares Results", "text": ""}, {"section_title": "20", "text": "To facilitate interpretation of effect sizes, both student and teacher test scores are standardized with a mean of zero and a standard deviation of one across countries and waves. Throughout our analysis, we cluster standard errors at the school level."}, {"section_title": "21", "text": "The results in Table 1 show a strong positive association between teacher subject knowledge and student performance in both math and reading. In the most parsimonious specification that includes only country fixed effects, a one SD increase in teacher subject knowledge is associated with a 0.12 SD increase in student performance in both subjects (Column 1). This association becomes weaker when student, classroom, and school characteristics are added as controls, but remains statistically significant (Columns 2-4). Interestingly, the coefficient on teacher subject knowledge changes only slightly when teacher characteristics, such as educational attainment and experience, are also controlled for (Column 5). In this most restrictive specification, a one SD increase in teacher subject knowledge is associated with a 0.07 (0.06) SD increase in student performance in math (reading)."}, {"section_title": "22", "text": ""}, {"section_title": "B. Student Fixed-Effects Results", "text": "The OLS estimates in Table 1 are likely biased due to omitted variables and nonrandom sorting across or within schools. Therefore, we now turn to the student fixed-effects model that identifies the impact of teacher subject knowledge based only on withinstudent variation between math and reading. The results, shown in Table 2 , indicate that teacher subject knowledge has a positive and statistically significant impact on student performance. When controls for subject-specific classroom and teacher characteristics are added, a one SD increase in teacher subject knowledge raises student performance 19. Using the same-teacher sample is equivalent to adding teacher fixed effects in Equation 2, thus exploiting only variation within students and within teachers. 20. Because the regressions in Table 1 use only within-country variation, the coefficients do not correspond to the cross-country correlations in the upper panels of Figure 1 . 21. The SACMEQ data include student sampling weights, and we confirmed that our coefficient estimates are virtually identical independently of whether we weight observations or not in the regressions. However, as described in Solon, Haider, and Wooldridge (2015) , using sampling weights may unnecessarily decrease precision, an issue that affects especially our regressions that focus on smaller subsamples of students. We therefore chose to report the unweighted estimates in the paper.\n22. An assumption embodied in the student fixed-effects model is that the effect of teacher subject knowledge is similar across subjects. Supporting this assumption, a cross-equation test indicates that one cannot reject the equality of OLS coefficients in math and reading (in the full-control models in Column 5, the respective p-value is 0.211).\nby 0.026 SD (Column 3). 23 This suggests that differences in teacher subject knowledge account for about 20 percent of the variation in teacher value added, with evidence on Socioeconomic controls include three student characteristics (age, gender, and repeated grade) and 13 family background measures (mother's education, father's education, number of books at home, and ten family resources). Classroom controls contain four classroom resources (availability of subject-specific textbooks, number of books in class, access to teaching guide, class size), and school controls include five measures of school resources and location (school facilities index [see Table 4 ], private school indicator, frequency of teacher absence at school, number of students in school, rural school indicator). Teacher controls include six teacher characteristics (gender, education, work experience, duration of subject-specific training, weekly teaching time, and frequency of meeting parents). All regressions include imputation dummies and a dummy indicating the SACMEQ wave. Robust standard errors, adjusted for clustering at the school level, are reported in parentheses. Significance levels: *p < 0.10, **p < 0.05, ***p < 0.01.\n23. Besides teacher subject knowledge, the only statistically significant explanatory variables are a dummy for female teachers and a dummy for teachers having access to a teaching guide for their subject; the coefficients on both variables are positive.\nteacher value added coming from India and the United States (Chetty, Friedman, and Rockoff 2014; Jackson, Rockoff, and Staiger 2014; Azam and Kingdon 2015) . Compared to the OLS estimates in Table 1 , the fixed-effects estimate on teacher subject knowledge is much smaller. One obvious explanation for this finding is that unobserved student characteristics correlated with both student and teacher test scores bias the OLS estimates upward. Another possible explanation is that attenuation bias due to measurement error in teacher subject knowledge is aggravated in the fixed-effects model (see Angrist and Krueger 1999) . In the Appendix, we show how the reliability ratios of the teacher math and reading tests can be used to correct for measurement error. We find that the measurement error-corrected coefficient on teacher subject knowledge is 50 percent larger than the baseline estimate (0.039 vs. 0.026 SD). However, this correction procedure hinges on several strong assumptions, such as that the measurement errors in math and reading tests are uncorrelated. Therefore, we report only the uncorrected, more conservative estimates throughout the paper.\n24 Figure 2 shows a nonparametric version of the regression in Column 3 of Table 2 . To create this binned scatterplot, we first regressed the differences between math and reading scores of students and the respective test score differential of teachers separately on all control variables (also in differences). We then divided the residualized teacher test scores into 20 equally sized groups and plotted the mean value in each bin against the mean value of the residualized student test scores. The figure suggests that the relationship between the test score differentials is roughly linear. To illustrate this result, Table 1 , among classroom characteristics, class size is excluded because it does not vary across subjects for the same student. All regressions include imputation dummies. Robust standard errors, adjusted for clustering at the school level, are reported in parentheses. Significance levels: *p < 0.10, **p < 0.05, ***p < 0.01.\n24. Note that differential measurement error in teacher math and reading knowledge could create bias in the student fixed-effects model. Reassuringly, however, test reliability in math and reading is very similar, with estimated reliability ratios of 0.83 for math and 0.75 for reading (see Appendix), thus indicating a similar degree of measurement error in both subjects.\nconsider two teachers: one teacher has average knowledge in both subjects (for example, math = 0, reading = 0), and the other teacher has higher math than reading knowledge (for example, math = 1, reading = 0). Suppose that the math knowledge of both teachers improves by one SD. Then, the relative math performance (versus reading performance) of the students of both teachers increases by the same amount. One important question concerning the interpretation of our results is whether the estimates capture the impact of teacher subject knowledge for only a single school year or, instead, the cumulative effect over several school years. Unfortunately, the SACMEQ data do not contain information on how long each teacher has been teaching a particular class. However, there is ample evidence that teacher turnover in Sub-Saharan Africa is high, with annual attrition rates ranging between 5 and 30 percent (Mulkeen et al. 2007 ). Moreover, a study from two Malawian school districts finds that almost 50 percent of the 188 teachers who began the school year were not teaching the same class nine months later (IEQ 2000) . Given this high turnover in the teacher workforce, our estimates likely capture a short-run effect of teacher subject knowledge on student performance."}, {"section_title": "Figure 2 Effect of Teacher Subject Knowledge on Student Performance", "text": "Source: SACMEQ. Notes: The figure displays a binned scatterplot corresponding to the student fixed-effects model in Column 3 of Table 2 ; see notes to Tables 1 and 2 for a list of the control variables. To construct the figure, we first regressed the test score difference between math and reading of students and teachers separately on all control variables (also differences between math and reading). We then divided the teacher test score residuals into 20 ranked equally sized groups and plotted the mean of the student test score residuals against the mean of the teacher test score residuals in each bin. The best-fit line, the coefficient, and the standard error (clustered at the school level) are calculated from regressions on the microdata."}, {"section_title": "The Journal of Human Resources", "text": ""}, {"section_title": "V. Robustness Checks", "text": ""}, {"section_title": "A. Subject-Specific Student Sorting", "text": "The student fixed-effects specifications identify the impact of teacher subject knowledge based only on within-student between-subject variation. Thus, they account for potential sorting of students to schools or teachers based on subject-invariant factors, such as students' overall academic ability. The estimates will be biased, however, if sorting is based on subject-specific factors. For example, our estimate will be biased upward if mathematically gifted students systematically attend schools with knowledgeable math teachers, or if principals tend to assign mathematically gifted students to teachers with high math knowledge. Columns 1-4 of Table 3 suggest that these mechanisms are unlikely to drive our results.\nWe first address the issue of sorting across schools by restricting the sample to students living in rural areas, where students likely have little choice between different schools. Column 1 of Table 3 shows that the estimated coefficient on teacher subject knowledge in this sample is similar to our baseline coefficient, suggesting that it is unlikely that nonrandom sorting of students across schools is biasing our results. To address the concern of sorting within schools, we focus on schools with only one sixth-grade classroom, meaning that principals cannot assign students to teachers based on their subject-specific ability. As shown in Column 2, the impact of teacher subject knowledge in this sample is similar to the estimate in the full sample. Column 3 shows that our results hold even when we restrict the sample to one-classroom schools in rural areas, simultaneously addressing across-school and within-school sorting.\nAn alternative way of accounting for potential sorting based on subject-specific factors within schools is to aggregate teachers' subject knowledge to the school level. Again, our estimate remains unaffected, suggesting that nonrandom sorting within schools is not driving our results (Column 4 of Table 3) . 25 Finally, a particularly salient motive for subject-specific sorting relates to the match between the language spoken by students at home and at school. 26 For example, students who speak English at home (\"native speakers\") may have a preference for schools with teachers who are proficient in English. However, we investigated whether the impact of teacher subject knowledge varies with the share of native speakers at school and found no evidence of such effect heterogeneity (for details, see Bietenbeck, Piopiunik, and Wiederhold 2015) ."}, {"section_title": "B. Unobserved Teacher Traits", "text": "Another concern is that our estimates reflect the effect of other, unobserved teacher characteristics correlated with subject knowledge. For example, teachers with high subject knowledge might also have excellent pedagogical skills, which would bias the coefficient of interest upward. In Column 5 of Table 3 , we address this concern by 25. For this analysis, all other teacher characteristics are aggregated to the school level as well. 26. In the countries covered by SACMEQ, English is typically both the official language of instruction and the test language. In practice, however, the dominant language of instruction and the language that students use in their daily lives may not be English. \nTeacher subject knowledge and waves. In Column 1, the sample includes only schools in rural areas. In Column 2, all schools with more than one sixth-grade classroom are excluded. In Column 3, the sample includes only schools in rural areas with just one sixth-grade classroom. In Column 4, teacher test scores and all teacher characteristics are collapsed at the school level. In Column 5, the sample includes only students who are taught both math and reading by the same teacher; teacher characteristics are excluded as they do not vary within the same teacher. In Column 6, the subject indicator is interacted with students' socioeconomic characteristics (but not with family resources) and school characteristics (see Table 1 ); in Column 7, the subject indicator is additionally interacted with the country fixed effects. Classroom and teacher characteristics are the same as in Table 2 . All regressions include subject fixed effects and imputation dummies. Robust standard errors, adjusted for clustering at the school level, are reported in parentheses. Significance levels: *p < 0.10, **p < 0.05, ***p < 0.01.\nrestricting the sample to students taught both math and reading by the same teacher (which is equivalent to including teacher fixed effects in the full sample). 27 Identification in this same-teacher sample is based only on variation in subject knowledge between math and reading within teachers. Hence, all subject-invariant teacher traits, such as general pedagogical skills or absenteeism, are controlled for. Using the sameteacher sample leaves our baseline student fixed-effects results unchanged, indicating that unobserved subject-invariant teacher characteristics are unlikely to bias our estimates."}, {"section_title": "28", "text": ""}, {"section_title": "C. Subject-Specific Impacts of Covariates", "text": "One of the assumptions underlying the student fixed-effects model is that subjectinvariant covariates (for example, parental education) have a similar impact on student performance in both math and reading. To account for the possibility that covariates affect math and reading performance differently, we estimate a model in which all student and school characteristics are interacted with a subject dummy. 29 As Column 6 of Table 3 shows, the estimated coefficient on teacher subject knowledge is slightly smaller but remains statistically significant."}, {"section_title": "30", "text": "In Column 7, we additionally allow country-specific impacts to differ across math and reading by including subject-by-country fixed effects. In this specification, identification is based only on within-student variation in teacher subject knowledge relative to the country mean. Since the between-country variation in both teacher subject knowledge and student performance is substantial (see Table A1 ), this implies that a considerably smaller part of the sample variation is used. The impact of teacher subject knowledge remains positive in this model, although the coefficient is substantially smaller than our main estimate. This suggests that the impact of teacher subject knowledge on student performance varies substantially across countries, an issue that we investigate in the next section.\n27. Across all countries in our sample, 35 percent of students are taught both subjects by the same teacher. 28. While we control for any differences between teachers that are similar across subjects-most importantly, general motivation and pedagogical skills-our results might still be affected by subject-specific teacher traits (for example, particularly high motivation in one subject) if correlated with subject knowledge. We do not have information on subject-specific teacher effort or motivation, and hence, we cannot include such measures in the set of controls. However, it seems likely that differences in unobserved teacher traits are much larger between teachers than within the same teacher across the two subjects. While we cannot test this assumption directly, our data allow us to assess between-teacher vs. within-teacher variation in observable teacher traits, that is, teacher subject knowledge. Using the math and reading scores of teachers observed in both subjects (that is, our same-teacher sample), we find that 71 percent of the test score variation is between teachers, and only 29 percent is within teachers. Still, just as in Metzler and Woessmann (2012) , our results should be interpreted as the impact of teacher subject knowledge and any subject-specific trait correlated with it. 29. Results are similar when additionally including interactions of the ten family resources with the subject dummy. 30. While we would also like to control for subject-specific instructional time as a potentially important determinant of student performance, our data do not provide this information. Existing evidence suggests that this is not a major concern since Metzler and Woessmann (2012) show for Peru that including subjectspecific instruction time does not affect the estimate of the effect of teacher subject knowledge on student performance."}, {"section_title": "VI. Heterogeneity", "text": "A unique feature of the SACMEQ data is that they provide comparable measures of teacher subject knowledge and student performance for a relatively large number of countries. Furthermore, the countries included in SACMEQ differ substantially in regard to economic development. For example, GDP per capita ranges from $595 in Mozambique to $17,811 in the Seychelles, a difference by a factor of 30.\n31 This allows us to investigate whether the impact of teacher subject knowledge varies systematically with a country's level of economic development.\nIn Table 4 , we find that teacher subject knowledge is effective only in more developed countries, as measured by higher GDP per capita (Column 1) or a higher rank on the Human Development Index (HDI), which is a broader measure based on income, life expectancy, and literacy (Column 2).\n32 In countries at a higher stage of development, a one SD increase in teacher subject knowledge increases student learning by 0.04-0.05 SD. While a causal interpretation of these results is clearly difficult because countries differ along several unobserved dimensions, the remainder of Table 4 digs deeper into one potential mechanism explaining this country heterogeneity."}, {"section_title": "33", "text": "One reason why the effect of teacher subject knowledge varies with the country's stage of development may be that richer countries have more resources to spend on schools.\n34 SACMEQ provides various measures of school resources at the student and school level, allowing us to test more directly whether the estimated teacher effect varies with a school's resource endowment. We first interact teacher subject knowledge with textbook availability during class, a crucial educational resource that is often lacking in Sub-Saharan Africa. Since each student reports the availability of textbooks separately for math and reading, we exploit within-student across-subject variation in both teacher knowledge and textbook availability. 35 Column 3 of Table 4 shows that an increase in teacher subject knowledge improves student performance twice as much for students who have textbooks during class compared to students without textbooks.\nIn Columns 4 and 5 of Table 4 , we consider more general school resources. SACMEQ contains information on the availability of a large variety of school resources (reported 32. The similarity of results for GDP per capita and HDI rank is not surprising since both indicators are highly correlated in our country sample (r = 0.92). 33. In additional analyses, we also estimated the impact of teacher subject knowledge separately for each country. We find positive point estimates in almost all countries. However, due to the small sample sizes, we could detect a significant effect in only two of 13 countries. 34. Another reason might be that GDP per capita reflects the quality of (educational) institutions (Hanushek, Link, and Woessmann 2013) . Other reasons could include differences between countries involving teacher absenteeism, teacher effort, and/or learning culture. 35. Students were asked \"How are the math textbooks used in your classroom during the lessons?\", with five answer categories: (1) There are no math textbooks; (2) Only the teacher has a math textbook; (3) I share a math textbook with two or more pupils; (4) I share a math textbook with one pupil; (5) I use a math textbook by myself. The analogous question was asked about reading textbooks. In line with Glewwe, Kremer, and Moulin (2009), we group students who use a textbook by themselves and students who share a textbook with only one other student because all these students can effectively use a textbook during class. This categorization is also consistent with experimental evidence from the Philippines that providing one textbook for every two students and providing one textbook for each student has very similar effects on test scores (Heyneman, Jamison, and Montenegro 1984) . The sample mean of our binary textbook variable is 0.56 for math and 0.58 for reading. Source: SACMEQ. Notes: Fixed-effects estimations. Dependent variable: student performance in math and reading. GDP per capita: gross domestic product divided by midyear population expressed in PPP-US$; data from the UNESCO Institute for Statistics. The following countries have a \"high\" (that is, above-median) GDP per capita: Botswana, Kenya, Namibia, Seychelles, Swaziland; data for Zimbabwe are not available (therefore excluded in Column 1). Human Development Index: summary measure of average achievement in key dimensions of human development: a long and healthy life, being knowledgeable, and having a decent standard of living; data are from the African Development Bank. The following countries have a \"high\" Human Development Index: Botswana, Kenya, Lesotho, Namibia, Seychelles, and Swaziland. We assign the same values to country-level variables in Tanzania and Zanzibar because Zanzibar is a semiautonomous part of Tanzania. Textbook availability: binary variable that equals 1 if a student shares his or her subject-specific textbook with exactly one other student or has own textbook; 0 otherwise. School facilities (index): counts the availability of all 31 school resources reported in SACMEQ: board, cafeteria, chairs, chalk, charts, classroom library, community hall, computer, drinking water, duplicator, electricity, fax, fence, first-aid kit, garden, locker, overhead projector, photocopier, playground, radio, school library, separate office for school head, shelves, storeroom, tables, tape recorder, teacher room, telephone, TV, typewriter, and VCR. Average class size: average number of students per classroom in sixth grade; 3,106 student observations are missing because some principals did not report the number of sixth-grade students in their school. To facilitate interpretation of coefficient magnitudes, the resource variables in Columns 4 and 5 are z-standardized across countries and waves. The main effects of the school-level resources and country-level variables cannot be estimated because these variables do not vary across subjects. Classroom and teacher characteristics are the same as in Column 3 of Table 2 . All regressions include subject fixed effects and imputation dummies. Robust standard errors, adjusted for clustering at the school level, are reported in parentheses. Significance levels: *p < 0.10, **p < 0.05, ***p < 0.01.\nby principals), ranging from blackboards, chairs, and tables to access to drinking water. We combine all 31 school resources into a single index by counting the number of available resources. 36 Column 4 suggests that teacher knowledge is more effective for student learning when more school resources are available. In contrast, we find no significant interaction between teacher subject knowledge and class size, suggesting that teachers with the same level of subject knowledge are as effective in large classrooms as in small ones (Column 5). Taken together, the results in Columns 3-5 suggest a potential mechanism explaining the cross-country differences in the teacher effect: more developed countries have better school resources, which may be complementary to teacher subject knowledge in educational production."}, {"section_title": "37", "text": ""}, {"section_title": "VII. Discussion of Teacher Subject Knowledge Impact", "text": "To put our results into perspective, we compare our teacher knowledge impact to effect sizes in other settings as well as to other types of education inputs and teacher incentives in (mostly) developing countries. Based on sixth-grade students in Peru and using the same identification strategy as is employed in this paper, Metzler and Woessmann (2012) find that a one SD increase in teacher knowledge raises student achievement by about 0.04 SD. This is similar to our estimated impact of teacher subject knowledge (0.03 SD). Teachers' overall impact on student achievement is commonly estimated in value-added (VA) models. Based on administrative data from the United States, Chetty, Friedman, and Rockoff (2014) estimate that a one SD improvement in teacher VA raises student achievement by about 0.14 SD in math and 0.1 SD in English. Assuming that the variation in overall teacher effectiveness in Sub-Saharan Africa is similar to that in the United States, this implies that teacher subject knowledge explains between 20 percent (in math) and 25 percent (in reading) of the variation in teachers' overall effectiveness.\nSeveral previous studies evaluate interventions that aimed to increase student achievement by providing schools with teaching inputs, such as textbooks (Glewwe, Kremer, and Moulin 2009; Sabarwal, Evans, and Marshak 2014) and flipcharts (Glewwe et al. 2004 ). These inputs typically fail to improve student achievement, either because they are not used or because teachers cannot use them effectively. Hence, the impact of teacher subject knowledge is larger than providing these resources.\nGiven that many students in low-income countries attend school for only half the day, another approach to improve children's performance is to expand instructional time. In evaluating the full school day program in Chile, Bellei (2009) finds that a reform that, among other things, increased the number of instruction hours per day by 10 percent improved the Spanish and math achievement of 10th graders by 0.02 SD and 0.03 SD, respectively. Lavy (2012) similarly finds that a 10 percent expansion in the weekly hours 36. To facilitate interpretation of results, we normalize all school-level variables to have a mean of zero and SD of one, such that the main effect of teacher subject knowledge reflects the impact at the sample mean of the respective resource variable. Because school-level resources do not vary across subjects, their main effects on student performance cannot be estimated. 37. In line with this interpretation, we observe that GDP per capita is strongly correlated with textbook availability (r = 0.61) and the index of school resources (r = 0.94) at the country level.\ndevoted to English, math, and science in Israel improved the achievement of fifth graders in these subjects by 0.03 SD. Increasing time spent on instruction thus has an impact on student achievement very similar to that achieved by improving teacher subject knowledge by one SD.\nThe extant literature also studies how incentives that reward teachers for additional effort or for improving their students' test scores affect student performance. Duflo, Hanna, and Ryan (2012) evaluated a program in India that rigorously monitored teacher attendance and offered them bonuses based on the number of days they attended school. This program strongly reduced teacher absenteeism and improved student achievement in math and Hindi by 0.17 SD after 30 months. Muralidharan and Sundararaman (2011) find that offering teachers cash incentives for improving their students' performance in standardized tests raised test scores in math and Telugu by 0.14 and 0.16 SD, respectively, after one year. Unconditional pay increases for teachers, in contrast, do not seem to work (de Ree et al. 2015) . Compared to well-designed (and costly) teacher incentive schemes, the impact of teacher subject knowledge is therefore rather small.\nThe above comparisons reveal that the impact of teacher subject knowledge on student performance falls somewhere between that of interventions with zero impact (for example, providing textbooks or flipcharts) and interventions with strong impacts (for example, paying teachers for attendance that is rigorously monitored)."}, {"section_title": "VIII. Conclusion", "text": "Student performance in Sub-Saharan Africa is low, which may partly explain the region's poor economic performance, given that the cognitive skills of a population are an important driver of economic growth (Hanushek and Woessmann 2012) . We investigate the role of teacher quality in explaining the low student performance, focusing on teacher subject knowledge as one central dimension of teacher quality. Our measures for teacher knowledge in math and reading are curriculum based, thus reflecting the subject knowledge required for teaching. To identify a causal effect of teacher subject knowledge, we exploit within-student variation across math and reading. We find that a one SD increase in teacher subject knowledge raises student performance by 0.03 SD. Results are robust to including teacher fixed effects and are not driven by sorting of students or teachers. Exploiting the vast differences in the countries' economic development, we also provide suggestive evidence that teacher subject knowledge is effective in improving student learning only in countries at a higher stage of development.\nAlthough the effects of increasing teacher subject knowledge on student performance are modest, they are comparable to other well-known interventions, such as expanding instructional time. Moreover, the low skills of teachers in developing countries may limit the impact of other educational interventions (for example, when it comes to using textbooks effectively). Hence, it seems essential to increase the skills of the teacher workforce, either by improving the skills of existing teachers or by hiring teachers with better skills. Bietenbeck, Piopiunik, and Wiederhold 571 As with any performance assessment, teacher subject knowledge in SACMEQ is likely measured with error. Measurement error in the explanatory variable may lead to a downward bias in the estimated coefficient, and this bias may be aggravated in the student fixed-effects models (Angrist and Krueger 1999) . In this appendix, we assess the importance of measurement error for our estimates and propose a way of correcting the corresponding attenuation bias.\nWe begin our analysis by assuming that teacher subject knowledge is measured with random noise. Let T ik * denote the true knowledge of student i's teacher in subject k, and let the observed teacher test score be denoted by T ik = T ik * + e ik .\n38 Assuming classical measurement error, E(e ik ) = 0 and Cov(T ik *, e ik ) = 0. In a bivariate model, the true effect of teacher subject knowledge on student performance, y ik , will then be asymptotically biased towards zero: (A1) y ik = bk k T ik + e ik \u201a where k k = Var(T \u00c3 ik ) Var(T \u00c3 ik ) + Var(e ik ) The factor l k indicates how much the true effect b is attenuated and is often referred to as the reliability ratio or signal-to-noise ratio.\nIn a first-differenced, that is, a student fixed-effects model, the attenuation bias due to measurement error is likely aggravated. Intuitively, teachers' levels of math and reading knowledge are more strongly correlated than the measurement errors in these variables, such that differencing the observed test scores decreases the signal-to-noise ratio. More formally, consider the case where the measurement errors are uncorrelated across subjects, that is, Cov(e im , e ir ) = Cov(T im * , e ir ) = Cov(e im , T ir *) = 0. In this case, the reliability ratio for the first-differenced model can be derived as (see Metzler and Woessmann 2010) :\nNote that the only unknown quantities in Equation A2 are l m and l r , while the variances and covariances of teacher subject knowledge can easily be computed from the data. Therefore, if the reliability ratios of the teachers' math and reading assessments were known, we could correct our baseline estimate for measurement error by multiplying the estimated coefficient with 1/l D .\nReferring to psychometric test theory, Metzler and Woessmann (2012) argue that Cronbach's a is a natural estimate for l k in the context of teacher subject knowledge. We compute Cronbach's a for the math and reading tests (which are not reported by 38 . For conciseness, we omit classroom and school subscripts in this discussion. "}]