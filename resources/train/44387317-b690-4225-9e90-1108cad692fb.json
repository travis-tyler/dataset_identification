[{"section_title": "List of", "text": ": Average mathematics scale scores of fourth-grade students, by country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4 Table 3: Average mathematics scale scores of eighth-grade students, by country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5 Table 4: Differences in average mathematics scale scores of fourth-grade students, by country: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6 Table 5: Differences in average mathematics scale scores of eighth-grade students, by country: 1995, 1999, and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7 Table 6: Average mathematics scale scores of fourth-grade students, by country: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8 Table 7: Average mathematics scale scores of eighth-grade students, by country: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9 Table 10: Differences in average science scale scores of fourth-grade students, by country: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .16 Table 11: Differences in average science scale scores of eighth-grade students, by country: 1995, 1999, and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .17 Table 12: Average science scale scores of fourth-grade students, by country: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .18 Table 13: Average science scale scores of eighth-grade students, by country: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .19 vii Table A1. Coverage of TIMSS grade 4 and 8 target population and participation rates, by country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .31 Table A2. TIMSS grade 4 and 8 student and school samples, by country: 2003 . . . . .34 Table A3. Distribution of new and trend mathematics and science items in the TIMSS grade 4 and 8 assessments, by type: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .40 Table A4. Number of mathematics and science items in the TIMSS grade 4 and 8 assessments, by type and content domain: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . .41 Table A5. Within-country constructed-response scoring reliability for TIMSS grade 4 and 8 mathematics and science items, by exact percent score agreement and country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .44 Table A6. Weighted response rates for unimputed variables for TIMSS grade 4 and 8: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .49 Table C4. Average mathematics scale scores of eighth-grade students, by country: 1995, 1999, and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .78 Table C5. Average mathematics scale scores of eighth-grade students, by country: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .79 Table C6. Percent correct of eighth-grade students in five mathematics content areas, by country: 1999 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .80 Table C7. Average mathematics scale scores of fourth-grade students, by sex and country: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .82 Table C8. Average mathematics scale scores of U.S. fourth-grade students, by selected characteristics: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .83 viii Table C9. Standard deviations of mathematics and science scores of fourth-grade students, by country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .84 Table C10. Average mathematics scale scores of eighth-grade students, by sex and country: 1995, 1999, and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .85 Table C11. Average mathematics scale scores of U.S. eighth-grade students, by selected characteristics: 1995, 1999, and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .87 Table C12. Standard deviations of mathematics and science scores of eighth-grade students, by country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .88 Table C13. Average science scale scores of fourth-grade students, by country: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .89 Table C14. Average science scale scores of eighth-grade students, by country: 1995, 1999, and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .90 Table C15. Average science scale scores of eighth-grade students, by country: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .91 Table C16. Percent correct of eighth-grade students in five science content areas, by country: 1999 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .92 Table C17. Average science scale scores of fourth-grade students, by sex and country: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .94 Table C18. Average science scale scores of U.S. fourth-grade students, by selected characteristics: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .95 Table C19. Average science scale scores of eighth-grade students, by sex and country: 1995, 1999, and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .96 Table C20. Average science scale scores of U.S. eighth-grade students, by selected characteristics: 1995, 1999, and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .98 Table C21. Standard deviations of mathematics and science scores of U.S. fourth-grade and eighth-grade students, by selected characteristics: 2003 . . . . . . . . . .99 Figure 1: Average mathematics scale scores of U.S. fourth-grade students, by sex, race/ethnicity, and poverty level: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . .11 Figure 2: Average mathematics scale scores of U.S. eighth-grade students, by sex, race /ethnicity, and poverty level: 1995, 1999, and 2003 . . . . . . . . . . . . . . . . .13 Figure 3: Average science scale scores of U.S. fourth-grade students, by sex, race/ethnicity, and poverty level: 1995 and 2003 . . . . . . . . . . . . . . . . . . . . . . .21 Figure 4: Average science scale scores of U.S. eighth-grade students, by sex, race /ethnicity, and poverty level: 1995, 1999, and 2003 . . . . . . . . . . . . . . . . .23 x"}, {"section_title": "ix", "text": ""}, {"section_title": "List of Figures", "text": ""}, {"section_title": "Introduction", "text": "The Trends in International Mathematics and Science Study (TIMSS) 2003 is the third comparison of mathematics and science achievement carried out since 1995 by the International Association for the Evaluation of Educational Achievement (IEA), an international organization of national research institutions and governmental research agencies. TIMSS can be used to track changes in achievement over time. Moreover, TIMSS is closely linked to the curricula of the participating countries, providing an indication of the degree to which students have learned concepts in mathematics and science they have encountered in school. In 2003, some 46 countries participated in TIMSS, at either the fourth-or eighth-grade level, or both. This summary highlights initial findings on the performance of U.S. fourth-and eighth-grade students relative to their peers in other countries on the TIMSS assessment. The summary is based on the findings presented in two reports published by IEA: \u2022 TIMSS 2003 International Mathematics Report: Findings from IEA's Trends in International Mathematics and Science Study at the Eighth and Fourth-Grades ) and \u2022 TIMSS 2003 International Science Report: Findings from IEA's Trends in International Mathematics and Science Study at the Eighth and Fourth-Grades ). These two IEA reports were simultaneously published with this summary report and are available online at http://www.timss.org. This summary report describes the mathematics and science performance of fourth-and eighth-graders in participating countries over time. For a number of the participating countries, changes in mathematics and science achievement can be documented over 8 years, from 1995 to 2003. For others, changes can be documented over a shorter period of time, 4 years from 1999 to 2003. Table 1 shows the countries that participated in TIMSS 2003, and their participation in earlier TIMSS data collections. The fourthgrade assessment was offered in 1995 and 2003, while the eighth-grade assessment was offered in 1995, 1999, and 2003.  Table A7 in appendix A groups the participating countries by continent and membership in the Organization for Economic Cooperation and Development (OECD), an intergovernmental organization of 30 industrialized countries that serves as a forum for members to cooperate in research and policy development on social and economic topics of common interest. Average student performance in the United States is compared to that of students in other countries that participated in each assessment. \u2022 At fourth grade, comparisons are made among students in the 25 countries that participated in TIMSS 2003, and in the 15 countries that participated in TIMSS 2003 and TIMSS 1995. \u2022 At eighth grade, comparisons are made among students in the 45 countries that participated in TIMSS 2003, and in the 34 countries that participated in TIMSS 2003 and at least one earlier data collection, either TIMSS 1995 or 1999, or both. \u2022 Results are presented first for mathematics and then for science at both grade levels. \u2022 All estimates for the United States are based on the performance of students from both public and private schools, unless otherwise indicated. All countries were required to draw random, nationally representative samples of students and schools. The U.S. fourth-grade sample achieved an initial school response rate of 70 percent (weighted); with a school response rate of 82 percent, after replacement schools were added. From the schools that agreed to participate, students were sampled in intact classes. A total of 10,795 fourth-grade students were sampled for the assessment and 9,829 participated, for a 95 percent student response rate. The resulting fourth-grade overall response rate, with replacements included, was 78 percent. The U.S. eighth-grade sample achieved an initial school response rate of 71 percent; with a school response rate of 78 percent, after replacement schools were added. A total of 9,891 students were sampled for the eighth-grade assessment and 8,912 completed the assessment, for a 94 percent student response rate. The resulting eighth-grade overall response rate, with replacements included, was 73 percent. Only the Flemish education system in Belgium participated in TIMSS in 2003. 3 England collected data at grade 8 in 1995, 1999 and 2003, but due to problems with meeting the minimum sampling requirements for 2003, its eighth-grade data are not shown in this report. Only Latvian-speaking schools were included in 1995 and 1999. For trend analyses, only Latvian-speaking schools are included in the estimates."}, {"section_title": "7", "text": "Because within classroom sampling was not accounted for, 1995 data are not shown for South Africa. NOTE: Countries that participated in 1995 and 1999 but did not participate in 2003 are not shown. Only countries that completed the necessary steps for their data to appear in the reports from the International Study Center are listed. In addition to the countries listed above, four separate jurisdictions participated in TIMSS 2003: the provinces of Ontario and Quebec in Canada; the Basque region of Spain; and the state of Indiana. Information on these four jurisdictions can be found in the international TIMSS 2003 reports. The Syrian Arab Republic participated in TIMSS 2003 at the eighth-grade level, but due to sampling difficulties, it is not shown in this report. Yemen participated in TIMSS 2003 at the fourth-grade level, but because it did not comply with the minimum sample requirements, it is not shown in this report. Countries could participate at either grade level. Countries were required to sample students in the upper of the two grades that contained the largest number of 9-year-olds and 13-year-olds, respectively. In the United States and most countries, this corresponds to grade 4 and grade 8. See table A1 in appendix A for details. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 1995(TIMSS), , 1999(TIMSS), , and 2003 In addition to the assessments, students, their teachers, and principals were asked to complete questionnaires related to their school and learning experiences. At fourth grade, the assessment took approximately 72 minutes to complete. At eighth grade, the assessment took approximately 90 minutes. Detailed information on data collection, sampling, response rates, test development and design, weighting, and scaling is included in appendix A. Example items from the fourth-and eighth-grade assessments are included in appendix B. Comparisons made in this report have been tested for statistical significance at the .05 level. Differences between averages or percentages that are statistically significant are discussed using comparative terms such as \"higher\" and \"lower. \" Differences that are not statistically significant are either not discussed or referred to as \"no measurable differences found\" or \"not statistically significant. \" In this latter case, failure to find a statistically significant difference should not be interpreted to mean that the estimates are the same or similar; rather, failure to find a difference may also be due to measurement error or sampling. In addition, because the results of tests of statistical significance are, in part, influenced by sample sizes, when subgroup comparisons are drawn within the United States, effect sizes are also included in order to provide the reader an increased understanding of the importance of the significant difference. These effect sizes use standard deviations, rather than standard errors, and thus are not influenced by the size of the subgroup samples. In social sciences, as used here, an effect size of .2 is considered small, one of .5 is of medium importance, and one of .8 or larger is considered large. Information on the technical aspects of the study can be found in appendix A, as well as in the TIMSS 2003 Technical Report . Detailed tables with estimates and standard errors for all analyses included in this report are provided in appendix C. A list of TIMSS publications and resources published by NCES and the IEA is provided in appendix E.\nBecause of changes in the population tested, 1995 data for Israel and Italy are not shown."}, {"section_title": "Mathematics", "text": "How did U.S. fourth-and eighth-graders perform in mathematics in 2003? Fourth Grade: \u2022 In 2003, U.S. fourth-grade students scored 518, on average, in mathematics, exceeding the international average of 495 (table 2 and table C1 in appendix C). U.S. fourth-graders outperformed their peers in 13 of the other 24 participating countries, and performed lower than their peers in 11 countries. \u2022 In comparison to students in the other 10 OECD-member countries participating in the fourth-grade TIMSS assessment, U.S. fourthgraders outperformed their peers in mathematics in 5 countries (Australia, Italy, New Zealand, Norway, and Scotland) and were outperformed by their peers in the other 5 countries (Belgium-Flemish, England, Hungary, Japan, and the Netherlands) (table 2). "}, {"section_title": "4", "text": "\nThe effect size of the difference between two means can be calculated by dividing the raw difference in means by the pooled standard deviation of the comparison groups (appendix A for an explanation). The effect size of the difference in mathematics achievement between U.S. boys and girls in 2003 is .11 (table C21 in appendix C for standard deviations of U.S. student population groups).  "}, {"section_title": "MATHEMATICS", "text": "Eighth Grade: \u2022 In 2003, U.S. eighth-graders scored 504, on average, in mathematics. This average score exceeded the international average as well as the average scores of their peers in 25 of the 44 other participating countries (table 3 and table  C2 in appendix C). U.S. eighth-graders were outperformed by students in nine countries: five Asian countries (Chinese Taipei, Hong Kong SAR, Japan, Korea, and Singapore) and four European countries (Belgium-Flemish, Estonia, Hungary, and the Netherlands). \u2022 In comparison to their peers in the other 12 OECD-member countries participating in the eighth-grade TIMSS assessment, U.S. eighthgraders outperformed students in mathematics in 2 countries (Italy and Norway) and were outperformed by their peers in 5 countries (Belgium-Flemish, Hungary, Korea, Japan, and the Netherlands) (table 3). \nDid the mathematics performance of U.S. fourthand eighth-graders change between 1995 and 2003? Fourth Grade: \u2022 In both 1995 and2003, U.S. fourth-graders had an average score of 518 in mathematics (table 4 and  table C3 in appendix C). Fourthgraders in six other countries also showed no measurable change in average mathematics performance over the same time period. \u2022 In contrast, fourth-graders in 6 of the 15 participating countries showed an increase in average mathematics achievement scores between 1995 and 2003: Cyprus, England, Hong Kong SAR, Latvia-LSS, 2 New Zealand, and Slovenia (table 4). Fourthgraders in two countries-the Netherlands and Norway-experienced a decrease in average mathematics achievement scores over the same period of time. Eighth Grade: \u2022 U.S. eighth-graders showed significant improvement in average mathematics performance over the 8year period between 1995 and 2003 (table 5 and table C4 in appendix C). In 1995, U.S. eighthgraders had an average score of 492. In 2003, U.S. eighth-graders improved their average mathematics score by 12 points, to 504. No measurable change was detected in the average U.S. mathematics performance between 1999 and 2003, thus indicating that the increase in average mathematics performance in the United States occurred primarily between 1995 and 1999. \u2022 In addition to the United States, eighth-graders in six other countries improved their average mathematics performance between 1995or between 1999). \u2022 Eighth-graders in 11 countries showed significant declines in their average mathematics achievement between 1995or between 1999 The remaining 16 countries showed no measurable difference in the average mathematics scores of their students (table 5). Met international guidelines for participation rates in 2003 only after replacement schools were included.\nHas the relative mathematics performance of U.S. fourth-and eighthgrade students changed since 1995? Fourth Grade: \u2022 Although the average mathematics score of U.S. fourthgraders was 518 in both 1995 and 2003 (table 4), the data suggest that the standing of U.S. fourthgraders relative to their peers in 14 other countries was lower in 2003 than in 1995 in mathematics (table 6 and  table C3 in appendix C). In 1995, on average, U.S. fourth-graders were outperformed in mathematics by fourth-graders in 4 of these countries and outperformed fourth-graders in 9 of these countries. In 2003, on average, U.S. fourth-graders were outperformed by fourthgraders in seven of these countries and outperformed fourth-graders in 7 of these countries. In 1995, Maori-speaking students did not participate. Estimates in this table are computed for students taught in English only, which represents between 98-99 percent of the student population in both years. NOTE: Countries are ordered based on the average score. Parentheses indicate countries that did not meet international sampling or other guidelines in 1995. All countries met international sampling and other guidelines in 2003, except as noted. See NCES (1997) for details regarding 1995 data. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one country may be significant while a large difference between the United States and another country may not be significant. Countries were required to sample students in the upper of the two grades that contained the most number of 9-year-olds. \nEighth Grade: \u2022 The average mathematics score for U.S. eighth-graders increased from 492 in 1995 to 504 in 2003. Over the same period, several countries whose eighth-graders outperformed the U.S. eighth-graders in 1995 experienced decreases in their average scores. The net effect resulted in a higher relative standing for U.S. eighth-graders in 2003 (table 7 and table C5 in appendix C). Among this group of countries, U.S. eighth-graders were outperformed by eighth-graders in 12 countries, on average, and outperformed eighthgraders in 4 countries in mathematics in 1995. In 2003, U.S. eighth-graders were outperformed by eighth-graders in seven of these countries, on average, and outperformed eighthgraders in six of these countries in mathematics. Eighth Grade:\nEighth Grade: \u2022 In 2003, U.S. eighth-grade boys and girls both showed improvement in mathematics compared to 1995 (figure 2 and table C10 in appendix C). U.S. eighth-grade boys outperformed girls in 2003. 8 In 2003, U.S. eighth-grade boys' average score in mathematics was 507. This is 12 score points higher than in 1995, when U.S. boys scored 495. U.S. girls' average mathematics score was 502 in 2003. This is also 12 score points higher than in 1995, when U.S. girls scored 490. \u2022 \u2022 As a result of the improvement in the average mathematics achievement of Black eighth-grade students between 1995 and 2003, the gap in average scores between White and Black eighthgrade students narrowed, from 97 score points in 1995 to 77 score points in 2003 (figure 2). 9 Although Hispanic eighth-grade students showed improvement in their mathematics performance between 1995 and 2003, there was no measurable change detected in the gap in average scores between White and Hispanic eighth-grade students. \u2022 In 2003, U.S. eighth-graders in U.S. public schools with the highest poverty level (75 percent or more of students eligible for free or reduced-price lunch) had lower average mathematics scores compared to their counterparts in public schools with lower poverty levels (figure 2). In contrast, students in schools with the lowest level (10 percent or less eligible students) had higher average mathematics scores than students in schools with poverty levels of 25 percent or more eligible. The difference in the average mathematics scores of students in schools with the lowest and highest poverty levels was 103 score points in 2003. 10 \u2022 As was the case in the aggregate, results by poverty level showed no measurable change in average mathematics achievement between 1999 and 2003, the two years for which data are available (figure 2 and table C11 in appendix C).\n*p<.05, denotes a significant difference from 2003 average score. NOTE: Reporting standards not met for Asian category in 1995 or 1999. Reporting standards not met for American Indian or Alaska Native and Native Hawaiian or Other Pacific Islander in 1995. Racial categories exclude Hispanic origin. Other races/ethnicities are included in U.S. totals shown throughout the report. Analyses by poverty level are limited to students in public schools only. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between averages for one student group may be significant while a large difference for another student group may not be significant.  \n"}, {"section_title": "5", "text": ""}, {"section_title": "6", "text": "In 1995, Maori-speaking students did not participate. Estimates in this table are computed for students taught in English only, which represents between 98-99 percent of the student population in both years. NOTE: Countries are ordered based on the 2003 average scores. Parentheses indicate countries that did not meet international sampling or other guidelines in 1995. All countries met international sampling and other guidelines in 2003, except as noted. See NCES (1997) for details regarding the 1995 data. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between averages for one country may be significant while a large difference for another country may not be significant. Countries were required to sample students in the upper of the two grades that contained the largest number of 9-yearolds. In the United States and most countries, this corresponds to grade 4. See   Table 5.  Differences in average mathematics scale scores of eighth-grade students, by country: Country 1995 Difference 1 (2003-1995) (2003-1999 National desired population does not cover all of the international desired population in all years for Lithuania, and in 2003 for Indonesia."}, {"section_title": "8", "text": "Because within classroom sampling was not accounted for, 1995 data are not shown for South Africa. NOTE: Countries are sorted by 2003 average scores. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between averages for one country may be significant while a large difference for another country may not be significant. Parentheses indicate countries that did not meet international sampling or other guidelines in 1995, 1999, or 2003. See appendix A for details regarding 2003 data. See  for details regarding 1995 and 1999 data. Countries were required to sample students in the upper of the two grades that contained the most number of 13-year-olds. In the United States and most countries this corresponds to grade 8. See table A1 in appendix A for details. Detail may not sum to totals because of rounding. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 1995(TIMSS), , 1999(TIMSS), , and 2003.\nThe effect size of the difference in mathematics achievement between U.S. eighth-grade boys and girls in 2003 is .07 (table C21 in appendix C for standard deviations of U.S. student population groups)."}, {"section_title": "9", "text": "\u2022 Between 1999 and2003, U.S. eighth-graders showed significant improvement in correctly answering items associated with two of the five content areas: Algebra (i.e., patterns, equations, and relationships) and Data. 3 There were no measurable differences detected in the average percentage of U.S. students who correctly answered items in Geometry, Measurement, andNumber between 1999 and2003 (table C6 in appendix C). \u2022 The United States was among 17 countries that showed significant change-either an increase or decrease-in the average percentage of eighthgrade students who were able to correctly respond to items in at least one of the five eighth-grade mathematics content areas between 1999 and 2003 (table C6 in appendix C).\nThe effect sizes of the differences in mathematics achievement between White and Black and between White and Hispanic eighth-graders in 2003 are 1.11 and .83, respectively (table C21 in appendix C for standard deviations of U.S. student population groups)."}, {"section_title": "10", "text": "Did the mathematics performance of U.S. population groups change between 1995 and 2003? Fourth Grade: \u2022 No measurable change was detected in the average mathematics achievement of U.S. fourth-grade boys or girls between 1995 and 2003 (figure 1). Nonetheless, U.S. boys outperformed girls in mathematics in 2003, which differs from 1995 when no measurable difference was detected. 4,5 \u2022 Fourth-grade boys and girls in 6 of the 14 other countries showed an improvement in average mathematics achievement: Cyprus, England, Hong Kong SAR, Latvia-LSS, New Zealand, and Slovenia (table C7 in appendix C). \u2022 In 2003, U.S. fourth-graders in U.S. public schools with the highest poverty level (75 percent or more of students eligible for free or reduced-price lunch) had lower average mathematics scores compared to their counterparts in public schools with lower poverty levels (figure 1). Fourth-graders in public schools with the lowest poverty level (10 percent or less eligible students) had higher average mathematics scores than students in schools with higher poverty levels. The difference in the average mathematics scores of students in schools with the lowest and highest poverty levels was 96 score points in 2003. 7\nThe effect size of the difference in mathematics achievement between eighth-graders in public schools with the lowest and highest levels of poverty in 2003 is 1.57 (table C21 in appendix C for standard deviations of U.S. student population groups)."}, {"section_title": "3", "text": "Although many of the participating countries collected data in all three years, analyses of changes in the mathematics content areas at eighth grade are limited to 1999 and 2003 due to the limited number of in common items from year to year."}, {"section_title": "Science", "text": "How did U.S. fourth-and eighth-graders perform in science in 2003? Fourth Grade: \u2022 In 2003, fourth-graders in the United States scored 536, on average, on the TIMSS science assessment, which was higher than the international average of 489 (table 8 and table C1 in appendix C). Of the 24 other participating countries, fourth-graders in 16 countries demonstrated lower science scores, on average, than fourthgraders in the United States, while students in 3 countries-Chinese Taipei, Japan, and Singaporeoutperformed their peers in the United States. \u2022 In comparison to the other 10 OECD-member countries in science, U.S. fourth-grade students outperformed their peers in seven countries in 2003 (Australia, Belgium-Flemish, Italy, the Netherlands, New Zealand, Norway, and Scotland; table 8). Japanese fourth-grade students were the only group of students to outperform U.S. fourth-grade students among the participating OECD-member countries. "}, {"section_title": "14", "text": ""}, {"section_title": "SCIENCE", "text": "Eighth Grade: \u2022 In science, U.S. eighth-graders exceeded the international average and outperformed their peers in 32 of the 44 other participating countries (table 9 and table C2 in appendix C). U.S. eighth-graders performed lower, on average, than their peers in seven countries and were not found to perform measurably different from students in five countries. \u2022 An examination of the performance of students from the other 12 OECD-member countries shows that U.S. eighth-grade students outperformed their peers in science in 5 of the countries (Belgium-Flemish, Italy, Norway, Scotland, and the Slovak Republic) and were outperformed by their peers in 3 of the countries (Hungary, Japan, and Korea; table 9).  (table 10 and table  C13 in appendix C). Fourth-graders in two other countries also showed no measurable change in science performance over the same time period.\nHas the relative science performance of U.S. fourth-and eighth-grade students changed since 1995? Fourth Grade: \u2022  (table  12 and table C13 in appendix C). In 1995, fourth-graders in one country, Japan, outperformed U.S. fourth-graders in science, while U.S. fourthgraders outperformed students in 13 countries. In 2003, U.S. fourth-graders were outperformed by students in two countries, on average, had average scores that were not measurably different from those of fourthgraders in four other countries, and outperformed students in eight countries.  (table 13  and table C15 in appendix C). In 1995, U.S. eighthgraders were outperformed in science by eighth-graders in nine of these countries, and outperformed eighth-graders in five of these countries. In 2003, U.S. eighth-graders were outperformed by students in 5 of these countries, and outperformed students in 11 of these countries. \nDid the performance of U.S. fourth-and eighth-graders in the science content areas change between 1995 and 2003? Fourth Grade: \u2022 Changes in average performance between 1995 and 2003 on the three science content areas measured in TIMSS in the fourth grade (Life Science; Physical Science; and Earth Science) could not be calculated due to a limited number of items in common between the two assessments. Eighth Grade: \u2022 Did the science performance of U.S. population groups change between 1995 and 2003? Fourth Grade: \u2022 The United States is one of four countries in which fourth-grade boys turned in a lower average science performance in 2003 than in 1995 (figure 3 and table C17 in appendix C). U.S. fourth-grade girls showed no measurable change in their average science performance. Fourthgrade girls in three countries showed a decline in their average science performance. \u2022 As a result of the lower performance of U.S. boys in science, the gap in the average science achievement of U.S. fourth-grade boys and girls narrowed between 1995 and 2003, from 12 points in 1995 to 5 points in 2003 (figure 3). 12 Nonetheless, on average, U.S. boys outperformed girls in science in 2003, which was the case in 1995 as well. 13 \u2022 As observed for mathematics, Black \u2022 As a result of significant changes in the average science scores of White and Black fourth-grade students, the average achievement gap between White and Black fourth-grade students narrowed from 110 score points in 1995 to 78 score points in 2003 (figure 3). Moreover, the gap in science achievement between Black and Hispanic fourthgraders also narrowed, from 41 score points in 1995 to 12 score points in 2003. There was no measurable difference in the score gap between White and Hispanic fourth-grade students over the same period of time. 14 \u2022 In 2003, U.S. fourth-graders in U.S. public schools with the highest poverty level (75 percent or more of students eligible for free or reduced-price lunch) had lower average science scores compared to their counterparts in public schools with lower levels (figure 3). Fourth-graders in public schools with the lowest poverty level (10 percent or less eligible students) had higher average science scores than students in schools with poverty levels of 25 percent or more. The difference in the average science scores of students in schools with the lowest and highest poverty levels was 99 score points in 2003. 15\n*p<.05, denotes a significant difference from 2003 average score. NOTE: Reporting standards not met for Asian category in 1995 and American Indian or Alaska Native and Native Hawaiian or Other Pacific Islander for both years. Racial categories exclude Hispanic origin. Other races/ethnicities are included in U.S. totals shown throughout the report. Analyses by poverty level are limited to students in public schools only. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between averages for one student group may be significant while a large difference for another student group may not be significant.  \n\u2022 In 2003, both U.S. eighth-grade boys and girls showed improvement in their average science performance compared to 1995 (figure 4 and table C19 in appendix C). 16 In 2003, U.S. eighthgrade boys scored 536 in science, on average. This was 16 score points higher than in 1995, when U.S. boys scored 520, on average. U.S. girls scored 519 in science, on average, in 2003. This was 14 score points higher than in 1995 and 1999, when U.S. girls scored 505, on average. \u2022 In 2003, U.S. eighth-grade boys outperformed girls in science, on average, which was also the case in 1995 and 1999 (figure 4). \u2022 \u2022 As a result of improvements in the average science achievement of Black and Hispanic eighthgraders, the achievement gap between White and Black eighth-graders narrowed from 122 score points in 1995 to 89 score points in 2003, and the achievement gap between White and Hispanic eighth-grade students narrowed from 98 points in 1995 to 70 points in 2003 (figure 4). 17 \u2022 In 2003, U.S. eighth-graders in U.S. public schools with the highest poverty level (75 percent or more of students eligible for free or reduced-price lunch) had lower average science scores compared to their counterparts in public schools with lower poverty levels (figure 4). In contrast, students in schools with the lowest poverty level (10 percent or less eligible students) had higher average science scores than students in schools with poverty levels of 25 percent or more eligible. The difference in the average science scores of students in schools with the lowest and highest poverty levels was 110 score points in 2003. 18 \u2022 With a single exception, U.S. eighth-graders who attended schools with varying percentages of students eligible to participate in the federal free or reduced-price lunch program showed no measurable change in their science achievement between 1999 and 2003, the 2 years for which data are available (figure 4 and table C20 in appendix C). U.S. eighth-graders who attended schools in which 50 to almost 75 percent of students were eligible for free or reduced-price lunch did, however, improve their science performance between 1999 and 2003. 16 See  for details on U.S. eighth-grade results for TIMSS 1999.\n*p<.05, denotes a significant difference from 2003 average score. NOTE: Reporting standards not met for Asian category in 1995 or 1999. Reporting standards not met for American Indian or Alaska Native and Native Hawaiian or Other Pacific Islander in 1995. Racial categories exclude Hispanic origin. Other races/ethnicities are included in U.S. totals shown throughout the report. Analyses by poverty level are limited to students in public schools only. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between averages for one student group may be significant while a large difference for another student group may not be significant. The United States met international guidelines for participation rates in 2003 only after replacement schools were included. See appendix A for more information.  \n"}, {"section_title": "15", "text": "\u2022 Fourth-graders in 9 of the 15 participating countries showed an increase in average science achievement scores between 1995 and 2003: Cyprus, England, Hong Kong SAR, Hungary, Iran, Latvia-LSS, New Zealand, Singapore, and Slovenia (table 10). Fourth-graders in three countries-Japan, Norway, and Scotland-experienced a decrease in average science achievement scores over the same period. Eighth Grade: \u2022 In 2003, U.S. eighth-graders improved in science compared to 1995 and 1999. U.S. eighth-graders scored 527, on average, in science in 2003, which was 12 score points higher than in 1999 and 14 score points higher than in 1995 (table  11 and table C14 in appendix C). The data indicate that the increase in average science performance in the United States occurred primarily between 1999 and 2003. \u2022 Eighth-graders in 11 other countries demonstrated a significant increase in their average science achievement between 1995 and 2003 or between 1999 and 2003: Australia, Hong Kong SAR, Israel, Jordan, Korea, Latvia-LSS, Lithuania, Malaysia, Moldova, the Philippines, and Slovenia (table 11). \u2022 Eighth-graders in 11 countries showed significant declines in their average science achievement between 1995 and 2003 or between 1999 and 2003 (table 11). The remaining 11 countries showed no measurable difference in the average mathematics scores of their students between 1995 and 2003, or between 1999 and 2003 (table 11).  1995, 1999, and 2003.\nThe effect size of the difference in science achievement between U.S. fourth-grade students in public schools with the lowest and highest levels of poverty in 2003 is 1.51 (table C21 in appendix C for standard deviations of U.S. student population groups)."}, {"section_title": "11", "text": "Although many of the participating countries collected data in all three years, analyses of changes in the science content areas at eighth grade are limited to 1999 and 2003 due to the limited number of in common items from year to year."}, {"section_title": "12", "text": "The effect size of the difference in science achievement between U.S. fourth-grade boys and girls in 2003 is .07 (table C21 in appendix C for standard deviations of U.S. student population groups)."}, {"section_title": "13", "text": "See NCES (1997) for details on U.S. fourth-grade results for TIMSS 1995. 14 The effect size of the differences between the average science scores of White and Black, and between White and Hispanic fourth-graders in the United States in 2003 are 1.15 and .94, respectively (table C21 in appendix C for standard deviations of U.S. student population groups)."}, {"section_title": "17", "text": "The effect size of the differences between the average science scores of White and Black, and between White and Hispanic eighth-graders in the United States in 2003 are 1.32 and .99, respectively (table C21 in appendix C for standard deviations of U.S. student population groups)."}, {"section_title": "18", "text": "The effect size of the difference in science achievement between U.S. eighth-grade students in public schools with the lowest and highest levels of poverty in 2003 is 1.67 (table C21 in appendix C for standard deviations of U.S. student population groups)."}, {"section_title": "Summary", "text": "Looking across the results in mathematics and science, the following points can be made. \u2022 In 2003, fourth-graders in 3 countries-Chinese Taipei, Japan, and Singapore-outperformed U.S. fourth-graders in both mathematics and science, while students in 13 countries turned in lower average mathematics and science scores than U.S. students (tables 2 and 8). U.S. fourth-grade students outperformed their peers in five OECD-member countries (Australia, Italy, New Zealand, Norway and Scotland) of which three are English-speaking countries (Australia, New Zealand and Scotland). \u2022 No measurable changes were detected in the average mathematics and science scores of U.S. fourth-graders between 1995 and 2003 (tables 4 and 10). Moreover, the available data suggest that the performance of U.S. fourth-graders in both mathematics and science was lower in 2003 than in 1995 relative to the 14 other countries that participated in both studies (tables 6 and 12). \u2022 On the other hand, fourth-grade students in six countries showed improvement in both average mathematics and science scores between 1995 and 2003: Cyprus, England, Hong Kong SAR, Latvia-LSS, New Zealand and Slovenia. At the same time, fourth-graders in Norway showed measurable declines in average mathematics and science achievement over the same time period (tables 4 and 10). \u2022 U.S. fourth-grade girls showed no measurable change in their average performance in mathematics and science between 1995 and 2003 (figures 1 and 3). U.S. fourth-grade boys also showed no measurable change in their average mathematics performance, but a measurable decline in science performance over the same time period. \u2022 U.S. Black fourth-graders improved in both mathematics and science between 1995 and 2003 (figures 1 and 3). Hispanic fourth-graders showed no measurable changes in either subject, while White fourth-graders showed no measurable change in mathematics, but declined in science. \u2022 As a result of changes in the performance of Black and White fourth-graders, the gap in achievement between White and Black fourthgrade students in the United States narrowed between 1995 and 2003 in both mathematics and science (figures 1 and 3). In addition, the gap in achievement between Black and Hispanic fourth-graders also narrowed in science over the same time period. \u2022 In 2003, U.S. fourth-graders in U.S. public schools with the highest poverty levels (75 percent or more of students eligible for free or reduced-price lunch) had lower average mathematics and science scores compared to their counterparts in public schools with lower poverty levels (figures 1 and 3). \u2022 Eighth-graders in the five Asian countries that outperformed U.S. eighth-graders in mathematics in 2003-Chinese Taipei, Hong Kong SAR, Japan, Korea, and Singapore-also outperformed U.S. eighth-graders in science in 2003, with eighthgraders in Estonia and Hungary performing better than U.S. students in mathematics and science as well (tables 3 and 9). Students in three of these Asian countries-Chinese Taipei, Japan, and Singapore-outperformed both U.S. fourth-and eighth-graders in mathematics and science on average (tables 2, 3, 8, and 9). \u2022 U.S. eighth-graders improved their average mathematics and science performances in 2003 compared to 1995 (tables 5 and 11). The growth in achievement occurred primarily between 1995 and 1999 in mathematics, and between 1999 and 2003 in science. Moreover, the available data suggest that the performance of U.S. eighthgraders in both mathematics and science was higher in 2003 than it was in 1995 relative to the 21 other countries that participated in the studies (tables 7 and 13). \u2022 In addition to students in the United States, eighthgraders in six other countries showed significant increases in both mathematics and science in 2003 compared to either 1999 or 1995: Hong Kong SAR, Israel, Korea, Latvia-LSS, Lithuania, and the Philippines (tables 5 and 11). On the other hand, eighth-graders in eight countries declined in their mathematics and science performance over this same time period."}, {"section_title": "SUMMARY", "text": "\u2022 U.S. eighth-grade boys and girls, and U.S. eighthgrade Blacks and Hispanics improved their mathematics and science performances from 1995 (figures 2 and 4). As a result, the gap in achievement between White and Black eighth-graders narrowed in both mathematics and science over this time period. \u2022 In 2003, U.S. eighth-graders in U.S. public schools with the highest poverty levels (75 percent or more of students eligible for free or reduced-price lunch) had lower average mathematics and science scores compared to their counterparts in public schools with lower poverty levels (figures 2 and 4).\n"}, {"section_title": "Data Collection", "text": "The TIMSS 2003 data were collected by each country, following international guidelines and specifications. TIMSS required that countries select random, nationally representative samples of schools and students. TIMSS countries were asked to identify eligible students based on a common set of criteria, allowing for adaptation to country-specific situations. In IEA studies such as TIMSS, the target population for all countries is called the international desired population. For the fourth-grade assessment, the international desired population consisted of all students in the country who were enrolled in the upper of the two adjacent grades that contained the greatest proportion of 9-year-olds at the time of testing. In the United States and most other countries, this corresponded to fourth grade. For the eighth-grade assessment, the international desired population consisted of all students in the country who were enrolled in the upper of the two adjacent grades that contained the greatest proportion of 13-year-olds at the time of testing. In the United States and most other countries, this corresponded to eighth grade. TIMSS used a two-stage stratified cluster sampling design. The first stage made use of a systematic probability-proportionate-to-size (PPS) technique to select schools. Although countries participating in TIMSS were strongly encouraged to secure the participation of schools selected in the first stage, it was anticipated that a 100 percent participation rate for schools would not be possible in all countries. Therefore, two replacement schools were identified for each originally sampled school, a priori. As each school was selected, the next school in the sampling frame was designated as a replacement school should the originally sampled school choose not to participate in the study. Should the originally sampled school and the replacement school choose not to participate, a second replacement school was chosen by going to the next school in the sampling frame. The second stage consisted of selecting classrooms within sampled schools. At the classroom level, TIMSS sampled intact mathematics classes that were offered to students in the target grades. In most countries, one mathematics classroom per school was sampled, although some countries, such as the United States, chose to sample two mathematics classrooms per school."}, {"section_title": "Exclusions in the TIMSS Sample", "text": "All countries were required to define their national desired population to correspond as closely as possible to the definition of the international desired population. In some cases, countries needed to exclude schools and students in remote geographical locations or to exclude a segment of the education system. Any exclusions from the international desired population were clearly documented. Countries were expected to keep the excluded population to no more than 10 percent of the national desired population. Exclusions could take place at the school level, within schools, or both. Participants could exclude schools from the sampling frame for the following reasons: \u2022 Locations were geographically remote; \u2022 Size was extremely small; \u2022 Curriculum or school structure was different from the mainstream education system; or \u2022 Instruction provided was only to students in the categories defined as \"within-school exclusions. \" Within schools, exclusion decisions were limited to students who, because of some disability, were unable to take part in the TIMSS assessment. The general TIMSS rules for defining within-school exclusion included the following three groups: \u2022 Intellectually disabled students. These students were considered, in the professional opinion of the school principal or other qualified staff members, to be intellectually disabled, or had been so diagnosed in psychological tests. This category included students who were emotionally or mentally unable to follow even the general instructions of the TIMSS test. It did not include students who merely exhibited poor academic performance or discipline problems. \u2022 Functionally disabled students. These students were permanently physically disabled in such a way that they could not participate in the TIMSS assessment. Functionally disabled students who could perform were included in the testing. \u2022 Non-native-language speakers. These students could not read or speak the language of the assessment and so could not overcome the language barrier of testing. Typically, a student who had received less than 1 year of instruction in the language of the assessment was excluded, but this definition was adapted in different countries. School-level and within-school exclusion rates for TIMSS 2003 are detailed in the next section. Exclusion rates for TIMSS 1995 can be found in chapter 2 of Martin and Kelly (1997); exclusion rates for TIMSS 1999 can be found in appendix 2 of ."}, {"section_title": "Response Rates", "text": "Based on the sample of schools and students that participated in the assessment, countries were assigned to one of four following categories: Category 1: met requirements \u2022 An unweighted or weighted school response rate without replacement of at least 85 percent and an unweighted or weighted student response rate of at least 85 percent or \u2022 The product of the weighted school response rate without replacement and the weighted student response rate of at least 75 percent. Category 2: met requirements after replacements \u2022 If the requirements for category 1 are not met but the country had either an unweighted or weighted school response rate without replacement of at least 50 percent and had either \u2022 An unweighted or weighted school response rate with replacement of at least 85 percent and a weighted student response rate of at least 85 percent or \u2022 The product of the weighted school response rate with replacement and the weighted student response rate of at least 75 percent. Category 3: close to meeting requirements after replacements \u2022 If the requirements for category 1 or 2 are not met but the country had either an unweighted or weighted school response rate without replacement of at least 50 percent and \u2022 The product of the weighted school response rate with replacement and the weighted student response rate near 75 percent. Category 4: failed to meet requirements     This change was made in an effort to reduce the design effects and to spread the respondent burden across schools districts as much as possible. The sample design for TIMSS was a stratified systematic sample, with sampling probabilities proportional to measures of size. The U.S. TIMSS fourthgrade sample had two explicit strata based on poverty. A high poverty school was defined as one in which 50 percent or more of the students were eligible for participation in the federal free or reducedprice lunch program; high poverty schools were oversampled (Ferraro and Rust 2003) This variable was not available for private schools, so they were all treated as low poverty schools. The target sample sizes were 120 high-poverty and 190 low-poverty schools. Within the poverty strata, there are four categorical implicit stratification variables: type of school (public or private), region of the country 19 (Northeast, Southeast, Central, West), type of location relative to populous areas (eight levels), minority status (above or below 15 percent). The last sort key within the implicit stratification was by grade enrollment in descending order. The TIMSS eighth-grade sample had no explicit stratification. The frame was implicitly stratified (i.e., sorted for sampling) by four categorical stratification variables: type of school (public or private), region of the country, type of location relative to populous areas (eight levels), minority status (above or below 15 percent). The last sort key within the implicit stratification was by grade enrollment in descending order. At the same time that the TIMSS sample was selected, replacement schools were identified following the TIMSS guidelines by assigning the two schools neighboring the sampled school on the frame as replacements. There were several constraints on the assignment of substitutes. One sampled school was not allowed to substitute for another, and a given school could not be assigned to substitute for more than one sampled school. Furthermore, substitutes were required to be in the same implicit stratum as the sampled school. If the sampled school was the first or last school in the stratum, then the second school following or preceding the sampled school was identified as the substitute. One was designated a first replacement and the other a second replacement. If an original school refused to participate, the first replacement was then contacted. If that school also refused to participate, the second school was then contacted. The schools were selected with probability proportionate to the school's estimated enrollment of fourth-and eighth-grade students from the 2003 NAEP school frame with 2000-01 school data. The data for public schools were from the Common Core of Data (CCD), and the data for private schools was from the Private School Survey (PSS). Any school containing a fourth or an eighth grade as of the school year 2000-01 was included on the school sampling frame. Participating schools provided lists of fourth-or eighth-grade classrooms, and one or two intact mathematics classrooms were selected within each school in an equal probability sample. The overall sample design for the United States was intended to approximate a self-weighting sample of students as much as possible, with each fourth-or eighth-grade student having an equal probability of being selected. The U.S. TIMSS fourth-grade school sample consisted of 310 schools, of which 300 were eligible schools and 212 agreed to participate. The school response rate before replacement was 70 percent (weighted; 71 percent unweighted). The weighted school response rate before replacement is given by the formula: where Y denotes the set of responding original sample schools with age-eligible students, N denotes the set of eligible non-responding original sample schools, W i denotes the base weight for school i, W i = 1/P i , where P i denotes the school selection probability for school i, and E i denotes the enrollment size of ageeligible students, as indicated on the sampling frame. In addition to the 212 participating schools, 36 replacement schools also participated for a total of 248 participating schools at the fourth grade in the United States. A total of 10,795 students were sampled for the fourthgrade assessment. Of these students, 49 were withdrawn from the school before the assessment was administrated. Of the eligible 10,746 sampled students, an additional 429 students were excluded using the criteria described above, for a weighted exclusion rate of 5 percent. Of the 10,317 remaining sample students, a total of 9,829 students participated in the assessment in the United States, since 488 students were absent. The student participation rate was 95 percent. The combined school and students weighted and unweighted response rate of 78 percent after replacement schools were included was achieved (66 percent weighted and 67 percent unweighted weighted school response rate before replacement"}, {"section_title": "19", "text": "Region is the 'state-based' region (NAEPRG_S on the output files). Northeast consists of Connecticut, Delaware, District of Columbia, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Central consists of Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. West consists of Alaska, Arizona, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oklahoma, Texas, Utah, Washington, Oregon, California, and Wyoming. Southeast consists of Alabama, Florida, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, and West Virginia. without replacement). As a result, the U.S. data for fourth-grade students are annotated to indicate that international guidelines for participation rates were met only after replacement schools were included. The U.S. TIMSS eighth-grade school sample consisted of 301 schools, of which 296 were eligible schools and 211 agreed to participate. The school response rate before replacement was 71 percent (weighted and unweighted). In addition to the 211 participating schools, 21 replacement schools also participated for a total of 232 participating schools at the eighth grade in the United States. A total of 9,891 students were sampled for the assessment. Of these students, 90 were withdrawn from the school before the assessment was administrated. Of the eligible 9,801 sampled students, an additional 279 students were excluded using the criteria described above, for a weighted exclusion rate of 5 percent. Of the 9,522 remaining sample students, a total of 8,912 students participated in the assessment in the United States, since 610 students were absent. The student participation rate was 94 percent (weighted and unweighted). The combined school and students weighted and unweighted response rate of 73 percent after replacement schools were included was achieved (66 percent without replacement schools). As a result, the U.S. data for eighth-grade students are in parentheses to indicate that United States did not meet international sampling guidelines. NCES standards require a nonresponse bias analysis if the school level response rate is below 80 percent (using the base weight). Since the U.S. school response rates at the fourth and eighth grades were below 80 percent, even with replacements, NCES required an analysis of the potential magnitude of nonresponse bias at the school level. To accomplish this analysis, two methods were chosen (Van de Kerckhove and Ferraro forthcoming). The first method was focused exclusively on the original sample of schools, treating all those that were substituted as nonrespondents. A second method focused on the final sample of schools (including replacements), treating as nonrespondents those schools from which a final response was not received. Both methods were used to analyze the U.S. TIMSS fourth-and eighth-grade data for potential bias. In order to compare TIMSS respondents and nonrespondents it was necessary to match the sample of schools back to the sample frame to detect as many characteristics as possible that might provide information about the presence of nonresponse bias. Comparing characteristics for respondents and nonrespondents is not always a good measure of nonresponse bias if the characteristics are unrelated or weakly related to more substantive items in the survey. However, this is often the only approach available. The characteristics that were analyzed based on the sampling frame were taken from the 2000-2001 Common Core of Data (CCD) for public schools, and from the 2000-2001 Private School Survey (PSS) for private schools. For categorical variables, the distribution of the characteristics for respondents was compared with the distribution for all schools. The hypothesis of independence between a given school characteristic and the response status (whether or not participated) was tested using a Rao-Scott modified Chi-square statistic. For continuous variables, summary means were calculated. The 95 percent confidence interval for the difference between the mean for respondents and the mean for all schools was tested to see whether or not it included zero. In addition to these tests, logistic regression models were set up to identify whether any of the school characteristics were significant in predicting response status because logistic regression allows investigation of all variables at the same time. Public and private schools were modeled together using the following variables: community type; public/religious affiliation; NAEP region; poverty level; number of students enrolled in fourth or eighth grade; total number of students; percentage Asian or Pacific Islander students; percentage Black, non-Hispanic students; percentage Hispanic students; percentage American Indian or Alaska Native students; and percentage White, non-Hispanic students. The investigation into nonresponse bias at the school level for TIMSS fourth grade generally showed that there was no statistically significant relationship between response status and the majority of school characteristics available for analysis. For the original sample of schools in TIMSS fourth grade, schools in the Northeast were less likely to respond than schools in the West, Southeast or Central regions of the coun-try. However, the regression did not confirm this result. The results for the final sample of schools showed a significant effect on the percentage of Black, non-Hispanic students (responding schools had more Black, non-Hispanic students than non-responding schools). However, the regression did not confirm this result. The investigation into nonresponse bias at the school level for TIMSS eighth grade showed that, for the original sample of schools, responding schools were more likely to be in rural areas than in central city or urban fringe areas, have fewer students than non-responding schools, have fewer Hispanic students, and were more likely to be Catholic or public schools. However, the regression confirmed only that responding schools in the original sample were more likely to be from rural areas and have fewer students than nonresponding schools. The number of Hispanic students in responding schools and their public/religious affiliation were not confirmed by the regression. The results with the final sample of schools were more complicated. The total number of students remained significant, but the additional variable of public/religious affiliation also appeared to be significantly related to response rate according to the logistic regression. Public and Catholic schools were more likely to respond than private, non-sectarian and private-other religious schools. Finally, while the first analysis indicated that schools in rural areas were more like to respond than schools in the central city or urban fringe, this was not confirmed by the logistic regression. The results of these analyses suggest that there is no statistically significant relationship between response status and the majority of the school characteristics tested, with the exception of the variables noted above at each grade level. The potential for nonresponse bias exists however. It is difficult to assess the amount of any bias in the survey as a result of the associations that exist. It is also not clear what effect the weighting adjustments for nonresponse have on any bias. In general, these weighting adjustments cannot address all of the potential bias, only some of it. There is no evaluation of how much effect the weighting adjustments have on the bias."}, {"section_title": "Test Development", "text": "TIMSS is a cooperative effort involving representatives from every country participating in the study. For TIMSS 2003, the development effort began with a revision of the frameworks that are used to guide the construction of the assessment (Mullis et al. 2001). The framework was updated to reflect changes in the curriculum and instruction of participating countries. Extensive input from experts in mathematics and science education, assessment, curriculum, and representatives from national educational centers around the world contributed to the final shape of the frameworks. Maintaining the ability to measure change over time was an important factor in revising the frameworks. As part of the TIMSS dissemination strategy, approximately one-third of the 1995 fourth-grade assessment items and one-half of the 1999 eighth-grade assessment items were released for public use. To replace assessment items that had been released in earlier years, countries submitted items for review by subject-matter specialists, and additional items were written to ensure that the content, as explicated in the frameworks, was covered adequately. Items were reviewed by an international Science and Mathematics Item Review Committee and pilot-tested in most of the participating countries. Results from the field test were used to evaluate item difficulty, how well items discriminated between high-and lowperforming students, the effectiveness of distracters in multiple-choice items, scoring suitability and reliability for constructed-response items, and evidence of bias towards or against individual countries or in favor of boys or girls. As a result of this review, 243 of the 435 new fourth-grade items were selected for inclusion in the assessment. In total, there were 313 mathematics and science items included in the fourth-grade TIMSS assessment booklets. At eighth grade, the review of the item statistics from the field test led to the inclusion of 230 of the 386 new eighth-grade items in the assessment. In total, there were 383 mathematics and science items included in the eighth-grade TIMSS assessment booklets. More detail on the distribution of new and trend items is included in table A3. The TIMSS frameworks included specifications for what are termed \"problem-solving and inquiry\" (PSI) tasks. PSI tasks were developed to assess how well students could draw on and integrate information and processes in mathematics and science as part of an investigation or in order to solve problems. The PSI tasks developed for TIMSS 2003 needed to be self-contained, involve minimal equipment, and be integrated into the main assessment without any special accommodations or additional testing time. While the PSI tasks are not full scientific investigations, the tasks were designed to require a basic understanding of the nature of science and mathematics, and to elicit some of the skills essential to the inquiry process. The tasks were designed to draw on students' understandings of and abilities with formulating questions and hypotheses; designing investigations; collecting, representing, analyzing, and interpreting data; and drawing conclusions and developing explanations based on evidence. The PSI tasks were assembled as longer blocks or clusters of items that, together, related to an overall theme (e.g., speciation). Nine PSI blocks were fieldtested at fourth grade. Of the nine blocks, six blocks were eventually incorporated into the fourth-grade assessment. The six blocks covered both mathematics and science, focusing on geometry, measurement, number, life science, earth science, and physical science. At eighth grade, 10 PSI blocks were field-tested. Of the 10 blocks, 7 blocks were eventually incorporated into the eighth-grade assessment. The seven blocks covered both mathematics and science, focusing on algebra, data, geometry, measurement, number, chemistry, physics, and life science. The PSI tasks were incorporated into the overall assessments and, thus, not reported separately at either grade level. "}, {"section_title": "Design of Instruments", "text": "TIMSS 2003 included booklets containing assessment items as well as questionnaires submitted to principals, teachers, and students for response. The assessment booklets were constructed such that not all of the students responded to all of the items. This is consistent with other large-scale assessments, such as the National Assessment of Educational Progress. To keep the testing burden to a minimum, and to ensure broad subject-matter coverage, TIMSS used a rotated block design that included both mathematics and science items. That is, students encountered both mathematics and science items during the assessment. The 2003 fourth-grade assessment consisted of 12 booklets, each requiring approximately 72 minutes of response time. The 12 booklets were rotated among students, with each participating student completing 1 booklet only. The mathematics and science items were assembled into 14 blocks or clusters of items. Each block contained either mathematics items or science items only. The secure or trend items were included in 3 blocks, with the other 11 blocks containing replacement items. Each of the 12 booklets contained 6 blocks (in total). The 2003 eighth-grade assessment also consisted of 12 booklets, each requiring approximately 90 minutes of response time. The 12 booklets were rotated among students, with each participating student completing 1 booklet only. The mathematics and science items were assembled into 14 blocks or clusters of items. Each block contained either mathematics items or science items only. The secure or trend items were included in 3 blocks, with the other 11 blocks containing replacement items. Each of the 12 booklets contained 6 blocks (in total). As part of the design process, it was necessary to ensure that the booklets showed a distribution across the mathematics and science content domains as specified in the frameworks. The number of mathematics and science items in the fourth and eighthgrade TIMSS 2003 assessments is shown in table A4. In addition to the assessment booklets, TIMSS 2003 included questionnaires for principals, teachers, and students. As with prior iterations of TIMSS, the questionnaires used in TIMSS 2003 are based on prior versions of the questionnaires. The questionnaires were reviewed extensively by the national research coordinators from the participating countries as well as a Questionnaire Item Review Committee. Like the assessment booklets, all questionnaire items were field tested, and the results reviewed carefully. As a result, some of the questionnaire items needed to be revised prior to their inclusion in the final questionnaires. The questionnaires requested information to help provide a context for the performance scores, focusing on such topics as students' attitudes and beliefs about learning, student habits and homework, and their lives both in and outside of school; teachers' attitudes and beliefs about teaching and learning, teaching assignments, class size and organization, instructional practices, and participation in professional development activities; and principals' viewpoints on policy and budget responsibilities, curriculum and instruction issues, student behavior, as well as descriptions of the organization of schools and courses."}, {"section_title": "Calculator Usage", "text": "Calculators were not permitted during the TIMSS fourth-grade assessment. However, the TIMSS policy on calculator use at the eighth grade was to give students the best opportunity to operate in settings that mirrored their classroom experiences. Beginning with 2003, calculators were permitted but not required for newly developed eighth-grade assessment materials. Participating countries could decide whether or not their students were allowed to use calculators for the new items; the United States allowed students to use calculators. Since calculators were not permitted at the eighth grade in the 1995 or 1999 assessments, the 2003 eighth-grade test booklets were designed so that trend items from these assessments were placed in the first half and new items in 2003 placed in the second half. Where countries chose to permit eighth-grade students to use calculators, they could use them for the second half of the booklet only."}, {"section_title": "Translation", "text": "Source versions of all instruments (assessment booklets, questionnaires and manuals) were prepared in English and translated into the primary language or languages of instruction in each country. In addition, it was sometimes necessary to adapt the instrument for cultural purposes, even in countries that use English as the primary language of instruction. All adaptations were reviewed and approved by the International Study Center to ensure they did not change the substance or intent of the question or answer choices. For example, proper names were sometimes changed to names that would be more familiar to students (e.g., Marja-leena to Maria). Each country prepared translations of the instruments according to translation guidelines established by the International Study Center. Adaptations to the instruments were documented by each country, and submitted for review. The goal of the translation guidelines was to produce translated instruments of the highest quality that would provide comparable data across countries. Translated instruments were verified by an independent, professional translation agency prior to final approval and printing of the instruments. Countries were required to submit copies of the final printed instruments to the International Study Center. Further details on the translation process can be found in the TIMSS 2003 Technical Report ."}, {"section_title": "Test Administration and Quality Assurance", "text": "TIMSS 2003 emphasized the use of standardized procedures in all countries. Each country collected its own data, based on comprehensive manuals and trainings provided by the international project team to explain the survey's implementation, including precise instructions for the work of school coordinators and scripts for test administrators for use in testing sessions. Test administration in the United States was carried out by professional staff trained according to the international guidelines. School staff were asked only to assist with listings of students, identifying space for testing in the school, and specifying any parental consent procedures needed for sampled students. Each country was responsible for conducting quality control procedures and describing this effort in the national research coordinators' report documenting procedures used in the study. In addition, the International Study Center considered it essential to monitor compliance with the standardized procedures. National research coordinators were asked to nominate one or more persons unconnected with their national center, such as retired school teachers, to serve as quality control monitors for their countries. The International Study Center developed manuals for the monitors and briefed them in 2-day training sessions about TIMSS, the responsibilities of the national centers in conducting the study, and their own roles and responsibilities."}, {"section_title": "Scoring Reliability", "text": "The TIMSS assessment items included both multiple choice and constructed-response items. A scoring rubric (guide) was created for every item included in the TIMSS assessments. These were carefully written and reviewed by national research coordinators and other experts as part of the field test of items, and revised accordingly. The national research coordinator in each country was responsible for scoring and coding of data in that country, following established guidelines. The national research coordinator and, sometimes, additional staff, attended scoring training sessions held by the International Study Center. The training sessions focused on the scoring rubrics and coding system employed in TIMSS. Participants were provided extensive practice in scoring example items over several days. Information on within-country agreement among coders was collected and documented by the International Study Center. Information on scoring and coding reliability was also used to calculate crosscountry agreement among coders. Scoring reliability for TIMSS 2003 is provided in table A5.   Hong Kong is a Special Administrative Region (SAR) of the People's Republic of China. NOTE: To gather and document within-country agreement among scorers, systematic subsamples of at least 100 students' responses to each constructed-response item was coded independently by two readers. The agreement score indicates the degree of agreement among coders on marking student responses in the same way. See  and   "}, {"section_title": "Weighting", "text": "Responses from the groups of students were assigned sampling weights to adjust for over-representation or under-representation from a particular group. The use of sampling weights is necessary for the computation of statistically sound, nationally representative estimators. The weight assigned to a student's responses is the inverse of the probability that the student would be selected for the sample. When responses are weighted, none are discarded, and each contributes to the results for the total number of students represented by the individual student assessed. Weighting also adjusts for various situations such as school and student nonresponse because data cannot be assumed to be randomly missing. The internationally defined weighting specifications for TIMSS require that each assessed student's sampling weight should be the product of (1) the inverse of the school's probability of selection, (2) an adjustment for school-level nonresponse, (3) the inverse of the classroom's probability of selection, and (4) an adjustment for student-level nonresponse. All TIMSS 1995 analyses are conducted using sampling weights. TIMSS 1995TIMSS , 1999TIMSS , and 2003 used item response theory (IRT) methods to produce score scales that summarized the achievement results. With this method, the performance of a sample of students in a subject area or sub-area could be summarized on a single scale or a series of scales, even when different students had been administered different items. Because of the reporting requirements for TIMSS and because of the large number of background variables associated with the assessment, a large number of analyses had to be conducted. The procedures TIMSS used for the analyses were developed to produce accurate results for groups of students while limiting the testing burden on individual students. Furthermore, these procedures provided data that could be readily used in secondary analyses. IRT scaling provides estimates of item parameters (e.g., difficulty, discrimination) that define the relationship between the item and the underlying variable measured by the test. Parameters of the IRT model are estimated for each test question, with an overall scale being established as well as scales for each prede-"}, {"section_title": "Scaling", "text": ""}, {"section_title": "Data Entry and Cleaning", "text": "Responsibility for data entry was taken by the national research coordinator from each country. The data collected for TIMSS 2003 were entered into data files with a common international format, as specified in the Manual for Entering the TIMSS 2003 Data. Data entry was facilitated by the use of a common software available to all participating countries (WinDEM). The software facilitated the checking and correction of data by providing various data consistency checks. The data were then sent to the IEA Data Processing Center (DPC) in Hamburg, Germany for cleaning. The DPC checked that the international data structure was followed; checked the identification system within and between files; corrected single case problems manually; and applied standard cleaning procedures to questionnaire files. Results of the data cleaning process were documented by the DPC. This documentation was then shared with the national research coordinator with specific questions to be addressed. The national research coordinator then provided the DPC with revisions to coding or solutions for anomalies. The DPC then compiled background univariate statistics and preliminary classical and Rasch Item Analysis. Detailed information on the entire data entry and cleaning process can be found in the TIMSS 2003 Technical Report ."}, {"section_title": "Weighting, Scaling, and Plausible Values", "text": "Before the data were analyzed, responses from the groups of students assessed were assigned sampling weights to ensure that their representation in TIMSS 2003 results matched their actual percentage of the school population in the grade assessed. Based on these sampling weights, the analyses of TIMSS 2003 data were conducted in two major phases-scaling and estimation. During the scaling phase, item response theory (IRT) procedures were used to estimate the measurement characteristics of each assessment question. During the estimation phase, the results of the scaling were used to produce estimates of student achievement. Subsequent analyses related these achievement results to the background variables collected by TIMSS 2003. fined content area specified in the assessment framework. For example, the TIMSS 2003 eighth-grade assessment had five scales describing mathematics content strands, and science had scales for five fields of science. TIMSS 1995 utilized a one parameter IRT model to produce score scales that summarized the achievement results. The TIMSS 1995 data were rescaled using a three-parameter IRT model to match the procedures used to scale the 1999 and 2003 TIMSS data. The three-parameter model was preferred to the one-parameter model because it can more accurately account for the differences among items in their ability to discriminate between students of high and low ability. After careful study of the rescaling process, the International Study Center concluded that the fit between the original TIMSS data and the rescaled TIMSS data met acceptable standards. However, as a result of rescaling, the average achievement scores of some countries changed from those initially reported in 1996 and 1997 NCES 1997). The rescaled TIMSS scores are included in this report."}, {"section_title": "Plausible Values", "text": "During the scaling phase, plausible values were used to characterize scale scores for students participating in the assessment. To keep student burden to a minimum, TIMSS administered a limited number of assessment items to each student-too few to produce accurate content-related scale scores for each student. To account for this, for each student, TIMSS generated five possible content-related scale scores that represented selections from the distribution of content-related scale scores of students with similar backgrounds who answered the assessment items the same way. The plausible-values technology is one way to ensure that the estimates of the average performance of student populations and the estimates of variability in those estimates are more accurate than those determined through traditional procedures, which estimate a single score for each student. During the construction of plausible values, careful quality control steps ensured that the subpopulation estimates based on these plausible values were accurate. Plausible values were constructed separately for each national sample. TIMSS uses the plausible-val-ues methodology to represent what the true performance of an individual might have been, had it been observed. This is done by using a small number of random draws from an empirically derived distribution of score values based on the student's observed responses to assessment items and on background variables. Each random draw from the distribution is considered a representative value from the distribution of potential scale scores for all students in the sample who have similar characteristics and identical patterns of item responses. The draws from the distribution are different from one another to quantify the degree of precision (the width of the spread) in the underlying distribution of possible scale scores that could have caused the observed performances. The TIMSS plausible values function like point estimates of scale scores for many purposes, but they are unlike true point estimates in several respects. They differ from one another for any particular student, and the amount of difference quantifies the spread in the underlying distribution of possible scale scores for that student. Because of the plausible-values approach, secondary researchers can use the TIMSS data to carry out a wide range of analyses."}, {"section_title": "Data Limitations", "text": "As with any study, there are limitations to TIMSS 2003 that researchers should take into consideration. Estimates produced using data from TIMSS 2003 are subject to two types of error, nonsampling and sampling errors. Nonsampling errors can be due to errors made in collecting and processing data. Sampling errors can occur because the data were collected from a sample rather than a complete census of the populations."}, {"section_title": "Nonsampling Errors", "text": "Nonsampling error is a term used to describe variations in the estimates that may be caused by population coverage limitations, nonresponse bias, and measurement error, as well as data collection, processing, and reporting procedures. The sources of nonsampling errors are typically problems like unit and item nonresponse, the difference in respondents' interpretations of the meaning of the questions, response differences related to the particular time the survey was conducted, and mistakes in data preparation."}, {"section_title": "Missing Data", "text": "There are four kinds of missing data: nonresponse, missing or invalid, not applicable, and not reached. Nonresponse data occurs when a respondent was expected to answer an item but no response was given. Responses that are missing or invalid occur in multiple-choice items where an invalid response is given. The code is not used for opened-ended questions. An item is not applicable when it is not possible for the respondent to answer the question. Finally, items that are not reached are consecutive missing values starting from the end of each test session. All four kinds of missing data are coded differently in the TIMSS 2003 database. Missing background data are not included in the analyses for this report and are not imputed. In general, item response rates for variables discussed in this report were over the NCES standard of 85 percent to report without notation (table A6). In general, it is difficult to identify and estimate either the amount of nonsampling error or the bias caused by this error. In TIMSS 2003, efforts were made to prevent such errors from occurring and to compensate for them when possible. For example, the design phase entailed a field test that evaluated items as well as the implementation procedures for the survey. It should also be recognized that most background information was obtained from students' self-reports, which are subject to respondent bias. One potential source of respondent bias in this survey was social desirability bias, for example, if students reported that they enjoyed mathematics."}, {"section_title": "Sampling Errors", "text": "Sampling errors occur when the discrepancy between a population characteristic and the sample estimate arises because not all members of the reference population are sampled for the survey. The size of the sample relative to the population and the variability of the population characteristics both influence the magnitude of sampling error. The particular sample of students in fourth and eighth grade from the 2002-03 school year was just one of many possible samples that could have been selected. Therefore, estimates produced from the TIMSS sample may differ from estimates that would have been produced had another student sample been drawn. This type of variability is called sampling error because it arises from using a sample of students in fourth or eighth grade, rather than all students in the grade in that year. The standard error is a measure of the variability due to sampling when estimating a statistic. The approach used for calculating sampling variances in TIMSS was the Jackknife Repeated Replication (JRR). Standard errors can be used as a measure for the precision expected from a particular sample. Standard errors for all of the estimates are included in appendix C. The standard errors can be used to produce confidence intervals. There is a 95 percent chance that the true average lies within the range of 1.96 times the standard errors above or below the estimated score. For example, the average mathematics score for the U.S. eighth-grade students was 504 in 2003, and this statistic had a standard error of 3.3. Therefore, it can be stated with 95 percent confidence that the actual average of U.S. eighth-grade students in 2003 was between 498 and 511 (1.96 x 3.3 = 6.5; confidence interval = 504 +/-6.5)."}, {"section_title": "Description of Background Variables", "text": "The international version of the TIMSS 2003 student, teacher and school questionnaires are available at http://timss.bc.edu. The U.S. versions of these questionnaires are available at http://nces.ed.gov/timss."}, {"section_title": "Race/Ethnicity", "text": "Students' race/ethnicity was obtained through student responses to a two-part question. Students were asked first whether they were Hispanic or Latino, and then asked whether they were members of the following racial groups: American Indian or Alaska Native, Asian, Black or African American, Native Hawaiian or other Pacific Islander, or White. Multiple responses to the race classification question were allowed. Results are shown separately for Asians, Blacks, Hispanics, and Whites. Students identifying themselves as Hispanic and also other races were included in the Hispanic group."}, {"section_title": "Poverty Level in Public Schools (Percentage of Students Eligible for Free or Reduced-price Lunch)", "text": "The poverty level in public schools was obtained from principal responses to the school questionnaire. The question asked what percentage of students at the school was eligible to receive free or reducedprice lunch through the National School Lunch Program around the first of October, 2002. The answers were grouped into five categories: less than 10 percent; 10 to 24.9 percent; 25 to 49.9 percent; 50 to 74.9 percent; and 75 percent or more. Analysis was limited to public schools only."}, {"section_title": "Confidentiality and Disclosure Limitations", "text": "The TIMSS 2003 data are hierarchical and include school data and student data from the participating schools. Confidentiality analyses for the United States were designed to provide reasonable assurance that public use data files issued by the IEA would not allow identification of individual U.S. schools or students when compared against public data collections. Disclosure limitation included the identification and masking of potential disclosure-risk TIMSS schools and adding an additional measure of uncertainty of school, teacher, and student identification through random swapping of data elements within the student, teacher, and school files."}, {"section_title": "Statistical Procedures", "text": ""}, {"section_title": "Tests of Significance", "text": "Comparisons made in the text of this report have been tested for statistical significance. For example, in the commonly made comparison of country averages against the average of the United States, tests of statistical significance were used to establish whether or not the observed differences from the U.S. average were statistically significant. The estimation of the standard errors that are required in order to undertake the tests of significance is complicated by the complex sample and assessment designs which both generate error variance. Together they mandate a set of statistically complex procedures in order to estimate the correct standard errors. As a consequence, the estimated standard errors contain a sampling variance component estimated by Jackknife Repeated Replication (JRR); and, where the assessments are concerned, an additional imputation variance component arising from the assessment design. Details on the procedures used can be found in the WesVar 4.0 User's Guide (Westat 2000). In almost all instances, the tests for significance used were standard t tests. These fell into two categories according to the nature of the comparison being made: comparisons of independent and non-independent samples. Before describing the t tests used, some background on the two types of comparisons is provided below: The variance of a difference is equal to the sum of the variances of the two initial variables minus two times the covariance between the two initial variables. A sampling distribution has the same characteristics as any distribution, except that units consist of sample estimates and not observations. Therefore, The sampling variance of a difference is equal to the sum of the two initial sampling variances minus two times the covariance between the two sampling distributions on the estimates. If one wants to determine whether the girls' performance differs from the boys' performance, for example, then as for all statistical analyses, a null hypothesis has to be tested. In this particular example, it consists of computing the difference between the boys' performance mean and the girls' performance mean (or the inverse). The null hypothesis is: To test this null hypothesis, the standard error on this difference is computed and then compared to the observed difference. The respective standard errors on the mean estimate for boys and girls ((\u00b5\u02c6b oys ) , (\u00b5\u02c6g irls )) can be easily computed. The expected value of the covariance will be equal to 0 if the two sampled groups are independent. If the two groups are not independent, as is the case with girls and boys attending the same schools within a country, or comparing a country mean with the international mean which includes that particular country, then the expected value of the covariance might differ from 0. In TIMSS, country samples are independent. Therefore, for any comparison between two countries, the expected value of the covariance will be equal to 0, and thus the standard error on the estimate is: with being any statistic. Within a particular country, any sub-samples will be considered as independent only if the categorical variable used to define the sub-samples was used as an explicit stratification variable. If sampled groups are not independent, the estimation of the covariance between, for instance, \u03bc (boys) and \u03bc (girls) would require the selection of several samples and then the analysis of the variation of \u03bc (boys) in conjunction with \u03bc (girls) . Such a procedure is of course unrealistic. Therefore, as for any computation of a standard error in TIMSS, replication methods using the supplied replicate weights are used to estimate the standard error on a difference. Use of the replicate weights implicitly incorporates the covariance between the two estimates into the estimate of the standard error on the difference. Thus, in simple comparisons of independent averages such as the U.S. average with other country averages, the following formula was used to compute the t statistic: Est 1 and est 2 are the estimates being compared (e.g., average of country A and the U.S. average) and se 1 and se 2 are the corresponding standard errors of these averages. The second type of comparison used in this report occurred when comparing differences of non-subset, non-independent groups, such as when comparing the average scores of males versus females within the United States. In such comparisons, the following formula was used to compute the t statistic: Est grp1 and est grp2 are the non-independent group estimates being compared. Se(est grp1 -est grp2 ) is the standard error of the difference calculated using Jackknife Repeated Replication (JRR), which accounts for any covariance between the estimates for the two nonindependent groups. "}, {"section_title": "52", "text": ""}, {"section_title": "Effect size", "text": "Tests of statistical significance are, in part, influenced by sample sizes. To provide the reader with an increased understanding of the importance of the significant difference between student populations in the United States, effect sizes are included in the report. Effect sizes use standard deviations, rather than standard errors, and are therefore not influenced by the size of the student population samples. Following Cohen (1988) and Rosnow and Rosenthal (1996), effect size is calculated by finding the difference between the means of two groups and dividing that result by the pooled standard deviation of the two groups: Est grp1 and est grp2 are the student group estimates being compared. Sd pooled is the pooled standard deviation of the groups being compared. The formula for the pooled standard deviation is as follows (Rosnow and Rosenthal 1996): Sd 1 and sd 2 are the standard deviations of the groups being compared. In social sciences, an effect size of .2 is considered small, one of .5 is of medium importance, and one of .8 or larger is considered large (Cohen 1988). Table A7 shows the countries that participated in TIMSS 2003 at fourth and eighth grades. The countries are grouped by continent. In addition, countries that are members of the Organization for         National desired population does not cover all of the international desired population. NOTE: Countries are sorted by 2003 average percent correct. Parentheses indicate countries that did not meet international sampling or other guidelines. See appendix A for more information. The international average reported here may differ from that reported in  due to the deletion of England. Countries were required to sample students in the upper of the two grades that contained the most number of 13-year-olds. In the United States and most countries, this corresponds to grade 8. See There is less oxygen in Z so the candle will use it up faster and go out. Hong Kong is a Special Administrative Region (SAR) of the People's Republic of China. NOTE: Countries are sorted by 2003 average percent correct. Parentheses indicate countries that did not meet international sampling or other guidelines. See appendix A for more information. The international average reported here may differ from that reported in  due to the deletion of England. Countries were required to sample students in the upper of the two grades that contained the most number of 13-year-olds. National desired population does not cover all of the international desired population. NOTE: Countries are sorted by 2003 average percent correct. Parentheses indicate countries that did not meet international sampling or other guidelines. See appendix A for more information. The international average reported here may differ from that reported in  due to the deletion of England. Countries were required to sample students in the upper of the two grades that contained the most number of 13-year-olds. In the United States and most countries, this corresponds to grade 8. See table A1 in appendix A for details. SOURCE: International Association for the Evaluation of Educational Achievement (IEA). Trends in International Mathematics and Science Study (TIMSS), 2003."}, {"section_title": "Country participation", "text": "Jupiter is much farther away from Earth than the moon is. 3 National desired population does not cover all of the international desired population. NOTE: Countries are sorted by 2003 average percent correct. Parentheses indicate countries that did not meet international sampling or other guidelines. See appendix A for more information. The international average reported here may differ from that reported in  due to the deletion of England. Countries were required to sample students in the upper of the two grades that contained the most number of 13-year-olds.     and  due to the deletion of England. Countries were required to sample students in the upper of the two grades that contained the most number of 13-year-olds.           1995, 1999, or 2003. See appendix A for details regarding 2003 data. See  for details regarding 1995 and 1999 data. Countries were required to sample students in the upper of the two grades that contained the most number of 13-yearolds. In the United States and most countries, this corresponds to grade 8. See   The analyses presented in this report examine the performance of U.S. fourth-and eighth-grade students in comparison to their counterparts in other countries. The TIMSS data are best understood in relation to data from other large assessments of similar subjects, such as the National Assessment of Educational Progress (NAEP) or the Program for International Student Assessment (PISA). Some of the TIMSS results for the United States mirror similar findings in the 2003 NAEP mathematics assessment (Braswell, Daane, and Grigg 2003). For example, as in TIMSS eighth grade, the national mathematics average of eighth-graders in NAEP increased from 1996, the most comparable dates between NAEP and TIMSS. However, some of the TIMSS results, particularly at fourth grade, do not mirror the findings in NAEP. Both TIMSS and NAEP are curriculum-based studies, while PISA, an international assessment of the reading, mathematics, and science literacy skills and abilities of 15-year-olds in the 30-member countries of the Organization for Economic Cooperation and Development, is less so. PISA 2003 results indicate that U.S. 15-year-olds performed relatively poorly in mathematical literacy in comparison to their peers in the other OECD-member nations (Lemke et al. 2004). In 2003, 15-year-olds in the United States scored below the international average in mathematical literacy and below their peers in 20 of the 28 other OECD-member countries. Consistent with the principle of assessing curriculumand school-based mathematics learning, NAEP and TIMSS focus on the performance of students in the same grade (fourth, eighth and, for NAEP, twelfth). This is important from the NAEP perspective because it allows the development of proficiency benchmarks-what students should know by the end of eighth grade-against which to compare what students actually know at the end of eighth grade. In the case of TIMSS, this allows comparisons of countries based on student populations with similar numbers of years of schooling. PISA, on the other hand, measures the demonstrated mathematics and science literacy of students of the same age-15-year-olds. This allows an internationally comparable measure of system yield: the knowledge output of the education system at a point when students are nearing the end of compulsory education. The scores on NAEP, TIMSS, and PISA are not directly comparable, for both technical and practical reasons. Rather, the information on student achievement collected through these three studies can be understood through comparisons of their conceptual frameworks as well as the assessment items. NCES sponsored two comparative studies of TIMSS, PISA, and NAEP items. The first was a comparison of the conceptual frameworks and assessment items using NAEP as the centerpiece (Neidorf, Binkley, Gattis, and Nohara forthcoming; Neidorf, Binkley, and Stephens forthcoming). The second was a comparison of the conceptualization of and implementation of problemsolving assessments items in PISA and TIMSS (Dossey, O'Sullivan, and McCrone forthcoming). Based on the NAEP conceptual framework, a panel of mathematics and science experts compared the mathematics assessment items from TIMSS, PISA, and NAEP on several dimensions: content, grade level, item type, and cognitive processes. The results of this study indicate that mathematics items from TIMSS 2003 and NAEP 2000 and 2003 appear more similar in content than do PISA 2003 andNAEP 2003 (Neidorf, Binkley, Gattis, andNohara forthcoming;Neidorf, Binkley, and Stephens forthcoming). Examination of the mathematics frameworks and items showed that a major difference is that both TIMSS 2003 and NAEP 2003 mathematics have a relatively high percentage (33 and 26 percent, respectively) of items focused on the content area Number compared to PISA, which has the highest percentage of items (40 percent) focused on the content area Data, the content area of least focus in TIMSS and NAEP (Neidorf, Binkley, Gattis, and Nohara forthcoming). Grade-level analysis suggests that an eighth-grade TIMSS mathematics item or a PISA item designed for 15-year-olds could also be an eighth-grade NAEP item-in other words, that almost all the items seemed to fit within the age/grade descriptions for each assessment. Examination of the science frameworks and items showed that while NAEP 2000 and TIMSS 2003 are generally similar in terms of their broad content areas in science, there is some difference in relative emphasis (Neidorf, Binkley, and Stephens forthcoming). For example, NAEP currently has a greater emphasis than TIMSS on Earth Science at both the fourth and eighth 102 grades than does TIMSS. TIMSS has a greater emphasis than NAEP on Life Science in the fourth grade and on Physical Science in the eighth grade. TIMSS also includes Environmental Science as an explicit part of its framework whereas NAEP does not. Over 80 percent of the science items from TIMSS and NAEP map to the other's framework at the corresponding grade level. The study also found that NAEP science items require more conceptual understanding than TIMSS science items, whereas TIMSS gives relatively more emphasis to items requiring factual knowledge than does NAEP. For more detailed information on the comparative item study, see Neidorf, Binkley, Gattis, and Nohara (forthcoming); and Neidorf, Binkley, and Stephens (forthcoming). In a separate study (Dossey, O'Sullivan, and McCrone forthcoming), PISA and TIMSS mathematics and science items were examined for their connection to problem-solving skills and abilities. While PISA 2003 provided students with a separate assessment focused on problem-solving, TIMSS 2003 incorporated problem-solving and inquiry (PSI) tasks into the regular assessment booklets. In addition to items that were specifically designed to tap into problem-solving skills and abilities, the remaining items were also examined for the range of problemsolving skills embedded in them. A review of all the assessment items in PISA 2003 andTIMSS 2003 showed that 38 percent of eighth-grade TIMSS 2003 mathematics items and 48 percent of PISA 2003 mathematical literacy items measured some aspect of problem-solving; similarly, 26 percent of eighthgrade TIMSS 2003 science items and 49 percent of PISA science literacy items measured problem-solving skills (Dossey, O'Sullivan, and McCrone forthcoming). More items in PISA were found to require students to critically evaluate information than in TIMSS, both in mathematics and science. A similar percentage of problem-solving items in TIMSS science and PISA science measured scientific inquiry skills (33 percent). Eighty percent of TIMSS science items required students to know science information and knowledge compared to 35 percent of PISA science items. And, PISA items were more likely to involve a reading passage than TIMSS items. NAEP and TIMSS were similar in the predominance of mul-tiple-choice items; PISA was more likely to employ extended-response items. For more detailed information on the comparative item study, see Dossey, O'Sullivan, and McCrone (forthcoming). In sum, among the three studies, TIMSS and NAEP appear to have the most in common, with a focus on material that is more likely to be taught through the school curriculum than PISA, which is more situationand phenomena-based. The content in TIMSS and NAEP mathematics and science overlap substantially. Nonetheless, NAEP was found to have a greater emphasis on Earth Science and TIMSS has a greater emphasis on Physical Science in the eighth grade. TIMSS also includes Environmental Science as an explicit part of its framework whereas NAEP does not. TIMSS and PISA appear to have less in common than TIMSS and NAEP. TIMSS and PISA differ in a number of respects, including a greater focus on factual knowledge in mathematics and science in TIMSS than in PISA, and a greater focus on problem solving and the critical evaluation of information in PISA than in TIMSS. Moreover, PISA has a greater focus on data analysis, statistics and probability in mathematics than either TIMSS or NAEP. The detailed examinations of the conceptual underpinnings and assessment items in TIMSS, PISA, and NAEP described above offer, among other possibilities, at least one way to understand the most recent results in mathematics and science from these studies. Assuming that TIMSS and NAEP mathematics and science have more in common than do either TIMSS and PISA or NAEP and PISA, it seems reasonable to have expected recent improvements in the average performance of eighth-graders on NAEP mathematics to be found in the TIMSS data as well. However, the TIMSS results at fourth grade do not mirror the most recent NAEP results. Assuming that PISA places more emphasis on items that require a greater focus on problem solving, the critical evaluation of information, as well as a greater focus on data analysis, statistics and probability in mathematics than either TIMSS or NAEP, it also seems reasonable to have expected the PISA results in mathematics to differ from results in either TIMSS or NAEP. A more thorough and detailed examination of the results from all three studies-TIMSS, PISA, and NAEP-may reveal other differences and similarities between them. Moreover, such analyses may provide insights into the actual reasons that U.S. students perform differently in seemingly similar subject areas on national and international assessments. Finally, the results from the comparisons among TIMSS, NAEP, and PISA frameworks and items, carried out in anticipation of the release of TIMSS and PISA 2003 data, will likely change in the future whenever any of the guiding frameworks for these three assessments are updated."}]