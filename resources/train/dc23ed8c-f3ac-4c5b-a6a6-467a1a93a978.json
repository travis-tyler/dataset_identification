[{"section_title": "vi EXECUTIVE SUMMARY", "text": "2012/17 BEGINNING POSTSECONDARY STUDENTS LONGITUDINAL STUDY (BPS:12/17) DATA FILE DOCUMENTATION deduced with certainty based upon logical or mathematical relationships among observed variables, the weighted sequential hot deck method was used to replace missing data by imputing plausible values from statistically selected donor cases (Cox 1980;Iannacchione 1982). Staff examined the data and resolved apparent anomalies as needed. Analysts then created the main study derived variables by examining the data available from the various data sources, prioritizing the data sources on an item-byitem basis, and reconciling discrepancies within and between sources. Details about the creation of each variable appear in the variable descriptions contained in the Tables and figures throughout this report present relevant analyses from the fullscale collection. Due to rounding, row and column entries in tables may not sum to their respective totals, and reported percentages may differ somewhat from those that would result from the rounded numbers. Rounding is used to ensure the confidentiality of study respondents."}, {"section_title": "Background and Purpose of BPS", "text": "BPS is one of several NCES studies developed to address the need for nationally representative data on important postsecondary education issues. BPS cohorts include FTB undergraduate students in postsecondary institutions. Follow-up data collections capture the academic progress and persistence in postsecondary education of a BPS cohort after its initial entry into a postsecondary institution. BPS data are collected from student surveys and administrative data sources that include postsecondary institutions, federal agencies, and other data providers. Examples of postsecondary institution administrative data include academic transcripts and financial aid records. Examples of federal administrative data include data on financial aid applications and student loans, and examples of other data providers include the National Student Clearinghouse (NSC) and College Board. The student survey instrument includes items grouped by six key content areas: enrollment, education experiences, financial aid, employment, income and expenses, and background. With these data, researchers, policymakers, practitioners, and other users can address topics such as persistence and attainment, employment during enrollment, financial aid and borrowing, education and career expectations, and employment outcomes after leaving postsecondary education. For more detailed information on the specific contents of the BPS:12/17 student survey and on administrative data sources, see chapter 3, section 3.1, and chapter 4, respectively. BPS FTB students are surveyed at three points in time: in the base year (through items that were included in the NPSAS student survey instrument) and at two subsequent intervals. The BPS:90 cohort was followed up 3 and 5 years after the NPSAS:90 student survey. The BPS:96 cohort established a pattern that has continued, following up with the students 3 and 6 years after the NPSAS:96 student survey. Previous cycles of NPSAS, as well as BPS, are authorized by the following legislation: \u2022 the Higher Education Opportunity Act of 2008, 20 U.S.C. \u00a7 1015(A)(k); and \u2022 the Education Sciences Reform Act of 2002, 20 U.S.C. \u00a7 9543. CHAPTER 1. OVERVIEW OF BPS:12/17 3 Table 1 shows the timeline for the base year and subsequent follow-ups for each BPS cohort. "}, {"section_title": "Overview of BPS:12/17 Design and Data Collection", "text": "The target population for BPS:12/17 consisted of all students who began their postsecondary education for the first time during the 2011-12 academic year at any Title IV eligible postsecondary institution in the United States, 2 and the BPS:12 cohort was sampled from the NPSAS:12 sampling frame. BPS:12/17 included both a student survey and collection of administrative data. The student survey instrument was tested in the BPS:12/17 pilot test before its design was finalized. 3 The BPS:12/17 student survey included core data elements used in previous BPS surveys, including elements identified through a redesign that used a research framework, the human capital model. These elements addressed the costs and benefits associated with enrolling and persisting in higher education, a key principle of the human capital framework (Becker 1975). Like the BPS:12/14 student survey, the BPS:12/17 student survey included an expanded employment section that provided additional data on students' labor market experiences and outcomes. In particular, new questions (first used in NPSAS:12) focused on respondents' choice of a major or field of study, nonmonetary benefits of education, and future expected earnings and occupation. Programmers developed the BPS:12/17 survey using proprietary software that provided specifications, programming, and testing interfaces for the student survey instrument. All information relating to the survey (including details about the study, what to expect, and confidentiality) was stored in a structured query language (SQL) server database and was made accessible to the BPS:12/17 sample members through web browser interfaces. The BPS:12/17 student survey incorporated a responsive-design data-collection plan similar to the one used in BPS:12/14, in which the web-based survey was administered in two separate substudies: a calibration study and the main study. The calibration study, initiated first, evaluated the use of a prepaid incentive to boost response rates. About 10 percent of the BPS:12/17 full-scale sample members were randomly selected to be included in the calibration study, with data collection beginning in early March 2017. The main study started approximately 7 weeks later and collected data from the remaining sample members in several phases which, for previous nonrespondents, involved targeted incentive boosts, targeted early offers of an abbreviated survey, and transfer of all remaining nonrespondents to an abbreviated survey. The most significant differences from the BPS:12/14 data collection were the addition of a special protocol for sample members who had not responded to either of the prior student surveys (neither NPSAS:12 nor BPS:12/14) and the addition of a randomized control-treatment experiment to measure the impact of the responsive-design targeted interventions on nonresponse bias. 4 To supplement the student surveys, BPS staff obtained additional information from a variety of administrative data sources. Staff matched and downloaded student financial aid data from CPS, which houses and processes data contained in the Free Application for Federal Student Aid (FAFSA) forms. Staff also obtained data from NSLDS, which contains Title IV loan and grant data, and NSC, which contains enrollment and degree-attainment information provided by participating institutions. Data-collection procedures were similar to those used in BPS:12/14. A multistep process (described in section 3.2.3), was used to locate, trace, and contact sample members. This process included a panel-maintenance mailing and e-mail, batch and intensive tracing, and a mailing and e-mail inviting sample members to complete the survey. During data collection, a study website and help desk provided information and support to sample members. Sample members could complete the survey independently on the Web, or interviewers trained in computer-assisted telephone interviewing (CATI) methods were available to help sample members complete the survey over the telephone. Quality control procedures used in student survey data collection included frequent monitoring of recorded interviews, a help desk to assist sample members who had questions about the study or completion of the web survey, quality circle meetings to facilitate communication among staff members, and debriefing meetings to identify areas of success and for improvement. Throughout the data-collection period, BPS staff processed and examined the data for quality. After the conclusion of data collection, BPS staff imputed missing data for selected variables included in the restricted-use research file and in the public-use data available through the NCES online application PowerStats. Analysts created the analytic variables by examining the data for each student from all available data sources, prioritizing the data sources on a variable-by-variable basis, and reconciling discrepancies within and between sources. Following imputation, BPS statisticians conducted weighting procedures to adjust for nonresponse and poststratification. Table 2 summarizes the schedule for the major BPS:12/17 data-collection activities."}, {"section_title": "Schedule and Products", "text": "Products include electronically documented restricted-use data files (with associated codebooks) and the NCES online application PowerStats, 5 which are available for use by researchers. Chapter 5 contains a description of the restricted-use research files and the datasets that they include. In addition to the restricted-use files and documentation, BPS:12/17 has produced a First Look report that provides descriptive information for the BPS:12 cohort, special tabulations on issues of interest to the higher education community, and descriptive reports of significant findings for dissemination to a broad range of audiences. A list of these publications is available at https://nces.ed.gov/surveys/bps/. This chapter describes the student universe and sampling methods used to produce a nationally representative sample of FTB students for BPS:12/17. It also discusses the sampling methods for the prior data collections with this BPS cohort, including the two-stage sampling design used for the first (base-year) data collection, NPSAS:12, which produced this BPS cohort. That design began with the selection of postsecondary institutions, followed by selection of students from these institutions. Pertinent to this sampling design was the definition of a base-year study member and the identification of FTB students. The discussion also outlines the sampling methods used in the first follow-up data collection, BPS:12/14."}, {"section_title": "Student Universe", "text": "The student universe for BPS:12/17 consisted of all students who began their postsecondary education for the first time during the 2011-12 academic year at any postsecondary institution in the United States that was eligible for inclusion in NPSAS:12. The BPS:12 cohort, or the BPS:12/14 sample, was created from the set of confirmed and potential FTB students identified in NPSAS:12. In turn, the BPS:12/17 sample was created from the BPS:12/14 sample, using additional data obtained during the BPS:12/14 data collection. The NPSAS:12 institution and student universes are defined in greater detail in the subsections that follow."}, {"section_title": "Institution Universe for NPSAS:12", "text": "To be eligible for NPSAS:12, students must have been enrolled in eligible institutions, which were required during the 2011-12 academic year to meet the following requirements: \u2022 offer an educational program designed for persons who have completed secondary education; \u2022 offer at least one academic, occupational, or vocational program of study lasting at least 3 months or 300 clock hours; CHAPTER 2. SAMPLING 9 2012/17 BEGINNING POSTSECONDARY STUDENTS LONGITUDINAL STUDY (BPS:12/17) DATA FILE DOCUMENTATION"}, {"section_title": "Institution and Student Samples for NPSAS:12", "text": "NPSAS:12 used a two-stage sampling design. The first stage involved the selection of institutions from a sampling frame that included 6,670 postsecondary institutions in the United States. In the second stage, students were selected from sampled institutions, with special emphasis placed on identifying FTB students. NPSAS:12 staff also created a study member definition by identifying key data elements across data sources that were necessary to support the analytic objectives of the study. Study members were students in the NPSAS:12 sample for whom a minimum number of these key data elements were collected."}, {"section_title": "Institution Sample for NPSAS:12", "text": "NPSAS project staff constructed the NPSAS:12 institution sampling frame of 6,670 institutions from the 2008-09 Integrated Postsecondary Education Data System (IPEDS:2008-09) Institutional Characteristics (IC), Fall Enrollment (EF), 12-month Enrollment (E12), and Completions (C) files. For the small number of institutions in the frame that had missing enrollment information, NPSAS statisticians imputed the data following IPEDS imputation procedures (see Ginder, Kelly-Reid, and Mann, 2016) so that all data derived from IPEDS were consistent. NPSAS statisticians selected institutions for the initial sample using sequential probability minimum replacement (PMR) sampling (Chromy 1979), which resembles stratified systematic sampling with probabilities proportional to a composite measure of size (Folsom, Potter, and Williams 1987). This is the same methodology that has been used since NPSAS:96. To prevent PMR from allowing institutions to be selected multiple times, all institutions with a probability of being selected more than once were included in the sample one time with certainty. NPSAS statisticians determined institution measures of size using enrollment data from the most recent IPEDS EF and E12 components. This helped to ensure that NPSAS achieved target sample sizes within institution and student sampling strata, while also achieving approximately equal student weights across institutions. The initial sample consisted of 1,970 institutions within 10 institutional strata. NPSAS staff divided the strata into the following categories based on institution control, level, and highest level of offering: 7 \u2022 public less-than-2-year; 10 CHAPTER 2. SAMPLING 2012/17 BEGINNING POSTSECONDARY STUDENTS LONGITUDINAL STUDY (BPS:12/17) DATA FILE DOCUMENTATION \u2022 public 2-year; \u2022 public 4-year, non-doctorate-granting; \u2022 public 4-year, doctorate-granting; \u2022 private nonprofit less-than-4-year; \u2022 private nonprofit 4-year, non-doctorate-granting; \u2022 private nonprofit 4-year, doctorate-granting; \u2022 private for-profit less-than-2-year; \u2022 private for-profit 2-year; and \u2022 private for-profit 4-year. Although prior NPSAS administrations aggregated private for-profit 2-year and 4-year institutions into one sampling stratum, NPSAS:12 split the two into separate strata to reflect the recent growth in enrollment in the private for-profit sector. Within each institution stratum, NPSAS statisticians accomplished additional implicit stratification by sorting the sampling frame within strata by the following classifications: 8 (1) Historically Black Colleges and Universities status; (2) Hispanic-Serving Institutions (HSIs) status; 9 (3) Carnegie classifications of degree-granting postsecondary institutions; 10 (4) 2-digit Classification of Instructional Programs code of the largest program for less-than-2-year institutions; (5) the Office of Business Economics Region from the IPEDS IC file (Bureau of Economic Analysis of the U.S. Department of Commerce Region); (6) state and system for states with large systems, e.g., the SUNY and CUNY systems in New York, the state and technical colleges in Georgia, and the California State University and University of California systems in California; and (7) the institution measure of size. The objective of this implicit stratification was to approximate proportional representation of institutions on these measures. From the initial sample of 1,970 institutions, 300 were selected for the field test using simple random sampling within strata. The remaining 1,670 were assigned to the fullscale sample. NPSAS statisticians freshened the institution sample using IPEDS:2009-10 IC, EF, E12, and C files when available. This freshening process identified 390 new and newly eligible institutions to produce a frame that more CHAPTER 2. SAMPLING 11 closely represented institutions eligible for NPSAS:12 in the 2011-12 academic year. NPSAS statisticians then determined the freshened sample size such that the additional institutions would have similar probabilities of selection to the originally sampled institutions within strata in order to minimize unequal weights and subsequent variances. Twenty institutions were added to the full-scale sample, resulting in a total of 1,690 sampled institutions. Table 3 shows size of universe, institution sampling rate, and the number of institutions sampled, by institution stratum.  Table 4 shows the counts of sampled, eligible, and participating institutions, as well as the weighted and unweighted participation rates, by control and level of institution. Almost all of the 1,690 sampled institutions met the eligibility requirements. 11 Of those eligible institutions, 88 percent provided enrollment lists, which is similar to rates obtained in previous rounds of NPSAS. \u2022 student's name; \u2022 Social Security number (SSN); \u2022 student ID number (if different from SSN); \u2022 student level (undergraduate, master's, doctoral-research/scholarship/other, doctoral-professional practice, other graduate); \u2022 FTB indicator; \u2022 class level of undergraduates (first year, second year, etc.); \u2022 date of birth; \u2022 Classification of Instructional Program code or major; \u2022 undergraduate degree program; \u2022 high school graduation date (month and year); and \u2022 contact information (local street address and telephone number, permanent street address and telephone number, and school and home e-mail address). Because locating data were included in enrollment lists, web-based student record collection and interviewing could begin almost immediately after sample selection. For institutions unwilling to provide locating data for the entire list of students, NPSAS statisticians selected the student sample from the identifying and classifying student data. They then requested locating data for only sampled students immediately afterward. Chapter 3, section 3.2.3, contains details about the locating process. Statisticians sampled students by means of stratified systematic sampling with predetermined sampling rates that varied by student stratum, as described in appendix B of the NPSAS:12 data file documentation (Wine, Bryan, and Siegel 2014). Following are the 11 student sampling strata: 1. FTB undergraduate students enrolled in certificate programs; 2. other FTB undergraduate students; 3. other undergraduate students; 13 4. master's degree students in science, technology, engineering, and mathematics (STEM) programs; 5. master's degree students in education and business programs; 13 Other undergraduate students are defined as any undergraduate students not classified as first-time beginning students. 6. master's degree students in other programs; 7. doctoral-research/scholarship/other students in STEM programs; 8. doctoral-research/scholarship/other students in education and business programs; 9. doctoral-research/scholarship/other students in other programs; 10. doctoral-professional practice students; and 11. other graduate students. 14 To eliminate cross-institution duplication of students, statisticians compared SSNs of sample members across institutions. Multiplicity adjustments in the sample weighting accounted for the fact that any students who attended more than one institution during the NPSAS year had more than one opportunity for selection, as described in section 6.3.3 of the NPSAS:12 data file documentation (Wine, Bryan, and Siegel 2014). NPSAS statisticians calculated initial student sampling rates for each sample institution, using sampling rates designed to generate approximately equal probabilities of selection within the ultimate institution-by-student sampling strata. However, they sometimes modified these rates as follows: \u2022 NPSAS statisticians increased student sampling rates so that the sample size for each sampled institution was at least 10 students (if possible) to ensure sufficient yield for variance estimation. \u2022 NPSAS statisticians decreased student sampling rates if the sample size was greater than 300, so that no institution would have more than 300 sample members. \u2022 To ensure that the desired student sample sizes were achieved, statisticians monitored sample yield throughout enrollment list collection and adjusted student sampling rates periodically for institutions for which sample selection had not yet been performed. The full-scale sample achieved a size of about 128,120 students-approximately 59,740 were potential FTB students, 51,050 were other undergraduate students, and 17,330 were graduate students (table 5). The achieved sample size was higher than originally targeted because institution participation rates were higher than estimated, Control and level of Institution Public Less-than-2-year 790 0.6 510 0.9 280 0.5 # # 2-year 37,000 28 members when a minimum number of variables were available. These study members were the NPSAS:12 unit of analysis. Specifically, a study member was any sample member who NPSAS staff determined to be eligible for the study, per the criteria delineated in section 2.1.2, and who had, at a minimum, valid data from any combination of sources for the following variables: \u2022 student type (undergraduate or graduate); \u2022 date of birth (or age); \u2022 sex; and \u2022 at least 8 of the following 15 variables: -parent education. The final sample numbered 128,120 students. Of the final sample, 96 percent (n = 123,600) were eligible for NPSAS based on the institution and student universe requirements defined in sections 2.1.1 and 2.1.2, respectively. On completion of data collection, NPSAS staff determined that 90 percent of the unweighted eligible sample and 91 percent of the weighted eligible sample had sufficient data to meet the definition of a study member."}, {"section_title": "Identification of FTB Students in NPSAS:12", "text": "To be included in the BPS:12 cohort sampling frame, students in the NPSAS:12 sample must have begun their postsecondary education for the first time, after completing high school, between July 1, 2011, and April 30, 2012. 15 Previous BPS cohorts did not require that the NPSAS institution be the first institution a student attended after completing high school in order for the student to be included in the study. Therefore, the first institution a respondent attended was not necessarily the NPSAS institution. For the BPS:12 cohort, students who reported in the NPSAS:12 student survey enrollment section that the NPSAS institution was not the first postsecondary institution they attended after completing high school (22 percent) were excluded from the BPS:12 cohort sampling frame. This change-requiring that the NPSAS institution be the first postsecondary institution attended-increased data coverage for the first school, which is of analytical value for questions related to student persistence and attainment. NPSAS survey staff prioritized the accurate identification of FTB students in NPSAS:12 to avoid misclassification, which can result in excessive cohort loss, costs to replenish the sample, and an inefficient sample design (excessive oversampling of potential FTB students) to compensate for anticipated misclassification error. In past NPSAS studies, institutions have not always been able to identify FTB students accurately. Specifically, some institutions had difficulty differentiating students who were simply new to the institution from true FTB students; that is, students enrolling in postsecondary education for the first time after completing high school. The participating institutions and several administrative data sources provided data to aid in properly classifying FTB students within the NPSAS:12 sampling frame. Key data the institutions provided included an FTB student indicator, high school graduation date, and date of birth. Administrative data sources, including the NSLDS, CPS, and NSC, provided data that were of particular use in identifying prior enrollment by students who had been identified as potential FTB students (false positives). The administrative databases were used in a presample matching process. NPSAS statisticians combined the FTB status indicator along with institutional data such as student's class and student levels detailed in section 2.2.2. These were used to identify and exclude misclassified FTB students in their third year or higher, as well as those who were not undergraduates. Statisticians combined date of birth with FTB status to identify students older than age 18 to send for presampling matching to one of the administrative databases. Because this administrative presampling matching was new to NPSAS:12, statisticians oversampled potential FTB students to ensure that NPSAS surveys would identify a sufficient number of confirmed FTB students. In addition, NPSAS statisticians set FTB student selection rates taking into account the error rates observed in NPSAS:04 and BPS:04/06 within each institutional stratum. Additional information on NPSAS:04 methodology is available in the study methodology report (Cominole et al. 2006) and the BPS:04/06 methodology report (Cominole et al. 2007). NPSAS statisticians adjusted these rates to reflect the expected improvement in the accuracy of the frame from the NSLDS, CPS, and NSC record matching. Statisticians used institutional strata-level FTB student error rates from the field test to help determine the rates necessary for fullscale student sampling. Prior to sampling, NPSAS statisticians matched all students listed as potential FTB students within the NPSAS:12 sampling frame to NSLDS and CPS records, simultaneously, to determine if they had a federal financial aid history predating the NPSAS year (earlier than July 1, 2011). Because NSLDS maintains current records of all Title IV federal grant and loan funding, statisticians could reliably exclude any student with data showing student loan disbursements in a prior year from the sampling frame of FTB students. The CPS file contains an indicator of student type, including a status for FTB students. The limitation of both administrative data sources is that neither can identify false positives among students who were enrolled before July 1, 2011, but did not obtain federal financial aid. However, about 60 percent of FTB students receive some form of Title IV aid in their first year, and the matching process improved the accuracy of the list prior to sampling, yielding fewer false positives. After the NSLDS and CPS matching, another matching process was performed with NSC data. Due to the expense of sending all cases to NSC, statisticians used information from BPS:04/06 to identify institutional strata that had high falsepositive rates in this previous BPS cohort. BPS:04/06 was the first BPS cohort that had a detailed investigation of the false-positive rates and was the previous BPS cohort, so it served as the best proxy for how the current BPS cohort would likely behave. As a result, statisticians limited the NSC matching to potential FTB students who were over the age of 18 and attending public 2-year and private for-profit institutions. When institutions did not provide an FTB indicator, NPSAS statisticians sampled a student as an FTB student if he or she was 18 years of age or younger and did not appear to be dually enrolled in high school. Conversely, if a student without an FTB indicator was over the age of 18 and not enrolled in high school, then NPSAS statisticians sampled that student as an \"other undergraduate.\" The \"other undergraduate\" students would only be included in the BPS:12 cohort if they identified themselves as FTB students during the student survey. Matching to NSLDS identified 20 percent of cases as false positives, and matching to CPS identified 17 percent as false positives (table 6). CPS also identified many of the false positives identified by NSLDS. Public less-than-2-year and private nonprofit less-than-4-year institutions had a high percentage of false positives but represented a small percentage of the total sample. Of the cases sent to NSC for matching, 7 percent were false positives. The NSC matching appeared most effective among public 2-year and private for-profit 2-year institutions. Overall, matching to all sources identified 27 percent of cases listed as FTB students as false positives. Of the 36,620 survey respondents sampled as potential FTB students, NPSAS staff confirmed by their survey responses that 78 percent were FTB students. Conversely, of the 48,380 survey respondents whom staff sampled as non-FTB other undergraduate or graduate students, 5 percent were determined to be FTB students (table 7). With the help of the presampling matching, NPSAS:12 reduced the observed false positives to 22 percent from the rate of over 50 percent observed in NPSAS:04. "}, {"section_title": "First Follow-up Sampling (BPS:12/14)", "text": "NPSAS:12 purposefully oversampled potential FTB students in order to provide a sufficient pool of study members from which to draw the BPS:12 cohort. BPS:12/14 was the first follow-up data collection with this cohort of FTB students who began their postsecondary education for the first time, after completing high school, between July 1, 2011, and April 30, 2012. The cohort consisted of three groups according to their NPSAS:12 response status: 1. NPSAS:12 study members who completed the student survey and were found, through their survey responses, to be FTB students; 2. a subsample of potential FTB students for whom enough administrative data were available to classify them as NPSAS:12 study members, but who were student survey nonrespondents; and 3. a subsample of potential FTB students who were not NPSAS:12 study members and were not student survey respondents. All students in group 1 were included in the BPS:12/14 sample because their FTB student status had been confirmed. For students in groups 2 and 3, multiple data sources were used to provide information regarding a student's FTB status, as described in section 2.2.4. The data elements used to estimate a student's likelihood of being an FTB student and to construct the frame for the BPS:12 cohort included the following: \u2022 indicator of FTB status from the institution enrollment lists used for NPSAS:12 student sampling; \u2022 indicator of FTB status from CPS; \u2022 indicator of FTB status from student-level data obtained from institution records; \u2022 student reports (obtained during the NPSAS:12 survey) indicating that they were FTB students during the 2011-12 academic year; \u2022 year of high school graduation; \u2022 receipt of Stafford Loan (date loan was first received and number of years loan was received); \u2022 receipt of Pell Grant (date grant was first received and number of years grant was received); and \u2022 undergraduate class level. Using these data elements, cases were identified that would be included or excluded from the BPS:12 cohort. Stratified simple random sampling was used to select the group 2 and 3 subsamples. Eligible sample members within each group were stratified by institution characteristics, and simple random samples were selected within each stratum. Table 8 summarizes the distribution of the sample from all three groups. Table 8. Distribution of BPS:12/14 full-scale sample, by base-year response status: 2012-14 Base-year response status Table 9 summarizes the BPS:12/14 sampled, eligible, and responding individuals by control and level of institution. 1 An eligible sample member who either completed a full survey, completed an abbreviated survey, or completed at least the enrollment section. 2 Control and level of institution are based on data from the 2011-12 National Postsecondary Student Aid Study sampling frame. 3 Sample member eligibility was determined during the student survey or from institution records in the absence of a student survey. 4 The weight used for this column is a base weight. NOTE: Sample sizes rounded to the nearest 10. Percentages are based on the unrounded count of eligible students within the row under consideration. Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/14 Beginning Postsecondary Students Longitudinal Study (BPS:12/14)."}, {"section_title": "Second Follow-up Sampling (BPS:12/17)", "text": "BPS:12/17 was the second follow-up data collection, conducted 3 years after BPS:12/14. Additional data collected during the administration of BPS:12/14 were available for use in preparing the sample for BPS:12/17, which was a subset of the BPS:12/14 sample. The main source of change from the BPS:12/14 sample to the BPS:12/17 sample was the exclusion of BPS:12/14 sample members who were determined to not be FTB students and were therefore ineligible for BPS. Deceased individuals were also excluded from the BPS:12/17 sample. Table 10 shows the distribution of the 35,540 sample members determined to be eligible at the start of BPS:12/17 data collection, by study member status and survey response status, and identifies groups that were fielded (included in data-collection activities with an objective of obtaining a complete survey) in BPS:12/17. These 35,540 sample members are the same individuals identified as eligible students in table 9 (section 2.3), which details the BPS:12/14 sample. Although BPS:12/17 sample members who did not respond to the BPS:12/14 survey and lacked sufficient information to be classified as NPSAS:12 study members were eligible for BPS:12/17, these sample members (groups 6 and 8 in table 10) were not fielded. This resulted in a little over 1,780 BPS:12/17 sample members not being fielded and asked to complete the student survey. In addition, early in data collection, the names of 30 sample members were matched to names on the Specially Designated Nationals and Blocked Persons List published by the U.S. Department of the Treasury, Office of Foreign Assets Control. The matched individuals were excluded from data-collection activities (group 9). Instead, they were treated as study nonrespondents for purposes of response rate calculation and were accounted for with weight adjustments. Table 11 summarizes the BPS:12/17 sampled, eligible, and responding individuals by control, level, and degree-granting status of institution. "}, {"section_title": "Chapter 3. Student Survey Design, Data Collection, Quality Control, and Evaluation", "text": "This chapter describes the BPS:12/17 student survey design, including the key content areas that form the survey and the systems used for survey instrumentation and data collection. It also describes the methods used to survey sample members and the outcomes of data-collection activities, including the number of sample members located and surveyed, time required to complete the survey, time spent contacting and interviewing sample members, number of calls to sample members, and rate of conversion for survey refusals. Finally, the chapter describes the quality control procedures used in student survey data collection 16 and the results of evaluations of data quality and response patterns. These evaluations included recoding and upcoding of data collected by survey coders and review of help-text access rates, conversion text success rates, and item nonresponse rates."}, {"section_title": "Student Survey Design and Systems", "text": "As part of a longitudinal study, the BPS:12/17 student survey was developed using core data elements from previous BPS:12 data collections. This section describes the six key content areas that form the survey and the various systems used to support the instrument development process and data collection. The BPS:12/17 student survey, administered between March 2017 and January 2018, included many long-standing BPS data elements such as postsecondary enrollment history, financial aid and borrowing, employment, and career expectations. The primary purpose of the BPS study is to contribute to a better understanding of how these factors relate to three key postsecondary outcomes: persistence, degree attainment, and employment. Input from the study's expert Technical Review Panel (TRP), cognitive and usability testing (used to refine the survey questions, maximize the quality of data collected, and provide information on issues with important implications for the survey design), and findings from the BPS:12/17 pilot test helped finalize the full-scale data elements. For a complete list of TRP members, see appendix B. For a summary of findings from the BPS:12/17 pilot test, see appendix C. The BPS:12/17 student survey included items grouped by six key content areas: enrollment, education experiences, financial aid, employment, income and expenses, and background. The principal topics of each content area are summarized below. For a complete list of data elements, see appendix D. For the complete BPS:12/17 student interview, see appendix E. Enrollment items included questions to determine study eligibility for sample members who were nonrespondents in both NPSAS:12 and BPS:12/14 (double nonrespondents). The section also captured high school completion information and information on postsecondary enrollment at the NPSAS institution and other postsecondary institutions attended between July 2011/2014 and June 2017 (depending on prior-round response status). Respondents were asked about the expected date of completion for their current enrollment and the highest degree they ever expected to complete. This content area also collected date of birth and marital status, which affected routing throughout the remainder of the survey. Education Experiences items collected information on respondents' remedial coursework since high school and their estimated grade point average and online course-taking at their primary institution. The primary institution was the current or most recent institution the respondent attended between July 2014 and June 2017. If the respondent attended more than one institution at the same time, the primary institution was the institution where they were completing the highest degree or certificate program. Financial Aid items collected information about sources of aid for undergraduate education in the 2016-17 academic year, such as private loans, employer-provided scholarships or tuition reimbursement, scholarships from a private organization, and veterans benefits. Respondents were also asked to report the amount of private loans they took out for their entire undergraduate education as well as monthly payment amounts, if in repayment. Employment items captured employment during the 2011-12 academic year and information about all employers who paid the respondent between July 2014 and June 2017, such as employer names and dates of employment. The employment section also collected more detailed information about the current or most recent employer, such as zip code and type of occupation, and asked whether the individual had looked for work while not employed. Income and Expenses collected financial information such as respondents' annual income, monthly rent or mortgage amount, and monthly car loan or lease amount. Additional items included information about the number of children and other dependents. The section concluded with a series of \"discount rate\" questions (i.e., the extent to which the individual prefers current over future rewards) to capture financial traits reflective of the human capital framework implemented for the BPS:12 cohort. Background obtained information about respondent demographics such as zip code of primary residence when they last attended school, military status, disability status, and voting behavior. This section concluded with a series of financial literacy questions."}, {"section_title": "Survey Mode Administration", "text": "For BPS:12/17, a single survey instrument was administered in two user modes: web (nonmobile and mobile) and telephone. The web survey was designed with deviceoptimized pages based on HTML5 web standards that ensured the instrument formatting would scale properly for all screen sizes. Screen-size optimization allowed respondents to complete the survey on a desktop or laptop computer while providing a mobile-friendly design for respondents who wished to complete the survey on a tablet or smartphone. For the telephone survey, a data-collection interviewer (DCI) accessed the web instrument through a CATI case management system (CMS), which assigned cases and provided the appropriate screens and scripts for the DCI to use. On-screen instructions gave DCIs guidance on administering each question (e.g., whether the DCI should read response options aloud, or when to probe). To minimize mode effects, the BPS web survey incorporated the following features to provide web respondents with assistance similar to that provided by a trained DCI: \u2022 a help-text button on every form (or web screen) to define key terms and clarify question intent; \u2022 prompts to correct out-of-range or incorrectly formatted responses; \u2022 conversion text to encourage responses to unanswered critical items; and \u2022 prompts to encourage response if a sample member left three consecutive questions unanswered."}, {"section_title": "Coding Systems", "text": "Assisted coding systems (coders) within the BPS:12/17 survey standardized the collection and coding of several pieces of information. Coders simplified data entry for four survey items with potentially complex text strings for answers: other postsecondary institution attended between July 2014 and June 2017, major or field of study at NPSAS institution or at other postsecondary institution attended, occupation, and zip codes of current/most recent employer and primary residence. Most coders used a predictive search format, first employed in the BPS:12/17 pilot test, that began returning a list of possible matches as soon as the respondent or DCI started typing. As the individual typed additional characters, the predictive coder updated the possible matches in real time. The individual was able to select an option based on a partial response as soon as it appeared rather than waiting to finish typing the response. The occupation coder was the only coder in the BPS:12/17 full-scale survey that did not use a predictive search format. This coder required the respondent or DCI to type a complete text string, click search, wait for the coder to return a list of possible matches, and then select an option from the list. The following are descriptions of the individual coding systems and sources: \u2022 The Postsecondary Institution coder linked to the complete list of postsecondary institutions contained in IPEDS:2015-16, developed by NCES (https://nces.ed.gov/ipeds/). This coder covered any postsecondary institutions the respondent attended, other than the NPSAS institution, between July 2014 and June 2017. For any institutions not listed in the database, follow-up questions asked respondents to provide the control (e.g., public or private) and level (e.g., 4-year or 2-year) of the institution, as well as the city and state in which the institution was located. \u2022 The Major coder used the 2010 Classification of Instructional Programs (CIP) taxonomy, also developed by NCES (https://nces.ed.gov/ipeds/cipcode). For any majors or fields of study not found in the CIP database, respondents selected a general major area and a specific discipline. \u2022 The Occupation coder linked to the 2015 Occupational Information Network Online (O*NET OnLine) database, version 20.1 (https://onetonline.org). For any occupations not found in the database, the respondents were given drop-downs where they could provide a general occupational area, specific occupational area, and a detailed classification area for the occupation. \u2022 The Zip coder linked to the 2017 ZIPList5 Max database ( ). The coder predictively searched the database using the numeric (i.e., zip code) or text (i.e., city and state) string entered by the respondent or DCI. A checkbox option was provided for locations not in the United States or a U.S. territory. https://zipinfo.com/products/z5max/z5max.htm Text strings that were entered but not matched to a code were reviewed by expert coders to determine if a code could be assigned to the string. This process, upcoding, is described in section 3.4.1."}, {"section_title": "Survey Design System", "text": "Instrument developers used a proprietary web-based system to create the BPS:12/17 survey instrument, conduct internal and external review and testing, and make subsequent modifications in the survey instrument. Staff stored all information relating to the instrument in a SQL server database made accessible through a web interface. Section 3.1.2 of the BPS:12/14 data file documentation (Hill et al. 2016) contains a more detailed description of the survey design system."}, {"section_title": "Student Survey Data Collection", "text": "The BPS:12/17 survey data collection employed a study website and help desk to provide information and support to sample members. Staff used a variety of methods to locate sample members. Sample members could complete the survey independently on the Web or over the telephone with DCIs trained in telephone interviewing methods."}, {"section_title": "Study Website and Help Desk", "text": "Communications with BPS:12/17 sample members included a link to the BPS:12/17 website, which provided general information about the study. This information included details about the study sponsor, how the data would be used, answers to frequently asked questions (FAQs), confidentiality assurances, and selected findings from BPS:12/14. The website also included contact information for the study help desk and project staff at RTI, as well as links to the main BPS website at NCES. Sample members could log in to the secure portion of the website to provide updated contact information and to complete the survey. Figure 1 shows the home page for the BPS:12/17 website. Designed according to NCES policies, the BPS:12/17 website used a three-tier security approach to protect all data collected. The first tier included secure log-ins, with a unique study ID and strong password provided to sample members prior to the start of data collection. The second tier protected any data entered on the website with Secure Sockets Layer (SSL) technology, allowing only encrypted data to be transmitted over the Internet. The third tier stored all collected data in a secured SQL server database located on an NCES server that was physically separate from the web server. In addition to the study information available on the website, BPS:12/17 staff implemented a help desk to assist sample members with matters ranging from general inquiries, to survey completion assistance, to incentive status updates. Staff responded to sample members' questions via the project help-desk number, and provided support for technical issues related to completion of the web survey. For each call, staff confirmed the sample member's contact information for security purposes and, if necessary, recorded a description of the problem and the resolution in the CATI-CMS. If technical difficulties prevented sample members from completing the web survey, DCIs were available to help sample members complete a telephone interview. Two common types of help-desk incidents were requests to retrieve log-in credentials and requests to complete an interview over the telephone. For the convenience of sample members, the BPS:12/17 website included a \"Forgot Password?\" feature. After sample members entered a few pieces of identifying information, their log-in credentials were automatically provided to them via e-mail."}, {"section_title": "Training of Interview Data-Collection Staff", "text": "The BPS:12/17 interview data-collection team included DCIs, quality experts (QEs), performance team leaders (PTLs), and intensive-tracing staff, all of whom participated in a comprehensive training program before beginning work on the study. Prior to study-specific training, all DCIs completed a general training program that included an overview of the CATI-CMS, confidentiality procedures and sample member rights, and proper interviewing techniques, such as proper enunciation and pace of speech. The training schedule and the number of data-collection staff members trained for each role are presented in table 12. Intensive-tracing staff. Intensive-tracing staff completed a 16-hour program on tracing procedures led by tracing managers in RTI's Research Operations Center. Tracing staff then received two additional hours of project-specific training, including an overview of BPS:12/17, FAQs, and tracing techniques most appropriate for BPS:12/17 sample members."}, {"section_title": "Locating Sample Members", "text": "BPS:12/17 project staff used several methods to locate fielded sample members in order to contact them and invite them to complete the student survey. Methods included a panel-maintenance activity before data collection, mailings to parents and sample members before data collection, batch and intensive tracing, and collection of information during unsuccessful telephone contacting attempts. Sample members were considered located if (1) the sample member started or completed the survey; (2) a project staff member spoke directly to the sample member or someone who knew the sample member's whereabouts; or (3) there was some indication that we had reached the sample member; e.g., a project staff member reached a voice mail announcement that mentioned the sample member's name and left a message. Locating rates ranged from a high 95 percent for students enrolled at private nonprofit 4-year, doctorate-granting institutions to a low of 77 percent for students enrolled at private for-profit less-than-2-year institutions (table 13). Survey staff located BPS:12/14 respondents (93 percent) at a significantly higher rate than BPS:12/14 nonrespondents (68 percent) (\u03c7 2 (1, n = 29,070) = 3346.7, p <.001). Panel maintenance. In November 2016, several months before the start of data collection, project staff sent a panel-maintenance mailing and e-mail to sample members. The mailing requested that sample members update their contact information through the study website. By the start of data collection, 3,150 sample members, about 9 percent of the sample, updated their information in response to the panel-maintenance request. Initial contact mailings to sample members and parents. An initial contact mailing reminded sample members about their inclusion in the study, alerted them to the forthcoming data-collection announcement, and notified them of the incentive they were eligible to receive for completing the survey. These letters were sent to calibration sample members in February 2017 and to the main sample in April 2017. Parents of sample members under the age of 27 were sent a lead letter and study brochure, as well as a form and business return envelope. Parents were encouraged to fill out the form with updated contact information for the sample member and mail it back to the project team. Batch tracing. A few weeks prior to the start of data collection, project staff used several batch-tracing databases to update or confirm students' contact information. Batch tracing, which was completed prior to data collection for all cases, included the following steps. Step 1: Staff sent cases with a valid SSN to CPS for record matching. CPS contains information on students who have applied for financial aid using the FAFSA. Staff then compared records obtained from CPS to existing contact data, updating locating information when necessary. Step 2: Next, staff sent cases with at least one valid address to LexisNexis (formerly FirstData) for matching with the U.S. Postal Service (USPS) National Change of Address (NCOA) database. Project staff updated records with new or updated address information based on the match. As part of this step, staff sent sample member information to LexisNexis's PhoneAppend telephone number lookup service to obtain updated telephone information. LexisNexis incudes approximately 718.8 million current and historical phone numbers. LexisNexis's PhoneAppend returns a single telephone number based on a search by name, street address, and zip code. Step 3: Before the first mailing, staff sent cases to LexisNexis's Single Best Address search. While NCOA only provides information for people who registered a change of address with the USPS, Single Best Address can provide new addresses, including those not registered with NCOA. Single Best Address uses a name and SSN to search multiple data sources, using progressive search logic to return the most current address available. Before cases went to intensive tracing, project staff used LexisNexis's Premium Phone batch tracing to inexpensively obtain new telephone information. Premium Phone is a residential telephone number lookup service that uses combinations of name and address information to match phone numbers to sample members. If Intensive tracing. Staff assigned cases that could not be located by prior methods to intensive tracing. Intensive tracing used a two-stage process, accessing both public-domain and proprietary databases. The first stage of intensive tracing identified sample members in consumer databases (e.g., LexisNexis, Experian, and Accurint) by SSN. If intensive-tracing staff found a new telephone lead, they sent the case back to the telephone interview queue for follow-up by DCIs. If the search resulted in a new address only, directory-assistance searches were conducted to obtain a telephone number for the contact. This \u2022 other ad hoc methods, such as calling individuals with the same unusual surname in small towns or rural areas to see if they were relations of, or knew, the sample member. After exhausting all possible leads for locating sample members, staff ceased telephone contacting efforts. Project staff continued to send e-mails and mailings; however, if the sample member did not complete a survey by the end of data collection, that sample member was classified as a nonrespondent. Intensive-tracing results. Overall, 2,720 cases, or 8 percent of the total fielded sample, required intensive tracing (table 15). By type of institution, the percentage requiring intensive tracing ranged from a high of 12 percent of students at private for-profit less-than-2-year institutions to 3 percent of students at private nonprofit 4-year, doctorate-granting institutions. Of BPS:12/14 respondents in the BPS:12/17 sample, 4 percent (1,010) required intensive tracing, while 19 percent of BPS:12/14 nonrespondents (1,700) required intensive tracing. phase, telephone interviewers were available to conduct the survey by telephone if sample members called the study help desk. The production phase began 4 weeks after the start of the early-response phase. During the production phase, outbound calls were made to all sample members with telephone numbers on file. DCIs called sample members to encourage survey completion immediately by telephone, but the web survey was also offered. The early-response phase with the calibration sample began on March 1, 2017. The early-response phase for the main sample and for any calibration sample members who had not yet completed a survey began on May 3, 2017. Survey data collection for all sample members ended on January 8, 2018. Data-collection mailings, e-mails, and text messages. At the beginning of data collection and throughout both the early-response and production phases, staff sent mail and e-mail to fielded sample members inviting them to complete the survey. These messages announced the availability of the web survey and included the survey URL and the sample members' log-in credentials. E-mails also contained a direct link to the survey. The web survey was available 24 hours a day, 7 days a week throughout the entire data-collection period. Text messages were sent to sample members who had previously given permission to be texted. See appendix G for examples of mail, e-mail, and text messages sent to sample members. Telephone contacting. Outbound calling was scheduled based on factors such as prior contact status (e.g., cases that were recently contacted or had never been contacted), refusal status, and appointments set during a prior contact attempt. For sample members with multiple telephone numbers on file, calls were prioritized and reprioritized during data collection, as new numbers were continuously added from contact attempts; batch and intensive-tracing efforts; and updates received through mailings, e-mails, and help-desk calls. Upon successfully reaching sample members, DCIs encouraged them to complete the survey immediately via telephone. Alternatively, a DCI could e-mail secure credentials for the web survey to sample members who preferred to complete the survey later. Abbreviated survey. In October 2017, the abbreviated version of the survey was made available to all nonresponding sample members. The total number of eligible sample members offered the abbreviated survey was about 15,370 (table 17). Of those who were offered the abbreviated survey, almost 29 percent ultimately completed it. The abbreviated survey included fewer questions and therefore required less time-an average of about 10 minutes-than the full survey to Of special interest was the average time it took respondents to complete the survey based on mode of administration, 18 prior-round respondent status, and survey type (i.e., full or abbreviated survey). The following cases were excluded from the survey time burden analysis: partially completed surveys, total time outliers, and cases that required imputation of the time spent for more than two forms due to multiple survey sessions. 19 To detect total time outliers, BPS:12/17 staff grouped cases by survey type (i.e., full and abbreviated) and mode of administration. Before outliers were determined, the distribution of survey times required that the data be normalized using a Box-Cox power transformation (Box and Cox 1964). Cases with total time values smaller than the 25th percentile minus 1.5 times the interquartile range or larger than the 75th percentile plus 1.5 times the interquartile range were deemed outliers and excluded from the subsequent analyses (Tukey 1977). Overall, the outlier detection method led to the exclusion of 4,010 interview cases, representing 18 percent of all completed full and abbreviated surveys as well as partial surveys. Given the parameters for including and excluding cases, approximately 18,530 of the 22,530 total surveys (82 percent) were included in the timing analysis. Table 21 shows the number of cases included and excluded in the timing analysis. 18 Mode of administration typically affects the time to complete a survey; for example, telephone mode tends to take longer than self-administered web surveys because, in telephone mode, each question must be read aloud. 19 Surveys completed in more than one session were included in the timing analysis when possible. This required imputing the time spent on the first form the respondent saw when he or she began a new session to continue the survey. The median time other respondents spent on the same form was used for imputation. To avoid introducing excessive imputation and uncertainty into the timing estimates, cases that required more than two form imputations per respondent were excluded. 13.3 NOTE: Analysis included only completed cases that had two or fewer forms of imputed timing data; partial surveys and outliers were excluded. Sample sizes rounded to the nearest 10. Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17)."}, {"section_title": "Survey Section Timing", "text": "The BPS:12/17 survey consisted of consecutive sections that each contained questions related to a general content area, such as enrollment or employment. The full survey consisted of six content sections: enrollment, education experiences, financial aid, employment, income and expenses, and background. There was also a section in the survey that allowed respondents to select how they would like to receive their incentive for completing the survey. Overall, the employment section took the longest to complete: 8.8 minutes on average. It was followed by the enrollment section, which took 4.7 minutes on average to complete and the background section, which took 3.4 minutes to complete. These sections included the most questions, and enrollment and employment contained looping sections to gather information about additional attendance at the NPSAS institution, postsecondary attendance at institutions other than the NPSAS institution, and employment history since July 2014. Table 23 shows the average time in minutes to complete the BPS:12/17 full survey by mode of administration and section. Similar to the full survey, the BPS:12/17 abbreviated survey was administered in sections, with a reduced number of questions from each full-survey section. The employment section took the longest to complete: 3.5 minutes on average, followed by the enrollment section, which took 3.0 minutes on average to complete. The longer times for these sections could be due to the looping in each of the sections. Although there were fewer questions in each loop, a respondent could go through each loop an unlimited number of times contingent upon their own situation. Table  24 shows the completion time for each section in the abbreviated survey by mode of administration and section. "}, {"section_title": "Coder Form Timing", "text": "Assisted coding systems (coders) were used to identify standardized codes for textstring responses. Each coder required respondents or DCIs to enter a text string, such as a major or occupation, into a text box that would then be used to perform a keyword search linked to an underlying database. The coder returned a list of possible matches for the respondent to review and select from. The BPS:12/17 full survey included four types of coders: school, major, occupation, and zip code of current/most recent employer and primary residence. See section 3.1.2 for a detailed description of each coder. The BPS:12/17 occupation coder (B17DOCC) took the longest amount of time to complete: 103.4 seconds on average. The occupation coder was not a predictive coder, as mentioned in section 3.1.2, so respondents could have needed more time to look for their occupation. Times for the other coders, which were all predictive coders, ranged from 60.1 seconds on average for NPSAS other attendance 1: major 2 coder (B17ANPMAJ201) to 19.5 seconds on average for ZIP code when enrolled at primary school (B17FDISTNC). Table 26 shows the mean and median times in seconds that respondents took to complete each coder in the BPS:12/17 full survey. The 10 noncoder forms with the highest completion times, by form type, are shown in table 27, along with the mean and median timing. "}, {"section_title": "Number of Calls to Sample Members", "text": "On average, DCIs made about 11 calls per sample member during the production phase. The average call counts for completed cases varied by factors such as priorround response status and mode of administration. Table 28 shows the average number of telephone calls by prior-round response status, control and level of institution, and BPS:12/17 mode of administration. "}, {"section_title": "Refusal Conversion", "text": "BPS:12/17 staff integrated refusal conversion techniques into DCI training, revisiting them throughout data collection in quality circle meetings. Project staff sorted sample members who ever refused to be interviewed, or had a gatekeeper refuse on their behalf, into a separate queue managed by a subset of DCIs who had received specialized refusal conversion training. Overall, nearly 7 percent of eligible sample members ever refused or had someone refuse on their behalf; of those who refused, over 20 percent subsequently completed the interview (table 29). BPS:12/14 nonrespondents refused at a higher rate than BPS:12/14 respondents (\u03c72 = 464.9, 56 CHAPTER 3. STUDENT SURVEY DESIGN, DATA COLLECTION, QUALITY CONTROL, AND EVALUATION 2012/17 BEGINNING POSTSECONDARY STUDENTS LONGITUDINAL STUDY (BPS:12/17) DATA FILE DOCUMENTATION p < .001) and of those who refused, BPS:12/14 nonrespondents completed surveys at a lower rate than BPS:12/14 respondents (\u03c72 = 464.9, p < .001)."}, {"section_title": "Table 29. Refusal and refusal conversion rates, by prior-round response status and control and level of institution: 2017", "text": "Prior-round response status and control and level of institution "}, {"section_title": "Evaluation of Responsive Design", "text": "BPS:12/17 tested a responsive design based on the use of an importance score to identify nonrespondents to target for specific interventions. A key requirement of this responsive design was the ability to identify nonrespondents, within categories defining the control and level of institution, who were most likely to contribute to nonresponse bias. The likelihood of nonresponse bias can then be reduced by increasing response among the identified cases. For BPS:12/17, staff targeted nonrespondents for key interventions using a value, called the importance score, which accounted for the current nonresponse bias contributed by that individual, along with the likelihood that individual would respond over the course of the data-collection period with an intervention. This section describes the dimensions of the importance measure used to target specific nonrespondents to reduce nonresponse bias, groups for level and control of institutions, and interventions used to convert nonrespondents. Finally, it presents the results of an evaluation of the responsive design. For this evaluation, staff selected a random subset of all sample members to be set aside as a control sample that was not eligible for intervention targeting. The remaining sample members were referred to as the treatment sample, and the targeting methods were applied to that group. The importance measure. BPS:12/17 employed an importance measure approach to reduce nonresponse bias in survey variables by directing effort and resources during data collection and to minimize the cost of doing so. This approach was designed to meet three conditions: (1) the targeted cases must be drawn from groups that are underrepresented on key survey variable values among those who already responded; (2) their likelihood of participation should not be excessively low or high (i.e., targeted sample members who will never respond no matter the intervention cannot decrease bias; targeting only easy to convert high-propensity cases can potentially increase the bias of estimates); and (3) targeted cases should be numerous enough to impact survey estimates within domains of interest. Although targeting cases based on response propensities alone may reduce nonresponse bias, bias may be unaffected if the targeted cases are extremely difficult to convert and do not respond to the intervention as desired. The importance measure approach targeted cases based on two dimensions: the likelihood that the case could contribute to nonresponse bias if not surveyed, and the likelihood that the case could be converted to a respondent. These dimensions form an importance score, such that Where I is the calculated importance score, is a measure of underrepresentativeness on key variables that reflects the case's likelihood of inducing bias if not converted, and ( ) is the predicted final response propensity, across sample members and data-collection phases with responsive-design interventions. The importance score was determined by the combination of two models: a response propensity model and a bias-likelihood model. Response propensity (P(R)) model. Before BPS:12/17 data collection, a single response propensity model was developed to predict the sample member's likelihood of responding to BPS:12/17 based on BPS:12/14 data and response behavior. The model predicted BPS:12/14 response using the following variables from the base NPSAS:12 study and the BPS:12/14 full-scale data collection: \u2022 responded during BPS:12/14 early completion period; \u2022 NPSAS:12 respondent; \u2022 \"ever refused\" status; \u2022 incentive amount offered in BPS:12/14; \u2022 age; \u2022 gender; \u2022 citizenship; \u2022 control and level of institution; \u2022 BPS:12/14 e-mail count; and \u2022 whether address information was updated in BPS:12/14. BPS:12/14 full-scale data and response were used to create this response propensity model because that study was similar in design and target population to the BPS:12/17 full-scale study and this model was finalized before BPS:12/17 data collection began. Bias-likelihood (U) model. A model to identify cases to be targeted for intervention should use covariates (Z) that are strongly related to the survey variables of interest (Y) to identify sample members who are underrepresented (using a response indicator, R) with regard to these covariates. We then have the following relationships, using a single Z and Y for illustration: Nonresponse bias arises when there is a relationship between R and Y. Just as in adjustment for nonresponse bias (see Little and Vartivarian 2005), a Z-variable cannot be effective in nonresponse bias reduction if corr(Z,Y) is weak or nonexistent, even if corr(Z,R) is substantial. That is, selection of Z-variables based only on their correlation with R may not help to identify cases that contribute to nonresponse bias. The goal is to identify sample cases with Y-variable values that are associated with lower response rates, as this is one of the most direct ways to reduce nonresponse bias in an estimate of a mean. The key Z-variable selection criterion should then be association with Y. Goodcandidate Z-variables would be the Y-variables or their proxies measured in a prior wave and any correlates of change in estimates over time. A second set of useful Z-variables would be those used in weighting and those used to define subdomains for analysis, such as demographic variables. This should help to reduce the variance inflation due to weighting and nonresponse bias in comparisons across groups. Key, however, is the exclusion of variables that are highly predictive of R but quite unrelated to Y. These variables, such as the number of prior contact attempts and prior refusal, can dominate in a model predicting the likelihood of participation and mask the relationship of Z-variables that are associated with Y. Prior to each targeted intervention, a separate logistic regression model was fit for each sector group (described below) in order to predict the survey outcome (R) through the current phase of collection using only substantive and demographic variables and their correlates from NPSAS:12 and the sampling frame (Z), along with select two-way interactions. The goal of this model was not to maximize the ability to predict survey response ( ) but to obtain a predicted likelihood of a completed survey reducing nonresponse bias if successfully interviewed. Because of this key difference, (1 -) was used to calculate a case-level prediction representing bias likelihood, rather than response propensity. The following variables were used in the bias-likelihood model: \u2022 race; \u2022 gender; \u2022 age; \u2022 control and level of institution; 21 \u2022 match to CPS; \u2022 match to Pell Grant system; \u2022 total income; \u2022 parent's highest education level; \u2022 job hours worked per week; \u2022 attendance intensity; \u2022 highest level of education ever expected; \u2022 dependent children and marital status; \u2022 federal Pell Grant amount; \u2022 direct subsidized and unsubsidized loans; \u2022 total federal aid; \u2022 institution aid total; and \u2022 degree program. Groups of level and control of institution. Individuals were targeted within groups of level and control of institution because nonresponse bias patterns differed by these groups. Targeting within these groups was designed to reduce nonresponse bias within specific sectors, rather than across the aggregate target population. The five groupings (table 30) were constructed by first identifying level and control of institutions with historically low response rates, as observed in BPS:12/14 and NPSAS:12, and then assigning those with the lowest participation to their own groups. The remaining pairings of level and control of institution were then combined into groups. The private for-profit institutions (groups C, D, and E) were identified as having low response rates. Public less-than-2-year and public 2-year institutions (group A) were combined because they were similar and because the public less-than-2-year level and control pairing was too small to function as a distinct group. Public 4-year and private nonprofit institutions (group B) remained combined because they have not historically exhibited low response rates. (Nonetheless, cases within this group are still eligible for targeting; the targeting model for group B included level and control of institution as a term to account for differences between the groups.) Targeted interventions. All NPSAS:12 study members who responded to the NPSAS:12 or BPS:12/14 student interviews were initially offered a $30 incentive. Following the $30 baseline offer, two different targeted interventions were used in the responsive-design approach: \u2022 First intervention (incentive boost): targeted nonrespondents were offered an additional $45 over their baseline incentive amount. \u2022 Second intervention (abbreviated survey): Targeted nonrespondents were offered an earlier abbreviated survey. (Note that all nonrespondents were offered an abbreviated survey during the final few weeks of data collection.) Before each targeted intervention, predicted bias-likelihood values were calculated for all survey nonrespondents. The product of the bias likelihood and response propensity were used to calculate the target importance score described above. Nonrespondents with propensity scores outside of high and low cutoffs, determined by a review of the predicted distribution, were excluded as potential targets during data collection. 22 The number of nonrespondents to be targeted was based on data from the BPS:12/14 data-collection period. The targeted nonrespondents were allocated proportionally to sector groups with lower response rates in BPS:12/14, as shown in table 31 for each of the interventions. Again, nonrespondents within the control group could not be targeted for either intervention. 1 Unweighted response rate, excludes ineligible sample members. NOTE: Group A = Public less-than-2-year, Public 2-year. Group B = Public 4-year, non-doctorate-granting; Public 4-year, doctorate-granting; Private nonprofit less-than-4-year; Private nonprofit 4-year, non-doctorate-granting; Private nonprofit 4-year, doctorate-granting. Group C = Private for-profit less-than-2-year. Group D = Private for-profit 2-year. Group E = Private for-profit 4-year. Sample sizes rounded to the nearest 10. Percentages are based on unrounded numbers. Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17). 22 These adjustments helped ensure that overrepresented groups, high-propensity/low-importance cases, and very-difficult-to-convert nonrespondents were not included in the target set of nonrespondents. The bottom 10 percent of propensity scores for each sector group were excluded from targeting. The top percentage of cases to be excluded varied from 4 to 12 percent by sector group. Evaluation of responsive design. Several questions were used to outline the analytical framework for the full-scale experiment. The questions, and the results of the related analyses, are described below. The approximately 1,780 BPS:12/17 sample members who were not fielded in the data collection were not part of the treatment or control groups because they were not exposed to any data-collection protocols. Only fielded, and eligible, individuals were examined for these analyses (n = 33,380), and response rates and nonresponse bias calculations will differ somewhat from other sections in this report. 1. Were response rates improved using the targeted interventions? Response rates were examined by level and control of institution group to determine whether the overall response rates for the treatment and control groups differed significantly. Although the goal of this approach was to minimize bias, and not necessarily to increase response rates, a higher response rate among the treatment group was a necessary requirement for a reduction in bias. table 32 shows the number of respondents and nonrespondents, the weighted response rate, and whether the difference between the treatment group and the control group was significant. The weighted response rates for the treatment group were always larger than those for the control group. They were only significantly higher for group D (58 percent vs. 51 percent) and overall (70 percent vs. 64 percent).  "}, {"section_title": "Did conversion of targeted nonrespondents reduce unit nonresponse bias?", "text": "The effect of the responsive-design approach on unit nonresponse bias was measured by comparing nonresponse bias estimates for the treatment group with nonresponse bias estimates for the control group. The null hypothesis is that there is no difference in unit nonresponse bias between respondents assigned to the treatment group and respondents assigned to the control group. Nonresponse bias was estimated and tested to determine if bias was significant at the p < .05 level for the variables described in chapter 6, section 6.3. The mean and median percent relative bias were computed across the categories of these variables as shown in table 33. Results showed that, before nonresponse adjustments, the mean percent relative bias was not significantly different between the treatment group and the control group for any comparisons, indicating that the inclusion of targeted interventions did not significantly reduce nonresponse bias. This result may have been due to a lack of power from too small a control group, as some of the overlap on the confidence intervals is very small. However, the confidence intervals for the mean percent relative bias between the control and treatment populations after nonresponse weight adjustments do show some significant differences. Significant reductions in mean percent relative bias can be seen for level and control of institution groups A and E. In addition, the overall confidence intervals for the control and treatment groups do not overlap, implying that the treatment group's mean percent relative bias of 2.89 is significantly lower than the control group's mean percent relative bias of 6.80.  1 Respondent and full-sample means are weighted using the student base weight adjusted for unknown eligibility. 2 Full-sample means are weighted using the student base weight, and respondent means are weighted using the base weight adjusted for student nonresponse. 3 Confidence intervals for mean percent relative bias were obtained using Monte Carlo estimates of the standard error. NOTE: CI = confidence interval. Group A = Public less-than-2-year, Public 2-year. Group B = Public 4-year, non-doctorate-granting; Public 4-year, doctorate-granting; Private nonprofit less-than-4-year; Private nonprofit 4-year, non-doctorate-granting; Private nonprofit 4-year, doctorate-granting. Group C = Private for-profit less-than-2-year. Group D = Private for-profit 2-year. Group E = Private for-profit 4-year. Relative bias and effect size are calculated using the weighted difference between respondent and full-sample means. Percent relative bias is calculated as 100 times the ratio of the estimated bias to the weighted full-sample mean. Effect size is calculated as the square root of the sum over categories of the squared differences over full-sample means. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Variables and characteristics that did not meet reporting standards in the control group or treatment group were excluded from summary statistics for both groups. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17)."}, {"section_title": "Evaluation of Student Survey Items", "text": "BPS:12/17 staff evaluated the student survey for data quality and response patterns. The evaluation activities included recoding and upcoding of data collected by survey coders. For further insight into which items may have proved problematic for respondents, staff analyzed help-text access rates, conversion text success rates, and item nonresponse rates."}, {"section_title": "Survey Coders", "text": "To reduce respondent burden and improve data quality, the BPS:12/17 student survey made use of assisted coding systems. These coders, as described in section 3.1.2, created standardized codes from text-string responses for several survey items: additional postsecondary institutions attended between July 2014 and June 2017, majors or fields of study at all postsecondary institutions, occupations, and zip codes of current or most recent employer and permanent address. The following analyses are limited to a sample of respondents who either completed the full survey, completed an abbreviated survey, or completed enough of the full survey to be deemed a final partial complete, meaning they completed the enrollment section of the survey (combined n = 22,530). Major codes included in the analyses were limited to responses from the student survey (not from administrative databases). Recoding. Ten percent of the major and occupation codes were randomly selected from the student survey for recoding, a process in which staff reviewed the codes chosen in the survey alongside the original text strings and determined whether the coder selection most accurately described the text string provided by the respondent. This review was conducted for majors and occupations due to the variability in names of programs of study across institutions and occupation titles given to the same or similar jobs across employers. Because respondents must scroll through a list of returned results on a small screen during web mobile mode administration, agreement rates between modes of administration were also compared to assess the impact that mode of administration may have on data quality. Overall, for the major code review, expert coding staff agreed with the respondent's choice from the survey 95 percent of the time (n = 700). For the occupation code review, coding staff agreed with 90 percent of responses chosen in the interview (n = 1,460). Of the major codes selected for recoding, 2 percent were assigned a different code by expert coders, and 3 percent of major strings provided were considered too vague to code. There were no significant differences between modes of administration for these recodes. Of the occupation codes selected for recoding, expert coders assigned a different occupation code from the one selected in the interview 4 percent of the time. There were no significant differences between modes of administration. Of occupation text strings provided by web mobile respondents, 9 percent were too vague for expert coders to code, significantly greater than the percentage for web nonmobile respondents (5 percent) (\u03c7 2 (1, n = 1,304) = 8.14, p < .01). Due to small cell sizes, no significant differences could be detected between the percentage of vague occupation text strings provided by telephone respondents and the percentage provided by web mobile and web nonmobile respondents. Table 34 shows the rates of recodes for major and occupation coders in the survey by coding system and mode of administration. Upcoding. Upcoding occurred when expert coding staff assigned a code to any text string without a corresponding standardized response. Overall, text strings from the occupation coder were upcoded most frequently at 9 percent. Text strings from the postsecondary institution coder were upcoded at an overall rate of 5 percent, and major text strings were upcoded at an overall rate of 2 percent. Table 35 shows the upcode rate overall and by coding system and mode of administration. "}, {"section_title": "Help Text", "text": "In the BPS:12/17 survey, respondents and DCIs could click a help button provided on each survey screen to obtain question-specific help text with definitions of key terms and phrases used in the screen and any other explanations thought to help clarify and standardize meaning for respondents. In addition, some questions included embedded hyperlinks to the help text on specific terms or phrases in the question itself. Whether accessed through the help button or through a hyperlink, each question had unique help text to assist respondents in completing the screen correctly. Overall, respondents and DCIs accessed help text less than 1 percent of the time. 23 DCIs accessed help text at a rate of 1 percent, while both web mobile and web nonmobile respondents accessed help text at a rate of less than 1 percent. The higher rate of help-text access by DCIs was expected because DCIs were trained to access help text when respondents expressed uncertainty about an answer. The question-level rate of help-text access was analyzed by mode of administration to identify whether specific questions proved to be more difficult to answer by mode. Eighteen survey questions administered to at least 10 respondents had an overall help-text access rate of 1 percent or greater. Fourteen of the eighteen survey questions meeting this help-text access threshold had embedded hyperlinks to helptext. 24 This result suggests that including help-text hyperlinks in survey questions can prompt respondents to review help text on forms that collect difficult or complex information. Table 36 shows the interview questions (forms) with the highest rates of help-text access.  Ever took out undergraduate private loans (B17CEVRPRVLN) had the highest overall help-text access rate at 7 percent. This form asked respondents whether they had taken out private student loans for any undergraduate education they had completed and contained an embedded hyperlink to the help text. Web nonmobile respondents accessed help text for this question 9 percent of the time, while telephone respondents accessed help text 6 percent of the time and web mobile respondents 5 percent of the time. Overall help-text access rates for the remaining 17 forms ranged from 1 to 3 percent."}, {"section_title": "Conversion Text", "text": "To minimize item-level nonresponse in the interview, the survey used conversion text to encourage reluctant respondents to provide a response to critical survey items. When a respondent left a survey question blank and hit \"next\" to move forward, additional text (i.e., conversion text) appeared to emphasize the importance of the question. This additional text encouraged respondents to provide an answer; however, if the answer was still missing, the respondent could proceed to the next question with no additional prompting. Of the questions in the survey, a subset of 28 critical questions included conversion text. To determine a conversion-text success rate, the total number of valid responses provided after conversion text was displayed was divided by the total number of cases in which conversion text was triggered. Overall, conversion text led to a survey response 83 percent of the time. Web nonmobile mode surveys accounted for 43 percent of the total instances in which conversion text was triggered and 45 percent of the total converted responses. Web mobile mode surveys accounted for 40 percent of the total instances in which conversion text was triggered and 42 percent of the total converted responses. The remaining 17 percent of total instances in which conversion text was triggered occurred in telephone surveys, accounting for 13 percent of the total converted responses. Among the 28 critical questions that included conversion text, analysis excluded two items in which conversion text was available on the form but was never triggered in any mode. Of the remaining 26 critical questions, the majority (23) did not display a \"don't know\" option when conversion text was triggered. Three questions related to income from the prior year did display a \"don't know\" option due to the sensitive nature of the questions. Table 37 shows conversion rates for the 23 critical questions that did not display a \"don't know\" option when conversion text was triggered. For these questions, total conversion rates ranged from 63 to 100 percent, with only five items resulting in a conversion rate lower than 80 percent. These five questions required respondents to recall specific amounts, dates since July 2014, or location (B17AOTENRL01, B17DEMPZIP, B17DWRKHRS01, B17DWKHREN01, and B17COTGRTAMT   Three of the 23 critical questions that did not display a \"don't know\" option triggered conversion text at least 200 times: other school attendance 1: attendance from July 2014 to June 2017 (B17AOTENRL01), primary employer: employer ZIP code (B17DEMPZIP), and employer 1: hours per week worked while not attending school (B17DWRKHRS01). Conversion rates were significantly different across the three modes of administration for B17AOTENRL01 (\u03c7 2 (2, n = 516) = 129.40, p < .001) and B17DEMPZIP (\u03c7 2 (2, n = 225) = 10.24, p < .01). 25 Specifically, the conversion rates in telephone mode were observed to be significantly lower than those for web nonmobile mode for B17AOTENRL01 (\u03c7 2 (1, n = 284) = 76.70, p < .001) and B17DEMPZIP (\u03c7 2 (2, n = 116) = 4.56, p < .05), as well as significantly lower than web mobile mode for B17AOTENRL01 (\u03c7 2 (1, n = 310) = 111.29, p < .001) and B17DEMPZIP (\u03c7 2 (2, n = 118) = 4.23, p < .05). No significant differences were observed between the conversion rates for web nonmobile and web mobile modes of administration. These questions required respondents to recall information like dates of enrollment, location of current or most recent employer, and number of hours worked, which could have been difficult for the respondent if they were not currently enrolled at the institution in question or currently working for the employer in question. Table 38 shows conversion rates for the three critical questions that did display a \"don't know\" option when conversion text was triggered. Of these three questions, total conversion-text rates ranged from 58 to 100 percent. Two of these survey questions also triggered conversion text over 200 times: Parents' (or guardians') income in calendar year 2016 (B17EPARNC) and respondent's income in calendar year 2016 (B17EINCOM). The conversion rate for B17EINCOM was significantly lower when administered by DCI (50 percent) compared to web nonmobile administration (68 percent) (\u03c7 2 (1, n = 159) = 5.00, p < .05). This lower conversion rate for telephone mode may be attributed to the presence of a DCI and the sensitive nature of the income information being requested. "}, {"section_title": "Item-Level Nonresponse", "text": "The analysis of the rate of nonresponse for individual items in the student survey identified potentially burdensome or sensitive questions. BPS:12/17 staff calculated item nonresponse rates from the student survey for all items administered to 10 or more respondents. The following analysis includes all items with an overall nonresponse rate of 10 percent or greater. Overall, only five items met this threshold. Table 39 summarizes these results. The item with the highest overall nonresponse rate was estimated amount of undergraduate private loans borrowed in 2016-17 academic year (B17CPRVEST). Of the 10 respondents who received this item, 17 percent did not provide an answer. This item was administered to only a small number of respondents because it was used as a nonresponse conversion for respondents who, on a preceding question, had not provided an exact amount of private student loans borrowed, and displayed a \"don't know\" option. Therefore, the higher nonresponse rate for this item was likely due to respondents' inability to recall this level of detail about their borrowing behavior. Other items with a nonresponse rate greater than 10 percent include employer 4: worked at prior employer between July 2014 and June 2017 (B17DPRIEMP04), which had a nonresponse rate of 16 percent; not working and actively looking for work: July 2014-June 2017 (B17DLKWKSTG), which had a nonresponse rate of 12 percent; estimated total amount borrowed in undergraduate private loans (B17CTLPRVEST), which had a nonresponse rate of 12 percent; and other school attendance 4: degree/certificate not yet awarded (B17AOTDGNO04), which had a nonresponse rate of 10 percent. Item-level nonresponse rates were also examined by mode of administration for the five items with 10 percent or more missing data. Higher rates of nonresponse were observed in telephone mode (21 percent) than in either web mobile mode (12 percent) (\u03c7 2 (1, n = 4,304) = 43.65, p < .001) or web nonmobile mode (10 percent) (\u03c7 2 (1, n = 10,952) = 91.83, p < .001) for not working and actively looking for work: July 2014-June 2017 (B17DLKWKSTG). Web mobile mode nonresponse rates for this item were also significantly higher than web nonmobile mode rates (\u03c7 2 (1, n = 5,476) = 13.16, p <.001). None of the other items had adequate cell sizes for comparison across all three modes of administration. Project staff collected CPS data, which come from the FAFSA, for the BPS:12/17 sample for the 2014-15, 2015-16, 2016-17, and 2017-18 financial aid years. The variable CPS ID-the sample member's SSN concatenated with the first two letters of their last name-was used to match BPS:12/17 and CPS records. Sample members without available SSNs were not included in the match. Any SSNs that had been obtained since the last match were included in later matches. CPS data are available directly from CPS for a limited time each year, after which they are available only from NSLDS. For the 2016-17 and 2017-18 financial aid years, these data were collected directly from CPS. For financial aid years 2014-15 and 2015-16, the required information was no longer available directly from CPS and was obtained from NSLDS. FAFSA data from NSLDS needed to be parsed and reformatted to match the CPS file format. Obtaining information from NSLDS provided the opportunity to also obtain additional FAFSA data for BPS sample members who OVERVIEW OF ADMINISTRATIVE RECORDS MATCHING 2012/17 BEGINNING POSTSECONDARY STUDENTS LONGITUDINAL STUDY (BPS:12/17) DATA FILE DOCUMENTATION applied late in the application cycle or whose application data were updated after the last direct CPS match was conducted. National Student Loan Data System. Student-level data on Pell Grants and federal student loans were obtained by matching sample members to the NSLDS database. In a cooperative effort, project staff and the U.S. Department of Education initiated a match between BPS records and the NSLDS database three times. The first and second matches, during data collection,provided data for preliminary analyses, and the third match occurred after the end of data collection to retrieve the most current NSLDS data. As with CPS, sample members missing SSNs were not part of the match. The BPS sample member had to have at least one valid grant or loan record in the NSLDS database for a successful match. The NSLDS Pell Grant and loan files included information on the year of interest and a complete federal grant and loan history for each student. All NSLDS data transfers used a passwordprotected NCES system transmitting over an encrypted SSL connection. National Student Clearinghouse. The NSC StudentTracker service provided enrollment data for the BPS:12/17 sample members. This administrative record match provided information on institutions attended, enrollment dates, and degree completions. An individual student record would match with NSC only if the student's institution was an NSC participant. 26 StudentTracker data were requested toward the end of data collection to ensure availability of the most up-to-date student identifying data for the match. All files were encrypted and transmitted over encrypted Secure File Transfer Protocol connections. Personally identifying data used for the match included sample member name, SSN, and date of birth. Veterans Benefits Administration. A file match with VBA was performed in collaboration with VBA staff to identify veterans, amounts of federal veterans education benefits, and any associated enrollment information. After the end of data collection, BPS staff provided a file containing SSN, name, and date of birth to VBA for data matching. The match used SSN as the primary identifier, with the other fields used to identify the proper person in rare cases of multiple matches. As with the NSLDS file matching, all data transmission used an NCES secure file transfer system."}, {"section_title": "Administrative Records Matching Outcomes", "text": "Central Processing System. Table 40 summarizes the results of student data  matching for financial aid years 2014-15 through 2017-18. The table shows the total  BPS:12/17 sample with records sent to CPS for matching (i.e., BPS:12/17 student records that included an SSN and last name were submitted to CPS for matching) and records matched to CPS by year. Sample members must have filed a FAFSA in that year in order for their records to match. The match rates for 2014-15, 2015-16, 2016-17, and 2017-18 were 36 percent, 29 percent, 21 percent, and 16 percent, respectively. Fellowship and assistantship amounts, which are usually not need based and do not require the completion of federal financial aid forms, are not included in the CPS data. National Student Loan Data System. NSLDS matching only returned records of sample members who, at some point in time, had received Pell Grant or federal student loan funding. A sample member had to have an apparently valid SSN in order to be sent for the match with NSLDS. Table 41 shows the overall NSLDS match rates for sample members. In this table, a match indicates that a student had at least one loan or Pell Grant, although not necessarily during 2017-18. Students who did not have at least one loan or Pell Grant are included in the category \"Did not match.\" The NSLDS match yielded loan matches for 24,270 sample members (70 percent of sample members). The match yielded Pell Grant matches for 24,030 sample members(69 percent of sample members). National Student Clearinghouse. The NSC match returned enrollment and degree records for the 2014-15, 2015-16, 2016-17, and 2017-18 academic years. An individual student record match was possible only if an institution the student attended was an NSC participant. NSC matches for sample members included the institution in which they were sampled for NPSAS and any other participating institutions they had attended since the 2011-12 academic year. Table 42 shows NSC match rates. Of the total sample members, approximately 30,250 (85 percent) matched to NSC. Sample members who did not have any NSC records for the NPSAS institution or others attended since the 2011-12 academic year are included in the category \"Did not match.\" Veterans Benefits Administration. Veterans education benefits information was obtained for 2,200 of the sample members (6 percent), as shown in table 43. No VBA information was available for the remaining sample members (included in the category \"Did not match\"). "}, {"section_title": "Administrative Records Matching Evaluation", "text": "Several checks were performed to verify the quality of administrative data. File layouts and code to read in the files were checked to make sure they were current and accurate. For example, with NSLDS, the file received must have a header and a footer; otherwise, the file and data are incomplete. If a source provided the personally identifying information from their database, names and dates of birth were compared to the study database to make sure the data were for the correct person. If the names and dates of birth did not match, the case was excluded from the administrative file. Project staff examined the files individually by running basic summary statistics such as the number of records and the range of values (e.g., dates and amounts) to check for potential outliers or abnormalities. Files were then checked to see if they related to one another as expected. For example, students who received veterans education benefit payments were expected to have at least one military service record on file. In the event that BPS staff found any disagreement or items that required clarification, they worked with VBA staff who extracted the data to correct the information or to understand why such occurrences existed."}, {"section_title": "Chapter 5. Data File Processing and Preparation", "text": "BPS:12/17 student-level data are compiled from student surveys and matches to governmental and administrative databases. These files are fully documented and available to researchers as a set of restricted-use microlevel data files. The public can generate tables of estimates and simple regressions based on restricted-use data via PowerStats and other publicly facing web tools available on the NCES website, https://nces.ed.gov/datalab/. This chapter provides details on the contents of the restricted-use BPS:12/17 files, processing of the survey data files, statistical imputations, and creation of derived variables."}, {"section_title": "Overview of the BPS:12/17 Study Files", "text": "Complete data for BPS:12/17 are contained in restricted-use files and documented in detail in the associated codebooks. The restricted-use files (table 44) are available to researchers who have applied for and received authorization from NCES to access those files. Researchers may obtain authorization by contacting the Institute of Education Sciences (IES) Data Security Office. 27 The primary analysis file (or derived file) for BPS:12/17 contains data for approximately 22,530 respondents and includes more than 1,700 variables. NSC and VBA data were also used to create derived variables, in combination with data from other sources (the student survey, CPS, and NSLDS). The NSC and VBA data files are not available as source files.   Table 44. BPS:12/17 restricted-use file, description, and file path: 2018-Continued   Restricted-use file  Description  File path  NSLDS outstanding  principal balance Contains loan-level outstanding principal balance data received from NSLDS for respondents who received federal loans as of early 2018. This is a history file with separate records for each balance update per loan. /data/source/bps17nsldsopb/bps1 7nsldsopb_datafile.csv"}, {"section_title": "NSLDS loan repayment history", "text": "Contains loan-level repayment data received from NSLDS for respondents who entered repayment and made any payments on their federal loans as of early 2018. This is a history file with separate records for each payment made on a loan. /data/source/bps17nsldsrpmthis/b ps17nsldsrpmthis_datafile.csv"}, {"section_title": "NSLDS loan repayment plan", "text": "Contains loan-level repayment plan information received from NSLDS for respondents who entered repayment on federal loans as of early 2018. This is a history file with separate records for each change to a loan's repayment. /data/source/bps17nsldsrpmtplan /bps17nsldsrpmtplan_datafile.csv"}, {"section_title": "NSLDS loan to IDR application data", "text": "Contains loan-application-level repayment plan data, where each observation represents a successful enrollment, recertification, or recalculation on an IDR plan resulting from the IDR application identification number linked to the observation. /data/source/bps17nsldsloanidrap pl/bps17nsldsloanidrappl_datafile .csv"}, {"section_title": "NSLDS IDR plan application data", "text": "Contains student-application-level data received from NSLDS for respondents who applied for any federal income-driven repayment plans for federal loans as of early 2018. Each observation represents an IDR plan application. /data/source/bps17nsldsidrappl/b ps17nsldsidrappl_datafile.csv NSLDS FAFSA history Contains student award year-level data from FAFSA, stored and obtained from NSLDS, for approximately 19,250 matched study members as of early 2018. This file includes one record for each year in which a respondent filed a FAFSA between the 1995 and 2019 federal award years. Each record includes income, expected family contribution, and select demographic information reported on the application. /data/source/bps17nsldsfafsa/bps 17nsldsfafsa_datafile.csv NSLDS award origin Contains student award year-level data on federal Direct Loans awarded to 15,790 study members as of early 2018. This file includes one record for each student and year during which the student was awarded a federal Direct Loan between 2012 and 2018. The file is an aggregation of loan-level data reported by institutions to the U.S. Department of Education's COD system and provides information on loan amount eligibility and the academic year periods associated with loans disbursed during the award year. /data/source/bps17nsldsaward/bp s17nsldsaward_datafile.csv NSLDS enrollment status Contains student-school-program-level enrollment information from NSLDS for 12,020 respondents. These data are incomplete in the first 3 years of the study, but are more reliable from the 2015 through 2017 academic years. This file includes one record for each program and enrollment status change for a student as reported to NSLDS.  "}, {"section_title": "Post-Data-Collection Editing", "text": "Throughout data collection, the interview data were reviewed weekly for the presence of data anomalies (e.g., data where not expected, high rates of missing data). All data were investigated by the interview data file team to determine the cause of the issue. If data anomalies were the result of consistency-code implementation, the program that implements consistency codes for missing data was reviewed and revised accordingly. If data anomalies were the result of the functionality of the interview (i.e., routing, validations, question wording, response options), instrumentation staff were alerted and necessary adjustments were made. Staff performed quality control checks on all student survey data to ensure their quality and accuracy. As one of the checks, staff examined all missing data to assign specific codes indicating why the data were missing (table 45). For example, staff examined skip-pattern relationships in the interview database by methodically crosstabulating gate items with their associated nested items. (An example of a gate item is a question asking whether the respondent was employed in a specific year, with nested items asking for details about that employer.) In many instances, gate-nest relationships spanned multiple levels within the instrument (i.e., items nested within a gate question could themselves also be gate items for others). Consequently, validating the complex series of gate-nest relationships often required several iterations and a series of multiway cross-tabulations to ensure that the final data adhered to the item routing that respondents experienced when navigating the interview. Cleaning and editing of the BPS:12/17 data files was a multistage process that consisted of the following: 1. As a first step, project staff replaced all blank values in the survey data files with the appropriate initial -9 data code. Then, one-way frequency distributions of every variable were reviewed to confirm that no missing or blank values remained. Labels of expected values were assigned to assist in the identification of anomalous categorical frequencies. Staff investigated anomalous data patterns and corrected as necessary. 2. Survey source code written by instrument programmers and survey logic documented in flowcharts were used to identify legitimate skips in the survey data. Staff defined gate-nest question relationships and examined data for adherence to logic established in the survey design. When an item was skipped in the survey, the -9 value of missing-response not provided was replaced with -3 for skipped-not applicable or -7 missing-not included in abbreviated interview. If the gate was left blank, a -9 value in the nested item was replaced with -4 missing-unable to determine applicability. If a \"yes\" response was provided on at least one item on a Likert grid form in the group and all other items were missing, the -9 was replaced with -5 missing-implied no/zero. 3. Staff evaluated cross-tabulations of each gate-nest combination and investigated high numbers of nonreplaced -9 codes to ensure conditional response integrity. If a -9 value was found where data were expected, and the result of the investigation revealed an error in instrument routing, the -9 was replaced with -8 missing-due to error. Staff further checked nested values to find instances in which a legitimate skip code overwrote valid response data. This typically occurred if a respondent answered a gate question and the appropriate nested items, but then backed up within the survey to change the value of the gate, leading to an alternate path of nested items. Responses to the first nested items remained in the database, which required further examination and editing. In this scenario, editing would involve removing from the data file the values provided in response to the first nested items prior to backing up. This is done to maintain conditional response integrity throughout the data. 4. Expert coders reviewed IPEDS, occupation, major, and zip codes (including the strings that interviewers or respondents could not resolve during the interview) and assigned new codes when necessary. Staff reviewed string data collected in occupation title and occupation duty variables, as well as major variables, and sanitized strings by removing any information that could be used to identify respondents. See section 3.1.2 for more information on coding systems. 5. Staff performed logical recodes of the interview data when the value of missing items could be determined from answers to previous questions. If respondents left an interview prior to completion but previous responses allowed for logical recodes of post-breakoff items, logical recodes were used. For example, if the respondent indicated they were currently enrolled in a degree program, the instrument skipped the question about degree completion. During logical recodes, this sequence would generate a value of \"no\" instead of a \"not applicable\" value. While cleaning data, staff documented question wording, response options, logical recoding, and the \"applies to\" text for each delivered variable from the interview data collection. For the interview questionnaire, see the student instrument facsimile in appendix E."}, {"section_title": "Data Perturbation", "text": "In preparing data files for release, NCES takes steps to minimize the likelihood that individual students participating in the study can be identified, including a formal disclosure risk analysis. Every effort is made to protect the confidentiality of information about specific individuals, including performing data swapping procedures on BPS:12/17 data to minimize disclosure risk. In data swapping, the values of the variables being swapped are exchanged between carefully selected pairs of records: a target record and a donor record. All cases were eligible for swapping. Swapping variables were selected from questionnaire and administrative record items. Perturbation was carried out through specific targeted, but undisclosed, swap rates. Because perturbation of the BPS:12/17 data could have changed the relationships between data items, an extensive data-quality check was carried out to assess and limit the impact of swapping on these relationships. For example, a set of correlations for a variety of variables was evaluated pre-and posttreatment to verify that the swapping did not greatly affect the associations. Therefore, the modifications used to reduce the likelihood that any respondent could be identified in the data generally did not affect the overall data quality. The swapping procedures, which the IES Disclosure Review Board reviewed and approved, preserved central tendency estimates but may have resulted in slight increases in nonsampling errors."}, {"section_title": "Statistical Imputations", "text": "Staff imputed missing data for all variables included in the restricted-use derived file (also used in PowerStats) in accordance with mass imputation procedures described by Krotki, Black, and Creel (2005). After filling in missing data for cases where values could be deduced with certainty based upon logical or mathematical relationships among observed variables (logical imputation), 28 staff used the weighted sequential hot deck (WSHD) method to replace missing data by imputing plausible values from statistically selected donor cases (stochastic imputation) (Cox 1980;Iannacchione 1982). The first stage in the imputation procedure was to determine the pattern and level of missingness and produce an initial set of imputations. Depending on patterns of missing data, some variables that were related substantively and required imputation were grouped into blocks (vectors), and the variables within a block were imputed simultaneously (vector imputation). Then, variables and vectors were prioritized for imputation based upon their level of missing data: variables and vectors with low levels of missingness were imputed before variables where the rate of missingness was greater. That is, variables with smaller amounts of uncertainty due to imputation were imputed first, and variables with larger amounts of uncertainty due to imputation were imputed next. For each variable and vector, staff identified imputation classes from which donor cases for the hot deck procedure would be selected. To develop those classes, they used nonparametric classification or regression trees to identify homogeneous subgroups of item respondents (Breiman et al. 1984), using complete response variables and any previously imputed variables as possible predictor variables. Within these classes, WSHD was used to select donors. In the second stage of imputation, for each variable or vector, in the same sequence as in the first stage, the missingness was reintroduced and the missing items were reimputed. This time, all complete response variables and all the other imputed variables on the dataset were available to form the imputation classes. To improve imputation quality, the procedure described above, using trees and WSHD, was combined and implemented with the cyclic p-partition hot deck (Marker, Judkins, and Winglee 2002) technique (cycling), as discussed in Judkins (1997). This imputation approach reinforces existing patterns within the observed data. This is an iterative process, and typically the result of cycling is a convergence to plausible values and maintenance of relationships that already exist. For BPS:12/17, there were five iterations, which improved quality without significantly slowing down the imputation process. To minimize the potential error due to imputation, staff performed quality checks throughout the imputation process. In particular, staff compared the distributions of the observed, imputed, and complete (observed and imputed) data to screen variables for further investigation. For example, the distributions of observed income and imputed income differ because the missing data are primarily for students who do not apply for federal financial aid. Those who do not apply tend to have a higher income than those who do apply. Consequently, the imputed income distribution is higher than the observed income distribution. In addition, staff verified that the distributions within imputation classes were similar for the observed and imputed data and concluded that the complete (observed and imputed) distribution for income was reasonable. Selected results from the imputation process are shown in appendix H, which shows the item response and nonresponse rates and pre-and postimputation distributions for each variable subject to imputation for all students."}, {"section_title": "Derived Variable Construction", "text": "Analysts derived the analytic variables by examining data available from the various data sources, prioritizing the data sources on an item-by-item basis, and reconciling discrepancies within and between sources. In some cases, staff created derived variables by simply assigning a value from the available source with the highest priority. (For a listing of the analysis variables derived for BPS:12/17, see appendix I.) Details about the creation of each variable appear in the variable descriptions contained in the PowerStats documentation and codebooks for the restricted-use files. Enrollment and attainment estimates using BPS:12/17 derived variables are higher than those in previous iterations of BPS, including those produced in BPS:12/14. Matching to administrative sources (see chapter 4) resulted in higher estimates of student enrollment. Administrative data matches identified additional student-school pairs not observed in the BPS:12/14 study. Including only sample members who were respondents in both BPS:12/14 and BPS:12/17, staff identified approximately 5,750 more student-school pairs in BPS:12/17 than in BPS:12/14, an increase of 23 percent. While 76 percent of students had the same number of student-school pairs across the two collections, the remaining students gained an average of 1.2 schools based on BPS:12/17 enrollment data. As a result of increased enrollment estimates, other estimates may be impacted (e.g., transfer, number of institutions attended, employment while enrolled)."}, {"section_title": "Chapter 6. Weighting and Variance Estimation", "text": "This chapter provides information about the weighting procedures for BPS:12/17. The development of statistical analysis weights for the BPS:12/17 sample is discussed in section 6.1. Section 6.2 gives weighted and unweighted response rates. Section 6.3 discusses the accuracy of BPS:12/17 estimates for precision and the potential for nonresponse bias. Analysis procedures that can be used to produce unbiased estimates of sampling variances are discussed in section 6.4, including variances computed using Taylor-series and bootstrap replication techniques. This section also describes how the Taylor-series strata, primary sampling unit (PSU) variables, and bootstrap replicate weights were constructed. The use of weights is essential to produce estimates that are representative of the target population of FTB undergraduate students from the NPSAS:12 sample. An analysis weight should be used to produce survey estimates. When testing hypotheses (e.g., conducting t tests, regression analyses, etc.) using weighted data from a study such as BPS:12/17 that has a complex design, analysts also should use methods to properly estimate variances. Two such methods are the Taylor-series linearization method and bootstrap replication. PSU and stratum identifiers are provided in the data file for use with the Taylor-series method with or without the correction for assuming a finite population, and bootstrap replicate weights are provided for use with the bootstrap replication procedure."}, {"section_title": "Analysis Weights", "text": "Because the BPS:12/17 sample members are a subset of the NPSAS:12 sample, the weights for analyzing the BPS:12/17 data were derived from the NPSAS:12 weights. The NPSAS:12 student design weights for BPS:12/17 sample members were adjusted to account for subsampling (see section 2.3) and for unknown student eligibility and nonresponse. They were also calibrated to weighted estimates obtained from NPSAS:12 and population estimates obtained from the IPEDS:2010-11 Fall Enrollment file (EF2011A) and the IPEDS:2011-12 Student Financial Aid and Net Price file (SFA1112). IPEDS data files can be downloaded from the online IPEDS data center at https://nces.ed.gov/ipeds/use-the-data. The BPS:12/17 sample was a subset of the BPS:12/14 first follow-up sample in which individuals who were deceased or determined not to be FTB students were excluded. Because some BPS:12/17 respondents did not respond in BPS:12/14, two analysis weights were constructed for BPS:12/17. The first weight, the crosssectional weight, was created for all BPS:12/17 survey respondents regardless of their NPSAS:12 or BPS:12/14 response status. The second weight, referred to as the panel weight, was created for all BPS:12/17 respondents who also responded in BPS:12/14. 29 Each weight component represents either a probability of selection or a weight adjustment. Using a weighting methodology described by Folsom and Singh (2000), staff computed all nonresponse and poststratification adjustments using the procedure WTADJUST in SUDAAN (RTI 2012). The WTADJUST procedure uses a constrained logistic model to predict response using bounds for adjustment factors and bounds on variance inflation. A key feature and advantage of this procedure is that the weight adjustments and weight trimming and smoothing are all accomplished in one step. Staff trimmed extremely large or extremely small weights by specifying minimum and/or maximum values prior to nonresponse and poststratification adjustments. In general, these bounds were set equal to the median value plus or minus 2.5 times the interquartile range, where the median and interquartile range were defined for each level-of-institution sector. Staff then set upper and lower bounds on the weightadjustment factors calculated by the weight-adjustment procedure. For the nonresponse adjustments, staff initially set the lower bound at 1; for the poststratification adjustment, staff initially set the lower bound at 0.01. During model refinement, staff ran the WTADJUST procedure with no upper limit. Once staff achieved convergence of the model, they tightened weight-adjustment bounds to reduce the magnitude of the weight-adjustment factors and the unequal weighting effects (UWEs). The final minimum weight-adjustment bound for all nonresponse adjustments was 1. The final nonresponse-adjustment models were unbounded above because the maximum adjustment factors were not found to be extreme. The poststratification adjustment had a lower bound of 0.00001 and was unbounded above in order to achieve convergence. In this way, staff controlled the extreme weights and reduced the design effect due to unequal weighting. The WTADJUST procedure is designed so that the sum of the unadjusted weights for all eligible units equals the sum of the adjusted weights for the respondents. The exact formula for the weight-adjustment factors calculated by the SUDAAN WTADJUST procedure can be found in the SUDAAN User's Manual (RTI 2012)."}, {"section_title": "Initial Base Weight for BPS:12/17", "text": "The 2011-12 National Postsecondary Student Aid Study (NPSAS:12) Data File Documentation (Wine, Bryan, and Siegel 2014) describes the development of the NPSAS:12 weights. The analysis weights compensated for the unequal probability of selection of institutions and students in the NPSAS:12 sample. The first eight weight components were 1. institution sampling weight (WT1); 2. institution subsampling weight (WT2); 3. institution multiplicity adjustment (WT3); 4. institution nonresponse adjustment (WT4); 5. institution poststratification adjustment (WT5); 6. student sampling weight (WT6); 7. student multiplicity adjustment (WT7); and 8. student unknown eligibility adjustment (WT8). Of the 20,574 FTB students who did not respond to the NPSAS survey or were classified as nonstudy members, 7,088 were subsampled for inclusion in the BPS:12/14 sample as described in section 2.3. This resulted in an additional adjustment (WT9) to account for subsampling of students for inclusion in BPS:12/14. During BPS:12/14, some BPS:12/14 sample members were determined to be ineligible, and an unknown eligibility adjustment (WT10) was developed. The BPS:12/17 base weight was formed as the product of each of these adjustment factors. Specifically, for each student, the BPS:12/17 base weight was computed as B17WT1 = WT1 \u00d7 WT2 \u00d7 WT3 \u00d7 WT4 \u00d7 WT5 \u00d7 WT6 \u00d7 WT7 \u00d7 WT8 \u00d7 WT9 \u00d7 WT10 This base weight was used for both the cross-sectional and panel analysis weights. WEIGHTING AND VARIANCE ESTIMATION 2012/17 BEGINNING POSTSECONDARY STUDENTS LONGITUDINAL STUDY (BPS:12/17) DATA FILE DOCUMENTATION"}, {"section_title": "Unknown Eligibility Adjustment (B17AWT1 and B17AWT2)", "text": "As stated above, the unknown eligibility adjustment due to ineligible individuals identified as part of BPS:12/14 was included in the BPS:12/17 base weight. As part of BPS:12/17 data collection, staff could not determine final eligibility status for the double nonrespondents (i.e., those who were nonrespondents in both NPSAS:12 and BPS:12/14) who were also nonrespondents in BPS:12/17. In order to adjust for this unknown status within the double nonrespondent group, staff estimated the rate of eligibility of unknown status to be the same among students of known status. Table  46 shows the weight-adjustment factors applied to the students with unknown eligibility status, referred to as B17AWT1. For the students known to be eligible, the weight-adjustment factor was set equal to 1. This unknown eligibility adjustment is the same for both the cross-sectional and panel weights. Summary statistics of the weight-adjustment factors follow: \u2022 minimum: 0.38; \u2022 median: 1.00; and \u2022 maximum: 1.00. After the unknown eligibility adjustment was applied to all individuals with unknown eligibility, a second adjustment was made to the known ineligible individuals, specified as B17AWT2. Their weights were adjusted upward to increase their total weight to be equal to their original weight plus the amount of weight reduced by the unknown eligibility adjustment. This was done so that the known ineligibles represented the total weight for known and unknown eligibility adjustments because the ineligible individuals were to be used in the later poststratification step to adjust the control totals."}, {"section_title": "Nonresponse Adjustment", "text": "The next adjustment was for study nonresponse. staff adjusted the student weights for nonresponse in three stages-first, inability to locate the student, then survey refusal, and finally, other nonresponse-because the predictors of response propensity were potentially different for each of these nonresponse outcomes. In addition, the three nonresponse-adjustment models differed for the cross-sectional and panel weights, resulting in six total adjustments. For each of the nonresponseadjustment models, they included the 35,170 students who were eligible and were not deceased."}, {"section_title": "Student not located adjustments (B17AWT3 and B17BWT1", "text": "). The first type of adjustment for student nonresponse was an adjustment for the inability to contact the student during data collection (\"not located\"). Independent model variables were those that (1) were considered to be predictive of response status, (2) were nonmissing for both study respondents and nonrespondents, and (3) included variables from the NPSAS:12 nonresponse-adjustment models. Variables in the nonresponse modeling included all of the following predictor variables, 30 as well as certain important interaction terms: \u2022 control and level of institution attended (from NPSAS:12); \u2022 region of institution attended (categorical-from NPSAS:12); \u2022 base-year enrollment of institution attended from IPEDS 2011-12 file (categorical-from NPSAS:12); \u2022 age group (categorical-from NPSAS:12); \u2022 cumulative Pell Grant amount (categorical-through June 2017); \u2022 cumulative Direct Loan amount (categorical-through June 2017); \u2022 cumulative Parent Loan for Undergraduate Students (PLUS) amount (categorical-through June 2017); \u2022 defaulted on a federal loan status (through June 2017); \u2022 institution aid receipt 31 (yes/no-from NPSAS:12); and \u2022 state aid receipt (yes/no-from NPSAS:12). To detect important interactions for each of the nonresponse models, a Chi-squared automatic interaction detection (CHAID) analysis was performed on the predictor variables (Kass 1980). CHAID is a hierarchical clustering algorithm that successively partitions individuals according to categorical predictors for a categorical dependent variable. The algorithm begins with all study individuals as a whole and cycles over each predictor, finding for each predictor an optimal partition of the individuals according to its levels. Staff retained the most significant optimal partition and applied the CHAID algorithm to the members of that partition to find further partitions using the remaining predictors. The algorithm was stopped after a specified number of partitioning steps or if the algorithm failed to find statistical significance among any of the partitions at a given step. The CHAID analysis divided the data into segments that differed with respect to the response variables (located, refusal and other nonresponse). CHAID was run for up to three segments, resulting in the identification of two-way and three-way interactions. The interaction terms (CHAID segments) identified were treated as additional candidate predictor variables. Candidate predictor variables that impeded the creation of a convergent model were dropped from the final model. Staff computed the weight adjustments using SUDAAN's WTADJUST procedure as described in section 6.1. Table 47 shows the final predictor variables used in the model to determine weight adjustments and the average weight-adjustment factors resulting from these variables for the cross-sectional weight. The cross-sectional student not located weightadjustment factors have the following characteristics: \u2022 minimum: 1.01; \u2022 median: 1.14; and \u2022 maximum: 3.66. The final lower bound was 1.0 and the final upper bound was unbounded to achieve convergence for this weight adjustment.   Table 47. Cross-sectional weight-adjustment factors for student not located adjustment: 2017-Continued"}, {"section_title": "Model predictor variables", "text": "\n"}, {"section_title": "Number located", "text": ""}, {"section_title": "Weighted response rate", "text": "Average weightadjustment factor (B17AWT3) Defaulted on federal loans as of 6/31/2017, received institutional aid, attended a public 2-year institution or a private for-profit less-than-2-year institution 50 60.7 1.61 Defaulted on federal loans as of 6/31/2017, received institutional aid, attended a nonprofit 4-year, nondoctorate-granting institution; a public 4-year, doctorate-granting institution, a public 4-year, nondoctorate-granting institution, a private nonprofit lessthan-4-year institution, a private for-profit 2-year institution, or a private for-profit 4-year institution 330 76.5 1.34 Defaulted on federal loans as of 6/31/2017; received institutional aid, attended a private nonprofit 4-year, doctorate-granting institution 100 93.0 1.07 Defaulted on federal loans as of 6/31/2017, institutional aid status unknown, in the Mideast, New England, Southeast, Far West, or Plains region 320 68.5 1.50  Table 48 shows the final predictor variables used in the model to determine weight adjustments and the average weight-adjustment factors resulting from these variables for the panel weight. The panel student not located weight-adjustment factors have the following characteristics: \u2022 minimum: 1.01; \u2022 median: 1.15; and \u2022 maximum: 3.98. The final lower bound was 1.0 and the final upper bound was unbounded to achieve convergence for this weight adjustment.    \nAverage weightadjustment factor (B17AWT4) Pell amount received $13,089 or more; PLUS amount received $34,350 or more; Attended a private nonprofit 4-year, non-doctorate-granting institution; a private nonprofit 4-year, doctorate-granting institution; a public 4-year, doctorate-granting institution; a public 4-year, non-doctorate-granting institution; a public 2-year institution; a private for-profit 2-year institution; a private nonprofit less-than-4-year institution; or a private forprofit less-than-2-year institution 100 100.0 1.00 Pell amount received $13,089 or more, PLUS amount received $34,350 or more, Attended a private for-profit 4-year institution 60 99.3 1.01 1 Control and level of institution were updated during the 2011-12 National Postsecondary Student Aid Study after data collection using newer Integrated Postsecondary Education Data System data. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas,Minnesota,Missouri,Nebraska,North Dakota,South Dakota;Southeast = Alabama,Arkansas,Florida,Georgia,Kentucky,Mississippi,North Carolina,South Carolina,Tennessee,Virginia,West Virginia;Southwest = Arizona,New Mexico,Oklahoma,Texas;Rocky Mountains = Colorado,Idaho,Montana,Utah,Wyoming;Far West = Alaska,California,Hawaii,Nevada,Oregon,Washington. 3 Cumulative Enrollment, Pell Grant, Direct Loan, and PLUS Loan categories were defined by quartiles. NOTE: CHAID = Chi-squared automatic interaction detection. Sample sizes rounded to the nearest 10.  Table 50 shows the final predictor variables used in the model to determine weight adjustments and the average weight-adjustment factors resulting from these variables for the panel weight. Summary statistics of the refusal weight-adjustment factors follow: \u2022 minimum: 1.00; \u2022 median: 1.03; and \u2022 maximum: 1.32. The final lower bound was 1.0 and the final upper bound was unbounded to achieve convergence for this weight adjustment.   Student other nonresponse adjustments (B17AWT5 and B17BWT3). The third, and final, stage of adjustment for student nonresponse was an adjustment for other nonresponse (i.e., contacted, but not surveyed before the end of the data-collection period), given that the student was located and did not explicitly refuse to participate. Staff made these additional student-nonresponse adjustments for the cross-sectional and panel weights to compensate further for potential student-nonresponse bias. As with the previous nonresponse adjustments, the same WTADJUST SUDAAN procedures and candidate predictor variables were used, and CHAID analyses on the predictor variables were run to detect important interactions. Table 51 shows the final predictor variables used in the model to determine weight adjustments and the average weight adjustment factor resulting from these variables for the cross-sectional weight. Summary statistics of the other nonresponse weightadjustment factors follow: \u2022 minimum: 1.01; \u2022 median: 1.22; and \u2022 maximum: 2.23. The final lower bound was 1.0 and the final upper bound was unbounded to achieve convergence for this weight adjustment.    Table 52 shows the final predictor variables used in the model to determine weight adjustments and the average weight-adjustment factor resulting from these variables for the panel weight. Summary statistics of the other nonresponse weight-adjustment factors follow: \u2022 minimum: 1.08; \u2022 median: 1.39; and \u2022 maximum: 3.13. The final lower bound was 1.0 and the final upper bound was unbounded to achieve convergence for this weight adjustment. WEIGHTING AND VARIANCE ESTIMATION 2012/17 BEGINNING POSTSECONDARY STUDENTS LONGITUDINAL STUDY (BPS:12/17) DATA FILE DOCUMENTATION  adjustment also helped increase the precision of the estimates for these key characteristics and any related characteristics. The control totals for FTB students, by spring and fall enrollment as well as yearly enrollment, for the BPS:12/14 weights were modified from weighted sums created during NPSAS:12 and from the IPEDS:2010-11 Fall Enrollment file (EF2011A) and the IPEDS:2011-12 Student Financial Aid and Net Price file (SFA1112). These same control totals were used for the BPS:12/17 poststratification adjustment for the cross-sectional weights. Individuals that were confirmed as ineligible non-FTB students during BPS:12/17 data collection, along with deceased individuals, were included in the poststratification adjustments, and their weights were dropped after the adjustment. The sum of the final weights estimates the number of NPSAS:12 students who were eligible for BPS and were still alive at the time of the BPS:12/17 survey. The resulting weight sums, with the ineligible and deceased individuals removed, were used as the control totals for the poststratification adjustment for the panel weights. Therefore, the weight sums agreed for the final cross-sectional and panel analysis weights. Control totals were established by summing the NPSAS:12 weights for the students identified as FTB students (including deceased students) and were derived from the sums of the NPSAS:12 weight for the following: \u2022 full-year FTB undergraduate student enrollment (full-and part-time), by control and level of institution; \u2022 estimated number of FTB Pell Grant recipients, by control and level of institution; \u2022 estimated total amount of Pell Grants received by all FTB students, by control and level of institution; \u2022 estimated total of Direct Loan amount received by all FTB students, by control and level of institution; \u2022 estimated number of FTB federal loan recipients, by control and level of institution; and \u2022 estimated total of federal loan amount received by all FTB students, by control and level of institution. The following variables, derived from the IPEDS:2010-11 EF file and 2011-12 student financial aid and net price file, were also used for developing control totals: \u2022 fall FTB undergraduate student enrollment (full-and part-time), by control and level of institution; \u2022 fall FTB undergraduate student enrollment (full-time), by control and level of institution; \u2022 number of full-time FTB students receiving Pell Grants, by control and level of institution; \u2022 amount of Pell Grants awarded to full-time FTB students, by control and level of institution; \u2022 number of full-time FTB students receiving federal student loans, by control and level of institution; and \u2022 amount of federal student loans awarded to full-time FTB students, by control and level of institution. Control totals for number and amount of Pell Grant and federal student loans were estimated from both NPSAS FTB students and IPEDS first-time students to control for potential inaccuracies within each data source. IPEDS only had data on fall FTB students, while the NPSAS FTB definition was based on year-round enrollment. Using both sources provided a better picture of spring and part-time enrollees as well. Table 53 shows the variables used for the poststratification, the values of the control totals, and the average weight-adjustment factors for each variable for the cross-sectional weights. Statistics for the poststratification weight-adjustment factors follow: \u2022 minimum: 0.00; \u2022 median: 1.60; and \u2022 maximum: 150.74. The poststratification adjustment had a lower bound of 0.00001 and was unbounded above in order to achieve convergence. After poststratification was performed, staff computed the final cross-sectional student weight (WTA000) as the product of the weight components described in this section. WTA000 = B17WT1 \u00d7 B17AWT1 \u00d7 B17AWT2 \u00d7 B17AWT3 \u00d7 B17AWT4 \u00d7 B17AWT5 \u00d7 B17AWT6 Table 53. Cross-sectional weight-adjustment factors for student poststratification: 2017"}, {"section_title": "Student refusal adjustments (B17AWT4 and B17BWT2).", "text": "The second stage of the student-nonresponse adjustments was adjustments for refusal, given that the student was located for both the cross-sectional and panel weights. These additional types of nonresponse adjustments were made to compensate further for the potential student-nonresponse bias. The same SUDAAN procedure that was used in the adjustments for students not located was used in the adjustments for refusal. The same candidate predictor variables were used to predict refusal, and the same type of CHAID analyses were used to detect important interactions. Table 49 shows the final predictor variables used in the model to determine weight adjustments and the average weight-adjustment factors resulting from these variables for the cross-sectional weight. Summary statistics of the refusal weight-adjustment factors follow: \u2022 minimum: 1.00; \u2022 median: 1.03; and \u2022 maximum: 1.40. The final lower bound was 1.0 and the final upper bound was unbounded to achieve convergence for this weight adjustment.   Table 49. Cross-sectional weight-adjustment factors for student refusal adjustment: 2017-Continued"}, {"section_title": "Number did not refuse", "text": ""}, {"section_title": "Poststratification categories Control total", "text": "Average weightadjustment factor (B17AWT6) Full-year FTB undergraduate student enrollment (full-and part-time), by control and level of institution Public less-than-2-year 31,010   Table 53. Cross-sectional weight-adjustment factors for student poststratification: 2017-Continued Table 54 shows the variables used for the poststratification, the values of the control totals, and the average weight-adjustment factors for each variable for the panel weights. Statistics for the poststratification weight-adjustment factors follow: \u2022 minimum: 0.01; \u2022 median: 1.55; and \u2022 maximum: 505.11. The poststratification adjustment had a lower bound of 0.00001 and was unbounded above in order to achieve convergence. After poststratification was performed, staff computed the final panel student weight (WTB000) as the product of the weight components described in this section. WTB000 = B17WT1 \u00d7 B17AWT1 \u00d7 B17AWT2 \u00d7 B17BWT1 \u00d7 B17BWT2 \u00d7 B17BWT3 \u00d7 B17BWT4 Table 54. Panel weight-adjustment factors for student poststratification: 2017\nAverage weightadjustment factor (B17BWT4) Full-year FTB undergraduate student enrollment (full-and part-time), by control and level of institution Public less-than-2-year 30,010 5.45 Public 2-year 1,598,520    "}, {"section_title": "Weighting Adjustment Summary and Evaluation", "text": "Cross-sectional analysis weight. Table 55 summarizes the student-weight distribution and the variance inflation caused by unequal weighting (i.e., UWEs by control and level of institution) for the BPS:12/17 cross-sectional analysis weight. The median student weight ranges from 25.23 for students in private for-profit 4-year institutions to 311.84 for students in public 4-year, doctorate-granting institutions. The mean student weight ranges from 48.02 for students in private forprofit 4-year institutions to 324.02 for students in public 4-year, doctorate-granting institutions. The unequal weighting effect is 2.46 overall and ranges from 1.30 for students in public 4-year, doctorate-granting institutions to 3.20 for students in private for-profit 2-year institutions. This means that for students, regardless of institutional control and level, the inflation on the variance of estimates due to the unequal weighting is relatively small and, even for those with the higher UWEs, there is little concern about the effects it could have on estimation. The sample design and sample sizes accounted for UWEs in this range to ensure precision of estimates. To assess the overall predictive ability of the cross-sectional student-nonresponse models, staff used a Receiver Operating Characteristic (ROC) curve (Hanley and McNeil 1982). The ROC curve provides a measure of how well the model correctly classified individuals of known response type-in other words, how well the model predicts a student's response propensity. 32 NPSAS staff developed the ROC curve in the following manner. The predicted probabilities of response (c) for the ROC curve associated with the nonresponse are the product of the predicted response probabilities obtained at each of the three nonresponse-adjustment steps. Note that for the second and third nonresponse adjustments (refusal and other nonresponse adjustments) predicted probabilities were calculated for all nonrespondents, but the models were developed excluding those students who had dropped out in the prior nonresponse adjustment. For any specified probability of response, c, two proportions were calculated: \u2022 the proportion of respondents with a predicted probability of response greater than c; and \u2022 the proportion of nonrespondents with a predicted probability of response greater than c. The plot of the first probability against the second, for c from zero to 1, resulted in the ROC curve shown in figure 3. The area under the curve equals the probability that the fitted model correctly classifies two randomly chosen individuals-one of whom is a true respondent, and one of whom is a true nonrespondent-where the individual with the higher predicted probability of response is classified as the respondent. An area of 0.5 under a ROC curve indicates that a correct classification is made 50 percent of the time, with the model providing no predictive benefit. An area of 1.0 indicates that the true respondent always has the higher predicted probability of response, as compared to the true nonrespondent, so the model always classifies the two individuals correctly. Figure 3 shows that the area under the ROC curve is approximately 0.68, so the predicted probabilities give the correct classification 68 percent of the time. Researchers can also interpret predictive probabilities from ROC curves in terms of the nonparametric Wilcoxon test statistic, which is used to determine if the level of a quantitative variable, such as predicted probability of response, is different between two samples (respondents and nonrespondents in this case). The ROC area equals the value of the Wilcoxon test statistic. Viewed in this way, the Wilcoxon test rejects the null hypothesis of no predictive ability by showing that the predicted probability of response for the respondents is larger than that for the nonrespondents. Analysts can interpret this result to mean that the variables used in the model are highly informative predictors of a student's overall response propensity. Panel analysis weight. Table 56 summarizes the student-weight distribution and the variance inflation caused by unequal weighting for the BPS:12/17 panel analysis weight. The median student weight ranges from 28.03 for students in private forprofit 4-year institutions to 330.34 for students in public 4-year, doctorate-granting institutions. The mean student weight ranges from 55.47 for students in private forprofit 4-year institutions to 357.63 for students in public 4-year, doctorate-granting institutions. The UWE is 2.77 overall and ranges from 1.74 for students in private nonprofit 4-year, doctorate-granting institutions to 3.24 for students in private for-profit lessthan-2-year institutions. Again, for students in most institutions, regardless of the institution's control and level, the inflation on the variance of estimates due to the unequal weighting is relatively small and, even for those with the higher UWEs, there is little concern about the effects it could have on estimation. To assess the overall predictive ability of the student panel nonresponse models, an ROC curve was developed as described above. Again, the predicted probabilities of response (c) for the ROC curve associated with the panel nonresponse are the product of the predicted response probabilities obtained at each of the three nonresponse-adjustment steps. Figure 4 shows that the area under the ROC curve is approximately 0.68, so the predicted probabilities give the correct classification 68 percent of the time. Predictive probabilities from ROC curves can also be interpreted in terms of the nonparametric Wilcoxon test statistic, where the ROC area is equivalent to the value of the Wilcoxon test statistic. Viewed in this way, the Wilcoxon test rejects the null hypothesis of no predictive ability by showing that the predicted probability of response for the respondents is larger than that for the nonrespondents. This result can be interpreted to mean that the variables used in the model are definitive predictors of a sample student's overall response propensity. "}, {"section_title": "Weighted and Unweighted Response Rates", "text": "The overall BPS:12/17 response rate is an estimate of the proportion of the study population directly represented by the respondents. Because the BPS:12/17 study includes a subsample of NPSAS:12 nonrespondents, the overall BPS:12/17 response rate is the product of the NPSAS:12 institution-level response rate times the BPS:12/17 student-level survey response rate. Therefore, the overall BPS:12/17 response rates can only be estimated directly for defined institutional characteristics. Table 57 gives the unweighted and weighted NPSAS:12 base-year institution and BPS:12/14 student survey response rate components by control and level of institution. The institution-level response rates shown in table 57 are the percentage of institutions that provided sufficient data to select the NPSAS:12 student-level sample; these rates are presented and discussed in the NPSAS:12 data file documentation (Wine, Bryan, and Siegel 2014, table 3, p. 11). Only the weighted response rates can be interpreted as estimates of the proportion of the BPS:12/17 population that is directly represented by the respondents. Using the cross-sectional weight, table 57 shows that, across all sectors, 67 percent of the eligible BPS:12/17 sample members were survey respondents. The unweighted rate varied from 53 percent to 79 percent by control and level of institution."}, {"section_title": "Nonresponse Bias Analysis", "text": "The accuracy of survey statistics is affected by both random and nonrandom errors. Random errors reduce the precision of survey estimates, and nonrandom errors may result in bias (i.e., estimates that do not converge to the true population parameter as the sample size increases without limit) or loss of precision. The sources of error in a survey are often dichotomized as sampling and nonsampling errors. Sampling error refers to the error that occurs because the survey is based on a sample of population members rather than the entire population. All other types of errors are nonsampling errors, including survey nonresponse (because of inability to contact sample members, their refusal to participate in the study, etc.) and measurement errors, such as the errors that occur because the intent of survey questions was not clear to the respondent, because the respondent had insufficient knowledge to answer correctly, or because the data were not captured correctly (e.g., because of recording, editing, or data-entry errors). Sampling errors are primarily random errors for well-designed surveys such as BPS:12/17. However, nonrandom errors can occur if the sampling frame does not provide complete coverage of the target population. The BPS:12/17 survey instrument and data-collection procedures were subjected to thorough development and testing to minimize nonsampling errors because these errors are difficult to quantify and are likely to be nonrandom errors. In this section, nonsampling error is observed by comparing BPS:12/17 nonrespondents and respondents using characteristics known for both groups. Section 6.4 discusses measurement of sampling error by variance estimation. NCES Statistical Standard 4-4-1 states that \"Any survey stage of data collection with a unit or item response rate less than 85 percent must be evaluated for the potential magnitude of nonresponse bias before the data or any analysis using the data may be released. Estimates of survey characteristics for nonrespondents and respondents are required to assess the potential nonresponse bias\" (Seastrom 2014). The bias in an estimated mean based on respondents is the difference between the expected value of this mean and the target parameter, , the population mean. Analysts can estimate the population mean for characteristics that are observed for both respondents and nonrespondents with the full-sample mean, which can be expressed in terms of the respondent mean and nonrespondent mean, , as follows: , where is the weighted unit (or item) nonresponse rate. For variables that are from the frame rather than from the sample, analysts can estimate without sampling error. They can then estimate bias as the difference between the respondent mean and the full-sample mean: . Equivalently, bias can be estimated as the difference between the mean for respondents and the mean for nonrespondents, multiplied by the weighted nonresponse rate: . Relative bias provides a measure of the magnitude of the bias relative to the sample mean and is estimated as . Effect size, as defined by Cohen (1988), is another measure of potential nonresponse bias. For continuous variables, it is computed as the estimated bias divided by the estimated standard deviation: . For categorical variables, it is computed as , where 0 is the proportion of the full sample in category , and 1 is the proportion of respondents in category . Effect sizes can be used in combination with bias and relative bias estimates and significance tests to evaluate the potential for nonresponse bias. Cohen classified an effect size as \"small\" when it is about 0.10, as \"medium\" when it is about 0.30, and as \"large\" when it is about 0.50. BPS staff conducted nonresponse bias analysis at the student level and item level for the overall sample and for each control and level of institution. (Control and level of institution were used for institution stratification, as described in chapter 2.) These analyses are described in the sections below. The student-level results are summarized in tables 59 through 61, and detailed tables are provided in appendix K. The item-level response rates are shown in appendix H, table H-1, and bias results are summarized in appendix K, table K-45."}, {"section_title": "Bias Analysis: Institution Level", "text": "An institution respondent is defined as any sampled institution that provided a student enrollment list from which a student sample was selected. As shown in table 4, the NPSAS:12 institution weighted response rate was 87 percent overall and ranged from 78 percent for 2-year private for-profit institutions to 92 percent for public 4-year, non-doctorate-granting institutions. NPSAS project staff conducted a nonresponse bias analysis during NPSAS:12 for each of these categories. Wine, Bryan, and Siegel (2014) provide more information regarding the institution bias analysis."}, {"section_title": "Bias Analysis: Student Level", "text": "Student-level bias analysis was conducted using two different definitions of respondents corresponding to the two analytic weights created for BPS:12/17, described in section 6.1. Results for the cross-sectional weight (WTA000) and the panel weight (WTB000) are summarized in section 6.1.4. For each analysis, nonresponse bias was estimated for characteristics known for most respondents and nonrespondents. Bias estimates for characteristic categories that did not meet reporting requirements (fewer than 30 nonrespondents) were excluded from calculations of summary statistics. The following characteristics were used for the nonresponse bias analysis: 33 \u2022 control and level of institution; \u2022 Bureau of Economic Analysis Code (Office of Business Economics [OBE]) Region; \u2022 institution total enrollment; \u2022 age as of December 31, 2011; \u2022 Pell Grant status; \u2022 total Pell Grant amount received; \u2022 Stafford Loan status; \u2022 total Stafford Loan amount received; \u2022 total PLUS amount received; \u2022 federal aid status; WEIGHTING AND VARIANCE ESTIMATION 2012/17 BEGINNING POSTSECONDARY STUDENTS LONGITUDINAL STUDY (BPS:12/17) DATA FILE DOCUMENTATION \u2022 institution aid status; \u2022 state aid status; \u2022 CPS record available; \u2022 major; \u2022 percentage of full-time, first-time degree/certificate-seeking undergraduate students who received any grant aid; \u2022 graduation rate of full-time, first-time degree/certificate-seeking undergraduate students within 150 percent of normal time to completion; \u2022 six-year federal loan default status; \u2022 public institution tuition and fees as percentage of core revenues (Governmental Accounting Standards Board [GASB] reporting); \u2022 public institution instructional expenses per full-time equivalent (FTE) enrollment (GASB reporting); \u2022 private institution tuition and fees as percentage of core revenues (Financial Accounting Standards Board [FASB] reporting); and \u2022 private institution instructional expenses per FTE enrollment (FASB reporting). First, for the variables listed above, the nonresponse bias was estimated for each category as the weighted difference between the means (proportions) of the respondents and of the full sample, and this estimated nonresponse bias was tested using a t test to determine if it differed significantly from zero at the 5-percent level. Relative bias was computed as the ratio of the estimated bias to the weighted fullsample mean. Second, nonresponse adjustments were computed to reduce or eliminate nonresponse bias for key variables. Third, using the weights adjusted for nonresponse, the reestimated nonresponse bias was tested for significance. These tests were complemented by effect size calculations. Finally, to better understand the effect of poststratification on efforts to reduce nonresponse bias, two additional sets of estimates were created. The first set of estimates equals the differences in respondent means before and after poststratification, which corresponds to the effect of poststratification on nonresponse adjustments. The second set of estimates, equal to the difference between base-weighted full-sample means and the poststratified respondent means, corresponds to the cumulative effects of all weighting and adjustment steps. Cross-sectional weight (WTA000) nonresponse bias analysis. As shown in table 57, the BPS:12/17 weighted survey response rate for students using the crosssectional weight was 67 percent overall and ranged from 51 percent for private forprofit less-than-2-year institutions to 76 percent for private nonprofit 4-year, doctorate-granting institutions. Therefore, a student-level nonresponse bias analysis was conducted overall and within each institution control and level for BPS:12/17. As shown in table 58, the student-nonresponse weighting adjustment eliminated some, but not all, significant bias on the observable characteristics for sectors that met reporting requirements (i.e., had at least 30 nonrespondents). Before weighting, the median effect size for all institutions was 0.08, ranging from 0.03 for students in public 2-year institutions to 0.20 for students in private nonprofit less-than-4-year institutions. The percentage of characteristics that were significantly biased for study members was 70 percent overall, ranging from 12 percent for students in private nonprofit less-than-4-year institutions to 40 percent for students in private nonprofit 4-year, non-doctorate-granting institutions. After the nonresponse weight adjustment, the median effect size for all study members was zero, ranging from 0.02 for students in public 2-year institutions to 0.12 for students in private nonprofit less-than-4-year institutions. The percentage of characteristics that remained significantly biased was 9 percent overall and ranged from 1 percent for students in private for-profit 4-year institutions to 22 percent for students in private nonprofit 4-year, doctorate-granting institutions. 1 Relative bias and effect size are calculated using the weighted differences between respondent and full-sample means. Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. Effect size is calculated as the square root of the sum over categories of the squared differences over full-sample means. 2 Respondent and full-sample means are weighted using the student base weight. 3 Full-sample means are weighted using the student base weight, and the respondent means are weighted using the base weight adjusted for nonresponse. NOTE: Variables and characteristics that did not meet reporting standards were excluded from calculation of summary statistics. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. As shown in table 59, the mean absolute difference between means for respondents before and after poststratification adjustment was 0.96 for students overall and ranged from 0.72 for students in public 4-year, doctorate-granting institutions to 5.76 for students in public less-than-2-year institutions. The median difference was 0.71 for students overall and ranged from 0.35 for students in public 4-year, doctorategranting institutions to 4.90 for students in private nonprofit less-than-4-year institutions. For the absolute differences between means for the full sample and respondents after poststratification adjustment, the mean difference was 0.90 for students overall and ranged from 0.67 for students in public 4-year, doctorategranting institutions to 5.70 for students in private nonprofit less-than-4-year institutions. The median difference was 0.65 for students overall and ranged from 0.42 for students in public 4-year, doctorate-granting institutions to 5.03 for students in private nonprofit less-than-4-year institutions. Detailed summaries are provided in appendix K, tables K-1-K-11 and K-23-K-33. Panel weight (WTB000) nonresponse bias analysis. As shown in table 57, the BPS:12/17 weighted survey response rate for students using the panel weight was 58 percent overall and ranged from 42 percent for private for-profit less-than-2-year institutions to 70 percent for private nonprofit 4-year, doctorate-granting institutions. Therefore, a student-level nonresponse bias analysis was conducted overall and within each institution control and level for BPS:12/17. As shown in table 60, the student-nonresponse weighting adjustment eliminated some, but not all, significant bias on the observable characteristics for sectors that met reporting requirements (i.e., had at least 30 nonrespondents). Before weighting, the median effect size for all institutions was 0.09, ranging from 0.05 for students in public 2-year institutions; public 4-year, doctorate-granting institutions; and private nonprofit 4-year, doctorate-granting institutions to 0.23 for students in private nonprofit less-than-4-year institutions. The percentage of characteristics that were significantly biased for study members was 73 percent overall, ranging from zero percent for students in public less-than-2-year institutions to 37 percent for students in public 4-year, non-doctorate-granting institutions. After the nonresponse weight adjustment, the median effect size for all study members was zero, ranging from 0.03 for students in public 2-year institutions; public 4-year, doctorate-granting institutions; and private nonprofit 4-year, doctorate-granting institutions to 0.16 for students in private for-profit less-than-4-year institutions. The percentage of characteristics that remained significantly biased was 11 percent overall and ranged from zero percent for students in public less-than-2-year institutions and private nonprofit less-than-4-year institutions to 21 percent for students in private for-profit 2-year institutions. 1 Relative bias and effect size are calculated using the weighted differences between respondent and full-sample means. Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. Effect size is calculated as the square root of the sum over categories of the squared differences over full-sample means. 2 Respondent and full-sample means are weighted using the student base weight. 3 Full-sample means are weighted using the student base weight, and the respondent means are weighted using the base weight adjusted for nonresponse. NOTE: Variables and characteristics that did not meet reporting standards were excluded from calculation of summary statistics. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. As shown in table 61, the mean absolute difference between means for respondents before and after poststratification adjustment was 1.10 for students overall and ranged from 0.91 for students in public 4-year, doctorate-granting institutions to 8.47 for students in public less-than-2-year institutions. The median difference was 0.80 for students overall and ranged from 0.47 for students in public 2-year institutions to 4.12 for students in private nonprofit less-than-4-year institutions. For the absolute differences between means for the full sample and respondents after poststratification adjustment, the mean difference was 1.04 for students overall and ranged from 0.86 for students in public 4-year, doctorate-granting institutions to 6.87 for students in public less-than-2-year institutions. The median was 0.72 for students overall and ranged from 0.56 for students in private nonprofit 4-year, doctorategranting institutions to 5.26 for students in private nonprofit less-than-4-year institutions. Detailed summaries are provided in appendix K, tables K-12-K-22 and K-34-K-44. "}, {"section_title": "Bias Analysis: Item Level", "text": "NCES Statistical Standard 4-4-3A states: \"For an item with a low total response rate, respondents and nonrespondents can be compared on sampling frame and/or questionnaire variables for which data on respondents and nonrespondents are available. Base weights must be used in such analysis. Comparison items should have very high response rates. A full range of available items should be used for these comparisons. This approach may be limited to the extent that items available for respondents and nonrespondents may not be related to the low response rate item being analyzed\" (Seastrom 2014). Moreover, NCES Statistical Standard 1-3-5 states: \"Item response rates (RRI) are calculated as the ratio of the number of respondents for whom an in-scope response was obtained (I x for item x) to the number of respondents who are asked to answer that item. The number asked to answer an item is the number of unit-level respondents (I) minus the number of respondents with a valid skip for item x (V x ). When an abbreviated questionnaire is used to convert refusals, the eliminated questions are treated as item nonresponse\u2026. In longitudinal analyses, the numerator of an item response rate includes cases that have data available for all waves included in the analysis and the denominator includes the number of respondents eligible to respond in all waves included in the analysis. In the case of constructed variables, the numerator includes cases that have available data for the full set of items required to construct the variable, and the denominator includes all respondents eligible to respond to all items in the constructed variable\" (Seastrom 2014). The RRI is calculated as A nonresponse bias analysis was conducted for all imputed items 34 and select analysis variables with a weighted response rate less than 85 percent for students overall or for students in a particular sector. The BPS:12/17 cross-sectional weight (WTA000) was used for computing item response rates, shown in appendix H, table H-1. The procedures and variables used for the item-level nonresponse bias analysis are the same as those used for the student-level nonresponse bias analysis presented above. A student was defined to be an item respondent for a variable if that student had data for that variable from any source, including logical imputation. As shown in appendix H, table H-1, the weighted item response rates for imputed and select analysis variables, for all students, ranged from 0.1 percent for Jobs while enrolled: hours worked for job 8 at year 1 (HRSWKJ8Y1) to 100 percent. A number of variables from BPS:12/14 that were not part of the BPS:12/17 data collection have been included in the BPS:12/17 restricted-use file. Some of these variables were collected in BPS:12/14 but not released, while others were rederived for the BPS:12/17 restricted-use file using updated enrollment and attainment information. Approximately 2,700 students responded to BPS:12/17 but not BPS:12/14 and hence are unit respondents for BPS:12/17 but item nonrespondents for all BPS:12/14 variables in the BPS:12/17 restricted-use file, lowering item response rates for these variables. Further, the number of students known to be eligible for some items tends to be small compared to the number of students whose eligibility is unknown, and unknown eligibility is considered to be nonresponse. For example, few students have eight jobs and thus few students are eligible to answer HRSWKJ8Y1; however, for students who do not provide employment information, including the aforementioned 2,700 BPS:12/14 nonrespondents, eligibility for HRSWKJ8Y1 is unknown. These factors are responsible for some items in appendix H, table H-1 having very low response rates. Of 298 imputed items and select derived analysis variables, 276 had a sufficient number of eligible students (at least 30) to permit estimation of response rates for students overall. Of these, 134 had an overall weighted response rate below 85 percent. An additional 79 items had an overall response rate greater than 85 percent but had a weighted response rate below 85 percent for at least one institution type, yielding a total of 213 items for which nonresponse bias analyses were conducted. The results of the nonresponse bias analyses varied across items. Appendix K, table K-45 provides a summary of the item nonresponse bias analysis for each item analyzed. Imputation procedures (described in section 5.4) were conducted with a goal of reducing or eliminating item nonresponse bias. Although bias after imputation is not directly measurable, it is possible to compare estimates before and after imputation to determine whether the imputation changed the estimates. Changes are generally indicative of a reduction in bias, whereas no change suggests bias was not reduced or was not present. For continuous variables, the difference between the preimputation mean and postimputation mean was computed; for categorical variables, the difference between the weighted preimputation and postimputation mean (proportion) was computed for each category. Student cross-sectional weights were used for these comparisons. All differences were tested for statistical significance using t tests. For categorical variables, the differences reported in appendix K, table K-45 are sizeweighted means of category-level differences 35 and are labeled as significant if any category-level difference is significant. These tests were complemented by effect size calculations. Effect sizes for categorical variables are calculated as , where 0 is the proportion of respondents in category after imputation, and 1 is the proportion of respondents in category before imputation. For continuous variables, effect size is the difference in preimputation and postimputation means, divided by the postimputation standard deviation. For students overall, statistically significant differences between the pre-and postimputation means were found for 39 percent of the variables (excluding those that did not meet reporting standards). Effect sizes for these differences range from 0.01 to 0.13. About 34 percent of the differences reported by sector were found to be statistically significant, with effect sizes for these differences ranging from 0.02 to 1.90."}, {"section_title": "Variance Estimation", "text": "Every estimate calculated from a probability-based sample survey, such as a mean, a percentage, or a regression coefficient, has an associated variance. Hypothesis testing, calculation of confidence intervals, and modeling that use complex survey data all require the calculation of variances using appropriate methods that account for the sampling design. Complex sample designs, like those used for NPSAS:12 and BPS:12/17, result in data that violate the assumptions that are normally required to assess the statistical significance of results. The variances of the estimates from complex surveys may vary from those that would be expected if the sample were a simple random sample and the observations were independent and identically distributed random variables. Two procedures for estimating variances of statistics from complex surveys are the Taylor-series linearization procedure and the bootstrap replication procedure, which are both available for the NPSAS data files. The analysis strata and PSUs created for the Taylor-series procedure are discussed in section 6.4.1, and section 6.4.2 contains a discussion of the replicate weights created for the bootstrap procedure. Use of software packages for proper variance estimation is discussed in section 6.4.3. The survey design effect for a statistic is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that sample had been selected). It is often used to measure the effects that sample design features have on the precision of survey estimates. For example, stratification tends to decrease the variance, but multistage sampling and unequal sampling rates usually increase the variance. In addition, weight adjustments for nonresponse (performed to reduce nonresponse bias) and poststratification increase the variance by increasing the weight variation. Design effects are discussed in section 6.4.4."}, {"section_title": "Taylor Series", "text": "The Taylor-series variance estimation procedure is a well-known technique used to estimate the variances of nonlinear statistics. The procedure takes the first-order Taylor-series approximation of the nonlinear statistic and substitutes the linear representation into the appropriate variance formula based on the sample design (Woodruff 1971). For stratified multistage surveys, the Taylor-series procedure requires variance estimation strata and variance estimation PSUs, also called replicates, defined from the sampling strata and PSUs used in the first stage of sampling. Because BPS:12/17 is a follow-up study of NPSAS:12, the variance estimation strata and PSUs for BPS:12/17 were derived from those developed for NPSAS:12. The steps used in the construction of the NPSAS:12 strata and PSU variables are described in chapter 6 of the NPSAS:12 data file documentation (Wine, Bryan, and Siegel 2014). The variance estimation formulas require at least two PSUs in each stratum. The NPSAS:12 variance estimation strata and PSUs were examined for the BPS:12/17 sample, and strata with only one PSU were combined with other strata to obtain at least two PSUs. The following three rules were used: variance estimation strata were combined with other variance estimation strata within the original NPSAS:12 sampling strata, certainty schools were combined with other certainty schools, and noncertainty schools were combined with other noncertainty schools. In addition, the original sort order that was used for constructing the NPSAS:12 variance estimation strata and PSUs was used. A variance estimation stratum was combined with the next stratum in the sorted list. If the stratum was the first in the sorted list, then it was combined with the next stratum in the list. The single PSU then became an additional PSU in the new variance estimation strata. The NPSAS:12 restricted-use data file provided two sets of variables for Taylorseries variance estimation, and BPS:12/17 also provides two sets of variables. One set of variables is used in software that assumes that the first-stage sampling units (institutions) were sampled with replacement (or with small selection probabilities) and does not account for the finite population correction (FPC) at the institution level of sampling. The other set of variables is used in software that assumes sampling of institutions without replacement in the calculation of variances and does account for the FPC. Both sets of variables are provided because not all survey data analysis packages have the option to incorporate the FPC in the variance calculations. When the first-stage units are sampled with very small probabilities, the estimated variances using the with-replacement variance formulas and the withoutreplacement variance formulas are the same. The set of variables used when assuming the first-stage units were sampled with replacement includes the analysis stratum (ANALSTR) and analysis PSU (ANALPSU). The set of variables used when assuming the first-stage units were sampled without replacement, and that account for the FPC, includes the analysis stratum (FANALSTR), analysis PSU (FANALPSU), analysis secondary sampling unit (SSU) (FANALSSU), and the count of PSUs in an analysis stratum (PSUCOUNT). Ultimately, FANALSTR equals the institutional variance estimation stratum ANALSTR, FANALPSU equals ANALPSU. FANALSSU was created by randomly dividing the NPSAS:12 analysis PSUs into two parts. These variables are a by-product of the bootstrap variance estimation weights (described in section 6.4.2), and the justification for using the without-replacement variance formulas follows from the assumptions described in Kott 1988. Some values of the variance estimation strata, PSU, and SSU variables were combined in order to have at least 2 SSUs in each PSU, and at least 2 PSUs in each stratum. The same stratum and PSU terms, under with-replacement and without-replacement assumptions, are used for analysis with both the cross-sectional and panel weights."}, {"section_title": "Bootstrap Replicate Weights", "text": "The replication variance estimation strategy that was chosen for BPS:12/17 is the same as that used for NPSAS:12 and BPS:12/14 and accounts for the following in order to produce accurate variance estimates: 1. stratification at all stages of sampling; 2. unequal weighting; 3. sample clustering; 4. weight adjustments for nonresponse and for poststratification of selected total estimates to known external totals; 5. nonlinear statistics and percentages, as well as for linear statistics; 6. FPCs at the institution stage of sampling and high sampling rates in some first-stage sampling strata; and 7. the ability to test hypotheses about students based on normal distribution theory by ignoring the FPCs at the student level of sampling. Commonly applied bootstrap variance estimation techniques account for 1 through 5 listed above; however, to account for 6 and 7 above, NPSAS staff applied a method adapted from Kott (1988) and Flyer (1987). The following notation is used in the steps delineated below:  This method uses random switching between PSU bootstrap sampling and SSU bootstrap sampling to represent the proper mix (in expectation) of the first-and second-stage variance components when an FPC is applied at the first stage of sampling. It extends the general method described by Flyer (1987) for half-sample replication to a more general bootstrap. This method incorporated the FPC factor only at the first stage, where sampling fractions were generally high. At the second stage, where the sampling fractions were generally low, the FPC factor was set to 1.00. Staff used the Flyer-Kott methodology to develop a vector of bootstrap sample weights that they added to the analysis file. These weights are zero for units not selected in a particular bootstrap sample; weights for other units are inflated for the bootstrap subsampling. The final student weight (WTA000) described in section 6.1 is used for computing estimates, such as means, percentages, and regression coefficients; and the vector of replicate weights allows for computation of additional estimates for the sole purpose of estimating variances. Assuming B sets of replicate weights, analysts can estimate the variance of any estimate,\u03b8 , by replicating the estimation procedure for each replicate and computing a simple variance of the replicate estimates, as follows: \u03b8\u02c6b is the estimate based on the bth replicate weight (where b = 1 to the number of replicates) and B is the total number of sets of replicate weights. The standard error for \u03b8 can be calculated as the square root of the estimate of variance, var( \u03b8 ). The number of replicate weights was set to 200 to ensure stable variance estimates for a variety of estimates. The unknown eligibility, nonresponse, and poststratification adjustments described in section 6.1 were applied to each replicate to create the 200 replicate weights included on the analysis file (WTA001-WTA200) so that the variances would be estimated to account for these weight adjustments. For some of the replicates, the bounds on the nonresponse and poststratification adjustment factors had to be loosened or model variables had to be collapsed because of model convergence problems (i.e., there was no solution to satisfy all model equations simultaneously). However, the model adjustments were not necessary for many replicates, and when it was necessary, the adjustments were minimal. Therefore, this approach worked well for BPS:12/17 to achieve model convergence for all replicates and to minimize the effect of different models on the variance estimates. Table 62 and table 63 summarize the weight and variance estimation variables and how they are used in selected software packages that allow for Taylor-series variance estimation with replacement (SUDAAN, Stata, the SAS survey data analysis procedures, IBM SPSS Complex Samples, and the R survey package), Taylor-series variance estimation without replacement (SUDAAN, Stata, and the R survey package), and bootstrap variance estimation (SUDAAN, Stata, the SAS survey data analysis procedures, WesVar, and the R survey package). The code shown in the table is intended for use within respective program statements or procedures and cannot be used alone as shown in the table. The code may need to be revised to be appropriate for a user's specific data file and coding decisions, and for that reason, the provided code may not work for all users and may require editing before it is implemented. Table 62 details the code for analyses using the cross-sectional weight, and table 63 shows the code for analyses using the panel weight.  "}, {"section_title": "Software Use for Variance Estimation", "text": ""}, {"section_title": "Variance Approximation", "text": "The survey design effect (DEFF) for a given estimate,\u03b8, is defined as The square root of the design effect (DEFT) is another measure that analysts can express as the ratio of the standard errors, or Most complex multistage sampling designs, like NPSAS:12 and BPS:12/17, result in design effects greater than 1.0. That is, the design-based variance is larger than the simple random sample variance. Appendix J provides design effect estimates for important survey domains and estimates for undergraduate and graduate students to summarize the effects of stratification, multistage sampling, unequal probabilities of selection, and the weight adjustments. These design effects were estimated using SUDAAN and the bootstrap variance estimation procedure described above. Large design effects imply large standard errors and relatively poor precision. Small design effects imply small standard errors and good precision. In general terms, a design effect less than 2.0 is low, from 2.0 to 3.0 is moderate, and greater than 3.0 is high. Moderate and high design effects often occur in complex surveys such as NPSAS and BPS. Unequal weighting causes large design effects and is often due to nonresponse and poststratification adjustments; however, in NPSAS and BPS, the unequal weighting is also due to the sample design and different sampling rates among institutional strata, as well as to the different sampling rates among student strata. As discussed above, Taylor-series linearization and replication techniques can be used to compute more precise standard errors for data from complex surveys. If statistical analyses are conducted using software packages that assume the data were collected using simple random sampling (i.e., adjustments are not made using the Taylor-series or bootstrap replication methods), the standard errors will be calculated under this assumption and will be incorrect. They can be adjusted using the average DEFT, although this method is less precise than Taylor-series or replication techniques. Analysts who must perform an analysis of BPS:12/17 data without using one of the software packages for analysis of complex survey data can use the design effect tables in appendix J to make approximate adjustments to the standard errors of survey statistics computed with the standard software packages that assume simple random-sampling designs. As the first step in the approximation of a standard error, the analyst should normalize the overall sample weights for packages that use the weighted population size (N) in the calculation of standard errors (SPSS but not SAS). The normalized weight will sum to the sample size (n) and is calculated as where n is the sample size (i.e., the number of cases with a valid main sampling weight) and N is the sum of weights. As the second step in the approximation, the standard errors produced by the statistical software, the test statistics, or the sample weight used in analysis can be adjusted to reflect the actual complex design of the study. To adjust the standard error of an estimate, the analyst should multiply the standard error produced by the statistical software by the square root of the DEFF or the DEFT. The DEFF and DEFT used to make adjustments can be calculated for specific estimates, can be the median DEFF and DEFT across a number of variables, or can be the median DEFF and DEFT for a specific subgroup in the population. Adjusted standard errors can then be used in hypothesis testing, for example, when calculating t and F statistics. A second option is to adjust the t and F statistics produced by statistical software packages using unadjusted (i.e., SRS) standard errors. To do this, first conduct the desired analysis weighted by the normalized weight and then divide a t statistic by the DEFT or divide an F statistic by the DEFF. A third alternative is to create a new analytic weight variable in the data file by dividing the normalized analytic weight by the DEFF and using the adjusted weight in the analyses. The pilot test included methodological experiments in the survey instrument to test new approaches to assisted coding systems, or \"coders,\" used to identify standardized codes, such as major/field of study. Using an experimental design, the pilot test included a predictive search algorithm that provides potential matching results in real time, in contrast to the traditional method in which respondents manually type an entire entry before querying the system. The pilot test also included an experiment to test new language for asking sensitive questions about students' grade point average (GPA). Findings from both experiments are presented in section 3. The data-collection period lasted approximately 5 weeks (March 9 through April 15, 2016), and focused primarily on encouraging sample members to complete the survey on the Web. Nonresponse conversion was limited to written communication and did not include telephone follow-up. No intensive tracing operations were employed, and contacting efforts consisted of e-mails and mailings. If a sample member preferred to complete the survey by telephone, he or she could contact the BPS help desk to speak with a data collection interviewer (DCI). After data collection was completed, the data were processed and compiled into a dataset. Staff conducted analyses to identify potential data quality problems, refine implementation procedures, and learn how respondents were likely to answer new questions in the full-scale collection. "}, {"section_title": "C.2.1 Target Population", "text": "The target population for the BPS:12/17 pilot test was all students who began their postsecondary education for the first time during the 2010-11 academic year at any Title IV eligible postsecondary institution in the United States. "}, {"section_title": "C.2.1.1 Institution Universe for NPSAS:12", "text": "To be eligible for the NPSAS:12 field test, students must have been enrolled at a NPSAS-eligible institution for a term or in a course of instruction at any time during the 2010-11 academic year. Institutions must have also met the following requirements: \u2022 offer an educational program designed for persons who have completed secondary education; \u2022 offer at least one academic, occupational, or vocational program of study lasting at least 3 months or 300 clock hours; \u2022 offer courses that were open to more than the employees or members of the company or group (e.g., union) that administers the institution; \u2022 be located in the 50 states or the District of Columbia; All eligible students from sampled institutions constituted the student sampling frame. 3 The institution samples for the NPSAS:12 field test and full-scale collections were selected simultaneously, prior to the field test, using stratified random sampling with probabilities proportional to a composite measure of size (Folsom, Potter, and Williams 1987). Institution measure of size was determined using annual enrollment data from the most recent IPEDS 12-Month Enrollment component and FTB student enrollment data from the most recent IPEDS Fall Enrollment component. Composite measure of size sampling was used to ensure that target sample sizes were achieved within institution and student sampling strata and that approximately equal student weights were achieved across institutions. The field test institution sample for NPSAS:12 was selected using statistical procedures (i.e., simple random sampling within strata) rather than purposive sampling as was done in past NPSAS cycles. This provided more control to ensure that the field test and the full-scale institution samples had similar characteristics. It also allowed inferences to be made to the target population, supporting the analytic needs of the field test experiments and instrument. This ability to make analytic inferences extends to the BPS:12/14 field test sample and the BPS:12/17 pilot-test sample. A total of 1,970 institutions from the stratified frame were selected to participate in either the NPSAS:12 field test or the full-scale collection. From the 1,970 institutions selected, a subsample of 300 institutions was selected using simple random sampling within institution strata to make up the field test sample. The sample for the fullscale study comprised the remaining 1,670 institutions. This sampling process eliminated the possibility that an institution would be burdened with participation in both the field test and full-scale collections, yet maintained the representativeness of the full-scale sample. The following institution strata used for the sampling design were based on institution control, level, and highest level of offering: \u2022 public less-than-2-year; \u2022 public 2-year; \u2022 public 4-year, non-doctorate-granting; \u2022 public 4-year, doctorate-granting; \u2022 private nonprofit less-than-4-year; \u2022 private nonprofit 4-year, non-doctorate-granting; \u2022 private nonprofit 4-year, doctorate-granting; \u2022 private for-profit less-than-2-year; \u2022 private for-profit 2-year; and \u2022 private for-profit 4-year. Due to the growth of the private for-profit sector, private for-profit 4-year and private for-profit 2-year institutions were separated into their own strata in NPSAS:12, unlike in previous administrations of NPSAS. Within each institution stratum, additional implicit stratification for the full-scale collection was accomplished by sorting the sampling frame within stratum by the following classifications: 4 (1) Historically Black Colleges and Universities status; (2) Hispanic-Serving Institutions (HSI) indicator; 5 (3) Carnegie classifications of degree-granting postsecondary institutions; 6 (4) 2-digit Classification of Instructional Programs (CIP) code of the largest program for less-than-2-year institutions; (5) the "}, {"section_title": "C.2.1.2 Eligible Student Universe for NPSAS:12", "text": "Students eligible for the NPSAS:12 field test were those who attended a NPSASinstitution during the 2010-11 academic year and who were \u2022 enrolled in either (1) an academic program; (2) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; (3) exclusively noncredit remedial coursework but determined by the institution to be eligible for Title IV aid; or (4) an occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; \u2022 not currently enrolled in high school; and \u2022 not solely enrolled in a high school completion program. The NPSAS:12 field test institution sample included all levels (less-than-2-year, 2-year, and 4-year) and controls (public, private nonprofit, and private for-profit) of Title IV eligible postsecondary institutions in the United States. The field test student sample was randomly selected from lists of students enrolled at sampled institutions between July 1, 2010, and April 30, 2011. The NPSAS:12 field test study year covered the time period between July 1, 2010, and June 30, 2011, to coincide with the federal financial aid award year. To facilitate timely completion of data collection and data file preparation, institutions were asked to submit enrollment lists for all eligible students enrolled at any time between July 1 and April 30 or, for institutions with continuous enrollment, between July 1 and March 31. The March 31 deadline for continuous enrollment institutions was used for the field test due to the compressed data-collection schedule and was not used in the full-scale collection. Because previous cycles of NPSAS have shown that the terms beginning in May and June added little to enrollment and aid totals, May-June starters were excluded to allow institutions to provide enrollment lists earlier, which in turn allowed the student survey process to begin earlier. In the full-scale collection, poststratification of survey estimates based on IPEDS records on enrollment and National Student Loan Data System (NSLDS) records on financial aid distributed were used to adjust for the data-collection period's inclusion of any terms that begin by April 30 and the consequent exclusion of a small number of students newly enrolled in May or June. To create the student sampling frame, each participating institution was asked to submit a list of eligible students. The requests for student enrollment lists specifically indicated how institutions should handle special cases, such as students taking only correspondence or distance learning courses, and foreign exchange, continuing education, extension division, and nonmatriculated students. The following data were required for each enrollee: \u2022 student's name; \u2022 student ID; \u2022 Social Security number; \u2022 date of birth; \u2022 date of high school graduation (month and year); \u2022 degree level during the last term of enrollment (undergraduate, master's, doctoral-research/scholarship/other, doctoral-professional practice, or other graduate); \u2022 class level if undergraduate (first, second, third, fourth, or fifth year or higher); \u2022 major; \u2022 CIP code; \u2022 indicator of whether the institution received an Institutional Student Information Record (an electronic record summarizing the results of the student's Free Application for Federal Student Aid processing) from the Central Processing System (CPS); \u2022 FTB student status; and \u2022 contacting information, such as cell phone number, local telephone number and address, permanent telephone number and address, campus e-mail address, and permanent e-mail address. Requesting contact information for eligible students prior to sampling allowed for student record abstraction and the student survey to begin shortly after sample selection, which helped to ensure the management of the field test schedule for data collection, data processing, and file development. Student sample sizes for the NPSAS:12 field test were formulated to ensure representation of various types of students. Specifically, the sample included a large number of potential FTB students, to provide a sufficient sample size to obtain a yield of at least 1,000 students for the BPS field test. The NPSAS:12 field test sample included 4,530 students, of whom 4,130 were potential FTB students, 200 were other undergraduate students, and 200 were graduate students. Students were sampled at fixed rates according to student education level and institution sampling strata. Sample yield was monitored, and sampling rates were adjusted when necessary, resulting in a statistical sample of the required sample size for the field test. The same approach was used for the full-scale collection. Student enrollment lists provided by the institutions were reviewed to make sure that required elements were included, and were also compared for consistency with counts from the 2009 IPEDS 12-Month Enrollment component."}, {"section_title": "C.2.1.3 Identification of FTB Students in NPSAS:12", "text": "To be eligible for BPS:12/14 or BPS:12/17, students must have begun their postsecondary education for the first time, after completing high school, between July 1, 2010 and June 30, 2011. Inaccurate identification of FTB students in the NPSAS field test could lead to unacceptably high rates of misclassification (e.g., false positives [students who had been identified as potential FTBs but were found to have prior enrollment]), which could result in (1) excessive cohort loss, (2) excessive cost to \"replenish the sample,\" and (3) an inefficient sample design (excessive oversampling of \"potential\" FTB students). Therefore, close attention was paid to accurately identify FTB students in the NPSAS field test in an effort to mitigate anticipated misclassification error. To address this concern, participating institutions were asked to provide additional information for all eligible students, and student names were matched to administrative databases to further eliminate false positives prior to sample selection. Participating institutions were asked to provide the FTB student status and high school graduation date for every listed student. High school graduation date was used to remove students from the frame who were dually enrolled in high school. FTB student status, along with class level and student level, were used to exclude misclassified FTB students in their third year or higher or those who were not undergraduate students, or both. FTB student status, along with date of birth, was also used to identify students older than 18 to send for presampling matching to administrative databases. If the FTB student indicator was not provided for a student on the list, but the student was 18 years of age or younger and did not appear to be enrolled in high school, the student was sampled as an FTB student. Otherwise, if the FTB student indicator was not provided for a student on the list and the student was over the age of 18, then the student was sampled as \"other undergraduate,\" but would be included in the BPS cohort if identified during the student survey as an FTB student. Prior to sampling, students over the age of 18 listed as potential FTB students were matched to NSLDS records to determine if any had a federal financial aid history predating the NPSAS year (earlier than July 1, 2010, for the field test). Because NSLDS maintains current records of all Title IV federal grant and loan funding, any student with disbursements from the prior year or earlier could be reliably excluded from the sampling frame of FTB students. Given that 60 percent of FTB students receive some form of Title IV aid in their first year, this matching process could not exclude all listed FTB students with prior enrollment; however, this process significantly improved the accuracy of the list prior to sampling, yielding fewer false positives. After NSLDS matching, students over the age of 18 still listed as potential FTB students were matched to the National Student Clearinghouse (NSC) for further narrowing of potential FTB students based on evidence of earlier enrollment. Matching to NSLDS identified 19 percent of cases as false positives, and NSC matching identified 14 percent of cases as false positives (table C-1). In addition to NSLDS and NSC, a subset of potential FTB students on the student sampling frame was sent to CPS to evaluate the benefit of the CPS match for the full-scale study. Of the 58,690 students sent, CPS matching identified 10 percent of cases as false positives. Overall, matching to all sources identified 32 percent of potential FTB students over the age of 18 as false positives, with many of the false positives identified by CPS also identified by NSLDS or NSC. The matching appeared most effective among public less-than-2-year and private for-profit institutions. Although public less-than-2-year institutions have a high percentage of false positives, they represent a small percentage of the total sample. Because this presampling matching was new to NPSAS:12, the FTB student sample size was set high to ensure that a sufficient number of true FTB students would be surveyed. In addition, the enlarged FTB student sample size took into account the error rates observed in the 2004 National Postsecondary Student Aid Study (NPSAS:04) and BPS:04/06 within each sector. These rates were adjusted to reflect   "}, {"section_title": "C.2.3 Second Follow-up Data Collection (BPS:12/17)", "text": "Of the 3,500 students in the field test sample in BPS:12/14, about 150 were found to be ineligible (100 NPSAS study members 7 and 50 NPSAS nonstudy members). Of the remaining eligible sample members (3,350), an additional 320 sample members were nonstudy members in the NPSAS:12 field test, and 740 were NPSAS:12 study 7 NPSAS:12 staff identified key variables across the various NPSAS:12 data sources-student records; student surveys; and administrative federal and private databases such as Central Processing System (CPS), National Student Loan Data System (NSLDS), National Student Clearinghouse (NSC), ACT files, and SAT files-to define a minimum set of data points necessary to support the analytic objectives of the study. Sample members for whom those key variables were available were classified as study members-the NPSAS:12 unit of analysis.  Sample members for the BPS:12/17 pilot test consisted of approximately 2,300 of the 3,500 BPS:12/14 study members. The pilot test consisted of NPSAS:12 study members who were surveyed in NPSAS:12, BPS:12/14, or both.  Section 3 outlines the data-collection process for the BPS:12/17 pilot-test student survey and discusses the following: (1) data-collection rates, including statistics such as the proportion of NPSAS:12 (base-year) and first follow-up study members who were located and surveyed; (2) outcomes from both completed interviews and incentive choice, by method of tracing and prior-round response status; and (3) outcomes from an evaluation of student survey items such as timing burden, help texts, item response rates, and cognitive interviews."}, {"section_title": "C.3.1 Pilot-Test Student Survey Data-Collection Outcomes", "text": "This section summarizes the results of the BPS:12/17 pilot-test student survey data collection, including located and interviewed rates, completed surveys by mode of administration, and incentive choice by prior-round response status."}, {"section_title": "C.3.1.1 Student Locating Results and Survey Response Rates", "text": "Limited locating efforts occurred before BPS:12/17 pilot-test data collection. All sampled cases were available for batch tracing efforts and sample members were contacted via the initial contact e-mail and asked to update their contact information before the start of data collection.  Located and interviewed rates by prior-round response status. Staff located and surveyed 51 percent of BPS:12/17 pilot-test sample members (table C-6). Sample members who completed both the NPSAS:12 and BPS:12/14 surveys were more likely to be located and to complete the BPS:12/17 pilot-test survey (63 percent) than sample members who completed only one survey during the two previous rounds of data collection (\u03c72 = 284.5, p < .001). Furthermore, sample members who completed only the BPS:12/14 survey were more likely to be located and complete the BPS:12/17 pilot-test survey (37 percent) than sample members that completed only the NPSAS:12 survey (20 percent) (\u03c72 = 28.1, p < .001). Completed surveys by mode of administration. For BPS:12/17, a single survey instrument was administered in two user modes: web (mobile and nonmobile) and telephone. Web mobile surveys were completed on mobile devices such as smartphones or tablets, while web nonmobile surveys were completed on nonmobile devices such as desktop or laptop computers. To complete by telephone, sample members had to call the help desk and speak with project staff; no outbound calls were made to sample members. Thirty-eight percent of the surveys were completed by web mobile (table C-7). Completion by mobile device was more popular among sample members who were respondents in only one of the prior rounds (i.e., either NPSAS:12 or BPS:12/14, but not both) than among double respondents (i.e., those who completed both the NPSAS:12 and BPS:12/14 surveys (\u03c72 = 12, p < .01)).  "}, {"section_title": "C.3.2 Evaluation of the Pilot-Test Student Survey", "text": "This section provides an evaluation of the student survey items, such as timing burden, help text, and item response rates. This section also includes the results from the evaluation of the predictive coders and results from cognitive testing."}, {"section_title": "C.3.2.1 Survey Timing Burden", "text": "To assess the burden associated with completing the BPS:12/17 pilot-test survey, the time required for each respondent to complete the survey was collected and analyzed. Special attention was paid to differences in the time required to navigate particular survey routing (i.e., paths through the instrument) and items with consistently high administration times. A time stamp was embedded on each form (web screen) to be used in calculating the time required to complete the survey. A start timer recorded the clock time on a respondent's or DCI's computer when a form was first loaded to get the start time; and an end timer recorded the clock time when the Next button on the form was clicked to get the end time. The time for each form was calculated by subtracting the start time from the end time. Total instrument time was calculated by summing the times recorded for each form. Overall, 1,170 surveys were completed in the BPS:12/17 pilot test. Surveys completed in more than one session, partial surveys, and total time outliers were excluded from timing calculations. To detect total time outliers, the distribution of survey times (highly right skewed) was first normalized using a Box-Cox power transformation (Box and Cox 1964). Cases were then excluded using an interquartile range formula (adopted from Tukey 1977) with a multiplier of 1.5. 8 Given these parameters, 950 cases (81 percent of the total completed surveys) were included in the timing analyses reported here. Table C-9 shows the number and percentage of surveys included in and excluded from the analyses in the timing report.  Longest form times. Average times to administer each form (web screen) were examined across all forms in the survey. The forms with the highest average administration times are listed in table C-11. Among the highest average form times were the coders (in decreasing length of time): most recent employer occupation (B17DOCC); original NPSAS major (B17BOMJ1A); and postsecondary institutions attended (B17AOTSCH01). The time required to complete these forms is not unexpected given that coders required the respondent to (1) enter text strings on the form, (2) hit \"Enter\" to conduct a keyword search of an underlying database, and (3) select a response from the returned list of possible matches. The other longest forms were the calendar forms, in which a respondent was asked to select a month or set of months from a calendar. Given that these forms required detailed recall of prior dates, their longer completion time was expected. Timing by use of mobile device. Average completion times and average section completion times by web mobile and web nonmobile modes are shown in table C-12. Although not statistically significantly different, pilot-test surveys completed by web mobile took an average of 22.8 minutes overall, compared with 23.5 minutes for web nonmobile. This finding was a departure from prior rounds in which web mobile surveys took significantly longer than web nonmobile surveys. The small sample size could contribute to the lack of significant differences, as could further improvements to optimize functionality on mobile devices. With regard to sections, there were no significant differences in survey completion times between surveys completed by web mobile and web nonmobile mode except in the Background section. "}, {"section_title": "C.3.2.2 Pilot-Test Experiments", "text": "Several experiments were included in the BPS:12/17 pilot-test student survey to evaluate alternative methods to administer questions. Three experiments tested new designs for assisted coding forms, or coders, used to identify standardized codes for text string responses. A fourth experiment tested revised question wording, using \"forgiving\" introductory language that could mitigate social desirability bias on a question intended to collect data on academic performance through GPA. Evaluation of predictive search coders. Predictive search tools, sometimes called predictive text or suggestive searches, have become commonplace for online search engines and websites. When this style of query is used, potential results appear as the user begins to type a search word. Coders in prior BPS student survey instruments have required the user to completely type the query and then click \"enter\" to begin the search. Sample members were randomized to either a treatment (n = 580) or control (n = 590) group. Members of the treatment group were eligible to receive all four experimental forms, and the control group was administered the nonpredictive search forms. Predictive search methods for survey coders were tested in the BPS:12.17 pilot test for the potential to reduce survey burden by decreasing the time required to code data, including major/field of study, postsecondary institutions, and zip codes. Evaluation of predictive search coders included comparing administration times for forms using predictive searches with times for traditional, nonpredictive searches, and comparing substantive (i.e., nonmissing) responses, measured as an item's percentage of missing data. Evaluation also examined the need for post-datacollection upcoding, in which expert coding staff attempt to identify an appropriate standardized response option for any text strings entered by a respondent for which a code was not selected. The remainder of this section summarizes the experimental forms, the results of the experiments, and the recommendations for the full-scale study. Major/field of study. The traditional major coder (figure C-1) required respondents to enter text strings that were used to perform a keyword search linked to an underlying database. After the respondent typed and submitted the string, the coder returned a series of possible matches for the respondent to review and select from. Using the new predictive major coder (figure C-2), when respondents entered three or more characters into the search field, the form immediately displayed potential matching results beneath the search field. The mean time to complete the experimental form (40.5 seconds) was less than the mean time for the control form (54.3 seconds) (t(835.9) = 3.59, p < .001). The percentage of missing data was not significantly different between the control (3 percent) and treatment (2 percent) groups (\u03c72 (1, n = 1,163) = 0.24, p = .62). As shown in table C-13, about 12 percent of the predictive major experimental coder responses required upcoding, compared with 4 percent for the traditional major control coder. However, after upcoding, 1 percent of text strings in each group could not be upcoded. Postsecondary institution coder. Similar to the major/field of study coder, the traditional postsecondary institution coder (figure C-3) required respondents to enter text strings that were used to perform a keyword search linked to an underlying database. The coder then returned a series of possible matches for the respondent to review and select from. Using the new predictive postsecondary institution coder (figure C-4), after respondents entered three or more characters into the search field, potential matching results were displayed immediately in the search field.  The mean time for the experimental postsecondary institution coder (34.5 seconds) was less than the mean time for the traditional item (54.7 seconds) (t(242.48) = 2.67, p < .01). Among cases in this analysis of timing difference between the control and experimental groups, there were not enough missing responses in either group to test for reduction in missingness. Only one respondent in each group skipped the institution coder form. Fourteen percent of the experimental predictive coder responses required upcoding, compared with 13 percent for the control coder (table C-14). However, after upcoding, 5 percent of remaining text strings for the predictive coder, and 6 percent of text strings for the traditional coder, could not be upcoded. Zip code coder. For the traditional zip code coder, a 5-digit numeric code was entered by the respondent and matched to a zip code database. The traditional form (figure C-5) required that all five digits be entered before executing a search. The experimental zip code coder (figure C-6) provided the ability to match a partially entered zip code to city and state names, resulting in a list of matched zip codes from which respondents could more easily select a response.   The mean time for the experimental, predictive zip code coder (24.3 seconds) was less than that of the control form (33.1 seconds) (t(727.92) = 4.49, p < .001). The percentage of missing data was less for the experimental, predictive zip code coder (7 percent) compared with that of the traditional coder (12 percent) (\u03c72(1, n = 1,961) = 4.20, p < .0001). Upcoding was not performed on partial zip codes. However, with the predictive search coder, if a respondent entered an entire 5-digit zip code but did not click \"enter\" to submit the zip code, the user-submitted data were added to the data file. Table C-15 shows the distribution of zip code results. Evaluation of revised question wording for GPA. Given the sensitive nature of questions related to academic performance and the social desirability of a higher GPA, survey respondents with lower grades may be motivated to inflate their GPAs when self-reporting. One approach to mitigate social desirability bias uses a \"forgiving\" introduction before the question, suggesting normative or otherwise comprehensible behavior (Sudman, Bradburn, and Wansink 2004). As mentioned by Tourangeau and Yan (2007), few studies have examined the validity of data reported using forgiving wording, and results from those have been mixed, showing no difference or small increases in response to sensitive questions (Abelson, Loftus, and Greenwald 1992;Catania et al. 1996;Holtgraves, Eck, and Lasky 1997). In this experiment, the control group was administered a GPA question, structured similarly to that used in the prior-round BPS:12/14, and the treatment group was administered the same question with a forgiving introduction in question wording. Examples of both the control and treatment question are provided in figures C-7 and C-8.   The research questions for the GPA experiment were (1) what impact will the addition of forgiving language have on GPAs reported; (2) will the addition of forgiving language increase the amount of substantive (i.e., nonmissing) responses; and (3) will timing be increased due to the addition of forgiving language? Table C-16 displays the response distribution among the control and treatment groups. To address the research question regarding impacts on reported GPAs, analyses were restricted to GPA responses on the 0-4.0 scale (i.e., excluding missing, \"Don't know my grades,\" and \"Describe grades differently\"). Value means were found to be lower, but not significantly so, among cases administered the experimental form compared with nonexperimental cases (t(1057.84) = .49, p = .63). An ordered probit model was also investigated, although no significant differences were detected between the treatment and control groups. Staff also constructed an indicator variable for values corresponding to a GPA at or above 3.25 to analyze the impact that the addition of forgiving language had on reported GPAs. This cut point was chosen to ensure the lower grade boundary included Bs. A logistic regression model was then used to examine the likelihood of a student being in the treatment group using the indicator, specified by where grade is a binary indicator for a student reporting a GPA of 3.25 or higher. Results, however, indicated that there was no statistically discernible difference between the treatment and control groups. Table C-17 shows the results of this model. There were insufficient missing data to compare the control and experimental forms; only one respondent (in the control group) skipped the GPA form. No significant difference was found between the mean time to complete the control form (16.3 seconds) and the treatment form (18.0 seconds) (t(903.99) = 1.36, p = .18). "}, {"section_title": "C.3.2.3 Help Text", "text": "During the BPS:12/17 pilot-test survey, respondents were able to click a help button provided on each survey screen to obtain question-specific help text. In addition, some questions included embedded hyperlinks to the help text on specific terms in the question itself. Whether accessed through the help button or through a hyperlink, each question had unique help text that provided definitions of key terms and phrases used in question wording and response options, and provided any other explanations thought to help clarify and standardize meaning for respondents. The number of times that respondents clicked the help button on each screen relative to the number of respondents who were administered the question determined the rate of help text access for that screen. Staff analyzed the screen-level rate of help text access overall and by mode of survey administration to identify screens that may have been problematic for respondents. Eight interview questions (forms) were administered to at least 10 respondents and had an overall help text access rate of 1 percent or greater (table C-18). Ever taken out undergraduate private loans (B17CEVRPRVLN) had the highest rate of overall help text access, at 6 percent. This form asked respondents if they had ever taken out any private loans for undergraduate education. However, help text access rates did not differ significantly between web or mobile modes for this form. Three forms, including Employer 1: ZIP code of employer (B17DJOBZIP01), at 2.2 percent access overall; Estimate of grades at primary school (B17BGPAEST), at 2.0 percent overall; and Employer 1: Months worked for pay in Years 4-6 (B17DWKMON01), at 1.4 percent overall all showed significantly more help text access on web platforms compared to mobile devices (\u03c7 2 (1, N = 516) = 5.72, p < .05; \u03c7 2 (1, N = 569) = 5.8, p < .05; and \u03c7 2 (1, N = 505) = 5.91, p < .05, respectively). All of the remaining forms did not differ significantly in help text access rates between web and mobile devices.  "}, {"section_title": "C.3.2.4 Item-Level Nonresponse", "text": "Staff used the rate of nonresponse to individual items to identify potentially troublesome items and better understand the experiences of sample members in completing the survey. Staff calculated total nonresponse rates for items with missing data (including don't know responses) that were administered to at least 10 respondents. Overall, the item-level nonresponse analysis showed that, of 960 items, seven items had more than 10 percent missing data. 10 Table C-19 summarizes the item-level nonresponse for items administered to at least 10 respondents with a rate of more than 10 percent missing data.  "}, {"section_title": "C.3.2.5 Cognitive Testing", "text": "In addition to the metrics used in this section to analyze the performance of the survey instrument, cognitive testing was conducted to collect information on users' interactions with the survey instrument, with a particular emphasis on new or modified items. Cognitive testing provides insights on the cognitive processes of respondents with characteristics similar to BPS respondents. Respondents provided feedback on their comprehension of questions, retrieval of relevant information, decision processes for answering questions, navigation through the instrument and interface with response options, and ability to align their responses to each question's response options. Two rounds of cognitive testing were conducted. The first round was completed using a reduced set of questions in a pen-and-paper format. The second round was conducted with the complete web-based pilot-test instrument on either a personal computer or handheld device. No telephone interviews were conducted. Respondents provided informed consent before beginning the process. They then participated in cognitive interviews that lasted 90 minutes and were given $40 upon completion. In both rounds, interviewers used scripted cognitive probes during the interview process to elicit information about items of interest. Scripted probes were refined as needed throughout cognitive interviewing based on the responses. Spontaneous probes were also used at the interviewer's discretion when the respondent asked questions, seemed hesitant to provide a confident answer, or made comments regarding the items being tested. Recruitment for cognitive interviewing was targeted to reach, screen, and enroll enough respondents to complete 40 interviews over two rounds of interviewing. Effort was made to accurately replicate the experience that respondents would have in the full-scale collection, and to elicit feedback from students with experiences like those of BPS:12/17 full-scale sample members. Thus, to reflect the types of students participating in the BPS:12/17 full-scale collection, respondents were recruited from three institution levels-4-year, 2-year, and less-than 2-year institutions-and from different enrollment statuses. Table C-20 shows the final status of the 280 participants who were recruited for cognitive interviews.   Input from cognitive interview subjects was used for decisionmaking regarding several items that had not been included in prior BPS surveys. This input was presented to members of the TRP and considered when making decisions about the contents of the full-scale survey. In particular, a question regarding high-impact activities was removed from the survey in part due to cognitive interview subjects' lack of familiarity with the response options, lack of ability to distinguish between response options, and high likelihood of false positive responses (i.e., high likelihood that respondents would incorrectly report participating in such activities). Cognitive interview subjects also provided comments that influenced decisions not to include questions about a spouse's student loans. Several respondents indicated that they were uncomfortable answering these questions about their spouse or were not confident recalling the details of their loans. A cognitive interview question about various dimensions of job satisfaction also yielded useful feedback for the full-scale survey. Predictive search coders were also tested during the second round of cognitive interviews. Subjects were given the opportunity to view both predictive and nonpredictive coders. Subjects were split on their preferences for one coder type versus the other, but neither coder type presented problems or misunderstandings for respondents. Cognitive interview subjects understood what information the coders were asking for and were able to respond accordingly."}, {"section_title": "C.4.1 Data-Collection Procedures", "text": "Pilot-test batch tracing successfully confirmed existing contact information, or provided additional contacting information, for 95 percent of cases sent to all tracing sources. Although limited locating efforts occurred before pilot-test data collection, staff located and surveyed approximately 51 percent of BPS:12/17 pilot-test sample members. The pilot test offered respondents a choice of three incentive options: check, PayPal, or eGift card. The eGift card option was offered for the first time in order to increase electronic incentive options for study participants and increase response rates. However, as discussed in section 3.1, checks were the most popular incentive choice (56 percent) among study participants, followed by PayPal (30 percent). The percentage of participants who chose the eGift card option was small (14 percent overall). Given the cost to provide this incentive option, project staff recommend offering only checks and PayPal as incentive options during the full-scale study. The pilot test incorporated three experiments to test new designs for assisted coding forms, used to identify standardized codes for text string responses. Based on evaluation of pilot-test results-including comparison of administration times, substantive (i.e., nonmissing) responses, and the need for upcoding-for predictive and traditional coders, the decision was made to use only predictive coders in the full-scale collection. A fourth experiment tested the effect of using forgiving wording in a question about GPAs on GPAs reported, the number of substantive responses, and timing. Because the forgiving wording did not result in significantly different substantive responses and did not demonstrate faster administration time, and because and there was no significant difference in the amount of missing data between the forms, it was not adopted for the full-scale collection."}, {"section_title": "C.4.2 Student Survey", "text": "Based on analysis of timing burden and item response rates, cognitive testing, and input from the TRP, some pilot-test items were removed or modified for the fullscale survey, as shown in   Removed: comparable data can be gathered from administrative sources; removal of the question reduces respondent burden. Financial aid B17CEVRFEDLN Ever taken out undergraduate federal student loans Removed: comparable data can be gathered from administrative sources; removal of the question reduces respondent burden."}, {"section_title": "Financial aid B17CFDRYST", "text": "Currently repaying undergraduate federal student loan Removed: comparable data can be gathered from administrative sources; removal of the question reduces respondent burden."}, {"section_title": "Financial aid B17CFLNMOS", "text": "Monthly undergraduate federal student loan payment Removed: comparable data can be gathered from administrative sources; removal of the question reduces respondent burden."}, {"section_title": "Financial aid B17CFAMLN", "text": "Anyone helping to repay loans Removed: item was not considered to be analytically valuable enough to justify burden on the respondent."}, {"section_title": "Financial aid B17CLNINC", "text": "Have loans influenced life situation or decisions Removed: item was not considered to be analytically valuable enough to justify burden on the respondent. Financial aid B17CLNICA How student loans have influenced life situation or decisions \u2212 Took job outside of field of study or training \u2212 Had to work more hours than desired \u2212 Had to work more than one job at the same time \u2212 Postponed attending graduate program in effort to being paying off loans \u2212 Could not afford to buy or keep a car \u2212 Had to delay purchasing a home \u2212 Had to move back in with parents or other family members \u2212 Other reason \u2212 None of the above Removed: item was not considered to be analytically valuable enough to justify burden on the respondent."}, {"section_title": "Financial aid B17CSPLN", "text": "Spouse taken out student loans Removed: item does not provide adequate analytic utility to offset respondent burden, and cognitive interviews indicated that respondents were uncomfortable giving this information about their spouse. Removed: the information desired from this item can be derived using previously collected data and by adjusting data collected for all jobs in the BPS:12/17 full-scale instrument."}, {"section_title": "Employment B17DFIRSTPAY", "text": "Pay in first job after enrollment Removed: the information desired from this item can be derived using previously collected data and by adjusting data collected for all jobs in the BPS:12/17 full-scale instrument."}, {"section_title": "Employment B17DFIRSTHRS", "text": "Hours worked in first job after enrollment Removed: the information desired from this item can be derived using previously collected data and by adjusting data collected for all jobs in the BPS:12/17 full-scale instrument."}, {"section_title": "Employment B17DCAREER", "text": "Consider first job part of career Removed: the information desired from this item can be derived using previously collected data and by adjusting data collected for all jobs in the BPS:12/17 full-scale instrument.  "}, {"section_title": "Employment", "text": ""}, {"section_title": "Income and expenses", "text": "\n\nB17EFAMLOAMT Amount of help from other family/friends for education or living expenses in 2015-16 school year Removed: analytical value is diminished in the second follow-up, when most of the cohort will no longer be enrolled; removal also reduces respondent burden.\nB17EFAMHIAMT Amount of help from other family/friends for education or living expenses in 2015-16 school year Removed: analytical value is diminished in the second follow-up, when most of the cohort will no longer be enrolled; removal also reduces respondent burden.   To review the letter we mailed, click here (PDF letter). To review the study brochure, click here (PDF brochure). See notes at end of table. To enter your major (or field of study): First type your major at [NPSAS] into the textbox, then click \"ENTER\" and a list of majors that most closely matches your entry will be displayed. From the responses displayed, click \"Select\" next to the major that most closely matches your entry and click \"Keep answer and continue\" on the confirmation box if this is your major. You will then be taken to the next question in the survey. Click \"Change answer\" on the confirmation box if the description of the major does not closely match the major you entered and review the other options that were returned. If your major is not listed in the list of majors displayed, click \"None of the above\" at the bottom of the list of majors and choose descriptions of the major from the dropdown boxes that appear.            [else] Was it\u2026 1 =Less than $250 2 =$250 -$500 3 =$501 -$1,000 4 =$1,001 -$1,500 5 =$1,501 -$2,000"}, {"section_title": "B17EFAMHELP", "text": "Help from other family/friends for education or living expenses in 2015-16 school year Removed: analytical value is diminished in the second follow-up, when most of the cohort will no longer be enrolled; removal also reduces respondent burden."}, {"section_title": "B17EFAMGATE", "text": "Amount of help from other family/friends for education or living expenses in 2015-16 school year Removed: analytical value is diminished in the second follow-up, when most of the cohort will no longer be enrolled; removal also reduces respondent burden."}, {"section_title": "B17EFAMHIAMT", "text": "(removed for full-scale) [Before July 2016] Is it..."}, {"section_title": "[else]", "text": "Was it\u2026 1 =$2,001 -$5,000 2 =$5,001 -$10,000 3 =$10,001 -$15,000 4 =$15,001 -$20,000 5 =$20,001 -$25,000 6 =More than $25,000 See notes at end of table.  The core data elements used in the 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17) full-scale student survey covered general topics that were organized into six key content areas: enrollment, education experiences, financial aid, employment, income and expenses, and background. Table D-1 provides a list of the data elements by section and topic. Some data elements were only administered to certain subgroups of respondents, as also noted in table D-1. For example, students who were nonrespondents in both the 2011-12 National Postsecondary Student Aid Study full-scale survey and the 2012/14 Beginning Postsecondary Students Longitudinal Study (BPS:12/14) fullscale survey, known as \"double nonrespondents,\" were required to satisfy eligibility for base-year and first-time beginning students to remain part of the BPS cohort and therefore received additional questions in the survey.   (Please choose all that apply) -Associate's degree (usually a 2-year degree) -Bachelor's degree (usually a 4-year degree) -Master's degree -Doctoral degree--research/scholarship (for example, PhD, EdD, etc.) -Doctoral degree--professional practice (including: chiropractic, dentistry, law, medicine, optometry, pharmacy, podiatry, or veterinary medicine) -Doctoral degree--other -1=Yes -0=No"}, {"section_title": "B17ABYE*", "text": "Based on your responses, it seems you may not be eligible for this study. We will review your responses and we may need to contact you again. -Please provide your e-mail address: -Please provide an address where you can be contacted: Street Address:   -1=Undergraduate level classes -2=Undergraduate certificate or diploma (usually less than 2 years), including those leading to a license (example: cosmetology) -3=Associate's degree (usually a 2-year degree) -4=Bachelor's degree (usually a 4-year degree) -5=Graduate level classes -6=Post-baccalaureate certificate -7=Master's degree -8=Post-master's certificate -9=Doctoral degree--research/scholarship (for example, PhD, EdD, etc.) -10=Doctoral degree--professional practice (including: chiropractic, dentistry, law, medicine, optometry, pharmacy, podiatry, or veterinary medicine) -11=Doctoral degree--other     "}, {"section_title": "B17ANPCUR01", "text": ""}, {"section_title": "B17AEXPEVR", "text": "What is the highest level of education you ever expect to complete at any school? -1=Undergraduate level courses, no undergraduate degree or certificate expected -2=Undergraduate certificate or diploma (usually less than 2 years), including certificates leading to a license (example: cosmetology) -3=Associate's degree (usually a 2-year degree) -4=Bachelor's degree (usually a 4-year degree) -5=Graduate level courses, no graduate degree or certificate expected -6=Post-baccalaureate certificate -7=Master's degree -8=Post-master's certificate -9=Doctoral degree, research/scholarship (including: PhD, EdD, etc.) -10=Professional doctoral degree (including: chiropractic, dentistry, law, medicine, etc.)"}, {"section_title": "B17AMARR", "text": "Thank you for describing your previous enrollment. The remainder of the survey asks about your latest experiences in college and your employment and family situations. So that [{if USERMODE = CATI} I {else} we] can ask you the right set of questions in the survey, please indicate your current marital status. -3=Vocational/technical training -5=Associate's degree (usually a 2-year degree) -6=Some college but no degree -7=Bachelor's degree (usually a 4-year degree) -8=Master's degree or equivalent -9=Professional degree (chiropractic, dentistry, law, medicine, optometry, pharmacy, podiatry, or veterinary medicine) -10=Doctoral degree (PhD, EdD, etc.) -11=Don't know"}, {"section_title": "B17BREMEVER", "text": "Remedial or developmental courses are used to strengthen your skills in math, reading, or other subjects. Students are usually assigned to these courses on the basis of a placement test taken before the school year begins. Often, these courses do not count for credit toward graduation. "}, {"section_title": "B17CLOANINT", "text": "You just indicated you took out undergraduate student loans for the 2016-2017 school year. There are two main types of loans: Private student loans are borrowed from a private lender, such as a bank (or sometimes a state), and usually require a co-signer. Federal student loans, such as subsidized and unsubsidized Direct Loans (previously known as Stafford Loans), are borrowed from the federal government. The next questions will ask about private student loans."}, {"section_title": "B17CPRVLN", "text": "Thinking only about the 2016-2017 school year, did you take out any private loans borrowed from a private lender for your undergraduate education? (Private loans are borrowed from a private lender such as a bank or sometimes a state, usually require a cosigner, and have market interest rates based on credit history. Click here for examples of private loans [end helplink].) -1=Yes -0=No"}, {"section_title": "B17CPRVAMT", "text": "For the 2016-2017 school year, how much did you take out in private loans for your undergraduate education? Do not include any money taken out in federal loans or any money borrowed from family or friends in your answer. (If you are unsure of the amount of your private loans, please provide your best guess.) -$|.00"}, {"section_title": "B17CPRVEST", "text": "For the 2016-2017 school year, please indicate the range for how much you took out in private loans for your undergraduate education. Would you say it was... -1=Less than $3,000 -2=$3,000-$5,999 -3=$6,000-$8,999 -4=$9,000-$11,999 -5=$12,000-$14,999 -6=$15,000-$17,999 -7=$18,000 or more -8=Don't know"}, {"section_title": "B17CBPSRCVLN", "text": "In the 2013-2014 school year, you told us you had taken out undergraduate student loans for your education. Is that correct? -1=Yes -0=No, I did not take out student loans in 2013-2014 "}, {"section_title": "B17CEVRRCVLN", "text": "Have you ever taken out any undergraduate student loans for your education? -1=Yes -0=No"}, {"section_title": "B17CLOANINT2", "text": "You just indicated you have taken out undergraduate student loans for your education. There are two main types of loans: Private student loans are borrowed from a private lender, such as a bank (or sometimes a state), and usually require a co-signer. Federal student loans, such as subsidized and unsubsidized Direct Loans (previously known as Stafford Loans), are borrowed from the federal government."}, {"section_title": "B17CBPSPRVLN", "text": "In the 2013-2014 school year, you told us you had taken out private loans borrowed from a private lender for your undergraduate education. Is that correct? (Private loans are borrowed from a private lender such as a bank or sometimes a state, usually require a cosigner, and have market interest rates based on credit history. [help text hyperlink] Click here for examples of private loans [end help text hyperlink]). -0=No, I did not take out private student loans in 2013-2014 -1=Yes"}, {"section_title": "B17CEVRPRVLN*", "text": "Have you ever taken out any private loans borrowed from a private lender for your undergraduate education? (Private loans are borrowed from a private lender such as a bank or sometimes a state, usually require a co-signer, and have market interest rates based on credit history. Click here for examples of private loans.)"}, {"section_title": "B17CTLPRVAMT", "text": "What is the total amount that you have taken out in private loans for your entire undergraduate education? Do not include any money taken out in federal loans or any money borrowed from family or friends in your answer. (If you are unsure of the amount of your private loans, please provide your best guess.) -$|.00"}, {"section_title": "B17CTLPRVEST", "text": "Please indicate the range for the total amount you have taken out in private loans for your entire undergraduate education. Would you say it was... -1=Less than $3,000 -2=$3,000 -$5,999 -3=$6,000 -$9,999 -4=$10,000 -$19,999 "}, {"section_title": "B17DACTLKWRK", "text": "At any point between July 2014 and June 2017, were there times when you were actively looking for work? (Actively looking for work includes activities such as sending out resumes or filling out job applications, placing or responding to job advertisements, contacting employers or employment agencies, and going on job interviews.) -1=Yes -0=No"}, {"section_title": "B17DLKWRK", "text": "Between July 2014 and June 2017, in which months were you not working and actively looking for a job for any part of the month? -July 2014 -June 2017"}, {"section_title": "B17DUNCMP", "text": "Did you receive unemployment compensation at any point between July 2014 and June 2017 while you were not working? -1=Yes -0=No"}, {"section_title": "B17DEDBENFTS", "text": "When thinking about a job now or in the future, salary may be only one part of why you choose that job. Compared to the salary, how important is each of the following to you? -Helping others as part of your job -Being seen as an expert in your field -Making your own decisions about how to get your work done -Balancing work and leisure time -Balancing work and family 1=Less important than salary 2=As important as salary 3=More important than salary"}, {"section_title": "B17EINCINTRO", "text": "The next set of questions will help us to better understand how the financial circumstances of students may affect their ability to attend college. -1=No income -2=Less than $1,000 -3=$1,000-$2,499 -4=$2,500-$4,999 -5=$5,000-$9,999 -6=$10,000-$14,999 -7=$15,000-$19,999 -8=$20,000-$29,999 -9=$30,000-$49,999 -10=$50,000 and above -11=Don't know"}, {"section_title": "B17EINCOM*", "text": ""}, {"section_title": "B17EINCSP", "text": "What was your spouse's income for calendar year 2016, prior to taxes and deductions? (Calendar year 2016 includes January 1, 2016 through December 31, 2016. Include all income your spouse paid taxes on. Do not include any grants or loans your spouse may have used to pay for school, or any money given to your spouse by family.) -1=No income -2=Less than $1,000 -3=$1,000-$2,499 -4=$2,500-$4,999 -5=$5,000-$9,999 -6=$10,000-$14,999 "}, {"section_title": "B17EDEP2", "text": "How many children did you support financially at any time between July 2016 and June 2017?"}, {"section_title": "B17EOTDEPS", "text": "Were you financially supporting anyone else at any time between July 2016 and June 2017?"}, {"section_title": "B17EOTDEPS2", "text": "How many others were you financially supporting at any time between July 2016 and June 2017? How many of your dependents attended a college, university, or trade school in the 2016-2017 school year (July 1, 2016-June 30, 2017)?"}, {"section_title": "B17EKIDCOL1", "text": "Did your dependent attend a college, university, or trade school in the 2016-2017 school year (July 1, 2016-June 30, 2017)? -1=Yes -0=No"}, {"section_title": "B17EREGSUPP", "text": "Since July 2014, have you regularly given any friends or family who do not live with you more than $50 per month to help them out? -1=Yes -0=No"}, {"section_title": "B17EPARHELP", "text": "Between July 2014 and June 2017, did your parents or will your parents help you pay for any of your education or living expenses? [{If BEFORE JULY 2017 and currently enrolled} If you plan on attending school this year, will your parents help pay for any of your education or living expenses?] (Tuition and fees or school books are examples of education expenses. Rent and food are examples of living expenses.) "}, {"section_title": "B17EPARGATE", "text": "Ok. Was this amount: -1=$2,000 or less -2=More than $2,000"}, {"section_title": "B17EPARLOAMT", "text": "Was it... -1=Less than $250 -2=$250 -$500 -3=$501 -$1,000 -4=$1,001 -$1,500 -5=$1,501 -$2,000"}, {"section_title": "B17EPARHIAMT", "text": "Was it... -1=$2,001 -$5,000 -2=$5,001 -$10,000 -3=$10,001 -$15,000 -4=$15,001 -$20,000 -5=$20,001 -$25,000 -6=More than $25,000"}, {"section_title": "B17ENUMCRD", "text": "Excluding debit or ATM cards, how many credit cards do you have in your own name that are billed to you? (Only include credit cards for which you pay at least some of the amount owed.) -0=None -1=One -2=More than one B17ECARRYBAL Do you usually owe an amount that is carried over on your credit cards from month to month? -1=Yes -0=No"}, {"section_title": "B17ECRDBAL", "text": "What was the total amount you owed on your credit card according to your last month's statement? -$|.00"}, {"section_title": "B17ECCPAYMT", "text": "What was the total amount you paid toward your credit card statement last month? "}, {"section_title": "B17EPARST", "text": "What is the current marital status of your parents or guardians? If your parents are divorced, please answer this question about the marital status of the parent or guardian whom you lived with most during the past 12 months. (If you did not live with one parent more than the other, answer about the parent who provided more financial support during the last 12 months, or during the most recent year that you received support from a parent.) -1=Married or remarried -2=Single -3=Divorced or separated -4=Widowed -5=None of the above -Both parents or guardians are deceased"}, {"section_title": "B17EPARNC", "text": "In calendar year 2016 (January 1, 2016 through December 31, 2016), what was the income, prior to taxes and deductions, of the parent or guardian whom you lived with most in the past 12 months? Would you say it was... (If you did not live with one parent more than the other, answer about the parent who provided more financial support during the last 12 months, or during the most recent year that you received support from a parent.) -1=Under $30,000 -2=$30,000 to $59,999 -3=$60,000 to $89,999 -4=$90,000 to $119,999 -5=$120,000 and above -6=Don't know Parents (or guardians) are deceased "}, {"section_title": "B17EPRHSD", "text": "Not including yourself or the parent or guardian whom you lived with most in the past 12 months, how many people (for example, brothers or sisters or grandparents) did this parent or guardian support financially during the most recent term you attended school in the 2016-2017 school year? (If you did not live with one parent more than the other, answer about the parent who provided more financial support during the last 12 months, or during the most recent year that you received support from a parent.) Not including yourself or your parent or guardian, how many of the people financially supported by your parent or guardian attended a college, university, or trade school during the most recent term you attended school in the 2016-2017 school year? "}, {"section_title": "B17FDISTNC", "text": "What was the 5-digit ZIP code of your permanent address when you last attended [{If PRIMARY POSTSECONDARY SCHOOL = missing} school {else} PRIMARY POSTSECONDARY SCHOOL]? Your permanent address is usually your legal residence, such as where you maintain your driver's license or are registered to vote. -Check here if the location is not in the United States or a US territory."}, {"section_title": "B17FMILIT", "text": "Are you a veteran of the U.S. Armed Forces, or are you currently serving in the Armed Forces either on active duty, in the reserves, or in the National Guard? (Please choose all that apply) -Veteran -Active Duty -Reserves -National Guard -None of the above"}, {"section_title": "B17FPHYSH", "text": "In general, how is your physical health?"}, {"section_title": "B17FMENTH", "text": "In general, how is your mental health? "}, {"section_title": "B17FMISSH", "text": "In the past 30 days, how often did a physical or mental health concern cause you to miss a day of school or work? -1=Never -2=A few times -3=About once a week -4=Almost every day -5=Every day"}, {"section_title": "B17FPRSVT", "text": "Did you happen to vote in the 2016 presidential election?"}, {"section_title": "B17F2000", "text": "How confident are you that you could come up with $2,000, from any available source, if an unexpected need arose within the next month? -1=I am certain I could come up with the full $2,000 -2=I could probably come up with $2,000 -3=I could probably not come up with $2,000 -4=I am certain I could not come up with $2,000"}, {"section_title": "B17FINTRST", "text": "Suppose you had $100 in a savings account and the interest rate was 2% per year. After 5 years, how much do you think you would have in the account if you left the money to grow? -1=More than $102 -2=Exactly $102 -3=Less than $102"}, {"section_title": "B17FINFLAT", "text": "Imagine that the interest rate on your savings account was 1% per year and inflation was 2% per year. After 1 year, how much would you be able to buy with the money in this account? -1=More than today -2=Exactly the same -3=Less than today"}, {"section_title": "B17FSTOCK", "text": "Do you think that the following statement is true or false? \"Buying a single company stock usually provides a safer return than a stock mutual fund.\" "}, {"section_title": "B17FWDFALL", "text": "If your household somehow were to get an extra unexpected $25,000 in the next few weeks, what would it do with the money? -1=Spend it on something that the household wants or needs -2=Pay off some household debts -3=Put it in savings or investments -4=Donate it to family or charity -5=Other"}, {"section_title": "B17FFEDACT", "text": "If a borrower is unable to repay their federal student loan, what steps can the government take to collect the debt? -Report that the student debt is past due to the credit bureaus -Garnish wages until the debt, plus any interest and fees, is repaid -Retain tax refunds and Social Security payments until the debt, plus any interest and fees, is repaid -None of the above Recently, we contacted you about your selection for the Beginning Postsecondary Students Longitudinal Study (BPS), a survey of students who first enrolled in postsecondary education during the 2011-12 school year. Data collection for BPS is now underway, and we would like to invite you to complete the survey. After you complete the survey, you will receive $30 as a token of our appreciation, payable by PayPal or check. The survey will take approximately 30 minutes to complete and can even be completed on your mobile device. To complete the survey, log on to our secure website at https://surveys.nces.ed.gov/bps/ using the Study ID and password below: Study ID = Your Study ID Password = Your password (password is case sensitive). You represent many students like you who were not selected for BPS and your participation is important to the success of the study. If you have questions, problems completing your survey online, or prefer to complete the survey over the telephone, simply call the BPS Help Desk at (800) 334-2321.                                                                              11.00 \u2020 Not applicable. 1 DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical). NOTE: PLUS = Parent Loan for Undergraduate Students. Responses that included logical skips, not applicable, missing, unknown, or multiple possible answers were excluded from analysis. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17). 1 DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical). 3 DEFT and DEFF values denoted as \"Not applicable\" were excluded from the calculations of the summary statistics.  1 DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical). 3 DEFT and DEFF values denoted as \"Not applicable\" were excluded from the calculations of the summary statistics. NOTE: PLUS = Parent Loan for Undergraduate Students. Responses that included logical skips, not applicable, missing, unknown, or multiple possible answers were excluded from analysis. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17). 1 DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical). 3 DEFT and DEFF values denoted as \"Not applicable\" were excluded from the calculations of the summary statistics. NOTE: PLUS = Parent Loan for Undergraduate Students. Responses that included logical skips, not applicable, missing, unknown, or multiple possible answers were excluded from analysis. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17). Table J-5. Design effects, using the cross-sectional weight, for selected variables for first-time beginning students whose base-year institution was private for-profit less-than-2-year: 2017 1 DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical). 3 DEFT and DEFF values denoted as \"Not applicable\" were excluded from the calculations of the summary statistics. NOTE: PLUS = Parent Loan for Undergraduate Students. Responses that included logical skips, not applicable, missing, unknown, or multiple possible answers were excluded from analysis. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17). Table J-6. Design effects, using the cross-sectional weight, for selected variables for first-time beginning students whose base-year institution was private for-profit 2-year or 4-year: 2017 DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  J-7. Design effects, using the cross-sectional weight, for selected variables for first-time beginning students whose base-year institution was public less-than-2-year or private nonprofit less-than-4-year: 2017 1 DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical). 3 DEFT and DEFF values denoted as \"Not applicable\" were excluded from the calculations of the summary statistics. NOTE: PLUS = Parent Loan for Undergraduate Students. Responses that included logical skips, not applicable, missing, unknown, or multiple possible answers were excluded from analysis. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17). DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical). NOTE: PLUS = Parent Loan for Undergraduate Students. Responses that included logical skips, not applicable, missing, unknown, or multiple possible answers were excluded from analysis. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17). DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical  DEFT is the square root of DEFF and can also be defined as the ratio of the design-based standard error over the standard error that would have been obtained from a simple random sample of the same size (if that were practical). 2 DEFF is the survey design effect for a statistic and is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical          J-31. Design effects, using the panel weight, for selected variables for first-time beginning students whose base-year institution was public less-than-2-year or private nonprofit less-than-4-year: 2017                        Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent; GASB = Governmental Accounting Standards Board. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.         Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FTE = full-time-equivalent; GASB = Governmental Accounting Standards Board. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.      Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FTE = full-time-equivalent; GASB = Governmental Accounting Standards Board. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.      Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FTE = full-time-equivalent; GASB = Governmental Accounting Standards Board. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.     1 Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.      .95 \u2021 Reporting standards not met (fewer than 30 unweighted nonrespondents). * p < 0.05 1 Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.      1 Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.      1 Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.      Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.      1 Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.      Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent; GASB = Governmental Accounting Standards Board. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.        Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FTE = full-time-equivalent; GASB = Governmental Accounting Standards Board. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.      Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FTE = full-time-equivalent; GASB = Governmental Accounting Standards Board. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.      1 Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FTE = full-time-equivalent; GASB = Governmental Accounting Standards Board. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.     Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles.      1 Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.      # Rounds to zero. \u2021 Reporting standards not met (fewer than 30 unweighted nonrespondents). * p < 0.05 1 Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.      1 Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.     .00 \u2021 Reporting standards not met (fewer than 30 unweighted nonrespondents). * p < 0.05 1 Bias in the sample mean is estimated as the difference between the mean of respondent cases and the mean of all sample cases, using the base weight. 2 Relative bias is calculated as 100 times the ratio of estimated bias to the weighted full-sample mean. 3 Base weight, adjusted for nonresponse. 4 Bias in the sample mean is estimated as the difference between the weighted respondent mean (using the base weight adjusted for nonresponse) and weighted full-sample mean (using the base weight). 5 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. Sample sizes rounded to the nearest 10. Effect size is calculated as the square root of the sum over categories of the squared differences (respondent vs. full sample) over full-sample means. Effect sizes are not reported if any variable categories do not meet reporting standards. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles.             2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FTE = full-time-equivalent; GASB = Governmental Accounting Standards Board. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17).         2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17).   2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17).   2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.    2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17).   2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17).   2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.        2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: . CPS = Central Processing System; FTE = full-time-equivalent; GASB = Governmental Accounting Standards Board. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.    2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FTE = full-time-equivalent; GASB = Governmental Accounting Standards Board. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility.    2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FTE = full-time-equivalent; GASB = Governmental Accounting Standards Board. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17).   2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles.    2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17).   2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles.    2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17).   2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17).   2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. NOTE: CPS = Central Processing System; FASB = Financial Accounting Standards Board; FTE = full-time-equivalent. \"Base weight\" refers to the student sampling weight (final institution weight times student sampling adjustment) adjusted for student multiplicity and unknown eligibility. Categories for Institution total enrollment, Pell Grant amount, Stafford Loan amount, PLUS amount, Percentage of students receiving any grant aid, and Graduation rate were defined by quartiles. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2012/17 Beginning Postsecondary Students Longitudinal Study (BPS:12/17)."}]