[{"section_title": "Abstract", "text": "Abstract -Brain extraction is an important step in the analysis of brain images. The variability in brain morphology and the difference in intensity characteristics due to imaging sequences make the development of a general purpose brain extraction algorithm challenging. To address this issue, we propose a new robust method (BEaST) dedicated to produce consistent and accurate brain extraction. This method is based on nonlocal segmentation embedded in a multi-resolution framework. A library of 80 priors is semi-automatically constructed from the NIH-sponsored MRI study of normal brain development, the International Consortium for Brain Mapping, and the Alzheimer's Disease Neuroimaging Initiative databases.\nIn testing, a mean Dice similarity coefficient of 0.9834\u00b10.0053 was obtained when performing leave-one-out cross validation selecting only 20 priors from the library. Validation using the online Segmentation Validation Engine resulted in a top ranking position with a mean Dice coefficient of 0.9781\u00b10.0047. Robustness of BEaST is demonstrated on all baseline ADNI data, resulting in a very low failure rate. The segmentation accuracy of the method is better than two widely used publicly available methods and recent state-of-the-art hybrid approaches. BEaST provides results comparable to a recent label fusion approach, while being 40 times faster and requiring a much smaller library of priors."}, {"section_title": "Introduction", "text": "Brain extraction (or skull stripping) is an important step in many neuroimaging analyses, such as registration, tissue classification, and segmentation. While methods such as the estimation of intensity normalization fields and registration do not require perfect brain masks, other methods such as measuring cortical thickness rely on very accurate brain extraction to work properly. For instance, failure to remove the dura may lead to an overestimation of cortical thickness (van der Kouwe et al., 2008) , while removing part of the brain would lead to an underestimation. In cases of incorrect brain extraction, subjects may be excluded from further processing, a potentially expensive consequence for many studies. The solution of manually correcting the brain masks is a labour intensive and time-consuming task that is highly sensitive to inter-and intra-rater variability (Warfield et al., 2004 ).\nAn accurate brain extraction method should exclude all tissues external to the brain, such as skull, dura, and eyes, without removing any part of the brain. The number of methods proposed to address the brain segmentation problem reflects the importance of accurate and robust brain extraction. During the last 15 years, more than 20 brain extraction methods have been proposed using a variety of techniques, such as morphological operations (Goldszal et al., 1998; Lemieux et al., 1999; Mikheev et al., 2008; Park and Lee, 2009; Sandor and Leahy, 1997; Ward, 1999) , atlas matching (Ashburner and Friston, 2000; Kapur et al., 1996) , deformable surfaces (Dale et al., 1999; Smith, 2002) , level sets (Baillard et al., 2001; Zhuang et al., 2006) , histogram analysis (Shan et al., 2002) , watershed (Hahn and Peitgen, 2000) , graph cuts (Sadananthan et al., 2010) , label fusion (Leung et al., 2011) , and hybrid techniques (Carass et al., 2011; Iglesias et al., 2011; Rehm et al., 2004; Rex et al., 2004; Segonne et al., 2004; Shattuck et al., 2001) . Studies evaluating these methods have found varying accuracy (Boesen et al., 2004; Fennema-Notestine et al., 2006; Hartley et al., 2006; Lee et al., 2003; Park and Lee, 2009; Shattuck et al., 2009 ).\nWhile some methods are better at removing non-brain tissue, at the cost of removing brain tissue, others are better at including all brain tissue, at the cost of including non-brain tissue (FennemaNotestine et al., 2006; Shattuck et al., 2009 ). This is a classic example of the trade-off between sensitivity and specificity."}, {"section_title": "inserm-00629187, version 1 -5 Oct 2011", "text": "Beyond the technical issues, the brain extraction problem is further complicated by the fact that no accepted standard exists for what to include in brain segmentation. While there is consensus among methods that obvious non-brain structures, such as skull, dura, and eyes should be removed as part of the brain extraction process, there are divergent opinions on other structures and tissues, such as the amount of extra-cerebral cerebro-spinal fluid (CSF), blood vessels, and nerves. Some methods define the target segmentation as white matter (WM) and gray matter (GM) only (Leung et al., 2011) , while others include CSF, veins, and the optic chiasms (Carass et al., 2011; Smith, 2002) . Depending on the objective for the subsequent analysis it is important to remove tissues that may be confused with brain tissue in the images. Most brain extraction methods are developed to work on T1-weighted (T1w) magnetic resonance images (MRI), since this is a common modality in structural neuroimaging as it provides excellent contrast for the different brain tissues. In addition, the brain segmentation performed using T1w images can be mapped to other modalities if needed. However, due to the various acquisition sequences and scanner types, the appearance of the brain in T1w images may vary significantly between scans, which complicates the task of developing a brain extraction method that works across sequences and scanners. A further complication is the anatomical variability of the brain. Neuroimaging studies are performed on individuals at all ages with and without tissue altered by pathologies. Therefore, existing brain extraction methods often need to be adapted specifically for a certain type of study or, in the best case, need to be tuned to work on a certain population. A method that works reliably and robustly on a variety of different brain morphologies and acquisition sequences without requiring adjustment of parameters would greatly reduce the need for manual intervention and exclusion of subjects in neuroimaging studies.\nBuilding on recent work on label fusion (Aljabar et al., 2007; Collins and Pruessner, 2010; Heckemann et al., 2006) , the multi-atlas propagation and segmentation (MAPS) method (Leung et al., 2010) was adapted to brain extraction to address the problem of variability in anatomy and acquisition, producing more robust results and leading to the best currently published results (Leung et al., 2011) . In label fusion approaches, multiple atlases are selected from a library of inserm-00629187, version 1 -5 Oct 2011 previously labelled images. After non-rigid registrations of these atlases to the target image, their labels are merged through a label fusion procedure (e.g.; majority vote, STAPLE, etc.) (Sabuncu et al., 2009; Warfield et al., 2004) to obtain the final segmentation. This type of method is dependent on the accuracy of the non-rigid registrations. Registration errors may result in segmentation errors, as all selected labels are typically weighted equally. Like many of the labelfusion methods, by using a large library of labelled images (priors), MAPS compensates for possible registration errors, which leads to superior results compared to other popular brain extraction methods. However, due to the large library and the time consuming multiple non-rigid registrations step in MAPS, the processing time per subject on an Intel Xeon CPU (X5472 3GHz) is 19 h. Furthermore, in many studies it is not feasible to build a large library of priors and the long processing time may be a bottleneck in the analysis pipeline.\nA recent framework inspired by nonlocal means MRI denoising (Buades et al., 2005; Coupe et al., 2008; Manjon et al., 2008) has been introduced to achieve the label fusion segmentation task.\nThis method has demonstrated promising segmentation results without the need for non-rigid registrations . Instead of performing the fusion of nonlinearly deformed atlas structures, this method achieves the labelling of each voxel individually by comparing its surrounding neighbourhood with patches in training subjects in which the label of the central voxel is known. In this paper, we present the adaptation of this patch-based segmentation approach to perform brain extraction. The patch-based segmentation method cannot be directly applied to brain extraction, because i) false positives are likely to occur as extra-cerebral tissue may resemble brain within the patch structure, and ii) the computational complexity is high and this becomes a significant problem for large structures. To address these issues, we propose to apply the patch-based segmentation within a multi-resolution approach to extract the brain. We validate the performance of the proposed method on multiple collections of T1w MRI and demonstrate that the method robustly and consistently extracts the brain from subjects at all ages (from children to elderly) and from healthy subjects as well as patients with Alzheimer's Disease (AD). The main contribution of this paper is the development of a robust procedure to identify accurate brain masks with an extensive validation on multiple datasets acquired on different scanners and from different populations.\ninserm-00629187, version 1 -5 Oct 2011\nADNI: Priors from the ADNI database were constructed using the semi-automatic segmentations used in MAPS (Leung et al., 2011) . These segmentations are accurate definitions of the GM and WM of the brain, but all interior CSF is excluded (see Fig 2A) . Therefore, we deformed a spherical mesh initialized around the brain to fit smoothly along the border of the segmentation.\nIn this manner, we obtained a similar definition of brain segmentation as for the NIHPD and ICBM data. Finally, these segmentations were manually corrected in the same way as was done for the NIHPD and ICBM data (Fig. 2B ).\nAll library priors were flipped along the mid-sagittal plane to increase the size of the library utilizing the symmetric properties of the human brain, yielding 160 priors (original and flipped)\nfrom the 80 semi-automated segmentations described above.\nfunction is locally adapted as in Coup\u00e9 et al. (2011) by using the minimal distance found between the patch under study and the patches of the library.\nThese calculations are computationally impractical if made for all patches in all library images.\nThus, to decrease computation time several strategies are used in our method.\nInitialization mask: First, to reduce the size of the area to segment, an initialization mask M is constructed as the union of all segmentation priors S i minus the intersection of all S i :\nThe patch-based segmentation is performed within this region of interest (ROI) only under the assumption that the library is representative of all brain sizes after spatial normalization. This approach reduces the ROI by 50% compared with the union of all S i and by 85% compared with the entire stereotaxic space (Fig. 3 ).\nTemplate pre-selection: Furthermore, the N closest images from the library are selected based on their similarity to the target image within the defined ROI (initialization mask, see Eq. 3). The similarity is calculated as the SSD between the target and each of the template images in the library.\nPatch pre-selection: Finally, to reduce the number of patches to consider, preselection of the most similar patches is done as proposed in Coup\u00e9 et al. (2011) using the patch mean and variance. The main idea is that similar patches should have similar means and similar variances.\nThus, patches that are dissimilar with regard to mean and variance are not used in the weighted estimation. We use the structural similarity (ss) measure (Wang et al., 2004) \nwhere \u03bc is the mean and \u03c3 is the standard deviation of the patches centered on voxel x i and voxel x s,j at location j in template s. Only patches from the library with ss > 0.95, when compared to the patch under consideration, are selected for the nonlocal means estimator at voxel x i .\ninserm-00629187, version 1 -5 Oct 2011\nThe high DSC of BEaST compared to VBM8 and BET in the LOOCV can be explained by the fact that BEaST learns from the priors, while the other methods have no segmentation priors.\nThis means that BEaST delivers segmentations we can expect to match the definition, while this is not the case for BET and VBM8. Thus the results of BEaST are biased toward the segmentations of the priors and the DSC may be artificially high in the LOOCV. That is why the independent validation using the SVE was necessary.\nThe bias toward the priors illustrates the flexibility of the patch-based approach. If another definition is needed, the right priors just need to be available for BEaST to provide consistent segmentations on new data. This is also a limitation of BEaST in its current form. While other pathologies, which do not significantly change the appearance of the brain tissues, such as fronto-temporal dementia, should be consistently segmented with the current library of BEaST, other pathologies, such as tumors and lesions, may impose a problem for the segmentation. Over time, a library representative for the large variety of brains may be constructed to overcome this limitation in the future."}, {"section_title": "Definition of brain mask", "text": "As mentioned in the introduction, no standard exists defining what should be included and excluded when performing the brain extraction. In our study, we aim to exclude all extra-cerebral tissues, which resemble GM or WM by image intensity and may affect subsequent analyses.\nSuch tissues include the superior sagittal sinus (may resemble GM) and the optic chiasms (may resemble WM). Following this principle, we accept inclusion of internal CSF and CSF proximate to the brain, as the T1w MR signal from CSF is easily separated from non-liquid structures and subsequent analyses may benefit from the inclusion of CSF as noted in (Carass et al., 2011) . We propose the following definition of a mask separating the brain from non-brain tissue:\nIncluded in the mask \uf0b7 All cerebral and cerebellar white matter Excluded from the mask \uf0b7 Skull, skin, muscles, fat, eyes, dura mater, bone and bone marrow \uf0b7 Exterior blood vessels -specifically the carotid arteries, the superior sagittal sinus and the transverse sinus \uf0b7 Exterior nerves -specifically the optic chiasms"}, {"section_title": "Proposed brain extraction method", "text": "The proposed Brain Extraction based on nonlocal Segmentation Technique (BEaST), is inspired by the patch-based segmentation first published in Coup\u00e9 et al. (2010) and extended in . As done in Coup\u00e9 et al. (2011) , we use sum of squared differences (SSD) as the metric for estimation of distance between patches. Using SSD as the similarity metric requires that the intensity of brain tissue is consistent across subjects and imaging sequences. Therefore, inserm-00629187, version 1 -5 Oct 2011\nwe perform intensity normalization and spatial normalization before constructing the library of priors. Because manual brain segmentation from scratch is an extremely time consuming process, and because some automated techniques yield reasonable results, the gold standard of library priors is constructed using a semi-automatic method that involves extensive manual correction of automatically generated brain masks.\nThe following describes the normalization, the construction of the library containing the priors, and the fundamental patch-based segmentation method as well as our contribution of embedding the method in a multi-resolution approach to improve segmentation accuracy and computation time."}, {"section_title": "Normalization", "text": "Image intensity normalization of the T1w MRI data is performed by first applying the bias field correction algorithm N3 (Sled et al., 1998) followed by the intensity normalization proposed in Nyul and Udupa (2000) . Spatial normalization is achieved by 9 degrees of freedom linear registration (Collins et al., 1994) to the publicly available ICBM152 average that defines the MNI Talairach-like stereotaxic space, and resampled on a 193\u00d7229\u00d7193 voxel grid with isotropic 1 mm spacing. A final intensity normalization is performed in stereotaxic space by linearly scaling the intensities to the range [0;100] using 0.1%-99.9% of the voxels in the intensity histogram within an approximate stereotaxic brain mask."}, {"section_title": "Construction of library", "text": ""}, {"section_title": "Datasets used", "text": "The library of segmentation priors is built from several datasets: the NIH-funded MRI study of normal brain development (termed here the NIH Paediatric Database, or NIHPD) (Evans, 2006) (age: 5-18y), the International Consortium for Brain Mapping (ICBM) database (Mazziotta et al., 1995) (age: 18-43y) , and the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (Mueller et al., 2005) (age: 55-91y). The NIHPD and ICBM databases consist of healthy subjects, while the ADNI database, in addition to cognitive normal (CN) subjects, contains scans of subjects with AD and mild cognitive impairment (MCI). This way, almost the entire human inserm-00629187, version 1 -5 Oct 2011 life span is covered and subjects with atrophic anatomy are included, which provides a representative library of priors for performing brain extraction.\nWe chose 10 random T1-weighted (T1w) magnetic resonance (MR) scans from each of the NIHPD and ICBM databases. From the ADNI database we chose 20 random T1w MR scans at the time of screening from each class (CN, MCI, AD). In total, our library consists of 80 template MRI images with their associated brain masks described below. All scans were acquired using 1.5T field strength."}, {"section_title": "Priors construction", "text": "Ideally, one would use MRI data with manually segmented brain masks from multiple experts to create the priors. Unfortunately, manual segmentation is heavily time consuming -taking between 6 and 8 h per brain for a 1mm 3 isotropic volume to generate a mask that is consistent in 3D in coronal, sagittal and transverse views. Furthermore, inter-and intra-rater variability can lead to errors in the priors. We have decided to take a more pragmatic approach where automated tools are used to get a good estimate of the cortical surface and manual correction is used afterwards to correct any errors. This way, we benefit from the high reproducibility of the automated technique as well as the anatomical expertise of the manual raters. Priors were generated using one of two strategies, depending on the source of the data."}, {"section_title": "NIHPD and ICBM:", "text": "The NIHPD and ICBM databases contain T2w and PDw images in addition to T1w images. T1w images have high signal for the brain tissue, while T2w and PDw images have high signal for CSF (see Fig. 1 ). We take advantage of this fact to build the priors library.\nBy adding intensities from the three different sequences, we obtained an image with a very high signal for the intracranial cavity (ICC) (Fig. 1A ), which could be easily extracted using the widely used Brain Extraction Tool (BET) (Smith, 2002) from the FMRIB Software Library (FSL, http://www.fmrib.ox.ac.uk/fsl) (Smith et al., 2004) (Fig. 1B) . From the ICC segmentation, we used Fast Accurate Cortex Extraction (FACE) (Eskildsen and Ostergaard, 2006) to delineate the boundary between GM and CSF in the cerebrum (Fig. 1C ). Cerebellum and brain stem were added by non-linearly fitting masks in stereotaxic space. Finally, extensive and careful manual corrections were performed to get an optimal brain segmentation matching our definition (see Section 2) (Fig. 1D ). On average, such corrections took between 1 and 2 h per brain."}, {"section_title": "Patch-based segmentation", "text": "The proposed method is an extension of the patch-based segmentation method described in Coup\u00e9 et al. (2011) . In brief, a label is applied to a given voxel in the target image based on the similarity of its surrounding patch P(x i ) to all the patches P(x s,j ) in the library within a search volume. For each voxel x i of the target image, the surrounding neighbourhood is searched for similar patches in the N library images. A nonlocal means estimator v(x i ) is used to estimate the label at x i :\nwhere l(x s,j ) is the label of voxel x s,j at location j in library image s. We used l(x s,j ) {0,1}, where 0 is background and 1 is object (brain). The weight w(x i , x s,j ) assigned to label l(x s,j ) depends on the similarity of P(x i ) to P(x s,j ) and is computed as:\nwhere ||\u2022|| 2 is the L2-norm, normalized by the number of patch elements and computed between each intensity of the elements of the patches P(x i ) and P(x s,j ). The parameter h of the weighting"}, {"section_title": "Multi-resolution framework", "text": "In order to obtain optimal performance for brain extraction, the patch size needs to be large compared to the patch sizes used to segment smaller structures such as the hippocampus. For example, a small patch in the dura may resemble gray matter of the brain as the T1 intensities of these structures often are similar. Thus, a large patch size, including more structural information, is needed to avoid inclusion of extra-cerebral tissue, such as dura or fat. This is computationally impractical in the stereotaxic resolution. Therefore, we suggest embedding the patch-based segmentation within a multi-resolution framework, which provides the opportunity to effectively have spatially large patch sizes while still being computationally practical.\nIn brief, the multi-resolution framework enables propagation of segmentation across scales by using the resulting segmentation at the previous scale to initialize the segmentation at the current scale.\nThe library images, labels, initialization mask, and target image at the stereotaxic resolution are propagated and processed at a higher resolution (V j\u2212k+1 ). This procedure is repeated until the resolution of the stereotaxic space V j is reached. In this manner, the initialization mask of each resolution step is limited to the voxels with uncertain segmentation at the previous step (Fig. 3 ).\nThis greatly reduces the computational cost. At the stereotaxic resolution, segmentation is done by thresholding the estimator v V j (x i ) at 0.5.\nDuring experiments, we used three resolutions (k = 2) with isotropic voxel spacing respectively of 4 mm, 2 mm, and 1 mm (stereotaxic space resolution) (see Fig. 3 ). We empirically chose inserm-00629187, version 1 -5 Oct 2011\nconfidence level \u03b1 and variable patch size and search area depending on the resolution (see Table   1 ). "}, {"section_title": "Validation", "text": "In our validation of the proposed method we used the Dice similarity coefficient (DSC) (Dice, 1945) adapted to binary images when comparing to the gold standard brain segmentations described above. The DSC is defined as\n, where A is the set of voxels in the proposed segmentation and B is the set of voxels in the reference segmentation and |\u2022| is the cardinality. To visualize errors, we generated false positive and false negative images for each segmentation using the gold standard. These error images were averaged and the resulting image intensities were projected onto the three principal planes (axial, coronal, sagittal) using mean intensity projection in a manner similar to that done in Segmentation Validation Engine (Shattuck et al., 2009 )."}, {"section_title": "Leave-one-out cross validation", "text": "To evaluate the robustness and the accuracy of BEaST, we measured the segmentation accuracy in a leave-one-out cross validation (LOOCV) fashion. Each of the 80 library images was processed with the remaining 79 images as priors (158 after mid-sagittal flipping), and the resulting segmentation was compared to the manually corrected labels in the library. In this inserm-00629187, version 1 -5 Oct 2011\nexperiment, we varied the number of selected priors from the library to evaluate the impact of N on segmentation accuracy. During our experiment, N varied from 2 to 40.\nThough the experiment showed an increase of accuracy with increasing N, we chose N=20 for further experiments as the higher accuracy comes at a cost of longer computation time. Figure 5a shows the segmentation accuracy within the different groups used in the experiments for N=20.\nWith an average DSC of 0.9901, the accuracy on ICBM data is significantly higher (p<0.001, two-tailed t-test) than the accuracy of the other groups tested. This may be due to the fact that the 10 ICBM data sets evaluated here were acquired on a single scanner, and thus are more homogeneous than the other groups, which lead to higher redundancy and better matches of patches during the segmentation process for this group of example data."}, {"section_title": "Comparison to other methods", "text": "A comparison to BET (Smith, 2002) and VBM8 (http://dbm.neuro.uni-jena.de/vbm/download) was performed. We chose to compare with BET, as BET is publicly available, widely used, and has been shown to perform well in several recent brain extraction comparisons (Carass et al., 2011; Iglesias et al., 2011; Leung et al., 2011) . The choice of VBM8 was based on its availability and the fact that it is the highest-ranking publicly available method in the archive of the online Segmentation Validation Engine for brain segmentation (Shattuck et al., 2009) (http://sve.loni.ucla.edu/archive/).\nBET iteratively deforms an ellipsoid mesh, initialized inside the brain, to the GM/CSF boundary.\nThe target of BET is very similar to our definition of the optimal brain segmentation (see Section 2). We used BET version 2.1 from FSL version 4.1. Since BET performs better with robust brain center estimation and when the neck is not visible in the image (Iglesias et al., 2011) , we applied BET on the normalized and stereotaxically aligned images with default parameters.\nVBM8 performs the brain extraction by thresholding the tissue probability map in stereotaxic space, generated using the SPM framework (Ashburner, 2007; Ashburner and Friston, 2009) , and followed by repeated morphological openings to remove non-brain tissues connected by thin bridges. We used release 419 of VBM8, which was the latest version by the time of writing. By experiments, we found that VBM8 provided better results when initialized with native images in contrast to stereotactically registered images. In order to perform the best fair comparison, this method was thus applied in native space.\nBET and VBM8 were applied on the entire library of scans and DSCs, FPRs, and FNRs were calculated using the gold standard segmentations.\nIn Table 2, and ICBM data, BET behaved quite well with only minor segmentation errors, such as inclusion of the transverse sinus and part of the eye sockets. On ADNI data, more serious errors were found using BET. These include inclusion of dura and marrow of the skull while gyri are often cut off in atrophic brains. VBM8 had a tendency to perform over-segmentations on all groups and sometimes included dura proximate to the brain, carotid arteries, ocular fat / muscle, and parts of the eyes. On the positive side, VBM8 rarely removes part of the brain due to the consistent over-segmentation (see Fig. 6 ). BEaST generally provided a more consistent and robust segmentation without serious errors. We measured the segmentation output for BEaST at each resolution by thresholding the nonlocal means estimator at 0.5. As shown, the accuracy increases along with scale, and at 2 mm voxel sizes (requiring about 1.25 min) BEaST has already significantly (p=0.01, paired t-test) higher median (and mean) accuracy than BET (Fig. 5b) . The difference in DSCs between the techniques may seem small. However, when measuring DSC in the context of whole brain segmentations, small changes in the coefficient correspond to large changes in volume as demonstrated in (Rohlfing et al., 2004 depending on brain size and the false positives -false negatives ratio. This volume is relatively large when compared to the size of the structures, which are usually measured in neuroimaging studies (e.g.; the size of the human hippocampus is about 3.5 cm 3 ). The varying bias of the DSC when segmenting structures of different sizes (Rohlfing et al., 2004) in our case is considered low, as the brains have been spatially normalized. The FPRs and FNRs shown in Fig. 5c-d illustrate the large effect of a small difference in DSC. Compared to VBM8, the FPR is reduced by 74% using BEaST, and FNR is reduced by 67% compared to BET. Because of the consistent over-segmentation, VBM8 has an FNR similar to BEaST at the highest resolution. Even though the results of BET have a similar median FPR compared to the FPR of BEaST, the FPR of BET is significantly (p=0.05) different from the FPR of BEaST."}, {"section_title": "Independent validation", "text": "Comparing the results of BEaST to gold standard segmentations, which are also used as priors, a bias may be introduced that affect the results in favour of BEaST. Such a comparison effectively demonstrates that the method can provide results similar to our definition. However, when inserm-00629187, version 1 -5 Oct 2011\ncomparing to methods with no priors, a bias is introduced. Therefore, we performed validation using an independent test set available in the online Segmentation Validation Engine (SVE) of brain segmentation methods (Shattuck et al., 2009 "}, {"section_title": "Robustness", "text": "Finally, we evaluated the robustness of BEaST, and compared it to BET, by applying the method to all 1.5T T1w baseline ADNI data (200 AD, 408 MCI, and 232 CN). A strict manual quality control procedure was carried out to label the results \"pass\" or \"fail\" corresponding to whether the individual brain mask met the definition given in Section 2 and whether the mask was sufficient for further processing in a cortical surface analysis study. Masks from BEaST and BET were rated in a blinded fashion (i.e., the rater did not know which of the 1680 masks came from which procedure. This way, the failure rate of BEaST was compared to the failure rate of BET on the same data. A comparison to BET was chosen as BET demonstrated better compliance with our brain mask definition (see Section 2) than VBM8 during our validation experiments. the dataset are aged from 7y to 18y. This suggests that the maturation of the brain and skull alters the structural appearance in the T1w scans. Thus, the structural redundancy of the \"youngest scan\" is low within the library, and increasing N does not increase the number of similar patches.\nAfter careful, blinded quality control of the 2\u00d7840 baseline ADNI data volumes from BEaST (N=20) and BET, 599 images processed with BEaST were found to be acceptable for further cortical surface analysis while only 125 images processed with BET were acceptable. This corresponds to a failure rate of 29% for BEaST and 85% for BET. Figure 8 shows examples of segmentations that failed the quality control. As seen from the figure, if any part of the cortex was removed or any part of the dura was included by the segmentation, the result was rejected.\nPerforming a second pass with BEaST (N=20) using the 599 accepted segmentations with corresponding images as priors, and re-applying BEaST (with the subject's MRI left out of the inserm-00629187, version 1 -5 Oct 2011 template library) the failure rate was reduced to 10% corresponding to 86 scans (see Fig. 8 for examples of improvements after second pass). Many of these persistently failing scans had motion or Gibbs ringing artifacts, and some had parts of the brain present outside the initialization mask. No catastrophic errors were detected and the manual corrections needed for passing the brain masks were small. In fact, for other types of analyses, such as segmentations of deep brain structures, all brain masks produced by BEaST would pass the quality control."}, {"section_title": "Results", "text": ""}, {"section_title": "Independent Validation", "text": "Images from the independent test dataset from the Segmentation Validation Engine were BEaST, the differences in results with these two other techniques are statistically significant (p<0.03, paired t-test)."}, {"section_title": "Computation time", "text": "In our experiments, with 20 images selected from the template library, the total processing time using a single thread on an Intel Core i7-950 processor at 3.06 GHz was less than 30 min per subject. With 10 images, the processing time was less than 20 min per subject. By contrast, without the multi-resolution step, but using the initialization mask, the processing time was around 320 min. Removing the initialization mask increased the processing time to 42 h. The average processing times of BET and VBM8 were about respectively 2.5 min and 12 min, including the spatial and intensity normalization. Obtaining the segmentation of BEaST at 2 mm voxel sizes takes about 2 min including the spatial and intensity normalization, and the corresponding DSCs are significantly (p<0.03) higher than either BET or VBM8 (Fig. 5b) . This suggests that a fast low-resolution result may be available for certain analyses that do not require a highly detailed mask. Compared to MAPS, which yields similar accuracy as BEaST, the processing time of BEaST is about 40 times shorter on similar hardware."}, {"section_title": "Discussion", "text": "The leave-one-out cross-validation showed that the segmentation accuracy is consistently high (average DSC for N=20: 0.9834\u00b10.0053) and that selecting more priors from the library increase the accuracy. However, there is a trade-off between the number of selected priors and segmentation accuracy, why we chose to set N=20 for our validation. The results showed a higher accuracy on ICBM data compared to the other groups tested. This may be caused by the fact that i) all ICBM images were acquired using the same scanner, and ii) the anatomical variability within this group may be smaller than the other groups studied. This suggests that the accuracy may be improved by extending the number of priors for the groups with higher inserm-00629187, version 1 -5 Oct 2011\nanatomical variability and multi-site acquisitions. Although the results show that only a relatively small library is needed, the library still needs to be representative of all the data for the patch-based segmentation to work optimally.\nThe excellent results on ICBM and NIHPD suggest that using an unbalanced library of priors does not impair the segmentation accuracy of the data, which is underrepresented in the library.\nWe used only 10 priors from each of these databases in the library, while using 60 priors from the ADNI database. The template pre-selection seems sufficiently robust to select the appropriate priors.\nThe chosen patch sizes and search areas seem appropriate for segmenting the brain. The choice of \u03b1=0.2 was chosen empirically. Generally, the choice of \u03b1 can be viewed as a trade-off between computation time and segmentation accuracy. However, performing the segmentations only at the highest resolution may result in false positives as illustrated in Fig. 3 , bottom row.\nThus, the aim of the low resolution segmentation is to exclude dura and other tissues with similar intensity compositions as those found within the brain. We found that setting \u03b1=0.2 consistently achieved this."}, {"section_title": "Comparison to publicly available methods", "text": "Our comparison to other popular brain extraction methods showed that BET and VBM8 provides very good results for scans of normal individuals, while pathological data seems to impose a problem for these methods. BET has widely been the preferred brain extraction method for almost 10 years, and for many purposes BET is still sufficient. The simplicity of the method without the need for priors or registration is appealing. However, the emergence of large databases with thousands of images with and without pathology calls for flexible and robust brain extraction methods. This can be achieved by using label fusion methods as demonstrated in (Leung et al., 2011) and our study.\nTesting on all baseline ADNI data demonstrated that BEaST reduced the failure rate from 85%\nto 29% when compared to BET. These high failure rates were caused by a very strict quality Compared to BET and VBM8, BEaST produced less than half of the segmentation errors, increasing the average DSC from respectively 0.9522 and 0.9647 to 0.9834. In terms of speed,\nBET is faster than BEaST, if the segmentations are performed at the highest resolution.\nHowever, stopping the processing at 2 mm voxel sizes results in computation times similar to BET, while still obtaining significantly (p=0.01, paired two-tailed t-test) higher segmentation accuracy. Compared to the combined atlas and morphological approach in VBM8, BEaST yields superior segmentation results on all data tested in the study. The error maps (Fig. 6) show that VBM8 consistently oversegments the brain compared to our definition. BET behaves similarly, but with less over-segmentations. To be fair, such segmentations may be useful in many cases and thus should not be considered as erroneous. However, for the application of cortical surface analysis, it is crucial to not include proximate dura in the brain segmentation, as this may lead to over-segmentations of the cortex and in turn to overestimations of cortical thickness (van der Kouwe et al., 2008) .\nA limitation of the quantitative comparison is that the DSC does not necessarily say anything about whether the resulting segmentation is sufficient for the subsequent processing. For example, many false negatives may be due to only removing CSF from the surface of the brain compared to the gold standard. As such, these discrepancies are not fatal errors for the subsequent processing."}, {"section_title": "Comparison to state of the art", "text": "In terms of Dice overlap, results obtained by BEaST are better than those reported from recent hybrid brain extraction approaches (Carass et al., 2011; Iglesias et al., 2011) and similar to those from a label fusion approach, MAPS (Leung et al., 2011) . In the label fusion approach, the library is more than 10 times larger and the processing time about 40 times longer. The short processing time in BEaST (<30 min) results from only needing linear registrations and the advantage of using the ROI in the multi-resolution strategy. The current implementation runs as a single thread. However, the nonlocal means calculations can easily be parallelized and implemented to exploit the common multi-core CPUs and even GPU processing (Palhano Xavier de Fontes et al., 2011), which will decrease processing time significantly, possibly making it close to real time.\nUsing the online segmentation validation engine (Shattuck et al., 2009) Also, separation of cerebellum and brain stem from the cerebrum may be achieved with high accuracy if the appropriate structural priors are available.\nRecent work by Wang et al. (2011) showed that several segmentation algorithms perform systematic errors, which can be corrected using a wrapper-based learning method. In the study,\nBET was used to demonstrate the wrapper-based approach, which improved the average DSC from 0.948 to 0.964. This similarity is still lower than the average similarity obtained by BEaST.\nThere are no indications that the accuracy of BEaST can be improved using the wrapper-based learning approach, as the error maps of BEaST show no systematic error (Fig. 6) . The false positives and false negatives are uniformly distributed across the brain. The segmentations of VBM8 may benefit from the wrapper approach, as these exhibit consistent over-segmentations.\nAll images used in this study were acquired using scanners with 1.5T field strengths. Though the results demonstrated robustness towards multi-site acquisition, the sensitivity to scanner field strength remains to be investigated. As shown in (Keihaninejad et al., 2010) , the scanner field strength has significant impact on intra-cranial cavity segmentations. A similar effect can be inserm-00629187, version 1 -5 Oct 2011\nexpected for brain extractions. However, our results indicate that extending the library with appropriate templates (in this case images from 3T scanners) may deal with a potential bias. This is supported by the results obtained by MAPS (Leung et al., 2011) on data from scanners with 1.5T and 3T field strengths."}, {"section_title": "Conclusion", "text": "In conclusion, we have proposed a new brain extraction method, BEaST, based on nonlocal segmentation embedded within a multi-resolution framework. Angeles. This research was also supported by NIH grants P30AG010129, K01 AG030514, and the Dana Foundation.\ninserm-00629187, version 1 -5 Oct 2011\nFigures Fig. 1 . Construction of library priors using multiple modalities. A) Intensities from T1w, T2w, and PDw images are added. B) BET (Smith, 2002 ) is used to produce an ICC mask. C) FACE (Eskildsen and \u00d8stergaard, 2006 ) is used to delineate the cortical boundary and produce a cerebrum mask. D) Cerebellum and brain stem are added by stereotaxic masks, and the mask is manually corrected. Fig. 2 . Adaptation of library priors using deformable surface. A) Semi-automatic GM/WM mask as used by MAPS (Leung et al., 2011) . B) Adapted mask generated by deforming a surface mesh to the boundary of the GM/WM mask and manually corrected. inserm-00629187, version 1 -5 Oct 2011 . False positive and false negative maps for BET, VBM8, and BEaST on NIHPD, ICBM and ADNI data. All the error maps are displayed with the same scale. BET provided errors mainly located in the cerebral falx and medial temporal lobe structures. On the ADNI data, BET had a few catastrophic failures, which is visible in the false positive image. VBM8 tended to produce a systematic over-segmention compared to the used manual gold standard. The errors obtained by BEaST were more uniformly distributed indicating non-systematic segmentation errors. inserm-00629187, version 1 -5 Oct 2011 Fig. 7 . Typical results using BET, VBM8 and BEaST on the five test groups. The figure shows sagittal slices and 3D renderings of the segmentations. Column 1-2: BET segmentation. Column 3-4: VBM8 segmentation. Column 5-6: BEaST segmentation. Blue voxels are overlapping voxels in the segmentation compared to the gold standard. Green voxels are false positives and red voxels are false negatives. inserm-00629187, version 1 -5 Oct 2011 Fig. 8 . Examples of ADNI brain masks produced by BEaST not passing the quality control in the first pass (first row) and passing the quality control after second pass (second row). Left segmentation is discarded due to cortex clipping, while right segmentation is discarded due to inclusion of dura as indicated by the arrows. After second pass these errors are removed.\ninserm-00629187, version 1 -5 Oct 2011"}]