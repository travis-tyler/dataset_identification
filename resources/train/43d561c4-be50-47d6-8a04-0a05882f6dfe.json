[{"section_title": "Abstract", "text": "Abstract: High-dimensional regression or classification models are increasingly used to analyze biological data such as neuroimaging of genetic data sets. However, classical penalized algorithms produce dense solutions that are difficult to interpret without arbitrary thresholding. Alternatives based on sparsity-inducing penalties suffer from coefficient instability. Complex structured sparsity-inducing penalties are a promising approach to force the solution to adhere to some domain-specific constraints and thus offering new perspectives in biomarker identification. We propose a generic optimization framework that can combine any smooth convex loss function with: (i) penalties whose proximal operator is known and (ii) with a large range of complex, non-smooth convex structured penalties such as total variation, or overlapping group lasso. Although many papers have addressed a similar goal, few have tackled it in such a generic way and in the context of high-dimensional data (\u2265 10 5 features). The proposed continuation algorithm, called CONESTA, dynamically smooths the complex penalties to avoid the computation of proximal operators, that are either not known or expensive to compute. The decreasing sequence of smoothing parameters is dynamically adapted, using the duality gap, in order to maintain the optimal convergence speed towards any globally desired precision (\u03b5).\nFirst, we demonstrate that CONESTA achieves an improved convergence rate than classical (without continuation) proximal gradient smoothing. Second, experiments conducted on both simulated and experimental MRI data, exhibit that CONESTA outperforms the excessive gap method, ADMM, classical proximal gradient smoothing and inexact FISTA in terms of convergence speed and/or precision of the solution. Third, on the experimental MRI data set, we establish the superiority of structured sparsityinducing penalties ( 1 and total variation) over non-structured methods in terms of the recovery of meaningful and stable groups of predictive variables."}, {"section_title": "Introduction", "text": "An increasing number of high-dimensional data sets are produced in situations where it is known that the underlying generating model is truly sparse and that there are structured correlations among the many sampled variables. In particular, many data sets of this type are present in the life sciences. Among others, they are encountered in multivariate neuroimaging studies [43] , multivariate genome-wide brain-wide association studies [41] , and gene expression studies [26] .\nThese kinds of high-dimensional problems are usually addressed with linear multivariate regression or classification methods in conjunction with penalties or constraints based e.g. on the 1 -or 2 -norms. The properties of these methods are well known and include their ability to deal with large numbers of variables. The elastic net penalty [45] is very popular and is implemented in many machine learning software libraries. However, experimenters often also require that the model solutions adhere to some domain-specific constraints and multivariate methods that can accommodate structured penalties are less widely available [29] .\nIdeally, the experimenter would like to combine a sparsity-inducing penalty with one or several structured penalties. The hypothesis is that this would improve the interpretability and potentially the accuracy of the model. For example, a group 1,2 penalty with overlap was used by Silver and Montana [39] in the framework of reduced-rank regression to combine genetic data and imaging traits in order to select single nucleotide polymorphisms (SNPs) that belong to sets of genes involved in the same biological process. The penalty describes groups of SNPs that are linked to specific genes and groups of genes that contribute to specific biological pathways. In a second example, Michel et al. [30] presented a method for predicting behavior from fMRI images by combining an 1 and a total variation (TV) penalty. The TV penalty accounts for spatial structure in the images and increases the interpretability of the results.\nIn this paper we consider an objective function that consists of the sum of: (i) a smooth convex loss; (ii) a convex non-smooth penalty whose proximal operator is known; and (iii) a complex, non-smooth convex structured penalty with unknown proximal operator. Various methods have been proposed to minimize such problems; however, each one requires several auxiliary calculations that limit its application to some particular cases. For instance, proximal algorithms such as the iterative soft-thresholding algorithm (ISTA) [18, 16, 34] , the fast, or accelerated ISTA (FISTA) [4, 34] can be used to account for only one of these non-smooth terms alone. Combining two or more non-smooth penalties is expensive, since the proximal operator of the sum of functions is not known in the general case. Besides proximal methods, many other optimization strategies allow minimization of this type of problems when combining several complex or sparsity-inducing penalties. For instance, we cite the algorithms presented in [24, 12, 9, 33, 32] . Nevertheless, as is explained in Section 2.2, some of them require access to unknown proximal operators of the non-smooth or even the smooth part of the objective function; others require the explicit solutions of some minimization problems that are unsolvable for many useful machine learning models. For example, the generalized forward-backward algorithms deal with the sum of non-smooth functions but require that each one is simple in the sense that its proximity operator is easy to compute. This paper focuses on proximal methods that may be classified based on two general strategies. The first one involves approximating the proximal operator associated with the convex non-smooth penalties. The second one relies on smoothing all or part of the non-smooth penalties such that the proximal operator becomes as simple as (for example) a soft-thresholding function. Progress has been made with respect to the first approach, i.e. the approximation of proximal operators using inexact proximal operators in FISTA [38, 30] . Michel et al. [30] approximated the proximal operator of the TV constraint. This procedure can also be applied to more complex composite non-smooth constraints, such as TV combined with the 1 penalty. Unfortunately, as we will see below, approximating the corresponding proximal operator causes the computational time to increase dramatically.\nWe therefore choose to investigate the second approach, i.e. smoothing the non-smooth penalty with unknown proximal operator, while keeping the exact 1 constraint and using a proximal gradient method to minimize the whole function. This approach, smoothing proximal gradient method (SPG), has been proposed by Chen et al. [15] . The authors combined an 1 penalty with a nonsmooth group-wise 1,2 penalty. They replaced the group 1,2 penalty with a smooth function using Nesterov's smoothing technique [32] . The resulting objective function was then minimized using FISTA. It is worth noting that Nesterov's smoothing procedure involves a smoothing parameter \u00b5 and that the choice of \u00b5 reflects a trade-off. The convergence rate of FISTA depends on \u00b5: the smaller \u00b5 is, the slower FISTA converges. On the other hand, a small \u00b5 guarantees a good approximation of the smoothed penalty.\nSavchynskyy et al. [37] addressed this issue and proposed an adaptive diminishing smoothing scheme for \u00b5. The authors proposed a continuation method that updates the regularization parameter at the beginning of each continuation iteration to increase the precision of the solution. However, they imposed a full regularization of the non-smooth part that prevents true sparsity. Moreover, their method is not straightforwardly applicable to structured linear regression. In fact, they considered a well-studied class in linear programming and involve many prior results that are specific to their problem and the imposed full smoothing.\nAnother theoretically sound continuation method is Nesterov's excessive gap method [32] . Nesterov showed that it achieves the best possible convergence rate towards the solution of the original optimization problem. Nevertheless, it requires to have access to the explicit solution of some auxiliary minimization problems that are generally not available. Thus, the excessive gap method suffers from some major limitations: restriction to the least-squares loss and full regularization of the non-smooth part of the objective function, thereby loosing any sparsity-inducing properties.\nThis brief review outlined the potential shortcomings of state-of-the-art methods. However, it suggested that Nesterov's smoothing combined with a continuation approach is a promising track to follow. This paper aims to overcome those limitations, and we propose a new continuation algorithm for the minimization of an objective function containing non-smooth convex terms whose proximal operator is unknown. We call this al-gorithm CONESTA, which stands for COntinuation with NEsterov smoothing in a Shrinkage-Thresholding Algorithm. The proposed algorithm addresses the limitations of the state-of-the-art methods through three main contributions:\n(i) The continuation scheme. It dynamically generates a sequence of decreasing precisions \u03b5 i , is calculated from an estimation of the current error (see duality gap in point iii). Such a continuation scheme ensures the convergence towards a globally desired final precision, \u03b5, which is the only parameter that the user needs to provide.\n(ii) Given a precision level, \u03b5 i , we propose an expression for the value of the smoothing parameter, \u00b5 i , that minimizes the number of iterations required for convergence to the specified precision, \u03b5 i . (iii) All error estimates are based on the duality gap: First, the duality gap is used as a stopping criterion for the outer (global) continuation. Second, the duality gap is also used to stop the inner application of FISTA, as soon as the prescribed precision \u03b5 i has been reached. This prevents unnecessary convergence toward the approximated (i.e., smoothed) objective function. Finally, the duality gap is used to dynamically determine the prescribed precision, \u03b5 i , with respect to the current error estimation of the true objective function. Those two points are crucial when considered together with item (ii): if the duality gap indicates that we are close to the solution, the precision, \u03b5 i , will be small, and thus, thanks to item (ii), the smoothing will be optimally adapted to the current situation.\nThe paper is structured as follows: After a detailed review of the state-of-theart methods in Section 2, we present CONESTA in Section 3 and the three main contributions that constitute it. In Section 4, we present its application to linear regression with both elastic net and TV penalties. We conducted validation studies on both a simulated data set, Section 5, and on a high-dimensional experimental 3D neuroimaging data set, Section 6.\nThe convergence speed of CONESTA was compared to that of several stateof-the-art minimization algorithms: the excessive gap method [32] , the alternating direction method of multipliers (ADMM) [9] , the fast iterative softthresholding algorithm (FISTA) [4] , the inexact fast proximal gradient algorithm [38] and the smoothing proximal gradient method, denoted \"FISTA with Chen's \u00b5\" below [15] .\nWe limited ourselves to these approaches in order to maintain readability of the paper and because these methods achieve optimal convergence speed. The comparisons clearly demonstrate that CONESTA outperformed the other stateof-the-art minimization algorithms in this study, when solving a linear regression problem with elastic net and TV penalties on both experimental and simulated data sets. We also demonstrate the relevance of TV for recovering stable and meaningful predictive support on neuroimaging data."}, {"section_title": "Background", "text": "Section 2.1 introduces to the general minimization problem and its application to penalized linear regression models. Section 2.2 presents the current state-ofthe-art optimization algorithms and their limitations that justify the relevance of the proposed contribution. Finally, Section 2.3 describes Nesterov's smoothing technique and Section 2.4 shows its use together with FISTA, which is the base on which we built the CONESTA algorithm."}, {"section_title": "Problem statement", "text": "The goal is to minimize the general optimization problem:\nwhere \u03b2 is the vector of parameters to be estimated, g is a differentiable convex function with a Lipschitz continuous gradient, h is a convex non-differentiable function with a known proximal operator and s is an additional convex nondifferentiable structured penalty whose proximal operator is not necessarily known or is difficult to express or compute. The real scalars \u03ba \u2265 0 and \u03b3 \u2265 0 are regularization parameters. We further denote\nThis class of optimization problems is very general and includes classical models such as the lasso, ridge regression and elastic net, the perhaps less common TV [10] and group lasso ( 1,2 ) [44, 35] models, and combinations of these. Consider a data set (X, y), where X \u2208 R n\u00d7p is a matrix of n input vectors of dimension p and y \u2208 R n denotes the corresponding n-dimensional response vector.\nIn this paper we utilize penalized linear regression functions, i.e. where the loss function is 1 2 X\u03b2 \u2212 y 2 2 , to illustrate the application of such optimization problems. We will additionally also focus on both an elastic net and a \"group\" penalty. The function we wish to minimize is therefore\nwhere \u03bb \u2265 0, the function h(\u03b2) = \u03b2 1 is a sparsity-inducing penalty whose proximal operator is known, s(\u03b2) = G\u2208G A G \u03b2 G 2 is a penalty on the structure of the variables, with G being a set of potentially overlapping groups (a group G has size |G|) of variables and A G a linear operator on R |G| (an example of such an operator is detailed in Section 4). TV and group 1,2 are two examples of such \"group\" penalties."}, {"section_title": "Relation to prior works", "text": "Inexact proximal-gradient: Schmidt, Le Roux and Bach [38] gave a general sufficient condition for applying inexact proximal gradient algorithms where the proximal operator (or the gradient) is approximated. They established a sufficient condition for the approximation of the unknown proximal operator to be applied at each step of the proximal gradient algorithm such that the optimal convergence speed is maintained. This method could be used in the application of complex penalties such as total variation or group lasso in linear regression. The approximation is performed by solving a subproblem to find the proximal operator. Details are provided in Section A 1.2 of the supplemental article [22] . Solving a subproblem like this is possibly a time-consuming process, especially when working with high-dimensional data.\nIndeed, to ensure convergence, the prescribed precision of the inner loop that solves the subproblem must decrease faster than O 1/k 4 (where k is the number of FISTA iterations) [38] . Thus, a precision of \u03b5 = 10 \u22128 is required already after a hundred FISTA iterations. To conclude, inexact proximal gradient algorithms seems perfectly adapted for low dimensional problems, since the number of FISTA iterations is optimal and the cost of the inner loop remains low. However with high-dimensional data this approach may become inefficient and even intractable. The method that we propose here, CONESTA, does not suffer from this limitation since the approximation is not found numerically, and so no inner loop is needed. Moreover, the conducted comparison study demonstrated that CONESTA is much faster than the inexact proximal gradient method in terms of the convergence time.\nPrimal-dual: Chambolle and Pock [13] proposed a general primal-dual method that minimizes a loss function with two different penalties, such as e.g. TV and 1 [21] . They consider a function in the form g(Kx) + h(x) where K is a linear operator and proved an optimal convergence rate for such non-smooth objective functions. However they assume to have access to the proximal operators of both the smooth function, g, and the non-smooth one, h. This is the main shortcoming of this method, as it would require the approximation of said proximal operators when they are not available, like in the case with logistic regression, for instance. It would then require an inexact approach like the one for proximal gradient methods developed by Schmidt, Le Roux and Bach [38] and discussed above. To the best of our knowledge, the inexact issue for this method is an open problem.\nExcessive gap: Nesterov [32] presented an optimal primal-dual method called the excessive gap method, or the excessive gap technique. As we will show later, this method can be used to minimize Equation 2.2 with optimal convergence rate. Nevertheless it shares the same shortcomings as the method by Chambolle and Pock [13] . In fact, a necessary step in the excessive gap algorithm is the computation of \u03b2, the primal variable that corresponds to a particular dual variable, u (see Section A 3.1 in the supplemental article [22] ). As far as the authors know, there is no explicit expression for this function when dealing with the general case of any convex loss function. An explicit application of the excessive gap method can be found in [14] where the authors applied it to canonical correlation analysis with group and fused lasso penalties. A problem with the excessive gap method (illustrated in [14] and reiterated below) is that it imposes a smoothing of all non-smooth parts, and in particular of the 1 penalty. This implies that the found solution will not be strictly sparse.\nSmothing proximal gradient: Nesterov's smoothing can also be used in conjunction proximal gradient algorithm [15] . The main issue of this approach is that accurate solution, with small smoothing, involve a slow convergence. In Section 2.3 we provide more details on this approach since it is precisely the contribution of this paper."}, {"section_title": "ADMM:", "text": "The alternating direction method of multipliers [9] is commonly used to minimize the sum of two convex functions. It can be adapted to the loss function of interest in this paper, Equation 2.2, but has some drawbacks in this context. In particular, it suffers from the same shortcoming as the method discussed above by Chambolle and Pock [13] . In fact, when dealing with a smooth loss function that has an unknown proximal operator, the proximal operator would have to be approximated numerically. Another computational limitation of ADMM when working with the ordinary least squares (OLS) loss and the 1 -norm and TV penalties, is that each update involves solving two linear systems, or, equivalently, computing the inversion of two large p\u00d7p linear operators, possibly ill-conditioned. In the case of total variation and 1 penalties, this is not a problem, because of the particular form of the resulting linear operator. Yet for more general linear operators, solving the associated linear systems would quickly become intractable, in particular for large p. Moreover the regularization parameter, \u03c1, in the associated augmented Lagrangian function is difficult to set (this is still an open problem), and even though ADMM converges for any value of this parameter, under mild conditions, the convergence rate depends heavily on it. We have employed some heuristics for selecting this parameter. This issue is discussed Section A 3.2 in the supplemental article [22] along with the details of our implementation of ADMM as described in [42] . For the sake of method comparison, we had to generalize it slightly and have added an 1 penalty. We note that this kind of modification would not be as trivial with general complex penalties.\nPRISMA: PRISMA [33] is a continuation algorithm for minimizing a convex objective function that decomposes into three parts: a smooth part, a simple non-smooth Lipschitz continuous part, and a simple non-smooth non-Lipschitz continuous part. They use a smoothing strategy similar to that used in this paper. The main limitation is that the two different penalties have to be simple such that their proximal operators are explicit (see Algorithm 1 in [33] ). Thus, as there is no inexact approach that allows to approximate any unknown proximal operator, while preserving the convergence, we can not apply PRISMA in a rigorous way when dealing with group lasso or total variation.\nOur proposed continuation algorithm addresses the two aforementioned deficiencies. Indeed, CONESTA (i) is relevant in the context of any smooth convex loss function because it only requires the computation of the gradient and (ii) estimates weights that are strictly sparse because it does not require to smooth the sparsity-inducing penalties. Additionally, CONESTA does not require solving any linear systems, or inverting any matrices, and can easily be applied with a variety of convex smooth loss functions and many different complex convex penalties."}, {"section_title": "Nesterov's smoothing technique", "text": "Our proposition is based on the assumption that many structured sparsityinducing penalties can be reformulated into a simple linear transformation of \u03b2 via the dual norm. It turns out that this assumption is often true, which can be proved by using results from [7] and is exemplified in [31, Section 4] and [15, Section 3] . Based on this assumption Nesterov's smoothing technique can be applied on all of the non-smooth convex functions we are dealing with, if they respect the condition presented in Equation 2.4. Using the duality concept between the q and q/q\u22121 norms (see [7] for instance), we can establish the dual form of s(\u03b2):\nwhere \u03b1 G \u2208 R |G| is a vector of auxiliary variables associated with A G . Let \u03b1 = [\u03b1 G1 , ..., \u03b1 G |G| ] be a vector of length G\u2208G |G| that is the concatenation of the \u03b1 G . Similarly, we combine the linear operators A G into a concatenated one, denoted A \u2208 R G\u2208G |G|\u00d7p . We thus obtain a more compact representation of s(\u03b2) by\nwhere\n\u2200G \u2208 G} and the set K is the Cartesian product of unit balls in Euclidean space and, therefore, a compact convex set. This expression of s(\u03b2) is a sufficient condition to apply Nesterov's smoothing technique. Nesterov's smoothing technique is then applied as\nfor all \u03b2 \u2208 R p , where \u00b5 is a positive real smoothing parameter and\nThe variable \u03b1 * \u00b5 (\u03b2) can be computed by projecting it onto the compact K, as stated in Equation 2.11.\nThe following theorem [31, Theorem 1] presents Nesterov's smoothing when applied to a class of complex penalties.\nTheorem 1 (from Nesterov's Theorem [31] ). Let the function s \u00b5 be Nesterov's smooth transform of a convex function s. We state three results:\n(i) First, s \u00b5 is convex and differentiable with gradient\n(ii) Second, the gradient is Lipschitz continuous with constant\nis the matrix spectral norm of A.\nThe proof of Equation 2.9 is based on the following Lemma.\nIn addition, we have the following result that allows us to practically compute the dual variable.\nLemma 2 (Projection formulation of \u03b1 * \u00b5 ). Using the same notation as above, we can express \u03b1 * \u00b5 as a projection of\n(2.11)"}, {"section_title": "FISTA with Nesterov's smoothing", "text": "Consider a function s, whose proximal operator is not known, or too expensive to compute. This function can instead be smoothed using Nesterov's technique. By this smoothing, we obtain a new optimization problem that is closely related to the one given in Equation 2.1, namely\nwhere s \u00b5 is Nesterov's smoothing of the function s. Additionally, we use the notation \u03b2 * \u00b5 = arg min \u03b2 f \u00b5 (\u03b2) to denote the regression vector that minimises f \u00b5 . Problems such as those in Equation 2.12 can be efficiently solved by proximal gradient algorithms such as FISTA [4] . Next we provide all the required quantities to run FISTA, provided in Algorithm 1:\n\u2022 A random starting vector: \u03b2 0 \u2208 R p .\n\u2022 A fixed smoothing parameter \u00b5.\n\u2022 A prescribed final precision of the smoothed problem: \u03b5 > 0.\n\u2022 A linear operator A, that encodes the structure of the complex penalty and the corresponding regularization parameter, \u03b3.\n\u2022 The gradient of the smooth loss plus the gradient of the smoothed complex penalty (Equation 2.7), i.e. \u2207(g+\u03b3s \u00b5 )(\u00b7). This function implicitly provides the data X, y and the 2 penalty parameter \u03bb.\n\u2022 The proximal operator of the non-smooth part, as defined Section A 1.2 in the supplemental article [22] , i.e. prox t\u00b5\u03bah (\u00b7). The proximal operator implicitly provides the parameter \u03ba of the 1 penalty.\n\u2022 The Lipschitz constant of the gradient of the loss function, i.e. L(\u2207(g)).\nIt is used to compute the step size t \u00b5 (Equation 2.14).\n\u2022 The stopping criterion (defined as StoppingCriterion) will be either\nFor the instant, the reader need only be concerned with the default stopping criterion. Beck and Teboulle [4] stated that t \u00b5 must be smaller than or equal to the reciprocal of the Lipschitz constant of the gradient of the smooth part, namely of g + \u03b3s \u00b5 . Using Theorem 1 to obtain the Lipschitz constant, and considering the fact that we want to take the largest possible step, we choose\nThis value of t \u00b5 is sufficient for the convergence of FISTA [4] , provided in Algorithm 1.\nFISTA is thus applied to a smoothed version of the original function. It will converge towards \u03b2 * \u00b5 , the minimum of f \u00b5 . Using Equation 2.10, we can prove that, for a prescribed precision \u03b5 > 0, selecting \u00b5 smaller than \u03b5/\u03b3M allows us to achieve the precision f (\u03b2 k )\u2212f (\u03b2 * ) \u2264 \u03b5 for f . For example, using \u00b5 = \u03b5/2\u03b3M and the precision \u03b5/2 as a stopping criterion in Algorithm 1, as was done by Chen et al. [15] , would enable us to achieve the precision \u03b5 for f (see Section 3.2 below).\nSince we do not make the assumption that g in Equation 2.1 is strongly convex, the convergence rate of FISTA, when applied to Equation 2.12, is governed by the expression [4] 15) where k is the iteration counter."}, {"section_title": "CONESTA: Continuation with Nesterov's Smoothing in a Shrinkage-Thresholding Algorithm", "text": "This section presents a novel minimization method that smooths a part of the non-smooth function. The maximal convergence speed towards the minimum of the original (non-smoothed) function is obtained by a continuation procedure that systematically decreases the smoothing parameter."}, {"section_title": "Continuation", "text": "The step size t \u00b5 used in Algorithm 1 depends on the smoothing parameter \u00b5.\nThey are linked to one another and vary in the same way, although they drive different aspects of the algorithm's convergence. Indeed, high precision, with a small \u00b5, will lead to a slow convergence (small t \u00b5 ). Conversely, poor precision (large \u00b5) will lead to rapid convergence (large t \u00b5 ). There is therefore a trade-off between speed and precision [6, 36, 37] .\nIt seems natural that we should perform successive runs of an iterative optimization algorithm with successively smaller values of \u00b5 in every run. Each run benefits from the starting point provided by the previous loop, and from the large (compared to subsequent runs) smoothing parameter which generates a larger step size and thus a smaller number of iterations. To achieve this, we would like to infer an optimal reduced \u00b5 after each FISTA loop."}, {"section_title": "Algorithm 1", "text": "The fast iterative shrinkage-thresholding algorithm (FISTA) with fixed \u00b5\nif StoppingCriterion \u2264 \u03b5 then 8:\nend if 10:\nWe propose a continuation process in which each FISTA loop achieves a precision \u03b5 i that decreases dynamically depending on the current estimation of the error. This sequence converges to the prescribed precision, \u03b5, initially chosen by the user. Once \u03b5 i is fixed, we calculate an appropriate smoothing parameter \u00b5 i to achieve this precision with the least number of iterations. We detail here these main steps of the CONESTA algorithm starting from the current \u03b2 i :\n1. Estimate the current error:\n. This estimation is found using the duality gap (see Section 3.3). 2. Compute the desired accuracy \u03b5 i+1 for the next FISTA loop in order to achieve a smaller error, f (\u03b2 i+1 ) \u2212 f (\u03b2 * ). To obtain \u03b5 i+1 , the current accuracy at the ith iteration, \u03b5 i , is decreased by a factor \u03c4 . 3. Compute the smoothing parameter \u00b5 i+1 that gives the smallest upper bound on the number of iterations required to achieve the precision\n, in the least number of iterations possible. See Section 3.2. 4. Run FISTA on f \u00b5 i+1 to obtain \u03b2 i+1 .\nWe then iterate until the error f (\nThe remainder of this section is organised as follows: First, we describe how to obtain an optimal value of \u00b5 that minimizes the upper bound on the number of iterations required to achieve a prescribed precision \u03b5 when minimizing f \u00b5 . Next, we establish an upper bound for the duality gap in Section 3.3. Finally, we present CONESTA."}, {"section_title": "Determining the optimal smoothing parameter", "text": "The smoothed function, f \u00b5 , as mentioned in Equation 2.10 and Equation 2.12, provides upper and lower bounds on the original function, f , such that\nwith M = max \u03b1\u2208K\nThe following theorem provides the \u00b5 that achieves the prescribed precision \u03b5 with the least number of iterations.\nTheorem 2 (Optimal smoothing parameter \u00b5). For any given \u03b5 > 0, selecting the smoothing parameter \u00b5 as\nminimizes the worst case bound on the number of iterations required to achieve the precision\nThe proof is provided in Section A 2.1 in the supplemental article [22] ."}, {"section_title": "Duality gap", "text": "Duality formulations are often used to control the achieved precision when minimizing convex functions. They can be used to provide an estimation of the error f (\u03b2) \u2212 f (\u03b2 * ), for any \u03b2, without knowing the minimum f (\u03b2 * ), which we never know in practice. The duality gap is the cornerstone of the CONESTA algorithm. Indeed, it is used three times:\n1. In the ith CONESTA iteration, as a way to estimate the current error\n. The error will be estimated using the gap of the smoothed problem. This is justified in Section 3.5. This value is then used to deduce all the other parameters for the next application of FISTA. The next desired precision and the smoothing parameter, \u00b5 i , are derived from this value. 2. As the stopping criterion in the inner FISTA loop. The criterion will be such that FISTA will stop as soon as the current precision is achieved using the current smoothing parameter, \u00b5 i . This prevents unnecessary convergence toward the approximated (smoothed) objective function. 3. Finally, as the global stopping criterion within CONESTA. This will guarantee that the obtained approximation of the minimum,\nWe first establish an expression of the duality gap for the problem in Equation 2.2. Next, we consider the smoothed version in Equation 2.12.\nThe Fenchel duality can be used as in [28] to rewrite the objective function in Equation 2.2 as \nwhere l * and \u03c8 * are the Fenchel conjugates of l and \u03c8, respectively. It is straightforward to show that l * can be expressed as l\n2 + x, y . Following Mairal [28] , the dual variable associated to a given \u03b2 k can be computed as\nThe duality gap is finite, it vanishes at the minimum and provides an estimate of the difference with the optimal value of the objective function. The duality gap has the following properties:\nThis requires the calculation of l * and \u03c8 * . However, to the best of our knowledge, there is no explicit expression for \u03c8 * when using a complex penalty such as total variation or group lasso. Therefore, we use an approximation that maintains the properties stated in Equation 3.6. We use Nesterov's assumption and rewrite \u03c8(\u03b2) as\nThen, we approximate the Fenchel conjugate of \u03c8 by that of \u03c8 k \u2261 \u03c8 \u03b1(\u03b2 k ) , which is the local approximation of \u03c8 at the current \u03b2 k . Next, we define an approximation for the gap by explicitly computing \u03c8 * k . When \u03bb = 0, the function \u03c8 * k may be infinite at some \u03c3(\u03b2 k ). Thus, we slightly change it in order to obtain feasible values. An explicit expression for the duality gap is presented in the following theorem.\nTheorem 3 (Duality gap for the non-smooth problem). Let \u03bb, \u03ba and \u03b3 be nonnegative real numbers. The following estimation of the gap satisfies Equation 3.6:\nwith \u03c8 * k (v) being the Fenchel conjugate:\nThe vectors s k , k k \u2208 R p are given by\nand the elements of k k are\nThe proof is provided in Section A 2.2 in the supplemental article [22] . The same approach can be used for the smoothed problem given in Equation 2.12. Using Equation 2.5 instead of Equation 2.4, the gap expression is now approximated by\nWhere the Fenchel conjugate of Theorem 3 should be replaced by\nif \u03bb = 0.\nwhere\nRemark: It is worth noting that in spite of the massive use of X and A in this duality gap computation, it does not limit this approach and the obtained formula to the least square loss or the elastic net-total variation penalty. In fact, the very same expression holds for more general models with different X, A and l. It just requires that the loss function can be expressed in the form l (X\u03b2) with explicit l * . Moreover, any constraint that fulfills the Nesterov assumption in Equation 2.4 could be used and eventually combined with the 1 or 2 norms as in Equation 2.2. For instance, by using results in Section D.2.3 of [28] , one can easily adapt this for the logistic loss function."}, {"section_title": "Convergence analysis", "text": "In this section, we derive the sequences \u00b5 be defined recursively as \n(iii) If we require f \u00b5opt (\u03b2 k ) \u2212 f (\u03b2 * ) < \u03b5 then the number of FISTA iterations is upper-bounded by\n(iv) Suppose that the minimum \u03b2 * is unique, thereby, if we require f \u03b2 i \u00b5 \u2212 f (\u03b2 * ) < \u03b5, the total number of FISTA iterations is upper-bounded by O(C/\u03b5), where C > 0 is a constant.\nThe proof of convergence is provided in Section A 2.3 of the supplemental article [22] .\nRemarks Theorem 4 gives a rigorous proof that the continuation technique improves the convergence rate compared to the simple smoothing using a single value of \u00b5. Indeed, it has been demonstrated in [5] (see also [15] ) that the convergence rate obtained with single value of \u00b5, even optimised, is O(1/\u03b5)+O(1/ \u221a \u03b5).\nHowever, the CONESTA algorithm achieves O(1/\u03b5) for convex functions. Theorem 4 uses the uniqueness hypothesis in (iv) just to get the boundedness of the FISTA iterations. Note that the authors in [11] have proved this property for a slightly modified version of the FISTA algorithm. We do not use these results for two reasons: First, this will slightly modify the \u00b5 opt and other expressions which we have not include in our simulations. Second, this result must be adapted to the smoothed version. In fact, we will need to establish the boundedness of the FISTA iterated uniformly with respect to \u00b5 as \u00b5 converges to zeros. On the other hand, if the objective function is strongly convex with strong convexity parameter \u03c3 > 0, then the FISTA algorithm enjoy global linear convergence rate and especially ensures the convergence of \u03b2 k to the minimum \u03b2 * . In fact, we have that \u03b2 k \u2212 \u03b2 * 2 < 2\u03c3/\u03b5 as far as f (\u03b2 k ) \u2212 f (\u03b2 * ) < \u03b5 (see remark 3 in [15] ). In this case, not only the hypothesis in (iv) is naturally satisfied, but also it is a favorable case to apply a continuation technique since the estimations on the successive intermediate \u03b2 i \u00b5 will ensure a faster convergence contrary to the simple convergence case in which we don't make use of this rate. Nevertheless, we did not include this case in this paper since it will engender the modification of the optimal smoothing value \u00b5 opt and consequently all the other estimations due to the different convergence rate of FISTA. All these questions will be considered in future works."}, {"section_title": "The algorithm", "text": "Most inputs of CONESTA Algorithm 2 are already detailed above for FISTA (Algorithm 1). Note that all functions that depend on \u00b5 in FISTA assume that this is a fixed value, while it is a free input parameter of CONESTA. In fact, it is a parameter that is calculated by CONESTA. The latter admit additional parameters: the duality gap function, Gap \u00b5 (\u00b7), defined in Equation 3.9, the factor \u03c4 , defined in Theorem 4, and the scalar M , defined in Equation 2.10.\nbreak 10:\nend if 11:\n\u00b5 i+1 = \u00b5opt \u03b5 i+1 13:\nHere we explain some of the steps of the algorithm and give references to the equations and theorems that are used.\n\u2022 In Line 1, the duality gap is used to estimate the current error, f (\u03b2 0 ) \u2212 f (\u03b2 * ), at the start vector \u03b2 0 for the non-smooth problem. In this particular case, we use Gap \u00b5 (\u00b7) with a negligible smoothing value of e.g. \u00b5 = 10 \u22128 .\n\u2022 Line 2 selects the optimal smoothing parameter, \u00b5 0 , such that we reach the prescribed precision \u03b5 0 , when using FISTA.\n\u2022 Line 5 defines the stopping precision for the FISTA loop. This choice ensures that at the next step, i.e. on Line 6, Gap \u00b5 i (\u03b2 i+1 ) < \u03b5 i \u2212\u00b5 i \u03b3M which implies that the error will be smaller than \u03b5 i . Note that by selecting the maximum compared to \u03b5, we avoid situations in which FISTA converges to precisions smaller than the globally prescribed one.\n\u2022 Line 7 estimates the current error (after FISTA) toward the non-smooth problem using Gap \u00b5 i (\u00b7) + \u00b5 i \u03b3M . We prefer to use Gap \u00b5 (\u03b2) over Gap(\u03b2)\nbecause it converges to zero for any fixed \u00b5 unlike Gap(\u03b2). Thus the application of FISTA will converge as shown in the proof of Theorem 4.\n\u2022 Line 8 checks the stopping criterion for the main algorithm. Together with Line 5 it ensures that the final computed\nRoughly speaking, this is equivalent to inspecting the final stopping criterion, f (\u03b2 i ) \u2212 f (\u03b2 * ) < \u03b5, at every iteration of the inner FISTA loop.\n\u2022 Line 11 computes the next precision \u03b5 i+1 in a decreasing geometric progression governed by \u03c4 .\n\u2022 Line 12 updates the smoothing parameter by selecting the optimal one with respect to the precision \u03b5 i+1 ."}, {"section_title": "Application to linear regression with elastic net and total variation penalties", "text": "This section describes how to implement the theory described above for the minimization problem of interest in this paper, namely that in Equation 2.2. The input parameters of the CONESTA algorithm were: a random starting vector \u03b2 0 \u2208 R p ; a precision \u03b5 = 10 \u22126 ; M = max \u03b1\u2208K To apply the presented theory in the specific context with a TV penalty, we need to define the linear operator for Nesterov's smoothing (denoted A) and how to compute the dual variables. The former is used to calculate the gradient \u2207(g + \u03b3s \u00b5 ) (Equation 2.7) and the gap Gap \u00b5 (\u00b7) (Equation 3.9).\nNow we show how we can express the TV operator in a dual form on \u03b2, i.e. in the form T V (\u03b2) = max\nwhere K T V is a compact space and A T V is a linear operator. This formulation, introduced in Equation 2.4, is at the core of CONESTA. It allows the determination of the linear operator A and the computation of \u03b1 * \u00b5 . We consider the general case where we have n input 3D images that are not defined over the entire\n. . , p} be the function that maps 3D coordinates, within the mask, onto the one-dimensional index of the masked and collapsed image. Finally, we denote by grad (i,j,k) (x) the spatial gradient of the collapsed vector x at \u03c6(i, j, k). We have, by definition, that\nLet (i, j, k) be a voxel inside the mask. In the context of TV, a group of variables G (as in Equation 2.3) is composed of this voxel and three of its \"face neighbors\", when they exist, namely voxels (i + 1, j, k), (i, j + 1, k) and (i, j, k + 1). Let us consider first the case where all the neighbors of (i, j, k) also belong to the mask:\nThe A r G is a linear operator from the reduced space R 4 and can be extended to R p in order to be directly applied on \u03b2. We denote this extended version A G . The other cases, where some neighbors of the considered point are outside the mask, are not detailed here, but do not change the theoretical result. Using Equation 2.3, we obtain a dual form of the TV:\nAs a consequence of this dual formulation, we obtain the desired result, namely that T V (\u03b2) = max\nwhere\nThe dual variable \u03b1 T V is the concatenation of all \u03b1 G and belongs to K T V = \u03a0 G\u2208G K G . The A T V is the vertical concatenation of all the A G matrices, and K T V , being the product of all the compact spaces K G , is itself a compact space.\nTo calculate Gap \u00b5 (\u00b7) and the gradient \u2207(g + \u03b3s \u00b5 ) we need to define the projection onto K. The latter is necessary in order to compute \u03b1 * \u00b5 (\u03b2) (see Equation 2.11) which is itself a function of the projections onto each compact set K G , defined by\nThe projection onto K is then the concatenation of all the projections onto the sets K G ."}, {"section_title": "Experiments on simulated 1D data set where the solution of min \u03b2 f is known", "text": "In this section we propose an extensive comparison of CONESTA with other state-of-the-art algorithms on simulated data, where we control all aspect of the data: the true minimizer, \u03b2 * , associated with the Lagrange multipliers \u03ba, \u03bb and \u03b3."}, {"section_title": "Data set generation", "text": "The experimental setup for the simulated 1D data set was inspired by that of Bach et al. [3] and is shown in Table 1 . This setup is a designed experiment with multiple small-and medium-scale data sets having combinations of low, medium and high degrees of correlation between variables, low, medium and high levels of sparsity, and low, medium and high signal-to-noise ratios.\nA candidate version of the predictors, i.e. the X 0 matrix, was drawn from a Gaussian distribution N (1, \u03a3), where \u03a3 was generated according to the constant correlation model described by Hardin, Ramon Garcia and Golan [23] , where diag(\u03a3) = 1 and off-diagonal elements are all equal to \u03c1. The \u03c1 is drawn from a distribution that approximately follows the Gaussian distribution\nHere, d c is the parameter controlling the dispersion of the correlation, and takes values of d c = 1 in the low-correlation case, d c = 4.5 in the mid-correlation case and d c = 8 in the high-correlation case.\nConcerning the weights vector, \u03b2 * , a sparsity parameter fixes the percent of null weights within it. The remaining weights, that contribute to the prediction, were drawn from U(0, 1) and sorted in ascending order.\nThe elements of the error vector, e, were all drawn from a Gaussian distribution, N (1, 1), and then scaled to unit 2 -norm.\nGiven a candidate data matrix X 0 , the true minimizer \u03b2 * , the true residual vector e, and regularization constants \u03ba, \u03bb and \u03b3, that were arbitrarily set to \u03ba = 0.618, \u03bb = 1 \u2212 \u03ba and \u03b3 = 1.618, we generated the simulated data set (X, y) such that \u03b2 * is the solution that minimizes Equation 2.2, with s(\u03b2 * ) = T V (\u03b2 * ). The data matrix X is obtained by finding scaling factors to be applied on the columns of the candidate data matrix X 0 . All the details of the scaling procedure to produce a data set with a known exact solution is detailed in the ancillary paper [27] . Once X is found, the y vector is computed as y = X\u03b2 * \u2212 e. The main benefit of generating the data in this way is that we know everything about it. In particular, we know the true minimizer, \u03b2 * , and we know the Lagrange multipliers \u03ba, \u03bb and \u03b3. There is no need to use e.g. cross-validation to find any parameters, and we can immediately determine whether the \u03b2 k we obtain is close to the true \u03b2 * or not. Five runs were performed for each of these settings. A total of 405 simulation runs where thus performed. For each combination of settings, we generate data such that we knew the exact solution of the given minimization problem."}, {"section_title": "Result of the comparison", "text": "We applied the minimization algorithms to simulated data that were generated using the settings shown in Table 1 . For each of the settings of the designed experiment, we measured the number of iterations and the time (in seconds) required to reach a certain precision level, \u03b5, with each of the tested minimization algorithms. For each precision level, ranging from 1 to 10 \u22126 , for each algorithm Table 1 The experimental setup for the first simulation study. The parameters varied were: the size of the data set (n, p), the correlation between variables, the sparsity of the data, and the signal-to-noise ratio. Five runs were performed for each combination of settings in order to assess the variability of the different algorithms. The medium level for sparsity (marked with an asterisk, '*') was only performed for the low and medium sizes. and for each data set, we ranked each algorithm according to the time it consumed in order to reach the given precision. Then, we averaged the ranks across data sets in order to evaluate the significance of the difference in ranks between algorithms. We used the Friedman test [19] to do this. Table 2 Average rank of the convergence speed of the algorithms to reach precisions (f (\u03b2 k ) \u2212 f (\u03b2 * )) ranging from 1 to 10 \u22126 . We have here reported whether the average rank of a given algorithm was significantly larger > (slower) or significantly smaller < (faster) than CONESTA (a missing '>' or '<' means non-significant, i.e. p > 0.05). We performed a post hoc analysis using the Friedman test. All reported significances were p < 10 \u22123 or smaller, corrected for multiple comparisons. For a given data set, all algorithms were evaluated with a limited upper execution time. Thus, some high precisions (e.g., 10 \u22125 , 10 \u22126 )\nwere not always reached within the limited time. For some algorithms (FISTA with medium/large \u00b5), the high precisions may in fact not be reachable at all. In those situations, the execution time was set to +\u221e.\nAverage rank of the speed to reach a given precision\nAs seen in Figure 1 and Table 2 , the excessive gap method is the fastest algorithm for the lowest (i.e., largest) precision, \u03b5 = 1. It significantly outperformed CONESTA; ADMM was the second fastest algorithm, but it was not significantly faster than CONESTA. CONESTA was the third fastest, it significantly outperformed the inexact FISTA and FISTA with medium or small \u00b5.\nThe average convergence rank of CONESTA improved for the higher precision levels. CONESTA remained the third fastest algorithm, for levels \u03b5 = 10 \u22121 and \u03b5 = 10 \u22122 , after ADMM and the excessive gap method, but it was no longer significantly slower than ADMM. However superiority over all FISTA with Nesterov's smoothing and inexact FISTA remained, and became significant. For all precisions higher (i.e., smaller) than 10 \u22123 , CONESTA outperformed all other algorithms. The superiority of CONESTA is significant for all precisions, except for \u03b5 = 10 \u22123 . We did not perform any particular code optimization in order to fairly measure the time difference between the algorithms. However, to avoid any suspicion of bias due to our implementation, Figure 2 presents error as a function of the number of iterations.\nADMM appears to be particularly fast in the beginning, where it converges quickly to the middle precision levels, after which it starts to level and be outperformed by CONESTA. This ADMM behavior is well-known, and is discussed by e.g. Boyd et al. [9] .\nThe inexact FISTA performs impressively when only looking at the number of iterations. However on large data sets, it took a similar number of iterations as CONESTA to reach the same precision. This implies an advantage to CONESTA, namely that it should scale better with larger data, i.e. that its asymptotic time complexity be lower than that of ADMM. The data used in the preparation of this article were obtained from the database of the Alzheimer's disease (AD) neuroimaging initiative (ADNI) (http://adni. loni.usc.edu/).\nThe MRI data set included standard T1-weighted images obtained with different 1.5-T scanner types using a three-dimensional MP-RAGE sequence or equivalent protocols with varying resolutions. Images were post-processed to correct for some artifacts [25] . As as result, 509 images [17] were segmented into GM (Gray Matter), WM (White Matter) and CSF (Cerebrospinal Fluid) using the SPM8 unified segmentation routine [2] .\nA total of 456 images were retained after quality control on GM probability. These images were spatially normalized into a template (dimension: 121\u00d7145\u00d7121, voxels size: 1.5 mm isotropic) using DARTEL [1] and modulated with the Jacobian determinants of the nonlinear deformation field.\nWe retain GM voxels with a minimum value of 0.01 and at least 10 \u22126 of standard-deviation across the participants' cohort. Then each voxel was centered and scaled at the cohort level. Those masked, warped and modulated GM images (286 214 voxels) were completed with three demographic predictors (age, gender and education level) leading to p = 286 217 input features. Those three demographic predictors were excluded from any penalization.\nSince the ultimate goal of such machine learning approaches is to predict the future \"medical\" outcome of subjects, we used as the target variable (y), the ADAS (Alzheimer's Disease Assessment Scale-Cognitive Subscale) score measured 800 days after the acquisition of the brain images. The ADAS score is one of the most frequently used tests to measure cognition in clinical trials and it is provided in the ADNI data set.\nAs participants, we considered one group of 119 control subjects (CTL) that never converted to AD within the six years of the study. As patients, we considered one group of patients with mild cognitive impairment (MCI) that converted to AD within 800 days. We pooled those two groups leading to a data set with n = 199 subjects."}, {"section_title": "Relevance of the TV penalty in the context of neuroimaging", "text": "Before comparing convergence speed of the optimization algorithms, this section demonstrates the improvements obtained with structured sparsity; or more precisely with TV in the context of neuroimaging. Indeed, the ultimate purpose of CONESTA is to provide an efficient algorithm to extract relevant biomarkers of brain diseases thanks to the benefit of the TV penalty in the context of very high dimensional data (> 10 5 features). Consequently, this section compares performances with and without TV, independent of the optimization algorithms.\nModels were compared using 5-fold cross-validation (5CV). To asses the predictive power of the model, we computed the coefficient of determination, Rsquared (R 2 ), obtained on the left-out samples. Along with the expectation of improved prediction performances, the TV penalty targets a more important goal: the estimation of reproducible \u03b2 maps against variations of the learning samples. Indeed, clinicians expect that the identified biomarkers, the non-null weights of the \u03b2 map, be similar if other patients, with similar clinical conditions, would have been used.\nWe used two similarity measures to asses the stability of those \u03b2 maps across the re-sampling. First we computed the mean correlation between pairs of \u03b2 maps obtained across the 5CV: pairwise correlations were transformed into zscores using Fisher's z-transformation, averaged and finally transformed back into an average correlation denoted r \u03b2 . Second, we computed an inter-rater agreement measure: the Fleiss' \u03ba statistics, to asses the agreement between supports (non-null weights of \u03b2 maps) recovered by the different penalties across the 5CV.\nThe weights of the five \u03b2 maps were partitioned into 3 categories according to their signs. This lead to negative, positive and, out-of-support voxels. The Fleiss' kappa statistics, \u03ba \u03b2 , was then computed for the five raters. Since the 5CV folds share many (\u223c64%) of their training samples, no unbiased significance measure can be directly obtained from the r \u03b2 and \u03ba \u03b2 statistics. Thus, we used permutation testing to asses empirical p-values. The null hypothesis was simulated by 1 000 random permutations of the target variable, y, within each permuted sample we executed the whole 5CV round. Then the statistics on the true data were compared with the ones obtained on the permuted ones. Table 3 Predictive performances: R-squared (R 2 ) and stability of \u03b2 maps measured as the average correlation between pairs of r \u03b2 (using Fisher's z-transformation) and the inter-rater measure (\u03ba \u03b2 , Fleiss' \u03ba statistic) of agreement between the supports recovered across the 5CV. Empirical p-values were assessed by 1 000 executions of the 5CV round on randomly permuted data. Significance notations: ***: P \u2264 10 \u22123 , **: P \u2264 10 \u22122 , *: P \u2264 0.05. The predictive performances, displayed in Table 3 , were not clearly improved with the TV penalty. However, 1 + T V was found significantly better than 1 alone. Figure 3 demonstrates that the TV penalty provides a major breakthrough in terms of support recovery of the predictive brain regions. Contrary to the elastic net ( 1 + 2 ) penalty, that highlights scattered and meaningless patterns, the elastic net + TV (i.e., 1 + 2 + T V ) penalties provide a smooth map that match the well-known brain regions involved in AD [20] . The most important cluster of non-null weights was found in the left hippocampus and the ventricular enlargement was identified. The right panel of Figure 3 simply confirm that TV is producing much more stable supports than elastic net only. Without TV, some (few) voxels within the hippocampus were at most selected 3 times during the 5CV round. Such results demonstrate that elastic net is effectively useless in the context of biomarker identification. Conversely, with the TV penalty, voxels within the hippocampus were always selected. The measures of the stability of \u03b2 maps presented in Table 3 demonstrated that TV significantly improves the reproducibility of the predictors map leading to meaningful biomarkers."}, {"section_title": "Comparisons of the convergence speed of the minimization algorithms", "text": "After having demonstrated the relevance of TV to extract stable biomarkers, we go back to the comparison of the convergence speed of CONESTA to that of the other state-of-the-art minimization algorithms mentioned in Section 2.2, namely: the excessive gap method, FISTA with three different fixed smoothing parameters (\u00b5, see Section 2.4) and inexact FISTA (the proximal operator is approximated).\nNote: ADMM has been excluded in this example. The version by Wahlberg et al. [42] is made for 1D underlying data, and the authors did not adapt the method to the case of 2D or 3D underlying data. Note also that without the particular form of the A A matrix (it is tridiagonal in the 1 + T V 1D case), ADMM would perform much slower, because it requires us to solve a linear system in each iteration. This is thus a major drawback of ADMM, that it can not easily be adapted to arbitrary complex penalties.\nFor FISTA with different values of \u00b5, we chose the three values of \u00b5 as follows: (i) Chen's \u00b5 with \u00b5 = \u03b5 2\u03b3M as explained in Section 2.4 (used by Chen et al. [15] ), (ii) Medium \u00b5 = (Chen's \u00b5) 1/2 and (iii) Large \u00b5 = (Chen's \u00b5) 1/4 . The first proposal for \u00b5 ensures that we reach the desired precision, but it may be slow to converge. The two other proposals are larger values of \u00b5 that do not guarantee that we reach the desired precision before convergence. But as we will see, FISTA with different values of \u00b5 will be tough competitors in the beginning when evaluating whether CONESTA can match their convergence speed.\nFirst we fixed a desired precision, = 10 \u22126 , which was used as the stopping Figure 4 shows that FISTA with fixed \u00b5 is either too slow (Chen's \u00b5) or, as expected, does not reach the desired precision (as with the two large \u00b5). However, CONESTA compete with FISTA with large \u00b5 and excessive gap, during the first iterations. This demonstrates two points: CONESTA dynamically picked an efficient (large enough) smoothing parameter and that the gap stopping criterion, used in the nested FISTA loop, allows us to stop before reaching a plateau and chose a smaller \u00b5. Figure 4 , right panel, illustrates that the number of iterations of inexact FISTA is low. However, the left panel of Figure 4 , shows that it is always considerably slower, in terms of the execution time, compared to the excessive gap method or CONESTA. Table 4 reveals that the execution time (as the ratio over CONESTA's execution time), to reach a given precision, increases up to a factor six. This demonstrates the hypothesis we stated in Section 2.2 that inexact FISTA becomes slower after many iteration due to the necessity to decrease the precision as 1/k 4 (k being the number of FISTA iterations), in the approximation. We experimentally observed the following phenomenon: during the first 100 FISTA iterations, only one iteration of the nested loop was necessary, but then this number increase progressively and exceeded 100 after few thousand FISTA iterations.\nAs a conclusion, Figure 4 and Table 4 demonstrate that on high-dimensional data sets, CONESTA outperformed all other algorithms, except in the very first iterations where excessive gap was faster, after which it is largely overtaken by CONESTA."}, {"section_title": "Conclusion and perspectives", "text": "We investigated in this paper the optimization problem where a linear regression model was combined with several convex penalties, including a structured penalty under very general assumptions on the structured penalty and the smooth loss function.\nWe proposed an algorithm, called CONESTA, that aims to overcome a set of difficulties experienced with current state-of-the-art algorithms, like the nonseparability and non-smoothness of the penalty. Our results show that the proposed algorithm enjoys both desirable theoretical convergence guarantees and practical scalability properties under various difficult settings involving complex structured constraints, and high-dimensionality. Specifically, the main benefits over the state-of-the-art methods can be summarized in the following points:\n1. CONESTA compares favorably to several optimal state-of-the-art optimization algorithms, such as the excessive gap method, ADMM and an inexact proximal gradient algorithm. It compares favorably in terms of execution time and precision of the solution. 2. The algorithm that we propose is able to include any combination of terms from a variety of smooth and non-smooth penalties. The algorithm does not require the proximal operators, or any other auxiliary minimization problem, contrary to many other similar state-of-the-art algorithms. For instance, any penalty in the form of a sum of q-norms of linear operators applied on all or on a subset of all variables could be used with this algorithm together with any smooth loss function for which we know the gradient. 3. The algorithm we propose has a rigorous stopping criterion that allows a desired precision to be reached with theoretical certainty.\nIn order to validate our method, we performed an extensive comparison in a realistic case that includes a real MRI data set and that utilizes the TV penalty. We chose TV since it outperforms some classical penalties, like elastic net or lasso, with the cost of added complexity. We showed that CONESTA brings a real advantage compared to other optimization frameworks, since it can easily implement the constraint and converges significantly faster.\nWe identified a few possibilities for improvement of CONESTA in order to either strengthen the theoretical results already obtained or to extend CON-ESTA to other machine learning methods. First, we have no theoretical proof of the order of the convergence rate for CONESTA. A theoretical study of this rate will confirm our empirical findings and show how it relates to the way we decrease \u03b5. Second, CONESTA can easily be adapted to many kinds of multivariate analyses. For instance, contrary to many of the other state-of-the-art methods, the logistic loss function can be used just as the mean square loss without additional development. We plan to test this structured classification model on a real data in a future work. Finally, we intend to apply the principles presented in this article to the joint analysis of multiple and heterogeneous data sets. Tenenhaus and Tenenhaus [40] proposed a general algorithm for regularized generalized canonical correlation analysis, subject to penalized terms. We intend to utilize the continuation method we presented in this paper in order to include complex penalties in this general multiblock method."}, {"section_title": "Supplementary Material", "text": "Supplementary document. \u2022 Url: https://github.com/neurospin/pylearn-parsimony \u2022 Description: ParsimonY is Python library for structured and sparse machine learning. ParsimonY is open-source (BSD License) and compliant with scikit-learn API.\nSupplementary materials 2/2: data sets and scripts.\n\u2022 Url: ftp://ftp.cea.fr//pub/unati/brainomics/papers/ols_nestv \u2022 Description: This url provides the data sets and the Python scripts used for the paper. We also provide a simple example that illustrates the usage of the ParsimonY library on neuroimaging data set. "}]