[{"section_title": "Abstract", "text": "Neural networks are increasingly being used in science to infer hidden dynamics of natural systems from noisy observations, a task typically handled by hierarchical models in ecology. This paper describes a class of hierarchical models parameterized by neural networks: neural hierarchical models. The derivation of such models analogizes the relationship between regression and neural networks. A case study is developed for a neural dynamic occupancy model of North American bird populations, trained on millions of detection/non-detection time series for hundreds of species, providing insights into colonization and extinction at a continental scale. Flexible models are increasingly needed that scale to large data and represent ecological processes. Neural hierarchical models satisfy this need, providing a bridge between deep learning and ecological modeling that combines the function representation power of neural networks with the inferential capacity of hierarchical models.\nA multi-species dynamic occupancy model for spatially referenced routes s = 1, ..., S, surveyed in years t = 1, ...T , for species j = 1, ..., J aims to estimate colonization and extinction dynamics through time. The true occupancy state z t,s,j = 1 if species j is present, and z t,s,j = 0 if species j is absent. The model represents z t,s,j as a Bernoulli distributed random variable, where Pr(z t,s,j = 1) = \u03c8 t,s,j . The probability of occurrence on the first timestep is \u03c8 t=1,s,j . Subsequent dynamics are determined by probabilities of persistence from time t to t + 1 denoted \u03c6 t,s,j , and probabilities of colonization from time t to t + 1 denoted \u03b3 t,s,j , so that the probability of occurrence in timesteps t = 2, ..., T is (MacKenzie et al. 2003) :\nLet y t,s,j represent the number of stops where species j was detected in year t on route s. Conditional on a species being present, it is detected at each stop with probability p t,s,j . Assume that there are no false-positive detections, so that if a species is absent, it cannot be detected (but see Royle & Link 2006) . With k = 50 replicate stops on each transect, the observations can be modeled using a Binomial likelihood for one species-route-year combination:"}, {"section_title": "Introduction", "text": "Deep neural networks have proved useful in myriad tasks due their ability to represent complex functions over structured domains (LeCun et al. 2015) . While ecologists are beginning to use such approaches, e.g., to identify plants and animals in images (Norouzzadeh et al. 2018; Fricker et al. 2019) , there has been relatively little integration of deep neural networks with ecological models.\nEcological processes are difficult to observe. Inference often proceeds by modeling the relationship between imperfect data and latent quantities or processes of interest with hierarchical models (Wikle 2003) . For example, occupancy models estimate the presence or absence of a species using imperfect detection data (MacKenzie et al. 2002) , and \"dynamic\" occupancy models estimate extinction and colonization dynamics (MacKenzie et al. 2003) . Population growth provides another example, motivating hierarchical models that link noisy observations to mechanistic models (De Valpine & Hastings 2002) . In such models, it is often desirable to account for heterogeneity among sample units (e.g., differences among habitats and survey conditions), to better understand ecological dynamics.\nMany hierarchical models in ecology account for heterogeneity among sample units using a linear combination of explanatory variables, despite there often being reasons to expect non-linearity (Lek et al. 1996; Austin 2002; Oksanen & Minchin 2002) . A variety of solutions exist to account for non-linearity. For example, Gaussian processes have been used for species distribution models (Latimer et al. 2009; Golding & Purse 2016) , in animal movement models (Johnson et al. 2008) , and in point process models for distance sampling data (Johnson et al. 2010; Yuan et al. 2017) . Generalized additive models also have been used to account for spatial autocorrelation (Miller et al. 2013; Webb et al. 2014) , nonlinear responses to habitat characteristics (Knapp et al. 2003; Bled et al. 2013) , and differential catchability in capture-recapture studies (Zwane & Van der Heijden 2004) .\nMachine learning provides additional tools for approximating nonlinear functions in hierarchical models. For example, Hutchinson et al. (2011) combined a site-occupancy model with an ensemble of decision trees to predict bird occurrence, combining a structured observation and process model from ecology with a flexible random forest model. Similar hybrid approaches could be developed for other classes of ecological models and/or machine learning methods. Neural networks seem particularly worthy of attention, given their success in other domains (LeCun et al. 2015) .\nNeural networks have been used in ecology, but to date have not been integrated into hierarchical models that explicitly distinguish between ecological processes and imperfect observations. For example, neural networks have modeled observed abundances of aquatic organisms, but without distinguishing observed from true abundance (Chon et al. 2001; Jeong et al. 2001 Jeong et al. , 2008 Malek et al. 2012) . Neural networks also have been used to model stock-recruitment and apparent presence/absence data (Manel et al. 1999; Chen & Hare 2006; \u00d6zesmi et al. 2006; Harris 2015; Chen et al. 2016 ), but have not yet been extended to account for imperfect detection, despite increasing recognition of its importance (Guillera-Arroita 2017; Tobler et al. 2019) . Notably, standard neural networks that classify or predict data are of limited use for ecological applications where imperfect data are used to learn about dynamics of hard to observe systems. As a solution, this paper describes neural hierarchical models that combine the function representation capacity of neural networks with hierarchical models that represent ecological processes."}, {"section_title": "Related work Neural networks", "text": "Neural networks are function approximators. Linear regression is a special case, where an input vector x is mapped to a predicted value y:\nwhere w is a parameter vector ( Fig. 1a) . Predicted values are linear combinations of the inputs x due to the product w T x, which restricts the complexity of the function mapping x to y.\nInstead of modeling outputs as linear functions of x, a model can be developed that is a linear function of a nonlinear transformation of x. Nonlinear transformations can be specified via polynomial terms, splines, or another basis expansion (Hefley et al. 2017) , but neural networks parameterize the transformation via a set of sequential \"hidden layers\" (Goodfellow et al. 2016) .\nIn a neural network with one hidden layer, the first hidden layer maps the length D input x to a length D (1) vector of \"activations\" a (1) = W (1) x, where W (1) is a D (1) \u00d7 D parameter matrix. The activations are passed to a differentiable nonlinear activation function g to obtain the \"hidden units\" of the first layer h (1) = g(a (1) ) ( Fig. 1b) .\nThe final layer of a neural network maps the a hidden layer to an output. For a neural network with one hidden layer, if the output variable y is a K dimensional vector, the output unit activations are given by a (2) = W (2) h (1) , where W (2) is a K \u00d7 D (1) parameter matrix. The output activation can be written as a composition of functions that transform the inputs x:\nSimilar to link functions in generalized linear models, outputs can be transformed by an output activation function. For example, if y is unbounded, the identity function can act as an output activation so that y = a (2) . Neural networks that predict probabilities typically use a sigmoid (inverse logit) activation function.\nNeural networks usually are trained using stochastic gradient based optimization to minimize a loss function, e.g., the negative log likelihood of a Gaussian distribution for a regression task, a Bernoulli distribution for binary classification, or a Poisson distribution for a count model. Partial derivatives of the model parameters are computed with respect to the loss via backpropagation, and parameters are updated to reduce the loss. In practice, these partial derivatives are often computed via automatic differentiation over a \"mini-batch\" of samples, which provides a noisy estimate of the gradient (Ruder 2016) .\nNeural networks are popular both because of their practical successes in a wide variety of applications, and because they possess some desirable theoretical properties. A neural network with suitable activation functions and a single hidden layer containing a finite number of neurons can approximate nearly any continuous function on a compact domain (Cybenko 1989; Hornik 1991) . \"Deep\" neural networks with many sequential hidden layers also act as function approximators (Lu et al. 2017) . The field of deep learning, which applies such networks, provides a variety of network architectures to account for temporal structure (Hochreiter & Schmidhuber 1997) , spatial structure on regular grids (Long et al. 2015) , or graphs (Niepert et al. 2016) , sets of unordered irregular points (Li et al. 2018) , and spatiotemporal data on grids or graphs (Xingjian et al. 2015; Jain et al. 2016) .\nThe potential for deep learning has been recognized in Earth science (Reichstein et al. 2019) , the natural sciences (Ching et al. 2018; Gazestani & Lewis 2019; Roscher et al. 2019) , physical sciences (Carleo et al. 2019 ), chemical sciences (Butler et al. 2018) , and ecology (Christin et al. 2018; Desjardins-Proulx et al. 2019) . For example, models of lake temperature that combine neural networks with loss functions consistent with known physical mechanisms perform better than physical models and neural networks applied alone (Karpatne et al. 2017) . Similarly, generative adversarial networks with loss functions that encourage mass-balance have expedited electromagnetic calorimeter data generation from the Large Hadron Collider (Paganini et al. 2018; Radovic et al. 2018) . Convolutional neural networks also have been successfully deployed in population genetics to make inferences about introgression, recombination, selection, and population sizes (Flagel et al. 2018) . There are various ways to combine science knowledge with deep learning. Ba et al. (2019) provide a useful taxonomy in the context of physics-based deep learning. In ecology, hierarchical models present an opportunity to build upon existing approaches to derive science-based deep learning methods."}, {"section_title": "Hierarchical models", "text": "Hierarchical models combine a data model, a process model, and a parameter model (Berliner 1996; Wikle 2003) . Data models represent the probability distribution of observations conditioned on a process and some parameters, e.g., the probability of capturing a marked animal, given its true state (alive or dead). Process models represent states and their dynamics, conditioned on some parameters. State variables are often incompletely observed, e.g., whether an individual animal is alive or whether a site is occupied. Parameter models represent probability distributions for unknown parameterspriors in a Bayesian framework. In a non-Bayesian setting, parameters are treated as unknowns and estimated from the data, but parameter uncertainty is not represented using probability distributions (Cressie et al. 2009 )."}, {"section_title": "Neural hierarchical models", "text": "Neural hierarchical models are hierarchical models in which the observation, process, or parameter model is parameterized by a neural network. These models are hierarchical (sensu Berliner (1996) ) if they distinguish between a modeled process and available data, e.g., between partial differential equations and noisy observations of their solutions (Raissi 2018) . \"Deep Markov models\" -hidden Markov models parameterized by neural networks -provide an example, with successful applications in polyphonic music structure discovery, patient state reconstruction in medical data, and time series forecasting (Krishnan et al. 2017; Rangapuram et al. 2018) . State-space neural networks that use recurrent architectures provide another example dating back two decades (Zamarre\u00f1o & Vega 1998; Van Lint et al. 2002) . This class of models inherits the flexibility and scalability of neural networks, along with the inferential power of hierarchical models, but applications in ecology and environmental science are only just beginning to emerge (Wikle 2019) .\nConstruction of such models from existing hierarchical models is straightforward. For example, one can propose neural variants of occupancy models (MacKenzie et al. 2002) , dynamic occupancy models (MacKenzie et al. 2003; Royle & K\u00e9ry 2007) , N-mixture models (Royle 2004) , markrecapture models (Jolly 1965; Calvert et al. 2009 ), and other hidden Markov models (Patterson et al. 2009 (Patterson et al. , 2017 Langrock et al. 2012) . Output activation functions can be determined from inverse link functions, e.g., sigmoid (inverse logit) activations for probabilities, and loss functions can be constructed from the negative log likelihoods (see Appendix S1 in Supporting Information for example model specifications). Specialized neural network architectures that operate on structured data can be readily integrated into such models. For example, Appendix S2 in Supporting Information provides a simulated animal movement case study where aerial imagery is mapped to state transition probabilities of a hidden Markov model with a convolutional neural network. To provide an empirical use case, a neural dynamic occupancy model is developed for extinction and colonization dynamics of North American bird communities."}, {"section_title": "Case study", "text": "The North American Breeding Bird Survey (BBS) is a large-scale annual survey aimed at characterizing trends in roadside bird populations (Link & Sauer 1998; Sauer & Link 2011; Sauer et al. 2013; Pardieck et al. 2018) . Thousands of routes are surveyed once a year during the breeding season. Surveys consist of volunteer observers that stop 50 times at points 800 meters apart on a transect, recording all birds detected within 400 meters for three minutes. Species can be present, but not detected, and may go locally extinct or colonize new routes from year to year, motivating the development of dynamic occupancy models which use imperfect detection data to estimate latent presence or absence states (MacKenzie et al. 2003; Royle & K\u00e9ry 2007) .\nThis case study uses BBS data from 1997-2018 excluding unidentified or hybrid species, restricting the analysis to surveys meeting the official BBS criteria (Pardieck et al. 2018) . The resulting data consists of 647 species sampled at 4,540 routes, for a total of 59,384 surveys (not every route is surveyed in each year), 2,937,380 observation history time series, and 38,421,448 detection/non-detection observations.\nEmbeddings are used extensively in language models such as word2vec, which maps words to vector spaces (Mikolov et al. 2013) .\nFurther, the multi-species model combined species embeddings with encoder-decoder components to estimate occupancy parameters. Encoder-decoder neural networks are used in sequence-to-sequence translation . They encode inputs (e.g., a sequence of words) into a latent vector space, then decode that vector representation to generate another sequence using a neural network. Similarly, the multi-species model encoded route-level features into vector representations. These route vectors were decoded by a recurrent neural network to generate a multivariate time series of latent vectors associated with colonization, persistence, and detection (Chung et al. 2014) . Finally, the multi-species model combined these route-level latent vectors with species-level embeddings to compute colonization, persistence, and detection probabilities ( Fig. 1e ). For additional details, see Appendix S3 in Supporting Information."}, {"section_title": "Model comparisons", "text": "To compare the performance of the three modeling approaches, the data were partitioned into a training, validation, and test set at the EPA level two ecoregion level (Roberts et al. 2017) . All routes within an ecoregion were assigned to the same partition. This resulted in 2154 training routes, 948 validation routes, and 1438 test routes. K-fold cross validation would also be possible, though it requires retraining of each model K times (Roberts et al. 2017) . Because of the size of the BBS data and the computational resources required to train these models, a simpler train/validation/test split was used.\nFor each of the three modeling approaches, routes in the training set were used for parameter estimation. Single-species models were fit separately for each species (modeling approaches 1 and 2), and one multi-species model (approach 3) was fit using all of the training data. Then, using the trained models, the mean predictive log-density of the validation data was evaluated to identify the best performing model (Gelman et al. 2014 ). This step indicated which model fit best to the withheld validation data. Finally, the best performing model was retrained using the training and validation data, and its predictive performance was evaluated on the withheld test set (Russell & Norvig 2016) .\nThe predictive performance of the three models (best case, point extraction, and convolutional) was compared across a gradient of training set sizes. This gradient included datasets that consisted of 16, 32, 64, 128, 256, 512 , and 1024 simulated trajectories, each of which consisted of 50 timesteps. Training set size Validation set performance Figure S2 .7: Model performance on withheld validation data. The x-axis is the number of movement trajectories in the training data. The y-axis shows the performance (predictive log-likelihood) on withheld validation data. Each point is the result of a simulation, and colored lines connect results for each model. ConvHMM is the convolutional hidden Markov model of animal movement.\nThe smaller datasets were subsets of the larger datasets. For each model/training set combination, predictive performance was evaluated using the out of sample log-likelihood, computed for the 1024 withheld validation trajectories using the forward algorithm (Patterson et al. 2017) . After using the validation data to compare models, the preferred model was retrained using the training and validation data, and final predictive performance was evaluated on the still withheld test set."}, {"section_title": "Final model evaluation", "text": "The final model's performance was evaluated quantitatively and qualitatively. Quantitative predictive performance was evaluated in two ways. First, 95% prediction interval coverage was computed for test set counts. Prediction intervals were constructed using the quantiles of the binomial distribution, marginalizing over latent occupancy states. Second, the area under the receiver operator characteristic curve (AUC) was computed for binarized test set counts. The AUC analysis also marginalized over latent states to derive predicted probabilities, e.g., Pr(y t,s,j = 0 | p t,s,j , \u03c8 t,s,j ) = 1 \u2212 \u03c8 t,s,j + \u03c8 t,s,j (1 \u2212 p t,s,j ) 50 , where the parameters p and \u03c8 are estimated by the model. These two approaches provide information on how well the final model could predict the number of stops with detections at each route, and whether any detections would occur on each route, respectively.\nQualitative analyses were based on predicted occupancy states from the final model. The most likely occupancy states were computed at all BBS routes for each year and species using the Viterbi algorithm (Viterbi 1967) . The estimated occupancy states were then used to compute finite sample population growth rates for each species (Royle & K\u00e9ry 2007) . Occupancy state estimates were also used to compute annual spatial centroids for each species, by taking the spatial centroids of occupied route coordinates. These are hereafter referred to as \"BBS range centroids\" to differentiate from the actual centroid of a species' entire range.\nTo evaluate whether the results were qualitatively consistent with previous findings about colonization and extinction gradients over species ranges, correspondence between BBS range centroids and both colonization and persistence probabilities were assessed using linear regression (Mehlman 1997; Doherty Jr et al. 2003; Royle & K\u00e9ry 2007) . For this analysis, the response variable was colonization or persistence averaged over time, the predictor was distance from BBS range centroid. Survey routes were the sample units, and separate analyses were conducted for each species. To avoid bias associated with recently added BBS routes and variance associated with rare species, these analyses only used data from BBS routes surveyed in every year, species observed in every year, and species that occurred in 100 or more routes (Sauer et al. 2017) . Finally, visualizations were developed to graphically interpret the final model. First, route-level features were visualized using t-distributed stochastic neighbor embedding, which maps high dimensional vectors to lower dimensional representations (Maaten & Hinton 2008) . In this lower dimensional space, routes with similar embeddings are close together, and routes with dissimilar embeddings appear distant (Rauber et al. 2016) . Second, species' loading vectors were compared in terms of cosine similarity, which measures the orientation of loading vectors in latent space. Species' occupancy should be positively related if loading vectors are oriented similarly. This expectation was checked graphically by comparing estimated occupancy time series of species that had the most similar and most different loading vectors."}, {"section_title": "Results", "text": "The multi-species neural hierarchical model performed best on the withheld validation routes. The difference in mean validation set negative log likelihood was 5.015 relative to the baseline model and 2.108 relative to the single-species neural hierarchical model. For the final model, 95% prediction interval coverage for observed counts at withheld test set routes was 93.6%, with a standard deviation of 1.5%, a minimum of 86.5%, and a maximum of 97.6%. The model also predicted whether species would be detected on test set routes fairly well, with a mean test set AUC of 0.953, an among-route standard deviation of 0.032, a minimum of 0.667, and a maximum of 0.993 ( Fig. 2 ).\nQualitative results related to range shifts and population growth rates were plausible given previous work. The model identified the invasive Eurasian Collared-Dove (Streptopelia decaocto) as having the greatest range centroid displacement from 1997 to 2018 and the highest finite sample population growth rate ( Fig. 3 ), consistent with its invasion of North America from Florida following its introduction in the 1980's (Bled et al. 2011) . Increasing trends also have previously been reported for the species with the next three highest population growth rates: Bald Eagle (Haliaeetus leucocephalus), Wild Turkey (Meleagris gallopavo), and Osprey (Pandion haliaetus) (Sauer et al. 2013) .\nThe majority (77%) of common species were less likely to persist at routes that were distant from their estimated BBS range centroids. Similarly, 98% of common species were less likely to colonize routes distant from their BBS range centroids. There were examples of species with positive and negative distance coefficients for persistence and colonization ( Fig. 4a ). Negative relationships were most apparent for common species that occupied a large fraction of BBS routes (Fig. 4b ). Results for representative species are displayed in Fig. 4c \nRoute vectors combined information from the categorical and continuous route-level features. Unsurprisingly (because ecoregion was an input feature), routes in the same ecoregions clustered together ( Fig. 5a ). Route embeddings also revealed relationships among ecoregions. For example, Marine West Coast Forest ecoregion routes were similar to Northwestern Forested Mountains routes, and most different from Northern Forests routes ( Fig. 5a ). Variation within clusters also related to continuous route-level features (Fig. 5b ).\nThe model predicted non-linear dependence among species that is interpretable in terms of the cosine similarity among species-specific parameter vectors. For example, parameters for Mourning Dove (Zenaida macroura) were closest to those of Barn Swallow (Hirundo rustica), and most different from (Myrtle Warbler) Yellow-rumped Warbler (Setophaga coronata coronata) ( Fig. 6a ). On BBS routes where Mourning Doves are likely to occur, Barn Swallow are also likely to occur, and (Myrtle Warbler) Yellow-rumped Warbler are unlikely to occur. Species pairs of the most similar and most dissimilar loadings are also provided for Eurasian Collared-Dove and Bald Eagle ( Fig. 6b-c ).\nAs the training set increased in size to more than 64 trajectories, the convolutional movement model's performance exceeded the point extraction baseline (Fig. S2.7) . When the training set was relatively small, the convolutional movement model performed worse than the point extraction baseline. Validation set performance continued to increase as training data were added for all modles. The rate of increase was greatest for the convolutional model, however, and validation set performance did not appear to saturate even for the largest training data set ( Fig. S2.7) . As expected, the best case model, which is correctly specified and has access to the \"correct\" input data (canopy height) always had the highest performance. But, restricting attention to models that only have access to the RGB imagery, the convolutional movement model was preferred.\nFinal predictive checks on the withheld test set indicated that the convolutional model was able to estimate the true state transition probabilities fairly well. Transition probabilities from \"in transit\" (s t = 1) to \"foraging\" (s t = 2) were not captured as well as transitions from \"foraging\" (s t = 2) to \"in transit\" (s t = 1) ( Fig. S2.8) . In particular, the estimated distribution of transition probabilities was bimodal, but not as sharply peaked as the distribution of true transition probabilities.\nLast, to provide a qualitative sense of what the final model had learned, test set image chips with the highest predicted transition probabilities are visualized in Fig. S2.9 . Consistent with the underlying generative model, image chips centered on tree canopies were associated with the highest probabilities of transitioning into the \"foraging\" state, and image chips centered on bare ground (often with intermittent rocks) were associated with the highest probabilities of transitioning into the \"in transit\" state.\nTaken together, these results indicate that simpler models might perform better when limited training data are available, but that neural hierchical models might provide predictive performance improvements for large datasets. Figure S2 .8: Joint densities of true (x-axis) and estimated (y-axis) state transition probabilities in the final convolutional movement model for the withheld test set. Panel (a) shows the distribution for \u03b3 1,2 (transitions to \"foraging\"), with marginal histograms for the true and estimated probabilities. Panel (b) shows the same for \u03b3 2,1 (transitions to \"in transit\"). Cell color represents the density of observations, with brighter colors indicating higher densities. Figure S2 .9: Top nine test set image chips with the highest probabilities of transitioning (a) from a \"foraging\" state to \"in transit\", and (b) from \"in transit\" to \"foraging\"."}, {"section_title": "Discussion", "text": "Neural hierarchical models provide a bridge between hierarchical models built from scientific knowledge, and neural networks that approximate functions over structured domains. This framework integrates research in science-based deep learning and ecological modeling, and can use existing hierarchical models as a starting point. The case study provides a proof of concept example of constructing a scalable and performant neural hierarchical model based on a multi-species dynamic occupancy model, providing insights about colonization and extinction dynamics of North American bird assemblages at a continental scale. The breeding bird survey case study indicates that neural hierarchical models can outperform simpler models. Notably, the final model was performant both quantitatively and qualitatively, detecting population increases and range expansions that are consistent with prior work. The case study extends previous analyses of persistence probabilities at range edges (Royle & K\u00e9ry 2007) , indicating that common species tend to have higher persistence and colonization probabilities at routes close to their BBS range centroid. Yet, interpreting this result is complicated by irregular range geometry which can lead to centroids that near the edge (or even outside) of a species range, and divergence between actual and estimated ranges due to incomplete sampling (Sagarin & Gaines 2002; Fortin et al. 2005; Dallas et al. 2017; Knouft 2018) . Additional complexity is apparent in Fig. 4c , which indicates that gradients in colonization and persistence may not be isotropic (the same in every direction). With those caveats, the results are broadly consistent with a theoretical expectation that range boundaries can arise from gradients in local extinction and colonization rates (Holt & Keitt 2000) .\nThe case study used a gated recurrent neural network architecture to handle temporal structure (Chung et al. 2014) , and architectures designed for other data structures present additional opportunities for ecological applications. For example, neural hierarchical models could be used to couple a convolutional neural network observation model for camera trap images (Norouzzadeh et al. 2018; Tabak et al. 2019) with an ecological process model that describes animal density, movement, or community composition (Burton et al. 2015) . Gridded data such as modeled climate data, remotely sensed Earth observations, and even 96 well microplates also might use convolutional neural network architectures (Rawat & Wang 2017) . Appendix S2 provides a simulated example where gridded data (aerial imagery) are mapped to a state transition matrix of a hidden Markov model. In addition, many ecological datasets exhibit graph structure related to phylogenies, social networks, or network-like spatial structure. While it is possible to adapt convolutional neural networks to operate on distance matrices computed from graphs (Fioravanti et al. 2018) , graph representation learning can also provide embeddings for nodes that encode network structure and node attributes (Hamilton et al. 2017; Cai et al. 2018) .\nNeural hierarchical models can scale to data that are too large to fit in computer memory. Indeed, memory limitations precluded comparison of a fully Bayesian multi-species dynamic occupancy model against the multi-species neural hierarchical model. The current state of the art multi-species occupancy models use approximately one tenth of the number of species, at about half the number of sites, and are static in the sense that colonization and extinction dynamics through time are not represented (Tobler et al. 2019) . Species distribution models are increasingly scalable due to advances in approximate Gaussian process models (Tikhonov et al. 2019 ), but multi-species dynamic occupancy models have not previously been reported at this scale. This is particularly relevant for extensions of the BBS case study, given the volume of (imperfect) bird data accumulating through citizen science programs (Sullivan et al. 2009 ). The key strategy providing scalability in the case study is stochastic optimization that uses mini-batches -small subsets of a larger dataset -to generate noisy estimates of model performance and partial derivatives that can be used during training (Ruder 2016) . With this approach, the entire dataset does not need to be loaded into memory at the same time. This could be useful as a way to scale models that integrate BBS, eBird, and other data (Ngiam et al. 2011; Pacifici et al. 2017; Zipkin et al. 2017; Lin et al. 2019) .\nThough neural hierarchical models can perform well in the large data regime, they might be overparameterized in a small data setting. For this reason, performance comparisons with simpler baseline models are useful. Appendix S2 provides a simulated example, where simple baselines work better with small datasets, and more complex neural hierarchical models work better with large datasets. Approaches familiar to ecologists such as k-fold cross validation and (to a lesser extent) information criteria such as the Akaike Information Criterion (AIC) have been used with neural networks, but such approaches are less common than cross-validation with training, validation, and test partitions of the data (Anderson & Burnham 2004; Jiang & Chen 2016; Ran & Hu 2017) . With large datasets, considerable computational resources are required for training, so that retraining the same model many times might make k-fold cross validation infeasible. As a practical matter, even if a neural hierarchical model demonstrates superior predictive performance, a simpler model might be preferred if interpretability and/or explainability is a high priority, as it might be in a decision-making context (Rudin 2018) .\nNeural networks have a reputation for being \"black-box\" models. However, the interpretation and explanation of such models is an active area of research (Roscher et al. 2019) . Interpretations map abstract concepts to domains that humans can make sense of, e.g., mapping neural network parameters to species identity or spatial location (Montavon et al. 2018) . Explanations are collections of features in such a domain that contributed to a prediction (Montavon et al. 2018 ), e.g., sensitivity of model output to perturbations of the input (Olden & Jackson 2002; Gevrey et al. 2003) .\nLooking ahead, in a forecasting or decision-making setting, it would be important to estimate both aleatoric uncertainty (arising from noise in an observation process) and epistemic uncertainty (arising from uncertainty in a model and its parameters) (Clark et al. 2001; . Recent advances in accounting for uncertainty in neural network parameter estimates and architectures could be applied in future work. Approaches include \"Bayes by backpropagation\" (Blundell et al. 2015) , normalizing flows for variational approximations (Kingma et al. 2016) , adversarial training (Lakshminarayanan et al. 2017) , methods that use dropout or its continuous relaxation , and ensemble approaches (McDermott & Wikle 2017) . These methods can also help explain neural networks, e.g., to probabilistically estimate sensitivity to model inputs to random masking (Chang et al. 2017 ), or to decompose predictive uncertainty into component parts (Thiagarajan et al. 2019) .\nThe potential for combining neural networks with mechanistic models was recognized more than thirty years ago (Psichogios & Ungar 1992; Meade Jr & Fernandez 1994; Lagaris et al. 1998 ). This potential is more easily realized today due to methodological spillover from deep learning into the natural sciences, but also increases in computing power, availability of ecological data, and the proliferation of educational content for quantitative ecology and machine learning. Further, modern deep learning frameworks provide abstractions that allow users to focus on model construction rather than the details of implementation, increasing accessibility in the same way that WinBUGS, JAGS, OpenBUGS, and Stan have done (Lunn et al. 2000; Plummer & others 2003; Spiegelhalter et al. 2005; Carpenter et al. 2017) .\nAlthough deep learning and ecological modeling may seem to be separate activities, neural hierarchical models bridge these disciplines. Given the increasing availability of massive ecological data, scalable and flexible science-based models are increasingly needed. Neural hierarchical models satisfy this need, and can provide a framework that links imperfect observational data to ecological processes and mechanisms by construction.\nThis appendix includes example specifications for neural occupancy, N-mixture, and hidden Markov models. The goal in providing these specifications is to give concrete examples of neural hierarchical models that are relatively simple, while drawing connections to existing models. PyTorch implementations in Jupyter notebooks are available at https://github.com/mbjoseph/neuralecology.\nGenerally, the construction of a hierarchical model requires:\n1. A process model that represents some ecological dynamics or states that can depend on unknown parameters. 2. An observation model that relates available data to the process model, possibly dependent on unknown parameters. 3. A parameter model for unknown quantities (e.g., their dependence on some input, or relationship to each other). Traditionally, this is discussed in terms of prior distributions, though there may be rich structure encoded in a parameter model as well (Wikle 2003) . For a fully Bayesian hierarchical model, all unknowns are represented by probability distributions.\nIn an empirical Bayesian hierarchical model, point estimates are typically used for top-level parameters instead, so that these top-level parameters are treated as fixed, but unknown (Cressie et al. 2009 ). In the examples below, neural networks are applied at the parameter model stage to learn a mapping from inputs to parameters of process and observation models.\nGiven these components, parameter estimation can proceed in a few ways, depending on the inferential framework used. A loss function can be constructed to find the parameter values that maximize the probability of the data, possibly with some penalties for model complexity (in a maximum likelihood/penalized maximum likelihood framework), find the most probable parameter values, conditional on the data (in a maximum a posteriori framework), or compute a probability distribution for all unknowns, conditional on the data (in a Bayesian framework). For many models applied in deep learning, it is often the case that fully Bayesian inference is complicated by high-dimensional multimodal posterior geometry.\nThe examples here focus primarily on penalized maximum likelihood methods, though Bayesian approaches stand out as a key area for future work (see main text for relevant citations). Often, L 2 norm penalties (also referred to as \"weight decay\") are applied on the parameters of neural networks in the loss function to penalize model complexity in an effort to avoid overfitting. Additional strategies include early stopping, where an iterative stochastic optimization scheme is terminated before the training set loss stabilizes (or when the validation set loss begins to increase), which is particularly useful for complex models that can overfit quickly. Finally, unlike maximum likelihood estimation for simple models, because of the multimodality of the likelihood surface for neural network parameters, there are no guarantees that a global maximum exists, or that a particular optimum is not a local maximum. In practice, this is not a major limitation. For more discussion, see Goodfellow et al. (2016) ."}, {"section_title": "A single-species single-season neural occupancy model", "text": "A single-species single-season occupancy model estimates presence/absence states from imperfect detection/non-detection data (MacKenzie et al. 2002) . Assume that n spatial locations are each surveyed k times, during a short time interval for which it is reasonable to assume that the presence or absence of a species is constant. Each spatial location has some continuous covariate value represented by x i for site i = 1, ..., n, that relates to occupancy and detection probabilities."}, {"section_title": "Observation model", "text": "Observations at site i consist of k surveys, where each survey results in a detection or non-detection. Let y i represent the number of surveys at site i for which a species is detected, and z i represent the true presence/absence state (if the species is present: z i = 1; if absent: z i = 0). Assuming that the probability of detecting a species on a survey conditional on presence is p i , and that each survey is conditionally independent, the observations can be modeled with a Binomial distribution:\nObservations at site j in survey k yield counts of the number of unique individuals detected, denoted y j,k for all j and all k. Assuming that the detection of each individual is conditionally independent, and that each individual is detected with site-specific probability p j , the observations can be modeled with a Binomial distribution where the number of trials is the true (latent) population abundance n j : y j,k \u223c Binomial(p j , n j ).\nObservations y i,t are made on time steps t = 1, ..., T , and if an individual is detected alive on timestep t, y i,t = 1, and if an individual is detected as recently dead y i,t = 2, otherwise if the individual is not detected y i,t = 0.\nLet \u2126 t denote a time-varying emission matrix containing state-dependent observation probabilities:\nwhere p t is the probability of detecting an individual on timestep t, conditional on the individual being alive, and \u03bb t is the probability of recovering a recently dead individual on timestep t that has died since timestep t \u2212 1. The rows of the emission matrix correspond to states (alive, recently dead, long dead), and the columns correspond to observations (not detected, detected alive, detected dead)."}, {"section_title": "Process model", "text": "The true population abundance n j is treated as a Poisson random variable with expected value \u03bb j :\nAt time t, the transition probability matrix \u0393 t contains the probability of transitioning from row j to column k:\nwhere survival probability is \u03c6 t = P (z i,t+1 = 0 | z i,t = 0), dead individuals stay dead, and the rows and columns of the transition matrix correspond to states (alive, recently dead, long dead)."}, {"section_title": "Parameter model", "text": "Heterogeneity among sites was accounted for using a single layer neural network that ingests the one dimensional covariate x i for site i, passes it through a single hidden layer, and outputs a two dimensional vector containing a detection probability p i and the expected abundance \u03bb i :\nwhere f is a neural network with two dimensional output activations h(x i ) computed via:\nand final outputs computed using the log and logit link functions for expected abundance and detection probability:\n.\nHere too W (1) is a parameter matrix that generates activations from the inputs, g is the rectified linear unit activation function, and W (2) is a parameter matrix that maps the hidden layer to the outputs. Additionally h 1 (x i ) is the first element of the output activation vector, and h 2 (x i ) the second element."}, {"section_title": "Loss function", "text": "The negative log likelihood was used as the loss function, enumerating over a large range of potential values of the true abundance (from min(y j .) to 5 \u00d7 max(y j .), where y j. is a vector of counts of length K) to approximate the underlying infinite mixture model implied by the Poisson model of abundance (Royle 2004) . It is also worth noting that alternative specifications based on a multivariate Poisson model are possible (Dennis et al. 2015) .\nA neural hidden Markov model: capture-recapture-recovery\nConsider a capture-recapture-recovery study aimed at estimating time-varying parameters (King 2012) . Assume that individuals i = 1, ..., N are initially captured, marked, and released on timestep t = 0.\nThe state of an individual i on time step t is denoted z i,t . Individuals are either alive (z i,t = 0), recently dead such that their bodies are discoverable (and marks identifiable) (z i,t = 1), or long dead such that their bodies are not discoverable and/or marks are no longer identifiable (z i,t = 2).\nThe negative log observed data likelihood was used as the loss function, and computed using the forward algorithm (Zucchini et al. 2017) . Observation histories for individuals were assumed to be conditionally independent. As an aside, one can specify such models in terms of the complete data likelihood (i.e., in terms of the hidden states) using programming frameworks that implement automatic enumeration of discrete latent variables with finite support, such as Pyro (Bingham et al. 2018) . "}, {"section_title": "Parameter Model", "text": "Heterogeneity among timesteps was accounted for using three two-layer neural networks (one for \u03c6, one for \u03bb, and one for p). Each network ingests a univariate time series and outputs a corresponding time series of parameter values (i.e., each network maps a sequence of inputs x = (x t=1 , ..., x t=T ) to a sequence of parameter values, e.g., p = (p t=1 , ..., p t=T ) for the detection probability network:\nwhere f (j) is a neural network for parameter j."}, {"section_title": "Behavioral states", "text": "Formally, consider a time series of length T containing the state of an animal at discrete times t = 1, ..., T , where s t = 1 means the animal is \"in transit\" and s t = 2 means the animal is \"foraging\" at time t. The state s t is either 1 or 2 for any particular t. The probabilities of transitioning between states is time-varying, and is summarized in a matrix \u0393 (t) , which contains the transition probabilities \u03b3 (t) i,j for states i, j = 1, 2 at time t. Each of these elements provides the probability of transitioning from one state to another, so that \u03b3\n1,2 would provide the probability of transitioning from \"in transit\" in time t (s t = 1) to \"foraging\" in time t + 1 (s t+1 = 2). At the first timestep, the state probabilities are contained in a row vector \u03b4, where \u03b4 i = [s t=1 = i], for states i = 1 and i = 2. In the simulation, the stationary state probabilities at randomly initialized starting locations were used as initial state probabilities.\nTo ensure that \"foraging\" was the more likely state in the canopy, and \"in transit\" was the most likely state over bare ground, the true state transition probabilities in the simulation were modeled as a logit-linear function of canopy height:\nwhere x t is the scaled canopy height at an animal's location in time t (Fig. S2.2A ). This fully specifies the state transition matrix, because the rows must sum to one, implying for example that \u03b3 "}, {"section_title": "State-dependent movement", "text": "The \"foraging\" and \"in transit\" behavioral states are associated with different movement patterns. Foraging is characterized by small step lengths with undirected turns. Movement trajectories for animals in transit are characterized by longer step lengths and more directed movements. Step size (m) Density (b) q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 \u03c0 2 \u03c0 3\u03c0 2 (c) Figure S2 .2: Panel A shows the true relationship between scaled canopy height (x-axis) and state transition probabilities in the simulation. Panels B and C show the densities of step sizes (B) and turn angles in radians (C) used in the simulation, colored by behavioral state.\nFormally, if the vector z t summarizes movement in the interval from time t to t + 1, it is common to consider two quantities: the step size l t and turning angle \u03c6 t , so that z t = (l t , \u03c6 t ) (Patterson et al. 2017) . Thus, the movement model is a hidden Markov model with states {s t } T t=1 , transition probability matrices {\u0393 (t) } T t=1 , and emissions {z t } T t=1 . In the simulation, step sizes were drawn from a gamma distribution with state-dependent parameters (Fig. S2.2B) :\nfor t = 1, ..., T . Turn angles were drawn from von Mises distributions with state-dependent parameters ( Fig. S2.2C) :\nvon Mises(0, 0.1) if s t = 2 (\"foraging\") , for t = 2, ..., T . Initial movement directions (at t = 1) were randomly drawn from the uniform circular distribution, though strictly speaking these are not turn angles which require three points to compute. Trajectories simulated within the study area were partitioned into three sets based on the northing coordinate boundaries of the trajectory extents. Trajectories in the northern third of the study area were assigned to the training set, those in in the southern third were used as a withheld test set, and those in the middle third were used as a validation set ( Fig. S2.3) . For each partition, 1024 trajectories were simulated, to generate 3072 total trajectories across all partitions."}, {"section_title": "Model descriptions", "text": "Three models were developed, each of which maps a different set of inputs to transition probability matrices: \nHere as before \u03b3 (t) i,j is the probability of transitioning from state s t = i to s t+1 = j. Intercept terms are represented by \u03b1 i,j , and slopes by \u03b2 i,j , with x t representing scaled canopy height. This model uses covariates for transition probabilities in the same way that many do: using a linear combination on a transformed scale ( Fig. S2.4 )."}, {"section_title": "Point extraction model", "text": "The point extraction model includes parameters to map the RGB image reflectance values (r t , g t , and b t ) to the transition probabilities using a linear combination on the logit scale:\nwhere \u03b2 k i,j is a coefficient for the (i, j) th transition probability and image band k. This model is also representative of the common approach taken to include covariates in such models -via a linear combination on a transformed scale ( Fig. S2.4) ,"}, {"section_title": "Convolutional hidden Markov model", "text": "The convolutional hidden Markov model is a neural hierarchical model that maps an image chip X (t) centered on an animal's location at time t to a transition probability matrix \u0393 (t) . This is a departure from the previous two models. The input X (t) is a multidimensional array instead of a real number (in the best case model) or a numeric vector (containing RGB reflectances in the point extraction model).\nTo generate image chip input arrays, square crops from the aerial RGB imagery were created centered on each simulated location. The spatial footprint of each chip was 128 \u00d7 128 pixels (\u2248 13 m \u00d7 13 m). This created a 3 \u00d7 128 \u00d7 128 array for each point location along each trajectory, where the three channels correspond to reflectance values in the red, green, and blue spectral bands (Fig. S2.5 ). To illustrate the potential for including additional raster data in addition to imagery, an additional band was concatenated to each chip which contained zeros everywhere except for a 2 \u00d7 2 region in the center of the 128 \u00d7 128 grid, generating 4 \u00d7 128 \u00d7 128 arrays. In real applications, this might represent additional raster data relevant to movement -inputs need not be images per se.\nThe convolutional hidden Markov model for animal movement mapped these 4 \u00d7 128 \u00d7 128 image chips to 2 \u00d7 2 state transition probability matrices {\u0393 (t) } T t=1 (Fig. S2.6 ). The architecture of the convolutional neural network is a simplified version of the AlexNet model that plays an important role in the history of deep learning in computer vision (Krizhevsky 2014) , though more modern architectures might perform better. Briefly, the input image is passed through a series of 2-dimensional convolutions, followed by nonlinear activation functions, followed by 2d max-pooling layers, creating a 64 \u00d7 2 \u00d7 2 lower spatial resolution array with many \"channels\". This three dimensional array is flattened to a one dimensional array, creating a vector of length 64 \u00d7 2 \u00d7 2, which is then passed to a series of fully connected hidden layers with nonlinear activations and dropout regularization (Srivastava et al. 2014) to create a vector of length 4. This vector is reshaped to a 2 \u00d7 2 matrix, then a softmax transformation is applied row-wise to ensure that the row probabilities sum to one (as they should in a state transition probability matrix). The resulting 2 \u00d7 2 matrix is the transition probability matrix \u0393 (t) , generated from the input X (t) .\nThe precise structure of the convolutional neural network that maps the input raster to the state transition probability matrix was as follows (in PyTorch-like psuedocode):"}, {"section_title": "Sequential(", "text": "Conv2d(4, 16, kernel_size=9, stride=3), LeakyReLU(), ..."}, {"section_title": "Raster input", "text": "States Observations Figure S2 .6: A convolutional neural network that maps a raster (in this case a 4 \u00d7 128 \u00d7 128 grid) to a state transition probability matrix of a hidden Markov model. Yellow boxes indicate input arrays and outputs from convolutional layers, with labeled dimensions. Red boxes represent two dimensional maximum pooling layers. Purple boxes represent fully connected hidden layers. The final vector of length 4 is reshaped to a 2 by 2 matrix, and then a softmax transform is applied row-wise to ensure that the rows sum to one for the transition probability matrix at timestep t, denoted \u0393 (t) .\nMaxPool2d(kernel_size=3, stride=2), Conv2d(16, 32, kernel_size=5), LeakyReLU(), MaxPool2d(kernel_size=3, stride=2), Conv2d(32, 64, kernel_size=3), LeakyReLU(), MaxPool2d(kernel_size=3, stride=2) view(-1), # flatten into a vector Dropout(), Linear(64 * 2 * 2, 128), LeakyReLU(), Dropout(), Linear(128, 64), LeakyReLU(), Dropout(), Linear(64, 4), view(2, 2), softmax(dim=-1) )\nIn addition to using dropout to regularize the convolutional model, an L 2 penalty of 10 \u22125 was also applied. To further regularize the model, image augmentation (random horizontal and vertical flips) was also applied while training, though this might not be desirable in cases where directional orientation of the input image is important (Simonyan & Zisserman 2014) . Image augmentation generally includes strategies to perturb data while training computer vision models in an attempt to generate a robust model (i.e., one that is insensitive to translation, orientation, hue, contrast, etc.).\nIn contrast to more common applications of convolutional neural networks where individual images are labeled (e.g., with bounding boxes and species identities in camera trap imagery), the observed movement trajectories (step sizes and turning angles) can be thought of as analogous to implicit \"labels\" for this convolutional hidden Markov model."}, {"section_title": "Implementation notes", "text": "The best case and point extraction models were both fit using the momentuHMM R package (McClintock & Michelot 2018b) . The convolutional movement model was implemented with PyTorch (Paszke et al. 2017) . All code required to reproduce the analysis is available on GitHub at https://www.github. com/mbjoseph/neuralecology. Figure S3 .2: Extended computational diagram for the multi-species neural hierarchical dynamic occupancy model. Outer grey boxes indicate the different levels of the model that index quantities inside the boxes. Yellow nodes indicate occupancy parameters, and red nodes indicate detection parameters. Outputs include initial occupancy (\u03c8 1 ), persistence (\u03c6), colonization (\u03b3), and detection probabilities (p). The box labeled GRU is a gated recurrent unit that decodes temporal sequences of hidden layers from encoded route vectors."}, {"section_title": "Concatenate", "text": ""}]