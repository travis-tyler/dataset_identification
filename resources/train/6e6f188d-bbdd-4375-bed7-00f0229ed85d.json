[{"section_title": "", "text": "Much has been learned from this study, which turned out to be more challenging and to take longer than we originally expected. An enormous effort was contributed by universities to collect and recheck the data, demonstrating their desire to identify comparative strengths and weaknesses and to show accountability. The study committee had to refine and revise its methodology as it sought to provide tools for evaluating and comparing programs. Although the data are based on the 2005-6 academic year, they permit many useful comparisons of programs across many dimensions. All those interested in graduate education can learn much from studying the data, comparing programs, and drawing lessons for how programs can be improved. The data for many variables can be updated and made current on a regular basis by universities. In order to identify variables most valued by doctoral faculty as well as to avoid using exclusively reputational rankings as was done in earlier graduate doctorate assessments, the committee employed two alternative ranking methods. The first method asked faculty in each field to assign a weight to each of the quantitative variables in the institutional surveys, and the weighted variables could then be used to determine ratings and rankings of programs. The second method was to survey a subset of faculty to ask them to rank a sample of programs in their field, and then to use principal components and regression analyses to obtain the implied weights for the institutional variables that would most closely reproduce the results. The committee initially envisioned combining the results of these two methods into a unified set of rankings. The production of rankings from measures of quantitative data turned out to be more complicated and to have greater uncertainty than originally thought. The committee ultimately concluded that it should present the results of the two approaches separately as illustrations of how individuals can use the data to apply their own values to the quantitative measures to obtain rankings suitable for their own specific purposes. The illustrative rankings, which are provided with ranges to show some of the statistical uncertainties, should not be interpreted as definitive conclusions about the relative quality of doctoral programs. Doctoral programs viii are valued for a variety of reasons, and their characteristics are valued in different ways by stakeholders; there is no single universal criterion or set of criteria. The illustrative rankings and their ranges do provide important insights on how programs can be ranked according to different criteria and what variables are most important to faculty, which typically are variables that measure per capita scholarly output. Faculty generally do not assign great importance to program size when assigning weights directly --but when they rank programs, program size appears to implicitly carry large weight. It is our view that strengthening graduate education will require paying attention to all of the variables in the dataset, not just those most important to faculty. Three additional metrics were presented in the report for each program; these focused separately on research activity, student support and outcomes, and diversity of the academic environment. A major value of the study is that this data set allows all stakeholders to assign weights which they believe to be important and then compare the programs on that basis. If a process of continuous improvement is to result from this exercise, all of the stakeholders interested in graduate education will need to focus upon steps to improve performance across the board. A major commitment by universities will be needed to update the data set on a regular basis, so that programs can continue to be compared and evaluated. If this is done with the updated dataset as an important new tool, and we strive to improve what is already the world's strongest system of higher education, we believe that American doctoral education can continue to bring enormous benefits to our citizens and remain the envy of the world. based on values of users that go into constructing rankings. The ranges of rankings that are shown convey some, but not all, of the uncertainties that can be estimated in producing rankings based on assigning importance weights to quantitative measures. The reader who seeks a single, authoritative declaration of the \"best programs\" in given fields will not find it in this report. The reason for this outcome is that no single such ranking can be produced in an unambiguous and rigorous way. To create illustrative rankings, the committee explored several approaches to evaluate and rate programs, with the subsequent rankings reflecting an ordered list of ratings from high to low. Program ratings depend on two things, namely the characteristics of the program (e.g., number of faculty, number of publications, citations, and other quantifiable measures) and the weighting, or value, that faculty assigned to each characteristic. The committee determined the weights to apply to important characteristics by two different methods based on faculty inputs. One method involved asking direct questions about what characteristics are important and how they should be weighed, while the second used an implicit method to determine the weights based on evaluations of programs by faculty raters. The results of these two approaches are different, and are presented separately in the report. The committee also developed three other rankings based on separate dimensions of the doctoral programs. All five approaches, which are explained in more detail in the following paragraphs, have strengths and deficiencies. The committee is not endorsing any one approach or any one measure or combination of measures as best. Rather, the user is asked to consider the reason a ranking is needed and what measures would be important to that ranking. The different measures should then be examined by the user and given appropriate weights, and the user should choose an approach that weights most heavily what is important for that user's purpose. As the committee has stressed repeatedly, the user may take the data that the study provides and construct a set of rankings based on the values that the specific user places on the measures. The faculty survey on the relative importance of various measures yielded weights that are used to develop one illustrative ranking, the S-ranking (for survey-based), for which we present ranges of rankings for each program. On a separate questionnaire, smaller groups of randomly selected faculty in each field were asked to rate programs from a sample of doctoral programs. The results of the regression of these ratings on the measures of program characteristics are used to develop another range of illustrative rankings, the R-rankings (for regression based). The ranges and weights for these two ways of calculating rankings-one direct (S-ranking) and one indirect (R ranking)-are reported separately and provided in an online spreadsheet (http://www.nap.edu/rdp) that includes a guide for the user. The ranking methodology utilized by the committee in these illustrative approaches has been chosen to be based on faculty values. This decision was made because perceived quality of the graduate doctoral program in a field is typically based on the knowledge and views of scholars in that field. Dimensional measures in three areas-research activity, student support and outcomes, and diversity of the academic environment-are also provided to give additional illustrative ranges of rankings of separate aspects of doctoral programs."}, {"section_title": "SUMMARY", "text": "\nThis chapter has provided a glimpse into the large amount of data about doctoral programs available in the study's online database. By matching programs, the committee was able to compare the 2006 and the 1993 data and see that the number of enrollments and the number of Ph.D.'s produced by the common doctoral programs have grown in most fields, with the exception of the humanities. Using NSF data, it also saw that the gender and racial and ethnic diversity of these programs has increased as well since the last study. The committee used the most current data to conduct an illustrative analysis in which it looked at program characteristics and size of program as measured by Ph.D. production. Although some smaller programs certainly have high research activity, generally the larger programs are associated with higher values for characteristics related to research. This association does not carry over to student support and outcome variables. The most consistent"}, {"section_title": "PREPUBLICATION COPY-UNEDITED PROOFS", "text": "\n\nMore broadly, since the 1995 study doctoral education has benefited greatly from dramatic increases in enrollments of international students and of domestic minorities and women. These gains demonstrate the ongoing desirability of American doctoral education in an educational world increasingly shaped by intense global competition for exceptional students. They also demonstrate the capacity to bring into doctoral education vital components of the national citizenry historically underrepresented in Ph.D. programs. The demographic group that has not shown gains during this period is the group that was long dominant in doctoral education-nonminority American males. Indeed, the domestic nonminority male population in doctoral education has decreased in both numbers and as a percentage of total doctoral enrollments. According to Science and Engineering Indicators, from 1996 to 2004 the percentage of doctoral degrees awarded to white, non-Hispanic U.S. citizens or permanent resident males decreased from 34.6 percent to 25.2 percent (Table 2-1). In absolute numbers, in the broad fields in this study the total number of doctoral degrees awarded to white, non-Hispanic U.S. citizens or permanent resident males decreased from 9,619 in 1993 to 8,392 in 2006. The largest decrease was in the social sciences and psychology: from 2,501 to 2,048. \nThe inclusion of immunology and infectious disease as a field exemplifies this change, as does the modification of pharmacology to include toxicology and environmental health. The growing importance of computation to biology is evident in the subfields of genetics and genomics and neuroscience and neurobiology, as well as in the presence of bioinformatics as an emerging field. All the biological science fields represented in the 1995 study are retained in this study with several noteworthy changes. Biochemistry now appears as biochemistry, biophysics, and structural biology rather than biochemistry and molecular biology. Biophysics and structural biology are new to this study, and the committee relegated molecular biology to subfield status after discussion of whether molecular biology has become more of a technique integrated into many areas rather than a separate field. Commonalities in research methodology along with research problems that increasingly merge traditional disciplines have resulted in greater integration across the life sciences. In the biomedical sciences in particular, newly developed programs offer students a common port of entry to a wide range of disciplines or, alternatively, degrees are offered in integrated programs without further differentiation. Programs draw on faculty from across the campus, making the assignment of faculty to programs more complex. These changes to traditional disciplinary structures have blurred the boundaries of research fields and departments and challenged the committee to define what was being rated. The inclusion of biology/integrated biomedical science accommodated these programs. The fields covered in the two studies are shown in Table 3  As early as 1996 a planning meeting was held to consider a separate study of the agricultural sciences, because they were not included in the 1995 study. That study did not go forward, however, because of funding considerations, and the decision was made to wait until a more comprehensive study was conducted to include these fields. Thus this study includes six agricultural fields in the agricultural sciences category and one agricultural field (agricultural and resource economics) in the social and behavioral sciences category. Most of these programs are located in colleges or schools of agriculture in the land grant universities or other public universities. Some of these fields include groupings of programs that may be separate entities as some institutions. For example, the plant sciences include programs that may be named agronomy, horticulture, plant pathology, or crop sciences at different institutions; the animal sciences include PREPUBLICATION COPY-UNEDITED PROOFS programs that might be named dairy science, animal science, or poultry sciences at different institutions. Many excellent research doctorate programs in the basic biomedical sciences are located in colleges or schools of medicine. The biological and health sciences taxonomy recognizes this fact and provides for the inclusion of such programs among the basic biological research doctorate programs. It also recognizes the maturation of several interdisciplinary programs, such as neuroscience, into established independent fields. The treatment of psychology as a field has changed from its treatment in the 1995 study, which included a number of programs in clinical psychology. During the late 1990s, some universities with established programs in clinical psychology awarded a Psy.D. degree, as opposed to a Ph.D. In its data collection, the committee asked universities to exclude their clinical programs even if they awarded a Ph.D. and their faculty from the study, but this request was not heeded in all cases. 3\nThe committee asked programs which definition they used and, under that definition, how many students they enrolled. The proliferation of multidisciplinary or cross-disciplinary programs presents a problem in how to \"allocate\" faculty. To ensure that the total number of faculty members across programs equaled the total number of faculty in the study, the committee had to allocate faculty to programs. A self-allocation procedure in which faculty assigned themselves or were assigned by their institutions was deemed unacceptable by most of the committee, because that procedure allowed allocations that did not accurately reflect the strength of programs. The formula eventually developed related allocation to the number of dissertations chaired by individual faculty members. 4 The resulting allocations were, however, reviewed by the institutions, which in a small number of cases revised the allocations if they felt they were unreasonable or not representative of a faculty member's scholarly efforts. Finally, the committee had to decide which kinds of data to collect. Two factors were important. First, the data had to be useful to the readers of the report, especially to 4 Faculty productivity (citations and publications) was allocated by the following formulas. For faculty members who are core in one or more programs that fall within the NRC taxonomy (regardless of the number of programs with which they may be associated), where A i is the share of publications and citations allocated to the faculty member in program i; P i is the number of committees in program i for which the faculty member serves as chair or principal adviser; n i is the number of committees in program i on which the faculty member serves in a capacity other than chair or principal adviser; d i is a variable that takes on the value of 1 if the faculty member is a core faculty member in program i and 0 otherwise; and m is the total number of programs in which the faculty member is a core faculty member. For faculty members who are core in a program in a nonincluded field but are listed as associate faculty in an included one, where P i and n i are defined as above. The factor of 2 in the denominator was included to reduce the overallocation of associate faculty members when information is not available on their core programs. The +5 that was there previously would have become proportionally smaller as these faculty sit on more and more committees outside their core program, making the allocation closer to 100 percent. To remedy this situation, the committee multiplied the denominator by 2 to effectively reduce the allocation to a reasonable fraction. With this modification, the allocation for associate faculty members (who are core in a nonincluded field or program) will never be greater than 50 percent. For new faculty members, all their publications and citations were allocated to their core program(s), because they will not yet have a record of dissertation committee service. For new faculty who are listed in more than one program (such as a joint appointment), their allocations were split evenly among their programs. These allocations were calculated directly by MPR from the faculty lists. PREPUBLICATION COPY-UNEDITED PROOFS potential students. Second, the data had to be consistent and available in an accessible form. Some data, such as publications in the scholarly literature and citation indices, can be obtained from commercial databases, and information about federal grants is available as well. By contrast, institutional data such as time to degree, levels of student support, and infrastructure investment are not uniform and not always available or as easily compared. The characteristics for which data appear for each program in the online data tables, and how they are measured, are shown in Table 3-3. Data from a review of 1,393 awards and honors from various scholarly organizations were used for this variable. The awards were identified by the committee as \"Highly Prestigious\" or \"Prestigious,\" with the former given a weight five times that of the latter. The award recipients were matched to the faculty in all programs and the total awards for a faculty member in a program was the sum of the weighted awards times the faculty member's allocation to that program. These awards were added across the faculty in a program and divided by the total allocation of the faculty in the program.\nNote: Number shown is average rank of the characteristic taken across the disciplines in the broad field. The five categories given the highest rankings are shown for each field. \"---\" indicates the characteristic was not one of the top five for the field. \"n.a.\" indicates not collected; GRE-Q = GRE-Quantitative Reasoning; GRE-V = GRE-Verbal. Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. Faculty values are also reflected in the relative importance of each category measured on the faculty questionnaire. For all fields the importance score for the faculty productivity variables was highest, followed by the student treatment category, with program demographic characteristics coming in last. These category importance values are shown in Table 5-2. One interesting observation is that although these weights are different from one another in a statistical sense, they are remarkably similar regardless of the field of the respondents. \n\nIncreasing gender and racial and ethnic diversity has been a goal of the graduate community for many years. Although substantial progress has been made, that goal is far from achieved. The percentage of underrepresented minorities by broad field for students and faculty is shown in Figure 7-2. With the exception of the humanities, in no field is more than 10 percent of the faculty from underrepresented minorities, and the sciences are at or less than 5 percent. Because larger percentages of doctoral students are from underrepresented minorities, it is likely there will be larger pools of Ph.D.'s from which to draw from in the future. However, the underrepresented minority enrollments in the agricultural and physical and mathematical sciences and engineering are still less than 10 percent. The percentage of minority students and faculty in all broad fields is less than 15 percent. Nevertheless, in some individual fields, listed in Table 7-11, more than 10 percent of enrollments are from underrepresented minority groups.\nCareer Goals Yet another measure is what students want to do when they graduate (Table 7-18). 2.8 Note: Omitted choices are \"professional services\" and \"other,\" and so the percentage across a row for a particular point in time does not add to 100 percent Overall, only 38.2 percent of programs showed an increase in student interest in research and development. Eighteen percent of programs saw an increase in students wanting to go into teaching, and 47.1 percent of programs saw an increase in student interest in management and administration. These findings suggest that as students learn what is actually involved in research and teaching, they become more interested in other, untried undertakings. To summarize, the student questionnaire reveals that students are generally pleased with their doctoral programs and that the programs are successful at improving student research productivity, but that by the time students are working at an advanced level at least some of them have shifted their career objectives away from research. This effect is not large, but it may explain in part the lower completion rates observed by the committee. Although it is likely that the decline in interest in research careers is the result of students learning more about what such a career entails, programs may wish to look at their individual results to determine the steps that might be taken to address this falloff in student interest in research.\nProgram size was very important for the R, or regression-based, approach, and various measures of research activity were very important for the S, or survey, approach. If there is an overall lesson to be learned, it is that people who use rankings should be cautious before relying on them. The production of rankings from quantitative measures of program characteristics turned out to be more complicated and to be associated with greater uncertainty than originally thought. Any set of evaluations rests on the core values given to program characteristics. In many other efforts of this type, the investigators have not been explicit about the basis for the values adopted. Users of this and other studies need to understand what goes into them-assumptions, weights, surveys, and uncertainty. In this study, if users relied on ranges of rankings alone, they would find a few programs at the top and the bottom with a narrow range of rankings. Most programs have a wide range of rankings and fall somewhere in the middle. This finding struck the committee as corresponding well to the way the world really is. Users need to go beyond rankings and examine the characteristics that are important for their purposes and concerns.  (1989)(1990)(1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001); professor of sociology (1976-present; vice president of arts and sciences, 1987-1989); and director, Center for the Social Sciences (1979Sciences ( -1987. His awards and memberships include the following: fellow, Center for Advanced Study in the Behavioral Sciences, Stanford, California, 1975-1976John Simon Guggenheim Foundation Fellowship recipient, 1975-1976 (2003)(2004)(2005)(2006). In 1988 Professor Nerem was elected to the National Academy of Engineering (NAE), and he served on the NAE Council (1998)(1999)(2000)(2001)(2002)(2003)(2004). In 1992 he was elected to the Institute of Medicine of the  1903-1932and Gertrude Stein: Writings 1932-1946  Every 10 or so years, the National Research Council conducts a study of national importance regarding the quality and characteristics of doctoral programs in the United States. This comparative assessment is designed to assist prospective doctoral students with selecting programs that best fit their interests and to permit programs to benchmark themselves against similar programs. The 2006 Assessment of Research Doctorate Programs collects data about the doctoral programs in over 60 areas of study in American universities. This Institutional Questionnaire is designed to collect data about institution-wide policies and practices. [Note: The web version of the questionnaire will allow the respondent to add as many zip codes as needed.] E. Academic Year E1. How is an academic year defined at this institution?\nYour program was selected because it satisfies at least three of the following four criteria for a doctoral program: 1. Enrolls doctoral students 2. Has a designated faculty 3. Develops a curriculum for doctoral study 4. Makes recommendations for the award of degrees. In addition, the program must have awarded 5 Ph.D.s during the period 2001/2 to 2005/6. If more than one doctoral degree granting program in this field exists at your institution: data and faculty lists for those programs will be provided to the NRC separately. The following other program(s) at your institution will also be part of the study in the field of_(Name of field in the NRC taxonomy): ____(Name of program that was identified by the institution)_____________ ____(Name of program that was identified by the institution)_____________ etc. We are interested in the number of core, new, and associated faculty in your program. Core Faculty are faculty members who: 1) have served as a chair or member of a program dissertation committee in the past 5 academic years (2001-2002 through 2005-2006), OR 2) are serving as a member of the graduate admissions or curriculum committee The faculty member must be currently (2006)(2007) and formally designated as faculty in the program, and not be an outside reader who reads the dissertation but does not contribute substantially to its development. Include emeritus faculty only if the faculty member has, within the past three years, either chaired a dissertation committee or been the primary instructor for a regular Ph. D. course.\n\nProgram Quality: Questions 19-24 provide respondents with an opportunity to provide their perceptions of program quality (curriculum, research experience, faculty teaching ability, dissertation supervision, and intellectual environment)."}, {"section_title": "SOME FINDINGS Changes between 1993 and 2006", "text": "Because the biological science fields have been extensively reorganized since 1993, when the last NRC assessment was carried out, it is difficult to make comparisons in these areas over time. Other programs that were not included in 1993 are included in this assessment, including many programs in the field of agricultural sciences. For fields in engineering, physical sciences, humanities, and social sciences, where comparisons between the previous study and this one are possible, we find that: \u2022 Since the last NRC study was published in 1995 (based on data collected in 1993), the numbers of students enrolled in the programs that participated in both studies have increased in some broad fields (in engineering by 4 percentage points, and in the physical sciences by 9 percentage points) and declined in others (down 5 percentage points in the social sciences and down 12 percentage points in the humanities). 5"}, {"section_title": "\u2022", "text": "The numbers of Ph.D.'s produced per program across these common programs has grown by 11 percent.\nAll the common programs have experienced a growth in the percentage of female students with the smallest growth (3.4 percentage points) in the humanities fields, which were already heavily female, and the greatest growth in the engineering fields (9 percentage points, increasing to 22 percent overall).\nFor all doctoral programs in fields covered by the study, there has been an increase in the percentage of Ph.D.'s from underrepresented minority groups 6 (a growth of 2.3 percentage points to 9.6 percent in the agricultural sciences, 3.7 percentage points to 9.8 percent in the biological sciences, 1.7 percentage points to 6.4 percent in the physical sciences, 5.2 percentage points to 10.1 percent in engineering, 5.0 percentage points to 14.4 percent in the social sciences and 3.5 percentage points to 10.9 percent in the humanities). 7\nBecause of differences between the definition of faculty in 1993 and 2006, we cannot strictly compare faculty sizes, but it appears that the number of faculty involved in doctoral education has also grown in most programs. 6 DATA-BASED ASSESSMENT OF RESEARCH-DOCTORATE PROGRAMS IN THE U.S."}, {"section_title": "Program Characteristics", "text": ""}, {"section_title": "Institutions", "text": "We found that doctoral education in the United States is dominated by programs in public universities in terms of numbers of doctorates produced. Seventy-two percent of the doctoral programs in the study are in public universities. Of the 37 universities that produced the most Ph.D.s from [2002][2003][2004][2005][2006] (making up 50 percent of the total Ph.D.s granted during this time), only 12 were private universities. The health of research and doctoral education in the United States depends strongly on the health of public education."}, {"section_title": "Size", "text": "As was found in the 1982 and 1995 reports, program size continues to be positively related to program ranking. This result holds despite our reliance in the current study on per capita measures of scholarly productivity. In most broad fields, the programs with the largest number of Ph.D.s publish more per faculty member, have more citations per publication, and receive more awards per faculty member than the average program."}, {"section_title": "Students", "text": "There is very little difference among fields in the percent of students who receive full support in their first year. For all fields, this percentage is somewhere between 79 percent (engineering) and 90 percent (physical sciences). The larger programs have significantly longer median times to degree in all fields except the biological sciences, and this is particularly true in the humanities (7.4 years as compared to 6.1 years for the broad field as a whole). There is no significant difference based on size in the percentage of students who have definite plans for an academic position upon graduation. There are, however, differences by field, ranging from a high of 46 percent for the humanities, to a low of 15 percent for engineering. In terms of completion, over 50 percent of students complete in six years or less in the agricultural sciences and in engineering, but a smaller percentage does so in the other broad fields. In the social sciences the percentage is 37 percent, which is the same percentage completion for the humanities after eight years. In the physical sciences, the six-year completion percentage is 44 percent.\nOnce students know the discipline in which they want to pursue doctoral study, they usually take the next step of discussing with their adviser or a professor in that field what that doctoral study would involve. Here are some examples of the questions that might be asked: \u2022 Do I know what I want to specialize in? \u2022 Do I want a program in a particular region-for example, near my home? \u2022 Do I want a large program or a small program? \u2022 Do I want a program in which a high proportion of the faculty has grants? \u2022 Are my GRE scores competitive with those of other students in programs that interest me? \u2022 Do I want a program in which a high proportion of students complete their degrees in a reasonable period of time? \u2022 Do I want a program in which I am likely to find other students and faculty like myself (e.g., who are female or from underrepresented minority groups)? \u2022 Do I want a program that funds most of its students in their first year? \u2022 Do I want a program that is interdisciplinary? \u2022 Do I want a program whose faculty are highly cited? Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. After choosing among the doctoral programs suggested by their adviser, students can then create a spreadsheet of those programs from the online data available from this study, which will allow them compare the programs on the various measures of interest. For example, Table 6-1 shows the rankings and data for 5 of the 34 chemistry programs in universities in the mid-Atlantic area."}, {"section_title": "Diversity", "text": "The faculty of doctoral programs is not diverse with respect to underrepresented minorities -5 percent or less in all broad fields except the social sciences (7 percent) and the humanities (11 percent). Student diversity is greater-10 percent or above in programs in all broad fields except the physical sciences (8 percent). The faculty is more diverse in terms of gender, with women making up over 30 percent of the doctoral faculty in the biological sciences (32 percent), social sciences (32 percent), and humanities (39 PREPUBLICATION COPY-UNEDITED PROOFS Students were also asked about their career objectives recalled from when they entered the program and when they answered the questionnaire. There was a decline in the percentage who said they had \"research and development\" as a career objective in all fields and a decline in those interested in teaching in all fields but neuroscience. The percent of students who had management and administration as a career objective grew, but was still below 10 percent in all fields. Research and development was still the predominant career goal, except in English, where teaching (52 percent) dominated. In summary, doctoral education in the United States is a vast undertaking comprising many programs in many fields with, overall, very high standards and intellectual reputation. For a long time, North American institutions of higher education have been the world's standard for the research doctorate. As universities across the globe compete with increasing intensity for the faculty and students who will advance the knowledge economy of the future, it is important that we take stock of the enormous value represented by the United States research doctorate programs. Taken together, these programs will produce the future thinkers and researchers for all kinds of employment as well as the faculty who nurture the next generation of scholars and the researchers. All are essential to scientific discovery, technological innovation, and cultural understanding in the United States and across the globe. This study cannot, of course, provide a comprehensive understanding of these research doctorate programs. The data collected for this study represent an unprecedented wealth of information, and the committee hopes that they will be updated and used for further analysis. 10 These data have been used to produce illustrative ranges of rankings of research doctorate programs aimed at reflecting the values of the faculty who teach in these programs. The intent is to illustrate how individuals can use the data and apply their own values to the quantitative measures to obtain rankings suitable for their specific purposes. But the data themselves, even more so than the weighted summary measures and the illustrative ranges of rankings, can lead to analyses that throw revealing light on the state of doctoral education in the United States, can help university faculty and administrators to improve their programs, and can help students to find the most appropriate graduate programs to meet their needs.\nAverage measures of various kinds of diversity are shown in Table 7-10. It also shows the results of tests to determine whether the largest quartile is different from the average. 17.0 a The largest quartile value is different from the average at the p = 0.05 level."}, {"section_title": "Introduction", "text": "This assessment of research doctorate programs conducted by the National Research Council (NRC) presents data that provide, for the first time in one place, basic information about many aspects of doctoral education in the United States. The data were compiled on a uniform basis 1 and collected from the universities themselves. Data from the assessment will allow comparisons of similar doctoral programs, with the goal of informing efforts to improve current practices in doctoral education, and will help matriculating students pick the graduate programs best suited to their individual needs. 2 The assessment, which covers doctoral programs in 62 fields 3 at 221 institutions, 4 offers accessible data about program characteristics that will be of interest to policy makers, researchers, university administrators, and faculty, as well as to students who are considering doctoral study. Furthermore, in an illustrative manner, the assessment analyzes and combines these data to create two ranges of rankings based on overall measures of program quality that were derived from faculty perceptions of program quality approached in two different ways. The National Research Council has a tradition of conducting careful assessments of doctoral education in the United States. The first NRC assessment, published in 1982, 5 was a rich source of data for educational planners and policy makers, as well as a source of reputational ratings of perceived program quality obtained from raters who were Distinct from the earlier studies, the primary purpose of the current study, as outlined in the study's statement of task, was the following: \"1) the collection of quantitative data through questionnaires administered to institutions, programs, faculty, and 'admitted to candidacy' students (in selected fields); 2) collection of program data on publications, citations, and dissertation keywords;[ 8 ] and 3) the design and construction of program ratings using the collected data including quantitatively based estimates of program quality.\""}, {"section_title": "WHO WILL FIND THESE DATA USEFUL?", "text": "These data will be useful to administrators, faculty, students considering doctoral study, and to those concerned with governance and policy related to doctoral education, as well as to the employers of Ph.D.'s outside of academia. In addition to comparisons of specific characteristics of interest, users will be able to understand the calculation of ranges of rankings of doctoral programs in each field through a spreadsheet downloadable from the National Academies Press Web site, http://www.nap.edu/rdp. Details of the illustrative rankings can be obtained by clicking on links provided in this spreadsheet. This study uses a methodology that permits users interested in rankings to understand the sources of those rankings. It also enables programs and individuals to benchmark themselves against peer or nearby programs using criteria that seem to them most appropriate. Examples are discussed in Chapter 5."}, {"section_title": "INTRODUCTION PREPUBLICATION COPY-UNEDITED PROOFS", "text": "\nFinally and most importantly, this study is a tool that can be useful to administrators, faculty, students, and others with an interest in the characteristics of doctoral programs. Users can pick programs of interest and measures of interest and make customized comparisons. For students, these comparisons may be along the lines of funding and completion rates, or characteristics of programs near their homes. Administrators may find comparisons with peer programs nationwide or regionally. With that in mind, six months after the release of this report and the accompanying data, the National Research Council will hold a workshop at which researchers and others who have used the data will report on the uses they have made of them. The proceedings of this workshop will be published as a workshop report and will expand on the descriptive summary discussion provided in this report. Whatever their interest, all users will find that they have access to information about doctoral programs that was not available in the past. The faculty member must be currently (2006)(2007) and formally designated as faculty in the program, and not be an outside reader who reads the dissertation but does not contribute substantially to its development. Include emeritus faculty only if the faculty member has, within the past 3 years, either chaired a dissertation committee or been the primary instructor for a regular Ph.D. course. New Faculty. Faculty who are not core and (1) do not meet the criteria for core faculty, but who have been hired in tenured or tenure-track positions within the past 3 academic years (2003-2004 through 2005-2006) and (2) are currently employed at your university and are expected to become involved in doctoral education in your program. Associated Faculty. Faculty who are neither core nor new, but (1) have chaired or served on program dissertation committees in the past 5 years (2001-2002 through 2005-2006), and (2) have a current (2006)(2007) appointment at your institution, but who are not designated faculty in the program. They should not be outside readers, or faculty currently employed at other universities, unless they are on leave from the faculty at your institution. Include emeritus faculty only if the faculty member has, within the past 3 years, either chaired a dissertation committee or been the primary instructor for a regular Ph.D. course. PREPUBLICATION COPY-UNEDITED PROOFS 17 2"}, {"section_title": "11", "text": ""}, {"section_title": "DESIGN OF THE STUDY", "text": "This study was designed with two objectives in mind: first, to collect comparable data across doctoral programs that would permit benchmarking for faculty and administrators, and second, to relate these data to measures of overall program quality and measures of particular aspects of doctoral programs. How the study was designed to achieve these objectives is described briefly in Chapter 2 and in considerable technical detail in A Revised Guide to the Methodology of the National Research Council Assessment of Doctorate Program. 9 Briefly, to characterize doctoral programs, data were collected from universities, their programs, and faculty in 62 fields, as well as from students in five fields. 10 The data reported by the programs reflected the size, scope, and practices of each program, as well as financial aid and training practices that affect students. In addition, data were collected about time to degree and completion rates and whether the program tracked its students after completion. Because interdisciplinarity is an issue of increasing importance for doctoral programs, the program questionnaire gathered data to address this issue by counting faculty from outside the program who were engaged in supervising dissertations and by asking directly whether the program was considered to be interdisciplinary. The faculty questionnaire collected data on funding, work history, and publications, as well as on demographic characteristics. One section of the questionnaire asked the respondent to rate the relative importance of program, faculty, and demographic characteristics to program quality. It also asked whether the faculty member would be willing to respond to a questionnaire asking for ratings of programs. The student questionnaire, administered to advanced 11 students in physics, chemical engineering, neuroscience, economics, and English, asked about student educational background and demographic characteristics, as well as research experiences in the program, scholarly productivity, career objectives, and satisfaction with a variety of aspects of the program. The size of the sample for each questionnaire and response rates are shown in Table 1-2. 9 National Research Council, A Revised Guide to the Methodology of the National Research Council Assessment of Doctoral Programs in the United States (Washington, D.C.: National Academies Press, 2010 ). This report is downloadable at no cost at www.nap.edu/catalog.php?record_id=12676 . The methodology described in this guide has since been changed so that two separate overall rankings are calculated rather than a ranking that combines the R and S approaches described in Chapter 2. 10 The five fields were chemical engineering, physics, neuroscience, economics, and English. These fields were chosen because they are large, represented all but one of the broad fields, and were viewed by the committee as appropriate for a pilot study to understand whether such a questionnaire could provide useful information. 11 \"Advanced\" means students who have been admitted to candidacy. The importance of the measured variables to perceived quality was ascertained in two ways: (1) from the relative importance of weights calculated from answers to the faculty questionnaire, and (2) from taking a sample of programs and faculty in each field and statistically deriving weights for each variable from the faculty's response to a rating questionnaire. 12 Both of these approaches reflect faculty values, which are discussed in Chapter 5. The method of obtaining rankings through two separate ways of calculating ranges of overall rankings is discussed briefly in Chapter 4 and in far more detail in the methodology guide. Chapter 3 also compares the current methodology to that of the 1995 study and explains some sources of noncomparability. Chapter 5 discusses the ways in which the study ascertains faculty values, which are key to understanding the rankings in the study. In Chapter 6 users learn how different groups may wish to approach and use these data. And Chapter 7 discusses some general patterns of the data and presents the principal characteristics of the programs in the study. It contrasts the methodology and results from the 1995 study with the current study and then presents a description of important findings about doctoral education in 2006-2007. 13 It also presents selected findings from the faculty and student questionnaires. The concluding Chapter 8 provides the committee's views of how the data from the study might be the subject of future work."}, {"section_title": "WHAT THIS STUDY HAS REVEALED", "text": "Doctoral education in the United States is a far-flung and varied enterprise. Every field has its highly ranked and renowned programs, which are typically characterized by a large size and the high research productivity of faculty. To be sure, there are also many smaller programs with high rates of completion and times to degree similar to highly ranked programs. However, doctoral education is in fact concentrated in relatively few 12 Each faculty member was asked to rate 15 programs, and these faculty ratings were then related to the variables for each sampled program. Data on numbers of program, raters, and raters per program for each field are shown in Appendix H. 13 Much more data are available than will be reported in the spreadsheet for this report. This report focuses on 20 program characteristics, but many more questions were asked. The committee plans to make the full database for all questionnaires except the rating questionnaire available to interested parties, unless particular items would violate individual confidentiality restrictions. Items whose answers would violate individual confidentiality restrictions will be masked for this dataset. Researchers who wish to use the full dataset with unmasked values must apply to the NRC and agree to comply with confidentiality restrictions in their published data."}, {"section_title": "INTRODUCTION", "text": "PREPUBLICATION COPY-UNEDITED PROOFS institutions whose programs have many students and faculty. Of the 221 institutions and combinations of institutions that participated in the study, half of the Ph.D.'s were granted by 37 universities, or 17 percent of the total participating in the study. Because most of these programs are in public institutions, the health of these institutions and the nation's ability to produce highly trained researchers and the next generation of professors are inextricably linked. As an illustration of the kinds of data-based rankings that can be produced, the committee explains and reports rankings based on two measures. One measure, the S ranking, is based on a survey of the importance to faculty in a given field of the general characteristics of doctoral programs. The other, the R ranking, is based on values reflected in ratings given to a sample of programs by a sample of faculty in a field. These latter measures are then related, through a regression, to the same measures used in the S ranking for the sampled programs, and the coefficients 14 from that regression are used as weights to calculate these rankings for all programs in the field. 15 The uncertainty in all rankings is measured in part by calculating the ranking 500 times, with a different half sample of raters taken each time, so that all rankings are presented as ranges of rankings. In addition to these overall rankings, the study provides ranges of program rankings, based on the weights obtained for subsets of the S measure in each field. These rankings address three specific dimensions of doctoral education: (1) research activity, (2) student support and outcomes, and (3) diversity of the educational environment. For all measures, attention is given to the presentation of statistical uncertainties in the reported results. The ranking methodology is based on faculty values, expressed either explicitly through the questionnaire results that are used to calculate S rankings or implicitly through the ratings of a sample of programs that are used to calculate the R rankings. The measures viewed as most important to the quality of a doctoral program are related primarily to faculty research productivity. According to faculty, publications, citations, grants, and awards matter more than other metrics. In some cases the ranges of R rankings and S rankings do not overlap. One interesting and important difference between the weights that result in the R and S rankings is that the one measure of program size-the average number of Ph.D.'s granted over the previous five years-often receives the largest weight in the R rankings but relatively small in the S rankings. Faculty appear to not assign as much importance to program size when assigning weights directly as when assigning them indirectly based on their rating of programs. Program size, while not likely to be a direct cause of higher program quality, may serve as a surrogate for other program features that do exert positive influences on quality. Another possible cause of these differences between the R and S measures is heterogeneity in the modes of scholarship in the field so that the statistical model does not fit very well. 16 A table showing the correlation of the medians of the two measures 14 The coefficient expresses the relation between the rating and a particular characteristic when all the other characteristics are taken into account-that is, through a multivariate regression. The committee interprets them as weights that express the contribution of the particular characteristic to the variation in the rating. 15 The sample was designed to reflect the national population of faculty in each field with respect to faculty rank, program size, and geographic distribution. 16 Heterogeneity would create problems if two subfields in the same discipline had different modes of scholarship, so that the relationship between number of publications per faculty member and rating was for programs in each discipline appears in Appendix G. Meanwhile, measures other than the range of R rankings and S rankings may be important to others engaged in doctoral education, such as granting agencies and the students themselves, and as such should not be ignored. The committee approached comparisons in three distinct areas through the dimensional measures. These measures summarize the program characteristics of research activity, student treatment and outcomes, and diversity of the academic environment. Student treatment and outcomes is related to research activity, because programs with a high level of research activity have the resources to treat students better. Programs with a high level of research activity have more faculty with research funding, and they typically exist in research universities with higher levels of available support. Many such programs have high rates of student funding in the first year and relatively high completion rates. They often do not, however, have shorter median times to degree. Based on data from the National Science Foundation Survey of Doctorate Recipients, the committee found that less than 50 percent of Ph.D.'s in each broad field has definite plans to seek an academic position or postdoctoral study in academia. Thus the findings of this study are important to employers of Ph.D.'s in the nonacademic sectors, as well as to academia. Furthermore, many Ph.D.'s are now employers in research-intensive businesses, and the characteristics of the programs from which they hire Ph.D.'s may be useful to them. Diversity among the faculty has improved impressively since the 1995 NRC study. Gender diversity has increased substantially in all fields, and the percentage of new Ph.D.'s who are female has risen from 38 percent to 45 percent overall, although the percentages are still low in the physical and mathematical sciences (30 percent) and engineering (20 percent). The racial diversity of Ph.D.'s has also grown markedly, at an average annual rate of 4.6 percent, whereas the number of nonminority Ph.D.'s has declined by 1.7 percent. Underrepresented minorities were 7.4 percent of Ph.D.'s overall in 1993 and were 13.5 percent in 2006, but their proportion remains low, especially in the more highly ranked programs in science and engineering. 17 Overall, the number of Ph.D.'s granted annually to white males declined from 12,867 in 1993 to 7,297 in 2006. 18 The ratio of faculty to students has changed since the 1995 NRC study. The ratio of faculty to Ph.D.'s graduated increased in most broad fields from 1993 to 2006, the years in which the data were collected. This finding may reflect a deeper faculty involvement in doctoral education, or it may be partially a result of definitional changes between the two studies. 19 different for each subfield. For example, if the rate of publication was much lower for programs in one subfield, highly rated programs dominated by this subfield would appear to be anomalous when combined with the subfield with a higher rate of publication. This problem could be solved by dividing the field and estimating the coefficients separately for the R ranking. 17 \"Underrepresented minorities\" refers to African Americans, Hispanics, and American Indians. 18 Source: National Science Foundation. 19 In the 1995 study, programs were asked for the \"names and ranks of all faculty members who participate significantly in education toward the research doctorate.\" In the 2006 questionnaire, programs were given a far more specific definition of faculty, who were divided into three categories: core, new, and associated. The definitions are as follows: Core Faculty. Faculty who (1) have served as a chair or member of a program dissertation committee in the past 5 academic years (2001-2002 through 2005-2006), or (2) are serving as a member of the graduate admissions or curriculum committee."}, {"section_title": "Context and Motivation", "text": "Doctoral education is at the heart of the U.S. system of innovation. It is the process that generates highly educated scholars and researchers, significant research results, and avenues for innovation, thereby creating the leaders needed to produce the research advances that will create new careers and economic vitality for the nation. 1 Doctoral education trains the professors of the future-it inculcates the habits of mind necessary for productive research and scholarship. Doctoral education is intimately involved in the creation of scholars whose ideas will shape both future innovations and how Americans use and understand innovation as it changes their lives. American graduate education draws students from across the United States and around the globe, particularly in the science, technology, engineering, and mathematics (STEM) fields, and has been the envy of the world since World War II. Now, however, the U.S. position is facing substantial challenges, from a growing emphasis on doctoral education in other countries to financial constraints stemming both from the economic downturn of 2008-2009 and from the continuation of declining trends in state support for higher education. 2 Several reports have highlighted the threats to U.S. leadership in innovation, including recently from the National Research Council, Rising Above the Gathering Storm: Energizing and Employing America for a Brighter Economic Future, which focuses especially on the need to improve U.S. graduate programs in STEM fields in order to improve U.S. economic competitiveness. The need for enhanced performance and accessibility is also highlighted in the recent report from the Council of Graduate Schools Graduate Education: The Backbone of American Competitiveness and Innovation. 3 These and other reports lay out clear frameworks for a focused commitment to improving graduate education. As additional resources are being considered for graduate programs, it becomes increasingly important to have structures in place to continually assess these investments. In addition to international competitive forces, strong drivers in the United States are underlying efforts to improve the quality and efficiency of graduate programs. For PREPUBLICATION COPY-UNEDITED PROOFS one thing, public universities are experiencing a sustained decline in state support that is forcing institutions to increase tuition and raise funds privately, thereby mirroring some of the features of private universities. For both public and private universities, doctoral education is expensive in the commitment of time and dollars both by those engaged in the enterprise and by its funders. Thus an assessment of program effectiveness to weigh the justification of that investment is always necessary. Finally, efforts to determine whether doctoral education is living up to its promise call for an evaluation of whether it has done so by expanding domestic sources of talent, improving time to degree, and raising rates of completion. These are just some of the challenges that this study has attempted to address in view of the fact that few previous studies have been able to investigate these challenges as thoroughly."}, {"section_title": "WHY ASSESS DOCTORAL PROGRAMS?", "text": "The assessment of doctoral programs dates back to 1925, when Raymond M. Hughes first conducted a survey to gauge faculty opinion of \"the esteem at the present time for graduate work in your subject.\" 4 His survey, which appeared in a report to the Association of American Colleges, was aimed at constructing rankings of doctoral programs. The results were greeted with both interest and criticism. Since then, however, reputational measures have been repeatedly used to assess the quality of doctoral programs. In the more than 80 years since the Hughes report, doctoral education has changed tremendously in size, number of fields, and the nature of employment destinations of Ph.D. recipients. The nature of assessing doctoral programs has changed as well, from reputational rankings provided by department chairs to studies that have increasingly included objective measures of aspects of doctoral programs. 5 Today, a similar kind of reappraisal, but with a different motivation, may be warranted. With the enormous importance of and investment in doctoral education comes the need for accountability, because many different sectors of the U.S. economy rely heavily on the quality of knowledge produced by the nation's Ph.D.'s. Colleges and universities across the United States and around the world rely on American doctoral programs to educate the next generation of faculty and professional researchers. Corporations depend on highly trained doctoral students ready to bring cutting-edge technology and science to their labs and offices. Federal agencies also invest considerable sums of money to support doctoral students as fellows, trainees, and research assistants, 4 Quoted in National Research Council, Research Doctorate Programs in the United States, 10. 5 For this discussion it is important to recognize the distinction between reputational measures and those called \"objective measures\" in this report. Reputations of program quality are derived, here and in the past NRC studies, from respondents' ranking of Ph.D. programs on a six-point scale from distinguished to poor, which includes one category that indicates that a respondent does not have enough knowledge of the program to rate it. These data are quantitative, objective, and measurable, just like the Likert-type scales that have been used in the social and behavioral sciences for decades. Objective measures, as used here, refer to measurements based on data derived from sources that yield faculty publication counts, citations of their work, and honorific awards, as well as measures of student treatment and outcomes and program diversity. These kinds of measured data may partially predict the reputational standing of a program. But reputations, as a composite subjective assessment translated into a score on a scale, may capture other elements of program quality that cannot be obtained by means of the objective measures used here."}, {"section_title": "PREPBULICATION COPY-UNEDITED PROOFS", "text": "as do private foundations. The provision of information for benchmarking and improvement is salient in all these sectors. For students considering a doctorate, the importance of accountability is no less striking. The decision to enroll in a doctoral program represents an enormous personal commitment. And the selection of a doctoral program is a life choice of great importance. How effective is a particular program in graduating its students in a timely way? What is the reputation of the program? What are its particular strengths and weaknesses? What kind of financial support will be available? What benefits are available for students with families? What kind of record does the program have in attracting, supporting, and graduating underrepresented students? Is the program successful in recruiting and supporting women in traditionally male-dominated fields? How do its graduates fare in the world? It is important that students considering doctoral education pose such questions and that there be places where they can find reliable answers to them. The availability of data that are comparable across similar programs can serve as a guide to areas that need action and thus the collection of such data was a goal of this study. Still in use today, the traditional measures for assessment of doctoral education have been time to degree and completion rates. 6 The shift toward including student opinion in perceived learning outcomes did occur until recently. 7 Additional measures used include race and gender diversity, test scores, financial support of students, percentage completing, relationship with mentor, and overall socialization. These measures were addressed in the student questionnaire. No one source, of course, can answer all questions about all doctoral programs for all prospective doctoral students, funding agencies, or university administrators. But one important purpose of this NRC study is to make a very large amount of information-arranged in as manageable a form as possible-available to those with a variety of interests: to students facing such choices and asking such questions, to agencies and government bodies and foundations that invest heavily in doctoral education, and to universities that must manage their own doctoral programs effectively. Even when the NRC study findings cannot answer all the important questions that the many constituencies of doctoral education will bring to it, the study will put them in a better position to know the questions that they then need to pose to the programs they are considering. As this committee understands, not only must an enterprise of this significance be operated effectively, but also constituencies crucial to the support of doctoral education must have access to the information that can help provide reliable assessments of its effectiveness. Likewise, using such information, policy makers must be sensitive to the changing characteristics, or evolution, of doctoral education, because such changes are PREPBULICATION COPY-UNEDITED PROOFS"}, {"section_title": "DIVERSITY OF FACULTY AND STUDENTS IN DOCTORAL PROGRAMS", "text": "An area of importance in assessing doctoral programs is the demographic characteristics of doctoral students. These characteristics include their international diversity, as well as their race, ethnicity, and gender. U.S. doctoral programs have attracted students from around the world for many years. These programs are also striving to become more diverse in race, ethnicity, and gender, and to some extent they are succeeding."}, {"section_title": "International Students", "text": "The number of international students pursuing doctoral programs in the United States has grown significantly since the 1990s. According to the Institute for International Education, the absolute numbers of enrollments of international doctoral students increased from 100,092 in 2003-2004 (academic year) to 108,976 in 2007-2008. 8 Graduate applications overall, however, moved in the opposite direction. International graduate applications for the 2003-2004 academic year dropped suddenly and sharply. Although this brief downward trend now appears to have slowed or stopped, the decline was sharp enough that graduate applications and new enrollments have not yet returned to pre-2003 levels. 9 One cause of this reversal in growth was the sensitivity of the international graduate application process to perturbations in visa policy and practices. Measures put in place after the terrorists attacks of September 11, 2001, not only made entry into the United States for study more difficult, but may also have had a chilling effect on the interest of international students in pursuing graduate study in the U.S. universities. Compounding the issue was an escalation in the level of competition worldwide for the best international doctoral students. For example, the European Union nations have recognized how important the knowledge and skills developed through doctoral education are to building a twenty-first century economy, and so those nations have given high priority to strengthening the doctoral education they offer. And China, which has provided large numbers of superb doctoral students for U.S. universities for the last halfcentury, has introduced ambitious programs to expand and strengthen doctoral education in its own universities. Furthermore, countries that provide doctoral study in English, such as Great Britain, Canada, and Australia, capitalized on the situation in the United States by moving rapidly to recruit more international students. Students from other countries enrolled in the entire range of doctoral programs were surveyed for this study. These students come from all over the world, but the number of students from India, China, and South Korea are particularly high. International enrollments are especially high in doctoral programs in engineering and the physical and mathematical sciences. Indeed, it is not unusual for major Ph.D. programs in engineering to award half or more of their doctorates to students from other countries. PREPUBLICATION COPY-UNEDITED PROOFS U.S. research universities have benefited greatly from the influx of doctoral students from other nations. While enrolled, these students characteristically show higher than average completion rates and shorter than average times to degree. 10 Their contribution to laboratory research in the STEM fields is enormous. In fact, the research productivity of U.S. universities is closely tied to their ability to recruit and retain talented students who come to this country to pursue doctoral study. Many successful international doctoral students stay in the United States. By becoming university faculty, by establishing start-up companies, and by contributing to the research enterprise of corporate America, these international Ph.D.'s are a powerful component of the research engine that fuels the American economy. When international graduates return to their country of origin, they take back with them an understanding of American culture and values that is important in clarifying and stabilizing the place of the United States in the global political and economic culture. Similarly, domestic students gain a more global perspective and benefit from collaborations with graduate students from other countries and are thus better prepared for the global workplace they will encounter after graduation. Overall, international recipients of American doctorates play important roles in the educational, social, political, economic, and cultural infrastructures of many countries. In the increasingly global arena of high-level research, the U.S. capacity to develop international research partnerships is greatly strengthened by the presence of former students from American universities in key positions in laboratories and universities around the world. In short, the appeal of the U.S. doctorate to students in other countries is one of its great, essential strengths. That importance is borne out by the data collected for this study. The continued success of the United States in this increasingly competitive arena, which is crucial to sustaining the excellence of U.S. doctoral education, is the responsibility of research universities, of state and federal government policy makers, of powerful funding agencies and foundations, and of all other stakeholders in the American doctorate and the vast research enterprise that depends on it."}, {"section_title": "Race, Ethnicity, and Gender of American Doctoral Students", "text": "Doctoral programs across the nation have recognized the implications of powerful demographic trends in the general growth of the population and in college graduation rates that shape the educated workforce in an increasingly knowledge-based economy. Growth rates in the underrepresented minority population outstrip those of the majority, and more women now are awarded bachelor's degrees each year than men. Both of these trends point to a change in the composition of participants in doctoral education in the United States. Through the 1970s doctoral education in almost all fields was largely a man's enterprise, and the number of underrepresented minorities pursuing Ph.D.'s was very small. It became clear, however, that if doctoral education was to serve the population equitably, and if domestic doctoral production was to adequately meet the research and professorial needs of the nation, universities must increase the participation in doctoral PREPBULICATION COPY-UNEDITED PROOFS education of underrepresented minorities and women. But success in this effort would require effective strategies that would address a range of cultural and historical forces that had long distributed educational opportunities unevenly across the population. In response to this situation, many universities have introduced programs to emphasize the importance of recruiting and supporting underrepresented minorities in virtually all fields, as well as the need to increase the presence of women in many of them. Such programs are a high priority of most graduate schools, which have developed procedures and funding mechanisms to encourage minority and female undergraduates to consider doctoral education and to provide support for women and minorities who enroll in doctoral programs. Indeed, government funding agencies-among them the National Institutes of Health (NIH) and the National Science Foundation (NSF) -have developed targeted programs to stimulate the recruitment, retention, and success of women and minorities in doctoral programs. Meanwhile, the significant gains in minority and female enrollments in undergraduate education are broadening the base from which these students may be recruited. Some of the results are encouraging. The data gathered for this study show considerable progress in these areas since the 1995 NRC study was conducted. The percentage of underrepresented minorities produced by doctoral programs overall has increased somewhat, and increases are apparent in all the fields surveyed in this study. Far more women are in doctoral education now than in the 1980s and 1990s, and in some fields once dominated by males, women doctoral candidates now are the majority. Despite these significant gains, underrepresented minorities are still a small proportion of students in many areas of doctoral study-a percentage that remains considerably lower in fact than at earlier levels of education. Women have made striking gains in some biological science and social science fields, but they remain underrepresented in many areas, especially in engineering and the physical and mathematical sciences. And the number of faculty who are women or minorities in many fields remains small. 11 Areas in which increases in underrepresented minorities and women have been most prominent include some fields not included in this study. A 2009 article in the Chronicle of Higher Education by Marc Goulden et al. noted that selection of disciplines represented in this study does not capture many of the fields in which the minority population is relatively high-for example, programs in education or social work. 12 For the fields surveyed in this study, NSF data indicate that the largest gains in underrepresented minorities have been in the humanities and the biological sciences. 13 The rate of growth from 1993 to 2006 for the humanities was 6.4 percent and 5.2 percent for biological sciences, compared with only 0.4 percent growth rate in the agricultural sciences for the same period. Nevertheless, the gains in minority and female representation in doctoral programs are one of the most notable trends in doctoral education since the 1995 NRC study was conducted."}, {"section_title": "THE DATA", "text": "All kinds of anecdotal evidence contribute to the reputations of doctoral programs, and all of them provide interesting, often useful information. Examples are stories of a long heritage of powerful research findings in a distinguished department; recollections of the accomplishments of famous graduates of years past; recounts of new faculty appointments made to strengthen particular areas of studies; lists of faculty publications that have shaped, changed, or even brought into existence whole fields of scholarship; recitations of the high hopes and aspirations engendered by the development of a new Ph.D. program; or reminders of the traditional high regard for the university in which a program is housed. These reputational dimensions can make a program look very attractive to prospective students, to prospective donors, and to funding agencies. But there are limits to the reliability of a picture of graduate program quality and opportunity that is based on reputation alone. A program's reputation may reflect PREPBULICATION COPY-UNEDITED PROOFS renowned professors long retired or the contributions of a handful of faculty in a large program. Doctoral programs that do not have storied histories may find it difficult to demonstrate their current strengths. Others may be more narrowly focused but excel in their areas of specialization. Some programs with excellent reputations but a narrow focus may not match the preferences of all students. Even when reputations for high quality are soundly based and current, they may not help guide prospective students to the best fit for the needs and ambitions they bring to doctoral study. Several important dimensions of doctoral programs become much clearer when viewed from the vantage point of reliable data. How long does it typically take for a student to earn a Ph.D. in chemistry at University A? How much financial support will likely be available for a doctoral student in history at University B, and for how long and in what form? How many students enroll for each one accepted for doctoral study in electrical engineering at University C, and what range of Graduate Record Examination (GRE) scores were likely expected for admission? Of the students who initially enroll for a doctorate in anthropology at University D, what percentage completes that degree within a six-year period? Which universities provide adequate health insurance programs and child care services for their doctoral students? Most universities now produce compelling statements in support of diversity in graduate education, but which doctoral programs demonstrate strong records in recruiting, retaining, and graduating underrepresented students? Will newly enrolled doctoral students be expected to join a graduate employees union if they attend University X? How much teaching, and of what sort, is expected of teaching assistants? Are funds available to doctoral students for travel to conferences or for research? Other questions might be: what is the record of research productivity as determined by objective measures among the faculty in any given doctoral program? Do the faculty fully understand their own disciplinary and cross-disciplinary academic interests? Are fields of study supported by enough faculty to make research in that area a viable doctoral option? What does the university do to facilitate interdisciplinary study, how fully are the faculty engaged in it, and how is such work across disciplinary lines reflected in the degree structure? Good, well-organized data in such areas provide ways to begin answering these questions and many more. The program questionnaire collected data relevant to many of these questions. The answers to some of them can be found in the full dataset, which is larger than the amount of data in the spreadsheet that accompanies the report. The data in the spreadsheet are being made easily accessible because they bear directly on the rankings or they had high response rates or both. The full dataset contains all the responses to all the questions on the program questionnaire, but response rates to some questions may be lower. This NRC study has engaged the surveyed universities in an unprecedented effort to identify the most appropriate data categories and the most effective ways to collect and organize data within them. The universities have responded in kind. An active group of institutional representatives, institutional researchers, and staff from the NRC and Mathematica Policy Research (MPR), the survey research firm engaged for data collection, spent many months refining the questionnaires that organized the information collected from universities. Once the data were collected, they were checked and rechecked via continued correspondence with the participating universities. The data are PREPUBLICATION COPY-UNEDITED PROOFS useful for comparative purposes only insofar as they are generated using definitions and collection procedures that are consistent across all programs and universities. Reaching a shared understanding of the kind of questions that would produce the best data in the most pertinent categories was a challenge of significant dimensions. On each campus enormous efforts went into collecting data consistent with the NRC definitions and methodology. Because this is the first study to make such extensive demands on programs to provide so much comparable data, nearly all programs had to adapt existing practices, or devise new ones, to produce the information required by the questionnaires. Individual doctoral programs, whether they were expected to organize their own records or check the data supplied from central sources, or, as in most cases, both, put much time and effort into the data collection process. Faculty asked to fill out questionnaires providing information about their scholarly records responded at an exceptionally high rate, as did doctoral students in the five disciplines selected for an experimental student survey. Graduate schools, or other institutional units asked to submit the data to the NRC, mobilized exceptional efforts to complete the forms. A productive side benefit of this study is that in many institutions the effort required by the NRC survey has contributed to better internal practices and improved understanding, both centrally and in individual doctoral programs, of data collection and self-assessment. Even though the data collected in 2006 for this NRC study are already dated, they will increase in usefulness as long as they are regularly updated. Updatable data in the key dimensions of doctoral study will enable programs to, for example, measure the success of their own reforms, identify possible slippage in quality, learn extensively from other universities that have introduced changes into their doctoral programs, and gauge program solidity through performance over a more extended period of time. Identifying the procedures needed to ensure that the data collected by this NRC study will be systematically updated at intervals timed to enable doctoral programs to assess their achievements and efforts to improve practices will pose a new set of challenges both for the NRC and for universities. But developing such procedures will be crucial to realizing the maximum benefit of the process begun with the extensive collection of data for this study. PREPUBLICATION COPY-UNEDITED PROOFS 27 3"}, {"section_title": "Study Design", "text": "The National Research Council's Committee on an Assessment of Research Doctorate Programs directed its research at fulfilling the following task: An assessment of the quality and characteristics of research-doctorate programs in the United States will be conducted. The study will consist of (1) the collection of quantitative data through questionnaires administered to institutions, programs, faculty, and admitted to candidacy students (in selected fields), (2) collection of program data on publications, citations, and dissertation keywords, and (3) the design and construction of program ratings using the collected data including quantitatively based estimates of program quality. These data will be released through a web-based, periodically updatable database and accompanied by an analytic summary report. Following this release, further analyses will be conducted by the committee and other researchers and discussed at a workshop focusing on doctoral education in the United States. The methodology for the study will be a refinement of that described by the Committee to Examine the Methodology for the Assessment of Research-Doctorate Programs, which recommended that a new assessment be conducted. This chapter describes how the study was organized for that purpose."}, {"section_title": "PH.D. PROGRAMS AS THE UNIT OF ANALYSIS", "text": "Like all large organizations, research universities in the United States consist of many related parts. These parts include the central administration, which oversees and coordinates the parts; the school or division, which has a faculty, admits students, and focuses on a large academic area such as engineering or arts and science; and the department, which tends to represent a discipline-that is, a field of teaching and learning within that large area. The faculty of a department specialize in the discipline and offer a curriculum that organizes and transmits disciplinary knowledge. For doctoral education another administrative unit is of central importance: the graduate program. In most graduate schools the program admits doctoral students, works PREPUBLICATION COPY-UNEDITED PROOFS with the graduate school to fund them, designs their course of study and advisement, establishes the partnerships between mentoring faculty members and students that are the bedrock of doctoral education, and recommends a successful student for a degree. The program best represents the site on which students do their studies and associate with other students and faculty. As a result, most of the data in this study are related to doctoral programs and their faculty. The committee's decision was logical, but it also presents some complex problems for the most accurate possible representation of doctoral education. Perhaps the most vexing issue the committee faced was how to reconcile the various ways that universities structure their graduate educational experiences. Universities do not follow one standard method of organizing graduate education. As a result, in many fields there is substantial variability in the names of programs and in their content. The years since 1993 have been characterized by the increasing interdisciplinarity in doctoral programs and the blurring of the boundaries across fields, which has been manifested in a variety of ways. An example would be a neuroscience Ph.D. program that involves faculty from several departments and literally \"cuts across\" departmental lines. Even when a Ph.D. program is offered by a single department, however, it may include faculty from other departments, called \"associated faculty\" here, and thus it will have an interdepartmental or interdisciplinary character. A major challenge faced by this study was to find measures that do justice to the growth of interdisciplinarity in doctoral education. In the end, the questions asked and the measures constructed to gauge interdisciplinarity met with limited success. One measure tried was to measure the proportion of faculty from outside the program who helped to supervise dissertations. This measure, however, underestimates interdisciplinarity that is internal to the program. The committee also asked programs whether they were interdisciplinary. A large proportion answered yes, suggesting more extensive interdisciplinarity than that measured by the share of associated faculty. In contrast to classifying graduate programs, classifying academic disciplines is comparatively straightforward because of the reasonably high level of consensus within a field about its general boundaries and its major subspecialties and subcategories. Some fields have relatively few subspecialties, and the basic predissertation years of doctoral education are similar for all students in the program. However, disciplines and specialties that have grown out of other disciplines-such as biochemistry-or that have emerged from earlier interdisciplinary work present knotty problems with program classification and with the variety of ways in which different universities organize doctoral education. The biological and health sciences, a broad field that proved difficult to address in prior assessments, again proved the most problematic in this assessment. The swift growth of knowledge in the biological and health sciences-revolutionary changes in only a few decades or less-has produced rapidly evolving and highly differentiated ways of organizing graduate education in this field. The increasingly interdisciplinary character of the biological and health sciences is both a cause and a consequence of these academic and institutional changes. Interdisciplinarity means that a plethora of faculty members from several disciplines and programs have multiple responsibilities for training graduate students and identify with several of the programs offered at the university. As a result, obtaining agreement on the classification of core programs within disciplines in this field proved a difficult task. The committee recognized that it had to PREPUBLICATION COPY-UNEDITED PROOFS disaggregate the unit of analysis beyond the general disciplinary name. It could not lump all biological and health science programs together and get an accurate representation of the experiences of students in various parts of the biological and health sciences at a university. Unfortunately, there was no consensus about the nomenclature for programs within the biological and health sciences, because different universities classify their biological programs differently. The committee thus worked closely with leaders in the disciplines before arriving at broadly acceptable names for the various programs that would be assessed. In asking about the student experience within these programs, the committee had to remember that students at some universities are admitted to biological and health sciences programs without having to choose an area of specialization until the second or third year of study. In principle, such an approach allows students to \"find\" their interest before choosing a special area of interest in which they will do their doctoral research. These programs often call themselves \"Biological Sciences\" or \"Integrated Biological Science.\" If the Ph.D. was offered in a program with this name, it was reported as such. If the Ph.D. was offered in a more specialized area, then the program was given the name of that area. Even as the committee sought to find reasonable patterns in the names of programs, it realized that increasingly \"the laboratory\" might be becoming the meaningful unit of analysis in some disciplines. Although this development is more often true at the postdoctoral level than at the Ph.D. level, the committee found evidence of graduate students identifying their own intellectual roots or heritage with the laboratory supported by their graduate thesis adviser or the professor who organizes a laboratory. In short, even the \"program\" as the unit of analysis may not fully capture the source of research training received by graduate students. And it seems increasingly true that faculty sponsors of doctoral students have a greater influence on the next steps in their careers than the program faculty as a whole. Yet on balance the committee believes that the core educational experience of doctoral students takes place within a program that embraces both the course work that they experience with multiple members of the faculty and the concentrated research experience within the laboratory of one or several faculty members or in seminars or in individual discussion with faculty. Outside of the biological and health sciences, the program as the unit of analysis also does some injustice to new, interdisciplinary programs that have not been sufficiently filtered into a standard curriculum and a standard method of organizing the educational experience. These programs transcend traditional boundaries and include experts from several existing disciplines. The names of these interdisciplinary programs often vary, and it is not altogether evident that what is being taught in the new programs is in fact comparable. For example, at the graduate level what are the academic relations between women's studies and gender studies? Thus admittedly, interdisciplinary programs, even though they are becoming increasingly important at universities, are shortchanged in the evaluation of more standard scholarly and scientific programs. Moreover, they may not be sufficiently numerous on the national scene to make comparative ratings possible. Interdisciplinary studies and collaborations may give rise to new programs of study that evolve over time into fields distinct from their origins. Examples of recently emerged, but now established and recognized, fields fully surveyed here are biomedical engineering and American studies. These fields reflect the maturation of research areas PREPUBLICATION COPY-UNEDITED PROOFS that originally were interdisciplinary. Other fields may emerge from a single discipline, just as aerospace engineering has arisen from mechanical engineering. Some of the currently emerging fields identified by the committee are nanoscience, systems biology, urban studies and planning, and film studies. For these emerging fields the committee collected data only on the number of faculty (core, new, and associated) and the number of students overall and in candidacy. This information should be useful for future benchmarking studies and may assist prospective students in the identification of these programs. A final challenge inherent in making the program the unit of analysis was how to measure the workload of faculty members, whose appointment generally lies in a single department but who participate in more than one graduate program. Programs draw on faculty from within the discipline and to some extent on colleagues in related disciplines. For example, a nanoscientist may offer instruction and research guidance in the fields of physics, applied physics, and chemistry. A neuroscientist may work with students in programs ranging from biochemistry to cognitive psychology. A history professor may work in history, African American studies, and American studies. Thus how were professors assigned to programs? The committee spent a lot of time discussing this question. It largely agreed on one principle: it should not allow double counting and should try to prevent universities from assigning their most prolific and distinguished faculty to multiple programs unless they actually expended \"effort\" within them. Faculty members would demonstrate the effort they put into each program primarily by stating the number of doctoral students whose dissertations they advised or on whose doctoral committees they served. The total amount of time spent by faculty members in all the programs in which they are involved could not exceed 100 percent. The committee was aware that allocations of faculty time are sometimes not easily determined. Moreover, some faculty members have huge responsibilities in multiple programs-many graduate students and many sponsored dissertations-while others do far less in training and mentoring students. In actual time and energy spent, 50 percent of effort by some faculty members in a program may in fact be greater than 100 percent effort by others. Of course, this observation also applies to human activity outside of research universities. Faced with the practical question of whether the allocations of faculty time were realistic, the committee counted the dissertations that faculty members were directing and allocated their time among the programs in which they served. It then asked institutional coordinators to consult with the programs to judge whether this numerical allocation adequately reflected how a faculty member's time should be allocated across several programs. In a few cases it did not, and the committee accepted the allocation provided by the institution. This decision was important because the total publications of faculty in a program were adjusted by the allocation of the faculty member to the program. Despite these problems of classification and assignment, the committee believes that the program continues to represent the unit that most accurately defines the range of experiences of the graduate student once admitted to a specific department or program. In this assessment, quantitative data on 4,838 programs have been assembled (see Table 3-1). These programs correspond to six broad fields and 59 different academic disciplines. Each of these programs was subjected to an overall, primary assessment represented by a range of rankings. In addition, assessments were conducted of three separate dimensions PREPUBLICATION COPY-UNEDITED PROOFS of doctoral education: (1) research activity; (2) student support and outcomes, a measure that reflects program characteristics that are specifically relevant to the student experience; and (3) diversity of the academic environment, a measure that includes the gender, racial, and ethnic diversity of the faculty and of the student body, as well as a measure of the percentage of international students. 1 Taken together, these individual assessments represent a comprehensive assessment of Ph.D. education in the United States. "}, {"section_title": "FIELD COVERAGE", "text": "The studies by the NRC in 1982 and 1995 focused primarily on fields in the arts and sciences and engineering. However, the committee recognizes that research doctorate programs have grown and diversified since 1993 and that research doctorates are not limited to the arts and sciences. Therefore, the taxonomy for this study has been expanded from the 41 fields in 1993 to the current 62 fields of which 59 have program rankings. In addition, it has placed more emphasis on studies that extend beyond a single field, and so 14 emerging fields are included to recognize the growth of multi-, cross-and interdisciplinary study. It is anticipated that many of these fields could become established areas of scholarship and eligible for inclusion in future studies. Finally, when the committee developed the taxonomy it expected that each field would have enough programs to be ranked, but after it administered the program questionnaires it found that three fields could not be ranked: languages, societies, and cultures (LSC), engineering PREPUBLICATION COPY-UNEDITED PROOFS science and materials (not elsewhere classified), and computer engineering. LSC could not be ranked because the subfields were too heterogeneous for raters to provide informed rankings across them, and no subfield was large enough that rankings could be calculated for it alone. Computer engineering was put forward as a field that was separate from electrical engineering, but the universities in the study reported only 20 computer engineering programs. Similarly, engineering science and materials (not elsewhere classified) did not have enough programs to be included in the rankings. Although rankings are not provided for these fields, full data are provided in the online data that accompany this study."}, {"section_title": "DEVELOPMENT OF THE TAXONOMY", "text": "Immediately after the release of the 1995 study, some institutions and users expressed their concerns about the scope of fields covered and the taxonomy. During the period leading up to the current study, some fields, such as communications, kinesiology, and theater research, matured and established themselves. Other areas, such as doctoral education in nursing, public health, and public administration, convinced the committee that they had emerged from predominantly master's fields to established areas of doctoral research. The 1995 study report specifically mentioned the difficulty encountered in defining fields in the biological and health sciences. Furthermore, the taxonomy did not cover fields in the agricultural sciences. Coverage of Ph.D. programs in the basic biomedical sciences that were housed in medical schools was spotty. In establishing the taxonomy of fields to be included in the current study, the committee used as a starting point Assessing Research-Doctorate Programs: A Methodology Study, the 2003 report of the Committee to Examine the Methodology for the Assessment of Research-Doctorate Programs. 2 On the one hand, it recognized that the taxonomy should build on previous taxonomies in order to maintain continuity with earlier studies, that it should correspond as much as possible to the actual programmatic organization of doctoral studies, and that it should capture the development of new and diversifying activities. On the other hand, it recognized that there was no \"right\" way of organizing academic fields. The organization used by one university as opposed to another is often an outcome of historical circumstances rather than some universal organizing principle. In general, faced with this variability, the committee adopted whatever seemed to be the most commonly used current taxonomic divisions. To go back to the example of biology, the changes in biology that were evident in the 1995 study have transformed the discipline. Biology is now a complex field that appears under the umbrella of the biological and health sciences, a grouping with 19 fields and 3 emerging fields. The impact of new technology, the digital revolution, and the explosion of knowledge at the molecular level have moved the biological sciences from fields defined by levels of organization to problem-based, interdisciplinary fields."}, {"section_title": "ELIGIBILITY CRITERIA FOR FIELDS AND PROGRAMS", "text": "The committee chose to preserve the criteria from the 1995 study for the selection of fields to be included in the current study. To be included, a field as a whole had to have (1) granted at least 500 doctorates in the last five years (2001-2002 to 2005-2006); and (2) be represented in at least 25 institutions. Taken together, these criteria ensure that the field is a significant presence in doctoral education and that there are enough programs nationwide to make comparison meaningful. Fifty-nine fields met these criteria. The unit of observation in this study is the doctoral program. A program is a unit of graduate study that is defined by its performance of at least three of the following four activities: 1. Enrolls students in doctoral study 2. Designates its own faculty 3. Develops its own curriculum 4. Recommends students for doctoral degrees. To be included in the study, a doctoral program meeting these criteria must also have produced at least five doctorates between 2001-2002 and 2005-2006. This quantitative criterion is designed to ensure that doctoral education and research are a central part or a mission of any included program. Given these ground rules, institutions were asked to name the programs they wished to see included in the study. They named 4,838 for which the committee calculated illustrative ranges of rankings."}, {"section_title": "PARTICIPATION IN THE STUDY", "text": "In September 2005 Ralph J. Cicerone, chair of the National Research Council, wrote the presidents of all universities offering doctoral programs to invite them to participate in the study. The invitation explained the purpose, organization, and time line of the study PREPUBLICATION COPY-UNEDITED PROOFS and encouraged the institutions to contribute funding to it. Contribution guidelines were determined by the number of Ph.D.'s granted in the fields in the NRC taxonomy over the period 2001-2002 to 2003-2004. Copies of the letter were sent to the provost and graduate dean. Although universities were asked to contribute to the study, and most did, a financial contribution was not a requirement for participation. Indeed, the financial contributions of U.S. institutions of higher education, while vital to the study, were small compared with the value of very significant efforts by senior staff at the participating institutions to gather, check, collate, and communicate the data requested from their schools. In many cases such data had not been collected in the past, and the efforts initiated in response to the questionnaires were far from trivial."}, {"section_title": "QUESTIONNAIRE DEVELOPMENT AND DATA COLLECTION", "text": "A Panel on Data Collection composed of graduate deans and institutional researchers was tasked with drafting questionnaires for this study. Starting from survey instruments drafted originally by the 2003 study committee that developed a methodology for the assessment, the panel drafted questionnaires for four groups of respondents: institutions, programs, faculty, and students. After approval by the committee, the questionnaires were posted on the project web site and participating institutions were asked to comment on them. The e-mail list created was open to anyone from participating institutions who was working on the study. Through the list, the NRC received hundreds of comments and suggestions. Answers were posted by both NRC staff and the survey contractor, Mathematica Policy Research. The comment and response processes were open and iterative and, as such, resulted in decisions that were acceptable to most institutions and programs, but did not fit all (see Appendix D for copies of the questionnaires). Each of the four questionnaires was also reviewed by the Nation Research Council's Institutional Review Board (IRB). And many institutions required that they be reviewed by their own IRBs. The introductory section of the questionnaires was revised to comply with their recommendations when needed. In designing the questionnaires the committee had to make many choices. In some cases the choices were obvious; in others they were less so and therefore engendered considerable debate among the committee members. These issues included definitions and choices of what information to collect. The program was chosen as the primary unit for the study because programs admit students, offer degrees, and are the obvious target of student interest. The treatment of faculty presented a more difficult problem. In many institutions emeritus faculty play an important role in teaching and research, as do adjunct faculty. For this study the committee chose to define faculty as those who had directed doctoral research dissertations within the last five years. It recognizes that many individuals whom it is not \"counting\" as faculty make valuable contributions, but for uniformity and consistency it chose this definition. There is also inconsistency across programs in the definition of a doctoral student. In most programs students apply directly for admission to the doctoral program without having first obtained a master's degree. Other programs, however, do not admit students to the doctoral program until they have satisfactorily completed a master's degree and shown they are capable of carrying out work in a doctoral program."}, {"section_title": "Data: Student Support & Outcomes X: Percent of First Year Students with Full Financial Support, Fall 2005", "text": "For each program, question E8 reported the type of support that fulltime graduate students received during the fall term each year of enrollment. For this variable the data for the first year were added for all types of support and divided by the total number of students."}, {"section_title": "Y: Average Completion Ratio, 6 Years or Less", "text": "Questions C16 and C17 reported for males and females separately the number of graduate students who entered in different cohorts from 1996-1997 to 2005-2006 and the number in each cohort who completed in 3 years or less, in their 4th, 5th, 6th, 7th, 8th, 9th years, and in 10 or more years. To compute the completion rate, the number of doctoral students for a given entering cohort who completed their doctorate in 3 years or less and in their 4th, 5th, 6th years were totaled and the total was divided by the entering students in that cohort. This computation was made for each cohort that entered from 1996-1997 to 1998-1999 for the humanities and 1996-1997 to 2000-2001 for the other fields. Cohorts beyond these years were not considered, since the students could complete in a year that was after the final year 2005-2006 for which data were collected. To compute the average completion rate, an average was taken over 3 cohorts for the humanities and over 5 cohorts for other fields. Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. PREPUBLICATION COPY-UNEDITED PROOFS Question C2 reported the median time to degree for full-time and part-time students. That reported number was used for this variable. The median was calculated from graduates who received doctoral degrees in the period 2003-2004 through 2005-2006. AA: Percent with Academic Plans A crosswalk was generated between the NSF Doctorate Record File Specialty Fields of Study and the fields in the study taxonomy. Data from the DRF for 5 years (2001)(2002)(2003)(2004)(2005) were matched by field and institution to the programs in the research-doctorate study. The percentage was computed by taking the number of individuals who have a signed contract or are negotiating a contract for a position at an educational institution and dividing by the number of doctorates in those years. Positions included employment and postdoctoral fellowships."}, {"section_title": "AB: Collects Data About Postgraduation Employment (1=Yes; 0=No)", "text": "This variable takes the value of 1 if the program collects data about the postgraduation employment of its graduates. A zero is given if otherwise."}, {"section_title": "Data: Diversity AC: Non-Asian Minority Faculty as a Percent of Total Core and New Faculty, 2006", "text": "For each program the data reported for question B7, the race/ethnicity of core and new faculty in the program, was used to compute the ratio of non-Hispanic Blacks, Hispanic, and American Indians or Alaska Natives to that of all faculty with known race/ethnicity. \"Core\" faculty are those whose primary appointment is in the doctoral program. \"New\" faculty are those with tenure track appointments who were appointed in 2003-2006."}, {"section_title": "AD: Female Faculty as a Percent of Total Core and New Faculty, 2006", "text": "For each program the data reported for question B5, the gender of core and new faculty in the program, was used to compute the ratio of core or new female faculty to the total of core and new faculty. Allocations were not used in the construction of this variable."}, {"section_title": "PREPUBLICATION COPY-UNEDITED PROOFS AE: Non-Asian Minority Students as a Percent of Total Students, Fall 2005", "text": "Question C9c reported the race/ethnicity of graduate students in the program. This was used to compute the ratio of non-Hispanic Blacks, Hispanics, and American Indians or Alaska Natives to that of the total of students with known race/ethnicity."}, {"section_title": "AF: Female Students as a Percent of Total Students, Fall 2005", "text": "Question C9 reported the gender of graduate students in the program. This was used to compute the percentage by taking the number of female graduate students divided by the total number of graduate students."}, {"section_title": "AG: International Students as a Percent of Total Students, Fall 2005", "text": "Question C9b reported the citizenship of graduate students in the program. These data were used to compute the percentage of international graduate students by taking the number with temporary visas and dividing it by the number of graduate students with known citizenship status."}, {"section_title": "Data: Other Overall Ranking Measures AH: Average Number Ph.D.s Graduated, 2002-2006", "text": "Question C1 reported the number of doctoral degrees awarded each academic year from 2001-2002 to 2005-2006. The average of these numbers was used for this variable. If no data were provided for a particular year, the average was taken over the years for which there were data."}, {"section_title": "AI: Percent of Interdisciplinary Faculty , 2006", "text": "Faculty were identified as either core, new, or associated. Percent interdisciplinary is the ratio of associated to the sum of core, new, and associated faculty. Allocations were not used in the construction of this variable."}, {"section_title": "AJ: Average GRE Scores, 2004-2006", "text": "For each program, question D4 reported the average GRE verbal and quantitative scores for the 2003-2004, 2004-2005, and 2005-2006 academic years and the number of individuals who reported their scores. A weighted average was used to compute the average GRE, which was calculated by multiplying the number of individuals reporting scores by the reported average GRE score for each year, adding these three quantities and dividing by the sum of the individuals reporting scores."}, {"section_title": "Notice: Embargoed Materials", "text": "This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010.\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. of uncertainty in these ratings (statistical variability from the estimation, program data variability, and variability among raters). The measure of uncertainty is expressed by reporting the endpoints of the 90 percent range of rankings 6 for each program-that is, the range that contains the middle 90 percent of a large number of ratings calculations that take uncertainty into account. 7 In summary, the committee obtained a range of rankings for each program in a given field by first devising two sets of weights through two different methods, direct, or survey-based, and regression-based. It then standardized all the measures to put them on the same scale and obtain ratings by multiplying the value of each standardized measure by its weights and adding them together. It acquired both the direct weights and the coefficients from regressions through calculations carried out 500 times, each time with a different set of faculty, to generate a distribution of ratings that reflects their variability. The range of rankings for each program was obtained by trimming the bottom 5 percent and the top 5 percent of the 500 rankings to obtain the 90 percent range. This method of calculating ratings and rankings takes into account variability in rater assessment of the things that contribute to program quality within a field, variability in the values of the measures for a particular program, and the range of error in the statistical estimation. It is important that these techniques yield a range of rankings for most programs. The committee does not know the exact ranking for each program, and to try to obtain one-by averaging, for example-would be misleading because it has not assumed any particular distribution of the range of rankings. 8 Thus within the 90 percent range, a program's rankings could be clustered at one endpoint or the other, so that averaging the two endpoints could be misleading. The datasheet that presents the range of rankings for each program lists the programs alphabetically and gives the range for each program. Users are encouraged to look at groups of programs that are in the same range as their own programs, as well as programs whose ranges are above or below, in trying to answer the question \"Where do we stand?\" A similar technique was used to calculate the range of rankings for each of the dimensional measures for each field. Some possible ways of using the ranges of rankings and the data tables are discussed in Chapter 6. The rankings for the overall R and S measures and for the dimensional measures for each of the programs in each of the 59 fields with ranges of rankings are available on the Web site (www.nap.edu/rdp) and should be taken as illustrative of the different approaches. 9 The 2009 Guide to the Methodology described a methodology that assumed that the R-based coefficients could be combined with the Sbased coefficients using a formula that appears on page 48 of the Technical Appendix to the pre-publication version of the guide. This formula relied on the variances of the samples used to calculate each set of coefficients. Upon looking at every field, however, the committee found that for some fields these variances could be very large, especially for those fields in which either the field was heterogeneous in the sense that the same 6 The committee calls these endpoint values the 95th percentile and the 5th percentile. 7 The 90 percent range eliminates the top and bottom 25 ratings calculated from 500 regressions and 500 samples of direct weights from faculty. The range contains 90 percent of all the rankings for a program. In the Guide to the Methodology, the range chosen was 50 percent, but the committee later decided that this range was overly restrictive. 8 Two programs with the same 90 percent range could have very different means and medians. 9 The 24,190 rankings (one range for each of the 5 measures for 4,838 programs) are too numerous to present in this written report.\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. field encompassed very different forms of scholarly productivity or there were relatively few raters. This situation resulted in R and S medians that did not correlate well, and so the committee abandoned its plan to combine the coefficients that were calculated in the two ways. Instead of one overall one overall range of rankings the committee presents these two measures separately. The fields for which the correlation of the two measures at the median was below 0.75 were listed earlier in Box 4-1 with details shown in Appendix G.\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. respondents about the nature of the data requested from them, or from problems within the public databases used. That said, even though the input data underwent numerous consistency checks and the participating respondent institutions were given the opportunity to provide additional quality assurance, the committee is certain that errors in input data remain, and that these errors will propagate through to the final posted rankings. Its hope is that after all of the input data are made public, any significant errors will be found and reported so they can be rectified in a timely fashion before the ranking and rating exercise is repeated. Some readers may be surprised about the degree to which program rankings will have changed from the 1995 report. These changes may stem from three factors: (1) real changes in the quality of the programs over time; (2) changes in the principles behind the ranking methods; and (3) simple error, either statistical or from faulty data. The reader should keep in mind that the charge to the committee and the consequent decisions of the committee may have increased the sensitivity of the results to the third factor, and it would now like to spell out some of these issues in greater detail\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. Copyright \u00a9 National Academy of Sciences. All rights reserved. related to graduate Ph.D. education. Because it is an important element in the measurement of program quality, the methodology was designed to utilize its virtues but avoid some of the attendant defects (such as time lag). And yet this decision, while required by the statement of task, remained controversial, with some committee members still preferring the direct use of reputational measures. As several committee members noted, some of the other \"quantitative\" measures used, such as honors and awards, were in fact very closely related to and reflected perceived quality-that is, reputation.\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. The data reveal clearly that these programs are different. The chemistry program in Institution A is large-it graduates almost 32 students a year compared with 20 or fewer for the remaining four programs. In terms of research activity, the first three programs are highly productive, and their range of rankings would likely place them among the top 20 programs in the field. One of the institutions has a prestigious (and likely older) faculty, as measured by awards. With one exception, all of the programs support all of their first-year students. All of the institutions have a time to degree of between four and six years, but in institutions A and D less than 50 percent of their students complete their degrees within six years. Institution B places almost a third of its graduates in academic positions, whereas institutions A and E place less than 20 percent. The comparisons based on the diversity variables are mixed. Women make up more than one-third of the students at all the programs, but the gender diversity of faculty varies. In all of the programs more than 20 percent of the students are international, but only one program has more than 10 percent of students from racial or ethnic minorities, and two programs have no minorities in their teaching faculty. This example illustrates that for many uses the data themselves may be more useful than any range of rankings. The temptation, however, will be to use the ranges of rankings.\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010.\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. The growth in the common programs is mirrored by the growth of doctoral recipients nationwide (see Table 7-4). The greatest percentage growth was in engineering. \nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010.\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. PREPUBLICATION COPY-UNEDITED PROOFS Some of these fields are training students who will work with underrepresented minority populations or specialize in studies related to the history and culture of underrepresented minorities, but all have focused on increasing minority participation and have, to some extent, succeeded. The increased participation of women, as both faculty and students, is even more of a success story (see Figure 7-3). Enrollments in a few broad fields (humanities, social and behavioral sciences, and biological and health sciences) are more than 50 percent female, but the representation of women in the faculty has yet to reach even 40 percent. In none of the broad fields in science or engineering is more than 30 percent of the faculty female. The disciplines in the science, technology, engineering, and mathematics (STEM) fields in which more than 15 percent of the doctoral faculty is female are shown in  One other aspect of diversity is the percentage of students who are from outside the United States. The variation in percentages of enrolled international students across the broad fields is considerable: engineering as a whole, 60 percent; the humanities, slightly more than 15 percent; the physical and mathematical sciences, 45 percent; the biological and health sciences and the social and behavioral sciences, less than 30 percent. Within the broad fields, some disciplines differ noticeably from the average. In economics, for example, 63 percent of its students are international.\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. The agricultural and the biological and health sciences appear to have fewer doctoral faculty under the age of 40 than the other broad fields. This is a result of different hiring patterns in the broad fields, as evidenced by the answers to the question about previous employment. About one-third of respondents in the agricultural, physical, and biological and health sciences have one or more postdocs before becoming doctoral faculty. By contrast, in the humanities, engineering, and social and behavioral sciences more than 20 percent of the faculty came to a faculty position directly from receiving their Ph.D. Engineering, which draws many of its faculty from industry brings in almost a quarter of its faculty from \"other,\" which includes nonacademic employment. Movement from within academia is about 25 percent in the sciences and engineering. The pattern is different for the humanities and social sciences, where more than 40 percent of respondents were employed in academia before moving to their current employer. The humanities distinguish themselves by the age of their doctoral faculty. More than 27 percent are over the age of 60, in contrast with 20 percent or less in the agricultural sciences, engineering, and the biological and health sciences. As for mobility, doctoral faculty tend to stay at one PREPUBLICATION COPY-UNEDITED PROOFS institution. About three-quarters of them, in all fields, have been at their current institution for eight years or more, and more than one-third have been in one place for more than 20 years. The one exception is the biological and health sciences. The composition of the faculty by racial and ethnic diversity and gender was discussed earlier in this chapter, and is confirmed here. Only the humanities draw more than percent of its faculty from underrepresented minorities. As for gender, in the social sciences, humanities, and biological and health sciences, one-quarter or more of the doctoral faculty is female.\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010.\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. finding is that the larger programs have somewhat longer times to the completion of a degree. Size is not consistently related to differences in diversity. The great deal of data to be made available from the faculty and student questionnaires can be used to explore relationships among program characteristics and the characteristics of faculty and of students in the five fields studied.\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. PREPUBLICATION COPY-UNEDITED PROOFS 105 8\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010.\nThis report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. C1. While many of the faculty members will have the Ph.D. as their highest degree, it will be important to know if the faculty have received other degrees. These data are not available from other data sources and are especially important in describing the background of faculty in the biomedical sciences. C2. The doctoral origins of the faculty for a program will provide data on the career paths of graduates from different institutions and provide a count of the number of foreign degree holders on faculties at U.S. doctoral institutions. It provides information about the segmentation of the academic labor market and is an indirect outcomes measure for those doctorate-awarding origins of those who are academically employed. C3. Field of Ph.D. or equivalent will provide information on whether the faculty member has changed research fields. It may also give a measure of interdisciplinarity. C4. Year of Ph.D. or equivalent will allow for cohort analyses and in conjunction with the next question will provide information about the postdoctoral experience. C5.-C7. There is very little known about the postdoctoral experience and these questions will provide information on the career paths of individuals who have held postdocs in terms of the number and duration and how that has changed over time for doctoral faculty. D. Scholarly Activity-The questions in this section of the questionnaire are designed to gather information that will be helpful in matching the faculty in a program to data from national databases of publications, citations and grants."}, {"section_title": "PREPUBLICATION COPY-UNEDITED PROOFS AK: Percent of Students with External Fellowships, 2005", "text": "For each program question E8 reported the type of support full-time graduate students received during fall term each year of enrollment. For this variable the data for the first year were added for support by externally funded fellowships and combinations of external fellowships and other internal support and then divided by the total number of students."}, {"section_title": "AL: Is Student Work Space Provided? (1=Yes; 0=No)", "text": "Question D12 reported the percentage of graduate students who have work space for their exclusive use."}, {"section_title": "AM: Is Health Insurance Provided? (1=Yes; 0=No)", "text": "Question A1 reported whether or not the institution provided health care insurance for its graduate students."}, {"section_title": "AN: Number of Student Activities (Max=18)", "text": "Question D8 listed 18 different kinds of support activities for doctoral students or doctoral education. This variable is a count of the number of student support activities provided by the program or the institution."}, {"section_title": "Data Not Used in Ranking", "text": ""}, {"section_title": "AO: Total Faculty, 2006", "text": "Questions B1 and B2, total responses."}, {"section_title": "AP: Number of Allocated Faculty, 2006", "text": "Calculated as the number of program faculty corrected for association with multiple programs. For more detail on how these data were calculated, refer to footnote 46 in A Data-Based Assessment of Research-Doctorate Programs in the United States (2010), Chapter 3, \"Study Design.\""}, {"section_title": "AQ: Assistant Professors as a Percent of Total Faculty, 2006", "text": "Of those faculty who reported any rank, the percentage of assistant faculty were calculated as the number of assistant professors divided by the number of total faculty."}, {"section_title": "AR: Tenured Faculty as a Percent of Total Faculty, 2006", "text": "Number of tenured faculty divided by the number of total faculty."}, {"section_title": "AS: Number of Core and New Faculty, 2006", "text": "Total number of core and new faculty."}, {"section_title": "AT: Number of Students Enrolled, Fall 2005", "text": "Question C9 reported the total number of students enrolled in the fall of 2005."}, {"section_title": "AU: Average Annual First Year Enrollment, 2002-2006", "text": "Question C3 reflects the number of first-time enrolled for 2001-2002, 2002-2003, 2003-2004, 2004-2005, and 2005-2006. An average was taken over 5 years."}, {"section_title": "PREPUBLICATION COPY-UNEDITED PROOFS AV: Percent of Students with Research Assistantships, Fall 2005", "text": "Question E8 reported the number of students who received support as a research assistant in the fall of 2005. A percentage was calculated over the total number of students."}, {"section_title": "AW: Percent of Students with Teaching Assistantships, Fall 2005", "text": "Question E8 reported the number of students who received support as a teaching assistant in the fall of 2005. A percentage was calculated over the total number of students."}, {"section_title": "Student Activities AX through BP", "text": "Question D8 reports whether the institution and/or program provides support for doctoral students or doctoral education. Key: 1= provided for by institution; 2= program support only; 3= both institutional and program support; 4= neither institutional nor program support Note: Unless otherwise noted, all data refer to the 2005-2006 academic year. Further details are provided in Appendix E. Data collection was administered by the project's survey contractor, Mathematica Policy Research. Responses at the institutional and program levels relied heavily on the institutional coordinator at each institution, who, depending on the administrative structure of the university, was either the graduate dean or the director of institutional research. This person knew how to find data about doctoral programs and made sure that the questionnaires were answered by knowledgeable respondents. Some institutions have highly centralized and automated data systems, and the institutional research office was able to provide many of the answers to the program and institutional questionnaires. Other universities relied on program and departmental administrators to provide the data. In addition, the universities provided MPR with faculty and student e-mail and student lists in order to administer their respective questionnaires. To preserve confidentiality, replies were sent directly to MPR."}, {"section_title": "DATA VALIDATION AND CLEANING", "text": "Once the data were collected from the universities, they had to be checked. The first data cleaning and accuracy check was conducted in 2007, after institutions had submitted program data for the study. This step involved returning the data for all programs, with a request that the data be checked for accuracy and missing data be supplied. To ensure that eligible programs that submitted data to the NRC could be included in the ratings, the NRC took several steps beyond the initial checking to \"clean\" the data. Second, in February 2008 the NRC contacted institutions to inquire about programs that were either missing too much data or for which it had identified some data as \"outliers.\" This process involved 107 institutions and 387 programs. To determine PREPUBLICATION COPY-UNEDITED PROOFS which programs required cleaning, the 2 sigma (outlier) test was performed for 14 key variables (see Box 3-1). In an e-mail to the institutional coordinators, NRC staff explained that the check was necessary to calculate ratings for the programs. Institutional coordinators received spreadsheets and were asked to fill in blanks and make sure the outlier values of variables for the programs were correct, changing them if necessary. During this process, 298 programs submitted new data or confirmed their existing data. Twenty-three programs requested to be removed from the ratings or the study or both. Sixty-six programs did not respond to requests or did not have the data available. An additional 95 programs (not in the original 387) submitted cleaned data. Following this process, NRC staff identified 70 programs that had left the health insurance or student outcomes variable, or both, blank. Most of the programs, though not all, responded with data. Third, in January 2009 members of the committee identified 27 programs that appeared to have been assigned to the wrong field. To check, NRC staff contacted 23 institutions to ask about one or more programs. Institutional coordinators responded, and, as a result, seven programs were moved to a different field. Another 20 programs did not move because the institutional coordinator explained why the school had placed the program in that field. Aside from external checks with the institutions, NRC staff and the committee performed repeated ongoing internal checks on the data. These checks included looking at grants, awards and honors, GRE scores, completion rates, and the like. In most cases anomalies in the ratings did not appear to be the result of data errors-that is, careful review by the committee did not find data from the anomalous program very different from that for similar programs. However, in one case there did appear to be an error. The calculated rating for one particular program was very low because of its GRE scores. After these data were questioned, the institutional coordinator submitted new data that reflected the scores of admitted students, as had been instructed, rather than applicants. Publication and citation data were obtained from Thompson Reuters (formerly ISI, the Institute for Scientific Information) and matched to faculty lists. Matching was checked PREPUBLICATION COPY-UNEDITED PROOFS both by examining outliers and by checking and eliminating attribution to faculty with similar names. Finally, no matter how careful the committee was in collecting data and designing measures, sources of error remain. Here are some examples: \u2022 Classification errors. The taxonomy of fields may not adequately reflect distinctions that the field itself considers to be important. For example, in anthropology physical anthropology is a different scholarly undertaking from cultural anthropology, and each subfield has different patterns of publication. By lumping together these subfields into one overall field, the committee is implying comparability. Were they separate, different weights might be given to publications or citations. Anthropology is not alone in this problem. Other fields are public health, communications, psychology, and integrated biological science. Although this study presents ranges of rankings across these fields, the committee encourages users to choose comparable programs and use the data, but apply their own weights or examine ranges of rankings only within their peer group. \u2022 Data collection errors. The committee provided detailed definitions of important data elements used in the study, such as doctoral program faculty, but not every program that responded paid careful attention to these definitions. The committee carried out broad statistical tests, examined outliers, and got back to the institutions when it had questions, but that does not mean it caught every mistake. In fields outside the humanities it counted publications by matching faculty names to Thompson Reuters data and tried to limit mistaken attribution of publications to people with similar names. Despite these efforts, some errors may remain. \u2022 Omission of field-specific measures of scholarly productivity. The measures of scholarly productivity used were journal articles and, in the humanities, books and articles. Some fields have additional important measures of scholarly productivity. These were included in only one field, the computer sciences. In that field peer-reviewed conference papers are very important. A discussion of data from the computer sciences with its professional society led to further work on counting publications for the entire field. 5 In the humanities the committee omitted curated exhibition volumes for art history. It also omitted books for the science fields and edited volumes and articles in edited volumes for all fields, since these were not indexed by Thomson-Reuters. All of these omissions result in an undercounting of scholarly productivity. The committee regrets them, but it was limited by the available sources. In the future it might be possible to obtain data on these kinds of publication from r\u00e9sum\u00e9s, but that is expensive and time-consuming. The Methodologies Used to Derive"}, {"section_title": "Two Illustrative Rankings", "text": "Ranking programs based on quantitatively based estimates of program quality is a highly complex task. Rankings should be based both on data that reflect the relative importance to the user of the available measures and on the uncertainty inherent in them. Users of rankings should clearly understand the basis of the ranking, the choice of measures, and the source and extent of uncertainty in them. It is highly unlikely that rankings calculated from composite measures will serve all or even most purposes in comparing the quality of doctoral programs. The committee has worked for more than three years on arriving at a satisfactory methodology for generating rankings for doctoral programs. This work was pursuant to the portion of the charge, which states: The study will consist of . . . 3) the design and construction of program ratings using the collected data including quantitatively based estimates of program quality. It is this portion of the charge that called for constructing program ratings and deriving rankings from them that reflect program quality. Were it not in the committee's charge, it would be a useful exercise in itself simply to collect program data under comparable definitions and share them widely. This chapter describes how the committee decided what kinds of data to collect and how to use those data to approach the task of providing ratings and rankings for programs. In pursuing this task, it was guided by some motivating ideas that reflected concerns in the higher education community about rankings and their uses and that were described in considerable detail in the 2003 study already noted, Assessing Research-Doctorate Programs: A Methodology Study. These concerns about the 1995 rankings and rankings from other sources were that they do the following: \u2022 Encourage spurious inferences of precision. As the committee describes in this report, there are many sources of uncertainty in any ranking, ranging from the philosophical-any ranking implies comparability of what may not be comparable-to the statistical-sources of variation are present in any aggregation of measures. \u2022 Overly rely on reputation. Reputation, although it has the advantage of reflecting dimensions of program quality that are difficult to quantify, may also be dated and include halo effects-that is, visibility effects that obscure the quality of smaller programs or good programs in less well-known universities. \u2022 Lack transparency. Even when it is based on explicit measures, the weighting of these measures in the ranking may not be discernable or may change from year to year in ways that are not made clear. In addressing these weaknesses in rankings, the committee sought to design a methodology that would result in rankings with the following characteristics: \u2022 Data-based. The rankings were constructed from observable measures derived from variables that reflected academic values. \u2022 A reflection of the prevailing values of faculty in each program area. The rankings were calculated using the opinions of faculty in each program area of both what was important to program quality in the abstract and, separately, how experts implicitly valued the same measures when asked to rate the quality of specific programs. \u2022 Transparent. Users of the rankings could understand the weights applied to the different measures that underlay the rankings and, if they wished, calculate rankings under alternative weighting assumptions. Achieving these seemingly simple objectives in a scientifically defensible way was not a simple undertaking. The committee had to undertake the following tasks: \u2022 Determine what kinds of measures to include. To be included, a measure had to be one that the participating universities either collected in the course of regular institutional research and management, such as enrollment counts, or that the committee felt should be known by a responsible doctoral program, such as the percentage of entering students who complete a degree in a given amount of time. \u2022 Ascertain faculty values. Faculty were asked, on the one hand, to identify the measures that were important to program quality and then asked, on the other, to rate a stratified sample of programs in their fields. \u2022 Reflect variation among faculty and faculty raters. Because faculty may not be in complete agreement on the importance of the different measures or the rating of sampled programs, differences in views were reflected by repeatedly resampling the ratings and, for each resampling, calculating the resulting weights or overall program ratings. This approach leads naturally to presenting a range of rankings on any measure for a given program."}, {"section_title": "\u2022 Design specific measures along separate dimensions of program quality.", "text": "Although overall measures are useful, some users may be particularly interested in measures that focus on specific aspects of a graduate program. The committee calculated ranges of rankings for three of these aspects: research activity, student support and outcomes, and diversity of the academic environment. That said, the two approaches provided in this report are intended to be illustrative of the process of constructing data-based ranges of rankings that reflect the values of the faculty who teach in these programs. It is also possible to produce ranges of rankings that reflect the values of the users. Production of the rankings turned out to be more complicated and to be accompanied by more uncertainty than originally thought. As a consequence, the illustrative rankings described in this chapter are neither endorsed nor recommended by the National Research Council as an authoritative conclusion about the relative quality of doctoral programs. In summary, the committee urges users of these rankings and data to examine them very carefully, as the committee has. It also apologizes for any errors that might be uncovered. It does expect that, as a result of this data collection effort, updating will be easier next time for the respondents and will result in fewer errors."}, {"section_title": "USE OF RANKINGS", "text": "In attempts to rank doctoral programs, sports analogies are especially inappropriate. There are no doctoral programs that, after a long regular season of competition followed by a month or more of elimination playoffs, survive to claim \"We're Number 1!\" Perceptions of the quality of doctoral programs are built over many years of making agonizing tenure decisions and making choices about areas of specialization and the resolution of competing views about the most fruitful direction of a field of study. The evidence of excellence is not easily summarized in runs batted in, earned run averages, or percentage of games won. Instead, it is the result of hundreds of judgments by peer reviewers for journals and presses, as well as citations that accumulate as an area of study develops and grows. The answer, then, to \"What is the best doctoral program in biochemistry?\" should not be the name of a university, but a follow-up question to the interlocutor about what he or she means by \"best\" and in what respects. The committee was keenly aware of the complexity of assessing quality in doctoral programs and chose to approach it in two separate ways. The first, the general survey (S) approach, was to present faculty in a field with characteristics of doctoral programs and ask them to identify the ones they felt were the most important to doctoral program quality. The second, the rating or regression (R) approach, was to ask a sample of faculty to provide ratings (on a scale of 1 to 5) for a representative sample of programs and then to ascertain how, statistically, those ratings were related to the measurable program characteristics. In many cases the rankings that could be inferred from the S approach and the R approach were very similar, but in some cases they were not. Thus the committee decided to publish both the S-based and R-based rankings and encourage users to look beyond the range of rankings on both measures. Appendix G shows the correlations of the medians of the two overall measures for programs in each field. The fields for which the agreement between the R and S medians is poorest are shown in Box 4-1."}, {"section_title": "SUMMARY OF THE METHODOLOGY OF THE ILLUSTRATIVE PROGRAM RANKINGS", "text": "Figure 4-1 shows the steps involved in calculating the two types of overall program rankings (R and S). 1. DATA Answers to questions provided by 4,838 doctoral programs at 221 institutions and combinations of institutions in 59 fields across the sciences, engineering, social sciences, arts, and humanities covering institutional practices, program characteristics, and faculty and student demographics obtained through a combination of original surveys and existing data sources (NSF surveys and Thompson-Reuters publication and citation data)."}, {"section_title": "WEIGHTS", "text": "In two surveys shown in Appendix D, program faculty provided the NRC with information on what they value most in Ph.D. programs: 1. Faculty were asked directly how important they felt 21 items in a list of program characteristics were (for S weights). 2. A sample of faculty rated a sample of programs in their fields. These ratings were then related through regressions to the same items as appeared in (1) using a principal components transformation to correct for colinearity (for R weights). 3. ANALYSIS \"Survey (S)\" and \"regression-based (R)\" weights provided by faculty were used to calculate separate ratings, reflecting the multidimensional views faculty hold about factors contributing to the quality of doctoral programs."}, {"section_title": "RANGES OF RANKINGS", "text": "Each program's rating was calculated 500 times by randomly selecting half of the raters from the faculty sample in step 2 and also incorporating statistical and measurement variability. Similarly, 500 samples of survey based weights were selected. The R weights and the S weights were then applied to 500 randomly selected sets of program data to produce two sets of ratings for each program. These ratings for each of the 500 samples determined the R and S rank orderings of the programs. A \"range of rankings\" was then constructed showing the middle 90 percent range of calculated rankings. What may be compared, among programs in a field, is this range of rankings."}, {"section_title": "Institutions and Programs Students Faculty Existing", "text": "Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. Faculty were surveyed to obtain their views on the importance of different characteristics of programs as measures of quality. 1 Ratings were then constructed based on these faculty views of how those measures related to criteria of program quality, as discussed in the section on dimensional measures. The views were related to program quality using two distinct methods: (1) asking faculty directly to rank the importance of characteristics in a survey (S); and (2) asking faculty raters 2 to provide reputational program ratings (R) for a sample of programs in a field and then relating these ratings, through a regression model that corrected for correlation among the characteristics, to data on the program characteristics. The two methods approach the ratings from different perspectives. The direct, or survey-based, approach is a bottom-up approach that builds up the ratings from the importance that faculty members give to specific program characteristics independent of reference to any actual program. The regression-based method is a top-down approach that begins with ratings of actual programs and uses statistical techniques to infer the weights given by the raters to specific program characteristics. The survey-based approach is idealized. It asks about the characteristics that faculty feel contribute to quality of doctoral programs without reference to any particular program. The second approach presents the respondent with 15 programs in his or her field and information about them 3 and asks for ratings of program quality, 4 but the responders are not explicitly queried about the basis of their ratings. The weights derived from each approach were then applied to the value of the 20 measures for each program to yield two sets of ratings for each program. Each rating was then recalculated 500 times using different samples of raters and varying the data values within a range. 5 The program ratings obtained from all these calculations could then be arranged in rank order and, in conjunction with all the ratings from all the other programs in the field, used to determine a range of possible rankings. Because of the various sources of uncertainty, each ranking is expressed as a range of values. These ranges were obtained by taking into account the different sources 1 All questionnaires, including that for faculty, appear in Appendix D. 2 The raters were chosen through a sampling process that was representative of the distribution in each field of faculty by rank, size of program, and region of the country. 3 The following data were given to the raters: the program URL, the list of program faculty, the average number of Ph.D.'s (2001)(2002)(2003)(2004)(2005)(2006), the percentage of new Ph.D.'s planning academic positions, the percentage of the entering cohort completing their degree in six years or less (fields outside the humanities fields) or eight years or less (humanities), the median time to degree (2004)(2005)(2006), the percentage of female faculty, and the percentage of faculty from underrepresented minorities. All data were for 2005-2006 unless otherwise indicated. 4 The question given raters about program quality was as follows: 5 The range of data values was either plus or minus 10 percent or the actual range of variation if multiyear data were collected on the questionnaire. A Monte Carlo selection was used to vary the selection of raters and of data."}, {"section_title": "DIFFERENCES FROM THE 1995 REPORT", "text": "The summary in Table 4-1 makes it immediately clear that there are significant differences in the methodology for the two studies. These differences alone can have an effect on the relative ranking of a program. Here are some of the more obvious sources of difference: Of the 104,600 total, 7,932 faculty were chosen through a stratified sample for each field to participate in the rating study. Faculty could be counted in more than one program, but were usually counted as \"core\" in only one. Faculty members were allocated fractionally among programs according to dissertation service so that, over all programs, he or she was counted no more than once."}, {"section_title": "Study 2010 Study Ratings and Rankings", "text": "Raters nominated by the institutional coordinators were sent the National Survey of Graduate Faculty, which contained a faculty list for up to 50 programs in the field. Raters were asked to indicate familiarity with program faculty, scholarly quality of program faculty (scale 1-6), familiarity with graduates of program (scale 1-3), effectiveness of program in educating research scholars (scale 1-4), and change in program quality in the last five years (scale 1-3). Rankings were determined for each program by calculating the average rating for a program and arranging all the programs from lowest to highest 1. All faculty were given a questionnaire and asked to identify the program characteristics in three categories that they felt were most important, and then identify the categories that were most important. This technique provided the survey-based (S) weights for each field. 2. A stratified sample of faculty in each field were given a stratified sample of 15 or fewer sampled programs to rate them on a scale of from 1 to 6. Included in the data for raters was a faculty list and program characteristics. These ratings were regressed on the program based on average ranking. characteristics to determine the regressionbased (R) weights. These weights were then assumed to hold for all programs in the field so that all programs could receive a rating based on these weights. 3. The S weights and the R weights, calculated as just described, were used to calculate S rankings and R rankings. 4. Uncertainty was taken into account by introducing variation into the values of the measures and by repeatedly estimating the ratings obtained by taking repeated halves of the raters chosen at random. Ratings were calculated 500 times. 5. The ratings in step 4 were ordered from lowest to highest. The ratings of all programs in a field were pooled and arranged in rank order. "}, {"section_title": "Measurement of Quality", "text": "The 1995 measure of program quality is known as a \"reputational measure\"-that is, raters judged the \"scholarly quality of program faculty.\" The 1995 study noted that this measure is highly correlated with program size and, quite possibly, with visible faculty. 10 Reputation may also be \"dated\" and not reflect recent changes in faculty composition. Finally, the reputation of program faculty may not be closely related to faculty performance in mentoring students or encouraging a high proportion to complete their degrees within a reasonable period of time. 11 By contrast, in its rating exercise the committee for the current study asked respondents for their familiarity with each program, and presented data on size, completion, time to degree, and faculty diversity. It also provided a Web site for the program, in addition to a faculty list. The task was to rate the program rather than the scholarly quality of program faculty. A rater had to rate at most 15 programs, not 50. Once the ratings were obtained, they were then related to the 20 measures through a modified regression technique. 12"}, {"section_title": "Specification of the Measures", "text": "In addition to the reputational measures the 1995 study provided a few program characteristics: faculty size, percentage of full professors, and percentage of faculty with research support. In addition, awards and honors received in the previous five years and the percentage of program faculty who had received at least one honor or award in that period were given for the arts and humanities. For engineering and the sciences, the percentage of program faculty who published in the previous five years and the ratio of these citations to total faculty, as well as the Gini coefficients for these measures (a measure of dispersion), were shown. Data were also presented on students: the total number of students, the percentage of students who were female, and the number of Ph.D.'s produced in the previous seven years. Finally, information was provided on doctoral recipients: the percentage who were female, minority, and U.S. citizens; the percentage who reported research assistants and teaching assistants as their primary form of support; and the median time to degree. But even though all of these \"objective measures\" were reported, they played no explicit part in determining program ranking. By contrast, the current study explicitly includes most of these measures and many more, and attempts to relate them directly to the rating that goes into the program ranking."}, {"section_title": "Overall Comparability", "text": "If the \"quality\" of a program is unchanged, will any of the present ranges of rankings be the same as the 1995 ranking? Although an excellent program is an excellent program by any measure, there is no reason to expect the 1995 rankings to match the present range of rankings on either the S-based or the R-based measure. As this description of the two studies makes clear, the studies used different methodologies for all three calculations. Some important sources of variability are as follows: \u2022 The current study is highly data-dependent. Although the data submitted by the universities were checked and verified repeatedly, errors may remain. And large errors could skew the rankings. Nonreputational data were not explicitly a part of the 1995 rankings, although they were reported in tables in the appendixes. \u2022 The research strength of the faculty as measured by publications and citations was an important determinant of quality in both studies, but the method of counting differed between the studies in two important respects: In the current study, publications for the previous 10 years for humanities faculty, which were not counted in 1995, were collected from faculty r\u00e9sum\u00e9s. Books were given a weight of 5, and humanities articles were given a weight of 1. Second, in non-humanities fields, the 1995 study counted citations for articles published by faculty that had appeared in the previous five years. In the current study, citations that appeared in 2001-2006 were traced to articles that had been published as far back as 1981. This method of counting had the advantage of including \"classic\" long-lived articles. Again, the committee was unable to collect citation data for the humanities. Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. \u2022 The committee for the current study asked the institutional coordinators to name the programs they wished to include, but it did define a program as an academic unit that fits at least three of these four criteria: \u23af Enrolls doctoral students \u23af Has a designated faculty \u23af Develops a curriculum for doctoral study \u23af Makes recommendations for the award of degrees. 13 Because separate programs were being housed in different academic units, a few institutions used this definition to split what would normally be considered a program into smaller units that still met the criteria-that is, what is normally perceived as a unified program was ranked as separate programs. In the rating sample, however, only the one program judged to be the major program in the field at that institution was included. \u2022 Dimensional measures were not included in the 1995 study. In the planning meetings that preceded the study, the point was repeatedly raised that earlier rankings had not explicitly taken into account measures that reflected on graduate education 14 or the diversity of the educational environment. In summary, the current study differs in methodology and conception from the 1995 study. Both studies do provide rankings, however, the current study provides ranges of rankings, reflecting a variety of sources of uncertainty. In addition, are illustrative of two different approaches. There are other approaches and weighting of characteristics that reflect alternative user values. It is possible to try to compare the sets of rankings from the two studies, but the definition of faculty, methods of enumerating publications and citations, and the inclusion of additional characteristics in this have all changed, as well as the methodology."}, {"section_title": "CAUTIONARY WORDS", "text": "A Guide to the Methodology of the National Research Council Assessment of Doctoral Programs (2009) details the methodology used to create the rankings in the current study. As noted in the previous section, the methodology adopted in the current work is substantially different from that used to obtain the rankings described in the 1995 National Research Council report An Assessment of Research-Doctorate Programs: Continuity and Change, although it is very similar to that proposed in the 2003 NRC report Assessing Research-Doctorate Programs: A Methodology Study. Under the current methodology, when program measures in a field are similar, program differences in the range of rankings can be highly dependent on the precise values of the input data and very sensitive to errors in those data. The committee and the staff have worked diligently in recent years to ensure the quality of the measures used to generate ratings and rankings and have tried to reduce measurement errors as much as possible. Such errors can arise from clerical mistakes, from misunderstandings by 13 These were the criteria listed on the NRC program questionnaire. 14 In the 1995 study \"93E\" was a reputational measure of the effectiveness of the program in graduate education, but was very closely correlated with \"93Q,\" the reputational measure of scholarly quality. The committee felt it needed a separate measure based on data."}, {"section_title": "Reputational Measures", "text": "From the outset the committee, responsive to the statement of task, favored producing a large variety of measures that correlate with the quality of Ph.D. programs. Those measures included publications and citations, peer recognition in the form of honorific awards, and indicators of the resources necessary to create new knowledge. One measure rejected was the direct use of perceived quality, or reputational standing, of these programs, even though this measure was the principal one used in the 1995 study. At present there is widespread distaste in the academic community for the use of reputational measures. On the one hand, reputational measures are generally recognized to have many strengths, including subtlety and breadth of assessment and the widespread use of such markers. On the other hand, reputational measures may reflect outdated perceptions of program strength as well as the well-known halo effect by which some weak programs at a strong institution may be overrated. 15 On balance, recognition of these shortcomings resulted in the committee's decision to reject the direct use of these perceived quality measures. But the committee was divided: some members did not want to collect data on perceived quality at all, while others favored the direct use of reputation. The policy finally adopted was an intermediate one-to collect direct data on the perceived quality of Ph.D. programs only for a sample of programs in each field and from a sample of faculty members who had responded to the faculty survey that produced the \"direct measures\" of quality. The ratings that resulted were then correlated with the measured variables (such as citations, honors, and awards), and \"weights\" were obtained for the latter to best \"predict\" the reputational measures. The idea here was to benchmark objective measures against a reputational measure, but not to use the reputational measure itself. This was the procedure recommended in the 2003 NRC report, and it had numerous consequences, foreseen and unforeseen. Perceived quality, or reputation, is, of course, real, and it is real in its consequences. Reputation affects students' and professors' perceptions and their actions 15 However, weak programs at strong institutions may benefit from the presence of the stronger programs in an increasingly interdisciplinary environment."}, {"section_title": "Weights and Measures", "text": "The committee collected an unprecedented amount of useful data on Ph.D. programs. But to turn this set of discrete measures into an overall set of rankings, it had to combine the various measures into a summary measure, which required, in turn, decisions about how much weight to give to each of the measures. One obvious way to weight the different quality measures was to use faculty ratings of the importance of the measures in assessing overall program quality, and this method was one of the two(the S measure) used to derive the criteria for quality. However, because faculty were not asked to evaluate reputation as a quality, it was excluded from the summary measure constructed from the weighted average of strictly objective measures. An attempt to model reputation was made by conducting a rating exercise for a sample of programs and then relating these ratings to the same characteristics as were included in the \"S\" measure. The result was a measure of quality (the R measure) based on statistical modeling of the quality evaluations (the regression-based model) but made up of the same components as the direct measures. The measures of importance to the faculty were correlated with the perceived quality measure, suggesting that these two parameters describe valid measures of real program quality. The R ranking, then, reflects the relation of the subjective ratings to the data, but by relying entirely on objective data, even this measure, in effect, eliminated any subjective adjustments raters might make in the way they perceived the quality of specific programs, as contrasted with the application of rules they might apply to evaluate programs in general. Reliance solely on data-based objective measures rather than the explicit use of direct measures of reputation sometimes resulted in ratings that appeared to some committee members to lack face validity. To take one example of what may be lost when using only the objective data, faculty members in almost all fields in the sciences give a high weight to citations. Citations, however, are a complex indicator of impact or quality, and, of course, they are indirect measures of reputation. Their complexity arises from the equally complex pattern of behavior of scholars and scientists when referencing works in their published writings. The pattern of citations varies considerably by field, by specialty, by the use of books versus journals as the principal mode of scholarly communication, by self-citation practices, and by the decay of citation frequency over time, among many other patterned differences. Take, for example, two equally distinguished statistics departments: one heavily emphasizes statistical theory, the other, biostatistics. Every member of these departments is honored in a variety of ways. And yet it is almost certain that the average number of citations will be far greater in the department that emphasizes biostatistics, because it is a much larger field with a far larger publishing audience than statistical theory. Thus the score on citations as a measure of quality will differ greatly between the two departments and will lead to very different ratings. A reputational measure would have confirmed the point that both of these departments are truly distinguished. The ranges of rankings based on S measures and R measures differ in the degree to which subjective considerations enter. The ratings on which the R measures are based may depend on the subjective assessment of omitted variables for which there may be no quantitative measures. The omission of subjective considerations in the regression-based measure is treated as an error term in the regression equation and does not appear in the model values reported in the rankings. The result is ranges of rankings for some programs that deviate markedly from what many experts in the fields might find convincing when they take subjective considerations into account. By contrast, the S measures may be subject to variations resulting from incorrect or misunderstood reports of data. Users of these ranges of rankings need to be aware of the consequences of using purely objective measures and interpret the range of rankings in light of the major methodological differences between what was done in this study and what has been done previously."}, {"section_title": "Principles of Academic Organization", "text": "The interpretation of ranking ranges is further complicated by two other decisions that the committee made in designing the study. It decided to accept the respondent institution's principles of academic organization and to thus include multiple programs from the same university in the same program category if they met the criteria for a separate program and the university submitted the data for assessment. Each of these programs was rated separately, but they were all included in computation of the range of rankings. For example, Harvard has three doctoral programs under \"Economics,\" and Princeton has two doctoral programs under \"History.\" 16 Because the assessed quality of these programs tends to be similar, multiple programs from the same university could occupy multiple slots in a similar position in the range of rankings, thereby \"crowding out\" or reducing the rankings of other programs entering higher-ranking ranges and thus distorting the reported results. Another factor is that the committee's definition of a program to be rated did not always produce uniform definitions of comparable program areas, leading in some cases to results that are difficult to interpret in terms of ranges of rankings. For example, some mathematics programs include statistics and applied math, whereas others do not. Some anthropology departments include physical anthropology, while others do not. Perhaps at the extreme is the broad field of public health. Different subfields, such as biostatistics and epidemiology, are included as if they are the same program area, when clearly they are different in kind. This situation produces results that are difficult to interpret. When these differences were known, they are noted, but the reader should be alert when comparing specific programs to the possibility that they are not completely comparable."}, {"section_title": "Counting Citations and Publications", "text": "The committee initially chose to collect citation data for relatively recent publications produced by core and new faculty in each of the Ph.D. programs. This decision, however, tended to bias the data against Ph.D. programs with more senior scholars, particularly in the social and behavioral sciences and humanities, where the pattern of publication over a career differs considerably from those patterns in the physical and biological sciences and engineering. After this bias was noted, the committee decided to collect citations over a much longer timescale. Thus publications going back roughly 20 years, to 1981, in the science, social sciences, and engineering fields are considered in the citation count. This set of procedures can lead to a bias either for or against senior scholars, and without further research even the sign of the effect is uncertain. The situation is inherently complex."}, {"section_title": "Summary of Cautions", "text": "The cautions mentioned here are intended to alert readers to aspects of the methods used in this study that differ from those used in other studies, including ones conducted by the National Research Council. These innovations may produce ranges of rankings that surprise knowledgeable people in a field and contradict their views of the actual quality of specific programs. An examination of the data on individual variables for a program, together with the weights assigned to the different objective measures for each program should help to clarify the reasons for the specific rankings. The subtle, nonquantifiable variables that might make reputation more than the weighted sum of objective variables are not captured by the method adopted by the committee. In view of these limitations to the methods for obtaining ranges of rankings, some members of the committee remained skeptical that these results capture fully the relative quality of the doctoral programs in certain fields. For this reason, they should be used as illustrative. In general, the range of rankings captures well the relative standing of most Ph.D. programs. Some outliers, however, cannot be explained by the data in hand, and it may be that had more robust measures of reputational standing, or perceived quality, been used, these anomalies might be better understood or might have disappeared. Therefore, the committee suggests that anyone making strong comparisons with the 1995 rankings using either the R or S measure be cautious. Such comparisons can lead to a misinterpretation of the \"actual\" rankings of programs, however they might be defined. It would be especially misleading to overstate the significance of changes in rankings from the 1995 NRC report in view of the differences in adopted methodologies. Finally, it is useful at this point to return to the topic of simple errors in the input data, because this is the most serious problem with which the staff and the committee wrestled. At a very late stage in its work the committee undertook a final set of \"sanity checks\"; the fields were divided up, and groups of academic fields were assigned to individual committee members to see if they could identify any anomalies in areas with which they were familiar. Many anomalies were found and were addressed, but surely some must have escaped notice. The committee thus urges readers to use the illustrative ranges of rankings with caution. Small differences in the variables can result in major This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. differences in the range of rankings, especially when a program is very similar on other measures to other programs in its field. But individual instances of programs that should have been ranked considerably lower or significantly higher than the tables indicate may emerge, and so it is strongly recommended that the rankings of individual programs be treated with circumspection and caution and analyzed carefully. Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010."}, {"section_title": "Faculty Values as Reflected in the Two Illustrative Rankings", "text": "This study is valuable for both the comparative data it makes available and the importance it attaches to some of the collected data by conducting a survey of faculty and relating program ratings to measured characteristics. The values used throughout this report, for the two overall rankings and, taken separately, for the dimensional rankings, derive in part from faculty members' answers to questions designed to measure faculty perceptions of the relative importance of program characteristics to the quality of doctoral programs. The 21 characteristics identified by the committee and in the literature as important were divided into three categories are shown in Box 5-1. Percentage of entering students who complete a doctoral degree c. Time to degree d. Placement of students after graduation e. Percentage of students with individual work space f. Percentage of health insurance premiums covered by the institution or program g. Number of student support activities provided at either the institutional or program level b a The committee initially believed this variable should be included, but later found no data to support it. It was eliminated from the calculation of weights, and the data tables in this report include 20, not 21, variables. b This variable is a tally of whether the following services are provided to graduate students at either the institutional or program level: orientation for new students, prizes or awards to doctoral students for teaching or research, formal training in academic integrity/ethics, travel funds to attend professional meetings, grievance and dispute resolution procedures, annual review of all enrolled doctoral students, training to improve teaching skills, institutionally supported graduate student association, information about employment outcomes of graduates, and on-campus graduate student research conference. Faculty respondents were asked to choose up to four characteristics in each category that they thought were important. They were then asked to indicate which one or two of the four they found the most important. The final task was to assign an importance score of from 0-100 to each category; the sum of the importance scores over all categories was to equal 100. 1 The five characteristics given the highest rating on each measure are shown in Table 5-1. Specifically, it shows the average ranking of the characteristic in the field across all 20 measures. 2 This table makes the differences in the two rating methods clear. On the general survey (S measure) in all fields, the publication measure was very important. It was less important in the regression-based R measure, where for all fields size, as measured by the average number of Ph.D.'s was important. The percentage of faculty with grants was highly ranked on the S measure in all fields but the humanities. Awards per allocated faculty, a measure that may reflect reputation, was important in all fields but the agricultural sciences, and it was highly ranked for both the R and S measures in three of the five broad fields. None of the diversity measures appeared to be important in either methodology. GRE scores were important for R measures, but not for S measures, while placement of students in academic positions was important for S measures, but not for R measures.    "}, {"section_title": "DIMENSIONAL MEASURES", "text": "Despite the relatively moderate importance that faculty placed on the student treatment and program diversity dimensions of doctoral education, the committee felt it was very important to measure and discuss these dimensions, in part because they have figured prominently in national discussions of doctoral education. 3 The dimensional measures were obtained by means of the faculty responses to Section G (see Table 5-1). 4 These measures take a subset of all the characteristics and recalculate the weights so that the total of the weights for the subset adds up to 1. The dimensional measures used in this study-research activity, student support and outcomes, and diversity of the academic environment-are described in the sections that follow."}, {"section_title": "Research Activity", "text": "This dimensional measure relates to the various ways in which to gauge the contribution of research: publications, citations (except for the humanities), the percentage of the faculty holding research grants, and recognition of scholarship as evidenced by honors and awards. The importance weights are shown in  allocated faculty member, 5 average citations per publication, percentage of core and new doctoral faculty respondents holding grants, and awards per allocated faculty member. 6 Publishing patterns and the availability of research funding and awards for scholarship vary by field, but the weight placed on publications per faculty member is remarkably consistent-about 30 percent-across fields. Research activity is the dimensional measure that most closely tracks the overall measure of program quality, because in all fields both the S measure-based on abstract faculty preferences-and the R measure place high weights on these characteristics. For the research activity measures, faculty in the sciences and engineering place the greatest weight on grants per faculty member. In some fields research funding is common, and grants are an important source of support for faculty and doctoral students. In the social and behavioral sciences and the humanities, the greatest weight is placed on publications. In one social science discipline, economics, the weight placed on citations is almost equal to that placed on publications, but in all other fields publication is more highly valued. Grants are a less important source of funding in the humanities, and for those fields publications and awards are the most important visible signs of research activity. The values evinced in the broad fields are, with the exception of the humanities, very similar."}, {"section_title": "Student Support and Outcomes", "text": "This measure combines data on the percentage of students fully funded in the first year, the percentage of students completing their degrees in a given time period, time to degree, and placement in academic positions (including academic postdoctoral positions). The committee found that faculty typically placed a larger weight on student support and completion rates than on median time to degree or academic placement. 7 Surprising uniformity appears across broad fields on the weights, which are shown in Table 5-2B. For the humanities, eight years are used in the completion measure. This completion measure is measured as the fraction of the entering cohort that has received a Ph.D. within six or eight years. b Time to degree has a negative weight reflecting that a shorter time is better. The sum of the absolute values of the weights is 1. The percentage of graduates obtaining academic positions dominates these measures, and, interestingly, the weight given to this variable (0.34 -0.37) is essentially the same in all of the broad academic fields. The negative sign on time to degree indicates that the shorter the time to degree, the better. Student support in the first year is also an important variable in all fields. Percentage of completion and time to degree are less important, and although these variables have been discussed within the community of graduate deans, they are not variables that faculty feel are important in determining the quality of a doctoral program."}, {"section_title": "Diversity of the Academic Environment", "text": "The diversity measures-percentage of faculty and of students from underrepresented minority groups, percentage of faculty and of students who are female, and percentage of students who are international (that is, in the United States on a temporary visa)-did not appear to be major factors in determining the overall perceived quality of doctoral programs. 8 When these measures are taken separately, definite patterns emerge for variables that faculty thought were more important, and these patterns vary by field. Most PREPUBLICATION COPY-UNEDITED PROOFS 71 fields place the highest weight on the percentage of students from underrepresented minority groups. In the biological and health sciences, social and behavioral sciences, and humanities, relatively high weights are also placed on the percentage of faculty who are underrepresented minorities. The percentage of international students was not highly weighted relative to the other diversity weights, except for the physical and mathematical sciences. These weights, by broad field, are shown in Table 5-2C. The preferences of faculty in the broad fields are very similar across fields. The physical and mathematical sciences place a greater weight on the percentage of students who are female than the percentage of students who from a underrepresented minority. This weighting is reversed for the other fields. None of the fields places a large weight on faculty diversity, although generally a slightly higher weight is placed on the percentage of faculty who are female. The physical sciences and engineering and, to some extent, the social sciences faculty indicate that a higher percentage of international students is beneficial and important to program quality. The relatively high weight for this measure for the humanities reflects high weighting in the foreign language fields and comparative literature."}, {"section_title": "SUMMARY OF THE FINDINGS", "text": "The findings of the committee fall into three areas: 1. Indicators of research activity are of the greatest importance to faculty in determining program quality by means of the S measures, which are based on the program characteristics that faculty say explicitly are important. In many cases program size is very important when quality is measured by the regression-based, or R measures. 2. Of the student support and outcome characteristics, placement in an academic position and support in the first year are highly weighted. Completion rates and time to degree are not. 3. Faculty view student diversity as important, when considered with other diversity measures, but not as a direct predictor of overall program quality. Some Uses of the Data Students and their families, faculty members, administrators, boards of higher education, trustees, and departments of education, both state and federal, as well as private sector employers and policy analysts and scholars, among others, should find the data in this study of interest. This chapter provides examples of some uses of the study data."}, {"section_title": "POTENTIAL USERS", "text": ""}, {"section_title": "Faculty and Administrators", "text": "It is hoped faculty and administrators will look at the data, ask what characteristics are important to the purpose at hand, and rank programs accordingly. They may, however, want to look at the illustrative R and S ranges, which are after all constructed from faculty opinions, and try to understand where their programs fall in these illustrative rankings. A detailed understanding of the generation of the rankings of a single program in biochemistry can be obtained in part through Table 6-2, which shows the overall and dimensional rankings for a program in that field of study. It is natural to ask at this point: where did these rankings come from? Table 6-3 provides the details for this program, which can be obtained by clicking on the link in the online spreadsheet. Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate  a Because of the Monte Carlo technique used to generate the ranking ranges, in some cases the R95 (or S95) standardized measure may be smaller than the R05 (or S05) measure for a particular variable. Table 6-3 shows the standardized values of the variables for the particular estimation that resulted in the program's rank for the 5th percentile and the 95th percentile for each methodology. Uncertainty has been taken into account for the value of each variable, and 500 separate sets of half-samples of the raters have been selected for each measure. The values of the variables have been selected from a random distribution within which the variable values can vary by plus or minus 10 percent or by the extent of their actual variation, if data were collected on that variation. 1 It is apparent from Table  6-2 that the range of S rankings is larger than the range of R rankings for this program, and that it does especially well on the research activity dimensional measure and less well on the other two measures. Closer examination reveals that the coefficients on both measures place a high weight on the research characteristics, where this program is well above average. Administrators with budget allocation responsibility will be particularly interested in an analysis of all the programs of one university or in one division of a university. Keeping in mind that each discipline is different, administrators may find the NRC data useful in making allocation decisions designed to shorten the time to degree, for example, or to improve completion rates in particular programs, or to enhance support for faculty research. But, again, the NRC data are just a start, and universities will want to supplement and update them. Comparison of a given program with those in a higher echelon will likely indicate that ratings will improve if the research impact of the faculty improves. Although hiring \"stars\" is one obvious solution, another is to enable faculty to increase their research productivity by seeding new research programs or giving faculty more time to spend on research. These data provide some quantitative measures to inform decisions about the balance between research and teaching for a given program. But, again, the NRC dataset is only part of the information needed for budget allocations. Indeed, the NRC data are only one way of measuring a doctoral program, and in all fields the ranges of rankings are heavily influenced by metrics about the research productivity of the faculty in terms of publications and citations as well as grants and awards. No effort has been devoted to assessing the outcomes of graduate education or to determining the effectiveness of the doctoral research experience in preparing students for a life of scholarly inquiry. Nor has effort been given to measuring or assessing the need for doctoral studies in any given area, or of the career outcomes of students who follow any particular course of study. These considerations also influence administrative decisions about resource allocations to graduate programs. Additional central university support may target not the highest-ranked programs or the lowest, but rather the units best equipped to use additional funding in a productive fashion."}, {"section_title": "Boards of Higher Education and Trustees", "text": "As the ultimate authority on which doctoral programs should be offered by a university or universities in each state, boards of higher education and boards of trustees are ultimately responsible for the quality of doctoral programs. They are also aware of the full range of doctoral programs in a state or university. They may find that a program does not rank highly, but it plays an important role in producing Ph.D.'s who well serve an industry that is important to the state's economy. Some programs offered in the same field by various institutions may display a measurable quality differential that is supported by differences in one or more selected characteristics, thereby indicating that consolidation could be the answer. However, resolving such matters requires fine-grained study of the programs and more detailed analysis than can be provided by the NRC data alone."}, {"section_title": "Private Sector Employers", "text": "Academia is no longer the majority employer of Ph.D.'s. They are employed by industry, nonprofit organizations, government, and consulting firms. Rather than just the programs with the highest rankings, these employers may wish to look for programs that tend to aim at the nonacademic sector, as well as programs that are more diverse and focused, as evidenced by higher completion rates and shorter times to degree."}, {"section_title": "Policy Analysts and Scholars", "text": "The data from this study contain a wide selection of information needed for policy analysis of graduate education. Additional data from the program, student, and faculty questionnaires will be made available in a public-use dataset. How to obtain access to the dataset will be announced following release of this report. These data will contain tabulations of the answers to most questions on the questionnaires aggregated by program, provided that this aggregation preserves individual confidentiality. Upon publication of this report, the analytic tables showing the derivations of the rankings for each program, and the master tables for programs by discipline, will be available. If a researcher needs individual-level records, these will be made available to researchers who agree to respect the confidentiality of respondents and sign a confidentiality agreement with the NRC. Researchers are warned, however, that the data not shown in the master tables have spottier response rates."}, {"section_title": "A WORKSHOP ON ANALYTIC USES OF THE DATA", "text": "A workshop on the ways in which universities and researchers have used the data will be held six months after the release of the report and data tables. The committee believes it is very important that ways of analyzing the data become widely known throughout the graduate and higher education research community and to enhance their usefulness."}, {"section_title": "7", "text": "The Data and Principal Findings All doctoral programs have similarities. Typically, they admit students, maintain a curriculum of study, adhere to a \"certification\" requirement (preliminary or comprehensive examinations), and require completion of a piece of original work that demonstrates the ability of their students to conduct research that advances the state of knowledge. But that is where the similarities end. Depending on the discipline, a student may spend years perusing archival materials, or conducting fieldwork, or working closely on a problem as part of a research team based in a laboratory. Thus each discipline has its own ways of educating doctoral students. It makes sense, then, to examine doctoral education in each broad field or discipline separately. It does not make sense to compare time to degree in anthropology, with its years of fieldwork and close observation, with that for biochemistry, where a student typically works in a laboratory on a problem until it is solved. Within broad fields, however, many programs may exhibit similar characteristics and faculty preferences. Thus because this study covers large numbers of programs, the committee chose to summarize the data for broad fields rather than for individual disciplines. The committee also found similarities in particular characteristics among programs in a discipline. For example, larger programs in a discipline may have more resources for research, but provide a more impersonal environment for students than the smaller ones. Across disciplines, there may be fewer resources available for student support at public universities than at some private ones and, as a result, students in programs in these institutions may take longer to complete their studies, even in the same field. This chapter examines the program data in each broad field in an attempt to discover commonalities and differences. The focus here is on the variables that can be affected by administrative decisions or changes in program practices. To convey a sense of how the doctoral education enterprise has changed since the 1995 study (for which data were collected in 1993), this chapter presents data for programs in four broad fields 1 that were included in both the 1995 study and the current study (designated here as \"common programs\"). It then moves to a discussion of the 2006 data on characteristics of all programs in the study grouped by broad field and classified by the following broad groupings of variables: research and faculty productivity, student support and outcomes, and diversity. The chapter concludes with a description of findings from the faculty and student questionnaires. 1 Agricultural sciences were not included in the 1995 study. The biological and health sciences field definitions changed too much to be strictly comparable. In the other broad fields, only the disciplines and programs included in both studies are included."}, {"section_title": "CHANGES IN DOCTORAL PROGRAMS BETWEEN 1993 AND 2006 2", "text": "As described in Chapter 3 of this report, the changes in the eligibility criteria and definition of doctoral faculty render comparisons between the two studies inexact. It is possible, however, to match programs that were in both studies and see how the characteristics that were measured in both have changed. Table 7-1 displays these results for engineering, the physical and mathematical sciences, the social and behavioral sciences, and the humanities. The number of programs grew in all fields and in all disciplines common to the two studies. 3 Among the comparable broad fields, the greatest growth in number of programs was in engineering, with bioengineering a relatively new field in 1993, leading the way. The only field in which the number of programs declined was aerospace engineering. The social and behavioral sciences were next in growth of programs. Geography, a field revolutionized by the availability of satellite information, experienced the greatest percentage of growth in programs, and psychology experienced the greatest growth in number of programs. Although the humanities as a whole saw an increase of 60 programs, declines in the number of programs were most prevalent in the humanities, especially in English and the foreign language fields. The exception was Spanish language and literature. The number of programs in history grew. The physical and mathematical science expanded by 90 programs, and the greatest increase was in the earth sciences, which added 40 programs. As for the programs common to both studies, the average number of faculty in the common programs increased in all broad fields, but by far the highest growth over the period was in engineering (82 percent). This growth was experienced to some extent by all the fields, but by biomedical engineering and chemical engineering in particular. The growth in interest and funding in bioscience has fueled much of this change. Faculty per program in all other broad fields grew by more than one-third. The only field in which average faculty declined was music (-13 percent). 4 Mathematics appears to have been a slow-growing field (9 percent), but this finding may be stem from the committee's decision in the 2007 survey to treat applied mathematics as a separate field. Table 7-2 summarizes these data for common programs. The size of programs, as measured by Ph.D. production, generally displays a skewed distribution. A substantial fraction of Ph.D.'s is produced by a much smaller fraction of programs. Consequently, although the average sizes of programs as measured by either Ph.D.s or enrollments are interesting, their significance for changes in individual program size is not always clear. The changes in program size as measured by enrollment and by faculty are shown in Tables 7-3 and 7-4. The changes in average measures of size for the common programs-Ph.D.'s per program and enrollment-are much smaller in magnitude than changes in the average number of faculty. The growth in the average number of Ph.D.'s per program in all broad fields was generally modest, less than 15 percent over 13 years for all the broad fields except the humanities. Notable field exceptions were music (-8 percent), earth sciences (-34 percent), physics (-6 percent), and linguistics (-9 percent). In all other fields the average number of Ph.D.'s per program increased. Such increases can be achieved in two ways: programs grow in size, or programs shorten the time to degree, thereby producing more Ph.D.'s in the same time period. Although good data are not available from the earlier study, there is no evidence that the time to degree has diminished significantly. It seems likely, then, that the existing programs increased enrollments. A more complete analysis of the faculty-to-student ratios in the institutions that produce most of the Ph.D.'s would be required to ascertain whether there has been much of a change for most students. It is possible that most of the growth in faculty has occurred in institutions that produce relatively few Ph.D.'s. 5 5 The difference in how the two studies defined doctoral faculty was discussed earlier. If anything, the definition in the 2007 survey was more restrictive than the definition used in 1993so we can be sure growth occurred but cannot be certain of its magnitude."}, {"section_title": "Growth in Postdoctoral Scholars", "text": "One of the major changes in the academic research enterprise since the last study is the increase in the number of postdoctoral scholars, primarily in the sciences. 6 Data on postdocs were not collected in the 1995 study; however, it is now clear that, especially in the biological sciences, these young scholars play a major role both in research and in the mentoring of Ph.D. students. In many respects they share some of the roles of both advanced graduate students and faculty. Thus, because some faculty time is spent on the education of postdoctoral scholars and part of postdoctoral scholars' time is spent on the education of graduate students, any interpretation of the NRC data on student-to-faculty ratios, for example, should consider whether the increased number of faculty directly affects Ph.D. student mentoring. This study addresses doctoral education specifically, so it does not address the participation of postdoctoral scholars. However, the presence of substantial numbers of postdoctoral scholars changes the context of graduate education, especially the Ph.D. research experience. The number and therefore the impact of postdoctoral scholars differ significantly across disciplines and size of programs-being more prominent in the sciences and much less so in the humanities. Table 7-6 shows the number of postdocs by broad field in 2006. The greatest impact of postdocs is clearly in the biological and health sciences. "}, {"section_title": "Changes in the Diversity of Programs", "text": "For all the common programs, with the exception of classics (-5.2 percent) and linguistics (-2.5 percent), in all broad fields, the percentage of enrollment for women increased. For the broad fields the absolute numbers of women enrolled also grew. Increases of greater than 10 percentage points were found in several engineering fields (biomedical, chemical, civil, and mechanical), several fields in the physical sciences (astrophysics, earth sciences, oceanography, and statistics), and one field in the social sciences (economics). All these fields had relatively low levels of female enrollment in 1993. Data on racial and ethnic diversity were not collected in 1993, but the NSF data shown in Figure 7-1 reveal a considerable increase in minority Ph.D.'s across the board. However, in some fields, especially engineering, the social and behavioral sciences, and the physical and mathematical sciences, the numbers of non-underrepresented minorities (Asian Americans and whites) have been declining. Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. In summary, the 13 years from 1993 to 2006 saw an increase in the number of doctoral programs in the common broad fields and disciplines, growth in the numbers of faculty and students per program, expanded production of Ph.D.'s per program, and an increase in the gender and ethnic diversity of programs. These quantitative changes were accompanied by changes in the average faculty-to-student ratios, which increased significantly over this period. This chapter now turns to a more detailed description of doctoral education in 2005-2006."}, {"section_title": "U.S. DOCTORAL PROGRAMS IN 2006: A DESCRIPTION", "text": "Although this study does not include all doctoral programs or all fields, it does cover the vast majority of research doctorate programs in the United States. 7 This section begins by describing the characteristics of the programs in the study. Of particular interest is the size of programs in these fields and the type of control (public or private). The section then moves to comparing and contrasting the fields along these dimensions."}, {"section_title": "Coverage", "text": "The programs with rankings in this study account for approximately 90 percent of the doctorates in the fields included in the study taxonomy in 2006. A comparison with NSF's Doctorate Record File (DRF) by broad field is shown in Table 7-6. In view of these high rates of coverage, the generalizations drawn from the study sample can, in all likelihood, be applied to U. S. doctoral education as a whole. a Integrated biological science, which appears in the NRC biological and health sciences taxonomy (Appendix B), is not a DRF field. The 867 Ph.D.'s from these programs were probably spread across the NSF biological sciences taxonomy. b Universities were asked to exclude programs in clinical and counseling psychology. The exclusion of these programs accounts for the low rate of coverage in the social and behavioral sciences. The coverage rate for all social science fields, excluding psychology, is 89 percent. Excluded from the NRC study are Ph.D.'s in professional fields as well as small fields and programs. The professional fields are excluded for historical reasons. The study originally included programs in the liberal arts and sciences. The committee then expanded this coverage to include fields outside the arts and sciences and Ph.D. programs in schools of medicine that award a research Ph.D., but professional fields with a substantial practice component were still excluded from the study on the grounds that publications in scholarly journals are not an adequate metric of the quality of these programs. 8 Small fields and small programs in the humanities were excluded because they provide too few observations for reliable statistical analysis."}, {"section_title": "Size and Control", "text": "For all fields, most doctoral programs, most enrolled doctoral students, and most Ph.D.'s are found in public universities. Programs in public universities are typically larger than those in private universities, and there are far more of them. Seventy-one percent of the programs ranked in the NRC study are in public universities. The proportion of programs in the universities with the largest programs is similar (70 percent). Among the 37 universities that produced 50 percent of Ph.D.'s from 2002 to 2006, 70 percent were public (see Table 7-7). Although public universities rely increasingly on nonpublic sources of funding, cutbacks in public funding for universities has a powerful effect on doctoral education simply because of how many large Ph.D. programs exist in public universities. These cutbacks will, of course, affect public higher education in general. "}, {"section_title": "Importance of Program Size", "text": "After release of the 1995 study, some readers, and the report itself, commented on how important program size was to the ranking of a program. 9 As mentioned in Chapters 5 and 6, the coefficient on size is especially large in most fields in the regression-based ranking (R ranking), and generally far less prominent in the survey-based ranking (S ranking). One example of an analytic use of the data is an investigation of the characteristics of programs as they relate to size as measured by number of Ph.D.'s. The committee divided programs in each field into size quartiles, 10 grouped the fields into broad ones, and investigated whether the quartile with the largest programs also ranked high on the 20 characteristics. It found that this quartile has the highest levels of publications per faculty member. Citations also follow this pattern, although the dominance of the largest quartile programs is not significant for engineering. In line with the findings for the other research variables, the largest programs also have a significantly higher average number of awards per faculty member. The greater research productivity in the largest quartile occurs even though our measures of research activity are on a per capita basis. These values are shown in Table 7-8."}, {"section_title": "Student Variables and Program Size", "text": "Findings for the student variables are shown in Table 7-9. Measures of interest to students include the annual average number of Ph.D.'s, GRE scores, completion rates, time to degree, employment destination upon graduation, and whether the program keeps track of its graduates after graduation. In the larger programs students have higher GRE scores, and there is a higher proportion of students with external funding than the average. The large humanities programs, on average, have lower GRE scores and longer times to degree, but a greater than average percentage of students who complete their degrees within eight years. Placement of Ph.D.'s in academic positions does not differ significantly by size quartile of programs. 11 This percentage is higher for the humanities than for any other broad field. Interestingly, the time to degree is significantly longer by about six months in the larger programs, although completion rates do not seem to vary with size. Finally, the largest physical science and social science fields have a higher percentage of programs that collect placement data for their students. The fourth quartile is different from the average at the p= 0.05 level. b Publications in the humanities are measured by books and articles with books given a weight of 5 and articles a weight of 1. Humanities publications are not comparable to publications in other fields, which are measured by articles. Among broad fields overall, GRE-Quantitative Reasoning scores are higher, as expected, in engineering and the physical and mathematical sciences than in other fields. The percentage of students with first-year support is greater than 75 percent in all fields and is nearly 90 percent in the physical and mathematical sciences. Also notable is the higher proportion of the graduates with definite plans to take academic positions in the humanities and social and behavioral sciences than in the other broad fields. The data on completion rates and average time to degree raise important questions about the proportion of students entering doctoral programs who actually complete a degree. The completion rate in six years ranges from nearly 60 percent (agricultural sciences) to 36 percent (social and behavioral sciences and yet the median time to degree, only for those who complete their degrees, has a narrower range (4.8-6.1 years). In the humanities, where 36 percent of enrolled students complete their degree in eight years and the median time to degree is 6.1 years, it can be inferred that a very high proportion of humanities students who enter doctoral programs never complete a Ph.D. degree. The factors that influence attrition rates and student success in research doctorate programs are certainly worthy of ongoing attention. a For the humanities, the time to completion is eight years and the GRE score is for GRE-V. b Indicates that the largest quartile value differs from the average at the p = 0.05 level. Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010."}, {"section_title": "FINDINGS", "text": "The spreadsheet online at http://www.nap.edu/rdp contain a vast amount of data that could be characterized, mined, and modeled. With the previous discussion, the committee offers only a glimpse into the descriptive richness possible from analyzing the data available for many of the programs in 59 fields 12 . To illustrate one possible analysis, it looked at the characteristics associated with program size. Program size is positively associated with most measures of the research productivity of doctoral programs, even when productivity is measured on a per capita basis. 13 As for student characteristics, the larger programs are also more likely to have higher average GRE scores, except in the humanities. There is a size difference for median time to degree; students in the larger programs take about half a year longer to complete their degrees. In the physical and social sciences a significantly greater percentage of large programs collect outcomes data for their students. Interestingly, size, analyzed within broad fields, does not appear to be associated systematically with the percentage of students with support in their first PREPUBLICATION COPY-UNEDITED PROOFS year, which is high across the board, or completion rates, or the percentage of students who plan on a position in academia (including postdoctoral study) after graduation. Readers should note that the committee has been careful to discuss association, not causation. The committee also looked at racial and ethnic and gender diversity. It found that much less diversity is found in the physical and mathematical sciences than in other broad fields. Although some fields have succeeded in becoming more diverse, most programs still have percentages of underrepresented minority students that are far under 10 percent. That said, although the percentages remain very low, they are in all cases significantly higher than were indicated by the NSF data from 1993 and discussed in the previous section. Thus doctoral programs have achieved far greater diversity with gender. This completes the discussion of examples of simple analyses that can be conducted using the program data. We now discuss a few findings from data obtained from the faculty questionnaire and the student questionnaire. Data from these questionnaires will be made available with the public use data set."}, {"section_title": "CHARACTERISTICS OF THE FACULTY OF THE DOCTORAL PROGRAMS", "text": "This report has focused primarily on information from the doctoral program questionnaire, but two other questionnaires-those directed at faculty and at students-also provide interesting insights into doctoral education. Although in the faculty questionnaire there were variations in the response rates by field, the overall response rate was 88 percent, and so it is likely that generalizations to doctoral faculty can be made from the survey responses. This section focuses on three areas: (1) the age profile of the faculty, (2) the length of time at their current institution, and (2) their demographic composition. Age is important because younger faculty are typically very active in research, although this activity may not yet translate into large numbers of publications or citations. Time at the current institution is a reflection of faculty turnover-the longer faculty stay in place, the less the turnover. The importance of demographic composition was discussed earlier. Selected data on faculty are shown in Table 7-13."}, {"section_title": "DATA FROM STUDENT QUESTIONNAIRES", "text": "The 4,838 ranked programs in the study include 236,417 students. Despite the high cost of sending questionnaires to all these students, the committee believed that the voices of the students should be heard. It surveyed students in disciplines in five of the broad fields: engineering, the physical and mathematical sciences, the biological and health sciences, the social sciences, and the humanities. The specific fields chosen were chemical engineering, physics, neuroscience, economics, and English. Each of the programs in these fields was asked for the names and e-mail addresses of their students who had been advanced to candidacy but had not yet completed their degrees. This group was chosen because the committee believed these students would have experienced many of the program practices and would have formed views of their doctoral programs. Confidentiality concerns prevent the committee from reporting results by program for the smaller programs. However, more than 90 percent of the responding students were in programs in which more than 10 students responded and thus could be reported on a program-by-program basis. In all, the responses that were reportable at the program level represented about 64 percent of all the programs in the five fields, and for the participating programs the student response rates were high. The response rates and other details of the student survey are reported in Table 7-14.  Table 7-15, which summarizes the results for each of the surveyed fields, reveals that most students are satisfied with their program. In all fields the percentages of programs whose students are not satisfied are less than 10 percent. On the whole, students value the intellectual environment provided by their programs. The main characteristic that receives a low rating from students is \"quality of (program-sponsored) social interaction.\" In the sciences and engineering, students report being highly satisfied with the quality of the research facilities available to them. Computing facilities are satisfactory in all fields, but students in programs in English and economics appear more critical of the research facilities and work space available to them. Investigators should look into the programs in these two fields in which students failed to say that research facilities were excellent or good. In fact, for the programs with more than 10 respondents, the questionnaire results may point to a follow-up agenda. The relatively low ratings in English and economics may indicate inadequate library facilities or inadequate support of other scholarly infrastructure. However, the survey did not collect data at this level of detail. PREPUBLICATION COPY-UNEDITED PROOFS"}, {"section_title": "Student Satisfaction", "text": ""}, {"section_title": "Student Productivity", "text": "Generally, the programs in all fields seem to be performing well in encouraging students to become productive scholars. As shown in Table 7-16, well more than half the students in all fields have presented papers at conferences on campus, and a similarly high proportion has presented papers at national or regional meetings, even if only a smaller proportion found funds for their travel. A high proportion of students in the science and engineering programs also report that they have published articles in refereed journals, but less so in economics and English. In all fields the percentage of students who have published either articles or book chapters has risen since they enrolled in doctoral study. It is clear from Table 7-16 that students produce papers in refereed journals while studying in their doctoral programs. The data for individual programs (not shown) reveal that in more than 90 percent of the individual programs in all five fields at least one student had published in a refereed journal. "}, {"section_title": "Advising and Academic Support", "text": "Assessment of student academic progress appears to be the norm in neuroscience. In all but nine programs surveyed, more than 75 percent of the students responding indicated that their programs provided an assessment of students' academic progress. Although not all fields reported this level of assessment, a high proportion of students in all fields indicated that they valued the assessments they did receive. A high proportion of students in virtually all programs also indicated that they received timely and helpful feedback on their dissertations. Doctoral education is characterized by the apprenticeship of students to mentors and advisers. For this reason a students' evaluations of their relationship with the faculty is both interesting and important. Across the five fields surveyed, about 50 percent of the students in all fields reported that they had highly interactive and supportive mentors and advisers. This uniformity is striking considering that students in the sciences and engineering might be expected to have more sustained interaction with faculty in laboratory settings. Interaction with other faculty members appears very limited. This finding was also consistent across the fields surveyed (see Table 7-17 for the results).  Chemical  engineering  80  50  90  90  90  50  20  Economics  80  60  80  90  90  50  20  English  90  50  80  80  80  50  20  Neuroscience  92  87  90  87  86  51  24  Physics  83  52  81  81  79  50  19 Doctoral students enter programs with career goals in mind, but in most fields that were queried these goals undergo modification during the course of graduate study. Doctoral students learn what kind of scholarly work they enjoy, and they also learn how good they are at it. With the exception of chemical engineering students, who were most likely to select careers in the private sector, most students anticipated a career in the education sector as they began doctoral study. But this interest tended to wane during graduate school, as students appeared to explore options in government or the private sector. Advisers and mentors are students' principal sources of career advice. Only students in chemical engineering reported making much use of university career centers. Students generally indicated that their advisers supported their career plans."}, {"section_title": "Looking Ahead", "text": "The charge to the committee for this study was the following: An assessment of the quality and characteristics of research-doctorate programs in the United States will be conducted. The study will consist of (1) the collection of quantitative data through questionnaires administered to institutions, programs, faculty, and admitted to candidacy students (in selected fields), (2) collection of program data on publications, citations, and dissertation keywords, and (3) the design and construction of program ratings using the collected data including quantitatively based estimates of program quality. These data will be released through a Web-based, periodically updatable database and accompanied by an analytic summary report. Following this release, further analyses will be conducted by the committee and other researchers and discussed at a workshop focusing on doctoral education in the United States. The methodology for the study will be a refinement of that described by the Committee to Examine the Methodology for the Assessment of Research-Doctorate Programs, which recommended that a new assessment be conducted. This study has completed the tasks specified in the charge, but they proved far more difficult and, as a result, took much more time than the committee initially anticipated. In this concluding chapter the committee looks at a few lessons learned from the conduct of the study and at other areas it has not fully explored and encourages researchers to use the study data to go farther."}, {"section_title": "LESSONS LEARNED", "text": "While conducting this study, and creating an unparalleled database on doctoral programs in 2005-2006, the committee learned many lessons about the data-based approach to describing doctoral education in the United States. These lessons are in the areas of taxonomy and multidisciplinarity, measurement, and the data-based construction of measures of perceived quality. In addition, the committee has areas that would be of great interest-such as the dimensional measures and the relation between postdoctoral scholars and doctoral study-that it did not have the time to investigate and on which it recommends further work."}, {"section_title": "Taxonomy and Multidisciplinarity", "text": "Although most doctoral work is still organized in disciplines, scholarly work in doctoral programs increasingly crosses disciplinary boundaries in both content and methods. The PREPUBLICATION COPY-UNEDITED PROOFS committee tried to identify measures of multi-and interdisciplinarity, but it believes it did not address the issue in the depth deserved, nor did the committee discover the kind of relation, if any, between multidisciplinarity and the perceived quality of doctoral programs. It therefore recommends that greater attention be paid to the relationship between multidisciplinarity and program quality the next time this study is undertaken."}, {"section_title": "Measurement", "text": "The validation of program data was a time-consuming process. The committee hopes that, based on the collection of data for this study, programs will better understand and have an easier time providing data for a future study. In particular, there should be greater clarity about what is meant by core faculty for a doctoral program and associated faculty. This distinction was made to prevent overcounting the productivity of faculty who are involved with multiple programs. In any case, techniques to check data are now in place and it is essential that they be further developed before the next survey is initiated and that instructions to the data providers be clear. Such steps could shorten the data validation process substantially."}, {"section_title": "Data-Based Construction of Measures of Perceived Quality", "text": ""}, {"section_title": "Ranking Programs", "text": "Initially the committee was deeply divided on whether an effort to rank programs should be undertaken at all. However, there was universal agreement within the committee that efforts that relied entirely on reputation or on single measures of scholarly productivity could be misleading to potential applicants and others. The quality of reputational measures depends critically on who is asked and how knowledgeable they are about scholarship in a discipline. Thus the committee focused on doctoral program faculty, who are presumably engaged both in scholarship and in hiring decisions that involve judgments of the scholarly quality of programs other than their own. The committee surveyed these faculty members about the factors they thought were important, ideally, to the quality of a doctoral program. A sample of them was then asked to rate actual programs, as described in Chapter 4. This \"anchoring\" rating study was a compromise. The committee sampled programs to ensure that a broad range of programs was included in the rating sample, but raters were more informed about program characteristics than in the 1995 study. The committee did not compare rating results from the two studies because the methodological differences were too great and the committee could not justify using the 1995 study as a benchmark. The committee also wanted to convey the degree of uncertainty in rankings. Very early in the study process the committee agreed that presentation of ranges of rankings would best convey the uncertainty inherent in any ranking study. It felt that a technique that combined the regression results with the survey results would give a more accurate estimate of program quality. The anchoring study, however, was based on relatively small samples of programs, and the committee found that the estimates of the ranges of rankings based on regression (R rankings) and general survey (S rankings) were not well correlated for some programs in some fields. This finding applied especially to fields with relatively few programs or to programs within a taxonomy that encompassed a diversity of scholarly practices. In any attempt to PREPUBLICATION COPY-UNEDITED PROOFS determine the values of faculty members as they relate to program ratings (R rankings), it is extremely important that the questions be tested for clarity and that the sample sizes be large enough to minimize statistical error. Thus, although the methodology for combining the coefficients, which lessens the weight on the coefficients with a larger standard error, could lead to better estimates of program quality in most cases, the committee agreed it would be better to show the regression-based and general survey-based results separately as additional information is conveyed. Further work that focuses on differences in the R's and S's and the circumstances under which coefficients could be validly combined would be helpful."}, {"section_title": "TWO AREAS FOR FURTHER STUDY", "text": ""}, {"section_title": "Dimensional Measures", "text": "As described in Chapters 3 and 4, the committee's reliance on faculty views of program quality and its determinants resulted in some variables that the committee strongly believed were important to doctoral program quality showing up with very low weights in the overall rankings. Perhaps scholarly activity is clearly of paramount importance to most faculty members and thus to the quality of doctoral programs that produce future faculty members. And yet additional aspects of the doctoral experience and environment may prove important to students, many of whom will not take academic positions, and to the faculty who prepare them. The committee took this factor into account in its data-based ranking methodology by constructing dimensional measures that maintained the relative weighting of the included characteristics, but only included the characteristics relevant to a particular dimension of doctoral education. A look at these measures reveals that many of the programs that rank high on the research dimension may not rank as well on the student support and outcomes dimension, or on the diversity dimension. Such an outcome might be expected because the committee was trying to capture separate aspects of doctoral education, but saw no reason why a program could not rank highly on all three dimensional measures. However, in general this failed to be the case. In the future a larger student survey and an effort to incorporate student values could enhance the study findings."}, {"section_title": "The Connection Between Postdoctoral Study and Doctoral Education", "text": "The connection between postdoctoral study and doctoral education was not explored in any depth in this study, although the committee did collect data about the number of postdoctoral scholars associated with each program. Especially in the biosciences, postdocs are part of a continuum of research training. Whether the characteristics of doctoral programs with many postdocs differ greatly from those with few should be studied. The difference may be in the nature of research being undertaken, or it may be that the nature of the doctoral education experience differs, depending on the number of postdocs associated with a doctoral program."}, {"section_title": "CONCLUSION", "text": "This study developed a methodology based on relating data about doctoral programs to the reputational ratings of particular programs and also to idealized preferences about program characteristics. For many fields it found that the separate approaches resulted in different characteristics appearing as important as determinants of rankings, depending on the measure."}, {"section_title": "A. Health Benefits and Services", "text": "From July 1st to June 30th Other, please specify:_______________________________"}, {"section_title": "F. Doctoral Student Representation in 5 Selected Fields", "text": "This section collects outcomes by race/ethnicity on the full-time doctoral students who are U.S. citizens or permanent residents in each of five broad fields 1) Life Sciences, 2) Physical Sciences and Mathematics, 3) Engineering, 4) Social and Behavioral Sciences, and 5) Arts and Humanities. \u2022 If the numbers in these tables are too small to release for reasons of confidentiality, please provide the raw data to the NRC and we will aggregate over cohorts so that the size of any cell is always greater than or equal to 5. \u2022 For purposes of this question only, \"Physical Sciences, Mathematics, and Engineering\" in the taxonomy have been disaggregated into two separate broad fields: \"Physical Sciences and Mathematics\" and \"Engineering.\" \u2022 Do not include Emerging Fields unless they are also included as part of a program in an established field within the taxonomy \u2022 Include doctoral students enrolled in your doctoral programs, whether or not they have been admitted to candidacy. \u2022 Do not include doctoral students who have declared that they only intend to earn a master's degree. Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. \u2022 Doctoral students who \"left the program\" are those who are no longer enrolled at this time. \u2022 Doctoral students who \"stopped out\" (left but later enrolled again) should not be counted as students who left if they are currently enrolled or completed the doctoral degree. Native Americans/Alaska Natives in the Life Sciences  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F1b. Of the Native American/Alaska Natives admitted to candidacy in the Life Sciences, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2003 2003-2004 2004-2005 2005-2006 Non-Hispanic Blacks in the Life Sciences  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F2b. Of the Non-Hispanic Blacks admitted to candidacy in the Life Sciences, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2002 2002-2003 2003-2004 2004-2005 2005-2006 Non-Hispanic Whites in the Life Sciences  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F3b. Of the Non-Hispanic Whites admitted to candidacy in the Life Sciences, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F4b. Of the Hispanics admitted to candidacy in the Life Sciences, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Asians and Pacific Islanders in the Life Sciences  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010."}, {"section_title": "2002-", "text": "\n"}, {"section_title": "2001-", "text": "\n"}, {"section_title": "Hispanics in the Life Sciences", "text": ""}, {"section_title": "1999-", "text": "F5b. Of the Asians and Pacific Islanders admitted to candidacy in the Life Sciences, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Native Americans/Alaska Natives in the Physical Sciences and Mathematics  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2005 2005-2006 F6b. Of the Native Americans and Alaskan Natives admitted to candidacy in the Physical Sciences and Mathematics, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Non-Hispanic Blacks in the Physical Sciences and Mathematics  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2002-20032003-20042004-20052005-2006 F7b. Of the Non-Hispanic Blacks admitted to candidacy in the Physical Sciences and Mathematics, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Non-Hispanic Whites in the Physical Sciences and Mathematics  1996-1997 1997-1998 1998-1999 1999-2000 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F8b. Of the Non-Hispanic Whites admitted to candidacy in the Physical Sciences and Mathematics, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F9b. Of the Hispanics admitted to candidacy in the Physical Sciences and Mathematics, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-19971997-19981998-19991999-20002000-20012001-20022002-20032003-20042004-20052005 This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F10b. Of the Asians and Pacific Islanders admitted to candidacy in the Physical Sciences and Mathematics, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-19971997-19981998-19991999-20002000-20012001-20022002-20032003-20042004 This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F11b. Of the Native Americans and Alaskan Natives admitted to candidacy in Engineering, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-19971997-19981998-19991999-20002000-20012001-20022002-20032003 This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2004 2004-2005 2005-2006 Non-Hispanic Whites in Engineering  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F13b. Of the Non-Hispanic Whites admitted to candidacy in Engineering, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2003 2003-2004 2004-2005 2005-2006 Hispanics in Engineering  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F14b. Of the Hispanics admitted to candidacy in Engineering, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2002 2002-2003 2003-2004 2004-2005 2005-2006 Asians  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F15b. Of the Asians and Pacific Islanders admitted to candidacy in Engineering, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Native Americans/Alaska Natives in the Social Sciences  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F16b. Of the Native American/Alaskan Natives admitted to candidacy in the Social Sciences, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Non-Hispanic Blacks in the Social Sciences  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F17b. Of the Non-Hispanic Blacks admitted to candidacy in the Social Sciences, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Non-Hispanic Whites in the Social Sciences  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F18b. Of the Non-Hispanic Whites admitted to candidacy in the Social Sciences, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010.\n"}, {"section_title": "2004-", "text": ""}, {"section_title": "2000-", "text": "\n"}, {"section_title": "2003-", "text": "\n\n"}, {"section_title": "1998-", "text": "F19b. Of the Hispanics admitted to candidacy in the Social Sciences, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Asians and Pacific Islanders in the Social Sciences  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010."}, {"section_title": "2005-2006", "text": "F20b. Of the Asians and Pacific Islanders admitted to candidacy in the Social Sciences, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Native Americans/Alaska Natives in the Arts and Humanities  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2004 2004-2005 2005-2006 F21b. Of the Native American/Alaskan Natives admitted to candidacy in the Arts and Humanities, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Non-Hispanic Blacks in the Arts and Humanities  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2001-20022002-20032003-20042004-20052005-2006 F22b. Of the Non-Hispanic Blacks admitted to candidacy in the Arts and Humanities, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Non-Hispanic Whites in the Arts and Humanities  1996-1997 1997-1998 1998-1999 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 1999-20002000-20012001-20022002-20032003-20042004-20052005-2006 F23b. Of the Non-Hispanic Whites admitted to candidacy in the Arts and Humanities, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Hispanics in the Arts and Humanities  1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 F24b. Of the Hispanics admitted to candidacy in the Arts and Humanities, record the number of students from each cohort listed below who completed degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. "}, {"section_title": "1997-", "text": ""}, {"section_title": "a. I believe my program may be ineligible (go to IN1)", "text": "2) The following other program(s) at your institution will also be part of the study in the field of_(Name of field in the NRC taxonomy): ____(Name of program that was identified by the institution)_____________ ____(Name of program that was identified by the institution)_____________ etc. 3) If other doctoral degree-granting programs in this field exist at your institution (see above), data and faculty lists for those programs will be provided to the NRC separately. Consequently, please do not include faculty members in those programs here, unless they actively participate in your program. In this section, we ask you to provide information about your faculty in three categories-core, new, and associated."}, {"section_title": "B1. Core Faculty. Please complete the table below with the names of faculty members who:", "text": "1) have served as a chair or member of a program dissertation committee in the past 5 academic years (2001-2002 through 2005-2006), OR 2) are serving as a member of the graduate admissions or curriculum committee The faculty member must be currently (2006)(2007) and formally designated as faculty in the program, and not be an outside reader who reads the dissertation but does not contribute substantially to its development. Include emeritus faculty only if the faculty member has, within the past three years, either chaired a dissertation committee or been the primary instructor for a regular PhD course. "}, {"section_title": "B3. Associated Faculty. Please complete the table below with the names of faculty members who:", "text": "1) have chaired or served on program dissertation committees in the past five years (2001 2002 through 2005-2006), AND 2) have a current (2006)(2007) 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 *C2. Of the doctoral graduates who received doctoral degrees in the period 2003-2004 through 2005-2006, what was the median time to degree? \u2022 The median is the mid-point measured from the date of first enrollment in the program to date of graduation-50 percent took a shorter time to complete their degrees and 50 percent took longer \u2022 When entering a number that includes a decimal, please type the decimal \u2022 If this program enrolls MD/PhD students and the time to degree for these students can be calculated separately, do NOT include these students below. You will be asked about the MD/PhD students later."}, {"section_title": "Median Number of Years", "text": "a. All full-time and part-time doctoral students |__| . |__| b. Doctoral students who were full-time during their entire time in the program |__| . |__| C3. For each academic year listed below, please indicate: This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 1) The number of doctoral students to whom your program offered admission AND 2) The number of doctoral students who then enrolled for the first time."}, {"section_title": "Number Offered Number Admission", "text": "First-Time Enrolled If none: enter zero If none: enter zero 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 [The program will check that for each row, the number entered in col 1 must be larger that the number entered in col 2.]"}, {"section_title": "C4. What is your program's policy regarding whether a master's degree in the field is required prior to admission to this program:", "text": "Mark one only It is required prior to admission It is expected that students will earn it as a stage in their doctoral program Neither of the above C5. Of the [program automatically calculates number from response to question C3] students who enrolled for the first-time in 2003-2004, 2004-2005, and 2005-2006 [The program will check that the number entered must be equal to or smaller than the total number of students in col 2 for years 2003-2006 in C3.] "}, {"section_title": "C6. Does your doctoral program have a continuous enrollment policy?", "text": "Continuous Enrollment means that a person is considered to be a doctoral student only if he or she is enrolled and pays tuition or a fee. Under this policy, a student who drops out must apply for reinstatement."}, {"section_title": "Yes No skip to C8", "text": "C7. To whom does this policy apply? Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. c. How will you be reporting the progress of the dual professional degree /PhD students enrolled in this program? Dual professional degree/PhD students include students such as MD/PhD, DVM/PhD or ThD/PhD students. Can report separately on the dual professional degree/PhD students Cannot report separately on the dual professional degree/PhD students (skip to C12)  C15a. In what year did the policy change? Year:"}, {"section_title": "C16.", "text": "The next series of questions collects information on how many of the full-time students in your program complete doctoral study by gender. [FILL if C10c = \"can report separately Since you will be reporting them separately, please do NOT include the program's dual professional degree/PhD students in the numbers reported for questions C16a through C17b [FILL if C10c = \"cannot report separately\" Please include the program's dual professional degree/PhD students in the numbers reported for questions C16a through C17b \u2022 To preserve confidentiality, if the numbers in cells equal less than 5, the NRC will aggregate over cohorts so that the size of any reported cell is always greater than or equal to 5 \u2022 Include doctoral students enrolled in your doctoral program, whether or not they have been admitted to candidacy Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. \u2022 Do not include students who only enroll with the intent of earning a master's degree and did not convert to doctoral students. \u2022 Doctoral students who \"left the program\" are those who are no longer enrolled at this time. \u2022 Doctoral students who \"stopped out\" (left but later enrolled again) should not be counted as students who left if they are currently enrolled or completed the doctoral degree \u2022 Admitted to Candidacy may be defined in different ways. If 1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 *C16b. Of the male students admitted to candidacy in your program, record the number who within the various time spans listed below completed doctoral degrees within the given number of years after enrolling.  1996-1997 1997-1998 1998-1999 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2004 2004-2005 2005-2006 *C18b. Of the dual professional degree/PhD students admitted to candidacy in your program, record the number who within the various time spans listed below completed doctoral degrees within the given number of years after enrolling. Dual professional degree/PhD students include students such as MD/PhD, DVM/PhD or ThD/PhD students.  1996-1997 1997-1998 1998-1999 1999-2000 2000-2001 2001-2002 2002-2003 2003-2004 2004-2005 2005-2006 C19. In order to analyze program interdisciplinarity through a review of dissertation key words, please enter the full names of every student who was awarded a doctoral degree in this program over the past three years (2003-04 through 2005-06) and the academic year in which that degree was awarded. In this section, we ask for information about the financial support your program provides to its full-time doctoral students."}, {"section_title": "Enter each student's name and the academic year on each line", "text": ""}, {"section_title": "E1. For the 2005-2006 academic year, what did your institution charge full-time first-year", "text": "doctoral students in your program for tuition, mandatory fees, and health insurance premiums? \u2022 Enter dollar amounts without commas or dollar signs ($). In-state Out-of-state students students Summer support: $__________ $__________ $__________ E5. How many of the full-time first-year doctoral students (FFDs) who entered your program in the 2005-06 academic year had\u2026.. \u2022 The appointee was recently awarded a Ph.D. or equivalent doctorate (e.g., Sc.D., M.D.) in an appropriate field; and \u2022 the appointment is temporary; and \u2022 the appointment involves substantially full-time research or scholarship; and \u2022 the appointment is viewed as preparatory for a full-time academic and/or research career; and"}, {"section_title": "Number of Students", "text": "Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. \u2022 the appointment is not part of a clinical training program; and \u2022 the appointee works under the supervision of a senior scholar or a department in a university or similar research institution (e.g., national laboratory, NIH, etc.); and \u2022 the appointee has the freedom, and is expected, to publish the results of his or her research or scholarship during the period of the appointment. (See: http://www.aau.edu/reports/PostDocRpt.html. Accessed 6/27/06) Every ten or so years, the National Research Council conducts a study of national importance regarding the quality and characteristics of doctoral programs in the United States. This comparative assessment is designed to assist prospective doctoral students with selecting programs that best fit their interests and to permit programs to benchmark themselves against similar programs. The 2006 Assessment of Research Doctorate Programs collects data about the doctoral programs in over 60 areas of study in American universities. Your institution has identified your program in: ____(Name of program that was identified by the institution)_____________ as an area of doctoral study that corresponds to the following emerging field in the NRC taxonomy: ____(Name of field in the NRC taxonomy)____________________________ Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010."}, {"section_title": "New Faculty are faculty members who:", "text": "1) do not meet the criteria for core faculty, but who have been hired in tenured or tenuretrack positions within the past three academic years (2003-2004 through 2005-2006) AND Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. 2) are currently employed at your university and are expected to become involved in doctoral education in your program Associated Faculty are faculty members who: 1) have chaired or served on program dissertation committees in the past five years (2001 2002 through 2005-2006), AND 2) have a current (2006)(2007) faculty appointment at your institution, but who are not designated faculty in the program. They should not be outside readers, or faculty currently employed at other universities. Include emeritus faculty only if the faculty member has, within the past three years, either chaired a dissertation committee or been the primary instructor for a regular Ph.D. course. Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. The National Academies National Research Council Assessment of Research Doctorate Programs"}, {"section_title": "Survey of Program Quality", "text": "Thank you for agreeing to participate as a rater in {taxonomy field name} in the Survey of Program Quality, a critical component of the National Research Council's Assessment of Research Doctorate Programs. This survey asks for your judgment-and the judgment of other faculty members like you-about the quality of a sample of doctoral programs in your field. How your judgments will be used. The judgments of over 200 raters in each field will be used to calculate ratings of perceived quality for a sample of the programs, rather than all the programs in a field. Previous research (Ostriker & Kuh, 2003 1 ) has shown us how to use faculty views on the strength of different PhD programs combined with objective data concerning program characteristics to produce ratings of additional programs. These new ratings are based on objectively measured characteristics, such as publications, citations and time to degree, but imitate, to the extent achievable, the judgment criteria of the initially surveyed faculty. Thinking about your perception of a program's quality. As part of this survey, you will be asked to rate 15 programs on a scale of 1 to 6 (1=a program not sufficient for graduate education, 6=a distinguished program). We urge you to keep two things in mind as you decide on your ratings: \u2022 Prior to rating these 15 programs, you will have the opportunity to view a list of all programs in your field. Keep this \"universe\" of programs in mind as you rate each of the 15 programs relative to this universe, not to each other. \u2022 Please reflect on what you consider important in a doctoral program as you decide on your ratings. To assist you, a link below each program's name goes to an information page that lists several program and faculty characteristics, a list of the program's faculty and a link to the program's web site as well, should you want to seek additional information before finalizing your rating. Your efforts will improve doctoral education through benchmarking and better information about programs. The survey is being conducted by Mathematica Policy Research (MPR), an organization experienced in the conduct of confidential surveys. Your responses will be compiled by MPR and provided to the NRC for their analyses. The National Research Council staff who analyze the data will sign non-disclosure confidentiality agreements to protect the identity of individuals participating in the survey. The survey will be conducted using secure PREPUBLICATION COPY-UNEDITED PROOFS web-based survey technology and any information that could be used to identify or link responses to an individual respondent for any survey question will be maintained in storage that is secure. Your identity will be known only to the National Research Council and Mathematica Policy Research who have signed non-disclosure agreements. Only aggregate information from the survey, such as means and distributions of ratings for programs, will be included in publications from the project. If you have any questions about the study or this questionnaire, please email us at NRC-Assessment@mathematica-mpr.com."}, {"section_title": "I provide my informed consent to participate in this study Yes No", "text": "Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010."}, {"section_title": "Instructions", "text": "1. Listed below are the 15 programs in your field that you are being asked to rate. Given the range of programs within some fields, you may or may not be familiar with all of the programs you are being asked to rate. Consequently, you will be asked two questions about each program. The first asks how familiar you are with the program and the second asks you to rate its quality. 2. Before considering programs individually, please take a moment to familiarize yourself with the larger range of programs in your field. To do so, please click on this link: Click here for a list of all institutions in the study with programs in this field: 3. To begin considering programs individually, click on the link provided for each institution. You will be taken to that program's information page. If it was provided to the NRC, the information pages will also list a link to that program's home page. 2 There will be a link to explain this term. 3 There will be a link to explain this term. 4 There will be a link to explain this term. Every 10 or so years, the National Research Council conducts a study of national importance regarding the quality and characteristics of doctoral programs in the United States. The 2006 Assessment of Research Doctorate Programs collects data on the doctoral programs and doctoral faculty in over 60 areas of study in American universities, along with some student data. This comparative assessment, the most comprehensive to date, is designed to assist prospective doctoral students with selecting programs that best fit their interests and to permit programs to benchmark themselves against similar programs. Your participation is important. By completing this questionnaire, you are providing information that will: (1) help the NRC identify the characteristics of successful graduate programs, (2) enable the NRC with collecting data on grants, citations, and publications from other sources; and (3) permit a statistical description of the faculty in the graduate program(s) or programs with which you are affiliated. For further information about the assessment, see www7.nationalacademies.org/resdoc/index.html. This site also has a list of Frequently Asked Questions and contains an Email link to request answers to questions you might have concerning the study or the questionnaire. All of the information you provide will be treated as confidential. The survey is being conducted by Mathematica Policy Research (MPR), an organization experienced in the conduct of confidential surveys. Your responses will be compiled by MPR and provided to the NRC for their analyses. Personally identifiable information, such as past employment and ZIP Codes, will be used to obtain data on publications, grants and awards and honors from other databases. The National Research Council staff who analyze the data will sign non-disclosure confidentiality agreements to protect the identity of individuals participating in the survey. The survey will be conducted using secure web-based survey technology and any information that could be used to identify or link responses to an individual respondent for any survey question will be maintained in storage that is secure. Any data, including race/ethnicity and gender, that is not currently available to the public will only be used in an aggregated form that cannot be used to discern the identity of any survey participant in any report or presentation concerning the survey or in the public use file that will be made available to the public at the conclusion of this study. The link between your name and the data you provide in this questionnaire will only be used to obtain publications and, awards and honors data from other databases and will be removed prior to the publication of the public use file. Your participation is voluntary. Completing the questionnaire averages about 14 minutes, not counting the time required to list or upload publications, which will vary from person to person. You may refuse to answer any question or discontinue participation at any point. There is no personal risk to you in responding to this questionnaire. Your identity will be known to only the National Research Council and Mathematica Policy Research. No information concerning respondents will be given to your institution. If you have any questions about the study or this questionnaire, please email us at NRC-Assessment@mathematica-mpr.com. Faculty must submit their competed questionnaire by February 15, 2007 if they wish to be considered as a program rater for the Rating Survey that follows this spring. Otherwise, the end date is April 1, 2007."}, {"section_title": "Click here to indicate your informed consent to participate in this study", "text": "Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010."}, {"section_title": "A. Program Identification", "text": "You have been identified by your institution as a faculty member who participates in doctoral education in one or more graduate programs that fall under one or more fields in the NRC taxonomy. The names of these programs are listed below in questions A2i and A2. However, if you are involved in a doctoral program that is not on this list, it is not part of this study and should not be considered when responding to this questionnaire."}, {"section_title": "A1. In what year did you become a faculty member at this institution?", "text": "Year:  \u2022 the appointee works under the supervision of a senior scholar or a department in a university or similar research institution (e.g., national laboratory, NIH, etc.); and \u2022 the appointee has the freedom, and is expected, to publish the results of his or her research or scholarship during the period of the appointment. (See: http: //www.aau.edu/reports/PostDocRpt.html.) Yes No skip to D1"}, {"section_title": "C6. How many postdoctoral appointments have you held?", "text": "Number of Postdocs Held: C7. For each postdoc held, please enter the number of years that you held the postdoc and the sector in which you were working. \u2022 "}, {"section_title": "D. Scholarly Activity", "text": "The questions in this section will help us match productivity data such as publications, citations, research grants and other types of scholarly productivity with the faculty who participate in the graduate program There will be two primary sources of data. The first will be the data provided by the journals monitored by the Institute for Scientific Information (ISI). The list can be found at: http://scientific.thomson.com/mjl/. The second will be your answers to the questions below. In counting publications, in most cases, the NRC will limit itself to books, monographs, and articles and reviews in refereed journals. It is especially important that you list books, monographs, and articles in edited volumes and in specialist journals not covered by ISI so that we have a full picture of your scholarly productivity. In addition, if there are other kinds of scholarly production that you feel give a complete picture of your scholarship, please list them below in D5 \u2022 If you are in the Humanities, please include the names or variants of your name under which you have published books or articles in the past 10 years (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)."}, {"section_title": "D2. Please list the Zip Codes that appeared on your publications as a reflection of your professional location between 2001 and 2006.", "text": "\u2022 If you are in the Humanities, please list the zip codes that appeared on your publications in the past 10 years (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006). \u2022 If you are in the Humanities, please list the titles of books you have authored, coauthored or edited in the past 10 years (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006) "}, {"section_title": "Specific Characteristics: Student Characteristics (Category II)", "text": "G2. In Column A, please select the characteristics in this category (up to FOUR) that you feel are the most important to program quality. In Column B, if you selected more than two characteristics, please select the TWO you feel are the most important. the institutional or program level (This variable will be a tally of whether the following services are provided to graduate students at either the institutional or program level: orientation for new students, prizes/awards to doctoral students for teaching and/or research, formal training in academic integrity/ethics, travel funds to attend professional meetings, grievance/dispute resolution procedures, annual review of all enrolled doctoral students, training to improve teaching skills, institutionallysupported graduate student association, information about "}, {"section_title": "CATEGORY II --Student Characteristics", "text": ""}, {"section_title": "General Characteristics", "text": ""}, {"section_title": "G4.", "text": "Please assign a score to each category with the total adding up to 100, where 0 indicates the category has no importance to your judgment of quality and 100 indicates it is the only category that is important. "}, {"section_title": "Category Score", "text": ""}, {"section_title": "H6. What is your racial background", "text": ""}, {"section_title": "Mark all that apply", "text": "American Indian or Alaska Native Native Hawaiian or other Pacific Islander Asian Black or African-American White I1. To help us understand the characteristics of faculty in doctoral programs without asking additional questions, and to enable us to access data from national databases (e.g., on citation counts), please attach your current C.V. when you submit this questionnaire."}, {"section_title": "C. V. attached", "text": "J1. Would you be willing to answer an additional questionnaire that would ask you to rate the overall quality of other doctoral programs in your field?"}, {"section_title": "Yes No", "text": "Ask J2 if J1 = yes J2. Good contact information is needed for those selected. Please fill in your preferred contact information below. ADDRESS: ___________________________________________________________ __________________________________________________________ __________________________________________________________ __________________________________________________________ CITY: _______________________ STATE: ______ ZIP CODE: _________ J3. Please provide your preferred e-mail address where you can be reached if there are responses in your questionnaire that require clarification or if you prefer to be contacted about the program ratings by email. Email address: _______________ Thank you for your time.\nNotice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate  .............................................................................................. a.  .......................................................................................................  Mentor or tutor a high school student? ................................ "}, {"section_title": "D1", "text": ". The request for the names faculty use on their publications will help in the matching process by eliminating false matches and by finding publications written before a name change, for example the name used before marriage. D2. In addition to using author names in the matching process, the ZIP Code for the location of the author will be used, since it is the only uniquely identifiable numeric piece of information that appears on a publication. Institutional names may be available, but they vary in form and it will be difficult to identify all forms that pertain to a particular institution. Also, if a faculty member moves from one institution to another, the ZIP Code of the prior institution will help in matching the earlier publications to the faculty member. D3. There is no good data source for matching the faculty in a program to the books they have authored. Sources, such as the Library of Congress and Books in Print, do not carry geographic information about the author and matching on name alone will provide multiple matches. The titles of the books can then be used to eliminate false matches. D4. ISI does not cover all possible journals. In particular, its coverage of highly specialized journals in the humanities may be very limited. A listing of these publications will be useful in obtaining more complete data on faculty productivity. D5. This question is intended to obtain a list of non-journal and non-print scholarly contributions. PREPUBLICATION COPY-UNEDITED PROOFS"}, {"section_title": "222", "text": "Welcome to the National Research Council's"}, {"section_title": "Assessment of Research Doctorate Programs", "text": "Admitted-to-Candidacy Doctoral Student Questionnaire This questionnaire is part of the National Research Council's 2006 Assessment of Research Doctoral Programs. The National Research Council (NRC) is the operating arm of the National Academy of Sciences, an institution that conducts studies on issues relevant to questions of importance to educational, scientific and technological policy. Its reports are highly respected and have important impact on national and institutional policymakers. This is the first NRC assessment of doctoral programs in over ten years. The study is an effort to gather data about doctoral programs nationwide and provide data that will be helpful to students, faculty, administrators and those who make educational policy. For the first time, the assessment is including a survey of doctoral students. By completing this questionnaire, you provide information that will: (1) bring a student perspective to the study; (2) permit a statistical description of the advanced doctoral students in your field, and (3) help the NRC identify the multiple dimensions of successful graduate programs. Further information about the assessment may be found at www7.nationalacademies.org/resdoc/ index.html. This site also has a list of Frequently Asked Questions and contains an Email link for submitting questions you might have about the study or the questionnaire. As a graduate student, this is an important opportunity for you to be heard on issues related to graduate education, both in your program and in general. If you and your fellow students respond at a high rate, the results will provide important information about and to your program that will help facilitate change in graduate education at the program level. Your responses to this online questionnaire will be entered directly into our database and treated as completely confidential by the NRC. Your individual answers will not be shared with faculty or administrators of your doctoral program. Any data, including race/ethnicity and gender, that is not currently available to the public will only be used in aggregated form that cannot be used to discern the identity of any survey participant in any report or presentation concerning the survey or in the public use file that will be made available to the public at the conclusion of this study. The link between your name and the data you provide will be removed prior to the publication of the public use file. In the case of questions with an open-ended response, comments will be reported only in an anonymous form that does not disclose the identity of the respondent. Your participation is voluntary. You may refuse to answer any question or discontinue participation at any point. There is no personal risk to you in responding to this questionnaire since your identify will be known only to the National Research Council and Mathematica Policy Research. No information concerning respondents will be given to your institution. If you have any questions related to the study or this questionnaire, please send an email to NRC-Assessment@mathematica-mpr.com"}, {"section_title": "II.", "text": "Please click here to indicate your informed consent to participate in this study Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010. Ask A6 if any \"yes\" responses to A4 or A5c or A5d"}, {"section_title": "A4", "text": ""}, {"section_title": "A6. Did you write a master's thesis?", "text": "Yes No"}, {"section_title": "A7.", "text": "While studying for the doctorate, will you receive a certificate in another field or skill area?"}, {"section_title": "C5.", "text": "Other than course grades, does your program provide an annual or more frequent assessment of your academic progress? (examples: a letter from the program, a meeting with your dissertation committee)  "}, {"section_title": "C20.", "text": "How much do you feel you have benefited from the: "}, {"section_title": "C21.", "text": "How satisfied are you with the quality of program-sponsored activities designed to promote social interaction of students with faculty and with other students? Very satisfied Somewhat satisfied Not satisfied"}, {"section_title": "C22. How much do you feel you belong to your program?", "text": "A lot Some Not at all"}, {"section_title": "C23.", "text": "In the space below, please provide any additional comments you would like to make about your doctoral program, its characteristics or quality: Notice: Embargoed Materials This report is intended for the sole use of institutions that participated in the study A Data-Based Assessment of Research-Doctorate Programs in the United States. THIS REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1 P.M. EDT ON TUESDAY, SEPTEMBER 28, 2010."}, {"section_title": "E6.", "text": "In what year were you born? Year "}, {"section_title": "Part D: Resources", "text": "This section collects data on respondent perceptions of the adequacy of the resources and benefits available for doctoral students. Education and Research Resources: The availability of adequate resources is important to both the speed and quality of a student's academic progress. Questions 1-4 collect data on respondents' perceptions of the resources available (from the institution or program) to support their education and research. They ask for perceptions of the adequacy of computer resources, research, laboratory, or studio facilities, library resources, and on-campus work-space. Social Integration: As noted above, the degree to which a student feels part of a department as a critical factor in determining whether a student completes a doctoral program. Question D5, along with other questions, collects data on opportunities for social interaction."}, {"section_title": "Quality of Life:", "text": "In addition to financial support and health care benefits, support for doctoral students may also include provision of housing or housing assistance, provision of child care or financial support for child care, and recreational facilities. These pieces of the support package a doctoral student can expect-particularly students with children-may affect the ability of students to matriculate, complete in a timely manner, or complete at all. Questions 6-8 collect data on respondent perceptions of these benefits."}, {"section_title": "Part E: Background Information", "text": "The information collected in this section of the questionnaire will allow analysts to examine the comparative demographics of programs, and also examine how the answers to questions in Parts A-D of the questionnaire may vary across such dimensions as age, gender, race/ethnicity, citizenship status, family background, marital status, and responsibility for dependents. The participation in doctoral education of students from a variety of backgrounds is important to the academic enterprise, the conduct of research, and society in general, so understanding how doctoral education works for students across groups will provide the opportunity to evaluate success to date and areas where further progress is necessary. answered this question in the affirmative was divided by the total respondents in the program and the percentage was then calculated. Percent Interdisciplinary: Data from the program questionnaire were used for this variable. Faculty were identified as either core, new, or associated. Percent interdisciplinary is the ratio of associated to the sum of core, new, and associated faculty. Allocations were not used in the construction of this variable. Percent Non-Asian Minority Faculty of Core and New Faculty, 2006 * * : Data from the program questionnaire were used for this variable. For each program the data reported for question B7, the race/ethnicity of core and new faculty in the program, was used to compute the ratio of non-Hispanic Blacks, Hispanic, and American Indians or Alaska Natives to that of non-Hispanic Whites, non-Hispanic Blacks, Hispanic, Asian or Pacific Islanders, and American Indians or Alaska Natives. Faculty with Race/Ethnicity Unknown were excluded from the ratio, as were faculty who were neither American citizens not permanent residents.. Allocations were not used in the construction of this variable. Percent Female Faculty of Core and New Faculty, 2006: Data from the program questionnaire were used for this variable. For each program the data reported for question B5, the gender of core and new faculty in the program, was used to compute the ratio of core or new female faculty to the total of core and new faculty as described above. Allocations were not used in the construction of this variable. Awards per Allocated Faculty: Data from a review of 1,393 awards and honors from various scholarly organizations were used for this variable. The awards were identified by the committee as \"Highly Prestigious\" or \"Prestigious\" with the former given a weight of 5. The award recipients were matched to the faculty in all programs, and the total awards for a faculty member in a program was the sum of the weighted awards times the faculty member's allocation in that program. These awards were added across the faculty in a program and divided by the total allocation of the faculty to the program. Average GRE, 2004-2006 (Verbal Measure for the Humanities, Quantitative Measure for All Other Fields): Data from the program questionnaire were used for this variable. For each program, question D4 reported the average GRE verbal and quantitative scores for the 2003-2004, 2004-2005, and 2005-2006 academic years and the number of individuals who reported their scores. A weighted average was used to compute the average GRE, which was calculated by multiplying the number of individuals reporting scores by the reported average GRE score for each year, adding these three quantities and dividing by the sum of the individuals reporting scores. the entering students in that cohort. This computation was made for each cohort that entered from 1996-1997 to 1998-1999 for the humanities and 1996-1997 to 2000-2001 for the other fields. Cohorts beyond these years were not considered, since the students could complete in a year that was after the final year 2005-2006 for which data were collected. To compute the average completion rate, an average was taken over 3 cohorts for the humanities and over 5 cohorts for other fields. Time to Degree (for Full-and Part-Time Graduates): Data from the program questionnaire were used for this variable. Question C2 reported the median time to degree for full-time and part-time students averaged over the years [2004][2005][2006]. That reported number was used for this variable. Percent Ph.D.'s with Definite Plans for an Academic Position, 2001-2005: Data from the National Science Foundation 2005 Doctorate Records File (DRF) were used for this variable. A crosswalk was generated between the DRF Specialty Fields of Study and the fields in the study taxonomy. Data from the DRF for 5 years (2001)(2002)(2003)(2004)(2005) were matched by field and institution to the programs in the research-doctorate study. The percentage was computed by taking the number of individuals who have a signed contract or are negotiating a contract for a position at an educational institution and dividing by the number of doctorates in those years. Positions included employment and postdoctoral fellowships."}, {"section_title": "Student Work Space:", "text": "Data from the program questionnaire were used for this variable. Question D12 reported the percentage of graduate students who have work space for their exclusive use. If reported percentage was 100 percent, then a value of 1 was given to this variable. Otherwise the value was -1. Health Insurance: Data from the institutional questionnaire were used for this variable. Question A1 reported whether or not the institution provided health care insurance for its graduate students. If the response to this question was yes, then a value of 1 was given to this variable. If it was no, then the value was -1. Student Activities: Data from the program questionnaire were used for this variable. Question D8 listed 18 different kinds of support for doctoral students or doctoral education. This variable is a count of the number of support mechanisms proved by the program or the institution. "}]