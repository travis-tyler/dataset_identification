[{"section_title": "Abstract", "text": "An adaptive enrichment design is a randomized trial that allows enrollment criteria to be modified at interim analyses, based on a preset decision rule. When there is prior uncertainty regarding treatment effect heterogeneity, these trial designs can provide improved power for detecting treatment effects in subpopulations. We present a simulated annealing approach to search over the space of decision rules and other parameters for an adaptive enrichment design. The goal is to minimize the expected number enrolled or expected duration, while preserving the appropriate power and Type I error rate. We also explore the benefits of parallel computation in the context of this goal. We find that optimized designs can be substantially more efficient than simpler designs using Pocock or O'Brien-Fleming boundaries."}, {"section_title": "Introduction", "text": "Prior uncertainty regarding treatment effect heterogeneity can pose a challenge to trial designers. If the treatment only benefits a subset of the population, standard clinical trials enrolling from the entire population may have low power. However, if the entire population benefits, a standard trial enrolling only one subpopulation will not provide any information about the complementary population.\nThese issues can be mitigated with the use of an adaptive enrichment trial design. Such a design involves a decision rule for early stopping of participant accrual in different population subsets based on interim analyses (Wang et al., 2009 ). For example, early stopping can occur if there is strong evidence early in the trial of the treatment's benefit or harm for a subpopulation. The design also includes a multiple testing procedure, since there is one null hypothesis to test for each population of interest.\nWe aim to optimize the enrollment modification rule and multiple testing procedure for an adaptive enrichment design. The goal is to minimize either the expected number enrolled or expected trial duration, under constraints on power and the Type I error rate. We focus on designs that are guaranteed to strongly control the familywise Type I error rate, i.e., where the probability is at most \u03b1 that one or more true null hypotheses is rejected, regardless of the (unknown) data generating distribution.\nGeneral approaches exist for constructing optimal designs for simpler problems, such as those involving a single null hypothesis (Eales and Jennison, 1992; Hampson and Jennison, 2013) . Hampson and Jennison (2015) extend this approach to handle multiple hypotheses in two-stage designs, but the resulting designs are not guaranteed to strongly control the familywise Type I error rate (which must be checked by simulation). Thall et al. (1988) perform a 2-dimensional grid search to minimize the expected number enrolled of a 2-stage trial comparing the effects of several treatments. Krisam and Kieser (2015) , Graf et al. (2015) , and , consider different adaptive designs involving two subpopulations, which they optimize over at most a few parameters. In contrast to this related work, our aim is to search over more flexible, higher dimensional families of designs. For trials involving two subpopulations, optimal 2-stage designs can be found via sparse linear programming (Rosenblum et al., 2014) , but this approach becomes computationally infeasible for more than two stages.\nThe optimization problems we consider are challenging in that no existing approach is guaranteed to find the global optimum. The main difficulty is that there are many design parameters to optimize over, as well as multiple constraints. The parameters in our adaptive enrichment designs include the following (plus additional parameters in some settings): the number of stages; per-stage sample sizes; and, an efficacy and futility boundary for each population at each stage. For example, in the case of 2 subpopulations and 5 stages, there are over 30 design parameters. To the best of our knowledge, we are the first to address the problem of optimizing adaptive enrichment designs with more than just a few parameters and more than 2 stages. Additionally, we explore a two step optimization strategy, where first an unstructured search is conducted, and then its results are used to define a more structured search.\nWhile our approach based on simulated annealing (described below) does not ensure that a global optimum is found, we show that it can substantially reduce the expected number enrolled compared to simpler designs. In one of our examples, the optimized adaptive design reduces the expected number enrolled by approximately 32% compared to simpler adaptive and non-adaptive designs; the cost is a 22% increase in maximum number enrolled. In another example, there is a longer follow-up time to measurement of the primary outcome, compared to the enrollment rate; the benefits of adaptation on expected number enrolled are meager since by the time sufficient information has accrued to make a useful decision, most of the total enrollment has already been completed.\nFor trial design problems where no existing approach is guaranteed to find an optimal solution, one may turn to general-purpose, approximate methods such as simulated annealing (SA). apply SA to optimize the worst-case expected number enrolled using a group sequential design, with penalties added to the objective function for violations of either Type I or Type II error constraints. Wason and Jaki (2012) extend these results by applying SA to optimize a multi-arm, multi-stage trial where several treatments groups are compared against a shared, single control group.\nOur optimization problem differs from that of Wason and Jaki (2012) in that our futility boundaries are non-binding (which is typically preferred by regulators such as the U.S. Food and Drug Administration, as noted by Liu and Anderson (2008) ), and our designs allow continuation after one null hypothesis is rejected (so other hypotheses may be rejected at later stages). Also, in our optimization procedure, we include a final adjustment step after the SA algorithm to ensure that power constraints are met. Without this, the optimal solution is generally not guaranteed to have the desired power. Another difference is that we apply a parallelized version of SA. These and other differences between our implementation of SA and that of Wason and Jaki (2012) are discussed in Section 4.\nOver the course of developing the general optimization approach in this paper, we also applied it in to a different problem than discussed here. The focus there was on illustrating a novel multiple testing procedure, rather than on the optimization, which was summarized in a paragraph and not otherwise mentioned. Unlike here, did not compare the optimized design to other designs in order to investigate the value added from the optimization.\nIn Section 2, we introduce motivating data examples based on a new surgical intervention for stroke, and on a hypothetical intervention for preventing progression to Alzheimer's disease. In Section 3, we introduce a class of adaptive enrichment designs, referred to hereafter as \"adaptive designs.\" We discuss how efficacy boundaries can be constructed by incorporating either the covariance of the test statistics , or by using alpha-reallocation (Maurer and Bretz, 2013) . We also introduce different levels of trial design complexity, which balance design flexibility versus 2 http://biostats.bepress.com/jhubiostat/paper279 simplicity. In Section 4, we outline our approach for optimization. In Section 5, we explore the performance of each type of trial, and compare to simpler trial designs using approximate O'Brien Fleming boundaries (O'Brien and Fleming, 1979) or Pocock boundaries (Pocock, 1977) . We end with a discussion of future work."}, {"section_title": "Applications", "text": ""}, {"section_title": "Application 1: Surgical Treatment of Stroke (MISTIE)", "text": "We first describe an example of planning a Phase III trial of a surgical treatment for stroke, which was also considered by . The treatment is called Minimally-Invasive Surgery Plus rt-PA for Intracerebral Hemorrhage (MISTIE), and is described in detail by Morgan et al. (2008) . Each participant had his/her functional disability measured 180 days from enrollment, based on the modified Rankin Scale (mRS). The primary outcome was the indicator of having mRS \u2264 3.\nIn planning the Phase III trial, the investigators were interested in two subpopulations defined by size of intraventricular hemorrhage (IVH) at baseline. \"Small IVH\" participants are defined to have IVH volume less than 10ml and not requiring a catheter for intracranial pressure monitoring. The remaining participants are called \"large IVH\". The Phase II trial only recruited small IVH participants. A preliminary analysis of the data resulted in an estimated treatment effect of approximately 12.1%. Knowledge of the underlying biology of these types of brain hemorrhage suggested a possible benefit for those with large IVH as well. However, there was greater uncertainty about the treatment effect in the large IVH subpopulation. Investigators inquired about the possibility of running a phase III trial that included both small IVH and large IVH participants (called subpopulation 1 and 2, respectively), but with the option to stop a subpopulation's accrual (using a preplanned rule) if interim data indicated that a benefit was unlikely. The proportion of participants in subpopulation 1 was projected to be 0.33; the enrollment rate was projected to be 420 participants per year from the combined population."}, {"section_title": "Application 2: Alzheimer's Disease Neuroimaging Initiative (ADNI)", "text": "We also consider an example using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging, positron emission tomography, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment and early Alzheimer's disease. We focus on subpopulations defined by a participant's apolipoprotein E (APOE) 4 allele carrier status, which is associated with increased risk of late onset Alzheimer's disease (Sadigh-Eteghad et al., 2012) . Non-carriers of the APOE 4 allele are called subpopulation 1, and carriers of at least one allele are called subpopulation 2. Clinical investigators, who were planning a Phase III trial of a new treatment to prevent progression from mild cognitive impairment to Alzheimer's disease, suspected that there may be treatment effect heterogeneity across carrier status. The proportion in subpopulation 1, based on the ADNI data, is 46.9%.\nThe primary outcome is the 2-year change in Clinical Dementia Rating Sum of Boxes score (CDR-SB), an aggregate measure of symptom severity. Enrollment would include those with baseline CDR-SB \u2265 0.5, which is indicative of mild cognitive impairment. The enrollment rate was projected to be 500 participants per year from the combined population."}, {"section_title": "Adaptive Trial Designs", "text": ""}, {"section_title": "Notation, Hypotheses, and Statistics", "text": "We consider two subpopulations that partition the overall population. Let j = 1, 2, C be an index respectively denoting subpopulation 1, subpopulation 2, or the combined population. The treatment effect for a population is defined as the difference between the mean outcome for treatment and control."}, {"section_title": "3", "text": ""}, {"section_title": "Hosted by The Berkeley Electronic Press", "text": "Outcomes may be continuous, binary, or on any scale that allows the treatment effect to be estimated with a difference in means z-statistic. The data collected for each participant is a vector (S, A, Y ) representing his/her subpopulation, study arm assignment, and outcome, respectively. We assume each participant's data vector is an independent, identically distributed draw from an unknown joint distribution on (S, A, Y ). We assume that study arm assignment is randomized with probability 1 2 independent of the subpopulation, and that each participant's outcome is measured after a delay of d years from enrollment.\nLet \u03c0j denote the proportion of the combined population in subpopulation j (with \u03c0C = 1 by convention). Let \u03b4j denote the average treatment effect in population j. It follows that \u03b4C = \u03c01\u03b41 + \u03c02\u03b42. Let H1, H2, and HC respectively be the null hypotheses of no average treatment benefit in populations 1, 2 and C, i.e., Hj : \u03b4j \u2264 0. Each corresponding alternative hypothesis has the form \u03b4j > 0. Let \u03c3 2 tj and \u03c3 2 cj denote the outcome variances in population j under treatment and control, respectively, with all variances assumed known. The combined population outcome variance is a function of \u03c01 and the outcome variances for each subpopulation. Define the global null hypothesis H0 : \u03b41 = \u03b42 = \u03b4C = 0; though this is not of primary interest, we refer to it in defining the different multiple testing procedures below.\nAnalysis timing is based on the cumulative number of outcomes observed from each subpopulation, referred to as the sample size. Because participant outcomes are measured with delay, there can be enrolled participants at interim analyses whose outcomes are not yet measured. These participants are referred to as \"pipeline participants.\" Pipeline participants do not contribute to test statistic calculations, although they always contribute to the number enrolled in the trial.\nOur designs have K > 0 stages, each concluding with an analysis. These analyses may lead to stopping accrual in one or both subpopulations, according to a set of predefined rules that are functions of the data collected so far. To stop accrual means to stop enrollment and follow-up of pipeline participants.\nWe assume enrollment is proportional to subpopulation size \u03c0j, and enrollment is uniform over time. Analysis k occurs when the k th stage is completed, defined to be the first time when \u03c0jn k additional participant outcomes have been measured from each subpopulation j for which accrual has not been previously stopped. The terms n k are design parameters that are not modified during the trial. Let N := K k=1 n k denote the maximum total sample size. The decision at the end of stage k takes as input the following cumulative test-statistic:\nfor each population j \u2208 {1, 2, C} that is still being enrolled, where \u03c0C = 1, and\u03b4j denotes the difference in sample means estimator for \u03b4j based on the accrued data. The combined population statistic Z (k) C is undefined if one or more subpopulations had accrual stopped early at a previous stage. Throughout, we make the asymptotic approximation that the above statistics have the canonical multivariate normal distribution from Jennison and Turnbull (1999, Chapter 3) with covariances:\n, for any j = 1, 2, C, and stages k and l such that 1 \u2264 k \u2264 l \u2264 K. These covariances depend only on the variables \u03c3 2 cj , \u03c3 2 tj , n k , and \u03c0j. The mean vector for the z-statistics is a function of these variables and the subpopulation treatment effects \u03b41, \u03b42.\nwhere \u03bb is a positive tuning parameter (set here to 10 6 ), and (x)+ = max{x, 0}. The first term can also be replaced with the expected trial duration. If any of the power constraints in Section 4.2 are violated, the objective function will incur a severe penalty. The exponent in the penalty term is meant to allow second order differentiability of J(D) with respect to the power of the trial. This exponent is not necessary, but is potentially useful for some of the approaches discussed in Section 5.\nEvaluating J(D) requires the calculation of several multidimensional integrals. Due to the computational obstacle of these calculations, we instead estimate J(D) via simulation. We used 10,000 simulation iterations, such that the Monte Carlo standard error for estimating a power close to 0.80 is approximately {0.8(1 \u2212 0.8)/10000} 1/2 \u2248 0.004.\nSince we parametrize the trial in terms of alpha allocations \u03b1 (k) j that sum to \u03b1 = 0.025, all of our proposed designs are asymptotically guaranteed to control the familywise Type I error as proved in Maurer and Bretz, 2013) , and it is not necessary to penalize for violations of the required familywise Type I error rate in the manner of (Wason and Jaki, 2012; ."}, {"section_title": "4", "text": "http://biostats.bepress.com/jhubiostat/paper279\nAt the analysis just after stage k, each statistic Z (k) j is compared against an efficacy boundary e (k) j and futility boundary f\nj , Hj is rejected. Whenever H1 and H2 are both rejected, we automatically reject HC as well. Rejecting any null hypothesis implies that the hypothesis remains rejected in all future stages. If\nj , then accrual in population j is stopped for futility. We use non-binding futility boundaries, i.e., strong control of the familywise Type I error rate is asymptotically guaranteed for all designs in this paper, even if futility boundaries are ignored and both subpopulations continue accrual through the end of the last stage K.\nFor each subpopulation j \u2208 {1, 2}, its accrual continues until one (or more) of the following occurs:\n. Under this setup, rejecting the combined population null hypothesis HC does not imply stopping all accrual; further tests of H1 and H2 may still be conducted. Since Z (k) C is only defined if the combined population accrual is not stopped early before stage k, we do not conduct the test\nC at stages after accrual for at least one subpopulation has stopped; it is still possible to reject HC at future stages, if both subpopulation null hypotheses are rejected.\nWe compare two methods for calculating efficacy boundaries. The first uses an error spending approach based on the covariances of the statistics Z (k) j . The second method involves a graphical approach to reallocating Type I error to the remaining null hypotheses after some null hypotheses have been rejected (Bretz et al., 2009; Maurer and Bretz, 2013) . We refer to these two types of multiple testing procedures as H COV and H M B , respectively. For both, Type I error is calculated under the assumption that futility boundaries are never adhered to (to give the worst-case Type I error under non-binding futility boundaries). Power, expected number enrolled, and expected duration are calculated under the assumption that futility boundaries are adhered to. propose a method for efficacy boundary calculation that incorporates the covariance matrix among the statistics (which is assumed known), both across stages and across populations. We refer to this approach as H COV . A key feature of this approach is that in order to guarantee strong control of the familywise Type I error rate, it is sufficient to control the familywise Type I error rate under the global null hypothesis H0 of no treatment effect in any subpopulation."}, {"section_title": "Multiple Testing Procedure 1: Covariance Approach", "text": "The first step of this approach is to prespecify an ordering for the null hypotheses. For example, the ordering H1, H2, HC implies that we always first test H1, then H2, and finally HC , at each stage where all three hypotheses are still being tested. If only a subset of hypotheses are still being tested at a given stage, the same ordering applies to the remaining hypotheses. The second step is to prespecify nonnegative values \u03b1 (k) j for each population j \u2208 {1, 2, C} and stage k : 1 \u2264 k \u2264 K, that sum to the desired familywise Type I error rate \u03b1 (e.g., \u03b1 = 0.025). The efficacy boundaries e (k) j are iteratively computed at the end of each stage k by solving the following for each j \u2208 {1, 2, C} according to the above ordering:\nusing the known covariance for the test statistics, where (k , j ) \u227a (k, j) means that either k < k or (k = k and j precedes j in the ordering). The required inputs to the multiple testing procedure H COV are nonnegative values \u03b1 (k) j that sum to \u03b1. We also consider simpler designs, called \"structured\", that restrict these values to have a particular form. Specifically, we consider setting the \u03b1 (k) j allocated to each stage k and hypothesis Hj to be wj \u00d7(a k \u2212a k\u22121 ), where\nj /\u03b1. This is the power family of error spending functions, described by Jennison and Turnbull (1999) . In this way, the search space is reduced from the 3 \u00d7 K values for the \u03b1 (k) j , to the six design parameters wj and \u03c1ej for j = 1, 2, C. For a group sequential design testing only one null hypothesis, setting \u03c1ej to be 1 or 3 results in efficacy boundaries that are similar to those of Pocock (1977) or O'Brien and Fleming (1979) 5\nHosted by The Berkeley Electronic Press respectively (Jennison and Turnbull, 1999) . We refer to the optimization problem where the search space consists of designs that have no restrictions on \u03b1 (k) j (except that they are nonnegative and sum to \u03b1) as \"unstructured.\"\n3.3. Multiple Testing Procedure 2: Alpha-Reallocation Approach Maurer and Bretz (2013) propose a procedure that reallocates alpha from a rejected null hypothesis to remaining null hypotheses; the result is a lowering of the efficacy boundaries for the remaining null hypotheses, thereby increasing power. In contrast to the approach in the previous subsection, the Maurer and Bretz (2013) procedure does not directly use the covariance among statistics for different hypotheses (though it uses the covariance among statistics from the same hypothesis across stages). We summarize this reallocation procedure, referred to as H M B ; a more detailed description is in the supplemental materials.\nThe first step in H M B is for investigators to prespecify a weighted Bonferroni procedure to adjust for multiple testing across the hypotheses H1, H2 and HC , with corresponding weights w1, w2 and wC . Specifically, for each j \u2208 {1, 2, C}, the null hypothesis Hj is tested using efficacy boundaries e (k) j , k = 1, . . . , K, corresponding to a standard group sequential design for a single null hypothesis with total Type I error wj\u03b1; the fraction of wj\u03b1 allocated to each stage is prespecified, and efficacy boundaries are created using an error spending function analogous to (1) except only considering a single null hypothesis. These boundaries are used until at least one null hypothesis is rejected, at which point reallocation occurs, as described next.\nAfter a null hypothesis Hj is rejected, its weight wj is reallocated across the remaining null hypotheses. This reallocation follows a graphical procedure of Bretz et al. (2009) , which must be prespecified. Each node in the graph corresponds to a null hypothesis, and there is a directed edge between each pair of nodes. The edge from Hi to Hj has a transition weight gij \u2208 [0, 1] that indicates the proportion of the weight at node i reallocated to Hj after rejection of Hi. Each rejection also requires updating the edges gij, as transitions to the rejected hypotheses become defunct. We use gij to refer to the initial transition weight, before any hypothesis has been rejected. Maurer and Bretz (2013) prove that their procedure strongly controls the familywise Type I error rate.\nThe required inputs to the multiple testing procedure H M B are nonnegative values \u03b1 j /\u03b1. These inputs correspond to the unstructured design using H M B ; a structured design, with fewer inputs, is defined analogously as in the previous subsection."}, {"section_title": "Optimization Problem", "text": ""}, {"section_title": "Search Space", "text": "Let D denote a trial design, which consists of the list of design parameters necessary to fully specify the decision rule and testing procedure for a trial, i.e., the following: the maximum number of stages K (which we restrict to be at most 10), the maximum sample size per stage {n k } k\u2264K (each n k being a positive integer), alpha allocations {\u03b1 (k) j \u2265 0 : j \u2208 {1, 2, C}, k \u2264 K} (which sum to \u03b1), futility boundaries {f (k) j \u2208 R : j \u2208 {1, 2, C}, k \u2264 K}, a hypothesis testing framework (H M B or H COV ), and the alpha reallocation rule if using H M B . The class of trial designs satisfying the above restrictions is the search space for the unstructured optimization problem. The structured optimization problem has a reduced search space as described at the end of Section 3.2."}, {"section_title": "Constraints", "text": "We next define the power and Type I error constraints. Let \u03b4 = (\u03b4 (1) , \u03b4 (2) ) denote a vector of possible values for the treatment effect in each subpopulation, and let \u03b4 min > 0 denote the minimum value of 6 http://biostats.bepress.com/jhubiostat/paper279 the treatment effect that is clinically meaningful. We consider the following values for \u03b4:\nLet 1 \u2212 \u03b2j(\u03b4 , D) be the power of the design D to reject at least Hj by the end of the trial when (\u03b41, \u03b42) is equal to the vector \u03b4 . Let P \u03b4 1 ,\u03b4 2 ,D and E \u03b4 1 ,\u03b4 2 ,D , respectively, denote probability and expectation with respect to the z-statistics defined in Section 3.1 for design D under treatment effects (\u03b41, \u03b42); we assume this distribution of z-statistics is multivariate normal with covariance matrix given in Section 3.1. We impose the following constraints on power and familywise Type I error: Constraints (a)-(c), respectively, represent having at least 80% power to reject H1 when the treatment only benefits subpopulation 1 (at level \u03b4 min ), at least 80% power to reject H2 when the treatment only benefits subpopulation 2, and at least 80% power to reject the combined population null hypothesis HC when the treatment benefits both subpopulations."}, {"section_title": "Objective Function", "text": "The objective function for the optimization problem is the expected number enrolled. (We also consider the expected trial duration.) The expected number enrolled is computed with respect to a prespecified distribution \u039b on the treatment effects (\u03b41, \u03b42). We refer to this as the prior distribution on the treatment effects. However, all of our designs have guaranteed asymptotic, familywise Type I error control without regard to this prior, i.e., it holds for any possible pair (\u03b41, \u03b42).\nWe aim to minimize the following objective function (representing expected number enrolled) over the space of trial designs D that satisfy constraints (a)-(d):\nwhere\u00f1(D) denotes the inner expectation on the right side of (2), which represents the expected number enrolled under P \u03b4 1 ,\u03b4 2 ,D . In this paper, we set \u039b to be a discrete distribution with equal mass at \u03b4 (0) , \u03b4 (1) , \u03b4 (2) , and \u03b4 (C) . Under such a prior, E\u039b{\u00f1(D)} is the expected number enrolled across these four scenarios. While minimizing expected number enrolled is our primary goal, we also consider the problem of minimizing expected duration -the expected time from the start of enrollment until all accrual has stopped. This expectation is taken with respect to the same prior for the treatment effects.\nThe power constraints (a)-(c), Type I error constraints (d), and objective function (2) depend on the design parameters (encoded in D) and the population parameters \u03c3 2 cj , \u03c3 2 tj , \u03c0j, \u03b4 min . This is because these parameters are sufficient to determine the distribution of the z-statistics in Section 3.1, and all designs here use the data only through these z-statistics. For a given application (such as the MISTIE and ADNI applications described below), it suffices to specify the population parameters \u03c3 2 cj , \u03c3 2 tj , \u03c0j, \u03b4 min , and the class of designs to be searched over, in order to fully define the corresponding optimization problem. The problem also depends on the prior \u0393, defined above.\nDue to the difficulty in directly solving the optimization problem (2) under the power and Type I error constraints, we instead define an unconstrained optimization problem where the constraints are incorporated as penalty terms as in (Wason and Jaki, 2012; . The unconstrained objective function we aim to minimize over D is\n7"}, {"section_title": "Optimization Using Simulated Annealing", "text": "We search for argmin D J(D) using simulated annealing (SA). The general form of SA is as follows. Given a trial design D as a reference point, SA randomly perturbs D in order to generate a new proposal design D . If J(D ) < J(D) then the proposal is \"accepted,\" and D becomes the new reference point. If J(D ) > J(D), then D is accepted according to a certain probability, and discarded otherwise. The nonzero probability of exploring undesirable regions of the parameter space allows SA to avoid becoming stuck at a local minimum. As the algorithm progresses, new proposal designs D are taken from a closer neighborhood around the reference design, and the probability of accepting inferior designs decreases. Both of these changes are modulated by a parameter known as the \"temperature,\" which decreases with each iteration. We use the variant of SA implemented in the optim function in R, which is based on the algorithm of (B\u00e9lisle, 1992) . We implemented SA in parallel across 100 nodes, each starting with a different random seed. Our implementation is \"embarrassingly parallel\" in that each node runs the SA algorithm independent of the others (i.e., without communication between nodes); when the SA search terminates for all nodes, we select the best design found, denoted DSA.\nThe search space for D is defined in Section 4.1. Separate searches are performed for the two hypothesis testing frameworks H M B and H COV . One difficulty is that the dimension of this search space changes with the value of K, since greater values of K require additional sample sizes, efficacy boundaries, and futility boundaries. We give details on our method to address this issue in the supplemental materials.\nThe SA algorithm allows design parameters to take any real values, which may violate the constraints on our search space of feasible designs. In particular, since the alpha allocated to each test at each stage must be bounded between 0 and \u03b1, we instead use SA to search for the logit transform of the alpha allocated, i.e., log{\u03b1\nWe then transform proposed values for logit(\u03b1 (k) j ) back to the (0,1) interval, and rescale them to sum to \u03b1. In the same way, we search over the logit of the graph transition weights gij \u2208 [0, 1] when using the framework H M B . Non-negative and integer constraints (e.g. for n k and K) are achieved by truncating and rounding respectively. We restricted the designs to have at most K = 10 stages. Additionally, rather than searching for each individual n k , we search across the space for N and separately search over the proportion of N allocated to each stage. Under this parametrization, the maximum sample size can naturally be changed without affecting the efficacy boundaries, as the efficacy boundaries depend only on the relative sample sizes n k /N for each k.\nPenalized approaches such as (Wason and Jaki, 2012; , or approaches based on (3), will not necessarily guarantee that the resulting optimized design meets the power constraints in Section 4.2, as there may be cases where a small penalty is outweighed by a larger reduction in expected number enrolled. For the designs proposed by (Wason and Jaki, 2012; , these concerns also apply to Type I error control.\nTo address the above issue, we built in an extra step to correct for cases where, after the SA algorithm completes, the resulting design DSA fails to satisfy one or more of the power constraints. This step involves starting with DSA, and increasing only the total sample size parameter N . A binary search over N is conducted to find the smallest value such that the constraints in Section 4.2 are met. During this search, all other elements of DSA are held constant. When implementing the SA procedure in parallel, we apply this extra step after SA completes in each node. These supplemental searches also reduce the danger of choosing the tuning parameter \u03bb in (3) to be too small. Our specific use of binary search is motivated by our empirical experience of E\u039b{\u00f1(D)} being monotonically increasing in N for a variety of tested scenarios, and by the fact that the power constraints can always be satisfied by a sufficient increase to N as long as each \u03b1 (k) j > 0. In order to derive designs that are simpler to interpret and perform approximately optimally, we propose a two-step procedure for discovering efficacy and futility boundaries. First, we optimize as above, and refer to the resulting design as \"unstructured\". Based on the resulting design, we next construct a lower dimensional parametrization that has a simpler form, and solve the same optimization problem in this restricted space. If the value of the objective function is very close to that attained in the unrestricted case, we report the simpler \"structured\" solution along with the \"unstructured\" one, as the former may be easier to communicate. We discuss our specific choice of structured boundaries in Section 5."}, {"section_title": "Comparison Designs", "text": "We compare optimized adaptive designs against three types of simpler designs, denoted \"non-optimized single stage designs,\" \"optimized single stage designs,\" and \"non-optimized multistage designs.\" The term \"optimized\" means that design parameters are optimized using our SA approach (or by grid search for the simplest designs). Single stage (i.e., K = 1) designs can be optimized in terms of their multiple testing procedure design parameters, such as \u03b1 (1) j and gij. We define non-optimized single stage designs as trials with equal alpha allocation (i.e., each \u03b1\n(1) j = \u03b1/3) and reallocation (i.e., each gij = 1/2). We define optimized single stage designs as trials where the alpha allocations and reallocations are computed either through a grid search (for H COV ) or through SA (for H M B ).\nWe define non-optimized multistage designs as 5-stage trials with each n k equal to a common value, equal alpha allocation and reallocation (i.e., each gij = 1/2) across hypotheses, futility boundaries set equal to zero, and the initial alpha allocations across stages set according to the structured alpha spending function in Section 3.2, with \u03c1ej set equal to either 1 or 3 for all j. These settings for \u03c1ej result in boundaries similar to those of Pocock (1977) or O'Brien and Fleming (1979) , respectively (Jennison and Turnbull, 1999) . We refer to these sets of boundaries as Pocock and O'Brien-Fleming boundaries, respectively. For all comparison designs, the maximum sample size was selected to be the smallest value that satisfied the power constraints in Section 4.2."}, {"section_title": "Results", "text": ""}, {"section_title": "Overview", "text": "We compare the performance of optimized adaptive designs versus the simpler designs as described in Section 4.5. We find that optimized designs can offer substantial reductions in expected number enrolled for trials where the delay time between enrollment and outcome measurement is relatively small compared to the enrollment rate. The reason is that in such trials, enrollment has not yet been exhausted before enough information has accrued to make a useful decision about changing enrollment criteria. The MISTIE example illustrates such a case, while the ADNI example demonstrates the opposite situation.\nSingle stage (non-adaptive) designs generally have a lower maximum number enrolled than group sequential designs and adaptive enrichment designs, but at the cost of a higher expected number enrolled due to their lack of ability to stop early. We illustrate this tradeoff in the MISTIE and ADNI examples."}, {"section_title": "9", "text": "Hosted by The Berkeley Electronic Press"}, {"section_title": "MISTIE Example", "text": "Before presenting results for the MISTIE example, we list the inputs to the corresponding trial design problem that are used to set the power constraints and objective function in Sections 4.2-4.3. The enrollment rate and subpopulation proportions are given in Section 2.1. Recall the primary outcome, measured at 180 days (d = 1/2 year), is the indicator of having mRS score 3 or less (called a successful outcome). The probability of a successful outcome after 180 days was projected to be 0.290 under control, for each subpopulation. Investigators aimed to satisfy the power constraints listed in Section 3 for \u03b4 min = 0.122. The variance of the outcome under control is assumed to be \u03c3 The first row of Figure 1 shows the z-statistic boundaries and per-stage sample sizes for the optimized adaptive designs using H COV and H M B , respectively. These boundaries are the result of the unstructured search described in the last paragraph of Section 4.4. For H M B , the boundaries shown are those before any alpha-reallocation has taken place (referred to as \"initial\"). Initial efficacy boundaries for H COV and H M B are similar, each roughly resembling Pocock boundaries. Futility boundaries are similar across hypothesis testing frameworks as well, with futility boundaries for H1 or H2 being highest at the midpoint of the trial, and futility boundaries for HC remaining low throughout the trial. Within a given design, symmetry between the futility boundaries for H1 and H2 is not necessarily to be expected, as \u03c01 = 0.33, \u03c02 = 0.67.\nWe implemented the 2-step procedure described in the last paragraph of Section 4.4.\nStep 1 was the optimization over unstructured designs leading to the sample sizes and boundaries in the top row of Figure 1 . Based on these results, we proposed the following structured form for the futility boundaries in step 2: set f\n, where cj, lj, and \u03c1 f j are unrestricted design parameters for j = 1, 2, C. This form encompasses boundaries similar to those of Pocock (1977) and O'Brien and Fleming (1979) , with additional shift parameters cj to capture the behavior discovered in the first row of Figure 1 . We also restrict to the structured efficacy boundaries described in Section 3.2. We reapplied our SA procedure optimizing over this restricted set of parameters that includes cj, lj, \u03c1 f j , wj, \u03c1ej (and K, n k , H M B or H M B , gij). Starting values for both steps of this search are given in the supplemental materials. The resulting optimized designs are shown in the bottom row of Figure 1 . These structured designs have expected number enrolled within approximately 1% of the unstructured optimized designs. Figure 2 shows the distribution of the number enrolled in the MISTIE example for the optimized structured multistage designs (i.e., those in the bottom row of Figure 1) , and for the simpler comparison designs described in Section 4.5. The distributions for multistage designs are shown as violin plots, and the fixed number enrolled for the optimized and non-optimized single stage designs are shown as horizontal lines. All distributions are calculated based on the prior distribution for the treatment effects in Section 4.2. Our optimized adaptive designs using COV and H M B , respectively; when using O'Brien-Fleming boundaries, the analogous results are 1682 and 1652. In summary, the optimized adaptive designs have a substantially lower expected number enrolled than all the simpler designs, but have higher maximum number enrolled than the single stage designs. Specifically, the optimized adaptive design using H M B has 32% (462 participants) lower expected number enrolled but 22% (319 participants) greater maximum number enrolled versus the the best competitor (the optimized single stage design) among those we considered.\nWe next examine the progress of the SA algorithm versus the number of search iterations. Figure for four different optimized adaptive designs (one in each panel) are shown. Boundaries for H 1 , H 2 , and H C are respectively shown as solid, dotted, and dashed lines. Efficacy boundaries (blue) are marked by circles, and futility boundaries (red) are marked by triangles. Boundaries are on the z-scale, and the horizontal axis represents the sample size at the time of the interim analysis (if there is no early stopping). Each of the 2 columns of panels corresponds to a different hypothesis testing framework, with H COV on the left and H M B on the right. The top and bottom rows of panels show boundaries for the designs resulting from optimizing using the unstructured and structured methods, respectively, described in the last paragraph of Section 4.4. For H M B , the boundaries shown represent those before alpha reallocation; the alpha reallocation rules are given in the supplemental materials, along with the initial alpha allocations for all four designs."}, {"section_title": "11", "text": "Hosted by The Berkeley Electronic Press .) The most notable increases in performance occur in the early stages of SA, after which the distribution of performance across nodes remains relatively constant. This implies that a reduced number of search iterations might achieve similar performance if the temperature parameter of the search was set to decrease more slowly.\nThe quartile lines in Figure 3 can be used to roughly approximate performance in cases where fewer computing resources would be available. For instance, if only 5 parallel nodes had been available, the probability of achieving a result below the first quartile would be approximately (1 \u2212 0.75 5 ) = 76%. This implies that there is a roughly 24% chance of getting a substantially larger expected sample size if the search used only 5 nodes compared to 100 nodes. This suggests that at least some parallelization is useful in the search. It is an area of future research to optimize the degree of parallelization in the search procedure."}, {"section_title": "ADNI Example", "text": "We next consider the ADNI data example. Inputs to the trial design problem, described next, were based on values observed in the ADNI data. The enrollment rate and subpopulation proportions are given in Section 2.2. Recall the primary outcome is the change in CDR-SB score measured at d = 2 years compared to baseline. The sample variance in the 2-year change in CDR-SB was approximately 3.35 for subpopulation 1, and 3.61 for subpopulation 2. We set \u03c3 adaptive trial in our search in the ADNI example had lower expected number enrolled relative to an optimized single stage design. As mentioned above, this can be largely attributed to the high enrollment required before any outcomes are measured. In such a case, it still may be possible to reduce the trial's expected duration using an adaptive enrichment design; such reductions can result from stopping the trial for efficacy or futility before all participants have their outcomes measured. We next optimized multistage designs for the ADNI example using trial duration rather than number enrolled in the objective function. Figure 4 shows performance comparisons for the ADNI example analogous to Figure 2 , but with the vertical axis showing trial duration rather than number enrolled. Relative to an optimized single stage design, the optimized multistage designs reduced expected duration by 5.4% for H COV , and 7.4% for H M B . However, this comes at the cost of a higher maximum duration. Relative to an optimized 1-stage design, the optimized multi-stage designs increase maximum duration from 5.07 to 5.84 years for H COV , and from 5.02 to 5.75 years for H M B ."}, {"section_title": "Alternative Optimization Algorithms", "text": "We also compared the performance of SA against other optimization algorithms available in the optim function in R. For each combination of testing procedure (H M B or H COV ), application (ADNI or MISTIE) and boundary form (structured or unstructured), each optimization method was allowed to run on 250 parallel nodes for either 4 hours or 2500 iterations, whichever occurred first. Rather than searching for the optimal number of stages (K), we fixed K within a node at either 2, 3, 4, 5, or 6. These values for K were evenly distributed such that each unique configuration was allotted 250/5 = 50 parallel nodes with different starting seeds. The minimum objective function value across all 250 parallel nodes was recorded for comparison.\nSA outperformed gradient methods such as BFGS, L-BFGS-B, and Conjugate Gradient by 3-6% in the ADNI example for expected duration and 7-27% in the MISTIE example for expected number enrolled. Nelder-Mead and SA performed much more similarly, with Nelder-Mead outperforming SA by approximately 1% in the ADNI example, and SA outperforming Nelder-Mead by approximately"}, {"section_title": "13", "text": "Hosted by The Berkeley Electronic Press "}, {"section_title": "1% in the MISTIE example.", "text": "We also compared against a version of SA where the objective function for the current design D is re-evaluated (using a new Monte Carlo sample) at each comparison to a new candidate design D , as discussed in the conclusion of Branke et al. (2008) . Such an approach doubles the number of simulations, but decreases the probability that the algorithm becomes stuck at an inferior design whose objective function value is initially underestimated due to Monte Carlo error. This altered SA algorithm improved over gradient based methods, but was outperformed by both Nelder-Mead and by standard SA."}, {"section_title": "Discussion", "text": "We showed empirical evidence that SA can produce multistage adaptive enrichment designs with substantially lower expected number enrolled compared to optimized single stage designs, and to nonoptimized multistage designs that use approximate Pocock or O'Brien-Fleming boundaries. Relative to single stage designs, optimized multistage designs come at the cost of increases in maximum number enrolled. There is an analogous tradeoff between expected and maximum number enrolled comparing standard group sequential designs versus single stage designs (Eales and Jennison, 1992) . In the MISTIE example, the most striking difference between optimized and non-optimized trials is the futility boundaries, while the other parameters are roughly similar. We conjecture that this change in futility boundary is an important driver of improved trial performance.\nIn both the MISTIE and ADNI examples, we compared covariance-based (H COV ) and alphareallocation-based (H M B ) multiple testing procedures, based on optimizing multistage designs using each procedure. The resulting designs were similar in both their design parameters and their performance.\nThe optimized designs in this paper are the best designs found by the SA algorithm we used.\n14 http://biostats.bepress.com/jhubiostat/paper279\nThese could be local optima. It is an open problem to determine how close these are to global optima for the problems we addressed. This is because no current method exists to find the global optimum in these problems due to the relatively large number of design parameters being simultaneously searched over. One exciting area of future work is to modify the search algorithm to actively account for the Monte Carlo simulation error in each objective function evaluation. Some optimization methods leverage noise present in the objective function, or add noise to the objective function (Kushner, 1987; Maryak and Chin, 2001) , in order to increase the probability of reaching a global minimum. In the specific context of SA, (Fink, 1998; Branke et al., 2008) argue that noise in the objective function is analogous to having a higher temperature parameter."}]