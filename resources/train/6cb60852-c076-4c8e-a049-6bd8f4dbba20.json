[{"section_title": "RESPONDING TO A CHANGING CLIMATE", "text": ""}, {"section_title": "The Need for Improved Decision Support", "text": "Much of U.S. energy infrastructure is located on its coasts, and in recent years major hurricanes have affected substantial portions of the nation's energy system. For example, Hurricane Katrina in August of 2005, followed by Hurricane Rita in September, delivered the \"world's first integrated energy shock\" (Yergin, 2006) as they disrupted oil, natural gas, and electric power generation, halting a quarter of U.S. oil production, one fifth of U.S. refining capacity and flooding several power generation plants. And in 2012, hurricane Sandy flooded extensive areas of New York City and Northern New Jersey. A large portion of the damage in these cases was the result of flooding from storm surge. In coming decades the risks of coastal damage are projected to increase. Climate change brings increasing sea surface temperature, the engine of tropical storms, allowing them to grow in intensity and destructive power (Emanuel, 2005). Rising atmospheric and ocean temperatures also are causing sea level rise, through thermal expansion of the oceans and melting of mountain glaciers and continental ice sheets. Further contributing to flood risk in some areas is subsidence, by natural geological process or as stimulated by the removal of subsurface water and fossil fuels. All these processes are subject to uncertainty, and thus public and private officials responsible for these facilities face complex decisions about if, when, and how to take protective measures. The main source of information about flood risk in the U.S. is the Federal Energy Management Agency (FEMA), which develops maps of 100-and 500-year flood risk (FEMA). It is a massive effort, covering the whole country, and the analysis of coastal flood risk includes elaborate statistical analysis of storm surge. FEMA mapping is the basis of the Federal Flood Insurance Program, and is the most important source of information for current public and private decisions about investment, building regulations, zoning, etc. Because of the focus on current facility and land-use decisions and insurance the FEMA analysis is based on current climatology and does not provide information about changing risk in future decades. Here we address what type of information would be useful as support for adaptation decisions over time and how it might be developed. In addition, we explore how risk information might be applied as an input to support current decisions considering the fact that adaptation is a sequential process. What is economic to do now is dependent on the opportunity for action later, which leads conveniently to the formation of decision support for adaptation in a dynamic programming framework."}, {"section_title": "A SAMPLE ANALYSIS", "text": "A complete analysis of risk to coastal energy infrastructure would need to consider a variety of types of facilities, different forms of damage (wind, flooding from rainfall), change in risks across locations, various emissions scenarios and multiple projections of climate response. To demonstrate a possible approach to the development of such studies, we narrow the scope of the analysis to the risk of flooding from sea level rise, storm surge and subsidence of a sample facility in Galveston Bay. The Galveston area has seen extremely destructive hurricanes, the deadliest of which in 1900 destroyed the city of Galveston and killed upwards of 6000 people. The 20 th century saw adaptation efforts, and still in 2008 Hurricane Ike caused nearly $30 billion in damage to the City. Galveston Bay is an active seaport populated with oil refineries, a number of natural gas and coal power plants and components of the U.S. Strategic Petroleum Reserve. A sample of the largest facilities is shown in Figure 1. The land around Galveston Bay is low lying, and much of this energy infrastructure sits 5 to15 feet above mean sea level. Further, the area is experiencing severe subsidence (Galloway et al., 1999); some regions have sunk as much as ten feet over the past century, thought to be mainly due to the removal of ground-water and the production of oil and natural gas. Two other points in Figure 1 are of special note. The center of the 100 km circle is the reference point for aspects of the hurricane analysis below. And Point A is the location of the particular facility that is carried through the analysis of changing flood risk and adaptation decisions. For this case study we consider conditions under just one emissions scenario, A1B, as defined by the IPCC and applied in its Fourth Assessment Report or AR4 (Solomon et al., 2007). Results from 18 Atmosphere-Ocean General Circulation Models (AOGCMs) reviewed in the AR4 project that temperature change over this century will be somewhere between 1.7 and 4.4 o C under this scenario, and that sea level rise will be between 0.21 and 0.48 m. We employ four of the 18 archived model scenarios in the analysis of hurricane arrivals, and an overlapping set of six provide inputs to estimates of one of the components of sea level rise. All the analysis that follows therefore is conditional on this A1B scenario. Also, the results of each AOGCM are conditional on just the particular run of the model included in the AR4 archive and do not consider any uncertainty in the model itself or its natural variability (Deser et al.)."}, {"section_title": "PHYSICAL ANALYSIS OF FLOOD RISK", "text": "For analysis of adaptation to changing flood risk we first need a description of the annual flood risk and how it is projected to change over time-in this study as influenced by the change between the climate in 1981-2000 and that projected for 2080-2100 1 . Two climate-related influences need to be considered: storm surge and sea level rise. To estimate the risk at a particular geographical point account also needs to be taken of land subsidence and astronomical tides. These four processes are then combined to produce the estimate of flood risk. The estimation of storm surge risk begins with the generation of the tracks and wind fields of a large set of synthetic hurricanes, driven by climatic conditions represented in the AOGCMs used, passing through Galveston Bay. Statistics are estimated, first for the wind conditions of the sample of storms and, after passing through a model of storm surge, for the associated surge level at the point of interest. Statistics for the storms that do arrive are then combined with analysis of the frequency of arrival to yield estimates of the annual probability of storm surge at different levels above mean sea level. These results are combined with an assumption about tidal conditions at the time of storm arrival. Sea level rise is considered in two parts: the contribution of thermal expansion of the ocean and the melting of glaciers and small ice caps, and the potential loss from the continental ice sheets of Greenland and Antarctica. Finally, since it is the relative sea level at the facility that matters, contribution to future risk of subsidence is added. The ultimate result is the probability that the facility will be flooded in 2000 and in 2100, which can be interpolated to the decades in between to support analysis (in Section 4) of when flood protection should be added to a facility at Point A in Figure 1, and at what level."}, {"section_title": "Storm and Storm Surge", "text": ""}, {"section_title": "Storm Generation", "text": "Each year approximately ten tropical storms develop in the North Atlantic, Caribbean, and Gulf of Mexico. On average six of these develop into hurricanes, of which only one or two make landfall in the U.S. (NOAA). Because the record of hurricane activity is so limited, analysis of flood risk cannot be based strictly on historical data. We apply a statistical-deterministic hurricane model developed by Emanuel et al. (2006) and Emanuel et al. (2008) to generate a large number of synthetic storms and a hydrodynamic model to generate the storm surges induced by these storms (Lin et al., 2010) and (Lin et al., 2012). The synthetic storms are simulated under the 2000 climate and projected 2100 climate to see how changes in climate alters storm and storm surge risk. Briefly summarized, the procedure involves three steps: \u2022 Genesis by Random Seeding. Storm seeds are distributed randomly throughout the Atlantic Basin as warm-core vortices with minimum peak winds. Most quickly decay.   This procedure is applied under conditions simulated in each climate model for current conditions (2000) and for 2100 based on the version of the A1B emissions scenario as recorded in the World Climate Research Program (WCRP) third Climate Model Intercomparison Project (CMIP3) multimodel data set (Meehl et al., 2007). The four models used are listed in Table 1. For each of the four climate models and for each of the two climate conditions, we generate 3000 storms for the study area. The output of each storm track provides information on a 2-hour time step including storm location, radius of maximum wind, maximum wind speed and storm center pressure. Wind speed provides one measure of storm intensity, and the 3000 tracks through the filter provide a basis for constructing a distribution of storm intensities at Galveston Bay, conditional on a storm arriving that meets the minimum wind speed criterion above. As an example, Figure 3 shows the cumulative distribution of the storm maximum wind speed when the storm is at its closest point to the reference location in the Bay. Included are results for the year 2000 for each of the four climate models. These distributions then differ between 2000 and 2100 and the results for windspeed for all four models are presented in Figure 4. The biggest difference in windspeed is seen most clearly in the GFDL model where the probability of an arriving storm having a maximum wind speed greater than 100 mph increases from 0.2 in 2000 to 0.3 in 2100. The other models produce a smaller change even though they project greater temperature increases. This difference among models is not surprising since neither atmospheric nor sea surface temperatures alone are enough to determine the intensity of tropical storms. Other environmental factors including vertical wind shear, temperature distribution of the upper ocean, and humidity may all contribute to a storm's intensity. 2"}, {"section_title": "Surge Simulation", "text": "As hurricane intensity increases there is an anticipation of increased storm surge, leading to a greater potential for flooding. Storm surge occurs when sustained wind forces act on a body of water, forcing coastal shallow water up onto the shore. The height of the surge is determined by a number of factors including storm track, intensity and size, as well as coastal geometry and bathymetry. Given all of these factors, it is difficult to make straightforward calculations relating storm intensity to storm surge. There are attempts to generalize the relationship between storm intensity and storm destructive potential (including the surge effect), like that described by the Saffir-Simpson scale, but since maximum wind speed alone does not determine the height of the surge, we apply the SLOSH model (Sea, Lake and Overland Surges from Hurricane) developed by the U.S. National Oceanic an Atmospheric Administration (Jelesnianski et al., 1992) for a more detailed storm surge analysis. Results from each hurricane run are input to this model of  Galveston Bay's surrounding coastline to generate corresponding surge events in the 2000 and 2100 climates. The storm surge heights used in this analysis are the maximum levels generated by SLOSH at the location closest to our facility of interest, which is 5 ft. above mean sea level (point A in Figure 1). Note that sea level rise is not included at this point in the analysis. We can compare the differences in surge heights across climates by contrasting the probability density function in 2000 against that in 2100 in each model. These results are shown in Figure 5, and the analysis indicates that, for the GFDL model, the probability of an arriving surge exceeding 5 ft at Point A is projected to increase from 5% in 2000 to 7% in 2100. This change is smaller for the other three models used in our analysis. (Note again that these distributions of surge heights are conditional on storm arrival, and the risk does not yet include astronomical tide, sea level rise or subsidence.) "}, {"section_title": "Frequency and Probability of Surge", "text": "A number of conditions must exist for the seeds to develop into tropical storms. Hurricanes require pre-existing weather disturbances, atmospheric moisture, warm ocean water, and small vertical shear of prevailing winds. The degree to which these ideal conditions are met changes with a warming climate, and a different potential is reached for these seeds to develop into storms. Each climate model projects different conditions and so produces a different frequency of intense storms. These annual frequencies for each model's 2000 climate are calibrated to be in statistical agreement with the historical annual frequency. This same calibration coefficient is applied respectively to the 2100 climate of each model. Also, an additional local calibration is performed using the historical storm frequency for the study site from the hurricane database, HURDAT, which is a historical record of Atlantic storms (Jarvinen and Lawrence, 1985). The HURDAT data set was updated in 2001 and again in 2002 to include reanalysis data for storm tracks as far back as 1851. In our analysis the annual frequency for the 2000 climate period in Galveston Bay, taken from HURDAT, is the observed number of tropical storms, above the specified minimum threshold of 34 mph, that pass through the filter in Figure 1 in a year. According to HURDAT, this frequency is 0.26, so the annual frequency for each model, under the 2000 climate, is set to this level, and the frequency for each model under the 2100 climate is then estimated based on the simulated frequency ratio (between the 2000 and 2100 climates). The estimated annual frequencies for ECHAM, GFDL, MIROC, and CNRM models are 0.28, 0.61, 0.29, and 0.28, respectively. Note that the GFDL model estimates a substantial increase in frequency relative to the other three models. We assume that hurricane arrival times are independent of one another and therefore that they follow a Poisson distribution with annual frequency as the parameter. The probability distributions of storm conditions in Figures 3 to 5 show the risk conditional on storm arrival. We seek, however, to find the distribution of the annual risk of flooding. Since this is a Poisson process, more than one storm could arrive in a year, but similarly there will be years with no storms. If more than one storm were to arrive in a year, we would only need to count the storm with the highest achieved surge height since any lesser storm's height would be exceeded by the most extreme. To combine all processes, we use a numerical approximation to derive the resulting distribution from the Poisson storm arrival process. Define N k as the number of storm arrivals in one year. We sample 100,000 times by first sampling the number of storms, N k , from a Poisson distribution, then drawing N k times from the distribution of storm surges given an arrival. We store the highest of the N k storms for our yearly arrival height; if N k = 0 then the highest surge height is stored at zero for that sample. The result is a new distribution, expressing the annual probability of surge heights, which we carry forward for the remainder of the analysis. In this analysis, the astronomical tides in Galveston are approximately one foot, we linearly add tidal heights to surge heights by numerically drawing from a sinusoidal curve of magnitude one foot. In locations where tides are greater, their impacts on the height of a storm surge has the potential to be more significant. An incoming storm arriving at the same time as an incoming tide usually contributes to a resulting water height less extreme than the linear combination of the tide and surge (e.g., Lin et al. (2012)). Therefore, using the linear combination is conservative in the sense that it assumes the less fortunate combination of events."}, {"section_title": "Sea Level Rise", "text": "The increasing risk from rising sea level is decomposed into thermal expansion plus flow from melting glaciers and ice caps (the better-understood component) and contributions from the continental ice sheets, especially the Greenland Ice Sheet (GIS) and the West Antarctic Ice Sheet (WAIS). For each contributor we include an analysis of uncertainty in change over the century. Considering the shortcomings of the underlying scientific understanding of the dynamics of the GIS and WAIS, our effort should be viewed less as a well-informed estimate and more as a demonstration of the type of analysis needed to support risk-based adaptation decisions."}, {"section_title": "Thermal Expansion Plus Glaciers and Ice Caps", "text": "There are six AOGCMs in the IPCC AR4 that include sea level projections in their results ( Table 3). These sea level projections include thermal expansion as well as small contributions from glaciers and ice caps. (They also include a small addition from continental ice sheets, but not the dramatic effect that could occur if rising air and ocean temperatures accelerate the flow off the GIS and WAIS.) We perform a linear regression with the GCM results in Table 3, to derive an estimate of the relationship between temperature change and their estimate of sea level rise. Katsman et al. (2008) suggest that the relationship between thermal expansion and atmospheric temperature is linear and this assumption underlies the procedure. The estimate is based on the decade-averaged change in temperature between 2000 and 2100 and the decade averaged change in sea level between 2000 and 2100. The temperature change pathway is not included in the analysis; we assume the resulting change that is projected at the end of the century to be sufficient. We also assume that the residuals are normally distributed. For details see Lickley. By this method the risk of thermal expansion follows a normal distribution with mean of 0.027T i + 0.19 where T i is the decade averaged temperature change across the century, and the norm of the residuals implies a variance of 0.17367 2 . The incorporation of this result in the overall risk analysis is discussed below."}, {"section_title": "Continental Ice Sheets", "text": "In the current century the main threat from loss of continental ice is from the GIS and the WAIS. Unfortunately, great uncertainty is attached to the behavior of these sources, presenting a severe challenge to the incorporation of this risk in an analysis of flood potential and adaptation decision. On the other hand, it would be misleading to ignore this component because some analyses project that the GIS and WAIS could contribute as much as 2 m of sea level by 2100. 3  to the RICE model, anticipates that, under an emission trajectory similar to A1B, we will see a sea level rise of 0.73 m by 2100. His analysis considers the GIS and WAIS ice sheets individually, in an analysis that extends out several hundred years. The loss from the GIS starts with an estimate of its long-term ice volume under different temperature conditions and imposes a dynamic process of progression over time. WAIS is assumed to begin to melt only at 3 o C above current temperature and to discharge from that point at an increasing rate as temperature further rises. This procedure leads to a GIS contribution of 0.2 m and a very small release from the WAIS. Some of these studies do not clearly distinguish the great ice sheets from other contributors to an overall sea level rise result, and they are based on different assumptions about the increase in global temperature over the period. Finally, most produce an estimate of global average sea level while the Katsman analysis tries to account for the rise at a particular point (the Netherlands). Taking these factors into account we represent the contribution of the continental ice sheets to sea level rise in Galveston Bay by a lognormal distribution with mean 0.6 m, and standard deviation of 0.14 m."}, {"section_title": "Subsidence", "text": "We approach this portion of the analysis again with wide uncertainty. The rate of subsidence will depend on several factors, including what is extracted from the ground (water, natural gas, oil). Thus the rate of subsidence can to some degree be altered by changes in policy and human activity. Given these unknown factors, predicting the rate of subsidence is a challenge. We base our estimate on the previous century's subsidence levels ( Figure 6). The location of interest sits in a zone that has seen roughly between 3 and 4 feet of subsidence between 1906 and 2000. To estimate this coming century's subsidence, we use a triangular distribution. We assume that there will likely be a slower rate of subsidence given an increased understanding of the causes and potential developments in technologies to reduce subsidence. We therefore assume a mean of 2 feet and use 4 feet as our upper bound. We use zero for our lower bound and construct a triangular distribution on these three parameters.  "}, {"section_title": "Annual Probability of Flood Events", "text": "To account for the combined impacts of all of these risks, we combine these four distributions. For present conditions sea level rise and subsidence are ignored and only storm arrivals under current intensities, frequencies and tides are considered. For the 2100 climate we use a numerical approximation by sequentially sampling from each of the distributions and adding the sampled sea heights (or sunken ground levels in the case of subsidence) together linearly. We repeat this Monte Carlo procedure 10,000 times for each climate to get an approximation of the combined risk distribution. The change in risk distribution across climates is shown in Figure 7. For the GFDL model and under our stated assumptions, the analysis concludes that, for a facility sitting at 5 feet at Point A in Figure 1, there is a 1.27% chance of flood heights reaching or exceeding that facilities elevation in one year under 2000 conditions and that this probability increases to 61.5% under 2100 conditions. See Table 4 for the results from all four models."}, {"section_title": "ECONOMIC ADAPTATION TO RISING RISK", "text": ""}, {"section_title": "A Dynamic Programming Formulation", "text": "We assume that adaptation of the facility at Point A to this increased risk of coastal flooding will involve the construction and maintenance of levees 4 , and that at any time the option will be available to build a levee or add to an existing one. As risks increase over time, there will come a growing need for adaptation or abandonment, and this decision depends on both the then current flood risk and the expected future risk. For example, a refinery at the water's edge facing a rising flood risk faces decisions about whether to keep the facility or abandon the site and, if it is to be kept, the degree of protection to add now, say by raising a levee. These choices depend on the value of the facility, on the cost of different levels of protection and the damage that it might prevented over time, and on the possibility that it can be augmented later. It is a dynamic programming problem that takes account of the influence of current and future conditions on todays choice. During each time period, we consider the state of the facility and all possible decisions available within that state. The state is the level of protection in place (the height of the levee in feet), and the decision options consist of the number of additional feet to build, including the option to do nothing. The expected loss at any point in time is a function of current as well as future states and actions. The procedure begins by optimizing in the last period and working back toward the present. To limit the decision options we only consider adaptation options between 2000 and 2100 and we allow decisions for additional structures to be made at each decade (this means that the last state under consideration for decision making is the 2090-2100). Further, we allow for up to a 20 foot levee to be added on in 2 foot increments. Define S t to be the state and A t (S t ) to be the best action respectively during decade t, yielding the lowest expected costs at decade t in state S t . Further, let C t (S t , A t ) be the expected costs during decade t given state S t and action A t . For each time period, we calculate the best action and lowest value such that where r is the discount rate. C t (S t , A t ) is defined as: where E(Damage|S t , A t ) is the expected damage during the decadal time period given the state and action made at the beginning of that time period, c m is the cost of maintaining one foot-mile of sea wall, c b is the cost of building an additional foot-mile of sea wall, V t (S t ) is the value of expected current and future costs discounted to decade t, and m is the number of miles of sea wall necessary to protect the facility. Since we choose to look at decisions made every decade, each of these costs are accumulated over the decade, with the discount rate of r. We obtain probabilities of yearly inundation by interpolating between the two probability distributions for each of the models in Figure 7. Figure 8 shows the interpolated risk profiles for GFDL. We interpolate the flood heights linearly between percentiles. For instance, if the 60 th percentile of flood heights is 0.5 feet in 2000 and 5.5 feet in 2100, then the 60 th percentile for the intermediate decades will be 1 foot in 2010, 1.5 feet in 2020, 2 feet in 2030 and so on. For the other components of the risk analysis (i.e. dynamic great ice sheet loss, subsidence and hurricane intensity), there is a lack of scientific understanding of how the ice loss will progress over the century. Due to the weak scientific basis, our default assumption is a linear trend in a changing risk profile. Future work is needed to develop simulations for the intermediate decades between 2000 and 2100. From these probability distributions, we can find P t (\u03b6 > \u03be + S t + A t ), which is the probability that the flood height, \u03b6, will exceed the wall height at elevation \u03be in a given year with a protection height of S t + A t . To derive the expected damages over the t th decade, we calculate the sum of expected damages for each year: where D is the damage incurred by a flooded facility and i is a year in decade t. By iterating backwards over time, we are able to derive the optimal wall height for each state."}, {"section_title": "The Current Decision to Protect", "text": "We illustrate this method using a case of a flooded facility of similar magnitude to facilities that exist at 5 feet above today's mean sea level in Galveston. The flood-related costs incurred by this facility include the sum of lost or damaged infrastructure, loss of operational capacity until repairs were finished, and any legal costs for damaged third-party property. Data is not available on potential damage to the refinery actually located at our point of interest in Galveston Bay, so to illustrate the method we construct a case of a 200,000 barrel per day refinery at that point and apply estimates loosely based on a facility flooded by hurricane Katrina. The resulting model parameters are the following: The result is the decision-making sequence that yields the minimum expected cost, flood damage plus protection. Note that this decision sequence is based on current perceived climate risks and will change as understanding of the risks changes over time. Under these parameters, the sequence of decisions for levee height with this model is outlined in Figure 9. Each climate model leads to a different sequence of heights with the GFDL model indicating the highest level of protection over time. CNRM and ECHAM produce an identical protection sequence, beginning with 4 feet of protection for the first decade. This decision-making framework also can be used to inform abandonment decisions. The associated costs with the GFDL sequence are $52.7 million. If today's net present value of the facility were lower than this amount, then the expected damages would outweigh the benefits of keeping the facility in operation and the economic response would be to abandon the facility. We apply this framework to a risk profile that remains constant over time, i.e. assuming no change in sea level or hurricane activity over time and no additional subsidence. The results show that the economic level of protection with no expectation of changing climate and continuing subsidence would be a 6 foot levee for both MIROC and GFDL, and 4 feet for ECHAM and CNRM. This framework also can illuminate situations where the economic decision might be viewed as infeasible at present. If a levee is thought unaffordable today or if regulations restrict the construction now, the analysis can estimate the cost of delay. For instance, if a sea wall can't be constructed in 2000 but is feasible in 2020, then the expected accumulated costs in 2020 will be $50 million. The total expected costs in 2000 would also need to include the expected costs of no protection for the first two decades. Using risk profiles for 2000 and 2010 the extra expected costs of leaving a facility unprotected during this period are an additional $148 million in expected damages incurred in that time frame, which could serve as a justification for relaxing financial constraints or changing regulations."}, {"section_title": "STEPS TO PRACTICAL APPLICATION", "text": "With the Galveston Bay example we demonstrate how analysis of changing coastal risk may be brought to useful application to decisions about adaptation. However, this sample application highlights the additional research and analysis needed to increase the usefulness of this approach to decision support. Some issues are well within the method applied here. Additional climate science research would include the calculation of one or more intermediate time steps in the storm and surge analysis, to inform the evolution of risk over time as displayed in Figure 8. Also, a more detailed surge model like ADCIRC could be applied. Additional economic analysis could make provision for economies In the economic of scale in protection investment, and could take account of the probability of levee failure. Current damage is considered to be equal for all flood levels. Variations in damage costs could be included for varying flood levels. Other potential improvements require more fundamental research. Of particular importance is the need for probabilistic analysis of the contribution of the continental ice sheets to sea level rise. Understanding of the dynamics of these ice sheets is poor, and knowledgeable scientists are understandably reluctant to put forward estimates of the likelihood of different levels of contribution-Analyses predict only a range-sea level shold rise more than X feet and may rise as much as Y feet. For decision support in the face of the combination of coastal risks more is needed, and in the absence of credible ice models, a process of expert elicitation of scientists engaged in the work would be superior to high and low scenarios containing no notion of likelihood. A further limitation of this and other efforts on coastal risk is that the risk results frequently are conditioned on so many assumptions. The analysis above is typical. First, it is conditional on the A1B emissions scenario, whereas there is considerable uncertainty regarding future emissions growth. Secondly, the result is conditioned on single runs of particular models. Some notion of the uncertainty in climate projection can be seen in the differences among the models used here (see Table 3). Unfortunately, this comparison among single runs of AOGCMs only partially accounts for the uncertainty in climate projection. Such studies have been done with earth models of intermediate complexity (e.g., Sokolov et al.), but those models do not represent the climate system with the complexity needed to support the track and intensity analysis developed by Emanuel et al. (2006) and Emanuel et al. (2008) and applied here. Therefore an important research task is to seek ways to introduce uncertainty analysis into the variables input to the storm simulations. If, however, some of these analysis problems can be alleviated, this approach can serve a number of emerging problems in the analysis of coastal risk and the timing and scale of adaptation measures. The estimation of the coming change in physical flood risk, even if conditioned on a number of assumptions as above, will be useful in anticipating the changes that may be required in the FEMA flood mapping system and the flood insurance programs that are tied to it. City, state and regional authorities faced with issues of zoning, building standards and efforts to anticipate future investment in protection would need analysis of this type to inform both magnitude and timing of actions. A productive avenue for next steps in research in this area, beyond the improvements in the analysis itself, is study of how such an effort might be structured, staffed and supported. Analysis of this type is available from consulting firms, for the private companies and government agencies that can afford to pay for it, but a more generally available source of analysis and insight is needed for the broader national challenge of dealing intelligently with the rising risk."}]