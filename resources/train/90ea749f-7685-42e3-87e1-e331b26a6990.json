[{"section_title": "", "text": "Doppler intuited that a sound's pitch could be altered by the relative velocity between the source and an observer-70 years later Hubble used the same principle and 42 data points to prove the universe was indeed expanding. 1 Arguably, no other data set of 0.042 Kb has done more to change our understanding of the cosmos. Although modest in volume, it took Hubble several years to acquire these precious numbers.\nNowadays we conduct neuroscience in a state of instant data overload. In a matter of hours we could produce a structural image of an individual's brain comprising a matrix of 256 \u00c2 256 \u00c2 128 \u00bc 8 388 608 data points, a resting-state functional magnetic resonance imaging (MRI) time series (83MB), and from a simple blood sample, derive the person's genetic sequence by GWAS (30 K), state of gene expression by microarray (another 30 K) and metabolomic profile using any of a number of commercially available chips (1 K). We need not necessarily stop there. In principle, the number of brain-gene-omic interactions on permutation alone approaches 10 19 . It is safe to predict that the ordinary person is not well placed to think about this uber matrix. Recall, the human brain 'only' contains 10 12 synapses. And even if we could literally combine brain power, it would take a cluster of 10 000 000 networked humans to allocate just a single synapse to any given interaction. Let's also ignore the fact that Matlab maxes out on a matrix greater than 8TB, or the pesky issue of Type I error.\nData reduction is therefore a key problem for a field trying to better understand the complexities of the human brain and how and why it sometimes breaks down. At the Workshop, we discussed two general counter-strategies. The first was datadriven, such as computationally intense graph theoretical analyses that fit within a wider systems biology framework that aims to reduce highly multi-dimensional data into less dimensional more tractable data. The second is having more time to think a great idea.\nGraph theory searches for invariant structures within large matrices, patterns of covariance that we now appreciate are related to the ability of complex systems to transfer information. Ed Bullmore identified four areas, where graph theory may help advance neuroscience. 2 First, it provides novel metrics for looking at pre-existing data in a new way. Second, it helps compare and converge results across multiple image (and non-image) modalities. Third, it may provide a tool for understanding how mechanistic changes at one level of enquiry relate to higher-order functional changes, as well as help translate findings from animal models to the human realm. 3 Finally, it may introduce new measures for use as outcomes in clinical trials, a topic of intense debate at the previous Workshop. 4 On the other hand, the psychology and neuroscience of great thinking is virtually non-existent. 5 Better insights come from literature and biography. Relativity is said to have occurred to Einstein as part of a process of mental simulation and imagination across a number of neighborhood walks. Proust began his Remembrances when the perfume of a madeleine over a lazy morning coffee triggered a wave of obsessional work.\nOrdinary mortals also need time to think. If you are coordinating 16 projects, supervising eight graduate students and drafting six papers simultaneously, it is unlikely you have much of it. Our Workshop under the warm Provencal sun is therefore designed to provide one of the predisposing conditions for great thinking, unfettered time.\nAlthough great thoughts can of course come anytime anyplace, the Workshop produced some contenders. Independently, two groups in Perth and Barcelona presented tantalizing new evidence about gene-environment interaction related to APOE4, otherwise considered a dementia and cardiovascular risk factor.\nDavid Bartres-Faz showed that TMS applied to the prefrontal cortex produces much more robust changes in functional connectivity in APOE4 positive elders than individuals without an E4 allele. Some of the resultant changes were an amplification of those associated with compensatory brain activity in successful ageing, and so may be of longitudinal benefit.\nRalph Martins presented how a general negative correlation between lifestyle-related physical activity and amyloid brain burden is much stronger in APOE4 carriers, suggesting that an exercise intervention may be particularly effective in those at most risk for developing dementia.\nSuddenly, APOE4 is seen in a new light. Perhaps not only is it a risk factor but also an opportunity to intervene more successfully. Perhaps, we need to combine TMS and exercise as part of a prevention trial in only APOE4 positive elders?\nYet the clearest example of original thinking was provided by Anders Fjell who approached old data with fresh eyes. He took advantage of the ADNI data set that is freely available to all researchers (http://adni.loni.ucla.edu). This wonderful resource means anyone with a decent computer can carry out a significant study on one of the world's largest brain MRI data sets at virtually no cost. Actually, not anyone. You need to know what to do with the data, and moreover, have an original idea. Anders' concern was the thorny issue of the distinction between so-called normal ageing and Alzheimer's disease (AD). 6 He separated those individuals with no evidence of long term cognitive decline or AD biomarkers-understood to preclude the possibility of subclinical AD-from several other patterns. He showed that longitudinal atrophy of the entorhinal cortex, one of the areas first targeted by AD, was detectable even in these 'super stable' cognitively-intact elders. Accelerated atrophy in those brain areas often linked with AD does not therefore uniquely signify neurodegenerative illness, but instead may also comprise 'normal aging'-whatever that is.\nTo be clear there is nothing wrong with big data-hungry science. Indeed, there has in recent years been an astonishing growth in generation of truly enormous data sets. Yet one forceful message from the Workshop was that along with truckloads of data we need a commensurate clearing out of mental space. Great thoughts are ultimately the best type of data reduction procedure."}]