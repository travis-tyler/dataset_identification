[{"section_title": "Abstract", "text": "The goal in network state prediction (NSP) is to classify the global label associated with features embedded in a graph. This graph structure encoding feature relationships is the key distinctive aspect of NSP compared to classical supervised learning. NSP arises in various applications: gene expression samples embedded in a protein-protein interaction (PPI) network, temporal snapshots of infrastructure or sensor networks, and fMRI coherence network samples from multiple subjects to name a few. Instances from these domains are typically \"wide\" (more features than samples), and thus, feature sub-selection is required for robust generalizable prediction. How to best employ the network structure in order to learn succinct connected subgraphs encompassing the most discriminative features becomes a central challenge. Prior work employs connected subgraph growth and sampling or graph smoothing within optimization frameworks, resulting in either large variance of quality or weak control over the connectivity of selected subgraphs.\nIn this work we propose an optimization framework for discriminative subgraph learning (DSL), which simultaneously enforces (i) sparsity, (ii) connectivity and (iii) high discriminative power of the resulting subgraphs of features. Our optimization algorithm is a single-step solution for the NSP and associated feature selection problem. It is rooted in the rich literature on maximal-margin optimization, spectral graph methods and sparse subspace self-representation. DSL simultaneously ensures solution interpretability and superior predictive power (up to 16% improvement in challenging instances compared to baselines), with execution times up to an hour for large instances."}, {"section_title": "Introduction", "text": "Global network state prediction (NSP) [16, 23, 8, 7] is a supervised learning problem in which features are embedded in a network as node/edge weights. The * University at Albany-SUNY, lzhang22@albany.edu \u2020 University at Albany-SUNY, pbogdanov@albany.edu basic premise in this setting is that the global state of the network is determined by local network processes which modify connected feature values in a predictable manner. Given a set of network samples over the same nodes and similar or identical interconnecting structure, how to select select connected subgraphs to accurately predict the global network states?\nThe NSP problem arises in multiple application domains: phenotype prediction based on gene expression within a protein interaction network [16] , learning rate prediction based on functional MRI scans [3] , congestion/normal regime prediction in communication networks, prediction of global phenomena in spatial sensor network samples and others. Common to all the above settings is the importance of network locality in selecting predictive features for the global state. In addition, datasets fitting this setting are typically \"wide\": involving many more features than labeled instances. Hence, it becomes imperative to learn robust and general predictors of the global state when only a subset of the features are considered, a problem commonly referred to as feature selection [5] . The distinctive characteristic of NSP is that the network structure can be exploited to detect robust and interpretable feature subsets.\nIntuitively, a good solution for the problem should identify a small number of features (i) sparsity, forming connected subgraphs (ii) connectivity, whose feature values accurately predict the global state (iii) discriminative power. Satisfying all three design principles simultaneously is a challenging task, hence, prior work typically prioritizes a subset of them. Some methods enforce connectivity by directly growing [16] or sampling [23] connected subgraphs. Such approaches suffer limited prediction quality and/or instability due to the local exploration of the exponential space of connected subgraphs. Other approaches enforce the design principles within optimization frameworks [8, 7] . Due to the inherent complexity of simultaneous optimization of all three, these methods partition the principles in independent steps resulting in sub-par performance.\nAn illustration of the above phenomenon is demonstrated in Fig. 1a by superimposing the subgraphs selected by (i) L1DSL (one of the methods proposed in this paper), (ii) the state-of-the-art optimization approach DIPS [8] and a ground truth (GT) subgraph injected in a Synthetic dataset. When constrained to select a fixed-size subgraph, our proposed method L1DSL recovers the connected GT subgraph, while DIPS recovers the GT only partially due to its two-step independent enforcement of the three design principles. Moreover our method consistently recovers 90% of the GT nodes for increasing noise added to non-GT feature values (Fig. 1b) , while the baseline's accuracy quickly degrades due to its susceptibility to noise. We address drawback of past work on NSP by proposing DSL (pronounced DieSeL): an optimization framework for NSP and the associated feature selection problem. We enforce the three design principles discussed above in a unified objective function and propose an algorithm for its optimization. We enforce sparsity by a self-representation objective designed to consistently select a sparse subset of features in all training network samples. Connectivity and discriminative power are enforced by appropriate regularization inspired by spectral graph methods and subspace maxmargin optimization. We construct a solver for the objective function and study its utility and effectiveness on synthetic and real-world datasets from multiple domains, demonstrating its superior quality compared to several baselines.\nOur main contributions in this work are as follows: 1. Novelty: We combine all three design principles for NSP: sparsity, connectivity and discriminative power in a novel unified optimization framework, called DSL. 2. Quality: DSL consistently outperforms state-ofthe-art baselines on real-world and synthetic datasets for both recovery of ground truth subgraphs and in classification accuracy (7% \u2212 16% improvement compared to baselines) employing small-size selected subgraphs. 3. Interpretability and wide applicability: DSL discovers interpretable feature subgraphs: known genes associated with liver metastasis in PPI networks and natural \"corridor\" patterns of bike commute behavior distinguishing between workdays and weekends."}, {"section_title": "Related work", "text": "Early existing methods for NSP focus on direct exploration of the space of connected subgraphs [16, 23] . NGF [16] explores the structure of PPI networks employing the random forest classifier iteratively fitted to growing connected structural subgraphs. MINDS [23] adopts a similar tree construction, while seeking to improve the running time and quality by a Markov Chain Monte Carlo (MCMC) sampling scheme in the subgraph space. Both methods suffer from limited quality and high running time due to the need to explore a large space of connected subgraphs.\nAn alternative family of approaches for NSP were recently proposed following an optimization strategy [7, 8] . DIPS [8] is the state-of-art approach, which introduces a two-stage solution to learn subgraphs: discriminative subspace learning followed by matrix approximation. This method avoids the search in the exponential space of candidate subgraphs, thus, addressing major drawbacks of NGF and MINDS. However, its 1 -norm based node selection mechanism is sensitive to noise and outliers and in addition subgraphs selection is performed in two independent steps, limiting the quality of obtained solutions.\nA different subspace learning problem is Sparse Subspace Clustering (SSC) [13, 19] , where the goal it to approximate unlabeled data by selecting a few feature comprising a subspace. This problem, however, is unsupervised and does not consider a network structure among features.\nThere is also related work on support vector machines (SVM) [6, 27] . SVM as a classification scheme finds an optimal separating hyperplane between classes. SVMs have been combined with subspace learning approaches such as matrix factorization [24, 10] . Our work is different from the above in that we consider a graph structure among features and employ self-representation as opposed to matrix factorization. We employ both the"}, {"section_title": "Notation and preliminaries", "text": "The global network state prediction can be viewed as a generalization of the classical supervised learning problem, where the knowledge of the network structure can be employed to improve the classification as well as provide explanation for the selected features. We first introduce the notation and preliminaries employed in our problem formulation and solution.\nThe input to the problem is a set of network samples (or graph signals). A network sample is a triple\n.., v m is a set of nodes, E i \u2286 V i \u00d7 V i is a set of undirected edges, and X i is a function labeling each node with a real number. The function X i can be thought of as a graph signal over the nodes of the sample.\nLet DS = {(S 1 , y 1 ), (S 2 , y 2 ), ..., (S n , y n )} be a network dataset that consists of n network samples annotated by corresponding discrete global states (or labels) y i . Similar to Dang et al. [8] , we adopt a summary graph structure S = (V, E, W ) to represent all\nEach edge E(p, q) \u2208 E is associated with a positive weight W pq defined as the fraction of network samples containing that edge in their structure, i.e.,\nThe combinatorial Laplacian matrix L associated with the aggregate network S is defined as L = D \u2212W , where D is the diagonal matrix of weighted node degrees with elements D pp = q W pq .\nArranging the node values of all networks samples X i in the columns of a matrix, we obtain the data matrix X \u2208 R m\u00d7n , where n is the total number samples and m is the total number of nodes in the network, also referred to as the feature dimension. In our formulation will enforce the selection of connected features (rows of X). In the sparse subspace clustering literature [13] such selection is enforced through the product X T \u03a6, where \u03a6 is a feature selection matrix with zero elements on the diagonal diag(\u03a6) = 0 to avoid individual columns being represented solely by themselves [13] ."}, {"section_title": "DSL: discriminative subgraph learning", "text": "Our goal is to simultaneously select connected subgraphs which are predictive of the global state. We formalize the problem as an optimization which linearly combines (i) selection of subgraphs, (ii) connectivity and (iii) discriminative power of the selection on the training network samples. Subspace selection. We enforce selection of a representative subset of features by minimizing the reconstruction error for the data matrix X via its subspace representation through an unknown feature selection matrix \u03a6:\n, where the reconstruction error is quantified in terms of the Frobenius norm of the residual matrix. To control how many features we select, we need to control the sparsity of the selection matrix \u03a6. A widely adopted approach is to add an 1 -norm regularizer \u03a6 1 , however, this choice would not enforce that the same node feature is selected across network samples. Intuitively we would like rows of \u03a6 to contain only high values (corresponding node is selected) or only values close to 0. To this end, we adopt the 2,1 -norm for the selection matrix defined as\n, where \u03a6 i is the i-th row of \u03a6. Subgraph connectivity. Our second goal is to ensure that our selection of nodes encoded in \u03a6 is also smooth (connected) with respect to the summary graph structure S interconnecting features. We achieve this by a regularizer involving the trace of the following quadratic form of the Laplacian matrix: tr \u03a6 T L\u03a6 . Each diagonal element in the product is of the form:\nwhere \u03a6 k is the k-th column in \u03a6, i.e. the selector vector for the k-th instance. Intuitively, this criterion penalizes for selection of non-neighbor features in each network sample. Discriminative power. Our third goal is to ensure that the selected subgraphs are disriminative, in other words the included features should be able to correctly separate networks instances with different global states.\nWe employ a loss function inspired by maximal margin optimization in SVM. Intuitively, feature values in selected subgraphs should render different class instances on opposite sides of a separation hyperplane (w, b), such that the margin defined by support vectors is maximized. We further allow for soft margin to avoid overfitting. DSL objective. Incorporating the above principles into a single objective, we obtain the following optimization for discriminative subgraph learning:\nThe first two terms in the objective reflect the subspace learning, the third term incorporates smoothness with respect to the graph structure, while the last term captures the soft margin maximization. The function y i , w Tx i + b is the hinge loss function, in which w is the normal vector to the hyperplane, b is an offset term, and C is the soft-margin control parameter. Each regularization has a corresponding balance parameter, namely \u03bb 1 and \u03bb 2 and \u03c0, controlling the importance of sparsity, graph smoothness and discriminative power respectively.\nThe norm f on the vector w orthogonal to the separation hyperplane w f is either f = 1 or f = 2, giving rise to two flavors of DSL: L1DSL and L2DSL respectively. In addition, the notationx i = \u03a6 T x i denotes the projection of the i-th sample value onto the selection matrix \u03a6. Intuitively, we are penalizing misclassification based on the sub-selection of the features through \u03a6 as opposed to when considering all features. Note that we also add the 0 constraint for diagonal entries of the selection matrix \u03a6, a typical constraint in sparse subspace learning to prevent the selection matrix from representing each feature by itself as opposed to as linear combination of other features.\nBy design our DSL objective can be viewed as a natural generalization of sparse subspace clustering, spectral graph partitioning and maximal margin learning. We optimize all those objectives simultaneously to learn discriminative connected subgraphs."}, {"section_title": "Optimization for DSL: Learning Algorithm", "text": "The optimization in the DSL objective is with respect to two sets of parameters: the selection matrix \u03a6, and the orthogonal vector to the separation hyperplane including offset (w, b). Since the hinge loss, 2,1 -norm and trace norm are not smooth, it's hard to develop an optimization on them simultaneously as Nesterov method [20] . Hence we design an alternating minimization method to optimize \u03a6 and (w, b). We next outline the optimization of the corresponding subproblems and list the steps of the overall algorithm. During the iteration, we first ignore the constraint of zero on the diagonal and add it to the result after the iteration. Updates for \u03a6. When the separating hyperplane and offset (w, b) are fixed, the optimization simplifies to:\nwhere C * = \u03c0C. Optimizing directly with the nonsmooth hinge loss function is challenging. Hence, in line with the optimization literature on SVMs, we introduce slack variables for each instance \u03be i , separating the nonsmooth hinge loss in constraints:\nTo solve the problem in Eq. 5.3, we construct the corresponding Lagrangian function and derive a closedform update. The Lagrangian has the following form:\nwhere \u03b1 and \u03b3 are vectors of Lagrangian multipliers of length DS. Setting the gradient \u2207 \u03a6 L = 0, we obtain:\nwhere D is a diagonal matrix with elements D ii = (2 \u03a6 i 2 ) \u22121 when \u03a6 i 2 = 0, and D ii = 0 otherwise. Similarly, solving \u2207 \u03be L = 0, we get:\nwhere 1 is a vector of ones of size |DS|. By substituting the optimal values of \u03a6 and \u03b3 back in the Lagrangian (Eq. 5.4), and after some simplifying variable substitutions we obtain the following dual Lagrangian function:\nwhere p ij , q i and g are defined as follows:\nThe detailed steps of the above derivation are available in the supplementary material. Based on Lagrangian duality, L d (\u03b1) provides a lower bound for the optimal solution of the original minimization problem w.r.t. \u03a6 from Eq. 5.2, as long as the KKT conditions for nonnegativity of the Lagrangian multipliers \u03b3 and \u03b1 are satisfied [4] . Hence, to obtain a minimizer for Eq. 5.2, we maximize the dual Lagrangian L d (\u03b1), while satisfying the KKT conditions. Note, that g can be discarded as it does not depend on \u03b1, leading to the dual optimization:\nwhere K \u2208 R n\u00d7n is a square matrix with elements K ij = 2y i y j p ij and q \u2208 R n is the vector of -q i elements. Note, that the added box constraint on \u03b1 ensures that \nthe non-negativity KKT conditions are satisfied for both \u03b1 and \u03b3 (due to Eq. 5.5). The resulting quadratic programming problem is concave (due to Lagrangian duality) and can be efficiently solved by the sequential optimization techniques widely employed in the the SVM literature [17, 21] . The obtained optimal \u03b1 is employed in Eq. 5.4 to derive the update of \u03a6. Note that the diagonal matrix D depends on \u03a6. We update it iteratively, based on the current \u03a6 from the previous iteration.\nUpdates for (w, b) : When \u03a6 is fixed, the optimization of (w, b) simplifies to the standard linear SVM:\nwhere minimization of w f ensures maximal margin and the hinge loss penalizes misclassification. It can be solved via quadratic programming (QP) optimization and we rely on efficient solves for this particular quadratic program [14] . The steps of the overall alternating optimization procedure are summarized in Alg. 1. After initialization of the selection matrix and classifier, we repeat the sequential updates until convergence (Steps 2-7). When the subgraph selection \u03a6 is fixed, we fit an optimal soft margin SVM for these features in Step 3 and then perform the necessary \u03a6 updates (Steps 4-7) . The dual Lagrangian is first maximized to obtain an optimal \u03b1 (Step 5), which is then employed with the current estimate of D (Step 6) in the update for \u03a6 (Step 7). Complexity analysis. Due to the enforced sparsity of \u03a6, most features loading shrink to zero quickly and as a result the \"feature-selected\" data matrixX = \u03a6 T X (after projection on \u03a6) will be much sparser than the full matrix. If s \u03a6 denotes the average number of non-zero elements inX, then the complexity of each SVM fit (Step 3) will incur O(s \u03a6 n) cost when employing fast sparse solvers [15] . We optimize \u03b1 by sequential minimal optimization (SMO) which has a cubic complexity O(n 3 ) in the worse case, but much faster running times have been demonstrated in practice to exhibit between linear and quadratic time costs [22] . The update of D's diagonal in (Step 6) is linear in the number of non-zero elements in \u03a6: O(s \u03a6 ).\nIf approached naively the update of \u03a6 in Step 7 has a complexity of O(m 3 ) as it involves an inversion of quadratic in m matrix. However, notice that two of the summands A = XX T + \u03bb 2 L are constants symmetric matrices and so is there sum A, the only varying term in the inversion is the diagonal matrix \u03bb 1 D. We can compute the inverse in O(ms X ) time, where s X is the number of non-zero elements of X by exploiting this sparse update structure via the Sherman-Morrison formula for sparse inverse updates:\nis a column vector, the square root is applied element-wise, and thus\nNote also that the second matrix in \u03a6's update is a linear combination of constant for the inner loop matrices weighted by \u03b1 plus a globally constant matrix 2XX\nT which can be pre-computed once at the cost of O(min(m 2 , s 2 X )) memory. Assuming this memory cost is paid this matrix can also be computed in O(ms X ).\nThe total complexity of the method is then\n, where t o and t i are the number of iterations of the outer and inner loops respectively. Assuming constant number of steps to convergence and that \u03a6 is sparser than X, the dominating factors in the complexity remain O(n 3 + ms X ), arising from (i) the SMO (Step 5) which as per Platt et al. [22] is at most quadratic as opposed to cubic; and (ii) \u03a6 in (Step 7). We employ a standard desktop machine with limited memory for our experiments, and thus, do not perform a high-memory-cost pre-computation, resulting in slightly higher experimental running times which still complete in at most 1h for our biggest instances.\n6 Experimental evaluation 6.1 Datasets. We employ both synthetic and realworld datasets for evaluation and summarize their statistics in Tbl. 1. Synthetic: We generate geometric synthetic networks by uniform sampling of node coordinates in a unit square and connecting two nodes if their distance is smaller than a threshold \u03c4 = 0.2. We select wellconnected subgraphs as the target (ground truth) discriminative subgraph and generate balanced set of in-stances labeled by two global states. Nodes in the target subgraph are randomly assigned values from [50, 100] in positive instances and [\u2212100, \u221250] in negative counterparts. All remaining nodes are assigned random values from a Gaussian distribution N (\u00b5 i , \u03c3 2 ), where \u00b5 i is the sample mean of the ground truth values in instance i and \u03c3 2 is a standard deviation which we vary. Real-world: We also employ five real-world datasets. Nodes in the Bike [2] are bike rental stations in the Boston city, while edges are connected based on a distance threshold, similar to our synthetic data (see Figure 4(a) ). Nodes' feature values correspond to the number of check-outs in a day, while the global states correspond to weekday versus weekends. We employ the last 299 days for this experiment as they span the time when the survice is rolled out to the whole city (initially only the downtown area was covered by the service).The CCT dataset contains city cellular HTTP traffic data records [25] for a large city with millions of people. Nodes correspond to stations at which hourly requests are counted (features) and node pairs are once again connected based on a distance threshold. Global labels associated with hourly samples reflect if the sample occurred within workday hours (8am-16pm) or outside this range. The ADNI [1] dataset contains fMRI resting state measurements for subjects labeled by AD: suffering Alzheimer's disease and NC: healthy normal controls. The graph structure associates functional links (nodes) with their level of coherence (feature values). Nodes are connected if the corresponding functional links share a brain region. We obtained the ADNI and the two PPI datasets discussed next from the authors of DIPS [8] , and thus, have effectively followed the same preparation protocol to enable a fair method comparison. We also employ two gene expression datasets: Embryonic development [12] and Liver metastasis [18] . Their network structures are functional protein-protein interaction (PPI) networks [9] , while node features correspond to binarized (in Embryo) or continuous (in Liver) gene expression values for healthy and normal subjects (global labels). Prepossessed data was kindly provided by Dang et Al [8] ."}, {"section_title": "Experimental setup", "text": "We evaluate our method's ability to discriminate between global labels, detect ground truth subgraphs, ensure connectivity in the selected subgraphs; and measure overall running time. Baselines: We compare the two flavors of DSL: L1DSL and L2DSL (employing f = 1 and f = 2 norm for the margin in Eq. 4) to the state-of-the-art method DIPS [8] . We also compare to two recent graph-agnostic feature selectors: FSASL [11] and UDFS [26] . FSASL takes into account an inferred notion of structure among feature angle similarity. UDFS selects discriminative subset from the full set of features and employs the same shrink-enforcing regularized based on th 2,1 norm. Linear SVM is then employed for prediction on the selected features for the latter two baselines. Our selection of baselines ensures that the state-of-the-art graph-aware method is considered, as well as graphagnostic alternatives which enforce sparsity and margin maximization similar to DSL, which constitute design advantages lacking in DIPS [8] . Metrics: When ground truth (GT) desired feature subgraphs are available, we calculate the area under the ROC (AUC) for recovering GT nodes in the selection. We quantify the testing prediction accuracy based on selected subgraphs (feature subsets) in 5-fold stratified cross-validation (CV). We also measure the \"community\" structure of selected subgraphs in terms of the conductance \u03c6 of their induced subgraphs within the summary network structure S. Implementation: Our methods are implemented in Matlab 2017b and all reported running times are for single-core (non-parallel) execution on an Intel(R) Xeon(R) Gold 6138 CPU @ 2.00GHz processor in a Dell PowerEdge system."}, {"section_title": "Classification accuracy", "text": "We first evaluate the ability of competing methods' selected features to discriminate between global states in cross-validation on all real-world datasets (Fig. 2) . For this experiments we fix the number of selected features for each of the competing techniques (we varying between 1% and 2% of all features in the respective datasets) and train a linear SVM (C=1) on only selected features. L1DSL consistently outperforms all alternatives for varying number of features with the gap in performance from the best baseline being highest for small number of selected features. The superior performance is due to the simultaneous sparse and consistent selection of discriminative connected subgraphs. Each baseline enforces only a subset of all those requirements: (i) DIPS employs a nonsparse discriminative subspace learning, which is then in an independently thresholded and smoothed against the graph structure; (ii) FSALSL and UDFS do not take advantage of the graph structure.\nIn the Bike dataset (Fig. 2a) , both DSL variants exhibit the largest improvement (15% higher accuracy) with as little as 10 selected features. More importantly DSL methods reach very close to the saturation accuracy of 95% with as little as 5 features. While the gap from DIPS closes, the latter continues to underperform DSL with higher number of features. L1DSL consistently outperforms alternatives by 10% for subgraph selection sizes between 40 and 100 on the CCT The advantage of our methods in Liver (Fig. 2c ) is also most evident when restricted to small number of features (both DSL variants are indistinguishable on this data). DSL's performance peaks at 95%, employing 30-node subgraphs, while DIPS reaches its highest 93% with more than twice the number of features. The other baselines also require higher number of features to reach their maximal accuracy. L1DSL dominates alternatives in ADNI reaching accuracy of 82% with 85 nodes, while the best accuracy of DIPS is 77% with also 85 nodes. UDFS and FSASL perform significantly worse on this data. L2DSL does not perform on par with L1SVM on ADNI, which could be explained by its higher propensity to consider more and redundant features to maximize the margin as it enforces less shrinkage via an L2 as opposed to L1 norm on w. ADNI and Liver are both complex networks (unlike Bike and CCT), featuring high node degrees and potentially some edges which are \"less aligned\" with the underlying process which determines the global network state. The optimal graph smoothness regularizer weights \u03bb 2 for these datasets are also lower (see Tbl. 1), corroborating the hypothesis of comparatively lower importance of the network structure."}, {"section_title": "Quality of feature selection", "text": "When ground truth features of interest are available, we compare the techniques by their ability to recover these GT features in their selected subgraphs. We first compare L1DSL (L2DSL's performance is indistinguishable from L1DSL's here) and DIPS on synthetic data and increasing variance \u03c3 2 applied to non-GT nodes' values ( Fig. 1) . Here we restrict both methods to report 15 features and provide a visual comparison of the selected features by DSL (squares) and DIPS (crosses) and the GT nodes (large round circles) in Fig. 1a (\u03c3 2 = 40). DSL matches the well-connected ground truth exactly, while DIPS selects only a subset of those nodes and also include noisy singleton nodes. We have selected the optimal graph smoothness regularizer for DIPS, so enforcing more smoothness for this method leads to strictly poorer AUC of recovering the GT. In other words DIPS is more sensitive to noise as in its first step (subspace projection) it considers all features to compute its crosssample similarity graphs. As we increase the variance of values in non-GT nodes, the GT-detection AUC decreases drastically for DIPS, while remaining more stable for L1DSL, opening a performance gap of 24% at \u03c3 2 = 10 2 (Fig. 1b) . We also quantify the feature selection quality for the Liver dataset for which we use the GT genes associated with the disease reported in the original paper [18] Fig. 3a . In this experiment we plot the ROC curves for the competing techniques. At small FPRs, DSL methods perform similar to DIPS and UDFS, however, the TPRs of L1DSL and L2DSL grow at a faster rate than that of alternatives. At FPR=0.5, the TPR of all alternatives does not exceed 0.5, while both DSL methods achieve a TPR of 0.8. It is important to note that the GT set of genes is likely incomplete resulting in limited TPR growth at low FPR regimes. However, the newly predicted genes by DSL are likely going to provide good targets for additional genes associated with the diseases as their selection optimizes both smoothness w.r.t. the PPI structure (guilt by association) and their discriminative power for the global state.\n7.1 Parameter sensitivity While our model requires three parameters: \u03bb 1 , \u03bb 2 and \u03c0, it is not very sensitive to their values. This is demonstrated by the relatively stable optimal parameters selected across datasets by cross-validation (last column of Tbl. 1). Particularly, the optimal weight of the SVM margin maximization \u03c0 is always one (and the performance is similar for a range of values). There is more variability in the optimal selections of \u03bb 1 and \u03bb 2 across datasets (typically small values between 0.01 and 0.1), however, the resulting accuracy is stable across wide ranges of those values within a dataset. We demonstrate this behavior by plotting the accuracy as a function of \u03bb 1 and \u03bb 2 for the Bike dataset in Fig. 3(b) . In this dataset the accuracy remains 4% of the optimal accuracy for wide ranges of the parameters. We observe similar trends in other datasets and almost no variation when taking \u03c0 between [1, 5] .\n7.2 Quality, connectivity and running time. Table 2 offers a comprehensive comparison of DIPS and DSL on all dataset, where accuracy is presented alongside with running time and conductance of the selected subgraphs. The best accuracy gap separation is at least 7% and reaches up to 16% difference on the Bike data. The selected subgraphs by DSL, not only have higher discriminative power, but are also better connected (lower conductance values signifies lower cut to volume ratio for selected node). One exception to this trend is the ADNI dataset in which the conductance of DSL's solution is slightly slower. Note that the structure of this dataset is very regular: S is the dual graph of a fully-connected coherence network among graph regions, and hence since there are no good cuts in this graph the conductance community measure is less informative. Our methods' superior quality come at with the cost of running time than that of DIPS. The main reason is our joint connected discriminative subgraph selection, which requires more computations than DIPS' two-step independent, though less accurate, optimization. Our methods' implementation complete in at most an hour on the biggest evaluation datasets. It is important to note that our current DSL implementation does not employ the optimal pre-computation of large static matrices (as discussed in Sec. 5), hence, its running times could be improved at the cost of higher memory footprint. "}, {"section_title": "Discriminative subgraphs in bike commutes", "text": "We visualize the selected subgraphs in the Bike data in Fig. 4 , where nodes are plotted according to the geographical coordinates of bike rental stations in the city of Boston. Consistent with the analysis on synthetic, DIPS and UDFS tend to select more disconnected subgraph which collectively have lower discriminative power for weekday versus weekend rental patterns. Interestingly, L1DSL selects a long NE-SW \"corridor\" which passes through downtown as the most discriminative, leading to an intuitive interpretation that this corridors rental pattern might discriminate well between weekday commutes to the downtown areas from the periphery and the likely lack thereof during weekends."}, {"section_title": "Conclusion", "text": "We presented a novel optimization framework for the problems of global network state prediction and feature selection, called DSL. It enforces simultaneously the natural requirements for sparsity, connectivity in the network structure, and discriminative power of selected subgraph solutions. We demonstrated DSL's superior quality when employed on both synthetic and realworld problem instances and in comparison to stateof-the-art baselines. Our method was able to recover ground truth subgraphs in synthetic and gene expression datasets with consistently better accuracy than competitors. The subgraphs learned by DSL enabled between 7% and 16% improvement of cross-validation classification accuracy compared to the closest baseline. In addition, we demonstrated the interpretability and applicability DSL's solutions by uncovering known target genes involved in liver metastasis and by uncovering intuitive commute subgraph patterns in transportation networks capable of distinguishing between workday and weekend activity."}]