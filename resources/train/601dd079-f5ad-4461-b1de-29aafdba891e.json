[{"section_title": "Abstract", "text": "Current applications of high-dimensional regression often involve multiple sources or types of covariates. We propose methodology for this setting, focusing on the \"wide data\" regime with large total dimensionality p and sample size n p. As a starting point, we formulate a flexible ridge-type prior with shrinkage levels that are specific to each data type or source. These multiple shrinkage levels are set automatically using empirical Bayes. Importantly, all estimation, including setting of shrinkage levels, can be formulated mainly in terms of inner product matrices of size n\u00d7n, rendering computation fast and scalable in the wide data setting, and the resulting procedures are free of userset tuning parameters. We consider sparse extensions via constrained minimization of a certain Kullback-Leibler divergence which allow adaptive and source-specific shrinkage. This includes a relaxed variant with a closed-form solution that scales to very large p. We compare these approaches to standard high-dimensional methods in a simulation study based on biological data and present results from a case study in Alzheimer's disease involving millions of predictors and multiple data sources."}, {"section_title": "Introduction", "text": "Advances in data acquisition and storage have meant that current and emerging studies are routinely including multiple sources of covariates, such as different data types, with one or more of the sources being high-dimensional. To fix ideas, consider a biomedical setting in which samples indexed by i = 1, . . . , n each have response y i and covariates of several types k = 1, . . . , K (representing say genetic data, imaging, clinical covariates and so on) with respective dimensionalities p 1 , . . . , p K . We refer to the different types of covariate as sources. The p k 's are the source-specific dimensionalities and p = K k=1 p k is the total dimensionality. We consider a specific example of this kind below, in the context of Alzheimer's disease.\n*Data used in preparation of this article were obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/ uploads/how_to_apply/ADNI_Acknowledgement_List.pdf Constructing regression models using such data is challenging, because the relevance of the sources may be quite unequal (and unknown in advance) and the total dimension p may be large. This motivates a need for methodology that can cope with multiple sources and that scales to high dimensions.\nMethods for high-dimensional regression are now well-established and include penalized least-squares approaches such as the lasso and extensions (Tibshirani 1996 , Tibshirani et al. 2005 , Yuan & Lin 2006 , Zou & Hastie 2005 , SCAD (Fan & Li 2001) and Bayesian analogues (see e.g. Kyung et al. 2010 , for a review). A range of Bayesian approaches have been proposed, notably those based on shrinkage priors, often coupled with variable selection, see for instance Yuan & Lin (2005) , Park & Casella (2008) , Hans (2010) , Griffin & Brown (2010) , Carvalho et al. (2010) and Armagan et al. (2013) , among others. However, in the very large p case many available methods become computationally cumbersome or intractable and effective source-specific shrinkage remains hard to achieve.\nIn this paper we put forward an approach to regression in the multiple-source, highdimensional setting. Specifically:\n\u2022 We consider a generalized ridge-type prior with shrinkage that adjusts to individual data sources, with the corresponding shrinkage levels estimated from the data.\n\u2022 We show that estimation can be formulated in a way that renders computation efficient for \"wide\" data, even for large p and over K sources, and including automatic setting of all tuning parameters.\n\u2022 We introduce sparsifications that achieve competitive prediction performance and that provide a fast yet multivariate technique for discarding non-influential predictors.\nThus, we consider the case of data from multiple sources with source-specific dimensionalities p k that could differ by many orders of magnitude (e.g. between clinical and genetic data), with total p large and a priori unknown source-specific importance. There has been much interesting work on group selection approaches in regression (reviewed in Huang et al. 2012) . The group lasso (Yuan & Lin 2006) allows specification of covariate subsets that can then be selected as groups; however, applying the group lasso in the current setting (by identifying groups with sources) would not be useful because sources would then simply be either included or excluded (without within-source regularization). The sparse group lasso (Simon et al. 2013 ) permits additional regularization via within-group sparsity but its use here would require a nontrivial extension to source-specific penalties whose tuning would be difficult if not intractable in the very high-dimensional, multiple source setting. Dondelinger et al. (2016) consider the case of penalized regression over multiple subgroups of samples; this is quite different from the present setting of sources of covariates (i.e. we focus on the columns, not the rows), also the authors do not tackle the very high-dimensional case.\nRidge-type estimators are among the oldest and best studied regularized regression tools, whether from a penalized likelihood or Bayesian viewpoint. Our results build on these classical tools, developing a variant of the ridge prior to deal with multiple source, high-dimensional problems. The subsequent sparsification step that we consider is an example of an emerging class of posterior post-processing methods and yields a solution which is similar to the penalized credible region (pCR, Bondell & Reich 2012) and the decoupled-shrinkage-selection (DSS, Hahn & Carvalho 2015) solutions. In contrast to pCR and DSS we develop our approach via a certain Kullback-Leibler divergence. Interestingly, we can recover the pCR solution as a special case. Importantly, our approach can take advantage of the information from the initial ridge step and thus allows parameter-specific as well as source-specific penalization. In addition, we propose a relaxed variant which leads to a closed-form solution that is immediately applicable to problems involving millions of predictors. We note that the sparse extensions we discuss are mainly aimed at achieving parsimonious prediction rather than variable selection per se.\nAs a topical example of a multiple-source, high-dimensional problem, we consider a case study in Alzheimer's disease (AD). AD is a neurodegenerative condition in which a number of pathological processes lead to progressive central nervous system damage and loss of cognitive function. There is increasing evidence that pathological processes in age-related AD are initiated years before onset of AD symptoms. At present there exists no disease-modifying therapy for AD and it is believed that effective treatment will require early intervention. For this reason, prediction of future disease course is a central topic in AD research. AD is multifactorial in the sense of being mediated via multiple underlying biological processes and several current and emerging large-scale studies have a multi-modal design, spanning several data types. These include the Alzheimer's Disease Neuroimaging Initiative (ADNI) (Mueller et al. 2005) , the Rhineland study (http://www.rheinland-studie.de) and the UK Biobank (http://www.ukbiobank.ac.uk) (the latter is a broader study that includes neurodegeneration-related data). The data we consider are from ADNI which is a large-scale longitudinal study involving multiple data modalities; we focus specifically on the prediction of future cognitive scores, as described in detail below.\nThe remainder of this paper is organized as follows. In Section 2 we introduce the scalable Bayesian regression (SBR) approach, describing model formulation, prior specification and tuning of shrinkage levels. Section 3 deals with the sparse extension of the methodology, sparse SBR (SSBR), including a general solution and a relaxed variant for the very large p case. The relationship between SSBR with pCR is discussed. We further introduce an adaptive approach to regulate induced sparsity. Results and comparisons with standard penalized likelihood approaches from a simulation study are presented in Section 4. The paper concludes with a discussion in Section 6."}, {"section_title": "Scalable Bayesian regression 2.1 Model", "text": "Let y be a n \u00d7 1 vector of responses and {X k : k = 1, . . . , K} a set of covariate or feature matrices from K data-sources. Each X k is of size n \u00d7 p k so that the total number of potential predictors is p = K k=1 p k . We consider the normal linear model\nwhere each \u03b2 k is a p k -vector of regression coefficients, N n (\u00b5, \u03a3) denotes a n-dimensional multivariate normal density with mean \u00b5 and covariance \u03a3 and I n is the n \u00d7 n identity matrix. Without loss of generality we assume throughout that all data are standardized. Let\nT denote the respective global n \u00d7 p predictor matrix and p-vector of regression coefficients (here, \"global\" means with all sources taken together). Then, with prior \u03c0 the full model under consideration is y \u223c N n (X\u03b2, I n \u03c3 2 ), with \u03b2|\u03c3 2 \u223c \u03c0(\u03b2|\u03c3 2 ) and \u03c0(\u03c3 2 ) \u221d 1/\u03c3 2 .\nThe improper prior for \u03c3 2 (Jeffreys' prior) is a common option for linear regression models. The crucial aspect of prior formulation for the multiple-source, high-dimensional setting under consideration is the construction of \u03c0(\u03b2|\u03c3 2 ). This we discuss in detail next."}, {"section_title": "The prior on \u03b2", "text": "The SBR approach is based on a natural generalization of the standard ridge prior. Specifically, the prior on \u03b2 is\nwhere\nHere each \u03bb k is a source-specific shrinkage level on the corresponding \u03b2 k . The special case K=1 recovers the standard ridge prior with just one shrinkage level (and indeed the solutions presented below could be used to give a scalable implementation of classical ridge with a single \u03bb). However, when dealing with multiple data sources one would expect different contributions from the various data-sources. This motivates a need for source-specific penalties that can adjust to account for these differences and additionally provide potentially useful information about the usefulness of specific data sources. At this point it is useful to define the quantity\nAll formulas presented in the remainder of this Section are cast in terms of G \u03bb . Importantly, this means that the key computations under SBR can be formulated so as to require only a one-off computation of these individual inner product (Gram) matrices X k X T k of size n \u00d7 n (these calculations can be easily implemented in parallel) followed mainly by operations on those matrices. As we show below, for wide data with large p, this gives a practical way to implement SBR."}, {"section_title": "Inference", "text": "Under the conjugate prior in (1) the posterior distribution of \u03b2 is given by\nwhere \u03b2 = \u03a3 \u03b2 X T y and \u03a3 \u03b2 = (X T X+\u039b) \u22121 . Calculating the posterior mode directly involves a p \u00d7 p matrix inversion. For p > n we instead use\nwhere w \u03bb = y \u2212 (I n + G \u03bb ) \u22121 G \u03bb y is an n-vector whose calculation involves only an n \u00d7 n matrix inversion. The derivation of (4) is provided in Appendix A. For very large problems the computation of the posterior mode can be done in parallel; additionally, we draw attention to the useful expression\nfor k = 1, . . . , K. Having obtained the posterior mode, prediction from an available X pred of dimensionality m \u00d7 p is straightforward via y pred = X pred \u03b2. In addition, when interest lies solely in prediction the corresponding calculation can be simplified to\nCalculating the posterior covariance matrix can also be simplified through the formula\nFor details see Appendix A. In practice we are not interested in evaluating the entire covariance matrix (for very large p this might in fact be difficult due to memory limitations). However, the methodology considered in Section 3.3 requires the diagonal elements of \u03a3 \u03b2 and in this case the formula in (7) facilitates computation as it allows for fast and parallel block matrix computations. Again, note that\nk , so here the magnitude of each p k , for k = 1, . . . , K, can guide us in determining whether to use block computations or not. Moreover, the posterior distribution of \u03c3 2 is given by\ni.e. an inverse-gamma distribution with shape a = n/2 and scale b = RSS/2, where\nis the residual sum of squares. Below we will make use of the marginal likelihood m(y|\u03bb). This is the likelihood obtained by integrating over the parameter space with respect to the joint prior distribution (here, over \u03b2 and \u03c3 2 ). Using the prior specification and results above we have\n2.4 Automatic setting of shrinkage levels \u03bb Specification of penalty parameters is often handled through cross-validation (CV) or generalized CV in a frequentist framework (Tibshirani 1996) , while Bayesian methods typically rely on empirical Bayes (EB) point estimates or data-dependent hyper-priors; see e.g. Yuan & Lin (2005) , Park & Casella (2008) , Balakrishnan & Madigan (2010) , Hans (2010) and Griffin & Brown (2010) . An alternative approach is considered by Lykou & Ntzoufras (2013) who tune the Bayesian lasso penalty based on Pearson correlations at the limit of significance determined by Bayes factors. Furthermore, fully Bayesian shrinkage methods include the horse-shoe prior (Carvalho et al. 2010 ) and the double generalized Pareto (Armagan et al. 2013) . Here, the tuning parameter \u03bb is vector valued and for the applications we consider we would like fast and efficient approaches by which to set it. We propose three EB point estimators obtained by (i) minimizing the leave-one-out CV error, (ii) maximizing the marginal likelihood and (iii) locating the posterior mode under a data-dependent prior. All three require no user input and are computationally fast. We discuss each in turn.\nLeave-one-out cross-validation (CV) estimator: Similar to the case of ordinary least squares under ridge regression the leave-one-out CV error in our case can be computed as\nA proof is provided in Appendix B.\nMarginal likelihood (ML) estimator: From Eq. (10) the quantity that maximizes the ML is given by\nPosterior mode (PM) estimator: We consider a product-exponential data-dependent prior for \u03bb with prior mode at zero, prior mean equal to \u03bb CV as given in (11) and prior\nkCV . The rationale is that a smaller individual estimated penalty \u03bb kCV corresponds to a stronger belief that the corresponding X k matrix contains useful signal and therefore to a smaller prior variance (especially when \u03bb kCV < 1). On the other hand as \u03bb kCV increases we let the quadratic prior variance account for the chance that there is actually some useful signal in X k which passes undetected by the leave-one-out CV approach. The resulting posterior mode estimate is\nNote that the number of available data sources K will typically not be large. Therefore, the aforementioned solutions are easy to find through standard optimization routines."}, {"section_title": "Sparse SBR", "text": "The SBR posterior mode in (4) is non-sparse (\"dense\") in the sense that the regression coefficients will not be set to exactly zero. In this Section we propose a methodology for \"sparsifying\" SBR. The idea is to find a sparse approximation to the full (dense) Bayesian solution that is closest to it in a Kullback-Leibler (KL) sense. To do so, we minimize the KL divergence with respect to the posterior distribution of the regression vector, but subject to a lasso-type L 1 constraint to ensure sparsity. We show first a general solution that is suitable for small to moderate p and then go on to consider a relaxed solution that is applicable to the large p case. The solutions presented below bear a resemblance to other Bayesian postprocessing approaches (Bondell & Reich 2012 , Hahn & Carvalho 2015 and to frequentist methods in the context of wavelet regression (Antoniadis & Fan 2001 ) and ridge logistic regression for text categorization (Aseervatham et al. 2011 ). However, these are rooted in different arguments and not equivalent to the KL-based approach below. The particular connection with the pCR solution of Bondell & Reich (2012) is discussed in Section 3.2."}, {"section_title": "Sparsification using the KL divergence", "text": "Let f (\u03b2) \u2261 N p ( \u03b2, \u03a3 \u03b2 \u03c3 2 ) denote the true posterior over \u03b2, conditional on \u03c3 2 , with mode and covariance as in Eqs. (4) and (7), respectively, and let q(\u03b2) \u2261 N p (\u03b3, \u03a3 \u03b2 \u03c3 2 ) denote an approximate conditional posterior where \u03b3 is the approximate mode (this will provide a sparsification of \u03b2). The idea is to minimize the KL divergence from q to f under an L 1 penalty on vector \u03b3 to induce sparsity. It is easy to show that the KL divergence from q(\u03b2) to f (\u03b2) is\nNote that D KL in (14) is a true distance metric (satisfying non-negativity, symmetry and the triangle inequality). Note also that the presence of the nuisance parameter \u03c3 2 cannot be ignored when the minimization also involves a L 1 penalty on \u03b3. In principle, one could work with the marginal posterior distribution of \u03b2 (a multivariate t distribution) in order to avoid consideration of \u03c3 2 . However, in this case working with the KL divergence is not straightforward. Another option would be to use a plug-in posterior point estimate in (14) such as the mode or mean of \u03c3 2 . Instead, here we pursue a tuning-free approach in which \u03c3 2 is integrated out; specifically, we work with\nwhere the posterior of \u03c3 2 is given in (8). The constant depends on sample size and the residual sum of squares, i.e. c = nRSS \u22121 , with RSS as given in Eq. (9). Note that using the posterior mean or mode of \u03c3 2 results in c = (n \u2212 2)RSS \u22121 and c = (n + 2)RSS \u22121 , respectively. Using (15), the general solution is\nwhere \u03b1 > 0 controls the sparsity of \u03b3. Clearly, the SSBR solution implies a lasso-type model with the particularity of a saturated design where the analogue to sample size equals p. This means that SSBR can include at most p predictors (unlike classical lasso which for p > n can include at most n predictors). The solution in (16) can be solved with the LARS algorithm (Efron et al. 2004 ) by setting y * = c 2 \u03a3 \u03b2 \u22121/2 \u03b2 as response variable and X * = c 2 \u03a3 \u03b2 \u22121/2 as design matrix. Note, however, that this involves computations with the inverse covariance matrix and is, therefore, not suited to settings where p is very large. The large p case (which is our main focus) can be treated through the approach presented in Section 3.3 which leads to a closed-form expression for \u03b3."}, {"section_title": "Relation to penalized credible regions", "text": "The pCR approach (Bondell & Reich 2012 ) seeks solutions of the form \u03b2 = argmin \u03b2 \u03b2 0 subject to \u03b2 \u2208 C \u03b1 , where C \u03b1 is the (1 \u2212 \u03b1) \u00d7 100% credible set. Under a normal ridgeprior on \u03b2 and a inverse-gamma prior on \u03c3 2 this translates to a feasible set of the form\n\u03b2 (\u03b2 \u2212 \u03b2) \u2264 C} for some C corresponding to a specific credible region. To tackle the obvious computational challenges of the above solution, the authors initially relax the L 0 norm to a smooth homotopy between L 0 and L 1 , and subsequently apply a local linear approximation which results in a convex L 1 optimization problem. The resulting solution is very similar to our solution obtained through the KL approach; in fact by setting the penalty in (16) as \u03b1 = c 2 \u03b2 \u22122 1 \u03be (now c/2 no longer affects the optimization and \u03be is the new penalty) we recover exactly the pCR solution. As noted in Bondell & Reich (2012) there is a one-to-one correspondence between C and \u03be; however, it is highly non-linear. Under this setting we have selection consistency (under mild regularity conditions) when p is fixed or as long as p/n \u2192 0 for n \u2192 \u221e. The authors also demonstrate selection consistency (under stricter conditions) for rates log p = O(n c ) for some c \u2208 (0, 1) using univariate thresholding rules on simple ridge estimates.\nIn practice, the \u03be that corresponds to the asymptotically consistent sparse set is not recoverable. Theoretically, the penalty should depend on sample size so that \u03be n \u2192 0 faster than the posterior distribution concentrates around the \"true\" parameter value; however, under finite samples the selection of \u03be crucially affects the sparsity of \u03b2 (Hahn & Carvalho 2015) . Therefore, tuning of \u03be is handled through common grid search and inspection of regularization plots/prediction errors in Bondell & Reich (2012) .\nFinally, as remarked at the ending of Section 3.1, any optimization involving \u03a3 \u22121 \u03b2 becomes infeasible for very large p; therefore, the pCR solution is not directly applicable to the ultrahigh dimensional problems considered here."}, {"section_title": "A relaxed solution applicable to the large-p case", "text": "Instead of the KL divergence to the posterior used above, consider the KL divergence between the quantities q * (\u03b2) = N p (\u03b3, V \u03b2 \u03c3 2 ) and f * (\u03b2) = N p ( \u03b2, V \u03b2 \u03c3 2 ), with V \u03b2 = diag(\u03a3 \u03b2 ). This amounts to setting as target distribution the product of the marginal posterior densities. The use of independent posterior factorizations is common in various settings; for instance, in marginal likelihood estimation (e.g. Botev et al. 2013 , Perrakis et al. 2014 , in expectationpropagation algorithms (e.g. Minka 2001 ) and in variational Bayes (Bishop 2006) .\nWorking with the diagonal matrix V \u03b2 leads to the following minimization\nwhere v j is the j-th element, for j = 1, . . . , p, of the main diagonal of V \u03b2 . Note that the main diagonal elements are feasible to calculate even for very large p; this can be achieved using Eq. (7) with parallel block computations. Moreover, the minimization in (17) has a closed-form solution which is as follows\nThe derivation of (18) is provided in Appendix C. Note that for fixed p and n \u2192 \u221e we obtain \u03b3 j = \u03b2 j which makes sense from an asymptotic perspective. However, when \u03b1 is a constant not depending on n either directly or indirectly (e.g. through \u03b2 j ), a \"non-sparsifying\" effect may be triggered even for moderate sample size, which is in contrast to our initial intent. Setting of \u03b1 is discussed next."}, {"section_title": "Tuning of \u03b1", "text": "Specification of \u03b1 can be handled via a grid search with the aim to find the \u03b1 that minimizes a specific criterion. This is expected to be relatively fast using the LARS algorithm for small/moderate p, while under the relaxed solution, once the variances are calculated, the grid search requires only checking a true/false statement. This strategy will typically produce a full path of solutions which can also be used to produce regularization plots. We acknowledge this as a valid common strategy, however, we do not further pursue it here. Instead, we consider a faster, tuning-free, alternative which borrows information from the SBR solutions.\nSpecifically, we propose parameter-specific and source-specific adaptive penalties for each \u03b3 jk , where j = 1, . . . , p k and k = 1, . . . , K. Specifically, we consider penalties of the form\nThe rationale in (19) is that the larger the magnitude of \u03b2 jk , the smaller the corresponding penalty. In addition, we restrict to w j \u2208 (0, 1) which guarantees reasonable shrinkage when | \u03b2 jk | > 1 and avoids extreme shrinkage when | \u03b2 jk | < 1 (which is the common case). Here the w j 's act as further tuning parameters with values close to one or zero encouraging sparse or dense solutions, respectively. At this point we borrow information from the available source-specific \u03bb k 's and treat the w k 's as power-weights, namely setting them equal to\nWith this approach we take advantage of all available information from the previous SBR step, i.e. parameter-specific shrinkage through \u03b2 jk and sourse-specific shrinkage through \u03bb k . Arguably, this strategy may result in undesirable non-sparse solutions, but that will be in the rare, and rather unrealistic, case where K is large and all sources are equally important in the sense that the \u03bb k 's will be more or less the same; a setting where in fact a single-\u03bb SBR approach is more suitable. Note that, to implement this approach under the general solution in (16) one has to find first \u03b3 * by applying the LARS algorithm once (with penalty set to one) using as design matrix X * = c 2 \u03a3 \u03b2 \u22121/2 A, where A is the diagonal matrix with elements the reciprocals of (19).\nThe solution is then \u03b3 = A\u03b3 * . As a final comment, we remark that despite the fact that this penalization approach depends indirectly on sample size through the regression coefficients in (19) and the shrinkage parameter in (20), it may still be sensitive to the \"non-sparsifying\" effect discussed at the end of Section 3.3. Controlling this effect requires scaling the penalty in (19) by a factor f n = f (n); however, automatic tuning of f n is not straightforward. Empirical results (see Section 4) suggest that f n = log(n) can lead to a reasonable balance between sparsity and predictive performance. We will call this extension \"controlled\" SSBR or cSSBR (as it \"controls\" for sample size). "}, {"section_title": "Simulation study", "text": "In this Section we present a simulation study aimed at mimicking a typical modern biomedical application involving multiple data types. Reflecting the relative ease with which multiple data modalities can now be acquired such designs are becoming common, with examples including the Cancer Genome Atlas 1 , the Alzheimer's Disease Neuroimaging Initiative 2 and the Rhineland Study 3 among many others."}, {"section_title": "Set-up", "text": "The problem. We consider a regression problem with covariates from three sources, namely clinical (CL), gene-expression (RNA) and genetic (single nucleotide polymorphism or SNP) data with respective (simulated) feature matrices X CL , X RNA and X SNP . The number of covariates in each data-source is set equal to p CL = 26, p RNA = 2000 and p SNP = 100000. Although the methods we propose can cope with larger p, we restrict total p in this Section to facilitate empirical comparison with standard methods.\nCovariates. The covariate matrices for the clinical and gene-expression variables are generated as X CL \u223c N p CL (0, \u03a3 CL ) and X RNA \u223c N p RNA (0, \u03a3 RNA ), respectively. Here \u03a3 CL and \u03a3 RNA are covariance matrices estimated from (real) phenotype and gene-expression data from the Drosophila Genetic Reference Panel (DGRP) (Mackay et al. 2012 ) (data available online at http://dgrp2.gnets.ncsu.edu/data.html). To simulate the genetic data X SNP we use a block-diagonal covariance structure (The International HapMap Consortium 2005). We specify\nSNP is of size S \u00d7 S (with S = p SNP /B) and generated from a inverse-Wishart with S degrees of freedom and identity scale matrix, i.e. \u03a3 b SNP \u223c IW(S, I S ) for b = 1, . . . , B. As X SNP dominates in terms of dimensionality the specification of B essentially controls the overall correlation level. We consider two simulation scenarios: (i) B = 1000 corresponding to 1000 blocks of size 100 (\"low-correlation scenario\") and (ii) B = 100 corresponding to 100 blocks of size 1000 (\"high-correlation scenario\"). We first generate X c SNP \u223c N p SNP (0, \u03a3 SNP ) and then discretize in correspondence to the common SNP encoding 0/1/2 (homozygous major allele/heterozygous/homozygous minor allele). The discretization is tuned in order to give a reasonable empirical distribution of SNPs; specifically, for j = 1, . . . , p SNP we discretize as\njSNP | \u2265 1.5 and |X c jSNP | < 2.5, 2, if |X c jSNP | \u2265 2.5.\nRegression coefficients and sparsity. For the (true) regression vectors \u03b2 CL , \u03b2 RNA and \u03b2 SNP we consider the following levels of sparsity (fraction of non-zero \u03b2's); s CL = 50%, s RNA = 5% and s SNP \u2208 {1%, 10%, 50%}. Varying sparsity of \u03b2 SNP gives three scenarios for overall sparsity s: (i) s \u2248 1% (sparse scenario), (ii) s \u2248 10% (medium scenario) and (iii) s \u2248 50% (dense scenario). Let p * CL , p * RNA and p * SNP denote the respective number of elements in the subvectors \u03b2 * CL , \u03b2 * RNA and \u03b2 * SNP containing the non-zero beta coefficients. The non-zero betas are generated from the generalized normal distribution (GND). Following the parameterization in Mineo (2003) the probability distribution function of a GND(\u00b5, \u03c3, u) with location \u00b5 \u2208 R, scale \u03c3 > 0 and shape u > 0 is given by\nThe GND includes as special cases the normal (u = 2) and the double exponential (u = 1) distributions. To avoid these particular cases (which could potentially bias the simulation towards ridge or lasso respectively) we set u = 1.5 and generate the non-zero effects as \u03b2 * jCL \u223c GND(0, \u03c3, 1.5), for j = 1, . . . , p * CL , \u03b2 * jRNA \u223c GND(0, \u03c3, 1.5), for j = 1, . . . , p * RNA , and \u03b2 * jSNP \u223c GND(0, 2 \u03c3/3, 1.5), for j = 1, . . . , p * SNP . The signal strength is controlled via the scale parameter \u03c3 (this is downscaled by a factor of 1.5 for the SNP coefficients to control the total amount of signal in the SNPs). To complete the specification of the simulation we set this scale parameter by considering the finite-sample risk in a simplified CL-only oracle-like set-up. Specifically we consider the correlation induced between predictions X test CL \u03b2 CL (under the OLS estimate using the low-dimensional CL data only) and out-of-sample test data (with the data-generating mechanism being a linear model with conditional mean X CL \u03b2 CL and error variance equal to unity). Specifically, we set \u03c3 = 0.1 which results in an average out-of-sample correlation of 0.6 when n=100 and n test =5000.\nGiven the above configurations (low/high correlation and sparse/medium/dense scenarios) we generate data from the model\nwhere \u03b5 i \u223c N (0, 1) and i = 1, . . . , n train with n train \u2208 {100, 250, 500}. The test sample size n test always equals 5000. Each simulation scenario is repeated 50 times."}, {"section_title": "Methods under comparison.", "text": "We consider the SBR method under the three EB estimates proposed in Section 2.4 and the corresponding SSBR solutions with the penalty terms in (19) the power-weights in (20) and no control for the effect of sample size. Furthermore, we consider cSSBR approaches with f\nn = log(n) and f (3) n = log(n). We present results obtained from f (2) n as this option led to a good balance between sparsity and predictive performance. We compare to standard ridge and lasso (with \u03bb set to minimize the mean squared error from 5-fold CV using package glmnet in R)."}, {"section_title": "Results", "text": "Boxplots of out-of-sample correlations between predictions and test data under the lowcorrelation and high-correlation simulations are presented in Figures 1 and 2 , respectively. In general, all proposed SBR methods (CV/ML/PM) demonstrate good and stable predictive performance, without any apparent differences among them. Specifically, we see:\n\u2022 In the sparse scenario SBR methods match or outperform lasso.\n\u2022 In the medium scenario SBR methods generally outperform ridge and lasso.\n\u2022 In the dense scenario SBR methods match or outperform ridge.\nConcerning the sparse extensions of the SBR approaches we see the following:\n\u2022 In the low correlation simulations SSBR/cSSBR match more or less the performance of non-sparse methods.\n\u2022 In the high correlation simulations SSBR performs well overall in the medium and dense scenarios for larger sample size. The cSSBR PM approach leads to more stable predictions in comparison to cSSBR CV/ML . Figure 3 shows the resulting values of \u2212 log \u03bb k for k = {CL, RNA, SNP} (higher values correspond to lower penalty, i.e. higher estimated importance) in the low-correlation simulations, and provides useful insights concerning the behaviour of SBR methods. Evidently, all estimates (CV/ML/PM) adjust well by appropriate source-specific penalization. This key attribute allows SBR to generally outperform ridge and lasso in prediction when dealing with multiple data sources. Note also that the PM estimator tends to penalize less. The corresponding plots from the high-correlation scenarios (not shown) are very similar.\nAs noted above, the sparse SSBR (without or with control) solutions seem to allow equally good predictive performance, in certain cases, as the dense SBR solutions. In addition, they employ fewer parameters; Table 2 shows the average sparsity (over the 50 repetitions) induced by SSBR methods under the various simulations. As seen, the solutions seem to display an adjustment to the true underlying sparsity. In addition controlling for the effect of sample size yields much sparser models.\nAll penalties were estimated directly from the data and not pre-specified in any way. To further investigate source-specific penalization, in Figure 6 we show boxplots over the source-specific \u2212 log\u03bb k values, for k = {CL, MRI, SNP} and n train = {100, 250, 500}. Under the ML estimator the MRI dataset is penalized less, while under the PM estimator it is the CL dataset which is penalized less. This appears to explain the slight differences between the two methods (and their sparse extensions) observed in Figure 5 with respect to predictive accuracy when including all sources."}, {"section_title": "Computational performance", "text": "We conclude by examining computational burden as a function of total dimension p and in comparison with the lasso. To do so we add a fourth \"data source\" which is simply Gaussian noise. The number of these additional covariates is set so that the total number of predictors is p \u2208 {5\u00d710 5 , 10 6 , 2.5\u00d710 6 , 5\u00d710 6 , 10 7 }. Sample size is set equal to 100. Computations were carried out on a compute server with 128 cores (2.28GHz) and 1TB of RAM. For lasso we treat the binding of individual matrices into one data matrix (an operation not needed for SBR) as a pre-processing step and do not include this in the reported runtimes for lasso. We use the parallel option in glmnet for estimation of the penalty parameter via 10-fold CV (the default option). For our methods we consider the SBR PM approach, which requires evaluation of both\u03bb CV in (11) and\u03bb PM in (13) (and is thus in principle the slowest), and also its corresponding sparse extension. We include the formation of transpose matrices and Sample size: n = 500\nFigure 3: Simulation study, source-specific shrinkage levels. Boxplots showing source-specific \u2212 log\u03bb values from the SBR methods in the low-correlation simulations under various levels of sparsity for n = 100 (top row), n = 250 (middle row) and n = 500 (bottom row)."}, {"section_title": "Method", "text": "Sample size\nSimulation scenario Low-correlation High-correlation calculation of gram matrices in reported runtime (although these could be regarded as preprocessing steps). We do not consider ridge because it can be seen as a special case of SBR and hence equally fast when implemented as described here. Figure 4 shows the average runtimes (in minutes) over five fits on a single simulated dataset. SBR and sparse SBR are considerably faster than lasso with the gap increasing with p. We note that by adding random noise variables as described above we in a way \"favour\" the lasso implementation in glmnet, as the screening rules that are used by default can relatively easily exclude these covariates. SBR is the fastest method, with the average runtime for p = 10 7 being approximately 5 minutes, net of all steps. Also, running sparse SBR with suitable parallel block-matrix calculations makes this method also very fast; average runtime was approximately 10 minutes for p = 10 7 . We note that our current implementations of the methods are certainly not optimal in terms of computational efficiency. We note also that our methods can fully utilize available cores for parallel computation, hence they should continue to gain in runtime if more cores are available even with no increase in clock speed. 5 Alzheimer's disease case study"}, {"section_title": "Data", "text": "The data we consider are from the Alzheimer's Disease Neuroimaging Initiative (ADNI) (Mueller et al. 2005) , a large-scale longitudinal AD study involving multiple data modalities. The specific subset of the ADNI data we use were previously used in a DREAM challenge (Allen et al. 2016 ) and consists of n = 759 baseline and 24-month follow-up records. Similarly to the DREAM challenge and follow-up work (e.g. Dondelinger et al. 2016) we consider as the response scores from a cognitive function test called the mini mental state examination (MMSE); in particular, the difference in MMSE between 24 month follow-up and baseline. Figure 5: Alzheimer's disease case study, predictive performance. Correlations between predictions and held-out test data from 10 random splits with n train = 500 and n test = 259 under SBR, SSBR and cSSBR using the ML (left) and PM (right) penalty estimates.\nWe consider three data sources: (1) clinical (CL) data consisting of p CL = 12 features (including, among others, diagnosis at baseline, Apolipoprotein E status, gender, age, years of education); (2) structural magnetic resonance imaging (MRI) data consisting of p MRI = 929 features; and (3) genetic data in the form of SNP data, consisting of p SNP \u2248 7.3\u00d710 6 features (this is the number of SNPs available after excluding those with zero variance across subjects and those with more than 10% missing entries). We apply the proposed methods to these data, treating the three data types (1)-(3) as sources.\nThe aim is to consider a real-world application with data sources of widely differing dimensionality and to investigate whether adding the complex MRI and genetic data to the clinical covariates can improve predictive ability. We emphasize that the goals of the present paper are mainly methodological and that the results we present at this stage should be regarded as illustrative of the capabilities of the methods rather than as candidate AD predictors for practical use. Figure 5 shows results using SBR with ML and PM estimators (we omit CV) applied to CL only, CL and MRI and finally all of the data (CL, MRI and SNP). For the latter case we also show results using SSBR and cSSBR with f n = log(n). Predictive performance is quantified via the correlation between predicted and observed values in held-out test data. The boxplots show the results of 10 random train/test splits (with n train = 500, n test = 259 in each split) annotated with the number of variables with non-zero coefficients after fitting the models in each case. We see that the addition of MRI features to clinical covariates improves predictions. In contrast to many studies including MRI data, here we included all available MRI features without any pre-selection. Adding the SNP features does not increase predictive accuracy further, rather it slightly worsens performance, in line with previous work suggesting that genetic data is not helpful when clinical covariates are already available (Allen et al. 2016) . Notably, SSBR PM yields almost the same predictive accuracy as the regressions that do not include the SNP matrix at all; this is because the sparsified analyses are able to set all SNP coefficients to exactly zero. More generally, the excess risk (over CL and MRI alone) is relatively small in magnitude, despite the vast number of additional covariates. This is due to the fact that the models have a separate tuning parameter for each data source and are therefore able to effectively \"switch off\" this source, while continuing to regularize the other covariates via source-specific penalties. In addition, cSSBR PM provides identical predictions with SSBR PM whilst employing fewer CL and MRI features."}, {"section_title": "Discussion", "text": "The aim of this paper was to introduce a framework for high-dimensional regression using multiple data-sources that allows efficient and fast computations in the wide data, very large p setting. We showed that the SBR method is effective in this setting, while the sparse extension SSBR gives parsimonious solutions that can be attractive in many cases.\nConcerning the predictive performance of the various SBR variants, our empirical results suggest that the choice of shrinkage estimator (CV/ML/PM) is not very crucial; all three estimators seem able to adjust to individual data sources. As a consequence, all SBR approaches showed effective and stable prediction performance in both sparse and dense conditions. The corresponding SSBR solutions have the potential to achieve similar predictive performance but with explicit sparsity. In addition, using some control for the effect of sample size on sparsity is desirable as it can lead to enhanced sparsity with no loss in terms of prediction. In this case, cSSBR using the PM shrinkage estimator and log(n) as sample size control factor seems to be a promising option.\nThe proposed methodologies are very fast even when the number of predictors is large. Practical feasibility was explored in an analysis of data from a longitudinal study of AD with three data sources of widely differing dimensionality. We found that SBR and SSBR were able to usefully combine MRI and clinical data, improving predictive power with respect to clinical data alone. Moreover, they were able to adaptively \"switch off\" the genetic data which did not appear to improve predictive ability, with the sparsified SBR methods setting all SNP coefficients to zero."}, {"section_title": "Software", "text": "R code for SBR/SSBR implementation, together with an illustrative example, and complete documentation are available at https://github.com/mukherjeegroup/SBR. the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimers Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California."}, {"section_title": "Appendices A Posterior mode and covariance matrix", "text": "For the derivation of the posterior mode and covariance matrix in Eqs. (4) and (7) we use Woodbury's matrix identity (Harville 1997, p.424 ). Specifically we have that\nFor \u039b as defined in Section 2.2 it holds that X\u039b \u22121 X T = K k=1 \u03bb \u22121 k X k X T k = G \u03bb , thus, leading to Eqs. (4) and (7)."}, {"section_title": "B The leave-one-out CV estimator", "text": "The leave-one-out CV estimates are obtained via Here \u03b2 \\i (\u03b8) is the posterior mode from the regression of y \\i (y without the i-th element) on X \\i (X without the i-th row). For simplicity \u03b2 \\i \u2261 \u03b2 \\i (\u03bb) henceforth. First, set A = \u039b + X T X and observe that\nFor the transition from the second to the first line we used the Sherman-Morrison formula (Harville 1997, p.424 ). Note that x T i A \u22121 x i = h ii , i.e. the i-th element of the main diagonal of the hat matrix H \u03bb = X(\u039b + X T X) \u22121 X T . Since \u03b2 \\i = (\u039b + X T \\i X \\i ) \u22121 X T \\i y \\i = (\u039b + i X T \\i X \\i ) \u22121 (X T y \u2212 x i y i ) from the result in (B.2) we have\nPluging in (B.3) in the quantity we wish to minimize in (B.1) we get\nSwitching to matrix notation we have\nThe derivation up to (B.4) is as in Meijer (2010) who considers the OLS case. Now let us further examine the quantity H \u03bb . From (A.1) we have\n(Woodbury identity)\nAs a result i + \u03b1|\u03b3 j |, with \u03b2 j = 0, and let \u03b3 j = argmin \u03b3 j f (\u03b3 j ) for j = 1, . . . , p. It is straightforward to see that For \u03b2 j > 0 : \u2200 \u03b3 j \u2265 0, f (\u03b3 j ) \u2264 f (\u2212\u03b3 j ) \u21d2 \u03b3 j \u2265 0 (C.1) For \u03b2 j < 0 : \u2200 \u03b3 j \u2264 0, f (\u03b3 j ) \u2264 f (\u2212\u03b3 j ) \u21d2 \u03b3 j \u2264 0 (C.2) From (C.1) and (C.2) we have that sign( \u03b3 j ) = sign( \u03b2 j ), \u2200 \u03b3 j = 0. In addition for \u03b3 j = 0 f (\u03b3 j ) = (\u03b3 j \u2212 \u03b2 j )cv \u22121 j + sign(\u03b3 j )\u03b1, therefore\nThus, when \u03b2 j > 0 from (C.1) and (C.3) we have that\n, otherwise and when \u03b2 j < 0 from (C.2) and (C.3) we have that\n, otherwise.\nWhich concludes the proof. For c = n RSS we obtain the solution in (18).\niii"}]