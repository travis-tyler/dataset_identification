[{"section_title": "Abstract", "text": "Inferring brain connectivity network and quantifying the significance of interactions between brain regions are of paramount importance in neuroscience. Although there have recently emerged some tests for graph inference based on independent samples, there is no readily available solution to test the change of brain network for paired and correlated samples. In this article, we develop a paired test of matrix graphs to infer brain connectivity network when the groups of samples are correlated. The proposed test statistic is both bias corrected and variance corrected, and achieves a small estimation error rate. The subsequent multiple testing procedure built on this test statistic is guaranteed to asymptotically control the false discovery rate at the pre-specified level. Both the methodology and theory of the new test are considerably different from the two independent samples framework, owing to the strong correlations of measurements on the same subjects before and after the stimulus activity. We illustrate the efficacy of our proposal through simulations and an analysis of an Alzheimer's Disease Neuroimaing Initiative dataset."}, {"section_title": "Introduction", "text": "Brain functional connectivity reveals the intrinsic functional architecture of brains by measuring correlations in neurophysiological recordings of brain activities (Varoquaux and Craddock, 2013) . Numerous studies have found that functional connectivity alters for individuals with neurological disorders, such as Alzheimer's diseases (AD) and autism spectrum disorder (Hedden and others, 2009; Rudie and others, 2013) , or after experiencing stimulus activities such as stress or therapy (Peck and others, 2004; van Marle and others, 2010) . The brain connectivity network is believed to hold crucial insight to help understand the pathologies of neurological disorders and to develop targeting treatment (Fox and Greicius, 2010; Quaedflieg and others, 2015) . Brain functional connectivity is commonly encoded as a network, or graph, with nodes representing brain regions, and links representing interactions and correlations between regions. Among multiple correlation measures, partial correlation is a well accepted and frequently used metric, and correspondingly, the connectivity network is portrayed by a partial correlation matrix (Ryali and others, 2012; Chen and others, 2013) . Current mainstream imaging modalities to study functional connectivity include electroencephalography (EEG), electrocorticography (ECoG), and resting-state functional magnetic resonance imaging (fMRI). After proper preprocessing, the resulting imaging data for each subject is summarized in the form of a location by time matrix, from which a partial correlation matrix is constructed to characterize brain connectivity. A central problem in connectivity analysis is inference. Unlike network estimation (Ahn and others, 2015; Chen and others, 2015; Kang and others, 2016; Qiu and others, 2016; Wang and others, 2016) , network inference aims to directly quantify the statistical significance of individual links or their differences, meanwhile explicitly controlling for the false discovery. Recently there have been proposals of partial correlation matrix based network inference for vector-valued data following a normal distribution (Liu and others, 2013; Xia and others, 2015) , or matrix-valued data following a matrix-normal distribution (Chen and Liu, 2019; Li, 2017, 2019) . For brain connectivity analysis, the data obtained from EEG, ECoG, or fMRI is of a matrix form, and the primary scientific interest is on the spatial but not the temporal correlation patterns of the brain. Directly applying the tests for vector-valued data to infer the spatial patterns ignores the temporal correlations among the columns of the matrix data, and is to result in distorted test size and false discovery rate (Xia and Li, 2017) . Whitening can alleviate this problem, and in effect transforms the matrix data back to the vector case (Narayan and others, 2015) . However, it does not utilize the data efficiently, would result in loss of power, and is also computationally intensive (Xia and Li, 2019) . Alternatively, Chen and Liu (2019) ; Xia and Li (2017) directly tackled inference of the matrix-valued data under the one-sample testing scenario, and Xia and Li (2019) tackled the two-sample scenario where the two groups of samples are independent. In addition to inference about brain network alternation across independent subject groups, it is of equal interest and importance to infer the change of brain network of the same group of subjects before and after a \"stimulus\" activity, which could be a treatment, a disease conversion, or a different experimental condition. For instance, Peck and others (2004) studied brain connectivity activities in auditory and motor cortices of aphasic patients before and after a therapy. Gianaros and others (2008) ; van Marle and others (2010); Quaedflieg and others (2015) studied amygdala-centered connectivity patterns in healthy subjects before and after the experimentally-induced stress. Cai and others (2015) studied alterations in brain functional networks in patients with primary angle-closure glaucoma before and after the surgery. Kang and others (2016) studied brain connectivity activities in left and right inferior frontal gyrus areas of the same subjects under different sleeping conditions. Ficek and others (2018) studied changes of functional connectivity before and after a language intervention therapy. In Section 5, we aim to identify the connectivity patterns that differ before and after a patient converted to AD. The two-sample test of Xia and Li (2019) does not directly apply to those studies, because of the strong correlations of brain measurements on the same subjects before and after the stimulus. For instance, a positive correlation before and after the stimulus would reduce the variance of the partial correlation difference between the two groups, causing the two-sample test to overestimate the variance and resulting in a low test power. On the contrary, a negative correlation would inflate the variance, causing the two-sample test to underestimate the variance and resulting in an inflated false discovery. In this article, we develop a paired test of matrix graphs to infer brain connectivity network when the groups of samples are correlated, such as in the scenario of before and after the stimulus. The key of our proposal is an innovative variance correction procedure that incorporates the spatial and temporal dependency between the paired samples. The proposed test statistic is both bias corrected and variance corrected, and is shown to achieve a sufficiently small estimation error rate asymptotically. This in turn ensures that the subsequent multiple testing procedure built on this test statistic can asymptotically control the false discovery rate at the pre-specified level. Our proposal extends the two-sample test of Xia and Li (2019) , but is considerably different. This extension is far from trivial, and the theoretical investigation of the paired test is much more involved, as one needs to carefully evaluate both within-sample and between-sample correlations. To our knowledge, there is no existing graph inference procedure for paired matrix samples, and our proposal offers a timely response to an important problem of both scientific and methodological interest. The rest of the article is organized as follows. Section 2 presents the formulation of the hypothesis testing problem, the proposed test statistic, the variance correction procedure for the paired samples, and the multiple testing procedure. Section 3 studies the corresponding asymptotic properties. Section 4 examines the empirical performance of the proposed test through simulations, and Section 5 analyzes a real fMRI dataset. The appendix collects all the technical assumptions, proofs and additional numerical results."}, {"section_title": "Paired test 2.1 Problem formulation", "text": "Let X (t) denote the p \u00d7 q matrix observed at time point t, t = 1, 2. In brain connectivity analysis, X (t) denotes the spatial-temporal imaging data before (t = 1) and after (t = 2) a stimulus activity or conversion, and each X (t) corresponds to p brain regions and the time course data of each region is of length q. We assume {X\n(1) , X (2) } follows a matrix normal distribution, i.e.,\nWithout loss of generality, the mean is assumed to be zero, \u2297 is the Kronecker product, and vec(\u00b7) is the operator that stacks the columns of a matrix into a vector. Furthermore, \u03a3 St \u2208 IR p\u00d7p denotes the covariance matrix of the spatial regions, \u03a3 Tt \u2208 IR q\u00d7q denotes the temporal covariance matrix of the time course data, at t = 1, 2, respectively, and \u03a3 S1,2 and \u03a3 T1,2 denote the between-sample spatial and temporal covariance, respectively. When \u03a3 S1,2 \u2297 \u03a3 T1,2 = 0, (2.1) reduces to the independent two-sample setting of Xia and Li (2019) . We remark that, the matrix normal distribution has been frequently adopted in numerous applications involving matrix-valued data (Yin and Li, 2012; Leng and Tang, 2012) , and is also scientifically plausible in neuroimaging analysis (Smith and others, 2004; Friston and others, 2007) . Moreover, Aston and others (2017) developed a test to check if the data conforms with the Kronecker product structure. In Section 4.2, we further carry out sensitivity analysis, and show that our proposed test works reasonably well even when the data deviates from the matrix normal distribution (2.1).\nLet \u2126 St = \u03a3 \ndenote the spatial partial correlation matrix. In brain connectivity analysis, the primary interest is to infer the connectivity network characterized by the spatial partial correlation matrix. The temporal covariance or precision matrix is of little interest in this context and is to be treated as a nuisance parameter. Consequently, we formulate our inference problem as simultaneously testing\nWe next derive the test statistic and the associated variance correction to account for the correlations of the paired samples."}, {"section_title": "Test statistic", "text": "Consider n pairs of samples X\n(1)\nfrom the joint distribution (2.1). To construct the test statistic for (2.2), we first remove the temporal correlations by the linear transformation Y\n. . , n, t = 1, 2, and\n3)\ndenotes the between-sample temporal covariance matrix of the transformed samples. Clearly, for the independent case, \u03a3 S1,2 \u2297 P T1,2 = 0. In practice, \u03a3 Tt and \u03a3 T1,2 are generally unknown. There are multiple ways to estimate \u03a3 Tt , or equivalently,\nTt . Examples include the sample covariance estimator, the banded covariance estimator (Bickel and Levina, 2008) , the adaptive thresholding estimator for \u03a3 Tt , or the Clime estimator (Cai and others, 2011) for \u2126 Tt . We adopt the banded estimator in this article, given its competitive performance in both the one-sample test and the independent two-sample test under the matrix normal distribution Li, 2017, 2019) . In the following, we first derive the test statistic with known \u03a3 Tt and P T1,2 , which helps simplify the notations considerably. We then extend it by plugging in an estimated \u03a3 Tt and P T1,2 . Accordingly, we will add the superscript (d) in the resulting statistics to represent this scenario when \u03a3 Tt and P T1,2 are estimated given the data. In Section 3 we show that the test statistics under the known \u03a3 Tt , P T1,2 and the estimated \u03a3 Tt , P T1,2 have the same asymptotic property. Consequently, they lead to the same multiple testing procedure with the guaranteed asymptotic control of false discovery. The construction of our test statistic is based on the fact that, under the normal distribution, the precision matrix can be described through the regression model (Anderson, 2003) ,\nwhere the error term\nk,\u2212i,l , and the subscript \u2212i means the ith entry is removed from a vector, or the ith row or column removed from a matrix. The regression coefficient \u03b2 (t) i can be estimated using Lasso or other methods, as long as the estimator \u03b2 (t) i satisfies the regularity condition (A5) or (A6) in the appendix. See Li (2017, 2019) and Section D of the appendix for a more detailed discussion on estimation of \u03b2 (t) i and the associated tuning procedure. Moreover, the error term satisfies that r\n. Therefore the element \u03c9 St,i,j of the spatial precision matrix R St , and in turn, the element \u03c1 St,i,j of the spatial partial correlation matrix R St can be represented in terms of r (t) i,j . Following Li (2017, 2019) , a bias-corrected estimator of r (t) i,j is obtained from fitting the regression model (2.4),\nk,j,l is the sample covariance between the residuals, i,j , we further obtain a bias-corrected estimator of the element \u03c1 St,i,j of the spatial partial correlation matrix R St as\nWe then construct our test statistic for the pair of hypotheses (2.2) as\nwhere \u0398 i,j is an estimator of var ( \u03c1 S1,i,j \u2212 \u03c1 S2,i,j ). We next develop such an estimator that incorporates the between-sample dependency of the paired samples."}, {"section_title": "Variance correction", "text": "We first recognize that the expression for the variance term var ( \u03c1 S1,i,j \u2212 \u03c1 S2,i,j ) is quite involved.\nTo alleviate this issue, we introduce an intermediate term,\n, where\n. Lemma B.1 in the appendix implies that the difference between \u03c1 St,i,j and U"}, {"section_title": "(t)", "text": "i,j is negligible. Consequently, we estimate var ( \u03c1 S1,i,j \u2212 \u03c1 S2,i,j ) by developing an estimator for\nFor the independent two-sample setting, \u0398 i,j = \u03b8\nFor the paired samples, however, it is crucial to account for the between-sample spatial-temporal dependency as presented in \u03a3 S1,2 and \u03a3 T1,2 when estimating \u0398 i,j . Next we derive such an estimator of \u0398 i,j . Later in Section 3, we show that this estimator is accurate, in the sense that its scaled version achieves an o p (1/ log p) convergence rate. This error rate is essential for the subsequent asymptotic false discovery control in multiple testing. The next proposition gives an explicit expression of \u0398 i,j under the dependent setting. Its proof is given in the appendix. The key is the separable spatial and temporal dependence structures between the paired samples, and the decoupling of \u03c1 (1,2) i,j;l1,l2 = corr\nwhere \u03c1 S1,2,i,j = r\nj,j \u2126 S1,i,\u00b7 \u03a3 S1,2 \u2126 S2,\u00b7,j accounts for the spatial correlation, and P T 1,2,l 1 ,l 2 captures the temporal dependency. Here \u2126 S1,i,\u00b7 denotes the ith row of the matrix \u2126 S1 , and \u2126 S2,\u00b7,j denotes the jth column of \u2126 S2 .\nProposition 2.1. Under the data distribution (2.3), we have,\nfor 1 \u2264 i < j \u2264 p, where \u00b7 F denotes the Frobenius norm."}, {"section_title": "Define", "text": "(1,2) i,j = \u03c1 S1,2,i,j \u00b7 tr(P T1,2 )/q, which is the correlation coefficient \u03c1 S1,2,i,j scaled by the term tr(P T1,2 )/q, and tr(\u00b7) denotes the matrix trace. We observe that\nTherefore we can estimate\nj,j , and cov(\nCorrespondingly, when \u03a3 Tt , \u03a3 T1,2 and thus P T1,2 are known, we can estimate \u0398 i,j by\nWe show in Section 3 that \u0398 i,j in (2.8) provides an accurate estimation of \u0398 i,j , with an error rate of order o p (1/ log p), when \u03a3 Tt and \u03a3 T1,2 are known. When \u03a3 Tt and \u03a3 T1,2 are unknown, we first estimate P T1,2 by\nwhere\n, and \u03a3 Tt is an estimator of \u03a3 Tt . We then plug (2.9) into (2.8). Again we show in Section 3 that this estimator also provides an accurate estimation of \u0398 i,j , with an error rate of order o p (1/ log p), when \u03a3 Tt and \u03a3 T1,2 are unknown. We make a few remarks about our proposed variance correction. First, a crucial component of our method is to pool data information of the p-dimensional spatial and q-dimensional temporal measurements of n subjects in our estimations. The data pooling is possible due to the facts that\n. Consequently, we can pool the columns of Y (t) to estimate \u03a3 S1,2 , and the rows of Y (t) to estimate P T1,2 , up to a constant. More specifically, when \u03a3 Tt and \u03a3 T1,2 are known, we pool 2nq samples to estimate the within-sample variance as in (2.5), and the between-sample spatial dependency as in (2.7) and (2.8). When \u03a3 Tt and \u03a3 T1,2 are unknown, we also pool 2np samples to obtain the estimates \u03a3 \u22121/2 Tt , t = 1, 2, and estimate the temporal dependency between the before-stimulus scan and the after-stimulus scan as in (2.9). Such data pooling is the main difference between our method and a naive solution, which estimates the dependency between the paired samples by the usual sample covariance, namely, estimating cov\nk,j,l2 , for each 1 \u2264 i < j \u2264 p, 1 \u2264 l 1 , l 2 \u2264 q. Note that, the latter approach only uses n observations to estimate the dependence structure without any data pooling, and as a result, it can not guarantee the estimation error rate required to ensure the performance of the test. Second, we note that the spatial and temporal covariances \u03a3 S1,2 and P T1,2 are only identifiable up to a constant. However, this does not affect our test statistic, nor our variance estimation. This is because, when replacing (\u03a3 S1,2 , P T1,2 ) with (c\u03a3 S1,2 , P T1,2 /c), where c is any positive factor, the terms\n2 remain the same, in which the factor c is cancelled.\nThird, Chen and Liu (2018) developed a variance correction method for matrix-valued data, but for a single group of samples. In contrast, we perform variance correction for two stages of samples from the same population. We first separate the spatial and temporal structures, so that the resulting test statistics do not require variance correction within each sample. Our variance correction differs from that of Chen and Liu (2018) considerably. On the other hand, if the temporal covariance between two stages has some particular structure, e.g., if it is sparse, then the method of Chen and Liu (2018) may be applied to our procedure, by thresholding P\nT1,2 in (2.9) accordingly. In this paper, however, we do not impose any structural condition on the temporal dependence, and thus we use the general sample covariance estimator in (2.9) instead."}, {"section_title": "Multiple testing", "text": "We next develop a multiple testing procedure for H 0,i,j : \u03c1 S1,i,j = \u03c1 S2,i,j , 1 \u2264 i < j \u2264 p, so to identify spatial locations with their conditional dependence changed before and after the stimulus. With a total of p(p \u2212 1)/2 simultaneous tests, the key is to control false discovery. Let h be the rejection threshold value such that H 0,i,j is rejected if |W i,j | \u2265 h, and H 0 := {(i, j) : \u03c1 S1,i,j = \u03c1 S2,i,j , 1 \u2264 i < j \u2264 p} be the set of true nulls. Then the false discovery proportion (FDP) and the false discovery rate (FDR) are computed as\nOur multiple testing procedure is based on the test statistic W i,j derived in Section 2.2, with the corrected variance estimates \u0398 i,j derived in Section 2.3. The rest of the procedure is similar to that of the two-sample independent test of Xia and Li (2019) . We thus only outline the main steps here. First, we compute the paired-test statistics W i,j in (2.5) for all 1 \u2264 i < j \u2264 p. Next we estimate the false discovery proportion by\nwhere \u03a6(\u00b7) is the standard normal cumulative distribution function. Here we conservatively estimate |H 0 | by (p 2 \u2212 p)/2, as it is at maximum (p 2 \u2212 p)/2 and is close to (p 2 \u2212 p)/2 when R S1 \u2212 R S2 is sparse. Next, we compute the rejection threshold value h \u03b1 under a given significance level \u03b1 as\nIf h \u03b1 does not exist, we set h \u03b1 = 2(log p) 1/2 . Finally, we reject H 0,i,j if and only if |W i,j | \u2265 h \u03b1 for each 1 \u2264 i < j \u2264 p. In Section 3 we show that the above multiple testing procedure can control FDR at the pre-specified level asymptotically."}, {"section_title": "Theory", "text": "We study in this section the asymptotic properties of the proposed testing procedure. In the interest of space, we present all the regularity conditions (A1)-(A7) in the appendix. We first show that the corrected variance estimator of \u0398 i,j we develop in Section 2.3 achieves the estimation error rate of o p (1/ log p). We then show that, based on such an error rate, the subsequent multiple testing procedure can control the false discovery asymptotically. When \u03a3 Tt and \u03a3 T1,2 are known, our variance estimator is \u0398 i,j as given in (2.8). The next proposition establishes its error rate.\nProposition 3.1. Suppose (A1), (A3) and (A5) hold. Then we have\nWhen \u03a3 Tt and \u03a3 T1,2 are unknown, we denote our variance estimator as \u0398 (d) i,j , which is obtained by plugging the estimator P (d) T1,2 in (2.9) into (2.8). The next proposition establishes its error rate.\nProposition 3.2. Suppose (A1), (A3) and (A6)-(A7) hold. Then we have\nThe above two propositions show that, the variance estimation error is bounded by the same error rate asymptotically, when \u03a3 Tt and \u03a3 T1,2 are unknown and when they are known. The next theorem shows that, for the dependent samples, as long as the majority of the regression residuals are not highly correlated with each other under the null hypothesis, then the FDR can be controlled asymptotically at the pre-specified level \u03b1 following the multiple testing procedure outlined in Section 2.4.\nc2 for some c 1 , c 2 > 0. Let h \u03b1 denote the threshold value in (2.10). Then, when (A1)-(A5) hold and \u03a3 Tt and \u03a3 T1,2 are known, or when (A1)-(A4), (A6)-(A7) hold and \u03a3 Tt and \u03a3 T1,2 are unknown, we have\nIn addition to false discovery control, the asymptotic power analysis is another interesting problem. It relies on the specific structure of the connectivity network. In Section 4, we conduct extensive simulations to study the power of our test under numerous network structures, and we leave the theoretical power analysis as future research."}, {"section_title": "Simulations", "text": ""}, {"section_title": "Empirical FDR and power with and without variance correction", "text": "We conduct numerous simulations to study the finite sample performance of our proposed variancecorrected testing procedure. We also compare with the two-sample test of Xia and Li (2019) , which ignores the correlation before and after the stimulus and does not correct the variance accordingly.\nIn all the simulations, we use Lasso to estimate the regression coefficient \u03b2\ni , and use the banded covariance approach to estimate \u03a3 Tt . We set the FDR level at \u03b1 = 1%. We examine a set of spatial and temporal dimensions, (p, q) \u2208 {(200, 50), (200, 200) , (800, 200)}, while we fix the sample size at n = 15. We consider two temporal covariance structures: an autoregressive structure, where \u03a3 Tt = (\u03c3 Tt,i,j ), \u03c3 Tt,i,j = 0.4 |i\u2212j| if t = 1, and \u03c3 Tt,i,j = 0.5 |i\u2212j| if t = 2, 1 \u2264 i, j \u2264 p, and a moving average structure, where \u03a3 Tt = (\u03c3 Tt,i,j ), \u03c3 Tt,i,j = 1/(|i \u2212 j| + 1) for |i \u2212 j| < 3 if t = 1, and \u03c3 Tt,i,j = 1/(|i \u2212 j| + 1) for |i \u2212 j| \u2264 4 if t = 2. We also consider three spatial covariance structures: a banded graph, with bandwidth equal to 3 (Zhao and others, 2012), a hub graph, with rows and columns evenly partitioned into 20 disjoint groups, and a small-world graph, with 5 starting neighbors and 5% probability of rewiring (van Wieringen and Peeters, 2016) . We first generate \u2126 S1 according to one of the above spatial structures, then construct \u2126 S2 by randomly eliminating 50% percent of the edges of \u2126 S1 . Moreover, we consider two settings of correlation patterns before and after the stimulus. In Setting I, we set \u03a3 S1,2 = \u03b3\u03a3 S1 , where \u03b3 is the overall correlation level and |\u03b3| \u2264 1. Since \u03b3 plays its role through \u03b3 2 , its sign does not matter, and we choose \u03b3 \u2208 {0, 0.2, 0.4, 0.6}. When \u03b3 = 0, it reduces to the two-sample independent case, whereas a larger value of \u03b3 implies a stronger before-and-after stimulus correlation. We next set P T1,2 as a diagonal matrix with P T1,2,i,i = \u22121 if i \u2261 k (mod 15), k \u2208 {1, 3, 5}, and 1 otherwise. Here for three positive integers a, b and c, a \u2261 b (mod c) means that, when divided by c, a and b have the same remainder that is non-negative and smaller than c. In this setting, it follows that\nas long as \u03b3 > 0, where we utilize the facts that \u03a3 S1,2 = \u03b3\u03a3 S1 = \u03b3\u2126 \u22121 S1 , and \u2126 S2 is a positive definitive matrix. Correspondingly, \u0398 i,j is smaller than that of the independent case, and the test statistic W i,j would be larger than that without variance correction in its absolute value. For this setting, the two-sample test without variance correction is to yield a smaller power, as it is more conservative in rejecting the null hypothesis in this setting. In Setting II, we set P T1,2 in the same way, but set \u03a3 S1,2,i,\nis the indicator function. In this setting, we no longer have a simplified expression for \u03c1\nS1,2,j,i , but empirically, we have observed that this term is negative for about half of (i, j) pairs regardless of the choice of the spatial structure and the dimension p. For those pairs, \u0398 i,j is larger than that of the independent case, and the test statistic W i,j would be smaller than that without variance correction in its absolute value. For this setting, the two-sample test without variance correction is to yield an overestimated FDR in this setting. Tables 1 and 2 report the empirical FDR and the empirical power, both in percentage, out of 100 data replications for the two settings, respectively. We make the following observations. For Setting I, when (p, q) = (200, 50), the test with variance correction controls the FDR around the anticipated level of \u03b1 = 1%, whereas the test without variance correction yields a much lower FDR than the significance level. Moreover, as the correlation strength \u03b3 increases, the power of the test with variance correction improves considerably compared to the test without correction. Similar qualitative patterns are observed for (p, q) = (200, 200) and (p, q) = (800, 200). For Setting II, for different combinations of (p, q) and spatial structures, the test with variance correction again controls the FDR close to the significance level, while the test without correction fails to control FDR as \u03b3 increases. When (p, q) = (200, 50), the test with correction is slightly inferior to that without correction for the banded graph in terms of power. This is not surprising though, as it is attributed to the inflated FDR. For other spatial structures, the test with correction clearly outperforms the one without correction. When (p, q) = (200, 200), we observe that the power of both tests increases to 100% or close. For FDR, the inflation issue still remains for the test without correction. When (p, q) = (800, 200), we observe a similar qualitative pattern. In summary, our proposed test with variance correction can control the false discovery and attain a good power for a range of strength of correlation before and after the stimulus. By contrast, the test without correction has inferior power performance for Setting I, and fails to control the FDR and yields an inflated power for Setting II as this correlation increases. We also report the mean squared error of \u0398 (d) in Section E of the appendix."}, {"section_title": "Sensitivity analysis", "text": "We next carry out sensitivity analysis to evaluate the performance of our test when the data deviates from the matrix normal distribution. We first replace the normal distribution by a t distribution. Specifically, we follow the data generation mechanism as before, while we set \u03a3 S1,2 as a diagonal matrix with \u03a3 S1,2,i,\n, k \u2208 {1, 3, 5}]) and \u03b3 = 0.6. Since a normal random vector X \u223c N (0, \u03a3) can be represented as X = \u03a3 1/2 Z, where Z \u223c N (0, I p ), we replace the Gaussian entries in Z with t-distributed random variables with degree of freedom df \u2208 {4, 6, 8}. We report the empirical FDR and power out of 100 data replications in Table 3 , part I. It is seen that, our test manages to control the FDR reasonably well, and attains a good power under different dependence structures. We then examine the performance of our method with regard to the off-diagonal Kronecker product structure, i.e, cov(vec{X\nWe again follow the data generation mechanism as before, but set \u03a3 S1,2 as a diagonal matrix with \u03a3 S1,2,i,\n, \u03b3 = 0.6, and (p, q) = (200, 50). We further perturb \u03a3 1,2 = \u03a3 S1,2 \u2297 \u03a3 T1,2 in two steps: we randomly sample p % entries of \u03a3 1,2 , where p \u2208 {0, 1, 5, 10}, then replace those entries with i.i.d. Gaussian random variables of mean zero and standard deviation \u03bd, where \u03bd = l \u00d7 the magnitude for the entries of \u03a3 1,2 , and l \u2208 {0.1, 1}. We report the empirical FDR and power out of 100 data replications in Table 3 , part II. It is seen that, our test maintains a reasonably good performance in this setup too. These results show that our method is relatively robust with regard to the joint matrix normal assumption (2.1). We also comment that, it is possible to extend our test to semiparametric normal copula setting. Liu and others (2012) and Xue and Zou (2012) studied the vector-valued case. Following a similar idea of marginal monotonic transformation, it is possible to develop a paired test in the matrix-valued setting. We leave the full investigation as future research."}, {"section_title": "Alzheimer's disease data analysis", "text": "Alzheimer's disease (AD) is an irreversible neurodegenerative disorder, and is characterized by progressive impairment of cognitive and memory functions. It is the leading form of dementia in the elderly subjects. With the aging of the worldwide population, the number of affected people is rapidly increasing and is projected to be 13.8 million in the United States, and 1 in 85 worldwide by year 2050 others, 2007, 2011) . It thus has become an international imperative to understand, diagnose, and treat this disorder. Accumulated evidences have suggested that alterations in brain connectivity networks are predictive of cognitive function and decline, and hold crucial insights about the disease pathology of AD (Fox and Greicius, 2010) . We analyzed a dataset from the Alzheimer's Disease Neuroimaing Initiative (ADNI). ADNI is an ongoing, longitudinal, multi-center study designed to develop clinical, imaging, genetic, and biochemical biomarkers for the early detection and tracking of AD. We focused on 23 subjects from ADNI who experienced conversion from mild cognitive impairment (MCI), a prodromal stage of AD, to AD during the 24-month follow-up. The primary scientific question of interest is to investigate the change of brain connectivity patterns before and after the conversion. All fMRI scans were resting-state and preprocessed, including slice timing correction, motion correction, spatial smoothing, denoising by regressing out motion parameters, white matter and cerebrospinal fluid time courses, and band-pass filtering. The data were then aligned and parcellated using the Anatomical Automatic Labeling atlas (Tzourio-Mazoyer and others, 2002) . The resulting data is a region by time matrix for each subject, with the spatial dimension p = 116 and the temporal dimension q = 130. We first examined the quantile-quantile plot, which shows no clear deviation from the normal distri-bution. We next applied the testing procedure of Aston and others (2017) to test if the data conforms with the Kronecker product structure. The p-values of the test before and after the conversion are 0.17 and 0.11, respectively, which suggests that the product structure seems to reasonably hold for this dataset. We then applied our proposed variance-corrected testing procedure to this data. In our analysis, we did not correct for potential confounder effects, but our test can be equally applied to the corrected data. Figure 1 plots those top differentiating links whose corresponding p-values are smaller than 1e \u2212 13, and the associated brain regions visualized with the BrainNet Viewer (Xia and others, 2013) . Table 4 further reports the top 30 links that were found different before and after the conversion, with their associated p-values and the directions of the change. It is seen that the differentiating links concentrate on the cerebellum. The cerebellum is critical in the distributed neural circuits subserving not only motor function but also autonomic, limbic and cognitive behaviors. There is recently increased interest in exploring the role of the cerebellum in neurodegenerative disorders, in particular Alzheimer's disease (Jacobs and others, 2018) . Our findings provide a useful support to the existing literature."}, {"section_title": "Supplementary material", "text": "The computer code in R for the simulation and data analysis can be found at https://github.com/ Elric2718/PairedTestPrecisionMatrix with the commit number 631c4e4. The Alzheimer's disease dataset can be found at https://doi.org/10.6084/m9.figshare.9643010.v3. Table 2 : The empirical FDR (with standard error in the parenthesis) and the empirical power, both in percentage, for Setting II. Table 3 : Sensitivity I (top): The empirical FDR (with standard error in the parenthesis) and the empirical power, both in percentage, for the sensitivity analysis on the normality. Sensitivity II (bottom): The empirical FDR (with standard error in the parenthesis) and the empirical power, both in percentage, for the sensitivity analysis on the Kronecker structure of the off-diagonal block, i.e., cov(vec{X Table 4 : Top 30 differentiating links of the brain connectivity networks of the 23 subjects of the ADNI database before and after the conversion from MCI to AD. The last column shows the direction of the link change. \"+\" represents the link gets enhanced after the conversion, and \"-\" represents the link gets weakened after the conversion. Cerebellum 7b L\u2194Temporal Inf R 7.02e-11 + Figure 1 : Top 10 differentiating links of the brain connectivity networks of the 23 subjects of the ADNI database before and after the conversion from MCI to AD. All the associated p-values are smaller than 1e \u2212 13. Red links are those enhanced after the conversion, and blue links are those weakened after the conversion."}, {"section_title": "A Regularity conditions", "text": "We first introduce a set of regularity conditions that are required to establish the asymptotic properties for our proposed testing procedure. In the following, we note that, Condition (A5) is for the case when \u03a3 Tt and \u03a3 T1,2 are known, and (A6)-(A7) are for the case when \u03a3 Tt and \u03a3 T1,2 are unknown. Conditions (A1)-(A4) are required for both cases. Denote \u03bb min (\u00b7) and \u03bb max (\u00b7) as the smallest and the largest eigenvalue, respectively, and \u03c3 max (\u00b7) as the largest singular value.\n(A1) There are constants c 0 , c 1 , c 2 > 0 such that (i) c\n(A3) Assume that log p = o{(nq) 1/5 }, and q = o{np/(log p) 2 }.\n(A5) When \u03a3 Tt and \u03a3 T1,2 are known, denote the corresponding regression coefficient estimate by \u03b2\n(A6) When \u03a3 Tt and \u03a3 T1,2 are unknown, denote the corresponding regression coefficient estimate by \u03b2\n. Assume that max 1\u2264i\u2264p,1\u2264l\u2264q\n, and max 1\u2264i\u2264p,1\u2264l\u2264q n,p,q,t , r\n, and 1(\u00b7) is the indicator function. Here, for a matrix\n|a i,j | is the matrix element-wise max norm, and\nMost of the above conditions are parallel to those for the two-sample test of matrix graphs of Xia and Li (2019) . Here we make remarks on a few different ones. In (A1), we have added two conditions, (ii) max{\u03c3 max (P T1,2 )|tr(\u03a3 S1,2 )|/p, \u03c3 max (\u03a3 S1,2 )|tr(P T1,2 )|/q} \u2264 c 1 , and (iii) |tr(P T1,2 ) tr(\u03a3 S1,2 )|/(pq) \u2265 c 2 . These are purely technical assumptions to simplify the proofs, and we view both conditions mild. Note that the three terms, \u03c3 max (P T1,2 )|tr(\u03a3 S1,2 )|/p, \u03c3 max (\u03a3 S1,2 )|tr(P T1,2 )|/q, and |tr(P T1,2 )tr(\u03a3 S1,2 )|/(pq), are all identifiable. It can be easily shown that both singular values, \u03c3 max (P T1,2 )|tr(\u03a3 S1,2 )|/p and \u03c3 max (\u03a3 S1,2 )|tr(P T1,2 )|/q, are bounded if \u03a3 defined in (2.1) of the paper has bounded eigenvalues. Furthermore, the before and after stimulus observations at the same time points or spatial locations are likely to have non-trivial temporal or spatial dependency, and thus it is reasonable to assume |tr(P T1,2 )tr(\u03a3 S1,2 )|/(pq) to be bounded away from zero. In (A3), we have added that q = o{np/(log p) 2 }. This is to ensure the convergence rate under the Frobenius norm and the maximum norm for the estimate of P T1,2 in computing the empirical covariance matrix by pooling 2p spatial locations. Again it is a mild technical condition."}, {"section_title": "B Technical lemmas", "text": "We next introduce some technical lemmas that are useful for the subsequent proofs. Define U (t)\nk,\u00b7,l = (\nk,\u00b7,l , t = 1, 2. Lemma B.1. Suppose that (A1), (A3) and (A5) hold. Then we have\nA similar lemma was proved in Xia and others (2015) , but we deal with nq inverse regression models here.\nThen, for some constant C > 0, \u03c3\ni,j satisfies the large deviation bound,\nThis lemma was proved in Lemma 4 of Cai and others (2013) .\nLemma B.3. Suppose (A1), (A3), (A6) and (A7) hold, then uniformly in 1\nfor t = 1, 2, where r\nThis lemma was essentially proved in the proofs of Theorems 5 and 6 in Xia and Li (2017) ."}, {"section_title": "C Proofs", "text": "We now prove the propositions and theorem in the paper. Let a n1 and a n2 satisfy max 1\u2264i\u2264p \u03b2\nwhen \u03a3 Tt and \u03a3 T1,2 are known, and satisfy max 1\u2264i\u2264p,1\u2264l\u2264q \u03b2\nwhen \u03a3 Tt and \u03a3 T1,2 are unknown. Then a n1 = o {log max(p, q, n)} \u22121 and a n2 = o (nq log p) \u22121/4 , respectively, following Assumption (A5) or (A6)."}, {"section_title": "C.1 Proof of Proposition 2.1", "text": "Note that\n(1) k,j,l1\nBy (2.3) and (2.4) in the paper, it follows that\nwhere\u03a3 (i,j,l1,l2) is a positive semidefinitive matrix. Due to the fact that var(\ni,i , we writ\u0207\nk,j ,l2 ), i , j \u2208 {i, j}; \u03c1\nk,j,l2 ), for any 1 \u2264 l 1 , l 2 \u2264 q, t = 1, 2, and corr(\u00b7, \u00b7) denotes the Pearson correlation between two random variables. It has been shown that \u03c1\ni,j;l1,l2 and \u03c1 (1,2) j,j;l1,l2 , it is easy to show that \u03c1 (1,2) i ,j ;l1,l2 = P T1,2,l1,l2 \u00b7 \u03c1 S1,2,i ,j , where \u03c1 S1,2,i ,j := r (1) i ,i r (2) j ,j \u00b7 \u2126 S1,i ,\u00b7 \u03a3 S1,2 \u2126 S2,\u00b7,j for i , j \u2208 {i, j}. By the property of an elliptically contoured distribution (Anderson, 2003) , it follows that\nThis completes the proof."}, {"section_title": "C.2 Proof of Proposition 3.1", "text": "Without loss of generality, we assume in our proof that \u03c9 St,i,i = 1 for i = 1, . . . , p, t = 1, 2. Let\nk,i,l , and the corresponding estimator is defined as\nk,i,l . By (2.4) in the paper, we have, uniformly in 1 \u2264 i, j \u2264 p,\n\u00b7,j,\u00b7 ), and \u03a3 (1,2) i,j = tr(P T1,2 )/q \u00b7 \u03a3 S1,2,i,j . We discuss each term in equation (C.1) separately. Note that\n2) For the first term on the right hand side of (C.2), we have that, for any M > 0, by (A1), there exists C > 0 such that\nIn addition, by the condition that \u03c3 max (\u03a3 S1,2 )|tr(P T1,2 )|/q \u2264 c 1 in (A1), it follows that\nBy (C.2), (C.3), (C.4), and Assumption (A5), it follows that\nNext, we control the bound of\n\u00b7,j ,\u00b7 )}. By (A1), max i,j \u03bd i,j is bounded. Then for any M > 0, there exists C > 0 such that\nConsequently, we have\nCombining (C.1), (C.5), (C.6), and by symmetry, it follows that, uniformly in 1 \u2264 i, j \u2264 p,\nNote that\nuniformly for 1 \u2264 i, j \u2264 p, and by Lemma B.1 that max\nBy Lemma B.1 again, we have\nThus, under (A1), we have, uniformly for 1 \u2264 i, j \u2264 p,\nEquivalently, we have max\n. This completes the proof."}, {"section_title": "C.3 Proof of Proposition 3.2", "text": "Without loss of generality, we assume in this proof that \u03c9 St,i,i = 1 for i = 1, . . . , p, t = 1, 2. The estimator of\n. We have\nIt has been shown in the proofs of Theorems 5 and 6 in Xia and Li (2017) that, uniformly for 1 \u2264 i, j \u2264 p,\nThen following similar arguments as in the proof of Proposition 3.1 and by Assumption (A6), we can show that, uniformly for 1 \u2264 i, j \u2264 p,\nNext, let\nT1,2 in (2.9) of the paper, it follows that\nwhere \u00b7 max denotes the element-wise maximum norm. Then Assumption (A7) implies that\nThus we have\nBy the assumption that {X (1) , X (2) } follows a matrix normal distribution, together with Assumption (A1), we have\nThis implies that |tr( P\n. By the arguments above, it follows that\nThus, under Assumption (A3), along with the condition that |tr(P T1,2 )tr(\u03a3 S1,2 )|/(pq) \u2265 c \u22121 1 , we have\nFinally, by Proposition 3.1, Lemma B.3, and Assumption (A1), we have\n. This completes the proof."}, {"section_title": "C.4 Proof of Theorem 3.3", "text": "This theorem can be proved by utilizing the same arguments as in the proofs of Theorems 1 and 2 in Xia and Li (2019) , with some modifications that we discuss below. Without loss of generality, we assume in this proof that \u03c9 St,i,i = 1 for t = 1, 2, i = 1, . . . , p.\nWhen \u03a3 Tt and \u03a3 T1,2 are known, for t = 1, 2, let\nAlso note that for (i, j) \u2208 H 0 \\ A \u03c4 , we have |\u03c9 St,i,j | = o{(log p) \u22121 }. Then by Lemma B.1 and Assumptions (A1) and (A2), it is easy to see that, for (i, j) \u2208 H 0 \\ A \u03c4 we have,\nLet \u03c9 S1,i,j = \u03c9 S2,i,j = \u03c9 i,j under H 0,i,j . For (i, j) \u2208 A \u03c4 , by Lemma B.1, we have\nwhere\ni,i, ) + \u03c9 i,j ( \u03c3 \nk,i,l ) 2\nk,j,l ) 2 \u2212 ( \nk,i,l ) 2 , and C is a bounded constant depending only on nq\u0398 i,j . Thus, we have P max where the last equality is a direct result of Lemma B.2. By the proof of Theorem 1 in Xia and Li (2019) , this conclusion indicates that the set A \u03c4 is negligible, and it suffices to focus on H 0 \\ A \u03c4 .\nNext, we re-define V m 's and V m 's used in Theorem 1 in Xia and Li (2019) as follows. We arrange the indices {(i, j) : (i, j) \u2208 H 0 \\ A \u03c4 } in any ordering and set them as {(i m , j m ) : m = 1, . . . , s} with with s =Card(H 0 \\ A \u03c4 ). Let \u0398 m = var(\n(1) k,im,l\nk,jm,l ), and define Z k,m,l = (\nk,jm,l \u2212\n(1) k,im,l\nk,jm,l ) \u2212 E(\nk,jm,l \u2212\n(1) k,im,l\nk,jm,l ) for 1 \u2264 k \u2264 n and 1 \u2264 l \u2264 q. Define "}, {"section_title": "D Parameter tuning", "text": "For the estimation of \u03b2 (t) i in (2.4) in Section 2.2, we propose to use Lasso. We adopt a similar tuning procedure for Lasso as developed in Xia and Li (2017) . The idea is to make the number of false rejections (i,j)\u2208H0 I(|W i,j | \u2265 h) and the estimator {2 \u2212 2\u03a6(h)}(p 2 \u2212 p)/2 close. Specifically,\nStep 1 Let \u03bb i,j , t = 1, 2.\nStep 2 Choose b as the minimizer of Step 3 The tuning parameters \u03bb (t) n,i are then set as, \u03bb\nn,i = b/20{ \u03a3 St,i,i log p/(nq)} 1/2 , t = 1, 2."}, {"section_title": "E Estimation of \u0398 i,j", "text": "To complement the simulations in Section 4.1, we report the mean squared error (MSE) of the estimator \u0398 (d) averaged over all pairs of (i, j) with i < j. That is, for each pair of (i, j), 1 \u2264 i < j \u2264 p, we first compute the MSE of \u0398\ni,j based on n replications. We then calculate i,j MSE( \u0398 (d) i,j )/ i,j \u0398 2 i,j , i.e., the average MSE over the average true \u0398 2 i,j . Table 5 reports the results. It is seen that the estimated \u0398 (d) with variance correction achieves a smaller estimation error than that without variance correction. This observation holds true across all scenarios, especially when the correlations are strong before and after the stimulus. averaged over all pairs of (i, j) with i < j. "}]