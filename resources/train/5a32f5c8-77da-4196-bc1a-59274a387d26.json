[{"section_title": "Abstract", "text": "Abstract. Computed tomography (CT) is critical for various clinical applications, e.g., radiotherapy treatment planning and also PET attenuation correction. However, CT exposes radiation during acquisition, which may cause side effects to patients. Compared to CT, magnetic resonance imaging (MRI) is much safer and does not involve any radiations. Therefore, recently, researchers are greatly motivated to estimate CT image from its corresponding MR image of the same subject for the case of radiotherapy planning. In this paper, we propose a datadriven approach to address this challenging problem. Specifically, we train a fully convolutional network to generate CT given an MR image. To better model the nonlinear relationship from MRI to CT and to produce more realistic images, we propose to use the adversarial training strategy and an image gradient difference loss function. We further apply AutoContext Model to implement a context-aware generative adversarial network. Experimental results show that our method is accurate and robust for predicting CT images from MRI images, and also outperforms three state-of-the-art methods under comparison."}, {"section_title": "Introduction", "text": "Computed Tomography (CT) imaging is widely used for both diagnostic and therapeutic purposes in various clinical applications. In the cancer radiation therapy, CT image provides Hounsfield units, which is essential for dose calculation in treatment planning. Besides, CT image is also of great importance for attenuation correction of positron emission tomography (PET) in the popular PET-CT scanner [10] .\nHowever, patients are exposed to radiation during CT imaging, which can damage normal body cells and further increase potential risks of cancer. Brenner et al. [2] reported that 0.4% of cancers were due to CT scanning performed in the past, and this rate will increase to as high as 1.5 to 2% in the future. Therefore, the use of CT scan should be done with great caution. Magnetic Resonance Imaging (MRI) on the other hand, is a safe imaging protocol which also provides more anatomical details than CT for diagnostic purposes, but unfortunately cannot be used for either dose calculation or attenuation correction. To reduce unnecessary imaging dose for patients, it is clinically desired to estimate CT images from MR images in many applications. It is technically difficult to directly estimate CT image from MR image. As shown in Fig. 1 , CT and MR images have very different appearances. MR images contain much richer texture information than CT images. Therefore, it is challenging to directly estimate a mapping from MRI to CT.\nRecently, many researches focus on estimating one modal image from another modality image, e.g., estimating CT image using MRI data. The first category of methods is image segmentation based methods. Zaidi et al. [20] developed a fuzzy clustering technique to segment MR images into different tissue classes, and then refined segmentation manually. Berker et al. [1] proposed four class tissue segmentation technique with a combined ultrashort-echo-time/Dixon MRI sequence to fulfill MRI-based attenuation correction. These methods segment MR images into different tissue classes, and then assign each class with a known attenuation property. This category of methods highly depends on the segmentation accuracy and always needs manual work to get final accurate results. The second category of methods is atlas-based methods. These methods first register an atlas (with the attenuation map) to the new subject's MR image, and then warp the corresponding attenuation map of the atlas to the new MR image as its estimated attenuation map [3] . However, the performance of these atlas-based methods highly depends on the registration accuracy. The third category is learning-based methods, in which a non-linear mapping model is learnt from MRI to CT image. Jog et al. [9] learned nonlinear regression using random forest to improve MR Resolution. Tri et al. [7] presented an approach to predict CT image from MRI using structured random forest. Since estimating MRI from CT is not a one-to-one mapping task, using only the intensity values cannot effectively distinguish the structural details. Such methods often have to first represent the input MR image by features and then map them to output the CT image. Thus, the performance of these methods is bound to the quality of the extracted features and how well they can represent the natural properties of the MR image.\nOn the other hand, recently the convolutional neural network (CNN) [11] became popular in both computer vision and medical imaging fields. As a multi-layer and fully trainable model, CNN is able to capture the complex non-linear mapping from the input space to the output space. For the case of 2D images, 2D CNN has been widely used in many applications. However, it is unreasonable to apply 2D CNN to process 3D medical images because 2D CNN considers the image appearance slice by slice, thus potentially causing discontinuous prediction results across slices. To address this issue, 3D CNNs have been proposed. Ji et al. [8] presented a 3D CNN model for action recognition in an uncontrolled environment. Tran et al. [16] used 3D CNN to effectively learn spatio-temporal features on a large-scale video dataset. Dong et al. [4] proposed a deep learning method for single image super-resolution. Li et al. [13] applied deep learning models to estimate the missing PET image from the MR image of the same subject.\nTypically an L2 distance is used as loss function with the assumption that the data is drawn from a Gaussian distribution. This can be problematic in the case of multimodal distributions and tends to produce blurry results in the output images [14] . Moreover, recently in the field of image generation, Generative Adversarial Networks (GAN) have achieved state of the art results producing very realistic images in an unsupervised setting [6] , [15] .\nIn this paper, we propose to learn the non-linear mapping from MR to CT images through a 3D fully convolutional neural network (FCN), which is a variation of the conventional CNN. Compared to CNN, FCN generates the structured output, which can better preserve the neighborhood information in the predicted CT image. This 3D FCN is used as generator in a Generative Adversarial framework where an adversarial loss term in addition to the conventional reconstruction error with the objective of producing more realistic CT data. The network is trained on patches which makes its view to be restricted to the patch itself and thus cannot provide long-range information. We use Auto-Context Model (ACM) where each stage is trained using the GAN framework for making it context-aware. To the best of our knowledge, this is the first application of the GAN framework in the field of synthetic medical image generation.\nThe proposed method is evaluated on two real CT/MR datasets. Experimental results demonstrate that our method can effectively predict CT image from MR image, and also outperforms three state-of-the-art methods under the comparison."}, {"section_title": "Methods", "text": "Deep learning models can learn a hierarchy of features, i.e., high-level features built upon low-level features. CNN [5, 11] is one popular type of deep learning models, in which trainable filters and local neighborhood pooling operations are applied in an alternating sequence starting with the raw input images. When trained with appropriate regularization, CNN can achieve superior performance on visual object recognition and image classification tasks [12] . However, most of CNNs are designed for 2D natural images. They are not well suited for medical image analysis, since most of medical images are 3D volumetric images, such as MRI, CT and PET. Additionally, the output of a conventional CNN is a single target value, which means that for a full volume, it would be necessary to compute one output for every voxel. This framework has the obvious disadvantage of requiring a large computational time, but also is unable to preserve neighborhood information in the output space. The task of image synthesis can be seen as a regression problem, where for every voxel in the input, an estimated output is required. Typically an Euclidean loss function is used during training for regression networks, which could potentially generate blurry images.\nTo address the above mentioned problems, we propose a generative adversarial network in which fully convolutional networks form the generator. First, we propose a basic 3D FCN structure to estimate the CT from MRI images. 3D operations can better model the 3D spatial information and thus could solve the discontinuity problem across slices, which can be present in 2D CNN. Second, we utilize the adversarial training strategy [6] for the designed network where an additional discriminator network can urge the generator's output to look like real CT as much as possible. We add an image gradient difference term to the loss function of the generator, with the aim of retaining the sharpness of the generated CT. Finally, we employ the Auto-Context model to iteratively refine the output of the generator.\nAt testing stage, an input MR image is first partitioned into overlapping patches. For each patch, the generator is used to predict the corresponding CT patch. Finally, all predicted CT patches are merged into a single CT image by averaging the intensities of overlapping CT regions.\nIn the following paragraphs, we will describe the framework that we use in the MRI-to-CT prediction."}, {"section_title": "Proposed Generative Adversarial Networks", "text": "Generative Adversarial Networks (GANs) have been successfully applied in the context of generative models. They were proposed in [6] , and have achieved impressive results in the context of image generation. GANs work by training two different networks: a generator network G, and a discriminator network D. G is typically an FCN which generates images G(z z z) from a random noise vector z z z, and D is a CNN which estimates the probability that an input image x is drawn from the distribution of real images; that is, it can classify an input image as real or synthetic. Both networks are trained simultaneously with D trying to correctly discriminate between real and synthetic data, while G is trying to produce realistic images that will confuse D. More formally, for D we would like to find its parameters:\nFor G we would like to optimize\nInspired by the work in [14] , where the authors use the GAN framework for video frame prediction using the input frames as input instead of a random vector, we design the GAN framework shown in Fig. 2 . Our network includes a generator which aims to estimate the CT and a discriminator which aims to distinguish the real CT from the generated one. Specifically, we minimize the binary cross entropy (bce) between the decisions of D and the correct label (real or synthetic), while the network G is trying to minimize the binary cross entropy between the decision done by D and the wrong label for the generated images, in addition to the traditional reconstruction error. In this way, D is trying to distinguish between real CT data, and the CT data generated by G. At the same time, G is trying to produce more realistic CT images such that D gets completely confused and cannot perform better than chance.\nConcretely, the loss function for D is defined as:\nwhere x is the input MRI image and\nN is the number of samples in the minibatch, y \u2208 {0, 1} represents the label of the input data (0 for the generated and 1 for the real CT), and\u0177 \u2208 [0, 1] is the estimated label by the discriminator network.\nIn the case of G, as mentioned above, we use a loss that includes an adversarial term and a reconstruction error. It is defined as:\nwhere Y is the corresponding ground-truth CT.\nSimilar to [14] , in order to deal with the inherently blurry predictions obtained from the L2 loss function (Eq.5), we propose to embed an image gradient difference loss function in the generator training. The gradient difference loss (gdl) between the generated CT and the real CT is given by:\nwhere Y is the ground truth image data, and\u0176 is the estimated by the network. This loss function tries to minimize the difference of the magnitude of the gradient between the estimated image and the ground-truth CT. In this way, the estimated data will try to keep zones with strong gradients (for example edges) for effectively compensating for the L2 term. This is approximated as finite differences during the implementation. Finally, the total loss used for training the generator G is defined as the weighted sum of all the terms as shown in Eq.7.\nThe training is performed in an alternating fashion; first, D is updated by taking a minibatch of real CT data and a mini-batch of generated (the output of G) data. Then, G is updated by using another mini-batch of samples including MRI and their corresponding CT.\nRegarding the architecture, we make use of batch normalization, and also we avoid the use of pooling layers since they reduce the spatial resolution of feature maps. Although this property is desirable for some tasks, such as image classification where the pooling over local neighborhoods can enhance the invariance to certain image distortions, it is not desired in the task of image prediction, where subtle image distortions need to be precisely captured in the prediction process. These properties agree with several works that have extended GAN, showing that some architectural constraints are necessary in order to achieve a stable training. The authors in [15] , for example, were able to obtain very realistic images by using FCN without max pooling and with batch normalization across different layers in both G and D in a way similar to what we propose.\nIn Fig. 2 , we also show the architecture of our generator network G which has the constraints mentioned above, where the numbers indicate the filter sizes. The network takes as input an MRI image, and tries to generate the corresponding CT image. It has 8 stages containing convolutions, Batch Normalization and ReLu operations with number of filters 32, 32, 32, 64, 64, 64, 32, 32, respectively. The last layer only includes 32 convolutional filters, and its output is considered as the estimated CT. Finally, the Discriminator is a typical CNN architecture including three stages of convolutions+Batch Normalization+ReLu+Max Pooling and a combination of convolution with three fully connected layers, where the first two use ReLu as activation function, and the last one uses sigmoid, whose output represents the likelihood that the input data was drawn from the distribution of real CT. The filter sizes are 5 \u00d7 5 \u00d7 5, the numbers of filters are 32, 64, 128 and 256 for the convolutional layers, and the numbers of output nodes in the fully connected layers are 512, 128 and 1."}, {"section_title": "GAN architecture", "text": "Discriminator Real Generated Generated CT 9  9 9  3  3 3  3  3 3  3  3 3  3  3 3  3  3 3  9  9 9 Conv+BN+ReLU Conv "}, {"section_title": "Auto-Context Model (ACM) for Refinement", "text": "Since our work is patch-based, the context information available for each training sample is limited inside of the patch. This affects the modeling capacity of our network. One way to enlarge the context during training is by using the Auto-Context model which is commonly used in medical image analysis applications and it has been shown to be very effective. It was first introduced by [17] , where it was used in the context of segmentation. The idea is to train several classifiers iteratively where each classifier is trained not only with the feature data, but also with the probability map obtained from the previous classifier, which gives to the classifier additional context information. At testing time, the features will be processed for each classifier one after the other, concatenating the probability map to the input features. This technique has shown to work remarkably well in the task of semantic segmentation of MRI data [19] , where Random Forest was used as classifier. In this work, we show that the ACM can be successfully applied also in regression tasks. Since the context features are extracted from the whole previously estimated CT, they can encode information that is not available using only patch based features. In our work, instead of building several classifiers, we use Generator networks, and also instead of concatenating probability maps, we concatenate the output of the previous generator network. Specifically, we train iteratively several GANs that take as input MRI patches and estimate corresponding CT patches. These patches are concatenated as a second channel in the MRI patches, and this new data is used as input during the training of the next GAN. An illustration of this scheme is shown in Fig.3 . "}, {"section_title": "Datasets", "text": "We use two datasets to test our proposed methods.\n-The brain dataset was acquired from 16 subjects with both MRI and CT scans in the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (see www. adni-info.org for details). The MR images were acquired using a Siemens Triotim scanner, with voxel size 1.2 \u00d7 1.2 \u00d7 1 mm 3 , TE 2.95 ms, TR 2300 ms, and flip angle 9 \u2022 ; The CT images, with voxel size 0.59 \u00d7 0.59 \u00d7 3 mm 3 , were acquired on a Siemens Somatom scanner. A typical example of preprocessed CT and MR images is given in Fig. 1 . -Our pelvic dataset consists of 22 subjects, each with MR and CT images. The spacings of CT and MR images are 1.172 \u00d7 1.172 \u00d7 1 mm 3 and 1 \u00d7 1 \u00d7 1 mm 3 , respectively. In the training stage, CT image is manually aligned to MR image to build the voxel-level correspondence. After alignment, CT and MR images of the same patient have the same image size and spacing. Since only pelvic regions are concerned, we further crop the aligned CT and MR images to reduce the computational burden. Finally, each preprocessed image has a size of 153\u00d7193\u00d750 and a spacing of 1 \u00d7 1 \u00d7 1 mm 3 .\nWe extracted randomly MRI patches of size 32 \u00d7 32 \u00d7 32 with corresponding CT of size 16 \u00d7 16 \u00d7 16 using the same center point as training samples. The networks were trained using the Adam optimizer with a learning rate of 10 \u22126 , \u03b2 1 = 0.5 as suggested in [15] , and mini-batch size of 10. The generator was trained using \u03bb 1 = 0.5, \u03bb 2 = \u03bb 3 = 1. The code is implemented using the TensorFlow library, and it will be publicly released upon acceptance. To demonstrate the advantage of the proposed method in terms of prediction accuracy, we compare it with three widely used approaches: an atlas-based method, a sparse representation based method, and structured random forest with autocontext model. We used our own implementation of the first two methods, while for the the structured random forest we show the results declared in [7] . And all experiments are done in a leave-one-out fashion.\n-Atlas-based method (Atlas): Here, the MR image of each atlas is first aligned [18] onto the target MR image, and the resulting deformation field is used to warp the CT image of each atlas. The final prediction is obtained by averaging all warped CT images of all atlases. -Sparse representation based method (SR): After warping the atlases to the target image space as described above, a local sparse representation is then performed. -Structured random forest and auto-context model (SRF+) based method: Besides the structured random forest, auto-context model (ACM) [17] is further used to iteratively refine the prediction of CT images.\nTo compare the performance of different methods, we utilize mean absolute error (MAE) and peak signal-to-noise ratio (PSNR) as measurements."}, {"section_title": "Impact of Proposed GAN model", "text": "To show the contribution of the proposed GAN model, we conduct comparison experiments between the traditional FCN (just the generator shown in Fig. 2 ) and the proposed GAN model. The PSNR are 24.7 and 25.9 for the traditional FCN and the proposed approach, respectively. These results do not include the ACM. We can visualize the results in Fig. 4 , where the leftmost image is the input MRI, and the rightmost image is the ground-truth CT. We can clearly see that the generated data using the GAN approach has less artifacts than the traditional FCN, by estimating an image that is closer to the desired output not only quantitatively but also qualitatively."}, {"section_title": "Impact of Auto-Context Refinement", "text": "To show the contribution of ACM, we present the performance of the proposed method with iterations of ACM in Fig. 5 . We can quantitatively observe that both MAE and PSNR are improved gradually and consistently with iterations, especially in the first two iterations. This is because ACM could solve the short-range dependency by providing long-range information. Considering the trade-off between benefit and training time, we choose 2 iterations of ACM in our experiments for both datasets. "}, {"section_title": "Experimental Results for Brain Dataset", "text": "To qualitatively compare the estimated CT by different methods, we visualize the generated CT with the ground truth in Fig. 6 . We can see that the proposed algorithm can better preserve the continuity, coalition and smoothness in the prediction results, since it uses image gradient difference constraints in the image patch as discussed in Section 2.1. Furthermore, the generated CT looks closer to the real CT compared to all others, and we argue that this is due to the adversarial training strategy which constrains the generated images to be so similar to the real ones that a even a complex discriminator cannot perform better than chance. We also quantitatively compare the predicted results in Table 1 in terms of PSNR and MAE. Our proposed method outperforms all other methods in both metrics, and it further demonstrates the advantage of our architecture."}, {"section_title": "Experimental Results for Pelvic Dataset", "text": "The prediction results by the same methods used above but on the pelvic dataset are shown in Fig. 7 . It can be clearly seen that our results are consistent with the groundtruth CT. The quantitative results based on the same metrics used in the previous dataset are shown in Table 2 .\nQuantitative results in Tables 2 show that our method outperforms the other methods in terms of both MAE and PSNR. Specifically, our method gives an average PSNR of 34.1, which is considerably higher than the 32.1 obtained by the state-of-the-art SRF+ method. "}, {"section_title": "Conclusions", "text": "We have developed a 3D GAN model for estimating CT images from MRI images by directly taking MR image patches as input and CT patches as output. The performance is improved by the use of ACM since the context of the GAN is effectively enlarged during the training process, which makes it context aware. We have applied this model to predict CT images from their corresponding MR images where the experiments demonstrate that our proposed method can significantly outperform three state-of-the-art methods, showing its suitability in regression tasks. Although we only considered the task of CT image prediction, our proposed model can also be applied to other related tasks involving a generative process in medical image analysis such as super-resolution, image denoising and so on. Fig. 7 . Visual comparison of original MR images, the estimated CT images by our method and other methods, and the ground-truth CT images on 1 typical subject on the pelvic dataset "}]