[{"section_title": "Abstract", "text": "As an important part of modern health care, medical imaging data, which can be regarded as densely sampled functional data, have been widely used for diagnosis, screening, treatment, and prognosis, such as finding breast cancer through mammograms. The aim of this paper is to propose a functional linear regression model for using functional (or imaging) predictors to predict clinical outcomes (e.g., disease status), while addressing missing clinical outcomes. We introduce an exponential tilting semiparametric model to account for the nonignorable missing data mechanism. We develop a set of estimating equations and its associated computational methods for both parameter estimation and the selection of the tuning parameters. We also propose a bootstrap resampling procedure for carrying out statistical inference. We systematically establish the asymptotic properties (e.g., consistency and convergence rate) of the estimates calculated from the proposed Statistica Sinica: Newly accepted Paper (accepted version subject to English editing) 2 T. Li, F. Xie, X. Feng, J. Ibrahim, and H. Zhu. estimating equations. Simulation studies and a real data analysis are used to illustrate the finite sample performance of the proposed methods."}, {"section_title": "Introduction", "text": "Medical imaging data, such as Magnetic Resonance Imaging (MRI), have been widely used to extract useful biomarkers that could potentially aid detection, diagnosis, assessment of prognosis, and prediction of response to treatment, among many others, since imaging data may contain important information associated with the pathophysiology of various diseases, such as breast cancer. A critical clinical question is to translate medical images into clinically useful information that can facilitate better clinical decision making (Gillies et al., 2016) .\nAddressing this clinical question requires the development of statistical models that use medical imaging data to predict clinical scalar responses. Standard functional linear model belongs to this type of statistical models (Ramsay, 2006) .\nThere is an extensive literature on the development of various estimation and prediction methods for functional linear models. See, for example, Cardot et al. (2003) , Yao et al. (2005) , Hall and Horowitz (2007) , Crambes et al. (2009) , Cai and Yuan (2012) , Crambes and Andr\u00e9 (2013) , and Hall and Giles (2015) , among many others. The aim of this paper is to propose a new functional linear ear models with nonignorable missing responses and covariates. However, these methods are limited to joint modeling of scalar predictors and scalar responses under MNAR.\nLittle has been done on joint modeling of functional predictors and missing scalar variables. Recently, Preda et al. (2010) defined the missingness of functional data and proposed a method based on nonlinear iterative partial least squares (NIPALS). Ferraty et al. (2013) studied mean estimation for the functional predictors under MAR. Chiou et al. (2014) proposed a missing value imputation and an outlier detection approach for traffic monitoring data. All these methods are limited to functional linear models for MAR and one-dimensional functional predictors.\nThe aim of this paper is to propose a new functional linear regression framework by integrating the exponential tilting model for MNAR and the standard functional linear model. We call it as ETFLR hereafter. We derive estimating equations (EEs) for ETFLR by combining the nonparametric kernel approach and the Functional Principle Component Analysis (FPCA) approach. We further derive an explicit formula for the computational solution to EEs and a method for choosing the tuning parameters. Theoretically, we investigate the consistency and convergence rate of the proposed estimates under some regularity conditions. We also propose a bootstrap procedure for carrying out statistical inference. We use simulated and real data sets to demonstrate the advantage of the proposed approach over competing methods under MCAR and MAR. Finally, we will develop companion software for ETFLR and release it to the public through http://www.nitrc.org/ and http://odin.mdacc.tmc.edu/bigs2/.\nThe rest of the paper is organized as follows. Section 2 introduces the model setting for ETFLR and presents the estimation procedure. Section 3 establishes asymptotic properties of the proposed parameter estimates. In Section 4, we apply ETFLR to investigate the predictability of brain images at baseline on learning ability scores at 18 months after baseline obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) data. Simulation results and additional results are given in the Appendix due to space limitations."}, {"section_title": "Functional Linear Regression for Missing Responses", "text": ""}, {"section_title": "Model Setup", "text": "Let (\u03b4 i , Z i , W i , Y i ), i = 1, 2, \u00b7 \u00b7 \u00b7 , n, be n independently and identically distributed realizations of the random vector (\u03b4, Z, W, Y ), where \u03b4 is an indicator, Z is a functional predictor (e.g., MRI data) belonging to a specific functional space H endowed with an inner product \u00b7, \u00b7 , W is a p \u00d7 1 random vector, and Y is a random scalar and subject to missingness. Define \u03b4 i = 1 if Y i is observed and \u03b4 i = 0 if Y i is missing for i = 1, . . . , n. It is assumed that \u03b4 i and \u03b4 j are independent for any i = j, and \u03b4 i only depends on Z i , W i and\nas the i\u2212th observation. For notational simplicity, we focus on one-dimensional functional data throughout the paper. Without loss of generality, we assume\nFor identification, it is assumed that the Z i 's are centered such that E(Z) = 0 (Crambles and Andr\u00e9, 2013).\nOur ETFLR consists of a functional linear model and an exponential tilting semiparametric model for the propensity score as follows:\nwhere \u03c0(Z, W, Y ) Pr(\u03b4 = 1|Z, W, Y ) is called the propensity score, \u03c6 \u2208 R is an unknown parameter that determines the amount of departure from the ignorability of the response mechanism, \u03b8(\u00b7) \u2208 H is an unknown functional coefficient function, and \u03b2 1 \u2208 R p is a p \u00d7 1 vector of unknown coefficients. Moreover, G \u2208 G is a nonparametric function, where G = {all continuous functions H \u00d7 R p \u2192 R}.\nTo include an intercept in (2.1), the first element of W i is set to be 1.\nThe inclusion of G(Z, W ) in the propensity score represents a major extension of the so-called exponential tilting (ET) model proposed by Kim and Yu (2011) . Such assumption is quite reasonable, since patients with severe or weak disease symptoms are more likely to be missing and imaging data may be strongly \nFor method development, we introduce some operators as follows:\nfor any u(\u00b7) \u2208 H. Moreover, due to the Hilbert-Schmidt theorem, it is commonly assumed that \u0393 and\u0393 n have the sequences of eigenvalues {\u03bb j } j\u22651 and {\u03bb j } j\u22651 , with corresponding sequences of eigenfunctions {v j (\u00b7)} j\u22651 and {v j (\u00b7)} j\u22651\nrespectively. Such a condition has been widely used in the FPCA literature."}, {"section_title": "Estimation Method", "text": ""}, {"section_title": "Estimating Equations", "text": "First, we consider the case when all responses are fully observed. In this case, parameter estimation is equivalent to solving a least squares (LS) problem.\nStatistica Sinica: Newly accepted Paper (accepted version subject to English editing)\nFor ETFLR, we minimize the following objective function given by\nUsing FPCA, we can estimate\u03b8 by minimizing (2.3) with respect to \u03b8 over the linear span of {v 1 (\u00b7), \u00b7 \u00b7 \u00b7 ,v kn (\u00b7)}, where k n is a positive integer. Therefore, by setting \u03b8 = kn j=1 r jvj ,\u03b8 can be solved by minimizing\nwith respect to r = (r 1 , \u00b7 \u00b7 \u00b7 , r kn ) T . Furthermore, it follows from the HilbertSchmidt theorem that we have\nFinally, minimizing (2.3) is equivalent to solving the following estimating equation (EE) given by\nSecond, we consider the case when some responses are missing not at random.\nWe define \u03b3 = \u2212\u03c6 and for j = 1, . . . , k n ,\nThen, (2.4) is equivalent to\nThe law of large numbers ensures that the expectation of the left side of (2.5) converges to zero as k n \u2192 \u221e, but this EE depends on missing data. By following the reasoning in Tang, et.al. (2014) , we have\nshares the same expectation with\nFinally, we propose to solve the following equation:\nwhere for any f (\u00b7), i and \u03b3, m 0\nTo calculate the estimating equation (2.6), we need to know both \u03c6 and k n and then approximate the conditional expectations in m 0 \u03c8,i,\u03b3 . In the following, we will discuss how to calculate them. We introduce a kernel function K(\u00b7) and\n, where h is a bandwidth. We us\u00ea\nas a nonparametric estimate of m 0 \u03c8,i,\u03b3 (\u00b7 \u00b7 \u00b7 ), where\nThe notation v is used to denote the l 2 norm of a vector v or the L 2 norm of a function v(\u00b7). Moreover, w 0 is a scalar introduced to balance the functional and nonfunctional parts of D l (\u00b7, \u00b7)."}, {"section_title": "Computational Method", "text": "We develop a computational method for our proposed estimating equation\n1 n be an n\u00d71 vector of ones, and \u039e = (w i,j ) with w i,j = w i,0 (Z j , W j ; \u03b3). We then discretize the observed function Z i to a fine grid of K equally spaced values t k that span the interval [0, 1] (Ramsay and Silverman 2006) . Denote the K equallyspaced discrete points by t = (t 0 = 0, t 1 , \u00b7 \u00b7 \u00b7 , t K = 1), and then we approximate the inner product Z, \u03b8 by\n, and \u03b8 =V kn r such that 8) where \u03a3 = D + diag \u039e(I n \u2212 D)1 n . If \u03c6 and k n , w 0 and h are given, the solution to (2.8) has an explicit form given by"}, {"section_title": "Selection of Smoothing and Tilting Parameters", "text": "When \u03c6 is given, similar to Crambes and Henchiri (2015) , the smoothing tuning parameters can be achieved by using the generalized cross-validation (GCV) criterion given by\nwhere\n\u2022' denotes the element-wise product. To select h and w 0 , we generate L random divisions and denote T as the test set of the \u2212th random division for\nSee the detailed algorithm in Section 4. Then, we use Repeated Random Sub-sampling Validation (RRSV) by minimizing Following Kim and Yu (2011) , we use an external validation sample, a followup subset of nonrespondents, chosen for further investigation to retrieve missing responses. We propose two approaches as follows. The first approach is the Missing Not At Random for Nonparametric (MNARN) method. In this approach, the validation sample is assumed to be randomly selected. Similar to Kim and Yu (2011) , \u03c6 = \u2212\u03b3 could be determined by the following estimating equation 12) where m 0 e,i,\u03b3 (Y i ) is defined in (2.7) for the identity function e(y) = y and \u03b4 * i = 1 if the ith subject belongs to the follow-up sample and 0 otherwise. It is easy Statistica Sinica: Newly accepted Paper (accepted version subject to English editing) to show that the expectation of the left-hand side of (2.12) is equal to zero.\nSpecifically, it follows from m 0\nComputationally, we approximate m 0\nThe second approach is the Missing Not At Random for Parametric (MNARP) method. In this approach, if G is specified to be a linear function of Z and W\n\u2208 H and \u03b2 2 \u2208 R p , then we estimate \u03c6 by maximizing the likelihood function of the logistic model (2.2) given by\nwith respect to (\u03c6, \u03b2 2 , s j : j = 1, . . . , k * n ). The tuning parameter k * n denotes the number of eigenfunctions used for estimating \u03c6. When the validation set is not large, we also add a penalty term, such as the LASSO, to the likelihood function.\nFor a fixed k * n , this optimization procedure can be directly implemented by the 'glmnet' R package (Friedman et al., 2009 ). The optimal k * n can be further determined by minimizing its corresponding cross-validation error."}, {"section_title": "Theoretical Results", "text": "To facilitate the theoretical development, some assumptions are needed.\nPrior to presenting assumptions, we list some notation. True values of \u03b2 1 , \u03b2 2 , and \u03b8(\u00b7) are denoted by \u03b2 1,0 , \u03b2 2,0 , and \u03b8 0 (\u00b7), respectively. Let a \u22972 = a T a for any vector or matrix a and \u03a3 = tr(\u03a3 T \u03a3) be the Frobenius norm of a matrix\n, and\nSolving (2.6) is equivalent to solving U (\u03b2 1 ) = 0, where U (\u03b2 1 ) is given by\nThen, we have the following theorem, whose assumptions and proofs can be found in the supplementary document.\nTheorem 1. Suppose Assumptions (A.1)\u2212(A.9) hold. Then, as n \u2192 \u221e, there exists a unique solution\u03b2 1 of U (\u03b2 1 ) = 0, which converges to \u03b2 1,0 in probability,\nMoreover, we have\nStatistica Sinica: Newly accepted Paper (accepted version subject to English editing) \nIn Assumption (A.7), k n \u2192 \u221e and k 5a+3 n n \u22121 \u2192 0 ensure that both the bias and variance of\u03b8 asymptotically converge to 0. For the selection of k n , a small k n may incur substantial information loss and cause bias, whereas a large k n can increase variance due to insufficient number of observations.\n. . , n+N 0 } as a test set with N 0 new observations. For i \u2264 n + N 0 , by using\u0177 i = Z i ,\u03b8 +\u03b2 T 1 W as the predicted response of the ith observation, the squared prediction error can be bounded by\nIn this case, we can obtain the optimal convergence rate O p (n (a+2\u22122b)/(4a+1+2b) + 1/ \u221a N 0 ) by minimizing k a+2\u22122b n + k 5a+3 n /n, which leads to k n = O(n 1/(4a+1+2b) ) Statistica Sinica: Newly accepted Paper (accepted version subject to English editing) and \u03b8 \u2212 \u03b8 0 = O(n (a/2+1\u2212b)/(4a+2b+1) ). Although these convergence rates are slower than those in Tang, et.al. (2014) and Hall and Horowitz (2007) , our ETFLR is much more complex due to the inclusion of G(Z, W ) in model (2.2).\nSecond, we consider some approximations to the terms in (2.6) discussed in Subsection 2.2.1 and then solve\u0168 (\u03b2 1 ) = 0, where\u0168 (\u03b2 1 ) is given by\nin whichr j is equal to\nSome additional assumptions (B.1)\u2212(B.6) are listed in the Appendix.\nTheorem 2. Suppose that Assumptions (A.1)\u2212(A.9) and (B.1)\u2212(B.6) hold.\nThen, as n \u2192 \u221e, there exists a unique solution \u03b2 1 of\u0168 (\u03b2 1 ) = 0, which converges to \u03b2 1,0 in probability and \u03b8 = kn j=1r j (\u03b2 1 )v j satisfies \u03b8 \u2212 \u03b8 0 L 2 \u2192 0 in probability, wherer j (\u03b2 1 ) is equal to\nMoreover, we have Finally, we present a theoretical result that justifies the computational method in Subsection 2.2.2.\nTheorem 3. If the tuning parameters h, w 0 , and k n and the tilting parameter \u03c6 are fixed, and for any f 1 and f 2 \u2208 H, f 1 , f 2 is defined as\n, then the solution to (2.6) is equal to the minimizer that minimizes (2.8)."}, {"section_title": "Application to the ADNI dataset", "text": "Alzheimer's disease (AD) is the most common form of dementia and is an escalating national epidemic and a genetically complex, progressive, and fatal neurodegenetive disease. The ADNI study is a large scale multi-site study which has collected clinical, imaging, and laboratory data at multiple time points from cognitively normal controls (CN), individuals with significant memory concern (SMC), early mild cognitive impairment (EMCI), late mild cognitive impairment (LMCI), and subjects with AD. One of the goals of ADNI is to develop prediction methods to predict the longitudinal course of clinical outcomes (e.g., learning ability) based on imaging and biomarker data. More information about data acquisition can be found at the ADNI website (www.loni.usc.edu/ADNI).\nTo illustrate the empirical utility of our proposed methods in imaging classification, we use a subset of the ADNI data which consists of 682 subjects (208 CN controls, 153 AD patients, and 321 LMCI patients), after removing missing or low quality imaging data. Among them, there are 395 males with average age 75.38 years old and standard deviation 6.48 years old, and 287 females with average age 74.81 years old and standard deviation 6.81 years old. The T1-weighted images for all subjects at baseline were preprocessed by standard steps including AC (anterior commissure) and PC (posterior commissure) correction, N2 bias field correction, skull-stripping, intensity inhomogeneity correction, cerebellum removal, segmentation, and registration (Wang et al., 2011) . Afterwards, we generated RAVENS-maps (Davatzikos et al., 2001 ) for the whole brain using the deformation field obtained during registration. We obtained the 256\u00d7256\u00d7256 RAVENS-maps and then down-sampled them to 128\u00d7128\u00d7128 for real data analysis.\nThe development of ETFLR is motivated by using imaging and clinical vari- Test Score) at the 6, 12, 18, 24, and 30 months after baseline. The missingness rates of the test scores at the 18th, 24th and 30th month are very high, e.g., the missingness rate at the 18th month is 53.1%. We are interested in examining the learning ability at the 18th month, at which all LMCI and AD patients were tested for learning ability. We have 682 individuals in total, among which 362 individuals have missing data. It is necessary to model the missing responses given the high missingness rate.\nWe applied ETFLR to this ADNI data set as follows. First, to determine the tilting parameter \u03c6, we obtained a validation set by investigating the responses at months other than the 18th month for those observations with missing responses.\nWe interpolated the responses at the 18th month by using those at other months by a linear regression, and then we calculated the p\u2212value associated with month.\nThe interpolations with p-values less than .05 were considered as the validation set, and their corresponding interpolated responses were approximately taken as the missing true responses. By using both MNARN and MNARP (which are named in Subsection 2.2.3), we calculated two estimates of \u03c6. Second, given\u03c6, the first k n components (columns) of the functional covariate Z, together with the non-functional covariates W (age, gender, etc.), we calculated the estimates of all coefficients by optimizing the quadratic form (2.8). We used GCV to choose k n as in (2.10).\nThird, we used RRSV to choose the optimal (h, w 0 ) as in (2.11). Given a grid of (h, w 0 ) values, we used the average prediction on the test set to choose the optimal (h, w 0 ). The criterion used is the Pearson correlation between the true and predicted responses. Specifically, we divided the dataset into half the test set and half the training set 500 times randomly, ensuring that both the missingness rate and the proportion of the validation set between the training set and the test set are the same. At each division, for every given h and w 0 , we calculated, Cor tr and Cor test for all approaches, where Cor tr is the correlation between the predicted responses and the true responses on the training dataset, and Cor test is the correlation between the predicted responses and the true responses on the test dataset. After 500 divisions, we calculated their averages in comparison with MCAR and MAR approaches. See Table 1 for such results. We found that both MNARP and MNARN outperform in almost all tuning parameters (h, w 0 )s', and the best (h, w 0 ) is achieved at (1.1 3 h min , 0.5) for MNARN.\nWe also examine whether the functional covariate leads to better prediction.\nSpecifically, by setting \u03b8 0 = 0, we repeated the same estimation procedure to calculate (Cor test , Cor tr ) at (1.1 3 h min , 0.5), leading to Cor tr = 0.257(0.054) and\nCor test = 0.127(0.059). Comparing such results with those in Table 1 reveals that RAVEN images can substantially improve prediction accuracy.\nAfter fixing h = 1.1 3 h min and w 0 = 0.5, we calculated the estimates of the non-functional covariates in Table 2 . The bootstrap resampling procedure was further utilized for inference. Specifically, we repeated the bootstrap resampling procedure 300 times. At each time, we calculated the parameter estimates and \u03c6. Subsequently, we calculated the 90% confidence intervals for \u03c6 and all other parameters and their associated p\u2212values by using the Fast Double Bootstrap (Davidson and James, 2007) . Table 2 also presents the bootstrap confidence intervals and their corresponding p\u2212values based on MNARN. Table 3 We have the following findings. First, MNAR performs well in both training and test sets for most window widths h and k n . Second, the four covariates, Education, Apoe4, whether the individual is divorced, and whether the DX-bl of the individual is the LMCI (= 1) or the AD (= 0), strongly influence the learning test score. Such findings are clinically significant in that AD has a more serious effect on the intelligence behavior than LMCI. Third, the negative value of\u03c6 in "}]