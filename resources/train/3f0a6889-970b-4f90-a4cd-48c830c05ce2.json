[{"section_title": "Abstract", "text": "The article considers algorithms and methods for the classification of neurodegenerative diseases, in particular Alzheimer's disease and dementia. Stages are considered for carrying out such studies, including the search for raw materials, preprocessing and processing of MRI images."}, {"section_title": "Introduction", "text": "Information technologies are presented in all areas of life, expanding opportunities and providing new tools. Medicine is no exception, as the technical complexity of the equipment used is also constantly growing. The quality and volume of information obtained from modern medical equipment makes possible statistical and other types of research based on stored data.\nOne of the most universal and fast tomographic methods for studying the human body is magnetic resonance imaging. It is widely used for the studies of brain, cerebral and neck vessels, temporomandibular joints, eye orbit, paranasal sinuses and oropharynx, soft tissues of the neck, spine, spinal cord, osteoarticular system, abdominal cavity organs and abdominal space, small organs Pelvis, chest, heart, arteries and veins, tumors and metastases.\nAt the moment, pictures taken by magnetic resonance imaging are stored in various digital formats. There are many software products with proprietary and open source code that allow specialists to view and process such images. But software products that provide the ability to conduct automated diagnostics are very few and they have many limitations.\nAlzheimer's disease is the most common form of dementia. Among all reported cases of dementia Alzheimer's disease is 60-80%. Moreover, among people who have reached the age of 65, about 5% suffer from this ailment. And among people older than 85 years, the diagnosis of \"Alzheimer's disease\" is already 30% [1] . Alzheimer's disease belongs to diseases that impose the heaviest financial burden on society in developed countries [2] . At the moment the disease is incurable. The body of patients in the end, in any case, will lose most of its functions, which will lead to death. The newest medicines make it possible to alleviate the symptoms and to postpone the moment of complete erasure of the patient's personality. The quality of treatment depends on the stage of the disease, which was diagnosed.\nAll images taken with MRI are currently being processed by software, in general, not including automatic diagnostic tools. The doctor, when studying a picture of a patient, may miss the initial stage of Alzheimer's disease, if specifically does not focus on this. The presence of an effective algorithm that gives a probabilistic assessment of the possibility of a neurodegenerative disease would allow one to notice Alzheimer's disease at an early stage and begin treatment, thereby prolonging the patient's full life. Magnetic resonance imaging (MRI) is a method of obtaining tomographic medical images for the study of internal organs and tissues using the phenomenon of nuclear magnetic resonance. The method is based on measuring the electromagnetic response of atomic nuclei, most often the nuclei of hydrogen atoms, namely, their excitation by a certain combination of electromagnetic waves in a constant magnetic field of high tension.\nAfter the research, a special file is created that contains information about the patient, research, and information for drawing the image. In fact, each file is a slice of some part of the body in any plane. The physical meaning of each pixel is the intensity of the return signal received by the scanner (simplified, tissue density of the body). The diagnostic station produces not one file, but several for one study. These files have a logical structure. Files are combined in a series and represent a set of consecutive sections of an organ. The series are combined in a stage. The stage determines the entire study. The sequence of the series in the stage is determined by the research protocol.\nTo visualize the data contained in the image, you need to compare the density of the texture and the color. Various transfer functions are used for this. Transfer functions are divided by type into absolute and relative ones. The absolute transfer function is constructed for all possible densities. MR-tomograph for each series generates its own set of densities. That is, for two series, the same density can correspond to different tissues of the body. Relative transfer function is built on the basis of the so-called window, which indicates which particular range of densities to draw.\nSince the set of sections for MR-tomography can be represented as three-dimensional data, the concept of a voxel is introduced. Voxel is an element of a 3D image containing the value of an element in a three-dimensional space. As a voxel value, color can generally be used, but density is often used. As for the voxel shape, in general, voxels can be cubic, or a parallelepiped [3] ."}, {"section_title": "Databases of MRI images", "text": "There are several databases with pictures of MRI of the brain. They are designed for research in the field of automated diagnosis of various diseases. Databases storing pictures of patients with Alzheimer's disease will be considered."}, {"section_title": "The BRAINnet Database", "text": "Brain Resource Ltd. Provides processed data from the Brain Resource International database available to BRAINnet for independent scientific use, freely and without restrictions on publication. The international database of brain resources is the largest accessible library of information on human brain health, obtained using standardized measures, several sources of data are available for the same individuals.\nThe database contains about 5000 pictures of healthy people, as well as about 1000 pictures of patients with various diseases, including Alzheimer's disease [4] ."}, {"section_title": "The Open Access Series of Imaging Studies (OASIS)", "text": "The Open Access Research Series (OASIS) is a project aimed at making the MRI data set available to the scientific community. By compiling and freely distributing MRI data sets, the project is aimed at future discoveries in basic and clinical neurology. OASIS is provided by the Research Center for Alzheimer's Disease Research in Washington, DC, by Dr. Randy Buckner at the Howard Hughes Medical Institute (HHMI) at Harvard University, the NRG Research Group at the University of Washington School of Medicine, and the BIRN [5] .\nContains two sets of data:\n\u2022 Cross-sectional MRI Data in Young, Middle Aged, Nondemented and Demented Older Adults\n\u2022 Longitudinal MRI Data in Nondemented and Demented Older Adults"}, {"section_title": "OpenfMRI", "text": "The OpenfMRI database is a repository of human brain imaging data collected using MRI and EEG techniques. The data is collected from 2010. Initially, the project included datasets of functional MR imaging, but subsequently became open to all forms of neuroimaging data that included MRI data. The success of the platform can at least partly be explained by the simplicity of the organization of data and the absence of any obstacles to accessing the data. To obtain data that is distributed by default using the Public Domain license, no registration or license agreement is required [6] ."}, {"section_title": "Alzheimer's Disease Neuroimaging Initiative (ADNI)", "text": "ADNI is a continuous, multifaceted study designed to develop various data for the early detection and tracking of Alzheimer's disease (AD). It is divided into two main study periods (ADNI1 in ADNI2). The study aimed to enroll 400 subjects with early mild cognitive impairment (MCI), 200 subjects with early AD, and 200 normal control subjects [7] ."}, {"section_title": "ROI extraction", "text": "Since the processing power is limited in order to be able to process such a large amount of data, and to facilitate the task of training neural networks, the ROI extraction prestep is using. There are two common used methods for ROI extraction. "}, {"section_title": "Methods are based on dictionary learning", "text": "Learning dictionaries is a learning method, the purpose of which is to search for a divided representation of input data as a linear combination of basic elements, as well as the basic elements themselves. These elements form the dictionary, and no requirement to contain them orthogonal, they can also be redundant. That makes the input signals to represent a larger dimension that the specifically observed signal. This qualities lead to the presence of redundant elements that provide improved flexibility of component separation and presentation. At the same time, they allow multiple representations of the same signal.\nThis method requires that the dictionary for learning should be composed of input data. The use of the dictionary learning method was due to the fact that signal processing usually requires the presentation of input data, in which a large number of different components are involved. Prior to this approach, the general practice was to use predefined dictionaries (such as the Fourier transform or the Wavelet transform). However, in some cases, a dictionary that is learned to customize input data can significantly improve component separation, which has value in decomposing, compressing, and analyzing data [8] [9]."}, {"section_title": "K-Mean method", "text": "The k-means method is the most popular method of clustering. He was invented in the 1950s by the mathematician Hugo Steinghaus and almost simultaneously by Stuart Lloyd. Particularly popular after McQueen's work. The action of the algorithm is such that it tends to minimize the total quadratic deviation of cluster points from the centers of these clusters: The algorithm is a version of the EM algorithm, which is also used to separate the Gaussian mixture. It splits the set of elements of the vector space into a known number of clusters k.\nThe basic idea is that at each iteration the center of mass is recalculated for each cluster obtained in the previous step, then the vectors are divided into clusters again according to which of the new centers is closer to the selected metric.\nThe algorithm is completed when, at some iteration, there is no change in the center of mass of the clusters. This happens for a finite number of iterations, since the number of possible partitions of a finite set is finite, and at each step the total quadratic deviation of V does not increase, so cycling is impossible [10] . "}, {"section_title": "Classification Methods Based on Neural Networks", "text": ""}, {"section_title": "A bag of visual words (BoVW) and the method of reference vectors", "text": "There is a study in which an approach combining several algorithms for the classification of MRI images is applied [11] . The scheme of the framework is shown in the figure 3. The method begins with the normalization of the image of the brain. Then the areas of interest (the hippocampus and the back waist crook) are extracted from normalized images, described by local visual descriptors, and processed within BoVW [12] . After decreasing the dimension, the resulting descriptors are classified using SVM.\nA bag of words is a method originally created for the analysis of texts. In fact, a bag of words is a collection of word pairs -the number of its appearances in the text. In the case of images, everything is the same, with the only difference that instead of words, averaged fragments of images are used.\nThe support vector machine (SVM) is a set of similar learning algorithms with the teacher used for classification tasks and regression analysis. Belongs to the family of linear classifiers. A special property of the support vector method is a continuous decrease in the empirical classification error and an increase in the gap, so the method is also known as the classifier method with the maximum gap. The main idea of the method is the translation of the initial vectors into a space of higher dimension and the search for a separating hyperplane with the maximum gap in this space. Two parallel hyperplanes are constructed on both sides of the hyperplane that separates the classes. The separating hyperplane is a hyperplane that maximizes the distance to two parallel hyperplanes. The algorithm works under the assumption that the greater the difference or the distance between these parallel hyperplanes, the smaller will be the average classifier error [13] .\nEach data object is represented as a vector (point) in p-dimensional space (an ordered set of p numbers). Each of these points belongs to only one of the two classes. The question is whether it is possible to separate points by a hyperplane of dimension p \u2212 1. This is a typical case of linear separability. The desired hyperplanes can be many, so it is believed that maximizing the gap between classes contributes to a more confident classification. That is, is it possible to find such a hyperplane so that the distance from it to the nearest point is maximal. If such a hyperplane exists, it is called the optimal separating hyperplane, and the corresponding linear classifier is called the optimally separating classifier.\nThe points have the form:\nWhere c(i) takes the value 1 or -1, depending on which class the point x(i) belongs to. Each x(i) is a p-dimensional real vector, usually normalized by the values [0,1] or [-1,1] .\nIf the points are not normalized, the point with large deviations from the average coordinates of the points will affect the classifier too much. We can treat this as a learning collection, in which the class to which it belongs is already assigned for each element. We want the algorithm of the support vector method to classify them in the same way. To do this, we construct a separating hyperplane that looks like this:\nThe vector w is the perpendicular to the separating hyperplane. If the parameter b is zero, the hyperplane passes through the origin, which limits the solution. Since we are interested in the optimal separation, we are interested in support vectors and hyperplanes parallel to the optimal and closest to the supporting vectors of two classes. If the training sample is linearly separable, then we can choose hyperplanes in such a way that no points of the training sample lie between them and then maximize the distance between the hyperplanes. The width of the strip between them is easy to find from considerations of geometry, so our task is to minimize ||w||."}, {"section_title": "Convolution artificial neural networks", "text": "The neural net (CNN) -a special architecture of artificial neural networks, is part of the technology of in-depth training. Uses some features of the visual cortex, in which so-called simple cells reacting to straight lines from different angles were discovered, and complex cells whose reaction is associated with the activation of a certain set of simple cells. Thus, the idea of convolutional neural networks is the interleaving of convolutional layers and subsampling layers. The network structure is unidirectional (without feedbacks), essentially multilayered. For training, standard methods are used, most often the method of back propagation of the error. The function of activation of neurons (transfer function) is any, at the choice of the researcher. The operation of a convolutional neural network is usually interpreted as a transition from specific image features to more abstract details, and further to even more abstract details, up to highlighting high-level concepts. At the same time, the network is self-tuning and develops the necessary hierarchy of abstract attributes (sequences of feature cards), filtering unimportant details and highlighting the essential [14] . "}, {"section_title": "Conclusion", "text": "Modern methods allow them to be used for processing MRI images for the purpose of early diagnosis of brain diseases. It is necessary to develop and improve technologies that could do this automatically in the framework of other brain research. Such technology will allow to increase the statistical probability of finding diseases which require early diagnosis for effective treatment, such as Alzheimer's disease. It also allows in the long term to conduct preliminary diagnosis of diseases and detection of the patient's entry into risk groups without direct medical involvement on the basis of a combination of factors derived from various diagnostic devices and the already collected medical history."}]