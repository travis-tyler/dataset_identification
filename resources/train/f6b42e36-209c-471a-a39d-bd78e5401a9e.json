[{"section_title": "Abstract", "text": "Disease progression modeling (DPM) analyzes patients' electronic medical records (EMR) to predict the health state of patients, which facilitates accurate prognosis, early detection and treatment of chronic diseases. However, EMR are irregular because patients visit hospital irregularly based on the need of treatment. For each visit, they are typically given different diagnoses, prescribed various medications and lab tests. Consequently, EMR exhibit irregularity at the feature level. To handle this issue, we propose a model based on the Gated Recurrent Unit by decaying the effect of previous records using fine-grained feature-level time span information, and learn the decaying parameters for different features to take into account their different behaviours like decaying speeds under irregularity. Extensive experimental results in both an Alzheimer's disease dataset and a chronic kidney disease dataset demonstrate that our proposed model of capturing feature-level irregularity can effectively improve the accuracy of DPM."}, {"section_title": "", "text": ": A patient's several consecutive electronic medical records, describing visit-level irregularity (top) and featurelevel irregularity (bottom) respectively. chronic diseases are likely to worsen with more severe comorbidities and complications. As a consequence, patients may suffer from poor health and bear high healthcare cost. To address this issue, disease progression modeling (DPM) [22] has been proposed, which employs computational methods to model the progression of a target disease. By predicting the health state of patients accurately, DPM facilitates early detection and treatment of chronic diseases before deterioration. Moreover, DPM not only helps patients to keep good health state and save money, but also eases the allocation for limited healthcare resources.\nDPM typically exploits electronic medical records (EMR) to analyze the state of patients. EMR data 1 records patients' visits to hospital with heterogeneous features, including diagnoses, lab tests, prescribed medications, procedures, doctor notes, and image data (e.g., MRI images) together with some demographic information.\nOne major challenge of DPM over EMR data is on handling the irregularity issue of the time series EMR data, as existing approaches mainly focus on non-time series data (e.g., [27] ) or regular time series data (e.g., [19] ). There are two levels of irregularities in the EMR data, namely visit-level and feature-level. Visit-level irregularity indicates that EMR data appears irregularly with time. This is due to the fact that patients visit hospital irregularly (mainly determined by their actual physical conditions). Taking Figure 1 as an example, this patient visits hospital four times and the time spans between consecutive visits vary a lot, i.e., nine days, three months and six months respectively. The features appearing in the four visits include diagnoses chronic kidney disease (CKD), diabetes mellitus (DM), acute kidney failure (AKF) and lab tests glucose (GLU), haemoglobin A1c (HbA1c). Feature-level irregularity indicates that the same feature appears irregularly in EMR data with time, which is caused by two factors. First, visit-level irregularity has a direct effect on feature-level irregularity. As shown in Figure 1 , for feature CKD and DM, they appear at every visit, however, the visits are irregular, which leads to irregular features. Second, patients do not receive the same set of lab tests and diagnoses at every visit. Hence, features could be missing in the records of some visits. For instance, in Figure 1 , AKF only appears at the second and the last visit. Feature-level irregularity reduces to visit-level irregularity when all features appear at every visit.\nMost of the existing work either ignores the visit-level irregularity by partitioning EMR data into regular time windows and aggregating the data within each window (e.g., [19] ), or handles the visit-level irregularity by simply adding the time span as one feature in the feature vector [7, 9] . Pham et al. [25] propose to consider the decaying influence of the time span -the longer the time span is, the less influence of the history poses on the current state. However, they only consider the visit-level irregularity and ignore the more fine-grained feature-level irregularity.\nWe argue that handling the irregularity issue at the feature level rather than the visit level can further improve the accuracy of DPM together with many other EMR data analytic tasks. There are two reasons. First, more fine-grained time span information is captured at the feature level, whereas the visit-level time span is not accurate for some features. Second, different features tend to expose different decaying behaviours, where some features may decay faster than others after the same time lapse. Hence, there is a need to handle various features differently.\nWe have the following contributions to highlight:\n\u2022 We identify the irregularity in EMR data and justify that this characteristic should be captured at the feature level for both reducing time information loss and modeling various features' decaying behaviours, such as decaying speeds.\n\u2022 We propose a new model to make use of the fine-grained feature-level irregular time span information based on the standard Gated Recurrent Unit [6] . Our model differentiates various features through learning their decaying parameters. Furthermore, our proposed model focuses on processing irregular input EMR data, hence is applicable to other EMR data analytic tasks.\n\u2022 We evaluate the effectiveness of our proposed Feature-Level Time Decay Model for DPM in two datasets. Through comparison with several deep learning-based methods, especially the Visit-Level Time Decay Model, and some advanced DPM methods, we demonstrate that our proposal of capturing feature-level irregularity in EMR data can achieve higher prediction accuracy.\nThe rest of the paper is organized as follows. In Section 2, we review existing related works. We formulate our problem in Section 3 and then describe our methodology of capturing feature-level irregularity in detail in Section 4. Section 5 contains our experimental evaluation in both a public EMR dataset and a private one from the local hospital. Finally, we conclude in Section 6."}, {"section_title": "RELATED WORK", "text": "There are three categories of related work for handling the issue of data irregularity in EMR data analytics. Approaches from the first category aggregate the time series data (i.e., records of all visits) of each patient into a single record and thus avoid the issue. Approaches of the second category transform the problem into a regular time series problem by partitioning the visits into regular time windows and aggregating the records within each time window. Approaches of the third category read the records of all visits directly without transformation. Some techniques are applied to handle the visit-level irregularity. However, notice that all existing methods fail to handle the feature-level irregularity."}, {"section_title": "Converting to Non-Time Series Data", "text": "Some works make use of patients' baseline features (i.e., features collected when patients visit hospital for the first time) or the average of patients' features aggregated over time. Among them, some works employ statistical regression models to discover the correlation between patients' features and patients' disease severity [11, 12, 26, 27, 29] . These statistical regression methods enjoy the advantages of simple computation and modeling.\nAnother line of research [23, 31, 32] is to employ multi-task learning [2] . In these works, patients' medical features collected during their first visit to hospital are utilized to predict their disease severity at multiple future time points. The prediction at each future time point is a regression task. Multi-task learning captures the relationship between multiple tasks to improve the performance of all tasks. However, some discriminative information in the historical records would be missed as they only use the record from the first visit.\nOne limitation in using EMR data as non-time series data might be the under-utilization of time series EMR data. For example, we cannot differentiate the importance of the features from different historical visits if the simple average operation is used to aggregate the features."}, {"section_title": "Transforming into Regular Time Series Data", "text": "After transforming the irregular time series data into regular time windows, traditional approaches for time series data can be applied. Some researchers employ dynamic Bayesian networks (DBN) or some variant graphical models [13, 28, 30] . Due to DBN's directed graph structure, it enjoys the advantages in causality and interpretability. However, in order to construct a DBN model, medical experts need to determine the causal structures in a certain medical domain in the model. This process may be time-consuming and if the exact interactions between some variables are not clear, it is difficult to determine the corresponding causal relationships. Moreover, this model construction may reduce the chance for finding some hidden variables associated with patients' disease progression Session 8D: Health Analytics 2/ Top-k CIKM'17, November 6-10, 2017, Singapore as well as some interesting, hidden causal relationships between variables.\nRecently, deep learning models [18] have been applied for regular time series data. Deep learning has attracted huge interests from both academia and industry because of its dramatically better performance in many areas for feature learning, including image classification, object detection and speech recognition. Recurrent neural networks (RNN) are designed for processing sequential data. Long short-term memory (LSTM) [17] and gated recurrent unit (GRU) [6] are two specific RNN models which have drawn much attention and achieved state-of-the-art performance in many applications.\nDeep learning methods have also been employed in a wide range of EMR data analytic tasks [21] , including ICU mortality prediction [4, 20] , diagnosis [19] , and interpretable phenotype learning in clinical decision making [5] . These works partition the irregular EMR data into regular time windows (non-overlapping [5, 19, 20] or overlapping [4] ). These methods may not be able to capture feature patterns within a short time (i.e., within a window) as they miss the fine-grained visit-level information."}, {"section_title": "Without Transforming Data", "text": "Choi et al. propose a couple of approaches for DPM without transforming the irregular time series data. In their \"GRAM\" model [8] , they employ a graph-based attention model, and input EMR data of patients' visits in chronological order without considering the intrinsic irregularity. Their focus is to incorporate domain knowledge in the form of a DAG. In their \"DoctorAI\" model [7] , they employ a RNN model and solve the irregularity by concatenating the time span between consecutive visits in the inputs. Similarly, in their \"RETAIN\" model [9] , they use a reverse time attention mechanism for providing precise interpretability. They utilize irregular EMR data by concatenating the visits' timestamps in the inputs. 2 In [3] , Che et al. impute the missing data of patients' records for utilization and solve the irregularity in EMR data through dynamic time warping to learn the similarity between temporal sequences. Then the proposed dynamic temporal matching technique can help the personalized prediction of Parkinson's disease.\nIn [25] , Pham et al. propose to use the time span as a decay term to modify the standard LSTM model for the DPM problem. Their proposed decay term is a visit-level decay term which loses featurewise time span information and does not distinguish between various features. On the contrary, our proposed model captures featurelevel irregularity with more fine-grained time information. At the same time, our method differentiates various features' decaying behaviours over time."}, {"section_title": "PRELIMINARIES", "text": "In this paper, we target at predicting a patient's future health state (or disease severity) in order to model his/her disease progression trajectory. The training data consists of a set of labeled samples, denoted as {< x, y, \u2206t >}, where x is a list of feature vectors of a patient extracted from his/her historical visits (one vector per visit), and y is the severity score after a period of time \u2206t since the last visit.\nThe feature vector of x consists of two types of features, namely medical features and demographic features. Medical features typically include diagnoses such as DM, CKD, AKF, lab tests such as HbA1C and GLU, medications like insulin and procedures like dialysis. They appear irregularly since patients visit hospital irregularly and different lab tests or medications are given to them for each visit. For a patient, we denote the medical feature vector at time t as (x\nwhere D is the total number of available medical features. The granularity of t could be a day, a week or a month. Each element could be a real number for the result of a lab test or a binary value for the existence of a diagnosis. Demographic features include height, gender, education (time) etc, which change slowly over time. Therefore, we assume the demographic features are constant in this paper. For a patient, his/her demographic feature vector is denoted as (\u0434 1 , . . . , \u0434 S ) T , where S is the number of unique demographic features. We collect the features mentioned above to construct our EMR data, which is formalized as follows: Definition 1. (EMR Data R) R is the union of all patients' EMR data. For each patient p, his/her EMR data is a list of feature vectors ordered by time. By concatenating the medical features and the demographic features, we have the feature vector of a patient at time t as\n, . . . , D}) denotes the value of the d-th medical feature. If the d-th medical feature is not available at time t (e.g., the lab test not conducted), then\nThe severity score y is typically derived from the results of some lab tests. We consult doctors to select the specific lab tests for each disease. For example, the value of Mini-Mental State Examination (MMSE) test is a good indicator for the severity of an Alzheimer's disease patient [24] and the value of Glomerular Filtration Rate (GFR) test represents the severity of a CKD patient [1] . For each patient in our training dataset, we select a cut point time to separate all visits into two parts. The early visits are used to construct x and the recent visits are used to derive y. The formal definition of the cut point is as follows. Definition 2. (CutPoint t \u03c8 ) Suppose patient p visits hospital at a set of time points {t 1 , t 2 , . . . , t n }. We define a CutPoint t \u03c8 such that the features at {t 1 , . . . , t \u03c8 } are used to construct x, and the features at {t \u03c8 +1 , . . . , t n } are used to derive y. Consequently, multiple training samples are constructed from this patient, i.e., {< x, y k , t \u03c8 +k \u2212 t \u03c8 >} for each k \u2208 {1, 2, . . . , n \u2212 \u03c8 } where the patient has the lab test (i.e., severity score) y at time point t \u03c8 +k .\nWith the description of the input (x, \u2206t) and the output y above, now we define DPM formally as follows. Definition 3. (Disease Progression Modeling (DPM)) Given a set of training samples {< x, y, \u2206t >}, the objective of DPM is to obtain a mapping function \u03a6 that minimizes the following loss function over all samples:\nThe loss function L(\u00b7, \u00b7) measures the discrepancy of the predicted severity score and the ground truth score. In our experiments, the squared \u2113 2 norm is used. After training, when we deploy the Feature vector of a patient at time t, GRU's current input at time t t \u03c8 CutPoint of a patient\nFeature-level irregularity information of a patient at time t h (t ) GRU's hidden state at time t y (n)\nModel's output prediction for a patient \u2206t Time between CutPoint and prediction time point y Labeled severity score of a patient model for new patients, the CutPoint is always the time of his/her last visit. In other words, we use historical information to construct x, and use x to predict y. We can compute a patient's severity at different future time points by changing the input value of \u2206t. In this way, we model this patient's disease progression trajectory and we define this process as DPM. We illustrate the process of DPM above in Figure 2 . Notations used in this paper are summarized in Table 1 ."}, {"section_title": "METHODOLOGY", "text": "In this section, we first describe how to apply the standard GRU model for the DPM problem and then introduce our proposal for handling the issue of feature-level irregularity in EMR data."}, {"section_title": "Standard GRU for DPM Problem", "text": "GRU is a popular model that performs well in capturing long-term patterns from time series data [10] . To apply the standard GRU for EMR data analytics, researchers have to either partition irregular time series into regular time windows [19] or concatenate the time span between consecutive inputs as an input feature [7, 9] as described in Section 2 3 . After the data transformation, the records (i.e., x) of each patient are fed into GRU as regular time series data. The detailed neural network structure is illustrated in Figure 3 and the computation is formalized as follows. First, we compute the reset gate r (t ) following Equation 2, which does affine transformation of the current input x (t ) and the previous hidden state h (t \u22121) , and then applies si\u0434moid(\u00b7) activation function.\nSecond, we compute the temporary hidden state h (t ) as shown in Equation 4 . We can see that the reset gate plays an important role in computing the new hidden h (t ) . If r (t ) is near zero, then most information from the previous hidden state will be discarded, and the new hidden state is computed mainly based on the current input x (t ) . The hyperbolic tangent function tanh(\u00b7) involved is defined in Equation 5 . \"\u2299\" represents element-wise multiplication.\ntanh(a) = e a \u2212 e \u2212a e a + e \u2212a\nThird, we compute the update gate z (t ) (Equation 6) to balance the importance between the temporary hidden state h (t ) and the previous hidden state h (t \u22121) for calculating the new hidden state (Equation 7).\nThe formulas discussed above is for one GRU at time t, where t \u2208 {1, 2, . . . , n} and n is the number of records for a sample. The hidden state of the final GRU is fed into the Prediction Module shown in Figure 3 to predict the severity score.\nSome existing approaches assume the future time points are with equal time spans [23, 31, 32] . Hence, their methods cannot predict patients' severity at any future time point. Some other existing approaches ignore the time span between prediction and current input (predict patients' state in the next visit to hospital regardless of the time span) [8, 9] . Therefore, their methods may lose important time information for the prediction. In order to predict a patient's severity in any future time, e.g. in a time period of \u2206t, we include \u2206t as an input argument to our Prediction Module. As a result, our DPM model is more flexible in that we manage to predict patients' future disease progression in any future time without constraints. In particular, we combine h (n) and \u2206t through affine transformation (\nThen, we apply a linear regression in the Prediction Module to obtain the predicted score y (n) in Equation 10 where q is a vector, and \"\u00b7\" denotes the dot product. \u03b5 is a \"noise term\", capturing other influencing factors other than c (n) .\nBased on the EMR data we can access, we model this prediction task as a regression problem which gives fine-grained severity prediction. Our proposed method is extendable to classification if we use a softmax function and a cross entropy loss function. Based on this, for diseases of which the severity is divided into classes, given specific medical endpoints, we can also perform classification tasks to predict changes of disease stages which are also interesting from the clinical perspective.\nThe discrepancy between y (n) and the ground truth y is evaluated by squared \u2113 2 norm as shown in Equation 11. Equation 11 is thus our training objective or loss function. Back-propagation algorithm is applied to update the model parameters, i.e., \u03b8 = {W r , U r , W , U ,"}, {"section_title": "Feature-Level Time Decay Model", "text": "The standard GRU model ignores the time span between each two consecutive time points. In other words, it assumes the time points of each sample are regularly spanned, which applies for certain applications like language modeling. However, EMR data is irregular. Consequently, we propose a new gate in GRU which controls the decaying scale of the hidden state by considering the time span of each input feature. In this way, fine-grained feature-level irregularity is captured in the adapted GRU.\nIn our model, the input of each visit includes the time span of each feature. For a visit at time t, we denote each feature d's time span from its nearest previous occurrence as \u03b4 4 is the time from the second visit to the fourth visit, which is five weeks; \u03b4 (6) 5 is the time between the third visit and the fourth visit, which is three weeks. The difference between \u03b4 4 In Figure 4 , we choose x D as the label feature for illustration purpose. Each appearance of x D after CutPoint gives one y for prediction. For the first visit of each patient, \u03b4\nis set as 0 for \u2200d \u2208 {1, . . . , D}.\nAll time spans at time t construct a vector \u03c4 (t ) = (\u03b4\nT , which is the feature-level time span information and is fed into the GRU in parallel with x (t ) as shown in Figure 5 .\nThe time span information is incorporated in our GRU to control the decaying weight of previous hidden state. In particular, our adapted GRU (illustrated in Figure 5 ) computes a decay term \u03f1 according to \u03c4 (t ) and multiplies \u03f1 to the update gate z (t ) (Equation 13) which trades off between the previous hidden state and the temporary hidden state for computing the current hidden state. The formula for \u03f1 is shown in Equation 12 . We employ the backpropagation algorithm for updating \u03b8 = \u03b8 {W \u03c4 , b \u03c4 }.\nThere are three principles in designing the new GRU: We choose tanh activation function in our decay term \u03f1 for its expressive power. As the value range of tanh function is (\u22121, +1), \u03f1's corresponding value range is (0, 2). In this case, when \u03f1 = 1, our proposed adapted GRU model turns to the standard GRU which does not capture feature-level irregularity. When \u03f1 < 1, we can forget more on the history by assigning less weight to the previous hidden state. When \u03f1 > 1, we give more weight to the previous hidden state to remember more of the history. We do not choose other common activation functions like si\u0434moid(\u00b7) or ReLU (\u00b7) due to their value ranges. For si\u0434moid(\u00b7) activation function, its range is (0, 1) which corresponds to \u03f1 \u2208 (0, 1) meaning that we always need to forget more on the history. For ReLU (\u00b7) activation function, its range is [0, +\u221e) which causes \u03f1 \u2208 (\u2212\u221e, 1]. Both cases are not suitable in our situation. Therefore, our adapted GRU with tanh function has enough expressive power to capture feature-level irregularity in EMR data. \u2022 Robustness The model should be robust to feature missing.\nIf patient p's feature d is missing at time t, we set \u03b4 is set to 0, this imputed feature value will not influence the learning of parameters W \u03c4 and b \u03c4 . Consequently, missing features will not affect the learning of features' behaviours under irregularity."}, {"section_title": "EVALUATION 5.1 Experimental Set-Up", "text": ""}, {"section_title": "Datasets.", "text": "We evaluate our proposed model using a public Alzheimer's disease dataset from Alzheimer's Disease Neuroimaging Initiative (ADNI) 5 , and a chronic kidney dataset from National University Hospital (NUH) in Singapore.\nADNI dataset is a public dataset whose EMR data includes proteomic information, lab tests, as well as some demographic features. All features of the ADNI dataset are divided into five categories as shown in Table 2 . We request all patients' data from ADNI1, which is a six-year study launched in 2004.\nWe denote the time of each patient's first visit as M00 and the visit time after M00 as Mxx where xx is the number of months since the first visit. For instance, M06 indicates six months after the first visit. We use such EMR data as the input feature vector of the machine learning models. All models are evaluated based on their performance in predicting the severity scores of patients in the future. The severity is measured by MMSE test (\u2208 [0, 30]), which is widely used for Alzheimer's disease patients [12, 24, 27] . The lower the MMSE is, the more severe the patient is. Using MMSE score, patients can be categorized into normal/elderly (MMSE=30), mild 5 http://adni.loni.usc.edu/ NUH dataset is a real-world EMR dataset recording medical features including diagnoses, lab tests, medications, procedures, case notes, and demographic features for more than 100, 000 patients admitted to the hospital in the year of 2012. We choose patients with Stage 3 CKD or higher as our cohort [1] and we denote it as the \"NUH-CKD\" dataset. All models are evaluated based on their performance in predicting patients' future GFR (\u2208 [0, 60]), which is a lab test indicating the severity of CKD patients. The lower this GFR value is, the more severe the patient is. According to the consultation with doctors and The Renal Association 6 , the loss of GFR score over a year is of vital importance to CKD patient management. If a patient loses GFR more than 5ml/min/1.73m 2 over a year, his/her health state is slowly deteriorating, and he/she needs assessment from specialists. In our experiments, we extract the first year data for each patient as input data to predict his/her future GFR scores. We summarize the statistics of the ADNI1 dataset and the NUH-CKD dataset in Table 3 . To be specific, we show the number of features (both medical and demographic ones), the number of patients and the time span for two datasets. Features within a short time period are aggregated into a single record for processing. The time period is six months for the ADNI dataset and one week for the NUH-CKD dataset. Notice that this aggregation is different from the window-based approach, which uses the records from empty windows whereas we remove empty windows. Consequently, their input data is regular and our input data is irregular. Moreover, we use multiple CutPoint settings in our experiments. The corresponding number of samples for each CutPoint setting is included in the table. The CutPoint setting is a trade-off between the sample length and number of samples. When the CutPoint is set larger (further 6 http://www.renal.org/information-resources/the-uk-eckd-guide/deterioratingfunction Session 8D: Health Analytics 2/ Top-k CIKM'17, November 6-10, 2017, Singapore from each patient's first visit), the sample has a larger length, i.e., more records, but the number of samples is reduced."}, {"section_title": "Evaluation Metrics.", "text": "Considering the task of patient severity prediction as a regression problem, we choose the following two performance metrics for all models, namely mean squared error (MSE), and Pearson product-moment correlation coefficient (R) value.\nSuppose the predicted severity value is y (n) and the ground truth value is y (\u0434t ) for patient p, then MSE can be computed by\nWe also compute R value to measure the linear relationship between predicted values and ground truth values.\nwhere\u0233 (n) is the average of all predicted values and\u0233 (\u0434t ) is the average of all ground truth values. This R value with the range [\u22121, 1] measures the linear relationship between two variables, so R > 0 means they are positively correlated and R < 0 means they are negatively correlated.\nTo sum up, a good regression model should have a small MSE value but a large R value."}, {"section_title": "Baseline Methods.", "text": "We denote our proposed model in Section 4.2 as Feature-Level Time Decay Model. It is proposed based on GRU. Hence, we first compare our model with three GRU-based baselines.\n\u2022 Window-Based Model partitions input time series (i.e., EMR data) into regular time windows and aggregates the records within each window. This method does not consider the data irregularity. [19] employs this method for diagnosis.\n\u2022 Visit-Level Model handles the visit-level irregularity by concatenating the time span between consecutive irregular visits as a dimension of the input feature vector [7, 9] .\n\u2022 Visit-Level Time Decay Model decays the influence of previous visits (i.e., records) on the current hidden state according to the time span between consecutive irregular visits [25] .\nTo obtain a more thorough analysis, we compare our proposed method with several advanced disease progression modeling methods proposed in [31] . [31] employs a multi-task learning (MTL) method, which uses the EMR data from a single visit to predict patients' severity of multiple future visits (one per task).\n\u2022 Least Convex Fused Group Lasso (cFSGL) [31] includes three penalty terms in the objective function: (i) Lasso penalty which can select a specific set of features for each task, (ii) Group Lasso penalty which can offer a common feature selection for all tasks and (iii) Fused Lasso penalty which incorporates the temporal smoothness. According to [31] , setting the parameter for the Group Lasso penalty term as 100 achieves the best results in all their settings. Hence, we also choose the parameter 100 and denote this method as cFSGL when reporting experimental results.\n\u2022 Least Non-Convex Fused Group Lasso (nFSGL) [31] is a non-convex progression model proposed to solve the problem in which convex models may be sub-optimal. Zhou et al.\npropose two formulations of such non-convex models and we denote them as nFSGL-1 and nFSGL-2 in our experiments. For all experiments, we partition all prediction samples into 80% (training data), 10% (validation data) and 10% (testing data) randomly."}, {"section_title": "Experimental Setting.", "text": "When carrying out experiments, we need to set the CutPoint defined in Section 3. For GRU-based models, we use all input before (inclusive) CutPoint as input data. For MTL-based methods, we use the nearest historical visit before the CutPoint as the input data. Then we define the \"task\" in MTL according to the time span between the input and the prediction (output) visit. For instance, in the ADNI dataset, Task1 predicts the severity for the visit that is six months later from the input visit. We compute the performance by a weighted-average over all tasks, with the weight being the portion of samples of each task. 7 We use the glorot uniform initializer [15] for initializing the parameters in all GRU-based models and choose the RMSProp optimizer [16] (with a weight decay of 0.9) which is usually good for recurrent neural networks. For training models, we choose learning rates 0.001, 0.002 and 0.004 dependent on different datasets or different CutPoint settings for the same dataset."}, {"section_title": "Experimental Results in the ADNI Dataset", "text": "In the ADNI dataset, we test the performance of all models by varying the CutPoint shown in Figure 6 . The nearer CutPoint is to M00 (the first visit), the smaller the input length is for GRU-based models, while the larger the number of prediction samples are.\nFor MSE results, our findings are as follows. For the same CutPoint setting, from Window-Based Model to our proposed FeatureLevel Time Decay Model, MSE value is mainly on the descending trend, which indicates that (i) handling the irregularity of EMR data is beneficial for DPM (Visit-Level Model achieves smaller MSE than Window-Based Model); (ii) modeling time span's decaying effect on Furthermore, when setting the same CutPoint, our proposed Feature-Level Time Decay Model can give more accurate predictions than MTL-based methods. However, not all GRU-based models can outperform MTL-based methods. When CutPoint is moved further from M00 (from M12 to M18, further to M24), MSE values of GRU-based models decrease as these methods manage to utilize more time series inputs, but the performance of MTL-based methods does not exhibit much difference. As a consequence, the advantage of GRU-based models over MTL-based methods becomes larger when CutPoint changes from M12 to M18 and then M24.\nAmong MTL-based methods, we observe that nFSGL-1 and nFSGL-2 obtain smaller MSE values than cFSGL, indicating that non-convex progression models can provide more accurate predictions than convex progression models. This finding agrees with the experimental results of [31] .\nFor R value results shown in Figure 6 , we can have similar observations as in MSE results. From Window-Based Model to FeatureLevel Time Decay Model, the general increasing trend of R values indicates that the performance is mainly improving. We also find that all GRU-based models tend to have higher R values than MTLbased methods. One reason is that GRU-based models exploit more time series information than MTL-based methods."}, {"section_title": "Experimental Results in the NUH-CKD Dataset", "text": "We report the experimental study in the NUH-CKD dataset using three different CutPoint settings -W16, W24, W32. The performance of all models is illustrated in Figure 7 . When looking at the MSE metric, in most cases, GRU-based models are ordered as Window-Based Model, Visit-Level Model, VisitLevel Time Decay Model and Feature-Level Time Decay Model, from larger MSE to smaller MSE. Similar to Section 5.2, such observations indicate that (i) fine-grained irregular visit-level time span benefits prediction accuracy; (ii) visit-level time span has a decaying influence in prediction; (iii) more fine-grained feature-level time span can further improve the prediction performance of DPM. For each CutPoint setting, GRU-based models outperform MTL-based methods. The reason could be the lack of sufficient samples for each task of MTL-based methods in the dataset. Session 8D: Health Analytics 2/ Top-k CIKM'17, November 6-10, 2017, Singapore\nWhen varying the CutPoint from W16 to W24, we find that GRU-based models tend to achieve larger MSE values, hence, the performance degrades. This may be caused by the decreasing number of samples from 3601 in W16 to 2793 in W24 shown in Table 3 . A small sample number poses a negative effect on the prediction performance as the model is likely to overfit without enough training samples. However, when further increasing the CutPoint from W24 to W32, MSE values of GRU-based models gets smaller. The reason may be when the CutPoint is W32, GRU-based models manage to utilize more time series features despite decreasing sample number from 2793 in W24 to 1585 in W32. In a nutshell, both the sample length and sample size affect the model performance.\nFor R values, in the same CutPoint, we find GRU-based models exhibit similar performance as in MSE values. Furthermore, GRUbased models can achieve higher R values than MTL-based methods. The reason may be two-fold: (i) GRU-based models manage to make full use of all time series data as input; (ii) MTL-based methods are affected by the small sample number for each task."}, {"section_title": "Case Study of DPM", "text": "To give more insight of DPM, we choose three example CKD patients corresponding to three representative disease progression trajectories from the NUH-CKD dataset and plot the prediction results of our proposed model in Figure 8 . To be specific, we use our proposed Feature-Level Time Decay Model trained with the CutPoint set to W32, which achieves smallest MSE value in all GRUbased models among all CutPoint settings (shown in Figure 7) . We use this model to predict the three patients' future severity in terms of GFR values after CutPoint. Note that a lower GFR value corresponds to a more severe state. In Figure 8 , the green triangles before the 32nd week represent patients' GFR values fed to our model as input. The orange ones denote the ground truth GFR values in the future between the 32nd and the 52nd week. As the data collected only covers one year for each patient, we have no access to patients' GFR values after the 52nd week. The blue curve is the predicted severity progression from our proposed model, which can extend to future time points after the 52nd week. These three patients correspond to three representative progression types demonstrated as follows.\nSevere and deteriorating progression Patient1's GFR values first keep decreasing in the first 32 weeks and drop to around 35. Then from the 32nd week, the GFR ground truth values remain in the descending trend, and our proposed model manages to predict this trend. Furthermore, our proposed model predicts that as time further goes on, the loss of Patient1's GFR values will exceed 5ml/min/1.73m 2 within one year. Our model would suggest Patient1 to consult specialists for expert assessment according to medical reference.\nMild yet deteriorating progression In the beginning, Patient2's state is mild as his/her GFR values indicate only moderately reduced kidney function. However, the GFR value decreases slowly over time before the 52nd week predicted by our model 8 . After the 52nd week, our model predicts that Patient2 will suffer from a minor 8 The ground truth value fluctuates without an obvious trend for this patient. drop in GFR value, indicating that the functioning of his/her kidney might slightly deteriorate. With this information in mind, healthcare workers may need to provide more aggressive interventions to Patient2 in advance according to his/her current condition.\nSevere yet stable progression Patient3 is already in CKD Stage5 [1] when his/her data is recorded. Through the whole year, this patient progresses stably without much change in GFR value, and our model gives the prediction that this stableness will maintain for a long time.\nFrom the analysis above, we can see that our proposed model provides accurate prediction for patients with different progression patterns. With a good DPM model, we can provide healthcare workers with the accurate disease progression trajectory of each patient. Based on the trajectory, they can give helpful advice to patients, take effective interventions beforehand and finally, have better disease management on patients. For instance, healthcare Session 8D: Health Analytics 2/ Top-k CIKM'17, November 6-10, 2017, Singapore workers may give more aggressive treatment to Patient1 and Patient2 in our previous example, and guarantee the monitoring for Patient3."}, {"section_title": "CONCLUSION", "text": "In this paper, we identify the irregularity characteristic residing in EMR data both at the visit level and at the feature level. We argue that capturing feature-level irregularity can benefit the performance of EMR data analytics based on: (i) the current state of a patient is inferred from his/her previous state and the current observations (i.e., lab tests, diagnoses, etc.); (ii) the influence of the previous state decays through time, and the decay term is related to the time lapse; (iii) different features have different effects on the decaying behaviours/speeds. We propose a model to handle feature-level irregularity based on GRU. It decays the influence of previous information on patients' current state and learns decaying parameters for different features. We use DPM as a representative EMR data analytic task for evaluating our proposal. We conduct experiments in both a public ADNI dataset and a private NUH-CKD dataset for two chronic disease cohorts. In the experiments, we compare with several GRU-based models and some advanced DPM methods -MTL-based methods. Experimental results on comparison with GRU-based models demonstrate that our proposal of capturing feature-level irregularity manages to improve the prediction accuracy of DPM task effectively."}]