[{"section_title": "Abstract", "text": "Abstract. There is growing body of research devoted to designing imaging-based biomarkers that identify Alzheimer's disease (AD) in its prodromal stage using statistical machine learning methods. Recently several authors investigated how clinical trials for AD can be made more efficient (i.e., smaller sample size) using predictive measures from such classification methods. In this paper, we explain why predictive measures given by such SVM type objectives may be less than ideal for use in the setting described above. We give a solution based on a novel deep learning model, randomized denoising autoencoders (rDA), which regresses on training labels y while also accounting for the variance, a property which is very useful for clinical trial design. Our results give strong improvements in sample size estimates over strategies based on multi-kernel learning. Also, rDA predictions appear to more accurately correlate to stages of disease. Separately, our formulation empirically shows how deep architectures can be applied in the large d, small n regime -the default situation in medical imaging. This result is of independent interest."}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD) affects over 20 million people worldwide, and in the last decade, efforts to identify biomarkers for AD have intensified. There is now broad consensus that the disease pathology manifests in the brain images years before the onset of AD. Various groups have adapted sophisticated machine learning methods, to learn patterns of pathology by classifying healthy controls from AD subjects. The success of these methods (which obtain over 90% accuracy [16] ) has led to attempts at more fine grained classification tasks, such as separating controls from Mild Cognitively impaired (MCI) subjects and even identifying which MCI subjects will go on to develop AD [14, 7] . Even in this difficult setting, multiple current methods have reported over 75% accuracy. While accurate classifiers are certainly desirable, one may ask if they address a real practical need -if no treatments for AD are currently available, is AD diagnosis meaningful? To this end, [9, 6] showed the utility of statistical learning methods beyond diagnosis/prognosis; they can in fact be leveraged for designing efficient clinical trials for AD. The basic strategy here uses imaging data from two time points (i.e., TBM data or hippocampus volume change), and derives a machine learning based biomarker. Based on this measure, the top one-third quantile subjects may be selected to be included in the trial. Using this \"enriched\" cohort, the drug effect can then be detected with higher statistical power with far fewer subjects, making the trial more cost effective and far easier to setup/conduct.\nIn this work, we ask if machine learning models can play a more fundamental role. Consider a trial where participants are randomly assigned to treatment (intervened) and placebo (non-intervened) groups, and the goal is to quantify any drug effect. Traditionally, this effect is quantified based on a \"primary\" outcome, like cognitive measure or brain atrophy. If the distributions of this outcome for the two groups are statistically different, we conclude that the drug is effective. When the effects are subtle, the number of subjects required to see statistically meaningful differences can be huge, making the trial infeasible. Instead, one may derive a \"customized outcome\" (i.e., a continuous predictor) from a statistical machine learning model. Here, the system assigns predictions based on probabilities of class membership (no enrichment is used). If these customized predictions are statistically separated (classification is a special case), it directly implies that potential improvements in power and the efficiency of the trial are possible. This paper is focused on designing specialized learning architectures towards this final objective. In principle, any machine learning method should be appropriate for the above task. But it turns out that high statistical power in these experiments is not merely a function of the classification accuracy of the model, rather the conditional entropy of the outputs (prediction variables) from the classifier at test time. An increase in classifier accuracy does not directly reduce the variance in the predictor (from the learnt estimator). Therefore, SVM type methods are applicable, but significant improvements are possible by deriving a learning model with the concurrent goals of classifying the stages of dementia as well as ensuring small conditional entropy of the outcomes.\nOur contributions. We achieve these goals by proposing a novel learning model based on deep learning. Deep architectures are non-parametric learning models [1, 3] that have received much interest in machine learning and computer vision recently. Although powerful, it is well known that they require very large amounts of unsupervised data, which is infeasible in neuroimaging, where the dimensionality d of the data is always much larger than the number of instances (n). A na\u00efve use of off-the-shelf deep learning models on neuroimaging data expectedly yields poor performance. In the last few months, however, independent of our work, deep learning methods have been used successfully in structural and functional neuroimaging [12, 5, 10] . To get around the difficulty highlighted above, [12] uses a region of interest approach whereas [5, 10] sub-samples each data instance to increase n. Our work provides a mechanism where no such adjustments are necessary. The key contributions of this paper include (a) Scalable deep architecture(s) for learning problems in neuroimaging where number of data instances is much smaller than the data dimensionality (i.e., our models permit whole-brain analysis) and (b) An imaging derived continuous measure with smaller variance that leads to efficient AD clinical trials with moderate sample sizes (and based only on one time-point data)."}, {"section_title": "Model", "text": ""}, {"section_title": "Stacked Denoising Autoencoders (SDA)", "text": "We motivate our formulation by highlighting the difficulty in using stacked denoising autoencoders (SDA) [1, 3] \nwhere \u03b3(.) is the point-wise stochastic corruption [1] . A stacked denoising autoencoder (SDA) greedily concatenates L(> 1) DAs, i.e., l th layer outputs are the un-corrupted inputs for (l + 1) th layer,\nwhere \u03b8 denotes the full set of stochastic gradient (SG) learning parameters (corruption rate, learning rate, hidden layer length). The transformations\nserve as a warm-start for supervised tuning where one compares the output of the L th layer to {y i } n 1 . This greedy layer-wise unsupervised training followed by supervised fitting is central to most deep architectures [3, 1] .\nRecall that SG learning is expected to converge to a local minimum only in the asymptotic setting (of large n). Hence, the warm-start described above is only reliable when large amounts of unsupervised data are available, which is the case in computer vision but not in neuroimaging. Otherwise, the network overfits whenever d is much larger than n. In neuroimaging, d is generally on the order of millions (number of voxels) and n < 1000. Hence, traditional SDAs cannot be directly used (they will generalize poorly). Recent work uses deep architectures in neuroimaging either by reducing d (using anatomical ROIs or feature selective) or increased n (splitting a data instance using sets of 2D slices) [12, 5, 10] . Nonetheless, frameworks to perform whole-brain analysis (the de-facto input when SVMs are used for brain image classification) will yield improvements by exploiting 3D local neighborhood dependencies directly."}, {"section_title": "Randomized Denoising Autoencoders (rDA)", "text": "In Section 1, we motivated the task of concurrently optimizing two goals. Our system should be able to capture differences across different dementia stages (i.e., controls, MCI, AD) while at the same time keeping intra-stage prediction variance as small as possible. In other words, we seek to decrease the prediction variance at no cost of approximation bias. Although, these seem like competing requirements, it turns out that this behavior is exactly what is offered by ensemble learning [2] . Recall that Ensembles are bootstrap randomizations around sets of weak learners which reduce the prediction variance in expectation. So, properly incorporating an ensemble approach within a deep architecture should yield the behavior we expect. We can generate the ensembles for a given learner in multiple ways [2] -a randomization over the number of features and/or data instances. Here, we already have n d, so randomization over n is infeasible. Instead, we distribute/randomize over the dimensions d where each weak learner will correspond to a SDA. This randomization allows a single SDA weak learner to process pathologically correlated voxels across 3D local neighborhoods while still operating on the whole-brain image. Unlike the SVM objective which has a global optimum, SDAs can only converge to a local optimum via SG [1] . We compensate for this by including a second level of randomization that samples sets of hyperparameters from a given hyper-parameter space. This basic structure drives the performance of our randomized denoising autoencoder (rDA).\nLet V = {1, \u00b7 \u00b7 \u00b7 , d} denote the indices of dimensions/voxels, and \u03c4 (v), a distribution over v \u2208 V. In the simplest case, this can be a uniform distribution. We generate a bootstrap sample of B \"blocks\" where each block corresponds to input data along s b dimensions/voxels (length of the block, fixed a priori). The mapping between voxels and blocks is given by \u03c4 (v). Note that blocks may not be mutually exclusive (a voxel may belong to multiple blocks). Each block will be presented to T weak learners. Each of these weak learners correspond to a unique \u03b8 t \u2208 \u0398 for t = 1, . . . , T where \u0398 is the given hyper-parameter space. This means that each sample from the hyper-parameter space yields a weak learner. \nB,T 1,1 , the weighted regression pooling gives\nwhere U are the regression coefficients and \u03bb is the regularization constant. z is the known weight vector on B \u00d7 T estimated L th layer outputs."}, {"section_title": "[[\u00b7]] denotes column-wise concatenation. The prediction for a new test input x i\u015d", "text": "The simplest choice for the block-wise sampler \u03c4 (v) (i.e., at b = 1) assigns uniform probability over all dimensions/voxels. However, we can assign large weights on local neighborhoods which are more sensitive to dementia progression, if desired. Since d is large, we modify \u03c4 (v) after each iteration (Reweigh step in Alg. 2.2) to prevent starvation of the previously unsampled dimensions. We can also setup \u03c4 (\u00b7) based on entropy or the result of a hypothesis test. Each weak learner output h L b,t is an estimate of y. Recall that SG learning is sensitive to the choice of hyper-parameters \u03b8 t , particularly, the number of epochs and gradient learning rate influences the range and variance of these estimates [1, 3] . Hence, randomization over \u03b8 t mitigates this dependency by averaging over T such estimates for each block (i.e., set of dimensions/voxels). We can pool the block estimates via various means -average, using a ridge regression or other sophisticated schemes."}, {"section_title": "Algorithm. rDA Blocks training", "text": "But since SDAs are already capable of learning complex concepts [1] , we use a simple linear combination with 2 -loss providing minimum mean squared error. This addresses our goal of reducing the stochastic error of final predictions. Observe that rDA extends easily to multimodal inputs by first constructing individual blocks for each modality and pooling across all modalities.\nThe sigmoid non-linearity ensures that rDA outputs are in \u2208 [0, 1]. By labeling healthy controls as 1 and AD subjects as 0, we can then project the dementia scale to [0, 1] . The pooled outputs, referred to as rDA measure (rDAm), are then imaging-derived continuous predictors. We can then compute the sample sizes using rDAm as a customized outcome [11] . Denoting the mean change of rDA for placebo and treatment groups by \u03bc p and \u03bc t respectively, the number of subjects per arm is given by 2(\n2 where 1 \u2212 \u03b2 is the desired power at significance level \u03b1. Using a conversion rate of \u03c1 \u2208 [0, 1] from MCI to AD, and inducing a drug effect of \u03b7 (i.e., the treatment decreases the mean change by a fraction \u03b7), the sample size expression then simplifies to 2c 2 where c = \u03c3/\u03bc is the coefficient of variation. Since we only use one time-point data, the proportion \u03c1 is set based on information from previously reported studies [13] Since rDA is an ensemble designed to reduce the prediction variance (and hence c), we hope to see much smaller sample sizes compared to others."}, {"section_title": "Evaluations", "text": ""}, {"section_title": "Data and Setup", "text": "We used Amyloid, FDG-PET and MRI data at baseline for 447 subjects (210 male, 237 female) from ADNI2 (Alzheimer's Disease Neuroimaging Initiative). 131 were healthy (CN), 92 were demented (AD), 120 and 104 had early and late MCI (EMCI and LMCI) respectively. The labeling of EMCI and LMCI (done by ADNI) is based on the cognitive status of each subject. Of the 224 MCIs, 100 had maternal family history (FH) of AD, 52 had paternal and 23 had both. Preprocessing included extracting grey matter in normalized space, and correcting PET for average intensities in ponsvermis (FDG) and cerebellum (Amyloid). We train rDA on ADs (labeled 0) and CNs (labeled 1) alone, and test on MCIs. We use a multi-modal (MKL) -support vector regression ( MKm) as the baseline learning model [7] . Firstly, we evaluate if rDAm differentiates EMCI from LMCI. Additionally, we evaluated parental family history as a contributing risk factor. Since rDAm is a continuous marker, its correlations with CSF levels -\u03c4 , p\u03c4 , A\u03b2, \u03c4/A\u03b2 and p\u03c4 /A\u03b2 (\u03c4 : \u03c4 -protein, p\u03c4 : phospho \u03c4 -protein, A\u03b2: Amyloid-\u03b2, are the cerebrospinal fluid protein levels, and very sensitive biomarkers of AD) are also assessed to verify if it is meaningful. We then estimate sample sizes using MCI to AD conversion rate of 37.7% [13] . rDA hyper-parameters in our experiments are L = 2, B = 1000 and T = 9, with uniform weighting (i.e., z b,t = 1 BT \u2200b, t). Table 1 (a) and (b) show that rDAm is highly sensitive to EMCI vs. LMCI and the influence of FH. Although the baseline MKm picks up these group differences, rDAm has much higher delineation power. In particular, the p-values for rDAm for FH positive vs. negative case are an order of magnitude smaller than that of MKm. These show that rDAm is at least as good as a current state-of-the-art machine learning derived measures. Table 2 shows that rDAm has significant correlation (higher than MKm in all but two cases) to CSF levels, which are proven biomarkers for MCI to AD progression [15] . Note that a negative correlation with say \u03c4 implies that rDAm decreases and the subject gets demented as \u03c4 increases. Specifically, higher correlations (with p \u2264 0.01) with p\u03c4 and p\u03c4 /A\u03b2 suggest that rDAm is a useful continuous predictor. In most cases, these significance levels increase as more modalities are combined. Table 3 shows that the coefficient of variation (CV) of rDAm for three different populations of interest -all MCIs, LMCIs and MCIs with positive FH. Observe that rDAm's CV is smaller than that of MKm for all three populations, and all possible combinations of modalities -making it a better candidate to be used as a prediction measure. Also, the CVs for MCIs with Fig. 1 shows the estimates on the same three populations of interest as above at 80% power (Refer to the supplement for more plots). Following Table 3 , it should be straight forward to expect smaller sample estimates for rDAm compared to MKm, which is exactly the case as shown in Fig. 1 . Fig. 1 ) lead to much smaller sizes compared to using all MCIs and late MCIs. To get a sense of the improvement with respect to non-imaging based markers, we compared the best estimate (over all modalities) of MKm and rDAm with that of MMSE and CSF levels in Table 4 . At 80% power, the best estimates across CSF markers was 973 and 975 (for \u03c4 and \u03c4/A\u03b2 respectively) compared to that of 193 using rDAm -more than 5-fold decrease. It should be noted that all these estimates use only \"single time-point\" data combined with known conversion rates, in contrast to direct longitudinal measurement [8, 4] . Hence, the sizes using MMSE and CSF are as high as 1500, indicating that estimates on the order of two hundred (that of rDAm) are highly significant. Overall, the results show that imaging-derived markers lead to much smaller trials than cognitive scores and/or CSF levels. Fig. 1 . Sample estimates per arm for rDAm vs. MKm using all MCIs, LMCIs and FH positive MCIs respectively, at 80% power and 0.05 significance level. Conversion rate is 37.7%, and the induced drug effect is 0.25. Refer to supplement for 85% and 90% plots. MKm is blue and rDAm is green. Table 4 . Best rDAm and MKm sample estimates perm arm (from Fig. 1) vs. MMSE and CSF levels."}, {"section_title": "Results and Discussion", "text": ""}]