[{"section_title": "Introduction", "text": "Data collected through surveys of human-beings are a vital input to pulbic and private policy-making, social engagment analysis, as well as academic research. A quick internet search reviews that nearly all US government departments have a branch that initiates or facilitates surveys and most major US universities have a survey center or close equivalent. Yet, participation in public surveys has decreased in recent decades in the United States, such as for the national household surveys (Hardigan, et al., 2012, Meyer, et al., 2015. This trend holds for many fields and in other countries too (Anseel, et al., 2010, Inaba, 2007. Decreasing participation is also occurring among surveys on US agriculture such as those implemented by the USDA and its agency, the National Agricultural Statistics Service (NASS), which generates crucial commodity information and is responsible for the Census of Agriculture, the basis of numerous social and farm programs, funding formulas, as well as planning and operational activities (Sartore, et al., 2019, Vogel, 1999. Response rates for NASS crop acreage and production surveys have declined from 80-85% in the early 1990s to below 60% in some cases with accelerating decline rate in the last few years (Johansson, et al., 2017). Participation in the US Census of Agriculture has declined since 2002, decreasing to 78.2% and 74.6% in 2007 and 2012, respectively. Initial responses for the 2017 Census of Agriculture were so low, triggering additional subsampling to improve response rate, but as a result increased the cost (Reist, et al., 2019). Agricultural economists also rely on the Agricultural Resource Management Survey (ARMS), which also faces severe nonresponse issues (Weber and Clay, 2013). Lower response rates potentially decrease the reliability of estimates and forecasts based on these surveys (Czajka and Beyler, 2016, Dillman, et al., 2014, Johansson, et al., 2017. NASS has stated that nonresponse bias is the largest source of non-sampling error among its survey (National Research Council, 2008). Low response rate is particularly concerning for the agricultural production sector because of the limited population size. Direct on-farm employment makes up less than 1.3% of the total US employment (USDA,2020). In order to obtain sufficient sample size, producers are repeatedly approached or reminded to answer existing or new surveys, causing survey fatigue (Groves, et al., 1992), which may in turn lead to further reduction in response rate. Researchers have explored a variety of tactics to improve survey participation, including personalization (Trespalacios andPerkins, 2016), reminders (Van Mol, 2017), and offering incentives (Cohen, et al., 2019). However, work focused on improving response rate among US agricultural producers is limited. Dillman, et al. (2014) offers a comprehensive review of the techniques inducing response rate and particularly noted the usefulness of incentives. One popular approach used to improve participation is a direct payment to the agricultural producer, though it faces several challenges, such as payment managmenet, anonymity, coercion, as well as income effects. Donations may motivate respondents for survey participation irrespective of their income, the amount of incentive, and to some degree the length of the survey. We follow the discussion in the literature and explore the effectiveness of direct payment (hereafter referred to as a 'payment') versus charitable donation (hereafter referred to as 'donation') to understand producers' willingness to participate in a future survey panel, conducted in the context of a survey of Kentucky soybean farmers. We find modest evidence that, relative to direct payments, donations positively influence farmers willingness to participate in a survey panel."}, {"section_title": "Literature Review", "text": "A major concern of declining participation is nonresponse bias, when those who choose to participate in the survey are systematically different from those who chose not to participate (Neuman, 2012). This bias poses no concern if the nonresponses are missing at random (Rubin, 1976), simply leading to greater variance in the error term. Conversely, missing not at random will create bias and may increase depending on the proportion of non participants. Several studies have shown that nonresponse is rarely missing at random in agriculture, nor is there usually enough data available to determine whether respondents are missing at random (Weber andClay, 2013, Zhong, et al., 2018). Several factors may explain non-response, outlined extensively by , broadly demonstrating that the benefits of participating are outweighed by the effort. First, society may be victim to survey fatigue, when participants feel that they completed too many surveys in the past (Richardson and Simmering, 2020). Availability of the Internet has exacerbated this by reducing the difficulty of contacting participants (Rogelberg, et al., 2007). Survey fatigue is more common for specific groups of interest such high income households (Schleifer, 1986), which could arguably be relevant to some agricultural households. Another related issue is survey burden (McCarthy, et al., 2006) as well as perception of survey burden. Bradburn (1978) outlines four components of survey burden: 1) the survey length, 2) required effort (e.g. question difficulty), 3) the stress on the respondent (e.g. sensitive questions), and 4) the frequency of survey participation. Ignoring these aspects (Neuman, 2012) or the respondent's perceived survey burden can decrease participation . Conversely, McCarthy, et al. (2006) found no evidence that refusal of future NASS surveys are affected by survey burden posed by NASS on respondents such as the number of previous NASS surveys completed. Other characteristics can affect participation, such as social attention, demographic characteristics, survey mode, and value of privacy and time. The effort to increase sample size typically increases expenses and may not be effective especially when the reason behind the low response rate is missing not at random and correlated to variables of interest in the survey (Johansson, et al., 2017). The effort may also be impractical in agriculture because of the limited number of farmers per commodity per region. Nevertheless, researchers have explored several approaches to improve participation. 1 Some focus on reducing survey length and difficulty . Some consider increasing the number of contacts and/or follow-ups with potential participants (Coon, et al., 2019, Johansson, et al., 2017, also used as a public relations opportunity (McCarthy and Becker, 2000). Utilizing several survey modes (e.g. mail and phone) can increase response rates, including difficult-to-reach minority groups (Bautista, 2012). These tactics are not costless though. In NASS surveys, the cost per respondent of follow-ups increases in each additional stage, escalating from $2 for an internet survey to $4 for mail, $12 for telephone, and more than $50 for personal interview (Johansson, et al., 2017). More contacts also increases survey burden. Lastly, brand recognition can improve response rates by inducing a more personal appeal such as personalizing invitation (Trespalacios and Perkins, 2016), using university/academic branding (Fox, et al., 1988) or notifying that the communication is from a government agency such as NASS (Riodolfo, et al., 2013). Incentives are another mechanism shown to improve response rates for several survey modes and contexts (Church, 1993, Singer, et al., 1999, including in the USDA's ARMS survey (National Research Council, 2008). (Brennan, et al., 1993, Larson andPoist, 2004) show that direct monetary payments may be cost-effective by improving early responses, reducing the expense of additional follow-up activities. The incentives can take several forms such as direct payments (Cohen, et al., 2019), gifts (e.g. chocolate (Brennan and Charbonneau, 2009), stamps (Dawson and Dickinson, 1988), etc.), lotteries/prize drawings (Bosjnak and Tuten, 2003), or non-monetary rewards (Church, 1993). Several studies have compared the efficacy of monetary and non-monetary incentives. Money is more effective than noncash incentives and prepayment is more effective than a promised incentive (Church, 1993). Nonmonetary incentive are also effective in online surveys (G\u00f6ritz, 2006). However, critical questions are still unanswered about the use of incentives if money always is better than nonmonetary incentives, or if the success depends on the population of interest and the topic of the surveys (Hansen, 1980). While cash is found to be generally more effective (Singer and Ye, 2013), it creates several challenges. Determining the optimal amount is difficult, varying with survey characteristics such as target sample size, survey mode, and population (Saunders, et al., 2006). The expense of incentives can be explicit (i.e. the direct payment to the participant), but also the additional management costs from issuing payment on a per-participant basis. There is also concern that payment may jeopardize anonymity (Alessi andMartin, 2010, Manzo andBurke, 2012) or be potentially coercive, though there is no evidence of this in surveys (Singer and Couper, 2008). Lastly, efficacy changes with respondent income (Singer and Kulka, 2002) as well as the form and timing of payment (Mann, et al., 2008), aspects which may be also be at play among farmers. Because of these challenges involved with direct payments, others have explored the charitable donations as an alternative. Administrative effort is substantially lower because the number of payments is reduced from sample size to the number of charities used (Robertson and Bellenger, 1978). Additionally, donations demand less personally identifiable information from the respondent. They also benefit the charities, increasing welfare to society (Skinner, et al., 1984) and increase public networking opportunities. The ability of charitable donations to improve response rates is variable (Boyle, et al., 2012). Robertson and Bellenger (1978)'s seminal work showed that the charity incentive response rate (41%) was significantly improved relative to a cash incentive (26%) and decreased nonresponse bias. Others find no significant improvements in response rates (Furse and Stewart, 1982, Hubbard and Little, 1988, Warriner, et al., 1996 or may even discourage participation (Gattellari and Ward, 2001). It may have a lower effect than incentive with personal effect (e.g., lottery scratch tickets) (Boyle, et al., 2012). (Gendall and Healey, 2008) argued that part of this may be a function of the charity used in the survey and population studied. They find that charities with broad appeal are more efficient than specialist, political, or religious charity donations and their efficacy varies with the respondent characteristics and the topic of the survey. The critical question is deciding which form of incentive can better motivate survey participation (Singer, 2002). Specific to agricultural producers, research has been limited to design practices or the effect of direct payments for encouraging participation. As far as we know, this is the first study to examine the form of incentive, specifically direct payment versus charitable donation on improving farmer participation."}, {"section_title": "Methodology", "text": "In order to compare the efficacy of direct payment versus charitable donations on response rate, we asked a set of questons on producers' willingness to take part in a series of future surveys given corresponding incentives. This set of questions (we refer to this as the participation survey hereafter) was appended to the end of a separate and irrelevant survey related to field management practices among Kentucky soybean farmers (we refer to this as the soybean survey hereafter). The soybean survey joint with the appended participation survey was designed to take approximiately eight minutes to complete, limiting the chance of survey fatigue. Since the length of the overall soybean and participation survey was kept brief, little was known about each respondent so establishing representativeness is not possible. The participation survey asked producers' interest in taking part in a future survey panel of Kentucky farmers, describing that all surveys brought to the panel would be approved by a public land-grant institution in the state, confidential and take about 20 minutes to complete per survey. The participation survey also included questions on producers' attitudes towards surveys in general as well as their opinion on survey fatigue. Producers' willigness to participate in future surveys was assessed thorugh a Discrete Choice Experiment (DCE) in the participation survey. DCEs are commonly used in the economic valuation literature to measure tradeoffs respondents make between the features of a product/service and its price (Louviere, et al., 2000). In this study, we are interested in understanding whether producers are willing to be a member of a future survey panel where they will be incentivized with a lump sum amount either as a direct payment or as a charitable donation and as a return, they are required to complete a number of surveys per year. The DCE was used to measure the trade-offs between the incentive and the number of surveys to be completed by being a member in the panel. Our DCE was limited to two attributes: the amount of the participation incentive (3 levels: $10, $20, or $30, and donated by variable Incentive), and the number of future surveys they would be expected to participate in during the coming year while being on the panel (3 levels: 1, 2, or 3 surveys, and denoted by variable Nsurvey), generating nine possible combinations known as alternatives. These alternatives were subsequently used to form choice sets. Past DCE literature commonly designs a choice set containing two alernatives and a third opt-out option. In a choice set, respondents are then asked to choose one and only one alternative from the two alternatives offered or to indicate they prefer neither of the two alternatives by choosing the opt-out option. Given a choice set allowing to compare two alternatives, the nine alternatives in our study generates a total of 81 possible choice sets. Avoiding choice sets with two identical alternatives or in which one alternative was strictly dominating (one alternative offers a higher incentive with less number of surveys to take), a viable DCE required nine choice sets, as shown in Table 1. Each respondent was randomly assigned to one choice set, but answered this choice set twice, once for direct payment and once for a charitable donation. The two choice sets were exactly the same except whether the respondent would receive direct payment or whether compensation would be in the form of a charitable donation to their local FFA chapter. The script describing the two situations are in Figure 1. The appearance order was randomized with half of respondents answering the payment scenario first and donation scenario second, and vice versa for the other half. To model these decisions, we rely on standard Random Utility framework as in equation 1. In our study, respondent i's utility is based on selecting the jth alternative in the choice set, with a combination of the opt-out indicator, incentive amount C and other features X (namely, number of surveys to be completed), and an unobservable component e. All \u2032 are parameters to be estimated. In addition to the DCE question, the participation survey also contains other survey-taking related questions. We interact the donation treatment indicator as well as several other control variables with the main effects of the three DCE attributes: opt-out, incentive amount, and number of surveys. The definition and description of the indicator and other control variables are in Table 2 and the expected sign of the interaction term appears in Table 3. The control variables includes the ordinal response to the number of USDA surveys the respondents completed in the past year, perceived inconvenience of USDA surveys, number of non-USDA surveys completed in the past year, along with perceived inconvenience of these surveys, and how often they have received financial incentives in past surveys. We model this decision using a mixed logit specification to allow for unobservable preference heterogeneity. We assume a normal distribution for coefficients for the opt-out and number of surveys attribute variables, where ( ) is the joint probability density function of the unknown parameters."}, {"section_title": "Results", "text": "The soybean and participation survey was conducted in-person during the fall of 2017. We received 233 responses, but after removing incomplete responses, utilized 147 in the analysis. This provides initial evidence that, within our case study, charitable donations may induce higher participation than a direct payment. We more formally test willingness to participate in a survey panel using equation 2, with model results shown in Table 4. Conditional logit model results are not shown because likelihood ratio tests show significantly improved model fit using mixed logit. Model I shows the basic mixed logit model results. Each variable is significant in the expected direction. The optout coefficient is significant and negative, indicating that on average respondents prefer not to optout, matching the preliminary results. Participants are more likely to select alternative that provide a larger incentive and less likely to select an alternative that requires more surveys. In order to ensure the correct implication on the sign of the incentive variable, we specified its coefficient as fixed (Train and Weeks, 2005). The significant and large standard deviation of the opt-out constant and number of surveys indicate sizeable unobservable preference heterogeneity among respondents. The sign and significance of these main effects is maintained in all following models. To test the effect of the donation treatment, Models II, III, and IV show model results adding one attribute interaction with the donation treatment. All three interactions are statistically significant in their respective models and in the expected direction. The negative sign for Optout*Donation in Modell II shows that a Donation incentive reduces the probability of opting-out. The positive sign of Incentive*Donation in Model III indicates high responsiveness to a change in price, respondents are more likely to participate relative to a direct payment scenario. Lastly, respondents in donation scenarios are more willing to take additional surveys compared to a direct payment, as shown by Nsurvey*Donation, as shown in Model IV. Overall, these results showcase promising evidence of the donations relative to direct payments. Conversely, when multiple of these interactions are modeled simultaneously (not shown), Donation no longer demonstrates significance. The sign and magnitude are similar to prior models, so we interpret this lack of significance as a lack of data to distinguish the impact of the three interactions. This is in part substantiated by the results of models V through VII. These models repeat the respective individual interaction models (II-IV), but incorporates additional interactions from the respondent. Importantly, even with these interactions, the relevant interactions (Optout*Donation, Incentive*Donation, Nsurvey*Donation) all maintain statistical significance. Among these extra interactions, interestingly the number federal government-initiated surveys or perceived inconvenience of these surveys have no impact on producers' willingness to participate in the university-led survey panel. The number of Other Surveys the respondent has taken in the past year and their opinion of the inconvenience of Other Surveys show statistical significance. Producers become more responsive to incentive levels and less responsive to the number of surveys they are required to take as the number of previous surveys increases. This may indicate the general willingness of the person to take such surveys. The perceived inconvenience of Other Surveys is also important, with higher inconvenience corresponding to a greater probability of opting out, being less response to higher incentives, and being much more negative towards higher survey requirements. Respondents' age also plays a marginally significant role such that older respondents are more responsive to both the incentive and the number of surveys they will be required to take. Using the model results from Table 4, we can determine each respondent's Willingness to Accept (WTA) to participate in an additional survey as a member of the proposed panel. WTA is ascertained by interpreting the coefficient on the number of surveys as the marginal disutility of participating in one more survey, then dividing it by the negative of the coefficient for the payment vehicle (Adamowicz et al., 1994). Based on this, the bottom of Table 4 shows that the WTA is roughly between $8.50 and $11 per additional survey required depending on which model is used for the calculation. Based on Model IV and holding all other factors unchanged, WTA declines by about $4.43 per additional survey if the incentive is a charitable contribution."}, {"section_title": "Discussion and Conclusion", "text": "Survey participation among farmers has declined precipitously in recent years and solutions to reverse this trend are necessary since sufficient response rate helps improve quality of data which in turn contributes to better understanding the needs of producers and more efficient agricultural policy. Use of direct payments does improve participation, but also comes with additional administrative burden and may be less effective among high-income individuals. This study examines the effectiveness of charitable donations rather than direct payments as a means of improving participation in a hypothetical survey panel among Kentucky soybean farmers. We find that a decreased number of surveys required and increased payment for participation lead to significantly higher willingness to participate, respectively. We find evidence that relative to direct payment, incentivizing participation among farmers with charitable donations increases the likelihood of participation by decreasing the likelihood of opting out, increasing the sensitivity to the amount of the charitable donation, and decreasing the sensitivity to the number of surveys requested. This means that surveys attracting participation using a charitable donation can increase participation relative to the same amount of money as a direct payment. The results indicate considerable heterogeneity in producers' response to opting out of the survey panel and the number of surveys required while being in the panel. Nevertheless, for the mean trend, we find that farmers are willing to participate in a university-administered survey panel. This serves as a reassurance that universities can be a trusted brand initiating such surveys. In addition, if multiple surveys can be administered by the same entity, questions such as demographic, geographic, and basic farm operation characteristics are no longer needed to be posed repetitively, thus increase survey efficiency and allow the surveys to accommodate more unique questions without increasing total survey time. Lastly, we find evidence that perception of survey burden (e.g. number of surveys and level of inconvenience) among non-federal government does impact willingness to participate in future surveys, but the number of federal government-initiated surveys or perceptions on how inconvenient they are have no impact. This may suggest that producers view surveys from the federal governments separately from other sources. Means to improve survey participation may also need to be addressed separately. Several issues exist in this study though that undermine its usefulness. First, a similar study on a broader and more diverse group of producers is needed to paint a more complete picture of agricultural survey response rates. Because of the limited information of the participants, there is likely selfselection. All participants in the survey had already demonstrated their interest in donations because they received $5 towards their local FFA chapter. Further, inducing participation with charitable donations may not alleviate the bias from the serial differences among non-participants.   Excluding this survey, estimate the total number of surveys from the federal government (i.e. USDA) you've been asked to take this past year. 0, 1, 2-3, 4-6, 7 or more Inconvenience USDA Surveys 3.12 (1.55) In general, how much of an inconvenience is it to fill out surveys from the federal government? 1: Not at all, 2: A little, 3: Unsure, 4: Moderate, 5: A lot #Other Surveys 3.05 (1.42) Excluding this survey, estimate the total number of surveys from all other entities including state/local government, universities, etc., you've been asked to take this past year. 0, 1, 2-3, 4-6, 7 or more Inconvenience Other Surveys 2.88 (1.47) In general, how much of an inconvenience is it to fill out these surveys from other entities? 1: Not at all, 2: A little, 3: Unsure, 4: Moderate, 5: A lot Financial Incentive 1.69 (0.94) How often do you receive any financial incentive for participating in surveys? 1: Never, 2: Rarely, 3: Sometimes, 4: Most times, 5: Always Age 3.10 1: 18-24, 2: 25-34, 3: 35-44, 4: 45-55, 5: 55-64, 6: 65-74, 7: 75+ a Interacted with #Surveys, $Amount, and Optout "}]