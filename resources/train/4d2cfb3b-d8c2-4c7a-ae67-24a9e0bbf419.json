[{"section_title": "Executive Summary", "text": "The structure of the United States and global economies has changed during the last two decades in at least three major ways. First, what used to be as simple as tracking domestic research and development (R&D) spending by a small number of large U.S. manufacturers has now blossomed into the need to monitor scientific, technology, and innovation (STI) activities across the globe and across a wide range of sectors, beyond manufacturing. Second, the type of information available to track innovation, R&D, and even the science, technology, engineering, and mathematics (STEM) workforce has changed as well. Historically statistical agencies have relied on probability surveys to collect consistent and unbiased information. In recent years, however, the amount of raw data that is easily available online has soared, opening up possibilities for the new STI indicators. Microdata from administrative records and other sources have been increasingly used to produce measures of capacities and trends in the global STI system. Also, frontier methods are emerging for monitoring the number of new product introductions through sophisticated web-scraping algorithms or tracing networks of scientists engaged in research. These data sources, although promising, may have uncertain biases and other deficiencies. Third, the statistical mission of the National Center for Science and Engineering Statistics has also changed recently, expanded to include condition and progress of U.S. STEM education, and the broader question of U.S. competitiveness in science, technology, and R&D. The combination of these three factors raises questions about whether the statistical activities are properly focused to produce the information that policy makers, researchers, and businesses need. The questions become especially acute give the downturn in the U.S. economy and the importance of innovation in producing new job opportunities. To answer these questions, the panel was charged to conduct a study of the status of the science, technology, and innovation indicators that are currently developed and published by the National Science Foundation's National Center for Science and Engineering Statistics. In carrying out its charge, the panel undertook a broad and comprehensive review of STI indicators from different countries, including Japan, China, India and several countries in Europe, Latin America and Africa. We also closely examined alternative methodologies for collecting relevant data. Our goal was not to come to any particular conclusion, but to keep an open mind to possibilities for improving and revamping the NCSES suite of statistical activities."}, {"section_title": "FINDINGS", "text": "Our first finding is that the depth and breadth of STI indicators across the globe is truly remarkable. Many countries are putting a high priority on collecting information on innovation and related activities, and they are gathering high-quality data. Second, no country seems to have \"cracked the code\" in terms of a clearly superior set of STI indicators. Everyone still seems to be figuring out the right questions to ask. For example, when it comes to R&D, does it matter where R&D is done? Where the R&D is used? Or where the resulting intellectually property is located legally? Obviously, it would be great to have information on all three, but no one really knows which of these factors is the most important. Third, the panel did not find any little-known, proven STI indicators and methodologies used by other countries that could be easily and inexpensively adopted by NCSES. New technologies for data collection are very promising, but none of them is ready for implementation at a federal statistical agency. This does not mean that NCSES's STI indicators cannot be improved. Indeed, there are several recommendations in this report and others to follow in the final report that propose ways and means for NCSES to improve its STI indicators program."}, {"section_title": "PLANS FOR FURTHER WORK AND FINAL REPORT", "text": "The panel's final report will offer a comprehensive set of recommendations (including those in this interim report) with priority rankings and implementation strategies, as well as a roadmap for how the recommendations relate one to another. Those recommendations are likely to require longer lead times for data and tool development, as well as coordination with specific divisions of other statistical agencies in the United States and abroad, than those included in this interim report. We will address the net value added of proposed indicators, and we expect to specify which data and indicators can be eliminated by NCSES. Criteria for prioritization will include policy utility and obtaining more comparability of STI indicators in the United States with those published by foreign organizations. To develop those recommendations, the panel will carry out a wide range of work, including: gap analyses of current STI indicators; performance tests of key STI indicators; ways to improve measures of innovation, technological diffusion, and other key elements in understanding innovation; new data developments at the U.S. Patent and Trademark Office; the use of microdata; the possibilities of developing subnational indicators; data linking; the role of institutions and regulations; and NCSES's potential role in coordination of federal STI statistics."}, {"section_title": "RECOMMENDATIONS", "text": "This interim report recommends near-term action by NCSES along two dimensions: (1) development of new policy-relevant indicators that are based on NCSES survey data or on data collections at other statistical agencies; and (2) exploration of new data extraction and management tools for generating statistics, using automated methods of harvesting unstructured or scientometric data and data derived from administrative records. Our six near-term recommendations are in descending priority order. The first five are about new and revised indicators; the sixth concerns new processes and techniques. "}, {"section_title": "Introduction", "text": "The National Center for Science and Engineering Statistics (NCSES), at the U.S. National Foundation, is 1 of 14 major statistical agencies 1 in the federal government, of which at least 5 collect relevant information on science, technology, and innovation activities in the United States and abroad. The America COMPETES Reauthorization Act of 2010 (H.R. 5116) expanded and codified NCSES's role as a U.S. federal statistical agency. 2 Important aspects of the agency's mandate include collect, acquire, analyze, report and disseminate data on (a) research and development (R&D) trends, on (b) U.S. competitiveness in science, technology, and research and development, and on (c) the condition and progress of U.S. science, technology, engineering, and mathematics (STEM) education. NCSES is also explicitly charged to support research on NCSES data and methodologies related to data collection, analysis and dissemination: these aspects of the new NCSES mandate are the most relevant for this study."}, {"section_title": "PANEL CHARGE", "text": "In response to a request from NCSES, the Committee on National Statistics, in collaboration with the Board on Science, Technology, and Economic Policy convened the Panel on Developing Science, Technology, and Innovation Indicators for the Future to examine the status of the NCSES's science, technology, and innovation (STI) indicators. The detailed statement of task to the panel is in Box 1-1."}, {"section_title": "BOX 1-1 Statement of Task", "text": "An ad hoc panel, convened under the Committee on National Statistics, in collaboration with the Board on Science, Technology, and Economic Policy, proposes to conduct a study of the status of the science, technology, and innovation indicators that are currently developed and published by the National Science Foundation's National Center for Science and Engineering Statistics. Specifically, the panel will: 1. Assess and provide recommendations regarding the need for revised, refocused, and newly developed indicators designed to better reflect fundamental and rapid changes that are reshaping global science, technology and innovation systems. 2. Address indicators development by NCSES in its role as a U.S. federal statistical agency charged with providing balanced, policy relevant but policy-neutral information to the President, federal executive agencies, the National Science Board, the Congress, and the public."}, {"section_title": "Assess the utility of STI indicators currently used or under development in the United", "text": "States and by other governments and international organizations. 4. Develop a priority ordering for refining, making more internationally comparable, or developing a set of new STI indicators on which NCSES should focus, along with a discussion of the rationale for the assigned priorities. 5. Determine the international scope of STI indicators and the need for developing new indicators that measure developments in innovative activities in the United States and abroad. 6. Offer foresight on the types of data, metrics and indicators that will be particularly influential in evidentiary policy decision-making for years to come. The forwardlooking aspect of this study is paramount. 7. Produce an interim report at the end of the first year of the study indicating its approach to reviewing the needs and priorities for STI indicators and a final report at the end of the study with conclusions and recommendations. Understanding the interaction between the demand side and the supply side of the indicators enterprise is the major focus of this study. On the demand side, NCSES wants the panel's assessment of the types of data, metrics, and indicators that will be particularly influential in evidentiary policy and decision making for the long term. NCSES's indicators program serves researchers, administrators, and policy makers around the world who want highquality, accessible, and timely observations about the global STI system. It is important that the resulting indicators and underlying conceptual framework have practical resonance with a broad base of the users of NCSES's indicators in the near, medium, and long term. On the supply side, NCSES charged the panel to recommend revised, refocused, and new indicators that reflect the fundamental and rapid changes in the global STI system. Although a clear focus of the panel is on recent efforts by NCSES to collect and disseminate measures of innovation in the United States and abroad, the panel is also assessing the need for revising existing indicators on research and development and on human capital. Understanding the network of inputs-which include data from NCSES surveys, other federal agencies, international organizations, and the private sector-that currently and should in the future feed into the indicators production function, is within the scope of study. However, NCSES did not ask the panel to recommend new survey designs or data taxonomies, nor to develop theoretical foundations of measurement for indicators that are derived from web sources or administrative records. 3 The panel is also not focusing on NCSES's dissemination practices, since a recent National Research Council (2011) report has already made recommendations to NCSES on this subject."}, {"section_title": "ORGANIZING FRAMEWORK", "text": "There is an extensive literature on measuring STI activities that informs data gathering and statistical analysis at NCSES. Models of the ecological system of scientific discovery and technological innovation have common themes, relating inputs (state of knowledge, government policy and funding, and education and training) to outputs (new ideas, new techniques, and new instruments as revealed by publications, patents, and new goods and services) and to outcomes (social well-being, such as spillovers to health, environmental, security, and other indicators of economic and social progress). Included in this framework are institutional elements that affect the functioning of the system, including activities at government, nonprofit, and for-profit research laboratories. Evidence-based science and innovation policies at various geographic scales depend on quantitative measures and qualitative descriptions of the nodes and linkages between the nodes in this model. Figure 1 illustrates Jaffe's (2011) conceptualization of this framework. In his paper, Jaffe points out that spillover effects and linkages are important but very difficult to measure. Tassey (2007) and Gallaher and Petrusa (2006) have variants of the STI systems model. They distinguish proprietary technologies from generic technologies that are public goods. A distinguishing feature of the Tassey-Gallaher-Petrusa conceptual framework is that it includes nodes for entrepreneurial, market development, and risk reduction activities within companies. In the past decade, researchers have focused anew on linking investments in innovation to total factor productivity. The growth accounting framework is used for this purpose (see Corrado, Hulten, and Sichel [2005]; Haskel and Wallis [2009]). Researchers and practitioners alike are experimenting with methods accurately to measure the value of intangible assets. FIGURE 1-1 Schematic overview of innovation system. SOURCE: Jaffe (2011, p. 194). Permission to reproduce granted by Stanford University Press. In summary, the theoretical foundations for STI indicators are myriad. Yet, there is agreement on broad categories for which measurement is needed to understand capacity and trends in human capital, R&D, and innovation in the United States, and how the United States compares to other nations in each of these areas. For decades, NCSES has published indicators on human capital and R&D. In 2010 the agency began to publish statistics on innovation as well. These three broad categories guide the panel's investigation."}, {"section_title": "WORK TO DATE AND PLANNED", "text": "The panel's findings and recommendations are informed by experts on data extraction, tools development, statistical measurement, and public policy. Commissioned studies will also allow the panel to deliver to NCSES feasible solutions to the issues raised by our charge. To "}, {"section_title": "Government", "text": ""}, {"section_title": "Policy and Funding", "text": "date, the panel has held three meetings, between April and September 2011, and three more meetings are scheduled during the first five months of 2012. Although this is an interim report and the panel is only mid-way through the deliberative process, we are able to offer some recommendations for near-term activities and indications as to how the panel will proceed on several fronts. At the outset of the study, the panel members determined that it was important to first ascertain the sponsor's perspective on specific areas of concern for improving data collection and development of STI statistics and on perceived opportunities for new products and partnerships. It was also important to begin the study by hearing from users of NCSES datasets and publications about what they thought were needs and priorities for STI indicators in the foreseeable future. This was accomplished at our first open meeting, and the members distinguished between near-term and long-term products and processes that NCSES can develop, given resource constraints. It was determined at that time that resource efficiency considerations would be a factor as the panel prioritizes recommendations to NCSES for the development and dissemination of new STI indicators. Given the long list of recommendations for new or revised indicators gleaned from that meeting-as well as new methods that could be used to generate data for indicators-the panel decided to prioritize which tasks could be implemented in the near term given existing resources and which items should be the subject of future research and development. Since one of the primary goals of this study is to determine how to improve international comparability of STI indicators in the United States and abroad, our second meeting was a workshop of researchers and practitioners from around the world. The workshop was held on July 11 and 12, 2011, in Washington, DC: see Appendix A for the agenda and list of participants. Participants discussed: (1) metrics that have been shown to track changes in national economic growth, productivity and other indicators of social development; (2) frameworks for gathering data on academic inputs to research, development and translation processes toward commercialization of new scientific outputs, with specific subnational outlooks; and (3) nextgeneration methods for gathering and disseminating data that give snapshot views of scientific research and innovation in sectors, such as biotechnology and information and communications technology (ICT). Presentations and networked discussions focused attention on the policy relevance of redesigned or new indicators. It was evident that there is a worldwide desire for policy-relevant measures of science, technology, and, especially, innovation. However, it was clear that measures of innovation are particularly difficult to obtain directly or to calibrate. There was also much enthusiasm for NCSES and other international statistical organizations to develop STI indicators at different geographical scales. At its third meeting, the panel focused on establishing the conceptual framework for the metrics and indicators that NCSES should be disseminating; gathering information on data origination to establish which data linkages among federal statistical agencies could improve and expand NCSES's STI indicators offerings; and deliberating on its findings and recommendations. During the open part of the meeting, experts on industrial organization and economic growth theory presented conceptual frameworks that should guide the data collection processes for STI indicators, with cautions about measurement biases and speculations regarding extensions to the framework to allow measurement of intangible assets and innovation diffusion. Staff from statistical agencies also presented opportunities for linking data among agencies, which could be used to augment existing statistics on innovation activities and human capital in science, technology, engineering, and mathematics (STEM) occupations. During the closed portion of the meeting, the panel agreed on short-term recommendations to NCSES and enumerated items that require further investigation. This interim report conveys the conclusions reached during those discussions. In addition to information gathered at meetings and the workshop, the panel also commissioned three papers that will inform its final report. In one paper, Andrew Reamer presents foundations for developing subnational STI indicators, primarily focusing on data sources that could augment NCSES's offerings in that area. Reamer held a Kauffman-sponsored roundtable discussion in June 2011, which was preceded by a short questionnaire in which roundtable participants gave their opinions and insights on data needs regarding R&D, innovation, and the STEM workforce, at the national and subnational levels. Reamer's paper will include a summary of his findings from the responses to that questionnaire. The second paper, by Bronwyn Hall and Adam Jaffe, will provide a conceptual framework for STI indicators' development. It is important for the panel to be informed about what elements are expected to be included in the canonical set of indicators, what is already available, and what needs to be developed. This paper will add a new modeling framework that is more relevant for service-sector R&D. The paper will also map the developments in the European Union regarding measurement of innovation to activities at NCSES. The third paper, by Sumiye (Sue) Okubo, is on data linkages among federal statistical agencies that can inform measures of international investments in R&D and international trade of R&D services. Okubo's paper will included an empirical analysis of payments and receipts for R&D services in the United States and abroad, comparing data from the Bureau of Economic Analysis and the Business Research and Development and Innovation Survey. In the coming months, the panel will carry out the following tasks, with the help of commissioned papers and consultants, as well as members' own and staff work: \uf0b7 Conduct gap analyses to determine the strengths, weaknesses, coverage, utility, and timeliness of STI indicators, with a view to developing a set of key national STI indicators. We will also consider the relative strengths and weaknesses of indicators produced by NCSES in comparison with those published by OECD, Eurostat, the United Nations Educational, Scientific and Cultural Organization, the European Union and other international organizations. At the subnational level, we will also consider the relative strengths and weaknesses of data from private and non-profit institutions. We will map NCSES's science and engineering indicator to our policy questions, and to indicators that are published by other institutions domestically and abroad. We will identify the gaps where work needs to be done, identify the overlaps were resources may be able to be saved, and prioritize activity that NCSES should do to produce new STI indicators based on feasibility and the relative importance of the policy issue. \uf0b7 Explore new data developments at the United States Patent and Trademark Office and international governing bodies for patents, copyrights and trademarks, and make recommendations to NCSES on readily available and reliable data on these indicators of invention and potential innovation. \uf0b7 Investigate further the use of microdata to develop STI statistics, including data retrieved using web tools and administrative records. The purpose of this task is to discover other occurrences of competitions and prizes that are used in the federal context for data development and suggest to NCSES the parameters that are necessary for such a competition to be successful. We will also further investigate the methodological issues that could limit the utility of indicators resulting from nonsurvey methods. \uf0b7 Investigate further the reliability and opportunity costs of NCSES's developing new subnational STI indicators. Since NCSES already produces science and engineering indicators at the state level, we will be looking into the production of STI indicators for finer geographic scales, including metropolitan areas. We will also consider whether data from various subnational scales can be aggregated to national levels, although this is typically fraught with problems. \uf0b7 Investigate further the potential and complexities of data linking among U.S. statistical agencies and international organizations that have STI data and statistics. The panel expects to offer specific recommendations on how to use such collaborations to produce new and better STI indicators. \uf0b7 Explore the role of institutions and regulations that inform STI activities. We will also consider what quantitative or qualitative information would inform the public on the role of culture in the innovation system and the public perception of science, technology and innovation in the U.S. and abroad. The National Science Board's Science and Engineering Indicators biennial volume contains a chapter on public attitudes and understanding of science and technology (see National Science Board, 2010). 4 \uf0b7 Explore further the possibility of a coordination role for NCSES on STI data and statistics. An interagency council or working group on STI statistics could be created to identify potential synergies among datasets at federal statistical agencies. Any coordination activity of the council for NCSES would only relate to STI, and not to other collections pertaining to economic, demographic, or other statistics that are gathered and disseminated at the federal level. The panel expects to offer recommendations that require longer lead times for data and tool development than those in this report, as well as recommendations on coordination with specific divisions of other statistical agencies in the United States and abroad. We will address the net value added of proposed indicators, and we expect to specify which data activities and indicators can be eliminated by NCSES. The panel's final report is scheduled to be released in December 2012."}, {"section_title": "Concepts and Uses of Indicators THE ROLE OF INDICATORS", "text": "There are myriad descriptions and definitions of indicators-their composition, use, and limitations. The National Center for Science and Engineering Statistics (NCSES) defines an indicator as \"a statistical proxy of one or more metrics that allow for an assessment of a condition.\" 1 Indicators allow one to assess the current status of a project, program, or other activity and how far one is from targets or goals. In many circumstances, an activity is not directly measurable, and therefore indicators provide analytically proximate values that are based on expert judgement. Indicators of science, technology and innovation (STI) often substitute for direct measures of knowledge creation, invention, innovation, technological diffusion, and science and engineering talent, which would be difficult if not impossible to obtain. Techniques are improving for obtaining data that directly measure innovation activities, and these data are already being used to complement indicators that are derived from traditional methods. STI indicators, however, will still have an important role to play in informing policy decisions, especially if they are based on tested analytical frameworks."}, {"section_title": "USES AND DESIRABLE ATTRIBUTES OF INDICATORS", "text": "STI indicators are often used to relate knowledge inputs to outputs, outcomes or impacts. At a very basic level, knowledge inputs include years of schooling, level of degree, and the amount of training an employee receives on the job. Outputs are specific products, processes, or services. Outcomes and impacts are the near-term and longer term effects and ramifications to the economy or society in which the technological ecosystem operates. 2 Indicators are relied on for both post-activity evaluations and analysis prior to an activity, although there are major limitations in using STI indicators for predictive exercises. Foresight is often the best that can be asked of indicators. A comprehensive review of the use of STI indicators for policy decisions is found in Gault (2010), who outlines four ways that indicators are used for policy purposes: monitoring, benchmarking, evaluating, and \"foresighting.\" 3 At the panel's workshop, several presenters described attributes of indicators that NCSES should keep in mind as it develops new STI indicators. One important desirable attribute that was emphasized is a low sensitivity to manipulation. In addition, STI indicators are like baseball statistics-it is unlikely that one single statistic tells the whole story. Instead, users will need to rely on a collection or suite of indicators. Mindfully, during the workshop, Hugo Hollanders, of UNU-MERIT, 4 stated that there is both political and media appeal of composite indices. 5 Other ideal characteristics of indicators that workshop participants mentioned included scientifically derived/evidence based; comparable across regions; powerful for communication; affordable; accessible; scalable; sustainable; and policy and analytically relevant. STI indicators need to be policy neutral, even though the particular ones selected may reflect the preferences of the stakeholders who request them. Although the production of indicators across many fields has an established history, there are at least three major cautions regarding their use that are important to note. \uf0b7 First, indicators can send mixed signals, which require expert judgment for interpretation. For example, increased innovation-which is key to advancing living standards-is often considered to enhance job creation. Policy makers discuss spurring innovation as a job-creation tactic. However, innovation can lead to fewer jobs if the process or managerial expertise increases efficiency. Short-term displacement of workers in one industry or sector can be counterbalanced in the longer term by development of new products, services, and even sectors and by increased market demand if process efficiencies drive down prices (see Pianta, 2005;Van Rennen, 1997). One way to be cautious about mixed signals is to develop STI indicators that support analysis of time scales, sectors, and geographic locations. \uf0b7 Second, a given metric, once it becomes widely used, changes the behavior of the people and practices it attempts to measure. The worst thing a metric can do is not just to deliver a bad (i.e., misleading) answer, but to incentivize bad practice (see, e.g., West and Bergstrom, 2010). It is important that indicators avoid sending distorted signals to users. \uf0b7 Not everything that counts can be counted, and not everything that can be counted counts (an idea attributed to Albert Einstein). It seems clear that some outcome measures that reflect the importance of research and development (R&D) and innovation to society are illusive. For example, social well-being is difficult to measure, yet one of the key interests of policy makers is the return on investment of public funding for science and technology for the good of society."}, {"section_title": "BEYOND SCORING TO POLICY RELEVANCE", "text": "An important aspect of the charge to this panel is the assessment of the utility of STI indicators. Although the National Science Foundation (NSF) does not do policy work, the statistics that NCSES produces are often cited in debates about policies regarding the science and engineering enterprise. For instance, the American Association for the Advancement of Science (AAAS) annually prepares a report giving various breakdowns of R&D expenditures in the federal budget. These data are informed by NSF's publications, National Patterns of R&D Resources and Federal Funds for Research and Development. In the latest report (American Association for the Advancement of Science, 2011), NSF data are used to show the role of innovation in productivity growth and how innovation affects the quality of life. The Congressional Research Service (CRS) 6 regularly refers to the National Science Board's Science and Engineering Indicators (SEI) biennial volumes (see National Science Board, 2010), which are prepared by NCSES. 7 The online version of SEI also has a sizable share of users outside the policy arena and outside the United States. There are several highly influential reports each year that rely on NCSES indicators to relate scientific inputs to socioeconomic outcomes. The final report of this panel will contain a comprehensive representation of the policy relevance of STI indicators. In the course of its work to date, the panel queried a variety of users, including policy makers, government and academic administrators, researchers, and corporate decision makers in high-tech manufacturing and service industries. We also sought input from developers of STI indicators and from individuals who are called on by policy makers to do assessments of hightech sectors in the United States and abroad. This input yielded dozens of questions that STI indicators could address. From the extensive list of questions and issues we received, the panel distilled eight key issues that are expected to be prominent in the minds of decision-makers for the foreseeable future: growth, productivity and jobs; STI activities; STI talent; private investment, government investment and procurement; institutions, networks, and regulations (including intellectual property protection and technology transfer); global STI activities and outcomes; subnational STI activities and outcomes; and systemic changes on the horizon. Box 2-1 shows the questions that flow from these issues. Although the policy relevance of the STI indicators is of primary importance for the panel's work, the recommendations here and in the final report will address fundamental aspects of monitoring and benchmarking that are of broader interest. \uf0b7 Subnational STI Activities and Outcomes How does innovation activity in a given firm at a given place contribute to that firm's productivity, employment and growth, and perhaps also to these characteristics in the surrounding area? How are those innovation supply chains working within a state? Are firms principally outsourcing new knowledge from customers or from universities? \uf0b7 Systemic Changes on the Horizon How will demographic shifts affect the STEM workforce, nationally and internationally? Will it shift the locus of the most highly productive regions? Will global financial crises slow innovation activities or merely shift the locus of activities? When will emerging economies be integrated into the global ecosystem of innovation and what impact will that have on the system? How are public views of science and technology changing over time?"}, {"section_title": "Measuring Human Capital", "text": "Human capital is the ability, knowledge and skill base that is typically acquired or enhanced by an individual through education and training. The National Center for Science and Engineering Statistics (NCSES) at the National Science Foundation (NSF) has a rich set of human capital indicators. NCSES's academic surveys provide information on sources of academic funding for science and engineering research, and those data give snapshots of the balance of investments among various fields of study and the infrastructure that is supported by federal dollars at academic institutions. 1"}, {"section_title": "AVAILABLE DATA", "text": "NCSES's Science and Engineering Statistical Data System (SESTAT) is a comprehensive database on education, employment, work activities, and demographic characteristics. SESTAT collects information from three biennial sample surveys of individuals: the National Survey of College Graduates; the National Survey of Recent College Graduates, and the Survey of Doctorate Recipients. At the Census Bureau, a 2009 change to the American Community Survey added a \"field of bachelor's degree\" question, which had been recommended by the National Research Council (2008). 2 NCSES plans to draw and refresh the entire National Survey of College Graduates from the American Community Survey. We note that NCSES is the international leader on science and engineering education statistics. The National Science Board's Science and Engineering Indicators (SEI) biennial report contains data on enrollments and degrees by demographic classification, including data by citizenship, place of birth, and postdoctoral fellowship. It has information on students by type of financial support in graduate school, including support from the federal government, by field of study. The data include \"stay rates\" and intent to stay in the United States. Data are also available on tertiary degrees conferred in other countries. NCSES currently publishes a range of statistics on published papers, including country, countries' shares of cited papers, and international collaboration. NCSES also uses its Business Research and Development and Innovation Survey (BRDIS) for employment statistics, publishing an InfoBrief on research and development (R&D) employment intensity, domestic and foreign R&D employment, and company-performed R&D expenditures per R&D employee. 3 The National Center for Education Statistics provides NCSES with elementary, secondary and tertiary education statistics for publication. 1 It should be noted that NSF includes social sciences and psychology in its definition of science and engineering: see http://www.nsf.gov/statistics/infbrief/nsf10315/nsf10315.pdf [December 2011]. 2 The Panel to Assess the Benefits of the American Community Survey for the NSF Science Resources Statistics Division recommended: \"The National Science Foundation should use current data from the American Community Survey to evaluate the degree to which the American Community Survey with the field-of-degree question would allow for the production of mandated indicator reports in the future\" (National Research Council, 2008, p. 7). 3 For an example, see http: There is a persistent problem of underutilization of existing data on science, technology, engineering, and mathematics (STEM) workers. Although worker mobility is undermeasured in traditional STI employment statistics, there are longitudinal studies that capture data on movement of workers with STEM degrees within and outside traditional science and engineering jobs. Staff from a range of agencies emphasized to the panel that they have much underutilized data, particularly regarding human capital. For example, at the panel's workshop Erika McEntarfer of the Census Bureau described a potential use of data from the Longitudinal Employer-Household Dynamics Survey that could link workers longitudinally across jobs. Her division is currently working on this project. Integrating these data (along with similar data on firm dynamics) into the statistics offered by NCSES would create a useful set of indicators. Trends on how macroeconomic fluctuations affect workers and knowledge flows in science and engineering occupations would be just one output from these data. More descriptive data on innovators would also broaden understanding of the skill sets that lead to advances in science and technological innovation. Getting this information would require case studies, which could enhance understanding of the statistics based on counting stocks and flows of individuals and knowledge capital. There are also data from BRDIS that have not been fully used. Headcounts and related statistics are available for the United States and worldwide for employment, R&D employment, R&D employment by occupation and gender, and highest degree earned. For the United States, there are also counts of H-1B and L-1 visa holders. 4 Full-time-equivalent (FTE) counts are available for science and engineering workers, as are the number of these FTEs that are funded by the federal government in the United States. Thus, NCSES could publish statistics other than those released in the 2010 InfoBrief on employment statistics. In particular, NCSES could use BRDIS alone to show the number of non-U.S. citizens who have H-1B or L-1 visas and are employed in the United States as R&D scientists and engineers. NCSES has also published an InfoBrief on foreign science and engineering students who are enrolled in schools in the United States. 5 Currently, NCSES publications do not include statistics on how many U.S. employees in science and engineering occupations were trained in a specific foreign country. Continued collaborative effort between NCSES and the Department of Homeland Security are expected to yield better STEM education and workforce indicators."}, {"section_title": "POLICY RELEVANCE", "text": "Based on a crude measure of interest to users of the SEI online statistics, education and workforce measures are the most viewed (unadjusted for length of views), with statistics on states a distant second. NCSES's bedrock statistics on human capital, therefore, are one of the agency's most important products. However, more extensive analytically based measures of 4 The H-1B and L-1 visas are for foreign workers in specialty occupations in fields that require highly specialized knowledge and intra-country transferees, respectively. For the fields covered, see http://www.uscis.gov/portal/site/uscis/menuitem.eb1d4c2a3e5b9ac89243c6a7543f6d1a/?vgnextoid=73566811264a3 210VgnVCM100000b92ca60aRCRD&vgnextchannel=73566811264a3210VgnVCM100000b92ca60aRCRD [January 2012]. 5 See InfoBrief NSF 10-324, July 2010: available http://www.nsf.gov/statistics/infbrief/nsf10324/nsf10324.pdf human capital, particularly related to inputs to innovation, would be valuable to policy makers and others. Policy makers, university administrators, and business strategists are all interested in an evidence-based answer to the following questions: How much knowledge capital does the United States have? How many people, possessing what kind of skills, are needed to achieve a robust STI system? How mobile are science and engineering workers between public and private sector jobs? Is the population of science and engineering researches aging, and if so, at what rate? There is also a keen interest in internationally comparative data: Where does the United States rank among nations on elements of advancement of scientific knowledge? In which fields is the United States a net exporter of knowledge? The America COMPETES Reauthorization Act of 2010 calls for NCSES to provide information on STEM education. NCSES has many elements already in its datasets to satisfy this requirement. At the panel's workshop, several presenters described measures of STI talent that go beyond the counts of science and engineering undergraduates, graduate students, doctoral recipients, and postdoctorate workers. For example, Howard Alper (University of Ottawa), listed items that would be useful in a suite of \"talent\" measures, including: data from the Programme for International Student Assessment (PISA) for 15-year-olds; percentage of population with tertiary education; numbers of bachelor-degree graduates in science and engineering-related disciplines; number of Ph.D.s in science, math, and engineering; and R&D personnel in business. We note that NCSES currently publishes statistics on each of these measures. Other suggested measures of STI talent included the pool of students who are initially trained in community colleges, as a pathway to a bachelor's degree and higher degrees. 6 It is often noted by engineers that many high-skilled jobs in certain engineering fields require a master's-level degree and not a doctorate. Therefore, NCSES has an opportunity to expand its human capital indicators by providing measures on master's-level STI talent. With such data, one could answer several related questions: Where is there excess demand for STI talent and what type of talent is in high demand? What additional sources of \"talent\" can best be tapped to supply these workers? More detailed data on immigrants, women, minorities, and people with disabilities in the science and engineering workforce would provide information to answer such questions about talent. Without good counts of these individuals, the full STI workforce capacity of the nation cannot be known. It is also important to consider what fields other than physical and biological sciences, technology, engineering, and mathematics are important for advances in STI. A careful assessment of network sciences, as well as behavioral and social sciences, would be valuable because they are in part responsible for acceleration of innovation. For example, social networks are used in \"collaboratories\" to further scientific inquiry without the need for brick-and-mortar facilities. These contributions to the scientific enterprise come from behavioral and social sciences. For advances in basic science, it is arguably true that the physical and biological science disciplines are the primary reservoir of talent. However, the broader scope of innovation, including managerial and organizational elements, includes the social, behavioral and managerial sciences as critical contributors to outputs and outcomes. Christopher Hill (2007) argues that the skill set is changing for advancing innovation: In the post-scientific society, the creation of wealth and jobs based on innovation and new ideas will tend to draw less on the natural sciences and engineering and more on the organizational and social sciences, on the arts, on new business processes, and on meeting consumer needs based on niche production of specialized products and services in which interesting design and appeal to individual tastes matter more than low cost or radical new technologies. 7\nOne impediment to understanding and assessing the country's innovation is the lack of comparability of U.S. STI indicators with those developed by other OECD nations and other countries. NCSES should develop more useful indicators of innovation-an outcome measure. The biennial publication of the National Science Board, Science and Engineering Indicators (SEI), now includes information on a range of factors, including science, technology, engineering, and mathematics (STEM) graduates, R&D, and patents, but these are intermediate inputs into or proxies for innovation. They are not indicators of innovation itself, which is the 1 It is widely known that the innovation statistics from BRDIS and the CIS lack comparability: see an explanation in Hall (2011, fn. 4). 2 See InfoBrief 11-300 (October 2010): available: http://www.nsf.gov/statistics/infbrief/nsf11300/ [December 2011]. The data are based on the 2008 BRDIS, which was launched in January 2009. 3 Tabulations on the 2009 data are due to be released in mid-2012, with an InfoBrief on the 2009 BRDIS data scheduled to be released in December 2011. The 2010 BRDIS data were still being collected in November 2011. combination of new products and services (see Organisation for Economic Co-operation and Development-Eurostat, 2005, pp. 89-104;2011, Chapter 5). Ideally, one would like to measure the contribution of innovation to total output (gross domestic product, GDP) in the United States and have these measures in a form that allows for cross-country comparisons. However, the currently available measures are imperfect. Policy makers and citizens would be well served by useful proxies that could mature into more refined measures of innovation. In this report we focus in on the need for NCSES to further use the BRDIS data to answer such key policy questions as: How influential is R&D for innovation and growth (in manufacturing compared with services)? At times of economic crisis, what is the contribution of scientific research and innovation to economic recovery? How important are different kinds of entities for advancing national innovation: small firms, young firms, high-growth firms, government labs and procurement activities, and nonprofit organizations? How does innovation activity in a given firm at a given place contribute to that firm's productivity, employment and growth, and perhaps also to these characteristics in the surrounding area? NEEDED DATA To answer these questions, four improvements seem feasible for NCSES in the near term, using BRDIS data: statistics that are more comparable with those of other OECD; better information on what firms characterize as innovation; data on high-growth firms and \"gazelles;\" and data on the relationship of new firms to innovation.\nNCSES's indicators are already used to inform many policy issues. Since 1953, the National Science Foundation (NSF) has published statistics on industrial R&D, including: R&D expenditures and performance by industrial sector, state and government agency; and company R&D performed outside the United States. 1 These data are useful for informing federal budget decisions, including measurements of R&D budgets at federal agencies (defense and nondefense) and comparisons of federal spending on physical and biological sciences. Still missing from these statistics, however, are some important measures of R&D and related spillover effects that would help to determine the effects of scientific investment on socioeconomic outcomes. Policy makers would benefit from statistics that address such questions as: What effect does federal spending on R&D have on innovation and economic health, and over what time frame? What is the international balance of trade in R&D services? How much R&D do U.S. multinational companies carry out outside the United States, and how much R&D do foreign multinational companies carry out in the United States? In this report, we focus on ways of improving measurement of R&D services; the panel expects to offer recommendations on other enhancements to NCSES's R&D indicators in our final report.\nSubnational statistics on science, technology, and innovation are especially policy relevant at this time when the nation's economy is struggling to adjust to structural displacement of workers. How does innovation activity in a given firm at a given place contribute to that firm's productivity, employment and growth, and perhaps also to these characteristics in the surrounding area? How do those innovation supply chains work within a state? Are firms principally acquiring new knowledge from customers or from universities? These are the questions for which governors as well as state and municipal representatives want data in order to be able to answer. From an investment perspective, policy makers, university administrators, and business developers want to see science, technology, and innovation (STI) indicators at scales that match their geographic interests. During the panel's workshop, Robert Atkinson (Information Technology and Innovation Foundation) said that subnational information would be particularly helpful for technology and innovation policy. Other workshop participants described subnational decompositions in several countries. Based on her extensive research on STI hot spots, Maryann Feldman (University of North Carolina) emphasized that \"economic growth does occur within these finer geographic units.\" She went on to stress that decision makers in the states and metropolitan areas are in great need of data on innovation activities at subnational levels. She said that NCSES needs to work with users to determine what statistics would be useful at the subnational level and what some of the users have already created that could be useful inputs into NCSES's subnational indicators. There are a few caveats to expanding NCSES's national indicators to include a variety of subnational measures. For example, Christopher Hill (George Mason University) raised the issue that there might be disparities between the disaggregated data and data at the national level if the proper weights are not applied in the aggregation process. Hill also voiced concern that data might not maintain the same type of meaning at disaggregate levels as they do at the national level. For example, he said, year after year the state-level data for Virginia show substantially higher levels of federal funding to private industry for R&D than is shown in the expenditures by private companies. He concluded that \"there's a whole lot more money going into the state than companies report spending.\" Hill also said: \"While place matters, place also is very leaky both in and out.\" Therefore, it is not necessarily clear what the geographical span of impact is for a university or a firm in a given locale. There is also the problem that multiplant firms that span more than one state might have difficulty allocating activities accordingly. This potential problem would clearly affect data quality and reliability at finer geographical levels."}, {"section_title": "CONCLUSIONS AND RECOMMENDATION", "text": "Since there are a rich set of topics on human capital that NCSES's indicators inform, the panel chose only a few items to focus on in this report. The panel's final report will address other, no less important, items. First, it is important to obtain better measures of rapid changes in the STI workforce, including job mobility. Worker mobility between jobs, occupations, and nations is important to measure since it is an indicator of how knowledge flows and spillovers occur and how knowledge markets operate. For instance, it is important to know in what industries people with degrees in science-in both STEM fields and social and behavioral sciences-work in throughout their lifetimes. It is also of national interest to have information on the flows of undergraduate and graduate students, postdoctoral fellows, and workers between states and between countries. Second, it is important to determine where there are overlapping datasets among the federal statistical agencies on STEM education and workforce statistics. NCSES has long-term relationships with several statistical agencies that provide education data for science and engineering indicators. Some education indicators are collected through NCSES surveys, while others are reported by NCSES on the basis of statistics generated at other agencies and organizations. In particular, NCSES and the National Center for Education Statistics (NCES) both collect data on higher education degree holders. Very recently, NCSES and NCES staff agreed to undertake a joint gap analysis to examine education data-gathering activities. This analysis will enable both agencies, and others, to determine where there is useful overlap, where there are voids, and where efficiencies can be achieved by streamlining efforts. A series of workshops on potential linkages, interoperability and rationalization of datasets on human capital will further improve efficiencies among the agencies. This will enable NCSES to plan for a greater role of data clearinghouse specifically on STI education and workforce data. Third, there are clear near-term opportunities for NCSES to mine BRDIS for indicators related to the STEM labor force. NCSES already has several activities under way or planned to update its portfolio on education and workforce statistics. For instance, NCSES is rethinking the collection of data on the science and engineering workforce as the National Survey of College Graduates transitions to a sampling frame completely built from the American Community Survey; it is also implementing the International Survey of Doctoral Recipients and integrating it into the Survey of Doctoral Recipients; and it is developing an Early Career Doctorate Project and considering new statistics on earnings of STI workers as a potential new indicator. All of these efforts can provide new indicators that address user needs. For example, a science and engineering wage index could facilitate international comparisons and become another explanatory variable for international flows of science and engineering students and workers. The panel's final report will offer recommendations on how these efforts should be prioritized. \nThe panel in its continuing work will determine the relative merits of NCSES's augmenting its indicators program to produce finer geographical granularity of STI indicators. It should be noted that NCSES could provide some of these indicators with data from current surveys. Data from other sources, both profit and nonprofit, could be made available through NCSES. The data from outside providers would necessarily be vetted through NCSES's data evaluation process. As a clearinghouse of STI data, NCSES could make great strides toward making high-utility indicators available to users, especially researchers and government organizations. \uf0b7 Traditional surveys face increasing expense, declining response rates, and lengthy time lags between when data are gathered and when derived indicators and other statistics can be published. \uf0b7 Tools for data extraction, manipulation, and analysis are rapidly evolving. \uf0b7 Repositories of STI measures that users demand are distributed among several statistical agencies, and private repositories. \uf0b7 Sources of knowledge generation and innovation are expanding beyond the traditional developed countries to emerging and developing countries. \uf0b7 Users' expectations are rising, and they are demanding more access to statistics that are closer to the actual measures of what they want to know. It is also expected that standards and taxonomies for data collection and analysis will change before the end of this decade. The Organisation for Economic Co-operation and Development's National Experts on Science and Technology Indicators are discussing revising the Frescati and Oslo manuals (Organisation for Economic Co-operation andDevelopment, 2002, 2005) on a rolling basis. The group plans to work on priority themes and to build a better bridge between the two manuals. The North American Industry Classification System (NAICS) codes and the Standard Occupational Codes may also undergo revisions in the next decade or less. In light of these likely changes, this chapter offers the panel's analysis and recommendations on activities that NCSES needs to consider in the near future to continue to prepare organizationally for these challenges and to improve its portfolio of STI indicators in this environment. The final report will provide recommendations and a roadmap on how to implement those recommendations. NCSES and, indeed, other government statistical agencies, confront a world of dizzying change in how information technology is integrated into their data gathering and data management activities. The World Wide Web (the web), in particular, has been transformational in making possible new kinds of forecasting and data collection methods that provide useful insights in almost real time. These tools provide information much more rapidly than the traditional surveys with up to multiple-year lags. In addition, other changes are occurring. In his November 2011 presentation at the annual meeting of the Consortium of Social Science Associations, Robert Groves (2011a) conveyed the status of U.S. surveys, noting: \"Threatened coverage of frames; falling participation rates; increasing reliance on nonresponse adjustments; and for surveys with high response rate targets, inflated costs.\" His proposed solution set for what agencies should do to address these issues is to develop an approach of a \"blended data world by building on top of existing surveys.\" 1 Groves (2011b) envisions multimodal data acquisition and manipulation of data, including: \"Internet behaviors; administrative records; Internet self-reporting; telephone, face-to-face, paper surveys; real-time mode switch to fill in missing data; and real-time estimation.\" NCSES needs to determine now how it will handle these changes if they materialize and how the types and frequencies of various STI indicators will be affected. During the panel's workshop, Alicia Robb (of the Kauffman Foundation) encouraged NCSES to explore the use of administrative records to produce STI indicators, but she also cautioned that ownership issues associated with use of those data will have to be addressed before they could become a reliable complementary data source to traditional survey data. Stefano Bertuzzi (of the National Institutes of Health and the STAR METRICS Program) also presented techniques of using administrative records at universities to determine the impact of federal research funds on scientific outputs and the development of human capital in the physical and biological sciences. There are also foresight questions that STI indicators can inform. Demographic, economic, technological, and organizational changes will all influence the subjects being measured, the mechanisms used to measure them, and the products offered by NCSES. STI indicators will be called on to answer the following questions: How will demographic shifts affect the science, technology, engineering, and mathematics (STEM) workforce, nationally and internationally? Will those shifts change the locus of the most highly productive regions? Will global financial crises slow innovation activities or merely change the locus of activities? When will emerging economies be integrated into the global ecosystem of innovation, and what effects might they have on the system? However, as cautioned above, indicators are not predictors. They can be used in isolation or in groups to show tendencies, voids, and at times what additional information is needed. All of this suggests a shift in emphasis over time for NCSES's indicators program. The agency will have to make decisions on whether and how to adopt the new techniques. Although NCSES is not expected to eliminate all traditional survey methods, it is expected that the prolonged austerity of federal budgets will necessitate increased reliance on web-based techniques and databases. On the horizon, the panel believes that NCSES will have to use surveys more efficiently and increase use of web-based tools for harvesting data, particularly on human capital measures and output measures related to scientific discoveries and innovation, and databases from other government agencies and private providers."}, {"section_title": "Measuring Innovation", "text": ""}, {"section_title": "CONCEPTS AND DEFINITIONS OF INNOVATION", "text": "The use of \"innovation\" in this report is taken from Schumpeter (1934, p. 66), who defined product innovation as \"the introduction of a new good . . . or a new quality of a good,\" and process innovation as ''the introduction of a new method of production . . . or a new way of handling a commodity commercially.\" This basic definition of innovation was codified by the Organisation for Economic Co-operation and Development-Eurostat (2005, p. 46): \u2026the implementation of a new or significantly improved product (good or service), or process, a new marketing method, or a new organizational method in business practices, workplace organisation or external relations.\u2026This includes products, processes and methods that firms are the first to develop and those that have been adopted from other firms or organisations. Innovations can be distinguished from inventions by the criterion that innovations are implemented in the marketplace. Innovation, therefore, is a new product or idea when it is commercialized, by definition. This definition does not mean that an innovation is necessarily widely distributed or diffused in a market. It does mean that development of a new product or process that is not marketed is not considered an innovation. However, research and development (R&D) and other activities related to innovation are counted as innovation activity in the Oslo Manual (Organisation for Economic Co-operation and Development-Eurostat, 2005, p. 18): . . . all scientific, technological, organisational, financial and commercial steps which actually, or are intended to, lead to the implementation of innovations. Some innovation activities are themselves innovative; others are not novel activities but are necessary for the implementation of innovations. Innovation activities also include R&D that is not directly related to the development of a specific innovation. Since innovation is a term widely used in society, the National Center for Science and Engineering Statistics (NCSES) goes to great lengths to convey to its survey respondents what is meant by innovation or innovation activities. Yet it is important to recognize that such concepts as invention, innovation, and technological diffusion are on a continuum, and there is still some debate regarding their respective space on that continuum. As NCSES develops surveys, new datasets and new indicators of innovation activities, it will be important to try to establish rigorous standards of definitions for these terms. NCSES's role in the working group of National Experts on Science and Technology Indicators (NESTI) of the Organisation for Economic Cooperation and Development (OECD) gives the agency good opportunity over time to establish clearer definitions as revisions are made to the Frascati manual on R&D (Organisation for Economic Co-operation and Development, 2002), the Oslo manual on innovation (Organisation for Economic Co-operation and Development-Eurostat, 2005), and, possibly, the Canberra manual on human resources (Organisation for Economic Co-operation and Development, 1995). Although there is no mandate to review and revise the Community Innovation Survey (CIS), the OECD has received recommendations on how to better design innovation questions. This will have implications for CIS, BRDIS and other innovation surveys internationally. Several things affect the lack of comparability of United States and European data on the innovativeness of firms-the framing effect of using a lengthy R&D survey, sampling errors, weighting issues, to name a few. NCSES and the OECD are actively collecting evidence to assess what factors may drive biases in international comparisons. Since 2008, the National Science Foundation (NSF) has collected data on product and process innovation from the Business Research and Development and Innovation Survey (BRDIS). NCSES augmented its R&D survey to measure innovation activities, allowing for comparisons of innovation statistics across several countries. 1 Although the 2008 BRDIS was a pilot survey, it did yield some data on the incidence of product and process innovation among firms by sector (including services), size class, and whether or not respondents reported R&D activity. 2 BRDIS questions on innovation were augmented in the 2009 and 2010 versions of the survey; the 2011 version is currently under development. 3 NCSES endeavors to gather more information on innovation activities, going beyond simple \"yes/no\" questions on whether a firm introduced or significantly improved goods, services or processes. These efforts have also included attempts to develop more comparability to key questions in the CIS and to ensure that the innovation questions are answered by firms that do not do R&D. Comparability of the BRDIS and CIS data also depends on surveying similar populations of firms and the techniques used to derive estimates from the data. BRDIS is still a work in progress. Complete cognitive testing of the innovation questions still remains to be done in both the United States and Europe. Nevertheless, the data are useful for preliminary snapshots."}, {"section_title": "Comparable Statistics", "text": "Understanding the nation's position on innovation would be greatly helped if NCSES developed statistics from its BRDIS data with the same cutoffs as those from other OECD countries. In addition to the number of innovative firms in various industries that have five or more employees, it would be helpful to have similar statistics for firms with 10 or more employees and firms with 20 or more employees. These data will show the size dependency of innovation activities. Also needed for comparability are statistics using the same set of industries typically used in statistics for other countries. An example is the core set of industries used by Eurostat for comparison of innovation statistics among European Union countries, including: mining; manufacturing; and selected service industries, such as wholesale trade, transports, financial services, information technology (IT) services, R&D services, and business services. These data could be used to compile a simple indicator of the share of product-process innovative firms, defined as firms that have implemented a product or process innovation. NCSES is developing another R&D and innovation survey on firms with fewer than five employees: MIST, Microbusiness, Innovation Science, and Technology. The survey is designed to yield data measuring the incidence of R&D activities among small businesses. NCSES has an interagency agreement with the Internal Revenue Service (IRS) to use IRS data for the sampling frame, for data quality review and to supplement the MIST data. Potential questions in MIST will query firms on: R&D and innovation funding; employment and owner characteristics; sales of new or significantly improved goods and services; technology transfer and knowledge diffusion; sources of technical knowledge; and measures of firms' entrepreneurial effectiveness. The MIST pilot is planned for 2012. Those data, together with BRDIS data for companies with 5+, 10+, and 20+ employees, will provide useful information on the size dependency of innovation (among other things). 4"}, {"section_title": "Understanding \"Innovation\"", "text": "Even with more comparable statistics on innovation, however, it will still not be clear to users that firms are representing the same or very similar things when they report product or process innovations. The survey questions are not transparent enough to provide a full understanding of what the resulting data and statistics mean. For example, users have no independent measure of whether the innovations of firms that innovate but do not do R&D are more or less important than those that do R&D. Users would have more confidence in and understanding of BRDIS innovation measures if they knew that knowledge input measures correlated with actual performance and even more confidence if they knew what some of the firms were calling innovation-how closely their reports matched the standards in the question. To obtain this information, NCSES could commission a study of a subset of firms to determine what they are measuring as innovation. It would be useful to have firms of different sizes, different sectors and different geographic locations represented in such a study."}, {"section_title": "Innovation and Firm Size", "text": "The data in BRDIS could be used to begin developing statistics on high-growth firms and \"gazelles.\" The Manual on Business Demography (Organisation for Economic Co-operation and Development-Eurostat 2008, Chapter 2) defines high-growth enterprises as \"All enterprises with average annualised growth greater than 20% per annum, over a three year period\u2026. Growth can be measured by the number of employees or by turnover\u2026. A size threshold has been suggested as 10 employees at the beginning of the growth period.\" Gazelles are the subset of high-growth enterprises that are up to 5 years old. These thresholds are arbitrary and only based on convention. NCSES could conduct its own sensitivity analysis to fine-tune the definitions of high-growth firms and gazelles. 5 Ditte Rude Petersen and Nadim Ahmad show a technique of conducting this type of analysis in \"High-Growth Enterprises and Gazelles-Preliminary and Summary Sensitivity Analysis (Organisation for Economic Co-operation and Development, 2007). During the panel's workshop, several speakers (Howard Alper, University of Ottawa; Robert Atkinson, Information Technology and Innovation Foundation; John Haltiwanger, University of Maryland; Hugo Hollanders, U.N. University's Maastricht Economic and Social Research Institute on Innovation and Technology [UNU-MERIT]; and Brian MacAulay, National Endowment for Science, Technology, and the Arts) mentioned the importance of tracking trends of sustainability of jobs in these types of firms during economic downturns (even if total employment is small). Atkinson said there was an interest in having these data at the national, state, and at finer geographical levels. It would also be useful to have those data to determine over time whether high-growth firms or gazelles in particular have a higher incidence of innovation activity. The connection between high-growth firms and innovation is complex. These data would enable researchers better to determine the relationship between the presence of high-growth firms and innovation. At the June 2011 roundtable workshop lead by Andrew Reamer (George Washington University) on innovation measures, B.K. Atrostic, Cheryl Grim, and Javier Miranda from the Center for Economic Studies at the U.S. Census Bureau suggested the development of a database of business dynamics statistics, which would provide information about births and deaths of firms, as well as the distribution of growth rates of gazelles and other types of high-growth firms. The suggested database would have information on firm size, age, location, and sector. This information would allow examination of the connection between innovative industries' job creation and high-growth firms in the context of economies at various geographical scales. In his presentation, at the panel's July 2011 workshop, Hollanders showed that highgrowth firms are significantly more innovative than other firms in his dataset. The statistics on high-growth firms and gazelles could, therefore, be used to answer the following question: Are these the types of firms that drive economic and job growth? A simple table could show highgrowth firms and other firms that are and are not innovative to compare economic characteristics, ideally over time. At the September 2011 panel meeting in Washington, DC, representatives from the Bureau of Labor Statistics (BLS), the U.S. Census Bureau, and the Bureau of Economic Analysis (BEA) mentioned that linking certain datasets among them would yield reasonable numbers on gazelles. Such a table could be added to the SEI or become the foundation of an InfoBrief. The following indicators could be produced by starting with the BRDIS data on high-growth firms and gazelles: rate of high-growth enterprises (number of high-growth enterprises as a percentage of the total population of active enterprises with at least n-number of employees); rate of gazelles among newly born enterprises (number of gazelles as a percentage of all active enterprises with at least n-number of employees that were born 4 or 5 years ago). These indicators would be comparable to those produced in several other countries, thus increasing users' understanding of the comparative position of the United States on an aspect of the country's innovative capacity. Clearly, getting publishable statistics on high-growth firms and gazelles is a multistage task that will require data acquisition and linking in addition to the data available in BRDIS. A good first step would be for NCSES to explore the matchup of their BRDIS data with data on firm dynamics from BLS. The panel's final report will address the complexities of data linking, particularly with the view to international comparability of the resulting statistics."}, {"section_title": "Relationship of New Firms and Innovation", "text": "New indicators that allow users to determine to what extent births of new firms are driving innovation would also be useful. At the panel's workshop, John Haltiwanger presented a compelling presentation on what he and other researchers have developed using Census Bureau data on firm dynamics-that is, firm births and deaths over time. These data can provide the groundwork to answer important questions from policy makers and researchers, such as: Does innovation come disproportionately from new firms? Do start-ups and young businesses create net new jobs and increase productivity growth? NCSES has a unique set of data in BRDIS, which, when combined with other datasets, can be instrumental in answering these and other important questions. Integrating firm dynamism (and the related employment effect) will take time and resources. During his presentation, Haltiwanger described three Census Bureau datasets that, together with BRDIS data, would allow NCSES to develop its business dynamics indicators: \uf0b7 Longitudinal Business Database-tracks all establishments and firms with at least one employee including startups from 1976 to present; \uf0b7 Integrated Longitudinal Business Database-tracks all nonemployer firms and integrated with employer firms from 1994 to present; and \uf0b7 Longitudinal Employer-Household Dynamics-tracks longitudinally all employeremployee matches and transitions (hires, separations, job creation, and job destruction) from 1990 to present. 6 Questions from the Census Bureau's 2007 and 2012 Economic Census, Company Organization Survey, and Management and Organizational Practices Survey will also yield useful information on R&D and other innovation activities for establishments. Infrastructure datasets can also track relationships between start-up and young high-growth firms and large, mature firms, and they can be linked further to patent and citation data. It is important as well to link the data on firm dynamics to those on innovation outputs, such as patent and citation data. In the near term, NCSES could begin to match up its BRDIS data to datasets at the Census and BLS to create indicators of firm dynamism. Haltiwanger proposed that indicators track dynamics by geography, industry, business size, and business age. Hollanders noted that European countries and other OECD members are continuing to fine-tune their measures of firm dynamism. NCSES's indicators on this dimension could further the international comparability of its STI indicators. Beginning to build the foundations on firm dynamics using BRDIS and other datasets would give NCSES a productive platform for developing several STI indicators that are policy relevant. There are many other avenues that NCSES could take to developing a complete suite of indicators on innovation. In the panel's final report, we will offer specific recommendations on how NCSES could develop statistics on expenditures on intangible assets and life lengths (as suggested by Haskel). For many of these measures, research is still needed to determine proper weighting methods and deflators and how to structure questionnaires (or modules to be placed on existing questionnaires)."}, {"section_title": "RECOMMENDATIONS RECOMMENDATION 2: The National Center for Science and Engineering Statistics should develop new indicators on innovation, based on data from its Business Research and Development and Innovation Survey (BRDIS). The agency should develop comparative statistics with the same cutoffs used by countries in the Organisation for Economic Co-operation and Development for its BRDIS data. RECOMMENDATION 3: The National Center for Science and Engineering Statistics should begin to match its Business Research and Development and Innovation Survey data to data from ongoing surveys at the U.S. Census Bureau and the Bureau of Labor Statistics to create indicators of firm dynamism. This is a necessary first step for developing data linkages that yield measures of activities by high-growth firms, and on births and deaths of businesses linked to innovation", "text": "outputs. These measures should be established by geographic and industry sectors and by business size and business age. Such measures would be an important step in furthering international comparability on innovation indicators. NCSES should conduct its own sensitivity analysis to fine tune meaningful age categories of highgrowth firms. This includes (a) activities aimed at acquiring new knowledge or understanding without specific immediate commercial applications or uses (basic research); (b) activities aimed at solving a specific problem or meeting a specific commercial objective (applied research); and (c) systematic use of research and practical experience to produce new or significantly improved goods, services, or processes (development)."}, {"section_title": "Measuring Research and Development Services", "text": ""}, {"section_title": "The term research and development does NOT include expenditures for:", "text": "\uf0b7 costs for routine product testing, quality control, and technical services unless they are an integral part of an R&D project; \uf0b7 market research; \uf0b7 efficiency surveys or management studies; \uf0b7 literary, artistic, or historical projects, such as films, music, or books and other publications; and \uf0b7 prospecting or exploration for natural resources. One important part of R&D is R&D services, which are services for the performance of R&D by one organization for another. R&D services are for the most part provided by companies and organizations in biotechnology; contract research (including physical, engineering, and life sciences firms); and professional, scientific, and technical areas (including social sciences and humanities). These are companies or organizations categorized under the North American Industrial Classification System (NAICS) code 5417 (scientific research and development services). Specifying specific NAICS codes for R&D services (as does BRDIS) is important, since firms in almost any industry can buy or sell R&D services. For example, Boeing can buy services to fill a gap in its R&D program for wing design; Wal-Mart can sell its knowledge, based on R&D, on supply chains: extraction firms can sell or buy R&D services related to extraction."}, {"section_title": "CURRENT AND NEEDED DATA", "text": "Currently, R&D services (sometimes called knowledge-intensive services) and foreign direct investment of R&D performed by affiliates are published in the biennial report of the National Science Board, Science and Engineering Indicators (SEI). For these reports, IHS Global Insight provides the tabulations on the trade balance in commercial knowledge-intensive services, while the Bureau of Economic Analysis (BEA) provides the data on international trade in research, development, and testing services. It would be useful if NCSES could include questions on its 2011 BRDIS that allow the development of comparable estimates to those that were historically developed by other agencies. The surveys on international transactions done by BEA and R&D surveys done by NCSES follow different guidance: BEA follows the International Monetary Fund (IMF) (2011) Balance of Payments (BOP) Manual and NCES follows the Frascati Manual (Organisation for Economic Co-operation and Development, 2002). However, the two approaches are not far apart. The BOP Manual includes some R&D and intellectual property elements that are consistent with the Frascati Manual. Therefore, geographic and ownership scope of the BEA's international transactions surveys and BRDIS are conceptually close. For example, BEA's international transactions surveys cover any company with activities in the United States, regardless of ownership. Transaction surveys cover transaction of U.S.-located units of foreign multinational enterprises with entities outside the United States, including transactions with their own foreign parents, and affiliated and unaffiliated trade. Similarly, for the United States, the survey covers affiliated and unaffiliated trade and transactions by purely domestic companies (no relationship with any multinational enterprise). BRDIS also covers any company with activities in the United States, regardless of ownership, and foreign affiliates of U.S. multinational enterprises. The 2008U.S. multinational enterprises. The , 2009, and 2010 BRDIS did not allow NCSES to collect all of the elements described above, but the 2011 questionnaire 2 is expected to be more comprehensive in this dimensioncollecting data on R&D production, funding, and transactions. In its surveys, BRDIS treats the foreign parent differently from both BEA's trade surveys and BEA's surveys of foreign direct investment. Other differences between BRDIS and BEA data on international balance of payments in R&D trade include: BEA's testing services, which are part of the research, development, and testing measure may include R&D and non-R&D components; and R&D, which is treated basically by NCSES as a cost measure, while transactions are treated closer to market values. Moris (2009, p. 184) suggests a matrix to parse out data from trade surveys and R&D surveys (including BRDIS). The panel believes that NCSES can provide these estimates and, if necessary, include appropriate questions on BRDIS in 2011 and subsequent years. Data would be available to produce statistics on payments and receipts for R&D services involving U.S. company affiliates at home and abroad, and how those data differ, if at all, from the BEA measures. Similar information on foreign company affiliates from other sources could be used for parallel comparisons. 3 NCSES could consider developing two series, payments and receipts of R&D services, for three to five leading countries. The resulting statistics would show what knowledge creation is outsourced and which countries are buying U.S. knowledge. This information would enable users to track trends over time and have a better understanding of the knowledge flows and R&D network formations. Over time, an interesting result of this exercise would be answers to a range of questions: What is the United States learning from other countries and what are other countries learning from the United States? In what technological areas are other countries accelerating development, using knowledge sourced in the United States? The data could also be used in regression analysis to answer another important question: What impact does the international flow of R&D have on U.S. economic performance? Users of the data on international flows of R&D services are likely to be interested to see how emerging economies are advancing in R&D capacity, in what fields are U.S. companies sourcing or outsourcing R&D and if it is increasingly sourced or outsourced in specific countries, and which countries 5-10 years from now-may be the hub of new scientific knowledge-possibly countries in Latin America, Sub-Saharan Africa or the Middle East. Another topic of interest is foreign direct investment of R&D. Recent data (Barefoot and Mataloni, 2011, especially Table 7) show that U.S. multinational corporations increased their share of R&D overseas from 14 percent in 1999 to 18 percent in 2009 and increased total employment overseas by 2 million while cutting U.S. employment by 0.8 million. The rational given for this shift in R&D performance outside the United States is that companies want to do R&D in the markets they serve. Statistics on trade in R&D services and the location of affiliated R&D performance together give a comprehensive outlook on sources of new knowledge."}, {"section_title": "SUMMARY AND RECOMMENDATION", "text": "BRDIS has data on domestic and foreign activities of firms and can provide a much more elaborate picture of R&D activities than has previously been possible or has been fully exploited. Specifically, BRDIS offers more information on R&D service production and flows, in the United States and in U.S. firms abroad, than has been published. Understanding R&D services is particularly important because the developed economies are dominated by service industries. BRDIS data can also support measures of payments and receipts for R&D services abroad, by leading countries, which is critically important for policy purposes. NCSES is expanding data linking activities to match BRDIS microdata with BEA survey data on U.S. foreign direct investment microdata. The agency also has fruitful interagency collaboration with BEA to integrate R&D into the system of national accounts. In its final report, the panel expects to offer specific recommendations on how to use these collaborations to produce new and better STI indicators."}, {"section_title": "RECOMMENDATION 4: The National Center for Science and Engineering Statistics should more fully use data from its Business Research and Development and Innovation Survey to provide indicators on payments and receipts for R&D services between the United States and other countries.", "text": ""}, {"section_title": "Developing Subnational Datasets and Indicators", "text": "The panel uses the term \"subnational\" to refer to state and finer geographical levels, including metropolitan areas. The National Center for Science and Engineering Statistics (NCSES) publishes state statistics on science and technology education and workforce inputs, financial research and development (R&D) inputs, R&D outputs, and the relationship of science and technology indicators to general economic indicators. Indeed, NCSES has a new interactive online tool for the Science and Engineering Indicators 2012 state data. However, statistics at geographic levels finer than states are often not published. The omission of subnational data is important because technology transfer between universities and firms (and venture capital investments) can occur in immediate proximity, or crossing state and even national boundaries. In the knowledge economy, political boundaries, rivers, and mountain ranges are not likely to determine the location and transfer of innovation activities, while colleges, universities, local enterprise zones, and venture capitalists are quite likely to determine where \"hot spots\" of innovation are located and transfers among them."}, {"section_title": "USERS' PERSPECTIVES", "text": "Users want more disaggregated STI information on multiple levels. They want STI comparisons across U.S. regions and between U.S. and foreign regions. Also, for some smaller countries, such as Finland or Sweden, comparisons to U.S. regions may be more appropriate. The panel's workshop yielded a plethora of subnational STI indicators that users said would be helpful. Participants mentioned a very wide range of information: \uf0b7 state, county, and metropolitan tables of data from the Business R&D and Innovation Survey (BRDIS) (covering R&D performance, workforce, and intellectual property); \uf0b7 degrees granted in science, technology, engineering, and mathematics (STEM) (production and migration); \uf0b7 academic R&D expenditures; \uf0b7 federal R&D expenditures; \uf0b7 total R&D (from a resurrected nonprofit R&D survey); \uf0b7 STEM jobs (Occupational Employment Statistics from the Bureau of Labor Statistics [BLS]); \uf0b7 STEM workforce migration (data on Local Employment Dynamics from the Census Bureau); \uf0b7 patent applications, grants, and citations (from the U.S. Patent and Trademark Office); \uf0b7 STI equity investments (from various sources); \uf0b7 STEM occupational projections (from BLS and the Employment and Training Administration [ETA]); \uf0b7 STEM occupation classification (from ETA); \uf0b7 STEM graduate and workforce migration (National Center for Education Statistics, from the Census Bureau and BLS); firm innovation processes (from the Economic Research Service [ERS] at the U.S. Department of Agriculture [USDA]); \uf0b7 propensity to innovate ratings; \uf0b7 mappings of entrepreneurial density; \uf0b7 industry support for R&D in universities; \uf0b7 firm births, mergers and acquisitions, deaths (\"business dynamics\" as characterized by Haltiwanger in the July 2011 workshop, including geography, industry, business size, business age); \uf0b7 venture capital investments; \uf0b7 state and federal grants and loans (STAR METRICS); 1 \uf0b7 initial public offerings; \uf0b7 new products (from Thomasnet.com); \uf0b7 drug and other approvals (from the Federal Drug Administration); \uf0b7 data on dealmakers and entrepreneurs, including number of connections among dealmakers and entrepreneurs); and \uf0b7 data on emerging industries, based on universities, government laboratories, firms, value chains, key occupations, and individuals. This long list clearly includes much that would be very difficult or expensive to collect, and would require consultation with state institutions, which in turn would require recognition that states drive industrial and trade policies. New sources for subnational data need to be considered. For instance, the Association of Public and Land Grant Universities (APLU)-funded by the National Science Foundation and the U.S. Department of Commerce-has developed a template for new measures of university contributions to regional economies under its Commission on Innovation, Competitiveness, and Economic Prosperity (CICEP). The goal of CICEP is \"to create a resource for universities to better measure and describe the broad range of their contributions to regional (and national) innovation and economic growth\" (Association for Public and Land Grant University, 2011, p. 1). At the panel's workshop, David Winwood and Robert Samors presented an overview of CICEP. They noted that there are dozens of measures that APLU would like universities and other organizations to collect in a range of categories: material transfer agreements; consortia agreements; sponsored research by industry; clinical trials; service to external clients; student employment on funded projects; student economic engagement; student entrepreneurship; alumni in the workforce; incubation and acceleration program success; relationships between clients/program participants and host university; ability to attract external investment. The U.S. Department of Agriculture's Economic Research Service is planning to conduct an establishment survey on innovation activities in rural areas. This survey will be based on the European Union's Community Innovation Survey. Researchers funded by the National Science Foundation, though the Science of Science and Innovation Policy program have also developed datasets that can be used to create subnational STI indicators. Some of those data are already available for public use. These and other issues are knotty problems that NCSES will have to navigate as it endeavors to create meaningful subnational STI indicators. It spite of the problems, however, the panel believes that the value of STI indicators at the state level and for regions where there are relatively high levels of science, technology, and innovation activities would contribute information needed by policy makers."}, {"section_title": "INDICATORS FROM FRONTIER TOOLS", "text": "At the panel's workshop, presentations by Erik Brynjolfsson (Massachusetts Institute of Technology), Lee Giles (Pennsylvania State University), Carl Bergstrom (University of Washington), and Richard Price (Academia.edu) provided insights about tools that can be used to give up-to-data information of science and engineering networks and linkages of human capital investments to STI outcomes and impacts. These experts showed panel members how to use nowcasting, netometrics, CiteSeerX, Eigenfactor, and Academia.edu (similar to Lattes in Brazil) to create scientometric 2 data to create STI \"talent\" indicators. Such tools can be used, say, to track intangible assets and knowledge flows from online recruiting sites and social networks."}, {"section_title": "Web Scraping", "text": "In addition to improving survey methods and using administrative records databases directly, another potential avenue for acquiring data is web scraping, that is, collecting data publicly available on the web. This approach is distinct from web-based survey methods, which use the web to administer a survey. For example, many job seekers now publish their r\u00e9sum\u00e9s online; students participate in social networks; and researchers use online collaboration tools and working paper repositories. Each of these kinds of web sites provides information about the population using them. Hence, there are two specific questions that could be addressed using web data: (1) How many engineers are working in the United States (or what fraction of the workforce is made up of engineers)? (2) How many undergraduate students are majoring in mathematics in the United States? In this section of the report, we explore how current questions could be addressed with new data sources. Many of these sources also incorporate social networks, which may allow the development of entirely new types of indicators. However, we note that it is possible that many of the questions that web-based data sources could address may be more efficiently addressed with administrative records. This is a matter for further research. Some web-scraping projects-for example Google's search engine-use an ad hoc approach to collecting data, examining every web resource that they can access for relevant information. This approach could also be useful for gathering STI statistics. However, a large portion of STI deals with the composition of the labor force and students, and information related to them is centralized in several large websites, rather than being distributed across individual home pages. There are at least three examples of these sites and the kind of information they could provide: \uf0b7 Facebook, Google+: number of students at a university, how many major in which fields; \uf0b7 Mendeley, Academia.edu, CiteULike: how many researchers are active in which fields, how many collaborations, who collaborates with whom, how useful is a given piece of research; \uf0b7 LinkedIn, Monster.com, Zerply: the composition of the labor force, geographic breakdown, skill sets, and similar information. One can collect data from a site such as Monster.com either by scraping information from the public website or by negotiating with the site for access to the data. Two reasons for preferring negotiation are legality and data structure. The legality of web scraping has been challenged several times in courts, both in the United States and abroad 3 and there does not appear to be a consensus about what is legal. However, all the cases to date that the panel found involved one for-profit company scraping data from another for-profit company's site for commercial use. For example, a travel company might use web scraping to collect information on ticket prices from an airline and then use those prices to make it easy for customers to do comparative shopping. During the course of this study, the panel has not found an example of a nonprofit or governmental organization or academic researcher being sued over to web scraping. The goal of web scraping is to take semistructured data from a public web page and register it into a structured database. The major search engines have started to collaborate on mechanisms for structuring data on the web. 4 The National Science Foundation (NSF) could consider participating or encouraging participation in the development of schema for structuring data relevant to indicators, such as adding fields for educational background to the Person schema or defining fields for a journal article as a specialization of the CreativeWork schema. 5 A company such as Monster.com already has such a structured database. 6 Although this database is not public, some companies have supplied parts of their databases as part of academic collaborations, and they may be willing to work with NCSES. If the data come directly from a company, there is no chance of introducing errors during the web scraping process. On the other hand, the company may not be willing to supply sufficient data for the agency's or may not be willing to supply it in a timely manner. An advantage of web scraping is that it could be carried out continuously, and it does not require cooperation with the company. Alternatively, a middle ground would be to work with companies on structured feeds or digests of information that would be updated continuously. Companies may well prefer this to being scraped, which can require significant server resources. The National Institutes of Health, for example, makes XML versions of its grants database available and provides ongoing updates. Many researchers today mine social networks for data (without any legal consequences, as noted above). There are three basic questions for identifying sources of data: 1. What STI topic areas does a particular company address? 2. Is the company willing to provide data directly? 3. How frequently is the company willing to update data? A fundamental question requires more examination: What kind of statistical methodology to apply to data from web scraping? There are other, related questions: What are the tradeoffs with using web-based data sources instead of survey data? Is it possible to adjust web-based data to represent a survey sample or to estimate errors? Is it possible to use a traditional survey to calibrate web-based data? How frequently must this be done? How frequently would NCSES want to publish new statistics? Would NCSES want to publish less reliable statistics if it means publishing them more frequently at lower cost? A company such as LinkedIn stores in its servers a social network representing all of its users and relationships between them, and techniques for accurately sampling this social network have been developed (see Maiya and Berger-Wolf, 2011;Mislove et al., 2007). However, to our knowledge, researchers have not yet addressed how well this social network represents the larger population. For example, if one is interested in measuring how many chemical engineers are working in the United States, some subset of these are represented in LinkedIn's social network, but it is unclear how to adjust this subset accurately to represent the target population or how to estimate the error incurred in using this subset. 7 It is important to understand how the data collected from websites compares with traditional survey data, particularly because different websites have very different coverage. Facebook, for example, covers a very large portion of the undergraduate population (at least for the next couple of years). However, sites such as Mendeley and Academia.edu definitely do not cover the entire population of researchers."}, {"section_title": "Combination Approaches", "text": "It could prove useful to adopt combination approach, in which web-based statistics are periodically calibrated against a traditional survey. Of course the survey would have to be administered less frequently than currently or there would be no cost or time savings. Since NCSES has reported that the response rates of some of their surveys are declining, questions arise about how well those data reflect the population sampled and how to calibrate web-based data to those surveys. It is relatively straightforward to calibrate to the Survey of Earned Doctorates (SED), which has a 100 percent response rate, but only once and on questions that the SED asks. One solution to this dilemma would be for NCSES to putting resources into getting close to a 100 percent response from a small number of people from a standard survey and use that to calibrate information from web-based sources or the rest of the survey. The calibration is similar to what computer scientists and mathematicians do with compressed sensing of data on pixels and is a very interesting and exciting area of research. It may not yet be possible to achieve rigor comparable to a traditional survey with these methods, and NCSES will need to consider what its tolerance is for publishing statistics that may not be as reliable as those they have previously published. In such consideration, NCSES will need to balance reliability against timeliness: since little time is required for data collection with data mining techniques in comparison with traditional surveys, releasing statistics on a much more frequent basis is possible. In principle, nothing prevents statistics from being periodically or continuously updated. For example, the national unemployment rate, gross domestic product, and consumer price index are periodically updated without diluting the measure's importance. The Billion Prices Project at the Massachusetts Institute of Technology uses an algorithm that collects prices daily from hundreds of online retailers worldwide, creating, among other things, a daily price index for the United States. 8 7 LinkedIn and similar data could be quite useful for questions involving relative rather than absolute measures. For example, are there more chemical than electrical engineers? Do chemical engineers change jobs more frequently than other engineers? Where in the country are chemical engineers most highly concentrated? 8 See http://bpp.mit.edu/ [January 2012]."}, {"section_title": "Developing Ideas Through Contests", "text": "One way to develop these ideas further would be through a contest or research funding or prize competition. There are several \"open innovation\" organizations that facilitate this type of contest, such as InnoCentive, the Innovation Exchange, and challenge.gov. Working with an outside entity to design and administer a contest would allow NCSES to focus on the problems it hopes to address rather than the implementation details of the contest. A National Research Council (2007) report, \"Innovation Inducement Prizes at the National Science Foundation,\" and the National Science Foundation's new Innovation Corps Program could also serve as useful models, although these resources are focused more specifically on technology commercialization. If the contest is designed to address the statistical questions around the usefulness of webbased data sources, it will be necessary to supply some sample data, and this might affect negotiations with companies. For example, LinkedIn might be willing to supply its data for NCSES to use but unwilling to allow its use in a public contest. How can a federal statistical agency develop and rely on web-based and scientometric tools to produce gold-standard data for periodic publication? This is a basic question that needs to be considered in the current climate of rapidly changing technologies and increasing demands for data. There are a raft of related questions, including: How can an agency overcome looming privacy and security issues? How many personnel internal to the agency will it take to develop and operate the tools to produce the indicators? These are all good questions that will need to be fully addressed before NCSES or any other federal statistical agency implements the frontier methods described in this section. One way to address these questions is by example. In 2011, the National Institutes of Health (NIH) decided to sponsor a competition 9 to find improved methods of using the National Library of Medicine (NLM) to show knowledge flows from scientific exploration through to commercialized products. The agency also wanted to use the NLM resource for taxonomic development and to show relationships between research activities. Knowledge spillovers and effects are difficult to measure. NIH determined that one way to mine millions of data entries was to automate the process. Yet, that was not the expertise of any specific department at NIH, and it was important to cast a broad net to get the best expertise addressing the problem. The competition was announced on challenge.gov and was titled: The NLM Show Off Your Apps: Innovative uses of NLM Information Challenge. The competition was open to individuals, teams of individuals, and organizations and its purpose was to \"develop innovative software applications to further NLM's mission of aiding the dissemination and exchange of scientific and other information pertinent to medicine and public health.\" 10 The competition ended August 31, 2011, and winners were announced on November 2. 11 "}, {"section_title": "A Note of Caution", "text": "On a cautionary note, Boyd and Crawford (2011) assert that \"[t]he era of Big Data has begun\" and \"it is imperative that we begin asking critical questions about what all this data means who gets access to it, how it is deployed, and to what ends.\" 12 While mining data at the project or individual level may yield valuable results, it is also the case that archival data from some sources are poor or nonexistent, and Boyd and Crawford (2011) also noted: \"There is a risk in an era of Big Data of treating every connection as equivalent to every other connection, of assuming frequency of contact is equivalent to strength of relationship, and of believing that an absence of connection indicates a relationships should be made.\" This is a very important point. NCSES will have to proceed with caution as it considers integration of frontier tools and datasets into its indicators production processes."}, {"section_title": "MULTIMODAL DATA DEVELOPMENT", "text": "One issue that needs to be explored is the feasibility of blending the use of administrative records, scientometric tools, and survey techniques to produce more accurate data on STI human capital measures and other indicators that NCSES produces, such as R&D input and performance measures. A multimodal approach would help to create longitudinal series using existing and new information. In the near term, the topic could be explored though a workshop specifically designed to discuss the conceptual framework and feasibility of blending data acquisition techniques and using this mixed-methods approach to develop new indicators. 13 This approach could be useful for developing real-time maps of networked scholars, while measuring return on investments from federal research funds as they are used and linking them to outputs (paper and patents). At the same time, this approach would periodically assemble basic data on education, employment, work activities, and demographic characteristics. We must stress, however, that it would be prudent to test the approach on data that are already well developed at NCSES. 14"}, {"section_title": "STI DATA LINKAGE AND COORDINATION", "text": "The panel discovered that there appear to be multiple agencies collecting information about innovation, including: NCSES; the Bureau of Labor Statistics (BLS) in the U.S. Department of Labor; the Census Bureau, the Bureau of Economic Analysis (BEA), and the National Institute of Standard and Technology in the U.S. Department of Commerce; and the 12 For other important references on the use of visual analytics tools to answer science policy questions, see Zucker and Darby (2011) and Thomas and Mohrman (2011). 13 Statistical Neerlandica has prepublication views of a series of articles on the use of administrative records for analytical purposes, including regression analysis: see http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1467-9574/earlyview [December 2011]. For theoretical foundations on how to combine information from multiple sources data, see Molitor et al. (2009). 14 In a recent article, Roach and Cohen (2011) use citation-based and survey-based approaches to obtain measures of knowledge flows. They find overall \"close correspondence between citation-based measures of knowledge flows and [the] survey-based measure at the industry level.\" However, when they control for industry fixed effects, the correlation between the two sets of data \"drop by approximately three quarters.\" Roach and Cohen acknowledge that there is measurement error in their survey data and the patent data that they use for comparison. They and other researchers are attempting to determine the nature of such errors to improve the reliability of proximate measurements of knowledge flows. One conclusion to be drawn here, though, is that developing indicators using different techniques will give users the relevant range for the measures that seek to use. U.S. Department of Agriculture. Indeed, if the subject is broadened to STI, we were told that at least 5 agencies collect these data. This suggests a need for an entity to assume a coordinating function, to ensure that STI data collection and reporting efforts across the government are efficiently distributed, to eliminate duplication, to take advantage of potential synergies, and to ensure high quality of the data and statistics. Such an entity could be an interagency council or a working group of agency representatives. The NCSES could take on an important role in such a council or working group, particularly given its function as a data clearinghouse with respect to STI data and statistics. The panel was told by staff in several agencies that it could be beneficial to the statistical system if NCSES became an enhanced data aggregator and data curator for STI-related information. NCSES could explore such a role through the creation of an interagency council on STI statistics. Such a council could identify and address STI statistical issues and opportunities among other statistical agencies. It could not only serve as a clearinghouse, but it could also initiate and monitor activities that serve the user community. Both the Office of Management and Budget and the Office of Science and Technology Policy could have roles in such a council. Better integration of data sources is needed to develop more robust STI indicators. John Haltiwanger (University of Maryland) suggests that infrastructure datasets could be fully integrated to track the career histories of innovators and entrepreneurs and track the relationships between startup/young/high-growth firms and large, mature firms. These infrastructure datasets could be fully integrated with all existing Census Bureau business surveys and other data: for example, one could integrate economic censuses and annual surveys to measure productivity, business practices, and R&D, linked to patent, citation, and other information about innovators from the U.S. Patent and Trademark Office. It will be important to integrate any new STI indicators that are developed into the existing infrastructure (if not at the person/business level, then at some level of disaggregation). Data sharing and synchronization would permit even richer integration of BLS and BEA firm data. At the panel's workshop, Matthieu Delescluse (European Commission) also remarked that the European Union (EU) is commissioning the linking of patent data with company databases to develop new indicators. For example, the relationship between small and medium firms and number of patents can be tracked over time. The EU is also using data from the Community Innovation Survey Business Register to determine the international sourcing of R&D. This statistic could also be developed in the United States by linking Census Bureau and BEA data. Employment dynamics, including worker mobility trends in science and engineering occupations, could be developed by linking Census Bureau, BLS, and BEA data. Existing research data centers or data enclaves could facilitate platforms for data integration and potentially data comparability with other nations that also follow similar data administration policies. 15 NCSES already has the infrastructure at the National Opinion Research Center (NORC) to house many of its survey data and allow licensed researchers access through remote dedicated computers. Data that will be available by the beginning of 2012 will come from the following surveys: Survey of Earned Doctorates (SED), National Survey of Recent College Graduates (NSRCG), Survey of Doctorate Recipients (SDR), and the Scientists and Engineers Statistical Data System (SESTAT) integrated database (which includes NSRCG, SDR, and the National Survey of College Graduates [NSCG]). The panel heard from several people that NCSES sees the NORC data enclave as a way to build its community of licensed researchers while enabling its own staff to spend more time in helping researchers with the substance of the data rather than paperwork. Additionally, NCSES has worked with NORC to build an infrastructure that allows research teams to share their work in a secure environment, whether they are physically in one location or not. There is strong interest in the dynamics of firm demographics, births, deaths, employment contributions by firms, and the role of the high-growth firm. These statistics can be developed by the Census Bureau by analyzing its business register. If these data are available to researchers-say, at the NORC data enclave-a broad spectrum on evidence-based statistics and indicators could be made publicly available. One means by which such a program could begin is through the initiation of a research program by NCSES. Such a program would energize the research community to use survey and other data as soon as the data arrive at the NORC data enclave. It could also be designed to incentivize researchers to develop new, high-utility statistics that are based on linked data from several agencies and that relate inputs to outputs, outcomes, and effects. NCSES strives to be the central repository in the federal government of data, knowledge, and expertise in all STEM (science, technology, engineering, and mathematics) topic areas, as it is so identified in America COMPETES Reauthorization Act of 2010. Acting as such a repository will involve obtaining data from other federal agencies, as well as from intergovernmental and private sources. The focus would be on STEM aspects that are the official responsibility of NCSES. Although NCSES staff may obtain access to BRDIS data directly through the Survey Sponsor Data Center that was scheduled to come online December 30, 2011, researchers will not have access to these data. Linking data from the BRDIS and the Human Resource and Education Development survey with administrative longitudinal data would provide a rich dataset for linking knowledge inputs to outcomes. This would provide a longitudinal component to BRDIS analysis. NCSES could explore avenues for developing this type of longitudinal capability and the in-house analytical capacity to produce STI indicators from those data."}, {"section_title": "CONCLUSION AND RECOMMENDATION", "text": "Integration of the frontier tools (discussed above) into practice at NCSES would represent a paradigm shift for the agency, at a critical time when they are reaping benefits from investments in revised surveys during the past four to five years. Therefore, the panel recommends that NCSES in the near term undertake pilot work to determine how its indicators program can incorporate the new techniques with traditional survey methods. A logical extension of this is that NCSES begins to expand its role as a clearinghouse for STEM information and coordinating data interoperability, standards for metadata descriptions (e.g., participating in schema.org), and perhaps developing tools like open source web crawlers. "}, {"section_title": "Conclusion", "text": "Measuring capacity and change in science, technology, and innovation (STI) has a long history, dating back decades in economics and management research. Since the 1950s, under congressional mandate, the U.S. National Science Foundation (NSF) has produced measures of research and development, as well as education and occupational statistics specifically for science and engineering fields. At this time, when the knowledge economy is a key driver for several elements of social well-being (better health outcomes, higher-paying jobs, and higher productivity), it is critically important that identifiable elements of STI activities be measured more accurately, more reliably, and in a more timely fashion. This interim report from the Panel on Developing Science, Technology, and Innovation Indicators for the Future responds to a request from the NSF's National Center for Science and Engineering Statistics (NCSES) to examine the status of STI indicators that are currently developed and published by NCSES. NCSES expressed specific interest in the international scope and comparability of STI indicators, particularly since part of its revised mandate is to measure elements of competitiveness. The panel is focusing on the development of statistics that are balanced, policy relevant but policy neutral, and useful to decision makers at all levels, including federal agency administrators, members of the National Science Board, and members of Congress, as well as the general public. This interim report conveys six recommended actions that NCSES could take before the panel delivers its final recommendations, scheduled for December 2012. These recommendations suggest the development of new and revised indicators based on existing survey data and research and experimental work that can be done to develop new methods for obtaining data. The final report will give a complete set of prioritized recommendations to NCSES, along with commissioned papers on developing subnational STI indicators, more comprehensive measures of trade in research and development services, and a conceptual framework for measuring innovation activities. "}]