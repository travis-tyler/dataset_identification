[{"section_title": "Foreword", "text": "This report describes the methods and procedures used for the 1996 National Postsecondary Student Aid Study (NPSAS:96). NPSAS:96 included important changes from previous NPSAS surveys (conducted in 1987, 1990, and 1993) in its sample design and collection of data. For example, the current study is the first to employ a single-stage institutional sampling design and to select a subsample of students for telephone interviews. It is also the first in the NPSAS series to take full advantage of extant data maintained in government data files. Even so, sufficient comparability in survey design and instrumentation was maintained to ensure that important comparisons with past NPSAS studies could be made. We hope that the information provided in this report will be useful to a wide range of interested readers. We also hope that the results reported in the forthcoming descriptive summary reports will encourage use of the NPSAS:96 data. We welcome recommendations for improving the format, content, and approach, so that future methodology reports will be more informative and useful.  Tables   Table 2.1 Target numbers of sample students by institutional stratum and type of student 2-6 Table 2.2 Institutional sampling rates and number of certainty and non-certainty institutions sampled, by institutional stratum 2-9 Table 2.3 Distribution of NPSAS:96 institutional sample by OBE region 2-9 Table 2.4 Realized student sample, by institutional and student strata 2-12 Table 2.5 Phase 1 student interview subsampling 2-13 Table 2.6 Phase 2 student interview subsampling 2-14 Table 2.7 Numbers of students selected for parent interviewing 2-15 Table 2.8 Start and end dates for major NPSAS:96 activities 2-18 Table 2.9 Summary of NPSAS:96 evaluation approaches 2-32 Table 2.10 Integrated management system processes and purposes 2-35 Table 3.1 Overall institutional eligibility and enrollment list participation rates 3-2  1996-97 (SAR 96-97), by selected domains 3-5 Table 3.3 Institution-level rates for obtaining institutional record data (CADE), by selected classifications 3-8 Table 3.4 Student-level rates for obtaining institutional record data (CADE), by selected classifications 3-10 Table 3.5 Results of NSLDS matching attempt, by selected institutional and student classificatidM2 Table 3.6 Results of Pell grant file matching attempt, by selected institutional and student classifications 3-13 Table 3.7 Student interview response rates, by selected classifications 3-18 Table 3.8 Phase-2 student locating rates and interview response rates conditional on locating, by selected classifications 3-20 Table 3.9 Overall parent supplement interview rates, by selected classifications 3-23 Table 3.10 Overall study student yield rates 3-25 Table 3.11 Reliability reinterview results, by student and institution classifiers  List of Tables (continued)   Table 4.1 Enrollment list receipt, by month and institutional calendar system 4-2 Table 4.2 Institutional NPSAS:96 enrollment list participation, by prior NPSAS participation 4-4 Table 4.3 Institutional participation in NPSAS by Carnegie classification category and year of stut155 Table 4. 4 NPSAS participation of historically black colleges and universities (HBCUs) 4-6 Table 4.5 Types of student lists provided by institutions, by highest level of offering 4-6 Table 4.6 Types of discrepancies encountered with student lists, by highest level of offering 4-8 Table 4.7 Extent of \"missed\" CPS matches by student type 4-10 Table 4.8 Extent and nature of CADE agreement with CPS EFC 4-12 Table 4.9 Nature and source of data elements preloaded into CADE 4-13 Table 4.10 CADE data elements with highest indeterminacy rate 4-16 Table 4.11 Institutional original and final choices of record abstraction method 4-17 Table 4.12 CADE duration statistics by institution type 4-20 Table 4.13 Percentage of students requiring intensive tracing procedures 4-23 Table 4.14 Average minutes to complete NPSAS:96 student telephone interview by section and student type 4-25 Table 4.15 Average minutes to complete the NPSAS:96 parent telephone interview, by section 4-26 Table 4.16 Number and result of calls made to sample members by type of institution and type of students 4-28 Table 4.17 FTB determination by student classification 4-31 Table 4.18 FTB determination for those identified by NPSAS institution as potential FIBS, by institution type 4-32 Table 4.19 FTB determination for those not identified by NPSAS institution as potential FTBs, by institution type 4-33 Table 4.20 Known FTBs by type of FTB and whether NPSAS institution was first institution attended 4-34 Table 4.21 Success rates for CATI on-line coding procedures 4-38 Table 4.22 Student interview item non-response for items with more than 10 percent \"don't know\" or \"refused\" 4-40 List of Tables (continued)   Table 5.1 Record counts from NPSAS: 96 data sources 5-2 Table 5.2 Description of missing data codes 5-5 Table 5.3 The statistically imputed variables and the amount of data imputed 5-7 Table 6.1 Weight adjustment factors for institution nonresponse 6-5 Table 6.2 Weight adjustment factors for unknown student eligibility status and for insufficient student CADE/CPS data 6-8 Table 6.3 Average sampling rates for selection into phase one and phase two of CATI 6-10 Table 6.4 Average weight adjustment factors from logistic model used to adjust CATI weights for student location nonresponse 6-13 Table 6.5 Average weight adjustment factors from logistic model used to adjust CATI weights for student interview nonresponse 6-15         Chapter 1 Introduction, Background, and Objectives NPSAS is a comprehensive nationwide study to determine how students and their families pay for postsecondary education, and to describe some demographic and other characteristics of those enrolled. The study is based on a nationally representative sample of all students enrolled in postsecondary education institutions, including undergraduate, graduate, and first-professional students. Students attending all types and levels of institutions are represented in the samples, including public and private for-profit and not-for-profit institutions, and from less-than-2-year institutions to 4-year colleges and universities. The study is designed to address the policy questions resulting from the rapid growth of financial aid programs and the succession of changes in financial aid program policies since 1986. The first study (NPSAS:87) was conducted during the 1986-87 school year; subsequently, NPSAS has been conducted triennially as NPSAS:90, NPSAS:93, and the current NPSAS:96. Each study is designed to cover the July 1 through June 30 financial aid award year. 'Research Triangle Institute, National Postsecondary Student Aid Study: 1996--Field Test Methodology Report, (Working Paper No. 96-17). National Center for Education Statistics: Washington, DC;Author;July 1996. NPSAS:96 Methodology Report 1-1 I CHAPTER 1: INTRODUCTION, BACKGROUND, AND OBJECTIVES A main objective of the study is to produce reliable national estimates of characteristics related to financial aid for postsecondary students. The data are part of the NCES comprehensive information on student financial aid and other characteristics of those enrolled in postsecondary education. The data focus on three general questions with important policy implications for financial aid programs: How do students and their families finance postsecondary education? How does the process of financial aid work, in terms of both who applies and who receives aid? What are the effects of financial aid on students and their families? The first and third questions address the basic purpose of financial aid and provide one measure of the success of financial aid programs, including the underlying strategies of students and families in financing postsecondary education (e.g., Do students avoid socially desirable career paths because of the need to repay higher levels of debt?) The Beginning Postsecondary Students and Baccalaureate and Beyond longitudinal studies, for which NPSAS provides base year information, also address the third question. The second question addresses the actual implementation of student aid programs. The implementation of Federal financial aid programs is greatly influenced by decisions and practices of school financial aid offices and banks which have the primary responsibility for providing information to students and awarding the various types of aid; consequently, information is needed regarding the types and amounts of aid awards being made by institutions."}, {"section_title": "METHODOLOGICAL ISSUES", "text": "As described subsequently in Chapter 2, the NPSAS survey design is both large and complex. Data are collected from a very large and diverse set of respondents; over 950 postsecondary institutions, 50,000 students, and 8,800 parents were selected for participation in NPSAS:96. A major methodological concern underlying NPSAS is designing a data collection system that has the flexibility to gather comprehensive financial data from the most appropriate source and concurrently provide some assurance of comparability in data collection for each element. Of the potential sources for NPSAS data--government data files, institutions, students, or parents--none alone can provide a complete and accurate summary of postsecondary education financing. Financial aid offices maintain accurate records of certain types of financial aid at that institution, but these records are not necessarily inclusive of all support and assistance. Such records may not contain financial aid provided at other institutions attended by the student and they cannot provide detailed information on sources of aid and educational financing other than those recorded by that financial aid office. (Two notable exceptions that are not maintained in many financial aid offices are employee benefits and graduate teaching or research assistantships.) Students and their parents are more likely than institutions to have a comprehensive picture of education financing, but may not have accurate memory or records of exact amounts and sources, which they may have earlier provided accurately to lending agencies Stages were: (1) geographic areas constructed from 3-digit postal Zip code areas; (2) institutions within sample areas; and (3) students within sample institutions. An area clustered design was necessary for NPSAS:87 because a complete institution frame was not available at that time, and the frame was supplemented with local sources. An area clustered design was used for the 1990 and 1993 studies to reduce the costs associated with sending field staff to sample institutions to abstract registration and financial aid data for sample students."}, {"section_title": "7", "text": "The field test sample of institutions was selected from those that were in neither of the institutional samples selected for the full-scale study, to insure that no institution would be burdened by participation in both surveys. 8 The relatively small cost differential between the two approaches results from greater use of two new procedures for collecting student financial aid information and other information from institutional records. The first of these procedures, introduced in the NPSAS:96 Field Test involved collecting, through EDI, SAR data from the ED central processing system (CPS) prior to any data collection at the involved institution. The second procedure, which was introduced in NPSAS:93, was the use of remote CADE, by institutional staff at most schools and by field staff only at the remainder, to obtain information from school records. with the three-stage design. Since a two-stage design can only improve precision and the cost penalty did not appear to be substantial, the two-stage design was fielded for the full-scale NPSAS:96. Comparisons of cost and precision estimates under the two sampling approaches are provided in Appendix A (Tables A.1 and A.2); greater detail on procedures and conclusions is provided in the NPSAS:96 field test report.9 An overview of the sequential statistical sampling process for NPSAS:96 is provided in Figure 2.1. The institutional sampling frame for NPSAS:96 was constructed from the  Integrated Postsecondary Education Data System (WEDS) Institutional Characteristics (IC) file. This data base is considered to provide nearly complete coverage of the institutions in the target population. Listings in the file that were not eligible institutions (e.g., institutions located outside the U.S. and Puerto Rico; central offices) were not considered. The remaining institutions were then partitioned into nine institutional strata based on institutional control and highest level of offering: The goal of all sampling activities was to attain sufficient numbers of eligible sample postsecondary students (within specified student and institution types). An important domain of the student sample was the set of students identified as FTBs,\" who would comprise the baseline cohort for the BPS longitudinal study. The desired number of sample students was determined by accounting for expected (from prior NPSAS studies) rates of ineligibility among sample students and rates of FTB misclassification, (as determined from NPSAS:90 and the NPSAS:96 field test). Since the student samples were selected on a flow basis as sample institutions provided their enrollment lists (in order to meet the data collection schedule), the students were sampled at fixed rates. Under this approach, the actual numbers of students sampled are random variables; however, the sampling rates were set to meet or exceed the sample sizes shown in Table 2.1. aEntries do not sum to total because a small sample of graduate and first-professional students was expected from private, for-profit institutions. tNot applicable. t3Students who began their postsecondary education during the NPSAS year. NPSAS:96 METHODOLOGY REPORT 2-6 2 B7,47 COPY AWALZ3LE CHAPTER 2: DESIGN AND METHOD OF NPSAS:96 The NPSAS:96 sample was also designed to achieve at least 30 student CATI respondents from each sample institution that had at least that many eligible students enrolled during the NPSAS year.' Consequently, institution sample sizes were determined to achieve an average of approximately 50 or more sample students per institution within each institutional stratum. Given these student sample size goals, it was determined that the survey should be based on approximately 850 participating institutions. Based on institutional eligibility and participation rates obtained in prior NPSAS implementations and the NPSAS:96 field test, an initial sample of 973 institutions was selected to yield at least the targeted 850 eligible and participating schools.\nThe NPSAS:93 lesson was clear in indicating that waiting to perform quality control on CADE data until after receiving them back in-house was too late. Therefore, when a user indicated a subsection was complete, the NPSAS:96 CADE software looked for missing data in specific fields; if missing data was discovered, the user was prompted to provide the missing information. 8While the CADE system was self-directing and contained a number of checks to ensure proper installation and use, training institutional staff on the proper use of the system was still needed. This was accomplished through a CADE Users' Manual and an embedded tutorial; also, a hot-line number was established by the contractor to address specific questions as they arose. NPSAS:96 METHODOLOGY REPORT 4-14 CHAPTER 4: EVALUATION OF OPERATIONS AND DATA Initial and final institutional choices of student record abstracting method are shown in Table 4.11 by institutional level, control, and sector, as well as by institution size and sample size. The large majority of coordinators (79 percent) from the 836 participating institutions initially chose the first option (self-CADE); the remaining coordinators chose the contractor FDC abstracting (field-CADE). In the final analysis, the proportion of self-CADE institutions fell to 57 percent. A total of 199 institutions changed methods during the CADE operations; only five of these changed from an initial choice of field-CADE to a final choice of self-CADE. Typically, an institution's abstraction method preference changed after determining that the abstracting job was greater than originally imagined and/or that institutional staff did not have sufficient available time to accomplish the task within the desired time frame. The NPSAS:96 field test, as well as NPSAS:93 experience, suggested that institutions with very large student enrollment were much more likely to ultimately choose the field-CADE method. This is attributable, at least in part, (and verified by informal IC debriefings) to the size of the sample at the large institutions. Because student sample size is positively related to institution enrollment,9 the burden of record abstraction becomes greater with increased enrollment.' For NPSAS:96, a pattern in the percent of self-CADE institutions within institutional size categories is not as pronounced. However, a clear trend appears when examining the self-CADE percentage by sample size categories, demonstrating the \"burden effect\" with regard to switches from self-CADE to field-CADE. The relatively lower proportion of \"initially self-CADE\" institutions within the public 4year doctorate/first-professional institutions in part reflects an adjustment to the institution contacting procedures based on the field test data collection experience. For the full-scale study, 4-year institutions on a semester calendar system were targeted as optimal for early field-CADE assignments, and efforts were made to persuade such schools to accept an FDC. The rationale for this adjustment was based on the shorter average duration of field-CADE and the need to complete abstraction very early for a sufficient number of institutions in order to begin CATI data collection. 10The increase in burden at larger schools is related to other factors than simply increased student sample size; among other things, larger schools also are less likely to have all needed records for all sampled students in a central location.  Results are limited to cases meeting the definition of a CADE respondent, and include only those items which were applicable to and answered for at least 1000 students, and in which more than 5 percent of the responses were \"Data Not Available.\" This excludes cases for which the element was not applicable. 'Percentages are based on the number of applicable cases for the row under consideration; the numerator of the percentages is not provided.  Note: Statistics are presented for all participating institutions. Choices included: (1) \"Self CADE,' abstraction by the staff of the participating nstitution and (2) \"fie d-CADE,\" abstraction by contractor field interviewers. A total of 199 institutions changed choices during CADE operations, 194 changed to field-CADE and 5 changed to self-CADE.\nREST COPY AVAI1LABLE 6-9 CHAPTER 6: WEIGHTING AND VARIANCE ESTIMATION 10Phase Two CATI Sampling Weight (WT10) A subsample of the nonrespondents in phase one of CATI were selected for phase two of CATI. These students were subjected to intensive tracing and numerous callbacks to maximize the sample yield and increase the overall response rate. As in phase one, the students were sampled at a fixed rate on a flow basis with midstream adjustments to the fixed rates as institutions were processed. Students who had a firm appointment or had a completed parent interview at the conclusion of the sixth call were selected with certainty. It is at this point where the weight components for the Study weights and CATI weights are calculated separately because of the different response definitions. For the CATI weights, the average sampling rates were simply the number of students selected for phase two divided by the total number of CATI nonrespondents in phase one. For the Study weights, however, students who were selected for CATI and had complete CADE data were considered Study respondents regardless of their CATI outcome. Thus, the average sampling rates associated with the Study weights were calculated after excluding the students with complete CADE data. The last two columns of Table 6.3 present the average phase two sampling rates associated with the CATI weights and Study weights. The phase two sampling weights were then defined as the reciprocals of these average sampling rates. The students selected with certainty were assigned a sampling weight equal to one."}, {"section_title": "Institutional Sample", "text": "The target population for NPSAS:96 includes nearly all postsecondary institutions in the 50 states, the District of Columbia, and Puerto Rico. Specifically, to be eligible for NPSAS:96 an institution is required, during the 1995-96 academic year, to:15 offer an educational program designed for persons who have completed secondary education; and offer more than just correspondence courses; and offer at least one academic, occupational, or vocational program off study lasting at least 3 months or 300 clock hours; and offer courses that are open to more than the employees or members of the company or group (e.g., union) that administers the institution; and be located in the 50 states, the District of Columbia, or Puerto Rico; and be other than a U.S. Service Academy. 16 Institutions providing only avocational, recreational, or remedial courses or only in-house courses for their own employees are excluded. The student sample was allocated to the separate applicable institutional and student sampling strata, defined above. Student sampling rates (used to define institution measures of size) were defined based on 1993-94 IPEDS IC file counts, a modeling procedure to partition FTBs and other undergraduates, and the required sample sizes (see Appendix A for details). I4This was to allow NCES to send each participating institution a report using with their students without violating confidentiality requirements. 15 The listed eligibility requirements are consistent with those used in previous 16r., these academies are not eligible for this financial aid study because of their the results of the interviews An independent sample of institutions was selected for each institutional stratum using Chromy's\" sequential, probability minimum replacement (pmr) sampling algorithm to select institutions with probabilities proportional to their computed measures of size. However, rather than allow multiple selections of sample institutions,' those with expected frequencies of selection greater than unity (1.00) were selected with certainty. The remainder of the institutional sample was selected from the remaining institutions within each stratum. The sampling algorithm was implemented with a random start for each institutional stratum to ensure the positive pairwise probabilities of selection that are needed for proper variance estimation.' The numbers of certainty and noncertainty schools selected, within each of the nine institutional strata, are shown in Table 2.2. Within each institutional stratum, additional implicit stratification was accomplished by sorting the stratum sampling frame in a serpentine manner' by: (a) institutional level of offering (where strata had been collapsed one level); (b) the IPEDS IC-listed Bureau of Economic Analysis of the U.S. Department of Commerce Region (OBE Region); and (c) the institution measure of size. The objectives of this additional, implicit stratification were to approximate proportional representation of institutions on these measures. Table 2.3, shows that the geographic distribution of the sample is consistent with that of the subset of institutions from which the sample was selected."}, {"section_title": "The Student Samples", "text": "In addition to the initial (basic) student sampling, additional student subsampling was implemented in NPSAS:96. Because of budgetary constraints, only a subsample of students were selected for interviewing; moreover, interviewing was conducted in two phases, and only a subsample of first phase nonrespondents were selected for the second interviewing phase. Additionally, certain students were selected for whom an interview with their parents would be required to obtain certain data elements. Finally, a small subsample of students was selected for reliability interviews. Initial student sampling rates were calculated for each sample institution using refined overall rates (see Appendix A) to approximate equal probabilities of selection within the ultimate institution-by-student sampling strata. However, these rates were sometimes modified for reasons listed below. The student sampling rates were increased, as needed, so that the sample size achieved at each sample institution would be at least 40 sample students, where possible.\" The student sampling rates were decreased if the sample size was more than 50 greater than the institution had been told to expect.24 The sample yield was monitored throughout the several months during which student lists were received, and the student sampling rates were adjusted periodically to ensure that the desired student sample sizes were achieved. These adjustments to the initial sampling rates (especially the first two types of adjustments) resulted in some additional variability in the student sampling rates, and, hence, in some increase in survey design effects (variance inflation--see Chapter 6). The actual sample sizes achieved in total and within each institutional and student stratum are shown in Table 2.4. In general, the 836 eligible and participating institutions classified fewer students than expected as potential FTBs; consequently, sampling rates for FTBs were increased to obtain the needed sample yield.\" By comparing Table 2.4 with  Table 2.1, it can be seen that the rate adjustment procedures were generally effective; the overall sample yield was actually greater than expected (63,616 students as compared to the target of 59,509)."}, {"section_title": "2.2.2.2", "text": ""}, {"section_title": "Student Interview Subsample", "text": "The student interview sampling design for NPSAS:96 required subsampling of those eligible students for whom CADE data had been obtained. In this subsampling , an attempt was made to maximize sample yield (completed interviews) without sacrificing response rates by using a two-phase, nonresponse subsampling design. 23 The reason for this constraint was to facilitate obtaining at least 30 responding students for most participating institutions, enabling NCES to send a report to the institution regarding its sample students, as a \"Thank You\" for participation without violating NCES confidentiality guidelines. 24This was to facilitate continued participation by the institutions for CADE data abstraction. 25 For 35 four-year institutions, this rate was increased by selecting a supplemental sample.  When EDI from the ED Central Processing System (CPS) and CADE data collection were completed for a sample institution, the students who had not been identified as ineligible through previously collected data, and for whom such previously collected data were sufficient, were considered eligible for CATI.26 Basically, all eligible sample students from schools that provided institutional records data were eligible for Phase 1 interviewing.27 Interview-eligibles were partitioned into seven previously identified strata for the Phase 1 interviewing subsampling. The student subsample for Phase 1 of interviewing is shown in Table 2.5. Potential FTBs and federal aid applicants were selected for Phase 1 interviewing with certainty. Excepting potential FTBs,\" the Phase 1 (and subsequent Phase 2) subsampling rates were set to yield an appropriate compromise between high sample yield (high Phase 1 sampling rates and low Phase 2 sampling rates) and low variance inflation (comparable Phase 1 and Phase 2 sampling rates).\nTo accommodate budget constraints, the student interview sampling design for NPSAS:96 required subsampling of those eligible students for whom CADE data had been obtained. Because subsampling was necessary, an attempt was made to maximize sample yield (completed interviews) without sacrificing response rates by using a two-phase, nonresponse subsampling design. Previously collected student record data (CADE and CPS) were used to partition the basic student sample into nine strata for the Phase 1 CATI subsampling: (1) potential FTBs. 2other undergraduate students who were financial aid applicants; 3other undergraduate students who were not financial aid applicants; (4) graduate students who were financial aid applicants; (5) graduate students who were not not financial aid applicants; (6) first-professional students who were financial aid applicants; (7) first-professional students who were not financial aid applicants; (8) CADE/CPS nonrespondents; and (9) students identified as NPSAS-ineligible based on CADE data. No students were selected from the last two listed strata; however, students were subsampled for CATI at fixed positive rates within each of the remaining (first seven) strata (potential FTBs and federal aid applicants were selected with certainty). When CPS matching and CADE data collection were completed for a sample institution, the students who had not been identified as ineligible through previously collected data, and for whom such previously collected data were sufficient, were considered eligible for CATI. For the few institutions that either provided no records data or provided them only after the interview subsamples, this determination of eligibility was based on CPS data alone. Basically, all eligible sample students from schools that provided institutional records data were eligible for Phase 1 interviewing. Among the institutions that did not provide institutional record data (or provided them late), most sample students with CPS matches were eligible for interviews. The weight adjustment for records data/CPS nonresponse accounts for the fact that only students with CPS matches (aid applicants) were eligible for interviewing in these institutions (see Chapter 6). In defining the seven sampling strata from which students were eligible, students were classified as federal aid applicants if either: (a) a match was obtained for the student in the CPS search of 1995-96 federal aid applications, or (b) a Student Aid Report (SAR) was found at the school during record abstraction Students were classified as undergraduate, graduate, or firstprofessional students based on their sampling strata with any appropriate revisions based on the institutional records data for their last term of enrollment during the NPSAS year (The last term was used for consistency with the sampling strata used for the basic student sample). Students who were classified as undergraduate students at the conclusion of CADE were further classified (modeled) as potential FTBs for this subsampling if: (a) either the records data or the SAR indicated that the student graduated from high school in 1995 or 1996; (b) the CPS year-in-school variable indicated that the student was an FTB; or (c) the student's high school NPSAS:96 METHODOLOGY REPORT: APPENDIX A !, A-27 APPENDIX A: NPSAS SAMPLING DETAILS graduation year and CPS year-in-school variables were both missing or had undetermined levels, but the student was 18 years of age or younger on December 31, 1995 (i.e., the student's year of birth was 1977 or later). All other students who were classified as undergraduate students at the conclusion of record abstraction were classified for student interview subsampling as indicated below. If they were not sampled as potential FTBs, they were classified as other undergraduate students for CATI sampling. If they were sampled as potential FTBs, they also were classified as potential FTBs for student CATI sampling unless: the records data indicated that the student was not a first-year student during the first term of enrollment during the NPSAS year, the records data indicated that the student attended the sample institution prior to the NPSAS year, or the records data indicated that the student had transfer credits from another postsecondary institution; Otherwise, they were reclassified as other undergraduate students. The students, within a particular institution, were then subsampled for Phase 1 of interviewing as shown in Table A.11. Potential FTBs and federal aid applicants were selected for Phase 1 interviewing with certainty. All potential FTBs were also retained for both Phase 1 and Phase 2 because of the need to obtain as many interviews with FTBs as possible for the BPS longitudinal follow-up study. For Phase 1 (and subsequent Phase 2), subsampling rates in other student sampling strata were set to yield an appropriate compromise between high sample yield (high Phase 1 sampling rates and low Phase 2 sampling rates) and low variance inflation  First-professional, not federal aid applicant 566 0.694"}, {"section_title": "26F", "text": "or the few institutions that either provided no records data or provided them only after the interview subsamples, this determination of eligibility was based on CPS data alone. 27 Among the institutions that did not provide institutional record data (or provided them late), most sample students with CPS matches were eligible for interviews. The weight adjustment for records data/CPS nonresponse accounts for the fact that only students with CPS matches (aid applicants) were eligible for interviewing in these institutions (see Chapter 6). 28 All potential FTBs were also retained for both Phase 1 and Phase 2 because of the need to obtain as many interviews with FTBs as possible for the BPS longitudinal follow-up study.  Phase 1 of CATI was defined to end when six telephone calls had been attempted without obtaining a completed interview, or the student: or his/her parent had been interviewed; was determined to be ineligible for NPSAS; initially refused to participate; required intensive tracing procedures, or was determined to be in a special population (i.e., deaf or language barrier). All students for whom the sixth call in Phase 1 resulted in a \"hard\" appointment or for whom a partial interview had been completed (with either the sample student or the student's parentsee Chapter 3) were retained for Phase 2 with certainty, as were all students in the potential FTB stratum. The remaining Phase 1 nonrespondents, who had not been determined ineligibles or exclusions during Phase 1, were subsampled for Phase 2 using specified rates. The numbers of eligible cases for Phase 2, numbers of certainty selections, sampling rates for noncertainty selections, and total Phase 2 selections are shown in  The selection rate was set to yield a sample of approximately 300 students among the expected respondents during the first three months of interviewing; the time factor was based on the built-in delay in administering the reinterview and the need to complete reinterviews within the same time frame as other interviews. Consequently, the sample was obviously most heavily weighted with students: (a) from institutions at which prior sequential operations (initial sampling, record abstraction) were completed earlier, and (b) who completed the first interview relatively early during the data collection period. Since fewer completions than anticipated were experienced during the first 3 months (see Chapter 3), the reinterview sample yield during that period was 273 students."}, {"section_title": "NPSAS:96 METHODOLOGY REPORT", "text": "\n\n\n3-1 Among the 73 institutions considered ineligible: 23 had closed since the sampling frame time frame; 4 were duplicates with other selected institutions (some through merger since the time frame of the sampling frame; and the remaining 46 failed to meet one or more of the criteria for institutional NPSAS eligibility. elinreadable electronic files were obtained from eight additional institutions. However, this table also shows that not all of the institutions agreeing to participate actually provided a useable enrollment list; in fact, 60 did not. Thus, 836 (92.9 percent) of the 900 eligible sample institutions actually provided a student enrollment list or data base that could be used for sample selection (another eight institutions provided electronic files that could not be processed). List provision (among eligible institutions) varied by type of institution considered; the percentage providing student sampling lists ranged from 81.3 percent of the private, for-profit, less-than-2-year institutions to 98.4 percent for public institutions with a doctoral or first-professional degree as the highest level of offering. As has been the case in prior NPSAS implementations, participation was highest among the public institutions and lowest among the private, for-profit institutions. Weighted response rates were calculated based on the institutional probabilities of selection and are also shown in Table 3.1.2 The overall weighted response rate of 91 percent and the weighted rates for all institution categorizations in Table 3.1 are similar to the unweighted rates. However, NPSAS:96 was designed to produce efficient estimates only at the student level. Institutions were selected with probabilities proportional to size; therefore, weighted institution-level estimates are subject to a high level of sampling variation, and the unweighted estimates may be more stable.\nBEST COPY AVAILABLE\n3-11 'Only sampled students, for whom an apparently legitimate ID number was available at that time, were submitted for NSLDS matching. Of the 62,717 submitted, 3,617 were determined to be ineligible for NPSAS:96. c Student level is based on the student's last term of enrollment at the NPSAS institution during the NPSAS year. The small number of graduate and first-professional students receiving Pell grants during the 1995-96 academic year were undergraduates during earlier terms of enrollment that year, and were eligible for such aid at that time. NPSAS:96 METHODOLOGY REPORT 7 0 BEST COPY AVA LABLE3-13 For this presentation, student and parent interviews will be discussed separately, in that order, even though the two types of interviewing proceeded simultaneously, as a single operation using the same interviewer pool. Both interview programs were stored in a single CATI record, so that if a sampled parent was contacted before interviewing the student (which occurred frequently while trying to locate the student), that parent could be interviewed while he/she was already on the phone.' The two-stage telephone interviewing of the previously selected sample of students (and parents of a subsample of those students) was sometimes as straightforward as placing a single telephone call; however, the operation frequently involved a number of sequential operations. The activities can be categorized into two major steps: locating (identifying an initial telephone number at which the sample member can be reached)15 and interviewing (convincing the sample member to cooperate and conducting the interview at a convenient time).16 For NPSAS:96, an automatic call scheduler was used to facilitate operations; while this scheduler served to optimize locating with as few calls as possible, it generated additional calendar time sequential dependencies.' Figure 3.1 presents a schematic of the student interviewing process. As indicated previously, not all students were selected for interviewing. Specifically excluded from the interview sampling process were students for whom no institutional record data had been obtained and those found to be NPSAS-ineligible during record abstracting. The former group I4While this procedure does represent real-time savings over a strictly sequential ordering of the two types of interviews, some inefficiencies were realized (i.e., some parents were interviewed before it had been determined that the student was NPSAS-ineligible).\n\n\nIn viewing the results by institutional sector, some dramatic disparities are evident. The most successful sector in determining FTBs accurately was that of the public, 4-year, doctoralgranting institutions, where 90.9 percent of the potential FTBs were confirmed as such in CATI, and only 5.3 percent of those in other student strata were determined in CATI to be FTBs. Each of the 4-year sectors had false positive rates below 20 percent and false negative rates below 7 percent. All of the less-than-4-year sectors had false positive rates in excess of 43 percent and false negative rates above 10 percent. In particular, private-for-profit less than two year schools had a false positive rate of 43.8 percent and a false negative rate of 48.7 percent. Two other sectors (public, less-than-2-year; private, not-for-profit, 2-year or less) had false positive rates in excess of 50 percent. 54.0 The inability of less-than-4-year institutions to identify FTBs, despite careful and comprehensive instructions provided to them, reflects the fact that these schools often do not have necessary information to make this determination. In many cases, students attend multiple schools without transferring credits from one institution to the next one. So, institutions would not know about such prior postsecondary experience. Four-year schools, on the other hand, have more information about prior institution attendance and student level, in general. If future BPS cohorts are planned, the NPSAS:96 experience will provide very good information for sampling potential FTBs in future NPSAS base year samples. For instance, it serves little purpose to ask less-than-2-year for-profit schools to attempt to identify separately their FTBs since both their false positive and false negative rates are around 50 percent. List acquisition procedures and sampling procedures may be refined to account for the disparate rates of actual FTBs across the various institutional sectors.  "}, {"section_title": "32", "text": "AST CO PY AVALA6L.--"}, {"section_title": "2-14", "text": "CHAPTER 2: DESIGN AND METHOD OF NPSAS:96"}, {"section_title": "Parent Interview Subsample", "text": "A subsample of the students selected for Phase 1 interviewing also were selected to have their parents interviewed, in order to obtain valid data regarding parents' financial resources (typically not well known by the student). The main purpose of the parent interview was to reduce the number of students, especially dependent students,' for whom the parents' income would otherwise have to be imputed. Another purpose was to collect parent income data for students who had recently become independent students (for the purposes of federal financial aid applications) as a result of recently becoming 24 years old. Consequently, all the students in the three previously-identified student sampling strata were selected with certainty for parent interviews. The strata were developed at the conclusion of institutional records collection, and the parent interview subsample was identified among those students selected for Phase 1 interviewing. When the student had been selected for parent interview and either a student interview had been completed in Phase 1 or the student had been selected for Phase 2, the parent interview was attempted throughout the remainder of the CATI interview period. The numbers of students selected for parent interviewing in Phase 1 and in Phase 2, are shown in Table 2.7 for each of the three parent sampling strata. aReflects student reclassification as a result of records data. bThis represents all eligible students in defined strata prior to Phase 1 sampling. This rate reflects only the overall Phase 1 student sampling (average rate across all sequential sampling implemented); all students within these defined strata, who were selected for Phase 1, were designated with certainty for parent interview. d Excludes Phase 1 respondents and sample members determined to be NPSAS ineligible or exclusions during Phase 1. Includes certainty selections. (Reflects average rate across all samples implemented (consequently, this rate includes Phase 2 certainty selections). 29An important historical gap in NPSAS student data has been the income of parents of dependent students who do not apply for financial aid; among those applying for (and/or receiving) aid, these data are usually obtained from existing records. 30 Parent data for these students will reduce reliance on imputed parent incomes when analyzing the effect that becoming classified as an independent student has on financing postsecondary education. \nA subsample of the students selected for Phase 1 of student interviewing also were selected to have their parents interviewed, in order to obtain valid data regarding parents' financial resources (typically not well known by the student). The main purpose of the parent interview was to reduce the number of students, especially dependent students, for whom the NPSAS:96 METHODOLOGY REPORT: APPENDIX A A-31"}, {"section_title": "2-15", "text": "CHAPTER 2: DESIGN AND METHOD OF NPSAS:96 2.3 Data Collection and Operational Design NPSAS:96 involved a multistage effort in collecting information related to student aid. For the first time in the NPSAS series, an initial NPSAS:96 data collection stage was planned, which involved collecting electronic SAR (ESAR) information directly from the Department of Education Central Processing System (CPS) for federal aid applications.31 A second stage involved abstracting information from the student's records at the school from which he or she was sampled; starting with NPSAS:93, these data have been collected through a Computer Assisted Data Entry (CADE) system (to facilitate both collection and transfer of the information to subsequent electronic systems). As in NPSAS:93, the CADE collection system was implemented, at institutional choice, by either school staff or field interviewers employed by the contractor. A third stage involved interviews with students (and in some cases their parents); as in all prior implementations of NPSAS, this data collection activity was conducted through a Computer Assisted Telephone Interviewing (CATI) procedure. For the first time in NPSAS implementations, only a subsample of the initial student sample was selected for the interview stage (in order to reduce overall costs for the NPSAS study). Final stages (again, for all eligible initial sample members) involved collecting additional ESAR data for the following  academic year, and collecting information from the Pell-Grant File and the NSLDS file maintained by ED. Both the \"following year\" ESAR data and the NSLDS data represented additions to the NPSAS data base that were first introduced during NPSAS:96. A schematic of the operational flow of major data collection components of the NPSAS:96 study is shown in Figure 2.2 and summarized in subsequent subsections. It should be noted that to meet established dates for conclusion of all study activities, while accommodating both differential dates at which student sampling could be initiated and differential timeliness of institutional turnaround, not all stages were implemented at the same time at all institutions. In fact the only fixed points in operations were: (a) selection of the institutional sample and initial institutional mailings; (b) cut-off of interviewing; and (c) the \"Post CATI\"32 collection of electronic data from existing ED data files. Start and end dates for the several study activities are shown in Table 2.8. The extent of activity overlap is clearly evident from this table. As examples: (a) the CADE record abstraction procedures had been ongoing for five months before the last student sample was selected; and (b) the cut-off date from preloading CADE data into the CATI records (necessary for CATI operations to commence) occurred almost 6 months after the initiation of  (throtigh fail 1996) 11/27/96 12/16/96 Date on which the activity was initiated for the first applicable school and/or its associated students (and parents). bDate on which the activity was completed for the last applicable school and/or its associated students (and parents). The institutional sample was drawn prior to selecting the field-test sample to ensure no overlap of the two. dEight institutions provided CADE data after the cut-off date for using those data in CATI."}, {"section_title": "Overview of Data Collection Instruments and Extant Data Sources", "text": "As noted previously, some study data were obtained from extant data bases. Information related to application for financial aid was obtained (for two academic years) from a central data system, the CPS (which is operated for ED by a separate contractor). This information is provided by students to the CPS contractor on a Free Application for Federal Student Aid (FAFSA) form and then converted to electronic form, analyzed, and provided to involved schools (and other approved parties) in ESAR or hard-copy SAR form. (To see the type of information available from the SAR, see the SAR section in Appendix C). Additionally, data on nature and amounts of received Pell Grant or NSLDS loans were obtained from other extant data bases maintained by ED. The Pell Grant file that was accessed was for the 1995-96 academic year. Data obtained from the NSLDS file were loan histories for each applicable student. The record abstraction instrument was programmed for CADE implementation. Each CADE student record was divided into three major sections (reflecting typically different physical locations of the records on campus) and eight subsections; this structure is shown in Figure 2.3. CADE was programmed so that this structure was not restrictive in access; specifically, the record abstractor could access any subsection through a menu and enter (or change) any data element in the section through scrolling. A summary of the major data elements collected through CADE is summarized below, by subsection. Locating. Collected up to four addresses and telephone numbers for student and/or parent/guardian (if not the student, names and relationship to student were collected). Characteristics. Collected student demographics (e.g., race, ethnicity, marital status, gender, date of birth, citizenship), type of high school diploma, and high school graduation date."}, {"section_title": "Figure 2.3Structure of the NPSAS:96 CADE instrument", "text": "It was recognized that this procedure would result in some wasted effort if the student was subsequently determined to be ineligible for NPSAS; however, this was more than compensated for by the savings from not having to try to relocate the parent."}, {"section_title": "2-22", "text": "CHAPTER 2: DESIGN AND METHOD OF NPSAS:96 Separate mailings to the ICs (containing all materials included in the initial mailing to the Chief Administrator) were initiated on a flow basis, as the ICs were designated. Followup telephone calls were, again, initiated one week following the mailing (the initial contact with the ICs typically involved a series of calls, including refusal conversion calls). ICs were advised of what would be expected from the school and asked to verify the IPEDS classification (institutional control and highest level of offering) and the calendar system used (including dates that terms started). ICs were also asked to (a) provide information on the school's record keeping approaches (including identifying the physical on-campus location of records needed for the subsequent record abstraction procedures), (b) identify their PC capabilities for operating the CADE software, and (c) set a date by which the school would provide student enrollment lists. The list(s) requested (preferably a single unduplicated electronic list) were to contain all eligible students enrolled in any term starting within the study-defined year. (Sampled schools with additional NPSAS-year terms starting after the date of the request obviously could not provide complete lists until after the last applicable term began.) The data items requested for each listed student were: full name; student identification (ID) number; Social Security number (possibly identical with student ID); educational level undergraduate, graduate or first professional--during the last term of enrollment during the study-defined year; and an indication of FTB status: as an undergraduate student who first enrolled at the school during a term in the study-defined year; who was classified as freshman or first-year student at that time; and who had no transfer credits from another postsecondary institution. Definitions of types of lists and information preferred, as well as instructions for preparing different forms of lists were included in the initial IC letter and further clarified, as needed, in follow-up telephone conversations. In such subsequent telephone contacts, contractor staff worked closely with the IC to determine the best reasonable alternative lists and student information that could be provided by the institution. Prompting telephone calls were made to institutions that had not provided lists by one week following the most recent delivery date previously agreed upon by the IC. Throughout the list acquisition process, attempts were made by the contractor to accommodate school constraints and to reduce their burden, including contractor unduplication of lists. Where requested, institutions were reimbursed for personnel and computer time in list preparation. Several checks on quality and completeness of provided student lists were implemented prior to actual student sampling. Institutions providing lists that failed these checks were called to rectify the detected problems. Completeness checks were failed if either of the below-listed conditions existed: potential FTBs were not identified (unless the institution explicitly indicated that no such students existed in their school); or student level--undergraduate, graduate, or first professional--was not clearly identified. Quality checks were performed by checking the unduplicated count from provided lists against the non-imputed unduplicated counts from WEDS IC files.' Nonimputed counts were available for quality checks for approximately 95 percent of the institutions. For applicable schools, separate checks were made for undergraduate, graduate, and first professional students; for schools serving only undergraduates, checks were made against total enrollment. Initially, the institution failed the check if the count for any unduplicated list differed by 25 percent from the WEDS non-imputed count.' In early May, with NCES approval, the quality checks were considerably relaxed.\" The revised checks remained in effect throughout the remainder of student sampling."}, {"section_title": "Student Record Abstraction", "text": "All prior NPSAS implementations have relied on collecting data of record regarding student financial need and aid as well as other features of his/her education, in order to: (a) reduce responding burden on the sampled students, and (b) obtain much more valid data than could be obtained through student interviews. In addition to collecting information from institutional files and from ED Pell Grant files (both of which have been accessed in prior NPSAS studies), NPSAS:96 also collected electronic SAR data of record from the ED CPS and from the ED NSLDS files. Both of these data collection activities also expanded the time horizon of data for the current NPSAS study. SAR data were obtained for both the NPSAS year  and through February of the subsequent academic year ; NSLDS data were obtained historically over all years of postsecondary education through the 1995-96 academic year."}, {"section_title": "36", "text": "The order of preference was the 1994-95 IPEDS file or the 1993-94 IPEDS file (from which the institutional frame was constructed). If counts on both these files were imputed, no quality check was performed. 371f provided lists were not unduplicated, the contractor estimated the unduplicated total by applying an empirically determined multiplicity factor to the count over provided lists; in these cases, the critical difference was also relaxed to 30 percent. 38Revised procedures involved: (a) no checks for less-than-2-year institutions, since most (68 percent) were failing the checks but few (9 percent) were providing replacement lists; and (b) lists passed the checks if the student count differed from the IPEDS count for that type of student by 500 students or more and the IPEDS count was not zero. To reduce institutional burden in the subsequent institutional record abstraction, the NPSAS:96 contractor, with the assistance of NCES, arranged to obtain, through EDI, information from the Central Processing System, the CPS (which is operated for the Department of Education by a separate contractor, National Computer Systems NCS). The data accessed included information from the FAFSA (provided by all federal financial aid applicants) and SAR. Generally, all such initial EDIs of SAR data were completed for an institution before CADE record abstraction was attempted for that institution."}, {"section_title": "CADE Data Abstraction from Students' Institutional Records", "text": "Data from sampled students' records at the NPSAS institution were collected using CADE technology, representing a refinement the procedures first used in NPSAS:93. CADE was programmed in CASES 4.1, for compatibility with CATI (see Section 2.3.4) and the Data Elements Dictionary used (see Section 2.3.6). Institutions were urged to have their own staff accomplish these activities, using the software, since this provided both cost efficiencies and better confidentiality protection for records of students not selected for the study. However, institutions were given the option of having the abstraction done by field data collectors (FDCs) employed by the contractor. Following the receipt of CPS information for students from an eligible institution, CADE materials and related information were prepared and packaged for delivery to either the IC or FDC, depending on stated institutional preference (which subsequently changed in some cases).39 To reduce burden in the CADE record abstractions, a number of data elements were preloaded into the CADE records for a particular institution. These preloaded elements included (a) CPS data, including the full SAR (where obtained), (b) previously verified institutional characteristics and identifiers from the contractor sampling files (e.g., level of offering, control, calendar system, institution-specific dates for terms of enrollment, grading system, stratum, whether clock or credit hours were awarded), (c) student names, ID numbers, and sampling strata (from the contractor sampling files), and (d) customized additional financial aid sources/programs unique to the specific institution and associated state. The preloaded CADE packages were distributed on a flow basis to either the IC (on floppy disks) or to the FDC (electronically). Packages also included a hard-copy Users' Guide, an embedded programmed Tutorial, and a Fact Sheet (which summarized information previously provided by the institution regarding the physical campus location of relevant data). Associated refinements to the CADE software included: installation checks, to determine if PC limitations would create problems in operation CADE (if so, a message was displayed stating the problem and providing a contractor \"hot line\" number to call for assistance); 39 Some institutions eventually photocopied relevant records and provided them to the FIs or contractor central staff for direct entry into CADE. quality control checks to identify (and notify the user of) student records that were incomplete (and the area of incompleteness) or had not yet been accessed (when trying to close either an individual student record or the entire CADE package); and a pop-up screen showing overall full and partial completion rates for record abstraction at the institution. Weekly telephone calls were initiated to ICs or FDCs (as applicable) to determine completion status for each school that was still active at that time. While CADE receipt was on a flow basis over institutions, the CADE package for a specific institution was not returned until it was completed. On receipt of the CADE package, each record was subjected to edit checks for completeness of critical items. Data from an institution failed the edit check if 50 percent or more of the student records failed all edit checks or if any anomalous data patterns were observed.' Study plans called for completion of the institutional record abstracting at a school before students from that school were interviewed (to allow preloading of the CADE data into the student CATI record). This was the case for all but 40 schools, which had agreed to provide CADE data but had not done so prior to the last date that CATI could be initiated. CATI was initiated for students from these schools prior to receipt of CADE data, but special requests were also made of these schools to provide hard copy transcript and financial aid information in hard copy form.'"}, {"section_title": "Other Post Hoc Student Record Data Obtained", "text": "Two additional EDI operations were performed shortly before the conclusion of CATI. Consistent with prior NPSAS studies, data were obtained from the ED 1995-96 Pell Grant files regarding applications for, and receipt of, Pell Grants. To facilitate the timely preparation of data files, this operation was initiated about four weeks prior to the cut-off date for interviewing. All initial sample members42 with \"apparently\" valid SSNs at time (62,717 sample members) were submitted for potential matching to the ED files.43 Extracted data from matching Pell Grant records' were downloaded and added to the data base. 4\u00b0Institutional failure of the edit check procedure led to follow-up contact with the involved IC toward resolving the existing problems."}, {"section_title": "41", "text": "Response to this request considerably exceeded expectations (see Chapter 3). 42Since this submission was accomplished prior to final data cleaning, even those sample members classified at that time as \"ineligible\" were submitted for matching. 43Additional delay of this activity until completion of CATI would have resulted in only a handful of additional students for whom valid SSNs were available. Some students yielded multiple record matches, indicating Pell Grant activity at two institutions during the NPSAS year. NPSAS:96 METHODOLOGY REPORT 2-26 CHAPTER 2: DESIGN AND METHOD OF NPSAS:96 NPSAS:96 represented the first attempt to incorporate data from the ED NSLDS files. This EDI activity was accomplished in roughly the same time frame as the Pell Grant matching and again was attempted only for the 62,717 initial sample members with apparently valid SSNs at the time of matching. Because the NSLDS file is cumulative, full histories (through the 1995-96 academic year) of NSLDS activity were obtained for matches."}, {"section_title": "2.3.4", "text": ""}, {"section_title": "Student and Parent Locating and Interviewing", "text": "NPSAS:96 student and parent interviews were principally conducted by telephone, using CATI technology, as has been the case for all prior NPSAS interviews. Like CADE, CATI was developed using CASES 4.1 software to facilitate preloading full-screen data entry and editing of \"matrix-type\" questions. The CATI system presented interviewers with screens of questions to be asked of the respondents, with the software guiding the interviewer and respondent through the interview, automatically skipping inapplicable questions based on prior response patterns or suggesting appropriate wording for probes should a respondent pause or seem uncertain in answering a question. Unlike prior NPSAS studies, CATI was initiated for only a subsample of the original student sample (see Section 2.2.2.2). Prior to initiating CATI, notification letters, on Department of Education stationery and with attachments, were mailed to students (and parents, where applicable). These letters (copies provided in Appendix B) notified sample member of the upcoming survey, pointed out the importance of the study, disclosed average time burden, and urged participation. Associated with the interviewing (and partially imbedded in the CATI instrument) was the necessity (due to incomplete or incorrect telephone numbers), in many cases, to locate the sample members. A major locating challenge for sample members at many institutions was the fact that by the time CATI was initiated, those individuals had moved from their \"local\" (school) address. To facilitate the tracing component, each CATI record contained roster lines for up to 20 telephone numbers (including numbers for individuals identified as tracing sources); each such roster line was associated with a history of the dates and results of all calls made to that number and a number-specific comment field. Locating calls, as well as interviews, were initiated according to a calling plan using an automatic call scheduler imbedded within the CATI software. This system allowed calls to be scheduled on the basis of established case priority, time of day, and history of success of prior calls at different times and on different days. It also allowed \"special queue\" access of certain special cases (e.g., limited English proficiency, prior refusals) only by appropriate special interviewers (e.g., bilingual interviewers, special \"refusal converters\"). If initial CATI tracing efforts (using information preloaded) failed, then CATI external student/parent tracing activities were executed. Such activities generally involved searches, by subcontractors, of various electronic databases. The specific CATI-external tracing activities are listed below (the order of listing generally represents the order in which these activities were sequentially implemented and increasing unit costs of the activities). Referral to Fast Data. This is a service whereby telephone numbers are obtained for known names and/or addresses through matching to an exiting national telephone data base.\" Matched cases were returned within 24 hours of submission.' Referral to Equifax. Equifax maintains credit files on a large number of individuals. Available databases include names, social security numbers, and current and former addresses and telephone numbers of individuals for whom credit histories have been assembled. Equifax also has arrangements with some states to access their drivers license databases. First, Equifax attempted a match with the electronic data; if this was unfruitful, an Equifax tracing expert reviewed the hard copy printout for possible leads, and implemented a check of such leads, if deemed appropriate. Turnaround was typically within two weeks. Referral to Telematch. This step was only used if Equifax returned an updated address without a telephone number. Tracing activity involved comparing the name and address to a database containing every published telephone number in the United States, with associated names and addresses. Turnaround was typically within 2 days. To reduce interview burden and to guide the interview through appropriate branchings (e.g., questions appropriate only for graduate students), considerable information, in addition to tracing data, was preloaded into the CATI records prior to interviewing. Such preloaded information included (a) data previously collected through CPS and/or CADE; and (b) information from the sampling file (e.g., name, Social Security number, school name, school and student stratum). In a number of instances, specific questionnaire items were not asked (or only verified) if that information had been collected previously.' Pre loading and implementing CATI occurred on a flow basis, as the CADE results were received from the institutions. As indicated previously (section 2.3.3.2), this general approach did not hold for sample members at 40 institutions that delayed CADE submission beyond the deadline for CATI with initiation; CATI was attempted for such students without benefit of preloaded CADE data, but other preload data (e.g., CPS data) were included, where available. NCES-developed, on-line coding programs (for industry/occupation, WEDS, and field of study coding) were imbedded in the overall interview administration system. These allow standard coding of responses while the respondent is still available to assist."}, {"section_title": "47", "text": "The NPSAS:93 experience suggested a number of areas in which interview information should be collected even though comparable data from student records had been collected. Results of CATI interviewing were monitored daily through the study Integrated Management System (IMS--see Section 2.3.6). Daily reports of production, with revised projections of future production to satisfy study requirements, were available to both NCES and contractor staff. CATI interviewing was also monitored through a formal, work-sample-based quality control procedure. Data to determine system level error rates and individual interviewer departures from system rates were collected daily; and distributed no less frequently than on a weekly basis."}, {"section_title": "2.3.5", "text": ""}, {"section_title": "Training Data Collectors", "text": "In any survey, comprehensive training of those who collect study information is critical to the quality of the end-product data. In NPSAS:96, separate training was required for three different types of data collectors: contractor telephone interviewers who collected data through CATI, together with their supervisors and monitors; contractor field data collectors (FDCs) who collected records data through CADE, together with their field supervisors (FSs); and institutional staff who collected records data through CADE. Training for the first two groups involved providing information regarding locating and dealing with students/parents or institutional staff, the nature of the data to be collected, and the nature of the computer program used for data collection; the latter group required only training regarding the computer program operation. Initial telephone staff training for the full student and parent interviews was conducted in late April of 1996, immediately prior to the scheduled start date for CATI operations. All supervisors were trained separately prior to training any of the interviewers, so that they could assist during the interviewer and monitor training, conducted in early May. Additional training sessions for interviewers were conducted, as needed, through November 199648. In total, 14 separate training sessions were held; four for day-shift interviewers and the remainder for night and weekend staff. In the initial training, 13 supervisors and 21 monitors were trained. Additionally, a total of 306 perspective telephone interviewers began training during NPSAS:96; of these, 269 successfully completed the training program and began interviewing. 48 Such additional training was required for unplanned replacement interviewers (to replace those who were dismissed or left the project), for planned supplemental interviewers (as students from additional schools were added to the CATI data base, operational efficiency required increasing the complement of interviewers), and for previously-trained interviewers found to need retraining."}, {"section_title": "2-29", "text": "Full interview training included 20 hours of instruction followed by 2 hours of observing supervisors conducting actual interviews. Instruction included an introduction to the nature and purpose of NPSAS:96, general interviewing techniques, general use of the CATI program, procedures for contacting sample members, review of all questions in both the student and parent interviews, practice with screens and subroutines requiring on-line coding or matrix entries, and practice with separate sections and the full interviews --including participation in \"mock\" interviews and observations of both \"mock\" and actual interviews. As a training aid, each interviewer was issued an Interviewer Manual\". The Table of Contents of that manual as well as a copy of the training agenda for the full interview is provided in Appendix E. Additional training was provided for administration of the reliability reinterviews, the abbreviated interviews, and the minimal interview. Reliability reinterview staff were chosen from among those who had shown high proficiency with the full interview, and the half-day training was restricted to familiarization with revised branching patterns and \"fills\" in the reinterview. Training for administration of the various abbreviated interviews was restricted to bilingual (Spanish/ English) interviewers and \"refusal converters,\" as appropriate. Since these specialty interviewers included only well-seasoned staff who had been trained for the full interview, the training was again restricted to familiarization with special procedures to access the specialty interviews and with the CATI screens for the abbreviated instruments themselves. The initial training for contractor CADE staff was conducted during March, 1996; all 11 field supervisors (FSs) and 65 of the 79 field data collectors (FDCs) used in the study were trained at that time. As a result of some loss of FDCs and an unexpected propensity of institutions that requested a change from their initial choice of \"self-CADE\" to the use of contractor FDCs, supplemental training of 14 additional FDCs was conducted during June, 1996. To reduce training travel costs for field staff, the initial training was conducted at two sites: in Research Triangle Park, NC for the East Coast, and in San Francisco for the West Coast. Since the supplemental training group was relatively small, only a single training session was held, at the East Coast site. The initial training sessions included a full day of training for the FSs prior to the FDC training; FDCs were subsequently trained, with assistance from the FSs, over three additional days (with after-class homework). Training consisted of an overview of NPSAS:96 objectives and time frame (including a brief explanation of how the financial aid process works on campuses), review of the architecture and nature of the CADE software, review of and practice with each section of the CADE instrument, procedures for dealing with the IC and other staff at the institutions, instruction in, and practice with, locating records (including, but not restricted to, use of the \"location of records\" lists provided by the ICs), and procedures for contacting FSs and electronic transmission of completed cases. During this training, considerable use was made of location and abstraction of records using mock student folders developed to represent diversity in record keeping at different types of postsecondary institutions. Each trainee was also provided with: a laptop computer to be used during their training and subsequent field work, a Field Data Collector Manual, and a CADE Users' Manual. The Table of Contents for both these manuals as well as the training agenda are provided in Appendix E. Training of Institutional Staff in use of CADE relied heavily on self-training, since the major objective of that training was familiarity with the CADE program. The program was self-installing (including an initial check of the host PC for sufficient memory), and a selfteaching tutorial was imbedded. Help screens were imbedded within the program and a \"hot line\" number was established through which users could obtain answers to specific or general questions from central contractor staff who developed the software. Additionally, institutional staff were provided with a CADE Users' Manual."}, {"section_title": "Evaluation and Quality Control Design", "text": "Evaluation of NPSAS:96 procedures have obvious implications for enhancements of subsequent waves of NPSAS and for possible methodological experiments within future NPSAS field tests. Each major component of the study was evaluated. Methodology consisted of both formative and summative evaluations. Formative evaluations were of an ongoing nature, designed to assess tasks at intermediate stages so that the effects of employing alternate methodologies could be analyzed and modifications and revisions could be employed and assessed prior to task completion. Such ongoing evaluations (many of which were imbedded within the study IMS) were a major part of NPSAS quality control. Summative evaluations assessed the results of the field test, including procedural changes instituted during the course of the study. A summary of NPSAS:96 field test evaluations that were planned and implemented is provided in Table 2.9. A critical part of operational evaluation and quality control was regular quality circle meetings with field interviewers, telephone interviewers, interview monitors, and interviewer supervisors. These meetings provided an easily available forum for production staff and project management to address the important topic of work quality, discuss issues of concern, identify problems with the survey instruments, share ideas for improving the instruments, and suggest various approaches for improving operations and/or results. To implement suggested improvements arising from some such meetings, the operational features of the CATI instrument was refined a number of times over the course of the data collection period. On completion of data collection, final quality circle meetings were held, serving as debriefing sessions for the full operational period.  Monitor and adjust student sampling rates. Analyze accurateness of two-stage sampling approach assumptions."}, {"section_title": "List acquisition", "text": "Analyze overall response rate, accuracy, costs, and time to produce lists. Monitor accuracy of lists (completeness and multiplicities); resolve, where needed Analyze accurateness of assumptions regarding FTB identification under new rules. Debrief institutional coordinators."}, {"section_title": "Record abstraction", "text": "Monitor all electronic data interchange (EDI) approaches. Monitor completeness of all returned CADE data; resolve, where needed. Analyze overall response rates, costs, and time to complete CADE. Analyze data quality (missing data) under conditions of self-CADE, field-CADE, and EDI approaches. Debrief institutional coordinators. Debrief field staff.  Interview administration/data Monitor (silent) CATI interviewer performance; correct or quality retrain as needed. Analyze silent monitoring quality control data. Analyze CATI operational parameters (e.g., numbers of calls per case, total interviewer hours per completed interview). Analyze effectiveness of refusal conversion approaches. Analyze efficacy of special case abbreviated interviews. Analyze effectiveness of mailings and leaving messages on answering machines. Debrief and conduct quality circle meetings with interviewers, monitors, and supervisors. Analyze rates and patterns of interview nonresponse. Analyze validity of student responses. Analyze response temporal stability (reliability) through reinterviews of selected items. Analyze times to complete interview sections. BEST COPY AVAILABLE As indicated in Table 2.9, the study design included a number of components for direct evaluation of data quality. Among these, a reliability reinterview was conducted with students about four weeks after the initial interview; this involved a random subsample of 250 respondents to the initial interview. The reliability reinterview contained only a small subset of the initial interview items. Also, validity of information collected from CATI was evaluated by comparison of certain CATI responses to information items available from institutional records.\" Both evaluation and quality control were greatly facilitated by the use of an Integrated Management System (IMS). All operational and management activities, (including sampling, locating, collecting institutional records data, interviewing, and data processing) were under this system, which consisted of a series of PC-based, fully linked modules. The various modules of the IMS provided the means to conduct, control, and monitor the complex, interrelated activities required in the NPSAS:96 study. Report production, data analyses, and document archiving were also integrated into this system. The IMS structure allowed for streamlining related tasks and served as a centralized, easily accessible repository for project data and documents. The IMS provided authorized project staff (and NCES staff as remote users) menu-driven access to all IMS modules quickly and easily. Its use also enabled the application of extensive quality control measures throughout the various project activities. Table 2.10 provides a listing of major IMS processes and their purposes. Figure 2.5 presents a schematic of various components and features of the IMS. The central system resided on a DEC PATHWORKS PC network, accessible to remote users through a dedicated network modem. Case-level status as well as routine summary reports were available across all components of the system. Summary reports and other project information were also accessible through the password-protected, restricted-use World-Wide Web page for the NPSAS:96 IMS. Information was integrated through the implementation of a case-level control system which monitored status in the various stages of production. Status from separate stages was incorporated in the master IMS to allow control of the flow of events in the system and monitoring of performance of study requirements."}, {"section_title": "52", "text": "50 Generally, students were not asked about items of information collected from institutions; however, some items were included in CATI to assess validity by having the student either (a) \"verify\" the institutional data, or (b) provide an independent response.  Chapter 3 Overall Institution, Student, and Parent Data Acquisition Rates and Related Outcomes Attaining the participation rates required for NPSAS:96, by NCES Statistical Standards, demands high levels of cooperation at all stages of the survey process. This chapter provides the overall participation outcomes; further examination of factors related to these outcomes, together with results of other evaluations, are provided in subsequent chapters of this report."}, {"section_title": "Institutional Participation", "text": "A total of 73 (7.5 percent) of the 973 institutions initially selected for the full scale study were found to be ineligible for NPSAS:96. Forty-six of these institutions failed to meet one or more of the NPSAS institutional eligibility criteria specified previously in Chapter 2; another 23 institutions had closed between the time sampling frame information was collected and institutions were first contacted about participation in the study; and four institutions were \"duplicated\" with other selected institutions. The latter group reflected either improper classifications on the sampling frame or a subsequent merger with another sampled institution. Eligibility rates are shown in Table 3.1, by institutional level of offering, control, and sector.' Institutional eligibility varies considerably with level of offering and control; it is markedly lower for less than 2-year institutions and for the private for-profit institutions. These differences were expected, and are directionally consistent with results from prior NPSAS studies. The 900 eligible sample institutions were asked to participate in NPSAS:96 by: (1) providing comprehensive lists of students for sample selection and (2) assisting in abstracting data from student records for sampled students. Hence, the potential for institutional nonresponse existed at these two points in the survey process. Participation at the list provision level is also shown in Table 3.1. From the table, it can be seen that nearly all (896 or 99.6 percent) of the 900 eligible institutions initially agreed to participate in the study. 11n this and subsequent tables, institutional classification errors on the sampling frame have been corrected; consequently, counts within corrected classification will differ somewhat from those in Chapter 2 based on uncorrected sampling strata."}, {"section_title": "3.2", "text": ""}, {"section_title": "Obtaining Student Records", "text": "As indicated previously, obtaining information from student records was a sequential three-stage process. The first stage, implemented for the first time in NPSAS:96, involved an electronic data interchange (EDI) with the ED CPS database of electronic SARs). The second stage involved collection of information from student records at the postsecondary institutions in the NPSAS sample using a CADE software system3; and the third stage involved EDIs with the ED Student Pell Summary records and ED's NSLDS database. Outcomes for these three activities are considered in separate subsections below. The CADE operation was implemented by either staff at the NPSAS institution or contractor field data collectors (FDCs). NPSAS:96 METHODOLOGY REPORT C 3-3 CHAPTER 3: OVERALL INSTITUTION, STUDENT, AND PARENT DATA ACQUISITION AND RELATED OUTCOMES 3.2.1 CPS SAR Data Table 3.2 summarizes results of matching and downloading data for SAR 96 and SAR 97, in total and by selected student classifications. Obtaining a match was determined by whether or not: (1) the student was listed on the CPS files (i.e., had applied for Federal financial aid during the 1995-96 academic year and entered on the file by the time the request was made) and (2) a valid CPS ID and name4 could be determined . While application for federal aid is one of the factors (and probably the principal one) affecting the match rate, differences shown in Table 3.2 should not be over-interpreted, since the percentages shown are unweighted. From Table 3.2, it should be noted that the SAR 96 matching attempt involved only 61,932 of the total; specifically, those for whom a CPS ID had been determined from information on the institution's enrollment lists. (A total of 23 institutions failed to provide sufficient information to construct a CPS ID; other institutions provided no information or inaccurate information for differing numbers of students. No matches were obtained for 16 additional institutions.) Matches were obtained, and some SAR data obtained, for 30,821 of those submitted (about 50 percent).6 SAR 96 matching rates were lowest among students at public institutions offering less than 4-year programs and among graduate students; they were greatest among students attending private for-profit institutions and first-professional students. These results are not particularly surprising. Federal aid applications at public community colleges and technical institutions are expected to be proportionately less than other sectors, and federal aid applications at private for-profit institutions proportionately greater. Moreover, firstprofessional students tend to rely more on federal aid (primarily loans) whereas graduate students generally rely more on institutional aid (teaching and research assistantships). application and the time that student lists were prepared, could create a non-match. s Recall that both CADE and CATI data collections were conducted only after the initial SAR 96 matching attempt; consequently the CPS IDs for the additional 1,148 students (obtained from either CADE or CATI data) were not available for the SAR 96 request. 6 For purposes of comparability, all percentages shown in. Table 3.2 are based on the full set of 63,080 students with apparently valid CPS IDs; consequently the SAR-96 rates are depressed from the values obtained using only the subset of 61,932 actually submitted. Original plans called for resubmitting these students for SAR 96 data following CATI; however, at that time CPS processing of the 1995-96 year had been discontinued. Only sampled students for whom an apparently legitimate social security number was available were submitted to CPS for matching. Of the 63,080 with valid CPS Ids, 3,643 were determined to be ineligible for NPSAS:96. cThese matching rates are somewhat depressed, since the 63,080 students include 1,148 students for whom CPS IDs were not determined until after CPS processing for the 1995-96 year had been discontinued; such students did not have the opportunity to match to SAR 96. d Student level is based on the student's last term of enrollment at the NPSAS institution during the NPSAS year."}, {"section_title": "2 BEST COPY AVAILABLE", "text": "CHAPTER 3: OVERALL INSTITUTION, STUDENT, AND PARENT DATA ACQUISITION AND RELATED OUTCOMES Following the completion of interviewing, the full set of 63,080 students was submitted for attempted matching to obtain SAR 97 data'. Matches were obtained for 22,924 (about 32 percent of the total). It is expected that the lower matching rate for SAR 97 represents changes in student status between the two academic years; e.g., leaving institution (through program completion or other reason) or changing status within institution (from undergraduate to graduate student). This hypothesis is supported by the fact that the match rate differential, while directionally consistent among all student classifications, is generally greatest among students at 2-year and less than 2-year institutions. Overall, SAR data from either 95-96 or 96-97 were obtained for 54 percent of the students submitted, and SAR data for both years were collected for about 31 percent of them. The high rate of matches for either year signals a high reliability of analytic data constructed from SAR data (e.g., expected family contribution, family income); however, the lower rate of matches for both years suggest that data are probably insufficient to analyze trends over years."}, {"section_title": "Abstracting Students' Institutional Records", "text": "The record abstraction phase of the study was restricted to those students enrolled in the 836 sample institutions providing an enrollment list from which a student sample could be selected (i.e., 63,616 students). As indicated previously, these data were to be entered into magnetic form, on site at the institution, using a computer-assisted data entry (CADE) program. To reduce the burden associated with record data abstraction, SAR96 data, where obtained previously from CPS, were preloaded into that section of the CADE record into which such data were, otherwise, to be abstracted. At all participating institutions, the Institution Coordinator (IC) was given two principal options as to how the student information was to be entered into CADE. One option (\"self-CADE\") was direct abstraction by institutional staff (guided by the CADE program8, with reimbursement on request); the second option (\"field-CADE\") was to provide contractor Field Data Collectors (FDCs) with access to the records and have the abstractions performed by the FDCs. Generally, self-CADE was the recommended option, since it was less expensive and ensured no contractor staff access, during abstraction, to records of students who had not been 'In an attempt to retrieve updated addresses and/or telephone numbers, 1,529 of these cases were actually submitted during CATI data collection. 8 While the CADE system was self-directing and contained a number of checks to ensure proper installation and use, training institutional staff on the proper use of the system was still needed. This was accomplished through a CADE Users' Manual and an electronic tutorial; also a hot-line number was established by the contractor to address specific questions as they arose. NPSAS:96 METHODOLOGY REPORT 3-6 sampled'. A total of 28 institutions did not use either of these CADE options; for various reasons' they chose to provide electronic or hard copy of selected records to the contractor, and contractor central staff then transferred (or keyed) relevant information from those records into the CADE format (these institutions are classified as field-CADE). Initially, 663 (79 percent) of the 836 participating institutions opted for self-CADE; however, only 474 (57 percent) actually completed the procedure under self-CADE. Among the 836 institutions, 199 ICs (24 percent of the total) changed their minds about the method of abstraction during the CADE operation; 194 of these changed from self-CADE to field-CADE, and the remaining 5 changed in the opposite direction. In addition to obtaining student financial aid data, enrollment data, and other factual postsecondary data from institution records (the most reliable and valid source available), CADE operations also were designed to obtain information necessary to contact the student. For these and other purposes, all data in the CADE record were preloaded into the CATI record, after final quality assurance checks and determination of student eligibility. During the CADE operation, 2,430 students were determined to be NPSAS-ineligible (in the bulk of these cases, some student record data had already been abstracted prior to that determination); an additional 1,326 sample members were determined to be NPSAS-ineligible during the subsequent interviewing stage (complete or partial CADE data had also been obtained for over half of these cases). Irrespective of the availability of CADE data, however, none of these 3,756 students are considered in discussions of rates for obtaining record data . Institutional response to the CADE data collection is shown in Table 3.3. Among the 836 institutions providing student lists, 804 (96 percent) provided complete or partial record data for at least one NPSAS-eligible student selected from that institution. It should be noted that eight of the 804 institutions providing such data did so only after the interviewing stage had been initiated for their students; consequently, even though all students from such institutions with sufficient CADE data are represented in the CADE database, only those students at the institution with sufficient SAR96 data were selected for (and are represented through) student interviewing. For purposes of presentation completeness, both weighted and unweighted institutional-level rates for obtaining record data are provided in Table 3.3. 9 An exception to this generality was institutions with large samples (particularly those with samples of graduate and first-professional samples--records for whom are frequently maintained in separate physical locations from that for undergraduates). Past experience, in both NPSAS:93 and the NPSAS:96 field test, suggested that a sizeable number of such institutions initially choosing self-CADE would subsequently discover the task too demanding and then request field-CADE for the (remaining) abstractions; to avoid the delay so introduced, field-CADE was highly recommended to such institutions from the outset. 10The typical reason expressed was unwillingness to have contractor staff accessing their files or unwillingness to perform self-CADE. Other institutions in this category resulted from those that initially attempted self-CADE but who admitted that they would be unable to do it too late in the process to schedule FDCs for field-CADE. 'Institutional classifications were verified by participating institutions to correct classification errors in the sampling frame. hThe eligible group is comprised of the 836 NPSAS-eligible institutions that provided lists for student sampling. Includes institutions providing only partial data and those providing data for only a subset of sampled students; eight of the institutions provided these data only after interviewing had been initiated for students selected from their institution. d NPSAS:96 was designed to produce efficient estimates only at the student level. Institutions were selected with probabilities proportional to size. Therefore, weighted institution-level estimates are subject to a high level of sampling variation, and the unweighted estimates may be more reliable."}, {"section_title": "3-8", "text": "CHAPTER 3: OVERALL INSTITUTION, STUDENT, AND PARENT DATA ACQUISITION AND RELATED OUTCOMES Even though the sample was not designed to optimize the precision of institution-level estimates, the weighted and unweighted rates are quite comparable\". Among the types of institutions categorized in the table, some variation is evidenced; however, the range of variation is relatively small (between 90 percent and 100 percent). As has been the case in prior NPSAS studies (with notable exceptions among the public institutions), unweighted rates for obtaining record data were lowest among the for-profit institutions and institutions offering programs of 2-years or less. Rates for obtaining institutional record data among the 59,860 NPSAS-eligible students sampled from the 836 participating institutions are shown in Table 3.4. Again, both weighted and unweighted results are shown; the weighted rates representing the CADE data coverage within the defined population of NPSAS-eligible students12. Overall, full or partial CADE data were obtained for about 93 percent of the selected students. Within the categories selected for presentation in the table, variations in rates is somewhat restricted, particularly among unweighted rates (unweighted rates range from a low of 88 percent to a high of 96 percent, while the weighted coverage rates range from a low of 81 percent to a high of 97 percent). Consistent with the institutional-level results, rates are generally lowest among students from institutions that offer less than a 4-year program or that are for-profit."}, {"section_title": "Other Post Hoc Records Abstracted", "text": "The EDIs with the NSLDS database (attempted for the first time in NPSAS:96) and with the Pell Grant files were initiated towards the end of CATI operations. As with the previously described procedures with CPS, matching of students to the files required CPS IDs. At the time of both of these requests, apparently valid CPS IDs were available for only 62,717 sample members, the number subsequently submitted for both attempted matchings and associated data downloads\". The factors that determined a match for these attempts are consistent with those discussed in section 3.2.2.1 for the CPS operation, as are associated cautions regarding over interpretations of matching results. \"A single exception involves the private, not-for-profit institutions offering less than a 4-year program; within this domain (represented here by less than 50 institutions), the weighted and unweighted rates differ by about 8 percentage points. 12 The specific applicable postsecondary student population is those enrolled in institutions that would provide student sampling lists, if asked. 130f these, 3,617 were ultimately determined to be ineligible for NPSAS. NPSAS:96 METHODOLOGY REPORT 3-9"}, {"section_title": "6'", "text": "CHAPTER 3: OVERALL INSTITUTION, STUDENT, AND PARENT DATA ACQUISITION AND RELATED OUTCOMES All percentages are based on the eligible total for the row under consideration. aBoth institutional and student classifications have been verified against collected data to correct classification errors in the sampling frames. bThe eligible group is comprised of all 63,616 sampled students minus the 3,756 students found to be NPSAS-ineligible at any stage of data collection. Includes obtaining only partial data. d Based on the student's last term of enrollment at the NPSAS institution during the NPSAS year. eThe difference between weighted and unweighted response rates is explained by one institution with 102 nonrespondents with larger-than-average weights.  Note: All percentages are unweighted and based on the eligible count within the row under consideration. 'Both institutional and student classifications used here have been verified to correct classification errors on the sampling frame. bOnly sampled students, for whom an apparently legitimate ID number was available at that time, were submitted for NSLDS matching. Of the 62,717 submitted, 3,617 were determined to be ineligible for NPSAS:96. The loan transaction matches for any year do not necessarily reflect a loan during the year . They may represent a consolidation or cancellation transaction. dOver all years of postsecondary education reflected in the NSLDS files. Student level is based on the student's last term of enrollment at the NPSAS institution during the NPSAS year. BEST COPY AVAUBLE Results of the NSLDS attempted matchings are shown in Table 3.5. Since NSLDS files are historical, information about receipt of such loans were available not only for the NPSAS year but also for prior years of postsecondary education (where applicable); therefore the table shows match rates for both the NPSAS year and historically. A total of 21,418 NPSAS sample members (34 percent of those submitted) were matched for the NPSAS year; 31,455 (50 percent) were matched over all years. NSLDS file records are maintained as \"transactions;\" consequently, in addition to loan receipt records, the files contain records for consolidations, cancellations, etc. Because of this and the historical nature of the files, several records were expected (and realized) for some students; in fact, for the 31,455 students matched, a total of 121,100 loan records were obtained (on average, almost four records per sample member). For NSLDS matches for the NPSAS year and within the student classifications considered, the relative numbers of matches follow a pattern quite similar to that seen for the CPS matching (e.g., low match rates for graduate students and for those in public institutions with program offerings of two years or less, but high match rates for first-professional students and those in for-profit institutions). The reasons for the NSLDS matching pattern is also probably consistent with that advanced for the CPS matching pattern (see section 3.2.1, above). The pattern is typically less distinct (and in some cases absent) for the historical match rates, certainly reflecting, at least in part, the fact that student status has changed over time (e.g., the higher historical rate for graduate students also reflect their aid packages when they were undergraduates). Results of attempted matches to the Pell Grant file for the 1995-96 financial aid year are shown in Table 3.6. Matches were obtained for 13,650 (22 percent) of the 62,717 submitted cases. A handful of the matches involved graduate and first-professional students who are not eligible for this form of financial aid. However, the student level classification used is based on the last term of enrollment at the NPSAS institution during the NPSAS year, and the matched graduate and first-professional sample members were undergraduates at some time during the year (and as such eligible for this type of aid during the year). Considerable variation in Pell Grant matching rates is observed over the categories of students shown in Table 3.6. Matching rates are lowest in the institutions offering programs at the graduate and first-professional level; however, this reflects the fact that the bulk of the graduate and first-professional students sampled from such institutions were not eligible for this form of aid during the year. Other than the confounding of the graduate and firstprofessional samples, the pattern of matches is fairly consistent with that observed for other EDI attempts; specifically relatively high match rates among sample members enrolled in forprofit institutions and relatively low match rates within public institutions with offerings of 2 years or less (probably for reasons similar to those advanced previously)."}, {"section_title": "Students", "text": ""}, {"section_title": "15", "text": "Sequential activities associated with locating can involve: sequencing through the preloaded telephone numbers until the operable one is found; calling new numbers uncovered during calls to preloaded numbers; contacting directory assistance for a name at an available address (when no phone number is available or when a number has been disconnected); calling college locator services and/or Alumni Offices; as well as more intensive tracing activities (e.g., referred to external sources)."}, {"section_title": "16", "text": "Sequential activities associated with interviewing can involve: reaching sample members when they are available; convincing the sample member initially to participate; scheduling (and rescheduling if an appointment is missed) a convenient time to conduct (or finish) the interview; referring respondents with English language weaknesses to specialized interviewers; converting initial refusals (usually involving at least two additional contacts); plus relocating sample members that move before completing the interview (e.g., between institution years). 17 Among other features, optimal calling plans involve (a) calling individuals who have yet to be reached during different time segments (early morning, mid-morning, mid-day, afternoon, early evening, late evening, Saturday, and Sunday) than those at which they were not reached in previous attempts and (b) calling individuals who have been reached (but for whom no call-back appointment exists) during similar time segment when they were reached previously. Within such systems, the potential for calendar delay between calls is obvious.   as excluded because, in the absence of record abstract data: (1) the interview data would have considerably reduced utility, and (2) no locating data would be available from which to even start the tracing process. Exceptions were made to this rule, if sufficient SAR data had been collected (through the CPS) related to both financial aid and locating'. Among the 59,593 NPSAS-eligible (as known at that time) students with sufficient CADE (or CPS) data, only 51,195 were sampled into the Phase-1 locating and interviewing stage (see Table 2.2.9 of Chapter 2 for strata and rates used in this sampling). Among those selected, 12,798 were both located and interviewed during Phase 1. The bulk of these cases (12,620) completed full interviews, but 178 completed only the Spanish-language abbreviated interview. An additional 580 students were finalized during Phase 1; of these 523 were found to be NPSAS-ineligibles (including deceased students) and 57 were determined to be exclusions (e.g., those determined to be incapacitated, incarcerated, institutionalized, out of the country for the duration of the data collection period). The designation \"exclusions\" indicates that the status of the case was resolved without an interview; such cases are considered \"out-ofscope\" for locating and interviewing by telephone. The remaining 37,819 students selected for interviewing were either not located or located but not interviewed during the established Phase-1 level of effort. From this group of Phase-1 \"nonrespondents,\" 27,178 students were selected for the more intensive (and more costly) Phase-2 locating and interviewing effort' (see Table 2.2.10 of Chapter 2 for strata and rates used in this sampling). The bulk of those selected (23,327) were ultimately located or otherwise resolved. Other \"resolutions\" included 803 additional NPSAS-ineligibles (again including deceased sample members) and 636 exclusions. As in Phase 1, exclusion cases consisted of those whose status (generally obtained through some contacted third party) was determined to be such that attempts at interviewing them during the CATI operational period would be futile. Not located cases are classified into two groups: (1) \"ran out of time,\" those for whom tracing either external to CATI or within the CATI-imbedded locator module was still ongoing (but still not fruitful) when data collection activities were ceased and (2) \"all leads exhausted\" cases, those for whom all tracing attempts both CATI internal and external had been exhausted with no success in locating. The first of these categories (which includes cases for whom additional locating leads had been obtained through CATI-external locating services) obviously represents an effect of the constricted time frame for those students sampled late in the process. Among the 21,888 located Phase-2 sample members who were not resolved as NPSASineligible or study exclusions, 18,530 were interviewed. Of these, 14,871 completed the main interview; the remaining 3,659 completed either parts of the main interview, an abbreviated interview, a minimal interview, or some combination thereof (see Section 4.2 for greater detail 18As indicated earlier, no students from 8 institutions (which were particularly late in returning records data) would have been included in the interview sample if this exception had not been in effect. I9Additional efforts in Phase 2 included: referring cases to subcontractors for more intensive tracing, use of specially-trained interviewers for refusal conversion, mailing abbreviated interviews (in hard copy form) to the hearing-impaired and those identified as not having telephones, leaving call-back messages on answering machines, and administering a minimal (5-minute) interview to those who would participate under no other circumstances. regarding partial completions). The 3,358 located sample members who were neither interviewed nor otherwise satisfactorily resolved are also classified into two major groups: final refusals and \"ran out of time\". The latter group contains some cases that were sampled late in the process and simply could not be completed (due to scheduling conflicts for a time to conduct the interview or lack of calendar time to implement full refusal conversion procedures) during the fixed data collection period. It also contains a set of students sampled earlier, who were contacted during the 1995-96 institution year, but who moved after the last term (and were subsequently untraceable) prior to completing enough of the interview to be deemed a respondent. Also, the group likely contains an unknown number of implicit refusal cases (those who, after first contact, use answering machines or friends/relatives as gatekeepers, and those who continue to make --and then break --appointments for an interview \"in the future\"). Weighted and unweighted response rates for Phase-1 and Phase-2 interviewing as well as a weighted overall effective response rate (coverage rate)2\u00b0 are shown in Table 3.7, overall and by selected domains. Those who were determined to be either NPSAS-ineligible or exclusions in either interviewing phase are not included in the computations. In all cases, the phase-specific weights used for rates reflect differential sampling into the applicable phase and have been corrected to account for the fact that NPSAS-ineligibles and exclusions exist within the groups of students that were not located or interviewed. The overall rate accounts for the fact that those selected into phase 2 carry greater weight (since they represent those Phase-1 nonrespondents not sampled for Phase 2); for any domain, this rate is derived as the ratio of the weight sum for all respondents in the domain of interest (using Phase-1 weights for Phase-1 respondents and adjusted Phase-2 weights for Phase-2 respondents) to the weight sum of all cases in the domain of interest who were selected for Phase 1. 20The 1 he effective response rate over both phases represents the interview coverage of NPSAS-eligible and non-excludable postsecondary students from NPSAS-eligible institutions that would have allowed sampling, if asked. An unweighted coverage rate would be relatively meaningless under the differential selection weights for Phase-2.  Respondent counts include partial interviews; a I weighted rates have been corrected to account for any estimated or actual NPSAS-ineligibles or exclusions among the nonrespondents. alnstitutional categories used here were verified by the institutions to correct classification errors on the sampling frames. Student interview strata reflect only those corrections to the initial sampling strata that were b available from CADE data; the few additional frame errors, detected only during the interview process have not been incorporated. The Phase-1 eligible group is comprised of the 51,195 students selected into Phase 1, minus 1,326 found to be NPSAS-ineligible and 693 found to be exclusions during interviewing. d cThe Phase-2 eligible group is comprised of the 27,178 students selected for Phase 2, minus 803 found to be NPSAS-ineligible and 636 found to be exclusions during Phase-2 interviewing. This rate reflects the extent of respondent \"coverage\" of the full CATI sample and accounts for differential Phase-2 sampling rates; under such rate differentials, an unweighted coverage rate would not be meaningful."}, {"section_title": "EST COPY AVAH ABLE", "text": "I\"1 The Phase-1 response rate achieved for all students was about 26 percent, and the weighted and unweighted rates within domain are quite similar (differing by no more than 2.1 percentage points). Phase-1 rates show some variation over the domains considered in the table, but are, for the most part within 2 and a half percentage points of the rate for all students (ranges are from 19 to 30 percent among unweighted rates and from 19 to 32 percent among weighted rates). First-professional students were the most difficult group to resolve in Phase 1 (this is also reflected in slightly lower rates for students at institutions from which these cases were sampled), while the potential FTBs (effort directed toward whom was greatest in all phases) were most easily resolved during the initial interviewing stage21. The unweighted Phase-2 interview rate for all students selected for that phase was 72 percent; the weighted rate was 68 percent. As reflected in that difference, the within-domain weighted and unweighted Phase-2 rates generally differ more from each other than was the case for Phase-1. Considerably greater variability of Phase-2 rates is also observed over the tabled domains; unweighted rates range from 58 to 80 percent, while weighted rates range from 57 to 76. As with many of the rates presented in this report, phase-2 rates are lowest among students from institutions that are for-profit or offer less than a 4-year program. Overall effective response rates are a complex function of both Phase-1 and Phase-2 rates; however, they are more heavily weighted by the Phase-2 results (due to both the considerably greater weights carried by those selected into Phase 2 and the relatively low Phase-1 rates). Coverage for the defined overall student population is 76 percent, and these rates vary from a low of 69 percent to a high of 82 percent across the domains presented in Table 3.7. As with other rates examined, rates are generally lowest among institutions offering programs of 2years or less and among for-profit institutions. Of some additional interest is the uniformly higher coverage rates for federal aid applicants than for non-applicants within the three student groups wherein such a breakdown is shown (i.e., non-FTB undergraduates, graduate students, and first-professional students). This directionality is also reflected (but to a lesser degree) in both the Phase-1 and Phase-2 rates. Federal aid applicants should be more easily located, since additional locating information is available from the SAR-96 data; also, a study of student aid should also be more relevant to recipients, which should lead to higher interviewing rates when they are located. Table 3.8 provides results that allow an examination of the effect of each of the two sequential operations (locating and interviewing, when located) in obtaining responses. For these purposes, only unweighted rates are considered; the examination is also restricted to Phase-2 cases (Phase-1 results are not particularly applicable, since all respondent cases completed therein were both located and interviewed). Consequently these results are based on the 27,178 sample members selected for Phase 2 minus the 1,439 found to be 21 The higher Phase-1 interviewing rate among potential FTBs may also be a function of the \"supplemental\" samples of FTBs who were selected (for whom local telephone numbers were \"fresher\" than for other groups selected at the initial sampling stage.  NOTE: All response rates are unweighted and apply only to eligible cases sampled for Phase 2 interviewing. Institutional categories used here were verified by the institutions to correct classification errors on the sampling frames. Student interview strata reflect only those corrections to the initial sampling strata that were available from CADE data; additional frame errors, detected only b during the interview process are not incorporated. The eligible group is comprised of the 27,178 students selected for Phase 2 interviewing, minus 803 found to be NPSAS-ineligible and 636 found to be exclusions during Phase 2 interviewing. cEligible students were considered located if they were personally reached by telephone or if a third party verified that the number reached d by phone was the student's residence. Including 14,871 complete interviews and 3,659 partial, abbreviated, and/or minimal interviews. (Rate is based on the number of eligible students within the row under consideration. f Rate is based on the number of located students within the row under consideration. BEST COPY AMU 78 CHAPTER 3: OVERALL INSTITUTION, STUDENT, AND PARENT DATA ACQUISITION AND RELATED OUTCOMES NPSAS-ineligible or exclusions during that phase. Rates for locating are based on eligible totals, while the conditional interviewing rates are based on the number located. Sample members were considered to be located if they were reached by telephone or if a third party verified that the phone number reached was at the sample members residence; interviewed cases include both complete interviews (14,871) and partial interviews (3,659). The product of locating rates and the conditional interviewing rate yields the overall unweighted Phase 2 response rate shown previously in Table 3.7. This relationship allows the determination of the relative effect of the two operations on overall response rates. Over all students, the locating rate is 85 percent, as is the conditional interviewing rate; however, over the different types of institutions attended by sample members, the variation in conditional interviewing rates are quite small (ranging only from 81 percent to 86 percent) when compared to those for locating (ranging from 69 percent to 90 percent). Consequently, the ability to locate plays a larger part in determining response rates when considering the types of institutions shown; specifically, the previously noted relatively low response rates among students from institutions that are for-profit and/or that offer programs of two years or less are principally a function of the difficulty in locating them. This suggests that such students are more mobile and independent and/or that the associated institutions are less punctilious in maintaining information allowing the students to be located after graduation. Since the bulk of the student samples from 2-year and less-than-2-year institutions were potential FTBs, it should not be surprising, in light of previous findings, that location rates for the potential FTB student sampling stratum was the lowest of all student strata considered. Among the remaining student strata, both locating and conditional interviewing rates show relatively small variation; however, (with a single exception) the directionality of rates for both locating and interviewing (after location) favors federal aid applicants (supporting the previously advanced rationale for the response rate differences)."}, {"section_title": "Parent Subsample", "text": "As indicated previously, a subset of students was selected for administration of interviews to their parent(s) to obtain supplemental interview data (e.g., parent demographics, finances, and postsecondary decision making regarding their child) that could not be reliably obtained from the student and that were not available from institutional or CPS records. This supplemental information was needed (and sought) for students who were: (1) dependent undergraduates who had not received federal aid; (2) dependent undergraduates who had received federal aid but for whom not all applicable data were obtained from extant records; and (3) \"newly independent\" undergraduate students (i.e., 24-and 25-year old students, who recently passed the age at which they could still be defined as dependent under Federal definitions)."}, {"section_title": "3-21", "text": "CHAPTER 3: OVERALL INSTITUTION, STUDENT, AND PARENT DATA ACQUISITION AND RELATED OUTCOMES Depending, in part, on the order in which they were contacted, the order in which parents and associated students were interviewed varied; however, due to the supplementary purpose of the parent interview, any data obtained therefrom were only applicable if the associated student was determined to be NPSAS-eligible and also interviewee. During the process of parent and student interviewing, parents of selected students were also found to be either ineligible (e.g., deceased, student improperly defined and thus not eligible for parent interview) or excludable from any further attempts at interviewing, even if the student responded. In addition to the typical exclusions discussed previously regarding students (e.g., institutionalized, out of the country, infirm, having no telephone), parents were also excluded if the student or associated institution emphatically insisted that the parents not be contacted'. The rates at which supplemental parent data were obtained among those NPSASeligible students who completed some full or partial form of the student interview are shown in Table 3.9. Among the 5,016 eligible parent interview cases,24 full or partial parent interviews were obtained for 3,352, for an overall unweighted supplementation rate of 67 percent.25 For the categories considered in the table, such rates range from a low of 51 percent among parents of applicable students sampled from private, for-profit, less-than-2-year institutions to a high of 76 percent among dependent, undergraduate, federal aid non-recipients. Even though statistics in Table 3.9 are contingent on response to the student interview, variations of the rates of supplementation among types of involved institutions mirror, in many ways, the variations in student interview rates; rates are lower among the for-profit institutions and among institutions offering programs of less than 4 years."}, {"section_title": "3.4", "text": ""}, {"section_title": "Overall Study Participation", "text": "The students included in the final NPSAS:96 analysis data base were defined to be the overall study respondents or, more accurately, \"yielding cases\".' Of the 63,616 sample students selected from eligible sample institutions, only 51,195 selected for Phase 1 of the CATI sample were considered eligible as yielding cases.\" As a result of the locating and 22 As might be expected, parent interview data were, in fact, collected for some students who were found NPSAS-ineligible or who were never interviewed; data from such interviews were not used. 23 One institution refused to participate unless no attempts were made to contact parents of any of their selected students; continued institutional and student participation were deemed more important to study success than pursuit of a parent interview in such cases. 24 Among NPSAS-eligible students who were interviewed, 5,531 had been selected for parent interview; among those, 60 parents were determined ineligible, and 455 were determined exclusions. 25 Because of the nature of parent interviews and the fact that all CATI-eligible students who fell into one of the three applicable student types were selected with certainty for such interviews, weighted analyses are not particularly meaningful (and were not conducted). The lower rate of parent interviews, when compared to student interviews, reflects the fact that the former were given lower priority than the latter. 26A more stringent response definition was imposed for the subset used as the baseline cohort for the beginning postsecondary student (BPS) longitudinal study. Response rates for first-time beginning students (FTBs) are presented elsewhere in this report. 27 As noted previously, institutional record data (CADE) are available for 55,665 students in a separate data file.  NOTE: Response rates are unweighted (weighted rates are considered relatively meaningless for this group) and based on total eligible parents in the row under consideration. 'Institutional categories used here were verified by the institutions to correct classification errors on the sampling frames. Student interview strata for parent interviewing reflect only those corrections to the initial sampling strata available from CADE data; additional frame errors, detected only during the interview process are not incorporated. bThe eligible group is comprised of the 5,531 parents of students who were both selected for parent interviewing and responded to some form of the student interview, minus 515 parents determined to be ineligible or exclusions during interviewing. interviewing operations, an additional 1,326 sample members were found to be ineligible (some of these were deceased, but most failed to meet one or more of the criteria for NPSAS eligibility); consequently, the number of sample members eligible to be a yielding case was reduced to 49,869. To assure that the study analysis file would contain sufficient meaningful data, an eligible student was defined to be a \"yielding case\" (included in the analysis data file), if either of the following conditions were satisfied: The items in Section A of the Student CATI were sufficiently completed to identify first-time beginning students, or an abbreviated or minimal version of the student interview was completed (mail, Spanish, or refusal conversion instrument), or 2. CADE was effectively complete (i.e., Section 2, regarding enrollment and tuition was complete; the characteristics and subsection of Section 1 was complete; and either Section 3 was complete or comparable CPS, Pell, or NSLDS information was obtained). Using this definition of the overall study yield status, Table 3.10 shows that 48,389 of the 49,869 eligible sample students were classified as \"yielding cases\" for an unweighted student yield rate of 97.0 percent. This table also presents the study yield rates, weighted and unweighted, by various institutional and student characteristics. The weighted yield rates are based on the student sampling weights with adjustments for institutional nonresponse and for student multiplicity (attendance at more than one NPSAS eligible institution during the NPSAS year). The overall weighted student yield rate in Table 3.10 is 96.3 percent. Both the weighted and unweighted yield rates shown in Table 3.10 are quite consistent and in all cases exceed 92 percent."}, {"section_title": "Reinterviews", "text": "Among eligible sample members who completed the NPSAS:96 interview, a sample was selected to participate in a reliability reinterview (containing a small subset of the interview items and to be conducted approximately one month after the initial interview).' A total of 273 respondents were selected for the reliability reinterview. The reinterview sample, together with rates of consent and subsequent participation in a reinterview, are shown in Table 3.11.29 28 Unfortunately, because of delays in development of the CATI reinterview program, as well as in relocating/recontacting some individuals selected for this substudy, the actual time interval between initial interview and reinterview was as long as six months. 29 Due to the built-in delay in administering the reinterview and the plan to complete reinterviews during the same time frame as other interviews, the reinterview population was more heavily weighted with those who responded relatively early to the initial interview; consequently, reported agreement and response rates are probably biased upwards. Reinterview respondents were also disproportionately represented by those most easily located and most easily convinced to participate in the initial interview.   Among the 273 student respondents sampled for reinterview, 250 (approximately 92 percent) agreed to participate. Agreement rate differences among subgroups are not marked, ranging from 89 to 95 percent, with the lowest rates for agreement to participate being among non-FTB undergraduates and students from private, not-for-profit institutions. Among the 250 selected students agreeing to participate in the reinterview, 233 (approximately 93 percent) completed the reinterview. Over half of those who agreed and were not interviewed represented individuals who could not be relocated with the other half representing explicit or implicit refusals at the time of recontact. It is interesting to note that those in the FTB student group (who were subjected to the longest interview) were the least likely student group to be reinterviewed. Also, students from private for-profit institutions were substantially less likely to be reinterviewed, following the initial agreement, than were students in other types of institutions. This group is generally more mobile than others, and thus more difficult to relocate. Despite the nature of the selection process, the reinterview sample was quite representative of the total respondent group in respect to institutional control. Specifically, the percentage distribution of the reinterview sample over institutional control as shown in Table 3.11 (second column) closely approximates that for all respondents (namely, public: 55 percent; private, non-profit: 32 percent; and private, for-profit: 13 percent). The distribution of reinterview selections over student types is also consistent with that for all respondents (potential FTBs: 50 percent; other undergraduates: 38 percent; and graduate/first-professional: Chapter 4 Evaluation of Operations and Data Evaluation studies were planned for NPSAS:96 as part of the overall study design (see Section 2.3.6). Many such evaluations were formative, to assist in day-to-day monitoring of the study and to allow quick detection (for subsequent correction) of problematic operations; other evaluations were summative. Evaluations were useful in identifying potential sources influencing study outcomes, both overall and within the several categories of institutions and students represented in the study. Such results, reported in this chapter, should prove useful in planning for subsequent waves of NPSAS."}, {"section_title": "Enrollment List Acquisition and Processing", "text": "Consistent with NPSAS:93 and to facilitate control over student sample yield, student sampling within an institution was deferred until enrollment lists were obtained for all applicable terms. Given the sequential nature of the student data collection (i.e., CPS matching followed by institutional records collection, and, in turn, telephone interviewing) and the timeframe allotted for these activities, it was important to obtain enrollment lists from a majority of institutions early in the 1996 calendar year and all lists before the summer of that year. However, under the adopted approach, delays were necessitated at schools using certain calendar systems. The process of contacting institutions and obtaining student enrollment lists spanned a seven-month period, from February through August 1996, during which time useable lists were obtained from 836 of the eligible sample institutions. Table 4.1 presents the number of enrollment lists returned by month and by type of institutional calendar system; cumulative receipt is depicted graphically in Figure 4.1. As can be seen, about two-thirds of the lists were obtained within the first three months of the process, and 96 percent of all institutions that ever provided lists did so within five months. Because schools using semester/trimester systems represent about two-thirds of the total participating schools, the \"all institution\" results most closely parallel those with this type of calendar system. Even though reimbursement was offered for computer and staff time needed to compile the lists, obtaining the lists at a number of schools involved a considerable number of prompting and follow-up telephone calls."}, {"section_title": "Ot)", "text": "4-1 CHAPTER 4: EVALUATION OF OPERATIONS AND DATA Other delays were strictly attributable to the particular calendar system used. Institutions using a semester or trimester system were considerably more likely than those on a quarter or continuous enrollment system to provide lists early; 59 percent on the semester/trimester schedule provided complete student lists within the first two months compared to only 4 percent of the schools on the quarter system and 15 percent of the schools on a continuous or other enrollment system. All statistics based on eligible institutions that provided lists. All percentages are based on the \"All Months\" total in the column under consideration. Institutional NPSAS:96 participation (through list provision) was also examined for potential effects of prior NPSAS participation. Arguments have been made on both sides of this issue; prior participation would either reduce current participation propensity (due to past burden experienced) or would lead to higher current participation rates reflecting an overall cooperative propensity. Summary results of these analyses are shown in Table 4.2. Among eligible institutions, the NPSAS:96 participation rate (defined as providing student lists) among the 504 institutions that had previously participated in NPSAS was 95 percent; significantly higher than the 90 percent rate among the 396 that had not. Further (not shown in the table), NPSAS:96 participation was greatest (97 percent) among the 148 institutions that had participated in all 3 prior NPSAS studies.   These results clearly support the participation propensity hypothesis. Moreover, the differences were directionally consistent within public and private, not-for-profit institutions (most pronounced among the latter). No consistent differences of any magnitude were observed among the private, for-profit institutions; also the directional difference was reversed among public institutions offering only programs less than 2 years. Institutional participation across NPSAS studies was also examined in terms of the Carnegie classification categories, as shown in Table 4.3. Table 4.4 shows the number of historically black colleges and universities participating in the current and prior NPSAS studies.   Table 4.5. A single, unduplicated (i.e., with duplicate entries over terms of enrollment removed) electronic list was preferred; however, any set of electronic lists was desirable since they could be more easily unduplicated and used. Overall, two-thirds of the institutions provided some type of electronic list(s); another 12 percent of participating institutions provided simple, unduplicated hardcopy list(s) and the remainder (22 percent) provided hardcopy lists that required manual unduplication. The propensity to provide electronic lists increases monotonically with the level of offering of the institution, probably reflecting the ability to provide such lists; this is quite likely related to the increasing average size of institutions (and associated increasing power and size of the computing facility and staff) as level of offering increases. The modal list type (42 percent) provided by less-than-two-year institutions was unduplicated hard-copy lists; only 17 percent of such institutions provided electronic lists. The percentage of lists requiring manual unduplication was also greatest among the less-than-2-year institutions, and decreased monotonically with increasing level of offering. Returned lists were also evaluated in terms of appropriateness of format and documentation (relative to instructions provided to ICs for preparing lists), and accuracy of student counts; where possible, more appropriate information was obtained from the institutions. Table 4.6 indicates the major types of discrepancies encountered with the lists received. Of considerable note is the fact that: (a) almost half of the institutions provided lists with one or more such problems and (b) among problems encountered, the principal one (involving well over a third of the institutions) was \"suspect count,\" even though that check was considerably relaxed in early May 1996 (actually suspended at that time for less-than-2-year institutions). This check involved disagreement, by 25 percent or more, between the count obtained from lists (after correction for duplication) and the nonimputed2 unduplicated counts from the 1994-95 or 1993-94 IPEDS IC file3. The check was suspended or relaxed because about 85 percent of the institutions that were called about the discrepancy indicated that the sampling list counts were correct. The next most frequent problem experienced with provided lists (involving about 11 percent of the institutions overall) was failure to identify FTBs. This problem decreased with increasing institutional level of offering, reflecting the fact that this type of problem was more prevalent on hard-copy lists (as shown above)."}, {"section_title": "4.2", "text": ""}, {"section_title": "CPS Matching", "text": "Two aspects of the Central Processing System (CPS) matching process (described in Chapter 2) were evaluated for this report. First, the percent of non-CPS match cases for which a Student Aid Report (SAR) was found in the institution records was examined to estimate \"misses\" in the CPS operation. Second, a comparison was made between the Expected Family Contribution (EFC) value obtained from the CPS and the updated EFC value as collected from institution records in CADE, to estimate corrections on the CPS data. 2 If only imputed data were available from both files, the check was not performed. 3 Separate checks were performed, where applicable, for undergraduates, graduate students, and firstprofessional students.  Note: Institutional total includes institutions providing unusable lists (i.e., lists that were not considered of acceptable completeness or accuracy for sample selection). 'Percentages are based on total institutions within institution type under consideration (shown n the first column of the table). Totals do not sum to 100 percent because some lists had multiple problems. \"These checks were considerably relaxed (and suspended for less-than-2-year institutions) on May 7, 1996."}, {"section_title": "EST COPY AVA11LARE", "text": "NPSAS:96 METHODOLOGY REPORT"}, {"section_title": "4-8", "text": "CHAPTER 4: EVALUATION OF OPERATIONS AND DATA An indicator of the comprehensiveness of the CPS matching process is the percent of non-CPS matches for whom a SAR was located at the institution. Since a hardcopy SAR in the institutional records implies that the student should have a record within the CPS database, this statistic can be thought of as a miss rate for CPS matching. That is, this is the rate at which students that should be in the CPS database were not successfully matched. Table 4.7 presents two such rates at which SARs were found for CPS non-matches. SAR rates are presented using both the initial and final counts of non-matches. The first rate is based on the initial CPS matching attempt, which was made prior to initiation of CADE data collection within the institutions and as a result of which over 36,000 students did not match. Among these, a SAR was located during the CADE record abstraction for about 12 percent. This rate is an upper bound estimate, which also reflects the realities of operating within a tight timeframe with less than perfect data. Specifically, students for whom SSNs were unavailable at the time of matching could not have been matched and students first applying for aid in later terms of the 1995-96 financial aid year may have been entered into the system subsequent to the time of initial matching. The second rate shown is based on a smaller subset of students who failed to match the CPS. This subset reflects an additional 3,971 matches that were obtained after collecting CADE records data, principally as a result of obtaining SSNs for additional students, and also for a considerably smaller relative number, availability of later entries made into the system for the 95-96 year. The reduction in the miss rate is dramatic over the first rate, demonstrating the confounding of the rates by inaccurate SSNs. Because this second \"nonmatch\" group still contains sample members for whom no SSN was obtained, these results too must be considered upper-bound estimates. Overall, a SAR was located and entered at the institution for 5 percent of this second group. Of particular note, however, the rate of 18 percent for students in private, forprofit schools and the 3 percent for students in the public institutions. The difference is most likely attributable to the differential rate at which institutions in the two sectors provided sufficient data on their enrollment list from which to construct a CPS ID number.4 Also, students may enroll prior to applying for financial aid. Another evaluation of the CPS matching process was performed using data from the 25,599 students for whom a CPS match was obtained prior to CADE and for whom an EFC value was obtained in CADE.5 Specifically, the CADE EFC value was compared to the CPS EFC value to determine how often the CPS EFC value was updated based on the EFC value in the institutions financial aid records."}, {"section_title": "4", "text": "About 9 percent of students from private, for-profit institutions could not have matched to the CPS, as compared to only 2 percent of those in public institutions. 5Differences between CPS values and institutional record data were assumed to be the result of financial aid administrators using their professional judgement or an institution EFC formula to modify a student's EFC. Hence, the institution's EFC value was considered to be more current that the CPS EFC.  As shown in Table 4.8, the CADE and CPS EFC values matched exactly for about 90 percent of the cases. When EFC updates did occur within CADE, they were typically from one non-zero EFC to another (either upward or downward)."}, {"section_title": "4.3", "text": ""}, {"section_title": "Institutional Record Abstracting", "text": "The use of CADE procedures, by both contractor field data collectors and institutional staff, to abstract information from institutional student records was first initiated in NPSAS:93. As a result of the NPSAS:93 experiences and informal feedback from NPSAS:93 institutional coordinators, a number of procedures were initiated for NPSAS:96 to enhance the effectiveness and user friendliness of the approach, particularly for the institutional CADE user. Other CADE procedural refinements were introduced to facilitate the timeliness of CADE completion, including: (a) prescheduling of schools for field staff, (b) maintaining a \"hot line\" for operational or interpretational problem resolution, (c) scheduled biweekly calls to prompt self-CADE schools and to offer answers to questions that may have arisen: and (d) scheduled weekly calls to field staff to assess their progress."}, {"section_title": "Ease of CADE Software Use", "text": "In general, the refinements to CADE resulted in more efficient operations and fewer reported problems than were experienced in NPSAS:93; however, some challenges were not fully met. Based on feedback from the institutional coordinators, the debriefing of field supervisors and field data collectors was extremely positive, and the relative number of questions from the field were considerably reduced over those experienced in NPSAS:93 or in the NPSAS:96 field test. Contractor field staff, particularly those who had participated in NPSAS:93, reported marked improvement in ease of CADE operation and found both the CADE User's Manual and the training quite helpful. Of particular note was the reported \"excellent depth and breadth\" of the training, particularly the mock situations prepared by NASFAA. The \"hot line\" established was generally well received. During the NPSAS:96 field test the most frequent \"hot line\" call requested information as to the specific meaning of one or more data elements that were being collected in CADE. This issue was substantially reduced through enhancements to the full scale version of the CADE User's Guide. The greatly expanded on-line help screens for the full-scale CADE instrument, including explanations as to the specific nature of information being requested, also played a part in reducing these types of questions. "}, {"section_title": "DES' COPY AVAIABL-f,", "text": "During the full scale study, a frequent problem arising at self-CADE schools involved specific situations of incompatible host systems or insufficient memory for installing CADE. A memory check was included as a part of the self-CADE installation routine, since CADE required approximately 300K of available conventional memory; however, this did not work particularly well for two reasons. First, schools were confused between conventional memory and total RAM. Second, the virus checking routine (a DOS batch program that called virus checking software) behaved somewhat unpredictably under certain Windows 95 configurations, reporting a virus detection when no virus actually existed. This was a relatively infrequent occurrence during the field test, and was unexpected during the full-scale study. Problems with packaging CADE data (i.e. preparing data files for shipment back to the contractor), which were common during the NPSAS:93, were significantly reduced; this was attributed to adding a confirmation prompt to the main CADE menu when the packaging option was selected. The prompt reminded users that packaging was a final step, and additional data entry would not be possible once packaging was complete. Additionally, material was added to the User's Manual to fully describe the consequences of packaging."}, {"section_title": "Pre loading Record Data into CADE", "text": "To reduce the CADE data entry effort, a large number of elements (summarized in Table 4.9) were preloaded into CADE records prior to collection on-site at the institution. This included customizing the financial aid award section of CADE to include non-federal aid that was common to a particular institution. Such customization proved highly successful during the field test, and was repeated for the full scale study. The most extensive set of preloaded data were obtained from the CPS for federal financial aid applicants. In addition to the CADE SAR items, student demographics such as date-of-birth, marital status, and veterans status were preloaded into CADE. Pre loading operations proceeded smoothly, using procedures developed and tested during the NPSAS:96 field test. Delays in this operation that had been experienced during NPSAS:93 were effectively eliminated which is partially attributed to the flexibility of CASES 4.1, in which the CADE software was programmed. "}, {"section_title": "CADE Data Completeness", "text": "Under the relatively stringent definition of a \"CADE respondent6\" in NPSAS:96, the vast majority of the eligible sample students (93 percent) were determined to be CADE respondents. Of the 804 institutions that provided any CADE data, 792 (over 98 percent) provided sufficient data for one or more students to be considered CADE respondents. In large measure this was due to the successful incorporation of data completion checks in the CADE software, which were implemented in the field. The relatively low rate of indeterminacy among the CADE responses is principally attributable to these checks.'. Each NPSAS institution is unique with regard to the type of data maintained for its students, and it was anticipated that not all desired information would be available at every institution. However, as a quality control measure the CADE software was designed to not allow \"skipping\" non-available items by leaving them blank. Instead, the CADE software allowed entry of a \"Data Not Available\" code. In general, the percent of indeterminate (data not available) responses was low for most CADE items. Table 4.10 lists the data elements with an indeterminacy rate of over 5 percent among the CADE respondents. Many of these items are applicable only to certain students (e.g. \"Year Student Took SAT\" is only applicable to those students for whom an SAT score was available at the institution). Consequently, the table is further restricted to items applicable to 1,000 or more respondents."}, {"section_title": "CADE Abstraction Modes", "text": "At all sampled schools, the IC were given an option as to whether information about sampled students would be abstracted (guided by the CADE program) by institutional staff or by a contractor FDC. The first option was the recommended option."}, {"section_title": "6", "text": "In order to be considered a CADE respondent, the student CADE record was required to contain an indication of the student's eligibility, enrollment data (full-time/part-time attendance status, number or credit/clock hours attempted, and/or tuition charges incurred) for at least one term and an indication of the student's financial aid status (aid recipient/non-recipient and/or aid applicant/non-applicant)."}, {"section_title": "BEST COPY MAIM", "text": "All percentages are based on the sample size for the row under consideration. alnstitution classification for this table was verified by the participating institutions. \"This choice was made by the institutional coordinator or chief administrator prior to any attempts at record abstraction. Final method is the procedure through which record abstraction was completed at the institution; the initial method may have been used to collect some data. d Included in this category are eight institutions that provided photocopies of applicable institutional records to the NPSAS:96 contractor; these records were than entered into CADE by contractor central staff."}, {"section_title": "BM? Con AVAILABER", "text": "CHAPTER 4: EVALUATION OF OPERATIONS AND DATA 4.3.5 Timeliness of Record Abstraction CADE systems were prepared on an institution-by-institution basis. The first CADE systems were shipped to self-CADE institutions in early March of 1996 and CADE systems continued to be sent to the field until early September, when the final sampling and CPSmatching were completed. Although the CADE data collection was more than 90 percent complete by the end of August,\" the remaining CADE systems that were ultimately received arrived between September 1 and December 12, 1996. All but eight of the outstanding CADE systems were received before the end of October; the eight were returned in early December. Figures 4.2 and 4.3 summarize the overall flow of completed CADE abstractions at the institutional and student levels, respectively. An indication of the duration of CADE activities, in days, by type of institution is provided in Table 4.12. The proxy measure used for time of abstracting is the number of calendar days between the date on which the CADE system for a institution was initialized at the main campus of the contractor and the date on which the completed and returned CADE data file was successfully read and loaded onto the master CADE data set at the contractor's main campus. This measure is a relatively good index for self-CADE institutions, for which the CADE package was typically sent to the institution within the week it was initialized. For field-CADE institutions, however, the measure typically represents a major overestimation of time needed, since most institutional CADE packages were sent to the field abstractors well in advance of their visit to the involved institution. The table provides minimum days, maximum days, and median days of CADE duration in total and for different institution characteristics. Considerable variation was experienced in CADE duration (from 5 days to 222 days, as is shown in the table). The median number of days to complete CADE was about seven weeks (50 days). Considering both median and maximum statistics, duration was greatest at 4-year institutions not offering doctorate and first-professional programs and least at private, for-profit institutions offering only programs of less than two years. While size of institution is positively related to duration of the CADE effort, the relationship with sample size is restricted to differences of 5 percentage points or less. Major differences (over 20 percentage points) in median CADE duration are associated with method of abstraction. Even though the duration measurement overestimates field-CADE duration (see above), duration was still lowest among institutions choosing this method initially and staying with that choice. Also, for institutions that started with self-CADE and subsequently decided that field-CADE was preferable, the duration statistics are comparable to those for institutions completing under self-CADE. 110f the 792 institutions that ultimately provided data for at least one CADE respondent, 725 (91.5 percent) had returned the CADE system on or before August 31, 1996. Of the 55,665 cases ultimately determined to be CADE respondents, 51,444 (92.4 percent) were received at RTI on or before August 31, 1996.   'The duration of CADE data abstraction for a given institution is defined as the number of calendar days between the date the CADE system was initialized at the contractor's main campus and the date the completed CADE data file was returned and successfully read and loaded into the master CADE data set at the contractor's main office. 'The count of participating institutions includes twelve institutions that provided only a minimal amount of data, thus resulting in no completed CADE cases from these institutions. cBecause appointment dates were established for field-CADE institutions, the CADE system was sometimes initialized well in advance of the CADE appointment data; consequently, the upper values of these duration statistics considerably overestimate the actual abstracting period. The \"Eventually field-CADE\" category represents institutions initially indicating a willingness to perform the data abstraction and subsequently requesting a field data collector. aEr COPY MIME. "}, {"section_title": "CATI External Tracing and Locating Operations", "text": "The NPSAS:96 data collection included several tracing procedures as well as the use of a modified \"locating\" module in the CATI system, which allowed greater flexibility in recording (and subsequently reviewing) tracing history for a given case. In addition, a subcontractor (Fast Data) was used to provide directory assistance database matches, and tracing procedures were also established to use a subcontractor (EQUIFAX) to assist in intensive locating of cases that could not be traced through more routine tracing approaches.' Prior experience of the contractor demonstrated the potential cost effectiveness of utilizing a \"data base search\" approach for obtaining directory assistance. This approach was implemented for NPSAS. Rather than having telephone interviewers call directory assistance when available telephone numbers for a case had been exhausted, the case was automatically labeled as needing directory assistance lookup. These cases, stored in a standard format within a computer file, were regularly shipped as a batch to Fast Data. For a minor cost per \"hit\", Fast Data returned the cases the following morning with up to six new phone numbers. Of the 51,195 students that were sampled for NPSAS CATI, 9,689 required Fast Data services at least once. For 5,053 (over half) of these case, no additional information was found; however, at least one new phone number was obtained for the remaining 4,636 cases (48 percent), and for 2,109 of these (22 percent of the total), two or more phone numbers were returned from Fast Data. A total of 6,884 cases received intensive tracing effort. While EQUIFAX was the primary mechanism by which intensive tracing was implemented, a portion of the CATI cases were also worked by a staff of specialized in-house tracers within the contractor telephone survey facility-5,002 of the intensive tracing were submitted to EQUIFAX, 1,192 were turned over to in-house tracers, and 690 cases were submitted to both EQUIFAX and the in-house tracing team. 12 CATI locating efforts are typically categorized as either \"routine\" or \"intensive\". Routine tracing efforts generally include calling all known phone numbers for the respondent, and contacting directory assistance to obtain additional phone numbers which may reach a subject. Intensive tracing efforts, such as performing data base searches and employing field staff to contact friends and neighbors, are more expensive due to their labor-intensive nature. An optimal tracing and locating strategy generally involves a sequential combination of routine followed by intensive tracing efforts.  Table 4.13 shows the breakdown of intensive tracing cases by institution and student characteristics. Because these tracing efforts were focused to a large extent on cases sampled as potential FTBs13, an artifactual difference in tracing requirements is shown between undergraduate students and graduate and first-professional students. Consistent with results from prior NPSAS implementations, a relatively higher proportion of students from private, for-profit institutions required intensive tracing. This reflects the overwhelming percentage of undergraduate students within these institutions, but also reflects the previously demonstrated greater mobility among such students. Of the 5,692 cases referred to EQUIFAX, new phone and address information was returned for 2,494 (44 percent) and address-only information for another 1,156 (20 percent). Ultimately, 2,406 (42 percent) of the intensive trace cases were successfully located and interviews were completed with 1,712 (71 percent) of those located. Results were comparable for the 1,882 cases referred to the contractor's in-house tracing team (including the 690 cases that were also submitted to EQUIFAX); 880 (47 percent) were successfully located and interviews were completed with 579 (66 percent) of the located cases."}, {"section_title": "CATI Tracing/Interviewing", "text": "A separate CATI input file for students selected for Phase 1 interviewing was prepared for each institution, containing student-level and institution-level data values used to customize the flow and appearance of the CATI interview. This file included preloaded data from completed CADE records. The first CATI input files (for 1,320 students) were created and loaded on May 6, 1996; an additional 3,423 cases were loaded the following day. Loading of data into the CATI system continued on a flow bases until the final set of CATI input files were loaded on October 26, 1996. A total of 51,195 cases were selected for student CATI and loaded into the system, although a portion of these cases were not selected for Phase 2 interviewing. CATI data collection continued for 35 weeks, ending on December 31, 1996, yielding 31,328 full or partial interviews. The duration of the CATI survey was principally attributable to delays in receiving institution enrollment lists, which in turn delayed the CPS matching and CADE data collection and, thus the flow of cases from CADE to CATI. I3This was partially due to the two-stage, sampling approach for locating/interviewing (see Chapter 2).  ZEST COPY AVARAIBLE Figure 4.4 depicts the cumulative nature, over time, of loading cases into CATI and of completing interviews. As is typical for most CATI surveys, the interview completion rate decreases (i.e., the slope of the cumulative line flattens) during the later portions of the study. This phenomenon results from the inevitable accumulation of refusal and unable-to-locate cases throughout the study. The approximately nine-week lag between the point at which the last cases were loaded into CATI and the end of data collection is not atypical."}, {"section_title": "Length of Interview", "text": "The time needed to conduct a student interview is shown, by interview section and student type, in Table 4.14. The administrative timing statistics were computed from time stamps imbedded in the CATI instrument. To use the most timing data available, results were computed for all cases that completed each of the separate sections of the interview and the section times were then aggregated to obtain total administrative time. Sections are listed in the table in the order in which they were presented. The bulk of the differences in numbers of cases contributing to the timing results over sections reflects \"break-off' interviews (which may have occurred with or without a scheduled call-back to complete the interview). a Student classifications reflect status as verified in CADE and CATI. bTotal is computed as the sum of individual section times. This section was only administered to FTBs and first and second year undergraduates. tNot applicable."}, {"section_title": "BEST COPY AVM LAKE", "text": "Average administration time to complete the student interview was 26.2 minutes for all students. Administration time varied by student type with the BPS cohort members (i.e., verified FTBs) requiring 29.4 minutes on average, and other undergraduates and graduate/firstprofessional students requiring 23.5 minutes and 21.5 minutes, respectively. The additional time required for the BPS cohort is principally attributable to Section I (which was only completely administered to FTBs and partially to other first and second year undergraduate students) and the time required to obtain the much more comprehensive Section J locating information for the longitudinal study sample. Other differences in administration time among the student groups are relatively small. Average administration times for the full scale interviews were considerably lower, across all student types, than were those for the field test versions of the interviews. This reflects both a shortening of the instrument and improvements in full-scale interviewer training procedures. Interview administration time, however, reflects only a small fraction of the time required to obtain a completed interview. Additional time is spent by interviewers in locating sample members, scheduling call-backs, attempting refusal conversion, and other related activities. This time is spent not only on cases that are ultimately interviewed but also on cases for whom no interviews are obtained. The average locator/interviewer time requirement for each completed interview is estimated to be about 2.0 hours. Administration time for the parent interview is shown in  "}, {"section_title": "Number and Result of Calls Made to Sample Members", "text": "As indicated in the previous section, a large effort was devoted to locating, contacting, and recontacting sample members. In fact, the large majority of interviewer time was spent in activities other than actually administering the student or parent interview. A small portion of this other time was required to bring up a case, review its history, and close the case (with appropriate comment) when completed. The vast majority of the other time was devoted to contacting (or attempting to contact) the sample members. Table 4.16 shows the total number of telephone calls made to sample members and the outcome of these calls. Calls resulting in voice mail or answering machines are broken out separately in this table, since this type of \"noncontact\" is by far the most frequent and has both cost and procedural implications for future surveys with similar populations. As shown in Table 4.16, the average number of calls made to all sample members was about 12 (although not shown in the table, this was also the average number of calls made to sample members who completed the interview). Locating calls typically represent about twothirds of the total calls. About one-third of all calls resulted in reaching someone, one-third resulted in contact with voice mail or answering machines, and one-third resulted in some other type of non-contact (e.g., busy signal, no answer, non-working number). On average, graduate and first-professional students required fewer calls than did undergraduate students. On the other hand, calls to graduate and first-professional students were much more likely to have contacted voice mail or answering machines than were calls placed to undergraduate students. This finding is consistent with the relatively smaller percentages of answering machine calls among students from 2-year and less than 2-year institutions than among students at 4-year institutions."}, {"section_title": "Identifying First Time Beginning Students (FTBs)", "text": "The NPSAS:96 study serves as the base year of a longitudinal study of students beginning their postsecondary education experience during one of the terms of the NPSAS sample year. Those determined to be such \"First Time Beginners\" (FTBs) will be followed at periodic intervals as part of the Beginning Postsecondary Students follow-up surveys (BPS:96), with the data collected during NPSAS:96 serving as the base year for the subsequent longitudinal studies. NPSAS:96 is the second NPSAS to \"spin off\" a cohort of beginning students; NPSAS:90 was the first. Based on our experiences in the NPSAS:96 field test, RTI implemented sampling and screening procedures which were targeted to yield an adequate number of students that are accurately identified as FTBs for the BPS:96 longitudinal study. Procedures specific to this purpose were implemented at almost every step of full-scale study operations (e.g., detailed instructions for enrollment list requests; sample selection procedures; wording of CADE items asked specifically about potential FTBs; comprehensive BPS-eligibility questions in the student CATI instrument to make the final FTB determination; and extra locating/ interviewing efforts applied to the sample from the student stratum of potential FTBs). Because FTB determination rates were not available from the NPSAS:90/BPS:90 sampling process, FTB sampling rates were based primarily on NPSAS:96 field test results combined with expected improvements based on those results and BPS:92/94 experience. The two major challenges in achieving adequate FTB yields are: (1) proper identification of a sufficieditp from which to obtain FTBs and (2) locating, identifying, and interviewing FTBs from that base in sufficient numbers. The previously agreed upon definition of a pure FTB is: one who enrolled in postsecondary education for the first time after high school during the NPSAS year. This definition was refined for the NPSAS:96 full-scale study to include those who had previously enrolled but who had not completed a postsecondary course for credit prior to July 1 1995 (effective FTBs). This expanded definition shifts the requirement from the act of enrollment to successful completion of a postsecondary course."}, {"section_title": "SequenUaD Procedures for Screenhg IFTBe", "text": "Locating and interviewing potential FTBs is particularly important, since final FTB determination rests on student responses to specific questions.' Student records maintained at most postsecondary institutions do not contain all information necessary to make accurate FTB determinations. Insufficiency of institution-level information is quite obvious when considering students who move from one sector of postsecondary education to another (e.g., from a certificate-granting technical institution to a degree-granting academic institution, or vice versa), and who, consequently, bring no transfer credits (or other records of such prior education) with them to the new postsecondary environment. Nonetheless, institutions can identify FTBs stochastically; however, instructions to institutions regarding preliminary identification of potential FTBs must also be sufficiently clear and viable that the institution can implement them correctly.' Sampling procedures implemented during NPSAS:96 accounted for potential definitional difficulties in a number of ways. As a first screening, institutions were asked to identify potential FTBs, using as criteria that such students be: undergraduate students, having a first enrollment at the institution in a term starting during the NPSAS:96 year (between May 1, 1995 and April 30, 1996), classified by the institution as freshman, or first-year student at the time of that first enrollment, and who had no transfer credits from another postsecondary institution. I5Simply asking the institution to identify students who enrolled in the institution for the first time is insufficient, since it can result in identification of undergraduate transfer students as well as first-time enrolling graduate and first-professional students. Based on prior experience, it was anticipated that two types of errors would still exist in lists provided by the schools; specifically, (1) students listed as potential FTBs would not be actual FTBs (a false positive group) and (2) students not identified as potential FTBs would, in fact, prove to be FTBs (a false negative group). The actual BPS:96 cohort would thus consist of those in the potential FTB group minus the identified false positives in that group plus any false negatives identified in other student strata. Because experience with BPS:90 indicated that the false positive rate would exceed (considerably) the false negative rate, the potential FTB stratum was considerably oversampled (see Chapter 2). A second stage of screening for FTB status occurred during record abstraction. Students who were classified as undergraduates at the conclusion of CADE were identified as potential FTBs for CATI subsampling if: the student graduated from high school in 1995 or 1996; the CPS year-in-school variable indicated that the student was an FTB; or high school graduation year and CPS year-in-school variables were missing, but the student was born in 1977 or later. Potential FTBs whose CADE data did not contradict the sampling list requirements (first-year students attending the NPSAS institution for the first time during the sample year, not transferring credits in) continued to be treated as potential FTBs for CATI subsampling. Other students were classified as \"probable non-FTBs.\"16 Classification at this stage was particularly important since potential FTBs were selected for CATI with certainty whereas subsampling occurred for other student strata (see Chapter 2). The final (CATI interview) FTB screening, was accomplished very early in the interview (immediately following NPSAS study eligibility determination).17 The FTB screening questions were asked of all interviewed students so that not only would false positives from the potential FTB stratum be eliminated from the BPS cohort but also false negatives from the other student strata would be identified and included in the BPS:96 cohort."}, {"section_title": "Basic Results for Establishing the BPS:96 Cohort", "text": "As mentioned above, FTB determination was a three-stage process, including information gathered during: list acquisition; CADE data collection; and then CATI data collection. Table 4.17 provides results of CATI-based FTB determination based on student classification. Overall, 39 percent of the students interviewed (12,207 students) were determined to be FTBs. Among those initially sampled as potential FTBs based on the list acquisition process, 72.4 percent of those subsequently interviewed were determined to be FTBs, yielding a 27.6 percent false positive rate. The false negative rate was 29.3 percent for the students for whom FTBlikelihood was not established by the institution, 9.1 percent for those identified as non-FTB undergraduates and 0.2 percent or less for those sampled as graduates or first-professionals. t6This indicator was temporary, since final FTB determination was not made until the student CATI interview."}, {"section_title": "17", "text": "FTB status was determined at the start of the student CATI interview, since many subsequent questions were to be asked only of the actual BPS cohort. The reasonably low false-negative rate results highlight the fact that, in general, schools were fairly successful in identifying students who were not likely to be FTBs. On the other hand, the high false positive rate revealed the difficulties that many schools experienced in accurately identifying FTBs. Table 4.17 also displays the confirmed FTB rates based on the student re-classifications done subsequent to CADE and prior to CATI. The false positive rate reduced to 21.8 percent for potential FTBs among those interviewed. The false negative rate for other students was much lower as well: 4.3 percent for other undergraduate students, 0.1 percent for graduate students, and 0.0 percent for first-professional students. This second sampling stage was quite effective in reducing both the false negative rate and the false positive rate; however, the false positive rate was higher than anticipated. Institutions, in many cases, did not have the necessary information to be able to classify FTBs correctly.  Table 4.18 reflects the results for those initially sampled as potential FTBs during list-sampling (incorrect classifications may be considered false positives) and Table 4.19 provides the results for students not sampled as potential FTBs (incorrect classifications may be considered false negatives). The 4-year institutions did much better than the less-than-4-year institutions in identifying potential FTBs. The FTB rate for potential FTBs at 4-year schools was more than 80 percent whereas it was less than 55 percent at other schools. Similarly, the false negative rate was much lower for 4-year schools. Private, forprofit schools had much higher false positive (only 55.3 percent confirmed FTBs among potential FTBs) and false negative (18.9 percent) rates than other institutions."}, {"section_title": "Types of First Time Beginners", "text": "As mentioned above, FTBs included those identified as pure (began postsecondary education for the first time during the NPSAS sample year) or effective (had not completed a postsecondary class prior to NPSAS sample year). In addition, a number of FTBs were identified as being FTBs, but attending another institution during the NPSAS sample year prior to attending the NPSAS institution. For instance, a student may have attended one institution during the summer of 1995 and then another institution beginning with the fall of 1995. Such students that were sampled at the second institution were appropriately treated during the NPSAS interview as FTBs. Institutional records, though, were only collected for the NPSAS sample institution, and not for the first institution. Table 4.20 presents a distribution of FTBs by FTB type and whether or not the student was a FTB at the NPSAS institution. More than 95 percent of the known FTBs were pure FTBs who attended their NPSAS institution first. Nearly 99 percent of the students determined to be FTBs were pure or effective FTBs at the NPSAS institution. About 97 percent of the known FTBs were pure FTBs. For the BPS:96 cohort, all of the pure or effective FTBs who attended the NPSAS institution first will be included in the sample as NPSAS respondents known to be BPSeligible (12,040 FTBs).  Monitoring CATI data collection in progress serves the dual purpose of (1) providing information about the overall level of error in the facility to ensure that the interviewing process remains in statistical control (and to keep overall error within acceptable limits), and (2) improving interviewer performance by reinforcing good interviewer behavior and discouraging poor behavior. The data reported in this section reflect the monitoring conducted during the NPSAS:96 data collection using the RTI telephone monitoring system. The system provides for sampling of interviewers and interview items during CATI operations. Monitors listen to sampled interviews and observe the data collection using remote monitoring telephone and computer equipment and software. They record their observations on laptop computers which contain computerized monitoring forms. During the NPSAS:96 training sessions for interviewers and supervisors, selected staff received training on the monitoring system. Interviewers were informed of what types of interviewer behavior would be allowed and what types must be avoided. Supervisors, who served as monitors, were instructed on how to identify proper and improper interviewer behavior and how to record their observations on the laptop computers. In addition, all staff were told the purpose of the monitoring data and how the data were to be reported. Monitors were instructed to focus on two interviewer behaviors including: delivery of questionnaire text--to identify proper reading of the questionnaire text (verbatim) or appropriate modifications of the text based on prior statements made by the respondent, or improperly skipping over questions; and CATI entry--to identify correct recording of responses to interview questions. Twenty monitors were responsible for quality control monitoring. They recorded a total of 102,000 observations during the data collection period. Monitoring data were collected during the first 28 weeks of data collection. Monitoring efforts were eliminated for the last seven weeks of data collection, given the greater experience of the remaining interviewers and satisfaction by project staff that the process was in appropriate control. The monitoring results are presented in Figures 4.5 and 4.6 for Question Delivery and Data Entry, respectively. Seven four-week periods of data collection are designated for each figure. On most studies, interviewers experience a learning curve, a short time at the beginning of their study efforts during which they are still developing their skills with a particular survey instrument and study procedures. This learning curve may not be clearly evidenced in these figures because the underlying data reflect attrition of some interviewers as well as the subsequent addition of new interviewers over time to accommodate that attrition.   Week The facility-level data shown in the figures include the percentage of instances where a behavior was observed.' A deviation outside of the control limits is indicative of an unexpected behavior pattern. These deviations can be explained either by a change in the interviewing process, a change in the monitoring process, or a problem with interviewer performance. In the discussion below, we summarize the monitoring data for the two behaviors that were observed. CATI Question Delivery. NPSAS telephone interviewers were trained to employ a conversational style interviewing approach, which is intended to reduce respondent burden and thereby enhance survey response rates. This approach requires \"active listening\" by the interviewer and appropriate use of information received from the sample member as part of, or in addition to, an answer to a question asked previously. For example: to the question \"What is your marital status?\", a person might say \"I'm married and my wife and I have two daughters, ages eight and six.\" In this case the respondent has provided a lot of information, in addition to the simple answer to the question posed, and this additional information can and should be used appropriately in the rest of the interview. For example, it would be awkward and inappropriate to then ask this person: \"Do you have any children?\" and, if so, \"How many of your children under age 21 live with you?\" Rather, adapting a conversational interviewing approach, the interviewer might say: \"You said you had two young children living at home, right?\" 1 8 The upper and lower control limits were calculated as plus (for upper limit) and minus (for lower limit) three times the standard error associated with the-cumulative proportion of errors to the number of questions observed for the given period. When operational changes were introduced into the system (e.g., major infusion of new interviewers), cumulative computation of limits was restarted. Note, however, that the interviewer attempts to obtain responses to the current question by confirming information obtained in response to a prior question; however, this approach does not justify \"skipping\" questions that are applicable to the particular respondent. Thus, given the conversational style interviewing approach used in NPSAS, the literal reading of the screen wording for each item, while acceptable, is not required for a question to be delivered correctly. However, any deviation from item wording that results in changing the intent of the question or obscuring the question meaning would constitute incorrect question delivery, as would skipping the question entirely. Figure 4.5 provides the monitored error rates, at four-week intervals, for CATI question delivery. As can be seen, errors of this type were quite low throughout the data collection period. The cumulative question delivery error rate, based on all observations across the 28-week monitoring period, was less than 2 percent. CATI Entry. CATI entry error occurs when the response to a question is recorded incorrectly. The error rates of CATI entry are somewhat higher than might be expected for this study (Figure 4.6). This may be attributed to two factors. First, it is difficult for monitors to observe the recorded entries before the screen switches to the next question, so the error rates may be misreported. Second, CATI monitoring included all types of items and, therefore, some of the error reported is attributable to simple misspelling of open-ended (literal) responses by the interviewers. These behaviors were observed during the early stages of data collection and addressed both with the monitors and the interviewers. The effect of such retraining is reflected in Figure 4.6, which shows a consistent decline in data entry error rates over the data collection period. The facility average data entry error rate for the full data entry monitoring period was less than 2 percent."}, {"section_title": "CATI On-line Coding", "text": "The NPSAS CATI system included tools which allowed computer-assisted on-line assignment of codes to literal responses. On-line coding systems are designed to improve data quality by capitalizing on the availability of the respondent at the time the coding is performed. Interviewers can request clarification or additional information if a particular open-ended value or text string cannot be successfully coded on the first attempt, an advantage not afforded when coding occurs after the interview is complete. Because the literal string as well as code selected are both captured in the data file, subsequent quality control recoding by central office staff can be easily incorporated into data collection procedures. The on-line coding occurred in three substantive areas: postsecondary education institution, major field of study, and industry/occupation."}, {"section_title": "Institutional coding was needed to assign a six-digit Integrated Postsecondary Education", "text": "Data System (IPEDS) identifier for institutions other than the sample institution that respondents reported attending during the 1995-96 year. The system relied on a look-up table, or coding dictionary, of institutions. The dictionary was constructed from the IPEDS institution database. Other information in the dictionary (institutional level of offering, institutional control, and annual undergraduate tuition information) was retrieved into CATI for later use (e.g., for branching or as prompts for certain questions), once the institution was identified and confirmed. Major field of study coding and industry/occupation coding utilized a dictionary of word/code associations. The on-line procedures for these coding operations were the following:"}, {"section_title": "4-37", "text": "CHAPTER 4: EVALUATION OF OPERATIONS AND DATA (1) the interviewer keyed the verbatim text provided by the respondent; (2) standard descriptors associated with identified codes were displayed for the interviewer; and (3) the interviewer selected a listed standard descriptor. Each coding operation was subjected to quality control monitoring. Although monitoring error rates were not maintained separately for the coding systems, they are reflected in the overall CATI monitoring error rates discussed above. In addition to the quality control monitoring, interviewers were routinely monitored specifically with regard to their performance in using the on-line coding software. Supervisors debriefed (and retrained when necessary) the interviewers based on this qualitative assessment. The coding systems also received prominent focus during the telephone interviewer quality circle meetings, with expert coders providing guidance regarding the appropriate coding of particularly difficult items. During these sessions, interviews were reminded to provide comprehensive information in the verbatim text, to facilitate any subsequent recoding. Coding results were extracted and examined approximately every two weeks during data collection. The verbatim strings were evaluated for completeness, as well as for the appropriateness of the assigned codes. Approximately five to seven percent of the strings were recoded, although very few resulted in a shift across broad categories. Following each extraction/examination, the industry/occupation and major field of study coding dictionaries were expanded to include codes for descriptions collected in CATI interviews that were not previously represented in the dictionary. Table 4-21 shows the results of the NPSAS on-line coding procedures. 'The IPEDS, Industry, and Occupation coding could occur multiple times within any single interview. For example, occupation data was collected from students regarding their current jobs and career aspirations. Similarly, students were asked about up to four institutions (other than the sample institution) they attended during the NPSAS year, and were also asked about institutions from which they may have received a bachelors degree earlier in their academic career. All these entries were subjected to the IPEDS coding system. As the table shows, interviewers experienced the most difficulty attempting to code institutions. The IPEDS coding system required the student to report the school name, as well as the city and state in which the school was located. An incorrect school name or city/state combination usually resulted in an unsuccessful coding of the institutions. Based on the relative completeness of the verbatim text available, 1,143 of the 2,450 unsuccessfully-coded cases were identified as candidates for post-data collection recoding and assigned to a team of expert coders. The team successfully coded 237 of the institutions, and another 737 institutions were in foreign countries (i.e. \"uncodable\" was a correct entry). This activity raised the IPEDS coding success rate to just under 90 percent. "}, {"section_title": "CATI Data Indeterminancies", "text": "A major goal of any survey is to minimize the amount of nonresponse to individual data elements as well as to entire survey instruments. For NPSAS:96, allowances were made in the CATI to accommodate responses of refusal and \"don't know\" to every item, by special keyed entry by the interviewers. Refusal responses to interview questions are most common for items considered sensitive by the respondent, while \"don't know\" responses may result from a number of potential circumstances. The most obvious reason a respondent will offer a \"don't know\" response is that the answer is truly unknown or in some way inappropriate for the respondent. But, \"don't know\" responses may also be evoked (1) when question wording is not understood by the respondent, without explanation by the interviewer; (2) when there is hesitancy on the part of the respondent to provide \"best guess\" responses, with insufficient prompting from the interviewer; and (3) as an implicit refusal to answer a question. Refusal and don't know responses introduce Indeterminancies in the data set and must be resolved by imputation or subsequently dealt with during analysis. Overall item nonresponse rates in the student CATI were fairly low, with only 54 of the over 1,000 variables included in the final CATI data set containing over 10 percent missing data. These items are shown in Table 4.22, grouped by interview section. Item nonresponse rates are calculated only for those sample members for whom each item was applicable and asked. Reliability, as examined here, involves the stability of responses over time (i.e., temporal consistency); consequently, analyses generally focus on data items that are expected to be stable for the time period between the initial interview and the reinterview (e.g., factual rather than attitudinal data). The design of the reinterview study called for reinterviews to be conducted within one month of the initial interview; allowing enough time for respondents to forget their previous answers but not enough time so that actual changes in status would make truthful answering produce different answers (which would be indistinguishable from unreliability). Unfortunately, time delays in conducting reinterviews may have contributed to the occurrence of real change (between the initial interview and reinterview) in the status of the information requested of respondents.19 19 The bulk of the initial interviews were conducted prior to the end of the financial aid year (June 30,1996 or current institution year); however, a number of questions use such time points as the end of a stated reference period, introducing the potential for real change during the time between the initial interview and the referenced time point. Also, some reinterview respondents were contacted up to six months after completing the main interview (in some cases after they had begun another year of school) and'for questions with unspecified reference periods, potential for change obviously exists.  Table 4.22-Student interview item non-response for items with more than 10 percent \"don't know\" or \"refused\"  Table 4.22-Student interview item non-response for items with more than 10 percent \"don't know\" or \"refused (continued) Statistics are based on student sample members for whom specific items were applicable and asked. Items applicable to less than 100 sample members were excluded from consideration. Items with the largest amount of nonresponse were those pertaining to undergraduate and graduate entrance examination scores, with two-thirds or more of the students interviewed and reporting having taken the SAT or GRE unable to recall their scores on these exams. Questions most likely to evoke explicit refusals we concerning student and parent income, assets, and debt, which also provided high rates of \"don't know.\" Many student respondents are reluctant to provide information about family finances and, among those who are not, many simply don't know."}, {"section_title": "Reliability of Interview Responses", "text": "The NPSAS:96 interview responses were also evaluated for reliability and validity, to estimate the consistency of the measurements. Reliability was assessed through reinterviews (see Section 3.5, above) using selected items of the full interview. Reinterviews were administered to a randomly selected subsample of those who completed the full interview; analyses were based on the 236 respondents to the reinterviews. The set of reinterview questions were chosen to broadly represent the student interview; however they were most heavily weighted to cover financial aid, CHAPTER 4; EVALUATION OF OPERATIONS AND DATA financial support for educational expenses from family, educational status of family members, and work experiences while enrolled in institution. Specific items selected were those that had not been evaluated in previous NPSAS studies or had previously proven problematic (and had been refined for the current study). A hard-copy facsimile of the reinterview is provided in Appendix D. Given the relatively small size of the reinterview sample, it is difficult to determine the extent to which the length of time before reinterview affected the amount of true change; however, the period between initial interview and reinterview was substantially longer that in previous reinterview studies. Consequently, these analyses provide a more conservative test of temporal consistency and derived reliability indices should be considered lower-bound estimates."}, {"section_title": "Analytic Method", "text": "Because not all items were applicable to all respondents (e.g., some questions were asked only of graduate/first-professional students, some only of dependent students) and because analyses were restricted to those cases with determinate responses in both interviews,' considerable variation exists in the number of cases on which reliability indices are based for the several items considered. Reliability indices computed for all paired responses were: (1) percent agreement between the two responses and (2) one or more relational measures of reliability. Percent Agreement. Most examined paired responses can be classified as categorical variables; possessing either nominal or ordinal scale properties; however, some variables (such as dollar amounts, hours worked per week, or untruncated counts) possessed interval or ratio scale properties. For categorical variables, agreement was defined as an exact match between the two responses. For the free response, qualitative variables, some additional latitude was allowed (since failure to produce the exact same number would not be evidence of serious unreliability). For such items, the standard adopted for \"agreement\" (as used previously with NPSAS reliability analyses) was within one standard deviation unit.21 20Additional restrictions existed for item 23: About how much did you earn from all jobs while you were enrolled?; the response to this item was qualified by the response to item 24 which indicated the units (simultaneously chosen by respondent) in which the item 23 was reported (amount per hour, per week, per month, per term, or for the entire enrollment period during that year). Only about half of those responding to these items used the same units in the initial interview and reinterview. Since conversion to a common metric would require considerably more information than was available in the reinterview, reliability analyses for item 23 were further restricted to those who also responded consistently to item 24. Moreover, analyses were conducted separately for each reporting unit, since analyses across units would produce spuriously high reliability indices. 21This is equivalent to within one-half standard deviation of the average (best estimate of actual value) of the two responses. NPSAS:96 METHODOLOGY REPORT 4-42 CHAPTER 4: EVALUATION OF OPERATIONS AND DATA Index of Reliability. An index of reliability, comparable to the interclass correlation coefficient used in assessing inter-rater reliability,' was computed for all paired observations, since it is defined for both categorical and quantitative items. For categorical items, the index can be computed for an entire item as well as for each response alternative of the item, which provides a mechanism whereby differential reliability of specific response alternatives can be evaluated. The reliability index is the additive inverse of the ratio of estimated error variance of an item to the to total item variance, and the ratio estimate used here is I, the inconsistency index, which is widely used by the Census Bureau for test/retest reliability studies'. Consequently, the reliability index used here is given as 1 I. The index of reliability for categorical variables is algebraically equal to kappa 00; Fleiss24 recommends the use of x for categorical data because it represents a chance-corrected percent-agreement score (the number of actual consistent answers divided by the number expected by chance based on the marginal probability of each answer -with the obvious implication that the raw percent agreement will always equal or exceed K) and it is the analog of the intraclass correlation coefficient for quantitative variables25. Other Relational Indices. While the index of reliability can be applied to data with scale properties greater than nominal, it does not completely reflect the nature of ordinal, interval, or ratio scale properties. To accommodate such properties, more traditional relational measures consistent with prior NPSAS reliability analyses have been used. For questions that are answered using ordered categories (including truncated counts) the Kendall's tau-b (rb) statistic, which takes into account the obviously-present condition of tied rankings, has been used.26. For items yielding interval or ratio scale responses (such as the amount earned), the Pearson productmoment correlation coefficient (r) is used. As would be expected, these relational statistics, which take into consideration the additional metric properties of the data, uniformly yielded estimates of reliability that were equal to, or greater than, that shown by the index of reliability."}, {"section_title": "4-43", "text": "CHAPTER 4: EVALUATION OF OPERATIONS AND DATA 4.6.4.2 "}, {"section_title": "Reliability Results", "text": "Results of the overall reliability analyses for the NPSAS:96 reinterview study, for specified content areas, are provided in Table 4.23 through Table 4.26; for the strictly nominal scale data items, this presentation provides only the aggregate (item level) value of the index of reliability. A copy of the reliability reinterview instrument and the variable names of the student CATI items associated with each reinterview item appear in Appendix D, starting on page D-111. Reliability indices for the educational finance items, as shown in Table 4.23 are generally acceptable, but are somewhat mixed. While all such items show agreement over the two administrations exceeding 80 percent, the relational statistic only exceeds 0.80 for item 18, regarding receipt of aid from an employer, and item 21, regarding total amount borrowed for postsecondary education. This latter item, treated as a quantitative or continuous variable, shows the highest relational statistic in the set (r = 0.89)27 but the lowest percent agreement (83 percent). The least reliable item in this set is item 20 (regarding acceptance of all aid awarded), yielding an 85 percent agreement rate and a relational statistic near zero (K = 0.16); the disparity between the two reliability indices lies with the instability of \"no\" responses to this item. Analyses are based on 236 respondents to the reliability interview.\u00b0A nalyses were conducted only for respondents with determinate responses on both the initial interview and the reinterview; not all questions were applicable to all respondents. 'Unless otherwise indicated, this percentage reflects an exact match of the paired responses. Unless otherwise indicated, this measure is the aggregate reliability index, K.\u00b0A greement was determined as response differences not exceeding 1 standard deviation unit. Relational statistic used here is the Pearson product moment correlation coefficient, r. 27 The value of ic for this item was also 0.89; however, the variability of amounts reported is quite large; this large variability also increases the value of the reliability index with all other things being equal. CHAPTER 4: EVALUATION OF OPERATIONS AND DATA Such negative responses are rare (between five and 10 percent for initial administration and reinterview), and reported quite inconsistently; of the 24 individuals who responded negatively in either administration, only 3 responded consistently in both28. A similar (but less extreme) situation, i.e., lower frequency of \"no\" responses that are relatively instable, applies to item 2 and item 12, and consequently depresses the consistency statistic. Even for the most reliable of the categorical items in the educational finance set (for which the percent agreement is 97 percent), the value of K has been similarly depressed to 0.82; in this case, due to the instability of the infrequent (only about 10 percent of respondents) \"yes\" responses. Although time referents were explicitly or implicitly (through prior \"set up\" questions) available for all items in this set, some inconsistencies still may have resulted from confusions on the part of respondents (e.g., assuming, during reinterview, that the 1995-96 school year included the spring term of 1996). Also, there is possibility for real change between the initial interview and reinterview. Both such cases, however, would typically result in changes of responses from \"no\" to \"yes,\" but no such trend was noted. In fact, for this set of items, response changes were generally more heavily weighted in the opposite direction. As presented in Table 4.24, two of the three items related to work experience, show only marginally acceptable reliability; agreement of initial and reinterview responses for these two items is less than 70 percent, and relational statistics (tb) are less than 0.65 (in one case less than 0.50). However, the remaining item in this set, item 22, regarding average hours worked per week, shows good reliability; 83 percent agreement and a Pearson correlation of 0.89. \"Analyses were conducted only for respondents with determinate responses on both the initial interview and the reinterview; not all questions were applicable to all respondents.\u00b0U nless otherwise indicated, this percentage reflects an exact match of the paired responses. Unless otherwise indicated, this measure is Kendall's Th. 'Agreement was determined as response differences not exceeding one standard deviation unit. Relational statistic used here is the Spearman product moment correlation coefficient, r. 28 The basic instability of negative responses to this item suggests that its meaning is being misunderstood; the wording should be reworked before the item is used again and interviewers should be better trained on administration of the item. For item 17, regarding number of jobs, and item 25, regarding frequency of work during enrollment periods, percent agreement between the two responses was about 68 percent. The relational statistics (tb was used in both cases to accommodate the ordinal properties of the response option sets) were 0.61 and 0.48, respectively. The reliability index (K) was lower in both cases, 0.51 and 0.42, respectively, suggesting that error accounts for about half (or more) of the observed variation of individuals' responses. For item 17, however, the value obtained is considered to be depressed due to real change that took place between the initial interview and January 30, 1997; namely, taking a summer job after the spring term of the 95-96 school year. The micro data are consistent with this hypothesis, since they show that the predominant (but not all) change involved an increase in number of jobs reported during the reinterview. The inconsistency of responses to item 25 (Did you work for all or most of the weeks you were enrolled?) are probably traceable to respondent (or interviewer) confusion with the compound nature of the question posed, particularly in interpreting the meaning of the available response alternatives29 in relation to that question. The most inconsistent response alternative was \"about half\" the weeks enrolled (among the eleven respondents who chose that response alternative in either administration, only one consistently gave this response) and the next most inconsistent response was \"most of the time\" while enrolled. The consistency index for these two response options were 0.14 and 0.37, respectively, probably reflecting the difficulty of determining the desired range for the two responses (e.g., is working 5 weeks out of 9 \"most of the time\" or \"about half \").3\u00b0T he items related to income were somewhat mixed (Table 4.25). The single categorical variable in this set yielded a percent agreement of slightly over 80 percent, but the reliability index was only 0.60, indicating that an estimated 40 percent of total item variance is attributable to error. The error is directional (a notable increase in \"yes\" responses during the reinterview), which is consistent with a relaxation in parental requirements of repayment, but there is no plausible reason to hypothesize such change from one school year to the next. Because the question is so straightforward, it seems more likely that the inconsistency is related to either: (1) misreporting (or misrecording) or (2) differential interpretation due to differences between the two interviews in the interview context, as established by preceding items, within which the item was embedded. The two items relating to personal (plus spouse's, where applicable) income during previous years (item 26 and item 27) seem acceptably reliable, particularly for reported income values, which are notoriously unreliable. Agreement of the paired quantitative responses to these items exceeds 87 percent, and the values of r are between 0.74 and 0.79. The discrepancy, for item 23, between the overall agreement of 78 percent and the Pearson correlation of near unity (0.99) is somewhat artifactual, resulting from a spuriously high correlation. Recall that this item was analyzed separately within groups that reported the same referent unit (e.g., per hour, per"}, {"section_title": "29", "text": "Response alternatives to the question were: (1) Yes, every week while enrolled; (2) Yes, most of the time while enrolled; (3) No, only about half the weeks while enrolled; and (4) No, less than half the weeks while enrolled. 30T he he lack of consistency in this item (and particularly its nature) strongly suggests that the item be reworked prior to additional use and/or that training of interviewers in administering this question be improved. NPSAS:96 METHODOLOGY REPORT 4-46 CHAPTER 4: EVALUATION OF OPERATIONS AND DATA month) in both interviews (which also accounts for the relatively small analysis group). While the correlations within the smaller analysis groups were also reasonably high (ranging from 0.77 to 0.98),31 the analysis of the combined groups (each with a different unit of measurement) creates a natural clustering of the reporting unit groups to produce an artificially high relationship. Although based on a relatively small number of cases, this item is considered acceptably reliable. Analyses are based on 236 respondents to the reliability interview. \"Analyses were conducted only for respondents with determinate responses on both the initial interview and the reinterview; not all questions were applicable to all respondents. \"Unless otherwise indicated, agreement was determined as response differences not exceeding one standard deviation unit.. Unless otherwise indicated, this measure is the Pearson product moment correlation coefficient, r. This percentage reflects an exact match of the paired responses Relational statistic used here is the aggregate reliability index, x. This value is artifactually inflated because of the clustering of the values provided by their unit of reference (i.e, from per hour to for the total time of enrollment); within specific unit-of-reference groups, however, the correlations were still high, ranging from 0.77 for those reporting earnings per week to 0.98 for those reporting a grand total. Reliability indices for items related to personal and family educational experiences are provided in Table 4.26. With one exception, item 4 (related to extent of completion of all requirements for degree among graduate and first-professional students), these items are acceptably reliable. While reliability estimates for this item are based on less than 30 cases, they are nonetheless quite low (about 61 percent agreement among the paired responses and a cb of 0.22). The reliability index for the item was also low (x = 0.21), indicating that about 80 percent of total variance is attributable to error. The reinterview item was asked retrospective to a specific date, which, for effectively all of these students, was toward the end of the final term of the 95-96 regular school year (the initial interview couched the question in terms of \"currently\"). 31 The computed values of r were: 0.97 for those reporting amount per hour, 0.77 for those reporting amount per week, 0.92 for those reporting amounts per month, and 0.98 for those reporting total amounts for the time they were enrolled.  'Analyses were conducted only for respondents with determinate responses on both the initial interview and the reinterview; not all questions were applicable to all respondents. \"All percentages are based on exact matches of the paired responses. Unless otherwise indicated, this measure is Kendall's Th. dRelational statistic used here is the aggregate reliability index, x It is quite conceivable that in the reinterview, the students interpreted the date as \"at the close of the term,\" in which case the potential for real change exists. There is fairly strong indication that a considerable part of the \"error\" may be in actuality attributable to such real change, since the preponderance (but not all) of the response changes are in the direction that would be expected32. Nonetheless, the item remains potentially problematic; since error may be introduced by confusion on the part of either the respondent or interviewer, the item should be tested further. A similar question (item 3) was asked of undergraduates. While similar arguments can be made regarding confusion of time points, distributional properties of this item do not support an hypothesis of real change, since net change was (slightly) in the direction of less completion reported in the reinterview. Again, however, the bulk of the inconsistency is traceable to the \"intermediate\" response option in this case \"No, completed program but have not yet received degree.\" The reliability index for this resource option was 0.12, and of the 10 individuals choosing this option in either the initial interview or the reinterview, only 1 chose it consistently; 32'I 'hat is, responses generally changed from \"not completed course work\" to \"completed course work but not all requirements\" and from this latter category to \"completed all requirements. suggesting that the intermediate options in both this item and item 4 could either be misunderstood by the respondents or miscoded by the interviewers. Even with this problem, however, item 3 yielded acceptable reliability (92 percent agreement, 'CI, = 0.72, and x = 0.68). The items 7, 28, and 29 also showed acceptable (but not high) reliability, and for all such items, the principal inconsistencies could be traced to specific response alternative sets. (It is considered unlikely that real change could have affected the reliability estimates of any of these items.) The major inconsistency in item 7 (related to living arrangements while going to school) lay in the \"non-standard\" living quarters response options (i.e., 'off-campus in school-owned housing,\" \"with relatives other than parents,\" and \"some place else\"). Although these options were combined into a single category for purposes of analysis, the index of reliability for the combined response options was only 0.37 (of the 20 respondents who chose any of these three options in either the initial interview or the reinterview, only 5 consistently chose one of the three). The reduction in reliability for item 28 and item 29, which are quite similar items related to father and mothers highest level of education, generally stemmed from quite similar sources. Specifically, response options were unstable if they indicated that the parent had completed some postsecondary education but had not completed at least a 4-year program.33 Such response options (see Table G.3 in Appendix G) required differentiation of how many years of postsecondary education the parent had completed as well as the type of postsecondary education involved (i.e., college or technical/ business institution), and such distinctions seemed to be too difficult for the respondents and interviewers to consistently determine. In all but one case (mother completing 2 or more years of college; reliability index of .73), the reliability index for these response options were less than 0.50, estimating that over half of the observed variance for all such response options was attributable to response error.' Item 30 (regarding the number of \"other\" family members yielded only moderately acceptable reliability indices (77 percent agreement, x = 0.67, and Tb = 0.72). While real change over time is possible (i.e., more family members attending postsecondary education), the crosstabular data do not fully support such a contention, since the changes in numbers reported are mostly reductions from what was initially reported. It seems most likely that the inconsistencies of responses to this item are due to differential interpretations on the part of respondents as to what \"other\" family members to consider in the count. As posed, the question does not necessarily exclude parents (although in the context of items 28 and 29 this may be assumed implicit) and provides nothing in the way of limiting the extent of family considered (e.g., Is this immediate family only?; Are grandparents, aunts, uncles included? For nontraditional students, are children included?), even though the presumed focus of the question (for traditional postsecondary students) is siblings. 33 For mother, the category of advanced professional degree was also very unstable, but only 2 respondents ever chose that category; for the father, the same category was quite stable (index of reliability of 0.91). 34The overall reliability of these two items could be improved dramatically by consolidating some of the response options yielding low reliability indices and/or through better training of interviewers on how to elicit consistent responses. "}, {"section_title": "Overview of the NPSAS Files", "text": "The NPSAS:96 data files contain student-level and school-level data collected from institutions records, government databases, and student and parent interviews. The primary analysis file, from which the study Data Analysis Systems (DASs) were constructed, contains data for a total of 48,389 students, including data for 31,328 students with whom a telephone interview was conducted, and an additional 17,061 students who were selected for, but did not complete, a telephone interview but whose CADE data record was essentially complete'. 'A student could be represented on the study analysis file if selected for Phase 1 of CATI, determined to be eligible for NPSAS (or imputed to be so), and either of the following conditions was satisfied: The items in Section A of the Student CATI that are needed to identify first-time, beginning students (\"pure\" FTBs) were completed, or an abbreviated version of the questionnaire was completed (mail, Spanish, or nonresponse conversion instrument), or 'CADE was essentially complete (Sections CHAR, ENR, TUI, and FINAID completed). The definitions of \"complete\" for the four CADE sections are:: CHAR -complete if we have a valid responses to gender, date-of-birth, marital status, and race in CADE. The first three items have CPS equivalent variables, and these are considered as well. Some valid responses could be -I's. ENR -complete if we have a valid responses to Beginning Month/Year, Ending Month/Year, and Enrollment Status for at least one term. Some valid responses could be -l's. TUI -complete if we have a valid response to the total tuition item. Some valid responses could be 0 or -1. FINAID -complete if we have a valid response (Y or N) to the Financial Aid gate question, or match to Pell or NSLDS. The analysis file contains over 1,000 variables, most of which were derived from multiple NPSAS:96 data sources. The NPSAS:96 data sources, along with the corresponding numbers of sample students for which data were obtained, appear in Table 5.1. Most, but not all, of the students for whom some data were obtained from one or more sources appear on the analysis files. b Counts of first-time beginning students include only those students determined conclusively to be FTB's based on CATI interview data. FTB counts are included in the counts of undergraduates. cBecause the student type classification is based on the student's last term of enrollment at the NPSAS school, a small number of graduate/first professional students who were undergraduates at a postsecondary institution sometime during the NPSAS year were found on the ED Pell payment files. A preliminary DAS was prepared, adjudicated by U.S. Department of Education staff and released in October of 1996, prior to the end of CATI data collection. This DAS included variables derived from data collected from institutional records, the CPS, the ED Pell Payment File, and the NSLDS. No CATI data were included in this DAS, and only the NPSAS sample students determined at that time to be recipients of federal financial aid were included. Following completion of all study data collection, separate Data Analysis System files were created for undergraduate and graduate/first professional students. In total, 41,359 of the analysis file cases were undergraduate students during their last term of enrollment at the NPSAS institution, and the remaining 7,030 cases were graduate or first professional students during their last term of enrollment. Because a portion of the 7,030 graduate/first professional students were undergraduates during their first term of enrollment (i.e. college senior in the Fall term but started a graduate program in the Spring), these students appear on both the undergraduate and graduate/first professional DASs. Of the 41,359 students on the undergraduate DAS, 12, 207 are first time beginning students (FTBs). Complete data obtained through the NPSAS:96 are contained on the Electronic Codebook (ECB) files, which are available to researchers who have applied for and received authorization from NCES to access restricted research files. The NPSAS:96 ECB contains the following files: NPSAS Analysis File Contains analytic variables derived from all NPSAS data sources as well as selected direct CATI variables for 48,389 students. This file also includes the 1995-96 financial aid application data and/or the 1996-97 application data for the students on this file who were successfully matched to the CPS data base."}, {"section_title": "CADE Data File", "text": "Contains raw data collected from institutional records for 55,665 students with sufficient data to be considered CADE respondents. This file excludes any \"verbatim\" CADE variables such as responses to \"Other, specify\" items. These variables appear on the separate Verbatim Data File."}, {"section_title": "Student CATI Data File", "text": "Contains raw data collected from 31,328 students who responded to the student interview. This file excludes any Student CATI items which already appear on the analysis file. The file also excludes CATI verbatim items, which are on the Verbatim Data File."}, {"section_title": "Parent CATI Data File", "text": "Contains raw data collected from 3,352 parents of students who responded to the student interview. This file excludes any Parent CATI items which already appear on the analysis file. The file also excludes CATI verbatim items, which are on the Verbatim Data File."}, {"section_title": "NSLDS Data File", "text": "Contains raw loan-level data received from the National Student Loan Data System for the 29,049 who appear on either the analysis file or the CADE file. "}, {"section_title": "CATI Coding Results File", "text": "Contains the verbatim text and resulting code for student field-of-study, and (for employed students) industry and occupation. These values exist for the 31,328 students on the student CATI data file."}, {"section_title": "Verbatim Data File", "text": "Contains item-level records (i.e. one record per variable) for text variables collected in either CADE or CATI (student and parent interviews). There are multiple records per student for most of the students who appear on either the analysis file or the CADE file."}, {"section_title": "Jackknife Replicate Weights File", "text": "Contains all the Jackknife replicate weights created for NPSAS:96"}, {"section_title": "Institution Data File", "text": "Contains selected institution-level variables for sample institutions from which one or more students, appear on any of the student-level files mentioned above. This file may be linked to student-level files by the IPEDS number, which appears on all ECB files."}, {"section_title": "5.2", "text": "Data Coding and Editing"}, {"section_title": "Coding", "text": "Most of the NPSAS:96 coding activities were completed during telephone interviewing. The NPSAS:96 CATI system included software components for use in coding student major field-of-study, and industry/occupation data for both students and parents. An IPEDS coding routine retrieved institution characteristics for postsecondary schools other than the NPSAS institution the student reported attending during the NPSAS year. Interviewer proficiency at using the online coding routines was monitored and retraining was conducted as necessary. Selected variables containing text responses were up-coded into response categories following data collection. These included both CADE and CATI items. Other, Specify responses for student and parent Race, Asian Status, and Hispanic Status were up-coded wherever possible, as were the names of admissions exams. A total of 420 \"other student race\" and 93 \"other parent race\" responses were successfully up-coded for CATI respondents. Hispanic Status was up-coded for 58 student CATI cases and Asian Status was up-coded for 307 student CATI cases. Approximately 400 CATI cases in which the IPEDS coding routine had flagged an other postsecondary institution as \"Not Found\" were examined and recoded where possible. NPSAS:96 METHODOLOGY REPORT 5 -4"}, {"section_title": "43", "text": "Since the CADE software did not include an on-line coding routine for student field-ofstudy, responses to this CADE item were subsequently coded using the CATI coding system. Results from this activity were used in the construction of the final field-of-study variables derived for the DAS."}, {"section_title": "Editing", "text": "Following the completion of data collection, all CADE and CATI data were edited to ensure adherence to range and consistency checks. Range checks are summarized in the variable descriptions contained in the ECB and DAS data. Inconsistencies, either between or within data sources, were resolved in the construction of the derived variables. Protocol for resolving these discrepancies are described in the variable descriptions for the derived variables found in the ECB and DAS. There are a variety of explanations for missing data within individual data elements. For example, an item may not be applicable to certain students, a respondent may refuse to answer a particular item, or the respondent may not know the answer to the question. To assist analysts in understanding the nature of missing data associated with NPSAS:96 data elements, a set of missing data codes were developed. These codes were applied to blank responses in the analysis file, as well as the CADE and CATI data files, Table 5.2 lists the missing data codes and their meanings. "}, {"section_title": "Composite and Derived Variable Construction", "text": "Analytic variables were created by examining the data available for each student from the various data sources, establishing relative priorities of the data sources, on an item-by-item basis and reconciling discrepancies within and between sources. In some cases the derived or composite variables were created by simply assigning a value from the available source of information given the highest priority. In other cases, raw interview items were recoded or otherwise summarized to create a derived variable. A listing of the set of analysis variables derived for NPSAS:96 appears in Appendix H. Specific details regarding the creation of each variable appear in the variable descriptions contained in the ECB and DAS."}, {"section_title": "Statistical Imputations", "text": "After the editing process (which included logical imputations), the remaining missing values for 22 analysis variables were statistically imputed. The imputations were performed primarily to reduce the bias of survey estimates caused by missing data. The imputed data also makes the data complete and easier to analyze. The variables were imputed using a weighted hot deck procedure,2 with the exception of estimated family contribution (EFC), which was imputed through a multiple regression approach. Table 5.3 lists the variables by the percentage of missing data imputed."}, {"section_title": "Hot Deck Imputation", "text": "It is easier to describe the weighted hot deck imputation procedure by first describing unweighted hot deck imputation. The unweighted procedure partitions the sample into imputation classes based on auxiliary data available for both nonrespondents and respondents. Within these classes, it is assumed the nonrespondents would have answered in a similar manner to the respondents. Also, the data records are often sorted within the classes to place individuals that share additional characteristics closer to each other. The procedure is implemented by sequentially processing the database and replacing missing responses with the response from the previous respondent within each imputation class. The unweighted hot deck procedure reduces nonresponse bias if the response distributions differ across the imputation classes. However, by ignoring the sample weights, bias may remain in the survey estimates due to the weighted distribution of the imputed data within the classes being different from the weighted distribution of the respondent data. MST COPY AVAILABLE The weighted hot deck procedure is an extension of the hot deck procedure which does consider the weighted distribution. The procedure takes into account the unequal probabilities of selection by using the student weights to specify the expected number of times that a particular respondent's answer will be used to replace missing data. By using these expected selection frequencies, the weighted distribution of the imputed data will replicate the weighted distribution of the respondent data. Hence, the weighted hot deck imputation is designed so that, within each imputation class, the weighted survey estimates based on the imputed data will be equal in expectation to the weighted survey estimates based on the respondent data. To implement the weighted hot deck procedure, imputation classes and sorting variables that were relevant for each item being imputed were defined. If more than one sorting variable was chosen, a serpentine sort was performed where the direction of the sort (ascending or descending) changes each time the value of a variable changes. The serpentine sort minimizes the change in the student characteristics every time one of the variables changes its value. The respondent data for six of the items being imputed was modeled using a CHAID (Chi-squared Automatic Interaction Detector) analysis to determine the imputation classes. These items were: Race/ethnicity, Parent income (imputed for dependent students only), Student income, Student marital status, Dependents indicator, and Number of dependents. A CHAID analysis was performed on these variables because of the their importance to the study and the large number of candidate variables available to form imputation classes. Also, for the income variables, trying to define the best possible imputation classes was important due to the large amount of missing data. The CHAID analysis divided the respondent data (of each of these six items) into segments which differ with respect to the item being imputed. The segmentation process first divided the data into groups based on categories of the most significant predictor of the item being imputed. It then split each of these groups into smaller subgroups based on other predictor variables. It also merged categories of a variable that were found insignificant. This splitting and merging process continued until no more statistically significant predictors were found (or until some other stopping rule was met). The imputation classes from the final CHAID segments were then defined. Appendix I presents the imputation classes and sorting variables used for all of the variables imputed by the hot deck approach."}, {"section_title": "Imputation for EFC", "text": "The federal methodology Expected Family Contribution (EFC) was available for 65 percent of the students in the NPSAS:96 sample. In 90 percent of the cases where a recorded EFC was available, the source for the EFC was the student financial aid record (ISIR) reported in the federal central processing system (CPS) for the 1995-96 academic year. Other less frequently used sources were the student aid report (SAR) at the institution, the 1996-97 central processing record, or the EFC recorded on the 1995-96 Pell Grant payment file. For Pell Grant recipients, the EFC in the Pell Grant file was always used if there was a discrepancy. In the NPSAS:96 analysis file the variable for the recorded EFC is called EFC3. These EFC's were imputed for 35 percent of the 48,389 students on the file. This included approximately 31 percent of the dependent students, 41 percent of the independent students without dependents, and 36 percent of the independent students with dependents. Imputation regression equations were developed separately for dependent and independent students using the data and EFC formula types (CPS160) available in the 1995-96 CPS student records. Details of the various EFC imputations are provided in Appendix I. an Variance Estimation The NPSAS:96 data base contains a total of eight analysis weights associated with the CADE respondents, Study respondents, and CATI respondents. This includes weights for separate analyzes on all students, undergraduate students, graduate students, and first-timebeginning (FTB) students. The weight names and associated data bases are as follows: CADEWT: for all CADE respondents 55,665DASWTO: for all students on the restricted-use analysis file (48,389) DASWT1: for Study respondents who were undergraduates in first term (41,482). These students are included in the undergraduate Data Analysis System."}, {"section_title": "DASWT2:", "text": "for Study respondents who were graduate/first-professional students in last term (7,030). These students are included in the graduate Data Analysis System CATIWTO: for all CATI respondents on the restricted analysis files (31,328) CATIWT1: for CATI respondents who were undergraduates in first term (27,414). These students were included in the undergraduate Data Analysis System. CATIWT2: for CATI respondents who were graduate/first-professional students in last term (4,017). These students were included in the graduate Data Analysis System."}, {"section_title": "BPSWT:", "text": "for CATI respondents who were FTB students (12,040). Students who went from an undergraduate student in the first term to a graduate or firstprofessional student in the last term have both positive undergraduate and graduate/firstprofessional weights. NPSAS:96 METHODOLOGY REPORT 6-1 CHAPTER 6: WEIGHTING AND VARIANCE ESTIMATION The CADE and CATI weights apply to the respondents from the CADE and CATI data collection procedures. The Study respondents apply to students who responded to specified CADE or CATI data items. The definitions of these three types of respondents are summarized below. CADE respondents: students whose CADE data indicated they were enrolled in the institutions and their aid status was known either from their CADE data or from their data obtained from the Pell payment file or the National Student Loan Data System. Study respondents: students selected for CATI who either had complete CADE data or had completed enough of Section A of the CATI interview to determine their FTB status. CATI respondents: students who had completed enough of Section A of the CATI interview to determine their FTB status."}, {"section_title": "6.1", "text": ""}, {"section_title": "Study and CATI Weight Components", "text": "The Study weights and CATI weights were calculated as the product of 14 weight components, each representing either a probability of selection or a weight adjustment. The weight adjustments included nonresponse and poststratification adjustments to compensate for potential nonresponse bias and frame errors (differences between the survey population and the ideal target population, as discussed in Section 2.1.1). Also, multiplicity and trimming adjustments were performed. Since the Study weights were restricted to students selected for CATI, the first nine weight components (WT1-WT9) of the Study weights and CATI weights were identical, which represent the sample selection and adjustment components through the first phase of CATI. The remaining weight components (WT10-WT14) are the same steps, but are performed separately because of the different response definitions. The sampling weight for each sample institution is the reciprocal of the probability of selection. As shown in Section 2.2.1, the probability of selection for institution-I is n, S r(i) for non-certainty selections Sr(+) 1 for certainty selections. Therefore, the institution sampling weight was assigned as follows: Adjustment for Institution Multiplicity WT2During institution recruitment, seven sample schools that had two records listed on the IPEDs frame were found. In most cases, it was caused by two schools that had recently merged. If both records were sampled, then one record was retained for tracking survey results and the other record was classified as ineligible. To account for the two chances that a school could be selected, a multiplicity adjustment was performed by first calculating the probability that either record could be selected, or P(A or B) = P(A) + P(B) P(A)P(B). Then, the new sampling weight was calculated as the reciprocal of this probability, or NEW_WT1 = 1 / P(A or B). Finally, the multiplicity adjustment factor was derived by dividing the new sampling weight by the old sampling weight, or WT2 = NEW WT 1 / WT 1. Note that the product of WT1 and WT2 equals NEW_WT1. For weighting purposes, a school was considered a responding school if it provided an enrollment list and then, after the student sample was selected, provided sufficient CADE data for locating at least one sample student. If sufficient CADE data could not be obtained for any of the sample students, the school was still considered a respondent if at least one sample student could be matched to the Central Processing System (CPS) file. A weighting class adjustment was performed to compensate for nonresponding institutions using institution type as the weighting class. The calculated response rates were enhanced by multiplying the institution's weight by its size measure, or where E w, .s, The weight adjustment factor was then the reciprocal of this response rate. This enhancement forced the estimated total measure of size (roughly the population total of eligible students) to be the same for the responding institutions as it was for the eligible institutions. Table 6.1 presents the response rates and the resulting adjustment factors by institution type. Note that all the response rates are high except for the private, for-profit, less than 2-year schools. The response rate for those schools was 78.7 which resulted in an adjustment factor of   Student Sampling Weight WT4The overall student sampling strata were defined by crossing the institution sampling strata with the student strata within institutions. The overall sampling rates for these sampling strata can be found in Chapter 2. The sample students were systematically selected from the enrollment lists at institution-specific rates which were inversely proportional to the institution's probability of selection. Specifically, the sampling rate for student stratum-s within institution-I was calculated as the overall sampling rate divided by the institution's probability of selection, or where fs = the overall student sampling rate, and n = the institution's probability of selection. As discussed in Chapter 2, the institution-specific rates were designed to obtain the desired sample sizes and achieve nearly equal weights within the overall student strata. NPSAS:96 METHODOLOGY REPORT 6-5 x..53 BEST C PY ,,VALA LE If the institution's enrollment list was larger than expected based on the IPEDS data, the preloaded student sampling rates would yield larger than expected sample sizes. Likewise, if the enrollment list was smaller than expected, the sampling rates would yield smaller than expected sample sizes. To maintain control on the sample sizes, the sampling rates were adjusted, if necessary, so that the number of students selected did not exceed by more than 50 students the estimate reported to the school based on the WEDS data. A minimum sample size constraint of 40 students was also imposed so that at least 30 respondents from each participating institution could be expected.' The student sampling weight was then the reciprocal of the institution-specific student sampling rates, or 5WT4 = 1 /fsli ."}, {"section_title": "Student Subsampling Weight (WT5)", "text": "When schools provided hard-copy lists for student sampling, they often did not provide separate lists by strata (e.g., potential FTBs and other undergraduate students were on the same list). When that happened, the combined list was sampled at the highest of the sampling rates for the strata contained within the list. Strata with the lower sampling rates were then subsampled to achieve the desired sampling rates. The student subsampling weight adjustment factor, WT5, is the reciprocal of this subsampling rate. This weight factor is unity (1.00) for most students because this subsampling was not necessary for most institutions. Students who attended more than one eligible institution during the NPSAS year had multiple chances of being selected. That is, they could have been selected from any of the institutions they attended. Therefore, these students had a higher probability of being selected than was represented in their sampling weight. Adjustment for this multiplicity was made by dividing their sampling weight by the number of eligible institutions attended. Specifically, the student multiplicity weight adjustment factor was defined as WT6 = 1 / M, NCES confidentiality guidelines require at least 30 respondents before NCES can send the school a statistical report on their sample students. where M is the multiplicity, or number of institutions attended. The multiplicity was determined from the CATI interview, the Pell payment file, and the National Student Loan Data System. Unless there were evidence to the contrary, the student multiplicity was presumed to be unity .00). 7Adjustment for Unknown Eligibility Status WT7After abstracting the student record data using CADE, a weight of zero was assigned to students determined ineligible. A sample of students was selected for CATI from the students who were not classified as ineligible. The final eligibility status was then determined from the CATI interviews. However, for the students whom RTI staff were unable to contact, the final eligibility status could not be determined. These students were treated as eligible and their weights were adjusted to compensate for the small portion of students that were actually ineligible. Weighting classes were defined by the cross of institution type and the students' matching status to financial aid files (CPS, Pell, and Stafford). The first two columns of Table 6.2 present the weight adjustment factors applied to the students with unknown eligibility. These weight adjustment factors were simply the eligibility rate estimated among students with known eligibility status. For the eligible students, the weight adjustment factor was set equal to one. The students who had sufficient locating data from the CADE data or from the CPS file were subsampled for CATI interviewing. To adjust for students without sufficient locating data, a weighting class adjustment was performed using the cross of institution type and the students' matching status to financial aid files as weighting classes (the same classes used for WT7). The last two columns of Table 6.2 present the weight adjustment factors. Since the response rates were quite high, most of the weight adjustment factors are near one. The only exception was for students from public, less than 2-year schools that had no matches to the financial aid files. For these students, the weight adjustment factor was 1.30 (response rate was 76.8 percent).    Phase One CATI Sampling Weight (WT9) As CADE data were received from each institution, students were sampled for phase one of CATI on a flow basis. The students were sampled at a fixed rate by generating a random number for each student and then comparing it against the sampling rate. In the middle of this process, some slight adjustments were made to the fixed sampling rates to stay on target for the desired total sample sizes. The rates were adjusted after accounting for the sample yield from institutions already processed and the expected yield from the remaining institutions. The first column of Table 6.3 presents the average of the phase one sampling rates over all the institutions. The phase one sampling weights were defined to be the reciprocals of these average sampling rates. -.0 (0.0.17 VIOLA. NPSAS:96 METHODOLOGY REPORT"}, {"section_title": "MCI COPY MAC 45LE", "text": "NPSAS:96 METHODOLOGY REPORT 6-10 CHAPTER 6: WEIGHTING AND VARIANCE ESTIMATION (11) Adjustment for Not Locating Student (WT11) The first type of student nonresponse was not being able to locate (contact) the student. Adjustments for this type of nonresponse were made to compensate for the potential nonresponse bias. Adjustment factors were used that were inverses of predicted response propensities derived from a logistic regression model. The logistic procedure, developed by Folsom (1991), adjusts the weights of respondents so that the adjusted weight sums of respondents reproduce the unadjusted weight sums of respondents and nonrespondents for the predictor variables included in the model.2 In addition, the procedure provides a formal setting for evaluating variables believed to related to response. To avoid excessive weight variation, the procedure can also constrain the adjustment factors by specifying lower and upper bounds if the bounds are not set too tightly. Because a much larger number of students was to be used for the CATI weight adjustment, a model for the CATI weights was first determined and then a similar model was used for the Study weights. Candidate predictor variables were chosen that were thought to be predictive of response and were nonmissing for most of the nonrespondents. The candidate predictor variables included Number of phone numbers obtained for student, Institution type, Region, Student type, Age group, Attendance status, and Aid status. Other variables that were considered but were dropped because of too many missing values for the nonrespondents (at the time we were performing the weight adjustments) included Race/ethnicity, Dependency status, Grade point average, and Family income. To detect important interactions for the logistic models, a CHAD) (Chi-squared Automatic Interaction Detector) analysis was performed on the predictor variables. The CHAD) analysis divided the data into segments which differ with respect to the response variable, ability to locate. The segmentation process first divided the sample into groups based on categories of the most significant predictor of response. It then split each of these groups into smaller subgroups based on other predictor variables. It also merged categories of a variable that were found insignificant. This splitting and merging process continued until no more statistically significant predictors were found (or until some other stopping rule was met). The interactions from the final CHAD segments were then defined. The interaction segments and all the main effect variables were then subjected to variable screening in the logistic procedure. Variables significant at the 10 percent level were retained, with the exception of institution type and student type which were retained regardless of their significance. Table 6.4 presents the final predictor variables used in the logistic model to adjust the CATI weights and the average weight adjustment factors resulting from these variables. For the Study weights adjustment model, there were not enough students to include the segment interactions. Therefore, the segment variable was replaced with the main effect variable for the number of phone numbers. The predicted probability of locating student-j from the logistic models is given by: The logistic adjustment factor is then simply the reciprocal of this predicted probability of locating the student, or The weight adjustment factors from the logistic adjustment are summarized below, and were constrained to not exceed the maximum shown.  The second type of student nonresponse was if the student refused to be interviewed given that the student was located. The candidate predictor variables were the same candidate variables used in the location nonresponse adjustment (WT11). As in the location adjustment a CHAID analysis was performed on the predictor variables to detect important interactions. The resulting segment interactions and all the main effect variables in the logistic modeling at the 10 percent significance level were then screened. Table 6.5 presents the final predictor variables used in the logistic model to adjust the CATI weights and the average weight adjustment factor resulting from these variables. The Study weights adjustment used the same model except replaced the segment variable with the aid status main effect variable. As in the location adjustment, the weight adjustment factor for student-j was the reciprocal of the predicted response probability, or A WT12 = 1 /pri. Demographic characteristics of CATI respondents and nonrespondents are compared in the table in Appendix J. This table shows that the distributions of demographic characteristics, such as age, race, income, and receipt of aid are significantly different for CATI respondents and nonrespondents. Some of the statistically significant differences are not large differences, but aid recipients are clearly more likely to be respondents. Therefore, the statistical weight adjustments for CATI locating and nonresponse are definitely important for reducing the potential for nonresponse bias due to these types of differences between the CATI respondents and CATI nonrespondents.  All of the nonresponse weight adjustments were performed to reduce potential nonresponse bias. Also, to a smaller extent, the multiplicity adjustments reduce potential bias by accounting for the actual probabilities of selection. However, the cumulative effect of all the adjustment factors can cause excessive weight variation causing inflated sampling variances which then increases the mean square error."}, {"section_title": "REST COPY MAL ME", "text": "The mean square error of an estimate, 0 , is defined as the expected value of the squared total error, or MSE(0)  This can be rewritten as MSE(0)  where the first term is the sampling variance and the second term is the bias squared. By truncating some of the largest weights and smoothing (distributing) the truncated portions over all the weights, the mean square error can usually be reduced by substantially reducing the variance and slightly increasing the bias. To evaluate the weight variation, the unequal weighting effects on the variance by the cross of institution type and student type were computed as UWE = n E w2 / (E w)2. When the cumulative effect of the weight adjustment factors caused the unequal weighting effects to be unreasonably large, an upper limit was established for truncation of the largest weights. To distribute the truncated portions, a smoothing adjustment ratio was calculated as the sum of the original weights over the sum of the truncated weights for each class as follows: where Wo(I) = the original weight (WT1WT2...WT12), and"}, {"section_title": "1164", "text": "CHAPTER 6: WEIGHTING AND VARIANCE ESTIMATION WT(I) = the truncated weight (the minimum of the original weight and the upper limit). The truncation and smoothing steps were then combined into one adjustment factor by defining the weight component as Poststratification Adjustment WT14To ensure population coverage, the sampling weights were adjusted to control totals with a generalized raking procedure, that derives adjustment factors from an exponential regression model (Folsom 1991).3 The algorithm for this procedure is similar to the algorithm used in the logistic procedure for the nonresponse adjustments. Control totals were established for annual student enrollment; number and amount of Pell grants awarded; and number and amount of Stafford loans awarded. The annual student enrollment was controlled by institution type, and student type. The Pell grants were controlled by number of Pell grants awarded by institution type, number of Pell grants awarded by dependency status, and total amount of Pell grant dollars awarded by institution type. And finally, the Stafford loans were controlled by number of Stafford loans by institution type, and total amount of Stafford dollars by institution type. The annual enrollment control totals were estimated by multiplying the \"known\" fall enrollment totals from the 1995-96 Fall Enrollment Survey by the ratio estimate (based on NPSAS:96 data) of annual enrollment over fall enrollment. Specifically, the annual enrollment control totals were computed as 3 Folsom, R.E. (1991). \"Exponential and Logistic Weight Adjustments for Sampling and Nonresponse Error Reduction.\" Proceedings of the Social Statistics Section of the American Statistical Association, NPSAS:96 METHODOLOGY REPORT 6-17 The exponential adjustment factor for student-j is then simply WT14 = . Table 6.6 presents the variables associated with the student enrollment control totals and the average weight adjustment factors by these variables. Similarly, Table 6.7 presents the variables associated with the Pell grant and Stafford loan control totals and the average weight adjustment factors. The weight adjustment factors from the exponential adjustment are summarized below, and were constrained to not exceed the maxima shown. After performing this last weight adjustment, the final Study weights and final CATI weights were computed as the product of the 14 weight components and then rounded to the nearest integer.  "}, {"section_title": "FTB Weights", "text": "Since FTB status is known only for CATI respondents, the CATI weights are the analysis weights for students who are known to be FTBs. However, FTBs whose first postsecondary institution was not the NPSAS sample institution will not be followed longitudinally in the BPS:96 study. Therefore, the FTB weights were computed by making a final weighting class adjustment to the CATI weights by institution type to compensate for excluding FTBs whose first school was not the sample institution. All the adjustment factors were close to one ranging from 1.00 to 1.02.  "}, {"section_title": "CADE Weights", "text": "The development of all the CADE weight components was similar to their counterparts in the Study weights and CATI weights. The only differences were that they applied to a different set of respondent data and did not include the CATI weight components. As mentioned earlier, students were considered CADE respondents if their CADE data indicated they were enrolled at the sample institution and their aid status was known either from their CADE data or from their data obtained from the Pell payment file or the National Student Loan Data System. The weight components were as follows: Poststratification Adjustment (WT9)."}, {"section_title": "Variance Estimation", "text": "For probability-based sample surveys, most estimates are nonlinear statistics. Hence, the variances of the estimates cannot be expressed in closed form. For example, a mean or proportion, which is expressed as Ewy/Ew, is nonlinear because the denominator is a survey estimate of the (unknown) population total. Two common procedures for estimating variances of survey statistics are the Taylor series linearization procedure and the Jackknife replicate procedure, which are both available for NPSAS:96. Section 6.4.1 discuss the analysis strata and replicates created for the Taylor series procedure and Section 6.4.2 discusses the replicate weights created for the Jackknife procedure. Also, to measure the effects the complex sample design features had on the variances of survey estimates, Section 6.4.2 presents design effect estimates for several analysis domains. "}, {"section_title": "Taylor Series", "text": "The Taylor series variance estimation procedure is a well-known technique to estimate the variances of nonlinear statistics. The procedure takes the first-order Taylor series approximation of the nonlinear statistic and then substitutes the linear representation into the appropriate variance formula based on the sample design. Woodruff (1971) presents the mathematical formulation of this procedure and presents an example of the technique to sample surveys.4 For stratified multistage surveys, the Taylor series procedure requires analysis strata and analysis replicates defined from the sampling strata and PSUs used in the first-stage of sampling. For NPSAS:96, analysis strata and replicates for three separate data bases were defined: all students, all undergraduate students, and all graduate/first-professional students. The first step was identify the sampling PSUs. As discussed in Section 2.2.1, the PSUs included the 842 noncertainty institutions. For the 131 certainty institutions, however, the students represent the first-stage of sampling. To resemble the noncertainty institution PSUs, two pseudo-PSUs were created within each certainty institution by randomly assigning sample students into two equal-sized groups. The next step was to sort the PSUs and pseudo-PSUs by the nine institutional strata and then by the implicit stratification variables used to select the noncertainty institutions. These variables were institution sector, region, and the institution's size measure. From this sorted list, an analysis stratum was then defined each time the value of region (the last discrete sorting variable) changed. Regions were collapsed or split to create approximately the same size analysis strata. This process resulted in 51 analysis strata for all students, 51 analysis strata for undergraduate students, and 42 analysis strata for graduate/first-professional students. The analysis replicates were then defined within the analysis strata by collapsing the PSUs and pseudo-PSUs as required so each replicate did not contain less than four CADE, Study, or CATI respondents. This sample size requirement will ensure stable variance estimates. The names of the analysis strata and replicates and the associated data bases are as follows:"}, {"section_title": "ANALSTR, ANALREP:", "text": "Analysis strata and replicates for all students, UANALSTR, UANALREP: Analysis strata and replicates for undergraduates, and"}, {"section_title": "GANALSTR, GANALREP:", "text": "Analysis strata and replicates for graduate/firstprofessional students."}, {"section_title": "6.4.2", "text": ""}, {"section_title": "Jackknife Replication", "text": "The Jackknife procedure is another available variance estimation procedure that computes the variance based on a set of \"sample\" replicates. A sample replicate is created by randomly removing an analysis PSU within an analysis stratum (as if that PSU had not been selected) and adjusting the weights of the remaining PSUs to preserve the stratum weight total. Typically, the Jackknife procedure assumes two analysis PSUs for each analysis stratum so that the number of replicate weights equals the number of analysis strata. For Jackknife variance estimation, thirty to sixty replicate weights are usually recommended (Rust, 1986).5 Adhering to this general recommendation, the Jackknife analysis strata were defined to be the same as the analysis strata defined for the Taylor series procedure. This included 51 Jackknife strata for all students, 51 Jackknife strata for undergraduate students, and 42 Jackknife strata for graduate students. Then, two Jackknife PSUs were created within each stratum by collapsing the Taylor series analysis replicates. Based on the Jackknife strata and replicate definitions, we created replicate weights associated with the Study weights, CATI weights, and CADE weights. For the Study and CATI weights, this included separate replicate weights for all students, undergraduates only, and graduates only. Thus, a total of seven replicate weight sets were created: JDWT1 To create the replicate weights, institution-level replicate weights were defined. For each replicate set, institution weights of one PSU within an analysis stratum were set to zero and the institution weights of the other PSU were ratio-adjusted to preserve the analysis stratum weight total. The institution weights in the other strata remained unchanged. Therefore, the number of replicates is identical to the number of analysis strata. Then for each set of institution-level replicate weights, all the sampling weight components and weight adjustments were computed as described in the previous sections. Therefore, the Jackknife replicate weights produce variance estimates which incorporate the variance components associated with the nonresponse weight adjustments."}, {"section_title": "6.4.3", "text": ""}, {"section_title": "Design Effects", "text": "The survey design effect for a statistic is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical). It is often used to measure the effects that sample design features have on the precision of survey estimates. For example, stratification tends to decrease the variance, but multistage sampling and unequal probabilities of selection usually increase the variance. Also, weight adjustments for nonresponse, which are performed to reduce nonresponse bias, increase the variance by increasing the weight variation. Because of these effects, most complex multi-stage sampling designs, like NPSAS:96, result in a design effects greater than one. That is, the design-based variance is larger than the simple random sample variance. Specifically, the survey design effect for a given estimate, A , is defined as Also, the square root of the design effect is another useful measure, which can also be expressed as the ratio of the standard errors, or "}, {"section_title": "I. Target Population", "text": "The ideal target population for NPSAS:96 consists of all students who were enrolled for in postsecondary institutions in the United States or Puerto Rico at any time during the 1995-96 federal financial aid award year, excluding students who were enrolled solely in a GED program or who were concurrently enrolled in high school. As such, the terms of instruction involved would be those beginning between July 1, 1995 and June 30, 1996; however, using this time period to define the survey population would mean that final sampling could not be attempted for many institutions until after the first 1996 summer session began. Consequently, the survey population was defined as those students who were enrolled in any term beginning between May 1, 1995 andApril 30, 1996. Both the survey and target populations cover a full 12 months of student enrollment, and most members of the target population are also members of the survey population; however, the adopted definition of the survey population allowed the student lists needed for sample selection to be obtained in January or February for most institutions (e.g., those on a semester calendar system). This definition of the survey population is also completely consistent with that used for NPSAS:93. It also provides substantial comparability with the survey populations for NPSAS:90 and NPSAS:87. (For NPSAS:90, the students sampled were those enrolled on August 1, 1989, October 15, 1989, February 15, 1990, or June 15, 1990 (however, the June 15 enrollees were not sampled for 4-year institutions because of budgetary limitations); for NPSAS:87, only "}, {"section_title": "II. Sample Design Overview", "text": "Area-clustered, three-stage sampling designs were used for all previous NPSAS studies, including as stages: (1) geographic areas constructed from 3-digit postal Zip code areas; 2institutions within sample areas; and (3) students within sample institutions. An area clustered design was necessary for NPSAS:87 because a complete instruction frame was not available at that time, and the frame was supplemented with local sources. An area clustered design was used for the 1990 and 1993 studies to reduce the costs associated with sending field staff to sample institutions to abstract registration and financial aid data for sample students. As part of the NPSAS:96 field test activities, the feasibility of increasing precision of study estimates by eliminating the geographic area stage of sampling was examined. Both a clustered sample of institutions and an unclustered sample of institutions were selected for NPSAS:96. (The field test sample of institutions was selected from the institutions that were in neither of the institutional samples selected for the full-scale study, to insure that no institution would be burdened by participation in both the field test and full scale surveys). A comparison of expected costs and precision for these two sampling designs (i.e., three-stage vs. two-stage) indicated that: (1) the expected difference in cost between the two designs was not substantial and (2) standard errors for important estimates were expected to be 10 to 25 percent smaller with the two-stage design than those with the three-stage design. The relatively small cost differential between the two approaches results from greater use of two new procedures for collecting student financial aid information and other information from institutional records. The first of these procedures, introduced in the NPSAS:96 Field Test, involved collecting, through Electronic Data Interchange (EDI), student aid data from the ED central processing system (CPS) prior to any data collection at the involved institution. The second procedure, which was introduced in NPSAS:93, was the use of remote CADE, by institutional staff at most schools and by field staff only at the remainder, to obtain information NPSAS:96 METHODOLOGY REPORT: APPENDIX A A-4 7 APPENDIX A: NPSAS SAMPLING DETAILS from school records. Since a two-stage design can only improve precision and the cost penalty did not appear to be substantial, the two-stage design was fielded for the full-scale NPSAS:96. Comparisons of cost and precision estimates under the two sampling approaches are provided in Tables A.1 and A.2; greater detail on comparisons of cost and precision estimates under the two sampling approach procedures are provided in the NPSAS:96 Field Test Methodology Report.' A schematic overview of the sequential statistical sampling process for NPSAS:96 is provided in Figure A.1. The goal of all sampling activities was to attain NCES-required numbers of eligible sample postsecondary students (within specified student and institution types). An important domain of the required student sample was the set of students identified as first-time, beginning students (FTBs), that is, students who began their postsecondary education during the NPSAS year. These students would comprise the baseline cohort for the BPS longitudinal study. Accounting for expected (from prior NPSAS studies) rates of ineligibility among sample students and rates of FTB misclassification, the desired number of sample students were initially determined as shown, by type of institution and type of student classification, in Table A.3. Since it was necessary to select the student samples on a flow basis as sample institutions provided their enrollment lists (in order to meet the data collection schedule), the students were sampled at fixed rates. Under this approach, the actual numbers of students sampled are random variables; however, the sampling rates were set to meet or exceed, in expectation, the sample sizes shown in Table A.3. The NPSAS:96 sample was also designed to achieve at least 30 student CATI respondents from each sample institution that had at least that many eligible students enrolled during the NPSAS year. This was to allow NCES to send each participating institution a report using the results of the interviews with their students without violating confidentiality requirements. Consequently, institution sample sizes were determined to achieve an average of approximately 50 or more sample students per institution within each institutional stratum.    "}, {"section_title": "Ill. The Institutional Sample", "text": "The target population for NPSAS:96 includes nearly all postsecondary institutions in the 50 states, the District of Columbia, and Puerto Rico. Specifically, to be eligible for NPSAS:96 an institution is required, during the 1995-96 academic year, to: offer an educational program designed for persons who have completed secondary education; and offer more than just correspondence courses; and offer at least one academic, occupational, or vocational program off study lasting at least 3 months or 300 clock hours; and offer courses that are open to more than the employees or members of the company or group (e.g., union) that administers the institution; and be located in the 50 states, the District of Columbia, or Puerto Rico; and be other than a U.S. Service Academy (which are not eligible for this financial aid study because of their unique funding/tuition base). Institutions providing only avocational, recreational, or remedial courses or only in-house courses for their own employees are excluded. The listed eligibility requirements are consistent with those used in previous NPSAS implementations. A."}, {"section_title": "Sample Frame Construction", "text": "The institution-level sampling frame for NPSAS:96 was constructed from the 1993-94 organizational entities providing only noncredit continuing education (CEUs); (d) schools whose only purpose is to prepare students to take a particular test, (e.g., CPA examination or Bar exams); or (e) branch campuses of U.S. institutions in foreign countries. The IPEDS IC file exclusions, themselves, eliminate some categories of ineligible institutions; however, additional deletion from this file was required. Starting with the 10,651 \"institutions\" on this database, records were deleted to yield a sampling frame containing 9,468 institutions appearing to be eligible for NPSAS:96 based on their 1993-94 IPEDS IC data. Deletions included: (1) administrative units; (2) U.S. Service academies; (3) schools outside of U.S. and Puerto Rico; (4) institutions offering no programs of at least 300 content hours, six semesters/trimesters, or 12 quarter hours and for which the highest level of offering was a certificate or diploma of less than one academic year; (5) Institutions offering only correspondence courses; and 12 institutions with reported, non-imputed zero enrollment for the 1992-93 academic year. Because enrollment data were needed to compute measures of size for sample selection, IPEDS \"unduplicated count\" enrollment data were edited and/or imputed to eliminate missing data. Missing graduate and first-professional enrollments were set to zero for institutions that did not offer that level of instruction. For institutions that provided only undergraduate instruction, missing undergraduate enrollment was obtained from the fall enrollment variables, if those were nonmissing. If summer session enrollment was reported and flagged as not included in the unduplicated head counts, it was added to the unduplicated head counts. Finally, sets of records were identified for which the enrollment data either: (a) were reported with another institution's , or (b) contained combined data. In such cases, the combined enrollment data were allocated equally to all institutions in the set. For the remaining 108 records with missing enrollment data, imputation classes (defined by institutional sector (level and control) and first-professional offering (yes or no), were created and missing enrollment data were imputed for such cases as the imputation class mean. This approach avoids imputing unusually large or unusually small enrollments."}, {"section_title": "NPSAS:96 METHODOLOGY REPORT: APPENDIX", "text": "The remaining institutions were then partitioned into nine institutional strata based on institutional control and highest level of offering: (1) Public, less-than-2-year; (2) Public, 2-year; (3) Public, 4-year, non-doctorate; (4) Public, 4-year, doctorate; (5) Private, not-for-profit, less-than-4-year; (6) Private, not-for-profit, 4-year, non-doctorate; (7) Private, not-for-profit, 4-year, doctorate; (8) Private, for-profit, less-than-2-year; and 9Private, for-profit, 2-year or more. A stratified sample of 973 institutions was then selected with probabilities proportional to size (pps); some of these institutions subsequently proved to be ineligible and others failed to participate."}, {"section_title": "B.", "text": ""}, {"section_title": "Selecting Sample Institutions", "text": "It was necessary to allocate the student sample to the separate applicable institutional (defined above) and student sampling strata. The student sampling strata used were: (1) potential first-time, beginning students (FTBs) (2) other undergraduate students. (3) graduate students; and (4) first-professional students. In determining the allocation, the below listed notation is used: (1) r = 1, 2, ..., 9 indexes the previously defined institutional strata;. (2) s = 1, 2, 3, 4 indexes the previously defined initial student strata; (3) j = 1, 2, ..., J(r) indexes the institutions within stratum \"r;\" where 4Mrs(j) = number of students enrolled during the NPSAS year who belong to student stratum \"s\" at the j-th institution in institutional stratum \"r;\" m = number of students to be selected from student stratum \"s\" within the r-th institutional stratum (referred to henceforth as student stratum \"rs\"); and 6;(j) = probability of selecting the j-th institution in institutional stratum \"r.\" The overall population sampling rate (f) for student stratum \"rs\" is given by The initially computed stratum-level student sampling rates, fps (used to define institution measures of size) are shown in Tables A.4 and A.5. Table A.4 presents the sampling rates for the three student domains consisting of undergraduate, graduate, and first-professional students based on the 1993-94 IPEDS IC file counts and the required sample sizes previously presented in Table   A.3. The IPEDS files do not provide separate counts for \"potential FTB\" students; consequently, the partitioning of total undergraduate enrollment into potential FTBs and other undergraduate students was modeled to arrive at the sampling rates for these strata that are shown in Table A.5. The assumptions used to divide the undergraduate student totals into the two sampling strata was conditional on institutional level. For less than-2-year institutions, 75 percent of the undergraduate students were assumed to be first-year students, and 90 percent of these first-year students were expected to be classified by the institutions (for sample selection) as potential FTBs. For 2-year institutions, 50 percent of the undergraduate students were modeled as first-year students, and 85 percent of those as being classified as potential FTBs. For 4-year institutions, the model assumed 30 percent of the undergraduate students would be first-year students and that 75 percent of those would be classified as potential FTBs. NPSAS:96 METHODOLOGY REPORT: APPENDIX A 2 6 A-13 (1) Less-than-2-year: 75 percent of undergraduates are first-year and 90 percent of those are potential FTBs; (2) 2-year: 50 percent of undergraduates are first-year and 85 percent of those are potential FTBs; (3) 4-year: 30 percent of undergraduates are first-year and 75 percent of those are potential FTBs. The composite measure of size for the j-th institution in stratum \"r\" was then defined to be 4 Sr() = E f m (i), sri which is the number of students that would be selected from the j-th institution if all institutions on the frame were to be sampled. An independent sample of institutions was selected for each institutional stratum using Chromy's2 sequential, pmr sampling algorithm to select institutions with probabilities proportional to their measures of size. However, rather than allow multiple selections of sample institutions, those with expected frequencies of selection greater than unity (1.00) were selected with certainty. (By precluding institutions with multiple selections at the first stage of sampling, it was unnecessary to select multiple second-stage samples of students.) The remainder of the institutional sample was selected from the remaining institutions within each stratum. Therefore, the probability of selection for the j-th institution in institutional stratum \"r\" is given by where for non-certainty selections,  a Stratum reflects institutional categorization as determined from the 1993-94 IPEDS IC file; some errors in this classification were uncovered when institutions were contacted. Within each of the \"rn institutional strata, additional implicit stratification was accomplished by sorting the stratum \"r\" sampling frame in a serpentine manner4 by: (a) institutional level of offering (where strata had been collapsed one level); (b) the IPEPS IC-listed OBE Region (with Alaska and Hawaii moved to Region 9) with Puerto Rico; and (c) the institution measure of size. The objectives of this additional, implicit stratification were to approximate proportional representation of institutions on these measures.  "}, {"section_title": "IV. The Student Samples", "text": "The initial student sample was selected from lists provided by 836 of the 900 institutions (from the original sample) that proved to be eligible. In addition to this initial (basic) student sampling, additional student subsampling was implemented in NPSAS:96. Because of budgetary constraints, only a subsample of students were selected for interviewing; moreover, interviewing was conducted in two phases, and only a subsample of first phase nonrespondents were selected for the second interviewing phase. Additionally, certain students were selected for whom an interview with their parents would be required to obtain certain data elements. Finally, a small subsample of students was selected for reliability interviews. NPSAS:96 METHODOLOGY REPORT: APPENDIX A A-18"}, {"section_title": "I S3", "text": "APPENDIX A: NPSAS SAMPLING DETAILS A."}, {"section_title": "Basic Student Sample", "text": "The postsecondary students eligible for NPSAS:96 were those who attended a NPSASeligible institution during the previously defined NPSAS year and who were: enrolled in either (a) an academic program; (b) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; or (c)occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; not concurrently enrolled in high school; and not enrolled solely in a GED or other high school completion program. 1."}, {"section_title": "Construction of Initial Sampling Frames", "text": "Each of the 900 sampled institutions that were verified to be eligible for NPSAS:96 was asked to provide lists of all its students who satisfied all the NPSAS eligibility conditions; preferably nonduplicated, machine-readable lists (diskette, magnetic tape, or electronic mail file), together with identifying and classifying information (see section 2.3.2 below). Although machinereadable files were preferred, the preferences of sample institutions were accommodated, and whatever type(s) of student list(s) they were able to provide were accepted. (Final  enrollment lists were available from some institutions as early as February, 1996; however, other institutions could not provide final lists until August, 1996.) Separate, unduplicated lists (in which each student's name appears only once) were requested for first-time beginning other undergraduate, graduate, and first-professional students (the basic student strata) were requested of those providing hard copy lists. As expected, however, many institutions sent separate lists for each term or course of instruction; in which cases an individual student's name could appear on more than one list. In such cases, sampling procedures were used to \"unduplicate\" the sample, to ensure that each student received only one chance of selection. 2."}, {"section_title": "Student Sample Selection", "text": "Students were sampled on a flow basis as student lists were received. machine-readable lists. For each institution, the student sampling rates, rather than the student sample sizes, were set to fixed values: to facilitate sampling students on a flow basis as student lists were received; to facilitate the procedures used to \"unduplicate\" the samples selected from (duplicated) hard-copy lists; and because sampling at a fixed rate based on the overall stratum sampling rates and the institutional probabilities of selection results in approximately equal overall probabilities of selection within the ultimate institution-by-student strata. Machine-readable lists were unduplicated by sorting on the student ID number and deleting duplicates prior to sample selection. In the case of duplicated hard-copy lists, a stratified systematic sample was selected from each list provided (typically separate lists by term). For unduplication, all students in the sample selected from the fall list were retained for the sample, and the samples selected from all other lists were \"unduplicated\" against the fall list. (The fall term was given precedence in this process for comparability with NPSAS:87, in which only fall enrollees were sampled.) If the institution did not have standard terms, other orderings of the student lists were used to achieve unduplication of the sample. Any students sampled from the next list who were on the full fall list were deleted since they already had a chance of selection from the fall list. In the same manner, samples from subsequent lists were compared to the full lists of all lists from which previous samples had been drawn. After the sample of students had been selected for an institution, the social security numbers of the sample students were compared to those of students who had already been selected from other institutions. When duplicates were detected, the duplicate was eliminated from the sample from the current institution. Multiplicity adjustments in the sample weighting (see below, Chapter 6) accounted for the fact that any students who attended more than one institution in the NPSAS population had more than one chance of selection. The development of student sampling rates within student stratum rs (i.e., the rth institutional stratum and the sth student stratum within institutional stratum) were previously discussed in section N PSAS: 96 METHODOLOGY REPORT: APPENDIX A."}, {"section_title": "125", "text": "A-20 III.B, and the notation used in that development will be used here. For graduate and firstprofessional students, these overall student sampling rates were shown in Table A.4. However, for potential FTB students and other undergraduate students, the data from the NPSAS:96 field test (as adjusted to accommodate the more refined full-scale study rules for institutional identification of potential FTBs, which information was unavailable when the institutional sample was selected) were used to update previous assumptions regarding the proportion of undergraduate students who would be classified by the institutions as potential FTBs and the proportion of potential FTBs who would ultimately be determined to be actual FTBs. Revised estimated percentages of undergraduate students who would be classified as potential FTBs by the sample institutions and percentages of those who would ultimately be classified as true FTBs are shown in Table A.8. These estimates were used to revise student sampling rates for the two undergraduate sampling strata, shown in Table A.9.  For the unconditional probability of selection to be a constant for all eligible students in stratum \"rs,\" the overall probability of selection should be the overall student sampling fraction, f,s; i.e., we must require that or equivalently,"}, {"section_title": "X27", "text": "APPENDIX A: NPSAS SAMPLING DETAILS Thus, the conditional sampling rate for stratum \"rs,\" given selection of the j-th institution, becomes = l / 7c,. It should be noted that, in this case, the desired overall student sample size, m, , is achieved only in expectation over all possible samples. Achieving the desired sample sizes with equal probabilities within strata in the particular sample that has been selected and simultaneously adjusting for institutional nonresponse and ineligibility requires that Emrsw = m, jeR where \"R\" denotes the set of eligible, responding institutions. Letting the conditional student sampling rate for stratum \"rs\" in the j-th institution be then requires where \"S\" denotes the set of all sample institutions, Er = the institutional eligibility factor for institutional stratum \"r,\" R,. = the institutional response factor for institutional stratum \"r,\" = the student eligibility factor for student stratum \"rs.\" These factors were the proportions of institution or students, respectively, expected to be eligible or responding within the defined strata. Since this determination was made after eligibility status had already been determined for some institutions, values of 0 (known not eligible) or 1 (known eligible) were used, if known at that time. Initial student sampling rates were calculated in this manner for each sample institution; these rates were designed to achieve equal probabilities of selection within the ultimate institution-by-student sampling strata. However, these rates were sometimes modified for reasons listed below."}, {"section_title": "NPSAS:96 METHODOLOGY REPORT: APPENDIX A A-24", "text": "The student sampling rates were increased, as needed, so that the sample size achieved at each sample institution would be at least 40 sample students, where possible (The reason for this constraint was to facilitate obtaining at least 30 responding students for most participating institutions, enabling NCES to send a report to the institution regarding its sample students, as a \"Thank You\" for participation without violating NCES confidentiality guidelines). The student sampling rates were decreased if the sample size was more than 50 greater than the institution had been told to expect (This was to facilitate continued participation by the institutions for CADE data abstraction). The sample yield was monitored throughout the several months during which student lists were received, and the student sampling rates were adjusted periodically to ensure that the desired student sample sizes were achieved. These adjustments to the initial sampling rates (especially the first two types of adjustments) resulted in some additional variability in the student sampling rates, and, hence, in some increase in survey design effects. The actual sample sizes achieved in total and within each institutional and student stratum, are shown in Table A.10. In general, institutions classified fewer students than expected as potential FTBs; consequently, sampling rates for FTBs were increased to obtain the needed sample yield (For 35 four-year institutions, this rate was increased by selecting a supplemental sample). By comparing Table A.10 with Table A.1, it can be seen that the rate adjustment procedures were generally effective; the overall sample yield was actually greater than expected (63,616 students as compared to the target of 59,509  "}, {"section_title": "393", "text": "Reflects student rrclassification as a result of records data. b Excludes 1,593 CADE nonrespondents and 2,403 sample members determined ineligible for NPSAS, from records data. Reflects average rate across all sequential samples implemented. Phase 1 of CATI was defined to end when six telephone calls had been attempted without obtaining a completed interview, or the student: or his/her parent had been interviewed; was determined to be ineligible for NPSAS; initially refused to participate; required intensive tracing procedures, or was determined to be in a special population (i.e., deaf or language barrier). All students for whom the sixth call in Phase 1 resulted in a \"hard\" appointment or for whom a partial interview had been completed (with either the sample student or the student's parentsee Chapter 3) were retained for Phase 2 with certainty, as were all students in the potential FTB stratum. The remaining Phase 1 nonrespondents, who had not been determined ineligibles or exclusions during Phase 1, were subsampled for Phase 2, using specified rates.  "}, {"section_title": "Other Student Subsamples", "text": "Some additional subsampling of students, was accomplished during the course of NPSAS:96 in order to: (a) complete important gaps in available data about the student's family financial information, or (b) provide the basis for methodological studies. 1."}, {"section_title": "Reliability Reinterview Subsample", "text": "Among eligible sample members who completed the NPSAS:96 interview, a subsample was selected to participate in a reliability reinterview (containing a small subset of the interview items and to be conducted 3 to 4 weeks after the initial interviewsee Chapter 3). The random selection algorithm was programmed directly into the CATI instrument so that selected respondents could be informed of their selection and allowed an opportunity to agree to (and schedule) the reinterview or to refuse it at that time. The selection rate was set to yield a sample of approximately 300 students among the expected respondents during the first 3 months of interviewing; the time factor was based on the built-in delay in administering the reinterview and the need to complete reinterviews within the same time frame as other interviews. Consequently, the sample was obviously most heavily weighted with students: (a) from institutions at which prior sequential operations (initial sampling, record abstraction) were completed earlier, and (b) who completed the first interview relatively early during the data collection period. Since fewer completion than anticipated were experienced during the first 3 months (see Chapter 3), the sample yield during that period was 256 students."}, {"section_title": "2.", "text": ""}, {"section_title": "2C 6", "text": "APPENDIX A: NPSAS SAMPLING DETAILS parents' income would otherwise have to be imputed. An important historical gap in NPSAS student data has been the income of parents of dependent students who do not apply for financial aid; among those applying for (and/or receiving) aid, these data are usually obtained from existing records. Another purpose was to collect parent income data for students who had recently become independent students (for the purposes of federal financial aid applications) as a result of recently becoming 24 years old. Parent data for these students will reduce reliance on imputed parent incomes when analyzing the effect that becoming classified as an independent student has on financing postsecondary education. Consequently, all the students in the three below-listed student sampling strata were selected with certainty for parent interviews. Dependent undergraduate students, not receiving federal aid. Dependent undergraduate students, receiving federal aid, whose parents' adjusted gross income was missing in CADE. Twenty-four and 25-year-old (newly independent) undergraduate students. The strata were developed at the conclusion of institutional records collection, and the parent interview subsample was identified among those students selected for Phase 1 interviewing. When the student had been selected for parent interview and either a student interview had been completed in Phase 1 or the student had been selected for Phase 2, the parent interview was attempted throughout the remainder of CATI the interview period. The numbers of students selected for parent interviewing in Phase 1 and in Phase 2, are shown in Table A  bThis represents all eligible students in defined strata prior to Phase 1 sampling. This rate reflects only the overall Phase 1 student sampling (average rate across all sequential sampling implemented); all students within these defined strata, who were selected for Phase 1, were designated with certainty for parent interview. dExcludes Phase 1 respondents and sample members determined to be NPSAS ineligible or exclusions during Phase 1. Includes certainty selections. (Reflects average rate across all samples implemented (consequently, this rate includes Phase 2 certainty selections). BEST COPY AV LA LE NPSAS:96 METHODOLOGY REPORT: APPENDIX A A-33 Your institution has been selected to participate in the 1996 National Postsecondary Student Aid Study (NPSAS:96), a major nationwide study conducted for the U.S. Department of Education of how students and their families finance education after high school. Thank you for your past participation in the study. Please appoint a NPSAS Coordinator for your institution to help provide information required for successful conduct of this study. The NPSAS Coordinator should be someone who can orchestrate the information gathering between various staff and departments within your school to identify and pull together information on the enrollment status, financial assistance, and demographic characteristics for each student that is sampled. Because your previous coordinator is familiar with NPSAS, we ask that you appoint the same person, if appropriate, and provide the information on the enclosed reply sheet. The person you appoint as coordinator of the study will be asked to send the enrollment lists/files for all students enrolled in 1995-96 to our contractor, Research Triangle Institute (RTI). After RTI has identified a sample of students from the enrollment lists provided by your coordinator, institutional records data on enrollment status and information on any financial aid data awarded to the sampled students will be collected. Technical staff from RTI will work with your NPSAS institution coordinator to arrange for data collection in an efficient and convenient manner. During the past year, the National Center for Education Statistics (NCES) tested procedures for the full-scale study, which will include a sample of approximately 950 institutions, 60,000 students, and 10,000 parents. Further details on the data collection procedures, our assurance of confidentiality, a listing of national organizations that have endorsed the study, and estimates of time commitments for your institution are enclosed. An RTI representative will contact your coordinator to answer any questions and to discuss the best method of data collection for your institution. If you have any questions about the study or procedures involved prior to this contact, please call Educational Analyst, Katy Ong (1-800 -334-8571) at RTI or the NCES Project Officer, Drew Malizio (202-219-1448). Thank you for your continued cooperation and prompt return of the enclosed reply sheet. Your institution has been selected to participate in the 1996 National Postsecondary Student Aid Study (NPSAS:96), a major nationwide study, conducted for the U.S. Department of Education, of how students and their families finance education after high school. tarn asking that you appoint a NPSAS coordinator for your institution, and provide the information on the enclosed reply sheet. Your institution's participation in NPSAS:96 is very important to the continued success of this study. NPSAS was first conducted by the National Center for Education Statistics (NCES) during 1986-87. The second and third cycles of NPSAS, completed during 1989-90 and 1992-93, enhanced the basic data collected to meet more fully the needs of the student financial aid community. The National Education Statistics Act of 1994 authorizes NCES to continue conducting this study in response to the need for information on postsecondary students, including financial aid data. During the past year, NCES tested procedures for the full-scale study which will include a sample of approximately 950 institutions, 60,000 students, and 10,000 parents. Further details on the data collection procedures, our assurance of confidentiality, a listing of national organizations that have endorsed the study, and estimates of time commitments for your institution are enclosed. We ask that you appoint as NPSAS Coordinator someone who can orchestrate the information gathering between various staff and departments within your school to identify and pull together information on the enrollment status, financial assistance, and demographic characteristics for each student that is sampled. This person will be asked to send the enrollment lists/files for all students enrolled in 1995-96 to our contractor, Research Triangle Institute (RTI). After RTI has identified a sample of students from the enrollment lists provided by your coordinator, institutional records data on the enrollment status and any financial aid data awarded to the sampled students will be collected. Technical staff from RTI will work with your NPSAS institution coordinator to arrange for data collection in an efficient and convenient manner. An RTI representative will contact your coordinator to answer any questions and to discuss the best method of data collection for your institution. If you have any questions about the study or procedures involved prior to this contact, please call Educational Analyst, Katy Ong (1-800 -334-8571) at RTI or the NCES Project Officer, Drew Malizio (202-219-1448). Thank you for your cooperation and prompt return of the enclosed reply sheet. The Chief Administrator of your institution has appointed you as Coordinator for the 1996 National Postsecondary Student Aid Study (NPSAS:96). NPSAS is being conducted by Research Triangle Institute (RTI) for the National Center for Education Statistics (NOES) of the U.S. Department of Education. During 1996. NCES will conduct the fourth cycle of NPSAS, a major study on how students and their families finance postsecondary education. In response to the continuing need for the data provided by NPSAS, Congress has mandated that NCES conduct this study periodically; prior NPSAS studies were conducted in 1987. 1990, and 1993. The Chief Administrator of your institution was sent a packet of information describing the study's background, purposes, and processes. In the enclosed binder, we have provided copies of the information the Chief Administrator was sent as well as more detailed information about the specific processes of the study and your important role as the NPSAS Coordinator. Information from institutions will be gathered in two stages. The first step is to obtain an enrollment file from which RTI will select a sample of students. After RTI has determined a sample of students from your institution, data abstraction of student records will begin. Abstracting student data involves entering data such as locating, demographic, and financial aid information from the sampled students' records using a Computer Assisted Data Entry (CADE) software program. Most NPSAS Coordinators will prefer to delegate this task to an appropriate institution staff member or to allow an RTI field staff member perform this work. To assist you in these tasks. the following 'terns are enclosed: General information that describes the institutional component of the study; A Coordinator Response Sheet to be returned to RTI; Specifications for preparing enrollment files; Administrative aids: -A Transmittal Sheet for returning the enrollment files; -A prepaid Federal Express label for returning the enrollment files; and -Labels to be attached to enrollment files for identification purposes. Please return the completed Coordinator Response Sheet to us at your earliest convenience. You may either FAX it to us or return it to us by mail in the enclosed postage paid envelope. A member of our staff will be contacting you shortly to verify that you have received this package, to discuss options for providing the enrollment files and the record abstraction process (CADE), and to answer any questions that you may have about the enclosed materials. NPSAS collects information on student demographics, family income, education expenses, employment, living expenses, education aspirations and how students and their families meet the costs of their education beyond high school. In addition to describing characteristics of students enrolled in postsecondary education, the results will be used, in part, to help determine future federal student financial aid policy. Because only about one of every 1500 students who were enrolled during 1995-96 has the opportunity to participate in this study, your experiences and opinions are very important to its success. Your responses will represent thousands of students like yourself. An interviewer from RTI will contact you by telephone sometime in the near future to ask you some questions about your postsecondary education experiences, especially for the 1995-96 school year. Based on prior studies, you can shorten the interview time if you have any documents about your income and any financial aid you may have received during 1995-96 available at the time of the interviewer's call. Students enrolled in less-than-2-year institutions, community colleges, 4-year colleges, and major universities in the United States and Puerto Rico, including those who do not receive financial aid and those who do receive aid participate in NPSAS. If you did not receive financial aid, we would like to know how you met the costs of attending school during the 1995-96 year. For example, did you take out any private loans, receive any employer tuition assistance or parental support? If you received student financial aid, did you receive enough to meet your education expenses? If not, did you borrow additional money from relatives? Your data, when combined with that of all students participating in the study, will be used by policymakers when they consider how much federal grant, loan, and work-study aid will be available in the future. The time required to gather the information needed and complete the telephone interview is estimated to vary from 20 to 45 minutes, with an average of about 35 minutes per telephone interview. If you have any comments concerning the accuracy of the time estimates or suggestions for improving the collection of information, write directly to: U.S. Department of Education, National Center for Education Statistics, NPSAS Project Officer #1850-0666, 555 New Jersey Ave NW D.C. 20208. I want to assure you that NCES and its contractors adhere to the highest standards in protecting the privacy of individuals involved in the studies it undertakes. Only a limited number of researchers may be authorized by NCES to access information that may identify individuals. They may use the data only for statistical purposes and are subject to fines and imprisonment for misuse. Data will be combined to produce statistical reports for Congress and others. No individual data that links your name, address, telephone number, or student identification number with your responses will be reported. Your participation in NPSAS is strictly voluntary; however, your responses are necessary to make the results of this study accurate and timely. Additional information explaining how you were selected and some of the information we have collected from the school you attended during 1995-96 is enclosed. If you have any questions about this study prior to your call from RTI, or if you would like to set up an appointment to be interviewed, please call Marty Nash at RTI. The toll-free number is 1-800-472-6094. Thank you very much. Your cooperation is greatly appreciated. Sincerely, lean ffith Acting Commissioner NOTE: Arrangements have been made to allow the participation of persons who are hearing or speech impaired. Call us (toll free) at 1-800-647-9659 (TDD)."}, {"section_title": "Enclosure", "text": "BEST COPY AVAILABLE Un entrevistador de RTI lo Hamad pot tel6fono en los proximOs dfas para hacerie algunas pieguntis acerta de su education pos-secundaria, especificamente durance el alto escolar 1995-96. En base a experiencia obtenida per medio de este estudio en Linos pasados, sabemos que la entrevista puede hacerse un poco mds breve si tiene a la mano documentos sobre sus ingresos y cualquier asistencia eCon6mica que hays recibido durance el alio escolar 1995-96 cuando se le haga la Ilarnada telefonica. Estudiantes que esten matriculados en institutions con programas educativos de menos de 2 afios, eicuelas comunitarias ( \"community colleges\"), escuelas de 4 altos, y universidades principales en los Estados Unidos y Puerto Rico, participarfin en NPSAS:96 --incluyendo a esos estudiantes que no reciben ayuda financiera comp tambien aquellos que sf la reciben. Si usted no recibi6 ayuda financiers, quisieramos saber tambien c6mo pag6 los gastos de asistir a la escuela durante el alto escolar 1995-96. Por ejemplo, owe que hacer un prestamo personal o recibi6 ayuda de su empleador o de sus padres para pagar la matricula? Si recibi6 ayuda financiera para estudiantei, qtrisieramos saber si recibi6 suficiente dinero para cubrir sus gastos educacionales. Si no, ttuvo que tomar prestado el dinero adicional a un miembro de su familial La information suya, cuando se combine con las de codes los dem& estudiantes que tomen pane en este estudio, ayudart a determinar cuanta ayuda econ6mica federal baba disponible en el futuro pot medio de becas, prestarnos, o programas de estudio y trabajo. El tiempo necesario para recopilar la information y completar la entrevista puede ser entre 20 a 45 minutos; el promedio es de 35 minutos por cads entrevista hecha por telefono. Si usted tiene algtin comentatio acerca de la exactitud del tiempo estimado para ser entrevistado o alguna sugerencia sobre como mejorar la recopilaci6n de esta informaci6n, escribanos directamente al: U.S. Department of Education, National Center for Education Statistics (NCES), NPSAS Project Officer #1850-0666, 555 New Jersey Avenue NW, Washington, DC 20208. Le aseguramos que NCES y sus representantes siguen las miss estrictas normas para protege( los derechos de privacidad de las personas que participan en estudios que se hacen bajo su direcci6n. Solamente un ntimero limitado de personas serzin autorizadas par NCES para tener acceso a la information que pudiera identificar a un individuo. alas personas pueden usar los datos dnicamente para propositos estadisticos y estdn expuestos a ser multados y encarcelados por mal use de los datos. Toda la information obtenida se unird para producir reportes estadtsticos para el Congreso de los EE. UU. u arras agencias que requieran esta information. La information individual que podrta identificar su nombre, direction. ndmero telefOnico, o ndmero de identificaciOn de estudiante, nunca sera relacionada con sus respuestas an ningein informe. Su participaciOn en NPSAS:96 as completamente voluntaria, atin act sus respuestas son necesarias para lograr que los resultados de este estudio scan precisos y actualizados. Encontrard adjunto informacion adicional donde se le explica cc5mo usted fue seleccionado para este estudio y tambien algunos informes que recopilamos sobre la escuela que usted asisti6 en el ano escolar 1995-96. Si tiene alguna pregunta sobre este estudio antes de recibir la Hamada de RTI, o si usted desea hacer una cita previa pare dejarnos saber cuando nos podemos comunicar con usted para ser entrevistado, por favor comuniquese con la Sta. Marty Nash al numero telefdnico, libre de cargos (\"toll free'), 1-800-472-6094. Muchas gracias por su participaci6n. Le agradecemos su cooperaci6n. Sinceramente, Pascal D. Forgione Comisionado"}, {"section_title": "NOTA:", "text": "Tenemos disponibles un servicio para personas con impedimentos de audici6n o del habla--en ingl6s solamente. Si usted requiere de este servicio, llamenos, libre de cargos, al 1-800-647-9659 (SRT). NPSAS collects information on student demographics, family income, education expenses, employment, living expenses, education aspirations, and how students and their families meet the costs of their education beyond high school. Your family's experiences, when combined with those of all students and parents participating in the study, will be used, in part, to help determine how much federal grant, loan, and work-study aid will be available in the future.\nTenemos disponibles un servicio para personas con impedimentos de audiciOn o del habla--en inglds solamente. Si usted requiere de este servicio. llamenos, libre de cargos, al 1-800-647-9659 (SRT). For each eligible student, please provide the following data. If student was enrolled in a course for credit during the study period (all terms that began between May 1, 1995, and April 30, 1996, should be included), list all terms for which the student was enrolled and provide the following information for each term: Attendance Status (use key below): 1 = Full-time 2 = Half-time or more, but less than Full-time 3 = Less than Half-time Credit or Clock Hours Masters of Public Administration (MPA) 6. Masters of Arts in Library Sciences (MALS) 7. Masters of Public Health (MPH) 8. Maters of Fine Arts (MFA) 9. Masters of Applied Arts (MAA) 10. Masters of Arts in Teaching (MAT) 11. Masters of Divinity (M.Div) 12."}, {"section_title": "8ES COPY", "text": "Because the parents of only about one of every 4000 students who were enrolled during 1995-96 have the opportunity to participate in this study, the information you provide on how you helped finance your child's postsecondary education is very important to its success. Your responses will represent thousands of parents like yourself. An interviewer from RTI will contact you by telephone sometime in the near future to ask you some questions about your experiences in financing your child's postsecondary education, especially for the 1995-96 school year. Based on prior studies, you can shorten the interview time if you have any documents about your income available at the time of the interviewer's call. Parents of students enrolled in less-than-2-year institutions, community colleges, 4-year colleges, and major universities in the United States and Puerto Rico, including those who do not receive financial aid and those who do receive aid, participate in NPSAS. If your child did not receive financial aid, we would like to know how you may have helped your child meet the costs of attending school during the 1995-96 year. For example, if you provided some financial support, did you take out private loans or use money that you may have set aside for other purposes? For families that received student financial aid, we want to know whether students received enough to meet their education expenses. "}, {"section_title": "B-10", "text": "The time required to gather the information needed and'complete the telephone interview is estimated to vary from about 10 to 25 minutes, with an average of about 15-20 minutes per telephone interview. If you have any comments concerning the accuracy of the time estimates or suggestions for improving the collection of information. write directly to: U.S. Department of Education, National Center for Education Statistics, NPSAS Project Officer #1850-0666, 553 New Jersey Avenue, NW, Washington, DC 20208. I want to assure you that NCES and its contractors adhere to the highest standards in protecting the privacy of individuals involved in the studies it undertakes. Only a limited number of researchers may be authorized by NCES to access information that may identify individuals. They may use the data only for statistical purposes and are subject to fines and imprisonment for misuse. Data will be combined to produce statistical reports for Congress and others. No individual data that link your name, address. telephone number, or your child's student identification number with your responses will be reported. Your participation in NPSAS is strictly voluntary; however, your responses are necessary to make the results of this study accurate and timely. Additional information explaining how you were selected and some of the information we have collected from the school your child attended during 1995-96 is enclosed. If you have any questions about this study prior to your call from RTI. or if you would like to set up an appointment to be interviewed, please call Marty Nash at RTI. The toll-free number is 1-800-472-6094. Thank you very much. Your cooperation is greatly appreciated. Sincerely, Jeanne E. Griffith Acting Commissioner NOTE: Arrangements have been made to allow the participation of persons who are hearing or speech impaired. Call us (toll free) at 1-800-647-9659 (TDD). information sabre varios temas tales como: estadisticas demogrMicas sobre los estudiantes, ingresos de familia, gastos educacionales, empleo, costo de vida, aspiraciones educacionales y los medios pot los cuales los estudiantes y sus familias logran pagar el costo de su educacidn pos-secundaria. La informacida suya, cuando se combine con la de todos los demtis estudiantes y padres de familia que tomen parte en este estudio, sera usada, en parte, para determinar cuanta ayuda economica federal habia disponible en el futuro pot. medio de betas, prdstamos, o programas de estudio y trabajo. Dodo que los padres de solamente I de coda 4,000 de los estudiantes que esurvieron matriculados durante el aft escolar 1995-96 tendrdn la oportunidad de participar en este estudio, la informaci6n que usted nos proves sabre cdmo le ayudo a su hijo(a) a pagar los gastos de su educacidn pos-secandaria es may importante para el trito del estudio. Sus respuestas representaran a mites de padres de familia como usted. Un entrevistador de RTI lo llama por telefono en los prdximos dias para hacerle algunas preguntas acerca de su experiencia financiera--es decir, cdmo pag6 por la educacidn pos-secundaria de su hijo(a)--especificamente durante el alto escolar 1995-96. En base a experiencia obtenida por medio de este estudio en atios pasados, sabemos que Ia entrevista puede hacerse un poco mss breve si tiene a to mano documentos sobre sus ingresos cuando se le haga la Ilamada celefonica. Los padres de estudiantes que esten matriculados en instituciones con programas educativos de me.nos de 2 altos, escuelas comunitarias (\"community colleges\"), escuelas de 4 altos, y universidades principales en los Estados Unidos y Puerto Rico, participaran en NPSAS:96 --incluyendo a esos estudiantes que oo reciben ayuda financiera como tambitht aquellos que si Ia reciben. Si su hijo(a) no recibi6 ayuda financiera, quisieramos saber como le pudo ayudar a su hijo(a) a pagar los gastos para poder asistir a la escuela durante el alto escolar 1995-96. minutos; et promedio es de 15-20 minutos por cada entrevista hecha por teldfono. Si usted tiene algtin cotnentario acerca de la exactitud del tiempo estimado para ser entrevistado o alguna sugerencia sobre como mejorar la recopilacik de esta informacik, escribanos directamente al: U.S. Department of Education, National Center for Education Statistics (NCES), NPSAS Project Officer #1850-0666, 555 New Jersey Avenue NW, Washington, DC 20208. Le aseguramos que NCES y sus representantes siguen las mds estrictas normas para proteger los derechos de privacidad de las personas que participan en estudios que se hacen bajo su direcci6n. Solamente un 'lamer\u00b0 lirnitado de personas seran autorizadas por NCES para tener acceso a fa informacion que pudiera identificar a un individuo. Estas personas pueden usar los datos sinicamenteyara propOsitos estadaticos y estan expuestos a ser multados y encarcelados por mal use de los dutos. Toda la informacion obtenidu se uriird para producir reportes estadtsticos para el Congreso de los LE. UL'. u otras agencias que requieran esta informacion. La informacion individual que podria identificar su nombre, direction, narnero telefonico. o namero de identificacidn de estudiante de su hijo(a), nunca sera relacionada con sus respuestas en ningan informe. Su participation en NPSAS:96 es completamente voluntaria, aan ast sus respuestas son necesarias para lograr que los resulzados de este estudio sean precisosy actualizados. Encontrard adjunto informacik adicional donde se le explica coma usted fue seleccionado para este estudio y tambien algunos informs que recopilamos sobre la escuela a la cual asistid su hijo(a) en el alto escolar 1995-96. Si tiene alguna pregunta sobre este estudio antes de recibir la Ilamada de RTI, o si usted desea hacer una city previa para dejaraos saber cuando nos podemos comunicar con usted para ser entrevistado, por favor comuniquese con la Sra. Marty Nash al ntimero telefonico, libre de cargos (\"toll free\"), 1-800-472-6094. Muchas gracias par su participacidn. Ge agradecemos su cooperacion. Sinceramente, Jeanne E. Griffith"}, {"section_title": "Cotnisionado Interino", "text": ""}, {"section_title": "Masters of Social Work (MSW)", "text": "13. Masters of Landscape Architecture (MLA) 14."}, {"section_title": "Masters of Professional Management MPM)", "text": "15. Which of the following Doctoral or First Professional degrees was the student working toward during Doctor of Philosophy (PhDO 2. Doctor of Education (Ed.D) 3. Doctor of Theology (ThDO 4. Doctor of Business Administration (DBA) 5. Doctor of engineering (D.Eng) 6. Doctor of Fine Arts (DFA) 7. Doctor of Public Administration (DPA) 8. Doctor of Science (Dsc/ScD) 9. Other Doctoral For each term attended by the student (those terms identified in the Enrollment/Term Sub-section above), specify amounts of tuition and fees charged. Please provide separate amounts for each term, if available."}, {"section_title": "BTUAMT01-BTUAMT12", "text": "Question 2. Total tuition and fees charged for all terms."}, {"section_title": "BTURNTTL (IF BY TERM) BTTUITOT (IF NOT BY TERM)", "text": "Question 3. For tuition purposes, this student was classified as: (Use key below) 1. In jurisdiction (e.g., in-state, in-district, etc.) 2. Out-of-jurisdiction (e.g., out-of-state, out-of-district, etc.) Graduate Fellowships NOTE: Institutional Grants and Scholarships -Items A, B, and C, vary by institution. Please refer to CADE for the specific items which should be included here for your institution ALSO NOTE: Items A, B, C, D, and E are requested for Undergraduates. All items are requested for Graduates, Doctoral, and First-Professional Students."}, {"section_title": "Other Awards", "text": "Please also report any other financial aid awarded to the student, provide: 1. the name of the award 2. the type of award (Use key below) 1. SESCHJOB --indicator that student held work study job or assistantship If SZFWS gt 0 or SZASST gt 0 then SESCHJOB=1; If SDAMT01--SDAMTIO (any one of the variables in the set) ge 1 and the corresponding SDTYPOI --SDTYPI 0 = 3 or 4, then SESCHJOB=1; If SDOAMT01 SDOAMT05 (any one of the variables in the set) ge 1 and the corresponding SDOTYPOI SDOTYPO5 = 3 or 4, then SESCHJOB=1.            You've been selected for a U.S. Department of Education study to determine what happens to students as they pursue their education. We would like to talk to you again in a couple years to see what you are doing and what has changed in your life. To find you, we need some locating information; which will be kept in strict confidence."}, {"section_title": "SJBPSCON", "text": "Would you please tell me the name, address, and phone number of a person --preferably a relative other than your parents --who lives at an address different from yours and who will always know where to get in touch with you? payments to the school for the year were... 1 = ...less than 1,000 2 = ...1 to 4 thousand (LESS THAN $5,000) 3 = ...5 to 9 thousand (LESS THAN $10,000) 4 = ...10 to 19 thousand (LESS THAN $20,000) 5 = ...or 20 thousand or more -1 =DK -2=RE ..less than 1,000 2 = ...1 to 4 thousand (LESS THAN $5,000) 3 = ...5 to 9 thousand (LESS THAN $10,000) 4 = ...10 to 19 thousand (LESS THAN $20,000) 5 = ...or 20 thousand or more -1 =DK -2 =RE PLGVTFEE Was that money intended for...  Please tell me if your employment income for 1995 was... 1 = Less than 5,000 2 = 5 to 9 thousand (LESS THAN $10,000) 3 = 10 to 19 thousand (LESS THAN $20,000) 4 = 20 to 29 thousand (LESS THAN $30,000) -1 = DK -2 = RE PNEST95M I'm going to read you some dollar ranges. Please tell me the range that best estimates your employment income for 1995... 1 = 30 to 39 thousand (LESS THAN $ 40,000) 2 = 40 to 49 thousand (LESS THAN $ 50,000) 3 = 50 to 59 thousand (LESS THAN $ 60,000) 4 = 60 to 69 thousand (LESS THAN $ 70,000) 5 = 70 to 79 thousand (LESS THAN $ 80,000) 6 = 80 to 89 thousand (LESS THAN $ 90,000) 7 = 90 to 99 thousand (LESS THAN $100,000) 8 = $100,000 or more -1 = DK -2 = RE"}, {"section_title": "PNEST94", "text": "Would you estimate that your 1994 employment income was more or less than $30,000?   Instructions: Please answer each question by placing a check (i) on the line next to the appropriate response or filling in the information requested. The NPSAS School referenced is the school shown on the label on this page. The study period of interest is the 1995-96 school year (between July 1, 1995 andJune 30, 1996). If you do not know an exact dollar amount for an item, please try to estimate the amount. Your participation in this study is completely voluntary and your decision to participate will not affect any financial aid or other benefits you are receiving. You may decline to answer any question. All information you provide is confidential. When you have completed your self-administered interview, please return it within 2 weeks in the selfaddressed, postage-paid return envelope provided. Thank you for participating in this very important study. requirements? If you did not complete high school, in what year A. YOUR ENROLLMENT AT THE NPSAS SCHOOL were you last enrolled in high school? 1. Were you enrolled at the NPSAS school sometime between July 1, 1995 and June 30, 1996? The NPSAS school is the school B. YOUR BACKGROUND identified on the label shown on this page. 0 No disabilities or impairments 1 A hearing impairment 2 A speech disability or limitation 3 An orthopedic or mobility limitation 4 A specific learning disability 5 A vision impairment that cannot be corrected with eyeglasses or legally blind 6 Other disability or limitation 15. During the 1995-96 school year, where did you live most of the time? _1 On-campus in school-owned housing _2 Off-campus in school-owned housing _3 In an apartment or house other than with parents/ guardians 4 With parents/guardians 5 With other relatives 6 Other (Please specify) 16. In the last year, how many hours of community service or volunteer work did you perform, other than courtordered service? Enter 0 if none. Instrucciones para completar esta encuesta: Por favor conteste cada pregunta haciendo una marca ( if) en la linea al lado de la respuesta apropiada o llenando la informacion requerida. El concepto de Escuela NPSAS al cual nos refirimos en varias preguntas es la escuela cuyo nombre esta en la etiqueta que se encuentra en esta pagina. El periodo que nos interesa para este estudio es el aim escolar 1994-95 (entre las fechas del primero de mayo, 1994 hasta el 30 de junio, 1995). Si usted no sabe la cantidad exacta para una pregunta, favor de estimar la cantidad. Su participacion en este estudio es completamente voluntaria y su decision de tomar parte no afectaria a ninguna de la asistencia economica la cual usted este recibiendo. Usted puede negarse a contestar cualquier pregunta. Toda la informacion que nos provea es confidencial. _1 Una vivienda propiedad universitaria hubicada en el \"campus\" 2 Una vivienda propiedad universitaria no en el \"campus\" 3 En un apartamento or casa que no fuera de los padres/tutores 4 Con los padres/tutores Haga un circulo al namero en la primera columna para su madre/tutora y en la segunda columna para su padre/tutor. "}, {"section_title": "SBSCHRES", "text": "While enrolled during 95-96, where did you live? 1 = on-campus in school-owned housing, 2 = off-campus in school-owned housing, 3 = in an apartment or house other than with your parents or guardians (including houses owned by fraternities and sororities), 4 = with your parents or guardians, 5 = with other relatives, or 6 = some place else? 5-3 5-3 5-3 5-6 5-6 5-6 5-7 5-7 5-8 5-8 5-8 5-9 5-9 5-9 5-10 5-11   "}, {"section_title": "LIST OF EXHIBITS", "text": "5-1 5-3 5-3 5-3 5-6 5-6 5-6 5-7 5-7 5-8 5-8 5-8 5-9 5-9 5-9 5-10  The Toshiba Satellite Computer with the Display Raised The Right Side Changing the Angle of the Keyboard  "}, {"section_title": "Statistical Analysis Considerations", "text": "The NPSAS:96 sampling design was a stratified two-stage design. A stratified sample of postsecondary institutions was selected with probabilities proportional to a composite measure of size at the first stage, and a stratified systematic sample of students was selected from sample institutions at the second stage. Moreover, a stratified subsample of students was selected for computer-assisted telephone interviewing (CATI). At the first stage, about ten percent of the eligible institutions were selected, but the institution sampling rates varied considerably by institutional level and control.' At the second stage, potential first-time, beginning (FTB) students were oversampled. Moreover, FTBs were retained for CATI with certainty, while about half of all other students were retained, and higher sampling rates were used for students whose CADE data indicated that they were financial aid applicants. Because of this complex sampling design, it is important that statistical analyses be conducted using software that properly accounts for the complex survey sampling design. Most commonly-used statistical computing packages (e.g., SAS and SPSS) assume that the data were obtained from a simple random sample; that is, they assume that the observations are independent and identically distributed. When data have been collected using a complex sampling design, the simple random sampling assumption can lead to an underestimate of the sampling variance, which can therefore lead to artificially small confidence intervals and anticonservative hypothesis tests results (i.e., rejecting the null hypothesis when it is in fact true more often than indicated by the nominal Type I error level) (Carlson et al, 1993). Statistical strategies that have been developed to address this issue include: first-order Taylor series expansion of the variance equation; balanced repeated replication; and the Jackknife approach (see, e.g., Wolter, 1985). Special-purpose software packages that have been developed for analysis of complex sample survey data include SUDAAN and WesVarPC. Recently, the statistical software package Stata has added features for analysis of complex survey data. Evaluations of the relative performances of these packages are reported by Cohen (1997). SUDAAN is a commercial product developed by Research Triangle Institute; information regarding the features of this package and its lease terms is available from the Web site http://www.rti.org/patents/sudaan/sudaan.html. WesVarPC is a product of Westat, Inc. and can be downloaded from the Web site http://www.westat.com/wesvarpc/index.html. NCES also has developed a software tool called the Data Analysis System (DAS) for analysis of complex survey data. Information about using the DAS is available from the Web site http://www.pedar-das.org. This site includes links to many NCES DAS files, including the NPSAS DAS files. I From about 3 percent for private, for-profit, less-than-2-year institutions to about 51 percent for public, 4year, doctoral/first-professional-granting institutions. NPSAS:96 Methodology Report: Appendix P F-3 If one must perform a quick analysis of NPSAS:96 data without using one of the software packages for analysis of complex survey data, the design effect tables in this appendix can be used to make approximate adjustments to the standard errors of survey statistics computed using the standard software packages that assume simple random sampling designs. For example, Table F.9 shows design effects based on the study weights for undergraduate students at public, 4-year, doctoral/first-professional institutions. If one had computed a statistic (e.g., mean Pell grant) for this domain of students using the study weights, then the summary statistics from Table F.9 suggest that the standard error computed from the standard statistical software package should be multiplied by a survey design effect of about 3.64 (the median for this domain). However, the range of design effects shown in Table F.9 for this domain is from 2.28 to 7.87. Therefore, one cannot be confident regarding the actual design-based standard error without performing the analysis using one of the software packages specifically designed for analysis of data from complex sample surveys. Section A Design effect tables for undergraduate students based on the study weights NPSAS:96 Methodology ilsport: Appendix F 5 F-5                                     "}, {"section_title": "3.72\u00b0", "text": "The design effect is undefined because the estimate is 100.00."}, {"section_title": "NEST COPY AVAr", "text": "Pg 379119 Appendix F            BEST COPY AVAILABLE   BEST COPY AVAUBLE                     "}, {"section_title": "497", "text": "BEST COPY AVA8"}, {"section_title": "BLE EFC Imputations", "text": "For expected family contribution (EFC), a regression approach was used for imputation. The goal was to obtain the most parsimonious and best fitting equations using information likely to be available for non-aided students (those most likely to have a missing EFC). The general appoach was to develop logistic regression models to estimate zero EFC cases, and then use ordinary least squares (OLS) regression models to estimate the predicted EFC for non-zero EFC cases. This approach was designed to accommodate the truncated EFC distribution (i.e., the large number of zero EFC cases in the population) and followed a conventional econometric approach for such cases. The EFC imputations were performed separately for three categories of students: Dependent Students, Independent Students without Dependents, and Independent Students with Dependents. The extent of the imputation is shown in Table I.1 for the total group and for three categories of student dependency. The first step in imputing EFC for independent students used the parameter estimates from logistic regressions to predict whether or not the student fell into the zero EFC group. If the estimate probability was below 0.5, the cases was estimated to have a non-zero EFC; if above 0.5, the case was estimated to have a zero EFC. For the non-zero cases, an ordinary least squares based regression formula was then used to estimate the independent student EFC. For independent students without dependents, the variables used in the imputation were: For independent students with dependents, the variables used in the imputation were: For dependent students, two components of EFC were estimated and the sum of the estimated components was used as the estimate of the dependent students' EFC. The two components of dependent student EFC were: (a) the adjusted parent contribution and (b) the dependent student contribution. The estimation proceeded along lines similar to those used for the independent students, with a logistic regression first used to estimate zero EFC, followed by OLS regression. However, the logistic prediction for the adjusted parent contribution was not satisfactory, so OLS was used to estimate the entire component for all cases. were: For dependent students, the variables used to predict the Adjusted Parent Contribution Parent Total Income (PARINC94) Dummy Variable based on PARINC94 GE $60,000 Family Size (PFAMNUM) Dummy Variable based on business or real estate assets GT $50,000 or savings more than $10,000 The correlation coefficients between estimated and actual EFC among those students with 1995-96 CPS records were: dependent .83; independent/no dependents .93; independent/with dependents .95. For independent students, over 90 percent of the predicted values were within one thousand dollars of the actual value for the EFC. As shown in Table 1.2, the results for dependent students were less satisfactory, with only about half of the values within one thousand dollars. When the equations were tested using the 1996-97 CPS records for those students who had financial aid application data available for both years, the results were similar. The composite EFC variable in the analysis file represents the actual recorded EFC (if available) or the EFC estimated by regression for the cases with no recorded information. The distribution of the recorded, imputed, and composite EFC values are shown in Table 1.3. Since higher income students and families are less likely to apply for financial aid, approximately one-half of the imputed EFC's are in the ranges above 9,500. BEST COPY AVM ABLE NPSAS:96 METHODOLOGY REPORT: APPENDIX I 3,074 6.4 Table 1.4 compares the average income by EFC level and dependency status before and after imputation. Table 1.5 compares the income distribution of dependent and independent students before and after the EFC imputations, as well as the average actual EFC and composite EFC for each income level before and after imputation. NPSAS:96 METHODOLOGY REPORT: APPENDIX I 1-1g    86.72' Notes: 1. There were 38,995 eligible students selected for the CATI subsample which resulted in 31,328 respondents and 7,667 nonrespondents. This table excludes 705 of the 7,667 CATI nonrespondents because they were also study nonrespondents and therefore had missing data for the analysis variables of interest. These exclusions could cause some bias in the nonrespondent estimates, but we expect that it is negligible because of the relatively small number of students excluded. 2. Tests for significant differences between the distributions of the respondents and nonrespondents were performed for each of the eight primary variables at the (0.05 / 8) level to account for multiple comparison effects. Except for gender, all of the variables were found to be significant. 3. Within each variable, the category percentages of respondents and nonrespondents were tested for significant differences at the (0.05 / (c -1)) level, where c is the number of categories. Estimates that were found to be significantly different are flagged with an asterisk. 4. The effects of these significant differences on the CATI-based estimates are mitigated by the CATI nonresponse weight adjustments, which explicitly accounted for differences in age distribution and aid status. 5. Some of the statistically significant differences may not be of practical significance (e.g., 4 percent versus 5 percent for dependent students with incomes of $100,000 or more). Difference between respondents and nonrespondents is significant at the 0.05/(c-1) level, where c is the number of categories within the primary variable."}, {"section_title": "BES copy MAL),", "text": "NPSAS:96 METHODOLOGY REPORT: APPENDIX a U .5 . GOVERNMENT PRINTING OFFICE: 1 997-434 -4 80 / 80 401 J-4"}]