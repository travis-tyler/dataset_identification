[{"section_title": "Abstract", "text": "In this paper, we consider the problem of estimating multiple graphical models simultaneously using the fused lasso penalty, which encourages adjacent graphs to share similar structures. A motivating example is the analysis of brain networks of Alzheimer's disease using neuroimaging data. Specifically, we may wish to estimate a brain network for the normal controls (NC), a brain network for the patients with mild cognitive impairment (MCI), and a brain network for Alzheimer's patients (AD). We expect the two brain networks for NC and MCI to share common structures but not to be identical to each other; similarly for the two brain networks for MCI and AD. The proposed formulation can be solved using a second-order method. Our key technical contribution is to establish the necessary and sufficient condition for the graphs to be decomposable. Based on this key property, a simple screening rule is presented, which decomposes the large graphs into small subgraphs and allows an efficient estimation of multiple independent (small) subgraphs, dramatically reducing the computational cost. We perform experiments on both synthetic and real data; our results demonstrate the effectiveness and efficiency of the proposed approach."}, {"section_title": "Introduction", "text": "Undirected graphical models explore the relationships among a set of random variables through their joint distribution. The estimation of undirected graphical models has applications in many domains, such as computer vision, biology, and medicine [11, 17, 44] . One instance is the analysis of gene expression data. As shown in many biological studies, genes tend to work in groups based on their biological functions, and there exist some regulatory relationships between genes [5] . Such biological knowledge can be represented as a graph, where nodes are the genes, and edges describe the regulatory relationships. Graphical models provide a useful tool for modeling these relationships, and can be used to explore gene activities. One of the most widely used graphical models is the Gaussian graphical model (GGM), which assumes the variables to be Gaussian distributed [2, 47] . In the framework of GGM, the problem of learning a graph is equivalent to estimating the inverse of the covariance matrix (precision matrix), since the nonzero off-diagonal elements of the precision matrix represent edges in the graph [2, 47] .\nIn recent years many research efforts have focused on estimating the precision matrix and the corresponding graphical model (see, for example [2, 10, 16, 17, 22, 23, 25, 26, 29, 30, 33, 47] . Meinshausen and B\u00fchlmann [30] estimated edges for each node in the graph by fitting a lasso problem [36] using the remaining variables as predictors. Yuan and Lin [47] and Banerjee et al. [2] proposed a penalized maximum likelihood model using \u2113 1 regularization to estimate the sparse precision matrix. Numerous methods have been developed for solving this model. For example, d'Aspremont et al. [8] and Lu [25, 26 ] studied Nesterov's smooth gradient methods [32] for solving this problem or its dual. Banerjee et al. [2] and Friedman et al. [10] proposed block coordinate ascent methods for solving the dual problem. The latter method [10] is widely referred to as Graphical lasso (GLasso). Mazumder and Hastie [29] proposed a new algorithm called DP-GLasso, each step of which is a box-constrained QP problem. Scheinberg and Rish [35] proposed a coordinate descent method for solving this model in a greedy approach. Yuan [48] and Scheinberg et al. [34] applied alternating direction method of multipliers (ADMM) [4] to this problem. Li and Toh [22] and Yuan and Lin [47] proposed to solve this problem using interior point methods. Wang et al. [40] , Hsieh et al. [16] , Olsen et al. [33] , and Dinh et al. [9] studied Newton method for solving this model. The main challenge of estimating a sparse precision matrix for the problems with a large number of nodes (variables) is its intensive computation. Witten et al. [42] and Mazumder and Hastie [28] independently derived a necessary and sufficient condition for the solution of a single graphical lasso to be block diagonal (subject to some rearrangement of variables). This can be used as a simple screening test to identify the associated blocks, and the original problem can thus be decomposed into a group of smaller sized but independent problems corresponding to these blocks. When the number of blocks is large, it can achieve massive computational gain. However, these formulations assume that observations are independently drawn from a single Gaussian distribution. In many applications the observations may be drawn from multiple Gaussian distributions; in this case, multiple graphical models need to be estimated.\nThere are some recent works on the estimation of multiple precision matrices [7, 11, 12, 13, 19, 20, 31, 49] . Guo et al. [11] proposed a method to jointly estimate multiple graphical models using a hierarchical penalty. However, their model is not convex. Honorio and Samaras [13] proposed a convex formulation to estimate multiple graphical models using the \u2113 1,\u221e regularizer. Hara and Washio [12] introduced a method to learn common substructures among multiple graphical models. Danaher et al. [7] estimated multiple precision matrices simultaneously using a pairwise fused penalty and grouping penalty. ADMM was used to solve the problem, but it requires computing multiple eigen decompositions at each iteration. Mohan et al. [31] proposed to estimate multiple precision matrices based on the assumption that the network differences are generated from node perturbations. Compared with single graphical model learning, learning multiple precision matrices jointly is even more challenging to solve. Recently, a necessary and sufficient condition for multiple graphs to be decomposable was proposed in [7] . However, such necessary and sufficient condition was restricted to two graphs only when the fused penalty is used. It is not clear whether this screening rule can be extended to the more general case with more than two graphs, which is the case in brain network modeling.\nThere are two types of fused penalties that can be used for estimating multiple (more than two) graphs: (a) pairwise fused or (b) sequential fused [37] . In this paper we set out to address the sequential fused case first, because we work on practical applications that can be more appropriately formulated using the sequential formulation. Specifically, we consider the problem of estimating multiple graphical models by maximizing a penalized log likelihood with \u2113 1 and sequential fused regularization. The \u2113 1 regularization yields a sparse solution, and the fused regularization encourages adjacent graphs to be similar. The graphs considered in this paper have a natural order, which is common in many applications. A motivating example is the modeling of brain networks for Alzheimer's disease using neuroimaging data such as Positron emission tomography (PET). In this case, we want to estimate graphical models for three groups: normal controls (NC), patients of mild cognitive impairment (MCI), and Alzheimer's patients (AD). These networks are expected to share some common connections, but they are not identical. Furthermore, the networks are expected to evolve over time, in the order of disease progression from NC to MCI to AD. Estimating the graphical models separately fails to exploit the common structures among them. It is thus desirable to jointly estimate the three networks (graphs). Our key technical contribution is to establish the necessary and sufficient condition for the solution of the fused multiple graphical lasso (FMGL) to be block diagonal. The duality theory and several other tools in linear programming are used to drive the necessary and sufficient condition. Based on this crucial property of FMGL, we develop a screening rule which enables the efficient estimation of large multiple precision matrices for FMGL. The proposed screening rule can be combined with any algorithms to reduce computational cost. We employ a second-order method [16, 21, 38] to solve the fused multiple graphical lasso, where each step is solved by the spectral projected gradient method [27, 43] . In addition, we propose a shrinking scheme to identify the variables to be updated in each step of the second-order method, which reduces the computation cost of each step. We conduct experiments on both synthetic and real data; our results demonstrate the effectiveness and efficiency of the proposed approach."}, {"section_title": "Notation", "text": "In this paper, \u211c stands for the set of all real numbers, \u211c n denotes the n-dimensional Euclidean space, and the set of all m \u00d7 n matrices with real entries is denoted by \u211c m\u00d7n . All matrices are presented in bold format. The space of symmetric matrices is denoted by S n . If X \u2208 S n is positive semidefinite (resp. definite), we write X 0 (resp. X \u227b 0). Also, we write X Y to mean X \u2212 Y 0. The cone of positive semidefinite matrices in S n is denoted by S n + . Given matrices X and Y in \u211c m\u00d7n , the standard inner product is defined by X, Y := tr(XY T ), where tr(\u00b7) denotes the trace of a matrix. X \u2022 Y and X \u2297 Y means the Hadamard and Kronecker product of X and Y, respectively. We denote the identity matrix by I, whose dimension should be clear from the context. The determinant and the minimal eigenvalue of a real symmetric matrix X are denoted by det(X) and \u03bb min (X), respectively. Given a matrix X \u2208 \u211c n\u00d7n , diag(X) denotes the vector formed by the diagonal of X, that is, diag(X) i = X ii for i = 1, . . . , n. Diag(X) is the diagonal matrix which shares the same diagonal as X. vec(X) is the vectorization of X. In addition, X > 0 means that all entries of X are positive.\nThe rest of the paper is organized as follows. We introduce the fused multiple graphical lasso formulation in Section 2. The screening rule is presented in Section 3. The proposed second-order method is presented in Section 4. The experimental results are shown in Section 5. We conclude the paper in Section 6."}, {"section_title": "Fused multiple graphical lasso", "text": "Assume we are given K data sets, x (k) \u2208 R n k \u00d7p , k = 1, . . . , K with K \u2265 2, where n k is the number of samples, and p is the number of features. The p features are common for all K data sets, and all K k=1 n k samples are independent. Furthermore, the samples within each data set x (k) are identically distributed with a p-variate Gaussian distribution with zero mean and positive definite covariance matrix \u03a3 (k) , and there are many conditionally independent pairs of features, i.e., the precision matrix\nshould be sparse. For notational simplicity, we assume that n 1 = \u00b7 \u00b7 \u00b7 = n K = n. Denote the sample covariance matrix for each data set\n, and\n. Then the negative log likelihood for the data takes the form of\nClearly, minimizing (1) leads to the maximum likelihood estimate (MLE)\u0398 (k) = (S (k) ) \u22121 . However, the MLE fails when S (k) is singular. Furthermore, the MLE is usually dense. The \u2113 1 regularization has been employed to induce sparsity, resulting in the sparse inverse covariance estimation [2, 10, 46] . In this paper, we employ both the \u2113 1 regularization and the fused regularization for simultaneously estimating multiple graphs. The \u2113 1 regularization leads to a sparse solution, and the fused penalty encourages \u0398 (k) to be similar to its neighbors. Mathematically, we solve the following formulation:\nwhere To ensure the existence of a solution for problem (2), we assume throughout this paper that\nis a sample covariance matrix, and hence diag(S (k) ) \u2265 0. The diagonal entries may be not, however, strictly positive. But we can always add a small perturbation (say 10 \u22128 ) to ensure the above assumption holds. The following theorem shows that under this assumption the FMGL (2) has a unique solution.\nTheorem 2.1. Under the assumption that diag(S (k) ) > 0, k = 1, . . . , K, problem (2) has a unique optimal solution.\nTo prove Theorem 2.1, we first establish a technical lemma which regards the existence of a solution for a standard graphical lasso problem. Lemma 2.2. Let S \u2208 S p + and \u039b \u2208 S p be such that Diag(S) + \u039b > 0 and diag(\u039b) \u2265 0. Consider the problem min\nThen the following statements hold:\n(a) Problem (3) has a unique optimal solution;\nwhere f * is the optimal value of (3).\nWe first claim that the feasible region of problem (4) is nonempty, or equivalently, there exists U \u2208 U such that \u03bb min (S + \u039b \u2022\u016a) > 0. Indeed, one can observe that\nwhere the second equality follows from the Lagrangian duality since its associated Slater condition is satisfied. Let \u2126 := {X \u2208 S p : tr(X) = 1, X 0}. By the assumption Diag(S) + \u039b > 0, we see that \u039b ij > 0 for all i = j and S ii + \u039b ii > 0 for every i. Since \u2126 \u2282 S p + , we have tr(SX) \u2265 0 for all X \u2208 \u2126. If there exists some k = l such that X kl > 0, then\nOtherwise, one has X ij = 0 for all i = j, which, together with the facts that S ii + \u039b ii > 0 for all i and tr(X) = 1, implies that for all X \u2208 \u2126,\nHence, (6) again holds. Combining (5) with (6), one can see that max U\u2208U \u03bb min (S+\u039b\u2022U) > 0. Therefore, problem (4) has at least a feasible solution. We next show that problem (4) has an optimal solution. Let\u016a be a feasible point of (4), and\nOne can observe that {S + \u039b \u2022 U : U \u2208 U} is compact. Using this fact, it is not hard to see that log det(S + \u039b \u2022 U) \u2192 \u2212\u221e as U \u2208 U and \u03bb min (S + \u039b \u2022 U) \u2193 0. Thus there exists some \u03b4 > 0 such that\nwhich implies that\nHence,\u03a9 is a compact set. In addition, one can observe that problem (4) is equivalent to\nThe latter problem clearly has an optimal solution and so is problem (4). Finally we show that\nis the unique optimal solution of (3), where U * is an optimal solution of (4). Since S + \u039b \u2022 U * \u227b 0, we have X * \u227b 0. By the definitions of U and X * , and the first-order optimality conditions of (4) at U * , one can have\nwhere \u2202(\u00b7) stands for the subdifferential of the associated convex function. For convenience, let f (X) denote the objective function of (3). Then we have\nwhich, together with X * = (S + \u039b \u2022 U * ) \u22121 , implies that 0 \u2208 \u2202f (X * ). Hence, X * is an optimal solution of (3) and moreover it is unique due to the strict convexity of \u2212 log det(\u00b7).\n(b) By statement (a), problem (3) has a finite optimal value f * . Hence, the above sub-level set L is nonempty. We can observe that for any X \u2208 L,\nwhere f * := inf{f (X) : X \u227b 0}. By the assumption Diag(S) + \u039b > 0, one has Diag(S) + \u039b/2 > 0. This together with statement (a) yields f * \u2208 \u211c. Notice that \u039b ij > 0 for all i = j. This relation and (7) imply that X ij is bounded for all X \u2208 L and i = j. In addition, it is well-known that det(X) \u2264 X 11 X 22 \u00b7 \u00b7 \u00b7 X pp for all X 0. Using this relation, the definition of f (\u00b7), and the boundedness of X ij for all X \u2208 L and i = j, we have that for every X \u2208 L,\nfor some \u03b4 > 0. In addition, notice from the assumption that S ii + \u039b ii > 0 for all i, and hence\nfor all i. This relation together with (8) implies that for every X \u2208 L and all i,\nand hence X ii is bounded for all i and X \u2208 L. We thus conclude that L is bounded. In view of this result and the definition of f , it is not hard to see that there exists some \u03bd > 0 such that\nBy the continuity of f on {X : X \u03bdI}, it follows that L is closed. Hence, L is compact.\nWe are now ready to prove Theorem 2.1.\n. . , K, it follows from Lemma 2.2 that there exists some \u03b4 such that for each k = 1, . . . , K,\nFor convenience, let h(\u0398) denote the objective function of (2) and\u0398 = (\u0398 (1) , . . . ,\u0398 (K) ) an arbitrary feasible point of (2). Let\nIn view of Lemma 2.2, we know that \u2126 k is compact for all k, which implies that\u03a9 is also compact. Notice that h is continuous and strictly convex on\u03a9. Hence, problem (9) has a unique optimal solution and so is problem (2)."}, {"section_title": "The screening rule for fused multiple graphical lasso", "text": "Due to the presence of the log determinant, it is challenging to solve the formulations involving the penalized log-likelihood efficiently. The existing methods for single graphical lasso are not scalable to the problems with a large amount of features because of the high computational complexity. Recent studies have shown that the graphical model may contain many connected components, which are disjoint with each other, due to the sparsity of the graphical model, i.e., the corresponding precision matrix has a block diagonal structure (subject to some rearrangement of features). To reduce the computational complexity, it is advantageous to first identify the block structure and then compute the diagonal blocks of the precision matrix instead of the whole matrix. Danaher et al. [7] developed a similar necessary and sufficient condition for fused graphical lasso with two graphs, thus the block structure can be identified. However, it remains a challenge to derive the necessary and sufficient condition for the solution of fused multiple graphical lasso to be block diagonal for K > 2 graphs. In this section, we first present a theorem demonstrating that FMGL can be decomposable once its solution has a block diagonal structure. Then we derive a necessary and sufficient condition for the solution of FMGL to be block diagonal for arbitrary number of graphs.\n. . , p}. We say that the solution \u0398 of FMGL (2) is block diagonal with L known blocks consisting of features in the sets C l , l = 1, . . . , L if there exists a permutation matrix U \u2208 \u211c p\u00d7p such that each estimation precision matrix takes the form of\nFor simplicity of presentation, we assume throughout this paper that U = I.\nThe following decomposition result for problem (2) is straightforward. Its proof is thus omitted.\nTheorem 3.1. Suppose that the solution \u0398 of FMGL (2) is block diagonal with L known C l , l = 1, . . . , L, i.e., each estimated precision matrix has the form (10) with\nThen there holds:\nwhere\nand S (k) corresponding to the l-th diagonal block, respectively, for k = 1, . . . , K, and\nThe above theorem demonstrates that if a large-scale FMGL problem has a block diagonal solution, it can then be decomposed into a group of smaller sized FMGL problems. The computational cost for the latter problems can be much cheaper. Now one natural question is how to efficiently identify the block diagonal structure of the FMGL solution before solving the problem. We address this question in the remaining part of this section.\nThe following theorem provides a necessary and sufficient condition for the solution of FMGL to be block diagonal with L blocks C l , l = 1, . . . , L, which is a key for developing efficient decomposition scheme for solving FMGL. Since its proof requires some substantial development of other technical results, we shall postpone the proof until the end of this section."}, {"section_title": ", L if and only if S", "text": "(k) , k = 1, . . . , K satisfy the following inequalities:\nOne immediate consequence of Theorem 3.2 is that the conditions (12) can be used as a screening rule to identify the block diagonal structure of the FMGL solution. The steps about this rule are described as follows."}, {"section_title": "Construct an adjacency matrix E", "text": "2. Identify the connected components of the adjacency matrix E (for example, it can be done by calling the Matlab function \"graphconncomp\").\nIn view of Theorem 3.2, it is not hard to observe that the resulting connected components are the partition of the p features into nonoverlapping sets. It then follows from Theorem 3.1 that a large-scale FMGL problem can be decomposed into a group of smaller sized FMGL problems restricted to the features in each connected component. The computational cost for the latter problems can be much cheaper. Therefore, this approach may enable us to solve large-scale FMGL problems very efficiently.\nIn the remainder of this section we provide a proof for Theorem 3.2. Before proceeding, we establish several technical lemmas as follows.\nLemma 3.3. Given any two arbitrary index sets I \u2286 {1, \u00b7 \u00b7 \u00b7 , n} and J \u2286 {1, \u00b7 \u00b7 \u00b7 , n \u2212 1}, let\u012a andJ be the complement of I and J with respect to {1, \u00b7 \u00b7 \u00b7 , n} and {1, \u00b7 \u00b7 \u00b7 , n \u2212 1}, respectively. Define\nwhere J + 1 = {j + 1 : j \u2208 J} andJ + 1 = {j + 1 : j \u2208J}. Then, the following statements hold:\n(i) Either P I,J = {0} or P I,J is unbounded;\n(ii) 0 is the unique extreme point of P I,J ;\n(iii) Suppose that P I,J is unbounded. Then, \u2205 = ext(P I,J ) \u2286 Q, where ext(P I,J ) denotes the set of all extreme rays of P I,J , and\nProof. (i) We observe that 0 \u2208 P I,J . If P I,J = {0}, then there exists 0 = y \u2208 P I,J . Hence, {\u03b1y : \u03b1 \u2265 0} \u2286 P I,J , which implies that P I,J is unbounded.\n(ii) It is easy to see that 0 \u2208 P I,J and moreover there exist n linearly independent active inequalities at 0. Hence, 0 is an extreme point of P I,J . On the other hand, suppose y is an arbitrary extreme point of P I,J . Then there exist n linearly independent active inequalities at y, which together with the definition of P I,J immediately implies y = 0. Therefore, 0 is the unique extreme point of P I,J .\n(iii) Suppose that P I,J is unbounded. By statement (ii), we know that P I,J has a unique extreme point. Using Minkowski's resolution theorem (e.g., see [3] ), we conclude that ext(P I,J ) = \u2205. Let d \u2208 ext(P I,J ) be arbitrarily chosen. Then d = 0. It follows from (13) that d satisfies the inequalities\nand moreover, the number of independent active inequalities at d is n\u22121. If all entries of d are nonzero, then d must satisfy d J \u2212 d J+1 = 0 and dJ \u2212 dJ +1 = 0 (with a total number n \u2212 1), which implies\nWe now assume that d has at least one zero entry. Then, there exist positive integers k,\nOne can immediately observe that\nWe next divide the rest of proof into four cases.\nCase (a): m 1 = 1 and n k = n. In view of (16), one can observe that d mi\u22121 \u2212 d mi = 0 and d ni\u22121 \u2212 d ni\u22121+1 = 0 for i = 2, . . . , k. We then see from (15) that except the active inequalities given in (17) , all other possible active inequalities at d are\n(with a total number k i=2 (m i \u2212 n i\u22121 \u2212 2)). Notice that the total number of independent active inequalities given in (17) is k i=1 (n i \u2212 m i + 1). Hence, the number of independent active inequalities at d is at most\nRecall that the number of independent active inequalities at d is n\u22121. Hence, we have n\u2212k+1 \u2265 n\u22121, which implies k \u2264 2. Due to d = 0, we observe that k = 1 holds for this case. Also, we know that k > 0. Hence, k = 2. We then see that all possible active inequalities described in (18) \nCase (b): m 1 = 1 and n k < n. Using (16), we observe that\nIn view of these relations and a similar argument as in case (a), one can see that the number of independent active inequalities at d is at most \nRecall that k \u2265 1 and the number of independent active inequalities at d is n \u2212 1. Hence, this case cannot occur.\nCombining the above four cases, we conclude that ext(P I,J ) \u2286 Q.\nLemma 3.4. Let P IJ and Q be defined in (13) and (14), respectively. Then, Lemma 3.5. Let x \u2208 \u211c n , \u03bb 1 , \u03bb 2 \u2265 0 be given, and let\nThen, f (y) \u2264 0 for all y \u2208 \u211c n if and only if x satisfies the following inequalities:\nProof. Let P I,J be defined in (13) for any I \u2286 {1, . . . , n} and J \u2286 {1, . . . , n \u2212 1}. We observe that (a) \u211c n = \u222a {P I,J : I \u2286 {1, . . . , n}, J \u2286 {1, . . . , n \u2212 1}}; (b) f (y) \u2264 0 for all y \u2208 \u211c n if and only if f (y) \u2264 0 for all y \u2208 P I,J , and every I \u2286 {1, . . . , n} and J \u2286 {1, . . . , n \u2212 1}; (c) f (y) is a linear function of y when restricted to the set P I,J for every I \u2286 {1, . . . , n} and J \u2286 {1, . . . , n \u2212 1}.\nIf P I,J is bounded, we have P I,J = {0} and f (y) = 0 for y \u2208 P I,J . Suppose that P I,J is unbounded. By Lemma 3.3 and Minkowski's resolution theorem, P I,J equals the finitely generated cone by ext(P I,J ). It then follows that f (y) \u2264 0 for all y \u2208 P I,J if and only if f (d) \u2264 0 for all d \u2208 ext(P I,J ). Using these facts and Lemma 3.4, we see that f (y) \u2264 0 for all y \u2208 \u211c n if and only if f (d) \u2264 0 for all d \u2208 Q, where Q is defined in (14) . By the definitions of Q and f , we further observe that f (y) \u2264 0 for all y \u2208 \u211c n if and only if f (d) \u2264 0 for all\nwhich together with the definition of f immediately implies that the conclusion of this lemma holds.\nhas a solution (\u03b3, v) if and only if (x, \u03bb 1 , \u03bb 2 ) satisfies the following inequalities:"}, {"section_title": "Proof. The linear system (19) has a solution if and only if the linear programming min", "text": "has an optimal solution. The Lagrangian dual of (20) is\nBy the Lagrangian duality theory, problem (20) has an optimal solution if and only if its dual problem (21) has optimal value 0, which is equivalent to f (y) \u2264 0 for all y \u2208 \u211c n . The conclusion of this lemma then immediately follows from Lemma 3.5.\nWe are now ready to prove Theorem 3.2.\nProof. For the sake of convenience, we denote the inverse of \u0398 (k) as W (k) for k = 1, . . . , K. By the first-order optimality conditions, we observe that \u0398 (k) \u227b 0, k = 1, . . . , K is the optimal solution of problem (2) if and only if it satisfies\nfor all i, j = 1, . . . , p, i = j, where \u03b3\n, and \u03c5 (k,k+1) ij\nNecessity: Suppose that \u0398 (k) , k = 1, . . . , K is a block diagonal optimal solution of problem (2) with L known blocks C l , l = 1, . . . , L. Note that W (k) has the same block diagonal structure as\n\u2032 . This together with (23)- (25) implies that for each\nUsing (26) and Lemma 3.6, we see that (12) \nsuch that (26) holds. Now let \u0398 (k) , k = 1, . . . , K be a block diagonal matrix as defined in (10) with\nSince \u0398 l is the optimal solution of problem (11), the first-order optimality conditions imply that (22)- (25) hold for all i,\nUsing this fact and (26), we observe that (22)- (25) also hold for all i \u2208 C l , j \u2208 C l \u2032 , l = l \u2032 . It then follows that \u0398 (k) , k = 1, . . . , K is an optimal solution of problem (2) . In addition, \u0398 (k) , k = 1, . . . , K is block diagonal with L known blocks C l , l = 1, . . . , L. The conclusion thus holds."}, {"section_title": "Second-order method", "text": "The screening rule proposed in Section 3 is capable of partitioning all features into a group of smaller sized blocks. Accordingly, a large-scale FMGL (2) can be decomposed into a number of smaller sized FMGL problems. For each block l, we need to compute its individual estimated precision matrix \u0398 l . In this section, we discuss how to solve those single block FMGL problems efficiently. For simplicity of presentation, we assume throughout this section that the FMGL (2) has only one block, that is, L = 1.\nWe now propose a second-order method to solve the FMGL (2). For simplicity of notation, we let \u0398 := (\u0398 (1) , . . . , \u0398 (K) ) and use t to denote the Newton iteration index. Let\n) be the approximate solution obtained at the t-th Newton iteration.\nThe optimization problem (2) can be rewritten as\nwhere\nIn the second-order method, we approximate the objective function F (\u0398) at the current iterate \u0398 t by a \"quadratic\" model Q t (\u0398):\nwhere q k is the quadratic approximation of f k at \u0398 (k)\nt , that is,\nt . Suppose that\u0398 t+1 is the optimal solution of (28). Then we obtain the Newton search direction\nWe shall mention that the subproblem (28) can be suitably solved by the non-monotone spectral projected gradient (NSPG) method (see, for example, [43, 27] ). It was shown by Lu and Zhang [27] that the NSPG method is locally linearly convergent. Numerous computational studies have demonstrated that the NSPG method is very efficient though its global convergence rate is so far unknown. When applied to (28) , the NSPG method requires solving the proximal subproblems in the form of\nfor some G = (G (1) , . . . , G (K) ) and \u03b1 > 0. By the definition of P (\u0398), it is not hard to see that problem (30) can be decomposed into a set of independent and smaller sized problems\nfor all i \u2265 j, j = 1, . . . , p, where (\u03b1 1 , \u03b1 2 ) = \u03b1(\u03bb 1 , \u03bb 2 ). The problem (31) is known as the fused lasso signal approximator, which can be solved very efficiently and exactly [6, 24] . In addition, they are independent from each other and thus can be solved in parallel. Given the current search direction D = (D (1) , . . . , D (K) ) that is computed above, we need to find the suitable step length \u03b2 \u2208 (0, 1] to ensure a sufficient reduction in the objective function of (2) and positive definiteness of the next iterate \u0398\nIn the context of the standard (single) graphical lasso, Hsieh et al. [16] have shown that a step length satisfying the above requirements always exists. We can similarly prove that the desired step length also exists for the FMGL (2).\n) be the associated Newton search direction computed according to (28) . Suppose D ="}, {"section_title": "0.", "text": "1 Then there exists a\u03b2 > 0 such that \u0398 (k) t + \u03b2D (k) \u227b 0 and the sufficient reduction condition\nholds for all 0 < \u03b2 <\u03b2, where \u03c3 \u2208 (0, 1) is a given constant and\nLemma 4.2. For \u0398 t in the t-th iteration, define the fixed set J f ixed as\nt,ij satisfy the inequalities below}.\nThen, the solution of the following optimization problem is\nProof. Consider problem (34) , which can be reformulated to\nwhere\nwe only consider the variables in the set J f ixed . According to Lemma 3.6, it is easy to see that D J f ixed = 0 satisfies the optimality condition of the following problem min\nLemma 4.2 provides a shrinking scheme to partition the variables into the free set J f ree and the fixed set J f ixed . With shrinking, each Newton step of the proposed second-order method falls into a block coordinate gradient descent framework [38] . Lemma 4.2 shows that when the variables in the free set J f ree are fixed, no update is needed for the variables in the fixed set J f ixed . Minimization of (28) restricted to the free set can therefore guarantee the convergence to the unique optimal solution [16, 38] . In addition, it has been shown that local quadratic convergence rate can be achieved when the exact Hessian is used (see, for example, [16, 21] ).\nThe resulting second-order method for solving the fused multiple graphical lasso is summarized in Algorithm 1."}, {"section_title": "Experimental results", "text": "In this section, we evaluate the proposed algorithm and screening rule on synthetic datasets and two real datasets: ADHD-200 2 and FDG-PET images 3 . The experiments are performed on a PC with quad-core Intel 2.67GHz CPU and 9GB memory.\nAlgorithm 1: Proposed second-order method for Fused Multiple Graphical Lasso (FMGL)\nDetermine the sets of free and fixed indices J f ree and J f ixed using Lemma 4.2.\nCompute the Newton direction D (k) , k = 1, . . . , K by solving (28) and (29) over the free variables J f ree . Choose \u0398 (k) t+1 by performing the Armijo backtracking line search along \u0398"}, {"section_title": "Simulation", "text": ""}, {"section_title": "Efficiency", "text": "We conduct experiments to demonstrate the effectiveness of the proposed screening rule and the efficiency of our method FMGL. The following algorithms are included in our comparisons:\n\u2022 FMGL: the proposed second-order method in Algorithm 1.\n\u2022 ADMM: ADMM method.\n\u2022 FMGL-S: FMGL with screening.\n\u2022 ADMM-S: ADMM with screening.\nBoth FMGL and ADMM are written in Matlab. Since both methods involve solving (30) which involves a double loop, we implement the sub-routine for solving (30) in C for a fair comparison. The synthetic covariance matrices are generated as follows. We first generate K block diagonal ground truth precision matrices \u0398 (k) with L blocks, and each block\n. . , L, k = 1, . . . , K has random sparsity structures. We control the number of nonzeros in each \u0398 (k) l to be about 10p/L so that the total number of nonzeros in the K precision matrices is 10Kp. Given the precision matrices, we draw 5p samples from each Gaussian distribution to compute the sample covariance matrices. The fused penalty parameter \u03bb 2 is fixed to 0.1, and the \u2113 1 regularization parameter \u03bb 1 is selected so that the total number of nonzeros in the solution is about 10Kp. We terminate the NSPG in the FMGL when the relative error\nFMGL is terminated when the relative error of the objective value is smaller than 1e-5, and ADMM stops until it achieves an objective value equal to or smaller than that of FMGL. The results presented in Table 1 show that FMGL is consistently faster than ADMM. FMGL converges much more quickly than ADMM. Moreover, the screening rule can achieve great computational gain. The speedup with the screening rule is about 10 and 20 times for L = 5 and 10 respectively."}, {"section_title": "Stability", "text": "We conduct experiments to demonstrate the effectiveness of FMGL. The synthetic sparse precision matrices are generated in the following way: we set the first precision matrix \u0398 (1) as 0.25I p\u00d7p , where p = 100. When adding an edge (i, j) in the graph, we add \u03c3 to \u03b8\nii and \u03b8\njj , and subtract \u03c3 from \u03b8 \nij . We randomly assign (2) in the same way. For each precision matrix, we randomly draw n samples from the Gaussian distribution with the corresponding precision matrix, where n varies from 40 to 200 with a step of 20. We perform 500 replications for each n. For each n, \u03bb 2 is fixed to 0.08, and \u03bb 1 is adjusted to make sure that the edge number is about 200. The accuracy n d /n g is used to measure the performance of FMGL and GLasso, where n d is the number of true edges detected by FGML and GLasso, and n g is the number of true edges. The results are shown in Figure 1 . We can see from the figure that FMGL achieves higher accuracies, demonstrating the effectiveness of FMGL for learning multiple graphical models simultaneously. "}, {"section_title": "Real data", "text": ""}, {"section_title": "ADHD-200", "text": "Attention Deficit Hyperactivity Disorder (ADHD) affects at least 5-10% of school-age children with annual costs exceeding 36 billion/year in the United States. The ADHD-200 project has released resting-state functional magnetic resonance images (fMRI) of 491 typically developing children and 285 ADHD children, aiming to encourage the research on ADHD. The data used in this experiment is preprocessed using the NIAK pipeline, and downloaded from neurobureau 4 . More details about the preprocessing strategy can be found in the same website. The dataset we choose includes 116 typically developing children (TDC), 29 ADHD-Combined (ADHD-C), and 49 ADHD-Inattentive (ADHD-I). There are 231 time series and 2834 brain regions for each subject. We want to estimate the graphs of the three groups simultaneously. The sample covariance matrix is computed using all data from the same group. Since the number of brain regions p is 2834, obtaining the precision matrices is computationally intensive. We use this data to test the effectiveness of the proposed screening rule. \u03bb 1 and \u03bb 2 are set to 0.6 and 0.015. The convergence criterion is 1e-5. The comparison of FMGL and ADMM in terms of the objective value curve is shown in Figure 2 . The result shows that FMGL converges much faster than ADMM. The computational times of FMGL and ADMM are 1557.08 and 8306.35 seconds respectively. However, utilizing the screening, the computational times of FMGL-S and ADMM-S are 18.08 and 119.23 seconds respectively, demonstrating the superiority of the screening rule. The obtained solution has 1443 blocks. The largest one including 634 nodes is shown in Figure 3 . The block structures of the FMGL solution are the same as those identified by the screening rule. The screening rule can be used to analyze the rough structures of the graphs. The cost of identifying blocks using the screening rule is negligible compared to that of estimating the graphs. For highdimensional data such as ADHD-200, it is practical to use the screening rule to identify the block structure before estimating the large graphs. We use the screening rule to identify block structures on ADHD-200 data with varying \u03bb 1 and \u03bb 2 . The size distribution is shown in Figure 4 . We can observe that the number of blocks increases, and the size of blocks deceases when the regularization parameter value increases. "}, {"section_title": "FDG-PET", "text": "In this experiment, we use FDG-PET images from 74 Alzhei-mer's disease (AD), 172 mild cognitive impairment (MCI), and 81 normal control (NC) subjects downloaded from the Alzheimer's disease neuroimaging initiative (ADNI) database. The different regions of the whole brain volume can be represented by 116 anatomical volumes of interest (AVOI), defined by Automated Anatomical Labeling (AAL) [39] . Then we extracted data from each of the 116 AVOIs, and derived the average of each AVOI for each subject. The 116 AVOIs can be categorized into 10 groups: prefrontal lobe, other parts of the frontal lobe, parietal lobe, occipital lobe, thalamus, insula, temporal lobe, corpus striatum, cerebellum, and vermis. More details about the categories can be found in [39, 41] . We remove two small groups (thalamus and insula) containing only 4 AVOIs in our experiments.\nTo examine whether FMGL can effectively utilize the information of common structures, we randomly select g percent samples from each group, where g varies from 20 to 100 with a step size of 10. For each g, \u03bb 2 is fixed to 0.1, and \u03bb 1 is adjusted to make sure the number of edges in each group is about the same. We perform 500 replications for each g. The edges with probability larger than 0.85 are considered as stable edges. The results showing the numbers of stable edges are summarized in Figure 5 . We can observe that FMGL is more stable than GLasso. When the sample size is too small (say 20%), there are only 20 stable edges in the graph of NC obtained by GLasso. But the graph of NC obtained by FMGL still has about 140 stable edges, illustrating the superiority of FMGL in stability. The brain connectivity models obtained by FMGL are shown in Figure 6 . We can see that the number of connections within the prefrontal lobe significantly increases, and the number of connections within the temporal lobe significantly decreases from NC to AD, which are supported by previous literatures [1, 14] . The connections between the prefrontal and occipital lobes increase from NC to AD, and connections within cerebellum decrease. We can also find that the adjacent graphs are similar, indicating that FMGL can identify the common structures, but also keep the meaningful differences."}, {"section_title": "Conclusion", "text": "In this paper, we consider simultaneously estimating multiple graphical models by maximizing a fused penalized log likelihood. We have derived a set of necessary and sufficient conditions for the FMGL solution to be block diagonal for an arbitrary number of graphs. A screening rule has been developed to enable the efficient estimation of large multiple graphs. The second-order method is employed to solve the fused multiple graphical lasso. The global convergence of the proposed method is guaranteed, and the convergence rate is local quadratic. A shrinking scheme is proposed to identify the variables to be updated during the Newton iterations, thus reduces the computation. Numerical experiments on synthetic and real data demonstrate the efficiency and effectiveness of the proposed method and the screening rule. We plan to explore the convergence properties of the second-order method using the inexact Newton direction. Due to the shrinking scheme, the proposed second-order method is suitable for warm-start techniques. A good initial solution can further speedup the computation. As part of the future work, we plan to explore how to efficiently find a good initial solution to further improve the efficiency of the proposed method. One possibility is to use divide-and-conquer techniques [15] ."}]