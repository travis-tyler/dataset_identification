[{"section_title": "Abstract", "text": "Abstract\nAnalyzing brain networks from neuroimages is becoming a promising approach in identifying novel connectivitybased biomarkers for the Alzheimer's disease (AD"}, {"section_title": "Introduction", "text": "As the most common form of dementia, Alzheimer's disease (AD) is a fatal and progressive neurodegenerative disease that has caused serious socioeconomic problems in developed countries. Early diagnosis of AD may benefit the patients with disease-interrupted therapies when the dementia is still mild. Neuroimaging techniques are important in AD study because they may provide more sensitive and consistent measures than traditional cognitive assessment.\nCurrently, neuroimage analysis has evolved from studying local morphometry to complex relationships and interactions across brain regions. This is because the brain is, by Figure "}, {"section_title": "Left: ROI partitions on MRI. Right: Some identified directional relationships discriminative for AD. (Please refer to Section 4.3. The figure is best viewed on monitor.)", "text": "nature, a complex network of many interconnected regions. A brain network is usually modeled by a graph with each node corresponding to a brain region and each edge corresponding to the connectivity between regions. The connectivity could be statistical dependencies (functional connectivity) or causal relationships (effective connectivity) [14] , represented by undirected or directed graph, respectively. This paper focuses on brain effective connectivity analysis, an endeavor that has gained research interest due to its ability to analyze the directional effect of one brain region over another. Effective connectivity analysis has been applied to fMRI [5] , PET [1] , and gray matter morphology in structural MRI [6] , and has exhibited promising potential in identifying novel connectivity-based biomarkers for AD.\nWith sparseness techniques, effective connectivity analysis has been able to handle medium to large scale brain networks. A remarkable recent work is from Huang, et al. [1] , where a sparse Gaussian Bayesian network (SGBN) is recovered from more than 40 brain regions in fluorodeoxyglucose PET (FDG-PET) images for AD analysis. That approach learns the Bayesian network (BN) structure and parameters simultaneously in one step, which demonstrates a more accurate network recovery than the conventional twostage approaches in sparse BN learning (such as LIMB-DAG [13] , MMHC [16] , TC and TC-bw [9] , etc.). Despite the effectiveness in network representation, the above methods (including [1] ) are all generative methods. By their nature, generative methods focus on representing an individual group, thus may not be discriminative. When analyzing brain networks, they are prone to over-emphasizing major structures within an individual group, and neglecting the subtle disease-induced structural changes across different groups. Therefore, generative methods are usually inferior in prediction compared with the discriminative methods that focus on the class boundary. However, discriminative methods are not amenable for interpretative analysis that is critical in exploratory research aimed at both understanding and diagnosing the disease. Therefore, we aim to integrate the merits of generative and discriminative methods to learn BNs that are not only representative but also discriminative. Recent progress in [10, 11] for learning discriminative BNs follows the conventional two-stage approach and works for discrete variables. They may not be suitable for brain network analysis where the brain regional measurements are usually continuous variables.\nTo achieve our goal, we improve the model of the SGBN in [1] , and further boost its discriminative power via a kernel learning approach that links the generative SGBN with the SVM classifiers. This paper includes several contributions: 1) We propose an augmented SGBN model (A-SGBN) by revisiting the method in [1] . A-SGBN fits the underlying distribution more precisely, therefore bringing better prediction. 2) By inducing Fisher kernel on our A-SGBN models, we provide a way to obtain subject-specific SGBN-induced feature vectors that can be used by discriminative classifiers such as SVMs. Through this, we integrate the generative and discriminative models. 3) More significantly, we convert the learning of SGBN parameters to the learning of discriminative Fisher kernels, which makes the optimization simple. Specifically, we jointly learn the SGBN parameters and the separating hyperplane of SVMs over Fisher kernel by minimizing a generalization error bound of SVMs. 4) We apply our method on ADNI 1 data to analyze brain effective connectivity for AD from both T1-weighted MRI and FDG-PET images. Our method significantly improves the discriminative power of the generative SGBN and the discriminative SVM classifier simultaneously. 5) By Fisher kernel, we obtain a new kind of features that reflect the changing rate of connection strength, which have not been investigated in conventional approaches."}, {"section_title": "Background and Notation", "text": ""}, {"section_title": "Gaussian Bayesian Network", "text": "Gaussian Bayesian network (GBN) is the fundamental tool that we use to learn brain effective connectivity in this paper. It is therefore briefly described here, together with the definition of symbols used throughout the paper.\nLet \nare called the parameters of a GBN. In this paper, following [1] , a m \u00d7 m matrix G is used to represent network structure, in which, if there is a direct edge from x i to x j , G ij = 1; otherwise, G ij = 0. In addition, another p \u00d7 p matrix P is also kept to record all the directed paths in the structure. If there is a directed path from x i to x j , P ij = 1; otherwise P ij = 0."}, {"section_title": "Sparse Gaussian Bayesian Network", "text": "The state-of-the-art work for brain causal relationship analysis in [1] underpins our study in this paper. In [1] , it is proposed to learn a sparse GBN (SGBN) for brain effective connectivity analysis utilizing FDG-PET images. Compared with the conventional BN methods that learn the network structure and parameters in two steps, SGBN simultaneously learns the structure and parameters by enforcing sparseness constraint on a GBN. This one-step learning approach outperforms the conventional two-step methods with higher accuracies for the network edge recovery. In particular, it is proposed in [1] to solve a constrained least-square fitting problem:\nHere f i and \u03b8 i are defined as above. The i-th row of the matrix Pa(x i ) correspond to the parent nodes of x i , which are initially set as all the nodes other than x i , and further filtered implicitly by the sparseness constraint over their regression coefficients \u03b8 i . In BN learning, a difficult problem is how to enforce the DAG property to ensure the validity of the resulting BN: there should be no directed cycles in the graph. In [1] it is proved that a sufficient and necessary condition for being a DAG is \u0398 ji \u00d7 P ij = 0 for all i and j. The P ij is computed by a Breadth-first search on G with x i being the root node. For more details, please read [1] ."}, {"section_title": "Proposed Method", "text": "In this paper, we study brain networks from two sources. The first source is gray matter morphology from T1-weighted MRI. It has been reported that the covariation of gray matter morphology might be related to the anatomical connectivity [15] . Studying brain morphology as a network can take the advantage of statistical tools from graph theory. The second source is FDG-PET images. The retention of tracer in FDG-PET is analogous to the glucose uptake, thus reflecting the tissue metabolic activity.\nBuilding brain networks includes identifying network nodes and reconstructing the connectivity. This paper focuses on the latter. Hence, after briefing how network nodes are defined in our method in Section 3.1, we concentrate on how to infer the effective connectivity that is both representative (Section 3.2) and discriminative (Section 3.3)."}, {"section_title": "Network Nodes Determination", "text": "MRI. This study involves 120 subjects including 50 MCI (mild cognitive impairment, a prodromal AD) patients and 70 NC (normal controls) from the publicly accessible data of ADNI. The T1-weighted MR images are segmented into gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) using FAST in the FSL 2 package after intensity correction, skull stripping [17] , and cerebellum removal. These tissue-segmented images are spatially normalized into a template space by HAMMER 3 , and partitioned into 100 Region of Interest (ROI) via an ROI atlas [3] . We use the GM volumes of each ROI as network nodes, and select 40 ROIs that have the highest correlation with class labels into our study. PET. This study involves 103 subjects including 51 AD patients and 52 NC whose FDG-PET and MR images are downloaded from ADNI. We first co-register the MR images into a template space and partition them into ROIs as mentioned above. Then the PET images are aligned with their MR images from the same subject by a rigid transformation. The average tracer uptakes within each ROI are used as network nodes. Similarly, we select 40 ROIs that are most discriminative with regards to AD."}, {"section_title": "SGBN Model Augmentation", "text": "A simple way to use generative BNs for prediction is to train each class a BN individually and classify a new sample x i by assigning it to the class with a higher likelihood. The more precisely the BN model reflects the underlying distribution, the more accurate the prediction is. To compare the likelihood for each class in the same space, the data should not be normalized separately for each class as in [1] where a single class is the focus.\nTo handle this, we introduce a bias term x 0 in the regression, i.e., x i = \u03b8 i [Pa(x i ), x 0 ] + \u03b5 i , and demonstrate it not a trivial improvement over the case when directly applying the SGBN in [1] . Accordingly, in the graph G, a bias node is added. It has no parent but is the parent of all the other nodes. If originally G is a DAG, adding x 0 in this way does not cause the violation of DAG. To be distinguished from SGBN in [1] , we call ours A-SGBN. Intuitively, there could be two reasons to include such a bias node into a brain network: i) there possibly exist some latent variables related to the disease, which are not included in the current study, and their influences may be absorbed by the bias node; or ii) the state of a node may depend not only on the interactions with other nodes, but also on the prior of itself. Our experiment in Section 4 demonstrates that A-SGBN is a more precise model than SGBN (smaller fitting errors for both training and test data) for our case, and effectively improves the classification. In addition to the advantages of A-SGBN over SGBN, in the following we show that actively learning the discrimination can further boost the classification performance."}, {"section_title": "Discriminative SBN Learning via Fisher Kernel", "text": "Both SGBN and A-SGBN learn the brain networks for AD or NC separately. This may ignore some subtle but critical network differences that distinguish the two classes. We argue that the parameters of the generative model should be learned from the two classes jointly to keep the essential discrimination. This can be achieved by maximizing the posterior probability p(y|x), where y is the class label of x. Although conceptually direct, this approach often leads to complicated optimization problems. This paper takes another approach. Specifically, we employ Fisher kernel to extract feature vectors from the SGBN models of two classes, and then convert the model parameter learning to Fisher kernel learning with SVMs. We find that the SGBN-induced Fisher vector (see below) is a linear function of parameters \u0398, which well simplifies the optimization."}, {"section_title": "Induction of Fisher vectors from SGBN", "text": "Below we introduce how to use Fisher kernel on SGBNs to obtain feature vectors used for kernel learning.\nFisher kernel provides a way to compare samples induced by a generative model. It maps a sample to a feature vector in the gradient space of the model parameters. The intuition is that similar objects induce similar log-likelihood gradients of the model parameters. Fisher kernel is computed as K(x, x ) = g x U \u22121 g x , where the Fisher vector g x = \u2207 \u03b8 log(p(x|\u03b8)) describes the changing direction of parameters to better fit the model. The Fisher information metric U weights the similarity measure, but is often set as an identity matrix in practice [2] .\nFisher kernel has recently witnessed successful applica-tions in image categorization [12, 4] for inducing feature vectors from Gaussian Mixture Model (GMM) of a visual vocabulary. Despite its success, to the best of our knowledge, Fisher kernel has not been applied to BN for brain connectivity analysis. More importantly, in the applications above, there is no discriminative learning for Fisher kernel as in this paper. The advantage of discriminative Fisher kernel has also been confirmed by a very recent study that uses a different learning criterion within a different context [8] .\nFollowing [1] , we only consider \u0398 as parameters and predefine \u03c3. Let L(x|\u0398) = log(p(x|\u0398)) denote the log-likelihood. Our Fisher vector for each sample x is\n, where \u0398 1 and \u0398 2 are the parameters of the SGBNs for the two classes (y = 1, 2), respectively. Recall that, using a BN, the probability p(x|\u0398) can be factorized as\nTaking partial derivative over \u03b8 i , we have\nwhere S(x i ) is a matrix and s 0 (x i ) is a vector. As shown, \u03a6 \u0398 (x) is a linear function of \u0398. This simple form of \u03a6 \u0398 (x) significantly facilitates our further kernel learning."}, {"section_title": "Discriminative Fisher kernel learning via SVM", "text": "As each Fisher vector is a function of the SGBN parameters, discriminatively learning these parameters can thus be converted to learning discriminative Fisher kernels. We require that the learned SGBN models possess the following properties. Firstly, the Fisher vectors induced by the learned SGBN model should be well separated between classes. Secondly, the learned SGBN models should maintain reasonable capacity of representation. Thirdly, the learned SGBN models should not violate DAG. We use the following strategies to achieve our goal. Firstly, to obtain a discriminative Fisher kernel, we jointly learn the parameters of SGBN and the separating hyperplane of SVMs with Fisher kernel. Radius-margin bound, the upper bound of the Leave-One-Out error, is minimized to keep good generalization of the SVMs. Secondly, to maintain reasonable representation, we explicitly control the fitting errors of the learned model during optimization.\nThirdly, we enforce the DAG constraint in [1] to ensure the validity of the graph. For convenience, we call our method DL-A-SGBN. More details are given below.\nIn order to use radius-margin bound, L 2 -SVM with soft margin has to be employed, which optimizes\nFollowing the convention in SVMs, x i is the i-th sample with class label y i , w the normal of separating plane, b the bias term, \u03be the slack variables and C the regularization parameter. L 2 -SVM can be rewritten as SVM with hard margin by slightly modifying the kernel K := K + I/C, where I is identity matrix. For convenience, in the following, we redefine \u03a6(\nThe vector e i has the value of 1 at the i-th element, and 0 elsewhere.\nIncorporating radius information leads to solving\nwhere R 2 denotes the radius of Minimal Enclosing Ball (MEB). It has been observed that when the sample size is small, the estimation of R 2 may become noisy and unstable. Therefore, it has been proposed to use trace-based scatter matrix instead for such cases [7] . We optimize\nHere tr(S T ) is the trace of the total scatter matrix S T , where\n, and m is the mean of total n samples in the kernel-induced space. It can be shown that tr(S T ) = tr(K) \u2212 1 K1/n, where 1 denotes a vector whose elements are all 1, and K the kernel matrix. Fisher vector \u03a6 \u0398 (x i ) is obtained as mentioned in Section 3.3.1. The function h(\u00b7) measures the squared fitting errors of the corresponding SGBNs for the data D 1 and D 2 from the two classes. It is defined as\nwhere all the symbols are defined as in Eqn. (1). The two user-defined parameters T 1 and T 2 explicitly control the degree of fitting during the learning process (Section 4.2). The DAG constraints here are the same to that used in Eqn. ( 1) . Recall that the DAG constraint is \u0398 ji \u00d7 P ij = 0, where P ij = {0, 1}, reflecting the structure of \u0398. It is observed that enforcing DAG in this way has somewhat enforced the graph sparsity. Therefore, to avoid complicating our optimization we do not impose additional sparseness constraints on \u0398 here. Our A-SGBN could serve as a good initial solution for this problem.\nOne possible approach for solving Eqn. (6) is to alternately optimize the separating hyperplane w and the parameter \u0398. That is,\nwhere\nNote that for a given \u0398, the term tr(S T ) is constant to Eqn. (8) . Due to the strong duality in SVM optimization, we solve the term w 2 2 by\nwhere \u03b1 i is the Lagrangian multiplier. Many quadratic programming packages could be used to solve Eqn. (7) . We use fmincon-SQP (sequential quadratic programming) in Matlab. Our learning process is summarized in Table 1 . \n) and \u03b1 by Eqn. (9) 5. J(\n6. Compute \u2207 \u0398 (t\u22121) J by Eqn. (10) 7. For a given \u03b1 , minimize Eqn. (7) using J(\u0398 (t\u22121) ) and\n9. Repeat Step 2-8 until convergence, let \u0398 = \u0398 (t)"}, {"section_title": "Discussion", "text": ""}, {"section_title": "Gradient of Eqn. (7). Gradient information is required by many optimization algorithms (including fmincon-SQP)", "text": "to speed up the line search. In our case, the gradient of the objective function in Eqn. (7) can be simply calculated as\nwhere \u03b1 maximizes Eqn. (9) . The symbols I and 1 are defined as before. The terms tr(S T ) and J 0 (\u0398) have been computed when evaluating the objective function J(\u0398) in Eqn. ( 7), thus introducing no additional computational cost.\nis just a linear function of \u0398:\nwhere x il denotes the l-th feature of the i-th sample, and S and s 0 are defined in Eqn. (3).\nLearning the whole set of SGBN parameters may encounter the \"curse of dimensionality\" when the training samples are insufficient. For example, we have less than 100 training samples, but 3600 parameters (from two classes) to learn. This may cause overfitting and make the estimation unstable. To handle this issue, we hypothesize that, learning only a selected subset of parameters may mitigate the overfitting and improve the discrimination. For this purpose, \u0398 is partitioned into two parts: \u0398 = {\u0398 sel , \u0398 nosel }. We keep using the whole \u0398 for computing K \u0398 , but optimize Eqn. (7) only over \u0398 sel . There are many options to determine \u0398 sel . We initially compute the Pearson correlation between each component of \u03a6 \u0398 and the class labels on the training data, and select the top \u03b8 i with the highest correlations. To keep our problem simple, only the parameters associated with edges present in the graph are optimized. In this way, the optimization may only eliminate but never add edges in the graph, which avoids the violation of DAG, as well as maintaining the sparsity of the initial A-SGBN. It is remarkable that even this simple selection process has been able to greatly improve the discrimination experimentally. Extension. Although focusing on each node corresponding to a scalar ROI feature, our method is readily extendable to handle multiple features (feature vector) of an ROI. In this case, the conditional distribution for node i becomes\n, where PA and M are both matrices. Our learning remains the same. In our future work, we will apply this extension to analyze fMRI where each ROI is associated with a vector of temporal signal."}, {"section_title": "Experiment", "text": "We evaluate our proposed A-SGBN and DL-A-SGBN against the baseline SGBN (B-SGBN) from [1] (without normalizing the data) in three aspects: i) model fitting, ii) discrimination, and iii) connectivity. Three data sets are used in our experiment: the MRI and FDG-PET data mentioned in Section 3.1, and another MRI-II data that uses the MR images from the same subjects as MRI, but involves 40 different ROIs. Although not as discriminative as that in MRI, the ROIs in MRI-II are more spread across the frontal, parietal, occipital and frontal lobes, thus specially used for a detailed lobe-to-lobe comparison on connectivity. We randomly partition each data set into 30 groups of training-test pairs. Each group includes 80 training and 40 test samples in MRI and MRI-II, or 60 training and 43 test samples in PET."}, {"section_title": "Comparison of Fitting", "text": "Our DL-A-SGBN targets to become discriminative without sacrificing too much power of data representation compared with B-SGBN. Since the change of data fitting from A-SGBN to DL-A-SGBN has been explicitly controlled by the user-defined parameters T 1 and T 2 , we simply compare the model fitting between A-SGBN and B-SGBN. The fitting errors are tested on both training and test data for each class in all three data sets. The root of mean squared fitting errors (RMS) are summarized in Table 2 . In order to test if the fitting errors of A-SGBN are statistically different from that of B-SGBN, a paired t-test (two-tailed) is conducted on the fitting errors over the 30 groups for each data set, respectively. The resulting p-value is also given in the last column in Table 2 . As shown, on all three data sets, our A-SGBN fits the data consistently better than B-SGBN. Such improvement is significant as indicated by the small p-values (except for MCI group in MRI-II). This finding indicates that our A-SGBN might better reflect the underlying distribution of the data, which makes it perform well on both the training and the test data. Another interesting finding is that the generative models explain the NC better than the MCI (in MRI data set) or the AD (in PET data set) patients. This may reflect the common impression that compared with the healthy population, the AD population might be more heterogeneous and therefore more difficult to be represented by a single Gaussian model."}, {"section_title": "Comparison of Discrimination", "text": "Our proposed learning process results in two kinds of models: two DL-A-SGBN models with one for each class, and one SBN-induced SVM classifier that considers only the boundary of the two classes. We test whether our learning can improve the discriminative power on both kinds. The A-SGBN models estimated separately for each class are used as the initial solution. In order to keep reasonable interpretation, we allow maximal 1% additional squared fitting errors (that is, T i = 1.01 \u00d7 T i0 , (i = 1, 2), where T i0 is the squared fitting error of the initial solution) to be introduced during the learning of DL-A-SGBN. We test both the SVM classifier and the DL-A-SGBNs. For the SVM classifier, we use L 2 -SVM with Fisher kernels . For DL-ASGBNs, as mentioned before, we simply compare the values of likelihood, and assign the sample to the class with a higher likelihood. We also conduct a paired t-test (twotailed) to examine the statistical significance of the improvement over the 30 groups for all three data sets. The results are summarized in Table 3 . It can be seen that, as expected, optimizing Eqn. (6) significantly improves the discriminative power of SVM classifiers by 3.08% for MRI, 7.68% for PET, and 4.58% for MRI-II. More importantly, by learning a discriminative SVM classifier, we also simultaneously improve the discriminative power of the generative models DL-A-SGBN by 3.75% for MRI, 4.11% for PET, and 5.67% for MRI-II. Such improvements are statistically significant as indicated by the small p-values. Moreover, when cross-referencing the third columns in Table 3 , it is noticed that our SVM classifiers perform just comparably (for MRI) or even worse (for PET and MRI-II) than our generative DL-A-SGBNs. This may be because our Fisher vectors have very high dimensionality, which causes the serious overfitting of data in SVM classifiers. Such situation might be somewhat improved for DL-A-SGBN since the simple Gaussian model may \"regularize\" the fitting. Based on this assumption, we further select a number of leading features from Fisher vectors by computing the Pearson correlation of the features and the labels, and use the selected features to construct the Fisher kernel for the SVM classifiers. As shown in the last column in Table 4 , the simple feature selection step can further significantly improve the classification performance of the Fisher-kernel based SVM: from 74.5% to 80.08% for MRI, from 65.43% to 77.83% for PET, and from 61.83% to 73.5% for MRI-II.\nIn Table 4 , the improvement from our proposed learning method is scrutinized at each processing step. Compared with the B-SGBN induced from [1] , introducing a bias node (A-SGBN) better fits the population, therefore improves the prediction on test data by 9% for MRI, 6.04% for PET, and 6.67% for MRI-II. The discriminative power of A-SGBN is further improved by 3 \u223c 6% via our discriminative parameter learning. This leads to generative models DL-A-SGBN achieving a classification accuracy above 70%, with no more than 1% increase of the squared fitting error. Moreover, by selecting leading features in the SGBN-induced Fisher vectors, we can construct more discriminative SVM classifiers with additional 6% or more improvement from our DL-A-SGBN to differentiate both MCI vs NC groups in MRI or MRI-II and AD vs NC groups in PET.\nIn sum, compared with the baseline B-SGBN, our proposed method can increase the prediction accuracy by as high as 18% for MRI, 16% for PET, and 20% for MRI-II, using Fisher kernel induced SVM classifiers with feature selection. Meanwhile, these SVM classifiers are linked to the learned generative model DL-A-SGBN whose discriminative powers have also been increased by about 10% from B-SGBN on all three data sets. Our DL-A-SGBN models are not only discriminative, but also descriptive with only a slight increase in the squared fitting errors (at most 1% increase, controlled by the optimization parameter). "}, {"section_title": "Comparison of Connectivity", "text": "In order to gain more insight into the results, we also conduct a lobe-to-lobe comparison on the connectivity derived by our methods and B-SGBN. It is found that, although the 40 ROIs used in MRI and PET are individually discriminative, they do not necessarily cover the representative regions across the whole brain. For example, the 40 nodes used in the MRI data set are mostly located in the temporal lobe and the subcortical region. Therefore, we specially design the MRI-II data set by selecting 40 regions that cover the frontal, parietal, occipital and temporal (including the subcortical region) lobes from MR images of the same subjects involved in MRI data. Although MRI-II (with the best test accuracy of 73.5%) is less discriminative than MRI (with the best test accuracy of 80.08%) as shown in Table 4 , we consistently observe significant improvements of our method over B-SGBN.\nThe structures of the brain networks recovered from NC and MCI groups are displayed in Fig. 2 by using B-SGBN and DL-A-SGBN, respectively. The network structure is obtained by binarizing the edges \u0398 with a threshold of 0.01. Each row i represents the effective connections (dark dots) starting from the node i, and each column j represents the effective connections ending at the node j.\nWith similar parameter settings, the B-SGBN produces 273 edges for NC, and 224 edges for MCI, while our DL-A-SGBN produces 285 edges for NC, and 236 edges for MCI. Note that DL-A-SGBN has an additional bias node corresponding to the last row and column. Because the bias node has no parent node, the last column is all zero. We check the edge difference between the two methods lobe by lobe, and give the result in Table 5 . As shown, the two methods produce similar network structures both visually and quantitatively in most brain regions. There are in total 36 different edges (less than 15%) for NC network, and 11 different edges (around 5%) for MCI network. About half different connections are identified within the temporal lobe (15 for NC, 5 for MCI), for which we also include subcortical structures such as hippocampus and amygdala. It is known that temporal lobe (and some subcortical structures) plays a very important role in the progression of AD. Such a structural difference in this lobe may potentially reflect the different capacity of prediction between our DL-A-SGBN and the B-SGBN. Traditional brain connectivity analysis focuses on the analysis of brain structure which is a binarized connectivity. For example, the network structures from both the B-SGBN and our DL-A-SGBN indicate the loss of effective connections (around 17%) in MCI group in almost all lobes (slightly in the frontal lobe), which agrees well with documented studies [1, 6] . However, binarizing connectivity depends on the selection of threshold. If some connection strength has been weakened by the disease but not reduced below the threshold, this change will be unnecessarily ignored when merely studying the brain structure. This observation is affirmed by our learning process that promotes the discrimination of A-SGBN. Simply optimizing the connection strength across a subset of selected nodes has already significantly improved the prediction with only a minimum (or mostly no) change of brain structure.\nMoreover, using SGBN-induced Fisher kernels, we are able to produce a new kind of features to analyze brain connectivity: the subject specific change of connection strength between nodes. We investigate the selected features of MRI-II used in our SBN-induced SVM classifier and visualize three most discriminative connection changes (Fig. 1  right) happening at \"middle temporal gyrus left\" (in brown) \u2192\"superior parietal lobe left\" (in purple), \"hippocampus right\" (in blue) \u2192\"superior parietal lobe left\", and \"inferior temporal gyrus left\" (in green) \u2192\"middle occipital gyrus right\" (in cyan). Also discriminative are the connections from the bias node to \"middle occipital gyrus right\" and to \"precuneus left\"."}, {"section_title": "Conclusion", "text": "In this paper, we present an approach to model brain effective connectivity encoded with essential discriminative information. With the link of Fisher kernel, our approach is able to simultaneously produce generative SGBN models and its associated SVM classifier, both of which possess sufficient discriminative power for brain network analysis of AD. In addition, by considering the changing rate of connection strength, our method also provides a new perspective for brain connectivity analysis."}]