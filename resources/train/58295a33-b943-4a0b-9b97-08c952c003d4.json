[{"section_title": "Abstract", "text": "Fluorodeoxyglucose (FDG) positron emission tomography (PET) measures the decline in the regional cerebral metabolic rate for glucose, offering a reliable metabolic biomarker even on presymptomatic Alzheimer's disease (AD) patients. PET scans provide functional information that is unique and unavailable using other types of imaging. However, the computational efficacy of FDG-PET data alone, for the classification of various Alzheimers Diagnostic categories, has not been well studied. This motivates us to correctly discriminate various AD Diagnostic categories using FDG-PET data. Deep learning has improved state-of-the-art classification accuracies in the areas of speech, signal, image, video, text mining and recognition. We propose novel methods that involve probabilistic principal component analysis on max-pooled data and mean-pooled data for dimensionality reduction, and multilayer feed forward neural network which performs binary classification. Our experimental dataset consists of baseline data of subjects including 186 cognitively unimpaired (CU) subects, 336 mild cognitive impairment (MCI) subjects with 158 Late MCI and 178 Early MCI, and 146 AD patients from Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. We measured F1-measure, precision, recall, negative and positive predictive values with a 10-fold cross validation scheme. Our results indicate that our designed classifiers achieve competitive results while max pooling achieves better classification performance compared to mean-pooled features. Our deep model based research may advance FDG-PET analysis by demonstrating their potential as an effective imaging biomarker of AD."}, {"section_title": "INTRODUCTION", "text": "In the study of Alzheimers disease (AD), neuroimaging based measures have shown high sensitivity in tracking changes over time and thus were proposed as possible biomarkers to evaluate AD burden and progression and response to interventions. In addition to the pathological amyloid and tau imaging measurements for AD, fluorodeoxyglucose (FDG) positron emission tomography (PET) characterizes the cerebral glucose hypometabolism related to AD and AD risk, offering a reliable metabolic biomarker even at its presymptomatic stage. Fig. 1 visualizes the neural activity in normalized PET scans of AD and normal subjects. We see that the central image in both cases displays loss of functionality for AD patients as compared to normal. There has been growing interest to study FDG-PET for AD and AD risk and particularly to identify and predict mild cognitive impairment (MCI). Although numerous analysis tools have been developed, much of the prior work (e.g. 1 ) , has relied on voxel-wise analysis corrected by multiple comparisons to discover group-wise differences and the general trend in data. However, there are a number of issues in extending the group analysis framework to compute AD risk on individual basis. For example, prior work has showed that the statistically significant pixels obtained in group difference studies do not necessarily carry strong statistical power for predictions. 2 To develop an effective y=-27 y=-20 precision medicine, one needs some system which may be able to measure subtle difference and make robust prediction/classification on an individual basis. Thus far, it is still challenging to build FDG-PET imaging diagnosis and prognosis systems because of the tremendous difficulty to optimally integrate global functional image information.\nFigure 1: Normalized PET image slices for CU and AD subjects.\nIn the study of Alzheimers disease (AD), neuroimaging based measures have shown high sensitivity in tracking changes over time and thus were proposed as possible biomarkers to evaluate AD burden, progression and response to interventions. In addition to the pathological amyloid and tau imaging measurements for AD, fluorodeoxyglucose (FDG) positron emission tomography (PET) characterizes the cerebral glucose hypometabolism related to AD and AD risk, offering a reliable metabolic biomarker even at its presymptomatic stage. Fig. 1 visualizes the neural activity in normalized PET scans of AD and normal subjects. We see that the central image in both cases displays loss of functionality for AD patients as compared to normal. There has been growing interest to study FDG-PET for AD and AD risk and particularly to identify and predict mild cognitive impairment (MCI). Although numerous analysis tools have been developed, much of the prior work (e.g. 1 ) , has relied on voxel-wise analysis corrected by multiple comparisons to discover group-wise differences and the general trend in data. However, there are a number of issues in extending the group analysis framework to compute AD risk on individual basis. For example, prior work has showed that the statistically significant pixels obtained in group difference studies do not necessarily carry strong statistical power for predictions. 2 To develop an effective precision medicine, one needs some system which may be able to measure subtle difference and make robust prediction/classification on an individual basis. Thus far, it is still challenging to build FDG-PET imaging diagnosis and prognosis systems because of the tremendous difficulty to optimally integrate global functional image information.\nRecently deep learning has helped achieve state-of-the-art classification results in myriad classification problems in the areas of signal, speech, text, image processing and medical imaging.\n3 Deep learning based feature representation using auto-encoders was recently used to achieve high accuracies using MRI and PET data. 4 Deep learning has also been used for classification using MRI and PET data.\n5 Classification has been improvised using a combination of multiple imaging modalities to improvise on neuroimaging biomarkers, requiring less labeled data. 6 The advance in deep learning research inspires us to develop novel deep learning methods to advance the FDG-PET analysis research which may facilitate their use in preclinical and clinical AD treatment development.\nIn this work, we propose a novel method that involves dimensionality reduction using Probabilistic Principal Component Analysis on max-pooled data and mean-pooled data, and a Multilayer Feed Forward Neural Network (also known as Multilayer Perceptron(MLP)) which performs binary classification. Fig 2 shows the pipeline of our system. We validated our algorithm in the Alzheimers Disease Neuroimaging Initiative (ADNI) baseline dataset (N= 668) consisting of baseline data of subjects including 186 healthy control (CU), 336 Mild Cognitive Impairment (MCI) with 158 Late MCI and 178 Early MCI, and 146 AD. The FDG-PET images were processed using SPM, 7 for alignment, segmentation and normalization. We measured F1-measure, precision, recall, negative and positive predictive values with a 10-fold cross validation. Our results indicate that our designed classifiers From the left to the right: pre-processed PET data is the initial input data; PET images are normalized using the software toolkit Statistical Parametric Mapping; Max-pooling/Mean-pooling for two classification pipelines performed on each subject's data to reduce the dimension of features from 79\u00d795\u00d779 to a vector of size 4050 \u00d7 1; probabilistic Principal Component Analysis (PCA) is applied to further reduce the number of features from 4050 to hundreds of features; new features with reduced dimensions per image is passed to train a Multilayer Feed-forward Neural Network; the neural network assigns binary class labels achieve competitive results while max pooling results into better classification performance compared to results on mean pooled features.\nOur work has three main contributions. First, we propose a coherent and efficient deep learning framework that well explores the possibility of FDG-PET for AD diagnosis. Secondly, we evaluated our work in a relatively large dataset and achieved competitive results. Thirdly, we exhibit the effective increase in classification performance with the addition of demographic variables (Age, Gender, APOE 1, APOE 2, and FAQ score ) to our max-pooled (intensity) data. We conduct thorough comparison experiments by comparing our work to other state-of-the-art FDG-PET analysis methods. Our work may inspire more deep learning based work on FDG-PET analysis and advance preclinical AD research."}, {"section_title": "DATA AND METHODS", "text": "We work on FDG-PET data from the ADNI-2 dataset. This dataset contains FDG-PET data that has been manually labeled into diagnostic categories by an expert. The baseline data of patients includes 186 healthy control (CU), 336 Mild Cognitive Impairment (MCI) with 158 Late MCI and 178 Early MCI, and 146 AD."}, {"section_title": "Data and Processing", "text": "The size of each FDG-PET image is 79 \u00d7 95 \u00d7 79. Table 1 shows the age distribution for our subjects. We normalize the data to linearly align all the images into a common space using the software toolkit Statistical Parametric Mapping. 7 The normalized FDG-PET images are of size 79 \u00d7 95 \u00d7 79. Each value is a voxel intensity value. We use the intensity values for the whole brain in our experiments. Each voxel is a feature and hence the feature dimensionality is 592895(f dim ) per image data sample. Since the number of data samples is much less than the number of features(n f dim ), we use dimensionality reduction techniques to reduce f dim . This is discussed in the next section. We then use a multilayer perceptron classifier to perform binary classification. A gene called APOE can influence the risk for the more common late-onset type of Alzheimer's. There are three types of the APOE gene, called alleles: APOE2, E3 and E4. The E2 allele is the rarest form of APOE and carrying even one copy appears to reduce the risk of developing Alzheimer's by up to 40%. APOE3 is the most common allele and doesn't seem to influence risk. The APOE4 allele, present in approximately 20% of people, increases the risk for Alzheimer's and lowers the age of onset. The National Institutes of Health recommends genetic testing for APOE status to advance drug research in clinical trials. APOE4 is just one of many risk factors for dementia and its influence can vary across age, gender, race, and nationality 8 ."}, {"section_title": "9", "text": "We use this information to further enhance our classification performance. We also use the age, gender and FAQ (Functional Activities Questionnaire) scores for each subject. FAQ is an informant-based measure of functional abilities. Informants provide performance ratings of the target person on ten complex higher-order activities. We have the FAQ scores, APOE1 and APOE2 values for each of our subjects. Table 2 : Linear SVM, an estimate of linear separability"}, {"section_title": "Extent of Linear Separation", "text": "The term \"classification\" can also be described as finding a clear separation between different classes. This can be thought of as separating two points in a 2D plane by a line. Similarly, if the training examples in an n-dimensional are linearly separable, we can easily separate them by constructing an (n-1)-dimensional plane. We use linear SVM, which is based on the implementation LIBLINEAR 10 to judge the linear separability in the maxpooled data. Based on our judgement, we then configure the multilayer perceptron to increase the linear separability in data representation. Table 2 shows the extent of Linear Separability between each pair of classes. We see that the linear separability is relatively low for EMCI/CU and least for LMCI/EMCI. These classes also have a smaller temporal gap in disease progression."}, {"section_title": "Sparse Representation of Data", "text": "This non-linearity of data can be visualized as a dense representation of the training examples. We need to sparsely represent the data in order to be able to distinguish between different classes. Sparse representations are useful in the following ways (for details please refer to Glorot et al.\n11 ):\n\u2022 Information disentangling: To disentangle the factors explaining the variations in the data. Lets say we have a densely populated region with data points (having dense representations, represented in small dimensions) from various classes. Learning a model on this representation does not guarantee correct classification for points that slightly vary in feature values, from the points the model has trained on. If a representation is both sparse and robust to small changes in feature values, the non-zero features will be conserved throughout training.\n\u2022 Efficient variable size representation: Due to the varying amount of information contained in every input, the number of active input neurons will vary. This may help control dimensionality of the representation for every input.\n\u2022 Linear Separability: Sparse representations induce linear separability of data due to high dimensionality of sparse representations.\nWe will see further that Rectified Linear Units when applied to neurons in the multilayer perceptron are able to help the network learn sparse representations of the data."}, {"section_title": "Feature Selection using Maxpooling", "text": "Feature selection, variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features of all the available descriptors of data. Y-Lan Boureau et. al. 12 show that maxpooling has better performance than other pooling operations. Pooling is widely used to reduce the number of features, to help boost classification performance. Since our dataset consists of 668 data samples with 592895 features each, the number of samples is much less compared to the number of features. Learning based on this representation does not help better classification. We therefore perform max-pooling to reduce the number of features by a large count, for a sample count in hundreds. While the features are pooled, we also make sure to keep consistent overlapping between the patches. So for every two consecutive patches we have three maxpooled values. Overlapping is necessary to store relational information in the features. Max-pooling with overlapping patches of size (10\u00d710\u00d710) reduces our feature dimensionality to 4050 per data sample. We perform maxpooling on the 3-dimensional PET images to make it 2 dimensional. We then run Linear Support Vector Machine (SVM) on our data to test its linear separability. Running linear SVM as shown previously by Asa Ben-Hur et. al. 13 displays the extent of linear separability between the various categories on maxpooled data. Running Linear SVM on a binary classification problem shows how linearly separable classes can be classified using a linear classifier like SVM. The hyper-plane (line in 2-d) is the classifiers decision boundary. A point is classified according to which side of the hyper-plane it falls on, which is determined by the sign of the discriminant function. Running Linear SVM on maxpooled data gives us results (measured as f1-score) as shown in Table 2 . From the table we see that AD and CU are to a large extent linearly separable (with a few incorrect classifications), and LMCI EMCI are not linearly separable, because the f1-score is 0.62. The patch size was varied for AD/CU classification from 5 \u00d7 5 \u00d7 5 to 15 \u00d7 15 \u00d7 15. We compare performance based on patch size for AD and CU subject data. For this experiment we used max-pooled data with age, gender, apoe1, apoe2 and FAQ score for all data samples. In max-pooling 3-dimensional patches of size p size \u00d7 p size \u00d7 p size are extracted uniformly from the 3-dimensional intensity value data. We vary the patch sizes in the range p size = 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15. For the experimentation without demographic features and just max-pooled values, the number of hidden layers is given by n hidden = 4 and the number of neurons in each hidden layer are n neurons = [1000, 500, 100, 10]. For the experimentation with demographic features in addition to max-pooled values as the input, the number of hidden layers is given by n hidden = 7 and the number of neurons in each hidden layer are n neurons = [1000, 800, 600, 400, 200, 100, 10].We compare both max-pooled data with demographics and performance on just max-pooled data. Table 3 We then select two patch sizes (p size = 9, p size = 10) to experiment with, on Max-pooled data along with 4 demographic features. The performance of both patch sizes is comparable and almost similar. We proceed with a patch size of 10 \u00d7 10 \u00d7 10 for all the following experiments."}, {"section_title": "Dimensionality Reduction using Probabilistic Principal Component Analysis", "text": "In machine learning, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration, via obtaining a set of principal variables. Dimensionality reduction is the introduction of new feature space where the original features are represented. The new space is of lower dimension that the original space. Weber N hwrp4 \u00f3anp 0rerts reduced dimensionality. PCA maximizes the variance of projected data(x), which is represented in a lower dimensional space using a set of orthonormal vectors W . PPCA is the following latent variable model:\n, where x \u2208 R p is one observation and z \u2208 R q is a latent variable vector, usually q p. The error covariance structure in PPCA is \u03c3 2 I. The Maximum Likelihood solution for PPCA is obtained as: Fig. 3(a) shows that PPCA, even with 3 components separates AD from Normal to a great extent, whereas in Fig. 3(b) we see that 3 of the 4050 maxpooled features are closely represented with possible overlaps for AD and Normal. The cumulative variance displayed in Fig. 3(c) is low (\u223c 35%), whereas for 250 components PPCA has a high cumulative variance (\u223c 97%) shown in Fig. 3(d) . We further need to reduce our feature dimensionality from 4050 to a count in hundreds, as training a neural net with features close to the number of samples will give us a model that can perform better classifications. Hence we use PPCA to reduce our 4050 max-pooled/mean-pooled features to features in the range 250 to 300. This range of feature dimensionality count gives the best variance from PPCA, as shown in Fig. 3(d) . "}, {"section_title": "Multilayer Perceptron", "text": "A perceptron produces a single binary output given several binary inputs x 1 ,x 2 , and so on. Fig 4 below shows a schematic of Rosenblatt's Perceptron with m inputs x 1 , x 2 , x 3 ... x m . Figure 4 : Schematic of Rosenblatt's perceptron Rosenblatt * proposed a simple way to compute the output. He assigned weights w 1 , w 2 , ..., to inputs signifying the importance of inputs in the determination of the output. In the modern sense, the perceptron is an algorithm for learning a binary classifier: a function that maps its input x (a real valued vector) to an output value f (x) (a single binary value):\nwhere w is a vector of real-valued weights, w \u00b7 x is the dot product sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training the network.MLP networks are typically used in supervised learning problems. This means that there is a training set of input-output pairs and the network must learn to model the dependency between them. The supervised learning problem of the MLP can be solved with the back-propagation algorithm. The algorithm consists of two main steps, forward propagation and back propagation. In the forward pass, the * https://en.wikipedia.org/wiki/Frank Rosenblatt"}, {"section_title": "Output Hidden layer 2", "text": "Hidden layer 1 Input predicted outputs corresponding to the given inputs are evaluated, by applying a set of weights to the input data. For the first forward propagation, the set of weights is selected randomly. In the backward pass, partial derivatives of the cost function with respect to the different parameters are propagated back through the network. Back propagation measures the margin of error of the output and adjusts the weights accordingly to decrease the error. A one hidden layer MLP can be represented using the function: f : R D \u2212 > R L where D is the size of input vector x and L is the size of the output vector f (x). Where,f (x) = G(b\n; weight matrices W (1) , W (2) and activation functions G and s."}, {"section_title": "Activation Function", "text": "We use the activation function Rectified Linear Unit for the activation units in the MLP. The rectifier activation function allows a network to easily obtain sparse representations, hence inducing the sparsity effect on networks. Figure 6 shows how the 1 st hidden layer has certain deactivated neurons because the output from their respective ReLu activation functions is zeroed out. Which means they do not contribute as inputs to the second hidden layer, and only 2 of the 6 neurons in the 1 st hidden layer contribute as inputs to the 2 nd hidden layer. Similarly sparsity is induced with ReLu in the 2 nd hidden layer. "}, {"section_title": "15", "text": "Experimental results show engaging training behavior of this activation function, especially for deep architectures, 15 i.e., where the number of hidden layers in the neural network is 3 or more. Which means that training proceeds better when operating neurons in a network are either off or operating mostly in a linear regime. MLP using a backpropagation algorithm is the standard algorithm for any supervised learning pattern recognition process and the subject of ongoing research in computational neuroscience and parallel distributed processing. They are useful in research in terms of their ability to solve problems stochastically, which often allows one to get approximate solutions for extremely complex problems like classification. After reducing the feature dimensionality for each sample, we pass these as inputs to our MLP. We tried various configurations to obtain the best performing models. We vary the number of hidden layers and the number of neurons in each layer for every classification experiment. One hidden layer MLPs are known to have good classification accuracies with x number of neurons where x = inp+out 2\n, where inp is the number of inputs and out is the number of outputs. The activation function for each neuron is Linear Rectification, which given an input y, returns f (y) = max(0, y). The learning rate for MLP is set to 0.001, and the loss minimization (Gradient Descent Optimization) is performed using the Adam(Adaptive Moment Estimation) Optimizer. "}, {"section_title": "Backpropagation", "text": "Backpropagation is a method of training artificial Neural Networks used in conjunction with an optimization method(such as gradient descent). Backpropagation calculates a gradient of a loss function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the loss function. Backpropagation requires a known, desired output for each input value in order to calculate the loss function gradient. It is therefore usually considered to be a supervised learning method, although it is also used in some unsupervised networks such as autoencoders. Backpropagation is used for computing the error \u03b4 l and the gradient of the cost function. The Backpropagation algorithm is as follows:\n1. Input x: Set a 1 (activation) for the input layer 2. Feedforward:\n5. Output: Gradient of the cost function is given by:\nThe loss minimization problem can be given by:\nwhere, {x i , y i } Adam adaptively selects a separate learning rate for each parameter. Parameters that would ordinarily receive smaller or less frequent updates receive larger updates with Adam (the reverse is also true). This speeds learning in cases where the appropriate learning rates vary across parameters."}, {"section_title": "Finding an Optimal Configuration", "text": "We further experiment to find an optimal deep neural network configuration for the MLP architecture. Our experimentation is as shown in the algorithm , which is purely trying to estimate in one fixed direction. According to this algorithm, we estimate a one hidden layer MLP that gives the maximum f 1 score when neurons are varied from 5 to 1000. We then fix the number of neurons in the first layer to the one that gives us the best f 1 score, and vary the number of neurons in the second layer from 5 to 1000, and estimate the number of neurons that give us the best f 1 score. We keep doing this for iterations counting the number of hidden layers given as input. Since this is purely based on an assumption that fixing the previous neural network configurations and varying the new hidden layer configuration will lead to better results, this may not be true. Also we only vary the number of neurons from 5 to 1000 at intervals of 5, increasing the upper and lower bound for this experiment may lead to different results. To investigate this approach, we use it for the classification of 7(excluding CU MCI) of the binary classification experiments. The results are as shown in Appendix A A This helps us reach better accuracies than a random brute force approach. A better approach would be to try all permutations and combinations for each hidden layer, but this approach is computationally expensive.\nAlgorithm Pseudo Code for Estimating an Optimal Configuration for n hidden layers\nfor numNeurons = 5;numNeurons+= 5;numNeurons\u2264 1000 do for k f old in 10 fold Cross Validation do model\u2190MLPClassifier(hiddenLayerSizes=arr sizes .append(numNeurons),activation ='ReLu', solver='Adam',maxIterations=1000) {Now train on k-1 folds and test on k th fold} model.fit([1 f old , 2 f old , ...k \u2212 1 f old ], trainLabels) {Add all confusion matrices for k-folds} confusionMatrix\u2190confusionMatrix+confMat(testLabels,model.predict(k th fold)) end for {Store the number of neurons for which we have the maximum Figure 7 shows the ROC and AUCs for each of our experiments with demographic data and without demographic data. In this section, we perform experiments to validate our proposed method on the ADNI2 dataset for evaluating its performance. We use FDG-PET baseline scans from the ADNI2 dataset. We use the software toolkit Statistical Parametric Mapping (SPM) 7 to linearly align all the images into a common space. We measure the classificaton accuracy using f1-measure. To evaluate the performance of our method, we perform 10-fold cross validation. Our experiments show that the proposed system is promising for AD diagnosis research. Whether or not this approach provides more statistical power than those afforded by other classification work requires careful validation for each application. We anticipate that this work will inspire more work that builds deep learning based systems for FDG-PET data analysis. We compare our results with the best achieved results as of yet using FDG-PET 17 images only, our results show an increase by 4.76% in the accuracy for AD Normal. "}, {"section_title": "RESULTS", "text": ""}, {"section_title": "Comparison with other Classification Algorithms", "text": "We compare our MLP results with other machine learning algorithms. This comparison has been done on maxpooled data combined with age, gender, APOE 1, APOE 2, and FAQ score information using 10 fold cross validation for each of the methods.\nComparison with Other Dimensionality Reduction Algorithms Table 6 shows comparison of PPCA as a dimensionality reduction technique to other prominently used techniques. This experimentation was done on max-pooled data along with demographic features(age, gender, apoe and FAQ scores), and 10 fold cross validation to evaluate performance. We see that PPCA outperforms the other techniques. These accuracies are achieved by trying random combinations of neural networks."}, {"section_title": "Effect of Demographic Features", "text": "We also include age and gender of the subjects as additional features to our maxpooled data. Our data matrix, input to the neural network is in this case of size n \u00d7 4052 where n is the number of samples (training + testing). n is 332 in the case of CU AD binary classification experiment (186 CU samples, 146 AD samples). We see in Table 7 the improvisations with the addition of age and gender(assigned values 0 for female and 1 for male) to maxpooled data. Our results clearly display a difference with the addition of just two demographic features, which means an addition of other features from ADNI subjects (such as MMSE score) will further help improvise prediction results. We also append AD/CU beta positive/negative values as features in addition to the demographic features along with maxpooled PET intensity values. The results are as represented in Table 8 . We follow the algorithm described in methods, and our best results for the classification of FDG-PET data with and without demographics is shown in Table 9 . These results are obtained for varying configurations of MLP, each of which has been discussed in this section and the previous sections. We further add beta positive/negative for AD and CU comparison experiment along with demographic features. The experiment is performed on max-pooled data from which 300 probabilistic principal components are extracted. We see in Methods how the number of principal components is selected for each experiment."}, {"section_title": "Comparison of Max-pooled data with Mean-pooled data", "text": "We performed Binary Classification experiments on 2 types of datasets (one with max-pooling and the other mean-pooling). We perform max-pooling on our data and mean-pooling on our data. We then compare the two datasets based on classification performance achieved by using a Multilayer Perceptron(MLP) classifier. Table  10 shows that max-pooling helps achieve better performance in majority of the binary classification experiments."}, {"section_title": "CONCLUSION AND FUTURE WORK", "text": "We present a deep learning framework for AD clinical group classification using FDG-PET imaging data. Our findings support that deep model may be useful to improve clinical diagnosis and prognosis of AD. In future, we will refine our research and apply our framework to further study cognitive score and treatment effect prediction problems."}, {"section_title": "APPENDIX A. CONFIGURING A 5-HIDDEN-LAYER MLP", "text": "We studied the relationship between the depths of the deep models and performance in a variety of experiments. The results are summarized in Table 11 , 12, 13, 14, 15, 16, 17. "}]