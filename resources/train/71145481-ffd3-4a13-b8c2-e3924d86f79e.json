[{"section_title": "Abstract", "text": "Mixtures of experts probabilistically divide the input space into regions, where the assumptions of each expert, or conditional model, need only hold locally. Combined with Gaussian process (GP) experts, this results in a powerful and highly flexible model. We focus on alternative mixtures of GP experts, which model the joint distribution of the inputs and targets explicitly. We highlight issues of this approach in multi-dimensional input spaces, namely, poor scalability and the need for an unnecessarily large number of experts, degrading the predictive performance and increasing uncertainty. We construct a novel model to address these issues through a nested partitioning scheme that automatically infers the number of components at both levels. Multiple response types are accommodated through a generalised GP framework, while multiple input types are included through a factorised exponential family structure. We show the effectiveness of our approach in estimating a parsimonious probabilistic description of both synthetic data of increasing dimension and an Alzheimer's challenge dataset."}, {"section_title": "Introduction", "text": "The Gaussian process [22] is a powerful and popular prior for nonparametric regression, due to its flexibility, analytic tractability, and interpretable hyperparameters. The GP assumes that the unknown function evaluated at any finite set of inputs has a Gaussian distribution with consistent parameters. It is fully specified by a mean function and symmetric positive definite covariance (or kernel) function, which together encapsulate any prior knowledge and/or assumptions of the regression function, such as smoothness and periodicity. While GP regression has been successfully applied to various problems, it only allows for flexibility in the regression function, assuming homoskedastic Gaussian errors. Many datasets further require flexibility in the errors, such as multi-modality or different variances across the input space. Moreover, for computational purposes, a stationary assumption of the GP is typically employed, which is inappropriate in many examples, by limiting the ability to recover changing behaviour of the function across the input space, e.g. different smoothness levels.\nDensity regression refers to the general problem of estimating the conditional density of the targets across the input space, or equivalently, flexible estimation of both the regression function and inputdependent error distribution. Mixtures of experts [11] address the density regression problem by probabilistically partitioning the input space. Each expert is a conditional model, and a gating network maps experts to local regions of the input space. Scalability is enhanced since each expert considers only its local region, and simplifying assumptions of the regression function need only hold locally in each region. Experts may range from simple linear models to flexible non-linear approaches. Tresp [26] combines mixtures of experts with GPs, resulting in a flexible nonparametric approach for both the experts and gating networks. GP experts allow the model to infer different behaviours, such as smoothness and variability, in local regions of the input space. In this work, we focus on alternative mixtures of GP experts [16] , which explicitly model the joint distribution of the inputs and targets. We highlight issues of this approach for multi-dimensional inputs, namely, poor scalability and the need for an unnecessary number of experts, and we construct a novel model to address these issues.\nThe paper is organised as follows. Related work is reviewed in Section 2. In Section 3, we construct a novel mixture of generalised GP experts, that extends Meeds and Osindero [16] for multiple response and input types and additionally utilises a nested partitioning scheme to improve prediction and uncertainty quantification. Posterior inference is described in Section 4. Section 5 illustrates the benefits in a non-linear toy example and a case study to predict cognitive decline in Alzheimer's."}, {"section_title": "Related work", "text": "Infinite mixtures of GP experts [21] allow the number of experts to be determined by the data and grow unboundedly as more data points are observed. The alternative infinite mixture of GP experts [16] models the joint distribution of the inputs and targets explicitly, and advantages include the ability to handle missing data and answer inverse problems, as well as simplified computations, relying on established algorithms for infinite mixtures of exchangeable data [e.g. 17]. Meeds and Osindero [16] assume a local multivariate Gaussian distribution for the inputs, and in multi-dimensions, complexity in the marginal distribution of the inputs may lead to the creation of an unnecessary number of experts, degrading the predictive performance and increasing uncertainty, due to small sample sizes for each expert. Yuan and Neubauer [31] remove this constraint by using a Gaussian mixture for the local input density; however, a finite approximation to the infinite mixture is used at both levels. Moreover, the local multivariate Gaussian input model scales poorly with the input dimension D due to the computational cost of dealing with the full D by D matrix.\nThe treed-GP [TGP, 8] is another example of a mixture of GP experts, where the input space is partitioned into axis-aligned rectangular regions. However, this axis-aligned approach also scales poorly in multi-dimensional input spaces, again leading to an unnecessarily large number of experts. More flexible partitioning approaches exist, such as Voronoi tessellations [20] ; however, inference is more computationally expensive, especially as the input dimension increases.\nInfinite mixtures of generalised linear experts [9] provide a unifying framework to model multiple response types. In this linear setting, the problems associated with an overly large number of experts is highlighted in Wade et al. [29] , where a loss of predictive accuracy and increased uncertainty is demonstrated, particularly as D increases. Due to the greater flexibility of GPs over linear experts, these problems are exacerbated for mixtures of GP experts. In the following section, we construct a novel model to overcome these issues."}, {"section_title": "Enriched mixtures of generalised GP experts", "text": "A mixture model for the joint density of the output y \u2208 Y and D-dimensional input x \u2208 X assumes f Q (y, x) = p(y|x, \u03b8)p(x|\u03c8)dQ(\u03b8, \u03c8).\n(\nThe three key elements are 1) the local expert p(y|x, \u03b8), a family of densities on Y for \u03b8 \u2208 \u0398; 2) the local input model p(x|\u03c8), a family of densities on X for \u03c8 \u2208 \u03a8; and 3) the mixing measure Q, a probability measure on \u0398 \u00d7 \u03a8. In the following, we define these three key elements for our model."}, {"section_title": "Local experts", "text": "We provide a framework for multiple output types by defining the local expert p(y|x, \u03b8) to be an extension of the generalised linear model (GLM) used in Hannah et al. [9] . Specifically, p(y|x, \u03b8) belongs to the exponential family, which in canonical form assumes p(y|x, \u03b8) = exp y\u03b7 \u2212 b(\u03b7) a(\u03c6) + c(y, \u03c6) .\nThe functions a, b, and c are known and specific to the exponential family; \u03c6 is the scale parameter; and \u03b7 is the canonical parameter with b (\u03b7) = \u00b5(x) = E[y|x] and g(\u00b5(x)) = m(x), where g is a chosen link function that maps \u00b5(x) to the real line. In GLMs [15] , a linear function of x determines the canonical parameter through a set of transformations, i.e. m(x) = \u03b1 + x\u03b2.\nInstead, we consider a general non-linear function and assign a GP prior to the unknown function:\nwith constant mean function, E[m(x)] = \u03b2 0 , and kernel function K \u03bb with hyperparameters \u03bb, defining the covariance of the function at any two inputs, Cov[m(x), m(x * )] = K \u03bb (x, x * ). The parameters of this generalized Gaussian process [GGP, 3] are \u03b8 = (m(\u00b7), \u03b2 0 , \u03bb, \u03c6). In many examples, it is common to use a zero-centred GP, which is made appropriate by subtracting the overall mean from the response. However, in our case, we must include a constant mean, as the partitioning structure is unknown and the data cannot be centred for each expert. Additionally, by including \u03bb in the set of mixing parameters \u03b8, we can recover non-stationary behaviour, e.g. different lengthscales in local regions of the input space. The GGP experts used in Section 5 are 1) Gaussian with identity link, p(y|x, \u03b8) = N(y|m(x), \u03c3 2 ); and 2) Ordinal with probit link for ordered categories l = 0, . . . , L,\nand cutoffs 0 = \u03b5 0 < \u03b5 1 < . . . < \u03b5 L\u22121 , which may be fixed due to the nonparametric nature of the model [14] . The ordinal model can be equivalently formulated through a latent Gaussian response:\nwith the ordered probit recovered after marginalisation of the latent\u1ef9. A list of GGP experts is provided in the Supplementary Material (SM), for studies with other output types."}, {"section_title": "Local input models", "text": "We assume a factorised exponential family structure for the local input model. Specifically, it factorises across d = 1, . . . , D, where each p(x d |\u03c8 d ) belongs to the exponential family, that is,\n, and t d , a d , and b d are known functions specified by the choice within the exponential family. The standard conjugate prior for \u03c8 assumes independence of\n, and parameters \u03c4 d and \u03bd d determining the location and scale of the prior, respectively. In this conjugate setting, \u03c8 can be marginalised, and the local marginal and predictive likelihood of the inputs are available analytically (specific calculations are provided in the SM). Examples used in Section 5 are the 1) Gaussian for x d \u2208 R, with local input model N(\nis a probability vector; and 3) Binomial for\nAdvantages of this factorised exponential form include improved scalability, inclusion of multiple input types, and richer parametrisation. Indeed, the mixtures of GP experts in [16, 31, 18] consider only continuous inputs with a local multivariate Gaussian density and conjugate inverse Wishart prior on the covariance matrix. However, even for moderately large D, this approach becomes unfeasible. Specifically, the computational cost of dealing with the full covariance matrix is O(D 3 ), which is reduced to O(D) in our factorised form. Furthermore, Consonni and Veronese [4] highlight the poor parametrisation of the Wishart prior; in particular, there is a single parameter to control variability. In our model, flexibility of the conjugate prior is enhanced, as it includes a scale parameter \u03bd d for each of the D variances. We emphasize that although the inputs are locally independent, globally, they may be dependent. For example, a highly-correlated, elliptically-shaped Gaussian can be accurately approximated with a mixture of several smaller spherical Gaussians. Here, Y j and\u1ef8 j denote the observed and latent outputs in cluster j, with X j denoting the inputs in cluster j for the DP and X l|j denoting the inputs in x-cluster l nested in y-cluster j for the EDP.\nOther types of inputs can be easily handled through the assumptions of local independence\nand that each parametric model p(x d |\u03c8 d ) belongs to the exponential family, that is, \nIn this conjugate setting, the parameters \u03c8 can be marginalised in each cluster analytically. Specifically, for the collapsed Gibbs sampler, we need 1) the marginal likelihood h(x n ) and 2) the predictive likelihood h(x n |X \u2212n l|j ), where X \u2212n l|j contains x n such that n = n, z n = (j, l). Additionally, for the spilt and merge moves we require the the joint marginal likelihood of h(X l|j ). We note that due to the assumption of local independence\nExamples (used in this paper) include:\nGaussian: for continuous input x n,d taking values in R with\nThe standard conjugate prior is the normal-inverse gamma distribution, 0,d , c d , a x,d , b x,d ). In this case, marginally x n,d has a non-central t-distribution,\n. The joint marginal likelihood of X l|j,d follows a multivariate t with mean u 0,d 1 N l|j , variance matrix\n, and degrees of freedom 2a\nCategorical: for discrete inputs x n,d taking unordered values g = 0, 1, . . . , G d with\nThe standard conjugate prior is the Dirichlet distribution with parameter\nIn this case, the marginal likelihood is the Dirichlet-multinomial with\nThe predictive likelihood of x n,d given z n = (j, l) is the Dirichlet-multinomial with\n, where\nThe joint marginal likelihood of X l|j,d follows a Dirichlet-multinomial with\n,\nBinomial: for discrete inputs x n,d taking ordered values g = 0, 1, . . . , G d with\nwhere \u03c8 d \u2208 (0, 1). The standard conjugate prior is the beta distribution with parameter\n). In this case, the marginal likelihood is the beta-binomial with\nThe predictive likelihood of x n,d given z n = (j, l) is the beta-binomial with\nThe joint marginal likelihood of X l|j,d follows a Beta-binomial with"}, {"section_title": "Mixing measure", "text": "The Bayesian model is completed with a prior on the mixing measure Q, and the Dirichlet process [DP, 7] is a popular nonparametric choice. Indeed, it is utilised in [21, 16, 9] , among many others. Instead, we propose to use the enriched Dirichlet process [EDP, 28] and highlight its advantages for improved prediction, better uncertainty quantification, and more interpretable clustering.\nDirichlet process. The parameters of the DP consist of the concentration parameter \u03b1 > 0 and the base measure Q 0 , a probability measure on \u0398 \u00d7 \u03a8. The DP is discrete with probability one, and realisations place positive mass on a countably infinite number of atoms. When utilised as a prior for the mixing measure Q \u223c DP(\u03b1, Q 0 ), this implies a countably infinite mixture for the joint density in (1) . For N data points (y n , x n ), n = 1, . . . , N , this induces a random partition of the data points into clusters. Introducing the latent variable z n denoting the cluster allocation of data point n, in order of appearance, and the parameters (\u03b8 j , \u03c8 j ) denoting the parameters of the j th observed cluster, the mixing measure Q can be marginalised. In this case, the model can be expressed as\nwhere (\u03b8 j , \u03c8 j ) iid \u223c Q 0 . The law of allocation variables is defined by the predictive distributions [2] :\nwhere k is the number of clusters and N j is the number of data points allocated to cluster j. In this setting, the number of clusters is determined by and can grow with the data.\nEnriched Dirichlet process. The EDP defines a prior for the joint measure Q on \u0398 \u00d7 \u03a8 by decomposing it in terms of the marginal Q \u03b8 and conditionals Q \u03c8|\u03b8 (\u00b7|\u03b8). The parameters consist of the base measure Q 0 on \u0398\u00d7\u03a8; a concentration parameter \u03b1 \u03b8 ; and a collection of concentration parameters \u03b1 \u03c8 (\u03b8) for \u03b8 \u2208 \u0398. The EDP assumes 1)\nfor all \u03b8 \u2208 \u0398; and 3) independence of Q \u03c8|\u03b8 (\u00b7|\u03b8) across \u03b8 \u2208 \u0398 and from Q \u03b8 . When utilised as a prior for the mixing measure Q \u223c EDP(\u03b1 \u03b8 , \u03b1 \u03c8 (\u03b8), Q 0 ), this induces a random nested partition of data points in y-clusters and x-subclusters within each y-cluster. The latent cluster allocation of each data point consists of two terms z n = (z y,n , z x,n ), where z y,n = j if the nth data point belongs to jth y-cluster with parameter \u03b8 j and z x,n = l if the nth data point belongs to lth x-cluster with parameter \u03c8 l|j within the jth y-cluster. After marginalising Q, the model can be expressed as\nInitialize: (z \nwhere k denotes the number of y-clusters of sizes N j and k j denotes the number x-clusters within the j th y-cluster of sizes N l|j . Further, hyperpriors on the concentration parameters assume \u03b1 \u03b8 \u223c Gam(u \u03b8 , v \u03b8 ) and \u03b1 \u03c8,j = \u03b1 \u03c8 (\u03b8 j ) are independent with \u03b1 \u03c8,j \u223c Gam(u \u03c8 , v \u03c8 ).\nA graphical comparison of the MoE with the DP and EDP priors is provided in Figure 1 . The DP mixture of GGP experts allocates data points to similar groups to obtain a good approximation to the joint density, with similarity measured by the local expert and input model. The local factorised exponential family for the inputs is crucial for scaling to multi-dimensions and inclusion of multiple input types. However, this results in a rigid similarity measure between inputs, and as D increases x tends to dominate the partitioning structure, typically requiring many small clusters to capture increasing departures from the local input model. This occurs despite the flexible nature of GPs, often requiring only a few GP experts to approximate the conditional of y given x, and results in degradation of regression and conditional density estimates, wide credible intervals, and uninterpretable clustering due to the small sample sizes for each expert. By replacing the DP with the EDP, the nested partitioning scheme allows the data to determine if the conditional of y given x can be recovered with fewer experts. The y-clustering is determined by similarity measured through the local expert and a more flexible local input model, which can itself be a mixture. Moreover, a simple analytically computable allocation rule is maintained, allowing the construction of efficient inference algorithms."}, {"section_title": "Posterior inference", "text": "For inference, we resort to Markov chain Monte Carlo (MCMC) and derive a collapsed Gibbs algorithm to sample the latent allocation variables z 1:N and unique y-cluster parameters (\u03b8 j ), with the x-cluster parameters (\u03c8 l|j ) marginalised. Additionally, we focus on the case when the functions m j (\u00b7) can be marginalised; this includes the Gaussian likelihood, but also the ordered probit, among others, through data augmentation. In the latter, the data is augmented with latent Gaussian outputs y 1:N , which have a deterministic relationship with the observed outputs. Algorithm 1 gives an overview of the MCMC scheme (with a full description in the SM). To improve mixing, two novel split-merge updates are developed for global changes to the allocation variables. Predictions. From the M MCMC samples, we compute predictions for the new output y * given x * . In the Gaussian case, the posterior expectation of y * is approximated by\nwhere C is the normalising constant; and for a new cluster, the predictive mean is simply the prior expectation of \u03b2 0,k (m) +1 , denoted by \u00b5 \u03b2 , while for an existing cluster, the GP predictive mean in cluster j is denoted by m (m) j (x * ). Thus, for each sample, the expectation is a weighted average of the GP predictions from each cluster and a new cluster, with input-dependent weights that flexibly measure the similarity between the new input and the inputs of each cluster through a mixture. Specifically, the weights of a new cluster and existing cluster j, for j = 1, . . . , k (m) , are, respectively,\nHere, h(x * ) is the marginal density of x * and h(x * |X l|j ) is the predictive marginal density of x * given X l|j , which contains the x n such that z n = (j, l). In contrast, for the DP, the weight of an existing cluster in (2) is more rigidly defined based on a single predictive marginal density, arising from the factorised exponential family. The predictive density or appropriate quantities for other output types are similarly computed. Another advantage of the joint approach includes the calculation of predictions based only on a subset of inputs. Full derivations are provided in the SM.\nClustering. The enriched MoE induces a nested clustering of the data points into y-clusters and nested x-clusters. This latent clustering may be of interest to identify similar groups of data points and to improve understanding of the model. The MCMC samples from the posterior over this nested clustering; to summarise the samples, we obtain the point estimate from minimising the posterior expected variation of information [VI, 27] , first estimating the y-level partition and then the nested x-level partition. To visualise uncertainty in the clustering structure, we also compute the posterior similarity matrix with elements p(z y,n = z y,n |y 1:N , x 1:N ) representing the posterior probability that two points are clustered together and approximated by the fraction of times this occurred in the chain.\nWe present the algorithm for a general setting, when the observed outputs y n are a deterministic function of latent Gaussian outputs\u1ef9 n . This includes the probit, ordered probit and multinomial probit, as well as the Gaussian example with y =\u1ef9, among others. The MCMC algorithm targets the posterior\nwhere we make use of the notation\u1ef8 j to denote the latent outputs\u1ef9 n such that z y,n = j and X l|j to denote the inputs x n such that z n = (j, l). The marginal likelihood of\u1ef8 j given \u03b2 0,j , \u03bb j and \u03c3 2 j , obtained from marginalising the unknown functions m j , is Gaussian, e.g. for the ordered probit,\nwhere K \u03bbj denotes the N j by N j matrix of the kernel function evaluated at every pair of inputs in y-cluster j. The marginal likelihood of X l|j , obtained from marginalising \u03c8 l|j , is also available in closed form and factorises over D, with examples in Section 2. The term p(y n |\u1ef9 n ) represents the deterministic function specifying the observed output y n given the latent Gaussian output\u1ef9 n ; examples are provided in Section 1.\nThe algorithm is a Gibbs sampler, which alternatively samples each set of parameters, 1) the allocation variables z 1:N , 2) the unique cluster parameters (\u03c3 Allocation variables. A non-conjugate collapsed Gibbs sampler is employed, combining Algorithm 3, when cluster parameters can be integrated, and Algorithm 8, when cluster parameters cannot be integrated, of Neal [7] , and extending this for the nested partitioning scheme. This consists of N Gibbs steps, where the allocation variable z n for each data point is updated conditioned on all others z 1 , . . . , z n\u22121 , z n+1 , . . . , z N . This procedure allows local changes to the allocation variables, and to improve mixing in high-dimensional input spaces, we additionally develop two novel split-merge updates for global changes to the nested partition. Throughout, we make use of the superscript notation \u2212n to denote the data points, parameters, and latent variables with the n th data point removed.\nThe local updates are described in the following steps:\n1. Remove singleton cluster:\n\u2022 Singleton y-cluster: If z y,n = z y,n for all n = n, i.e. data point n is in a singleton y-cluster, remove that cluster and set (\u03c3 2 k \u2212n +1 , \u03b2 0,k \u2212n +1 , \u03bb k \u2212n +1 , \u03b1 \u03c8,k \u2212n +1 ) equal to the values of the singleton cluster parameters.\n\u2022 Singleton x-cluster within a non-singleton y-cluster: If z y,n = z y,n for some n = n and z x,n = z x,n for all n = n such that z y,n = z y,n , i.e. data point n is in a singleton x-cluster within a non-singleton y-cluster, remove that cluster. 2. Calculate the allocation probability for each occupied cluster: j \u2208 {1, . . . , k \u2212n } and l \u2208 {1, . . . , k\n3. Calculate the allocation probability for a new x-cluster within each occupied y-cluster:\n4. Calculate the allocation probability for m new y-clusters: sample m new parameters (or m \u2212 1 new parameters if z y,n was in a singleton y-cluster) from the prior (\u03c3\n5. Update the allocation variable z n using the allocation probabilities. All empty clusters are removed, and if one of the m new clusters is selected, set z n = (k \u2212n + 1, 1) and the parameters (\u03c3 After the full Gibbs sweep for the N allocation variables, two Metropolis-Hastings steps are performed to improve mixing and allow global changes to the allocation variables. The first proposes to move an x-cluster to be nested within a different or new y-cluster and is a 'smarter' version of the move described in Wade et al. [9] , by proposing moves that are more likely to be accepted. This step is separated into three possible moves: 1) an x-cluster, among those within y-clusters with more than one x-cluster, is moved to a different y-cluster; 2) an x-cluster, among those within y-clusters with more than one x-cluster, is moved to a new y-cluster; 3) an x-cluster, among those within y-clusters with only one x-cluster, is moved to a different y-cluster. Define\nAt every iteration, Move 1 is performed if k x,2+ > 0. Next, with probability 1/2, Move 2 is performed, otherwise, Move 3 is performed (with the exception that when k x,1 = 0, Move 2 is performed with probability 1, or when k x,2+ = 0, Move 3 is performed with probability 1).\nThe global updates to the y-clusters are described in the following steps:\n1. Move 1: an x-cluster (nested within a y-cluster with more than one x-cluster) is uniformly selected with probability k \u22121\nx,2+ and moved to be nested within a different y-cluster selected with probability proportional to the conditional marginal likelihood. Specifically, suppose x-cluster l in y-cluster j is first selected, then it is moved to be nested within y-cluster h with probability proportional to h(\u1ef8 l|j |\u1ef8 h , \u03c3 2 h , \u03b2 0,h , \u03bb h ). Let z * 1:N denote the proposed allocations defined by moving x-cluster l in y-cluster j to be nested within y-cluster h for h \u2208 {1, . . . , j \u2212 1, j + 1, . . . , k}. The acceptance probability is min (1, p) , where\n, where\u1ef8 * h contains the outputs under the proposed allocation with z * y,n = h , e.g.\u1ef8 * j contains the N j \u2212 N l|j outputs with the N l|j points removed from y-cluster j. The notation k *\nx,2+ represents the number of x-clusters within a y-cluster with more than one x-cluster under the proposed partition, i.e. k * x,2+ = k x,2+ \u2212 1(k j = 2) + 1(k h = 1). 2. Move 2: an x-cluster (nested within a y-cluster with more than one x-cluster) is uniformly selected with probability k \u22121\nx,2+ and moved to be nested within a new y-cluster. In this case, we propose new parameters (\u03c3 k+1 , \u03b2 0,k+1 , \u03bb k+1 , \u03b1 \u03c8,k+1 ) for the new y-cluster from the prior. The acceptance probability is min (1, p) , where\nwhere k *\nrepresents the number of x-clusters within a y-cluster with only one x-cluster under the proposed partition. 3. Move 3: an x-cluster (nested within a y-cluster with only one x-cluster) is uniformly selected with probability k \u22121\nx,1 and moved to be nested within a different y-cluster selected with probability proportional to the conditional marginal likelihood. Specifically, suppose x-cluster l in y-cluster j is first selected, then it is moved to be nested within y-cluster h with probability proportional to h(\u1ef8 j |\u1ef8 h , \u03c3 2 h , \u03b2 0,h , \u03bb h ) . Let z * 1:N denote the proposed allocations defined by moving x-cluster l in y-cluster j to be nested within y-cluster h for h \u2208 {1, . . . , j \u2212 1, j + 1, . . . , k}. The acceptance probability is min (1, p) , where\nThe second set of split-merge updates consists of the pair of 'smart-split' and 'dumb-merge' moves and the pair of 'dumb-split' and 'smart-split' moves, inspired from [10] , but tailored for the nested clustering structure of the EDP to propose global updates to the x-clusters. In this split moves, one x-cluster is selected and split into two x-clusters, still contained within the same y-cluster. In the merge moves, two x-clusters, within the same y-cluster are merged. The 'smart' moves propose clustering allocations that are more likely and are paired with the corresponding 'dumb' moves, with random cluster allocations, to increase the probability of the reverse move and acceptance of the smart moves. In the first pair of moves, a smart-split or dumb-merge is proposed with probability 1/2, and in the second pair of moves, a dumb-split or smart-merge is proposed with probability 1/2 (unless, there are only singleton x-clusters or only one x-cluster within each y-cluster). Again, we define k x,2+ as the number of x-clusters within a y-cluster with more than one x-cluster, i.e. the number x-clusters than may be merged, and additionally define\nas the number of x-clusters with more than one data point, i.e. the number x-clusters than may be split.\nThe global updates to the x-clusters are described in the following steps:\n\u2022 Smart-Split/Dumb-Merge: with probability 1/2, one of the following two moves is proposed. 1. Smart-Split: x-cluster l within y-cluster j is selected among the k x,1+ x-clusters containing more than one data point with probability proportional to 1/h(X l|j ). The proposed allocation z * 1:N is constructed sequentially by reallocating the data points currently allocated to x-cluster l within y-cluster j, in order of observation, to x-cluster l or a new x-cluster k j + 1 with sequential probabilities\nwhere X * l|j,n\u22121 denotes the set of x n such that z * n = (j, l) for n < n, and similarly, X * kj +1|j,n\u22121 denotes the set of x n such that z * n = (j, k j + 1) for n < n. Note that through sequential allocation, there is a positive probability that all points may be allocated to one cluster, and in that case the move is accepted with probability one, i.e. we remain at the current allocation. The probability of proposing the smart-split z * 1:N from z 1:N is\n, where the factor of 2 is is needed as the proposed state z * 1:N is equivalent when the labels l and k j + 1 are interchanged. The acceptance probability is min (1, p) , where\n, where k * x,2+ is equal to k x,2+ + 2 if x-cluster l was the only cluster within y-cluster j, i.e. k j = 1, and k * x,2+ is equal to k x,2+ + 1 otherwise. 2. Dumb-Merge: x-cluster l within y-cluster j is selected uniformly among the k x,2+\nx-clusters contained within a y-cluster with more than one x-cluster with probability 1/k x,2+ and a second x-cluster l = l within y-cluster j is selected uniformly among the k j \u2212 1 remaining x-clusters within y-cluster j with probability 1/(k j \u2212 1). The probability of proposing the dumb-merge z * 1:N from z 1:N is\n, where the factor of 2 is needed as the proposed state z * 1:N can also be reached by first selecting x-cluster l within y-cluster j and then selecting x-cluster l within y-cluster j. The acceptance probability is min (1, p) , where\nh(x n |X zx,n|j,n\u22121 ) h(x n |X l|j,n\u22121 ) + h(x n |X l |j,n\u22121 ) .\n\u2022 Dumb-Split/Smart-Merge: with probability 1/2, one of the following two moves is proposed. 1. Dumb-Split: x-cluster l within y-cluster j is uniformly selected among the k x,1+ x-clusters containing more than one data point with probability 1/k x,1+ . The data points in x-cluster l within y-cluster j are then randomly reallocated to x-cluster l or a new x-cluster k j + 1 with probability 1/2. Note that again there is a positive probability that all points may be allocated to one cluster, and in that case the move is accepted with probability one, i.e. we remain at the current allocation. The probability of proposing the dumb-split z * 1:N from z 1:N is\nwhere the factor of 2 is is needed as the proposed state z * 1:N is equivalent when the labels l and k j + 1 are interchanged. The acceptance probability is min (1, p) , where\n2. Smart-Merge: x-cluster l within y-cluster j is selected uniformly among the k x,2+ x-clusters contained within a y-cluster with more than one x-cluster with probability 1/k x,2+ . A second x-cluster l = l within y-cluster j is selected among the k j \u2212 1 remaining x-clusters within y-cluster j with probability proportional to h(X (l,l )|j ), where X (l,l )|j denotes the set of x n under the merger of x-clusters l and l . The probability of proposing the smart-merge z *\n, which is the sum of the probability of first selecting l and then l and vice versa, as the proposed state z * 1:N is equivalent under these proposals. The acceptance probability is min (1, p) , where\n, where k * x,1+ is equal to k x,1+ + 1 if two singleton clusters are merged; k x,1+ if one of merged clusters is a singleton; and k x,1+ \u2212 1 if neither cluster is a singleton.\nCluster parameters. The parameters for each cluster are conditionally independent across j = 1, . . . , k with full conditional\nwhich is not available in closed form. We use Hamiltonian Monte Carlo [1] to sample from the full conditional.\nMass parameters. The concentration parameters \u03b1 \u03b8 and \u03b1 \u03c8,1:k are updated using the auxiliary variable technique of Escobar and West [2] . For \u03b1 \u03b8 , sample an auxiliary variable \u03be \u223c Beta(\u03b1 \u03b8 +1, N ); set v \u03b8 = v \u03b8 \u2212 log(\u03be) and\nand sample \u03b1 \u223c Gam( u \u03b8 , v \u03b8 ). Similarly, for \u03b1 \u03c8,j , for j = 1, . . . , k, sample an auxiliary variable \u03be j \u223c Beta(\u03b1 \u03c8,j + 1, N j ); set v \u03c8,j = v \u03c8 \u2212 log(\u03be j ) and\nand sample \u03b1 \u03c8,j \u223c Gam( u \u03c8,j , v \u03c8,j ).\nLatent outputs. The latent outputs are independent across cluster j = 1, . . . , k, with full conditional\nIn the Gaussian case, p(y n |\u1ef9 n ) = 1(y n =\u1ef9 n ), and this step is not needed. For the other probit-type models, the full conditional of the latent outputs in cluster j is a truncated multivariate Gaussian, which is sampled through a Gibbs algorithm combined with cumulative distribution function inversion techniques [4] ."}, {"section_title": "Examples", "text": "We demonstrate the advantages of the enriched MoE, namely, improved predictive accuracy, smaller credible intervals, and more interpretable clustering, in two examples. The first demonstrates increasing improvement over the DP as D increases, and the second shows the range of applicability of our model for ordinal outputs with multiple input types. Code to implement the model and reproduce the results is publicly available at GitHub link, alongside further plots and videos. Prior parameter specification, further algorithm details, and epoch times are detailed in the SM."}, {"section_title": "Simulated mixture of damped cosine functions", "text": "In the first example, N = 200 points are generated with only the first input as a predictor. The true output model is a highly non-linear regression obtained as a mixture of two damped cosines [24] . The inputs are independently sampled from a multivariate normal, with the additional inputs positively correlated among each other but independent of the first input. We compare the DP and EDP, with automatic relevance determination (ARD) squared exponential kernels for the GP experts.\nA heat map of the posterior similarity matrix from the DP MoE in Figures This leads to more accurate predictions and tighter credible intervals. We quantify the predictive accuracy with the approximate L 1 distance between the estimated predictive response density and true data generating density, averaged across test samples. These errors are depicted in Figure 3 (b), alongside the average length of 95% credible intervals in Figures 3(c) , comparing the EDP with the DP, Lasso, GP, and TGP. While the L 1 errors generally increase with D, the EDP is the most robust. As expected, the Lasso, an effective tool for sparse linear regression, performs poorly in this highly non-linear example, but interestingly, the GP and TGP perform just as bad due to the inability to cope with bimodality. Moreover, the EDP produces tighter credible intervals across D, compared with the other methods, while maintaining similar coverage."}, {"section_title": "Alzheimer's challenge", "text": "Motivated by the Alzheimer's Disease Big Data DREAM Challenge (https://www.synapse.org/ #!Synapse:syn2290704/wiki/60828), this study aims to predict cognitive scores 24 months after initial assessment. This can potentially assist in early diagnosis of Alzheimer's disease (AD) and provide personalised predictions with uncertainty for patients and their families. Training data is extracted from the Alzheimer's Disease Neuro-Initiative (ADNI) database (www.adni-info.org). We emphasise that the competition test data can no longer be accessed, and the test results presented here are based on a random split of the data into training and test sets of sizes N = 384 and N * = 383. The ordinal response y n is the mini-mental state exam (MMSE) score at a 24 month follow-up visit; MMSE is an extensively used clinical measure of cognitive decline, defined on a 30 point scale with lower scores reflecting increased impairment. The D = 6 inputs include baseline age (in fraction of years); gender; baseline MMSE; education; APOE genotype, with values 0, 1, or 2, reflecting the number of copies of the type 4 allele; and diagnosis at baseline of cognitively normal (CN), early mild cognitive impairment (EMCI), late mild cognitive impairment (LMCI), and AD, respectively. The winners of this subchallenge were GuanLab (GL) and ADDT. GL [32] separated training data into three groups of CN, MCI or AD and trained support vector machines (SVM) within each group; SVMs provide non-probabilistic predictions, and for comparison, we also train linear regression models within group to obtain prediction intervals in GL2. ADDT [10] used robust regression based on M-estimation, optimally combined diagnosis and APOE4, and included interactions.\nThe EDP MoE can flexibly recover non-linear trajectories of the cognitive decline, while also clustering patients into input-dependent groups of similar trajectories. We consider the ordered probit GGP with fixed cutoffs 0 = \u03b5 0 < \u03b5 1 = 1 < \u03b5 2 = 2 . . . < \u03b5 29 = 29. Table 1 summarises the test performance of the methods via the mean absolute error and empirical coverage and average length of the 95% credible intervals. Compared to the DP, the EDP performs slightly better in mean absolute test error and has smaller uncertainty, reflected in a reduced average credible interval length, while maintaining good coverage (\u2248 0.95). This improvement is due to its ability to capture the relationship between y and x with fewer clusters, also leading to more interpretable clustering. Indeed, the VI estimate of the y-clustering of the EDP has only three clusters, while the DP has seven. Similar to GL, the EDP identifies three clusters of mostly CN, MCI, and AD individuals, with some adjustments for other variables, particularly, MMSE scores. The SM contains a deeper discussion on the clustering. The EDP MoE produces flexible nonparametric density estimates of MMSE follow-up scores that change smoothly with the inputs. Specifically, Figure 4 shows how the densities become less peaked with larger variability for decreased baseline MMSE, increased APOE4, and increased severity in diagnosis. Instead, GL and ADDT are not able to capture this behaviour, e.g. with a minimum prediction interval length of 8 for ADDT, despite the high probability of follow-up MMSE close to 30 for CN individuals with a baseline MMSE of 30 in Figure 4 . Also, note the apparent difference across APOE genotype for AD patients, with an increased probability of progressing to severe dementia for carriers in Figure 4 . Thus, the EDP provides much improved uncertainty in predictions, which is particularly important in clinical settings and in relation to established cutoffs for MMSE."}, {"section_title": "Discussion", "text": "Infinite mixtures of GP experts are flexible models, that can capture non-stationary functions and departures from the typical homoscedastic normality assumptions on the errors. In this work, we proposed a novel enriched mixture of GGP experts, with local independence of the inputs, to increase scalability and allow inclusion of multiple input types, and a nested partitioning scheme, to improve predictive accuracy, uncertainty, and interpretability of the clustering. Moreover, through the generalised GP framework, we can account for different output types.\nA number of proposals extend mixtures of linear experts for high-dimensional inputs using regularisation or variable selection, e.g. [19, 1] . Here, we focus on multi-dimensional input spaces, with GP experts and ARD kernels, that allow determination of the local relevance of each input. An important future research direction will incorporate methods to scale the GP experts to higher-dimensions, for example, using simple isotropic kernels or dimension reduction techniques [25] . To scale to larger datasets, future research will also focus on fast approximate inference such as MAP techniques [23] . "}, {"section_title": "Generalised Gaussian process experts", "text": "Examples of generalised GP experts include:\nGaussian: for y \u2208 R, with identity link function,\n, where the link function maps (0, 1) to the real line, e.g. logistic, probit. For the logistic link function,\n.\nFor the probit link function, P(y = 1|x, \u03b8 j ) = \u03a6(m j (x)), where \u03a6 denotes the standard normal cumulative distribution function. In this case, the model can be equivalently formulated through a latent response\u1ef9 that is Gaussian distributed with mean m j (x) and unit variance. In particular,\u1ef9|m j (x) \u223c N(m j (x), 1) and\nThe probit model is recovered by marginalising the latent\u1ef9.\nwhere the link function maps the L-dimensional simplex to R L . For the multivariate logistic link function,\nand has a multivariate Gaussian distribution with mean m j (x) = (m j,1 (x), . . . , m j,L (x)) T and covariance matrix \u03a3 j , which may be the identity matrix, or treated as a more general scale parameter (in this case, care should be taken to avoid identifiability issues). The prior on the vector-valued unknown function m j (x) can be extended to independent GPs across l = 1, . . . , L or, more generally, a matrix-variate GP. * Equal contribution\nOrdinal: for y taking ordered values l = 0, . . . , L and cutoffs 0 = \u03b5 0 < \u03b5 1 < . . . < \u03b5 L\u22121 ,\n, where the link function maps (0, 1) to the real line. Due to the nonparametric nature of the model we consider fixed cutoffs \u03b5 1 , . . . , \u03b5 L\u22121 [5] . For the logistic link function,\n.\nFor the probit link function,\nwith\nThe ordered probit model is recovered by marginalising the latent\u1ef9.\nPoisson: for y \u2208 {0, 1, 2, . . .},\n, where the link function maps (0, \u221e) to R. For the log link function with \u03bb j (x) = exp(m j (x)),\nAlternatively, a non-negative integer-valued output y \u2208 {0, 1, 2, . . .} can be modelled through a discretised latent Gaussian as in (1), with fixed cutoffs 1 = 1, 2 = 2, . . .."}, {"section_title": "Predictions for the enriched mixture of generalised GP experts", "text": "Letting \u03b6 = (z 1:N , \u03c3 , for m = 1, . . . , M , from the posterior. In the Gaussian example, the posterior density at y * given a new x * is given by\nIn this case, we have a weighted average of the GP predictive densities across clusters and the marginal likelihood h(y * ) for a new cluster. Note that the marginal likelihood h(y * ) for a new cluster is unavailable in closed form as it requires integration over the parameters (\u03b2 0 , \u03bb, \u03c3 2 ). However, we can compute a simple Monte Carlo estimate of this quantity by sampling from the prior,\nFor other types of outputs through probit models, we can similarly use the MCMC output to compute predictive quantities of interest at a test input x * . For example, considering the ordered probit with ordered categories l = 0, . . . , L and fixed cutoffs 0 = \u03b5 0 < \u03b5 1 < . . . < \u03b5 L\u22121 , we first note that we can compute the expectation and density of the latent continuous\u1ef9 * given the test input x * , as in the Gaussian example. The posterior probability that y * = l given the test input x * is P(y * = l|x * , y 1:N , x 1:N ) = P(y * = l|x * , y 1:\nFor cluster j of sample m, the probability that y * = l is with mixture weights, p (x n,1 ), equal to\nThe damped cosines are parametrised by \u03b2 1 = (0.1, 0.6) , \u03b2 2 = (\u22120.1, 0.4) with \u03c3 1 = 0.15, \u03c3 2 = 0.05. The mixture model is parametrised by \u03c4 1 = \u03c4 2 = 0.8, \u00b5 1 = 3 and \u00b5 2 = 5. The inputs are independently sampled from a multivariate normal x n \u223c N(\u00b5, \u03a3), centred at \u00b5 = (4, . . . , 4), with standard deviation of 2 along each dimension, that is \u03a3 h,h = 4. The covariance matrix \u03a3 assumes with the additional inputs positively correlated among each other, with \u03a3 h,l = 3.5 for h = l, h > 1 and l > 1 , but independent of the first input, with \u03a3 1,l = 0 for l > 1.\nFor both the DP and EDP mixtures of GP experts, we employ the same prior choices, based on identified reasonable ranges for the parameters. For the ARD squared exponential kernels of the GPs, 1  1  5  6  7  7  2  1  6  7  8  7 we utilise a Gamma(3, 1) prior on the first input dimension length-scale, Gamma(10, 1/2) prior on the other input dimension length-scales and a Gamma(2, Posterior inference for both models is performed with 5000 total iterations and a burn-in of 1000. Average epoch times (in seconds) after burn-in are reported in Figure 2(b) . When fewer experts are identified through the nested clustering of the EDP (e.g. D > 2 in our example), average epoch time is reduced for the EDP compared with the DP. Each run was performed independently and in parallel using the high performance computing resources provided by commented for blind review.\nThe VI distance between the true and estimated (y-level) clustering is depicted in Figure 2 (a), with dashed lines representing the size of the 95% VI credible balls. For the DP, the distance increases greatly with D, and the true clustering is far from the credible ball. The behaviour of the y-level clustering of the EDP is more robust to increasing D, while the x-level clustering requires an increasing number of clusters. Figure 1 depicts the heat map of the posterior similarity matrix for the x-clustering within the two estimated y-clusters, and Table 1 reports the number of x-clusters in the VI estimated x-clustering within the two estimated y-clusters.\nWe plot the estimates for the predictive response density and mean against the first input over a dense grid. These are presented in Figure 3 , for different choices of D. In the second and fourth rows the additional inputs are fixed to their sample means (approximately 4) for the DP and EDP models, respectively. Further, in the third and fifth rows, the additional inputs are marginalised.\nFinally, coverage plots are presented in Figure 4 . Centered around the true values (sampled from the data generating distribution of equation (2)), these plots show the 95% highest posterior density credible intervals for randomly sampled inputs (in some cases this may be a union of intervals). When the sample of the truth lies within our credible interval the line is blue, otherwise it is red. The increasing uncertainty of the DP for increasing D is clearly visible from Figure 4 , while the EDP retains smaller credible intervals, with similar coverage. Figure 2 (c) summarises the coverage across the competing models. All GP-based methods show a decrease in coverage with increasing D. In order to cope with the additional noisy inputs, length-scale priors with heavier tails may be required to effectively identify the relevant inputs."}, {"section_title": "Alzheimer's Challenge", "text": "Training data for the challenge is extracted from the Alzheimer's Disease Neuro-Initiative (ADNI) database 2 . This data set consists of six inputs: age (in fraction of years), gender, the baseline mini mental-state exam (MMSE) score, the number of years an individual has spent in education, APOE 2 The ADNI was launched in 2003 by the National Institute on Aging (NIA), the National Institute of Biomedical Imaging and Bioengineering (NIBIB), the Food and Drug Administration (FDA), private pharmaceutical companies and non-profit organizations, as a $ 60 million, 5-year public-private partnership. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD). Determination of sensitive and specific markers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as lessen the time and cost of clinical trials. The Principal Investigator of this initiative is Michael W. Weiner, MD, VA Medical Center and University of California-San Francisco. ADNI is the result of efforts of many co-investigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 adults, ages 55 to 90, to participate in the research, approximately 200 genotype (recoded to reflect the number of copies of the type 4 allele), and the clinical diagnosis assessed at the baseline. The output is the MMSE score taken at a 24 month follow-up visit, and the task is to predict the cognitive decline in a patient over this period.\nWe again employ the same prior choices for both mixtures of GP experts models, based on identified reasonable ranges for the parameters. We consider an ARD squared exponential kernel for the GP with Gamma(a l,d , b l,d ) priors on the length-scales with a l = (3, 2, 3, 5, 3, 2), and b l = (3/20, 5, 1, 1, 5, 4), in order of the inputs listed above. Additionally, we specify a Gamma(a m , b m ) prior on the magnitude with a m = 2 and b m = 1. These parameters were selected to reflect our prior knowledge on the relationship between follow-up MMSE and the inputs and based on the range of the inputs. The GP is assumed to have a prior constant mean with a N(20, 7.5\n2 ) prior. The variance \u03c3 2 y has a Gamma(a y , b y ) prior with a y = 1.5 and b y = 0.5.\nFor the DP, the mass parameter has hyper-parameters (u a = 1, v a = 1), and for the EDP, the mass parameters have hyper-parameters (u \u03b8 = 1, v \u03b8 = 1) and (u \u03c8 = 1, v \u03c8 = 1). The parametric local model for x n is the product of a normal density for age, a categorical density for gender, and four binomial densities for baseline MMSE, education, APOE4, and diagnosis. The input hyperparameters are u 0 = 72, c = 2, b x = 10, and a x = 2 for age; \u03b3 2 = (1, 1) for gender; \u03b3 3 = (5, 1) for MMSE; \u03b3 4 = (3, 2) for education; \u03b3 5 = (1, 3) for APOE4; \u03b3 6 = (1, 1) for diagnosis. Posterior inference for both models is performed with 5000 total iterations and a burn-in of 1000.\nFor comparison with the best performers of this subchallenge, the GuanLab and ADDT teams, we implemented the models using publicly available packages in R. For the GuanLab model, we used the svm function of the e1071 package [6] . For the ADDT model, we used the rlm function of the MASS package [8] . For both mixtures of experts, posterior medians, i.e. the point estimate under the absolute error loss, are used to predict MMSE scores, which are appropriate due to the heavy left tail of the predictive densities.\nHeat maps of the posterior similarity matrices are provided in Figure 5 , and visualizations of the VI clusterings through side-by-side bar plots of MMSE baseline, MMSE follow-up, education, diagnosis, APOE4, gender and age are provided in Figure 7 , with colours representing clusters. Interestingly, cognitively normal older individuals to be followed for 3 years, 400 people with MCI to be followed for 3 years and 200 people with early AD to be followed for 2 years. For up-to-date information, see www.adni-info.org. the enriched model identifies three clusters consisting mostly of cognitively normal (black), mild cognitive impairment (red), and AD (green) individuals, similar to the GuanLab model, with slight modifications considering the other variables, particularly, MMSE baseline and follow-up scores. For example, one late MCI individual is allocated to the AD (green) cluster in Figure 7 (d) due to the observed sharp drop in MMSE from 27 at baseline to 8 at follow-up. Additionally, we observe that the relative proportion of individuals in the red and green clusters increases with higher APOE4, but does not (marginally) depend on gender and age.\nThe DP, on the other hand, further subdivides clusters due to multimodality in education. Similarly, for the enriched model, the VI estimate of x-clustering within each VI estimated y-cluster, contains Figure 8 depicts the heap map of the posterior similarity matrix for the x-clustering within each estimated y-cluster and also shows the VI estimate of x-clustering within each VI estimated y-cluster for education, with each estimated x-clustering containing two clusters.\nWe can further appreciate the difference between the deterministic clustering of the GuanLab model and the stochastic clustering of the enriched model in Figure 6 , which shows the allocation probabilities of a new test point for MMSE baseline scores of 20-30 and diagnosis of CN (Figure 6 individuals with baseline MMSE \u2265 27 have the highest probability of being allocated to the black cluster, while this baseline MMSE cutoff is increased to 28 and 30 for eMCI and lMCI individuals, respectively. Below these respective cutoffs, CN, eMCI, and lMCI individuals have the highest probability of being allocated to the red cluster (apart from lMCI individuals with baseline MMSE of 20 that are allocated to the green cluster with highest probability). Instead, AD individuals have the highest probability of belonging to the red cluster for baseline MMSE \u2265 25 and to the green cluster otherwise. We note that for CN individuals with low MMSE baseline (not observed), there is a small probability of allocation to a new (blue) cluster. Figure 9 shows how the predictive densities of MMSE follow-up scores change given different combinations of baseline MMSE, diagnosis, and APOE4. For CN individuals, the differences between APOE4 type are minor and the posterior mass is very concentrated on high follow-up MMSE scores given high baseline MMSE scores. More evident differences between APOE4 type are visible for more severe diagnosis, and in general, we observe a greater decrease in follow-up scores with more uncertainty for more severe dementia and increased APOE4. In particular, for AD patients that are carriers of APOE4, there is a visible probability of progressing to severe dementia (MMSE\u2264 12) , that increases with decreased baseline MMSE. "}]