[{"section_title": "", "text": "Instead I would like to focus on criteria that might be used to distinguish good social science from good literature. Even if one stipulates that a good story need only sound \"believable\" or \"entertaining\" -in social science I believe we should aim for a different standard.\nOne sensible criterion is that claims in the social sciences should distinguish itself by the \"severity\" of the \"tests\" to which they are put.\n5 Mayo (1996) cites the American philosopher C. S. Peirce to provide a nice short account of what a \"scientific\" approach is and what is meant by a \"severe test\": 6 [After posing a question or theory], the next business in order is to commence deducing from it whatever experimental predictions are extremest and most unlikely ...in order to subject them to the test of experiment.\nThe process of testing it will consist, not in examining the facts, in order to see how well they accord with the hypothesis, but on the contrary in examining such of the probable consequences of the hypothesis as would be capable of direct verification, especially those consequences which would be very unlikely or surprising in case the hypothesis were not true.\nWhen the hypothesis has sustained a testing as severe as the present state of our knowledge ...renders imperative, it will be admitted provisionally ...subject of course to reconsideration. 7 The context of Peirce's remarks is a discussion of the importance and usefulness of bringing statistical reasoning to bear on history, though clearly they apply more broadly. While accepting the notion that putting our questions to a severe test is a good idea, for most problems there is no simple formula for assessing severity. Nonetheless, it seems like such a sensible criterion that it might come as a surprise that much economics research is of the first sort mentioned by Peirce -evaluating how well the facts accord with a given economic hypothesis. Undergraduate economics textbooks are filled with stories, very few of which have been forced to bear mild, let alone severe scrutiny, but are \"broadly consistent\" with the data.\nA convenient place to begin is the issue, raised several times in Freakonomics (and the student guide, which refers to it as a \"basic economics concept\"), of whether an alleged relationship is \"cause\" or \"correlation.\" Indeed, Freakonomics invokes several different notions of causality and, I begin by reviewing some of what it has to say on the subject. Stripped to its essence my argument is that such a debate often seems beside the point: \"cause\" means many things. A more relevant question about a correlation is whether it provides a severe test of a hypothesis.\nNext I turn to a description of the randomized controlled trial (RCT), not as an exemplar of what all, or even most, social science should be, but rather as an exemplar of subjecting a hypothesis to a severe test.\nA basic precondition to severe testing, of course, is to formulate questions that can be put to some kind of test. Unfortunately, many social science questions often fail to meet this precondition. I take a couple of examples from Freakonomics and argue that some of the questions it addresses are \"uninteresting\" because it is impossible to even imagine what a good answer would look like. Somewhat ironically, the issues in Freakonomics that have generated the most popular debate seem are the ones that seem to have no good answers.\nI conclude with some thoughts about the role of economic theory in generating interesting questions and/or answers."}, {"section_title": "Correlation Is Causation?", "text": "Causes make appearances in Freakonomics in many different and confusing ways. 8 In some places Freakonomics seems to invoke causation as \"explanation\" or \"motive\":\nobservation that \"probably the mountain of evidence piled up during almost 30 years of social experimentation was high enough to convince the [Spanish] government that nothing could be gained by further attempts to make the Indians live like 'Christian laborers in Castile.\" (page 71) 5 For an insightful and much more careful exposition of the notion of \"severe\" testing that motivates this discussion see Deborah Mayo (1996) . 6 Perhaps an even pithier summary was provided by Lecam (1977) in his critique of Bayesian solutions to the problems of inference: \"the only precept or theory which really seems relevant is the following: 'Do the best you can.' This may be taxing for the old noodle, but even the authority of Aristotle is not an acceptable substitute.\" 7 Peirce (1958) 7.182 (emphasis added) and 7.231 as cited in Mayo (1996) . 8 This is unfortunate as \"causation\" is a notoriously difficult topic even when treated by a serious philosopher and I can not do it minimal justice here. One place to start for a more ably argued introduction to these issues in economics is Cartwright and Reiss(2007) .\nGuerry this view was based on the observation that the departments where education is least widespread are those where the most crimes are committed [ (Guerry 1883) , page 87, emphasis in original]. Guerry was able to refute conventional wisdom on the subject by merely demonstrating (with better data) that the correlation between education and crime at the department level was not negative, but positive. Moreover, Guerry apparently felt little need to consider the possibility of what we might call \"confounders.\" Having established that the correlation was positive, he adduced further evidence that the conventional view was wrong by demonstrating the stability of the correlation over time: a law of sorts.\nThe impulse to embed statistical uncertainty in an otherwise \"determinist\" world view led to arguably one of the most bizarre intellectual strands in social science: the idea that statistical laws vitiated free will (Hacking 1990 , Hacking 1983a . A wonderful illustration comes from Dickens' Hard Times (1854) . Mr. Gradgrind (who it may be recalled named two of his sons \"Malthus\" and \"Adam Smith\"! ) was in part the satirical embodiment of statistical fatalism.\n11 If the number and proportion of crimes displayed statistical regularities, could the criminals really have free will? When Mr. Gradgrind's son Tom is revealed to be a thief, Tom responds to his father's shock and dismay this way:\n'I don't see why,' grumbled the son. 'So many people are employed in situations of trust; so many people, out of so many, will be dishonest.' I have heard you talk, a hundred times, of its being a law. How can I help laws? You have condemned others to such things, Father. Comfort yourself. (Book Three, Chapter 7) While we have long abandoned the view (I hope) that statistical laws have anything to say about free will, still with us is the idea that statistical distributions are \"laws\" that regulate human behavior on some macro scale."}, {"section_title": "12", "text": "Notwithstanding the paucity of economic \"laws\", the idea that mere empirical regularities might embody \"causes\" can not always easily be dismissed. One might argue that Newton's law of gravitation was an example of an empirical regularity -correlation -that became a \"cause.\" Surely we can talk about gravity causing my cup of coffee to fall off the table after I pushed it. However limited we might find such an account of gravity as a cause, the testable predictions from the law of gravitation can ultimately be put to rather severe tests in an awe-inspiring variety of contexts. Thus, one can sympathize with Leibniz's opposition to the concept of gravity -which he dismissed as an \"occult\" force -while maintaining it is a useful and powerful idea. Whether gravity is \"real\", a law of nature, or whether it is really a \"cause\" seems beside the point."}, {"section_title": "Distant and Subtle Causes", "text": "The word \"cause\", unfortunately, can mean many different things. Herbert Simon once observed that \"in view of the generally unsavory epistemological status of the notion of causality, it is somewhat surprising to find the term in rather common use in scientific writing.\" (Simon (1953) as cited in Zellner (1984) . Indeed one of the most confusing themes in Freakonomics is that \"distant and subtle causes can have dramatic effects.\"\nTheir claim about \"distant and subtle causes\" is confusing in a couple of ways.. First, it doesn't seem to speak to the type of \"manipulationist\" notions of causality that concern many in social science. Second, the claim evokes an echo of the Laplacean determinism I discussed above. While it is not precisely clear what notion of \"cause\" is being invoked, it seems to speak to some \"causal\" antecedent which sets off a long chain of events ultimately resulting in a specific event. It is a common narrative device in fiction -many a character's fate can be \"traced back\" to a single fateful act.\nThe search for the single (or small number of) causal antecedent(s) of an event is surprisingly common among economists: \"what caused wage inequality to increase in the 1980s\" or \"what caused the Great Depression\" or \"what caused crime to fall in the 1990s\" (a question taken up in Freakonomics) are three examples that come to mind. I won't deny that the search for answers to such questions can sometimes be informative. Nonetheless, except for the very simplest phenomenon, it is rarely clear what constitutes a good answer to such a question."}, {"section_title": "13", "text": "11 And economics as well. The colorful protagonist Sissy defines the basic principle of Political Economy as \"To do unto others, as they would do unto me\"! 12 See for example, DeVany (2007) and the references cited within. 13 For a more optimistic view about the types of questions that are \"susceptible to empirical investigation\" see Zellner (1984) Consider something as simple as \"the cause of death.\" Enumeration of such causes dates back at least to 1592: \"the occasion of keeping an accompt of burials arose first from the plague\" (Graunt 1676) . Not surprisingly the victims of the plague were not drawn randomly from rich and poor; neither was the focus on the cause of death politically inert. Fagot (1980) reports on one Doctor Vacher who, seeking to understand the dramatic increase in deaths during the 1870 siege of Paris went back to study an even earlier four-month siege of Paris in 1590. After studying the data he was led to conclude that one of the \"effects of insufficient food\" was that the lethality of diseases such as typhoid was much greater. Nonetheless, \"hunger\" or \"lack of food\" was rarely cited as a \"cause\" of death, although he identified undernutrition as an \"underlying potential cause.\"\nThis arbitrariness, of course, persists. In the U.S., for example, most people die of more than a single cause of death; yet even on the death certificate, where up to 20 causes of death can be reported, the distinction between \"underlying causes\" and other types of cause remains! (Center For Health Statistics 1998).\n14 Despite its arbitrariness such information can be useful. Indeed, if there is any clear doctrine on how to attribute the cause of death, perhaps it is the requirement that the classification scheme is somehow minimally \"useful\" (Fagot 1980) . No amount of diligent record keeping, however, will be able to create a \"complete\" description of \"why\" some people die -debate on \"why\" Jesus died continues! (Edwards, Gabel and Hosmer 1986 , Anonymous 1986 , Gosling 1987 , Brenner 2005 , ur Rehman 2005 , Saliba 2006 )."}, {"section_title": "Cause as Explanations", "text": "Surely \"crime\" or other social science issues are at least as complicated as \"death\". Yet it is surprising how much social science research seems dedicated to telling simple stories. This suggests another related notion that might be called \"cause as explanation.\" While such stories appear to have great appeal, I must confess I don't understand why.\nA well known reductio ad absurdum of this type of reasoning concerns the famous Dr. Pangloss in Candide."}, {"section_title": "15", "text": "At one point Candide is reunited with his former teacher Dr. Pangloss, who has been reduced to a beggar with his nose half-eaten off, covered in scabs. Surprised by this (and a lot of other) misfortune Candide \"inquired into the cause and effect, as well as into the sufficing reason that had reduced Pangloss to so miserable a condition.\" We learn that Dr. Pangloss had \"tasted the pleasures of Paradise\" with Pacquette, a pretty servant girl, who had, as it turns out, been infected with a disease, the impressive genealogy of which Dr. Pangloss is able to trace back to a Countess, a Jesuit, a novitiate (among others), and ultimately Christopher Columbus. Candide asks why did Dr. Pangloss suffer such a horrific fate? What caused his degradation? For Dr. Pangloss, causal questions were straightforward: things could not be otherwise than they are, all things are created for some end, and thus all things are created for the best. In this case, Dr. Pangloss concludes his suffering was \"a thing unavoidable, a necessary ingredient in the best of worlds\" for had this disease not come to pass \"we should have had neither chocolate nor cochineal.\""}, {"section_title": "16", "text": "The humor in Candide comes from the creativity with which one can generate a \"theoretically justified\" explanation of \"why\" for any set of facts. One obvious problem with Dr. Pangloss' explanations is the impossibility of putting such views to a severe test of any sort. 17 14 The U.S. Implementation of the International Classification of Diseases includes this instruction: \"A cause of death is the morbid condition or disease process, abnormality, injury, or poisoning leading directly or indirectly to death. The underlying cause of death is the disease or injury which initiated the train of morbid events leading directly or indirectly to death or the circumstances of the accident or violence which produced the fatal injury. A death often results from the combined effect of two or more conditions. These conditions may be completely unrelated, arising independently of each other or they may be causally related to each other, that is, one cause may lead to another which in turn leads to a third cause, etc.\" (page 6, National Center for Health Statistics (2006) 15 Voltaire describes Pangloss this way: \"[He] was a professor of metaphysico-theologo-comsolo-nigology. He could prove, to admiration, that there is no effect without a cause; and, that in this the best of all possible worlds, the baron's castle was the most magnificent of all castles, any lady the best of all possible baronesses. It is demonstrable, said he, that things cannot be otherwise than as they are: for all things having been created for some end, they must necessarily be created for the best end. Observe, that the nose is formed for spectacles, and therefore we wear spectacles. The legs are visibly designed for stockings, and therefore we come to wear stockings.\" Chapter 1, (Voltaire 1796) 16 See Chapter 4 of Voltaire (1796) . The translator of this version of Voltaire's story attributes this style of reasoning to the \"maxims of Leibniz\" and as put into the mouth of Dr. Pangloss is a \"most Capital and pointed stroke of Satire.\" Cochineal is apparently a red dye made from ground up insects. 17 Kenyes (1921,page 297) argues that \"the discussion of [Aristotelian] final causes and of the argument from design has suffered from its supposed connection with theology.But the logical problem is plain and can be determined upon formal and abstract considerations.\"He illustrates the case with the evidentiary value of observing of some unusual event and ascribing its cause to some \"supposed conscious agent.\" Much economics as \"explanation\" it seems to me, resembles Dr. Pangloss' explanations. With enough cleverness one can dream up a mathematical model of utility-maximizing individuals to explain anything. It is not always clear what purpose such explanations serve. In a throw-away line in Freakonomics, for example, the authors attribute the putative fact that \"the typical prostitute earns more than the typical architect\" (page 106) to a \"delicate balance\" of \"four meaningful factors\"\n18 . I don't mean to deny that such factors often play some rolecertainly, for example, an intervention to make a job more unpleasant may act to reduce the number of people willing and able to do that job. But even if we stipulate to these \"four meaningful factors\", that is only the beginning of an explanation at best. Some writers have sought to define a cause as something that arises from the predictable consequence of an intervention that can be evaluated by something approximating randomized design. As the foregoing has made clear, this definition is too limited given the many different notions of the word \"cause.\" Rather than \"no causation without manipulation\" (Holland 1986 ) it might be more truthful to say that discussions in social science about causes are more intelligible when they involve an intervention of some sort; moreover, a focus on such \"policy evaluation\" questions often leads to more interesting questions, and importantly often leads to situations when we may able to subject our views to some kind of test. In a helpful discussion, Cartwright and Reiss (2004) suggest the slogan \"disambiguate before you evaluate.\"\nMy purpose in discussing a randomized controlled trial (RCT) is that it is useful to review a framework where what question is being asked, and the ground rules under which we might find an answer credible is arguably more transparent than is usual. As is common practice, I will describe questions answered in such a framework as \"causal\", although they are often causal in a very limited sense. 20 Indeed, the origins of the RCT lie in the attempt to put some of the \"squishiest\" beliefs to a severe test -some of the earliest examples arose in the study of telepathy. (Hacking 1988) ."}, {"section_title": "21", "text": "In Freakonomics, regression analysis is described as the tool of someone who can't conduct a RCT:\nIn a perfect world, an economist could run a controlled experiment just like a physicist or a biologist does: setting up two samples, randomly manipulating one of them, and measuring the effect. But an economist rarely has the luxury of such pure experimentation. (That's why the school-choice With a simple application of Bayes' rule he does conclude that \"no conclusion, therefore, which is worth having, can be based on the argument from design alone; like induction, this type of argument can only strengthen the probability of conclusions, for which there is something to be said on other grounds.\" 18 \"When there are a lot of people willing and able to do a job, that job doesn't generally pay well. ...the others are the specialized skills a job requires, the unpleasantness of a job, and the demand for services that the job fulfills.\" (Page 105) 19 As to the truth of the claim about prostitute wages, it is too imprecise to verify or deny; moreover Dubner and Levitt provide no reference. In a previous version of this essay I concluded that it would be a major project to verify such a claim. Putting aside the almost insuperable problems of defining prostitution and measurement of hours worked, a comparison of data from a probability sample of Los Angeles prostitutes, Lillard (1998) 20 For those who prefer a definition of \"cause\", one that seems to capture some of the ideas I have in mind is due to Heckman (2005) :\nTwo ingredients are central to any definition [of causality]: (a) a set of possible outcomes (counterfactuals) generated by a function of a set of \"factors\" or \"determinants\" and (b) a manipulation where one (or more) of the \"factors\" or \"determinants\" is changed. An effect is realized as a change in the argument of a stable function that produces the same change in the outcome for a class of interventions that change the \"factors\" by the same amount. The outcomes are compared at different levels of the factors or generating variables. Holding all factors save one at a constant level, the change in the outcome associated with manipulation of the varied factor is called a causal effect of the manipulated factor.\nFor a discussion of the limitations of any single definition of causality relevant for economists see and Cartwright and Reiss (2003.) For a thoughtful and well-reasoned discussion of views about causation which do not have a central role for \"manipulation\" see Zellner (1984) Zellner prefers to work with a definition proposed by Feigl \"the clarified (purified) concept of causation is defined in terms of predictability according to a set of laws\" which appears to be able consider many sorts of questions, albeit subject to a logical (Bayesian) calculus, which could not be put to a severe test in my way of viewing of the issue. 21 It is also unsurprising that C. S. Peirce was one of the earliest to conduct a high quality RCT (Stigler 1978 lottery in Chicago was such a happy accident.) What an economist typically has is a data set with a great many variables, none of them randomly generated, some related and others not. From this jumble, he must determine which factors are correlated [sic] and which are not.\n22 (Page 162) Putting aside whether this is a description of good practice, the view that regression is a (sometimes inadequate) substitute for a randomized controlled trial is not universally held by economists. More surprisingly, perhaps, is that as a philosophical matter \"it is hard to think of a more controversial subject than that of randomization (Suppes 1982) . 23 Convincing Bayesian rationales for randomization, for example, are evidently difficult to generate 24 and this difficulty has been the source of criticism of Bayesian methods for being unable to \"distinguish between experience and experiment\" (Lecam 1977) . 25 Rather than hold up the RCT as a paradigm for all research, I review it here because it represents a single case in which we sometimes have some hope of evaluating (limited, context dependent) causal claims, and because what constitutes a severe test is somewhat clearer.\nSecond, the RCT is a useful framework to discuss the \"intelligibility\" of putatively causal questions. That is, if one is discussing a \"causal\" question, whether or not one is discussing an RCT, the RCT often provides a useful template to evaluate whether the causal question is answerable. It allows us to try to answer the question \"what do you mean by a causal effect\", as well as the related question \"how credible is your inference about the \"cause.\"\nA natural by-product of considering the RCT is that the limitations of a research design to answer \"interesting\" questions (and what might provide evidence for and against the validity of the design) is easier to understand. Ironically, I suspect that some of the disenchantment with RCTs relates to the relatively transparent notion of \"cause\" -in particular the possibility that the putative cause under examination is \"implementation -specific\", which I discuss below."}, {"section_title": "Randomized Controlled Trials", "text": "In an RCT, a single potential cause is randomly \"assigned\" to a treatment group and an (inert) placebo is assigned to the control group.\nLet y i be an outcome which can be measured for all individuals, and let T i =1 signify that person i has been assigned to treatment and T i =0 otherwise. Suppose the following characterizes the true state of the world 26 :\nwhere \u03b1 and \u03b2 are constants, f(\u22c5) is some unknown function of all the observable characteristics that affect y i before being assigned to the treatment or control, and i \u03b5 is all the other unmeasurable influences. Even at this level of 22 It is not clear what is intended. Even in a \"jumble\" of data, determing what variables are correlated is straight forward. 23 For a sample of this debate see Suppes (1982) , Harville (1975) , Swijtink (1982) , and the illuminating debate in Savage (1961) . 24 There are many different flavors of Bayesian arguments against randomization. One argument, not necessarily the best, will be familiar to economists. From Berry and Kadane (1997) \n. If the randomization is non-trivial, i.e. if 0<\u03bb<1, then randomization could be optimal only when\n, and even then a non-randomized decision would be as good.\" Another rationale is that it helps \"simplify\" the appropriate likelihood (Rubin 1978) . 25 Although salient to this discussion, limitations of scope do not permit an extensive discussion of these issues. For a useful discussion and a defense/reformulation of classical statistical inference see Mayo (1996) . 26 Another way to proceed, which is often helpful, is to establish a notation for counterfactuals. Let Y i (1) be the outcome when the person is assigned to the treatment and let ) 0 ( i Y be that same person's outcome when they are assigned to the control. The treatment effect for person i\n. It is generally impossible to observe \u03c4 i since the individual is in one state or the other. We could then talk about trying to define E[\u03c4 i ] (for some population) as the object of interested. See Holland (1986) for an exposition along these lines. See Heckman (2005) for a critique of that approach and related points.\ngenerality, it takes a considerable leap of faith to think that this simple (partially) linear representation can yield anything but the most limited understanding of the effect of T even when some understanding is possible. A fundamental problem we face is that for an individual i we can only observe the person in one of the two states -treatment or control. Another related problem is that we don't observe everything that affects the outcome y. For any individual then, we can never be certain that some unobserved determinant of the outcome y is changing at the same time we are assigning the person to treatment or control.\nThe key to this design is that by coin toss, nature, or some other contrivance that generates \"random numbers\", persons are next assigned to either treatment or control in a way that is independent of their characteristics. If this assignment is conducted on a random sample of individuals from a particular population, then the mean outcome for individuals in the treatment group - (Provided, of course, that there is in fact some stable relationship between the cause and the outcome.) 27 The difference between these two means is likewise a \"good\" estimate of the average treatment effect for this group."}, {"section_title": "28", "text": "The assertion that the estimate so formed is a \"good\" one is fortunately not one that has to be taken solely on faith: it can be tested. While not \"assumption free\", our confidence in estimates generated this way does not rely on us having complete knowledge of the data generation process given by equation (1). Specifically, it is reasonable to hope that we can get a good answer without having to hope that somehow we can \"control\" for all possible confounds.\nIn a typical RCT, in fact, any of the variables in X are generally not used for any purpose but to test the design. Under random assignment, any X should be the same on average for the two groups. This is, of course, a consequence of random assignment that is routinely tested in every RCT. If the groups look very different on average, this is generally considered evidence against the design, and one reason to have less confidence in the results. A related implication is that in an RCT, the answer should be insensitive to the addition of additional controls."}, {"section_title": "29", "text": "It is the fact that the important X's are the same on average that gives us some reason to believe that the same is true for the \u03b5. Even in this simple case, we can never be sure that this is true. At best, the answers from identical experiments have the \"tendency\" to be correct.\nSeveral attractive features of a well designed RCT that are usually too obvious to deserve mention become more important when one turns to the sorts of \"approximations\" we are often faced with in social science:\n1. Pre-specified research design. In an RCT, the researcher specifies in advance to the extent possible the conditions that have to be satisfied, and what will be concluded under every possible result of the experiment. (This is articulated with the usual degree of tentativeness associated with any technique involving sampling.) 30 27 Already this aspect of the RCT highlights its weakness for a lot of social science questions. Many social scientists are interested \"why\" someone does what they do or why things turned out as they did. In the RCT, however, the credibility of the answer hinges on the fact that part of human choice has been handed over (implicitly or explicitly) to a (hypothetical) chance set up. This is also a source of the considerable ethical problems that are frequently involved in RCTs. 28 Even in this short description we have swept several very important issues under the rug that can arise even in a simple medical example. For instance, we are assuming that \"general equilibrium\" effects are unimportant so that one isn't concerned that the controls are affected by the treatment also. These and related concerns become even more important when we raise our ambitions to seek to extrapolate the results of the experiment to other possibly different contexts. There is a long tradition in economics of seeking answers to these more difficult questions that dates back at least to the Cowles Commission (see Heckman (2000) and Heckman (2005) for useful discussions). I focus on \"simpler\" less ambitious questions. (Heckman and Vytlacil 2005) 29 This is, one is tempted to speculate, the source of the intuition, that many appear to have, that somehow if a result \"survives\" the inclusion of a long list of covariates, it is a more trustworthy estimate. 30 I don't mean to advocate a simple-minded caricature of the Fisher or Neyman-Pearson significance testing approach. Long-standing criticisms of insisting on pre-specification is that they are rarely strictly applied (with good reason). See Mayo (1996) for a discussion of the debate about \"predesignation\" and a helpful reformulation of Neyman-Pearson \"error statistics.\" In her framework, violations of predesignation are licensed when they don't make the test of the hypothesis less \"severe.\" Surprisingly, some Bayesians argue for the irrelevance of pre-If we are assessing the efficacy of a drug, for instance, it is pointless to decide in advance that the drug \"works\" and then massage the data, sample, specification, etc. until we \"reach\" that conclusion. Doing so would seem to vitiate using the RCT (or regression more generally) as a method for anything but confirming our previously held beliefs."}, {"section_title": "31", "text": "Indeed, historically and etymologically the notion of an \"experiment\" is intimately related to the effort to put one's views to the test (DiNardo 2006b). Clearly, after the fact research design is less \"severe.\" 2. \"Transparent\" research design. In the classical RCT, as one example, it is transparent what constitutes evidence against the design (for example, if the predetermined characteristics of the treatment and control are very different) and what comparison or regression coefficient constitutes evidence in favor of, or against, the claim. Another set of assumptions -again usually too obvious to be discussed in the case of the RCT -deal with whether a question or set of questions are \"well posed\" or whether the answer suggested by RCT addresses the \"intended\" question.\n1. We can identify a \"treatment\" or \"policy.\" At one level, since we are dealing with human beings, one often has to carefully distinguish between \"assignment to treatment\" and the \"treatment.\" You can assign someone to take a specific medicine but it isn't always reasonable to assume that the person has taken the medicine. Even if we can ignore such distinctions it may be difficult to identify what our treatment is. Even the most routine, minor medical manipulation often comes bundled with other things. Many years ago it would have been a sound inference based on much unfortunate experience that the causal effect of a spinal tap (lumbar puncture) would be a serious headache afterward. Is this effect caused by the substance used to sterilize the needle? The type of needle? The size of the needle? Despite the fact that lumbar punctures have been performed for more than 100 years (Sakula 1991) , these questions continue to be subject of debate despite many randomized controlled trials (Armon and Evans 2005) . 2. The effect of a treatment is always relative to the control. The state of being assigned to the control is the \"counterfactual\" against which the treatment is evaluated. An effect is a comparison of outcomes in different possible states. 3. The treatment involves an \"intervention\" and/or is \"manipulable\". In the RCT, this is so basic it hardly deserves mention; it is, however, a subject of some debate among economists. 32 In the limited way I wish to use the word \"cause\", it is not meaningful to question the effect of \"being black\" on one's propensity for crime. Only in a fantasy world does it make sense to consider the fate of John DiNardo as a \"black man.\" If a misguided social scientist had been able to secretly reach back into the womb to manipulate John DiNardo's DNA to make him \"black\" (something that would have no doubt come as a surprise to his Italian parents) would it even be meaningful to describe the person generated from that process as the \"black John DiNardo? \" to which the \"white John DiNardo\" could be compared? The issue is not \"Is such a manipulation possible?\" but \"Were such a manipulation conceivable, would it answer the question we are asking?\" If the answer to that question is \"no\", I would describe the question as ill-posed or unintelligible even if it is the answer to a different well-posed question. I have no wish to overstate this issue: some of debate may be of no greater moment than questions of terminology. For example, I think it is possible to talk in a limited way about the effect of changing a person's perception of the race of, say, a job applicant because it is perhaps meaningful to think about manipulating a person's perception of race. 33 designation on the grounds that the \"mental state\" of the person collecting the data should have no relevance for the evidential import of the data. See Mayo and Kruse (2002) and the references therein for a useful discussion. 31 For an illustration of evolving definitions of the \"appropriate\" specification after having seen the results, and the consequences of failing to adopt a pre-specified research design, see the discussion of Welch (1974) , Siskind (1977) , Welch (1976) and Welch (1977) in chapter 6 of Card and Krueger (1995) . Although the extent of this research style is unknown, I suspect that the example is unusual only because it is documented. 32 See Granger (1986) for example. 33 Moffitt (2005) for example, explains that \"[The argument in Holland (1986) that race can not be a cause because it can not be manipulated results from] ...a mistaken application of the experimental analogy, and the more basic counterfactual analogy is the superior and more general one. It does make conceptual sense to imagine that, at any point in the lifetime of (say) an African-American, having experienced everything she has experienced up to that time, her skin color were changed to white (this is sometimes called a gedanken, or thought, experiment). Although it is a well-defined question, it may nevertheless be 4. Related to this last issue is the hope that \"how\" the treatment is assigned is irrelevant to the effect (\u03b2) on the outcome. If the effect of the putative cause is implementation specific, it is often more helpful to abandon the effort to find the effect of the putative cause and \"settle\" for the effect of the \"implemented cause.\" For example, if the effect of aspirin on headache differs when it is given to a patient by a nurse than when it is given to a patient by a doctor, the most we may be able to do is describe the causal effect of \"nurse administered aspirin\" or \"doctor administered aspirin.\" In the limit, of course, if only the method of administration matters we might even wish to conclude that aspirin qua aspirin doesn't cause anything to do with headache. At a very minimum, if such were the case, a debate about the causal effect of aspirin would be, at a minimum, unintelligible. 5. I would add, although this is not properly thought of as a \"requirement\", the most interesting studies involve manipulations that correspond to real policies. In these cases, even if we learn little about the \"structure\" of a true model, we have perhaps learned something about the consequences of one possible action we have taken. I do not mean to suggest by the foregoing that a RCT is always or usually the \"best\" evidence. Quite to the contrary, I don't even think that a singular focus on \"well-posed\" questions would be a good idea."}, {"section_title": "34", "text": "I would go even further and suggest that in many areas under study by economists, the focus on \"treatments\" can be, perhaps unintentionally, narrow. As Thacher (2001) observes, \"Reducing crime is clearly one important goal for the police. But it must compete with other goals like equity, due process, just desserts, and parsimony.\" Rather I argue that if a putatively causal question can not be posed as some sort of \"approximation\" to a question satisfying the above desiderata, the burden of explaining what is meant in plain language should be borne by the author. Too frequently, however, it is not. "}, {"section_title": "Just Because We Can Manipulate It Doesn't Mean We Can Learn About It", "text": "One of the serious problems with a focus on the RCT is the misleading view that we can always learn about causes from manipulations. Cartwright (2003a Cartwright ( , 2003b ) makes the point with greater generality than I can here. Rather I would like to focus on one class of problems with direct relevance for much of the research described in Freakonomics. My argument is simply that although we can learn about the effect of an intervention in a welldesigned study, we aren't guaranteed to learn about the putative cause in question, because the cause under consideration may be inextricably implementation specific. Consider the \"causal effect\" of obesity on all-cause mortality. The literature hardly seems to doubt that it is possible to measure such an effect, though there may be problems -perhaps BMI (Body Mass Index) is an inappropriate measure, for example.\nNonetheless, I would argue that it is unlikely that anyone will devise a severe test of the proposition that obesity causes an increase in all-cause mortality. Simply put, the effect of obesity (or of ideal weight) is inextricably implementation specific. That is, it is not helpful to think about the \"effect\" of obesity for the same reason it is not helpful to debate the \"causal effect of race on income\" (Granger 1986 ).\nMany of us suspect, for example, that encouraging obese individuals to \"starve themselves\" for short periods of time might help one lose weight, but wouldn't necessarily promote longevity (although it might, who knows? ).\nunanswerable, and it may not even be the main question of interest. For example, would the individual in question move to a different neighborhood, live in a different family, and go to a different school? If not, the question is not very interesting.\"\nWhile a distinction between comparisons one could make and those that are possible is important (I wish to think of manipulable quite broadly), I find such discussion confusing. If I were to wake up tomorrow and discover that my skin color had changed dramatically, one possible reaction might be a visit to the Centers for Disease Control to learn if I had acquired an obscure disease! Whether or not I moved to a different neighborhood, or divorced my wife, if that response were typical of other white folks who woke up one day to find themselves \"black\", I would nonetheless hesitate to say that the \"causal effect of being black\" (or white) is an increase in the probability that one makes a visit to the CDC [as above], though it could be so described. Again, absent some discussion of a class of counterfactual states and hypothetical manipulations, for me it is hard to know what to make of such causes, even when they can be defined. 34 In this regard, the philosopher Ian Hacking has done a great deal to show that useful work can be done in areas that vary quite widely in how well posed the questions are. For a study of statistical questions, see Hacking (1965) , the role of experimentation in natural science (Hacking 1983b) , multiple personality disorder (Hacking 1995) and the \"social construction of reality\" (Hacking 2000) , for example. 35 This point is not in anyway unique to me. For different, but not unrelated, views of these issues with relevance to social science, see Holland Similarly, we might expect weight loss that results from increased physical activity to be more protective than weight loss that results from increased life stress.\nThe experience in the U.S. with the drugs fenfluramine and dexfenfluramine (Redux) is a case in point. Despite good evidence that the causal effect of taking Redux was weight loss, the drugs were pulled from the market because a \"side effect\" of the medication was an increase in potentially serious heart problems (Food and Administration 1997) .\nIndeed, it would appear that the presumption that obesity is a cause of ill health made it virtually impossible to debate whether non-obesity was the cause of the increased heart problems. Rather, the consensus seems to be that the heart problems were not caused by non-obesity, but rather by Redux's \"side effects.\" 36 I don't want to argue that \"ideal weight\" is bad for one's health, only that this example highlights the fact that the effect of weight-loss or weight-gain is inextricably implementation-specific. If one accepts, this logic, much of the research claiming that being non-obese (and non-underweight) is causally related to better health demonstrates no such thing. Indeed, this literature is filled with \"anomalous\" results:. 37 Moreover, \"theory\" seems to provide little help: even the weakest research design often comes with an impressive theory. Cournot, et.al (2007) , for example, finds an \"association\" between obesity and lower \"cognitive functioning\" (verified by a simple cross-sectional design regressing measures of cognitive functioning and a small number of covariates and BMI) and posits one possible \"theoretical\" reason for why the link might \"causal\": \"direct action of adiposity on neuronal tissue through neurochemical mediators produced by the adipocyte\" (fat cell).\nMy point is simple: when each way of \"assigning\" obesity that we can imagine would be expected to produce a different effect on all-cause mortality or other outcome, it is not at all clear that it is helpful to debate the \"effect of obesity.\" It seems more intelligible (and more policy relevant) to discuss the effect of Redux or exercise than it is to talk about the \"effect\" of obesity."}, {"section_title": "How Much Do Parents Matter?", "text": "Though some of the \"interesting\" questions in economics might admit of a meaningful causal (or other) interpretation, one often hopes for more explanation than is provided in several of the examples in Freakonomics. Indeed, the obesity example above is arguably a bit clearer than the question they pursue in two chapters -\"how much do parents really matter? \" Let me begin with stating that there is much I agree with in the chapter: 1. The advice of \"parenting experts\" should be met with deep skepticism at best. 2. The research in Cullen, Jacob and Levitt (2003) justifies a long discussion (in my view, even more than the two pages the book provides. It is qualitatively several notches above most of the research done on school choice, evaluates an actual (not a hypothetical) policy, and is a marvel of clarity and honest reporting of results.) They exploit a randomized lottery that determines whether some children get to \"choose\" the public school they attend. Perhaps surprisingly, those who win the lottery perform no better on the usual measures of \"performance\" (and sometimes worse) than lottery losers. 3. Even though I can't come up with a simple \"experiment\" to test the hypothesis that \"honesty may be more important to good parenting than spanking is to bad parenting\" (Page 171), I think honesty is a good strategy (even if it didn't have a causal effect on a child's test scores; the salient issues have to do with ethical behavior.) In the setup to this discussion, Levitt and Dubner begin with a summary of previous work: \"A long line of studies, including research into twins who were separated at birth, had already concluded that genes alone are responsible for perhaps 50 percent of a child's personality and abilities.\" As any student of regression knows, this statement doesn't even make sense unless the world is of the simplest sort imagined by regression's eugenicist 36 I am merely stipulating to the existence of a distinction between \"effects\" and \"side effects\"; frequently the distinction seems to be based on marketing rather than scientific concerns.. 37 In an excellent but unfortunately unpublished study, Gronniger (2003) finds that if one includes more careful OLS type controls for income the putative effect of obesity is actually protective for many income groups. In arguing against viewing obesity as a \"causal\" factor in all-cause mortality, he also observes that the salient policy question \"is not what obesity does to people, but what removing obesity would do to people.\" A heavily abridged version of the article appears as Gronniger (2006). forefathers."}, {"section_title": "38", "text": "Obviously as careful as Cullen, Jacob and Levitt (2003) is, it is completely silent on this unanswerable question.\nMuch of the chapter, a discussion of Fryer and Levitt (2004b) (pages 163 -176) , is a long hike in a forest of confusion. Surprisingly, the authors use it to deliver a short tutorial about regression analysis (\"knowing what you now know about regression analysis, conventional wisdom, and the art of parenting\") and they spend a great deal of time discussing what is essentially a pair of \"kitchen sink regressions\" (regressions with enormous numbers of covariates) from Appendix A-2 of Fryer and Levitt (2004b) using data from the Early Childhood Longitudinal Study of test scores. In their presentation, they invite the reader to consider several things that are positively correlated with a child's test scores (presumably after conditioning on a huge laundry list of [unmentioned] variables): the child has highly educated parents, the child's parents have high socio-economic status, the child's birth mother was thirty or older at the time of her first child's birth, the child had low birth weight, the child's parents speak English in the house, the child is adopted, the child's parents are involved in the PTA, the child has many books in his home. As well as things that \"aren't correlated\" 39 :\nthe child's family is intact, the child's parents recently moved into a better neighborhood, the child's mother didn't work between birth and kindergarten, the child attended Head Start, the child's parents regularly take him to museums, the child is regularly spanked, the child frequently watches television, the child's parents regularly read to him every day. At some points, they seem to suggest that the results of this analysis speak to nothing causal: \"the ECLS data don't say that books in the house [or any of the variables in their analysis] cause high test scores; it says only that the two are correlated.\" Elsewhere they seem to suggest the opposite:\nNow a researcher is able to tease some insights from this very complicated set of data. He can line up all the children who share many characteristics -all the circuit boards that have their switches flipped in the same direction -and then pinpoint the single characteristic they don't share. This is how he isolates the true impact of that single switch -and, eventually, of every switch -becomes manifest. (Page 162) I would maintain that, even allowing for the simplification of the argument for a general audience, this is a bad description of what makes for credible research -nothing is being severely tested.\nFor example, whatever one thinks of Head Start, accepting Dubner and Levitt's observation that \"according to the [kitchen sink regression using] ECLS data, Head Start does nothing for a child's future test scores\" seems unwise at best. The research design can not credibly support that inference. To make this clear, consider other inferences (though undiscussed in Freakonomics) from the same regressions. Why not, for example, observe that participation in WIC (Women, Infants, and Children) significantly lowers test scores?\n40 . Perhaps such assistance actively harms children. I would argue that the good reason for avoiding that inference works just as well as a rationale for avoiding the inference they do make about Head Start: there is no reason to believe that (conditional on the other non-randomly assigned regressors) that a coefficient in a kitchen sink regression reliably informs us about causation in any sense.\nAgain, even kitchen sink regressions have their place: one can sometimes make a case for inclusion of scores of covariates in some very selected contexts. However, an algorithm which allows the researcher to decide which coefficients represent \"causal\" effects and which ones are regression artifacts after one has seen the regression output is unlikely to result in much progress in understanding. It is the very antithesis of a severe test. 38 Suppose the world was as complicated as Behavior=G+E-G\u22c5E where G is some index summarizing \"genes\" and E is some index summarizing \"environment.\" In this simple example, the fraction of variation in behavior induced by differences in genes isn't separable from the environment -indeed, the effect of genes is a function of the environment. In some environments, introducing differences in genes would introduce little change in behavior, and in some environments it would change behavior a lot. For a useful discussion that addresses this and other related points, see Heckman (1995) . 39 I think they mean \"so imprecisely estimated that a null hypothesis of no correlation can not be rejected using standard procedures.\" 40 From Appendix A-2, when the dependent variable is Math Score the coefficient on WIC is -0.120 with a standard error(0.020). When the dependent variable is reading scores, the coefficient on WIC is -0.104 with a standard error (0.021)."}, {"section_title": "Can Regression Help Distinguish \"Cause\" from \"Consequence\"?", "text": "Chapter 6, \"Perfect Parenting, Part II; or: Would a Roshanda by Any Other Name Smell as Sweet? \" begins this way:\nLevitt thinks he is onto something with a new paper about black names. He wanted to know if someone with a distinctly black name suffers an economic penalty. His answer -contrary to other recent research -is no. But now he has a bigger question: Is black culture a cause of racial inequality or is it a consequence? For an economist, even for Levitt, this is new turf -\"quantifying culture\" he calls it. As a task, he finds it thorny, messy, perhaps impossible, and deeply tantalizing. As with eugenics, the history of social science suggests that scholarly research into race that makes extensive use of correlations should be taken with a large grain of salt.\n41 When talking about race, it is my view that being clear about what is meant is even more important.\nAs someone who is frequently called upon as an econometric \"script doctor\" to \"fix the econometrics\" of some existing paper which is putatively about \"causation\", I have found it useful to begin with two seemingly simple questions:\n1. What is y, the outcome, you wish to explain? 2. What are your key x variables and what potential \"causes\" or \"interventions\" are you interested in? As a practical matter, the inability to provide a simple reply to the question is a good predictor of my inability to understand the empirical work.\nThe above quote from Freakonomics is in a chapter which, inter alia, discusses research from Fryer and Levitt (2004a) and (far more briefly) Bertrand and Mullainathan (2004) . In Fryer and Levitt (2004a) much of the evidence on whether \"black names\" are cause or consequence comes from two types of regressions involving their measure of \"black culture\" the \"Black Name Index\" (BNI)."}, {"section_title": "42", "text": "It is not clear whether the BNI is an x or a y: superficially, it would appear that they run the regressions \"both ways\": in one set of regressions, BNI is an independent variable, in a second set, it plays the role of a dependent variable. As is well-appreciated, this is a problem even when it occurs in different literatures (Kennan 1989) .\nFurther inspection suggests that this is not strictly the case: in the first set of regressions (see Table II \"Determinants of name choices among blacks\" of Fryer and Levitt (2004a) ) the dependent variable is the BNI of a given child, and the explanatory variables are a number of things, many of which are presumably correlated with outcomes (mother's age at time of birth, father's age at time of birth, months of prenatal care, percentage of Black babies in zip code, per capita income in the birth place, parental education, etc.). In another set (Table III \" The Relationship Between Names and Life Outcomes\"), BNI becomes an explanatory variable, and the dependent variables are outcomes such as \"percent Black in residential zip code as an adult\", years of education (the woman herself), the woman's age at first birth, etc. Fryer and Levitt (2004a) , are forthright in admitting that their evidence is consistent with a number of very plausible (but very different) alternatives that are consistent with their regressions but not necessarily with their conclusion: \"With respect to this particular aspect of distinctive Black culture, we conclude that carrying a black name is primarily a consequence rather than a cause of poverty and segregation.\"\nI have no wish to dispute their conclusion; rather, I wish to suggest that there is no configuration of the data of which I am aware which would credibly support the view held by Fryer and Levitt and not support very different alternatives. In short, this is because it is very difficult to know what is being asked and what would constitute an 41 The most notorious example perhaps is the controversy over the 1840 census which involved the putative negative correlation between the number of \"insane and idiotic colored persons\" living in a state and the proportion that were slaves. The data, whichare still available today from the ICPSR show that incidence of insanity was far, far lower in the South, and the implication for the debate on slavery was clear (Grob 1978) . (A far different version of \"acting white\" is mentioned several times in Freakonomics.) 42 I am stipulating, of course, that Levitt and Fryer's measure of \"distinctiveness\" of a \"Black\" name (BNI) -crudely put a function of the relative frequency with which a specific name is chosen for black children and the relative frequency with which the same name is chosen for white children -provides a measure of whatever \"culture\" is.. A lot of non-obvious measurement issues arise. A few moments reflection, for instance, makes clear that the level of \"black culture\" is, by definition, a function of \"white\" culture although one doubts this research design would have found much appeal as a study of \"white culture.\" Second, a white man named Maurice Ravel might be measured as have more black culture than a black man named Paul Robeson, Jr. regardless of their actual \"culture\" if Maurice was relatively more popular among blacks than Paul.\nanswer. Put differently, there is at least one ill-posed question floating about. Is it possible to talk meaningfully about \"manipulating\" culture? (And if one could, would one want to?) 43 Might reasonable people agree on some variable or policy that served exclusively to manipulate black culture and affected economic outcomes only through its effect on \"culture?\" It is not even clear that \"culture\" and \"economic outcomes\" or \"racial inequalities\" are distinct entities. Indeed, as the word is often understood, culture often includes the distribution of \"economic outcomes.\" For instance, one might remark: \"the fact that Bill Gates earns several times more in a year than the sum earned by all Chicago Public School teachers is a distressing fact about U.S. culture.\"\nFurther muddling the issue is the way Levitt and Dubner discuss studies such as Bertrand and Mullainathan (2004) :\nSo how does it matter if you have a very white name or a very black name? ...In a typical audit study, a researcher would send two identical (and fake) r\u00e9sum\u00e9s, one with a traditionally minoritysounding name, to potential employers. The \"white\" r\u00e9sum\u00e9s have always gleaned more job interviews. ...The implication is that black-sounding names carry an economic penalty. Such studies are tantalizing but severely limited, for they can't explain why [someone with a black sounding name like] DeShawn didn't get the call. (page 186) Even if one stipulates to the claim of a severe limitation because such studies can't explain \"why\" (although the concern for \"why\" is not pressed very hard elsewhere in Freakonomics regarding the motives of Sumo wrestlers or school teachers, for example) Bertrand and Mullainathan (2004) clearly explain that they are not interested in the lifetime \"economic cost\" of a black sounding name -which is not obviously an interesting or well-posed question. Rather they are interested in \"experimentally manipulat[ing] [employer's] perception of race.\" Unlike \"culture\" or an individual's \"black name\" Bertrand and Mullainathan's question seems well-posed: it is much easier to conceive of a salient experiment manipulating \"perceptions\" than a salient experiment manipulating the naming decisions of parents. One can argue that the causal effect of manipulating perceptions of race is \"uninteresting\" on a number of grounds, not the least of which is that the manipulation itself doesn't suggest an intervention we might wish to undertake as a society. On the other hand, in contrast with some experiments in \"experimental economics\" their study is embedded rather more deeply in \"real life\" than experiments that occur in a lab. Nonetheless, the question seems well-posed and may be answerable with regression, even if one wants to argue that it is uninteresting on other grounds. 44 Second, although Dubner and Levitt are correct to argue that studies involving r\u00e9sum\u00e9 randomization are unlikely to provide convincing evidence on \"why DeShawn gets fewer callbacks\", it is not clear what a satisfactory explanation of \"why\" would look like. It is even harder to understand how the type of regressions performed in Fryer and Levitt (2004a) would, in principle, be relevant to this discussion. (Again, they might be, but the link is not obvious to me.) Perhaps like Dr. Pangloss, we could trace Jamal's bad luck with employers to necessity: it is necessary for this to be the case, for us to be able to live in this the best of all possible worlds.\nMore generally, reasoning backward from a single effect (not calling back Jamal) to a \"cause\" (why employers don't call Jamal) in social science is generally fraught with peril; people are complicated enough that there is rarely a single answer to the question \"why\" -often there are many interacting \"reasons.\" Absent some fairly articulated model of how the world works, it seems difficult even to know what would constitute a good answer. A severe test of the claim seems even more unlikely. Moreover, it often seems that putative explanations of \"why\" some complex human interaction occurs are frequently used as a device to end a debate just at the point when the issue begins to get interesting. If X is the reason Y occurs, why look further? Many readers might be familiar with this aspect of some answers to \"why\" questions: one thinks of a parent who tries to end a long conversation with a child who, in response to a parent's increasingly complicated responses, keeps asking \"Why? \" Again it is not that a satisfactory answer to such question is not desirable: it just seems like way too much to hope from a small set of OLS regressions.\nFinally, in asking a regression to distinguish \"black culture\" as a cause from black culture as a consequence of economic conditions, we are very far from the types of questions I discussed in section 3. But there is no clear discussion in Freakonomics of what question is being asked nor the \"ground rules\" that we might use to determine when the question has been answered satisfactorily. It is possible that the question is well posed, but at a minimum, it is not very obvious. After reading Freakonomics and the original source material, I haven't gained any understanding of the issues involved or even how to think about what are the answerable questions."}, {"section_title": "Why a Transparent Research Design Helps -Abortion as a \"Cause\"", "text": "For me the most confusing section of Freakonomics is the discussion of \"Why do drug dealers live with their moms\" and \"Where have all the criminals gone?\" Between them, the chapters contain references to scores of articles of varying degrees of scholarship. Much of the former chapter discusses Levitt's work with sociologist Sudhir Alladi Venkatesh who collected a large amount of detailed data on one Chicago gang. For those surprised as to why gang members don't frequently live in the nicest homes in town, it will be a useful corrective. (For an earlier discussion that covers similar ground see Reuter, MacCoun and Murphy (1990) .) The discussion also includes the conclusions of some very careful work by Almond, Chay and Greenstone (2003) that document the key role that hospital integration in Mississippi played in improving the appalling infant mortality rate of black children -before integration, these infants were often left to die of very preventable causes such as diarrhea and pneumonia.\nMuch of the chapter on \"where have all the criminals gone? \" deals with Romania's abortion ban, which I have discussed elsewhere (DiNardo 2006a) . This chapter also includes the controversial material on whether \"abortion lowers crime rates.\"\nAs a purely personal matter, given the long, deep, and ugly relationship between statistical analysis and eugenics, what might emerge from this debate seems too meager to justify the effort on this subject. I don't find the question \"interesting.\"\n45 Merely participating in the discussion one runs the risk of coarsening the debate on how we treat the poor -the usual the target of eugenic policies. 46 Caveats aside, here goes. 45 Eugenics, often popular among \"progressive\" members of the elite, was a leading motive for the development of regression. Sir Francis Galton, who gave us the word \"regression,\" was an ardent eugenicist. For example, what is now the \"Galton Laboratory, Department of Human Genetics and Biometry\" at University College London, was originally named the \"Galton Laboratory of National Eugenics.\" 46 Indeed, the debate has grown coarser. William Bennett, a former government official, after appearing to dismiss the \"abortion -crime\" hypothesis in Freakonomics, remarked in a talk show that: \"I do know that it's true that if you wanted to reduce crime, you could -if that were your sole purpose, you could abort every black baby in this country, and your crime rate would go down. That would be an impossible, ridiculous, and morally reprehensible thing to do, but your crime rate would go down.\" I of course agree that \"it would be a morally reprehensible thing to do.\" On the other hand, the premise that \"you could abort every black baby in this country and the crime rate would go down\" is unsupportable at best, racist at worst. Levitt's thoughts on the subject (as well as a transcript of the relevant portion of Bennett's remarks) are available at the website http://www.freakonomics.com/2005/09/bill-bennett-and-freakonomics.html.\nFor what it's worth, Levitt's remarks are admixture of what strike me as reasonable assertions and others that are confusing at best, wrong at worst. For example, on his blog www.freakonomics.com Levitt argues:\n... 6 \"If we lived in a world in which the government chose who gets to reproduce, then Bennett would be correct in saying that \"you could abort every black baby in this country, and your crime rate would go down.\" Of course, it would also be true that if we aborted every white, Asian, male, Republican, and Democratic baby in that world, crime would also fall. Immediately after he made the statement about blacks, he followed it up by saying, \"That would be an impossible, ridiculous, and morally reprehensible thing to do, but your crime rate would go down.\" He made a factual statement (if you prohibit any group from reproducing, then the crime rate will go down), and then he noted that just because a statement is true, it doesn't mean that it is desirable or moral. That is, of course, an incredibly important distinction and one that we make over and over in Freakonomics. 7 \"There is one thing I would take Bennett to task for: first saying that he doesn't believe our abortion-crime hypothesis but then revealing that he does believe it with his comments about black babies. You can't have it both ways.\"\nAs far as I can tell the Levitt's statement about lowering the level of crime by aborting Native American, Republican, ...fetuses is a nonsequitor at best. Bennett is clearly talking about the rate of crime. I can only make sense of the statement by construing it to mean that ridding the planet of human life would eliminate crime (at least that caused by humans).\nAs to the rest of the explanation: \u2022 Levitt gives no reason to believe that \"if we lived in a world in which the government chose who gets to reproduce, then Bennett would be correct in saying that 'you could abort every black baby in this country, and your crime rate would go down.'\"\nIn their original article, Donohue and Levitt (2001) cite two possible \"theories\" about the consequences of abortion legalization. Neither of them fit well into the framework described in section 3. 47 The \"effect\" of abortion legalization on crime, of course, is a whole different matter. Donohue and Levitt (2001) discuss two possible mechanisms at length. Donohue and Levitt (2001) first argue that \"The simplest way in which legalized abortion reduces crime is through smaller cohort sizes.\" 48 While possibly \"simple\", it is amazingly difficult to articulate clearly in a regression framework where the unit of observation is the individual. At its core this hypothesis appears to include the implicit assertion that among other things, my mother's decision not to abort the fetal John DiNardo caused some other children's propensity to commit crime to increase. (Although it should be said, it clearly raised mine! ) Such effects are difficult to identify, even in the easiest cases (Manski 1993) .\nA far more subtle mechanism is distinct from the first, although it could certainly interact with it. \"Far more interesting from our perspective is the possibility that abortion has a disproportionate effect on the births of those who are most at risk of engaging in criminal behavior.\" Donohue and Levitt (2001) Even if we could agree that the effect of abortion legalization is independent of other aspects of the society (access to birth control, women's rights in other spheres, etc.), for anyone who has given the problem of \"missing data\" some thought, it is difficult to be sanguine about the possibility of inferring much about the criminal propensities of those who are never born. Even in the context of a medical RCT, the analogous problem of attrition is often distressingly difficult to cope with. Moreover, the problem is so difficult that in the RCT one often abandons hope of modeling non-response or sample selection and seeks merely to bound the difference between the treated and control groups (Horowitz and Manski 1998) .\nMoreover, as Donohue and Levitt (2001) observe, there are many mechanisms besides abortion either to stop the \"criminogenic\" fetus from being born or to prevent the child from becoming a \"criminal\" once born.\nEquivalent reductions in crime could in principle be obtained through alternatives for abortion, such as more effective birth control, or providing better environments for those children at greatest risk for future crime. Donohue and Levitt (2001) Ironically, this observation points to a lot of (unasked) questions which are interesting and might conceivably be put to a severe test. The focus in Freakonomics unfortunately is elsewhere:\nHow, then, can we tell if the abortion-crime link is a case of causality rather than simply correlation?\nOne way to test the effect of abortion on crime would be to measure crime data in the five states where abortion was made legal before the Supreme Court extended abortion rights to the rest of the country ...And indeed, those early-legalizing states saw crime begin to fall earlier than the other fortyfive states and the District of Columbia. Between 1988 and 1994, violent crime in the early-legalizing states fell 13 percent compared to the other states; between 1994 and 1997, their murder rates fell 23 percent more than those of the other states. (page 140) Of the identification strategies employed in this literature, this is the most transparent. To understand what is going on, assume that pre-Roe legalization provided a Brandeisian natural experiment. Instead of the individual being the unit of observation, think of each state as a sort of identical petri dish to which a drop of abortion legalization is being added. Fifteen to twenty five years later, the petri dishes will be checked again to see how"}, {"section_title": "\u2022", "text": "Contrary to Levitt's claim, I do not think it necessary to believe that the termination of black fetuses would lower the crime rate even if the \"causal effect of abortion legalization\" in the U.S. had been a reduction in crime. As I explain below, even if one stipulates that crime reduction was a causal effect of abortion legalization in the U.S. this would tell us nothing about the causal consequences of aborting black (or any) fetuses. 47 One could conceive of cases where abortion might be thought of (for better or worse) as a treatment: that is generally true when the subject of interest was child-bearing women (not their fetuses). The question of what happened to the welfare of women who are given the choice of having an abortion relative to those that have been denied such choice, is well posed. One merely would seek to compare a group of women given the opportunity to have an abortion to those who did not. Even putting aside the serious ethical questions, this is much easier said than done (and indeed is the subject of much of the pre- Donohue and Levitt (2001) work by economists on the consequences of abortion legalization). 48 I have not been able to figure out what role this hypothesis plays in the empirical work. See Foote and Goetz (2007) for an attempt to make sense of this; they come to different conclusions than Donahue and Levitt (2001.) much per capita crime is occurring. If legalization had been an actual experiment (perhaps run by a dictator), we might have expected half the states to be legalizers and the other half to never legalize (assume that items in the petri dishes can't jump into other petri dishes.) That of course did not happen. In this case, the experimenter added a drop of legalization to 5 states in 1970, and then added a drop to the remaining states a scant three years later. Of course, it wouldn't be clear that even in this experiment you could detect an \"effect\" on crime, unless the effect was large relative to the variation across the petri dishes expected in the absence of any experiment."}, {"section_title": "49", "text": "Though one would not know from reading Freakonomics, Donohue and Levitt (2001) argue that this research design is inadequate.\n50 Consequently, much of this is beside the point. Donohue and Levitt (2001) argue that evidence from such a research design is only \"suggestive.\"\nThe bulk of their argument centers on their attempts to \"more systematically\" analyze the relationship with an analysis of state level crime data on lagged \"abortion rates.\"\nConsider equation (1) \nThey label A t the \"effective abortion rate.\" The \"a\" subscript denotes a particular \"age\" group. 51 Using data on state s at time t, they then divide this by the number of live births to get an \"effective abortion ratio\": (2) where each observation is the relevant state/year average or value. The st X are a set of covariates, \u03b3 s are a set of state dummy variables and \u03bb t are a set of year fixed effects. The \u03b5 is a random disturbance that is presumably uncorrelated with any of the regressors. Up to a constant that differs by states, absent variation in X or the (modified) abortion ratio, it is assumed that trends across state in crime would be the same."}, {"section_title": "52", "text": "Stipulating that all of the data used to generate this specification are fine 53 , I find it impossible to interpret the coefficients at all. In common econometric parlance, the abortion ratio is \"endogenous.\" Indeed, some work has 49 Indeed, this or similar identification strategy is employed in such work as Charles and Stephens (2006) , Gruber, Levine and Staiger (1999) , Bitler and Zavodny (2002) , as well as Joyce (2004) . Gruber, Levine and Staiger (1999) detect a rather small (and brief) effect on the total number of children born from this identification strategy. Note of course, that such an experiment would provide us essentially no information on the \"mechanisms\" -the \"effect of abortion legalization\" could be a complicated interaction of many things having little to do with selective abortion or cohort size per se. Merely the option of having an abortion might change outcomes for many reasons. 50 They argue against the identification strategy both on a priori grounds and on ex post grounds (the implausibility of the results so obtained).\nIn Donohue and Levitt (2001) , for example, when they deploy that identification strategy, they report that \"the cumulative decrease in crime between 1982-1997 for early-legalizing states compared with the rest of the nation is 16.2 percent greater for murder, 30.4 percent greater for violent crime, and 35.3 percent greater for property crime. Realistically, these crime decreases are too large to be attributed to the three-year head start in the early-legalizing states.\" The reservations in Donohue and Levitt (2001) about the estimates generated with this identification strategy do not appear in Freakonomics which selectively discusses some comparison between early and late legalizing states. 51 The asterisk appears to be undefined in the text and may be a typographical error. 52 As it turns out, the description of the regressions in the text and the actual regressions run are not always the same. See Foote and Goetz (2007) . 53 This is perhaps more than we should stipulate to: our knowledge of the number of illegal abortions today or abortions that preceded abortion legalization in the 1970s is meager at best. Moreover, Donohue and Levitt (2001) and other researchers typically do not have data on the looked at the effect of economic and other conditions on abortion (Blank, George and London 1996) : that is, something akin to A is the dependent variable in the regression. Donohue and Levitt (2001) , however, spend surprisingly little time discussing the issue."}, {"section_title": "54", "text": "What are the \"ground rules\" that a skeptical, but persuadable person should use for evaluating this regression? Other than that \"the coefficients look reasonable\", what would speak to the credibility of the research design, or what should lead me to reject it?\nNot obvious is the notion that we should be reassured about the existence of an \"abortion-crime\" link because the OLS coefficient on A in a regression like equation (2) is robust to the inclusion of some covariates or slight modifications of the sample. One \"intuition\" that motivates investigating whether a result is \"robust\" to the inclusion of a large number of explanatory variables comes from the RCT. On average, if we repeat the experiment, the answer we get from including covariates and from excluding covariates should be the same.\nOn the other hand, clearly it makes no sense to think of A as \"randomly assigned.\" Indeed, if abortion legalization is all about \"selection\" -i.e. the difference in the crime propensities of those born and those not bornpure random assignment of abortion (a thought too grotesque to even contemplate) would not merely leave the statistical problem unsolved; it would answer a different (even more uninteresting) question. For example, in one version of the Donahue-Levitt story, abortion matters for crime because it is the consequence of choice made by women to selectively abort some fetuses and not others. \"Random abortion\" would, on the other hand, would produce no \"selection effect\" -studying such \"random\" variation in abortion ratios would be silent about the putative effects of legalizing abortion.\nIf thinking about the regression as an approximation to some sort of RCT doesn't help, how is one to even assess or interpret the specification? What covariates \"should\" be included? Missing from this research is either a similarity to the simple type of question I described in Section 3 or an explicit model of the link between abortion legalization and cohort size. With an explicit structural model, one might in principle be able to wrap one's mind around what question is being asked. The mere presence of an explicit model might help, although we would still be faced with the task of putting it to some sort of test. Absent that, it is hard to understand why this (or similar evidence) should persuade anyone (one way or the other.)"}, {"section_title": "Catching Cheaters", "text": "I have suggested that a focus on actual policies and their potentially predictable consequences often goes a long way to making some queries intelligible. Even where questions about causes are not the only ones in question, such considerations can remain important.\nOne example is the discussion entitled \"What Do School Teacher's and Sumo Wrestler's Have in Common\", the authors discuss some work by Levitt on detecting \"teacher cheating.\" In the telling, the cast of heroes includes the CEO of the Chicago Public School system, and the villains include the school teachers and their labor union (\"When [Duncan] took over the public schools, his allegiance lay more with the schoolchildren and their families than with teachers and their unions.\")\nThe basic method is to analyze the pattern of test answers. Answers that depart from the posited (ad hoc) data generation process are flagged as \"cheating.\" For obvious reasons, at no point in the process are actual data on observed teacher cheating used. As a consequence, the algorithm described has no way of discriminating between the case in which a teacher selectively \"corrects\" a subset of answers for a class, from those cases in which the students (unknown to the teacher) have obtained copies of a subset of the answers, to name one (perhaps unlikely) situation. At a most basic level, of course, there is no perfect way to \"detect teacher cheating\" with statistical analysis 55 amount of crime committed by individuals of a given age. At best one has very crude proxies. See Charles and Stephens (2006) or Joyce (2004) for discussion. 54 In the published version of the paper, the word \"endogeneity\" appears only regarding a discussion of two right hand side variables -number of police and prisons -which are \"lagged to minimize endogeneity.\" The word \"exogeneity\" appears in a confusing discussion about the difference between high and low abortion states (page 401). 55 To make this clear, consider an analysis made by officials responsible for New York's Powerball lottery. In the March 30, 2005 drawing, a startling number of persons (110) got five out of six numbers correct. According to a news report (Lee 2005) , past experience with the lottery had led them to believe that in the 29 states where the game is played, the average number of winners would be more like four or five. After considering numerous hypotheses including fraud, lottery officials finally concluded that some of the winners had chosen their number on the Indeed, the chapter indicates that the \"teacher cheating\" algorithm was not the sole method used to assess guilt (as one hopes) but remarks with little further curiosity that \"the evidence was strong enough only to get rid of a dozen of them.\" Given the rest of the discussion, this might come as quite a surprise. Why would such a clever algorithm work so poorly in a situation when there was so much cheating?\nAnything but a perfect \"test\" for the existence or \"non-existence\" of something (a virus, for example) commits two types of error -in unhelpful terminology, Type I and Type II.I find the legal metaphor the easiest way to remember the distinction. The legal system in the U.S. putatively attempts to minimize Type I error -sending an innocent person to jail. Type II error is the opposite mistake -exonerating the guilty. In practice, there is a tradeoff between the two types. One way to avoid Type II error is to declare everyone guilty; declare everyone innocent and one avoids Type I error at the expense of Type II error. This is of course relevant if one is interested in the causal impact of implementing a cheating detection algorithm. Here I focus only on the narrowest causal question: how many innocents are punished by such a system? There are others to be sure.\nIf the fact that only a \"handful\" were caught is a surprise to the reader, it wouldn't be a surprise to those familiar with the findings of Tversky and Kahneman (1974) who argue that people are frequently inattentive to \"base rates\" (although that interpretation is the subject of a lively debate.) The canonical problem can be illustrated by making a few assumptions about the algorithm discussed in Freakonomics. Suppose that the probability of one's cheating being detected, given that you cheat is 0.90 -the probability of Type II error is .1. Also assume that the probability the algorithm incorrectly identifies you as a cheater when you are not is .06 -Type I error. Further suppose that 4 percent of teachers cheat -this is the crucial \"base rate.\" Slightly more formally: I wasn't able to locate the actual numbers in Freakonomics and the ones I have chosen seem a bit optimistic for the algorithm they describe (albeit a bit pessimistic about the fraction of cheating teachers). If they were correct, however, it would explain why only a handful of those identified by the algorithm were finally identified as cheaters -despite the large pool of potential cheaters. Many statistically naive readers might conclude that virtually all of those identified as guilty were indeed guilty. The test looks pretty accurate. Few detected cheaters are innocent, and cheaters have a good chance of being caught. However, even in this example, of the roughly 9 percent of teachers classified as cheating on the basis of the algorithm, the majority (about 62 percent) would actually be innocent. This strikes me as a frighteningly high percentage, but perhaps others will disagree. 56 A more thoughtful analysis would go even further: does it treat different but morally homogeneous groups differently? It would almost certainly give one a moment's pause if an algorithm was only (or mostly) able to detect cheating among the lowest paid teachers with the most difficult students, as well as being scarcely able to detect cheating among the most affluent. Freakonomics unfortunately discusses none of these issues."}, {"section_title": "The Power of Theory", "text": "If what we mean by theory is an articulation of the premises of the questions we are attempting to answer, it is fair to say that a little bit of theory can go a long way.\nDespite Freakonomics' various encomiums to the \"powerful toolkit of economics\" (apart from \"regression\" which manages to rise to a level no higher than \"art\") I could detect very little in the research that depended on \"economic theory\" in anything but the most superficial way. Perhaps that is all for the best. Nancy Cartwright (1999) makes a convincing argument that the \"trouble with [more mathematically sophisticated theory basis of a fortune cookie. Lottery investigators finally even managed to locate the fortune cookie maker who verified that his factory had produced the fortune cookies with the winning number. 56 The calculation is: 1-P(C|D)=1-(P(D|C)\u22c5P(C))/(P(D|C)\u22c5P(C)+P(D|~C)\u22c5(1-P(C)))=0.615 . It should also be noted that the usual way to minimize this problem is to test the teacher more than once: this works, of course, only in the highly improbably case that one might consider errors from the proposed procedure as independent. models] is not that [they are] too rigorous, but rather ...not rigorous enough.\" Unrealistic assumptions are necessary in all theorizing, but the problem is that the alleged \"tendencies\" isolated in such models are usually modeldependent in a way that vitiates any \"external validity.\" 57 It's one thing to find that a model helps illuminate some small aspect of human responses in a few contexts. In my field of labor economics, for example, the canonical labor-leisure model might help me predict how working mothers in female headed households adjust their hours of market work to a small change in an existing government welfare program, but I wouldn't use the model to tell me much else about working mothers.\nIf were one to judge Economics by reading Freakonomics alone, however, it would appear that some economists are successfully going after much bigger game, with economic theory leading the way. How else could a study about professional Sumo speak to anything but the unusual context of Sumo? Presumably some \"model\" takes us from a finding for wrestlers to something of more general interest. On the other hand, all we learn about economic theory is that it appears to be the premise that \"incentives matter.\" Whatever enthusiasm one might have for the power of that insight, it is not clear what an incentive \"is.\" The helpful index to the book lists the following: incentives, bright line versus murky, as a cornerstone of modern life, criminal, definitions of, discovery and understanding, economic, of experts, invention and enactment of, moral, negative versus positive, power of, of real estate agents, schemes based on, of schoolteachers, social, study, tinkering with, trade-offs inherent in. Indeed, in Dubner and Levitt's hands, the assertion that incentives are the \"cornerstone of modern life\" often comes off as a two part tautology. The first part of the tautology is: \"when incentives matter, they matter.\" The second part of the tautology is that when incentives don't matter, it is because of \"moral incentives.\"\nLess than a theory, perhaps, it describes a world view that evokes a sort neo-Skinnerian behaviorism that in popular writing was most cogently demolished by Chomsky (1971) . For example, it was quite easy for me to get confused, when reading Freakonomics, about whether negative and positive incentives were merely synonyms for the Skinnerian notions of negative and positive reinforcement.\nPerhaps I read more into the use of the word incentives than is there. However consider Dubner and Levitt's description of the \"typical economist's view\" of incentives:\nEconomists love incentives. They love to dream them up and enact them, study them, and tinker with them. The typical economist believes the world has not yet invented a problem that he can not fix if given a free hand to design the proper incentive scheme. His solution may not always be pretty -it may involve coercion or exorbitant penalties or the violation of civil liberties -but the original problem, rest assured, will be fixed. An incentive is a bullet, a lever, a key: an often tiny object with astonishing power to change a situation. Nonetheless, as elastic as the notion of incentives is, I think it is still way too narrow. Speaking of B.F. Skinner's views of the power of \"reinforcement\", Chomsky's words about B.F. Skinner seem to apply with equal force to Dubner and Levitt's typical economist:\nHumans are not merely dull mechanisms formed by a history of reinforcement and behaving predictably with no intrinsic needs apart from the need for physiological satiation. Then humans are not fit subjects for manipulation, and we will seek to design a social order accordingly. (Chomsky 1971) I do not mean to suggest that Dubner and Levitt believe that humans are merely \"dull mechanisms\" formed only by a history of \"incentives.\" My point is merely that as a framework for understanding human behavior the typical economists' focus on \"incentives\" ignores much that is important. Buchanan, for example, writes that any person's ideal situation is one that allows him full freedom of action and inhibits the behavior of others so as to force adherence to his own desires. That is to say, each person seeks mastery over a world of slaves 58 I hope I never live to meet this sort of homo economicus. At a minimum, Buchanan's ideal appears more like a dystopian nightmare, notwithstanding the fact that his description of homo economicus follows logically from his 57 I have done considerable violence to her argument by this very brief summary. See \"The Vanity of Rigour in Economics: Theoretical Models and Galilean Experiments\" (1999) for a useful philosophical perspective. 58 Buchanan (1975) page 92; Chapter 6, \"The Paradox of 'Being Governed'\" at Buchanan (1999) http://www.econlib.org/LIBRARY/Buchanan/buchCv7Contents.html. premises about human motivation. It also highlights the problem with the view that \"theory is evidence too\" (June O'Neill as cited in Deaton 1996) -most commonly our models are perfect environments to do \"experiments\" on people we would never hope to meet, in situations in which they could never find themselves. If even \"the laws of physics lie\" 59 and physicists often fruitfully use different and mutually inconsistent models from the same theory, it seems far more modesty about economic theory/models is warranted than Freakonomics seems to suggest. This is not to deny that simple economic models can be put to good use, but economic theory -whatever it is -shouldn't be epistemologically privileged. Even if social scientists were to learn something \"deep\", \"fundamental\", or \"primordial\" about human behavior that was previously unknown to the skilled novelist, it is unlikely to inform us very much about the type of policies we might like to pursue. As Lecam (1977) observed about a genuine example from real science, \"even those physicists who are most fascinated by the kinetic theory of gases would hesitate to use it to compute the size of wood beams for their own abode.\" Simply put questions about the predictable consequences of our actions are not well-answered by untested or untestable insights from some \"general\" economic theory; rather we might learn a little about the predictable consequences of our actions-if we are lucky -by formulating ideas that can be put to a test."}]