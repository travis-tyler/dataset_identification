[{"section_title": "Abstract", "text": "Deformable registration is ubiquitous in medical image analysis. Many deformable registration methods minimize sum of squared difference (SSD) as the registration cost with respect to deformable model parameters. In this work, we construct a tight upper bound of the SSD registration cost by using a fully convolutional neural network (FCNN) in the registration pipeline. The upper bound SSD (UB-SSD) enhances the original deformable model parameter space by adding a heatmap output from FCNN. Next, we minimize this UB-SSD by adjusting both the parameters of the FCNN and the parameters of the deformable model in coordinate descent. Our coordinate descent framework is end-to-end and can work with any deformable registration method that uses SSD. We demonstrate experimentally that our method enhances the accuracy of deformable registration algorithms significantly on two publicly available 3D brain MRI data sets."}, {"section_title": "INTRODUCTION", "text": "Image registration or image alignment is the process of overlaying two images taken at different time instants, or different view points, or from different subjects in a common coordinate system. Image registration has remained a significant tool in medical imaging applications [1] . 3D data in medical images, where image registration is applied, generally includes Computed Tomography (CT), Cone-beam CT (CBCT), Magnetic Resonance Imaging (MRI) and Computer Aided Design (CAD) model of medical devices.\nAmong all the image registration methods, deformable image registration is important in neuroscience and clinical studies. Diffeomorphic demons [2] and Log-domain diffeomorphic demon [3] algorithms are popular deformable registration methods. In these optimization-based methods, the deformable transformation parameters are iteratively optimized over a scalar valued cost (e.g., SSD) representing the quality of registration [4] . To impose smoothness on the solution, typically a regularization term is also added to the registration cost function. These costs are non-convex in nature, hence the optimization sometimes get trapped in the local minima. On the basis of the cost functions, different optimization algorithms are used. In the Gauss-Newton method for minimizing SSD, projective geometric deformation is used [5] . The method is sensitive to local minima. Levenberg-Marquadt algorithm was used in [6] to minimize the difference in intensities of corresponding pixels. This method update the parameters between gradient descent and Gauss Newton and accelerates towards local minima. The combination of LevenberMarquadt method and SSD is used in [7] .\nIn order to improve the solution of registration, in other words, to find better local minima in the SSD cost, we propose a novel method to modify the reference image by introducing a heatmap (essentially another image) produced by a Fully Convolutional Neural Network (FCNN) with a skip architecture [8] . This modified reference image helps to create a tight upper bound to the SSD registration cost that we refer to as UB-SSD. Next, we minimize the UB-SSD by adjusting parameters of the FCNN as well as deformation parameters of the registration algorithm. We refer to our proposed method by deep deformable registration (DDR).\nFCNN is type of deep learning machine that has has been successfully used for semantic segmentation in [8] . In [9] convolutional network has been used to classify 1.2 million images in 1000 different classes. However, our proposed method (DDR) does not employ any learning, rather it uses FCNN to optimize SSD registration cost. It is a newer trend in computer vision and graphics, where deep learning tools are used only for optimization purposes and not for learning. One prominent example is artistic style transfer [10] .\nPrior to our work, convolutional network was used for image registration in [11] , where the authors trained the parameters of a 2-layers convolutional network. The network was used to seek the hierarchical representation for each image, where high level features are inferred from the low level network. The goal of their work was to learn feature vectors for better registration. In contrast, DDR focuses on finding better solution during optimization, where we make use of end-toend back-propagation and the deep learning architecture."}, {"section_title": "PROPOSED METHOD", "text": "In this section, we provide detailed descriptions of each component of our solution pipeline. We start with the overall registration framework. Then, we illustrate our FCNN architecture and define upper bound of the SSD cost. We end the section by explaining the registration module."}, {"section_title": "Deep Deformable Registration Framework", "text": "Throughout this paper the moving image is represented as M (x) and the fixed (or reference) image is represented as F (x). The output of the fully convolutional neural network is represented as h(x). T (x) represents the deformation produced by a registration algorithm. The complete DDR framework is shown in Fig. 1 . In DDR, we have considered SSD as the registration cost between the moving and the fixed image. For simplicity, we omit any regularization term here. Hence, the SSD registration cost is as follows:\nIn DDR, the FCNN is followed by a non-linear function \u03c3 as shown in Fig. 1 . A suitable design of \u03c3 ensures the upper bound of the original cost (UB-SSD):\nIn order to minimize the UB-SSD given in (2), backpropagation is applied. \u03b1 1 is error that back-propagates from registration module and \u03b1 2 is the error of the non-linear module. The output of the FCNN is the heatmap h(x), which is modified by the non-linearity function \u03c3 to h m (x):\nThis modified heat map is added pixel-wise with the fixed image F (x) as a distortion. In DDR, the registration module minimizes the UB-SSD, which will ensure minimization of the original SSD cost.\nThe DDR framework works in an iterative coordinate descent manner by alternating between the following two steps until convergence: (a) fix h(x) and optimize for deformable parameters T (x) and (b) fix T (x) and optimize for heatmap h(x) by back-propagation. Thus, the DDR framework works in an end-to-end fashion. The error signals \u03b1 1 and \u03b1 2 are as follows:\nand\nThus, the DDR framework enhances the space of optimization parameters from T to a joint space of T and h (or h m ). So, when the registration optimizer is stuck at a local minimum of T , the alternating coordinate descent finds a better minimum for h in the joint space, and the registration proceeds because of the upper bound property of the cost function. The decrease of UB-SSD and SSD is shown in Fig. 2 over iterations of the coordinate descent for a registration example. Fig. 2 . Original cost and UB-cost vs iterations."}, {"section_title": "Tight UB-SSD", "text": "In DDR, (2) serves as an upper bound to (1) . Using this condition, we obtain:\nWe ensure condition (6) by realizing \u03c3 as a soft thresholding function with threshold t:\nNote that \u03c3 is applied pixel-wise on the heatmap h(x). For a tight UB-SSD condition (6) can be restated as follows:\nwhere is a small positive number. The following simple algorithm makes sure that with a soft thresholding function \u03c3, condition (8) is met. Fig. 2 demonstrates UB-SSD is quite tight on the SSD cost. "}, {"section_title": "FCNN Architecture", "text": "We have used VGG-net [12] to serve as FCNN. This network contains both convolutional and deconvolutional layers. In the VGG-net we decapitated the final classifier layer and convert all fully connected layers to convolutional layers. This is followed by multiple deconvolutional layers to bi-linearly up-sample the coarse outputs to pixel dense outputs. The convolutional part consist of multiple convolutional layers, ReLU layers and max-pooling layers. The deconvolutional part consists of deconvolutional layers. We have skipped multiple layers and fused them with the deconvolved layers to introduce local appearance information. A typical skip architecture used in our module is shown in details in Fig. 3 ."}, {"section_title": "Registration module", "text": "In order to register the moving image with the modified reference image we have used the demons [2, 3] method. To find the optimum transformation T, we optimize the following cost:\n(9) Due to the large number of transformation parameters in the transformation field T , we use limited memory BFGS (LBFGS) algorithm to find the optimum transformation field. This algorithm is computationally less extensive than BFGS when the number of optimization parameters are large. While calculating the step length and direction, LBFGS store the hessian matrix for the last few iterations and use them to calculate the direction and step length instead of updating and storing the hessian matrix in each iteration. After finding the optimum transformation field, the error is back-propagated through the pipeline which helps the FCNN in finding the necessary distortion required to reduce the energy further down. Output image -48x48x128\nOutput image -24x24x256\nOutput image -12x12x512\nOutput image -6x6x512\nOutput image -6x6x1\nUp-2, Deconv"}, {"section_title": "+", "text": ""}, {"section_title": "Output image -24x24x1", "text": "Up-4, Deconv\nOutput image -48x48x1\nHeat map -192x192 Fig. 3 . FCNN with skip architecture."}, {"section_title": "RESULTS", "text": ""}, {"section_title": "Registration Algorithms", "text": "To establish the usefulness of DDR, the following two deformable registration algorithms, each with and without DDR, are used:\n1. DDR + Diffeomorphic demon 2. Diffeomorphic demon 3. DDR + Log-demon 4. Log-demon.\nIn our setup, to register images using DDR + diffeomorphic demons, we have used FCNN-16s network [8] and for registration using DDR + log-demon, we have used FCNN-32s architecture for the FCNN."}, {"section_title": "Registration Evaluation Metrics", "text": "For performance measures, we have used structural similarity index (SSIM) [13] , Peak signal to noise ratio (PSNR) and the SSD error. SSIM can capture local differences between the images, whereas SSD and PSNR can capture global differences."}, {"section_title": "Experiments with IXI Dataset", "text": "IXI dataset (http://biomedic.doc.ic.ac. uk/brain-development/index.php?n=Main. Datasets) consists of 30 subjects, which are all 3D volumetric data. Among them, we have randomly chosen one as our reference volume and we have registered the others using the aforementioned four algorithms.\nThe improvements in SSIM, PSNR and SSD error with diffeomorphic demon are provided in Fig. 4 . The improvement in registration with log-demon is shown in Fig. 5 . These results are summarized in Table 1 where we show average percentage improvements in 3D volume registration. From these results we observe significant improvements gained by using DDR, especially in reducing the SSD cost, because the optimization has targeted SSD. However, other measures such as SSIM and PSNR have also decreased significantly. Fig. 8 shows a residual image (difference image) for the log-demon method with and without using DDR. Significant reduction in residual image intensity is observed when DDR is used. "}, {"section_title": "Experiments with ADNI Dataset", "text": "In these experiments, we have randomly selected 20 MR 3D volumes from the ADNI dataset (http://adni.loni. ucla.edu/). Among them, one is randomly selected to be the template, and the rest are registered with it using Diffeomorphic Demons and Log-Domain Diffeomorphic Demons algorithm. The SSIM, PSNR and SSD values are calculated and plotted in Fig.7 and Fig. 6 . These improvements are summarized in summarized in Table 2 . Once again, we observe significant gains in registration metrics using the proposed DDR. "}, {"section_title": "CONCLUSIONS AND FUTURE WORK", "text": "We have proposed a novel method for improving deformable registration using fully convolutional neural network. While previous studies have focused on learning features, here we have utilized FCNN to help optimize registration algorithm better. On two publicly available datasets, we show that improvements in registration metrics are significant. In the future, we intend to work with other diffeomorphic registration algorithms, such HAMMER [14] . "}, {"section_title": "Acknowledgments", "text": "Authors acknowledge support from MITACS Globallink and Computing Science, University of Alberta. "}]