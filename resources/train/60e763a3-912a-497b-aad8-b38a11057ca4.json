[{"section_title": "Introduction", "text": "International large-scale assessments (LSAs) such as the Trends in International Mathematics and Science Study (TIMSS) and the Programme for International Student Assessment (PISA) are charged with monitoring educational achievement around the world in a number of learning areas, including math, science, and reading. The National Assessment for Educational Progress (NAEP) is tasked similarly in the United States. As one part of their mandate, these studies track the educational achievement, across time and geographic region, of different policy-relevant populations, including males and females and students from an immigration background. The scale and scope of such studies necessitate that sophisticated data collection designs are used whereby each individual student is administered just a small number of the total possible items, yet all items are administered throughout each of the reporting groups. This approach to item administration is often referred to as item-sampling (Lord, 1962) or, more commonly in current LSA literature, as multiple-matrix sampling (Shoemaker, 1973). Although this method of item delivery is efficient from an administration perspective, the approach poses currently intractable challenges for precisely estimating individual student achievement. Because only a fraction of the students in the population take any one item, and any selected student takes only a fraction of the total available items, the actual distribution of student ability cannot be approximated by its empirical estimate (Mislevy, Johnson, & Muraki, 1992). To overcome the methodological challenges associated with multiple-matrix sampling, LSA programs adopted a population or latent regression modeling approach that uses marginal estimation techniques to generate population-and subpopulation-level achievement estimates (Mislevy, 1991;Mislevy, Beaton, Kaplan & Sheehan, 1992;Mislevy, Johnson & Muraki, 1992). Under the latent regression modeling approach, consistent population-and subpopulation-level ability estimates are achieved by treating achievement as missing (latent) data. These data points are missing for all examinees and are 'filled in' using an approach analogous to multiple imputation (Rubin, 1976;1987). As in multiple imputation methods, an imputation model (called a \"conditioning model\") is developed to predict individual student achievement values (from the posterior population model). This model uses all available student data (cognitive as well as background information) to generate a conditional proficiency distribution for each student from which to draw a number of plausible values (usually five) for each student on each latent trait (e.g. mathematics, science and associated subdomains). Subpopulation estimates of achievement derived from the conditioning models used in this approach are less biased than those estimated via traditional item response theory (IRT) methods (Mislevy, 1991;Mislevy, Beaton, Kaplan, & Sheehan, 1992;von Davier, Gonzalez, & Mislevy, 2009). Although these methods are wellestablished theoretically and empirically, little is known regarding the influence of less-thanoptimal quality background data on subpopulation estimates. Further, previous research provides some evidence of measurement error in background variables used in the conditioning model (Rutkowski, 2011). This paper focuses on the degree to which conditioning model parameter estimates and resulting subpopulation estimates might be biased as a result of systematically misclassified group membership. Specifically, a Monte Carlo approach, with known item and examinee characteristics, is used to investigate the behavior of model parameter and subpopulation achievement estimates when varying proportions of examinees are misclassified on a selection of background variables that are used in the latent regression model. Earlier findings suggest that poor quality background data can lead to under-or over-estimates of group differences, particularly across countries and over time (Rutkowski, 2011). The potential policy implications of this research, particularly as they relate to disadvantaged populations and likely areas for improvement are also discussed."}, {"section_title": "Background", "text": "Disaggregation in test reporting. There is often keen interest on the part of stakeholders such as educators, researchers, and policy makers regarding the academic performance of policyrelevant subpopulations. For example, in the U.S., research that examines achievement gaps based on gender (Penner & Paret, 2008;Robinson & Lubienski, 2011), ethnicity (e.g. Clotfelter, Ladd & Vigdor, 2007;Farkas, 2009), and socioeconomic status (e.g. Evans & Rosenbaum, 2008) receives substantial public attention. And policies to close these gaps receive considerable public (e.g. the No Child Left Behind Act of 2001) and private money (e.g. The Achievement Gap Institute, 2011). In response, assessment programs take careful steps to disaggregate data at the subpopulation level. In fact, by law NAEP must disaggregate and report test results by gender, race, ethnicity, and socioeconomic status, among other variables (NAEP, 2009). As such, test results must stand intense scrutiny and precisely estimating achievement in subpopulations is a critically important issue from both a policy and a technical perspective. Multiple-matrix sampling. NAEP, TIMSS, and PISA use multiple-matrix sampling in conjunction with a rotated booklet design that ensures that each item receives sufficient exposure and that each examinee receives an adequate number of items to estimate population-level achievement in, possibly, several domains and subdomains. For example, more than 10 hours of testable material was available for the TIMSS 2007 assessment (Olson, Martin, and Mullis, 2008). To minimize individual examinee burden, test developers used an assessment design that distributed 429 total mathematics and science items across 14 non-overlapping mathematics blocks and 14 non-overlapping science blocks. That is, the blocks exhaustively and mutually exclusively contained all available testing material. The blocks subsequently were arranged into 14 booklets containing two science and two mathematics blocks each, with no block-wise overlap within a booklet. That is, no block would appear more than once within a booklet. This design ensured linking across booklets since each block (and therefore each item) appeared in two different booklets. Further, the total assessment material was divided into more reasonable 90 minute periods of testing time for each student. It is important to note that this is just one of many possible designs that might fall under the umbrella of multiple-matrix sampling. Proficiency estimation. Operational assessment administration methods that use a rotated booklet design minimize testing time for students who participate in LSAs; however, as noted above, individual achievement estimation is currently not possible with these methods and can result in biased or inconsistent variance estimates of population parameters (Mislevy, Beaton, Kaplan, & Sheehan, 1992;von Davier, Gonzalez, & Mislevy, 2009). Further, traditional methods of estimating individual achievement introduce an unacceptable level of uncertainty and the possibility of serious aggregate-level bias (Little & Rubin, 1987;Mislevy, Beaton, Kaplan, & Sheahan, 1992). In response to the complexities associated with multiple-matrix sampling and the sparse item response data that results, LSA programs estimate population and subpopulation achievement via a latent regression modeling approach (Mislevy, 1991;Mislevy, Beaton, Kaplan, & Sheehan, 1992;Mislevy, Johnson, & Muraki, 1992). Using information from background questionnaires, other demographic variables of interest and responses to the cognitive portion of the test, achievement ( ) is treated as a latent or unobserved variable, which is estimated via a latent regression or population model. In other words, responses to the limited subset of administered cognitive items and complete student background questionnaires are used in conjunction with a measurement model-based extension of Rubin's (1987) multiple imputation approach to generate an examinee ability distribution for the population (or subpopulation) of interest (Beaton & Johnson, 1992;Mislevy, Beaton et al., 1992;Mislevy, Johnson & Muraki, 1992;von Davier et al., 2006). A slightly more technical explication follows. As in multiple imputation methods, an imputation model (called a \"conditioning model\") is developed to predict individual student achievement values (from the posterior population model). This model uses all available student data (cognitive as well as background information) to generate a conditional proficiency distribution for each student from which to draw a number of plausible values (usually five) for each student on each latent trait (e.g. mathematics, science and associated sub-domains). For a detailed description of this method, interested readers are directed to Mislevy (1991), Mislevy, Johnson, and Muraki (1992), or Mislevy, Kaplan, Beaton, and Sheehan (1992). An accessible primer on population modeling methods can be found in von Davier, Gonzalez, and Mislevy (2009). Because is a latent, unobserved variable for every examinee, it is reasonable to treat as a missing value and to approximate statistics involving by its expectation. That is, for any statistic , , where is a matrix of item responses for all examinees and is the matrix of responses of all examinees to the set of administered background questions. Because closed-form solutions are typically not available, random draws from the conditional distributions are drawn for each sampled examinee, i (Mislevy, Johnson, & Muraki, 1992). In line with missing data practices (Rubin, 1987), values for each examinee are drawn multiple times. These are typically referred to as plausible values in LSA terminology or multiple imputations in missing data literature. Using Bayes' theorem and the IRT assumption of conditional independence, where is the likelihood function for induced by observing and is the distribution of for a given vector of response variable. The distribution of is assumed normal with a mean given by the following linear model (the conditioning model) such that is the vector of (usually assumed) perfectly measured background variables, Potential impact of measurement error. Given that the conditioning model (2) is fit to self-report data from children as young as eight or nine (in the case of PIRLS), it is reasonable to imagine that some inaccuracies or inconsistencies will be present in student background data. Further, the nature of the questions, such as the education and occupation of both parents (key socio-economic background variables in PISA), might not be known with precision among 15 year-olds. In fact, previous research has documented substantial discordance between parent and student responses on identical items (Rutkowski & Rutkowski, 2010). For example, the correlation between parent and student responses on the number of books in the home on PIRLS 2006 was quite low (1 = :17 in Indonesia), suggesting that error-free covariates in the conditioning model may be an untenable, although currently operational, assumption. In other words, , where is a column vector of true scores on the conditioning variables and is a vector of measurement error. Then, the conditioning model, (2) is modified in the following way: . In traditional regression where the outcome is observed, a theoretical and empirical consequence of ignoring measurement error is that na\u00efve estimates of and will be biased, the degree to which is difficult to state except under certain fairly strict conditions (Fuller, 1991;Buonaccorsi, 2010, p. 79). As such, it is hypothesized that students who systematically (although possibly mistakenly) self-report their status on demographic variables (e.g. students from an immigration background that report themselves as born in the country of the test) will result in measurable bias in conditioning model parameter estimates and resultant plausible value estimates. Moreover, I hypothesize that the higher the proportion of examinees that incorrectly classify themselves, the more severe the bias in sub-population achievement estimates. Evidence in favor of this hypothesis would imply that subpopulation achievement differences can be under-or over-estimated, which can have important policy consequences."}, {"section_title": "Methods", "text": "To mimic a reasonable multiple-matrix sampled assessment design, 70 multiple choice TIMSS 2007 8th grade mathematics items were selected with their associated item parameter estimates (Olson, Martin, & Mullis, 2008). To estimate item parameters, the TIMSS 2007 measurement model used for these data was a three-parameter logistic item response theory (3PL IRT) model (see Embretson & Reise, 2000 for an example of this model). The 70 items were then assembled into seven booklets containing three blocks with ten multiple-choice items each. Under this design every examinee attempted 30 items. This rotated booklet design is illustrated in Table 1, where cells marked with a '1' indicate that a particular block is contained in a given booklet. For example, Booklet 1 is comprised of Blocks A, B, and D. Also, Block A can be found in Booklet 1, 5, and 7. Here, we one can see that every examinee attempts 30 items. And, by randomly assigning booklets to students in a systematic rotation, every item is attempted by 43 percent of the sample while each block (and therefore item) appears three times per booklet rotation. This design is similar to the balanced incomplete block design used by PISA (OECD, 2009); however, the number of items per block differs. Average item parameters by booklet are located in Table 2. As is typical in IRT notation, a, b, and c correspond to the discrimination, difficulty, and \"guessing\" parameters, respectively. This arrangement, of several considered, provided a reasonable balance of difficulty and discrimination across booklets. Table 2 about here For the latent regression, two uncorrelated background variables with four levels (high, medium-high, medium-low, and low) were used. These variables were specified as uncorrelated in an effort to isolate the effect of measurement error in a single variable while the second variable is free of measurement errorperfectly measured. A marginal and fully conditional mean and variance were chosen for each level of the background variable to span a typical continuum from [-2, +2]. The known proficiency mean and variance associated with each level were used as generating ability distributions for each of the resultant mutually exclusive subpopulations. To avoid confounding measurement error in the background variables with sample sizes and ability variance, these values were fixed to 1,000 and 1.5, respectively, in each fully conditional subpopulation (N total = 16,000). Sample sizes were chosen so that subpopulation sizes would approximate those found in typical LSA studies. For example, in TIMSS 2007, the variable books in the home had between 1,000 and 2,000 respondents in each of five categories (Foy & Olson, 2009). According to the design for the current study, a concrete example of such an arrangement might include the background variables socioeconomic status (high, medium high, medium low, and low); and learning self-efficacy (high, medium high, medium low, and low). Then one of 16 fully conditional sub-populations could be someone from a high socioeconomic background with low learning self-efficacy. The marginal and fully conditional generating ability distributions and sample sizes for each group and level are presented in Table 3. It is notable that the background variables were designed to be correlated, one weakly and one strongly, with \u00b5 so that misclassification on the background variables could reasonably be expected to have some sort of impact on subpopulation differences (\u00bd BV1;\u00b5 = :70; \u00bd BV2;\u00b5 = :15). Table 3 about here Using the simulated sample of 16,000 examinees with generating ability distributions specified by subgroup membership, booklets were randomly assigned to examinees in a rotated fashion to ensure that every block (and therefore every item) was administered an approximately equal number of times. Using known item parameters and specified generating examinee ability distributions, responses to the 70 items were subsequently simulated, with the probability of a correct answer determined by an examinee's ability. Individual probabilities were compared with a random draw from a uniform distribution. If an examinee's probability of a correct answer was greater than the value from the random draw, the item was marked correct; otherwise, the item was marked incorrect. In order to assess the stability of the results, the test administration with perfectly measured (e.g. free of measurement error) background data was replicated 250 times. Three-parameter IRT models were then fit to the resulting 250 examinee by item response matrices to estimate item parameters. The next step in the process of data simulation and preparation was to create patterns of noise misclassification or error in the background data. To model the effect of misclassified background information, several conditions were simulated with varying percentages and degrees of misclassification for the 250 examinee-by-item response matrices according to the mechanisms and misclassified data percentages discussed subsequently. Misclassification percentages were chosen based on measurement error studies conducted by the U.S. National Center for Education Statistics (NCES; see a comprehensive review by NCES, 1997 for details). In re-interview studies, inconsistent responses rates on the order of 2 to more than 50 percent were found; however, typical rates ranged from 10 to 25 percent (NCES; Conley, Fink, & Saba, 1996). In light of this research, background data were simulated such that 10, 20, and 30 percent of examinees with a low level of background variable 1 were randomly reclassified as mediumhigh on the same variable. This had the effect that a random subset of relatively low achievers  Under this condition, background variable 2 was free of measurement error. Similarly, 10, 20, and 30 percent of examines with a high level of background variable 2 were randomly reclassified as low on the same variable. Or . In this case, the true score is 1, the error value is 3, and the observed value is 4. This reclassification resulted in a group of relatively high achievers ( ) erroneously endorsing their group membership as that of a group of relatively low achievers ( ). Under this condition, background variable 1 was free of measurement error. Based on this design, seven conditions, as detailed in Table 4, were examined. Notable is that for each conditions 2 to 7, only one of the two background variables is error-prone while the other background variable is considered to be perfectly measured. In the first condition, both variables are considered to be free of error. Given that the marginal ability distributions are different for the two background variables; these conditions and misclassification mechanisms examine a spectrum of possibilities with respect to conditioning model misclassification. - A conditioning model, with background variables 1 and 2 used as predictors, was then fit to each data set for each of seven conditions to generate the posterior distribution from which five plausible values were then drawn. The conditioning model was specified as: where latent achievement is a function of an intercept ( ), an effect for the level of background variable 1 ( ), an effect for the level of background variable 2 ( ), plus a residual ( ). This step was replicated 250 times in each condition (from correctly classified to 30% misclassified) on both background variables. Parameter estimates from the latent regression model for each condition, namely and , were then compared to the results for the perfectly measured, accurate background information. Similarly, subpopulation achievement estimates for each condition were compared to the results for perfectly measured, accurate background information. To For the current analysis, data was generated using a modified macro to simulate a multiple matrix-sampled design (Gonzalez, 2009). The 3-PL measurement models were fit to the data using Parscale 4.1 (Scientific Software International, Inc., 2003) "}, {"section_title": "Results", "text": "I begin with a comparison of the parameter estimates from the latent regression model followed by the resultant subpopulation achievement estimates for each group and condition. Estimates of the latent regression intercepts, slopes for background variable 1, and background variable 2 can be found in Figures 1, 2, and 3, respectively. Specifically, scatter plots of the coefficients (intercepts and slopes) in each misclassified condition are compared against the perfectly measured (no measurement error) condition for all 250 replications. Data points falling on the diagonal black reference line through the origin would indicate a perfect match between parameter estimates in the misclassified and the perfectly measured conditions. A review of Figures 1, 2, and 3 gives rise to several interesting findings. First, as could be expected, misclassification in the conditioning model resulted in biased parameter estimates for the intercepts ( ) and slopes for both background variables. FurtherIn addition, the higher the rates of misclassification, the further from the black reference line are the data points. For example, consider the intercept estimates in Figure 1. For both background variables, estimates from the 10% misclassified condition are closer to the reference line than are the estimates from the 30% misclassified condition. FurtherAlso, estimates for background variable 1 misclassification are consistently underestimated, while estimates for background variable 2 misclassifications are consistently overestimated. Also notable in Figure 1 is that the estimates in both conditions are approximately the same vertical distance from the reference line when the percent of misclassified data is equivalent, regardless of the background variable that is misclassified. That is, the magnitude of bias in the intercept estimates is similar when either background variable is misclassified at similar rates (e.g. comparing 10% misclassification rate on background variable 1 to 10% misclassification rate to on background variable 2). -   for each misclassification rate. Indeed, a steady decline was observed: ; ; and . A plausible explanation for this finding is the fact that the misclassification mechanism for background variable 2 introduces more error into the variable. That is, as an The following results pertain to plausible value estimates for all four levels of background variable 1 across all four conditions, from correctly classified to 30% misclassified. level of latent ability) were incorrectly classified as having a medium high level of background variable 1, the average plausible value fell by 79.3%. This has the effect that group differences will be markedly minimized when comparing the medium high group to the medium low or low group. Similarly, group differences will be exaggerated when comparing the medium high group to the high group. The plausible value estimates for all four levels of background variable 2 across all four conditions are located in Table 6. Similar to the results for background variable 1, misclassification of background variable 2 had a meaningful impact on subpopulation estimates. Recall that the misclassification mechanism reclassified between 10 to 30% of examinees with a high level of background variable 2 as someone with a low level of background variable 2. Reclassifying a subset of relatively high achievers as belonging to a subpopulation of relatively low achievers had the predictable effect of pulling up the average plausible value estimate (e.g. pv1 error-free = -.27; pv1 30% misclassified = -.19 or a 26% increase in achievement). Further, the degree to which examinee achievement for a low level of background variable 2 is biased depends on the rate of misclassification. As with background variable 1, subgroup differences will be biased with underestimated comparisons between low background variable 1 examinees and all others. And difference underestimates will be more severe for higher rates of misclassified data. -------------------------- Table 5 about here introducing noise into the conditioning model used to generate marginal achievement distributions, findings were generally in line with measurement error literature (Buonaccorsi, 2010;Carroll, Ruppert, & Stefanski, 2006;Hodges & Moore, 1972). That is, unacknowledged measurement error in predictor variables impacts latent regression parameter estimates and resulting predicted outcomes will suffer from bias attributable to measurement error in the covariates, which are normally assumed to be error-free. In particular, regression coefficients are generally attenuated (Bollen, 1989) and the residual variance increases (Bollen) as a result of measurement error in the model. According to the simulation used for this study, the conditioning model regression coefficients and error variance estimates were found to be biased, the degree to which depended on the rate of background variable misclassification, the magnitude of the shift due to misclassification (amount of error introduced) and on the correlation between the background variable and achievement. Further, the impact on regression coefficients was far greater when the coefficient in question was also associated with misclassified data. The opposite was also true: misclassified data on one background variable had little effect on the regression coefficients of another background variable. The magnitude of parameter estimate bias was generally consistent across replications for most coefficients under most conditions; however, regression coefficient estimates for background variable 2 when background variable 2 was misclassified were varied across replications, with more variance related to higher rates of misclassification. Nevertheless, the general pattern of more bias as a result of greater misclassification was consistently found. This suggests that attempts to detect the impact of misclassification on subpopulation achievement, such as implementing a careful sensitivity analysis on extant assessment data where a certain degree of noise is introduced into a background variable, might not be reliable and can instead capitalize on chance. As could be expected, bias in parameter estimates also translated into biased subpopulation achievement, which results in over-or under-estimated subpopulation achievement differences on the background variable in which there is measurement error. As such, the current paper provides evidence that currently used estimation methods may not be robust to departures from thethat the assumption of a fully-measured, error-free vector of background variables. Further, may be untenable and that misclassification rates as low as 10% can have meaningful impacts on sub-population achievement estimates and group differences. Findings from the current paper support related work ([Author reference removed] Rutkowski, 2011), which demonstrated that inaccuracies in background information effect subpopulation estimates. Findings were generally predictable in that bias in achievement estimates could be overwhelmingly explained by the misclassification mechanism. That is, misclassifying relatively high (or low) achievers into relatively low (or high) subpopulations resulted in pulling up (or down) subpopulation achievement estimates for the group into which misclassified examinees were moved. Also notable was that moving a subset of examinees out of a background variable level had little or no impact on achievement estimates for the subpopulation from which the examinees were moved since the only difference in the donor subpopulation was smaller sample size. It is important to note several limitations to the current study. First, the study was a simulation that is a simplification of what is generally found operationally. For example, the number of covariates in operational conditioning models is generally on the order of dozens. Further, to isolate the effects of measurement error, the subpopulation sample sizes and proficiency variance were fixed to constant values. This is also not typically seen operationally and variation in both of these values could have an impact on the results. We canIt is reasonable to have some confidence in the findings from this study, however, given that the design of the study was such that misclassification had a necessary consequence of changes in the subpopulation sample sizes as examinees were mistakenly shifted from one population to another. Of particular importance here is that in those subpopulations that donated examinees into misclassified categories, no substantial changes were noted in their achievement estimates. The same cannot be said for changes to the proficiency variance, since this value was fixed regardless of the presence or absence of measurement error. The current study also considered a single overall population. In studies such as NAEP and especially international studies such as TIMSS and PISA, decidedly heterogeneous populations participate. For example, highly industrialized countries participate alongside economically developing countries. In operational situations, it is reasonable to expect that all of the parameters used in the current study can differ across these populations, with results differing accordingly. The design of the study was almost certainly tidier than what is typically seen in real data. That is, given an error-prone variable, the errors will likely be limited neither to a single category nor in a single direction. Therefore, it is possible that actual biases will be less severe; however, an advantage of this sort of simulation is that we one can investigate effects in a virtual controlled situation, less contaminated by unintended effects and confounders. Despite these limitations, the findings are generally consistent with measurement error research in observed variable models (Buonaccorsi, 2010;Carroll, Ruppert, & Stefanski, 2006;Hodges & Moore, 1972). As such, it is reasonable to expect that these findings are to some degree generalizable to operational situations. As noted in the introduction, subpopulation test performance is a frequent focus of research and large-scale policy intervention. Given the political sensitivities and possible stakes associated with making comparisons across subgroups on variables such as socioeconomic status and immigrant background, it is important to understand the impact that less-than-optimal background instruments can have on achievement estimates in large-scale assessment. The current paper investigated, in a limited but controlled context, the impact of misclassified background data on subpopulation achievement. Findings suggest that systematic response errors, due to misunderstanding the question, fatigue, language, or other issues, can have a meaningful impact on achievement estimates for policy-relevant subpopulations. Implications can vary widely, depending on the degree to which policy makers are attuned to and react to assessment results. For instance, if large-scale assessment data are used to provide evidence in favor of policies or changes in funding to address achievement gaps, inequities, or other policyrelevant issues, no or unexpected results could occur when group differences are misestimated. Consider as an example a policy initiative directed at closing gender gaps where none actually exist. This misdirected initiative would be a waste of resources and could have limited impacts if extra efforts are directed at a group that is not actually lagging behind. Further, if efforts are directed away from some population of students who appears to be on par with a comparison group but is in fact lower achieving, the result could be a further drop in achievement (or other important outcomes) as the needy population languishes. In addition, Ggaining an understanding of patterns of achievement and where errors in our estimates can occur is an important step toward identifying exceptionally talented populations of students or those groups that are most in need, both of which are important for developing a well-educated and globally competitive workforce and to meet the growing demand for a highly-skilled workforce internationally (National Science Board, 2003). As a final point, it is worth noting possible candidate strategies for minimizing measurement error where possible. Given that surveys such as PISA and TIMSS are administered to adolescents in dozens of diverse educational systems internationally, a 'one size fits all' approach to minimizing measurement error might not be especially effective. The same could be said of NAEP, which measures students in several age groups across a heterogeneous population of students. In spite of the diversity inherent in these sorts of surveys, strategies including cognitive laboratory interviews (e.g. Lee, 2012) during the field trial phase could highlight background questions that are broadly misunderstood or incorrectly endorsed by many study participants. A second possibility is for educational systems, especially in international surveys, to take full advantage of national option questions that can be tailored to a country's specific context, thereby allowing countries to target items in a way that limits extraneous information from items. In both cases, research in this area is needed to determine whether implementing these extra measures would in fact reduce measurement error on important items to a degree that would merit the extra time and money.            "}]