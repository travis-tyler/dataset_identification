[{"section_title": "", "text": "INTRODUCCI\u00d3N. La base del conocimiento pedag\u00f3gico que un maestro debe tener respecto a las matem\u00e1ticas se sustenta de una manera especial en el conocimiento que tiene de los contenidos de la materia. El objetivo de este trabajo es la elaboraci\u00f3n de una prueba de diagn\u00f3stico de habilidades matem\u00e1ticas, a partir de la selecci\u00f3n y adaptaci\u00f3n de algunos de los \u00edtems del Trends in International Mathematics and Science Study (TIMSS) de 2011, as\u00ed como los aspectos t\u00e9cnicos desde la aplicaci\u00f3n del modelo de Rasch. M\u00c9TODO. Los \u00edtems de la prueba elaborada se seleccionaron siguiendo la estructura original de bloques de contenido y dominio cognitivo establecida en el marco de TIMSS 2011 (Mullis, Martin, Ruddock, O'Sullivan y Preuschoff, 2009). El procedimiento de traducci\u00f3n y adaptaci\u00f3n se ha desarrollado con el objetivo de garantizar una mayor validez ling\u00fc\u00edstica. El estudio se ha realizado con una muestra de 477 estudiantes del primer curso de universidades de la Comunidad de Madrid (Espa\u00f1a). Estos estudiantes estaban matriculados en el primer a\u00f1o de los diferentes grados de Magisterio de Educaci\u00f3n Primaria. RESULTADOS. Los 30 \u00edtems seleccionados han sido de dos tipolog\u00edas, \u00edtems de respuesta seleccionada e \u00edtems de respuesta construida. Dadas estas caracter\u00edsticas particulares en el formato ha sido necesario estudiar la fiabilidad de la correcci\u00f3n realizada por los evaluadores para garantizar la concordancia en su correcci\u00f3n. DISCUSI\u00d3N. El modelo de Rasch utilizado para la validaci\u00f3n de la prueba nos ha permitido generar evidencias que demuestran un alto grado de confiabilidad, se\u00f1alando las diferencias de comportamiento de los \u00edtems de respuesta seleccionada y de respuesta construida (elementos que requieren que los estudiantes construyan su propia respuesta escrita). El modelo nos ha permitido una clasificaci\u00f3n de los resultados de los \u00edtems por nivel de dificultad, hecho que facilita una interpretaci\u00f3n de los resultados de acuerdo a los dominios de contenido y cognitivo. Palabras clave: TIMSS, Competencia matem\u00e1tica, Modelo de Rasch, Adaptaci\u00f3n de pruebas, Formaci\u00f3n docente inicial."}, {"section_title": "Introducci\u00f3n", "text": "La evaluaci\u00f3n diagn\u00f3stica es una herramienta crucial para planificar y dise\u00f1ar programas de formaci\u00f3n efectivos. El objetivo y dise\u00f1o de este tipo de evaluaciones se centra en la recogida y estudio de informaci\u00f3n sobre el grado de desarrollo de las aptitudes del alumnado con el fin de conocer, pronosticar y tomar decisiones que favorezcan el pleno desarrollo educativo de los estudiantes. Este tipo de evaluaci\u00f3n toma una relevancia especial en un entorno donde el objetivo es alcanzar determinadas competencias (Cano, 2008). Adem\u00e1s, en las recomendaciones dadas por la NCTM (1991) para la evaluaci\u00f3n en matem\u00e1ticas se contempla de manera espec\u00edfica el diagn\u00f3stico en t\u00e9rminos de lo que comprende el estudiante. La utilizaci\u00f3n de evaluaciones diagn\u00f3sticas nos permite adaptar los procesos de ense\u00f1anzaaprendizaje a las caracter\u00edsticas particulares de cada una de las personas y, en consecuencia, potenciar la mejora del elemento evaluado (P\u00e9rez y Soto, 2011; Tour\u00f3n, 2009). Para establecer el significado de ser matem\u00e1ticamente competente, vamos a referirnos a las aportaciones de Llinares (2003: 14) cuando dicta las necesidades para un estudiante de primaria y que podemos generalizar, en este caso, a sus maestros al se\u00f1alar que \"debe relacionarse con ser capaz de realizar determinadas tareas matem\u00e1ticas y comprender por qu\u00e9 pueden ser utilizadas algunas nociones y procesos para resolverlas, as\u00ed como la posibilidad de argumentar la conveniencia de su uso\". Estas habilidades tienen una elevada complejidad para ser evaluadas en cuanto a los niveles de desarrollo que pueden tener, por lo que resultar\u00eda complejo recogerlas en toda su magnitud. Evaluar cada una de estas habilidades en un solo instrumento se hace dif\u00edcil, pues cada una de ellas es compleja en s\u00ed misma y el nivel de desarrollo alcanzado por un sujeto est\u00e1 ligado al aprendizaje progresivo del individuo. La evaluaci\u00f3n diagn\u00f3stica bien planteada debe \"evaluar habilidades cognitivas de alto nivel (como son planificaci\u00f3n, estructuraci\u00f3n de tareas, obtenci\u00f3n de informaci\u00f3n, construcci\u00f3n de respuesta, explicaci\u00f3n de procesos, integraci\u00f3n de conocimientos e informaciones)\" (Castro, 2011: 111), donde la tipolog\u00eda de \u00edtems que se utilicen debe tener distintas caracter\u00edsticas para conseguir evaluar capacidades m\u00e1s complejas. Para elegir un instrumento adecuado para utilizar en un contexto de maestros en formaci\u00f3n acudimos a las referencias especializadas, lo que nos facilit\u00f3 una cantidad considerable de resultados referidos a la evaluaci\u00f3n de creencias y actitudes hacia las matem\u00e1ticas (White, Way, Perry y Southwell, 2005;Hodgen y Askew, 2007;entre otros). No obstante, son menos las investigaciones relacionadas con el nivel de logro en estudiantes de magisterio (Tatto et al., 2008;Sandoval, Frit, Maldonado y Rodr\u00edguez, 2010;Nortes y Nortes Checa, 2017). Sin embargo, parece importante que el maestro domine los contenidos que ense\u00f1a, para respaldar explicaciones o diagn\u00f3stico de obst\u00e1culos de aprendizaje, aportando as\u00ed la base para el conocimiento pedag\u00f3gico (Varas, Lacourly, L\u00f3pez y Giacono, 2013;Ernest, 1989). As\u00ed, el objetivo principal de este trabajo es la construcci\u00f3n y validaci\u00f3n de un instrumento de medida de la competencia matem\u00e1tica basado en la evaluaci\u00f3n TIMSS 2011. Para ello como objetivos espec\u00edficos se plantean: \u2022 Seleccionar y adaptar al castellano \u00edtems de la evaluaci\u00f3n TIMSS 2011. \u2022 Analizar la fiabilidad de los \u00edtems de respuesta construida cuya puntuaci\u00f3n es determinada por un evaluador. \u2022 Validar el instrumento construido bajo los supuestos de la Teor\u00eda Respuesta al \u00cdtem (TRI). El dise\u00f1o de la prueba de diagn\u00f3stico de este estudio se centra en las habilidades de conocimiento y razonamiento matem\u00e1tico que son las que est\u00e1n \u00edntimamente ligadas a la capacidad de resolver problemas matem\u00e1ticos. El marco te\u00f3rico de TIMSS 2011 vincula el \u00e9xito en la resoluci\u00f3n de problemas con la comprensi\u00f3n conceptual y el razonamiento (identificar, inferir y reorganizar la informaci\u00f3n), y es por ello que los \u00edtems planteados en esta prueba se consideran adecuados para definir una primera evaluaci\u00f3n de diagn\u00f3stico, cuyo objetivo es medir el desempe\u00f1o matem\u00e1tico con el que los alumnos acceden al Grado de Magisterio en Educaci\u00f3n Primaria. Se opta por este m\u00e9todo de construcci\u00f3n de la prueba, ante la ausencia de pruebas espec\u00edficas para este tipo de poblaci\u00f3n, unido a la buena calidad psicom\u00e9trica de la prueba TIMSS. Los \u00edtems son de dos tipolog\u00edas distintas: \u00edtems de respuesta seleccionada e \u00edtems de respuesta construida. Estas dos tipolog\u00edas facilitan la recogida de datos no \u00fanicamente referida a la demanda cognitiva, sino al rango de cognici\u00f3n que implica la resoluci\u00f3n de cada uno de los \u00edtems (Mart\u00ednez, 1999)."}, {"section_title": "M\u00e9todo", "text": "El objetivo del instrumento es la medici\u00f3n del desempe\u00f1o matem\u00e1tico en estudiantes que acaban de acceder al Grado de Magisterio de Educaci\u00f3n Primaria. Se trata de una prueba diagn\u00f3stica de corte transversal. Los \u00edtems de la prueba pertenecen a la prueba TIMSS 2011 de rendimiento en matem\u00e1ticas para 8\u00ba grado (en Espa\u00f1a, 2\u00ba de ESO). La estructura de esta prueba internacional responde a dos dimensiones, una de contenido y otra cognitiva (Mullis et al., 2009). La dimensi\u00f3n de contenido especifica cuatro dominios: \u00e1lgebra, n\u00fameros, datos-azar y geometr\u00eda. La dimensi\u00f3n cognitiva describe tres procesos a evaluar: conocer, aplicar y razonar. Se realiza una selecci\u00f3n, con un dise\u00f1o donde todos los estudiantes respondan a los mismos \u00edtems (Fern\u00e1ndez-Alonso y Mu\u00f1iz, 2011) y que tengan elementos con distinta dificultad pertenecientes a todos los dominios cognitivos, respetando la estructura original de la prueba en cuanto al porcentaje de \u00edtems de cada uno de los dominios de contenido. Para la selecci\u00f3n se tiene en cuenta la media del n\u00famero de respuestas correctas a nivel internacional de cada \u00edtem en la prueba original, seleccionando aquellos \u00edtems que corresponden a los tres cuartiles, en el caso de que se seleccionen tres \u00edtems de esa dimensi\u00f3n, o los dos cuartiles superiores, en caso de que se seleccionen dos. Se realiza de esta forma debido a la mayor capacidad informativa que tienen los \u00edtems con una dificultad media (Mart\u00ednez, Hern\u00e1ndez y Hern\u00e1ndez, 2006). El n\u00famero de \u00edtems de cada contenido, dominio cognitivo y dificultad se muestran en la tabla 1. Se utiliza la versi\u00f3n original en ingl\u00e9s para realizar la traducci\u00f3n, intentando evitar errores como los se\u00f1alados por Solano, Contreras y Backhoff (2006) al referirse a traducciones anteriores para este tipo de pruebas internacionales. La validaci\u00f3n de contenido sirve para admitir parte de un instrumento que fue construido para una poblaci\u00f3n de segundo curso de Educaci\u00f3n Secundaria, y que ahora queremos utilizar para estudiantes que acceden al grado de maestro. Este primer proceso tras la selecci\u00f3n de \u00edtems, requiere un proceso de traducci\u00f3n, adaptaci\u00f3n y estandarizaci\u00f3n (Hyrk\u00e4s, Appelqvist-Schmidlechner y Oksa, 2003). El procedimiento de traducci\u00f3n se realiza en dos fases para dar una mayor validez ling\u00fc\u00edstica de partida; primero se realiza la traducci\u00f3n al castellano de la prueba y posteriormente una persona distinta que no conoc\u00eda la versi\u00f3n original realiza la traducci\u00f3n de manera inversa (Beaton, Bombardier, Guillemin y Bosi, 2000). Ambas versiones son revisadas por un equipo de jueces expertos (Escobar-P\u00e9rez y Cuervo- Mart\u00ednez, 2008) en el \u00e1rea de did\u00e1ctica de la matem\u00e1tica para confirmar la equivalencia de los resultados. La validaci\u00f3n de la traducci\u00f3n se lleva a cabo a trav\u00e9s de un juicio de expertos. Para ello, se selecciona una muestra intencional de diez jueces expertos en did\u00e1ctica de las matem\u00e1ticas en activo en distintas universidades espa\u00f1olas; para esta selecci\u00f3n se tiene en consideraci\u00f3n la experiencia docente en los grados de maestro, su conocimiento del sistema educativo y de evaluaciones internacionales de matem\u00e1ticas. Se pide que eval\u00faen el criterio de claridad de la redacci\u00f3n (se comprende desde el punto de vista sint\u00e1ctico y sem\u00e1ntico) (Quesada, Rodr\u00edguez-G\u00f3mez e Ibarra, 2013); adem\u00e1s de un aporte cualitativo que indica fundamentalmente aspectos de redacci\u00f3n o t\u00e9rminos matem\u00e1ticos sin\u00f3nimos a los empleados que puedan aumentar el aspecto comprensivo. La consistencia interjueces se analiza mediante el an\u00e1lisis de concordancia basado en el coeficiente W de Kendall, un coeficiente de acuerdo para datos ordinales basado en el grado de varianza de la suma de rangos obtenidos de los distintos jueces (Siegel y Castellan, 1995). Los valores a partir de 0.4 se consideran aceptables (Landis y Koch, 1977). Una vez elaborada la prueba y despu\u00e9s de su aplicaci\u00f3n se selecciona una muestra de diez cuadernillos que hab\u00edan sido corregidos por los mismos dos correctores y se estudia la fiabilidad de la correcci\u00f3n con el estad\u00edstico Kappa de Cohen. Para este estad\u00edstico, los valores cercanos a 1 se\u00f1alan un alto grado de acuerdo (Viera y Garett, 2005) entre evaluadores. Finalmente, para la validaci\u00f3n del instrumento de medida y la estimaci\u00f3n de puntuaciones de rendimiento se emplea la Teor\u00eda Respuesta al \u00cdtem siguiendo los supuestos del modelo de Rasch (1960) y la adaptaci\u00f3n de Masters (1982) para \u00edtems de respuesta construida (Modelo de Cr\u00e9dito Parcial, MCP). Estos modelos asumen que la puntuaci\u00f3n del sujeto en el instrumento de medida, entendida como suma total de las respuestas de un sujeto al conjunto de \u00edtems, y la puntuaci\u00f3n en el \u00edtem (dificultad), entendida como la suma de las respuestas de los sujetos en un \u00edtem, son estad\u00edsticos suficientes para estimar los par\u00e1metros del modelo. Al contrario de lo que ocurre en los modelos de Thurstone, que estiman el mismo valor del rasgo para los sujetos que obtienen la misma puntuaci\u00f3n en el test. En el modelo de Rasch la funci\u00f3n de probabilidad de respuesta de un sujeto a un determinado reactivo depende \u00fanicamente de estos estad\u00edsticos suficientes (rasgo estimado y dificultad del \u00edtem). En el caso de \u00edtems dicot\u00f3micos es: P i (\u03b8): Probabilidad de acertar el \u00edtem i para un valor determinado de habilidad. En modelos polit\u00f3micos, como el MCP, es la probabilidad de superar una categor\u00eda del \u00edtem. b i es el par\u00e1metro de dificultad, o el nivel de rasgo necesario para responder correctamente al \u00edtem, es decir, el valor de la habilidad en el que la probabilidad de acertar el \u00edtem supera el 0.5. En los modelos polit\u00f3micos es el nivel de rasgo necesario para superar ese paso o categor\u00eda dentro del \u00edtem. \u03b8 es el valor del rasgo estimado y D es una constante, igual a 1.7, que aproxima los valores a la distribuci\u00f3n normal. La probabilidad de acertar o no un \u00edtem depende de estos dos par\u00e1metros y toma la siguiente forma para valores de x igual a 0 y 1. En los modelos con m\u00e1s de una posibilidad de puntuaci\u00f3n, no solo acierto o error, es necesario estimar una curva de probabilidad para cada puntuaci\u00f3n en el \u00edtem porque no supone el mismo nivel de dificultad obtener los distintos valores en ese \u00edtem. En el modelo de cr\u00e9dito parcial la probabilidad de que el sujeto complete cada paso o categor\u00eda del \u00edtem puede explicitarse mediante un modelo de Rasch. En nuestro caso, un \u00edtem con tres categor\u00edas (R es el total de categor\u00edas) de respuesta (0, 1, 2) quedar\u00eda formulado de la siguiente forma:"}, {"section_title": "Siendo", "text": "La calidad de los \u00edtems se analiza a partir de diferentes indicadores: \u2022 La curva caracter\u00edstica del \u00edtem (CCI): describe los cambios en el nivel del rasgo relacionados con cambios en la probabilidad de responder a un \u00edtem. Por tanto, sujetos con diferentes niveles de rasgo tendr\u00e1n distinta probabilidad de acertar el \u00edtem. En \u00edtems de respuesta seleccionada con una \u00fanica respuesta correcta predice la probabilidad de \u00e9xito en el \u00edtem a partir del nivel de rasgo que posee el sujeto. En \u00edtems el caso de los polit\u00f3micos (respuesta construida) predice la probabilidad de las respuestas en cada categor\u00eda a partir del nivel de rasgo. \u2022 La curva caracter\u00edstica del test: relaciona las puntuaciones estimadas con el modelo de Rasch con la suma de respuestas correctas. \u2022 Los estad\u00edsticos de ajuste de los \u00edtems basados en los residuos como diferencia media al cuadrado (MNSQ) entre la respuesta emp\u00edrica y las probabilidades esperadas que representan los gr\u00e1ficos CCI: \u2212 MNSQ (sin ponderar): no pondera los residuos, por lo que representa el ajuste externo al ser sensible al comportamiento inesperado de \u00edtems cuya pendiente se aleja del nivel de habilidad del sujeto, es decir, sujetos con una habilidad alta que valoran categor\u00edas bajas o al rev\u00e9s. \u2212 MNSQ (ponderado): es el mismo \u00edndice pero ponderado por la cantidad de informaci\u00f3n de un \u00edtem en el intervalo de habilidad. Se corresponde con el ajuste interno, se adec\u00faa a \u00edtems con pautas de respuesta irregulares impl\u00edcitas en las personas y viceversa porque las personas con un nivel de rasgo cercano a la dificultad influyen m\u00e1s en el residual. Un buen ajuste se obtiene cuando sus valores se aproximan a 1, pero los valores son aceptables mientras no excedan los l\u00edmites del intervalo de confianza. El estad\u00edstico tambi\u00e9n se presenta de forma transformada a distribuci\u00f3n normal como prueba de hip\u00f3tesis T (valores por encima de 2 se\u00f1alar\u00e1n malos ajustes) (Navarro, Exp\u00f3sito, L\u00f3pez y Thoilliez, 2014)."}, {"section_title": "Resultados", "text": "En primer lugar, el estudio de la claridad de los \u00edtems por parte de los expertos obtiene, como muestra la tabla 2, un \u00edndice de concordancia aceptable (W=.414, p=.000). No obstante, se consideran las observaciones cualitativas aportadas para mejorar la traducci\u00f3n de los \u00edtems. En segundo lugar, el estudio de fiabilidad de la correcci\u00f3n que se lleva a cabo muestra un acuerdo perfecto entre evaluadores, con valores de Kappa iguales a uno en todos los \u00edtems analizados. Por \u00faltimo, para la validaci\u00f3n el instrumento, en las tablas 3 y 4 se presentan los \u00edndices de ajuste para \u00edtems de respuesta seleccionada y construida respectivamente. Como se observa, el ajuste ponderado obtiene valores aceptables en todos los \u00edtems de respuesta seleccionada, con valores de MNSQ cercanos a 1 y valores de T por debajo de 2. Si se observan los par\u00e1metros de dificultad (B) se comprueba que hay muchos valores negativos, lo que indica que hay muchos \u00edtems que se podr\u00edan considerar f\u00e1ciles o, en otras palabras, que el nivel de rendimiento en matem\u00e1ticas que se necesita para contestar correctamente a ese \u00edtem est\u00e1 por debajo de la media. El \u00edtem 1(p66) es el m\u00e1s f\u00e1cil. De la misma forma los \u00edtems 7 (p72) y 9 (p74) son los m\u00e1s dif\u00edciles, es decir, el nivel de rendimiento en matem\u00e1ticas que necesita un estudiante para responder correctamente est\u00e1 por encima de la media. Conviene recordar que los resultados estimados inicialmente se encuentran en una escala tipificada. Sin embargo, no ocurre lo mismo para los de respuesta construida, el \u00edtem 4 (p69) parece presentar problemas de ajuste como indican los valores de T por encima de 2. Tampoco parece que haya una buena estimaci\u00f3n de los par\u00e1metros de dificultad en los \u00edtems, 4 (p69), 8 (p73) y 10 (p75), donde las categor\u00edas de respuesta m\u00e1s bajas necesitan un mayor nivel de rendimiento que las superiores. En cualquier caso, que un \u00edtem no ajuste completamente en el modelo de Rasch no significa que est\u00e9 funcionando mal en este caso particular. Este tipo de an\u00e1lisis necesita grandes muestras para aumentar la fiabilidad de las estimaciones y, en los \u00edtems de respuesta construida, tener representadas las distintas categor\u00edas de valoraci\u00f3n del \u00edtem. Para comprobar el funcionamiento de las puntuaciones estimadas se han calculado los promedios en matem\u00e1ticas de los estudiantes que estaban en cada uno de los niveles de respuesta de esos \u00edtems (tablas 9 a 11) y se observa que unos mejores resultados se corresponden con una categor\u00eda m\u00e1s alta en el reactivo. Para observar con m\u00e1s detalle los \u00edtems de respuesta construida se presentan en la figura 1 los gr\u00e1ficos CCI (Curva Caracter\u00edstica del \u00cdtem) y las proporciones de casos situados en cada categor\u00eda en las tablas 5 a 8. Estos gr\u00e1ficos muestran las probabilidades de situarse en cada uno de los niveles de respuesta del \u00edtem (eje y) en funci\u00f3n del nivel de resultados de matem\u00e1ticas (eje X). Por ejemplo, en el gr\u00e1fico del \u00edtem 4 (p69), se observa que hay mayor probabilidad de obtener un valor 0 si se obtiene una puntuaci\u00f3n baja en el resultado, sin embargo, para obtener un valor 2 (valor m\u00e1s alto) necesitas unos mejores resultados. No obstante, en el valor 1 la probabilidad es similar a lo largo de todos los valores del resultado. Este aspecto puede llevar a un mal ajuste del \u00edtem y puede ser debido por la poca variabilidad de respuestas en esa categor\u00eda, como demuestra la tabla 4, solo hay 21 casos en la categor\u00eda 1. En el \u00edtem 8 (p73), si ponemos atenci\u00f3n el gr\u00e1fico de CCI, se observa que la probabilidad de obtener un valor 2 aumenta a medida que aumenta la puntuaci\u00f3n en matem\u00e1ticas. Sin embargo, hay un grupo de casos que tambi\u00e9n obtienen esa puntuaci\u00f3n y finalmente no tienen una buena Tabla 4. \u00cdndices de ajuste de los \u00edtems de respuesta construida de matem\u00e1ticas (cont.) fiGura 1. Gr\u00e1ficos CCI de los \u00edtems de respuesta construida de matem\u00e1ticas puntuaci\u00f3n final. Se debe a que la mayor parte de los casos se sit\u00faan en esa categor\u00eda del \u00edtem, lo que indica una baja variabilidad en las respuestas. En el \u00edtem 10 (p75) ocurre lo opuesto, la mayor\u00eda obtienen un valor 0, y la tendencia es que la probabilidad disminuya a medida que se obtienen mejores resultados en matem\u00e1ticas, pero incluso con puntuaciones por encima de la media puedes obtener un valor 0 en este \u00edtem. En las tablas 5 a 8 se presentan las proporciones de respuestas en las categor\u00edas de los \u00edtems polit\u00f3micos, para demostrar que los que tienen un ajuste inferior es por la poca representatividad de alguna de las categor\u00edas en sus niveles de respuesta. Por ejemplo, la categor\u00eda 1 del \u00edtem 4 (p69). Como muestran las tablas (9 a 11), el promedio en el logro en matem\u00e1ticas aumenta a medida que se incrementa la categor\u00eda del \u00edtem, aspecto que indica una buena estimaci\u00f3n de la puntuaci\u00f3n de rendimiento con el modelo de Rasch. Tambi\u00e9n a modo de ejemplo, se incluyen las CCI de un \u00edtem que puede considerarse f\u00e1cil y otro dif\u00edcil para visualizar las diferencias (figura 2). El \u00edtem 1 (p66) es f\u00e1cil, incluso con valores bajos de resultados en matem\u00e1ticas hay alta probabilidad de responder correctamente. Y el \u00edtem 7 (p72) puede considerarse m\u00e1s dif\u00edcil, para tener una probabilidad mayor que 0.5 de responder correctamente al \u00edtem se necesitan unos resultados en matem\u00e1ticas por encima de la media. Respecto a la informaci\u00f3n global de la prueba de matem\u00e1ticas, la fiabilidad obtenida ha sido de .776. Un valor cercano al .8 que se considera el valor \u00f3ptimo para pruebas de rendimiento, sobre todo, si se van a tomar decisiones en funci\u00f3n de los resultados. La figura 3 muestra la curva caracter\u00edstica del test (CCT) que relaciona la suma de respuestas correctas con las puntuaciones estimadas con el modelo de Rasch. Finalmente, en la figura 4 se incluye un mapa para representar las dificultades estimadas para los \u00edtems y las puntuaciones. De esta forma es posible identificar qu\u00e9 \u00edtems son necesarios responder correctamente para situarse en los distintos tramos de la escala. Por ejemplo, el \u00edtem 1 es m\u00e1s f\u00e1cil y necesita valores del rasgo por debajo de -4; y el que m\u00e1s nivel de rendimiento necesita es el segundo paso del \u00edtem 10. En la tabla 12, se recoge la distribuci\u00f3n de los \u00edtems por dominio de contenido y nivel de desem pe\u00f1o, resultantes de la muestra. Dado que la prueba de matem\u00e1ticas se ha construido desde la prueba TIMSS, se ha considerado mantener la estructura de los resultados de la prueba de acuerdo al dominio cognitivo y al nivel de desempe\u00f1o de los resultados obtenidos (tabla 13). Podemos ver en las tablas 12 y 13 c\u00f3mo en algunas variables aparecen dos niveles de logro (entre par\u00e9ntesis), son aquellas de respuesta construida, correspondiendo el primer valor a la respuesta 1 y el segundo a la respuesta 2. "}, {"section_title": "Conclusiones", "text": "Hemos construido un instrumento dise\u00f1ado para medir las habilidades matem\u00e1ticas de los maestros en formaci\u00f3n, que se ha utilizado como elemento de diagn\u00f3stico. La prueba tiene una finalidad diagn\u00f3stica, de la misma forma que ocurre con TIMSS o las evaluaciones generales que realiza el MECD (https:// www.mecd.gob.es/inee/evaluaciones-nacionales. html). El prop\u00f3sito es conocer el nivel de dominio de las competencias matem\u00e1ticas de los futuros docentes antes de comenzar el t\u00edtulo de grado. Con este diagn\u00f3stico, las universidades tendr\u00e1n la capacidad de reforzar alg\u00fan aspecto de su programa formativo o implementar actuaciones espec\u00edficas sobre el alumnado si las consideran necesarias. Partimos de la consideraci\u00f3n de que la prueba TIMSS utilizada como elemento base para la elaboraci\u00f3n de la prueba diagn\u00f3stica est\u00e1 dise\u00f1ada para una poblaci\u00f3n distinta; sin embargo podemos considerar el resultado en la misma como dato de logro inicial en el que los estudiantes deben acreditar un dominio suficiente. El an\u00e1lisis de los \u00edtems de la prueba mediante el modelo de Rasch de la TRI facilita un an\u00e1lisis detallado de cada \u00edtem referido a su calidad t\u00e9cnica y al aporte individual que hace al conjunto de la prueba. Este hecho ha resultado fundamental en este caso, dado que estamos utilizando parte de una prueba previamente dise\u00f1ada para una poblaci\u00f3n distinta, aportando adem\u00e1s un error de medida en caso de existir. Este modelo genera puntuaciones en una escala de intervalo, lo que significa que diferencias entre distintos tramos de la escala suponen los mismos cambios en las probabilidades de responder de manera correcta a los \u00edtems con la misma dificultad (Prieto y Delgado, 2003). Los \u00edtems, en general, tienen un buen funcionamiento y se ajustan al modelo psicom\u00e9trico de Rasch. No obstante, existe una mayor presencia de \u00edtems que pueden considerarse f\u00e1ciles que est\u00e1 provocada porque se dise\u00f1aron para una poblaci\u00f3n de menor edad. Se observa un buen funcionamiento de la escala en su conjunto de acuerdo al modelo de Rasch, aunque cabe mencionar el comportamiento diferencial de los \u00edtems de respuesta construida respecto a los de respuesta seleccionada. Este hecho se justifica por la escasa variabilidad de la respuesta como han evidenciado los an\u00e1lisis particulares de este tipo de \u00edtems. Por ejemplo, en el \u00edtem 4, una categor\u00eda con escasa elecci\u00f3n (1) con el 4,9% de los valores. La forma de mejorar este tipo de resultados implicar\u00eda el uso de una muestra mayor, aunque cabe destacar que en nuestra investigaci\u00f3n funciona de manera adecuada la puntuaci\u00f3n del rasgo. La prueba funciona bien observando la curva caracter\u00edstica, adem\u00e1s la suma entre el nivel del rasgo y la suma de respuestas correctas est\u00e1n vinculadas. Los \u00edndices de ajuste estimados con el modelo de Rasch muestran el correcto funcionamiento de los \u00edtems incluidos en la prueba. La falta de ajuste de los \u00edtems de respuesta construida est\u00e1 justificada por la baja variabilidad de las respuestas, no obstante, su funcionamiento es correcto para diferenciar entre los niveles de rendimiento. De esta forma, es posible concluir que la prueba es v\u00e1lida para comprobar el nivel de la competencia matem\u00e1tica de los estudiantes de grado. En relaci\u00f3n a los resultados del nivel de dificultad por dominio de contenidos, los \u00edtems de mayor dificultad para la muestra coinciden con los que tuvieron un menor porcentaje de respuesta correcta para los estudiantes de octavo grado a nivel internacional (7 y 10), ambos pertenecientes al dominio de contenido de \u00e1lgebra. Como conclusi\u00f3n final y recomendaci\u00f3n del presente trabajo, planteamos la necesidad de construir unos est\u00e1ndares que definan los conocimientos matem\u00e1ticos que un estudiante necesita para acceder a los estudios del grado de magisterio."}, {"section_title": "Agradecimientos", "text": "Este trabajo es producto de una investigaci\u00f3n financiada por la Comunidad de Madrid, desde la Consejer\u00eda de Educaci\u00f3n e Investigaci\u00f3n en el a\u00f1o 2017.   Preuschoff, 2009). The translation and adaptation procedure has been developed in order to ensure greater linguistic validity. The study was carried out with a sample of 477 students enrolled in the first year of the different Bachelordegrees of Primary Education School Teacher that are offered in universities across the Autonomous Community of Madrid (Spain). RESULTS. We considered two types: multiple-choice items and constructed response items. Given the characteristic of the format it has been necessary to assess the reliability of the correction made by the evaluators to guarantee the agreement in their correction. DISCUSSION. The Rasch model used for the validation of the test has allowed us to generate evidence that shows a high degree of reliability, pointing out the differences in behavior between the multiple-choice items and the constructed response items (elements that require students to construct their own written response). The model allowed us to classify the results of the items by level of difficulty, which facilitates an interpretation of the results according to both the content domain and the cognitive domain. Keywords: TIMSS, Mathematical competence, Rasch model, Assessment adaptation, Pre-service teacher preparation. Mots cl\u00e9s: TIMSS, comp\u00e9tence math\u00e9matique, mod\u00e8le de Rasch, adaptation des tests, formation initiale des enseignants."}, {"section_title": "R\u00e8sum\u00e8", "text": ""}, {"section_title": "Perfil profesional de los autores", "text": "Blanca Arteaga Mart\u00ednez (autora de contacto) Profesora ayudante doctora. Doctora en Ciencias de la Educaci\u00f3n. Licenciada en CC. Matem\u00e1ticas. En la actualidad imparte docencia en la Facultad de Educaci\u00f3n de la Universidad de Alcal\u00e1, con un perfil de Did\u00e1ctica de las Matem\u00e1ticas. Ha trabajado en distintas universidades, presenciales y online, donde adem\u00e1s ha desempe\u00f1ado distintos cargos de coordinaci\u00f3n acad\u00e9mica. Ponente en cursos de formaci\u00f3n de profesores. Miembro del grupo de investigaci\u00f3n \"Pedagog\u00eda Adaptativa\" de la Universidad Complutense de Madrid. Correo electr\u00f3nico de contacto: blanca.arteaga@uah.es Direcci\u00f3n para la correspondencia: Facultad de Educaci\u00f3n. C/ Madrid, 1, 19001 Guadalajara."}, {"section_title": "Enrique Navarro Asencio", "text": "Profesor ayudante doctor. Doctor en Pedagog\u00eda por la Universidad Complutense de Madrid (UCM), obteniendo el premio extraordinario de doctorado. Actualmente es profesor ayudante doctor en la UCM y su l\u00ednea de trabajo est\u00e1 relacionada con psicometr\u00eda y evaluaci\u00f3n del rendimiento acad\u00e9mico y factores asociados. Correo electr\u00f3nico de contacto: enriquen@ucm.es"}, {"section_title": "Ar\u00e1ntzazu Fraile Rey", "text": "Profesora visitante. Doctora en Ciencias de la Educaci\u00f3n. Licenciada en CC. F\u00edsicas. En la actualidad imparte docencia en matem\u00e1ticas y educaci\u00f3n matem\u00e1tica en la Facultad de Educaci\u00f3n de la Universidad de Alcal\u00e1 en los grados de Educaci\u00f3n Primaria y Doble Grado de Humanidades y Educaci\u00f3n Primaria. Su l\u00ednea de investigaci\u00f3n est\u00e1 centrada en la resoluci\u00f3n de problemas. Correo electr\u00f3nico de contacto: arantzazu.fraile@uah.es"}, {"section_title": "Pedro Ramos Alonso", "text": "Profesor titular. Doctor en Ciencias Matem\u00e1ticas por la Universidad Polit\u00e9cnica de Madrid. Su investigaci\u00f3n se ha centrado en geometr\u00eda discreta y computacional. Desde el a\u00f1o 2010 trabaja en el \u00e1mbito de la educaci\u00f3n matem\u00e1tica. Es profesor titular de Universidad en la Universidad de Alcal\u00e1. Imparte cursos de formaci\u00f3n de profesorado y colabora con las editoriales Polygon Education y SM en la implantaci\u00f3n de la metodolog\u00eda Singapur en matem\u00e1ticas. Correo electr\u00f3nico de contacto: pedro.ramos@uah.es"}]