[{"section_title": "Abstract", "text": "Abstract-Alarmingly increasing prevalence of Alzheimer's disease (AD) due to the aging population in developing countries, combined with lack of standardized and conclusive diagnostic procedures, make early diagnosis of Alzheimer's disease a major public health concern. While no current medical treatment exists to stop or reverse this disease, recent dementia specific pharmacological advances can slow its progression, making early diagnosis all the more important. Several noninvasive biomarkers have been proposed, including P300 based EEG analysis, MRI volumetric analysis, PET based metabolic activity analysis, as alternatives to neuropsychological evaluation, the current gold standard of diagnosis. Each of these approaches, have shown some promising outcomes, however, a comprehensive data fusion analysis has not yet been conducted to investigate whether these different modalities carry complementary information, and if so, whether they can be combined to provide a more accurate analysis. In this effort, we provide a first look at such an analysis in combining EEG, MRI and PET data using an ensemble of classifiers based decision fusion approach, to determine whether a strategic combination of these different modalities can improve the diagnostic accuracy over any of the individual data sources when used with an automated classifier. Results show an improvement of up to 10%-20% using this approach compared to the classification performance obtained when using each individual data source."}, {"section_title": "", "text": "INTRODUCTION lhzeimer's disease is a neurodegenerative disorder, causing neuronal death that leads to cognitive function decline. Two misfolded proteins, \u03b2-amyloid that causes plagues and hyperphosphorylated-\u03c4 that causes neurofibrillary tangles are often blamed, yet, the genesis of these proteins, and in fact the true cause of the disease, are still unknown.\nPerhaps due to overall life expectancy increase, the number of AD cases has been growing over the last decades. Vast majority of AD patients are over the age of 65, with 19% between 75 and 84, and 42% ~50% above the age of 85 [1] . With over 5.2 million people suffering from AD in the U.S. alone, along with its enormous financial (over $140 billion / year) and emotional cost of the disease on the patient, family, and society, Alzheimer's disease is now justly considered a major health concern. Yet, there is still no procedure for conclusively and definitively diagnosing AD, and misdiagnosis is quite common. The only definitive diagnosis remains the post-mortem analysis of the brain tissue under the microscope for the presence of plagues and tangles. This work is supported by NIH grants AG022272 and AG10124, PA Dept. of Health, SAP4100027296, by NSF grant no ECCS 0239090 and NSF grant no ECCS 0926159.\nC. Tilley, B. Hillis and R. Polikar are with Electrical and Computer Eng. Dept. at Rowan University, Glassboro, NJ 08028, USA. (e-mail: tilley89@students.rowan.edu, hillis78@students.rowan.edu).\nC.M. Clark is with the Department of Neurology, University of Pennsylvania, Philadelphia, PA (e-mail: chris.clark@uphs.upenn.edu).\nContact author: Robi Polikar, phone: (856) 256 -5372; fax: (856) 256-5241. E-mail: polikar@rowan.edu.\nThe most common method of pre-mortem diagnosis, then, is neuropsychological clinical evaluation, performed over the course of a year or longer, which includes behavioral tests (such as memory tests) as well as interviews of the patient and their caretakers. The goal of such an evaluation is to monitor the cognitive decline over time and eliminate other possible disorders. Therefore, clinical evaluations require the expertise of a qualified neuropsychologist, can be quite subjective (some hospitals use a committee of neurologists to reduce the subjectivity), and due to longitudinal nature of the process, can be quite expensive. However, such an evaluation can achieve over 90% accuracy, when conducted by expert neurologists at large hospitals or dementia clinics, however, geographical and financial restrictions generally limit most patients to smaller hospitals and clinics, where the diagnostic accuracies are estimated to be around 75%, even with the benefit of longitudinal evaluations [2] .\nBecause of the cost, subjectivity, and the general difficulty of clinical evaluations, several biomarkers have been researched and proposed over the years for detecting AD, with cerebrospinal fluid (CSF) concentrations of tau and \u03b2-amyloid proteins being the most promising. However, measuring these concentrations requires highly invasive, expensive, potentially painful lumber puncture (spinal tap), which also requires specialty clinics, research or university hospitals -severely restricting the utility of these biomarkers. In an attempt to find non-invasive biomarkers, researchers have investigated the feasibility of standard neuroimaging tools, such as the MRI and PET imaging [3] , as well as neurophysiological measurements using EEG.\nThe MRI primarily provides a volumetric measurement, where the loss of brain tissue in predefined regions is measured over a longitudinal time frame. Since AD causes neuronal death, which in return causes loss of brain volume, MRI can easily identify such a change. PET imaging follows a different, but equally effective approach, by measuring the glucose metabolism of the brain. Hypometabolism, a drop in metabolic activity in the brain possibly can be easily measured by the PET scan. Finally, the event related potentials (ERPs) obtained from the electroencephalogram (EEG) provides another potential biomarker. Several studies using the well-known oddball paradigm have demonstrated that the decreased amplitudes and increased latencies of the so-called P 300 component of ERPs -a positive peak that occurs around 300 ms after a stimulus is observed by the subject -is linked to the cognitive decline [4] [5] [6] . Various signal processing approaches on the raw EEG or the P 300 has been conducted, verifying the presence of a statistical correlation, with limited success in patient specific diagnosis [7] [8] [9] . In fact, in our previous work, we have also shown that discrete wavelet coefficients of the ERPs, can be used as biomarkers in patient specific AD diagnosis, particularly when the ERPs in response to different types of stimuli are combined [10;11] . Previously, we have shown that an ensemble of classifiers based decision fusion approach, combining ERP data acquired from different electrodes is a feasible approach for automated early diagnosis. In this contribution, we investigate the obvious question of whether these three different modalities, MRI, PET and EEG, carry complementary information, and whether a strategic combination of these modalities can improve the diagnostic accuracy over any of the individual data sources when used with an automated classifier. While MRI, PET and CSF combination has been very recently explored [12] , to the best of our knowledge, this is the first effort of its kind in investigating an MRI, PET and EEG data fusion for early diagnosis of AD. Initial results, reported in this paper, appear to be quite promising."}, {"section_title": "II. EXPERIMENTAL SETUP A. Patient Cohort", "text": "The study cohort used in this analysis was chosen from a larger cohort of over 300 subjects who had undergone different biomarker tests. Not all subjects had all imaging tests done, hence a subset of 73 subjects, 37 with AD (15 male, 22 female, with mean age \u03bc AGE(AD) =74.5) and 36 cognitively control (10 male and 26 female, \u03bc AGE(CN) =71.2) were chosen based on the constrain that all selected subjects had at least one MRI scan, one PET scan and one EEG recording within relatively close period of each other.\nThe inclusion criteria for AD group was satisfying the NINCDS-ADRDA criteria [13] for probable AD, which includes a battery of memory tests, interviews with the subject and their caregivers, clinical dementia rating (CDR) score of 0.5 or higher for AD cohort (\u03bc CDR(AD) =1.12) and 0 for the normal cohort. All subjects were over 60 years old. Exclusion criteria for both groups were evidence of any other central nervous system damage or use of sedatives, anxiolytic or antidepressants within 48 hours of ERP acquisition."}, {"section_title": "B. The Oddball Paradigm and ERP Acquisition", "text": "The ERPs were collected using an auditory oddball paradigm protocol. Electrodes were placed according to 10-20 standard (Fig 1) , whose impedances were kept below 20\u2126. Each subject was tested for 30 minutes with approximately three minutes of rest for every five minutes of testing. 1,000 random stimuli were presented, 65% consisting of standard tones of 1 kHz, 20% as target tones of 2 kHz, and 15% as novel sounds. A random inter-stimulus interval of 1.0 to 1.3 seconds was inserted. Standard and target stimuli were presented in 100ms busts with a 5ms on/offset envelope. The novel stimuli were environmental sounds about 200ms long, each unique and never repeated. The subjects were instructed to press a button each time they heard the target tone only. The data was sampled at 256 samples /second.\nThe preprocessing of the EEG signals included notch filtering at 59-61 Hz, followed by removal of major artifacts using a 20 th order derivative based thresholding. EEG recordings were epoched with respect stimulus type, averaged and time-locked with 200ms pre-stimulus and 800ms poststimulus intervals. The pre-stimulus baseline was removed from the entire ERP, resulting in one second length of 256 samples per stimulus type, per electrode channel, per patient.\nWe have previously reported that discrete wavelet transform (DWT) coefficients corresponding to certain frequency bands are particularly informative [10;11;14] . In this study, the decomposition levels 6,7 and the approximation level 7 were of primary focus. These bands were chosen as ERP signals generally reside within 0 -4 Hz. 30 feature sets were generated, 15 from responses to novel tones and 15 from responses to target tones using 5 different electrode locations in the parietal region: P 3 , P 4 , P 7 , P 8 , and P Z , with three frequency bands each. These locations were chosen due parietal region being the first affected region in AD, an outcome that was also verified in previous analyses."}, {"section_title": "C. MRI Acquisition & Processing", "text": "Different stages of AD can be characterized by the amount of atrophy in the brain. The change of volume in particular regions of the brain, as well as the overall brain volume, can be detected by volumetric MRI imaging techniques. MRI enhances the differences in tissue matter based on the ratio of bound to unbound water molecules. Since different types of brain tissue have different such ratios, a quantification of regional volumes can be obtained. In this effort, the so-called T 1 weighted MRI was used, which refers to the duration of the net magnetization vector to return to its initial state after being rotated by an RF pulse. Tissues that have a large ratio of bound to unbound water have short T 1 durations. Brain tissue has a high amount of bound water compared to the surrounding CSF and therefore appears accentuated in a T 1 image. Original raw MRI data consists of images of the brain taken in consecutive slices perpendicular to the coronal and parallel to the transverse planes. An elastic warping algorithm was used that standardizes images to a reference topology, while preserving the morphological characteristics of the individual brain [15] . The raw image is segmented into white and gray matter, cerebrospinal fluid, and ventricles. Automated region of interest (ROI) analysis then determines the brain regions visible in the image, allowing the areas corresponding to the skull to be removed and the volumes of specific areas to be computed. Fourteen such areas were identified on each side of the brain, giving 28 features extracted per image. The volumes corresponding to these 28 areas were normalized with respect to total intracranial volume before being used to train a classifier."}, {"section_title": "D. PET Acquisition & Processing", "text": "Unlike MRI, Positron Emission Tomography (PET) is a nuclear imaging technique, based on the detection of gamma rays emitted by a radioactive tracer introduced into the body using a biologically active molecule. The most common molecule used for this purpose is fluorodeoxyglucose (FDG), and hence this imaging technique is also called FDG-PET. The images used in this study were FDG-PET images, where scanning began 30 minutes after the FDG injection and lasted also about 30 minutes. The imaging used the ADNI (Alzheimer's Disease Neuroimaging Initiative) protocol [16] . Stereotactic surface projection analysis was applied to these images, an approach that was shown to be very useful for AD diagnosis [17;18] . This analysis, made by an automated software library called Neurostat SSP (freely available for download at [19] ) provide relative glucose metabolic rates in 43 predetermined regions of interest, which were used as the features to train the classifiers."}, {"section_title": "III. DATA ANALYSIS", "text": ""}, {"section_title": "A. Ensemble Based Classification Systems", "text": "An ensemble of classifiers based decision fusion approach was implemented for automated classification. An ensemble-based system is obtained by combining a set of diverse classifiers, where diversity is typically achieved by using different training parameters for each classifier. If proper diversity is achieved, different errors are made by the individual classifiers, strategic combination of which can reduce the total error. The diversity can be obtained in several ways, including using different subsets of the training data obtained by resampling, different parameters of a classifier model, different classifiers all together, or different subsets of the features of the given dataset. The latter one is commonly referred to as random subspace analysis.\nIn a sense, an ensemble based system implements a decision -fusion approach, where the decisions made by component classifiers are combined to arrive at a final decision. The goal is to improve the generalization performance over a single classifier based system. However, the structure of this approach also lends itself naturally to data fusion applications, where information from different sources can be combined. The goal in data fusion applications is to improve the accuracy of the final decision compared to a decision made based on a single source of data. Using the ensemble approach for a data fusion application is quite straightforward: a separate classifier can be trained on each dataset that come from a separate source, and the classifiers can then be combined using an appropriate combination rule [20] .\nIn this analysis we combine the two uses of the ensemble systems, that is, we use an ensemble of classifiers both for improving the accuracy over a single classifier, as well as for achieving data fusion. In essence, we first train an ensemble of classifiers for each of the EEG, MRI and PET based data to obtain three \"experts.\" We then combine these ensembles of classifiers to achieve decision fusion based data fusion of EEG, MRI and PET data. Our goal is to investigate whether these different modalities provide complementary information for the diagnosis of Alzheimer's disease. Figure 2 illustrates this general approach.\nOnce the individual classifiers are generated, they can be combined by any one of several combination rules. Among these, perhaps the most commonly used ones are the sum rule and the majority voting rule. Let us define the decision of the i th classifier as ,\nwhere L is the number of classifiers and c is the number of classes. If i th classifier chooses class j, then d i,j = 1, and zero, otherwise. In majority voting, the ensemble decision is chosen as the class that receives the highest number of votes, that is, we first compute the total support S J for class J as\nand choose the class with this highest support as the winning class. Alternatively, if classifiers can provide continuous outputs for each class, representing the support provided for that class, then the sum rule can also be used to combine the classifiers, where the total support for class J is obtained as the sum of all classifiers' class J outputs. The final decision is then the class that receives highest sum support\nFeature -specific expert ensembles of classifiers "}, {"section_title": "B. Classification Protocols", "text": "We used multilayer perceptron (MLP) as the component classifier, although any supervised learning algorithm can be used. All MLPs were trained with gradient descend based backpropagation with an error goal of 0.01 and a momentum term of 0.95. As mentioned above, the individual features were i) the (8~10) DWT coefficients of the ERPs for specific 0-1, 1-2 and 2-4 Hz frequency bands for five parietal electrode locations; ii) the volumetric readings of 28 regions of interest for MRI; and iii) glucose metabolisms from 43 regions of interest for the PET. A total of 30 feature sets generated for the EEG data allowed us to interpret each feature set as a different source of data and use an internal EEG specific data fusion to determine whether there is complementary information among the EEG channels. Each MLP for the EEG data had 10 hidden layer nodes, and 30 such MLPs were generated, one for each feature set. Using different feature sets provided a natural diversity for the EEG based classifiers. The MRI and PET data did not have individually separate datasets. However, in order to give equal total voting weight to EEG, MRI and PET classifiers, we decided to train 30 MRI classifiers and 30 PET classifiers. In order to achieve the diversity for MRI and PET classifiers, we used the random subspace method, training each MRI and PET classifier with 2/3 of the total available features (18 of 28 for MRI, and 28 of 43 for PET). Sum and simple majority voting were used as combination rules. All experiments were repeated using a five-fold cross validation, whose results are provided below.\nWe should mention that in this preliminary study for combining EEG, MRI and PET data, our goal was not so much to obtain the best classification performance, but rather to determine whether these modalities provide complementary information, and whether the ensemble based approach can extract such information. Therefore, individual classifiers' free parameters (e.g. number of hidden layer nodes) were not optimized, but used with common sense default values."}, {"section_title": "IV.", "text": "RESULTS The average diagnostic performances of the EEG, MRI and PET classifiers were 56.7%, 67.22% and 70.3%, respectively, in identifying the test dataset subjects as AD vs. control. We have then used the sum and simple majority voting (SMV) rules to obtain EEG+MRI, EEG+PET, MRI+PET and EEG+MRI+PET data fusion diagnostic accuracies. Two sets of each experiment were done, once using all 30 classifiers for each data source, and once using the top 15 classifiers for each data source. The top 15 EEG classifiers would then correspond to those channel and frequency bands that performed better than the others, and the MRI and PET classifiers would represent those feature subsets whose combination provided better performance. The results are tabulated in Table 1 . We note from Table 1 that each combination provided better diagnostic performance than the corresponding individual data sources, and the combination of all three sources performed better than the other combinations. This outcome indicates that each of these modalities do carry complementary information and the simple combination of classifiers trained on these different modalities can improve the diagnostic performance. Finally, the sum rule appears to provide better classification performance then majority voting rule."}, {"section_title": "V.", "text": "CONCLUSIONS We described a simple data fusion approach for combining data from different sources with a specific application on diagnosis of Alzheimer's disease from EEG, MRI and PET data. The approach generates an ensemble of classifiers for data obtained from each modality. For EEG, where different channels and frequency bands provided a natural individual data subsources, a single classifier was trained with each such source. For MRI and PET, random subspace analysis was used to generate the ensembles. In all cases, the combinations showed better performances than the classifiers trained on individual data sources. Considering that each modality actually measures a very different (electrophysiological vs. anatomical vs. metabolic) quantity, this outcome is in fact reassuring, whereas the ability of the approach to extract such complementary information is very promising. While the goal of this study was not to obtain the best possible classification (and hence the individual classifiers were not optimized), the results are nevertheless shows significant potential."}]