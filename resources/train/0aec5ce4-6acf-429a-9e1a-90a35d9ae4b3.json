[{"section_title": "Abstract", "text": "The use of machine-learning in neuroimaging offers new perspectives in early diagnosis and prognosis of brain diseases. Although such multivariate methods can capture complex relationships in the data, traditional approaches provide irregular ( 2 penalty) or scattered ( 1 penalty) predictive pattern with a very limited relevance. A penalty like Total Variation (TV) that exploits the natural 3D structure of the images can increase the spatial coherence of the weight map. However, TV penalization leads to non-smooth optimization problems that are hard to minimize. We propose an optimization framework that minimizes any combination of 1, 2, and T V penalties while preserving the exact 1 penalty. This algorithm uses Nesterov's smoothing technique to approximate the T V penalty with a smooth function such that the loss and the penalties are minimized with an exact accelerated proximal gradient algorithm. We propose an original continuation algorithm that uses successively smaller values of the smoothing parameter to reach a prescribed precision while achieving the best possible convergence rate. This algorithm can be used with other losses or penalties. The algorithm is applied on a classification problem on the ADNI dataset. We observe that the T V penalty does not necessarily improve the prediction but provides a major breakthrough in terms of support recovery of the predictive brain regions."}, {"section_title": "I. INTRODUCTION", "text": "Multivariate machine-learning applied in neuroimaging offers new perspectives in early diagnosis and prognosis of brain diseases. However, it is essential that the method provides meaningful predictive patterns in order to reveal the neuroimaging biomarkers of the pathologies. Penalized linear models (such as linear SVM, penalized logistic regression) are often used in neuroimaging since the weight map might provide clues about biomarkers.\nIn particular, we are interested in penalized logistic regression in order to predict the clinical status of patients from neuroimaging data and link this prediction to known neuroanatomical structures. When using the 2 penalty with such data, the weight maps are dense and potentially irregular (i.e. with abrupt, high-frequency changes). With the 1 penalty, they are scattered and sparse with only a few voxels with non-zero weight. In both cases, the weight maps are hard to interpret in terms of neuroanatomy. The combination of both penalties in Elastic Net (see [1] ), promotes sparse models while still maintaining the regularization properties of the 2 penalty.\nA major limitation of the Elastic Net penalty is that it does not take into account the spatial structure of brain images, which leads to scattered patterns.\nThe Total Variation (T V ) penalty is widely used in 2D or 3D image processing to account for this spatial structure. In this paper, we propose to add T V to the Elastic Net penalty to improve the interpretability and the accuracy of logistic regression. We hypothesize that the predictive information is most likely organized in regions rather than scattered across the brain.\nThe difficulty is that 1 and T V are convex but not smooth functions (see section II for the precise definition of smoothness used in this paper). Therefore, we cannot use classic gradient descent algorithms. In [2] , the authors use a primal-dual approach for 1 and T V penalties (which can be extended to include 2 ) but their method is not applicable to logistic regression because the proximal operator of the logistic loss is not known. Another strategy for non-smooth problems is to use methods based on the proximal operator of the penalties. For the 1 penalty alone, the proximal operator is analytically known and efficient iterative algorithms such as ISTA and FISTA are available (see [3] ). However, as the proximal operator of the T V penalty is not analytically defined, those algorithms won't work in our case.\nThere are two general strategies to address this problem. The first one involves using an iterative algorithm to numerically approximate the proximal operator of each convex nonsmooth penalty (see [4] ). This algorithm is then run for each iteration of ISTA or FISTA (leading to nested optimization loops). This was done for T V alone in [5] where the authors use FISTA to approximate the proximal operator of T V . The problem with such methods is that by approximating the proximal operator we may loose the sparsity induced by the 1 penalty. The second strategy is to approximate the non-smooth penalties for which the proximal operator is not known (e.g. T V ) with a smooth function (of which the gradient is known). Non-smooth penalties with a known proximal operator (e.g. 1 ) are not changed. Therefore it is possible to use an exact accelerated proximal gradient algorithm. Such a smoothing technique has been proposed by Nesterov in [6] .\nWe choose to apply the second strategy. We will present an algorithm able to solve T V -Elastic Net penalized logistic 978-1-4799-4149-0/14/$31.00 c 2014 IEEE regression with exact 1 penalty and evaluate it on the prediction of the clinical status of patients from structural magnetic resonance imaging (MRI) scans. The paper is organized as follows: we present the minimization problem and our algorithm in section II, the experimental dataset is described in section III, and section IV presents the classification rates and weight maps. Finally, we conclude in section V."}, {"section_title": "II. METHOD", "text": "We first detail the notations of the problem. Then we develop the TV regularization framework. Finally, we detail the algorithm used to solve the minimization problem."}, {"section_title": "A. Problem statement", "text": "We place ourselves in the context of logistic regression models. Let X \u2208 R n\u00d7p be a matrix of n samples, where each sample lies in a p-dimensional space and let y \u2208 {0, 1} n denote the n-dimensional response vector. In the logistic regression model the conditional probability of y i given the data X i is defined through a linear function of the unknown predictors \u03b2 \u2208 R p by\nand p(y i = 0|X i ) = 1 \u2212 p i . Therefore, looking for the maximum of the log-likelihood with structured and sparse penalties, we consider the following minimization problem of a logistic regression objective function with Elastic Net and TV penalties:\nwhere f (\u03b2) is the sum of a smooth part, g(\u03b2), and of a nonsmooth part, h(\u03b2), such that\nwhere \u03bb 2 , \u03bb T V and \u03bb 1 are constants that control the relative strength of each penalty. In this context, a function is said to be smooth if it is differentiable everywhere and its gradient is Lipschitz-continuous.\nwhere grad i,j,k (I) \u2208 R 3 is the numerical gradient of I at coordinates (i, j, k) and the sum runs over all voxels of I.\nIn our case, rows of X are composed of masked and flattened 3D images arranged into vectors of size p < p x \u00d7p y \u00d7p z . Similarly, the vector \u03b2 belongs to R p . For now on each voxel is identified by its linear index in X, noted i (1 \u2264 i \u2264 p). Special care must be taken for the computation of the gradient on the flattened vector \u03b2, because, due to the existence of a mask and border conditions, not all the neighbors of a voxel i exist in the data. Given this precaution, we can compute the gradient for each \u03b2 i and then compute T V (\u03b2). More details regarding the TV penalty in the context of 3D image analysis can be found in [5] ."}, {"section_title": "B. Regularization framework", "text": "A sufficient condition for the application of Nesterov's smoothing technique to a given convex function s is that it can be written on the form\nfor all \u03b2 \u2208 R p , with K a compact convex set in a finitedimensional vector space and A s a linear operator between two finite-dimensional vector spaces.\nIn [7] the authors show that T V (\u03b2) can be written as\nand A i is a sparse matrix that allows to compute the gradient at position i (A i depends on the mask M ). This can be further written as\nwhere \u03b1 is the concatenation of all the \u03b1 i , A is the vertical concatenation of all the A i matrices and K is the product of all the compact convex spaces K i (as such, K is itself a compact convex space). Note that K and A are specific to T V .\nGiven this expression for T V , we can apply Nesterov's smoothing. For a given smoothing parameter \u03bc > 0, T V is approximated by the smooth function\nThe value that maximizes Equation 5 is\nThe function T V \u03bc is convex and differentiable. Its gradient can be written (see [6] ) as"}, {"section_title": "The gradient is Lipschitz continuous with Lipschitz constant", "text": "where A 2 is the matrix spectral norm of A."}, {"section_title": "C. Algorithm", "text": "A new optimization problem, closely related to problem 1, arises from this regularization:\nwhere\n\u03b2 * \u03bc approximates \u03b2 * , the solution to the original problem 1, since f \u03bc \u2212 f \u2264 \u03bcp 2 . Since we are now able to explicitly calculate the gradient of the smooth part, its Lipschitz constant and the proximal operator of the non-smooth part, this new problem can be solved by FISTA [3] . The convergence rate of FISTA is governed by\nwhere k \u2265 1 is the iteration number and t \u03bc is the step size that must be chosen smaller than or equal to the inverse of the known Lipschitz constant of the gradient of the smooth part. Note that the convergence depends on the initial value \u03b2 (0) .\nIf \u03bc is small the algorithm will converge with a high precision (i.e. \u03b2 * \u03bc will be close to \u03b2 * ) but in this case it will converge slowly (because small \u03bc leads to small t \u03bc ). Thus, there is a trade-off between speed and accuracy. We therefore propose to perform successive runs of FISTA with decreasing values of the smoothing parameter (to increase precision) but using the regression vector obtained at the previous run as a starting point for FISTA to increase convergence speed. We denote \u03b2 (i) the regression vector after the ith run of FISTA.\nThe key point is how to derive the sequence of smoothing parameter \u03bc (i) . Our approach involves two steps. First, we describe how to obtain a value of the smoothing parameter \u03bc opt (\u03b5) that minimizes the number of iterations needed to achieve a prescribed precision \u03b5 > 0 when minimising 1 via 6 (i.e. such that f (\u03b2 (k) ) \u2212 f (\u03b2 * ) < \u03b5). Next, given a predefined sequence \u03b5 (i) of decreasing precision values, we can define a continuation sequence of smoothing parameters such that \u03bc (i) = \u03bc opt (\u03b5 (i) ). Concerning the first point we can prove that for any given \u03b5 > 0, selecting the smoothing parameter as\nwhere M = p/2 and L 0 is the Lipschitz constant of \u2207(g) (following [5] , we have L 0 = 2\u03bb 2 + A 2 /(4n)) minimizes the worst case bound on the number of iterations needed to achieve the precision \u03b5 when minimizing 1 via 6. The proof is inspired by the proof of Lemma 3 in [8] . In this article, we use a fixed sequence of precision \u03b5 (i) = (1/2) i\u22121 . The only parameter of the algorithm is then the initial point \u03b2 0 . In these experiments, we used a random vector with a unit norm.\nWe call this algorithm CONESTA (for COntinuation with NEsterov smoothing in a Shrinkage-Thresholding Algorithm). The algorithm is presented in Algorithm 1. The convergence proof will be presented in an upcoming paper. We denote the total number of FISTA loops used in CONESTA by K. We have experimentally verified that the convergence rate to the solution of problem 1 is O( 1 /K 2 ) (which is the optimal convergence rate). Also, the algorithm works even if some of the weights \u03bb 1 , \u03bb 2 or \u03bb T V are zero, which thus allows us to solve e.g. the Elastic Net or pure lasso using CONESTA."}, {"section_title": "III. DATASET", "text": "The data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative Algorithm 1 CONESTA Require: \u03b2 0 , the initial regression vector.\n1: i = 1 2: repeat 3:\nuntil Convergence (ADNI) database (http://adni.loni.usc.edu/). The MR scans are T1-weighted MR image acquired at 1.5 T according to the ADNI acquisition protocol (see [9] ). The image dimensions were p x = 121, p y = 145, p z = 121. The 510 T1-weighted MR images were segmented into GM (Gray Matter), WM (White Matter) and CSF (Cerebrospinal Fluid) using the SPM8 unified segmentation routine [10] . 456 images were retained after quality control on GM probability. These images were spatially normalized using DARTEL [11] without any spatial smoothing. From the 456 registered images we use only the 148 control (CTL) subjects and the 122 Alzheimer's Disease (AD) subjects. Thus, the total number of images was n = 270. A brain mask was obtained by thresholding the modulated gray matter map, leading to the selection of p = 311,341 voxels. According to the assignments found in [12] , those 270 images were split into 132 training images, used in the learning phase, and 138 images used to test the algorithms. Table I presents the prediction results obtained on the test samples. It shows that using the 1 penalty alone decreases the predictive performance. We suspect that the 1 penalty is inefficient in recovering the predictive support on nonsmoothed images. The T V penalty does not significantly increase nor decrease the performances except when it is combined with the 1 penalty. Figure 1 demonstrates that the T V penalty provides a major breakthrough in terms of support recovery of the predictive brain regions. Conversely to the 2 penalty that highlights an irregular and meaningless pattern, 2 + T V provides a smooth map that match the well-known brain regions involved in AD [13] . A large region of negative weights was found in the temporal lobe. This region includes the superior and middle temporal gyri, the parahippocampal gyrus and the entorhinal cortex, the fusiform gyrus, the amygdala, the insula and the hippocampus. As expected, this pattern was predominantly found on the left hemisphere. The bi-lateral ventricular enlargement is sharply identified, the surprising positive sign of the weights is explained in Figure 1 . Atrophy in the frontal lobe (inferior frontal gyrus) was found. Positive weights within the whole cingulum region reflect tissue shift due to periventricular atrophy. In the occipital lobe, positive weights were observed within the calcarine fissure and the cuneus."}, {"section_title": "IV. EXPERIMENTAL RESULTS", "text": "As hypothesized, the combination of the 1 + 2 penalties provides scattered patterns with a very limited relevance.\nFinally, 1 + 2 + T V provides a summary of the 2 + T V pattern: most of the identified regions are are the same as when using 2 + T V but with limited extent. For example, It should generally be interpreted as a increase/decrease of GM in the AD group. Positive weights (increase of GM in AD) may be found where negative weights are expected. For example, positive weights surround the whole bi-lateral ventricles. We hypothesize that we observe the negative pattern of an underlying global atrophy: the GM surrounding the ventricles shift away from them thus we observe GM in AD patients where controls have WM tissue. The map obtained with 1 + T V has been omitted since it provides similar results as those found with 1 + 2 +T V . The map obtained with 1 alone has no relevance.\nthe whole temporal atrophy found by 2 + T V is now limited to the hippocampus. Noticeably, the right hippocampus is no longer a predictive region due to the property of the 1 penalty. This suggests that sparse patterns should be considered with caution."}, {"section_title": "V. CONCLUSION", "text": "We proposed an optimization algorithm that is able to minimize any combination of the 1 , 2 , and T V penalties while preserving the exact 1 penalty. This algorithm uses Nesterov's technique to smooth the T V penalty such that objective function is minimized with an exact accelerated proximal gradient algorithm. The approximation of T V is controlled by a single smoothing parameter \u03bc . Our contribution was to propose a continuation algorithm with successively smaller values of \u03bc to reach a prescribed precision while achieving the best possible convergence rate. Average execution time is one hour on a standard workstation involving 13,000 FISTA iterations.\nWe observed that by adding the T V penalty, the prediction does not necessarily improve. However, we demonstrated that it provides a major breakthrough in terms of support recovery of the predictive brain regions.\nIt should be noted that the algorithm can be extended to minimize any differentiable loss (logistic, least square) with any combination of 1 , 2 penalties and with any non-smooth penalty that can be written in the form of Equation 4. This includes Group Lasso and Fused Lasso or any penalty that can be expressed as a p-norm of a linear operation on the weight map."}]