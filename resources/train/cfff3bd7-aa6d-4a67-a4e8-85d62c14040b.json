[{"section_title": "Abstract", "text": "Large-scale studies, such as the Trends in International Mathematics and Science Study (TIMSS), provide data to understand cross-national differences and similarities. In this study, we aimed to identify factors predicting mathematics achievement of Turkish students by comparing to Australian students. First, construct equivalence and item bias were evaluated to check the comparability. Second, factors predicting mathematics achievement of Turkish and Australian students were identified. Then, propensity score matching on background variables was conducted to identify the remaining achievement differences. Results indicated that mathematics skills were free of construct bias in these groups. After removal of some biased items, we obtained an item bias free booklet. Additionally, students' self-confidence and educational resources at home were significant predictors of achievement. Propensity score analysis indicated that educational resources and to a somewhat lesser extent self-confidence were effective in explaining achievement differences between these two countries."}, {"section_title": "INTRODUCTION", "text": "In this study we aimed to identify factors predicting mathematics achievement of Turkish students by comparing the TIMSS achievement of Turkish and Australian students using powerful and novel statistical techniques such as differential item functioning [DIF] , measurement invariance and propensity score procedures. Comparing countries according to their mathematics achievement levels and providing clues that are important in predicting achievement has become an important research topic using data from international assessment programs such as Trends in International Mathematics and Science Study [TIMSS] . These largescale studies provide data to understand crossnational differences and similarities. Turkish students, on average, get scores below the TIMSS global mean score. For instance, the Turkish mean mathematics scores of 432 in TIMSS 2007 and 452 in TIMSS 2011 were well below the global average score of 500 (Mullis, Martin, & Foy, 2008; Mullis et al., 2012) . By comparing the achievement of Turkey with a higher achieving country, it is expected to identify the factors behind the fairly low Turkish scores and to provide policy-relevant information about how to remedy this situation. We chose Australia as a reference country because TIMSS items were originally prepared in English and Australia is one of the English speaking countries that got higher achievement scores than Turkey in TIMSS 2007 and TIMSS 2011; the mean scores for Australia were 496 and 505, respectively."}, {"section_title": "Cross-cultural comparability", "text": "In making cross-cultural comparisons a number of methodological issues need to be taken into consideration. It is important to establish to what extent observed cross-cultural differences in scores refer to mathematics differences and to what extent these are influenced by nuisance factors. These nuisance factors are collectively known as bias. Construct, item, and method bias are three types of bias that can challenge cross-cultural comparability (Van de Vijver & Poortinga, 1997; Van de Vijver & Tanzer, 2004) . Construct bias occurs when a target construct is not identical for students of different groups or nations. For instance, when measuring practical intelligence, the main focus is on cognitive performance, such as reasoning or previously acquired knowledge in western countries (Van de Vijver & Hambleton, 1996) . However, asking what you would do when your house is on fire might have different answers for a student in the USA and in Zambia (Serpell, 1993) . Therefore, the context of the question is important and might not be generalized to every culture. Multi-group confirmatory factor analysis methods, one of the procedures in structural equation modeling (SEM), are often used to show measurement invariance of constructs (absence of construct bias) measured in a test (Wu, Li, & Zumbo, 2007) . Item bias, also called differential item functioning, occurs when students of different groups/nations with the same level of ability on the underlying construct show a different mean score on the item ( Van de Vijver & Leung, 1997) . For instance, in an international assessment, if an arithmetic item involves the mental manipulation of money given in Turkish Lira, it would be more realistic and easier to do the computations for Turkish students than other students who use another currency. Differential familiarity with the stimuli can create crosscultural differences in scores that are undesirable as such familiarity differences do State of the literature \uf0b7 In making cross-cultural comparisons a number of methodological issues need to be taken into consideration as studies that compare multiple language forms of largescale assessments, differentially functioning items are found, which threatens the comparability. \uf0b7 Turkish students show scores below the average score in large-scale mathematics tests, such as TIMSS and PISA. \uf0b7 In many achievement models, three general groups of factors are defined to influence student academic achievement, namely student aptitude variables, environmental variables, and instructional variables."}, {"section_title": "Contribution of this paper to the literature", "text": "\uf0b7 We evaluated construct and item bias of the cognitive domain structure of TIMSS. This study is the first to evaluate measurement invariance of cognitive domains of TIMSS. \uf0b7 We used a propensity score approach to identify effectiveness of significant factors that predict mathematics achievement. In this novel procedure, students are first completely matched on background variables and then countries are compared. \uf0b7 Our study suggests that Turkish students experience more problems with applying their knowledge in both routine and nonroutine problem situations than Australian students."}, {"section_title": "TIMSS mathematics achievement", "text": "\u00a9 2016 by the author/s, Eurasia J. Math. Sci. & Tech. Ed., 12(8) , 2041 not reflect country differences in mathematics, which is the target of the instrument. Using statistical methods to detect items showing DIF and removing these items from the instrument is required for item bias free and valid score comparisons. Otherwise, bias could create differences in observed scores which are not related to differences in the underlying trait or ability (He & Van de Vijver, 2013) . Finally, method bias occurs when samples are not comparable or when familiarity of students to the test is different or when there are some problems related to the administration of the tests ( Van de Vijver & Tanzer, 2004) .\nhigh self-confidence and having more educational resources at home were significantly and positively related with mathematics achievement of students in both countries (See Figure 1) . However, positive affect toward mathematics, valuing mathematics and classroom learning activities were not significantly related to mathematics achievement in any country. The third step was to evaluate latent factor means on all latent variables. Especially, latent mean differences on significant factors that were important in predicting mathematics achievement could be expected to explain the nature of mathematics achievement difference between these two countries. Table 4 showed that Australian students had a significantly higher level of educational resources at home and had higher self-confidence in the TIMSS 2007 data set whereas Turkish students had a significantly higher level of positive affect towards mathematics, valuing mathematics and classroom learning activities. Having observed that there are significant country differences in antecedent variables, we now turn to the question of how effective these variables are to explain country differences in achievement, using propensity score matching."}, {"section_title": "TIMSS assessment framework", "text": "In the TIMSS 2007 mathematics assessment framework, Mullis et al. (2005) explained how mathematics is conceptualized in TIMSS. For the fourth and eighth grades, TIMSS mathematics assessment emphasized two domains: content domain and cognitive domain. The content domain of TIMSS assessment is related to numbers, geometric shapes and measures, and data display for the fourth grade, and to numbers, algebra, geometry, and data and chance for the eighth grade. The cognitive domain of TIMSS assessment is related to the thinking processes that are measured, namely knowing, applying, and reasoning for both fourth and eighth grades. The knowing domain is related to facts, concepts, and procedures that students are expected to know; applying is related to students' ability to apply knowledge and conceptual understanding to solve problems; finally, reasoning is related to dealing with unfamiliar situations, complex contexts and multistep problems. The three domains, knowing, applying and reasoning, have increasing levels of cognitive complexity. Measuring mathematics in such a structure can be expected to give detailed feedback about mathematics outcomes of countries."}, {"section_title": "Turkish and Australian mathematics curriculum in 2007", "text": "The Turkish mathematics curriculum for primary and secondary schools were fundamentally revised in 2005. The Turkish curriculum was reformed to emphasize process-based outcomes (Koc, Isiksal, & Bulut, 2007) . Before the revision, teachercentered pedagogical approach and rote-learning were the main characteristics of the curriculum. The focus in the mathematics curriculum was on the result rather than on the process. In the 2005 Turkish curriculum, the focus on complex cognitive processes, measured in TIMSS, was partially reflected. In 2007, when TIMSS was administered, the 2005 curriculum that pays more attention to solution processes was not yet fully implemented. The teachers had not gone through comprehensive in-service teacher training, therefore they probably relied on their old pedagogical knowledge. As a result, the classes were still predominantly teacher-centered and learning activities were mostly focused on acquiring knowledge rather than problem-solving skills.\nIn 2008, it was decided that the Australian curriculum had to be renewed. In this new mathematics curriculum, started to be used in 2012, the aim was to use deep knowledge to create new ideas and translate them into practical applications. The new curriculum aimed to be effective in a way that teachers would be flexible to care for the diverse needs of their students. The mathematics curriculum focused on four proficiency strands of understanding, fluency, problem solving and reasoning which are parallel to the TIMSS cognitive domain classification. Before 2012, there was no national curriculum for mathematics. Although each state had similar strands and outcome expectations, there was a need to increase the level of standards across the country (ACARA, 2012) . Therefore, in both Turkey and Australia, the students who took the exam in 2007 would not have benefited extensively from new curriculums."}, {"section_title": "S. Arikan et. al", "text": ""}, {"section_title": "2042", "text": "\u00a9 2016 by the author/s, Eurasia J. Math. Sci. & Tech. Ed., 12(8) , Factors important in predicting mathematics achievement As TIMSS provides not only student achievement data but also student background data via self-report questionnaires, it is possible to identify student level factors that are important in predicting mathematics achievement. Comparing countries in terms of factors that are important in predicting achievement gives a broad understanding of the nature of cross-cultural score differences and can help to establish whether achievement differences between countries are related to specific student background characteristics, such as parental education. Walberg's (1981 Walberg's ( , 1984 Walberg's ( , 2004 Model of Educational Productivity is one of the widely used achievement models that formed a basis for many achievement studies (Young, Reynolds, & Walberg, 1996) . In the model, three general groups of factors are defined to influence student academic achievement, namely student aptitude variables, environmental variables, and instructional variables. Student aptitude variables are related to information about characteristics and background information of students. Examples of environmental variables are socialpsychological climate of home, classroom, and peer group. Instructional variables refer to classroom activities that might affect learning. Student aptitude is generally measured as variables such as ability, prior achievement, motivation, self-concept, age, and developmental level; environmental variables generally involved characteristics of the home, classroom, peers, and exposure to mass media related variables; instructional variables are generally represented by quantity, time and quality of instruction. Examples of student aptitude variables of Walberg's model in TIMSS are students' self-confidence, positive affects toward mathematics, and valuing mathematics; an example of environmental variables is educational resources at home; an example of instructional variables is student-centered classroom activities.\nMany scholars showed that as self-confidence level increases, so does the achievement level of the students (Liu & Meng, 2010; Shen & Pedulla, 2000; Stankov, Morony, & Lee, 2014) . Similarly, Marsh et al. (2013) , investigating TIMSS 2007 data, reported that mathematics achievement had a higher correlation with self-concept than positive affect towards mathematics or valuing mathematics in four Arabspeaking and four English-speaking countries. Shen (2002) investigated correlates of mathematics achievement in TIMSS 1999 for 38 countries including Australia and Turkey. Students who reported that they liked math, did well in math, and students who thought that math is easy were more successful in math in almost all countries. Educational resources at home are also one of the important factors that were found to be related to achievement by many scholars (Chevalier & Lanot, 2002; Fuchs & W\u00f6bmann, 2007; Song, Perry, & McConney, 2014) . Educational resources at home can enhance the effectiveness of learning time out of school and are positively correlated with student achievement (Kaya & Rice, 2010; Walberg, 2004) . The research results about student centered instruction are less clear-cut. Some studies found that student centered instruction is positively associated with achievement (Sabah & Hammouri, 2010) whereas others found that student-centered instruction is either neutral or negatively associated with achievement (Atar & Atar, 2012; Kalender & Berberoglu, 2009; Von Secker & Lissitz, 1999) ."}, {"section_title": "Present study", "text": "There are some studies testing measurement invariance of TIMSS' content domain (Wu, Li, & Zumbo, 2007) and TIMSS' socioeconomic status indicators (Hansson & Gustafsson, 2013) ; however, to our knowledge no research has addressed measurement invariance of cognitive domains of TIMSS. This study is novel in that we evaluated construct and item bias of the cognitive domain structure of TIMSS. Testing measurement invariance of cognitive domains among groups is required to have valid achievement comparisons in terms of these cognitive domains. All language versions of TIMSS are translated from English (Olson, Martin, & Mullis, 2008) and all students are assessed in their language of instruction. In studies that compared many language forms of TIMSS mathematics tests, differentially functioning items were found (Arim & Ercikan, 2014; Ercikan & Koh, 2005; Klieme & Baumert, 2001 ).\nThis study is also novel in a way that we used a propensity score approach (e.g., Rubin 2006) to identify the relative importance of significant factors that predict mathematics achievement. Among the significant variables that predict mathematics achievement, the best predictor was identified. In a propensity matching procedure, students are completely matched on background variables (educational resources at home and self-confidence) and countries are then compared as if they were fully matched on these background variables. So, the question is addressed what the results would be if Turkish students had the same background variables as Australian students. The propensity score approach identifies remaining differences among these students. In educational comparative research, the propensity score approach is increasingly used (Lottridge, Nicewander, & Mitzel, 2011; Ruzek, Burchinal, Farkas, & Hibel, 2010; Sullivan & Field, 2013) .\nIn this study we aimed to identify factors predicting mathematics achievement of Turkish and Australian students. In order to achieve this goal, construct bias and item bias were evaluated to assess comparability of data. Then, by building an achievement model, measurement invariance of the model was tested, significant factors were identified and using a propensity score approach, presumably important antecedents of mathematics achievement were evaluated. This study contributes to the literature in a methodological way by showing steps of conducting a comparative study and in a substantive way by identifying correlates of mathematics achievement measured by various cognitive dimensions. This study is mainly based on TIMSS 2007 data and the same procedures were repeated with TIMSS 2011 data. The research questions of this study are:\n1. To what extent can the TIMSS mathematics achievement differences between Turkish and Australian students be accounted for by bias (construct and item bias)?\n2. To what extent can the TIMSS mathematics achievement differences between Turkish and Australian students be accounted for by pupil background characteristics (attitudes/motivations and home resources)?"}, {"section_title": "METHOD Participants", "text": "The data of this study were obtained from the TIMSS 2007 and TIMSS 2011 database. In TIMSS, the target population comprises all students at the fourth and eighth grade of participating countries. This study used all Turkish and Australian eighth grade students who answered released items. The 2007 data from 1588 Turkish students (732 females and 856 males) and 1463 Australian students (646 females and 817 males) and the 2011 data from 503 Turkish students (270 females and 233 males) and 524 Australian students (256 females and 268 males) were investigated."}, {"section_title": "Measures", "text": "TIMSS gathered data on student's mathematics achievement and student's background characteristics via achievement tests and a student questionnaire, respectively. The mathematics achievement items in TIMSS are either in multiple choice format or in open-ended format. The TIMSS report mentions for each achievement item which of the three cognitive domains it represents. The questionnaire items are generally in Likert format asking endorsement. In the TIMSS questionnaire, related to student aptitude variables of Walberg's model, information was collected about self-confidence, positive affects toward mathematics, and valuing mathematics. The indicators of the latent variables of self-confidence, positive affect toward math, and valuing mathematics were selected in line with TIMSS definitions. The TIMSS report also mentions indexes for each of these latent variables (Mullis, Martin, & Foy, 2008) . Self-confidence was represented by four questionnaire items, such as \"I usually do well in mathematics\", and \"Math is harder for me\". Positive affect toward mathematics was represented by three items, such as \"I enjoy learning mathematics\" and \"Mathematics is boring\". Valuing mathematics was represented by four items such as \"I think learning mathematics will help me in my daily life\" and \"I need mathematics to learn other school subjects\". Related to environmental variables of Walberg's model, information about home educational resources was collected in TIMSS. The observed items related to educational resources at home that had at least three response categories were used to represent educational resources at home latent variable. In the study, the construct of educational resources at home was represented by items such as \"the number of books in student's home\" and \"the highest educational level of mother\". Finally, student-centered classroom learning activities were represented by four items about practices in mathematics lessons, such as \"we explain our answers\" and \"we decide on our own procedures for solving complex problems\"."}, {"section_title": "Data analysis", "text": "As a first analysis, construct equivalence and item bias were evaluated to ensure that the released achievement items represented the same cognitive constructs and they were item bias free for Turkish and Australian students. For construct equivalence analysis, the three-dimensional cognitive domain structure proposed by TIMSS in mathematics test was evaluated. In the analysis, each item was associated with its own cognitive domain and invariance of this structure between the two countries was tested. This analysis was conducted for booklet one to five, as these booklets had more than one item associated with the relevant cognitive domains. These five booklets were administered to both Turkish and Australian students. Following construct equivalence evaluation, item bias was investigated using structural equation modeling (SEM). In SEM, the metric and the scalar model are expected to have values of CFI and TLI that are .90 or above (Cheng & Rensvold, 2002) . If the difference in fit between metric and scalar model is larger than .01, modification indices are investigated to identify items that affect this difference, possibly followed by the removal of these items.\nThe TIMSS study employed standardized procedures for administration, which reduced the likelihood of administration differences as a source of method bias. Van de Vijver, Hofer, and Chasiotis (2010) suggested several steps to minimize the method bias in cross-cultural studies. Test administrators should be given an intensive training, a detailed instructions and a manual for administration, scoring, and interpretation should be prepared, and important variables related to samples should be balanced. The technical report published after each TIMSS administration reported how operations were done and how qualities of procedures were assured . It is stated that the TIMSS & PIRLS International Study Center prepared a document that describes step by step all operational activities. National research coordinators of each country were provided special software to support sampling, tracking classes and students, administering questionnaires, documenting scoring reliability and creating and checking data files. The TIMSS & PIRLS International Study Center also provided intensive training for each country in these administrative jobs. Additionally, International Quality Control Monitors (IQCM) had visited at least 15 schools to control the administration of the assessment and data collection process, and to evaluate the quality of the testing sessions. All of these strict controls are expected to lead a measure that minimizes administration differences as sources of method bias. Additionally, Hooper, Arora, Martin, and Mullis (2013) examined whether extreme response sets occurred for each country in TIMSS 2011 questionnaire and they reported that in both Turkey and Australia, only 0.1% of the students marked \"agree a lot\" regardless of the direction of the items. This value ranged from 0.0% to 2.2% among other countries. This report also showed that for Turkey and Australia, method bias was not a serious problem for the results obtained in this study. An important source of method bias in the cognitive instruments that could not be controlled was familiarity with the types of tests used in TIMSS. The released data set does not provide information about factors such as previous test experience. As a consequence, we could not evaluate the influence of test familiarity as a source of method bias.\nAfter having evaluated construct bias and item bias, the nature of mathematics achievement differences among Turkish and Australian students was explored. In this analysis, firstly, a structural equation model related to factors predicting mathematics achievement was formed and tested for measurement invariance. Secondly, factors predicting mathematics achievement of Turkish and Australian students were identified. Lastly, mean differences of these latent factors were analyzed to understand whether means of significant latent factors also differed between these countries by conducting latent mean structure analysis. In the analysis, Turkey was chosen as reference country. The MPLUS 7.11 program was used for testing construct equivalence and item bias, for examining the relationships between background variables and test achievement, and for latent mean structure analysis.\nFinally, we used propensity score matching to identify the remaining achievement differences between these countries (if any), when students in both groups are matched based on background variables with a presumed bearing on mathematics achievement scores. Consequently, we matched students based on the background variables and evaluate what would be the results if Turkish students had the same background variables as Australians. In order to conduct this analysis, propensity scores of each student were estimated by full optimal matching procedure (e.g., Guo & Fraser, 2009) , which is a novel approach. A full optimal matching procedure was selected because in this method all students were kept in the analysis, whereas in other matching procedures unmatched parts of the sample are discarded from the analysis (e.g., exact matching, neighbor matching, and optimal matching) (Stuart, 2010) . The MatchIt R package (Ho, Imai, King, & Stuart, 2007) was used to do the matching and to estimate propensity scores. Then, the propensity scores were used as a covariate in a MANCOVA analysis to test remaining differences among these students."}, {"section_title": "RESULTS", "text": "We first present the results of the TIMSS 2007 dataset, followed by the crossvalidation of the results based on the TIMSS 2011 dataset."}, {"section_title": "Internal consistency analysis of the instrument", "text": "The values of Cronbach's alpha reliability coefficients in TIMSS 2007 mathematics test booklets showed values ranging from .732 to .903 for Turkish students and from .710 to .875 for Australian students. These values are satisfactory (Cicchetti, 1994) ."}, {"section_title": "Construct equivalence of TIMSS cognitive domain structure and item bias", "text": "In order to answer the first research question, construct equivalence of the cognitive structure and item bias for Turkish and Australian students were evaluated. A central question to be addressed in construct equivalence was whether the three-dimensional structure proposed by TIMSS was invariant and therefore had the same meaning for Turkish and Australian students. Configural model results showed that for all five booklets, the fit values were within acceptable ranges as CFI and TLI values were .90 or above (Cheung & Rensvold, 2002 ) (see Table 1 ). This evidence supportive of construct equivalence implied that mathematics skills as measured by the TIMSS mathematics tests were free of construct bias for both groups of students. Table 1 showed that for all five booklets the differences between the metric and scalar model were larger than .01, which implied that there might be some biased items. Modification indices were used to evaluate the difference between the metric and scalar model smaller and to identify biased items. For booklet one, modification indices suggested that items M022049 and M042301A were problematic in terms of bias. The problematic items in the first booklet are given in Appendix A. According to the TIMSS classification, item M022049 was related to the geometry content domain and the reasoning cognitive domain whereas M042301A was related to the algebra content domain and knowing cognitive domain. These two items were evaluated in terms of language by three experts and no major problems with translation of the items were identified. However, in these items visualization and generalization skills were measured. Turkish students generally work with routine problems that require a solution process mainly consisting of conducting calculations. For instance, if only the sum of interior angles was asked in item M042301A, the items might not be identified as biased. When it is required to link and generalize the sum of interior angles with the number of triangles in it, the item probably became relatively difficult for Turkish students. Therefore, the reason why these items functioned differentially might be that these items required visualization and generalization which were not well covered in the Turkish educational system at that time. Removing these two items produced a model (\u0394CFI = .006) that was invariant for the two countries. For the other four booklets, removing items that were suggested by modification indices did not produce an invariant model (See Table 1 ) unless many items were removed, thereby challenging the coverage of the underlying concept by the remaining items. Therefore, it was concluded that only the first booklet, with some revision, was invariant and free of item bias across groups, a prerequisite for our propensity scoring analyses. These two biased items in booklet one were removed from following analyses.\nIn Table 2 , achievement differences between Turkish and Australian students before and after removing biased items were reported. One biased item from knowing and one biased item from reasoning were removed from the first booklet. Then achievement differences before and after removing were evaluated. This was done by conducting a MANOVA with country as the independent variable and the achievement scores as dependent variables. In the first step all item scores were used to compute the achievement score, in the second step scores on the presumably biased items were not used to compute the total scores. We followed the same procedure for the second booklet where we removed four items in two steps of two items. As can be seen in Table 2 , effect sizes did not change after presumably biased items were removed. So, our results suggested that mathematics achievement differences between Turkish and Australian students could not be accounted for by item bias. Therefore, the answer for the first research question is that mathematics achievement differences between Turkish and Australian students could not be accounted for by item bias. It is unlikely that achievement differences between Turkish and Australian students are due to the fact that the items were not originally developed in Turkish but in English."}, {"section_title": "Nature of mathematics achievement differences", "text": "The second research question dealt with factors that can predict mathematics achievement, whether these factors differ in magnitude, and whether these factors were equally predictive in Turkey and Australia. A SEM model was formed by using related TIMSS student factors as achievement predictor and by using students' TIMSS mathematics cognitive domain scores as achievement indicator. In the model, the latent factors that were expected to predict mathematics achievement were selfconfidence, positive affect toward mathematics, valuing mathematics, educational resources at home, and classroom learning activities. In the model, mathematics achievement (based on the first booklet only to avoid bias problems) was represented by knowing, applying, and reasoning cognitive domains and the students' domain scores were estimated by using an IRT two-parameter logistic model. The first step in evaluating the model was to test measurement invariance of the achievement model for Turkish and Australian students (See Figure 1) . The demonstration of measurement invariance was a necessary condition to make evaluations about effects of these factors in both countries. Configural, weak (factor loadings are invariant), and strong (factor loadings and regression coefficients are invariant) invariance were supported as the \u0394CFI value was less than .010 (See Table 3 ). This result implied that the same factors were effective in predicting mathematics achievement of both countries. Therefore, the same model given in Figure 1 could be used to identify significant predictors of mathematics achievement. The model explained 52% of the variance of mathematics achievement.\nThe second step was to identify which factors of the model could predict mathematics achievement. Standardized regression coefficients showed that having "}, {"section_title": "Effects of propensity scores on cross-national achievement differences", "text": "We were interested in the relative importance of educational resources at home and self-confidence in predicting cross-cultural achievement differences, given that these variables are very different in nature. To achieve this, propensity scores for both groups were estimated by matching Turkish and Australian students firstly on educational resources at home, then on self-confidence, and finally on both variables using full optimal matching procedure. Propensity score matching provides an estimate of what the achievement difference would be between Turkish and Australian students if they had an equal level of these background variables. Table 5 shows MANOVA results before correcting for propensity scores and MANCOVA results after correction. Before correcting, Australian and Turkish students had similar scores on knowledge items but Australian students had higher achievement scores on applying and reasoning cognitive domain scores, with effect sizes of .08 and .02, respectively (medium and small according to Cohen's, 1988, guideline) . After using only the self-confidence propensity score as a covariate, the effect size of the difference decreased; yet, Australian students were still more successful than Turkish students in applying and reasoning cognitive domains. After using only the educational resources at home propensity score as a covariate, the direction of achievement difference was reversed and Turkish students were more successful than Australian students in the knowing and reasoning domains. Finally, when correcting for both sets of variables, the results were similar to those found when only educational resources at home were used for matching. Therefore, it was concluded that educational resources and self-confidence were indeed effective in explaining achievement differences (with the former being more effective than the latter)."}, {"section_title": "Cross-validation of the results with TIMSS 2011 dataset", "text": "We investigated whether results obtained with the TIMSS 2007 dataset could be replicated in the TIMSS 2011 dataset (using booklet 1 of that data set). Booklet 1 of TIMSS 2011 had 26 items and all of these items were released. Invariance results are reported in Table 6 . As can be seen there, the CFI differences between metric and scalar model were larger than .01, which implied that there might be some biased items for Turkish and Australian students. Modification indices were used to make the difference between metric and scalar model smaller and to identify biased items. Modification indices suggested that items M052061, M052214, M052408 and M052429 revealed bias. Removing these items produced an unbiased form of the test for Turkish and Australian students.\nIn Table 7 , achievement differences between Turkish and Australian students before and after removing biased items were reported. Effect sizes did not change dramatically after presumably biased items were removed. So, our results suggested that mathematics achievement differences between Turkish and Australian students could not be accounted for by bias. Therefore, the answer to the first research question is that mathematics achievement differences between Turkish and Australian students could not be accounted for by bias. It is unlikely that achievement differences between Turkish and Australian students are due to the fact that the items were not originally developed in Turkish but in English, which replicates findings for the 2007 dataset. In order to identify which factors predict mathematics achievement, whether these factors differ in magnitude, and whether these factors were equally predictive in Turkey and Australia, a SEM model was made by using related TIMSS student factors as achievement predictor and by using students' TIMSS mathematics cognitive domain scores as achievement indicators. In the model, the latent factors to predict mathematics achievement were self-confidence, positive affect toward mathematics, valuing mathematics, and educational resources at home. Classroom learning activities items were not included in the 2011 questionnaire; therefore, this latent variable could not be used. Configural, weak (factor loadings are invariant), and strong (factor loadings and regression coefficients are invariant) invariance were supported (See Table 8 ). This result implies that the same factors were effective in predicting mathematics achievement of both countries as in the TIMSS 2007 dataset.\nThe second step was to identify which factors of the model could predict mathematics achievement. Standardized regression coefficients showed that having high self-confidence (\u03b2 = .67) and having more educational resources at home (\u03b2 = .34) were significantly and positively related with mathematics achievement of students in both countries. Positive affect toward mathematics was found to be negatively related to mathematics, yet with a small effect size (\u03b2 = -.12). Valuing mathematics was not significantly related to mathematics achievement in any country.\nThe third step was to evaluate latent factor means on all latent variables. Especially, latent mean differences on significant factors that were important in predicting mathematics achievement would expect to explain the nature of mathematics achievement difference between these two countries. Table 9 showed that Australian students had a significantly higher level of educational resources at home and they value mathematics whereas Turkish students had a significantly higher level of positive affect towards mathematics.\nWe were interested in the relative importance of educational resources at home and self-confidence in predicting cross-cultural achievement differences, given that these variables are very different in nature. In order to achieve this goal, propensity scores for both groups were estimated by matching Turkish and Australian students firstly on educational resources at home, then on self-confidence, and finally on both groups of variables using full optimal matching procedure. Before correcting, Australian and Turkish students had similar scores on applying and reasoning items but Australian students had higher achievement scores on knowledge items (See Table 10 ). After using only self-confidence propensity score as a covariate, the effect size of the difference decreased; however, Australian students were still more successful than Turkish students in knowledge cognitive domains. After using only the educational resources at home propensity score as a covariate, Turkish students were more successful than Australian students in applying and reasoning. Finally, when correcting for both sets of variables, the results were similar to those found when only educational resources at home were used for matching. Therefore, it was concluded that educational resources and self-confidence were indeed effective in explaining achievement differences (with the former being more effective than the latter), which we also found in the 2007 dataset."}, {"section_title": "DISCUSSION", "text": "We were interested in understanding the relatively low mathematics achievement level of Turkey in international studies. We used data from the international TIMSS 2007 and 2011 study to compare Turkey with Australia in terms of mathematics achievement. Australia was chosen as a reference as it is a country with a typical Western type of education and a higher achievement level in TIMSS than Turkey. In the study, we were interested in identifying predictors of mathematics achievement in Turkey and Australia and understanding how many achievement differences would still be remaining if we statistically correct for bias and if we match the Australian and Turkish samples on relevant background variables. Identifying significant predictors and evaluating their effects on achievement are expected to increase our insights and to contribute and help stakeholders of education to appreciate how to increase scores in these large scale tests, especially if the predictor variables could be influenced by educational policy measures.\nIn a first step we identified construct and item bias. Construct bias and item bias results indicated that conditions of construct and metric invariance were met in all five booklets but that only the first booklet showed scalar invariance after the removal of two items and could be used to compare Turkish and Australian students. Having a booklet free of construct and item bias was an important condition for subsequent analyses as we wanted to compare achievement levels that were not influenced by biasing factors. Our item bias analysis led to two conclusions. First, we found that even the carefully constructed educational achievement tests of TIMSS produced much item bias. Follow-up studies would be needed to address the nature of the bias. So, we can only speculate here about the nature of the bias. Van Schilt-Mol (2007) identified item bias in nation-wide administered educational tests (Cito tests) in Dutch primary schools. The immigrant sample comprised many ethnic groups, among which Turkish-Dutch. She found that there was no single reason of item bias but that specific words or concepts were less familiar to immigrant pupils compared to mainstream pupils. She found that removal of such words or concepts could eliminate ethnic group performance differences. Second, a comparison of the country differences before and after removing biased items in the first two booklets, which contained only a few biased items, revealed that it is unlikely that achievement differences in TIMSS scores found in Turkey and Australia can be accounted for by item bias in either the 2007 or 2011 datasets. In the achievement model, which was found to hold in both countries, the selfconfidence of the students, which is related to the student aptitude variable of Walberg's model, and their educational resources at home, which is related to environmental variable of Walberg's model, were two significant predictors of achievement. Australian students reported both in 2007 and in 2011 that they had not only more educational resources at home but they also had a higher selfconfidence (although the latter difference was only significant in 2007). Matching Turkish and Australian students on these two background factors using a propensity score matching approach indicated that educational resources at home were more effective than self-confidence in explaining achievement differences. This confirms earlier findings; many scholars found a similar relationship between having more educational resources at home and being more successful (Berberoglu et al. 2003; Chevalier & Lanot, 2002; Fuchs & W\u00f6\u00dfmann, 2007; Kaya & Rice, 2010 ) and a similar relationship between self-confidence and achievement (Abu-Hilal, 2000; Marsh, 1986; Shen & Tam, 2008) .\nThe classroom learning activities, related to the third dimension of Walberg's achievement model, were not significant in predicting achievement in 2007. In 2011, this dimension was not represented in the questionnaire. We could not find any relationship between student-centered classroom activities and mathematics achievement. One of the reasons for this might be that student-centered activities might not be performed adequately during the lessons. In student-centered teaching, not only students but also teachers should be active. It is expected from students to construct new information by using prior knowledge under teacher guidance. The coaching role of the teacher in the student-centered learning environment is crucial. Letting students work on their own is not the target of student-centered teaching. Similar conclusions were reached by other scholars who showed that student-centered learning activities were not positively associated with achievement (Kalender & Berberoglu, 2009; Von Secker & Lissitz, 1999) . Moreover, another reason for the lack of correlation could be that, as indicated earlier, in Turkey and in Australia, the new curricula were not yet effective in 2007. Therefore, teachers still adhered to teacher-centered practices. More research is needed before recommending how pupil-centered practices should be implemented in the Turkish math classroom.\nIn the study, mathematics achievement was modeled by using the cognitive domain structure of TIMSS, comprising knowing, applying, and reasoning. There is a hierarchical structure among these three cognitive domains. Going from knowing to reasoning, the cognitive demands to answer the items are assumed to increase. Results showed that Turkish and Australian students had similar mean scores in the knowing domain in 2007. These findings suggest that Turkish and Australian students do not show appreciable differences in knowledge of basic mathematical facts, concepts, and procedures. However, when an item is related to applying or reasoning, Turkish students tended to show a lower achievement than their Australian peers. Application items are related to students' ability to apply knowledge and conceptual understanding to solve problems whereas reasoning items require the use of mathematics in unfamiliar situations, complex contexts, and multistep problems. So, our study suggests that Turkish students experience more problems with applying their knowledge in both routine and non-routine problem situations than Australian students. Similarly, 2011 data suggested that Turkish students got a significantly lower score in knowing and reasoning and a slightly lower score in applying (although the latter was not significant). By implication, our study points to a need to improve cognitive skills of Turkish students. In 2013, the mathematics curriculum of the fifth up to the eighth grade (middle school) was updated. The general purpose of the curriculum is stated as helping students to acquire mathematical knowledge, skills, and attitudes. It is also claimed that emphasis is given to conceptual learning, fluency in operation, mathematical communication, valuing mathematics, and developing problem solving skills (MONE, 2013) . This policy addresses the problem we observed in our study. No systematic evaluation of the impact of the new policy is available yet. Data, predating the policy change, indicate that the Turkish educational system was not much focused on the development of higher-order skills. Classroom activities in Turkey were still mainly focusing on items that are related to basic skills as comprehension rather than higher order thinking skills as problem solving (Doganay & Bal, 2010) . Similarly, preservice teachers in Turkey reported that they experience difficulties in finding non-routine problem situations (Temur, 2012) . Based on TIMSS 2007 and 2011 data, our findings show that higher-order cognitive processes, such as problem solving and reasoning, are not adequately developed among Turkish students. In order to obtain firmer evidence, outcomes of Turkish students in coming cycles of TIMSS, for instance TIMSS 2019 and PISA 2023, need to be investigated for the achievement levels in higher-order thinking skills to see the effects of 2013 curriculum.\nAll findings indicate that as educational resources at home are more effective in predicting mathematics achievement and as Turkish students were less successful in many cognitive domains, it is important to emphasize that parents could be actively involved in stimulating their children by providing a cognitively enriching environment. The higher education level of parents and the possession of more books at home are proxies for a complex set of variables which are presumably related to a cognitively more stimulating environment in the home. Even if the mechanisms behind the enrichment are not yet well understood, it is clear that providing a stimulating learning at home can contribute considerably to the child's cognitive development.\nOur study has some limitations. First of all, only data of one booklet was suitable to compare Turkish and Australian students. Although the number of items to estimate achievement level of students was not very small (27 in 2007 and 26 in 2011) , only students who answered the first booklet could be used. As we estimated student abilities based on released items and as we did not compare results of students from different booklets, we did not include sampling weights in our estimation. This might lead to an increase in the standard error of the estimation. Another limitation is related to the choice of comparison country. Australia was chosen as TIMSS items were originally prepared in English and Australia is one of the English speaking countries that got higher achievement score than Turkey in TIMSS 2007 and 2011. In future research, the same procedures described here could be repeated by using data from other English speaking countries or from any other high achieving country. Additionally, there might be other factors, such as teacher and school level, that are significant in predicting student achievement. As this study focused on student level variables and propensity score application to student level predictors, these factors were not included in our study."}, {"section_title": "ACKNOWLEDGEMENT", "text": "This paper is a part of a research project supported by the Scientific and Technological Research Council of Turkey (TUBITAK) with program no. 2219. Any opinions expressed here are those of the authors and do not necessarily reflect the views of the TUBITAK."}]