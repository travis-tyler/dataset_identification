[{"section_title": "LIST OF TABLES", "text": ". Results of Fit Indices of J by K for DINA(-H) when N=300\u2026\u2026\u2026\u2026. Table A2. Results of Fit Indices of J by K for DINA(-H) when N=1000.\u2026\u2026\u2026.. Table A3. Results of Fit Indices of J by K for DINA(-H) when N=3000.\u2026\u2026\u2026.. Table A4. Summary Statistics for the Guessing Parameter of J by K for DINA(-H) when N= 300\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026. Table A5. Summary Statistics for the Guessing Parameter of J by K for DINA(-H) when N= 1000..\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026. xiv Table A6. Summary Statistics for the Guessing Parameter of J by K for DINA(-H) when N= 3000\u2026..\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026. Table A7. Summary Statistics for the Slip Parameter of J by K for DINA (-H) when N= 300\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026. Table A8. Summary Statistics for the Slip Parameter of J by K for DINA(-H) when N= 1000\u2026\u2026\u2026\u2026\u2026\u2026...\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026. Table A9. Summary Statistics for the Slip Parameter of J by K for DINA(-H) when N= 3000\u2026\u2026...\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026. Table A10. Results of Fit Indices of J by K for DINO(-H) when N=300\u2026\u2026\u2026\u2026. Table A11. Results of Fit Indices of J by K for DINO(-H) when N=1000.\u2026\u2026\u2026.. Table A12. Results of Fit Indices of J by K for DINO(-H) when N=3000.\u2026\u2026\u2026.. Table A13. Summary Statistics for the Guessing Parameter of J by K for DINO(-H) when N= 300\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026. xv Table A21. Differences of Summary Statistics in the Guessing Parameter between the DINA(-H) and DINO(-H) Models for the Interaction Effect of N by K\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026 Table A22. Differences of Summary Statistics in the Slip Parameter between the DINA(-H) and DINO(-H) Models for the Interaction Effect of N by K\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.           Summary statistics for the guessing parameter estimates for the three-way interaction effect of N by J by K for DINO(-H)\u2026\u2026\u2026... Figure 4-10. Summary statistics for the slip parameter estimates for the threeway interaction effect of N by J by K for DINO(-H)\u2026\u2026\u2026\u2026\u2026\u2026. xvii identify the examinees' ability to master fine-grained skills based on a pre-specified matrix. Cognitive diagnostic tests can be used to identify the skill combinations that the examinee is likely to either possess or not possess. The results provide specific informational feedback to the examinees so they can make inferences about their mastery of different cognitive skills. CDMs have been increasingly valued in the literature of measurement in recent years, although the popularity of their applied research remains lower than the more common Item Response Theory (IRT) models. Unlike the IRT models that focus on item-level analysis and assign scores to examinees, CDMs focus more on providing examinees and teachers with informative and diagnostic feedback on the specific skills examinees need to improve. More specifically, they assign examinees attribute profiles indicating the skills that they have or have not mastered. In CDMs, examinees' attribute profiles (sometimes called the estimated skill patterns) are categorical latent variables (discrete variables). Thus, in the scoring process, CDMs classify examinee responses into various discrete latent classes (i.e., a set of combinations of 0s and 1s, where 0 means non-mastery and 1 means mastery of an individual skill). Almost all CDMs require specifying a K J \u00d7 Q-matrix, in which K stands for the number of skills being measured by the test and J stands for the number of items in a test (Tatsuoka, 1995). Applying CDMs has both pros and cons. CDMs are well-known for their advantages in providing informational feedback about examinees' ability to master multiple fine-grained skills. One common limitation in current CDMs is that they use 0/1 discrete variables to represent true person profiles, rather than a continuous variable. However, while scoring, it is possible to report the probability or percentage of mastering each attribute (e.g., report the probability estimates of mastery from EAP scoring results for each attribute for an examinee) to make up for the limitation at some point. A continuous score scale could be used for reporting purposes, instead of just 0/1. de la Torre and Karelitz (2009) examined the nature of the underlying latent trait (continuous or discrete) and varying estimation models, and found that low diagnostic items (i.e., high misclassification rate of attribute) were more influential on estimation inaccuracy than the noise introduced by fitting the wrong model and the nature of the latent trait."}, {"section_title": "Motivation for the Study", "text": "Researchers have suggested that mathematical and scientific concepts (and other conceptual domains) are not independent knowledge segments, and there are learning sequences in the curriculum that fits learners' schema-constructing process (e.g., Kuhn, 2001;Vosniadou & Brewer, 1992). Since the skills in mathematics are not independent of each other, it is crucial to use an estimation model that is consistent with the assumptions about relationships among attributes. Specifying attribute profiles incorrectly would affect the accuracy of estimates of item and attribute parameters. Considering the hierarchical nature of mathematics attributes, the conventional CDMs that do not assume this character, and the results of the calibration based on these models may be biased or less accurate. Hence, the relationships among attributes and the possible attribute profiles need to be identified correctly based on content specific theoretical background along with a careful look at the test blueprint before a CDM calibration is conducted and interpreted. Several CDMs have been applied to parameterize the latent attribute space to model the relationships among attributes and help improve the efficiency in estimating parameters. These approaches include log-linear (Maris, 1999;Xu & von Davier, 2008), unstructured tetrachoric correlation (Hartz, 2002), and structured tetrachoric correlation (de la Torre & Douglas, 2004;Templin, 2004). The log-linear models parameterize the latent class probabilities using a log-linear model that contains main effects and interaction effects (all possible combinations of attributes). The unstructured tetrachoric models represent the tetrachoric correlations of all attributes pairs directly, and reduce the complexity of model space. The structured tetrachoric models impose constraints on the tetrachoric correlation matrix to simplify the estimation process using prior hypotheses about how strongly attributes are correlated. However, none of these approaches incorporate the hierarchical nature of cognitive skills and reduce the number of possible attribute profiles directly. The attribute hierarchy method (AHM) (Gierl, 2007;Gierl, Cui, & Zhou, 2009;Gierl, Leighton, & Hunka, 2007;Leighton, Gierl, & Hunka, 2004) is another cognitive diagnostic psychometric method designed to explicitly model the hierarchical dependencies among attributes underlying examinees' problem solving on test items. The AHM is based on the assumption that test items can be described by a set of hierarchically ordered skills, and that examinee responses can be classified into different attribute profiles based on the structured hierarchical models. Researchers' attention to the impact of cognitive theory on test design has been very limited (Gierl & Zhou, 2008;Leighton et al., 2004). The assumption of skill dependency that AHM holds is consistent with findings from cognitive research (e.g., Kuhn, 2001;Vosniadou & Brewer, 1992) that suggests some preliminary knowledge can be defined as the foundation for other more sophisticated knowledge or skills with higher cognitive loadings. The concept of hierarchically ordered cognitive skills is clearly observed in mathematics learning. For example, a student learns to calculate using single digits before learning to calculate using multiple digits. If the hierarchical relationships among skills are specified, the number of permissible items is decreased and the possible attribute profiles can be reduced from the number of 2 K (Gierl et al., 2007;Leighton et al., 2004;Rupp, Templin, & Henson, 2010). The AHM could be a useful technique to design a test blueprint based on the cognitive skill hierarchies. Since the AHM is more like an analytical method and a test developing guideline that focuses on estimating attribute profiles, it will be beneficial if a model based on AHM is developed to allow item parameters to be estimated directly. In addition, de la Torre and Karelitz (2009) tried to estimate item parameters based on a linear structure of five attributes with each item class unidimensionally represented as a cut point on the latent continuum. The focus of their study was to transform the IRT 2PL item parameters into the DINA model's slip and guessing parameters, and then examine how the congruence between the nature of the underlying latent trait (continuous or discrete) and fitted model affects DINA item parameter estimation and attribute classification under different diagnostic conditions, rather than focusing on the hierarchical structures of attributes and its estimation. If an assessment is developed based on hierarchically structured cognitive skills, and the Q-matrix for each test is built up and coded based on those skills, analyzing the tests using CDMs would directly provide examinees, teachers, or parents with more valuable information about which fundamental, intermediate, or advanced skills the testtakers possess. Instructors can also take the feedback to reflect on their teaching procedures and curricular development. Moreover, for some test batteries that target various grade levels, conducting CDM calibrations incorporating the hierarchically structured cognitive skills would help estimate both item parameters and examinee attribute profiles based on different requirements about the mastery of various levels of skills. While CDMs offer valuable information about specific skills, their usefulness is limited by the time and sample sizes required to test multiple skills simultaneously. To estimate examinees' ability parameters via a CDM, examinees' skill response patterns are classified into different attribute profiles (latent classes). For most CDMs without any relationship or constraints imposed on the latent classes, the maximal number of possible latent classes is 2 K , in which K is the number of attributes measured by the assessment. There are 2 K -1 parameters that need to be estimated by implementing CDMs with dichotomous latent attribute variables. As the number of attributes increase, the number of estimated parameters also increases, as does the required sample size and computing time needed to attain reliable results. Analyzing an assessment measuring many attributes is difficult due to the large sample size required to fit a CDM, and to obtain reliable parameter estimates, convergence, and computational efficiency. Most CDM application examples in the literature are limited to no more than eight attributes (Hartz, 2002;Maris, 1999;Rupp & Templin, 2008b) because of the long computing time for models with larger numbers of attributes and items. If the number of latent classes can be reduced from 2 K , the sample size needed to obtain stable parameter estimates from CDM calibrations will decrease. This will also result in faster computing time. One solution to decrease the number of latent classes is to impose hierarchical structures (Leighton et al., 2004) on skills. The resulting approach is able to assess and analyze more attributes by reducing the number of possible latent classes and the sample size requirement (de la Torre, 2008Torre, , 2009de la Torre & Lee, 2010). Two methods to estimate attributes with hierarchical structures could be as de la Torre (2012) suggested: First, keeping the EM algorithm as is, but without any gain in efficiency, the prior value of attribute patterns not possible under the hierarchy can be set to 0, and second, for greater efficiency, but requiring minor modifications of the EM algorithm, attribute patterns not possible under the hierarchy can be dropped. (session 5: p.7) To explore whether data from a test with the hierarchical orders fit CDMs, this study intended to consider hierarchically structured cognitive skills when determining attributes and identifying attribute profiles to reduce the number of possible latent classes and decrease sample size requirements, which is the more efficient method suggested by de la Torre (2012). The deterministic, inputs, noisy, \"and\" gate (DINA; Haertel, 1989Haertel, , 1990Junker & Sijtsma, 2001) model and the deterministic, inputs, noisy, \"or\" gate (DINO; Templin & Henson, 2006) model are employed in the study. The DINA model is increasingly valued for its interpretability and accessibility (de la Torre, 2009), for the invariance property of its parameters (de la Torre & Lee, 2010), and for good model-data fit (de la Torre & Douglas, 2008). Being one of the simplest CDMs with only two item parameters (slip and guessing), the DINA model is the foundation of other CDMs; it is easily estimated and has gained much attention in recent CDM studies (Huebner & Wang, 2011). Hence, the DINA model is a good choice to apply the proposed approach of imposing skill hierarchies on possible attribute profiles. Likewise, the DINO model is a statistically simpler CDM like the DINA model, and is the counterpart of the DINA model based on slightly different assumptions about the possibility of answering an item correctly. Hence, these two models provide a good comparison in understanding the feasibility of analyzing the hierarchically structured test data using CDMs. The proposed models with a skill hierarchy constraint on the possible attribute profiles are different from the conventional DINA and DINO models, and are referred to in this study as the DINA-H and DINO-H models. They are not different models from the conventional models in terms of mathematical representation. They differ only in the constraint on defining attribute profiles according to the skill hierarchy. The intent of this study is to apply the proposed skill hierarchy approach in conjunction with the DINA and DINO models to analyze both the real data and simulated data. The purpose for conducting the real data analysis is to illustrate the proposed approach of applying the DINA-H and DINO-H models, to compare the results of conventional DINA and DINO models to their hierarchical counterparts with different sample sizes, and to promote the potential contributions of constructing skill hierarchies for teachers and students. The released Trends in International Mathematics and Science Study 2003 (i.e., TIMSS) math test was used in the study. The cognitive dimension was categorically defined as knowing facts and procedures, using concepts, solving routine problems, and reasoning. However, the four categories of cognitive domains defined in TIMSS items are not deemed as hierarchically ordered. This is because the first three levels (i.e., knowing facts and procedures, using concepts, and solving routine problems) of cognitive domains are not considered prerequisite to their next level in mathematics learning. Hence, the study adopted the Common Core State Standards (CCSS) as the attributes to develop the Q-matrix and its hierarchy. The purpose of conducting the simulation analysis is to evaluate and provide more detailed and supportive evidence about the effectiveness of the proposed models under various conditions (e.g., different number of attributes, different test lengths, sample sizes, and different structures of hierarchies). This would also allow for testing the reduction of the required sample sizes, model fits, the issues of convergence, and item parameter recoveries. Moreover, the simulation analysis is intended to explore the effect when the specified skill hierarchy in the data is inconsistent with the estimation model."}, {"section_title": "Significance of the Study", "text": "The study is unique in its distinctive feature of incorporating hierarchically structured skills into the conventional DINA/DINO models. The nature of mathematics concepts is that they are not independent of each other (Battista, 2004). For example, number and operation, algebra, geometry, measurement, and probability are not independent domains. Educators have discussed the proper learning sequences in mathematics teaching and learning (Baroody, Cibulskis, Lai, & Li, 2004;. There is a need for a model whose model specification, the relationships among attributes, possible attribute profiles, and Q-matrix are consistent with the theoretical background and test blueprint. The current DINA and DINO models assume independent skills up to 2 K of attribute profiles, without considering the situations when skills are hierarchically related in a certain structure. From mathematics educators' perspectives, mathematics concepts are hierarchically ordered. This needs to be reflected and considered in identifying the relationships among attributes, possible attribute profiles, and designing Q-matrix. The conventional DINA/DINO models only work when the skills are independent. If the skills are hierarchically related and the conventional models are applied, the parameter estimates could be biased and less accurate. New models are developed to apply when the skills are hierarchically ordered. This study contributes to education practices by incorporating skill hierarchies with assessments. The contributions include providing detailed informational feedback on students' learning progresses on varying hierarchical levels, and also promoting teacher enhancement of instructional procedures to match student development in the future. Specifically, by using the proposed models, the examinees' estimated attribute profiles can be obtained and then compared to the pre-specified attribute profiles. Using this feedback, teachers can determine whether their teaching sequence matches students' learning sequences, and whether their instructional procedures need to be modified. The simulation analysis would provide valuable information about the potential inaccuracy of parameter estimates due to misspecification of the relationships between attribute and possible attribute profiles. The simulation study examines the model fit and item parameter recovery when the data simulation models are different from the estimation models. Specifically, both the conventional DINA/DINO and new hierarchical models are applied when the attributes are independent or dependent in a specified hierarchical structure. The simulation study contributes to exploring the effect when model specification is consistent or inconsistent to the assumptions and characteristics of skills. The illustration of applying CDMs to a large-scale assessment demonstrates the feasibility of retrofitting (i.e., analyzing an already existing data set) TIMSS 2003 data. Studies of international assessments, such as the TIMSS, allow for worldwide comparisons. Although the intention of TIMSS is not to provide individual level scores or comparisons, successful application of a CDM to a large scale assessment can be a promising way to provide informational feedback about examinees' mastery in varying levels of skills. While other studies have tried retrofitting the TIMSS data, no research has applied or studied the concept of hierarchically ordered skills. The current study provides information that allows future research to conduct international comparisons and identifications of how examinees do or do not master specific fine-grained fundamental, intermediate, and advanced skills. Such comparisons will provide educators and policymakers with information on student achievement across countries that will be helpful in evaluating curricular development and in developing education reform strategies. The last contribution of the current study is to examine the performance of the proposed DINA-H and DINO-H models, and to provide information about model fit and item parameter recovery under varying conditions of different numbers of attributes, different test lengths, and sample sizes. The results of the study are expected to demonstrate the benefits, efficiencies, and feasibility of the proposed DINA-H and DINO-H models, and to facilitate the reduction of possible attribute profiles and the large sample size requirements in analyzing a CDM. The study also contributes to the analysis of tests that assess more attributes, and promote computational efficiency."}, {"section_title": "Research Objectives and Questions", "text": "This section includes two parts. The first part describes the objectives of the study, and the second part presents the research questions."}, {"section_title": "Objectives of the Inquiry", "text": "The study proposes to develop two modified CDMs, the DINA-H and the DINO-H models, which involve the hierarchical structure of cognitive skills. The purpose of these two models is to construct the hierarchies of cognitive skills in the model estimation process. This can facilitate reporting the mastery/non-mastery of skills with different levels of cognitive loadings. The intention of the study is to apply the proposed skill hierarchy approach in conjunction with the DINA and DINO models to analyze both the real data and the simulated data. Adopting the CCSS as attributes to code the Q-matrix and skill hierarchies, the analysis of the TIMSS 2003 data demonstrates the proposed models and the feasibility of retrofitting. The purpose for conducting the real data analysis is to illustrate the proposed models, to compare the results of conventional DINA and DINO models to their hierarchical counterparts with different sample sizes, and to promote the potential contributions of constructing skill hierarchies to teachers and students. The purpose of conducting the simulation analysis is to evaluate the effectiveness of the proposed models under various conditions (e.g., different number of attributes, different test lengths, and sample sizes). The objective includes examining item parameter recovery by analyzing and comparing various CDMs under varying assumptions of dependent or independent attributes. More specifically, the simulation study evaluates the item parameter recoveries and model fits when the specified skill hierarchy in the data is consistent or inconsistent with the estimation model, given a type of attribute profile patterns with dependent or independent skills, by using the conventional DINA and DINO models as well as the proposed DINA-H and DINO-H models."}, {"section_title": "Research Questions", "text": "This study is designed to address the following research questions: 1. How do the proposed DINA-H and DINO-H perform? Research questions 1.1, 1.2, 1.3, and 2.1 are addressed by the real data analysis; research questions 2.1, 2.2, 3.1, and 3.2 are addressed by the simulation analysis. This study analyzes both the real and simulated data. The fit statistics and other evaluation criteria are discussed in Chapter III. Throughout the study, the CDMs are assumed to hold. Details about CDMs and their assumptions are provided in Chapter II, while the procedures for the analyses are described in Chapter III."}, {"section_title": "Overview of the Dissertation", "text": "The dissertation is composed of five chapters, including the current chapter. Following Chapter I, Chapter II contains an introduction of CDMs, learning sequence, and attribute hierarchy. Chapter III presents a description of the methodology used in this investigation. Chapter IV provides the results of the study. Chapter V summarizes the findings and considers the implications of the results for psychometricians and educators."}, {"section_title": "CHAPTER II LITERATURE REVIEW", "text": "Chapter II consists of two main sections. The first section introduces general information about the cognitive diagnostic models, followed by more specific descriptions of several popular models. The second section discusses learning sequences and then introduces cognitive attribute hierarchies."}, {"section_title": "Cognitive Diagnostic Models", "text": "CDMs are developed to identify examinees' mastery or non-mastery of multiple fine-grained attributes based on a pre-specified matrix of attributes. Unlike Item Response Theory (IRT), which focuses on item-level analysis and assigning scores to examinees, CDMs emphasize providing examinees and teachers with informative and diagnostic feedback that allows examinees to know which specific skills they should improve."}, {"section_title": "Overview of Cognitive Diagnostic Models", "text": "CDMs have several synonyms that appear in the literature. These synonyms include cognitive diagnosis models (or cognitively diagnostic models) (de la Torre, 2009;Henson & Douglas, 2005;Huebner, 2010;Tatsuoka, 1995), diagnostic classification models (DCM) (Rupp & Templin, 2008a;Rupp et al., 2010), cognitive psychometric models (Rupp & Mislevy, 2007), multiple classification (latent class) models (Maris, 1999), latent response models (Maris, 1995), restricted latent class models (Haertel, 1989(Haertel, , 1990, structured IRT models (Rupp & Mislevy, 2007), and structured located latent class models (Xu & von Davier, 2006, 2008. These different terms highlight specific aspects of the model, such as the theoretical background, the statistical model, or the examinee respondent scoring. The most common term, CDM, is used consistently throughout the study. In CDMs, examinee attribute profiles contain categorical latent variables (discrete variables). These variables are the center of the scoring process, in which CDMs classify examinee responses into discrete latent classes (i.e., a set of combinations of 0 and 1 values where 0 means non-mastery and 1 means mastery of an individual skill). Tests that implement CDMs consist of dichotomous items, polytomous items, or mixed formats. Recently, CDMs have been implemented in assessing both academic performance (i.e., achievement tests) and psychological properties (i.e., psychological disorder assessments). The former more often relies upon the term \"skills\" to describe the kind of cognitive skills an assessment is intended to measure, whereas the latter tends to use the term \"attributes\" to address the features of a certain symptom. These two terms (skills and attributes) are used interchangeably throughout this study, and are not meant to indicate different, discrete categories of analysis or not intended to be discriminatory. In other words, attributes are equivalent to basic knowledge, skills, or cognitive processes."}, {"section_title": "Assumptions", "text": "Various CDMs hold different assumptions about how all the skills measured by an individual assessment interact with each other. CDMs also consider how examinees' skills influence their test performance. Some models, such as those implemented in assessing mathematics or reading comprehension skills, assume that an examinee must possess all the required skills for an item to answer that item correctly. For example, many studies adapted the DINA model to estimate the fraction and subtraction data collected by K. Tatsuoka (1990) and more recently by C. Tatsuoka (2002) (de la Torre, 2009;de la Torre & Douglas, 2004). Other models, like those often applied in medical and psychological diagnoses, assume that the absence of certain attributes (i.e., symptoms) can be compensated by the presence of other characteristics. For example, the DINO model was first advocated in analyzing gambling addictions (Templin & Henson, 2006). Despite the assumptions individual CDMs might hold, classifying these models based on their assumptions occurs inconsistently within the literature. According to some introductory articles (DiBello, Roussos, & Stout, 2007;Roussos, Templin, & Henson, 2007;Huebner, 2010), CDMs fall into two categories: non-compensatory and compensatory. The term \"non-compensatory\" means that an attribute does not compensate for the deficiency of another attribute in order to correctly respond to an item. Under this scheme, the term \"conjunctive\" is used interchangeably with noncompensatory. The non-compensatory models have the statistical representation of a product term over attributes in their model equations (Henson, Templin, & Willse, 2009). Hence, a successful task requires mastery of all necessary skills for successful performance, and a lack of competency in any one of the required skills causes an unsuccessful performance on the task. The non-compensatory models include the DINA model (Haertel, 1989;Junker & Sijtsma, 2001), the non-compensatory reparameterized unified model (NC-RUM or RUM), and the reduced reparameterized unified model (rRUM) (DiBello et al., 2007;Hartz, 2002), the noisy inputs, deterministic, \"and\" gate (NIDA) model (Junker & Sijtsma, 2001), the HYBRID model (Gitomer & Yamamoto, 1991), the unified model (UM;DiBello, Stout, & Roussos, 1995), and the conjunctive multiple classification latent class model (the conjunctive MCLCM; Maris, 1999). The term \"compensatory\" means that in order to perform a task successfully, a high enough level of competence in one attribute can compensate for a deficiency or low level of competence in another attribute, through the interaction of skills required by that task. Under this scheme, the term \"disjunctive\" is used interchangeably with compensatory. The probability of a positive response for an item is high when test takers master (or possess) at least one of the required attributes. Some of the compensatory models include the DINO model (i.e., the counterpart of the DINA model) (Templin & Henson, 2006), the compensatory reparameterized unified model (C-RUM), the noisy input deterministic \"or\" (NIDO) model (i.e., the counterpart of the NIDA model) , the General Diagnostic Model (GDM;von Davier, 2005), and the compensatory multiple classification latent class model (the compensatory MCLCM; Maris, 1999). Certain literature reviews (Henson et al., 2009;Rupp et al., 2010) classify CDMs in a different manner. According to these reviews, different CDMs fall into the noncompensatory and/or compensatory categories. However, each of these categories contains both conjunctive and disjunctive models. Under this scheme, conjunctive is not an alternative term for non-compensatory, and disjunctive is not an alternative term for compensatory. Instead, the non-compensatory models are characterized by \"the conditional relationship between any attribute and the item response depending on mastery or non-mastery of the remaining attributes\" (Henson et al., 2009, p. 192). Compensatory models are characterized by the \"conditional relationship between any attribute and the item response not depending on mastery or non-mastery of the remaining required attributes\" (Henson et al., 2009, p.192). Furthermore, the term \"conjunctive\" refers to the presence of all attributes leading to a positive response, from a deterministic perspective (Rupp & Templin, 2008a). That means a missing skill cannot be made up by the mastery of other skills. An examinee needs to possess all the required skills for an item in order to answer that item correctly. Disjunctive models define the probability of a correct response such that mastering a subset of the attributes is sufficient to have a high probability of a correct response. According to this system, certain CDMs are classified in different categories as compared to the previous paragraph. For example, the DINA model (e.g., de la Torre & Douglas, 2004;Junker & Sijtsma, 2001) is viewed under both systems as a non-compensatory model that uses a conjunctive condensation function (Maris, 1999). However, the DINO model is classified as a non-compensatory model with a disjunctive condensation function, instead of a compensatory disjunctive model. CDMs also hold assumptions about the conditional independence of attribute profiles and independence among examinees. Conditional independence means that item responses are independent conditionally on the latent class of the examinee, which is similar to local independence in IRT (Rupp et al., 2010). For those latent classes without any relationship or constraints imposed, the maximum number of possible latent classes is 2 K where K is the number of the attributes measured by an assessment. There are 2 K -1 parameters, which need to be estimated by implementing most CDMs with dichotomous latent attribute variables. Therefore, the number of parameters will be greater if more attributes are measured. To improve the efficiency of implementing CDMs, a variety of methods have been developed to reduce the number of parameters that need to be estimated. One method that reduces the complexity of the model space indirectly via parameters is a loglinear model (Henson et al., 2009). This method estimates the main effects and the interaction effects for each possible combination of attributes. In other methods, the relationships among attributes are assumed to be unstructured tetrachoric, structured tetrachoric, or hierarchical ordered in different CDMs. The unstructured tetrachoric models represent the tetrachoric correlations of all attribute pairs directly, without placing any constraints on the patterns of the tetrachoric correlations (Hartz, 2002). The difficulty in estimating multiple tetrachoric correlations and threshold parameters occurs when the structured tetrachoric models assume a priori about the tetrachoric correlations among the attributes. Two examples are the higher order DINA model in de la Torre and Douglas (2004), and the generalized normal-ogive model in Templin (2004). Another method has been proposed that applies hierarchies among cognitive skills in test developing process, and hence, reduces the number of possible attribute profiles (e.g., the attribute hierarchy method in Gierl et al. (2007)). To conduct CDM estimations, several approaches have been applied to model the relationship among attributes, and different attitudes toward the assumptions lead to the selection of a specific CDM."}, {"section_title": "Estimation", "text": "Estimating parameters using CDMs includes estimating the item parameters (or attribute parameters) and attribute profiles (i.e., respondent profiles or respondent parameters). Item parameters and attribute profiles may be estimated simultaneously by joint maximum likelihood estimation or marginalized maximum likelihood estimation using the expectation-maximization (EM) algorithms, such as the DINA model estimation described in de la Torre (2009), and the estimation of the generalized diagnostic model in von Davier (2005). Alternatively, item parameters and attribute profiles may be estimated simultaneously using a Markov Chain Monte Carlo (MCMC) approach, such as in the higher order DINA model estimation by de la Torre and Douglas (2004). Both approaches have their own advantages and disadvantages (Rupp et al., 2010). Initializing the EM algorithm requires setting up the starting values of the item parameters that must be estimated. However, choosing different starting values could result in a slight change in item parameter estimates. Moreover, computing the EM algorithm becomes more intensive as the number of latent classes increases. Sometimes imposing more constraints on the relationships among attributes makes the computation more complex. Conversely, conducting MCMC estimation does not require pre-calculation of the maximum likelihood estimators of the item parameters. MCMC estimation occasionally results in convergent issues, and the results are not reproducible because of its random simulations, whereas EM results can be replicated. Employing a CDM to analyze data, another common approach can be applied in which the marginal maximum likelihood (MML) algorithm is used by first assuming a prior population distribution of latent classes, estimating item parameters under this assumption, and then estimating the individual respondent parameters (Templin, 2004). To score and classify examinee respondents, there are three common approaches in assigning an examinee into a latent class: the maximum likelihood estimation (MLE), the maximum a posteriori (MAP) estimate of the posterior distribution, and an expected a posteriori (EAP) estimate for each attribute (Huebner & Wang, 2011). For MLE classification, the likelihood is computed at each attribute profile, and the examinee is assigned an estimated attribute profile that maximizes the likelihood. Sometimes when the distribution of attribute profiles is expected, the prior probabilities are obtained. At this point, MAP classification can be applied by computing the posterior probability using Bayes' theorem. EAP provides probability estimates for each attribute, whereas MLE and MAP do not (Huebner & Wang, 2011). However, computing MLE and MAP are more statistically straightforward. MLE and MAP find the mode of the posterior distribution, whereas EAP finds the average (Embretson & Reise, 2000). EAP calculates the probabilities of mastery for each attribute for an examinee and sets up a cutoff probability value (usually at 0.5) to classify the attribute into mastery or non-mastery (Huebner & Wang, 2011). Hence, this cutoff can be altered based on different research purposes. To evaluate the model fit for the CDM estimation, the common fit statistics used are the Akaike Information Criterion (AIC; Akaike, 1973Akaike, , 1974, the Bayesian Information Criterion (BIC; Schwarz, 1978), and the Bayes factor (Kass, 1993;Kass & Raftery, 1995). The BIC and Bayes factor were considered the same meaningfully, and BIC was easier in its computation (de la Torre & Douglas, 2008). For a given dataset, the larger the log-likelihood, the better the model fit; the smaller the AIC, BIC, and Bayes factor values, the better the model fit (Xu & von Davier, 2008)."}, {"section_title": "Q-Matrix", "text": "Specifying a K J \u00d7 Q-matrix is required for implementing almost all CDMs (Tatsuoka, 1995). Using K for the number of skills being measured, and J for the number of items in a test, the Q-matrix can be illustrated as follows: where J is the number of rows and K is the number of columns, There are 2 K -1 possible rows for a Q-matrix because there is no item measuring no attributes (i.e., the case of all zeros doesn't exist). For item j, attribute k, and examinee i, \u03b1 i = { \u03b1 ik } represents the examinee's binary skills vector, k=1, . . . , K, where 1 on the k th element denotes mastery of skill k, and 0 denotes non-mastery. otherwise 0 skill the mastered has examinee the if 1 th k ik \u03b1 when there are 2 K possible examinee attribute profiles. For example, with K = 2 skills, there are 2 K = 4 possible skill patterns of mastery and non-mastery: {0,0}, {1,0}, {0,1}, and {1,1}.\nTo analyze the real data using CDMs, the first step was to construct a Q-matrix that specified the skills necessary to solve each item. The current study adapted the attributes from the CCSS (National Governors Association Center for Best Practices, Council of Chief State School Officers, 2010), and Q-matrix from the consensus of two doctoral students majoring in secondary teaching and learning. These two content experts were former middle and high school math teachers. Independently, the two experts first answered each item, wrote down the strategies/process they used to solve each item, and then coded and matched the attributes for each item. The attributes used for coding were adapted from grades six to eight CCSS. A follow-up discussion time was scheduled to solve the coding inconsistencies between the experts, and reach an agreement. When they were not able to reach an agreement for an item through discussion, a professor in secondary school mathematics solved the conflict. Table 3-7 provides the attributes modified form the CCSS and their corresponding TIMSS items. To illustrate, Table 3-8 shows one item from each booklet with the attributes being measured. The Q-matrices of B1 and B2 are shown in Table 3-9 and 3-10, respectively. The percentage of coders' overall agreement for constructing the Q-matrices is 88.89%. The next step was for the two experts to arrange and organize the attributes into a hierarchical order which they thought reasonable based on the CCSS mathematics grade level arrangement. To determine the hierarchies among the attributes, the following order was followed to arrange those within-a-grade attributes: recognize/understand, use, compare, apply, and then solve real-world problems. The coders worked together and reached an agreement for the final decision of the hierarchical structure. "}, {"section_title": "DINA Model", "text": "The DINA model, one of the most parsimonious CDMs that require only two interpretable item parameters, is the foundation of other models applied in cognitive diagnostic tests (Doignon & Falmagne, 1999;Tatsuoka, 1995Tatsuoka, , 2002. The DINA model is a non-compensatory, conjunctive CDM, and assumes that an examinee must know all the required attributes in order to answer an item correctly (Henson et al., 2009). An examinee mastering only some of the required attributes for an item will have the same success probability as another examinee possessing none of the attributes. For each item, the examinee item respondents are scored into two latent classes: one class indicates answering the item correctly (scored 1), containing examinees who possess all attributes required for answering that item correctly; the other class indicates incorrectly answering the item (scored 0), containing examinees who lack at least one of the required attributes for answering that item correctly. This feature is true for any number of attributes specified in the Q-matrix (de la Torre, 2011). The complexity of the DINA model is not influenced by the number of attributes measured by a test because its parameters are estimated for each item but not for each attribute, unlike other non-compensatory conjunctive cognitive diagnostic models (e.g., the RUM) (Rupp & Templin, 2008a). The DINA model has two item parameters, slip (s j ) and guess (g j ). The term \"slip\" refers to the probability of an examinee possessing all the required attributes but failing to answer the item correctly. The term \"guess\" refers to the probability of a correct response in the absence of one or more required attributes. However, the two item parameters also encompass other nuisances. Those nuisances confound the reasons why examinees who have not mastered some required attributes can answer an item correctly, and the reasons why examinees who have mastered all the required attributes can miss the correct response. Two examples of the common nuisances are the misspecifications in the Qmatrix, and the usage of alternative strategies, as Junker and Sijtsma (2001) described when they first advocated the DINO model. Below are the mathematics presentations for the two item parameters: and the item response function in the DINA model is defined as where the \u03b7 matrix refers to a matrix of binary indicators showing whether the examinee attribute profile pattern i has mastered all of the required skills for item j. The formula is defined as: where ik \u03b1 refers to the binary mastery status of the k th skill of the i th skill pattern (1 denotes mastery of skill k, and 0 denotes non-mastery). And, as discussed in the previous section, q jk here is the Q-matrix entries specifying whether the th j item requires the th k skill. for validating the Q-matrices when implementing the DINA model. In his method, \u03b4 j is defined as \"the difference in the probabilities of correct responses between examinees in groups \u03b7 j = 1 and \u03b7 j = 0\" (i.e., examinees with latent responses 1 and 0) (as cited in de la Torre, 2008, p. 344). \u03b4 j serves as a discrimination index of item quality that accounts for both the slip and guessing parameters. Below is the computation formula for item j: The higher the guessing and/or slip parameters are, the lower the value of \u03b4 j . This signifies that the less-discriminating items have high guessing and slip parameters, and have a smaller discrimination index value of \u03b4 j . In contrast, an item that perfectly discriminates between examinees in groups \u03b7 j = 1 and \u03b7 j = 0 has a discrimination index of \u03b4 j = 1 because there is no guessing and slip. Therefore, the higher the value of \u03b4 j is, the more discriminating the item is. An extension of the DINA model is the higher-order DINA (HO-DINA) model (de la Torre & Douglas, 2004). The HO-DINA model is a higher order unidimensional latent trait model, and has the same basic specifications as the DINA. The main difference between these two models is that the HO-DINA model assumes that the independency of the mastery of one attribute to other attributes is conditional on a higher order latent ability variable \u03b8. The basis of the higher order latent trait approach is to parsimoniously model the joint distribution of the attributes. The model is very similar to the 2 PL IRT model: where k 1 \u03bb refers to the slop parameter for attribute k, and ok \u03bb refers to the difficulty parameter."}, {"section_title": "DINO Model", "text": "The DINO model is the disjunctive counterpart of the DINA model (Templin & Henson, 2006). Similar to the DINA model, the DINO model has two item parameters: s j and g j . In the DINO model, examinees are divided into two groups. The first group of examinees have at least one of the required skills specified in the Q-matrix (\u03c9 ij = 1), and the second group of examinees do not possess any skills specified in the Q-matrix (\u03c9 ij = 0). At least one Q-matrix skill must be mastered for a high probability of success in the DINO model. Hence, the slip parameter (s j ) indicates the probability that examinee i, who masters at least one of the required skills for item j, answers it incorrectly. The guessing parameter (g j ) refers to the probability of a correct response when the examinee possesses none of the required skills. In other words, the DINO model assumes that the probability of a correct response, given mastery on at least one skill, does not depend on the number and type of skills that are mastered. It allows for low levels on certain skills to be compensated for by high levels on other skills. The item parameters are defined as: g j = P(X ij = 1| \u03c9 ij = 0) , and the item response function in the DINO model is defined as where . Both the DINO and DINA models are simpler CDMs. They assign only two parameters per item and partition the latent space into exactly two sections (i.e., mastery and non-mastery). Other more complex models (such as rRUM, linear logistic, etc.) assign K, K+1, or more parameters per item, and partition the latent space into multiple sections. Nevertheless, the DINO model is more popular in the medical, clinical, and psychological fields, because such diagnoses in these fields are typically based on the presence of only some of the possible major symptoms. The absence of certain symptoms can be compensated for by the presence of others."}, {"section_title": "rRUM", "text": "The reduced Reparameterized Unified Model (rRUM;DiBello et al., 2007) is a non-compensatory conjunctive CDM. It assumes that an examinee must know all the required attributes in order to answer an item correctly (Henson et al., 2009). The rRUM is a more complicated model, as it allows for different probabilities of item response depending on what required attributes an examinee has mastered. It discriminates between different examinee response classes, and allows items with the same Q-matrix coding to have different response probabilities (Hartz, 2002;Templin, Henson, Templin, & Roussos, 2008). The rRUM also addresses the issue in the DINA model that all examinees who lack at least one required attribute have the same probability of answering an item correctly (Henson et al., 2009). The rRUM model includes two different parameters, * j \u03c0 and * jk r : * j \u03c0 refers to the probability of correctly applying all the required skills for the j th item, and * jk r refers to the penalty for not mastering the k th skill to item j. For every single non-mastered attribute, the probability of answering an item correctly is reduced by the penalty, * jk r , that discriminates non-masters to masters. The smaller * jk r is, the higher the level of discrimination. The mathematical presentations for the two parameters are:  where jk \u03c0 refers to the probability of correctly applying the mastered attribute k to item j and means the probability of not slipping at the attribute level, and jk r stands for the probability of guessing for attribute k to item j. In addition, * j \u03c0 is meaningfully the same as one minus the probability of slipping at the attribute level in the DINA model (Rupp et al., 2010), assuming conditional independence: The parameter jk r is the probability of guessing at the attribute level, and is defined as a ratio of slipping and guessing probabilities: The probability of correctly answering item j for examinee i with \u03b1 \u03b1 \u03b1 \u03b1 i is where jk \u03b1 refers to the binary mastery status of the k th skill of the j th item, in which 1 denotes mastery of skill k of item j and 0 denotes non-mastery, and \u03b7 matrix represents binary indicators signifying whether the j th item requires mastery of the k th skill. The rRUM is a simplified version of the Reparameterized Unified Model (RUM, DiBello et al., 2007). The RUM includes one more continuous latent variable, and is sometimes called the full RUM. It is a cognitive diagnosis model developed by Hartz (2002), and is designed to \"model the probability of a correct response as a function of both attribute specific item characteristics and attribute specific examinee characteristics\" (Henson, Templin, & Douglas, 2007, p. 561). Practically, the simplified rRUM is used more often than the full RUM, because the \"continuous residual ability parameter designed to capture the influence of skills is not explicit in the CDMs\" (Henson et al., 2007, p. 561). In addition, the higher-order rRUM, implemented by Templin (2004), used a similar concept of the HO-DINA model to construct a slightly different unidimensional higher order approach. The HO-rRUM provides a one-factor model for the tetrachoric correlation of each pair of attributes. Its general robustness and the associated estimation procedure were validated by Templin et al. (2008)."}, {"section_title": "Learning Sequence and Attribute Hierarchy", "text": "This part contains three sections. The first section briefly introduces learning theories that signify the importance of identifying learning sequences. The second section describes the definition of attribute hierarchies. The third section introduces the attribute hierarchy method."}, {"section_title": "Significance of Learning Sequences", "text": "Educators and researchers have long focused their attention on learning sequences, and advocated the importance of ordering instructions to build up learning sequences. As early as 1922, Thorndike claimed that significant instructional time and effort was wasted because the associations between previous and later learning (\"the laws of learning\") were neglected and not used to facilitate learning (Baroody et al., 2004). Thorndike recommended that educators recognize the relation of learning processes to principles of content prior to the initiation of learning or instruction. Gagn\u00e9 and Briggs (1974) developed a hierarchy of goals based on logical and empirical task analyses, which they applied to develop curricula for elementary education. In the mid twentieth century, information-processing theories used the input-process-output metaphor to describe learning processes (Baddeley, 1998). In the late twentieth century, constructivism became popular and is now commonly applied to develop mathematics curricula and sequences of instructions. Constructivists believe learners actively experience and construct their new knowledge. Previous research has shown that students' development of conceptualizations and reasoning can be classified into different levels of sophistication (Battista & Clements, 1996;Battista & Larson, 1994). Teachers need to know what cognitive processes and conceptualizations that students must acquire to make progress in constructing meaning of the new mathematic idea (Battista, 2004). Cognitive research has suggested that some preliminary knowledge can be defined as the foundation for other more sophisticated knowledge (e.g., Kuhn, 2001;Vosniadou & Brewer, 1992). The associations of knowledge skills are especially important for conceptual understanding and problem solving. Conceptual understanding implies that students have the ability to use knowledge, to apply it to related problems, and to make connections between related ideas (Bransford, Brown, & Cocking, 2000). This means that building conceptual understanding involves connecting newly introduced information to existing knowledge as the student builds an organized and integrated structure (Ausubel, 1968;Linn, Eylon, & Davis, 2004). Mathematics educators have clarified levels of development in students' understanding and constructing of mathematics concepts from early number and measurement ideas, to rational numbers and proportional reasoning, to algebra, geometry, calculus, and statistics (Lesh & Yoon, 2004). These levels of knowledge development are structured by researchers in ladderlike sequences, with each successive run closer to the most sophisticated level. There have been studies about constructing instructional or learning processes in various subject areas. For example, one recent significant study, called the Hypothetical Learning Trajectories (HLTs, , emphasizes the goals for meaningful student learning, tasks geared to achieve this learning, and hypotheses about the process of student learning. HLTs, based on constructivism, provide richer information of learners' requisite knowledge, development, and difficulties than previous efforts to define learning sequences. In addition, in science education, learning progressions are research-based descriptions of how students build their knowledge, and gain more expertise within and across a discipline over a broad span of time (Duschl, Schweingruber, & Shouse, 2007;Smith, Wiser, Anderson, & Krajcik, 2006). Learning progressions describe a potential learning path that can provide a guide for a coherent curriculum and as such inform the design of coherent curriculum materials. Empirically tested learning sequences should be fully articulated for curriculum developers to use as a ready-made artifact in developing coherent curricula. Researchers have called for the need for developing learning sequences to inform the development of coherent curricula over the span of K-12 science education (Krajcik, Shin, Stevens, & Short, 2010). Results from the TIMSS have shown that a coherent curriculum is the primary predictor of student achievement (Schmidt, Wang, & McKnight, 2005). If the curriculum is not built coherently to help learners make connections between ideas within and among disciplines or form a meaningful structure for integrating knowledge, students may lack foundational knowledge that can be applied to future learning and for solving problems that confront them in their lives (Krajcik et al., 2010;Schmidt et al., 2005)."}, {"section_title": "Sequential Nature of Mathematical Concepts", "text": "Mathematics encompasses a wide variety of skills and concepts. These skills and concepts are related and often build on one another (Sternberg & Ben-Zeev, 1996). Some math skills obviously develop sequentially. For example, a child cannot begin to add numbers until he knows that those numbers represent quantities. Solving mathematical problems frequently involves separate processes of induction, deduction, and mathematical conceptualization (Nesher & Kilpatrick, 1990). However, certain advanced skills do not seem to have a clear dependent relationship. For example, a student who often makes simple calculation errors may still be able to solve a calculus problem that requires sophisticated conceptual thinking. Educators have tried to identify sets of expected milestones for a given age and grade as a means of assessing a child's progress, and of better understanding in which step students go wrong (Levine, Gordon, & Reed, 1987 (NCTM, 2000). A developmental progression embodies theoretical assumptions about mathematics; for example, a student needs to be able to build an image of a shape, match that image to the goal shape by superposition, and perform mental transformation in order to solve certain manipulative shape composition tasks (Clements, Wilson, & Sarama, 2004). Researchers have been devoted to finding evidence to support the assumptions. For example, the findings from Clements, Wilson, et al. (2004) suggested that students demonstrate varying levels of thinking when given tasks involving the composition and decomposition of two-dimensional figures, and that the older students with previous experience in geometry tend to evince higher levels of thinking. Their results also showed that students moved through several distinct levels of thinking and competence in the domain of composition and decomposition of geometric figures. Researchers' efforts to identify learning sequences in mathematics have also attracted the attention of educators and policy makers. For example, in the area of algebra it has been claimed in the Final Report from the National Mathematics Advisory Panel The coherence and sequential nature of mathematics dictate the foundational skills that are necessary for the learning of algebra. The most important foundational skill not presently developed appears to be proficiency with fractions (including decimals, percent, and negative fractions). The teaching of fractions must be acknowledged as critically important and improved before an increase in student achievement in algebra can be expected. (p. 18). The recognition of the sequential nature of mathematical concepts impacts the development of curriculum design and student learning. The attention on developing students' learning sequences in mathematics also impacts teacher education in mathematics. Researchers suggested that teachers in mathematics must be well-trained to demonstrate competencies in knowledge and skills in teaching mathematics, understanding of the sequential nature of mathematics, the mathematical structures inherent in the content strands, and the connections among mathematical concepts, procedures and their practical applications (Steeves & Tomey, 1998). For example, the licensure regulations for mathematics specialists for elementary and middle education by the Virginia Department of Education requires understanding of the sequential nature of mathematics and the mathematical structures inherent in the content strands (http://www.doe.virginia.gov/VDOE/Compliance/ TeacherED/nulicvr.pdf). Teachers' knowledge of the sequential nature of mathematical concepts and capability of applying this knowledge to their instruction will benefit students in learning mathematics."}, {"section_title": "Attribute Hierarchies", "text": "Attribute hierarchies represent the interdependency among cognitive attributes. It refers to situations in which the mastery of a certain attribute is prerequisite to the mastery of another attribute. The attribute with the lower cognitive load is developed earlier than attributes with higher cognitive loads. Thus, the first attribute is located in the lowest layer of the hierarchy, and the second attribute is in the next highest layer of the same hierarchy. Four common types of cognitive attribute hierarchies are linear, convergent, divergent, and unstructured (Gierl et al., 2007;Leighton et al., 2004;Rupp et al., 2010). These four hierarchies are shown in Figure 2-1 taken from Gierl et al. (2007) and Leighton et al. (2004), using six attributes as an example. The linear attribute hierarchy requires all attributes to be ordered sequentially. If an examinee has mastered attribute 2, then he or she has also mastered attribute 1. Furthermore, an examinee who has mastered attribute 3 has also mastered attributes 1 and 2, and so on. The convergent attribute hierarchy specifies a situation in which a single attribute could be the prerequisite of multiple different attributes. It also includes situations where a single attribute could require the mastering of one or more of the multiple preceding attributes. In this case, an examinee mastering attributes 3 or 4 has also mastered attributes 1 and 2. An examinee mastering attribute 5 has mastered attribute 3, attribute 4, or both, and has also mastered attributes 1 and 2. This implies that an examinee could achieve a certain skill level through different paths with different mastered attributes. The divergent attribute hierarchy refers to different distinct tracks originating from the same single attribute. In a divergent attribute hierarchy, an examinee mastering attributes 2 or 4 has also mastered attribute 1. An examinee mastering attributes 5 or 6 has mastered attributes 1 and 4. The unstructured attribute hierarchy describes cases when a single attribute could be prerequisite to multiple attributes, and where those attributes have no direct relationship to each other. For example, an examinee mastering attributes 2, 3, 4, 5 or 6 means only that he or she has mastered attribute 1. Tables 2-1 through 2-4 from Rupp et al. (2010), modified as indicated, show the matrices of attribute profiles associated with each kind of attribute hierarchies, also using the previous example of six attributes; Table 2-2 was modified by adding two missing profiles in their tables. As with the Q-matrix, 0 means the attribute is not mastered, and 1 means the attribute is mastered. The number of possible attribute profiles is different for various attribute hierarchies. The more independent the attributes, the larger the number of possible attribute profiles. The higher the dependency among the attributes, the fewer the number of possible attribute profiles. An assessment could be a combination of various attribute hierarchies, and thus the possible number of attribute profiles is uniquely different for each assessment. Varying types of structures could appear for a certain type of hierarchy. When a test is developed based on attribute hierarchies, the number of possible attribute profiles reduces dramatically from 2 K . Hence, the complexity of estimating a CDM is decreased, and the sample size requirement is lowered."}, {"section_title": "Attribute Hierarchy Method", "text": "The Attribute Hierarchy Method (AHM) (Gierl, 2007;Gierl, Cui et al., 2009;Gierl, Leighton et al., 2009;Gierl et al., 2007;Gierl & Zhou, 2008) is a psychometric method which applies the structured hierarchical models in developing item and classifying examinee profiles into different attribute patterns. By specifying the hierarchical relationships among attributes, the number of permissible items can be reduced. Without specifying the hierarchical relationships among attributes, there are 2 K -1 possible rows (items) for a Q-matrix, except for no item measuring no attributes. Imposing the constraints of the attribute hierarchies can produce a reduced Q-matrix. This reduced Q-matrix is a J by K matrix where J is the reduced number of items and K is the number of attributes. The reduced Q-matrix is used to develop items that measure each specific attribute combination specified in the hierarchy, and represents only those items that fit the dependencies defined in the attribute hierarchy. Similarly, by specifying the hierarchical relationships among attributes, the number of possible attribute profiles can be reduced. Without specifying the hierarchical relationships among attributes, there are 2 K possible attribute profiles. The reduced \u03b1-matrix consists only of the reduced number of attribute profiles, and is applied in the estimation process. This AHM method is therefore useful to design a test blueprint based on the cognitive attribute hierarchies. In the statistical pattern classification process, the examinees' observed attribute profiles are compared to their expected attribute profiles under the assumption that the cognitive model is true. In the IRT-based AHM approach (Gierl et al., 2007;Leighton et al., 2004), the expected item characteristic curve can be calculated for each item using the 2 PL IRT model. The a-and b-item parameters for each item can be estimated based on the expected item response patterns, under the assumption that examinees' responses are consistent with the attribute hierarchy. In the non-IRT based classification method (Gierl & Zhou, 2008), the AHM focuses on classifying examinees' attribute profile patterns and estimating attribute probabilities and item probabilities, unlike the IRT and other CDM models (e.g., DINA, DINO, RUM, etc.) that can estimate item parameters. Furthermore, a person-fit statistic, the Hierarchy Consistency Index (HCI; Cui & Leighton, 2009) has been developed to examine whether examinees' actual item response patterns match the expected response patterns based on the hierarchical relationship among attributes measured by test items. AHM is a relatively new measurement model, and its related research has grown in recent years. Currently, this approach has not obtained item parameter estimates based on the observed respondents from the real data, and the estimated attribute probabilities are not group invariant (i.e., they are different for different samples). Practically, it is difficult to establish an AHM-based item pool using what the IRT-based approach does conventionally. It is also difficult to conduct linking studies, which are commonly needed for testing programs in developing new forms.  Profile 16  1  0  0  1  0   Profile 17  1  0  0  0  1   Profile 18  1  1  1  1  0   Profile 19  1  1  1  0  1   Profile 20  1  1  1  0  0   Profile 21  1  1  0  1  1   Profile 22  1  1  0  1  0   Profile 23  1  1  0  0  1   Profile 24  1  0  1  1  1   Profile 25  1  0  1  0  1   Profile 26  1  0  1  1  0   Profile 27  1  0  0  1  1   Profile 28  1  1  1  1  1   Profile 29  1  1  1  0  1   Profile 30  1  1  0  1  1   Profile 31  1  0  1  1  1   Profile 32  1  1  1  1  0   Profile 33  1  1  1 "}, {"section_title": "Research Purposes and Questions Revisited", "text": "The purposes of the study are to apply the hierarchical models of the cognitive skills when using two cognitive diagnostic models -DINA and DINO-to analyze both simulated data and the retrofitting of TIMSS 2003 mathematics data. To achieve these purposes, three research questions are to be answered. This section describes how each research question is addressed."}, {"section_title": "Research Question 1", "text": "Research Question 1 examines the proposed DINA-H and DINO-H models."}, {"section_title": "How do the proposed DINA-H and DINO-H perform?", "text": ""}, {"section_title": "Do the DINA-H and DINO-H models provide reasonable item parameter estimates?", "text": "To address Research Question 1.1, the calibration results from the real data analysis using the DINA-H and DINO-H models were evaluated and compared to the results from the DINA and DINO models. The following statistics were compared: The AIC and BIC for the model fit, and the \u03b4 and IDI for the item fit."}, {"section_title": "Do the DINA-H and DINO-H models provide stable calibration results with small sample size?", "text": "The calibration results from the real data analysis using the DINA/DINO models and the DINA-H/DINO-H models with the US sample size were compared to the results with a larger sample size. The following statistics were compared: The AIC and BIC for the model fit, and the \u03b4 and IDI for the item fit."}, {"section_title": "Which CDM (the DINA-H or DINO-H model) performs better while applying a skill hierarchy?", "text": "The calibration results from analyzing two datasets of the two TIMSS 2003 mathematics booklets and the simulation study with the DINA and DINA-H models were compared to the results analyzed via the DINO and DINO-H models. To address Research Question 1.3, the AIC and BIC for the model fit, the \u03b4 and IDI for the item fit, and correlations of item parameter estimates between different models and sample sizes were compared."}, {"section_title": "Research Question 2", "text": "Research Question 2 concerns the issue when the assumptions about the relationships among attributes are inconsistent with the estimation models in the simulation study."}, {"section_title": "When skills are ordered hierarchically, how do the conventional DINA and DINO models and the proposed DINA-H and DINO-H models compare? 2.1 How do the conventional and new models compare in terms of parameter estimates?", "text": "The attributes adapted in developing the Q-matrix for the TIMSS 2003 mathematics test were constructed into a hierarchical structure by two content experts in mathematics learning. The calibration results from the real data analysis and the main effect of model consistency from the simulation analysis using the DINA-H and DINO-H models were evaluated and compared to the results from the DINA and DINO models. In addition, the results from the simulation analysis for data simulated from the hierarchically ordered skills were analyzed and compared via the new DINA-H and DINO-H models and the conventional DINA and DINO models. The following statistics were compared: The AIC and BIC for the model fit, the \u03b4 and IDI for the item fit, and the ASB, AVAR, and AMSE of item parameter estimates for each condition."}, {"section_title": "How do the performances of the conventional and new models compare under varying conditions of different numbers of attributes, different test lengths, and sample sizes?", "text": "In the simulation analysis, data simulated from different hierarchically ordered skills were analyzed via the new DINA-H and DINO-H models and the conventional DINA and DINO models. The calibration results from the new DINA-H and DINO-H models were compared to the conventional DINA and DINO models for the main effects of different numbers of attributes, test lengths, and sample sizes, and the interaction effects of test length by attribute, sample size by attribute, and sample size by test length, and the three-way interaction effect of sample size by test length by attribute. The ASB, AVAR, and AMSE of item parameter estimates for each condition were compared. Research Question 3 concerns the misspecification of a skill hierarchy on the results of model estimations."}, {"section_title": "What is the impact of misspecification of a skill hierarchy on the DINA(-H) and the DINO(-H) models?", "text": "3.1 Do the item parameters recover well, when a specified skill hierarchy is inconsistent with an estimation model? The calibration results from simulation analysis using varying estimation models consistent or inconsistent with the specifications on the skill hierarchies were compared to each other. The following statistics were compared under each condition: The AIC and BIC for the model fit, and the ASB, AVAR, and AMSE of item parameter estimates for each condition."}, {"section_title": "How do the models perform and compare under varying conditions with different numbers of attributes, test lengths, and sample sizes?", "text": "The calibration results from simulation analysis using varying estimation models consistent or inconsistent with the specifications on the skill hierarchies were compared to each other for the main effects of different numbers of attributes, test lengths, and sample sizes, and the interaction effects of test length by attribute, sample size by attribute, and sample size by test length, and the three-way interaction effect of sample size by test length by attribute. In addition, the calibration results from analyzing the DINA-H/DINO-H models with different skill hierarchies for different sample sizes were compared to each other, and also to the DINA/DINO model. More specifically, the bias indices for the DINA-H/DINO-H models with higher dependent skill hierarchy were compared to the DINA-H/DINO-H models with the lower dependent skill hierarchy for various sample sizes in the simulation analysis. The ASB, AVAR, and AMSE of item parameter estimates for each condition were evaluated."}, {"section_title": "Proposed DINA-H and DINO-H Models", "text": "The study proposed two modified CDMs: The DINA with hierarchy (DINA-H) model and the DINO with hierarchy (DINO-H) model. Both models involve the hierarchical structures of the cognitive skills in the estimation process and were introduced for situations where the attributes are ordered hierarchically. The proposed DINA-H and DINO-H models were applied with real data and simulation analyses and were compared with the conventional DINA and DINO models. The DINA-H and DINO-H models have the same basic specifications as the conventional DINA and DINO models. The only difference is that the pre-specified possible attribute profiles under a certain skill hierarchy are adapted in the DINA-H and DINO-H models. In the conventional DINA and DINO models, the number of possible attribute profiles L is equal to 2 K (where K refers to the number of skills being measured). In the DINA-H and DINO-H models, L is equal to the number of all possible attribute profiles specified for each unique model. In the DINA and DINO models, the initial possible attribute profiles \u03b1 is all the 2 K possible combinations of 0s and 1s, whereas \u03b1 is set to be the possible attribute profiles specified for each unique DINA-H and DINO-H model. Examinees are classified into these specified possible attribute profiles during the estimation process. The number of parameters in the conventional DINA and DINO models is equal to 1 2 2 \u2212 + K J (where J refers to the number of items in a test). For the DINA-H and DINO-H models, the number of parameters is equal to where L represents the number of all possible attribute profiles specified for each unique hierarchical model.  Table 3-1) of eight attributes requires all attributes to be ordered sequentially. In a linear hierarchy, for an examinee to have mastered attribute 8, he or she must have also mastered attributes 1 through 7. The convergent attribute hierarchy (see Table 3-2) specifies a situation in which a single attribute could be the prerequisite of multiple different attributes and situations in which a single attribute could require mastering one or more of the multiple preceding attributes. The relationships among attributes 2 to 5 are the same as specified in the condition of six attributes. In the condition of eight attributes, attribute 5 is the prerequisite of attributes 6 and 7, and mastering either attribute 6 or 7 could lead to mastering attribute 8. The divergent attribute hierarchy (see Table 3-3) refers to different distinct paths originating from the same single attribute. The relationships among attributes 1 and 4 to 6 are the same as specified in the condition of six attributes. In the condition of eight attributes, attributes 7 and 8 appear parallel at the end of attribute 3. That means when an examinee has mastered attributes 7 or 8, he or she has also mastered attributes 1 to 3. The unstructured attribute hierarchy (see Table 3-4) describes cases when a single attribute could be prerequisite to multiple different attributes which have no direct relationships to each other. Similar to the condition of six attributes, attribute 1 is the common prerequisite to attributes 2 to 8 in the condition of eight attributes. However, a certain type of a hierarchy model could have various types of structures, except for the liner hierarchy model. The specified conditions in Figure 3-1 are ones among many possible structures under a hierarchy. This is especially so for the convergent and the divergent hierarchies. Based on the attribute hierarchies, the number of attribute profiles was found for each hierarchical model. The attribute profiles for the condition of six attributes were listed in Chapter II. Tables 3-1 to 3-4 list the attribute profiles of each attribute hierarchy for the condition of eight attributes. Table 3-5 presents the total number of attribute profiles of each attribute hierarchy model for the condition of both six and eight attributes. The number of possible attribute profiles is different for various attribute hierarchies. The more independent the attributes, the larger the number of possible attribute profiles. The higher the dependency among the attributes, the fewer the number of possible attribute profiles. Since the convergent and the divergent hierarchies could have varying structures, the numbers of possible attribute profiles were different for various structures."}, {"section_title": "Real Data Analysis", "text": "The data used in this study included both real and simulated data. The real data came from the TIMSS 2003 U.S. eighth grade mathematics test. This real data analysis is a retrofitting analysis, which means it is an analysis of an already existing assessment using other models (e.g., the DINA and the DINO models). The goal of the analysis of the real data is to benchmark the proposed models and to address research questions regarding whether the DINA-H and the DINO-H models provide reasonable parameter estimates, and whether they provide more stable calibration results than the conventional DINA and the conventional DINO models when there is a hierarchy in attributes.  (Neidorf & Garden, 2004). Each student took one booklet containing both mathematics and science items, which were only a subset of the items in the whole assessment item pool. The TIMSS scale was set at 500 and the standard deviation at 100 when it was developed in 1995. The average score over countries in 2003 is 467 with a standard deviation of 0.5. The assessment time for individual students was 72 minutes at fourth grade and 90 minutes at eighth grade. The released TIMSS math test included five domains in mathematics: Number and operation, algebra, geometry, measurement, and data analysis and probability. The items and data were available to the public, and could be downloaded from TIMSS 2003 International Data Explorer (http://nces.ed.gov/timss/idetimss/)."}, {"section_title": "Description of Data", "text": "Two types of content domains, number-and-operation and algebra, were used in the study because the ability to do number-and-operation is the prerequisite for algebra, and also there were more released items available. The hierarchical ordering of mathematical skills in both number and algebra was found in other empirical studies. For example, Gierl and Leighton et al. (2009) used the think aloud method to identify the hierarchical structures and attributes for Basic Algebra on SAT, and identified five attributes, single ratio setup, conceptual geometric series, abstract geometric series, quadratic equation, and fraction transformation, for a basic Algebra item. Booklets 1 and 2 from TIMSS 2003 were used in the study. One number-andoperation item and one algebra item were excluded in the analysis because they were too easy and only measure elementary-level attributes. There were 18 number-and-operation items, 11 algebra items, and 757 U.S. examinees for booklet 1 (B1). There were 21 number-and-operation items, 9 algebra items, and 740 U.S. examinees for booklet 2 (B2). About half of the items were released in 2003 and the others were released in 2007. Four number-and-operation items in booklet 1 were in constructed-response format, and five number-and-operation items in booklet 2 were in constructed-response format. One algebra item in booklet 2 was a constructed response item. Three of the constructed response items in number-and-operation in booklet 1, one of them in number-and-operation in booklet 2, and one in algebra in booklet 2 were multiple-scored items. However, in the current study, these items were rescored as 0/1 dichotomous items in the examinees' score matrix to conduct the CDM calibration. For those examinees who got full score points of 2 were rescored as 1, and who got score point of 1 were rescored as 0. In addition to the small U.S. sample, a larger sample size including the benchmark participants for each booklet was also applied for the comparison analysis. "}, {"section_title": "Analysis", "text": "This section describes the steps conducted in the model estimation. The first part lists the conditions, the second part briefly reviews the models used in this study, the third part describes the model estimation process using the EM algorithm, and the final part describes the evaluation indices."}, {"section_title": "Conditions", "text": "To understand whether the DINA-H and the DINO-H models worked in practice and provided reasonable parameter estimates, the U.S. sample was analyzed and compared via the DINA, DINA-H, DINO, and DINO-H models. To further investigate whether the DINA-H and the DINO-H models provided more stable calibration results than the conventional DINA and the conventional DINO models when sample size was smaller, the item fit indices for the large benchmark samples were analyzed and compared to the U.S. sample size via the same four models. There were eight conditions of grouping for each booklet for the real data analysis."}, {"section_title": "Models", "text": "The real data were analyzed by fitting the DINA, DINA-H, DINO, and DINO-H models, using the expectation-maximization (EM) algorithm based on the marginal maximum likelihood estimation. The EM approach was chosen because its results were reproducible and could be replicated. The DINA and DINO models were described in detail in Chapter II. Both models require a specified binary Q-matrix, which identifies the attributes measured by the items. The DINA-H and DINO-H models were described earlier in the chapter."}, {"section_title": "Parameter Estimation Process", "text": "To estimate examinee attribute profiles and item parameters, the procedure outlined by de la Torre (2009) was followed. The EM algorithm was implemented by first setting initial parameter values, then estimating the expected value of the unknown variables, and giving the current parameter estimates. The final step was to re-estimate the posterior distribution, by maximizing the likelihood of the data by computing the marginal maximum likelihood estimation given the expected estimates of the unknown variables. The estimation steps were repeated until convergence was achieved. Because the estimation process was initialized with a flat prior distribution, the prior was updated in each iteration using the expected proportions of examinees in each latent class (Huebner & Wang, 2011). This method is referred to as the empirical Bayesian method in Carlin and Louis (1996). The estimations began with all the slip and guessing parameters set to 0.2 (Huebner, 2009;Huebner & Wang, 2011). It was shown in a simulation study that the results from the EM estimation were not sensitive to the initial values of the parameters as long as the true guess and slip parameters reasonably fall between 0 and 0.5, and it was demonstrated that the differences in results at varying levels of initial values was negligible (Huebner, 2009). In the DINA and DINA-H models, the \u03b7-matrix is used to indicate whether examinee i has mastered all of the required skills for item j, whereas in the DINO and DINO-H models the \u03c9-matrix ( used. In the DINA and DINO models, the number of possible attribute profiles L equals 2 K , whereas L equals the maximum number of possible attribute profiles specified for each unique DINA-H and DINO-H models. In the DINA and DINO models, the initial possible attribute profiles \u03b1 contain all 2 K possible combinations of 0s and 1s, whereas \u03b1 are the possible attribute profiles specified for each unique DINA-H and DINO-H models. The major steps of EM computation described in de la Torre (2009) were outlined as follows, using the notation for the DINA model as an example. The first step in computing the posterior matrix was to calculate the matrix of marginalized maximum likelihood estimation likelihood. For the observed data X and the attribute profiles \u03b1 : where ) ( Likelihood i X is the marginalized likelihood of the response vector of examinee i, and ) ( l p \u03b1 \u03b1 \u03b1 \u03b1 is the prior probability of the attribute profile vector The next step toward computing the posterior matrix was to multiply the columns of the likelihood matrix by the prior probability for the corresponding skill pattern with a flat (non-informative) prior distribution, meaning that each skill pattern had a probability of 1/L, where L equals 2 K in the conventional DINA and DINO models and L equals the number of all possible attribute profiles specified for each DINA-H or DINO-H model. Parameter estimation based on the marginalized likelihood (i.e., the marginal maximum likelihood estimation) was implemented using the EM algorithm. To obtain the maximum likelihood estimate, the following is maximized: The expected number of examinees with attribute profile where is the posterior probability that examinee i has the attribute profile Moreover, the expected number of examinees with attribute profile l \u03b1 \u03b1 \u03b1 \u03b1 answering item j correctly is defined as: Finally, item parameters were estimated when and ) 1 ( As mentioned earlier in this section, the algorithm started with initial values for g and s both equal to 0.2. Next, ) were computed based on the current values of g and s. Then, the values of g and s were found and updated. The steps were repeated until convergence was achieved. The criterion for convergence was set to be smaller than 0.001 for both real data and simulation studies. The number of iteration cycles was smaller than 100 across all the conditions of the simulation. The study modified scripts written for use in the R software environment (R Development Core Team, 2011) to incorporate the hierarchy in the DINA-H and DINO-H models for statistically conducting the real data and the simulation analyses."}, {"section_title": "Evaluation Indices", "text": "The study evaluated and compared model fit and item fit for each condition in the real data analysis for the DINA, DINA-H, DINO, and DINO-H models. The details for assessing each evaluation index are described in the next section.\n"}, {"section_title": "Model Fit Indices", "text": "The model fit statistics used in this study included convergence, the AIC (Akaike, 1973(Akaike, , 1974, and the BIC (Schwarz, 1978). The \u03b4 index (de la Torre, 2008) and the item discrimination index (IDI; Robitzsch, Kiefer, George, & Uenlue, 2011) were used as the item fit criteria. These values were computed for each condition in the real data analysis. First of all, convergence was monitored and recorded for each condition. The estimated parameter difference between two iterations was set to be smaller than 0.001 as the criterion for convergence. Second, the AIC is defined as: where ln(Likelihood) is the log-likelihood of the data under the model (see Equation 16 and 17) and p is the number of parameters in the model. For the conventional DINA and DINO models, . For the DINA-H and DINO-H models, 1 2 where L is equal to the maximum number of possible attribute profiles specified for each unique model. For a given dataset, the larger the log-likelihood, the better the model fit; the smaller the AIC value, the better the model fit (Xu & von Davier, 2008). Third, the BIC is defined as: where N is the sample size. Again, the smaller the BIC value, the better the model fit. The AIC and BIC for each condition are reported in the results section."}, {"section_title": "Item fit Indices", "text": "The item fit indices included the \u03b4 index and the IDI. The \u03b4 index is the sequential EM-based \u03b4-method, and serves as a discrimination index of item quality that accounts for both the slip and guessing parameters. \u03b4 j is defined as \"the difference in the probabilities of correct responses between examinees in groups \u03b7 j = 1 and \u03b7 j = 0\" (i.e., examinees with latent responses 1 and 0) (as cited in de la Torre, 2008, p.344) in the DINA and DINA-H models, and in groups \u03c9 j = 1 and \u03c9 j = 0 in the DINO and DINO-H models. The higher the value of \u03b4 j , the lower the guessing and/or slip parameters are, which means the more discriminating the item is. The computational formula for \u03b4 j was provided in equation 5 in Chapter II. An additional item discrimination index applied in the study was the IDI, which provides the diagnostic accuracy for each item j. A higher IDI value means that an item has higher diagnostic accuracy with low guessing and slip. IDI is defined as: The mean and standard deviation of \u03b4 j and IDI for each condition are reported and evaluated in the Chapter IV. In addition, correlation, mean, and standard deviation of both item parameters for each condition are reported."}, {"section_title": "Simulation Study", "text": "The simulation studies attempted to address the accuracy of the item parameter recovery when the cognitive skills were in a specified hierarchy. Varying estimation models were applied. The simulation analysis examined and compared the impacts of the misspecification of a skill hierarchy on various estimation models under their varying assumptions of independent attributes or hierarchically related attributes. The simulation design described in the next session was used to investigate the relationship between the numbers of attributes, test lengths, sample sizes, and various CDMs. Attention was also paid to the conditions while analyzing the hierarchically structured CDMs with smaller sample sizes and contrasting them with the conventional CDMs."}, {"section_title": "Simulation Design", "text": "This section includes two parts. The first part of the simulation procedure describes each factor manipulated in the simulation study, and the second part of simulation procedure describes the steps carried out in the simulation process."}, {"section_title": "Simulation Factors", "text": "The simulation design manipulated five factors: the number of attributes, the test lengths, the data generating models, the sample sizes, and the estimation models. Table 3-11 displays the values of these simulation factors. The first simulation factor was the number of attributes. There were six or eight attributes assessed in each test, which were within the usual range of those found with current applications of CDMs (Rupp & Templin, 2008). A review of the literature on multiple classification models showed that most application examples used about four to eight attributes (Hartz, 2002;Maris, 1999;Rupp & Templin, 2008b). Six and eight attributes were chosen in the current study because the intention of the study was to maintain consistency with the previous studies using six attributes (Gierl, et. al., 2007;Leighton et. al., 2004;Rupp, et. al., 2010), and also to understand the effect of reducing the sample size requirements for tests measuring more attributes. The second simulation factor was test length (i.e., the number of items). There were 12 or 30 items in each test. These numbers were chosen because the usual range of items measured in the most current applications of CDMs is two to four items for every single attribute (i.e., all columns of the Q-matrix sum up to 2 or 4) (Rupp & Templin, 2008a). The third simulation factor was the identity of the data-generating model. To demonstrate the proposed model and to effectively answer the research questions, the study chose two of the skill hierarchies to apply in the simulation analysis, which were the linear hierarchy and the unstructured hierarchy The fourth simulation factor was the sample size. Three values were used: 300, 1,000, and 3,000 examinees. Most simulation studies typically use larger sample sizes to estimate models, with the samples ranging from 500 to 10,000 respondents (e.g., Rupp & Templin, 2008a). The smallest sample size in the current study was chosen to examine the performance of the proposed DINA-H and DINO-H models under small sample-size conditions. The fifth factor was the identity of the estimation model. Six different models were considered: the DINA, DINA-H L , DINA-H U , DINO, DINO-H L , and DINO-H U models. While estimating with each skill hierarchical model, the specified possible attribute profiles of the model was applied to be \u03b1 i = { \u03b1 ik } (the examinee's binary skills vector) in the EM algorism, and to replace the amount of 2 K attribute profiles in the conventional DINA and DINO models. In addition, the study only considered the cross comparisons of estimating data generated from the DINA, DINA-H L , and DINA-H U models by using DINA-based models, and estimating data generated from the DINO, DINO-H L , and DINO-H U models by using the DINO-based models. Cross estimations between the DINA and DINO models were not considered."}, {"section_title": "Simulation Procedure", "text": "The simulation steps were listed below. 1. The study first simulated four item-by-attribute Q-matrices (i.e., 12 items measuring 6 attributes, 12 items measuring 8 attributes, 30 items measuring 6 attributes, and 30 items measuring 8 attributes). To be closer to the practical testing, the distribution of the percentage of items measuring varying numbers of attributes from TIMSS data was used as the guideline in simulating the Q-matrices. The current study adapted the distribution based on the Qmatrix in Park, Lee, and Choi 2010 3. Based on the generated Q-matrix, the simulated guessing and slip parameters, and the \u03b1-matrix, the \u03b7-matrix was computed using Equation 4 in Chapter 2 for the N examinees (i.e., 300, 1000, or 3000) for the DINA(-H) model, and the \u03c9-matrix was computed using Equation 10 for the DINO(-H) model. Once the \u03b7-matrix and \u03c9-matrix were obtained, the probability to answer each item correctly (P, from Equation 3 for the DINO(-H) and Equation 9 for the DINO(-H) model) was computed for each examinee. Based on these probabilities, the 0/1 data were simulated randomly from the binomial distribution for each examinee under different simulation conditions. Each data were simulated differently for each replication under each condition; however, their corresponding true item parameters were set the same for the 50 replications under each condition to make comparisons easier. These simulation studies examined the question of which simulated data best fit the evaluating models, based on different skill hierarchies under different conditions. Using simulated data, model parameter recovery was evaluated. Special attention was paid to the robustness of the models, and whether they were able to fit data that were consistent or inconsistent with the assumptions of the relationship among attributes held by the evaluating models. "}, {"section_title": "Fit Indices", "text": "Model fit evaluations were to address the question of whether the data simulated based on different hierarchical models under varying conditions fit the evaluating models. Again, special attention was paid to the robustness of the model to fit the simulated data that were not consistent with the evaluating model. The mean of AIC and BIC for each condition are reported in the result section in which the terms MAIC and MBIC are used.\nFor the model fit indices MAIC and MBIC, all the conditions that used the estimation models consistent with their data generating models show better model fit results (i.e., smaller MAICs and smaller MBICs) than the conditions that used the estimation models inconsistent with their data generating models, as shown in Table 4-26. The results confirm the assumption that a better model fit can be obtained from the calibration results when the relationships among attributes specified in the data generating model are consistent with those in the estimation model. The results also confirm that when skills are ordered hierarchically, the proposed DINA-H models perform better. When the data generating model is DINA-H L , using the DINA-H U model to calibrate has better model fit results than using the conventional DINA model (see Table   4-26). When the true model is DINA-H U , using the DINA estimation model produces smaller MAIC and MBIC values than using the DINA-H L model. If data are generated via the conventional DINA model, the DINA-H U model shows better calibration results than the DINA-H L model. The results are due to different levels of dependency among the hierarchically ordered skills. The DINA-H L model has the highest dependency of skills among the three hierarchical models, but the convention DINA model does not assume any dependency among skills.\nFor the model fit indices MAIC and MBIC, all conditions, except one, that used the estimation models consistent with their data generating models show better model fit results than the conditions that used the estimation models inconsistent with their data generating models (see Table 4-39). The only exception is the smaller MBIC that occurs when using the DINO-H L model to estimate data generated via the DINO-H U model. Generally speaking, the results confirm the main effect of model consistency that better model fit can be obtained from the calibration results when the relationships among attributes specified in the data generating model are consistent with those in the estimation model. Similar to what was found under the DINA-H model, the results also confirm that when skills are ordered hierarchically, the proposed DINO-H model performs better and is preferred to be applied, in terms of the model fit. Due to the different levels of dependency among the hierarchically ordered skills, using the conventional DINO model to estimate data generated via the DINO-H L model produces poorer model fit results than those estimated by the DINO-H U model. However, the results do not show any consistent pattern for the data generated via the conventional DINO or the DINO-H U models.\nGenerally speaking, for the main effect of model consistency, both the DINA (-H) and DINO(-H) models confirm that using the estimation models consistent with the data generating models show better model fit results than the conditions using the estimation models inconsistent with their data generating models, as shown in Tables 4-26 Tables 4-29 and 4-42). Generally speaking, the DINO model outperforms the DINA model in estimating slip parameters for both conditions of J=12 and J=30, whereas the DINA model outperforms the DINO model in obtaining better guessing parameter recovery with smaller AMSE when J=12 (see Table 4-55)."}, {"section_title": "Summary Statistics", "text": "Three bias indices were used to evaluate model parameter recovery: the average squared bias (ASB), the average variance (AVAR), and the average mean-square error (AMSE) of item parameter estimates (s and g) for each condition. Under each simulation condition, the bias for each item was defined as the difference between the average item parameter estimates over replications and their corresponding true generating values. The following formulas use the guessing parameter as an example. where j g is the average g parameter estimate for item j over replications, * j g is the true generating value for item j, and R refers to the number of replications under each condition. The ASB for each condition is defined as the average squared bias: Furthermore, AVAR is defined as the average variance of an item parameter across replications for each condition as: where g jr is the guessing parameter estimate for item j for replication r. Finally, since the mean squared error is equal to the squared bias plus variance, the AMSE is regarded as a combination of information from variance and bias, and is defined as:  Attribute  1  2  3  4  5  6  7  8  Profile 1  0  0  0  0  0  0  0  0  Profile 2  1  0  0  0  0  0  0  0  Profile 3  1  1  0  0  0  0  0  0  Profile 4  1  1  1  0  0  0  0  0  Profile 5  1  1  1  1  0  0  0  0  Profile 6  1  1  1  1  1  0  0  0  Profile 7 Table 3-2. Convergent Attribute Hierarchy Profiles of Eight Attributes   Attribute  1  2  3  4  5  6  7   Profile 1  0  0  0  0  0  0  0   Profile 2  1  0  0  0  0  0  0   Profile 3  1  1  0  0  0  0  0   Profile 4  1  1  1  0  0  0  0   Profile 5  1  1  0  1  0  0  0   Profile 6  1  1  0  1  1  0  0   Profile 7  1  1  1  0  1  0  0   Profile 8  1  1  1  1  0  0  0   Profile 9  1  1  1  1  1  0  0   Profile 10  1  1  0  1  1  1  0   Profile 11  1  1  1  0  1  1  0   Profile 12  1  1  1  1  1  1  0   Profile 13  1  1  0  1  1  0  1   Profile 14  1  1  1  0  1  0  1   Profile 15  1  1  1  1  1  0  1   Profile 16  1  1  0  1  1  1  1   Profile 17  1  1  1  0  1  1  1   Profile 18  1  1  1  1  1  1  1   Profile 19  1  1  0  1  1  1  0   Profile 20  1  1  1  0  1  1  0   Profile 21  1  1  1  1  1  1  0   Profile 22  1  1  0  1  1  0  1   Profile 23  1  1  1  0  1  0  1   Profile 24  1  1  1  1  1  0  1   Profile 25  1  1  0  1  1  1  1   Profile 26  1  1  1  0  1  1  1   Profile 27  1  1  1  1  1  1  1  Table 3- Profile 11  1  1  0  1  0  1  0   Profile 12  1  1  0  1  1  1  0   Profile 13  1  1  1  1  0  0  0   Profile 14  1  1  1  1  1  0  0   Profile 15  1  1  1  1  0  1  0   Profile 16  1  1  1  1  1  1  0   Profile 17  1  1  1  0  0  0  1   Profile 18  1  1  1  1  0  0  1   Profile 19  1  1  1  1  1  0  1   Profile 20  1  1  1  1  0  1  1   Profile 21  1  1  1  1  1  1  1   Profile 22  1  1  1  0  0  0  0   Profile 23  1  1  1  1  0  0  0   Profile 24  1  1  1  1  1  0  0   Profile 25  1  1  1  1  0  1  0   Profile 26  1  1  1  1  1  1  0   Profile 27  1  1  1  0  0  0  1   Profile 28  1  1  1  1  0  0  1   Profile 29  1  1  1  1  1  0  1   Profile 30  1  1  1  1  0  1  1   Profile 31  1  1  1  1  1  1  1    Table 3-4. Unstructured Attribute Hierarchy Profiles of Eight Attributes   Attribute  1  2  3  4  5  6  7   Profile 1  0  0  0  0   0  0   Profile 2  1  0  0  0  0  0  0   Profile 3  1  1  0  0  0   0   Profile 4  1 Profile 31  1  1  1  1  0  0   0    Profile 32  1  1  1  0  1  0   0    Profile 33  1  1  1  0  0  1   0    Profile 34  1  1  1  0  0  0   1    Profile 35  1  1  1  0  0  0   0    Profile 36  1  1  0  1  1  0   0    Profile 37  1  1  0  1  0  1   0  Table 3-4.\u2212\u2212continued   Attribute  1  2  3  4  5  6  7   Profile 38  1  1  0  1     1    Profile 39  1   0  1   0   0    Profile 40  1  1  0  0  1    0    Profile 41  1  1 Profile 52  1  0  1  0  1  0   0    Profile 53  1  0  1  0  0  1   1    Profile 54  1  0  1  0  0  1   0    Profile 55  1  0  1  0  0  0   1    Profile 56  1  0  0  1  1  1   0    Profile 57  1  0  0  1  1  0   1    Profile 58  1  0  0  1  1  0   0    Profile 59  1  0  0  1  0  1   1    Profile 60  1  0  0  1  0  1   0    Profile 61  1  0  0  1  0  0   1    Profile 62  1  0  0  0  1  1   1    Profile 63  1  0  0  0  1  1   0    Profile 64  1  0  0  0  1  0   1    Profile 65  1  0  0  0  0  1   1    Profile 66  1  1  1  1  1  0   0    Profile 67  1  1  1  1  0  1   0    Profile 68  1  1  1  1  0  0   1    Profile 69  1  1  1  1  0  0   0    Profile 70  1  1  1  0  1  1   0    Profile 71  1  1  1  0  1  0   1    Profile 72  1  1  1  0  1  0   0    Profile 73  1  1  1  0  0  1   1    Profile 74  1  1  1  0  0  1   0  Table 3-4.\u2212\u2212continued   Attribute  1  2  3  4  5  6    Profile 75  1  1   0     1    Profile 76  1   0  1   1   0    Profile 77  1  1  0  1 Profile 89  1  0  1  1  0  1   1    Profile 90  1  0  1  1  0  1   0    Profile 91  1  0  1  1  0  0   1    Profile 92  1  0  1  0  1  1   1    Profile 93  1  0  1  0  1  1   0    Profile 94  1  0  1  0  1  0   1    Profile 95  1  0  1  0  0  1   1    Profile 96  1  0  0  1  1  1   1    Profile 97  1  0  0  1  1  1   0    Profile 98  1  0  0  1  1  0   1    Profile 99  1  0  0  1  0  1   1    Profile 100  1  0  0  0  1  1   1    Profile 101  1  1  1  1  1  1   0    Profile 102  1  1  1  1  1  0   1    Profile 103  1  1  1  1  1  0   0    Profile 104  1  1  1  1  0  1   1    Profile 105  1  1  1  1  0  1   0    Profile 106  1  1  1  1  0  0   1    Profile 107  1  1  1  0  1  1   1    Profile 108  1  1  1  0  1  1   0    Profile 109  1  1  1  0  1  0   1    Profile 110  1  1  1  0  0  1   1    Profile 111  1  1  0  1  1  1   1  Table 3-4.\u2212\u2212continued   Attribute  1  2  3  4  5  6    Profile 112  1  1   1     0    Profile 113  1   0  1   0   1    Profile 114  1  1  0    14. Solve multi-step word problems posed with whole numbers and having whole-number answers using the four operations, including problems in which remainders must be interpreted. Represent these problems using equations with a letter standing for the unknown quantity; Generate a number or shape pattern that follows a given rule. Identify apparent features of the pattern that were not explicit in the rule itself. 10, 14, 27 2, 6, 12, 14 15. Use equivalent fraction as a strategy to add and subtract fractions. 1, 17, 22 9, 20, 21, 26  Item\\Attribute 1 2 3 4 5 6 7 8 9 11 12 13 14 Sum 1 M012001 1 0 0 0 0 0 0 0 0 0 0 1 0 3 2 M012002 0 0 0 0 0 1 0 0 0 0 0 0 0 1 3 M012004 0 1 0 0 0 0 1 0 0 0 0 0 0 2 4 M012040 0 0 0 0 0 1 1 0 0 0 0 0 0 2 5 M012041 1 0 0 0 0 0 0 0 0 0 0 0 0 1 6 M012042 0 0 0 0 1 0 0 0 1 0 0 0 0    Item   1  2  3  4  5  6  7  8  9 10 11 12   Total  Number   1  1  0  0  0  0  0  0  0  0  0  0   1  2  1  0  1  0  0  0  0  0  0  0  0   2  3  0  0  1  0  1  0  0  0  0  0  0   2  4  0  1  0  0  1  1  0  0  0  0  0   3 2  20  0  0  1  1  1  1  0  0  0  0  0   4  21  0  0  0  0  0  0  1  1  1  0  0   3  22  0  0  0  0  0  0  0  1  1  0  1   3  23  0  0  1  1  1  0  0  0  0  0  0   3  24  0  0  1  0  0  0  0  0  0  0  0   2  25  0  0  1  0  0  0  0  0  0  0  0   2  26  1  0  0  0  0  0  0  0  0  0  0   2  27  0  0  0  1  0  0  0  0  0  0  0   2  28  0  0  1  0  0  0  1  0  1  0  1   4  29  1  0  0  1  0  0  0  0  0  0  0   2  Sum  9  4 15  9  8  3  5  2  5  1  3   Note: The table was from Park et al. (2010) and Choi (2011).       \nThe results of the summary statistics for the main effect of model consistency show that using the consistent estimation model with the data generating model obtained better item parameter recovery (i.e., the smaller ASB and AMSE of guessing and slip parameter estimates) when compared to the results of using the inconsistent model, as shown in Table 4-27. The only two exceptions are that the smaller AVAR of the guessing parameter estimates appears when using the DINA model to calibrate other DINA-H model data. Similar to the results from the evaluation of the fit indices, the results of the summary statistics generally confirm the assumption that better item parameter recovery can be obtained from the calibration results when the relationships among attributes specified in the data generating model are consistent with those in the estimation model, and also confirm that the proposed DINA-H models should be used to calibrate items when cognitive skills are ordered hierarchically.\nThe results of the summary statistics for the main effect of model consistency show that using the consistent estimation model with the data generating model produces more accurate results of guessing and slip parameter estimates with the least AMSE and ASB, compared to the results of using the inconsistent model (see Table 4 and 30 items although the larger ASB is found under the NO_NO condition when J=12. For the slip parameter estimates, the HU_HU condition is better when J=12 and the NO_NO condition is better when J=30."}, {"section_title": "RESULTS", "text": "This chapter describes results from the study. It contains two main sections. The first section describes the results from the real data analysis. The second section presents the results from the simulation study."}, {"section_title": "Results of Real Data Analysis", "text": "This section presents the calibration results from the real data analysis of the DINA-H and DINO-H models. The results for the DINA-H and DINO-H models were compared to the results for the DINA and DINO models, respectively. DINA and DINA-H The following paragraphs provide the results of the model fit, item fit, and item parameter estimates from the real data analysis using the DINA and DINA-H models based on two TIMSS 2003 booklets with different sample sizes."}, {"section_title": "Model Fit", "text": "The results of model fit for both the smaller U.S. and the larger benchmark samples of both booklets show that the values of both AIC and BIC for the DINA-H model are smaller than those of the conventional DINA model because the numbers of parameters (i.e., possible attribute profiles) are largely decreased in the hierarchical models. For a given dataset, the smaller the AIC or BIC value, the better the model fit. As shown in Table 4-1, the differences were computed by subtracting the DINA-H condition values from those of the DINA. The positive values in the differences of AIC and BIC, thus, indicate that the DINA-H model performs better than the DINA model for both the smaller U.S. and the larger benchmark samples of both booklets. Using 0.001 as the criteria for convergence, all the conditions took fewer than 60 cycles of iterations to reach convergence, except for the conditions of using DINA(-H) to estimate B1 benchmark data which took more than 100 cycles to converge. For additional information, the computation of the model fit indices is illustrated as follows.  Table 4-2). In terms of the IDI results, about 45% of the items perform better under the DINA model for the large sample, and about 21% under the DINA-H model (see Table 4-3). For TIMSS B2, about 23% of the items produce better results in the \u03b4 index under the DINA model for the large sample, and about 27% under the DINA-H model (see Table 4-4). About 20% of the items show better IDI results for TIMSS B2 under the DINA model for the large sample, and about 23% under the DINA-H model (see Table 4-5). For B1, it shows that the DINA model is a better model if using larger sample sizes and the DINA-H model is more appropriate to apply under a small sample condition. However, the results for B2 are inconsistent with those found in B1. In B2, the DINA-H model is not necessarily superior to the DINA model under a small sample condition. This may be due to the small difference in sample sizes between the U.S. and the benchmark data or the sample dependent calibration results in CDMs, and will need more analyses using different datasets to provide more evidence.\nFor both the U.S. and the benchmark samples of both booklets, the results of model fit for the DINO-H model are better than those of the conventional DINO model because the numbers of parameters are largely decreased in the conventional hierarchical models. Similar to "}, {"section_title": "Item Parameter Estimates", "text": "The correlations of both the slip and guessing parameter estimates between the DINA and the DINA-H models are very high (i.e., all larger than 0.95) for the smaller U.S. sample for both booklets, as shown in Table 4-6. For the larger benchmark sample, the high correlational results are only found in B2. The correlations between the two models are slightly lower for the B1 data. The correlations of both item parameter estimates between the smaller U.S. and the larger benchmark samples are very high (i.e., all larger than 0.90) for the DINA-H model for both booklets. For the DINA model, the correlations between two sample sizes are also high for the B2 data; however, the results are less similar for the B1 data. The correlations between models are higher than the correlations between sample sizes conditions. The results of item parameter estimates, guessing and slip, for TIMSS B1 and B2 data under the DINA and DINA-H models are shown in Tables 4-7 to 4-10, respectively. The means of the guessing and slip parameter estimates for both U.S. and benchmark data under the DINA-H model are slightly higher than those in the DINA model for both TIMSS booklets. The standard deviations of the guessing and slip parameter estimates for both U.S. and benchmark data under the DINA-H Model are slightly lower than those in the DINA model for both TIMSS booklets, except for the results of the U.S. sample in B1. Generally speaking, in terms of parameter estimates, items perform similarly under the conventional DINA and the DINA-H models for both small and large sample sizes. The means of the differences of parameter estimates between the two models are less than 0.07 for B1 and less than 0.03 for B2. The mean of the differences of parameter estimates between the small and large sample sizes are also small for both booklets."}, {"section_title": "DINO and DINO-H", "text": "This section shows the results of the model fit, item fit, and item parameter estimates from the real data analysis using the DINO and DINO-H models calibrating two TIMSS 2003 booklets with different sample sizes."}, {"section_title": "Main Effect of Numbers of Attributes", "text": "Since the main effect of model consistency was confirmed and supported, the following main effects of other study variables and their interaction effects were examined by comparing only the results of using the consistent models in generating and estimating data. Note also that magnitudes of the model fit indices (i.e., MAIC and MBIC) depend heavily on the numbers of items, attributes, and examinees (see The results of the summary statistics for the main effect of numbers of attributes show that the conditions of eight attributes obtain better item parameter recovery with smaller ASB, AVAR, and AMSE when compared to the results of six attributes (see Table 4-28). The only exception is that AVAR of slip parameter estimates is smaller for the condition of six attributes. For both conditions of six and eight attributes, the guessing parameter estimates show better recover than the slip parameter estimates. Additionally, increasing the number of attribute from six to eight improves the recovery of guessing parameter estimates, with the amount of ASB, AVAR, and AMSE decreasing. Although slip parameter estimates show the same pattern, the amount of the improvement of AMSE from K=6 to K=8 is only about half of that for the guessing parameter estimates. For both the guessing and slip parameter estimates and for both conditions of numbers of attributes, the condition of using the DINA-H L model shows the more accurate calibration results than the DINA and DINA-H U models."}, {"section_title": "Main Effect of Test Lengths", "text": "The results of the summary statistics for the main effect of test lengths show that the condition of 12 items in a test has smaller ASB and AMSE for both the guessing and slip parameter estimates when compared to the results of 30 items, as shown in "}, {"section_title": "Main Effect of Sample Sizes", "text": "The results of the summary statistics for the main effect of sample sizes consistently show that using N=3000 has the least ASB, AVAR, and AMSE for both the slip and guessing parameter estimates, compared to the results of using N=300 or 1000 (see Table 4-30). It means that using the larger sample size would result in better item parameter recovery. The results of the summary statistics support the assumption that better item parameter recovery can be obtained from the calibration results when large sample sizes are applied. and K=8, all other summary statistics results show that the guessing parameter recovery is better than the slip parameter recovery.\nAll the results of the summary statistics for the main effect of varying numbers of sample sizes show that the condition of the largest sample sizes (i.e., N=3000) produced the smallest ASBs, AVARs, and AMSEs for both the guessing and slip parameter estimates, as shown in  Tables 4-41 and 4-42). The interaction effect can be observed in Figure 4-6 showing that the conditions of more items with fewer attributes produce better item parameter recovery than the conditions of fewer items with more attributes.  Tables 4-46 and 4-47, respectively. The results of summary statistics for the guessing parameter estimates show that the condition of N=3000 with K=6 obtained the best item parameter recovery results with the least mean AMSE and AVAR, although the condition of N=1000 with K=6 produced the least mean ASB. The difference of mean ASB between the conditions of N=1000 with K=6 and N=3000 with K=6 is relatively small (i.e., 0.00002). The results of summary statistics for the slip parameter estimates show that the condition of N=3000 with K=8 obtained the least mean AVAR and AMSE, and the condition of N=1000 with K=8 produced the least mean ASB. The difference of mean ASB between the conditions of N=1000 with K=8 and N=3000 with K=6 is 0.000014. For the main effect, the condition of six attributes performs better than eight attributes in estimating the guessing parameters, but the condition of eight attributes provides better results for the slip parameter estimates (see Table 4-41), and the condition of N=3000 provides the best results for both the guessing and slip parameter estimates (see Table 4-43). Generally speaking, the interaction effect shows that the condition of larger sample sizes with fewer attributes in the Q-matrix show better item parameter recovery for the guessing parameter estimates, but the condition of larger sample size with more attributes is better for the slip parameter estimates, as shown in Figure 4- estimates. The trend of the main effects of sample sizes and test lengths is clearer than the interaction effect between the two variables (see Figure 4-8), as the main effects show that the condition of N=3000 and the condition of J=30 are preferred (see Tables 4-42 and 4-43).  Tables 4-41 to 4-43). For the three-way interaction effect for the guessing parameter estimates, the condition of N=3000 by J=30 by K=6 produced the best item parameter recovery results with the least mean AMSE, ASB, and AVAR (see Table 4-50). For the slip parameter estimates, the condition of N=3000 by J=30 by K=8 shows the best item parameter recovery results with the least mean AMSE and AVAR, while the condition of N=3000 by J=30 by K=6 has the least mean ASB (see \nAll the results of the summary statistics for the main effect of sample sizes under both the DINA(-H) and DINO(-H) models show that using the larger sample size (i.e., N=3000) would result in better item parameter recovery for both the guessing and slip parameter estimates (see Tables 4-30 and 4-43). Some mixed findings appear in the differences of item parameter recovery between the DINA(-H) and DINO(-H) models. For the guessing parameter recovery, the DINA model performs better in the conditions of sample sizes of 300 and 3000, whereas the DINO model performs better in the condition of sample size of 1000 (see Table 4-56). The DINO model outperforms the DINA model in obtaining the better slip parameter recovery, regardless of the sample sizes. The comparisons of the summary statistics for all interaction effects between the DINA(-H) and DINO(-H) models are shown in Tables A19 to A26 in Appendix.      Note. Item 2 was removed because its IDI of the DINO model is -65444066333947.9 and is -19610.25 of the DINO-H model for the U.S. sample.      AVAR(g) AMSE(g) ASB(g) AVAR(g) AMSE(g) N=300 J=12 0.000079 0.004693 0.004772 0.000533 0.002612 0.003145 J=30 0.000051 0.001445 0.001496 0.000067 0.001191 0.001258 N=1000 J=12 0.000158 0.001904 0.002062 0.000137 0.000678 0.000815 J=30 0.018400 0.000876 0.019276 0.000020 0.000344 0.000363 N=3000 J=12 0.000164 0.000560 0.000724 0.000226 0.000183 0.000409 J=30 0.000020 0.000189 0.000208 0.000009 0.000116 0.000125     Note. The highlighted values indicated that the DINA(-H) model performs better than its DINO(-H) counterpart (i.e., the smaller ASB, AVAR, and AMSE in the DINA(-H) model). The bold characters indicate the conditions whose data-generating model is consistent with its estimation model. These rules are the same for the following tables.  HL_HL -HL_HL -0.00467 0.00053 -0.00414 -0.00487 -0.00054 -0.00541 HU_HU -HU_HU -0.00439 0.00598 0.00159 -0.00470 -0.00120 -0.00590 NO_NO -NA_NA -0.00441 0.00200 -0.00242 -0.00492 -0.00198 -0.00690 Mean -0.00449 0.00284 -0.00166 -0.00483 -0.00124 -0.00607 N=3000 HL_HL -HL_HL -0.00011 0.00019 0.00008 0.00001 -0.00016 -0.00016 HU_HU -HU_HU 0.00014 0.00262 0.00277 0.00000 -0.00043 -0.00043 NO_NO -NA_NA 0.00017 0.00065 0.00082 -0.00012 -0.00070 -0.00082 Mean 0.00007 0.00115 0.00122 -0.00004 -0.00043 -0.00047 In general, the DINA-H and DINO-H models show better model fit, better item fit, and better item parameter recovery than the conventional DINA and DINO models when skills are hierarchically ordered. The misspecification of a skill hierarchy has a negative impact on all models. The item parameters are poorly recovered when a specified skill hierarchy is inconsistent with an estimation model. The study suggests that the DINA-H/ DINO-H models, instead of the conventional DINA/ DINO models, should be considered when skills are hierarchically ordered. The major findings of the study are summarized in the order of the research questions proposed in Chapter I. 1. How do the proposed DINA-H and DINO-H perform? The DINA-H and DINO-H models produce better model fit results than the conventional DINA and DINO models for both the smaller U.S. and the larger benchmark samples of both booklets. This is so because the numbers of parameters in the hierarchical models are smaller than those in the conventional models. The item fit results are inconsistent with the model fit results. Items display better item fit in the conventional DINA and DINO models than in the DINA-H and DINO-H models for both small and large sample sizes. However, the values of item fit indices decrease (i.e., worse fit) when applying the conventional models to the smaller sample size condition, whereas the results are either very similar or sometimes become better when applying the DINA-H and DINO-H models to the smaller sample size condition. It implies that the conventional models are more sensitive to the small sample sizes, while the DINA-H and DINO-H models perform consistently across different sample sizes. The DINA and DINO models are better models if using a larger sample size, and the DINA-H and DINO-H models are superior and more appropriate to use for a small sample size. This finding supports the assumption that decreasing the number of possible attribute profiles will decrease the sample size requirement for conducting CDM calibrations. Comparing the performances of the DINA and DINO models when applying a skill hierarchy, the results of analyzing two TIMSS 2003 mathematics datasets show that the DINA-H model outperforms the DINO-H model, whereas the simulation results support that the DINO-H outperforms the DINA-H model. This is related to the different evaluation indices used in the studies. The model and item fits were examined in the real data analysis, while the item parameter recovery was evaluated in the simulation study. The findings of the simulation study that the DINO-H model is better than the DINA-H model when incooperating a skill hierarchy may be due to the assumption of the DINO model. One of the DINO model's main assumptions is to allow the compensation among the mastery of each attribute. This assumption may be more consistent with the hierarchically related cognitive skills because low levels on certain skills could be compensated for by high levels on other skills. The DINO model is more often to be used in medical and psychological assessments; however, the DINA model which assumes that the skills could not be compensated for each other is preferred in educational assessment. This may be the reason why the DINA model fits the TIMSS data better than the DINO model. Research Question 2 2. When skills are ordered hierarchically, how do the conventional DINA and DINO models and the proposed DINA-H and DINO-H models compare? The real data analysis shows that the DINA-H/ DINO-H models outperform the conventional DINA/DINO models in the model fit results, but not in the item fit results. The hierarchical models perform consistently across various sample sizes, while the conventional models are more sensitive to and perform poorly for small sample sizes. The main effect of model consistency from the simulation analysis shows that better results can be obtained when the relationships among attributes specified in the data generating model are consistent with those in the estimation model. Hence, when skills are ordered hierarchically, the proposed DINA-H and DINO-H models should be considered, rather than the conventional DINA and DINO models. The simulation analysis provides valuable information about the inaccuracy of parameter estimates due to misspecification of the relationships among attributes and possible attribute profiles. The simulation study examined model fit and item parameter recovery when the data simulation models are different from the estimation models. Specifically, both the conventional DINA and DINO models and new hierarchical models are applied when skills are independent or dependent in a specified hierarchical structure. The simulation study confirms that model specification needs to be consistent with the assumptions and characteristics of skills in order to obtain better model fit, item fit, and item parameter recoveries. The current study contributes to the examination of the performance of the "}, {"section_title": "Interaction Effect of Sample Sizes by Numbers of Attributes", "text": "Tables 4-33 and 4-34 and Figure 4-2 present the ASB, AVAR, and AMSE of guessing and slip parameter estimates for the interaction effect of N by K for the DINA and DINA-H models. The results of summary statistics for the guessing and slip parameter estimates are different. The smallest mean AMSE was shown in the condition of N=3000 and K=8 for the guessing parameter estimates while the smallest mean ASB appears in the condition of N=300 and K=6. For the slip parameter estimates, the best item parameter recovery was observed in the condition of N=3000 and K=6. For the main effect, the condition of eight attributes and the condition of N=3000 perform better (see Tables 4-28 and 4-30). In terms of interaction effect, the condition of N=3000 and K=8 provides the best results for the guessing parameter estimates, and the condition of N=3000 and K=6 provides the best results for the slip parameter estimates (see Tables 4-33 and 4-34). In terms of AMSE for the guessing parameter estimates, the condition of N=3000 and K=8 performs the best, next the condition of N=3000 and K=6 followed by the condition of N=1000 and K=8. For the slip parameter estimates, the top three conditions are the condition of N=3000 and K=6, the condition of N=3000 and K=8, and the condition of N=1000 and K=8. The item parameter recovery results are better for the guessing parameters than for the slip parameter estimates. The only exception is that the smaller AVAR appears under the condition of N=1000 and K=6 for the slip parameter estimates rather than guessing. Comparing the conditions of different estimation models, for the guessing parameter estimates the DINA-H L model shows better item parameter recovery of the smaller AMSE than the DINA and DINA-H U models, although slightly larger ASB is shown for the conditions of N=1000 and K=6, N=1000 and K=8, N=3000 and K=6, and N=3000 and K=8. The NA_NA condition is better than the HU_HU condition for varying sample sizes when K=6, but this is not so when K=8. For the slip parameter estimates, the DINA-H L model performs the best, and the conventional DINA model obtains the poorest item parameter recovery results. Consistently across all conditions, the guessing parameter results are better than the slip parameter results. Tables 4-35 and 4-36 list the ASB, AVAR, and AMSE of guessing and slip parameter estimates for the interaction effect of sample sizes by test lengths for the DINA and DINA-H models. For both parameter estimates, the condition of N=3000 and J=30"}, {"section_title": "Interaction Effect of Sample Sizes by Test Lengths", "text": "obtains the best item parameter recovery results and the next better results are shown under the condition of N=3000 and J=12, while the condition of N=1000 and J=30 obtains the worst item parameter recovery results. But, for the main effects, the condition of N=3000 and the condition of J=12 produce the best item parameter recovery results (see Tables 4-29 and 4-30). The AMSE, ASB, and AVAR decrease when sample size increases under the condition of J=12 for both guessing and slip parameter estimates (see Tables 4-35 and 4-36). When the sample size is 300 or 3000, the conditions with more items produce more accurate item parameter recovery results for both guessing and slip parameter estimates (see Figure 4-3). The guessing parameters are recovered better than the slip parameters with smaller error across all conditions. The DINA-H L model shows the best item parameter recovery results under all conditions for both the guessing and slip parameter estimates. In general, for the guessing parameter estimates, the conventional DINA model recovers item parameter better (i.e., smaller AMSE) than the DINA-H U model with some exceptions for ASB and AVAR (see For the main effects of both the guessing and slip parameter estimates, best results were obtained from the conditions of N=3000, J=12, and K=8 (see Tables 4-28 to 4-30). For the three-way interaction effect for the guessing parameter estimates, across all samples size conditions, the condition of J=30 and K=8 obtained the best item parameter recovery with the smallest ASB, AVAR, and AMSE appearing under the condition of N=3000 by J=30 by K=8, followed by the condition of N=3000 by J=30 by K=6, and then the condition of N=1000 by J=30 by K=8 (see Table 4-37 and Figures 4-4 and 4-5). For the slip parameter estimates, the condition of N=3000 by J=30 by K=6 obtained the best item parameter recovery with the smallest ASB, AVAR, and AMSE. The next better results are shown under the condition of N=3000 by J=30 by K=8 and then the condition of N=3000 by J=12 by K=6 (see Table 4 and the three-way interaction effect of sample size by test length by attribute."}, {"section_title": "Main Effect of Model Consistency", "text": "This part discusses the results of the fit indices and summary statistics for the main effect of model consistency when using varying estimation models consistent or inconsistent with the specifications of the skill hierarchies under the DINO and DINO-H models from the simulation study.\nThis part contrasts the results of the fit indices and summary statistics between the DINA(-H) and DINO(-H) models from the simulation study for the main effect of using varying estimation models consistent or inconsistent with the specifications on the skill hierarchies."}, {"section_title": "Limitations", "text": "This section provides a list of limitations of this study. First of all, the development and the misspecification of the Q-matrix and hierarchy in the real data analysis is one concern, although two independent coders separately coded the Q-matrix and construct the skill hierarchy based on the CCSS. There is still a possibility that other alternate hierarchical structures are available because teachers may use different instructions and students may use varying learning strategies and various problemsolving strategies in answering an item. Empirical and theoretical evidence needs to be provided to justify the distinct hierarchies for a test before conducting real data analysis and evaluating the fit. In addition, the misspecification of a Q-matrix would introduce bias and the resultant outcome of analysis would be questionable. Pilot studies could be helpful in validating the Q-matrix. Sometimes inconsistent findings appear between the DINA(-H) and the DINO(-H) models, between the guessing and slip parameter estimates, between model fit and item fit indices, and between the two TIMSS booklets. This may be due to the differences in the nature of the two models and in the two fit indices. In the item fit results of the real data analysis, the DINA-H model is shown to be a better model than the conventional DINA model when the sample size is smaller in one booklet; however, this finding is not fully supported by the results based on the other booklet data. The somewhat dissimilar results between the two booklets data may be due to the differences in the items and attributes of the two booklets. The DINO-H model is shown to be a more appropriate model with smaller sample size based on both booklets. This may be due to the small sample size difference between the U.S. and the benchmark data or sample dependent calibration results in CDMs, and will need more analyses using different datasets to provide conclusive evidence. In addition, the real data analysis is a retrofitting analysis. TIMSS study was not originally developed and intended to be analyze via CDMs. Due to the scope of the study, only a few variables and a limited number of conditions for each simulation factor were considered in the simulation design. Future research could incorporate other factors that were not considered in the current study. The comparisons between the DINA/DINA-H and the DINO/DINO-H models are only based on numeric data. In practical situations, it is crucial to consider the rationality of different assumptions of these two models. Additionally, other confounding factors such as the dependency among skills or the response patterns, dimensionalities and item difficulties, are not completely excluded in the analysis."}, {"section_title": "Future Research Questions", "text": "Some suggestions of how to address the limitations in the study and suggestions for future research are discussed below. First, the study only implemented two types of cognitive attribute hierarchies, the linear and unstructured hierarchies. Other types (e.g., the convergent and divergent hierarchies) could also be investigated and compared to the results from the current study. The study implemented the EM algorithm, which is one of the popular estimating methods in CDMs. Other algorithms, for example MCMC, may be applied and compared to the results of using the EM algorithm. Since initializing the EM algorithm requires setting up the starting values of the item parameters that need to be estimated, the impact of varying starting values on the parameter estimates can also be compared and investigated. Additionally, the current study initialized a flat (non-informative) prior, assigning a probability of 1/L to each of the L skill patterns. Future studies can compare different prior distributions or adapt the prior distribution obtained from different models with the same dataset. In addition to the model fit, item fit, biases, and AMSEs, the differences in scoring and classifying examinee respondents between the conventional models and the proposed hierarchical models can be evaluated. The three common approaches in assigning an examinee into a latent class, MLE, MAP, and EAP, can be compared. As mentioned earlier, the common limitation in current CDMs is that they use 0/1 discrete variables to represent true person profiles, rather than a continuous variable. A different way of score reporting could be to report the probabilities or percentages of mastering each attribute based on the probability from EAP results in future studies. EAP calculates the probabilities of mastery for each attribute for an examinee and sets up a cutoff probability value (usually at 0.5) to classify the attribute into mastery or non-mastery (Huebner, & Wang, 2011). This cutoff can be altered based on different research purposes, for example, setting a lower cutoff value for lower grade students and a higher value for higher grade students. It is also possible to report more than two mastery categories, such as low, medium, high, and advanced levels. Future research can compare various CDMs when using with multiple mastery levels. In future studies, ideas about how students from different countries vary in reaching mastery levels of expected content knowledge and skills will provide opportunities to reform and to improve students' performance by applying findings of this study to curriculum development, teacher education, and other kinds of support in education. Future research could also incorporate a number of other factors that were not considered in the current study, for example, different groups of examinee ability levels, different proportions of items measuring varying numbers of attributes, more varieties of Q-matrices and sample sizes as well as different content subjects."}, {"section_title": "Conclusions", "text": "When cognitive skills are ordered hierarchically, leading to a smaller number of attribute profiles than the full independent attribute profiles, an appropriate model should incorporate the hierarchy in the estimation process. The DINA-H and DINO-H models are introduced to fulfill the goal of providing models whose model specifications, the relationships among attributes, possible attribute profiles, and Q-matrices are consistent with the theoretical background. Through the analysis conducted in the study and the evaluation indices, in general, the DINA-H and DINO-H models are deemed to be a better option with better model fit and better item parameter recovery when calibrating items with hierarchically structured attributes and with smaller sample sizes. The simulation analysis provides valuable information that using larger sample sizes, using more items measuring fewer attributes, and using larger sample sizes with more items in a test produce better item parameter recovery under both the DINA (-H) and DINO(-H) models. The real data analysis shows the illustration of applying CDMs to a large-scale assessment, which demonstrates the feasibility of retrofitting. This successful application can be a promising way to provide informational feedback about examinees' mastery in varying levels of hierarchically ordered cognitive skills. This can help inform instructors to reflect on their teaching procedures and curricular development. Specifying attribute profiles incorrectly would affect the accuracy of estimates of item and attribute parameters. When attributes are hierarchically ordered, the conventional CDMs might produce results that are less accurate. The study is unique in its incorporation of hierarchically structured skills into the estimation process of the conventional DINA/DINO models, by proposing the new DINA-H and DINO-H models. To sum up, the results of the study demonstrate the benefits, efficiencies, and feasibility of the proposed DINA-H and DINO-H approaches, which facilitate the reduction of possible attribute profiles in analyzing a CDM. Table A4. Summary Statistics for the Guessing Parameter of J by K for DINA(-H) when N= 300 K=6 K=8 ASB(g) AVAR(g) AMSE(g) ASB(g) AVAR(g) AMSE(g)"}, {"section_title": "J=12", "text": "HL_300_NA 0.003397 0.001739 0.005136 0.001877 0.001016 0.002893 HL_300_HL 0.000012 0.002256 0.002268 0.000146 0.000842 0.000988 HL_300_HU 0.000728 0.003603 0.004331 0.002823 0.001005 0.003827 HU_300_NA 0.003221 0.007370 0.010591 0.000445 0.003674 0.004119 HU_300_HL 0.006349 0.014933 0.021282 0.007866 0.002812 0.010677 HU_300_HU 0.000091 0.008503 0.008595 0.000395 0.003290 0.003684 NA_300_NA 0.000134 0.003319 0.003453 0.001058 0.003704 0.004763 NA_300_HL 0.012215 0.006017 0.018232 0.006039 0.001809 0.007847 NA_300_HU 0.009027 0.003963 0.012990 0.002212 0.003830 0.006042 Mean 0.003908 0.005745 0.009653 0.002540 0.002442 0.004982"}, {"section_title": "J=30", "text": "HL_300_NA 0.000064 0.001263 0.001327 0.000272 0.001150 0.001422 HL_300_HL 0.000026 0.001197 0.001223 0.000021 0.001065 0.001086 HL_300_HU 0.000036 0.001228 0.001264 0.000148 0.001110 0.001258 HU_300_NA 0.000732 0.002633 0.003365 0.000106 0.001261 0.001367 HU_300_HL 0.018832 0.002913 0.021744 0.008670 0.002341 0.011010 HU_300_HU 0.000100 0.002023 0.002123 0.000097 0.001240 0.001338 NA_300_NA 0.000027 0.001115 0.001143 0.000083 0.001267 0.001350 NA_300_HL 0.017199 0.000786 0.017985 0.007462 0.001562 0.009024 NA_300_HU 0.010637 0.001015 0.011652 0.000905 0.001588 0.002493 Mean 0.005295 0.001575 0.006870 0.001974 0.001398 0.003372 Table A5. Summary Statistics for the Guessing Parameter of J by K for DINA (-H)    HL_3000_NO 0.001732 0.000239 0.001971 0.000299 0.000091 0.000390 HL_3000_HL 0.000036 0.000218 0.000254 0.000002 0.000084 0.000086 HL_3000_HU 0.002714 0.000207 0.002921 0.000254 0.000092 0.000345 HU_3000_NO 0.000279 0.000363 0.000643 0.000434 0.000116 0.000551 HU_3000_HL 0.008989 0.000433 0.009422 0.006503 0.000070 0.006572 HU_3000_HU 0.000245 0.000288 0.000533 0.000290 0.000100 0.000389 NO_3000_NO 0.000138 0.000269 0.000407 0.000209 0.000165 0.000374 NO_3000_HL 0.010720 0.000725 0.011445 0.008498 0.000104 0.008602 NO_3000_HU 0.010307 0.000540 0.010848 0.003798 0.000177 0.003975  "}, {"section_title": "Mean", "text": ""}]