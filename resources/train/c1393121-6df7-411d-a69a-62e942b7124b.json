[{"section_title": "Abstract", "text": "Cortical thickness estimation performed in-vivo via magnetic resonance imaging (MRI) is an effective measure of brain atrophy in preclinical individuals at high risk for Alzheimer's disease (AD). However, the high dimensionality of individual cortical thickness data coupled with small population samples make it challenging to perform cortical thickness feature selection for AD diagnosis and prognosis. Thus far, there are very few methods that can accurately predict future clinical scores using longitudinal cortical thickness measures. In this paper, we propose an unsupervised dictionary learning algorithm, termed Multi-task Sparse Screening (MSS) that produces improved results over previous methods within this problem domain. Specifically, we formulate and solve a multi-task problem using extracted top-p significant features from the Alzheimer's Disease Neuroimaging Initiative (ADNI) longitudinal data. Empirical studies on publicly available longitudinal data from ADNI dataset (N = 2797) demonstrate improved correlation coefficients and root mean square errors, when compared to other algorithms."}, {"section_title": "INTRODUCTION", "text": "Cortical thickness estimation in magnetic resonance imaging (MRI) is an important research that has been applied to detect localized neuroanatomical differences. It has been carefully studied in Alzheimer's Disease (AD) research as a potential imaging biomarker to evaluate AD risk, track AD progression, especially in preclinical individuals at high risk for AD (e.g. Mild Cognitive Impairment (MCI)), and facilitate early interventions [5] . A number of cortical thickness estimation methods have been developed [4, 11, 8, 17, 19] and they were well adopted in neuroimaging research. Despite advances in cortical thickness estimation used to track symptomatic patients, there is still a lack of accurate and\nThe research was supported in part by NIH (R21AG049216, RF1AG051710, R01EB025032 and U54EB020403) and NSF (DMS-1413417 and IIS-1421165). reliable brain imaging systems that can predict future clinical decline by analyzing longitudinal cortical thickness changes.\nIn neuroimaging research, there is also constant interest in building prediction models by analyzing global brain structural changes. A growing number of longitudinal preclinical or clinical AD imaging cohorts are under development. However, the main challenge in developing brain imaging systems to assist AD diagnosis and prognosis arises from the fact that the neuroimaging data dimensionality is intrinsically high while only a small number of samples are typically available. In this regard, feature selection is necessary to overcome this so-called \"large p, small n\" problem (p = 129, 600 in this paper). Feature selection reduces the feature dimension by selecting a subset of original variables [9] , which are considered high quality training data. However, most current feature selection methods either demand careful parameter tuning or generate features that cannot be easily interpreted or even visualized or focus on classification problem [16] . Lasso [18] is one of the most popular feature selection methods due to its computational feasibility and amenability to theoretical analysis, but the Lasso estimator is not variable selection consistent if the irrepresentable condition fails [21] , which means the correct sparse subset of the relevant variables cannot be identified asymptotically with large probability. In addition, over-fitting becomes one of the biggest concerns for building prediction models in brain imaging research. Recent developments from dictionary learning [14, 12, 13, 26] can offer valuable insights for above challenges. However, most existing works on dictionary learning focus on the prediction of a target outcome at a single time point [24, 22] or some regionof-interest [23, 25] . To address above challenges, a joint analysis of imaging features from multiple time points is expected to improve AD prediction performance.\nIn this paper, we propose a novel feature selection model integrated with an unsupervised framework that we termed as Multi-task Sparse Screening (MSS) algorithm. Our framework has two components. Stage one selects the top-p significant features by screening group Lasso based feature ranking [20] , which is theoretically rigorous and more robust against data perturbation and less sensitive to the input parameters. The second stage is formulated as a multi-task dictionary learning problem which uses shared and individual dictionaries to encode both consistent and varying imaging features along longitudinal time points. We evaluate the proposed framework on brain images from Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu) (N = 2797) by analyzing longitudinal cortical thickness features to predict future cognitive scores, including the Mini Mental State Examination (MMSE) and Alzheimer's Disease Assessment Scale cognitive subscale (ADAS-cog). Our experimental results outperform seven other methods and demonstrate the superiority of the proposed algorithm.\nOur main contributions are threefold. First, we select top-p stability features by robust group Lasso screening from longitudinal cortical thickness features. Second, we consider the variance of subjects from different time points by an unsupervised dictionary learning method (i.e. MSS). To the best of our knowledge, it is the first learning model that unifies both sparse learning feature screening and multi-task learning with dictionary learning research for brain imaging analysis. Thirdly, in our empirical experiments on a relatively large ADNI dataset (N = 2797), the proposed MSS achieved lower estimation errors and higher correlation coefficients when compared to seven other algorithms."}, {"section_title": "MULTI-TASK SPARSE SCREENING", "text": "Let X \u2208 R d\u00d7n represents the cortical thickness features and y \u2208 R n as a vector of n observations (responses) which are responses of the clinical scores (e.g., MMSE and ADAScog) in regression problem. Because we use three time points (baseline, 6-month, 12-month) to predict the clinical scores at 24-month, we divide total d features into L groups, with d l as the number of features in group l. We use a data matrix,\n, to represent the predictors corresponding to the lth group, with corresponding coefficient vector [\u03b1] l . We assume that y and X have been normalized so that all variables have a mean of zero. With the group information available, the group Lasso problem [21] takes the form of solving the convex optimization problem as follows: Set all the elements of \u0393 to be zero.\n3:\nOptimize the Group Lasso with \u03bb j , X t , y t , get the optimal solution a * (\u03bb j )."}, {"section_title": "5:", "text": "For each group l in the feature space L,\nis not zero. Rank \u0393 in descending order, select the top p features from X t to construct X t . 8: end for 9: for k = 1 to \u03ba do 10: for t = 1 to T do 11: Get feature matrix X t . [18] at the group level and it does reduce to lasso when the group sizes are all one. The optimal solution vector \u03b1 of the group Lasso problem above is sparse at the group level but could be made sparse within a group by adding an L1 norm penalty term. we consider the non-zero elements as the important features because of the sparsity of the solution vector. Therefore we propose a group lasso screening process in this study to select the top p relevant features to the original ADNI dataset.\nWe optimize the group Lasso screening in a sequence of parameter values \u03bb 0 > \u03bb 1 > ... > \u03bb J . For each parameter value \u03bb j where j \u2208 (0, J), we obtain the optimal solution vector \u03b1 * (\u03bb j ). Then we record the index of non-zero elements in the solution vector. We use an index factor \u0393 to record the frequency of non-zero elements in each \u03b1 * (\u03bb j ) where \u0393 \u2208 R L . For each group l in the feature space L, we increase the \u0393 l by \u0393 l = \u0393 l + 1 when the elements of [a] * l (\u03bb j ) are not equal to zero in the l group. Finally, after obtaining the index frequency \u0393, we rank \u0393 by descend and select the top p frequent ones to construct the feature matrix X from X.\nAfter the screening procedure, to use features from different time points, we defined our multi-sparse learning model. Given the reconstructed feature matrices from T different time points: { X 1 , X 2 , ..., X T }, our objective is to learn a set of sparse codes {Z 1 , Z 2 , ..., Z T } for each time point where X t \u2208 R p\u00d7nt , Z t \u2208 R lt\u00d7nt and t \u2208 {1, ..., T }; n t is the number of subjects for X t and l t is the dimension of each sparse code in Z t . When employing sparse coding [15] to learn the sparse codes Z t by X t individually, we obtain a set of dictionaries {D 1 , ..., D T } but there are no correlations between the learned dictionaries. One solution is to construct the feature matrices { X 1 , ..., X T } into one feature matrix X to obtain the dictionary D. However, only one dictionary D is not enough to show the variation among subjects from different time points. To address this challenge, we integrate the idea of multi-task learning into sparse coding and propose a novel multi-task algorithm termed Multi-task Sparse Screening (MSS), to learn features from different time points.\nFor the feature matrix X t of a particular time point, MSS learns a dictionary D t and sparse codes Z t . D t is composed of two parts:\nD is common among all the learnt dictionaries whileD t is different from each other and only learnt from X t . Thus, objective function can be reformulated as follows:\n|| \u00b7 || F is Fibonacci norm and \u03bb is the positive regularization parameter. Fig. 1 illustrates the MSS process example with subjects of ADNI from three different time points: baseline, 6-and 12-months. We used different colors to denote different important features selected for different time slots. We initializeD by randomly selectingl samples from feature matrices across different time points { X 1 , \u00b7 \u00b7 \u00b7 , X T } to construct it. For the individual part of each dictionary, we randomly selectl subjects from the corresponding matrix X t to constructD t . After initializing dictionary D t for each time point, we set all the sparse code Z t to be zero at the beginning. The key steps of MSS are summarized in Algorithm 1.\nIn Algorithm 1, \u03a6 represents the shared part of each dictionary. Then, we learn the sparse code z k+1 t from Z t by several steps of Cyclic Coordinate Descent (CCD) [2] where "}, {"section_title": "EXPERIMENT", "text": "We used all available brain longitudinal imaging data from ADNI. The subject demography table is shown in Table 1 including AD, MCI and Cognitively Unimpaired (CU). FreeSurfer [7] is adopted to compute the cortical thickness by deforming the white matter surface to pial surface and then measuring the deformation distance as the cortical thickness. FreeSurfer also produces a spherical parameterizations for each pial surface, which is used by weighted spherical harmonics representation (WSHR) [3] to register pial surfaces across subjects. WSHR fixes the Gibbs phenomenon (ringing effects) associated with the traditional Fourier descriptors and spherical harmonic representation by weighting the series expansion with exponential weights [3] . The exponential weights make the representation converges faster and reduces the amount of wiggling. We used WSHR to help create consistent features across subjects."}, {"section_title": "Experiment setting", "text": "We set the columns of D t to be 1000; 500 for the shared part and 500 for the individual part. We test whether MSS provides a statistically powerful clinical score prediction solution on three groups of experiments, including (1) The whole ADNI dataset N = 2797 (Whole), (2) AD and MCI, (3) MCI and CU. We did four times 5-fold cross validation to estimate the prediction power and report the averaged results. For each set of experiments, we selected 5000 features along a sequence of \u03bb values equally spaced on the linear scale of from 0.1 to 1 and selected \u03ba = 10 epochs. We used output of MSS as regressors and 2 clinical scores (MMSE and ADAS-cog) as responses in the Lasso regression. For testing, we used baseline dictionary to predict 24-months clinical scores since this dictionary also has the common dictionary already including baseline, 6-month and 12-month information. We evaluated the overall regression performance measures by using Correlation Coefficient (CC) and root Mean Square Error (rMSE)as employed in [28] . The definitions of , and\n\u03c3y\u03c3\u0177 ,where y is the ground truth of target at a single time point and\u0177 is the corresponding prediction by a prediction model. cov(y,\u0177) is the covariance and \u03c3 y and \u03c3\u0177 are the standard deviations. We release our source code of MSS on Github (https://github.com/zj00377/ multi-task-sparse-screening).\nTo validate the effectiveness and accuracy of the proposed method, we consider three different conditions: 1) To show the validity of the feature screening strategy, we performed MSS without prior feature screening and we used MS-N to indicate that no feature screening was involved in. 2) Another concern is whether our MSS has better power to predict clinical decline or whether it has better performance than single task sparse coding (SSC). To this end, we compare the MSS with online dictionary learning [14] referred as SSC for single-task based sparse coding. 3) We also want to know whether our group Lasso screening produces powerful features. We used SSC-N to refer single-task sparse coding without features screening. Besides, we compared with four methods: two single-task regression methods including ridge regression (Ridge) [10] and Lasso and two multi-task methods including Multi-Task Learning (MTL) [6] and L 2,1 norm regularization with least square loss [1] . Table 2 shows regression performance on three different experiments, we marked the first and second rank of all sets of the experiments. Firstly, we can see the feature selection on the high-dimensional features before learning sparse codes is important because the MS-N and SSC-N have poorer performance compared to MSS and SSC. Secondly, for sparse coding and the dictionary learning method, our MSS and MS-N have better performance than SSC and SSC-N, which shows our multi-task dictionary learning is superior to single-task dictionary learning methods. Meanwhile, comparing MS-N to two single-task regression methods, Lasso and Ridge, our proposed method also performs better in term of correlation and rMSE. Thirdly, our proposed method has better performance than two multi-task regression methods MTL and L21, which may be because we selected the top-p features and used the common and individual features of different time points for different tasks."}, {"section_title": "Experimental Results", "text": "Moreover, our proposed MSS method consistently outperformed the competing methods for the cases of different pairs of clinical labels. In the ADNI study, CU individuals and stable MCI patients (i.e. nondemented subjects) are less likely to have significant changes on the cognitive scores. We apply our models to the subgroup that consists of AD and MCI, MCI and CU patients, respectively, and the results show that our MSS still achieved the best results among all methods tested. For MCI vs. CU group, our method showed the best CC of 0.80 on MMSE and 0.77 on ADAS-cog and best rMSE of 4.27 for MMSE and 6.52 on ADAS-cog. In AD vs. MCI group, our method achieved the best CC of 0.76 on MMSE and 0.81 on ADAS-cog and best rMSE of 3.13 on MMSE and 6.51 on ADAS-cog. We show the scatter plots for the predicted values versus the actual values for MMSE and ADAS-Cog on the whole dataset in Fig. 2 . We see the predicted values and actual clinical scores have a high correlation. Our experimental results validated the effectiveness of our MSS algorithm."}, {"section_title": "CONCLUSION AND FUTURE WORK", "text": "We propose a novel algorithm Multi-task Sparse Screening. Our extensive experimental results demonstrate that the proposed framework is more effective than seven other standard methods. In future work, we will investigate our algorithm on visualizing our selected cortical thickness features."}]