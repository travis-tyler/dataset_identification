[{"section_title": "Abstract", "text": "Disease progression modeling (DPM) using longitudinal data is a challenging machine learning task. Existing DPM algorithms neglect temporal dependencies among measurements, make parametric assumptions about biomarker trajectories, do not model multiple biomarkers jointly, and need an alignment of subjects' trajectories. In this paper, recurrent neural networks (RNNs) are utilized to address these issues. However, in many cases, longitudinal cohorts contain incomplete data, which hinders the application of standard RNNs and requires a pre-processing step such as imputation of the missing values. Instead, we propose a generalized training rule for the most widely used RNN architecture, long short-term memory (LSTM) networks, that can handle both missing predictor and target values. The proposed LSTM algorithm is applied to model the progression of Alzheimer's disease (AD) using six volumetric magnetic resonance imaging (MRI) biomarkers, i.e., volumes of ventricles, hippocampus, whole brain, fusiform, middle temporal gyrus, and entorhinal cortex, and it is compared to standard LSTM networks with data imputation and a parametric, regression-based DPM method. The results show that the proposed algorithm achieves a significantly lower mean absolute error (MAE) than the alternatives with p < 0.05 using Wilcoxon signed rank test in predicting values of almost all of the MRI biomarkers. Moreover, a linear discriminant analysis (LDA) classifier applied to the predicted biomarker values produces a significantly larger area under the receiver operating characteristic curve (AUC) of 0.90 vs. at most 0.84 with p < 0.001 using McNemar's test for clinical diagnosis of AD. Inspection of MAE curves as a function of the amount of missing data reveals that the proposed LSTM algorithm achieves the best performance up until more than 74% missing values. Finally, it is illustrated how the method can successfully be applied to data with varying time intervals. This paper shows that built-in handling of missing values in training an LSTM network benefits the application of RNNs in neurodegenerative disease progression modeling in longitudinal cohorts."}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD) is a chronic neurodegenerative disorder that begins with memory loss and develops over time, causing issues in conversation, orientation, and control of bodily functions (McKhann et al., 1984) . Early diagnosis of the disease is challenging and is usually made once cognitive impairment has already compromised daily living. Hence, developing robust, data-driven methods for disease progression modeling (DPM) utilizing longitudinal data is necessary to yield a complete perspective on the disease for better diagnosis, monitoring, and prognosis (Oxtoby and Alexander, 2017) .\nExisting longitudinal DPM methods model biomarkers as a function of disease progression using continuous curve fitting. In the AD progression modeling literature, a variety of regression-based methods have been proposed to fit logistic or polynomial functions to the longitudinal dynamic of each biomarker (Jedynak et al., 2012; Fjell et al., 2013; Oxtoby et al., 2014; Donohue et al., 2014; Yau et al., 2015; Guerrero et al., 2016) . However, parametric assumptions on the biomarker trajectories not only limit the flexibility of such methods but also lead to the necessity of aligning subjects' trajectories. In addition, the existing approaches mostly rely on independent biomarker modeling, and none of them consider the temporal dependencies among measurements.\nRecurrent neural networks (RNNs) are non-parametric sequence based learning methods that, by design, do not require alignment of subject trajectories. They offer continuous, joint modeling of longitudinal data while taking temporal dependencies among measurements into account (Pearlmutter, 1989) . Long short-term memory (LSTM) networks, the most widely used type of RNNs, developed to effectively capture long-term temporal dependencies by dealing with the exploding and vanishing gradient problem during backpropagation through time (Hochreiter and Schmidhuber, 1997; Gers et al., 1999; Gers and Schmidhuber, 2001 ). They employ a memory cell with nonlinear reset units -so called constant error carousels (CECs) -and learn to store history for either long or short time periods. Since their introduction, a variety of LSTM networks have been developed for different time-series applications (Greff et al., 2017) . The vanilla LSTM that utilizes three reset gates with full gate recurrence is the most commonly used LSTM architecture. It applies the backpropagation through time algorithm using full gradients to train the network and can include biases and cell-to-gates (peephole) connections.\nHowever, since longitudinal cohorts often contain missing biomarker values due to, for instance, dropped out patients, unsuccessful measurements, or different assessment patterns used for different subject groups -as seen in the Alzheimer's Disease Neuroimaging Initiative (ADNI) (Petersen et al., 2010) , standard RNNs inclunding LSTMs cannot be directly applied. Pre-processing methods such as data imputation and interpolation are the most common approaches to handling missing data in RNNs. These twostep procedures decouple missing data handling and network training, resulting in a sub-optimal performance that is heavily influenced by the choice of data pre-processing method (Lipton et al., 2016) . Although RNNs themselves have been used for estimating missing data (Parveen and Green, 2002; Yoon et al., 2018) , the lack of methods to inherently handle incomplete data in RNNs is evident (Che et al., 2018) . Other approaches update the architecture to learn or encode the missing data patterns (Che et al., 2018; Lipton et al., 2016) . These methods are typically biased towards specific cohort or demographic circumstances correlated with the learned missing data patterns and introduce additional parameters in the network which increases the complexity of the network.\nIn this paper, we propose a generalized method for training LSTM networks that can handle missing values in both input and target. This is achieved by applying the batch gradient descent algorithm in combination with the loss function and its gradients normalized by the number of missing values in input and target. Our goal is different than the approaches that encode the missing values' patterns (Che et al., 2018; Lipton et al., 2016) ; we want to train RNNs robust to missing values to more faithfully capture the true underlying signal and to make the learned model generalizable across cohorts. The proposed LSTM algorithm is applied to AD progression modeling in the ADNI cohort (Petersen et al., 2010) based on volumetric magnetic resonance imaging (MRI) biomarkers, and the estimated biomarker values are used to predict the clinical status of subjects. MRI is known to be the best noninvasive way to examine changes in the brain in vivo during the course of AD (Biagioni and Galvin, 2011; Wu et al., 2011) , and volumetric analysis is a widely used ROI-based method to estimate brain atrophy.\nThe main contribution is three-fold and can be summarized as follows:\n\u2022 First, a generalized formulation of backpropagation through time for LSTM networks is proposed to handle incomplete data, and it is shown that such built-in handling of missing values provides a better modeling and prediction performance compared to using data imputation with standard LSTM networks.\n\u2022 Second, temporal dependencies among measurements in the ADNI data are modeled using the proposed LSTM network via sequence-to-sequence learning."}, {"section_title": "Hidden", "text": ""}, {"section_title": "Input", "text": ""}, {"section_title": "Output", "text": "Figure 1: Illustration of how the normalization factors are related to the input and output of an unfolded RNN. Assume an RNN with three consecutive time points {t \u2212 1, t, t + 1}, three input nodes, four hidden nodes, and two output nodes. Missing data for an instance observation j is illustrated as black nodes. We wish to weight the loss function and its gradients according to the number of available points in the input and output nodes. In this specific example, subject j has only one measurement available for its n-th input node and the same many for its m-th output node. Hence, the loss function and its gradients are weighted by 1/3. Moreover, since there is a total of five measurements available in the input layer, the loss function is weighted by 5/9. The later weighting factor is to ensure that the loss function takes the number of available points in the input layer into account.\nTo the best of our knowledge, this is the first time such multi-dimensional sequence learning methods are applied to neurodegenerative DPM.\n\u2022 Third, an end-to-end approach, without need for trajectory alignment, is proposed for modeling the longitudinal dynamics of imaging biomarkers and for clinical status prediction. This is a practical way of implementing a robust DPM for both research and clinical applications.\nA preliminary version of this work appeared in proceedings of the International Conference on Medical Imaging with Deep Learning (Mehdipour Ghazi et al., 2018) . The present study contains a more detailed presentation and additional experiments to investigate statistical significance, robustness as a function of amount of missing data, and situations with varying time steps."}, {"section_title": "Proposed LSTM algorithm", "text": "The main goal of this study is to minimize the influence of missing values on the learned LSTM network parameters. This is achieved by using the batch gradient descend method in combination with the backpropagation through time algorithm modified to take into account missing values in the input and target vectors. More specifically, the algorithm sets input missing values to zero, backpropagates zero errors corresponding to the target missing points, and uses an L2-norm loss function with residuals weighted according to the number of available time points per target biomarker node (\u03b2 j m ) and according to the total number of available input values for all visits of all biomarkers (\u03b2 j x ). In addition, it normalizes input weight gradients of the loss function according to the number of available time points per input biomarker node (\u03b2 j n ). Figure 1 provides an illustration of how the normalization factors are related to the input and output of an unfolded RNN. Note that the use of batch gradient descend ensures the availability of at least one data point per biomarker that can proportionally contribute in the weight update rule. Figure 2 shows a typical schematic of a vanilla LSTM architecture. As can be seen, the topology includes a memory cell, an input modulation gate, and three nonlinear reset gates, namely input gate, forget gate, and output gate, each of which accepting current and recurrent inputs. The memory cell learns to maintain its state over time while the multiplicative gates learn to open and close access to the constant error/information flow, to prevent exploding or vanishing gradients. The input gate protects the memory contents from perturbation by irrelevant inputs, and the output gate protects other units from perturbation by currently irrelevant memory contents. The forget gate deals with continual or very long input sequences, and finally, peephole connections allow the gates to access the CEC of the same cell state."}, {"section_title": "The basic LSTM architecture", "text": ""}, {"section_title": "Feedforward in LSTM networks", "text": "Assume x t j \u2208 R N \u00d71 is the j-th observation of an Ndimensional input vector at current time t. If M is the number of output units, feedforward calculations of the LSTM network under study can be summarized as\nare j-th observation of forget gate, input gate, modulation gate, cell state, output gate, and hidden output at time t before and after activation, respectively. Moreover,\nare sets of connecting weights from current and recurrent inputs to the gates and cell, respectively, {V f , V i , V o } \u2208 R M \u00d71 is the set of peephole connections from the cell to the gates,\nrepresents corresponding biases of neurons, and denotes element-wise multiplication. Finally, \u03c3 g , \u03c3 c , and \u03c3 h are nonlinear activation functions assigned for the gates, input modulation, and hidden output, respectively. Logistic sigmoid functions are applied to the gates with range [0, 1] while hyperbolic tangent functions are applied to modulate both cell input and hidden output with range [\u22121, 1]. Hence, the measurements need to be in the same range [\u22121, 1]."}, {"section_title": "Robust backpropagation through time", "text": "Let L \u2208 R M \u00d71 be the loss function defined based on the actual target s and network output y. Here, we consider one layer of LSTM units for sequence learning which means that the network output is the hidden output. The main idea is to calculate the partial derivatives of the normalized loss function (\u03b4) with respect to the weights using the chain rule.\nwhere\nare normalization factors to handle missing values of the j-th observation with batch size J and sequence length T . Also, |x j | and |y j (m)| denote the total number of available input values and the number of available target time points in the mth node, respectively. The backpropagation calculations through time using full gradients can be obtained as\nFinally, if \u03b8 \u2208 {f, i, z, o} and \u03c6 \u2208 {f, i}, the gradients of the loss function with respect to the weights are calculated as\nis the normalization factor handling missing input values and |x j (n)| is the number of available time points in the input's n-th node. Here, we use a fixed sequence length of T to proportionally consider subjects based on their available visits. However, the robust backpropagation algorithm can easily be generalized for a dynamic sequence length."}, {"section_title": "Momentum batch gradient descent", "text": "As an efficient iterative algorithm, momentum batch gradient descent is applied to find the local minimum of the loss function calculated over a batch while speeding up the convergence. The update rule using L2 regularization can be written as\nwhere \u03d1 is the weight update initialized to zero, \u03c9 is the to-be-updated weight array, \u03b4\u03c9 is the gradient of the loss function with respect to \u03c9, and \u03b1, \u03b3, and \u00b5 are the learning rate, weight decay or regularization factor, and momentum weight, respectively."}, {"section_title": "Experiments", "text": ""}, {"section_title": "Data", "text": "Data used in the preparation of this article is obtained from the ADNI database. The ADNI was launched in 2003 as a public-private partnership, led by principal investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging, positron emission tomography, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment and early Alzheimer's disease. To be more specific, we use The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE) challenge dataset (Marinescu et al., 2018) which is composed of data from the three ADNI phases ADNI 1, ADNI GO, and ADNI 2. This includes roughly 1,500 biomarkers acquired from 1,737 subjects (957 males and 780 females) during 12,741 visits at 22 distinct time points between 2003 and 2017. Table 1 summarizes statistics of the demographics in the TADPOLE dataset. Note that the subjects include missing values and clinical status during their visits.\nIn this work, we have merged existing groups labeled as cognitively normal (CN), significant memory concern (SMC), and normal (NL) under CN, mild cognitive impairment (MCI), early MCI (EMCI), and late MCI (LMCI) under MCI, and Alzheimer's disease (AD) and dementia under AD. Moreover, groups with labels converting from one status to another, e.g. MCI-to-AD, belong to the next status (AD in this example).\nMRI biomarkers are used for AD progression modeling. This includes T1-weighted brain MRI volumes of ventricles, hippocampus, whole brain, fusiform, middle temporal gyrus, and entorhinal cortex. We normalize the MRI measurements by the corresponding intracranial volume (ICV). Next, we filter within-class outliers of each biomarkeracross all subjects and their visits -by assuming them as missing values and normalize the measurements by scaling them linearly to [\u22121, 1]. Out of 22 visits, we initially select 11 regular visits with a fixed interval of one year including baseline. Finally, subjects with less than three distinct visits for any biomarker are removed to obtain 742 subjects. This is to ensure that at least two visits are available per biomarker for performing sequence learning through the feedforward step and an additional visit for backpropagation.\nFor evaluation purpose, we partition the entire dataset to three non-overlapping subsets for training, validation, and testing. To achieve this, we randomly select 10% of the within-class subjects for validation and the same for testing. More specifically, we randomly pick subjects based on their baseline labels while ensuring that subjects with few and large number of visits are included in each subset. This process results in 592, 76, and 74 subjects for training, validations, and testing, respectively. Details on the amount of available visits in the obtained evaluation subsets are shown in Table 2 . As can be deduced from the "}, {"section_title": "Evaluation metrics and statistical tests", "text": "Mean absolute error (MAE) and multi-class area under the receiver operating characteristic (ROC) curve (AUC) are used to assess the performance of modeling and classification, respectively. MAE measures accuracy of continuous prediction per biomarker by computing the absolute difference between actual and estimated values as follows\nwhere s t j and y t j are the ground-truth and estimated values of the specific biomarker for the j-th subject at the t-th visit, respectively, and I is the number of available points in the target array s.\nMulti-class AUC (Hand and Till, 2001 ) is a measure to examine the diagnostic performance in a multi-class test set using ROC analysis. It is calculated using the posterior probabilities as follows\nwhere n c is the number of distinct classes, n i denotes the number of available points belonging to the i-th class, and SR i is the sum of the ranks of posteriors p(c i |s i ) after sorting all concatenated posteriors {p(c i |s i ), p(c i |s k )} in an ascending order, where s i and s k are vectors of scores belonging to the true classes c i and c k , respectively. The modeling performance is statistically assessed for different methods using the paired, two-sided Wilcoxon signed rank test (Wilcoxon, 1945) applied to the obtained absolute errors. Also, classification performance is analyzed using McNemar's test (McNemar, 1947) applied to the hard classification results (clinical status) obtained from a linear discriminant analysis (LDA) classifier with predicted MRI measurements as input."}, {"section_title": "Experimental setup", "text": "The following methods are evaluated in our conducted experiments:\n\u2022 LSTM-Robust: an LSTM network trained based on the proposed robust backpropagation through time algorithm by setting input missing values to zero and backpropagating zero errors corresponding to the target missing points while training.\n\u2022 LSTM-Mean: an LSTM network trained using the standard backpropagation through time algorithm with missing values imputed based on mean imputation method prior to training (Che et al., 2018 ).\n\u2022 LSTM-Forward: an LSTM network trained using the standard backpropagation through time algorithm with missing values imputed based on forward imputation method prior to training (Lipton et al., 2016) .\n\u2022 Regression-Based: a parametric, regression-based method (Jedynak et al., 2012 ) that automatically handles missing values. The parameters of the algorithm are initially estimated using linear regression in 15 iterations and are optimized using sigmoidal functions in 35 additional iterations where all parameters converge.\nAll the methods are developed in MATLAB R2017b and run on a 2.80 GHz CPU with 16 GB RAM. We initialize the LSTM networks' weights by generating uniformly distributed random values in range [\u22120.05, 0.05] and set the weights' updates and weights' gradients to zero. The batch size is set to the number of available training subjects, and the first ten visits are used to estimate the second to eleventh visits per subject for evaluation purpose. It should be noted that when data imputation is applied, the robust backpropagation formulas simply generalize to the ones for the standard LSTM network.\nWe utilize the validation set to tune all the networks' optimization parameters, each time by adjusting one of the [3, 7] parameters while keeping the rest at fixed values to achieve the lowest average MAE. Peephole connections are used in the networks since they tend to improve the performance (Greff et al., 2017) . Based on these strategies, the optimal parameters are obtained as \u03b1 = 0.1, \u00b5 = 0.9, and \u03b3 = 0.0001 with 1,000 epochs. The corresponding MAEs for the validation set are also calculated as 0.00296, 0.00025, 0.01494, 0.00024, 0.00076, and 0.00097, for ventricles, hippocampus, whole brain, entorhinal cortex, fusiform, and middle temporal gyrus, respectively. It takes about 340 seconds to train the network and 0.025 seconds to estimate all the validation measurements. It is worthwhile mentioning that all the estimated measurements are linearly scaled from [\u22121, 1] to the original range of biomarkers using the original minimum and maximum values while calculating MAEs."}, {"section_title": "Results and discussion", "text": "After successfully training the LSTM networks and the regression-based method for DPM, they are all evaluated using the test set. Table 3 compares the test MRI biomarker modeling performance (MAE) using aforementioned methods. Even though the performance is reported per biomarker, the models are jointly fitted to all biomarkers. As it can be deduced from Table 3 , LSTM-Robust significantly outperforms the other methods in all MRI biomarkers except for whole brain where the regression-based approach performs significantly better and for middel temporal gyrus where there is no difference between the proposed method and LSTM-Forward."}, {"section_title": "Biomarker modeling", "text": ""}, {"section_title": "Predicting clinical status", "text": "To assess the ability of the estimated measurements in predicting the clinical status, we train an LDA classifier using the estimated training measurements and apply it to the estimated test data to compute the posterior probabilities. The obtained scores are then used to calculate diagnostic AUCs. The diagnostic prediction results for the test set are shown in Table 4 . As can be seen, LSTMRobust outperforms all other methods in predicting clinical status of subjects per visit with a multi-class AUC of 0.76, which reveals the effect of modeling on classification performance. One could of course use other classifiers or train the LSTM network directly for classification based on sequence-to-label learning to potentially improve the diagnostic AUCs. However, the focus of this work is on DPM based on sequence-to-sequence learning. In addition, sequence-to-label learning would only be able to utilize the part of the training data which has available clinical status.\nThe multi-class AUC of 0.76 obtained using predicted measurements from the proposed approach is within the top-five AUCs of the state-of-the-art, cross-sectional MRIbased classification results of the recent challenge on Computer-Aided Diagnosis of Dementia (CADDementia) (Bron et al., 2015) that ranged from 0.75 to 0.79. It should, however, be noted that there are important differences between this study and the CADDementia challenge. Firstly, this work has the advantage of training and testing data from the same cohort whereas CADDementia algorithms were applied to classify data from independent cohorts. "}, {"section_title": "Robustness as a function of amount of missing data", "text": "To evaluate the modeling robustness of the proposed method compared to the alternatives for different amounts of missing data, we construct subsamples of the training dataset by randomly removing up to 50% of the actual data per biomarker and train the methods on the smaller datasets. Figure 3 illustrates the modeling performance of the different methods on various amounts of missing measurements, from 0% to 50%. It is important to note that the training data already includes a large number of missing values at missing rate of 0% -i.e. 63% of actual data as seen on Table 2 . For better comparison, we take the average of MAEs normalized by the range of corresponding biomarkers to obtain a single curve per method. As can be seen, the result of the proposed method is superior to those of the benchmarks up until missing around 74% of the data. For higher rates of missing data, basic LSTM with forward imputation outperforms all other methods. One reason for why LSTM with forward imputation is robust to the higher rates of missing data could be due to the fact that it replaces the missing values placed at the beginning of a sequence with the whole training data median."}, {"section_title": "Irregular time intervals", "text": "As final experiment, we assess generalizability of the proposed method for predicting measurements of irregular visits. In general, standard LSTM networks are designed to handle evenly spaced sequences. We used the same approach in our baseline experiments for AD progression modeling application by disregarding visiting months 3, 6 and 18, and confined the experiments to yearly follow-up in the ADNI data. Now, we employ the available measurements of the 6-th and 18-th visiting months from the TADPOLE dataset and predict biomarker values of halfyearly follow-ups by assuming unavailable visits as missing data. In this experiment, 78% of the actual data is missing. We apply the same methods to the extended data. Table 5 details the test modeling performance of the MRI biomarkers for half-yearly predictions using the different DPM methods. As can be seen, our proposed DPM method outperforms all other methods in all categories. More interestingly, considering the corresponding results from Table  3 for yearly predictions, one can deduce that the modeling performance of the proposed method improves by utilizing the irregular visits. However, the additional time points in the LSTM increases the required time for training and validation to 1,090 seconds and 0.061 seconds, respectively. As an alternative, one could utilize modified LSTM architectures where the networks learn a number of parameters to encode visiting patterns among longitudinal patient records (Baytas et al., 2017; Neil et al., 2016) . However, using such methods not only increase the complexity of the network but also risk learning any time spacing patterns in the data."}, {"section_title": "Conclusions", "text": "In this paper, a training algorithm was proposed for LSTM networks aiming to improve robustness against missing data, and the robustly trained LSTM network was applied to AD progression modeling using longitudinal measurements of MRI biomarkers. To the best of our knowledge, this is the first time RNNs have been studied and applied to DPM within neurodegenerative disease. Moreover, since RNNs are non-parametric learning methods, the proposed approach can be applied to different timeseries data and characteristics than the monotonic behavior that one typically encounters in MRI-based neurodegenerative disease progression modeling. The proposed training method demonstrated better performance than using imputation prior to standard LSTM network training and outperformed an established parametric, regression-based DPM method in terms of both biomarker prediction and subsequent diagnostic classification. This method is also applicable for other types of RNNs such as gated recurrent units (GRUs) (Cho et al., 2014) . This study highlights the potential of RNNs for modeling the progression of AD using longitudinal measurements, provided that proper care is taken to handle missing values and time intervals. "}]