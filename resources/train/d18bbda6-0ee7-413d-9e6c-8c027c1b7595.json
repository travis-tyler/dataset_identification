[{"section_title": "Abstract", "text": "We study the problem of modeling species geographic distributions, a critical problem in conservation biology. We propose the use of maximum-entropy techniques for this problem, specifically, sequential-update algorithms that can handle a very large number of features. We describe experiments comparing maxent with a standard distribution-modeling tool, called GARP, on a dataset containing observation data for North American breeding birds. We also study how well maxent performs as a function of the number of training examples and training time, analyze the use of regularization to avoid overfitting when the number of examples is small, and explore the interpretability of models constructed using maxent."}, {"section_title": "Introduction", "text": "We study the problem of modeling the geographic distribution of a given animal or plant species. This is a critical problem in conservation biology: to save a threatened species, one first needs to know where the species prefers to live, and what its requirements are for survival, i.e., its ecological niche (Hutchinson, 1957) .\nThe data available for this problem typically consists of a list of georeferenced occurrence localities, i.e., a set of geographic coordinates where the species has been observed. In addition, there is data on a number of environmental variables, such as average temperature, average rainfall, elevation, etc., which have been measured or estimated across a geographic region of interest. The goal is to predict which areas within the region satisfy the requirements of the species' ecological niche, and thus form part of the species' potential distribution (Anderson & Mart\u00ednez-Meyer, 2004) . The potential distribution describes where conditions are suitable for survival of the species, and is thus of great importance for conservation. It can also be used to estimate the species' realized distribution, for example by removing areas where the species is known to be absent because of deforestation or other habitat destruction. Although a species' realized distribution may exhibit some spatial correlation, the potential distribution does not, so considering spatial correlation is not necessarily desirable during species distribution modeling.\nIt is often the case that only presence data is available indicating the occurrence of the species. Natural history museum and herbarium collections constitute the richest source of occurrence localities (Ponder et al., 2001; Stockwell & Peterson, 2002) . Their collections typically have no information about the failure to observe the species at any given location; in addition, many locations have not been surveyed. In the lingo of machine learning, this means that we have only positive examples and no negative examples from which to learn. Moreover, the number of sightings (training examples) will often be very small by machinelearning standards, say a hundred or less. Thus, the first contribution of this paper is the introduction of a scientifically important problem as a challenging domain for study by the machine learning community.\nTo address this problem, we propose the application of maximum-entropy (maxent) techniques which have been so effective in other domains, such as natural language processing (Berger et al., 1996) . Briefly, in maxent, one is given a set of samples from a distribution over some space, as well as a set of features (real-valued functions) on this space. The idea of maxent is to estimate the target distribution by finding the distribution of maximum entropy (i.e., that is closest to uniform) subject to the constraint that the expected value of each feature under this estimated distribution matches its empirical average. This turns out to be equivalent, under convex duality, to finding the maximum likelihood Gibbs distribution (i.e., distribution that is exponential in a linear combination of the features). For species distribution modeling, the occurrence localities of the species serve as the sample points, the geographical region of interest is the space on which this distribution is defined, and the features are the environmental variables (or functions thereof). See Figure 1 for an example.\nIn Section 2, we describe the basics of maxent in greater detail. Iterative scaling and its variants (Darroch & Ratcliff, 1972; Della Pietra et al., 1997) are standard algorithms for computing the maximum entropy distribution. We use our own variant which iteratively updates the weights on Figure 1 . Left to right: Yellow-throated Vireo training localities from the first random partition, an example environmental variable (annual average temperature, higher values in red), maxent prediction using linear, quadratic and product features, and GARP prediction. Prediction strength is shown as white (weakest) to red (strongest); reds could be interpreted as suitable conditions for the species.\nfeatures sequentially (one by one) rather than in parallel (all at once), along the lines of Collins, Schapire and Singer (2002) . This sequential approach is analogous to AdaBoost which modifies the weight of a single \"feature\" (usually called a base or weak classifier in that context) on each round. As in boosting, this approach allows us to use very large feature spaces.\nOne would intuitively expect an oversize feature space to be a problem for generalization since it increases the possibility of overfitting, leading others to use feature selection for maxent (Berger et al., 1996) . We instead use a regularization approach, introduced in a companion theoretical paper (Dud\u00edk et al., 2004) , which allows one to prove bounds on the performance of maxent using finite data, even when the number of features is very large or even uncountably infinite. Here we investigate in detail the practical efficacy of the technique for species distribution modeling. We also describe a numerical acceleration method that speeds up learning.\nIn Section 3, we describe an extensive set of experiments we conducted comparing maxent to a widely used existing distribution modeling algorithm; results of the experiments are described in Section 4. Quite a number of approaches have been suggested for species distribution modeling including neural nets, genetic algorithms, generalized linear models, generalized additive models, bioclimatic envelopes and more; see Elith (2002) for a comparison. From these, we selected the Genetic Algorithm for Ruleset Prediction (GARP) (Stockwell & Noble, 1992; Stockwell & Peters, 1999) , because it has seen widespread recent use to study diverse topics such as global warming (Thomas et al., 2004) , infectious diseases (Peterson & Shaw, 2003) and invasive species ; many further applications are cited in these references. GARP was also selected because it is one of the few methods available that does not require absence data (negative examples).\nWe compare GARP and maxent using data derived from the North American Breeding Bird Survey (BBS) (Sauer et al., 2001) , an extensive dataset consisting of thousands of occurrence localities for North American birds and used previously for species distribution modeling, in particular for evaluating GARP (Peterson, 2001) . The comparison suggests that maxent methods hold great promise for species distribution modeling, often achieving substantially superior performance in controlled experiments relative to GARP. In addition to comparisons with GARP, we performed experiments testing: (1) the performance of maxent as a function of the number of sample points available, so as to determine the all important question of how much data is enough; (2) the effectiveness of regularization to avoid overfitting on small sample sizes; and (3) the effectiveness of our numerical acceleration methods.\nLastly, it is desirable for a species distribution model to allow interpretation to deduce the most important limiting factors for the species. A noted limitation of GARP is the difficulty of interpreting its models (Elith, 2002) . We show how the models generated by maxent can be put into a form that is easily understandable and interpretable by humans."}, {"section_title": "The Maximum Entropy Approach", "text": "In this section, we describe our approach to modeling species distributions. As explained above, we are given a space X representing some geographic region of interest. Typically, X is a set of discrete grid cells; here we only assume that X is finite. We also are given a set of points x 1 , . . . , x m in X, each representing a locality where the species has been observed and recorded. Finally, we are provided with a set of environmental variables defined on X, such as precipitation, elevation, etc.\nGiven these ingredients, our goal is to estimate the range of the given species. In this paper, we formalize this rather vague goal within a probabilistic framework. Although this will inevitably involve simplifying assumptions, what we gain will be a language for defining the problem with mathematical precision as well as a sensible approach for applying machine learning.\nUnlike others who have studied this problem, we adopt the view that the localities x 1 , . . . , x m were selected independently from X according to some unknown probability distribution \u03c0, and that our goal is to estimate \u03c0. At the foundation of our approach is the premise that the distribution \u03c0 (or a thresholded version of it) coincides with the biologists' concept of the species' potential distribution. Superficially, this is not unreasonable, although it does ignore the fact that some localities are more likely to have been visited than others. The distribution \u03c0 may therefore exhibit sampling bias, and will be weighted towards areas and environmental conditions that have been better sampled, for example because they are more accessible.\nThat being said, the problem becomes one of density estimation: given x 1 , . . . , x m chosen independently from some unknown distribution \u03c0, we must construct a distribution\u03c0 that approximates \u03c0. In constructing\u03c0, we also make use of a given set of features f 1 , . . . , f n where f j : X \u2192 R. These features might consist of the raw environmental variables, or they might be higher level features derived from them (see Section 3.3). Let f denote the vector of all n features.\nIn general,\u03c0 may be quite distant, under any reasonable measure, from \u03c0. On the other hand, for a given function f , we do expect\u03c0[f ], the empirical average of f , to be rather close to its true\nIt is natural, therefore, to seek an approximation\u03c0 under which f j 's expectation is equal (or at least very close) to\u03c0[f j ] for every f j . There will typically be many distributions satisfying these constraints. The maximum entropy principle suggests that, from among all distributions satisfying these constraints, we choose the one of maximum entropy, i.e., the one that is closest to uniform. Here, as usual, the entropy of a distribution p on X is defined to be\nThus, the idea is to estimate \u03c0 by the distribution\u03c0 of maximum entropy subject to the condition that\u03c0[f j ] =\u03c0[f j ] for all features f j . Alternatively, we can consider all Gibbs distributions of the form q (x) = e \u00b7f (x) /Z where Z = x\u2208X e \u00b7f (x) is a normalizing constant, and \u03bb \u2208 R n . Then, following Della Pietra, Della Pietra and Lafferty (1997), it can be proved that the maxent distribution described above is the same as the maximum likelihood Gibbs distribution, i.e., the distribution q that minimizes RE(\u03c0 q ) where\nNote that the negative log likelihood\u03c0[\u2212 ln(q )] (also called log loss) only differs from RE(\u03c0 q ) by the constant H(\u03c0); we therefore use the two interchangeably as objective functions."}, {"section_title": "A sequential-update algorithm", "text": "There are a number of algorithms for finding the maxent distribution, especially iterative scaling and its variants (Darroch & Ratcliff, 1972; Della Pietra et al., 1997) as well as the gradient and second-order descent methods (Malouf, 2002; Salakhutdinov et al., 2003) . In this paper, we used a sequential-update algorithm that modifies one weight \u03bb j at a time, as explored by Collins, Schapire and Singer (2002) in a similar setting. We chose this coordinate-wise descent procedure since it is easily applicable when the number of features is very large (or infinite).\nSpecifically, our very simple algorithm works as follows. Assume without loss of generality that each feature f j is bounded in [0, 1]. On each of a sequence of rounds, we choose the feature f j to update for which\nis maximized, where \u03bb is the current weight vector (and where RE(p q), for p, q \u2208 R, is binary relative entropy). We next update \u03bb j \u2190 \u03bb j + \u03b1 where\nThe output distribution\u03c0 is the one defined by the computed weights, i.e., q . Essentially, this algorithm works by altering one weight \u03bb j at a time so as to greedily maximize the likelihood (or an approximation thereof). This procedure is guaranteed to converge to the optimal maximum entropy distribution. The derivation of this algorithm, along with its proof of convergence are given in a companion paper (Dud\u00edk et al., 2004) and are based on techniques explained by Della Pietra, Della Pietra and Lafferty (1997) as well as Collins, Schapire and Singer (2002) .\nTo accelerate convergence, we do a line search in each iteration: evaluate the log loss when \u03bb j is incremented by 2 i \u03b1 for i = 0, 1, . . . in turn, and choose the last i before the log loss decreases. This is similar to line search methods described in (Minka, 2001 )."}, {"section_title": "Regularization", "text": "The basic approach described above computes the maximum entropy distribution\u03c0 for which\u03c0[\nHowever, we do not expect\u03c0[f j ] to be equal to \u03c0[f j ] but only close to it. Therefore, in keeping with our motivation, we can soften these constraints to have the form\nwhere \u03b2 j is an estimate of how close\u03c0[f j ], being an empirical average, must be to its true expectation \u03c0[f j ]. Maximizing entropy subject to Eq. (2) turns out to be equivalent to finding the Gibbs distribution\u03c0 = q which minimizes\nIn other words, this approach is equivalent to maximizing the likelihood of the sought after Gibbs distribution with (weighted) 1 -regularization. This form of regularization also makes sense because the number of training examples needed to approximate the \"best\" Gibbs distribution can be bounded when the 1 -norm of the weight vector \u03bb is bounded. (See (Dud\u00edk et al., 2004) for details.) In a Bayesian framework, Eq. (3) corresponds to a negative log posterior given a Laplace prior. Other priors studied for maxent are Gaussian (Chen & Rosenfeld, 2000) and exponential (Goodman, 2003) . Laplace priors have been studied in the context of neural networks by Williams (1995) . The regularized formulation can be solved using a simple modification of the above algorithm. On each round, a feature f j and value \u03b1 are chosen so as to maximize the change in (an approximation of) the regularized objective function in Eq. (3). This works out to be Table 3 . AUC values averaged over 10 random partitions of occurrence localities. Predictions analyzed are as in Table 2 .\nThe maximizing \u03b1, must be either \u2212\u03bb j or Eq.\n(\n. Thus, the best \u03b1 (for a given f j ) can be computed by trying all three possibilities. Once f j and \u03b1 have been selected, we only need update \u03bb j \u2190 \u03bb j + \u03b1. As before, this algorithm can be proved to converge to a solution to the problem described above. Throughout our study we reduced the \u03b2 j to a single regularization parameter \u03b2 as follows. We expect"}, {"section_title": "Experimental Methods", "text": ""}, {"section_title": "The Breeding Bird Survey", "text": "The North American Breeding Bird Survey (Sauer et al., 2001 ) is a data set with a large amount of highquality location data. It is good for a first evaluation of maxent for species distribution modeling, as the generous quantities of data allow for detailed experiments and statistical analyses. It has also been used to demonstrate the utility of GARP (Peterson, 2001) . Roadside surveys are conducted on standard routes during the peak of the nesting season. Each route consists of fifty stops located at 0.5 mile intervals. A three-minute count is conducted at each stop, during which the observer records all birds heard or seen within 0.25 mile of the stop. Data from all fifty stops are combined to obtain the set of species observed on the route. There are 4161 routes within the region covered by the environmental coverages described below."}, {"section_title": "Environmental Variables", "text": "The environmental variables (coverages) use a North American grid with 0.2 degree square cells, and are all included with the GARP distribution, available at http://www.lifemapper.org/desktopgarp. Some coverages are derived from weather station readings during the period 1961 to 1990 (New et al., 1999) . Out of these we use annual precipitation, number of wet days, average daily temperature and temperature range. The remaining coverages are derived from a digital elevation model for North America, and consist of elevation, aspect and slope. Each coverage is defined over a 386 \u00d7 286 grid, of which 58,065 points have data for all coverages."}, {"section_title": "Experimental Design", "text": "We chose 12 out of the 421 species included in the Breeding Bird Survey to model, and considered a route to be an occurrence locality for a species if it had a presence record for any year of the survey. The chosen species and the number of routes where each has occurrence localities are shown in Table 1 . The occurrence data was divided into ten random partitions: in each partition, 50% of the occurrence localities were randomly selected for the training set, while the remaining 50% were set aside for testing.\nWe chose four feature types for maxent to use: the raw environmental variables (linear features); squares of environmental variables (quadratic features); products of pairs of environmental variables (product features); and binary features derived by thresholding environmental variables (threshold features). The latter features are equal to one if an environmental variable is above some threshold, and zero otherwise. Use of linear features constrains the mean of the environmental variables in the maxent distribution, linear plus quadratic features constrain the variance, while linear plus quadratic plus product features constrain the covariance of pairs of environmental variables.\nOn each training set, we ran maxent with four different subsets of the feature types: linear (L); linear and quadratic (LQ); linear, quadratic and product (LQP); and threshold (T). We also ran GARP on each training set.\nThe output of GARP and maxent have quite different interpretations. Nevertheless, each can be used to (partially) rank all locations according to their habitability. To compare these rankings, we used receiver operating characteristic (ROC) curves. For each of the runs, we calculated the AUC (area under the ROC curve), and determined the average AUC over the ten occurrence data partitions. See Section 3.5 for further discussion of this metric.\nThe AUC comparison is somewhat biased in maxent's favor, as a continuous prediction will typically have a higher AUC than a discrete prediction. We therefore do a second comparison, where we select operating thresholds for GARP that have been widely used in practice, and compare the algorithms only at those operating points. We call this an \"equalized area test\", and the details are as follows. We applied two thresholds to each GARP prediction, namely 1 and 10, corresponding to at least one, or all, best-subset models predicting presence (see Section 3.4 for GARP details). These are the most-often used GARP thresholds (Anderson & Mart\u00ednez-Meyer, 2004) . For each of the two resulting predictions, we set thresholds for the maxent models that result in prediction of the same area (geographic extent) as GARP. The predictions, now binary and with the same predicted area, are then simply compared using omission rates (fraction of test localities not predicted present). Again, averages were taken over the 10 random partitions of the occurrence data.\nMost applications of species distribution modeling have much less data available than for North American birds. Indeed, species of conservation importance may have extremely few georeferenced locality records, often fewer than 10. To investigate the use of maxent in such limited data settings, we perform experiments using limited subsets of the Breeding Bird data. We selected increasing subsets of training data in each partition, ran all four versions of maxent, and took an average AUC over ten partitions.\nIn order to determine sensitivity of maxent to the value of \u03b2 and its interaction with sample size, we varied \u03b2 and the number of training examples and took an average AUC over ten partitions for all four versions of maxent. Lastly, to measure the effect of our acceleration method, we performed runs using the first random partition for the Loggerhead Shrike and the Yellow-throated Vireo, both with and without line search for \u03b1 (as described in Section 2), and measured the log loss on both training and test data as a function of running time."}, {"section_title": "Algorithm implementations", "text": "For the maxent runs, we ran the iterative algorithm described in Section 2 for 500 rounds, or until the change in the objective function on a single round fell below 10 \u22125 . For the regularization parameter \u03b2, to avoid overfitting the test data, we used the same setting of 0.1 for all feature types, except threshold features for which we used 1.0. In Section 4.4, we describe experiments showing how sensitive our results are to the choice of \u03b2.\nTo reduce the variability inherent in GARP's random search procedure, we made composite GARP predictions using the \"best-subsets\" procedure (Anderson et al., 2003) , as was done in recent applications Raxworthy et al., 2004) . We generated 100 binary models, using GARP version 1.1.3 with default parameter values, then eliminated models with more than 5% intrinsic omission (negative prediction of training localities). If at most 10 models remained, they then constituted the best subset; otherwise, we selected the 10 models whose predicted area was closest to the median of the remaining models. The composite prediction gives the number of best-subset models in which each point is predicted suitable (0-10). For Cassin's Vireo, the best subset was empty for most random partitions of occurrence localities, so we increased the intrinsic omission threshold to 10% for that species."}, {"section_title": "ROC curves", "text": "An ROC curve shows the performance of a classifier whose output depends on a threshold parameter. It plots true positive rate against false positive rate for each threshold. A point (x, y) indicates that for some threshold, the classifier classifies a fraction x of negative examples as positive, and a fraction y of positive examples as positive. The curve is obtained by \"joining the dots\".\nThe area under an ROC curve (AUC) has a natural statistical interpretation. Pick a random positive example and a random negative example. The area under the curve is the probability that the classifier correctly orders the two points (with random ordering in the case of ties). A perfect classifier therefore has an AUC of 1. However, to use ROC curves with presence-only data, we must interpret as \"negative examples\" all grid cells with no occurrence localities, even if they support good environmental conditions for the species. The maximum AUC is therefore less than one (Wiley et al., 2003) , and is smaller for wider-ranging species."}, {"section_title": "Results", "text": ""}, {"section_title": "Equalized Area Test", "text": "The results of the equalized area test are in very low average omission (with the exception of GARP on Cassin's Vireo). A threshold of 10 causes less overprediction, and reveals more differences between the algorithms. The best results are obtained by maxent with two of the feature sets (LQP and T). These two are superior to GARP on all species, often very substantially; LQ is superior to GARP for all species but BhV. Table 3 shows the AUC for each species, averaged over the 10 random partitions of the occurrence localities. Example ROC curves used in computing the averages can be seen in Figure 2 , which shows the performance of the algorithms on the first random partition for the Loggerhead Shrike and Yellow-throated Vireo."}, {"section_title": "ROC analysis", "text": "The AUC for maxent improves dramatically going from linear (L) to linear plus quadratic (LQ) features, with a small further improvement when product features are added (LQP). The AUC for threshold features (T) is similar to LQP. For all species, the AUC for GARP is lower than for all maxent feature sets except sometimes L. Note that GARP is disadvantaged in AUC comparisons by not distinguishing between points in its highest rank (those points predicted present in all best-subset models), as can be seen in Figure 2 , where GARP loses area at the left end of the ROC curve. Nevertheless, wherever GARP has data points, maxent with the better feature sets is quite consistently as good as or better than GARP. Figure 3 shows the AUC averaged over 10 partitions for an increasing number of training examples on eight of the species. We also include GARP results for full training sets as a base line. As expected, models with a larger number of features tend to overfit small training sets, but they give more accurate predictions for large training sets."}, {"section_title": "Learning Curve Experiments", "text": "Linear models do not capture species distribution very well and are included only for completeness. With the exception of the Plumbeous Vireo, three remaining versions of maxent outperform L models already for the smallest training sets. LQP models become better than LQ for 30-40 training examples; their performance, however, matches that of LQ already for smaller training sets. T models perform worse than both LQ and LQP for small training sets, but they slightly outperform LQP once training sets reach 400 examples. Learning curves for species with large numbers of examples indicate that for both LQ and LQP about 50-100 examples suffice for a prediction that is close to optimal for those models. Figure 4 shows the sensitivity of maxent to the regularization value \u03b2 for LQP and T versions of maxent. Due to the lack of space we do not present results for L and LQ versions, and give sensitivity curves for only four species. Curves for the remaining species look qualitatively similar. Note the remarkably consistent peak at \u03b2 \u2248 1.0 for threshold feature curves; theoretical reasons for this phenomenon require further investigation. For LQP runs, peaks are much less pronounced and do not appear at the same value of \u03b2 across different species. Benefits of regularization in LQP runs diminish as the number of training examples increases (this is even more so for LQ and L runs, not presented here). This is because the relatively small number of features (compared with threshold features) naturally prevents overfitting large training sets."}, {"section_title": "Sensitivity to Regularization", "text": ""}, {"section_title": "Feature Profiles", "text": "Maxent as we have described it returns a vector \u03bb that characterizes the Gibbs distribution q (x) = e \u00b7f (x) /Z minimizing the (regularized) log loss. When each feature is derived from one environmental variable then the linear value of environmental variable additive contribution to exponent threshold, \u03b2=1.0 threshold, \u03b2=0.01 linear+quadratic, \u03b2=0.1 dtr6190 ann h aspect h dem h slope pre6190 ann tmp6190 ann wet6190 ann Figure 5 . Feature profiles learned on the first partition of the Yellow-throated Vireo. For every environmental variable, its additive contribution to the exponent of the Gibbs distribution is given as a function of its value. This contribution is the sum of features derived from that variable weighted by the corresponding lambdas. Profiles for three types of maxent runs have been shifted for clarity -this corresponds to adding a constant in the exponent; it has, however, no effect on the resulting model since constants in the exponent cancel out with the normalization factor.\ncombination in the exponent of q can be decomposed into a sum of terms each of which depends on a single environmental variable. Plotting the value of each term as a function of the corresponding environmental variable we obtain feature profiles for the respective variables. This decomposition can be carried out for L, LQ and T models, but not for LQP models. Note that adding a constant to a profile has no impact on the resulting distribution as constants in the exponent cancel out with Z . For L models profiles are linear functions, for LQ models profiles are quadratic functions, and for T models profiles can be arbitrary step functions. These profiles provide an easier to understand characterization of the distribution than the vector \u03bb. Figure 5 shows feature profiles for an LQ run on the first partition of the Yellow-throated Vireo and two T runs with different values of \u03b2. The value of \u03b2 = 0.01 only prevents components of \u03bb from becoming extremely large, but it does little to prevent heavy overfitting with numerous peaks capturing single training examples. Raising \u03b2 to 1.0 completely eliminates these peaks. This is especially prominent for the aspect variable where the regularized T as well as the LQ model show no dependence while the insufficiently regularized T model overfits heavily. Note the rough agreement between LQ profiles and regularized T profiles. Peaks in these profiles can be interpreted as intervals of environmental conditions favored by a species. However, from a flat profile we may not conclude that the species distribution does not depend on the corresponding variable since variables may be correlated and maxent will sometimes pick only one of the correlated variables."}, {"section_title": "Acceleration", "text": "For the LQP version of maxent, line search on \u03b1 substantially accelerated convergence when meaured in terms of log loss both on training and on test data. Log loss on test data in the first partition decreased with running time (measured on a 1GHz Pentium) as follows: The observed acceleration is similar to that obtained by Goodman (2002) . Line search made no discernible difference for threshold features. Indeed, while there is an approximation made in the derivation of \u03b1 in Sections 2 and 2.2, the derivation is exact for binary features, hence line search is not needed. Maxent was much faster with threshold features: log loss was within .001 of convergence in at most 50 seconds for both species."}, {"section_title": "Conclusions", "text": "Species distribution modeling represents a scientifically important area that deserves the attention of the machine learning community while presenting it with some interesting challenges.\nIn this work, we have shown how to use maxent to predict species distributions. Maxent only requires positive examples, and in our study, is substantially superior to the standard method, performing well with fairly few examples, particularly when regularization is employed. The models generated by maxent have a natural probabilistic interpretation, giving a smooth gradation from most to least suitable conditions. We have also shown that the models can be easily interpreted by human experts, a property of great practical importance.\nWhile maxent fits the problem of species distribution modeling cleanly and effectively, there are many other techniques that could be used such as Markov random fields or mixture models. Alternatively, some of our assumptions could be relaxed, mainly that of the independence of sampling. In our future work, we plan to address sampling bias and include it in the maxent framework in a principled manner. We leave the question of alternative techniques to attack this problem open for future research."}]