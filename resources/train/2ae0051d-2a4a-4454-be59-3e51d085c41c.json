[{"section_title": "Abstract", "text": "Neuroimage analysis usually involves learning thousands or even millions of variables using only a limited number of samples. In this regard, sparse models, e.g. the lasso, are applied to select the optimal features and achieve high diagnosis accuracy. The lasso, however, usually results in independent unstable features. Stability, a manifest of reproducibility of statistical results subject to reasonable perturbations to data and the model (Yu 2013), is an important focus in statistics, especially in the analysis of high dimensional data. In this paper, we explore a nonnegative generalized fused lasso model for stable feature selection in the diagnosis of Alzheimer's disease. In addition to sparsity, our model incorporates two important pathological priors: the spatial cohesion of lesion voxels and the positive correlation between the features and the disease labels. To optimize the model, we propose an efficient algorithm by proving a novel link between total variation and fast network flow algorithms via conic duality. Experiments show that the proposed nonnegative model performs much better in exploring the intrinsic structure of data via selecting stable features compared with other state-of-the-arts."}, {"section_title": "Introduction", "text": "Neuroimage analysis is challenging due to its high feature dimensionality and data scarcity. Sparse models such as the lasso (Tibshirani 1996) have gained great reputation in statistics and machine learning, and they have been applied to the analysis of such high dimensional data by exploiting the sparsity property in the absence of abundant data. As a major result, automatic selection of relevant variables/features by such sparse formulation achieves promising performance. For example, in (Liu, Zhang, and Shen 2012) , the lasso model was applied to the diagnosis of Alzheimer's disease (AD) and showed better performance than the support vector machine (SVM), which is one of the state-of-the-arts in brain image classification. However, in statistics, it is known that the lasso does not always provide interpretable results because of its instability (Yu 2013) . \"Stability\" here means the reproducibility of statistical results subject to reasonable perturbations to data and Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. the model. (These perturbations include the often used Jacknife, bootstrap and cross-validation.) This unstable behavior of the lasso model is critical in high dimensional data analysis. The resulting irreproducibility of the feature selection are especially undesirable in neuroimage analysis/diagnosis. However, unlike the problems such as registration and classification, the stability issue of feature selection is much less studied in this field.\nIn this paper we propose a model to induce more stable feature selection from high dimensional brain structural Magnetic Resonance Imaging (sMRI) images. Besides sparsity, the proposed model harnesses two important additional pathological priors in brain sMRI: (i) the spatial cohesion of lesion voxels (via inducing fusion terms) and (ii) the positive correlation between the features and the disease labels. The correlation prior is based on the observation that in many brain image analysis problems (such as AD, frontotemporal dementia, corticobasal degeneration, etc), there exist strong correlations between the features and the labels. For example, gray matter of AD is degenerated/atrophied. Therefore, the gray matter values (indicating the volume) are positively correlated with the cognitive scores or disease labels {-1,1}. That is, the less gray matter, the lower the cognitive score. Accordingly, we propose nonnegative constraints on the variables to enforce the prior and name the model as \"non-negative Generalized Fused Lasso\" (n 2 GFL). It extends the popular generalized fused lasso and enables it to explore the intrinsic structure of data via selecting stable features. To measure feature stability, we introduce the \"Estimation Stability\" recently proposed in (Yu 2013) and the (multi-set) Dice coefficient (Dice 1945) . Experiments demonstrate that compared with existing models, our model selects much more stable (and pathological-prior consistent) voxels. It is worth mentioning that the non-negativeness per se is a very important prior of many practical problems, e.g. (Lee and Seung 1999). Although n 2 GFL is proposed to solve the diagnosis of AD in this work, the model can be applied to more general problems.\nIncorporating these priors makes the problem novel w.r.t the lasso or generalized fused lasso from an optimization standpoint. Although off-the-shelf convex solvers such as CVX (Grant and Boyd 2013) can be applied to solve the optimization, it hardly scales to high-dimensional problems in feasible time. In this regard, we propose an efficient algo-rithm that solves the n 2 GFL problem exactly. We generalize the proximal gradient methods (such as FISTA) (Beck and Teboulle 2009 ) to solve our constrained optimization and prove its convergence. We then show that by using an element-wise post-processing, the resulting proximal operator can be reduced to the total variation (TV) problem. It is known that TV can be solved by parametric flow algorithms (Chambolle and Darbon 2009; Xin et al. 2014) . In the present study, we provide a novel equivalence via conic duality, which gives us a minimum quadratic cost flow formulation (Hochbaum and Hong 1995) . Fast flow algorithms (including parametric flow) are then easily applied. In practice, our algorithm runs hundreds of times faster than CVX at the same precision and can scale to high-dimensional problems.\nRelated work. In addition to sparsity, people leverage underlying data structures and introduce stronger priors such as the structured sparsity (Jacob, Obozinski, and Vert 2009) to increase model stability. However, for voxel-based sMRI data analysis, handcrafted grouping of the voxels or sub-structures may not coincide with various pathological topology priors. Consequently, group lasso (with overlap) (Jacob, Obozinski, and Vert 2009; Jenatton et al. 2012; Rao et al. 2013) is not an ideal model to the problem. In contrast, the graph-based structured sparse models adapt better to such a situation. The most popular one is referred here as LapL 1 , which adopts l 2 norm regularization of neighborhood variable difference (e.g. (Ng and Abugharbieh 2011; Grosenick et al. 2013) ). However, as we will show in the experiments, these models select many more features than necessary. Very recently, generalized fused lasso or total variation has been successful applied to brain image analysis problems inducing the l 1 difference (Gramfort, Thirion, and Varoquaux 2013; Xin et al. 2014) . In the experiments, we show that by including an extra nonnegative constraint, the features selected by our model is much more stable than that of such unconstrained models. A very recent work (Avants et al. 2014 ) also explored this positive correlation (partially supporting our assumption), but the problem formulation was quite different: neither structural assumption was considered, nor the stability of feature selection was discussed. From the optimization standpoint, the applied framework is similar to that of (Xin et al. 2014 ) but two key differences exist: (1) the FISTA and soft-thresholding process applied in (Xin et al. 2014) do not generalize to constrained optimization problems, we show important modifications and provide theoretical proof; (2) we propose a novel understanding of TV's relation with flow problems via conic duality and prove that the minimum norm point problem of (Xin et al. 2014 ) is a special case of our framework."}, {"section_title": "The Proposed Method", "text": "Nonnegative Generalized Fused Lasso (n 2 GFL)\nbe a set of samples, where x i \u2208 R d and y i \u2208 R are features and labels, respectively. Also, we denote by X \u2208 R d\u00d7N and y \u2208 R N the concatenations of x i and y i . Then, we consider the formulation\n( 1) where \u03bb 1 , \u03bb 2 \u2265 0 are tuning parameters. l is a loss term of variable \u03b2 (assumed to be convex and smooth). w ij s are pre-defined weights. Here, the variables (e.g. the sMRI voxel parameters in the AD problem) are supposed to have certain underlying structure represented by a graph G = (V, E) with nodes V and edges E. Each variable corresponds to a node on the graph. As mentioned above, in many brain image analysis, there exist strong directional correlations (positive or negative) between the features and the labels, thus we assume \u03b2 \u2265 0 (or \u03b2 \u2264 0). Due to the l 1 penalties on each variable as well as each adjacent pair of variables in (1), solutions tend to be both sparse and smooth, i.e., adjacent variables tend to be similar and spatially coherent. Also because we have added the nonnegative constraints, the model will not select negatively correlated features as support. In practice, we notice that unconstrained models will systematically select many negatively correlated features. The nonnegative constraints greatly reduce these falsely recovered variables and encourage genuine disease-related features to be selected."}, {"section_title": "Efficient Optimization of n 2 GFL", "text": "The optimization of n 2 GFL is convex and off-the-shelf solver such as CVX can be applied. However, this solution hardly scales to a problem sized of thousands (mainly due to its choice of general second order frameworks), see Table  1 . In this regard, we propose certain modifications to scalable first order methods (e.g. accelerated proximal methods (Beck and Teboulle 2009) ). This is done by exploring Lagrange multiplier method to deal with the constraints. From the optimization standpoint, these modifications are nontrivial and compose one major contribution of this work.\nWe first extend the (fast) iterative shrinkage thresholding algorithm (ISTA and FISTA) (Beck and Teboulle 2009 ) as follows. Proposition 1. Let \u03b2 * be the optimal solution to (1) and \u03b2 k defined as follows\nwhere L > 0 is the Lipschitz constant of \u2207l(\u00b7) and k is the iteration number.\nwhere\nThe proof can be viewed as an instantiation of the convex analysis introduced in (Nesterov and Nesterov 2004) . We provide a rigorous proof in the supplementary file. Now the key to solve (1) is how efficiently we solve (2). If there were no constraints, (2) is the fused lasso signal approximation proposed in (Friedman et al. 2007) , where it was shown that by utilizing the separability of the l 1 norm, an element-wise soft-threshold technique can be applied to remove the sparse term. Since the constraints of (2) are also separable, we show how (2) can be further reduced likewise. Proposition 2. If we defin\u1ebd\nthen the optimal solution to (2) (denoted as \u03b2 * ) can be achieved by an element-wise post-processing to\u03b2 as follows\nwhere is an element-wise product operator.\nrespectively for all i \u2208 V and (i, j) \u2208 E. We denote \u03b2 as the optimal solution of the unconstrained problem of (2). According to (Friedman et al. 2007 ), \u03b2 i = sign(\u03b2 i ) max(|\u03b2 i | \u2212 \u03b8 i , 0). We now consider the nonnegative constraints in (2). According to the Karush-Kuhn-Tucker (KKT) conditions, the necessary and sufficient conditions for \u03b2 *\nwhere \u03b1 i \u2265 0 are the Lagrange multipliers and s, t are subgradients: (5) is to set the derivative of the Lagrange function to zero and the constraint equations are obtained from the complementary slackness condition. We now consider two cases of \u03b2: Case 1 \u03b2 i \u2265 0: Note that by setting \u03b1 i = 0, \u03b2 i \u2265 0 satisfies the conditions in (5), thus \u03b2 *\nis the solution of (2). Case 2 \u03b2 i < 0: We can set \u03b2 * i = 0 and\nHence, in summary, we have\nNotice that, (3) is a (continous) total variation problem, which is known can be efficiently solved by parametric flow algorithms in (Chambolle and Darbon 2009; Xin et al. 2014) . Here we present a more general perspective of such an equivalence via conic duality, which gives us a natural and novel minimum quadratic cost flow formulation. Fast flow algorithms, such as but not limited to parametric flow (Gallo, Grigoriadis, and Tarja 1989) etc. are then easily applied. For example, we show that the minimum norm point problem solved by parametric flow in (Xin et al. 2014) can be viewed as a special case of the proposed dual."}, {"section_title": "Conic Dual to Total Variation", "text": "To solve (3), we apply generalized inequalities and its corresponding Lagrange duality introduced in (Boyd and Vandenberghe 2004) . Specifically, we first define a set C such that\nLemma 3. The set C is a proper cone. This can be easily shown by checking all the properties required by a proper cone. See the supplementary file for a proof. We now consider the following problem (we keep\nSince (\u03b2 ij , \u03b1 ij ) \u2208 C, then |\u03b2 i \u2212 \u03b2 j | \u2264 \u03b1 ij , therefore (6) is indeed equivalent to (3). Moreover, because C is a proper cone, we can rewrite (6) as follows,\nwhere \u03b2 C 0 \u21d0\u21d2 \u03b2 \u2208 C is defined as generalized inequality (Boyd and Vandenberghe 2004) . We call (7) the primal problem (which equals to the original TV problem). Since the primal problem is both convex and satisfies Slater's condition, strong Lagrange duality holds (under generalized inequality). We define the Lagrange function as\nwhere (\u03be ij , \u03c4 ij ) are the Lagrange multipliers and C * is the dual cone of C, defined as C * = {v | w T v \u2265 0, \u2200w \u2208 C}. To formulate the dual problem, we take the derivative of L(\u00b7) with respect to the primal variables (\u03b2, \u03b1) and we have (s,i) has 0 cost and a maximum capacity \u03b3i and a minimum capacity \u03b3i, this ensures a flow \u03b3i; each node-to-node edge (i,j) has 0 cost and a maximum capacity \u03b8ij; each node-to-sink edge (i,t) has infinite capacity and a cost (in red)\n. All flows through node 2 is highlighted in blue for an example.\nBy applying (9) to (8), the dual problem is written as\nPlease find the proof in the supplementary file. Accordingly, the dual problem becomes\nwhere we omit z 2 2 from the objective since it is a constant with respect to \u03bes and we also changed the sign of all \u03bes for better illustration of the flows.\nProblem (11) can be viewed as the following minimum quadratic cost flow formulation,\nwhere we have induced \u03b3 (\u03b3 i = max {|z i |, j,(i,j)\u2208E \u03b8 ij }) and denote y=z + \u03b3 to ensure y \u2265 0 and (\u03be ij + \u03b3) \u2265 0. Thus each feasible \u03be of (12) Figure 1 illustrates such flows by taking node 2 as an example. Thus to minimize the objective of (12) is equivalent to computing a minimum cost flow on this graph. Since the cost is quadratic with respect to the flow, this problem is a minimum quadratic cost flow problem. According to (Hochbaum (Gallo, Grigoriadis, and Tarja 1989) . Note that in (Xin et al. 2014) , TV is shown equivalent to a minimum norm point (MNP) problem under submodular constraints which is solved via parametric flow. We now discuss the relation between the dual problem i.e. (12) or (11) and the MNP considered in (Xin et al. 2014) .\nRecall that the MNP problem is defined as follows\nwhere f c(S) is a cut function, defined as f c (S) = i\u2208S,j\u2208V\\S w ij and B(\u00b7) is the base polyhedron of f c . Proposition 5. For any minimizer \u03be * of (11), define\u015d such that\u015d = (i,j)\u2208E \u03be * ij , then\u015d is a minimizer of (13). For any minimizer s * of (13), there exists a decomposition such that s * = (i,j)\u2208E\u03be ij , where\u03be is one minimizer of (11).\nAccording to Prop. 5, the MNP problem i.e. (13) can be viewed as a special case of (11) (the conic dual), where (i,j)\u2208E \u03be ij =s. Moreover, since (11) has relatively \"looser\" constraints, it is possible to devise more efficient algorithms (than parametric flow) to solve (11) and thereafter TV. For example, in (Mairal et al. 2011 ), a faster (than parametric flow) flow algorithm is proposed to solve their specific minimum quadratic flow problem. Hence, the conic dual perspective opens a new opportunity to solve the famous TV problem more efficiently. Optimization summary. In summary, by applying Prop. 1, we can solve n 2 GFL by iteratively solving (2). By applying Prop. 2, we further reduce (2) to the TV problem defined in (3), we then transform it to a minimum cost flow algorithm via conic duality and solve it by a fast flow algorithm.\nIn Tab. 1, we compare the proposed algorithm with an off-the-shelf solver on synthetic data. We generate a random \u03b2 \u2208 R d and a 2D grid graph of d nodes with each node having four neighbors. We then generate N = d/2 samples: x i \u2208 R d and y i = \u03b2 T x i + 0.01n i , where x i and n i are drawn from the standard normal distribution. All experiments are carried out on an Intel(R) Core(TM) i7-3770 CPU at 3.40GHz. The experiments show that the proposed optimization algorithm is more efficient and scalable."}, {"section_title": "Application to the Diagnosis of AD", "text": "In the diagnosis of AD, two fundamental issues are AD/NC (Normal Control) classification and MCI/NC (Mild Cognitive Impairment) classification. Let jects' sMRI voxels and y i = {\u22121, 1} be the disease status (AD/NC or MCI/NC). Since the problems are classifications, we use the logistic regression as the loss term\nwhere c \u2208 R is the bias parameter (to be learned). For the graph structure, we define each voxel as a node and their spatial adjacency as the edges, i.e. w ij =1 if voxels i and j are adjacent and 0 otherwise. The data are obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database 2 . We split all the baseline data into 1.5T and 3.0T MRI scans datasets (named 15T and 30T). 64 AD patients, 90 NC and 208 MCI patients are included in our 15T dataset; 66 AD patients and 110 NC are included in our 30T dataset. (Most 30T MCI data are in an on-going phase and are not included). Data preprocessing follows the DARTEL VBM pipeline (Ashburner and others 2007) as commonly done in the literature. 2,527 8\u00d78\u00d78 mm 3 size voxels that have values greater than 0.2 in the mean gray matter population template serve as the input features. We design experiments on three tasks, namely, 15ADNC, 30ADNC, 15MCINC.\nClassification Accuracy. 10-fold cross-validation (CV) evaluation is applied and the classification accuracy for all tasks are summarized in Tab. 2. Under exactly the same experiment setup, we compare n 2 GFL with the state-of-theart classifiers: logistic regression (LR), SVM, sparse models 2 http://adni.loni.ucla.edu e.g. the lasso and its graph Laplacian structured variants, i.e. the LapL, the unconstrained GFL (Xin et al. 2014) , and the \"MLDA\" model (Dai et al. 2012) , which applies a variant of Fisher Discriminant Analysis after univariate feature selection (via T-test). For each model, we used grid-search to find the optimal parameters respectively. Note that our accuracies may not be superior to the recent work (Liu et al. 2014) , the main reason is that in (Liu et al. 2014) , multi-modality data (including PET and sMRI data) are used. Nevertheless, Tab. 2 demonstrates that n 2 GFL outperforms all the other models using only voxel-based sMRI data.\nFeature selection. For each task, the selected features are those whose \u03b2 are not zero . In Figure 2 , the result of 30ADNC is used to illustrate the feature selection by different models (using the parameters at their best accuracy). As shown, the selected voxels by both GFL and n 2 GFL cluster into several spatially connected regions, whereas those of lasso and T-test/MLDA scatter around. Also, as mentioned before, the LapL tends to select much more voxels than nec- The top row illustrates voxels selected by the lasso model, the mid row illustrates those of GFL and the bottom row shows those of n 2 GFL. essary due to the l 2 regularization. Moreover, the selected voxels by GFL and n 2 GFL are concentrated in Hippocampus, ParaHippocampal gyrus (which are believed to be the early damaged regions). On the other hand, the lasso and Ttest/MLDA either select less lesion voxels or select probably noisy voxels not in the early damaged regions.\nFeature Stability. In Figure 3 , we show the selected voxels across different folds of CV 3 . As shown, the selected voxels by lasso vary much across different folds, whereas the selected voxels by GFL are more stable. However, by assuming the positive correlation between the features and the disease labels in n 2 GFL, we further increase the stability. To quantitatively evaluate the stability gain, we denote the variables of the kth fold of CV as \u03b2(k). We introduce two measurements here. In (Yu 2013), the Estimation Stability (ES) is proposed to measure the stability of the estimation\nwhere\u03b2 = K k=1 \u03b2(k)/K. It is shown in (Yu 2013) that ES is a fair measurement of the estimation stability. To further understand the stability of feature selection, we also extend the Dice coefficient (Dice 1945) to multiple sets and apply the multi-set Dice Coefficient (mDC) as a measurement. 3 Here, parameters were determined by accuracy. Similar results were observed using parameters producing same level of sparsity. We denote set S(k) = {i : \u03b2 i (k) = 0} and define mDC as\nwhere # is the number of elements in a set. In Tab. 3, both measurements quantitatively suggest n 2 GFL obtains much more stable voxels due to the consideration of the correlation between the features and the disease labels 4 ."}, {"section_title": "Conclusions", "text": "In this paper, we explore the nonnegative generalized fused lasso model to address an important problem of neuroimage analysis, i.e. the stability of feature selection. Experiments show that our model greatly improves the stabilities of feature selection over existing methods for brain image analysis. Although n 2 GFL is applied to the diagnosis of AD problem, it can be applied to solve more general problems. Moreover, we believe that the theoretical points made here e.g. nonnegative FISTA, soft-thresholding and the conic dual of TV, provide motivation for future work of general interest."}]