[{"section_title": "Introduction", "text": "Since scientific discovery drives economic growth, determining the optimal level of R&D is a main goal of many policymakers and administrators. In developed economies, two broad, secular trends are changing the parameters governing this problem. The first is a shift to university-led R&D. 1 The second is increasing reliance on internal, rather than government or private-sector, sources for the funding of that R&D. 2 This issue is particularly salient in the US, where the share of internally-funded university R&D work has nearly doubled over the last four decades. While the US has historically been the global leader in R&D activity, it is expected to lose this position to China in as early as 2022. 3,4 The policy debate has been contentious. In 2007, US Congress provided bipartisan support for the American COMPETES Act-legislation intended to double the level of funding to certain natural science, technology, engineering, and mathematics (STEM) fields. However, budget sequestration and austerity pressures have both delayed and reduced the scope of the original legislation. Advocates for government funding of R&D emphasize path-breaking innovations generated by university research. The Director of the National Institute of Health stated, for example, that without added support, \"A lot of good science just won't be done.\" 5 Critics, on the other hand, cite allegedly wasteful spending at the margin. This paper addresses two questions at the core of this problem. First, how large of an impact do institutionally-funded university research expenditures have on scientific productivity? There is little empirical work on the direct effect of money on science, even though understanding the knowledge production function is crucial to informing a policy debate that stretches back at least to works by Nelson [1959] and Arrow [1962]. 6 Moreover, there is no work specific to institutional funds, which are increasingly relied on but can differ from other funding sources in important ways. 7 The empirical challenge of identifying the impact of money on science using school-year observations is clear: a large set of controls is needed to disentangle the causal effect from unobservable factors that drive both expenditures and output simultaneously, but its inclusion can sweep away helpful exogenous variation, exacerbate an errors-in-variables problem, and drive coefficient estimates to zero [Adams and Griliches, 1998]. Second, does the pool of knowledge derived from academic research create valuable downstream 1 OECD. OECD Reviews of Innovation Policy-Sweden. Organisation for Economic Co-operation and Development, 2013. 2 OECD. OECD Science, Technology and Industry Outlook 2014. Organisation for Economic Co-operation and Development, 2014. 3 Battelle Memorial Institute. 2014 Global R&D Funding Forecast, 2013. 4 Note that if investments in R&D were to result in purely public goods, then all countries benefit from others countries' R&D investments. However, intellectual property (IP) laws allow for global protection of R&D outputs, so profits accrue to only the country making the initial R&D investment. Empirical evidence provides support that this is indeed the case, and further suggests that knowledge spillovers are highly localized (e.g. Jaffe, Trajtenberg, and Henderson [1993]). 5 Vergan, Dan. USA Today, February 25, 2013, \"Science faces sequestration cuts.\" 6 Nelson's paper provides fascinating historical perspective on the topic. At the time, the clear comparison to the US was not China but Russia. In fact, the second sentence of that paper states, \"Since Sputnik it has become almost trite to argue that we are not spending as much on basic scientific research as we should.\" 7 For example, an application process for funds within the university may allow for easier transmission of \"soft\" information. technology at the intensive margin, or simply accumulate inside the \"ivory tower\"? This transport of ideas is at the core of the economics of innovation and technology, crucial to endogenous growth [Romer, 1990], and the motivation for a large literature studying \"real effects\" of university research, starting with influential work by Jaffe [1989]. However, while much of this literature employs geographic variation to identify spillovers from academic activity to employment, wages, and firm behavior, these outcomes do not provide ipso facto evidence that university R&D activity generates valuable downstream technology. For example, while university R&D spending increases the local pool of knowledge, it also increases the local supply of newly-minted science and engineering PhDs and the local demand for technical equipment and services. Moreover, while publications, patents, and their citations are helpful for understanding the nature of knowledge production, they cannot directly speak to its private or social value [Griliches, 1990, Jaffe, Trajtenberg, andFogarty, 2000]. We use exogenous variation in research expenditures to assess the impact of internally-funded university R&D expenditures on a commonly-used set of outputs and then augment this set with technology licensing revenues to establish a lower bound on the value of derived innovations. The source of this variation is unexpected college football outcomes. Football team performance affects cash flow to the university and, in turn, the funds available for research. Even if unobserved school-specific factors that drive research output also influence football team success, they are unlikely to influence unexpected within-season changes in team success. Moreover, they are unlikely to be correlated with measurement error in expenditures, thus mitigating or eliminating the errors-in-variables problem. We measure football team success using the Associated Press Top 25 Poll, and use the difference between postseason and preseason vote counts as the instrumental variable. Since the individual voting results of the Poll are made public, and professional sportswriters who vote have a significant reputation stake in correctly forecasting teams' true prospects, the difference between postseason outcomes and preseason expectations can be treated as random. 8 Since each respondent ranks 25 teams, the number receiving positive votes is much larger, ranging between 35 and 52 and averaging 42. Three aspects of the football-funding relationship aid greatly in obtaining results. The first is the degree to which swings in football fortunes impact overall school finances. Since the late 1980s college football has generated tens of billions in cash flow to American universities. One of the more prominent examples is the University of Texas at Austin: its football team generates more revenue than the majority of professional National Hockey League teams (on a per-season basis) and more than seven-times the median major league baseball team (on a per-game basis). 9 At Louisiana State University, football revenue is nearly a third of total tuition receipts. A large portion of this revenue is plowed back into the athletic department, but a sizable part is returned to the school's general account in the form of unrestricted funds. In addition, a successful football season on the field usually translates to a successful fundraising campaign off the field. For example, Texas A&M University raised more money the night after its star quarterback Johnny Manziel's Heisman Trophy win than it typically raises in a month, in turn setting records for quarterly and annual alumni giving. 10 The second is that this source of funds is highly volatile, which means that administrators are likely to treat these changes as temporary windfalls rather than opportunities to start long-term projects. The third is that much of within-season football success and failure is unexpected. We model knowledge production as a function of faculty, facilities, and research support-close analogs to labor, capital, and materials in our setting-and a Hicks neutral total factor productivity residual. We assume that the first two inputs are fixed in the short term but the last one is adjustable. The instrument creates surprise, marginal shifts in the budget used to finance research projects, so it should impact research support only, leaving faculty and facilities unaffected. We show the data is consistent with this, which allows us to isolate the impact of the most contested knowledge production input. In addition, the data provides us with two exogeneity checks. First, we test whether football success impacts any source of research expenditures other than institutional funds. For example, football should not impact federal grants. Second, we test if football success impacts contemporaneous R&D expenditures. Since budgets need time to adjust, only subsequent expenditures should be impacted. There is strong evidence the instrument passes both checks. These expenditures support research projects. Successful projects yield academic publications and patents. We estimate the dollar elasticities of scientific productivity for each of these two output measures using a two-stage least squares (2SLS) specification. The elasticity is 0.16 when output measure counts articles and 0.28 and 0.35 when it counts the citations that accrue to those articles. For patents, the elasticities are much larger-even implying constant-to-increasing returns-to-scale. The elasticity is 0.77 when output is measured by patent filings and 1.73 when measured by the citations that accrue to them. These results indicate that research quality is not compromised as quantity rises. The cost of generating a patentable idea is around $2.5 million. All specifications include school and year fixed effects as well as school-specific time trends. Standard errors are clustered at the school level, and the estimates are all significant at the 5% level. Our 2SLS elasticity estimates contrast sharply with OLS estimates that use the same set of controls. In particular, they are between three and eighteen times higher. This sharp difference is policy-relevant, since low elasticities could lead to institutional or even national under-investment. It also confirms the stopped (accessed December 2, 2014). suspicion of prior authors Adams and Griliches [1998] that argued their estimates were unreasonably low-likely due to measurement error in the inputs. Our estimates also exceed those specific to federally funded expenditures provided by Jacob and Lefgren [2011]. They argue that their low returns are due, in part, to the competitive funding environment, which ensures that projects that go unfunded at the margin typically find alternative means of support. One possibility is that internally-sourced funding reflects soft information exchanges and support projects that are truly promising but nonetheless may look unattractive in the arm's-length grant process. Our productivity estimates are close to-but still a bit lower than-those estimated by Azoulay, Graff-Zivin, Li, and Sampat [2014], who study private sector pharmaceutical firms. To answer the more fundamental question of whether investments in academic research generate valuable, downstream technology at the margin, we combine the logic of revealed preference with data on university licensing revenues. That is, the amount that unaffiliated, private-sector firms will pay for research output is a straightforward lower bound on its value. The availability of this measure is a convenient consequence of the fact that universities rarely commercialize their inventions, resulting in intermediate transactions that price the technology and sidestep the \"major difficult[y]\" of measuring research output [Griliches, 1979]. A one percent change in research support expenditures generates at least about $30,000 in upfront licensing revenues. This equates to 4-5% upfront returns and 9% total returns on investment where total returns factor in average royalty payments). These findings reject the claims of those opposing investments in STEM field university R&D, who argue that even if these expenditures produce \"paper\" output, i.e. articles and patents, they would not generate genuinely useful innovations. This paper contributes to several literatures. It most closely connects to work on the knowledge production function [Griliches, 1979]. We build on Adams and Griliches [1998], who measure the impact of university research expenditures on scholarly articles and their citations. They find that adding institutionlevel controls yields implausibly low elasticities and argue this is due to right-hand side measurement error. This echoes a more general frustration in the literature on production function estimation-that the inclusion of fixed effects results in very low factor-output elasticities [Griliches and Mairesse, 1998]. Olley and Pakes [1996] instead take a control function approach to the problem and find more sensible estimates, although recent work by Collard-Wexler and De Loecker [2016] suggests that the errors-invariables problems persists, at least when using data from developing countries. In particular, they show that instrumenting for capital with investment produces much higher coefficients than using measured values in Slovenian and Indian datasets. We complement these findings by estimating the degree of bias using a very different type of variation: in their framework, our instrument would represent a cost shifter, moving the shadow price of R&D investment within the university bureaucracy. Recent research takes an IV approach similar to ours but instead studies federally funded research expenditures. Jacob and Lefgren [2011] measure the impact of NIH grants on publications and patents and find lower elasticities than ours, the potential reasons for which we discuss below. 11 Azoulay, Graff-Zivin, Li, and Sampat [2014] measure the impact of public grants on patenting for private sector pharmaceutical firms and only slightly lower estimates than we do. This paper also closely relates to work evidencing the \"real effects\" of academic research [Jaffe, 1989, Acs, Audretsch, and Feldman, 1992, Jaffe, Trajtenberg, and Henderson, 1993, Henderson, Jaffe, and Trajtenberg, 1998, Kantor and Whalley, 2014, Furman and MacGarvie, 2007, Hausman, 2013. These papers rely on the geographic coincidence of university activity and private sector outcomes, including wages, employment, and profits. Pakes [1986] provides an alternative approach to directly measuring innovative output, which uses patent renewal decisions to bound the underlying value of inventions. In contrast, we use licensing revenues to measure impact, which Jensen and Thursby [2001] have demonstrated is a salient outcome for university faculty and staff. The paper is organized as follows. Section 2 describes the setting, the empirical challenge, and motivates our solution. Section 3 describes the data. Section 4 provides a model of the knowledge production function and the estimation strategy. Section 5 assesses the impact of our instrument on research expenditures, and Section 6 estimates the impact of these expenditures on scientific output. Section 7 concludes."}, {"section_title": "Research and Development at US Universities", "text": ""}, {"section_title": "Background", "text": "A primary goal of a research university is to increase the pool of available knowledge. Hence, large and successful ones play a key role in economic growth. In the US, universities account for 15% of total R&D spending, which totals nearly $500 billion each year, and more than half of basic science spending 12 . As in most of the developed economies, their share is growing. 13 The US has historically been the global leader in R&D investment, and the productivity of its university scientists and engineers has been viewed as a comparative advantage. However, emerging economies, and especially China, are gaining ground. 14 If knowledge were an easily transported and public good, policymakers could of course free ride off others' investments; however, IP protection and the local nature of many R&D spillovers make these developments serious concerns. In large part, China's aggressive government support of R&D is driving their relative 11 Payne and Siow [2003] use Congressional Appropriations Committee representation as an instrument for federal funding but do not interpret their estimates as causal impacts of federal support alone, since they are correlated with other factors as well. Their instrument has a positive effect on articles but interestingly has no effect-or even a slightly negative effect-on citations per article. 12 Battelle Memorial Institute. growth. At the same time, as in other developed economies, universities in the US are more and more relying on internal rather than federal, state, or private sector sources to fund R&D. 15 To create knowledge, universities unite skilled labor with physical capital and fund and coordinate research support. For a host of reasons, the policy debate has focused on the last of these, particularly in the STEM fields. 16 The disagreement centers on the magnitude of the return on research expenditures. "}, {"section_title": "A First Look at Knowledge Inputs and Outputs", "text": "Given its policy relevance, why has so little empirical work addressed these issues? The obstacle for randomized trials, Azoulay [2012] argues, is an unwillingness of policymakers to gamble with the careers of scientists. He also points out that the current process is no less random, since it is virtually unstudied and many proposals go unfunded. The obstacle for using historical data, on the other hand, is scarcity of robust quasi-experiments. Unfortunately, learning about the input-output relationship from time series or cross-sectional variation is fraught with endogeneity, but adding controls can worsen right-hand side 15 OECD. OECD Science, Technology and Industry Outlook 2014. Organisation for Economic Co-operation and Development, 2014. 16 Faculty expenditures are a secondary concern because they partly provide instruction, are mostly fixed over time, and are tied to long-term contracts that make layoffs difficult or impossible. Land, building, and large equipment are also largely fixed over time, and large additions are frequently directed investments of individual benefactors. 17 For example, the 2014 Wastebook cites a $856,000 National Science Foundation grant devoted to studying whether a mountain lion can be taught to use a treadmill. (The answer is yes, but it takes eight months). The Wastebook then points out that this research was published in the Journal of Neural Engineering. We note that this was a misrepresentation of the work but nonetheless effective. 18 Simmons, Dan. February 6, 2015. \"Rebecca Blank: Scott Walker's budget would mean $91 million budget hole at UW-Madison.\" http://host.madison.com/news/local/education/university/rebecca-blank-scott-walker-s-budget-would-mean-millionbudget/article_52c6749b-4cc9-53bb-b07e-83c3f227c23c.html Retrieved November 3, 2015. measurement error problems. Figure I illustrates the problem for patent applications. 19 In each panel, the x-axis measures the log of school-funded research support expenditures while the y-axis measures the total number of articles published in the natural sciences in the subsequent period. Plotted points represent school-year observations binned according to x-axis values. The slopes represent elasticity estimates-a linear relationship between log expenditures and log applications. [ Figure I about here.] The left panel simply pools all schools and years together. The slope of this relationship is precisely estimated and represents an elasticity of 0.53. Alternatively, the right panel plots residuals of the x-and y-axis measures after accounting for school and year fixed effects and school-specific time trends. The addition of these simple controls cuts the elasticity estimates down by more than 90% to 0.04. This result was first shown by Adams and Griliches [1998], who used an analogous but earlier dataset. They state the problem as a clear lack of exogenous variation: \"To date we have little hold over changes in financial and other circumstances that bring about a change in the stream of a university's research output.\" Universities fund their operations primarily through tuition, philanthropic donations, and auxiliary enterprises (like healthcare and athletics). For many schools-in particular, public schools-state appropriations also account for a significant share of incoming funds. Schools use these funds mainly for student instruction, research, administration, and running the auxiliary programs. Budgets adjust each year, and surpluses are rarely carried over to the next year. 20 It turns out that college football represents a major source of variation in university budgets. We discuss this at length below."}, {"section_title": "The Role of Football in Funding", "text": "Since the late 1980s, college football has generated billions of dollars for US universities. It contributes to finances through two channels. The first goes through the athletic department, or more generally auxiliary operations, revenues. Their size can be staggering. At the University of Texas at Austin, football revenues are approaching $150 million each year. For perspective, this is larger than the median professional hockey team (on a per season basis) and many times larger than the median Major League Baseball team (on a per game basis). These revenues are also large relative to other university earning streams. At Louisiana State University and the University of Nebraska, they are nearly one-third the size of the entire tuition bill. For many schools-the majority of those in our panel-football generates more revenue than all other sports combined. The second channel is alumni donations. Football success is a major catalyst for philanthropic fundraising shocks [Meer andRosen, 2009, Anderson, 2012]. For example, Texas A&M University raised more money the night after its freshman quarterback, Johnny Manziel, won the Heisman Trophy than it typically raises in a full month. That year, the school announced it received a record-setting $740 million in donations. 21 The university chancellor John Sharp highlighted the significant role college football played in their fund-raising efforts, stating, \"Football is one heck of a megaphone for us to tell our story.\" 22 Philanthropy represents an important source of university science funding overall and the majority of its recent growth [Murray, 2013]. Moreover, schools can also directly tie athletic privileges to academic donations. In March 2015, two months after winning the National Championship, Ohio State notified fans that a $3,000-5,000 donation to the university fund was necessary to purchase a parking pass for the following season (on top of the standard $375 cost). This move was expected to generate $18 million in contributions. 23  The success underlying these contributions are volatile. More than half of the schools in our panel, described in the following section, have competed for the national championship-that is, ranked #1 or #2 in the country-at some point between 1987 and 2012. Yet, every one of these teams was unranked at some other point over the same period. Within-season reversals of fortune are just as common. Almost a quarter of teams finishing first or second began the season outside the top ten, and nearly as many begin first or second and finish outside the top ten. For example, Boise State University began the 2006 season unranked but finished undefeated and in fifth place, resulting in a marked increase in university donations, a 66% increase in sales of university merchandise at the bookstore, and a 60% increase in sales of the subsequent year's seasons tickets [Grant et al., 2008]. A share of these revenues and donations are returned to the general university fund and ultimately support academic endeavors. For example, in 2012, the Louisiana State University team pledged over $36 million over 5 years to support the school's academic mission. In 2005, the Notre Dame football used $14.5 million of its postseason bowl winnings to fund academic priorities. From 2011 to 2012, the University of Florida team gave $6 million to cover shortfalls in university funding [Dosh, 2013]. From 2012 to 2013, the University of Texas -Austin gave $9.2 million of its $18.9 million football surplus back to the university fund while the University of Nebraska -Lincoln did the same with $2.7 million of its $5.2 million surplus [Lavigne, 2014]. The clearest admission is a quote from E. Gordon Gee, former President of The Ohio State University: \"We took direct dollars from the athletic budget and put it into academic programs.\" 24 In fact, we see this precise relationship in the data. Figure II provides an illustration. The x-axis is a standardized measure of unexpected college football outcomes. 25 The y-axis measures the log of schoolfunded research support expenditures in the subsequent year. Plotted points represent residuals after accounting for school fixed effects, year fixed effects, and school-specific time trends, and then binning the data according to x-axis values. [ Figure II  The result is an unambiguous positive relationship and the primary motivation for using unexpected within-season vote changes as an instrument for subsequent year research support expenditures. These and related results are revisited below."}, {"section_title": "Data", "text": ""}, {"section_title": "Sources", "text": "The data come from several sources. The instrument is derived from Associated Press Top 25 Poll (\"AP Poll\") vote data. The poll surveys sixty-five sportswriters and sports broadcasters. Each provides a ranking for the top twenty-five teams from NCAA Division I. Each team receives 25 points for each 1st place vote, 24 points for each 2nd place vote, and so forth, and the votes are aggregated over survey responses. 26 The AP publishes the vote totals of all teams. Ballots are collected weekly through the season, with results made public and published at the end of the week. We measure the within-season change in team quality by subtracting preseason votes from postseason votes. Polls varied slightly in the number of voters and, in 1987 and 1988, the number of points allocated, so we normalize the measure by standard deviation. This data is widely disseminated each week of the season and has a special place in college football; unlike professional sports or other college athletics, which rely on playoffs and divisional rank and record, polls 24 \"Dropping The Ball: The Shady Side Of Big-Time College Sports,\" The Bob Edwards Show (Washington D.C.: Public Radio International, January 4, 2015). 25 Details follow in the next section. 26 The exceptions are 1987 and 1988, where voters ranked only the top 20 teams. For these polls, teams received 20 points for each 1st place vote, 19 points for each 2nd place vote, and so forth. were the sole source of determining an NCAA football champion until 2013. 27 At least three other polls are widely published, although the AP Poll is the best known and longest-running. Moreover, although they are closely correlated with the AP Poll, the other major polls had obvious limitations for our setting. 28 The relevant time variable for this data is the fiscal year in which a season is wholly contained. Fiscal years coincide with the academic calendar for schools in our data. There are three types of input expenditures. All are in terms of constant 2009 dollars. Separatelybudgeted research and development expenditures (\"research support\") data come from the annual NSF Higher Education Research and Development Survey. This covers all science and engineering R&D that is separately budgeted and accounted for. 29 The survey spans all institutions spending at least $150,000 in separately budgeted R&D and breaks expenditures down by source: federal government, state and local governments, institution, business, and other. Responses are carefully reviewed and verified as needed. 30 The other two, faculty and facility expenditures, come from the Integrated Postsecondary Education Data System (IPEDS). 31 Faculty headcount and salaries cover all tenure-track and tenured faculty employed in fall quarter. Facilities comprise all plants, property, and equipment less depreciation at fiscal year-end. There are five measures of research productivity. The first two are published articles (\"articles\") and the citations that accrue to them. These come from the Thomson-Reuters Incites database. Observations are specific to a calendar year, institution, and academic discipline. Since the instrument only has variation at the institution-year level, we aggregate up to this level by taking a sum over all relevant disciplines. We include two publication measures. The first article-based measure covers only the natural sciences, 32 while the second covers all STEM fields. 33 That is, the second merely adds engineering and medical-related 27 In 2014, a playoff system was instituted. 28 The BCS Poll, for example, did not cover our full sample. The Coaches Poll could, hypothetically, be contaminated by strategic voting. Other polls were much less widely known and relied upon. 29 As per Office of Management and Budget survey B.1.b and A21 (revised), this comprises sponsored research funding (from both federal and non-federal entities) and university research funding (for which there was an internal application for the funds). It excludes instruction and related departmental expenditures. 30 There are two cases where we were sure of survey response error, and confirmed them with the NSF. In both cases, the NSF had already asked for clarification from the university. This gave us confidence that the data was reviewed by the NSF. Ronda Britt at the National Center for Science and Engineering Statistics was particularly helpful. Our main issue was missing values for Boise State University prior to 1992 and in 2005 and 2006. In the earlier years, the institution was below the survey threshold. For the later two years, the NSF confirmed the school made an error in reporting due to a personnel change. We omit these years from our analysis, although the results are robust to dropping this institution entirely. In any case, this strongly supports the notion that measurement error is present in the expenditure data. 31 IPEDS aggregates all departments. We cannot separate out the science and engineering departments' faculty, but science and engineering departments account for the majority. For instance, approximately 62% and 68% of Pennsylvania State and Ohio State University faculty were in science and engineering departments, respectively, in 2013 We also expect the departments to be highly correlated. We also cannot separate out facilities devoted to research from those devoted to instruction, administration, and auxiliary enterprises. However, anecdotally, we expect athletics-related cash shocks to affect auxiliary and student-related functions of the school at least as much as research. Hence, it suffices to show no aggregate impact on facilities to conclude there is no impact on the research-related portion. 32 As defined by the National Science Board, which is under the direction of the NSF, in their biennial Science Indicators report, the natural sciences include physical sciences (chemistry, physics, and space science), logical sciences (mathematics, computer science), biological and earth science (biology and biochemistry, microbiology, plant and animal science, agricultural science, environmental science and ecology). 33 We do not consider arts, humanities, or social sciences in the paper. These are not at the center of the current policy debate, and have wide variation in the time to conduct and publish research. The expenditure data also does not cover a portion of these fields. disciplines to the first. The next two are new patent applications (\"patents\") and the citations that accrue to them. These come from Thomson Innovation. 34 We use the browse feature in Thomson Innovation Assignee/Applicant search field to identify all possible university name variations together with unique 4-letter Assignee Codes identifying one of approximately 22,300 patenting organizations worldwide. This enables us to count and aggregate patent applications wherever a university appears as an assignee or applicant on the patent record. Again, we extract a count of new patent applications filed and a count of the citations that accrue to those patents (up to the date of data retrieval). Although patents are assigned into technological classes, there is no clear map to academic disciplines. Thus, we aggregate up to the institution-year level by taking a sum over all classes. We assemble this data on a fiscal year basis. More details on patent dataset construction are provided in Appendix E. The final measure is technology licensing revenue. This comes from an annual survey administered by the Association of University Technology Managers (\"AUTM\"). The respondents are staff of each university's technology transfer office (\"TTO\"), the group responsible for licensing and managing the intellectual property created by faculty. TTOs are rewarded for completing the survey by receiving data on all other institutions, so while completion is not compulsory, non-response is rare. Fiscal year results are surveyed. Most licensing deals stipulate a stream of payments rather than a one-time cash transfer, so TTO revenue in any year is swamped by prior licensing activity. AUTM, fortunately, seperates \"running royalites,\" i.e. those tied to product sales, from the totals. Thus, we construct our output measure as total licensing income, net of the portion passed through to other parties, multiplied by one minus the ratio of running royalties to total licensing income. AUTM also collects the year each TTO was established, so they avoid mis-classifying some observations as zero. 35 In a few cases, AUTM data required supplementary information, e.g. TTO annual reports that could fill in missing responses. Appendix F provides details on each correction."}, {"section_title": "Length and Scope", "text": "The beginning of the panel coincides with the start of the \"modern era\" of college football and covers the period of exploding popularity and profitability of the sport, more than sufficiently large to shift universities budgets. It begins with the 1984 Supreme Court ruling in NCAA v. Board of Regents of the University of Oklahoma. 36 Prior to this ruling, the NCAA restricted the number of games that could be broadcast, threatening non-complying schools with an association-wide boycott. In 1981, two schools challenged the NCAA's authority and in 1984, the Burger court ruled that the NCAA violated antitrust laws by controlling television broadcasting rights. Effectively, schools and their conferences were now free to negotiate directly with broadcasters. Broadcast networks treated the first year or two as a trial for the new arrangement, but by 1987 the number of televised games and the exposure of the league surged, leading to an unprecedented financial gains. That year featured the highly contentious Fiesta Bowl, which became one of the most watched college games in history, and marks the start of our panel of football outcomes. 37 Patenting and licensing data, however, begin in 1996. Although we observe data for earlier periods, the international harmonization of the Unites States patent system in the early 1990's created a large spike in the number of filings and seemingly increased the overall level of patenting. If the response of patenting behavior to research funding was different prior to 1996, and the goal is to recover parameter estimates that are informative for current policymaking, then including data on filings prior to 1996 would lead to incorrect estimates. The dataset ends in 2011. After accounting for lags, described in Section IV, we are left with 22 and 16 years of observations for scholarly articles and patents/licenses, respectively. The cross-section of the panel covers forty schools. The median team receives zero votes in a poll, so using the universe of teams would result in a large number of zeros in the instrument. Thus, we order the schools in terms of their total variation in the instrument and select the forty largest, i.e. one-third of the 120 NCAA Division I teams. The resulting list includes most large public universities and is quite close to simply selecting the most successful programs over the panel. It includes private (e.g. Stanford, Notre Dame) and public (e.g. Alabama, Nebraska) institutions as well as relatively small (e.g. Boise State) and large (e.g. Texas) ones. The magnitude of our estimates are not very sensitive to the size of the panel. 38 Table I provides a summary of the data. First, there is a great deal of variation in both inputs and outputs. Standard deviations are in the neighborhood of the mean values for most variables. There is 36 See NCAA v. Board of Regents of the University of Oklahoma, 468 US 85 (1984). 37 The game pitted Penn State against a heavily-favored University of Miami. The pre-game antics of Miami, including dressing in military fatigues for the flight to the game, and controversial remarks by both sides at a joint team dinner the night before the game contributed to wide-spread media attention. For the first time in history, a sitting US President (Ronald Reagan) was interviewed at the halftime show. Penn State won 14-10. The national press coverage of the players, coaches, their backgrounds, and the developments leading up the game are all common in the \"modern era\" but were unheard of prior to 1987."}, {"section_title": "Summary Statistics", "text": "38 At panel breadth of 35, 40, and 45 schools, our estimates for the elasticity with respect to articles are .161 (.071), .161 (.063), and .184 (.075), respectively. These elasticities with respect to article citations are .258 (.137), .277 (.130), and .426 (.196), respectively. These elasticities with respect to patent applications are .740 (.419), .769 (.386), and .926 (.447), respectively. These elasticities with respect to patent citations are 1.66 (.670), 1.73 (.653), and 2.06 (.780), respectively. To summarize the impact of changes in panel breadth over a larger range, a decrease in the sample size (clearly) hurts statistical power, while an increase adds a substantially large number of zeros to the instrument. also considerable right skew, so most of our analysis uses log transformations of left-and right-hand side variables. [ Table I about here.] Schools in our panel contribute about a quarter of the total $295 million in research support. The majority of this support comes from the federal government, e.g. the NSF, NIH, and agencies like the Departments of Defense and Energy. Schools average roughly 1,000 faculty. The median and mean pay are just above and below $100,000. Facilities are valued just above $1 billion. Schools produce on average 937 articles in the natural scieces and 1,564 in the STEM fields each year. Approximately 25,000 and 46,000 citations eventually accure to these articles, on average, respectively. They also produce about 38 patents generating 526 citations. 39 Upfront licensing revenue totals nearly $1.5 million per year. Total licensing income, which includes running royalites, is several times as large (not shown). The total number of observations related to inputs and article-related outputs is 880 and to patentingand licensing-related outputs is 640. However, some data is missing. Twelve observations are missing for research support. We have confirmed with the NSF these were genuine data entry errors and treat these as missing at random. Forty observations are missing for faculty, due to the absence of data for 2001 (relating to a budget cut), so we also treat these as missing at random. For facilities, there are 109 missing observations. This is mainly due to an absence of data for 1990 and 2000, and the fact that UCLA is not distinguished from the University of California system. We treat these and the remaining ten missing observations as missing at random. Missing output values are more problematic, since they may represent zeros. For articles and their citations, there are no missing values. Twenty-six patent observations are missing, 16 of which are due to the absence of the University of Oregon. The school was simply not provided a lookup code by Thomson-Reuters data, which we view as an error and thus treat these as missing-at-random. The remaining ten are spread across four schools and reflect the fact that schools did not patent in those years. Twenty-two licensing revenue observations are not available, but not all are missing; some pertain to periods when the university had yet to establish a TTO. Details are in Appendix F."}, {"section_title": "Model", "text": ""}, {"section_title": "Empirical specification", "text": "The knowledge production function [Griliches, 1979] of university i in period t is given by The function takes as its arguments L, K, and M, which for our purposes represent faculty, facilities, and research support, as well as a Hicks neutral \"total factor\" productivity shock [Syverson, 2011]. If the function is Cobb-Douglas, then taking logs yields The total factor productivity shock is a linear function of controls, denoted W, and a residual so that In all main specifications, W i,t comprises school fixed effects, year fixed effects, and schoolspecific time trends. Note that estimation of Equation 1 is still not straightforward. If administrators choose high m when is also high, right-hand side observables and unobservables are determined simultaneously. Moreover, if the university is liquidity constrained-the conjecture that motivates this analysis in the first place-then a factor outside the model could drive both funding access and the total factor productivity residual simultaneously. Rather than place considerably more structure on the problem, we use an instrument to exogenously shift m. Since the hiring and firing of faculty and the construction and demolition of facilities are very slow, L and K are fixed in the short-term. Research support, on the other hand, consists of things like laboratory materials, non-faculty research staff, and software. These are likely to be adjustable. Therefore, we assume the instrument will not affect faculty or facilities, but will affect research support. This allows us to isolate the impact of the last of these inputs. One clear concern, in the event this is wrong, is that schools with cash windfalls poach faculty from less fortunate schools and that this behavior is the real driver of output changes. The data, fortunately, strongly support our assumption, which we show in the following Section. Measurement error in the right-hand side variables of Equation 1 also creates a problem. If the econometrician sees onlyx, which equals expenditure x measured with noise e x , then regressing output on observed input expenditures results in downward bias. Moreover, the degree of this bias may be highly dependent on the type of controls included in the econometric specification-the richness of W i,t -since conditioning variables will sweep away potentially helpful exogenous variation along with the unobservables. 40 This creates serious problems in practice [Griliches and Mairesse, 1998]. 41 Again, we rely on our instrument: it is unlikely to be correlated with measurement, so resulting estimates will be unbiased. We estimate a linear impact of the instrument on expenditures by type, which is given by where x equals l, k, or m. The data provides confidence that \u03b1 is close to zero for labor and capital, so Thus, our first stage estimating equation is where \u03b3 i , \u03b4 t , and \u00b5 i represent the school fixed effects, year fixed effects, and school-specific time trends. Since E[z i,t e x i,t |W] = 0 for each x, this does not complicate recovery of \u03b1 x . The second stage estimating equation regresses predicted left-hand side values from Equation 3 onto output, conditional on controls. This is given by where  The inequality follows from the fact that \u03b6 and , then an increase in simulateneity bias must be offset by increase in errors-in-variables bias. Inequality 5 provides a bound on the proportion of \"within\" variation derived from measurement error. Since the denominator can be estimated directly, it is easy to establish a lower bound on \u03c3 2 e itself. Dividing this bound by \u03c3 2 m , the total variation in observed expenditures also provides a bound on the proportion of total variation attributable to measurement error."}, {"section_title": "Summary of timing", "text": "In the model, t subscripts denote periods in the abstract and thus merely summarize the relationship between the instrument, expenditures, and outputs. They need not-in fact, do not-refer to exact calendar dates. Budgeting, experimenting, and publishing take time, resulting in delays that create lags between the measured values. Background knowledge helps inform the model, but timing is ultimately an empirical question. Thus, while our main specifications assume an explicit lag structure, each is accompanied by a test against the alternatives and a discussion of what drives the results. Together, these provide confidence that the simple, assumed structure captures the first-order effects. Our main specifications reflect the following timing. In the first period, fiscal year \u03c4, teams play out the season from mid-August to early-January and generate values for the instrument. Realizations drive cash flows from, e.g., apparel sales, playoff appearance payouts, and donations, in time to shift the subsequent year's budget and locate highest-return projects. Survey data shows that insufficient/decreased R&D support poses the largest challenge for research faculty, 43 suggesting both an abundance of projects bottlenecked by money and a high likelihood that marginal funds are spent close to when budgets are replenished. Hence, in the next period, fiscal year \u03c4 + 1, budgets adjust and faculty conduct research. Faculty are encouraged to protect their ideas as soon as results provide proof of concept. Thus, in this same period, fiscal year \u03c4 + 1, they file provisional patent applications, which are cheap and fast to write and submit, in the same period. Once they do, TTOs are free to license the technology, which they do starting in the same period, fiscal year \u03c4 + 1. In the subsequent period, calendar year \u03c4 + 2, faculty publish articles in conference proceedings and academic journals. (Note that moving from a fiscal to calendar year basis is simply due to the data collection window determined by Thomson Reuters). This timing reflects manuscript submission-to-acceptance times in the STEM fields that are measured in days or weeks rather than months or years, as in the social sciences. More detailed discussions of each of these points are provided in the following sections."}, {"section_title": "External validity", "text": "The underlying budget variation derived from football has two features that require discussion of how externally valid, i.e policy valid, our results are. First, the underlying shocks are treated as temporary. In theory, the university could take its unexpected surplus and make long term investments, e.g. a faculty member or construction project. Absent organizational constraints and behavioral reasons, they may. Nonetheless, they do not. Second, the underlying shocks are roughly zero sum on average. If one school exceeds expectations, then some other school falls short by about the same amount. Thus, while our instrument measures the impact of temporary budget shocks that are zero sum across schools, the policy-relevant budget shocks would likely be permanent or aggregate or, more likely, both. For a temporary shift of internally provided research expenditures, the results are very informative. Administrators should treat those budget changes similarly to those generated by the instrument and confine themselves to adjusting research support. Whether the shift is an idiosyncratic or aggregate one is unlikely to matter, since research support is relatively elastically supplied in the short run. For example, software can be replicated very quickly, and bacteria and mice nearly as quickly. For a permanent shift, the answer is more complicated. Universities may choose to poach faculty or undertake capital projects, which is problematic in light of Goolsbee [1998a,b]. These papers showed that while the intention of federal R&D subsidies and investment tax credits is to boost research and investment activity, respectively, the inelasticity of the supply of these inputs means that price increases can absorb a large portion of government expenditures. The latter of these papers finds, for example, that 30-50% of additional federal research expenditures may go towards private sector scientist and engineering wages. However, Goolsbee focuses on short and medium run effects because the policies studied are aimed at troughs in activity at weak points in the business cycle. Since faculty require three to six years of PhD or PhD-equivalent training, depending on discipline, they will be supplied at least as inelastically as their private sector counterparts, suggesting we overstate the returns to a policy change. However, permanent shifts consider long run impacts. Over this horizon, faculty and facilities can adjust. By the envelope theorem, our results likely provide a lower bound for the policy-relevant parameters. R&D business stealing effects require a more nuanced qualification. If disciplines have a common set of questions, researchers compete for whom will answer them first, resulting in duplicate research that may only yield slightly earlier results. Although privately beneficial, this competition is then socially wasteful. Idiosyncratic budget shifts may determine which institution's researchers answer first rather than how many or how well questions get answered. Thus, business stealing effects cannot be completely ruled out. In our experience, however, at least in the social sciences, academic research agendas are diverse enough to make these effects small. school and year fixed effects as well as a school-specific time trends, and allows for arbitrarily correlated standard errors within a university over time. The first column provides the coefficient estimates from the regression in Figure II. A one standard deviation change in the instrument translates to a 3.33% increase in the support. The estimate is significant at the 1% level, and the corresponding F-statistic is 18.58 (shown in Table III)."}, {"section_title": "The Effect of Unexpected Football Outcomes on Expenditures", "text": ""}, {"section_title": "Magnitudes, by Input", "text": "[ Table II about here.] To put these results in perspective, a one thousand vote change is equivalent to moving from 17th place to 1st or from unranked to 10th. Using a simple rank-order of schools by 2012 team revenue, estimated by Forbes magazine, this translates to roughly $50 million difference. 44 If, say, 5-10%, or $2.5-5 million, of this are allocated to R&D, then the $3 million predicted by the model is certainly reasonable. The next column shows a near-zero effect on faculty salaries. To rule out any impact of the instrument, the variance around this point estimate must also be small. This is apparent from the standard errors. To emphasize the point, the final row in the table includes 95% confidence intervals for the estimate. For instance, the interval on research support covers from 1.7% to 5.0% while the interval on faculty salaries covers from -0.3% to 0.5%. One concern is that faculty expenditures may require an additional period to adjust, so the effect is present but lagged one more year. Column 3 rules this out. For robustness, columns 4 and 5 substitute faculty headcount, which may be a more sensitive measure of the input, for salaries. The results do not change. The final two columns indicate zero impact on facilities, too. While the standard errors are larger here, we emphasize that the lower bound of the 95% confidence interval on research support is greater than the upper bound on the interval of the other inputs, regardless the timing or measure. Note that when output is measured by licensing revenues, there is an additional concern. Even if our budget variation does not induce faculty hiring, it may lead to investments in more TTO staff or better counsel, and ultimately a simultaneous rise in licensing. Fortunately, AUTM collects supplementary data on TTO employment and legal fees, and in Appendix B we show that the instrument has no effect on these outcomes. 45"}, {"section_title": "Exogeneity Checks", "text": "The NSF data breaks down research support by source. So far, we have considered only support provided by the school. Although football should impact school-funded support, it should not impact money coming from the federal or other sources not affiliated with the institution. Testing this provides a check on the exogeneity of the instrument. Figure III shows that the data is consistent with this. The left panel duplicates Figure II and is provided only for graphical comparison. The middle and right panels plot the same relationship, but with the left-hand side variable measuring support from the federal government and other sources, respectively, rather than from the school itself. 46 The coefficients on the latter two funding sources are near zero, with the federal expenditures in particular being very precisely estimated near zero. [ Figure III about here.] Since the football season cannot impact support that has already been budgeted, football should not impact contemporaneous or past expenditures. Figure IV shows the data is consistent with this, too. Once again, the left panel duplicates Figure II. The middle and right panels plot the instrument against contemporaneous and lagged school-funded research support expenditures, respectively, and unlike the left panel, reveal a slope of very near zero. [ Figure IV about here.]"}, {"section_title": "Expenditure timing", "text": "We assume that fiscal year \u03c4 football season mainly funds research in fiscal year \u03c4 + 1. That is, expenditures are lagged one year.  Table II. The contemporaneous effect of the instrument lies close to zero (which was already shown in the exogeneity check above). The once-lagged instrument effect is nearly twice as large as any other lagged effect, and is the only one statistically difference from zero at 95% significance. Over time, the effects tend toward zero. [ Figure V about here.] These results square with intuition. The season concludes in the first days of January, providing nearly six months for the subsequent year's budget to reflect cash flow changes affected by the team's success. Moreover, budgets replenish at the start of the fiscal year, which conveniently corresponds to a period when students are on break. Thus, with survey data indicating funds are the most frequent bottleneck in the research process 47 and with classes not in session, much of the year's research may take place in early summer, and is certainly unlikely to be delayed to another period."}, {"section_title": "Main Results", "text": ""}, {"section_title": "Articles and Articles Citations", "text": "We begin the discussion of our main results with the impact of expenditures on publishing. Table III reports these results. The controls and standard errors match the prior table (and future tables as well). The first four columns consider only natural science articles and their citations, while the following eight columns consider articles in the STEM fields. A striking feature of the results is that virtually all OLS specifications yield estimates near zero. This result is not new. Adams and Griliches [1998] found results with similar magnitudes, but with less power, with an analogous dataset covering an earlier period. Equally striking is the sharp contrast of these results with the 2SLS estimates, which range between three and fifteen times larger. The F-statistic on the excluded instrument that generates the first stage variation is just under 20, while the R 2 statistics never fall below 0.98-unsurprising due to granularity of the controls and approximately normal distribution of the log values of both the right-hand and left-hand side variables. [ Table III about here.] When the article measures are restricted to natural science fields, the elasticities on the paper and citation counts are 0.16 and 0.28, respectively. When we extend these measures to include all STEM fields, which factor in technology, engineering, and medicine alongside the natural sciences, the elasticities drop in magnitude and significance. Unfortunately, the new output measure is dominated by the medical fields. Since eleven schools in the panel lack a medical school, 48 cross-sectional comparisons are hurt, so to restore them, we run a second set of regressions restricting the panel to universities with a medical school. This cuts the sample size by more than a quarter, decreasing first stage power and pushing the F-statistic below ten, although the resulting elasticities now match those found in the first four columns. These figures also provide a stark contrast to prior 2SLS estimates specific to federally-funded expenditures. Jacob and Lefgren [2011] show that NIH grants worth $1.7 million translate to less than one additional publication over the following five years. 49 According to Table III, one publication would cost only around $250,000 if institutional funds are used. What drives the difference? Jacob and Lefgren [2011] argue that their low estimates are due a competitive funding environment: projects that marginally lose NIH funding due to a statutory quirk easily find alternative sources of money. Our estimates, on the other hand, reflect internally-allocated funds. The committee awarding funds internally may have access to more or more timely information, or \"softer\" information that allows them to select out inherently promising projects that do not look appealing in the arm's-length federal grant process. For these projects, a failure to be internally funded means the research is not conducted. 50"}, {"section_title": "Patents and Patent Citations", "text": "Our next set of results focus on patenting behavior. 49 Payne and Siow [2003] use alumni representation on the US Congressional appropriations committees to derive variation in federal university funding. Their instrument drives research support and other expenditures, so they interpret their coefficients as \"the total change in output when an institution obtains an additional unit of the input that may be correlated with other inputs that affect the output measure.\" 50 This would square with results obtained by Agarwal and Hauswald [2010], who show that proximity of branch banks to their borrowers facilitate the collection of soft information and that these information is use strategically. measure and increasing returns-to-scale for our patent citation measure. When we include the missing values and treat them as zero, we regress on the log of one plus the outcome measures. Regardless of whether or not we estimate the coefficient on an indicator variable for these missing values, the 2SLS specifications yield similar results: there are nearly-increasing returns-to-scale for our patent count measure and strongly increasing returns for our patent citation measure. Power is not a problem. All but one of the coefficients is significant at the 5% level, with the final column's coefficient significant at the 1% level. Despite the shorter panel, the R 2 values are still around 0.85 to 0.9. The first stage F-statistic is above ten when the missing values are included, although it falls slightly below when they are omitted. As with the article-related measures, 2SLS estimates are much higher than analogous OLS estimates-3.3 to 8.2 times higher, depending the specification. [ For patents, it may be helpful to translate these elasticities into costs in levels. Since the Cobb-Douglas functional form specified in Section IV is non-linear in the inputs and therefore does not have a constant level cost, we take two approaches: convert the elasticities to levels, or re-estimate the first and second stage in levels rather than logs. (The latter is misspecified, at least with respect to our assumed production function.) In lieu of a more detailed discussion of their relative merits, it perhaps suffices to know these yield roughly equivalent results. Using the last year of data, the cost of generating a patentable idea is between $2.51 million and $2.63 million. Using the full panel, which averages across years where patenting was much less common, 51 the cost is between $3.78 million and $3.97 million. The remarkable thing here is that these figures are almost identical to-but still lower than-the cost-per-patent of private sector pharmaceutical and biotech firms (estimated by Azoulay, Graff-Zivin, Li, and Sampat [2014]). They cover a slight earlier time period and find a cost of $4.35 million."}, {"section_title": "Technology licensing", "text": "The more fundamental question underlying these results is whether the pool of knowledge generated by research support expenditures actually creates valuable technology. We take a revealed preference approach. That is, we use the fact that a straightforward lower bound for value is what unaffiliated, private-sector firms are willing to pay for these innovations. These amounts are reflected in university technology licensing revenue. The availability of this measure is a convenient consequence of the fact that universities rarely commercialize their inventions, resulting in intermediate transactions that price the output of the research. This sidesteps the \"major difficult[y]\" of measuring R&D output [Griliches, 1979], and while it limits our ability to study the later stage of this process, policy discussions focus almost exclusively on the \"research\" portion of \"research and development\" anyway. Moreover, the measure is salient to decision-makers. Jensen and Thursby [2001] show that both university administrators and technology transfer office employees rank licensing revenue as the most important outcome of their work (compared to the number of agreements executed, inventions commercialized, sponsored research, and patents awarded). Faculty rank it second only to sponsored research, although this gap has likely dissipated since their survey was administered in the late-1990s. Panel A of Table V reports these results. A one percent increase in research support generates roughly $30,000 in upfront licensing revenues. The estimates are robust to changing specifications. Failing to treat missing values as zeros reduces first stage power but moves estimate by only about 12%. Estimating a dummy on those missing values had little or no effect. Licensing revenues feature more right skew than our other output measures, although outliers are not driving the results. Winsorizing the top 2.5% revenue slightly increases coefficient estimates while decreasing standard errors only slightly. [ Table V about here.] These translate into large returns at the margin. Dividing these revenue changes by 1% of the mean and median values of school-funded research support expenditures yields returns of 4.0% to 5.1%, respectively. Re-estimating the first stage in terms of levels rather than log values yields returns of 5.3%. It is important here that our revenue measure has stripped out running royalties, i.e. revenues related to product sales, as well as litigation settlements (where estimates were available). Scaling up the above estimates by the median ratio of upfront-to-total licensing income yields returns of 9.0-9.4%. Re-estimating the first stage in levels and re-estimating the second stage in total licensing income similarly yielded slightly higher returns-roughly 15%-although without much precision. These estimates reject the claims of STEM funding critics who argue marginal returns are low."}, {"section_title": "Output timing", "text": "Our estimates above reflect a particular temporal relationship between inputs and outputs in the knowledge production function. The lag structure makes strong assumptions on timing to simplify the model but, to be transparent, captures only a part of the unobservable, heterogeneous delays in the research process with which may readers are no doubt familiar. The alternative is to include a set of distributed lags, to which this panel simply does not lend itself. The data below, however, suggest the model captures the first-order effects of expenditures on publishing, patenting, and licensing activity. For researchers in the social sciences, the timing may be unfamiliar and seem fast, so background on the process is provided alongside the results. Since faculty are strongly advised to protect their inventions as soon as research outcomes provide proof of concept, we begin there. Maximum intellectual property protection requires submitting information disclosures ahead of seminars, conference talks, and working papers. For example, Virginia Tech's TTO advises faculty, \"If you are going to publish or present your idea... let us know in advance. We can file a provisional application to keep the idea protected.\" 52 Unlike academic publications, the measured date of a submission does not include a delay, since priority dates of applications reflect filing dates. Moreover, applications are fast to draft: as a pointed example, several applications for inventions aimed at increasing airline security were filed within days of September 11, 2001, with at least one of those filed by an individual on the date of the attack itself, and another filed by Honeywell employee on the day after. 53 Thus, initial patent filings should follow expenditures closely. It is somewhat surprising that the coefficients fall rather sharply afterwards, although the twice-and thrice-lagged coefficients are still positive and their wider confidence intervals permit the one-lagged coefficient. Moroever, our results mirror earlier work [Pakes andGriliches, 1984, Pakes andSchankerman, 1984]. For example, Hall et al. [1986] studied manufacturing industry investments to R&D from 1972 to 1979 and stated, \"R and D and patents appear to be dominated by a contemporaneous relationship.\" [Figure VI about here.] Concurrent with patent filing, faculty, TTO staff, and third-party brokers begin to contact potential licensors. For example, Stanford University's Office of Technology Licensing states, \"Concurrently with making the patent decision, [a Licensing Associate] will market and, if successful, begin licensing negotiations with potential licensees.\" In fact, there is some evidence that this process may begin at or even ahead of initial patent filings-at least for well-connected faculty and TTO administrators. For example, Stanford's OTD also advises researchers that, \"it is desirable to have an interested potential licensees [hic] before committing to patent filing.\" 54 As to the specific timing, Virginia Tech's TTO advises, \"Under the right circumstances, especially when there is an immediate interest in an invention disclosure the process of protection and licensing can be accomplished in a matter of months or even weeks.\" 55 Figure VII reports the lagged effects of the instrument on licensing. In line with the TTO statements above, technology transfer revenue increases alongside patent filings, i.e. it rises sharply one year after instrument values are realized. Unlike patent filings, however, the rise persists into the subsequent year. On the one hand, this suggests the strict timing assumptions miss some portion of the R&D return. On the other hand, the confidence intervals on this coefficient admit zero, and more than half of this effect can be explained by the fact that instrument values impact expenditures in this year as well. [ Figure VII about here.] Since it takes between one and four years for a patent to be granted, the data imply that firms are licensing technology prior to the United States Patent and Trademark Office (USPTO) granting protection. This is consistent with TTO statements above but still somewhat surprising. In their conversations with us, though, TTO professionals agreed with this point explicitly. One administrator from a Top 10 research institution stated, \"[The] majority of technologies are licensed before patents are granted-often companies license the technology just on the provisional patent application.\" This is particularly informative because provisional applications are in effect for at most twelve months. The data also imply that firms are licensing relatively early stage projects, in line with the findings of Jensen and Thursby [2001]. When they asked TTOs what stage inventions were in when they were licensed, 48% replied \"proof of concept but no prototype\" while 29% replied \"prototype available but only at lab scale.\" This finding, nonetheless, is at odds with work by Gans, Hsu, and Stern [2008]. 56 Using a sample of technology licensing deals between start-up innovators and downstream firms, they find that licensing increases at the grant date and that the majority occurs after the grant date. This implies inefficiencies in the licensing market due to either information asymmetries, search costs, or a need to disclose complementary unprotected knowledge. A number of factors can explain this conflict. Start-up firms with private information about the value of their idea may have trouble signaling it credibly. Research universities in our panel, however, have a much higher probability of survival and by definition a longer track record. Their reputation is important, and they can rely on it to sidestep this issue. Relatedly, their long history makes them less likely to be overly optimistic about the likelihood of a patent being granted. The disclosure of complementary but unprotected information is less likely to harm universities, whose projects are likely differentiated from downstream labs, at least relative to start-up firms. Search costs are less of an issue 55 \"Frequently asked questions.\" Virginia Tech Intellectual Properties. http://www.vtip.org/downloads/VTIP%20FAQ.pdf (accessed September 10, 2016). 56 We thank Alberto Galasso for pointing this out. in our setting, since universities have staff dedicated to promoting and licensing new inventions. Also, their panel covers 1990 to 1999 while ours covers 1995 to 2011 (with the bulk occurring in the latter period). Digitization and the internet have lowered search costs, so the efficiency of the IP market has improved over time. The last step is for faculty publish their findings for academic audiences. Relative to the social sciences, the STEM publication process is very fast. In these areas, research is quicker and papers are shorter. It is not uncommon to see junior faculty with over a hundred publications. For a back-of-the-envelope illustration, we combined all NSF grants for years 2000, 2005, and 2010, and found 26% of physics-related proposals generated papers in the grant year while only 5.4% of economics-related proposals did. For grants that resulted in any publication at all, 46% of physics-related proposals generated papers in the grant year while only 13% of economics-related proposals did. Review time is also much shorter: the submission-to-acceptance time for manuscripts published in non-social science journals is around a third of those published in economics journals. For example, the average time between first submission and final publication was between 13 and 22 weeks at the five main journals of the American Physics Society (Pattard [2010]) but 62 weeks at the American Economic Review (Moffitt [2009]). 57 Also, natural science disciplines in particular tend to rely more on short papers, proceedings, and letters, which have much shorter review times. For example, \"rapid communication\" section of the five main journals of the American Physics Society has an average receipt-to-acceptance time of between 9 and 15 weeks and a minimum of only two days. Also, note to a data limitation, publications are measured on a calendar year basis (while the other variables are measured on a fiscal year basis). This adds an additional six month gap between funding and publishing. Finally, from Section 5 that much of this money may be spent very close to the beginning of the period-some of it perhaps right when the fiscal year turns and research budgets are replenished. This also adds an additional (mean) gap between expenditures and articles. Figure VIII reports the lagged effects of the instrument on publishing. It supports the notion that unlike patenting and publishing, publications do not respond immediately. Working papers and conference proceedings take longer to write than invention disclosures and, legally, should follow them anyway. Instead, there is a sharp, statistically significant rise in the year subsequent to the expenditure spike. As with licensing, the effect seems to persist for a year before tending towards zero. Once again, however, that effect is not statistically significant and is mainly explained by the impact of the twice-lagged instrument on expenditures. [ Figure VIII about here.]"}, {"section_title": "Input measurement error", "text": "The ratios of OLS to IV estimates suggest a severe errors-in-variables problem. On the one hand, measurement error in research expenditures accounts for only about 3% to 4% of total right-hand side variation. On the other hand, it accounts for 60-90% of the residual variation that remains after accounting for school and year fixed effects and the school-specific time trends. Several factors contribute to this, many of which are generally present in production function estimation. First, since the NSF expenditure categorizations differ from financial reporting standards, the data simply cannot excerpt from audited statements. The NSF asks for only science and engineering research expenditures whereas the universities publicly report total research expenditures, so even \"top line\" expenditures figures may require a judgment call on behalf of the respondent. This problem surely worsens as the survey questions become more detailed, e.g. require discipline-level expenditures by source. It was apparent from the data that the demarcation between biology and medicine is particularly difficult for respondents. Anecdotally, these demarcations often reflected the geography of the campus, e.g. determined by the proportion of biology equipment or staff housed within medical center facilities. It was also apparent that this demarcation shifted at times within the institution. Again anecdotally, this frequently reflected changes in which university staff were filling out the NSF form. Second, although there is a large penalty for not returning the survey, there is little harm in making mistakes. In some cases, we spotted obvious errors in even aggregate expenditure figures. To the credit of the NSF staff, when we followed up with them, it was clear they were aware of the mistakes and often had already queried the schools about them. Presumably we saw the selected set of errors that were simply too difficult to resolve. On the one hand, cost accounting is hard and time consuming, so expecting a painstaking \"re-audit\" of a large portion of outgoing university cash flows is often too much to ask. On the other hand, as we show, errors can badly bias the estimates of research productivity that rely on them, shifting public policy in undesirable ways. This problem extends, to varying degrees, to many other settings and survey instruments like heavily relied upon Census publications (e.g. the Annual Survey of Manufactures). Our results raise questions of whether incentives to accurately report are large enough for datasets used in \"high stakes\" policymaking. The rich set of controls deliver the sharp difference in the proportions of total and residual variance reflects the rich set of controls. Fixed effects, for example, sweep away the large persistent expenditure differences between, for example, the University of Michigan and Boise State University, or as another example, the first and last years of our panel. The inclusion of school-specific trends strips out even more variation. Combined with the fact that university budgets are fairly stable and predictable over time (except for a small amount of variation from activities like athletics), it is not surprising that what is left is mostly noise. The instrument selects out the portion of variation that is separate from noise."}, {"section_title": "Conclusion", "text": "Unanticipated college football outcomes shift research support expenditures but not faculty or facilities. Since they do not otherwise impact research productivity and are uncorrelated with right-hand side measurement error, these shifts provide helpful variation to identify the impact of money on science. We use this variation to measure the dollar elasticity of scholarly articles published, new patent applications, and the citations that accrue to these. The elasticities derived from our 2SLS specifications are sharply higher than both the OLS estimates and prior work studying the causal impact of federal support. We also use this variation alongside data on university technology licensing revenues to provide the direct evidence that the pool of knowledge created by academic research, at the intensive margin, generates valuable technology. These results suggest fruitful avenues of future research. First, these results indicate that internal funds generate higher returns than those coming from federal sources. This may reflect an information advantage of internal funding sources relative to an arm's-length external grant process. Although this would square with prior finance research on branch banking investments [Agarwal and Hauswald, 2010], clear evidence of this advantage in our context would suggest changes to the funding allocation process. Second, the plausibility of our exclusion restriction and sharp contrast between our OLS and IV estimates suggest substantial right-hand side measurement error, an issue that prior work (e.g., Adams and Griliches [1998], Griliches and Mairesse [1998]) suspected was a large problem but was unable to estimate without quasi-experimental expenditure variation. More work-presumably dedicated to finding cost shifters-is needed to determine the scope of this problem for other inputs, datasets, and applied settings. Third, we cannot rule out \"business stealing\" effects across projects. Races to answer outstanding questions can result in duplicate research efforts that result in modestly faster discovery but do little to move the knowledge frontier. In this case, investments can be socially wasteful while still privately beneficial. Understanding the magnitude of this effect is critical to determining how much care should be exercised in coordinating the funding process across related projects. Fourth, we do not explore potentially interesting interactions with co-authorship relationships and incentive schemes, which further help unpack the knowledge production function. Finally, our research design precluded us from unpacking the potential underlying heterogeneity in these effects across institutions, time, and fields. Each carries substantial policy relevance for economies that increasingly rely on universities to generate R&D-led growth."}, {"section_title": "A Robustness of instrument", "text": "Votes are bounded from below by zero and from above by 25 multiplied by the number of voters. Thus, teams that receive zero preseason votes have a weakly positive expectation in their vote change, and teams that receive close to the maximum number of preseason votes have a weakly negative expectation about their vote change. Thus, the instrument may depend inversely on preseason votes. If team quality-as measured by preseason votes-is partly determined by the same institution-specific and time-varying (de-trended) unobservables that also drive research productivity, then 2SLS estimates are biased downward. A simple test, described below, mitigates or eliminates this concern. The reduced form relationship between the instrument and outcomes of interest is where \u03c0 i , \u03ba t , and \u03bd i represent the school fixed effects, year fixed effects, and school-specific time trends. Instead of prior assumptions on z, let z comprise unexpected changes in team quality, denoted z , and the mechanical relationship, denoted \u03c1, with preseason votes, denoted V. In this case, Equation 6 becomes where, for ease of notation, W i,t \u03a0 = \u03c0 i + \u03ba t + \u03bd i t. At issue is whether the presence of \u03c1(V(\u03be i,t )) results in bias. This requires W to insufficiently control for unobservables, V to reflect these unobservables, i.e."}, {"section_title": "E[V(\u03be)|\u03be] E[V(\u03be)]", "text": ", and the boundedness of total votes to actually bind, i.e. \u2202\u03c1/\u2202V 0. To test this, we include functions of V on the right-hand side, which can approximate the function \u03c1, and compare the estimated coefficients against those from the main specifications. For simplicity, assume \u03c1(\u2022) is additively separable in functions of V. If, for example, we temporarily let \u03c1 be linear in V, so that \u03c1(V) = \u03c1 1 V, we can re-write Equation 7 as where \u03b8 1 = \u03b8 and \u03b8 2 = \u03b8\u03c1 1 . Substituting in for z yields When boundedness is a problem, and \u03c1(V) is linear in V, Equation 8 shows that the parameter of interest is \u03b8 1 . The test for bias compares the coefficients on z recovered from taking Equations 6 and 9 to data. For robustness, we consider alternate specifications for \u03c1(V). These include \u03c1( We also consider analogous robustness tests on the first stage regression. Table VI reports these results. The first column considers the first stage regression, while the second, third, and fourth columns consider the relationships between the instrument and the log of citationweighted STEM articles, the log of one plus patents, and licensing revenue, respectively. Each estimate is the result of a separate regression of the outcome measures on z, i.e. postseason less preseason votes (un-standardized across polls). In the first row, only the standard set of controls are included-school and year fixed effects and school-specific time trends. The remainding rows include functions of V on the right-hand side. In particular, the second, third, and fourth rows let [ Importantly, all estimates from our main specifications are close to those obtained in rows 2-4, and on average differ by only about 7.5%. This supports the notion that the large set of controls has removed most of the unobservable heterogeneity. The leftover (school-specific and time-varying de-trended) residuals are either small or not reflected in preseason votes. There is some heterogeneity across outcome measures. They differ only by roughly 1-3% when output is measured in articles, and as much as 13% when output is measured in patents. More flexible functions of V did not yield very different results. If residual, unobservable heterogeneity does persist, it is likely positively correlated with preseason votes, which are slightly inversely correlated with the instrument, resulting in downward bias. The data supports this. Adding V or functions thereof always returned higher estimates. Taken together, these results suggest relatively few complications caused by the vote truncation. An alternative instrument could be constructed as a residual from a regression of z on V, or functions thereof. The result is an instrument that relies more on z than z; this would help statistical power throughout the paper, but in the interest of transparency, was not used."}, {"section_title": "B Supplementary TTO specifications", "text": "Using additional survey responses in the AUTM survey, we can rule out that higher licensing revenues are driven by investments in the technology development office itself. In particular, the office might hire additional staff or retain better counsel. If, for example, the additional staff market and sell university intellectual property, or high quality attorneys provide expertise in reaching firms and writing lucrative contracts, then licensing revenues will rise even in the absence of increases in innovative output. With respect to the first, AUTM collects data on the number of employees \"whose duties are specifically involved with the licensing and patenting processes,\" i.e. \"licensing\" employment, and the number of employees \"whose duties are to provide professional, administrative, or staff support of [TTO] that are not included in [licensing employment],\" i.e. \"other\" employment. With respect to the second, AUTM collects data on the total legal fees expenses by the university TTO. Table VII reports the impact of the instrument on these outcomes. Panel A covers employment while Panel B covers legal expenses. Regardless of the left-hand side measure or specification, OTD staff and legal fees are unaffected by unexpected college football outcomes. A one-standard deviation increase in the instrument results in never more than a roughly 1% increase in TTO employment or legal expenses. The confidence intervals are tight around zero, except for those in columns 2 and 5 of Panel B, which treat missing responses to the legal fee survey question as zeros. It is likely they are not zeros, since estimating a dummy for missing values in the subsequent columns results in estimates that are close to those of columns 1 and 4, respectively. [ The relationships between the instrument and the output measures are reported in Figure IX. The y-axis in the top, middle, and bottom panels are measured by the log of citation-weighted STEM articles, the log of one plus citation weighted patents (with a dummy estimated for missing values), and licensing revenues (with a dummy estimated for missing values), respectively. That is, the plots reflects analogs of the final columns of Tables III, IV, and V, respectively. The estimating equations are given by where y is the specified outcome. [ Figure IX   Plotted values are residuals after controlling for school fixed effects, year fixed effects, and school specific time trends. Observations are \"binned\" according to their x-axis values.     Table IV. The specifications include the full set of controls and cluster the standard errors at the school level. The x-axis measures the lag of the instrument used, e.g. \"0\" denotes the impact of the contemporaneous instrument values, \"1\" denotes the impact of once-lagged instrument values, et cetera.  Note. -The x-axis measures the log of school-funded research support expenditures. The y-axis measures the log of citation-weighted STEM articles in the top panel, the log of one plus citation-weighted patents in the middle panel, and licensing revenues in the bottom panel. The plotted values are \"binned\" according to x-axis values. The left panel pools all observations. The right panel plots residuals after controlling for school fixed effects, year fixed effects, and school specific time trends. Note.-There are a maximum of 880 observations, i.e. forty schools and 22 years, for the full panel and 640 for the patenting and licensing variables, since that data begin in 1996.   -The right-hand side variable is the predicted values of the log of school-funded research support expenditures. The left-hand side variables are the log values of either the count of articles published or the citations that accrue to those articles. All specifications include school fixed effects, year fixed effects, and school specific time trends. Standard errors are clustered at the school level. The last row reports the F-statistic for the (excluded) instrument in the first stage. The final four columns restrict the sample to universities that have a medical school; the lower first stage power reflects the smaller sample.  "}, {"section_title": "Tables", "text": "-The right-hand side variable is the predicted values of the log of school-funded research support expenditures. The left-hand side variables are technology licensing revenues. The top panel uses \"unwinsorized\" data while the bottom panel winsorizes the highest 2.5% of values. Missing values are treated as zeros, but dropped in Columns 1 and 4. All specifications include school fixed effects, year fixed effects, and school specific time trends. Standard errors are clustered at the school level. The final row reports the F-statistic for the (excluded) instrument in the first stage. The lower F-Statistics in Columns 1 and 4 reflect the smaller sample size.  -The right-hand side variable is unexpected college football outcomes in standard deviations. In Panel A, the left-hand side variable is employment in the technology development office. The first three columns reflect only licensing employees, while the second three reflect other employees, including management, accounting, compliance, and related roles. In Panel B, the left-hand side variable is the technology development office legal fee expense. The first three column use a level measure, while the second three use a per-patent expense measure. Missing values are treated as zeros, but dropped in columns 1 and 4. All specifications include school fixed effects, year fixed effects, and school specific time trends. Standard errors are clustered at the school level. "}, {"section_title": "D List of universities (For online publication)", "text": ""}, {"section_title": "E Patent dataset construction: Allocating filings to universities (For online publication)", "text": "Identifying cohesive patent portfolios and patent applicants and assignees can be a difficult task. Numerous variations in names of patent-seeking institutions appear in USPTO records caused by either the variation in patent-prosecuting law firms or human errors and incorrectly spelled names. For example, there are 157 variations of assignee/applicant names grouped under the \"University of California\" umbrella in our Thomson Innovation patent sample. These names range from \"The Regents of the University of California\" to \"University of California Berkeley\" and \"University of California Los Angeles.\" In addition, to better identify university owned patents and patent applications, we utilize DWPI assignee classification available in Thomson Innovation: a unique 4-letter identifying code assigned to approximately 22,300 international patentees. For example, The University of California is assigned a unique 4-letter code \"REGC\", and in order to retrieve all patent records assigned to the University of California, we query Thomson database for all variations of assignee/applicant string grouped under \"University of California\" and associated with \"REGC\" assignee code for earliest patent priority years 1996-2011. It is important to note that, while we collect patent data starting with 1987, our panel officially starts in 1996. We start the panel in 1996 because of the effects that the international harmonization of the Unites States patent system in early 1990's had on university patenting behavior. As shown in Appendix Figure X, one of the patent law amendments with a significant impact was the introduction of provisional patent applications in June 1995. 58 [Figure X about here.] Since the introduction of provisional patenting provided a convenient solution for academic researchers faced with publish-or-patent-first dilemma, it resulted in a sharp increase in university patent applications. A published article, a conference presentation, or even as much as a conversation describing an invention before a patent is filed represents a public disclosure, and can deem that invention unpatentable. To the extent that the scientific work in academia is first and foremost driven by article considerations in peer-reviewed journals, provisional patent applications are an exceptionally good fit for this environment as they enable the university to lock an early priority date, while providing additional 12 months for inventors to publish, disseminate and improve the invention. Universities use provisional patent applications to reduce uncertainty surrounding market value of inventions and make a more informed decision of whether to prosecute full patents. Indeed, many university Technology Transfer Offices laud provisional patent applications as the first order of business after being informed of a new invention. For examples, see the Prevalence of provisional patenting in academia was the main reason behind our decision to stop our panel with patent applications filed in 2011. Since provisional patent applications take 12 months before they are published, patent application data from 2012 would be missing all provisional patents applied for in that year, and would result in a severely truncated patent count. We use priority dates rather than application dates to count patent records because priority years most closely correspond with the date when the invention was first applied for. While the patent priority date is most often no different than the regular patent application date, in a case of a converted provisional application, a priority date will be earlier than the regular application date. This is especially the case when a divisional or a continuation application was filed. In addition, we use the DWPI Patent Family list available in Thomson Innovation database to assign all retrieved patent records to unique groups sharing the same priority application. This enables us to more closely identify patent groups surrounding the same invention and ensures that we do not overcount patent records in the sample. Each DWPI Patent Family is counted only once, and all forward patent citation counts are aggregated on a DWPI Patent Family level. To further exacerbate the problem of allocating patent applications to universities, some university systems do not specify campus locations where the invention was made when filing their patents. For example, almost 75% of all patent applications from University of California System in our sample are assigned to \"The Regents of The University of California\" without any additional information about invention-originating campus. Consequently, we do not know if the invention was made at The University of California at Berkeley, The University of California at Los Angeles, or any other campus in the system. Since our instrument works on the individual campus level only, and does not propagate through the whole system, we need to allocate patent applications to individual campuses within the university. In other words, unexpected success of The University of California at Berkeley football team will not impact R&D expenditures at The University of California at Los Angeles, and vice versa. To rectify this problem, we use the inventor's home address information provided on US patent records and use Google Maps API to calculate the \"by car\" estimated travel time from inventor's home address to every campus in the university system. We then systematically examine the patent portfolio and count a patent as originating at a specific university campus if at least one inventor lives less than 26 minute drive from that campus. 60"}, {"section_title": "F Licensing data construction: Supplementing AUTM (For online publication)", "text": "Licensing revenues are derived from survey responses as described in Section 3. In a few cases, the data had problems with over-aggregation, loss of continuity, or missing/erroneous values. The majority of problems were easily resolved, since many universities have published and made available annual reports for their TTOs. In the remaining few cases, we contacted university administrators, who often could resolve the problem. Each change to the AUTM data is described below so that our licensing revenue dataset can be replicated exactly from the original AUTM survey responses. (Prior versions of this paper took a less comprehensive approach. For example, UCLA was omitted since it was not distinguished from the University of California system in AUTM, although annual reports are available from the state system that distinguish the schools' revenues from one another.) Aggregation. AUTM data provides separate data for The University of Texas at Austin until 2008, but in 2009 starts aggregating all licensing at the University of Texas System level. However, The University of Texas at Austin maintains a separate Office of Technology Commercialization, which publishes independent reports. These reports state that licensing revenues were $10. 9MM, $14.3MM, and $25.6 million in 2009, 2010. As stated previously, AUTM data has always aggregated the University of California schools up to the system level. However, the University of California Technology Transfer program publishes annual reports that separate the schools' revenues. These reports state that licensing revenues for UCLA were $7.468, $8.383, $10.118, $10.969, $13.963, $19.488, $18.880, $20.911, $32.827, $22.557, $27.485, $16.153 million for fiscal years 2000 through 2011, respectively."}, {"section_title": "Missing licensing revenues.", "text": "When observations were missing from the AUTM data altogether, we contacted the schools for the relevant data and/or annual reports covering these periods. AUTM did not provide data for Stanford in 2005. However, the Stanford Office of Technology Licensing publishes an annual report. The 2005 report states that revenues were $48 million, net of a $336 million payment related to Google shares, of which $1.5 million was passed through. The remaining cases where the AUTM data does not provide any licensing data are treated as missing values."}, {"section_title": "Missing revenue breakdown.", "text": "In a few cases, itemized below, AUTM provides total income but does not separately provide running royalties and pass-through revenue. In these cases, we impute the running royalties based on the average ratio of running royalties to total income for that school. Note that this imputation is, in effect, only a school-specific scaling and does not \"fill in\" top-line values. We did this for the cases listed above, where total licensing was obtained from an annual report. The other cases where running royalties were omitted include Arizona (2005), Clemson, (2001), Florida (2005, Illinois (1996, 2000), Louisville (1999, 2010, and 2011), Mississippi (2011), Stanford (1997Stanford ( , 1998Stanford ( , and 1999, Texas A&M (2005), Virginia Tech (2010)."}, {"section_title": "Errata.", "text": "AUTM data for the University of Colorado erroneously included legal settlements in total licensing income and did not classify a monetization of running royalties as such. The University of Colorado Technology Transfer Office has published and made available annual reports back at least to 2002. The 2004 report states that its figures exclude $28.1 million of a $34.8 legal settlement from Wyeth Corporation (Table 1, Footnote 1). Colorado seemingly added this amount to cashed-in equity, which rose to $28.5 million, despite a zero value in the prior six years. After this adjustment, the AUTM total income figure is $5.6 million and almost identical to the $5.8 million found in the annual report. The 2005 report states the remainder of the settlement, $6.7 million, was realized in that year. After that adjustment, AUTM data comes very close to the annual report figures, e.g. a $300,000 difference in 2003, a $200,000 difference in 2004, a $100,000 difference in 2008, and a $100,000 difference in 2009, except for three consecutive, consistent differences of $16.9, $15. 5, and $16.7 million, respectively, for 2005, 2006, and 2007. The 2007 report states that these figures reflect \"a three-year monetized royalty stream\" and a \"medical diagnostic license that produced over $12 [million] royalty the past few years,\" so we adjust by these amounts. Similarly, AUTM data for the University of Washington erroneously did not classify a legal settlement on running royalties as such. Total licensing income less running royalties is $18 million in 2007, despite that figure consistently falling between roughly $4 and $8 million for the twelve surrounding years. This likely reflects a well-publicized $15 million settlement with CSR plc over Bluetooth technology. Also, AUTM data for The University of Michigan erroneously did not classify a monetization of running royalties as such. The University of Michigan Office of Technology Transfer has published and made available annual reports back to at least 2001. Their 2015 report states that \"the spike in revenues in 2010 came from the monetization of royalties of FluMist, an influenza virus vaccine as a nasal mist.\" While the AUTM data matches the annual report on total licensing and reflects this large spike, running royalites in the AUTM data do not. They rise only $3 million from the priod period. There is also a large spike in the AUTM data in 2008. The changes are reflected in the AUTM variable \"License Income: Other Sources,\" which is typically stable at between $1 million and $4 million, but increase sharply by $14 million and $20 million in 2008 and 2010, respectively. The FluMist monetization deal was completed in the third quarter of 2007 and totaled approximately $35 million, 61 which is very close to the cummulative increases for 2008, the year after the deal was completed, and 2010, the year stated in the 2015 annual report. Thus, we adjust by $15 million and $20 million, respectively, for these years. AUTM data for the University of Wisconsin likely erronenously did not classify a legal settlement on running royalties as such. The Wisconsin Alumni Research Foundation (\"WARF\"), which manages the intellectual property for the university, filed well-publicized, related lawsuits against Sony in 2003 and IBM and Samsung in 2004, and these settled throughout 2004 and 2005. A spokesman confirmed for us that the sharp increase in non-running revenue in the AUTM data in 2004 and 2005 was due to litigation. Confidentiality of the settlement precluded them from providing the correction, and no estimates are available in the press. Without any basis to estimate the correction, we simply drop those two observations. 61 Begin, Sherri. Crain's Detroit Business, July 16, 2007, \"UM sells vaccine royalties.\" Note.-A legislative change both generates a one-time spike in the data in 1995 and fundamentally changes the nature of patent filing activity. The vertical line marks the date of the change. Our analysis begins in the following year."}]