[{"section_title": "Introduction", "text": "In the recent past, many strides have been made towards assessing and using the implied uncertainty of echosounder systems for processing and evaluation of hydrographic data. Most of these efforts have been directed towards modern surveys on the reasonable grounds that these are most important for our future needs. However, the vast majority of hydrographic data holdings in the US are archive datasets collected with either Vertical Beam Echosounders (VBES) or leadlines and stretch back over a hundred and fifty years. These datasets are typically sparse (i.e., at the scale of the survey smoothsheet or less), and in most cases, if the original data was denser (e.g., continuous recording fathograms with paper or digital recording), this data is no longer extant. In some cases the only record is the hard-copy smoothsheet and the only digital forms of these sheets are formed by scanning and re-registering the data from the hard-copy. If we are to assess the uncertainty of the data, we have to do it based on whatever we have, with at best descriptive reports from the hydrographic effort for support. Our ultimate goal is to construct grided bathymetry from the archive data that is compatible with the forms being used for modern surveys, i.e., a depth and uncertainty pair. This inevitably leads to interpolation and hence we must also account for uncertainty amplification caused by the methods that we use, in addition to the basic measurement uncertainties of the data and that engendered by sub-sampling the dataset before committing it to the archive. In the remainder of this paper, we consider these error sources in detail and show methods for their estimation. As an example, we consider an area off the east coast of the US where we have leadline (ca. 1936-38), VBES (ca. 1975-76) and EM1000 MBES (1996, [17]) data to compare. We then conclude with some observations on the generality of these methods, including their use with dense modern VBES data."}, {"section_title": "Methods", "text": "We aim to construct a surface from the archive data so that it can be used in the same way as current generation systems [23,24]. However, in doing so we have to interpolate the data in some form, introducing another error source due to the measurement errors inherent in the data (which we do not know a priori). We also move, literally, into uncharted territory, since we have no real knowledge of what is between the soundings. In this case, we must estimate our best understanding of the depth and its associated uncertainty. We assume that we know the technology used and since these change infrequently, we may also assume that the achievable accuracy of the survey instruments should change slowly given the platform. Therefore, if we can determine a location where we have overlap between modern high resolution data and archive data of suitable vintage, we can use the former to calibrate the latter, and hence determine the actual accuracy of the archive surveys, at least in terms of the increase of uncertainty as a function of depth. With the mild assumption that the survey instrumentation has not changed, we can then apply this model elsewhere that the same survey platform operated, providing a much wider degree of applicability. This measure reflects the vertical error of the data, but says nothing about the horizontal errors. Again, we appeal to temporal continuity and assume that we can assess the errors of the horizontal positioning for a survey by knowledge of the equipment in use at the time. Since we are concerned primarily with vertical incidence sounding in relatively shallow water, we may also assume that the positioning error of the sounding is approximately that of the positioning system itself, avoiding any amplification due to off-nadir refraction of the single beam. This may not be valid in all cases, and particularly where there is significant local subfootprint roughness. However, it is typically not possible to assess deviations from this assumption from archive scale surveys, and we retain it for simplicity. Geostatistical techniques can be used for uncertainty estimation in addition to interpolation, and our approach is similar in spirit to Kielland & Dagbert [15], save that we must work only with the archived data. We are assisted by the realisation that a hydrographic survey is not conducted at random: there are very distinct reasons why the data that were preserved were preserved. Typically, a hydrographic survey also has cross-lines used to check the consistency of the soundings. Therefore, we gain some insight into the variability of the bathymetry at scales closer than might be implied by the survey scales, and can use it to determine the likely variability of the surface between the mainscheme survey lines. Since the goal is to make a surface suitable for hydrography, however, we must also be careful to ensure that we account for 'the thing unseen'; that is, there is a possibility of a geological erratic or man-made object to occur at scales less than that of the closest pair of points in the archive dataset, a situation in which geostatistics can say nothing of value. In this case we appeal to a measure of hydrographic caution and observe that we may add an arbitrary additional variability to the surface to reflect our comfort level for the area of the world and the significance of the hydrographic regime. We pursue this in the same geostatistical framework using a decomposition of the overall surface into microscale and macroscale roughness. It is possible to express horizontal uncertainty in a geostatistical framework, but we pursue an alternative solution using a Monte Carlo approach [13] which simulates multiple virtual datasets from the archive data using an estimate of the horizontal error magnitude. The resulting datasets are processed in the same manner as the archive data, and a sample estimate of depth variation is computed and used to estimate the stability of the reconstruction of the depth surface given the density of the survey, typically emphasising slopes where any horizontal error immediately gives rise to vertical errors. A source of error we do not treat here is aliasing noise cause by sub-sampling of the seafloor both through the use of survey lines, and through subsampling of those survey lines to give the archive dataset. Aliasing is difficult to assess a posteriori, although methods for its estimation exist [21,22], and have the potential to be verified where alternative higher resolution data exists. An interesting phenomenon is that hydrographic surveys are not sampled at random: from all of the data gathered, a specific subset is chosen (in a shoal-biased manner) to best represent the most hydrographically significant detail in the area. This significantly complicates the analysis of the aliasing effects, and is the topic of on-going research. We return to the implications in section 5. The flow-path of the method is outlined in Figure 1. Inputs are marked with square terminals, and the composite outputs consist of grided depth and uncertainty in a co-registered pair. Note that although we show MBES data, it is used only to calibrate the measurement error model of the archive data, and is not required after this stage."}, {"section_title": "Datasets", "text": ""}, {"section_title": "Archive Datasets", "text": "We focus on an area of the New Jersey shelf around 39 \u2022 12'N 72 \u2022 50'W, Figure 2, where we have extant MBES data gathered in support of the US Office of Naval Research's Strataform [10] program area. Through use of a GIS developed at CCOM [14], we identified two generations of US National Ocean Service (NOS) archive survey, as indicated in Table 1. The portions of the databased surveys that overlap   with the work area were extracted from the NGDC GeoDAS database CD [20] as flat-file data, including latitude, longitude, depth (converted to meters) and survey ID. The data were positioned with respect to NAD27, and were converted using the NGDC software to NAD83. The soundings were then projected using UTM coordinates (zone 18N) and the WGS-84 ellipsoid; all data were gathered correct to Mean Low Water (MLW). The archive provided 9344 leadline soundings, of which 2813 are coincident with the study region, and 16791 VBES soundings, of which 5031 are coincident. The study area is 78.7 km \u00d7 72.0 km (EW\u00d7NS) in size."}, {"section_title": "Multibeam Data", "text": "The MBES data was collected with the CHS Creed in April 1996 [17] using the Simrad EM1000 MBES system and a POS/MV 320 motion sensor. The data was available in the manufacturer's archive format and in hand-edited form using the University of New Brunswick's (UNB) archive format. We re-processed the data from the UNB format since it incorporates empirical refraction corrections that were essential to reducing the effects of sound speed changes observed in the area; a number of artifacts remain in the data, but are of small magnitude [10] and we ignored these in this work. Data were corrected to Mean Lower Low Water (MLLW) using the NOAA tide gauge at Atlantic City (Station ID 8534720). CUBE [1,2] was used at 5 m resolution to carry out the processing, limited to at most 110 m (for stability across all datasets). A comparison of the level of detail available in the data is shown in Figure 3, illustrating the loss of detail inherent in the archive dataset. The CUBE process produces an estimate of uncertainty in addition to the bathymetry, which highlights a number of other problems with the data (Figure 4)."}, {"section_title": "Dataset Comparison", "text": "Preliminary comparison of surfaces generated by redigitising TINs of the archive datasets, separated by generation (i.e., leadline in one surface, VBES in another) showed the same general character of the geomorphology, but variability estimates generated by comparing source soundings from one archive set to the surface generated from the other showed a significant shift in mean of 1.67 m (standard deviation 1.87 m) with the leadline data being shoalest; compared against the MBES surface, the leadline is still biased (mean 1.48 m, standard deviation 1.48 m), while the VBES is approximately unbiased (mean \u22120.02 m, standard deviation 0.75 m). Geological context from the area suggests that changes in morphology should be slow; iceberg scour features in the northeast of the region are more than 10,000 yr old, and are still evident in the MBES data. The region was chosen for the Strataform study because it is sediment starved, so sedimentation is unlikely (and in any case would generally make the VBES archive shoaler). Geological opinion is divided as to whether there is migration of features on the shelf due to storm rework. A preliminary examination of historical hurricane tracks [19] from 1939 to 1974 and 1977 to 1995 suggests that there was no significant storm activity. As corroboration, this is unlikely to be a suitable explanation since the bias occurs approximately equally at all depth ranges. Storm rework would tend to redistribute the material, so a consistent bias would be unlikely. Many other explanations for the difference were considered, including tidal datum differences and epoch shifts, errors in sound speed profile determination from the VBES datasets, errors from current effects on sounding machine leadline measurements, and systematic bias in the sounding machine measurements. However, the differences in each case are small, certainly too small to explain the significant bias term. NOAA CO-OPS observations indicate that tidal level changes at the Atlantic City gauge have averaged approximately 4 mm/yr since 1900 [18] giving approximately 0.11 m of shift given the datum epochs which were pre-1924 and 1941-59 for the two surveys; at the last datum change (from 1960-78 to 1983-2001), the MLW level changed by 0.37 m, and the difference between MLW and MLLW was 0.06-0.17 m, none of which are big enough to explain the observation. Errors in   sound speed determination were probably on the order of 5 ms \u22121 in 1975-76 [26], which would only contribute approximately 0.20 m, and would be randomly distributed in any case instead of forming a bias as observed. Effects on current putting a bow into the leadline are probably small, and would (a) be compensated in measurement, and (b) would cause the line to read deep; sounding machine biases were likewise small, probably no more than 1-2%. However, examination of the differences as a scatterplot against depth, using the MBES depth surfaces, immediately suggests a more likely cause. Figure 5 shows clearly that the depths are heavily quantised, which is consistent with the attribution in the metadata that the data comes from \"Smooth sheets digitized for N.O.S. under contract.\" In these surveys, soundings were portrayed in integer fathom units (i.e., 1.8288 m), except for H06346, which is in feet, and H06345, which is in fathoms and fractions, both of which are only used to a very limited extent in the very shallow regions of the survey area (they may be seen in the reduced quantisation error in the 30-50 m depth range). However, the depths were reduced to integer fathoms according to 'hydrographic rounding rules' or tables; the NOS survey manual of the period [25, \u00a77716] indicates that surveys of this type would be carried out in fathoms using a sounding machine (i.e., a mechanical leadline), captured to fractions of a fathom in the survey log, and then rounded on the smoothsheet such that \"When the units are the same as those in the sounding record, but in integers on the sheet, any partial units shall be converted into whole units by changing 0.75 or more into the next greatest integer, and changing decimals below 0.75 into the next lowest integer unit.\" That is, we might see a shoal-bias of around 1.37 m from the transfer of the soundings on to the smoothsheet and into the archive. Therefore, we conclude that the use of an archive smoothsheet, re-digitised, is the most probable cause of the bias, and hence that the leadline data cannot be combined with the other data sources and must be dropped from further consideration in this study given we have alternative data. The VBES data, under the same comparison, shows an approximately central, unbiased (mean estimate \u22120.02 m over all depth ranges, and < 0.8% of depth in all ranges of interest) distribution of error with standard deviation of 0.75 m overall. The surveys were recorded in fathoms and tenths, so that there is little quantisation noise and rounding bias (at worst \u2248 0.1-0.15 m). Hence, we conclude that the singlebeam archive data may be compared against the MBES data for further processing.  Figure 5: Scatterplot of leadline soundings as compared to the MBES depth surfaces. Significant evidence of quantisation is observed, generated by rounding to integer fathoms on the smoothsheet, which is the source for this digital data. The asymmetric rounding of hydrographic practice causes a significant bias to develop."}, {"section_title": "Error Sources and Modeling", "text": ""}, {"section_title": "Measurement Error", "text": "Our assessment of measurement error extends directly from the distribution of VBES data about the MBES surface. Sample statistic estimates of the variance of the soundings were computed at different depths using depth bins of 5.0 m. The values in the bins centred at 42.5 m, 47.5 m and 52.5 m were censored due to the limited number of data points in them and the evidence of outliers in the sample. Further work requires a model that describes potential measurement error at all depths, and so a model of the form: corresponding to the IHO S.44 error model for soundings [12] was fitted to the data. The Levenberg-Marquadt method [16] for non-linear least squares was used to make the fit suitably robust, Figure 6, resulting in estimates of: This is approximately in accordance with the current S.44 error limits, which have \u03c3 f = 0.5 m and  k v = 0.013 for Order 1 survey. The increased base uncertainty may come from at least three sources. First, we are comparing against a surface that itself has an uncertainty, which will increase the estimate accordingly. Second, low numbers of samples in some depth ranges can lead to reduced stability of the sample variance estimator, which is not robust to small sample variations. Third, there is the potential for a small upward bias to the soundings due to hydrographic rounding and shoal-bias in selection of points in a traditional hydrographic processing chain (i.e., the soundings selected are chosen as the shoalest of all those available in the immediate vicinity, with no respect to measurement error, leading to artificial shoaling of the data). This bias will leak into a variance estimate inflation. Overestimation of variance, as long as it is not egregious, is not objectionable. This does not really affect the depth predictions to be made, just the uncertainties. If anything, this will make the predictions safer."}, {"section_title": "Interpolation Error", "text": "We aim to generate a surface from the sparse archive data; interpolation is therefore inevitable. However, since each measurement over which we interpolate has errors, we must also consider the associated uncertainty. We restrict ourselves to linear interpolators [8], so that the operation can be written in general form as:\u1e91 where is the set of neighbours of the location s, and the \u03bb i are suitable weighting values that can be described as a function solely of the location s i of the samples, and not the actual value. Then, under assumptions of normality and independence of the z i , we can predict the variance of the estimate as: (see [21]). Interpolation models make some assumption as to the nature of the underlying surface from which the data is taken. Following Cressie [6], we decompose the surface into a mean trend, a stochastic process, and a measurement error: where \u00b5(s) is the mean trend, R(s) is a second order intrinsically stationary random process, and (s) \u223c N (0, \u03c3 2 m (s)) is a spatially uncorrelated non-stationary noise process representing measurement error. We estimate the measurement noise as above, estimate the mean trend using a suitable smoothing function, and finally model the residuals at the data points after removing the trend by a suitable variogram and Kriging analysis. Each of these processes contribute some error to the final depth surface, and we compute the final error by quadratic sum of the standard deviations (i.e., addition of the variances). Following Plant et al. [21], we concentrate on the Quadratic Loess interpolator (QLI) [4,5] for the trend surface, which fits a quadratic surface to the data given weighting to limit the bandwidth of the surface that is reconstructed [22]. This allows us to better decompose the surface into trend and stochastic functions, and allows us to focus on sampling rates and aliasing. We use the model:\u1e91 and determine \u03b2(s) by minimising q 2 (s): The weighting function w i (s) provides the localisation and weighting of the QLI, and limits the range of inter-polation to \u00b1d 0 , hence also limiting the spectral components of the interpolated surface. Solution of Equation 11 can be specified in terms of the normal equations (suppressing the spatial notation for simplicity), solving for \u03b2 Q in: To give an explicit model in the form of Equation 5, we observe:\u1e91 Let Solving for y , we may then compute \u03bb Q = y T \u2126 to avoid computing D \u22121 explicitly, and hence compute the trend surface and interpolation error via Equations 8 and 6 respectively. The stability of the QLI depends on the configuration of the points in space around the point of interest. To robustify the estimation, we compute the sample mean vector m(s) and covariance matrix C(s) of the neighbour points, and then remove any estimates which are more than three units of Mahalanobis distance from the mean. The Mahalanobis distance is defined as: which may be evaluated as above by substituting y = C \u22121 (s \u2212 m) and solving Cy = s \u2212 m. An example of the trend estimator for d 0 = 2500 m and d 0 = 5000 m are shown in Figure 7 and Figure 8, which also show the associated uncertainty. The increase in smoothing is readily evident as the fitting distance increases, and the uncertainty reduces accordingly. The sample spacing in the majority of the area is approximately one sample every 500 m along-track; the line spacing is slightly less. Hence, we cannot resolve any features with wavelengths below approximately 1 km, and we use d 0 = 5.0 km to compute the trend surface. There is no loss here, since the residuals modeled by the stochastic process contain all of the 'information' removed by the trend surface. Over-smoothing   the trend surface also aids in stabilising the variogram estimates described below. Residuals are computed by interpolating to the position of the inputs: and are modeled as an intrinsically stationary second order process by estimating the isotropic variogram 2\u03b3 0 (h) = E (r i \u2212 r j ) 2 |d(s i , s j ) = h with the Method of Moments [6] estimator: using \u2206h = 100 m and trimming the residuals to the estimated 95% density region to avoid skewing the variance estimate by significant outliers. A robustified (fourth-root [6, \u00a72.4.3]) variogram estimator was also used, and confirms the values reported here. The estimate takes the form of a spherical model with sill distance of approximately 2.1 km. A spherical model of the form: was fitted using the Levenberg-Marquadt non-linear least-squares method as before [16], Figure 9. The parameters determined are: This model is known to be unconditionally valid in R 2 [3]. Note that the variogram estimates are stable below 500 m (the nominal minimum observable separation) due to cross-lines in the archive dataset, although the stationarity of these estimates is not well defined. The assumptions made above only require that the increments r i \u2212 r j are stationary, however, so there is probably little loss. Of course, in more complex environments, multiple variograms might be required depending on the complexity of the topography, a matter for further research. Note also that there is a non-zero variance at zero lag, representing the mean measurement error in the surface. We remove the a 0 term from the variogram before further work, and add the spatially varying (s) term later. This has the side effect of enforcing the residuals where they occur (i.e., of exact interpolation) so that values of the archive soundings are maintained after the trend surface is reinserted. We turn now to the problem of hydrographic certitude. It is not possible to see very small details in the surface from the sub-sampled soundings, and it is possible that, in some environments, an object might not be observed if it were to fall between the \u223c 500 m line spacing of the main-scheme lines. Along-track, there is no reason to believe that this is the case, since the soundings are selected for recording based on rules which would preserve exactly this sort of effect. However, small erratics might be missed and we must allow for some model of this in the uncertainty measurements. It is not possible to predict where these erratics might occur, and therefore the only suitable model is to incorporate a variance term corresponding to 'hydrographic oversight', which increases as a function of distance away from the soundings according to some suitable form. Since a sum of two variograms is also a valid variogram, we may implement this simply by adding another term to the estimate variogram above. We have chosen a linear isotropic variogram form: although it might be argued that the variogram should be anisotropic to follow the survey line's direction. This would overly complicate the implementation, and require meta-information that would be difficult to extract robustly from the archive dataset; we therefore simplify by assuming isotropic form. The next question is how to choose the a 1 value (we assume a 0 0). This choice is essentially arbitrary, and reflects our degree of 'hydrographic comfort' with the data and the geology of the area (or of the likelihood of small man-made objects). For example, if we are working in a relatively flat shelf environment such as the Gulf of Mexico, we might expect to see no erratics, and set a 1 \u223c 6.51 \u00d7 10 \u22124 m so that 2\u03b3 H (h) increases to give a 95% CI envelope of 0.05 m/m. In a glacial moraine (or a harbour), we might suspect something rather rougher, and increase a 1 accordingly. The Strataform area is south of the main glacial moraines on the east coast, which occur in Long Island Sound, although there are iceberg scours in the northeast section of the extended area (towards the Hudson Canyon). Therefore, in this example, we have set a 1 = 6.51 \u00d7 10 \u22124 m. Once the variograms are established, we proceed by Ordinary Kriging of the residuals [6,7]. That is, with the same model as before, we minimise: (where n = |N (x, y)|, the number of neighbours at the location of interest), subject to the constraint that n i=1 \u03bb i = 1. In matrix form, and again dropping the explicit spatial notation for simplicity, we solve: where m is a Lagrangian multiplier to enforce the constraint on the coefficients. Solution for \u03bb K (s) follows immediately, and we predict interpolated residualr(s) and uncertainty\u03c3 2 (s) as: An example of the residual surface and the associated uncertainty are shown in Figure 10. The surface contains all of the small detail in the data lost in the QLI trend, and the uncertainty reflects the positions of the data, the data density and the observed variabilities, including the 'hydrographic comfort' variogram \u03b3 H (h). A close-up of the area in the west of the region, Figure 11(a), shows the expected form 'egg-cup' shapes around the sounding locations, and we observe predictions of approximately 1-2 m (95%) in the (relatively) sparse areas. Much of this comes from \u03b3 H (h) as can be seen from Figure 11(b), which was computed without this amplification factor. Some caution in the arbitrary use of \u03b3 H (h) is clearly justified, since we will otherwise find all surveys to be insufficiently accurate!"}, {"section_title": "Stability Estimation", "text": "Each sounding in the archive set contains some horizontal error which depends on the age of the survey, the positioning method in use and, potentially, environmental effects. Since the horizontal positioning error is relatively small with respect to the achievable resolution with the archive dataset, the primary significance of the horizontal error will be in variation of the reconstructed bathymetry due to slopes in the surface. We estimate this effect using a Monte Carlo approach [9,11] where we follow the methodology of Jakobsson et al. [13] and generate multiple pseudo-datasets from the archive set, perturbing each one according to a model of the horizontal error probable for the survey. We repeat the estimation of the surface via QLI and Kriging using the variogram estimated in the base case with unperturbed data, and compute sample estimates of standard deviation over the realisations of the surfaces in order to estimate the expected effect of random horizontal variation on the output surface. We assume that the errors in positioning are stationary over the survey area, although this might not be the case in practice. The VBES archive soundings were positioned using range-range medium frequency devices which gradually degrade with distance from the shore stations [26]; if a model of positioning uncertainty were available, it could be readily incorporated. We also assume that positioning errors are uncorrelated and equal variance in the horizontal dimensions. The errors are almost surely correlated and related to the lines of position between the platform and the survey shore stations. However, this detailed information is not generally available, and we are forced to neglect the term for computational feasibility. We ran 100 iterations of the Monte Carlo estimator with the horizontal error set to 50 m (95% CI), corresponding to 1970's electronic range-range medium frequency positioning systems. We find that the standard deviation estimate over all realisations is generally on the order of < 0.5 m mostly probably (Figure 12), al-  Figure 10: Kriging surface for residuals, and predicted uncertainty (95% CI). Vertical exaggeration is 250\u00d7. The uncertainty mainly follows the density of the survey data, and clearly highlights where there is a relatively sparse area and hence the potential for problems.   though the tails of the distribution are significant and spatially localised on significant slopes, Figure 13, and can reach up to \u223c 3 m on the most significant slope in the northwest of the region. Standard error estimates for the Monte Carlo standard deviation estimator indicate that the standard deviations are significant in over 96% of the area (i.e., are more than three standard errors from zero), indicating that the estimates are relatively stable, although more replicas would continue to improve the estimates. This is simply a matter of the time that can be expended on the process."}, {"section_title": "Overall Estimate", "text": "Assembly of the final estimate surface is a matter of adding together the components from the measurement error, QLI trend, Kriging analysis, and stability analysis. The variances from each component are added, and the output scaled to appropriate units. Figure 14 shows the final depth estimate with colourcoded uncertainty co-registered. The uncertainty pattern reflects primarily the uncertainty associated with the Kriging analysis, although the stability effects are evident on the slope in the northwest corner. This is particularly evident in Figure 15, which shows the uncertainty at the 95% CI. A more intriguing comparison is to scale the predicted uncertainty by the limits implied by the IHO S.44 standard for geophysical models [12]. Figure 16 shows this, and it is evident that the uncertainties being predicted are often higher than the suggested limits for IHO Order 1 survey, although those at Order 2 ( Figure 17) are almost always Figure 13: Surface constructed from the Monte Carlo stability estimate confidence intervals (colour-coding represents 95% CI in meters). Vertical exaggeration is 250\u00d7. The most significant uncertainties are associated with slopes as expected, and can be significant (\u223c 3 m). below the limit (except in the significant slope in the northwest corner of the survey area), which is more in keeping with the survey scales (1:40,000 to 1:80,000) and the location of the survey area at the edge of the continental shelf."}, {"section_title": "Discussion", "text": "These results require careful interpretation: Figure 16 does not say that the soundings themselves do not meet the accuracy requirements, nor does it say that the survey did not adequately determine the depths in the area at the scale required. What it does say is that a model built from these data does not constrain particularly well the depths where soundings were not taken, primarily due to the sparseness of the soundings and their ability to represent well the bathymetry at smaller scales. It is, however, the best information that we typically have, and this must be factored into decisions on use of the data and representation of this information for the end-user. It is evident from the survey scales and survey specifications that these surveys were intended as \"offshore\" and hence should more properly be judged as modern Order 2 surveys. In this light, almost all of the area is covered adequately with the exception of the slopes. This is a general problem with all pre-GPS survey (and indeed with DGPS surveys where the slopes are signif-    icant), and cannot be avoided. Any horizontal error component will translate into vertical error when applied on a slope. There are a number of limitations with the current methodology. We assume that the variogram can be modeled isotropically and although this is probably reasonable in many cases, it remains to be seen whether this is always the case. Indeed, there is evidence in the reconstructed surface of artifacts induced by closer spacing of along-track soundings than soundings from adjacent tracks, leading to saddles between data points where the survey lines run obliquely across ridges in the northeast and southwest portions of the study area. It is probable that a decomposition along and across strike would be beneficial in explaining the variability and reducing the uncertainty, but this poses problems of assessment of geological variability since this effect will not be stationary and may even appear and disappear in some regimes. The problem of 'patchiness' occurs also in the estimation of the trend surface, where we have assumed that there should be one limiting wavelength in all of the study area. Since this is related primarily to the data sampling density, it would be better if the smoothing wavelength were adaptively set depending on data density and local roughness. This poses no significant theoretical difficulty in the method, and it may be possible to drive the adaption by an estimate of consistency of the variogram. We have purposely neglected temporal increase of uncertainty (i.e., how much our knowledge of the depth degrades over time). Comparison of the VBES and MBES data show it to be most likely a small effect in this case, although in some situations (e.g., river deltas or tidally driven sand-ripple fields), it may be more significant. Finally, the choice of the 'hydrographic comfort' variogram is essentially arbitrary, and would also be better adapted based on an estimate of the likelihood of erratics, objects and other unknowns. This might depend on the local roughness but will depend primarily on the geological context of the area, something which is notoriously difficult to quantify with any certainty. Other factors will include the age of the survey, location (e.g., harbour approaches, offshore reconnaissance survey), and hydrographic intent for the composite grid model. Uncertainty may also be capped above by other factors, such as knowledge of a sidescan survey conducted with the archive dataset (generally limiting this situation to modern VBES surveys). The argument [23] is that a co-registered sidescan would indicate objects of more than, say, \u223c 1 m relief above the surface. Therefore, if no objects are detected (and if they were, they would be represented by a least-depth sounding in the archive dataset), then the uncertainty for 'unobserved variability' should be limited to a maximum of 1 m (at 95% CI for safety). It is important to note, however, this this only limits the unobserved variability factor, and not, e.g., measurement error, stability or trend surface interpolation. We have neglected here the effects of aliasing error, that 'noise' injected into the dataset and our estimates by spatial under-sampling of the surface. Plant et al. [21] consider this in the context of a spectral representation of the interpolation operator [22], and show that it may be possible to estimate the aliasing uncertainty through an 'Empirical Transfer Function' given the interpolation coefficients for a point and some estimate of the measurement noise and pre-sampling surface spectral density. In the same way that we use the extant MBES data to calibrate the measurement error in the archive dataset, it may be possible to calibrate the aliasing error from the higher resolution surface. However, this error will depend strongly on local surface roughness and structure, which are obviously nonstationary, and hence the assumption that we can calibrate once and move the calibration anywhere is less tenable than it is with the measurement error model. The problem again is one of geological context, which cannot be avoided. The magnitude of aliasing error in these datasets is currently unknown, but is a topic of continuing research. We have dismissed leadline archive data from this study due to the observed bias. However, if this is the only survey data available, this may not be a permissible solution. It is in theory possible to estimate the measurement error directly, given the bias, and proceed. However, the implication of the bias is that the uncertainty of the surface should, on average, be lower than otherwise computed-the depths are intentionally and provably shoaler than the known true depths in the area, therefore the shoal-side uncertainty implied for the hydrographic user is lower. How much less is, however, a more complex problem for more study; some dependence on geological context is expected. There is nothing intrinsic to the model that would limit it to archive datasets of this type, and it could equally well be applied to modern VBES data. In this case, the advantage of having better knowledge of measurement error, variability along-track and any contemporary MBES data would make a number of steps of the process significantly simpler. Use of contemporary MBES data may also allow assessment of the aliasing error induced by VBES line spacing, an effect which we have otherwise neglected in this treatment. This method would then provide compatible products from VBES and other sparse data for combination with high-density MBES surveys."}, {"section_title": "Conclusions", "text": "The majority of the area covered by hydrographic surveys in the US is only covered by archive datasets of VBES or leadline data. To bring this data to a state where it can be readily combined with modern data and modern methods, it needs to be made into a surface, and we need to know the uncertainty of the predictions that are made where soundings do not exist. We assume in this process that we are restricted to the soundings from the survey archives, with at best survey reports to document equipment used and survey procedures applied for horizontal and vertical control. We allow, however, that we will have-in limited quantities-co-registered modern high-resolution MBES data. We use this to construct estimates of measurement errors for the archive dataset, and to test for biases and other archive quality issues. We have shown through the use of an example dataset in a relatively benign hydrographic environment that archive datasets may be significantly biased by their method of curation from source to current databases. Particularly, the leadline data from 1936-38 found in the study area showed a significant shoal bias of approximately 1.48 m with respect to the MBES surface. This cannot be readily explained by any reasonable physical means; we believe the cause to be hydrographic rounding practices and reduction of the soundings into integer fathom units. This makes the data unusable for further comparisons. We have outlined a methodology that uses MBES data in the same area as the archive data to calibrate a measurement error model for the archive VBES data. A Quadratic Loess interpolator was used to implement scale-controlled interpolation for a trend model of the data, and the residuals after this trend was removed were modeled by Ordinary Kriging. We show that hydrographic uncertainty can be added at the Kriging stage as a separate variogram scaled to account for the hydrographer's level of comfort on the nature of the survey area. Stability of reconstruction was assessed by Monte Carlo methods. The composite output surface from all of these sources contains co-registered estimates of depth and associated uncertainty. Through the example archive dataset, we have illustrated the method, and have shown that the predicted depth uncertainty for the model is higher than might be preferred, being influenced primarily by the uncertainty due to sparse soundings and variability expressed through the Kriging analysis. Comparison with relevant IHO standard show that some of the area exceed the limits for geostatistical models at Order 1, but most meets Order 2 specifications."}]