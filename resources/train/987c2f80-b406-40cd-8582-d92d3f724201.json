[{"section_title": "", "text": "To Dave Hill, I would like to expresses sincere appreciation and gratitude. As my advisor your continuous encouragement, support, and guidance was a major source of strength in making this dissertation possible. You taught me a lot about what it means to be an academic and how to swim in these unknown waters. To my second \"unofficial advisor\", Peter Ruggiero, your belief and support in me and my sometimes outlandish ideas were critical in keeping me going and growing as a researcher. Your enthusiasm for science and discovery were fuel to keep me excited about the world waiting to be explored. Thank you for everything you put into this thesis and my development as a researcher. To my other committee members, Tuba Ozkan-Haller, Sarah Emerson, and Emily Shroyer, I would like to express sincere thanks. Your time, guidance and feedback helped to make this thesis stronger. The other faculty members in coastal and ocean engineering, ocean engineering, and statistics were all foundational in helping me understand my corner of the coastal engineering world. Little bits and pieces of you all are scattered about this thesis. To my many friends in Corvallis, you were the support structure that holds this structure up. You made life here in Oregon wonderful and I couldn't have asked for a better group of people to spend the last 5 years with. Writing this thesis was bitter sweet knowing it meant closing the curtain on this stage of life that was spent with you all. Finally to Isabel, you are the person who made this all possible. You convinced me everything was going to be ok when I was overworked and on the edge of giving up. You listened to me babble on and on about ideas when I was over-excited about research. Playing with you in the mountains and streams of Oregon is what recharged my batteries and my mind. You are the counterbalance to this whole unstable out-of-control apparatus and I would make you a co-author on this whole thesis if I could.                           Stations transition from the tide gauge (station 1) to the far river outlet of the estuary (see Figure 15). Panel      : Panel (a) shows three random sample functions drawn from the prior distribution. Panel (b) shows 3 random sample functions drawn from the posterior distribution after 4 training observations (dark black points). The effect of training is to constrain possible functions to only those that go through observation points. In panel (b), the shaded region represents the 95% Bayesian credible interval.    Table 3). The bold line represents zero error while the dotted line is the mean error of the ensemble. Subplot (a) is computed at the tide gauge location while subplot (b) is at WL station 7 (see Figure 25)    Each subpanel (a through e) is one of the top 5 storms of record (Table  3). Due to windowing for spectral filtering, storm 5's observed NTR is calculated using the subtraction method rather than the Bromirski method.         Table  Page   Table 1: Coos and Tillamook estuarine characteristics (after Engle et al., [2007]). Wind characteristics are at gauge locations. Wave characteristics are offshore at buoys 46002 and 46005. Streamflow values are from this study's hydrological analysis .....    Table 5: RMSE values comparing Hs model output to observations at various simplification levels. Rows represent station locations while the two column groupings represent instrument deployments. Hs station 0/7 was relocated so "}, {"section_title": "LIST OF TABLES", "text": ""}, {"section_title": "Chapter 1 -General Introduction", "text": "Throughout history, the coast has been a hub for human development, providing transportation, industry, food, aesthetics, and countless other motivations for settling along the shoreline. This has led to the coastal zone being one of the most densely populated on earth [Nicholls and Small, 2002], a trend that is projected to increase into the future [Neumann et al., 2015]. While coastlines provide many diverse benefits, they additionally represent an added risk in the form of coastal hazards such as large waves, storm surges, erosion, and flooding. Society's disproportionate investment in coastlines means that coastal hazards represent a rapidly growing exposure to risk for communities, infrastructure, and economies [Jongman et al., 2012]. Desire to manage this risk has resulted in a demand for coastal hazards research and for tools capable of accurately quantifying and predicting hazards. The push for advancing coastal hazards science has recently come into a new focus through the lens of climate change. The modern era has forced an abandonment of baseline assumptions of stationarity in our coastal systems [Milly et al., 2008]. For hazards research, this new paradigm has necessitated a broad reframing of the very definition of risk [Read and Vogel, 2015]. How do we understand common concepts like 100-year return interval events when we can no longer rely on the past as a valid analog for the future? How do we incorporate extreme uncertainty in the future climate into our hazard assessments and how we build our communities? These questions and many more are especially pertinent in the context of coastal research where the effects of a changing ocean (via sea level, chemistry, temperature, dynamics and many more) are already visible in observational records [Rintoul et al., 2013]. This dissertation brings these themes together through the goal of trying to better understand the future of coastal hazards in a changing climate. In particular this work is focused on flooding hazards in Pacific Northwest estuaries. The following subsections describe the general context of this research question as well as this dissertation's strategy for approaching the problem."}, {"section_title": "Estuaries in the Pacific Northwest", "text": "This dissertation is focused on the U.S. Pacific Northwest (PNW), both as a study site and as a testbed for the ideas developed within. The PNW is subject to strong wave forcing with winter significant wave heights averaging 3-4 meters and annual events commonly exceeding 10 meters [Ruggiero et al., 2010]. This is coupled with an equally energetic storm climate highlighted by extratropical cyclones that produce extreme coastal winds and precipitation [Allan and Komar, 2002a;Mass and Dotson, 2010]. Tides are mesotidal and observed water levels exhibit a variety of large-scale oscillations, the most important of which is the El Nino -Southern Oscillation (ENSO) [Chelton and Davis, 1982]. The PNW has one of the rainiest climates in the United States producing significant freshwater discharges to the coast which can dominate nearshore dynamics [Mazzini et al., 2014]. From a coastal hazards perspective, this results in an environment defined by significant system complexity and a broad range of hazard drivers [Cheng et al., 2015b;Olabarrieta et al., 2011]. As a function of diverse forcing, PNW extreme events are often the result of compound events [Serafin et al., 2014]. This means that extremes are commonly caused by combinations of forcings for which individual components may not necessarily be extreme [Moftakhari et al., 2017;Wahl et al., 2015;Leonard et al., 2014]. Consideration of compound events and changes to their probability of occurrence under climate change has been shown to be of considerable importance to flooding [Wahl et al., 2015]. However, compound events are difficult to model as they require multivariate considerations and are challenging to predict. This precludes commonly used event based approaches which assume that extreme water levels are always associated with a certain type of event (generally hurricanes) [Orton et al., 2016]. Extremes are less clear in the PNW with non-intuitive combinations of forcing leading to flooding. This research focuses on estuarine systems rather than open coastlines for variety of reasons, the first of which is that estuaries are of considerable societal importance. Estuaries provide a wide range of ecosystem services [Barbier et al., 2011] as well as often being hubs of coastal infrastructure due to harbors and ports. The second reason is that flooding in PNW estuaries is less well understood than open coasts. Flooding risk along PNW open coastlines has seen significant research in recent decades [Serafin and Ruggiero, 2014;Barnard et al., 2014;Ruggiero, 2013]. This is less true for estuaries, mainly due to a larger complexity and broader range of forcing controls. This mismatch between societal importance and relative understanding make PNW estuaries a rich subject for research."}, {"section_title": "Climate Change in the PNW", "text": "Coastal communities in the PNW are actively engaged with climate change research because they are already experiencing the effects of a changing environment. For example, research has documented an increase in extratropical storms in recent decades [Graham and Dias, 2001;Favre and Gershunov, 2006]. Similarly, and likely linked, there is evidence of wave climate intensification [Allan and Komar, 2002b;Allan and Komar, 2006;Ruggiero et al., 2010]. Tide gauge analysis shows that sea level is rising at around 1.6 mm/year for the PNW, although the locally experienced rates of sea level rise diverge significantly from this value due to alongshore varying vertical land motions [National Research Council, 2012]. Climate projections show wetter winters and drier summers for the PNW [Mote and Salanth\u00e9, 2010] so stream hydrographs are additionally expected to change. PNW streams are particularly vulnerable to climate change as a large percentage of the snowpack is at or near the freezing point [Watts et al., 2016]. How these various changes will combine to produce future flooding hazards remains a significant unknown. This is particularly true for estuarine systems because of the numerous pathways through which climate change can affect estuarine hydrodynamics. Considerable research has shown that sea level rise (SLR) is expected to intensify flooding events in estuaries [Dawson et al., 2007;Rilo et al., 2013;Smith et al., 2010]. However the specific impact of SLR on flooding has been shown to be complex and non-linear [Atkinson et al., 2013, Ding et al., 2013. Furthermore, SLR impact has been shown to be coupled with other processes (such as changes to tropical cyclones) further increasing exposure to flood risk [Frey et al., 2010;Mousavi et al., 2011]. Even dynamics that are often considered deterministic, such as tides, have been shown to be variable into the future. Specifically, SLR can modify tidal wave propagation speed, basin shape, resonance and other factors creating spatially complex and sometimes drastic changes to tidally induced water levels [Pelling and Green, 2013]. Overall there is evidence that the climate in the PNW is changing and that this will impact estuarine flooding hazards. This dissertation provides tools and guidance for understanding the future of estuarine flood hazards in the PNW."}, {"section_title": "Research Strategies", "text": "This thesis investigates climate change impacts to flooding using two approaches. A fully process-based approach is implemented as well as a \"hybrid\" approach that is targeted at bringing statistical model simulation speeds to process-based models. Exploring the same question from different perspectives allows a holistic view of the scope of the problem as well as a deeper understanding of the built in biases, strengths, and limitations of each methodology. The following two sections describe the research strategies implemented within each dissertation chapter."}, {"section_title": "A process-based modeling perspective", "text": "Chapter 3 of this dissertation approaches climate change impacts to estuarine flooding using a process-based modeling framework. This approach allows an exploration of how multiple changing forcing components will result in an integrated change to estuarine hydrodynamics. Research has shown that considering a holistic view of climate change, through viewing multiple changing physical processes, is vital as the elements of extreme water levels are coupled [Wolf, 2009;Odigie and Warrick, 2018] and nonlinear [Ding et al., 2013]. This is accomplished by using a variety of sub-models that track how each forcing component of extreme water levels (e.g. waves, tides, winds, streamflow, etc.) evolves under a changing climate. The outputs from these sub-models provide inputs to a hydrodynamic model which is run continuously for the periods of interest. Climate change impacts are quantified through decadal scale simulations run for historic and future periods at two study sites. A process-based modeling perspective requires some quantification of change to the system. This is commonly provided by global climate models (GCMs), although statistical [Wahl et al., 2015] or semi-empirical projections [Rahmstorf, 2007] are also used. GCMs rely on our understanding of physical mechanisms to predict how changes to the environment will result in changes to the climate [Solomon et al., 2007]. The complexity of global climate physics means that there are a variety of uncertainties in GCM predictions [Parker, 2011;Petersen, 2000], including systematic bias and an inability to reproduce observations [Jun et al., 2008;Maraun et al., 2010]. These issues become problematic when GCM data serve as forcing for other models, transferring the bias and potentially resulting in unphysical results [Christensen et al., 2008]. This problem was encountered when trying to integrate GCM data into Chapter 3's modeling framework and so was further explored. Chapter 2 details a study investigating a variety of methodologies for correcting GCM bias, although through the specific lens of wave modeling."}, {"section_title": "A statistical (emulator) based perspective", "text": "One limitation of process-based models is that they are computationally expensive which can limit the types of questions that can be asked. For example, the extensive computation time required for the approach taken in Chapter 3 forced the usage of a single climate model. This is problematic in that climate models predict a wide range of potential future climates [Solomon et al., 2007]. Being able to encapsulate this uncertainty into future hazard assessments makes probabilistic approaches an attractive option for climate change studies. This said, probabilistic modeling requires multiple model implementations and this is currently not viable for flooding hazard studies of the complexity exhibited in Chapter 3. This sets up a strong motivation to develop some way of bridging computationally intensive physical models and fast simulation times. Chapter 4 explores an approach to this problem through an \"emulator\" that uses a statistical representation of a physical model to rapidly produce water level predictions."}, {"section_title": "Organization of Dissertation", "text": "This dissertation consists of three core chapters, each of which is in journal manuscript form covering a distinct topic in regards to this thesis's overall research question. These chapters are followed by a brief discussion and synthesis in Chapter 5."}, {"section_title": "Chapter 2 -Evaluation of Bias Correction Methods for Wave Modeling Output", "text": "Kai Parker, David Hill Abstract: Models that seek to predict environmental variables invariably demonstrate bias when compared to observations. Bias correction (BC) techniques are common in the climate and hydrological modeling communities, but have seen fewer applications to the field of wave modeling. In particular there has been no investigation as to which BC methodology performs best for wave modeling. This paper introduces and compares a subset of BC methods with the goal of clarifying a \"best practice\" methodology for application of BC in studies of wave-related "}, {"section_title": "Introduction", "text": "One of the key drivers for the development of wave models has been the need for high resolution data distributed across both large areas and large time windows. These data are integral for navigation, hazard forecasting, recreational purposes, and a broad array of ocean science applications. Due in part to high operational costs, the observational in-situ record is sparse and cannot practically cover all areas of the ocean at all times ( Figure 1). Therefore, models serve to \"fill the gaps\" and provide a more complete understanding of the wave climate. Models are further essential for any study of future wave conditions where data clearly do not exist. However, data from wave models can and do consistently exhibit bias (defined is this study as a systematic deviation from the corresponding observed \"true value\") that results from a variety of factors including inherent simplifications and inadequate model physics (parameterizations, assumptions, etc.), numerical solution schemes, resolution, insufficient or imperfect calibration datasets, and incorrect boundary forcing data. Model bias is not unique to the field of ocean sciences. In particular, the atmospheric and hydrologic science communities have developed a mature body of literature dealing with the subject. This is primarily due to a reliance on general circulation model (GCMs), which are highly prone to bias [Mehran et al., 2014;Mueller and Seneviratne, 2014]. Since climate model output is often the input for other models, GCM biases propagate downstream and detrimentally impact other modeling results [Xu, 1999;Christensen et al., 2008]. To resolve this, methods for bringing model output back into alignment with observations have been sought. This demand is the foundational driver for the development of bias correction (BC) procedures. At the conceptual level, BC methods (as explored in this paper) define a transfer function that transforms model data to a new dataset with fewer statistical biases. How this transfer function is defined ranges from a simple shift in the mean value to increasingly complex techniques that can fully correct statistical distributions. For example, the widely used quantile mapping methods [Panofsky and Brier, 1958;Wood et al., 2004;D\u00e9qu\u00e9, 2007;Piani et al., 2009] attempt to match the CDF (Cumulative Distribution Function) of the model time series to that of a target, typically an observational time series. Recent advances in BC techniques of the type considered in this paper have expanded into the multivariate domain and attempt to incorporate the relationship between variables as well. In cases of dependent variables, well-intentioned univariate BC can lead to incorrect inter-variable correlations and non-physical results [Chen et al., 2011;Thrasher et al., 2012]. This is important in the field of wave modeling as wave parameters (height, direction, period, etc.) are highly correlated [Mathisen and Bitnergregersen, 1990;Ferreira and Soares, 2002;Repko et al., 2004;De Waal and van Gelder, 2006;Corbella and Stretch, 2012]. Recent contributions to multivariate BC differ in how they treat inter-variable relationships and include a data binning technique [Piani and Haerter, 2012], a direct bivariate distribution approach based on copulas [Li et al., 2014], and a shuffling technique [Vrac and Friederichs, 2014]. For convenience these methods will be called the Binning Method, the Direct Method, and the Shuffling Method respectively. These three BC techniques will be explained in detail in the methods section of the paper. From a broad perspective, many modeling practices can be considered a form of \"bias correction.\" For example, model tuning or data assimilation are both procedural ways of attempting to bring model output into agreement with observations. In the meteorological community, Model Output Statistics (MOS) are routinely used to remove bias in numerical weather prediction, albiet in a format that may be more easily recognized as statistical downscaling. For brevity, this study does not consider all bias-reducing techniques. Instead, it focuses on statistical bias correction methods that correct statistical distributions of model variables. With this constraint, there are three options when looking at bias correction in a wave modeling problem: A) Apply no BC [Leake et al., 2007;Lionello et al., 2008;Grabemann and Weisse 2008;Mori et al., 2010] B) Apply BC to the input data [Wang and Swail, 2002;Hemer et al., 2011, Hemer et al., 2012Durrant, 2013;Wang et al., 2014] C) Apply BC to the output wave fields Sterl, 2005, Cavaleri andSclavo, 2006;Andrade et al., 2007;Tomas et al., 2008;Charles et al., 2012]. Consideration should be given as to which of these methodologies is the most applicable to the particular study since each has associated strengths and weaknesses. Option (A) is the ideal case and the most theoretically robust. The gradual improvement of physical models is undeniably the end solution to bias. BC can be thought of a temporary solution to bring currently flawed model predictions into alignment with reality but with associated limitations. Ehret et al. [2012] reviews the broad issues with BC, including the lack of a sound physical basis , impossibly restrictive assumptions, a masking of uncertainty, introduction of physical inconsistencies between other model variables, and alteration of the spatial and temporal covariance structure of variable fields [Johnson and Sharma, 2012]. This being said, if model results are sufficiently biased, analysis may be restricted to only being relative (non-absolute). While this is acceptable for a comparison of results (say historic and future simulations), using uncorrected wave model output to force additional models (e.g., coastal sediment transport) will simply propagate the bias. Option (B) has proven effective at improving modeled wave parameters [Caires et al., 2004;Hemer et al., 2011;Durrant et al., 2013] since many wave modeling errors can be directly traced to input wind fields [Cardone et al.,1996;Rogers and Wittmann, 2002;Durrant et al., 2013]. Additionally, this option has the advantage of correcting model output over the entire model domain. This said, bias correction of wind fields has significant disadvantages including being computationally expensive [Wang and Swail, 2002] and oftentimes practically difficult. The process is complicated by sparse observational information across the ocean basins, both spatially and temporally, leading to target datasets of limited length, spatial coverage, and accuracy. Furthermore, even with BC of wind fields the wave model output will likely still exhibit bias due to wave modeling errors [Rogers et al., 2005]. Option (C) is well positioned to ensure that wave model output will be statistically in agreement with wave climate observations. Despite this, there have been relatively few studies of the application of BC methods to wave model output. To discuss a few examples, Andrade et al. [2007] used a variant of quantile mapping that fits a log-normal distribution to significant wave height and changes the parameters to match probability distribution functions (PDFs). Additionally, Caires and Sterl [2005] \nEstuaries are important intersections of human and natural systems, serving as both some of the most resource-rich ecosystems on Earth and some of the most densely populated. Asked to meet many, at times conflicting, needs, estuaries require careful management. Unfortunately, coastal planning is limited by an insufficient understanding of how estuaries will respond to future conditions. In particular, extreme water levels (WLs) are of first-order importance with flooding putting both lives and physical infrastructure at risk. In the U.S. Pacific Northwest (PNW), considerable progress has been made towards an understanding of flooding risk at open beaches as controlled by a combination of forcing drivers, including waves, tides, winds, and others [Barnard et al., 2014;Ruggiero, 2013;Serafin and Ruggiero, 2014]. Flooding risk in PNW estuaries is less well understood, primarily due to the greater complexity of the estuarine environment. Estuarine hydrodynamic climate has the additional driver of streamflow and is controlled by a much more complicated topographical context (e.g. embayment and complex bathymetry). This makes estuaries difficult to simplify as they exhibit nonlinear water column response [Ding et al., 2013] and are difficult to uncouple [Wolf, 2009]. an efficient model to determine which events will cause flooding and a more complex model to accurately quantify the dynamics of those events. As a second example, Orton et al. [2016] simply model the coastal responses to a large set of hurricane events. Both of these studies focus on the eastern U.S. coastline where tropical cyclones are the principal driver of flooding events. Locations less dominated by tropical cyclones have a more diverse and balanced set of contributions to flooding. In these locations, extreme events can occur due to combinations of forcings which are not themselves individually extreme, a phenomenon discussed in the literature as \"compound events\" [Leonard et al., 2014;Moftakhari et al., 2017;Wahl et al., 2015]. This makes it difficult to know a priori which events will result in maximum WLs. Recent advances in computing power and parallel processing have opened up an alternative possibility of running continuous time series hydrodynamic models at climate change scales (decades to centuries). This allows examination of extremes without a priori knowledge of which events will cause extremes. Additionally, a continuous time series analysis has other desirable properties. For example, an event based approach limits analysis to only large RI events, eliminating information on higher probability, lower magnitude, events. This is undesirable since so-called \"nuisance flooding\" can, over time, lead to a higher aggregate cost than extreme events, especially when considering sea level rise (SLR) [Maftakhari et al., 2015;2017]. A continuous time series approach also allows a continuous consideration of the integrated and coupled effect of the various changing controls on estuarine hydrodynamics. Many studies have focused on individual components of climate change (e.g., just SLR) but few have addressed their combined effects on estuarine flooding. This is problematic since there can be interactions among processes. As an example, SLR has been shown to non-linearly modulate storm surge [Smith et al., 2010]. Studies that have attempted to holistically model estuarine flooding include Cloern et al. [2011] who studied century-scale change in San Francisco Bay. Their study used a hybrid approach, coupling process-based and statistical sub-models to evolve water column properties over time. Barnard et al.'s [2014] Coastal Storm Modeling System (CoSMoS) also used an interlinked model framework but with less focus on estuary WLs (although CoSMoS 2.0 improves upon this). Cheng et al. [2015a] did a preliminary study of a single PNW estuary using a fully process-based model framework. The current study builds upon this effort by including additional physical processes, conducting a comparative study, and applying the results to the production of flood mapping products. The objective of the present study is to further develop this research direction through applying a comprehensive process-based modeling framework to the problem of estuarine flooding under current and future conditions. A process-based approach allows direct modeling of climate induced changes to all drivers (streamflow, wave forcing, etc.) of estuarine WLs. This study hypothesizes that considering integrated forcing on estuaries results in significant spatial variability in extreme WLs. This hypothesis is in contrast to the common procedure of a static \"bath-tub\" approximation (i.e., the assumption of a horizontal water surface), which has been shown to result in significant errors [Gallien et al., 2014]. Results from this study will quantify this error as well as provide information on how it may be evolving as a result of climate change. Additionally, flood surface information will be combined with a high resolution digital elevation model (DEM) to determine the extent of flooding and how it may be changing over time. This paper is organized with the initial section providing a description of the two study sites (section 3.2). The overall modeling framework is then introduced (section 3.3) with description of the individual sub-model components. This is followed by information on nonstationary RIs (section 3.4). The paper then moves on to the results (section 3.5) and closes with a discussion of findings (section 3.6).\nModeling estuarine hydrodynamics remains both a challenge and a goal for the scientific community. Estuaries and bays are often densely populated with significant economic and cultural investment [Pendelton, 2010]. They are also subject to a unique flood hazard environment, with high water levels (WLs) driven by numerous contributing processes including both offshore and local waves, storm surge, and river inflows, among others. Over the past several decades, research efforts have led to improved computational models and increased physical understanding of estuarine flood dynamics [Bode and Hardy, 1997;Ganju et al., 2014;Kantha and Clayson, 2000]. However, increasing hydrodynamic model predictive skill has often been at the cost of simplicity. Complex physics has gradually replaced empiricism, leaving models more accurate yet often more difficult to implement. This has led to computational time, rather than a physical understanding of the problem, being the primary control on our ability to answer questions about estuarine flooding. Steadily increasing computer processing power and code parallelization has pushed the boundary for what can be explored with complex computer codes. However, even with these advances, many questions still cannot be comprehensively addressed due to computational limitations. One example is the recent focus by the scientific community on uncertainty in model results [Green et al., 2011;Mastrandrea et al., 2010]. In the field of flood hazards, a major thrust area has been probabilistic assessments, which brings the benefits of uncertainty quantification, utility as a stakeholder-centered decision making tool, better handling of extreme events, and more skillful flooding estimates [Cloke and Pappenberger, 2009;Dale et al., 2014, Di Baldassarre et al., 2010. However, the combination of multiple model iterations required for probabilistic modeling and large per-run computational costs has remained a barrier for moving forward. Often the solution is a compromise, such as simplifying or eliminating various forcing components [Lin et al., 2010;Purvis et al., 2008]; using smaller ensemble sizes [Davis et al., 2010]; or simplifying model physics [Dawson et al., 2007;Moel et al., 2012]. A promising recent development has been to implement variable model complexity, with a fast coarse model being run to determine relevant or extreme events and then a more highly-resolved, accurate model being used to simulate the extremes [Lin et al., 2010[Lin et al., , 2012Orton et al., 2016]. This technique has been demonstrated to be successful for hurricane-induced flooding but is potentially problematic for other regions. For example, environments not dominated by tropical cyclones often are defined by compound events where combinations of non-extreme forcings can combine to create extremes [Leonard et al., 2014;Moftakhari et al., 2017;Wahl et al., 2015]. In addition, event based techniques can still be considered computationally limited as the full parameter space cannot usually be explored. There remains a need for a modeling technique that can bridge the gap between complex, time-intensive models and fast simulation times. This paper investigates emulation as a technique for the efficient prediction of estuarine hydrodynamic variables in Grays Harbor, Washington USA. The foundational idea of emulation (also referred to as surrogate modeling, response surface modeling, meta-modeling, among others) is the replacement of a slower processes-based model (a simulator) with a fast statistical model (an emulator) [O'Hagan, 2006;Razavi et al., 2012]. In the standard modeling paradigm, the map between simulator inputs and outputs is based on the laws of physics as implemented within a process-based model [Castelletti et al., 2012]. In emulation, this map is approximated using a statistical model. Following an upfront computational expense to create a training dataset, the emulator is nearly instantaneous. Thus, emulation represents a tradeoff between short simulation times and errors from the approximation. This tradeoff suggests that emulation may be ideal for probabilistic flood modeling along with several other potential applications including model uncertainty, model optimization, sensitivity analysis, real time forecasting, and extreme event analysis [Kennedy et al., 2006;Levy and Steinberg, 2010;Oakley, 1999]. The general concept of emulation originated in the 1980s through the idea of computer experiments [Sacks et al., 1989]. Since then, emulation ideas have spread widely, resulting in a rich literature of applications, emulator formulations, and theories from numerous fields. Razavi et al. [2012] reviews emulation just in the field of water resources, with over 30 studies revealing a wide range of applications and emulation approaches. In terms of coastal problems,  and Rueda et al. [2016] successfully implemented emulators for wave prediction problems using SWAN [Booji et al., 1997] as a simulator. Timmermans [2015] used emulation to explore how tuning parameters effect uncertainty in results from the Wave Watch III [Tolman, 2009] wave model. Liu and Guillas [2016] investigated the effect of uncertainty in bathymetry on tsunami height predictions using a novel merging of Gaussian process emulation with dimensional reduction techniques. In the context of flooding, emulation has been applied to river channel flooding [Apel et al., 2008] and coastal dyke systems [Moel et al., 2012], although from the relatively simplistic perspective of lookup tables. Surge response functions [SRF; Resio et al., 2009;Song et al., 2012] can be considered a specific case of emulation through regression of dimensionless cyclone scaling terms. However, SRFs are limited in application to tropical cyclones, and have been shown to perform poorly in complex environments [Taylor et al., 2015]. As an alternative to SRFs, Kim et al. [2014] used an artificial neural network to emulate coupled ADCIRC-STWAVE calculated surge from tropical cyclones. This approach was enhanced by Bass and Bedient [2018] who used a similar strategy but with the addition of a coupled hydrologic model and Gaussian process regression as the emulator formulation. Overall, multiple studies have demonstrated the potential of emulation in a coastal hazards setting. Surge from tropical cyclones has, in particular, seen a variety of successful implementations of emulators. This study builds on these previous studies but explores a location in the Pacific Northwest (PNW) outside of tropical cyclone forcing. This results in a unique challenge in terms of handling diverse forcings and a potentially larger input parameter space. For example, the similarly targeted studies (in terms of examining WLs) by Kim et al. [2014] and Bass and Bedient [2018] were able to reduce input dimensionality through considering only cyclones and using discrete cyclone characteristics as input dimensions. This study considers a general application of emulating the coupled ADCIRC-SWAN ([ADCSWAN;] simulator in which any combination of forcings can be used to calculate WLs. This paper is intended as a rigorous investigation into the applicability of emulation in this new context. Therefore, this paper is focused primarily on describing the methodology and validation with only a single application to decomposing extreme water levels presented."}, {"section_title": "Data", "text": "The data utilized for inter-comparison of BC techniques were provided by the Australian Commonwealth Scientific and Industrial Research Organization (CSIRO). CSIRO has produced wind-wave climate projection datasets [Hemer et al., 2015] using WAVEWATCH III [v3.14;henceforth WW3;Tolman, 1991;Tolman, 2009] forced by an ensemble of atmosphere-ocean general circulation models (AOGCMs). These datasets cover runs from many different AOGCMs, multiple emissions scenarios (CMIP3 and CMIP5) and several climatological periods (1980-2005; 2026-2045; 2080-2099). The model implementation is detailed in Hemer et al. [2012; and the reader is directed to these primary sources for details regarding the model setup and production methodology for the data used in this study. The CSIRO datasets are an ideal testbed for this intercomparison of BC methods since: (a) the results were found to demonstrate bias , (b) the data are available globally, allowing for the investigation of regional variability, and (c) the models are available for several climatological periods, allowing for testing of nonstationary BC. A single AOGCM, MRI-CGCM3 [Yukimoto et al., 2012], was arbitrarily chosen from the CSIRO ensemble. This choice should not be significant as the statistical testing methodology described in this paper is designed to reveal robust differences between methodologies regardless of the specific dataset. Two timeslices were used for this study, CSIRO's historical    It should be noted that in principle, BC methodologies can be extended to the ndimensional case, incorporating more than the two variables considered here. For wave modeling applications this would mainly be of interest in order to include wave direction within the BC framework. This is excluded from the present paper for two reasons. First, BC methods work best with climatological (typically ~ 30 years) periods of record and this duration of wave direction time series is generally unavailable. Second, the comparative bivariate analysis presented here is relatively complex and lengthy. We therefore defer tri-variate methods to a future study in order to maintain clarity and a reasonable length for this study."}, {"section_title": "Methods", "text": "\nThis study utilizes a suite of models and data sources to determine the hydrodynamic response of the study sites to climatic forcing. The overall workflow is that an Atmosphere-Ocean Global Climate Model (AOGCM) serves as the \"parent\" model providing forcing to a suite of \"child\" models. These in turn provide the forcing to the hydrodynamic model of the estuaries themselves. This modeling framework is conceptually illustrated in Figure 14.  As is common in climate change impact studies, this study uses paired simulations with hindcast and forecast boundary conditions. Two simulations were carried out; one for the period 1979-1999 and the other for the period 2041-2070. The hindcast period is used rather than direct observations to control for biases in the AOGCM and modeling framework.\nA conceptual overview of the process used for constructing an emulator, in the context of this study, is provided in Figure 24. The following sections describe the study site (section 4.2.1) as well as the observations used as forcing dimensionality and evaluating emulator performance (section 4.2.2). The specifics of each step in developing and testing the emulator, as well as a discussion of the various options for emulator development are then presented. "}, {"section_title": "Bias Correction Methods", "text": "A subset of available BC methods were chosen to illustrate the differences between univariate and multivariate methods, parametric and empirical methods, and to look at the performance of available multivariate methods. In this section we will indicate model data with (M), target buoy data with (T), and bias corrected model data with (B)."}, {"section_title": "Univariate Quantile Mapping", "text": "Several studies have found that distribution based methodologies, in particular quantile mapping, are most effective for BC of climate model data [Theme\u00dfl et al., 2011;Teutschbein and Seibert, 2013]. Quantile mapping attempts to match the CDF of a modeled time series to that of an observed time series. This is done most commonly using the direct constraint of D\u00e9qu\u00e9 [2007] which can be expressed as: where CDF refers to the cumulative distribution of the time series and the subscripts follow the notation specified above. For example, \u22121 is the inverse of the target dataset's CDF. Equation 1 specifies that each individual model value is replaced with the target value having the same quantile. This will result in a \"scaling\" of the model time series such that the CDF of the bias-corrected model data exactly matches that of the target. Parametric and empirical versions of quantile mapping differ in how the CDF and its inverse are calculated. The parametric method involves fitting a theoretical (parametric) distribution to the data. Various \"long-term\" wave parameter distributions have been identified by the literature with the most common being the Log-normal or Weibull for both Hs and Tp [Bauer and Staabs, 1998;Ferreira and Soares, 2000;Ferreira and Soares, 2002;De Waal and Van Gelder, 2006;Holthuijsen, 2007]. It should be noted that there is no universal best fitting distribution because the statistical  Lafon et al. [2013] looked at a comparison of empirical and parametric techniques, with a focus on the effect of using a variable number of quantiles. They cautioned that too many quantiles will result in an \"overfitting\" to the target dataset, causing a loss of robustness due to an oversensitivity to the calibration period. The climate, if treated as a stochastic process, would be expected to have some random variability around its \"true\" distribution. A perfect fitting of the model to a training segment of the climate time series being considered (target) could theoretically overfit the correction, causing error between the BC and the true distribution. This issue is subtle and many authors simply fit the data to the empirical CDF at every data point (each model value has is its own quantile). Attempting to resolve this uncertainty is one of the goals of this study. Equation 1 is formulated to bias correct model data when observations are available. To bias correct future model data (where observations are not available), the so called equidistant quantile matching method of Li et al. [2010] can be used. This method is described by Equation (2) and can be seen conceptually in Figure 4. In this equation, the subscripts h and f are used to denote historic and future values. As shown in Figure 4, the first step (1) is to find the quantile corresponding to the future model variable value , ( ). The variable value of both the target and historic model are evaluated (using the inverse CDF) at this quantile value and the difference is found (2). This difference is the model predicted change from the historic run to the future run. The difference between these two values is then applied to the model future value to acquire the BC value (3). Of note, this method necessarily makes the assumption that the statistical properties of the AOGCM systematic errors are stationary, a result that is not necessarily demonstrated in the context of climate change and is itself a topic of research [Charles et al., 2012;Teutschbein and Seibert, 2013]. "}, {"section_title": "The Binning Method", "text": "The Binning method [Piani and Haerter, 2012] is a multivariate method that was originally developed for jointly correcting surface temperature and precipitation meteorological fields. This technique handles correlation by first correcting one of the variables with a standard 1D BC method and then correcting the other variable conditionally based upon bins of the first variable. This is conceptually illustrated in Figure 5. The first step (1), not shown in the figure, is to use standard quantile mapping on one of the variables (marked with an X subscript). The second step (2) is to select a bin (this study uses 10 bins, or deciles) of the first variable (X) from both the target and model datasets. Note that the bins are selected in CDF space so a decile is shown in the figure as a single bar of data ( .2 > \u2265 .1), indicated with red symbols. The second step is to determine CDFs from these bins of data (3). Finally a normal quantile mapping is used with these decile dependent CDFs (4) so as to bias correct the second variable (Y). Step 3 is repeated for each model value within the decile. Then a new bin is selected and the process is repeated. The Binning method has a free variable in the number of bins to be used. The number must be large enough to properly distinguish correlation but small enough that sufficient data are within each bin to allow an accurate inversion of the CDF. Piani and Haerter [2012] found that surprisingly few bins (<5) were required to accurately correct the correlation. This study's choice of deciles was arbitrary and it is hypothesized, though untested, that a smaller number of bins would have produced similar results. Note that the Binning method is a framework for dealing with correlation and is not dependent on the univariate BC technique. Any univariate BC could be substituted for quantile mapping and the methodology would be the same. An interesting question is whether or not the choice of the first variable to be corrected is significant. As these methods correct this first variable in a univariate way, and then correct the second conditionally based on this first variable, it is not unreasonable to assume that the first variable will be better matched to the target statistically. Colloquially, the second variable would \"sacrifice\" its ability to perfectly match the target statistically in order to match the correlation between the variables. For all results presented in this paper Hs was used as the first variable corrected. However, a single buoy (#46029) was used to determine the sensitivity of results to this decision by comparing results to the case when Tp was used as the first variable. While univariate quantile mapping has an accepted methodology for \"future\" runs (Equation 2; Figure 4), the authors know of no similar procedure for the multivariate BC methods. As the Binning method is a framework built around univariate BC, an extension based on the principles of equidistant quantile matching was developed for this paper and is detailed in Appendix A."}, {"section_title": "The Direct Method", "text": "A second technique for bivariate BC was proposed by Li et al. [2014] and is based on a direct defining of a bivariate distribution between the variables of interest using copulas. This allows conditional correcting of one of the variables based upon the bivariate distribution's definition of the relationship between the two variables. The mathematics of the Li et al. [2014] method are significantly more in-depth than those of the previous methods and this paper only presents a procedural overview. Figure 6 shows the general process for correcting a single model output Hs, Tp pair. The first step of the methodology (1) involves a univariate BC of the first variable, similar to that of Binning method (not shown in Figure 6). The next step is to fit a copula to both the model and target datasets (2 Once a copula has been fit to the data, the next step is to determine the bivariate CDF value of the model copula (3). For normal quantile mapping, BC is based on evaluating the model quantiles with target data and this is similar to what will be done here, just in a bivariate capacity. Following this template, the analysis then moves to the fitted target copula. We are trying to find the comparable quantile to that of the model copula but since copulas are bivariate there are multiple locations of identical CDF magnitude, i.e., a so called level curve. This complicates analysis as more than just the bivariate CDF value is required for determining a unique location on the CDF surface. We can escape this problem because of the two bias corrected value pairs, one is already known from the univariate BC (step (1)). Using this information it is possible to take a slice of the bivariate CDF at the known BC value (4). Since we know the bivariate CDF value of the model (evaluated at (3)), we can take the inverse of this slice to find the CDF value for the second value (based on the target copula) (5). With this quantile we can then invert the target CDF to get the bias corrected model value (6). Similar to the Binning method, a nonstationary methodology for future runs was developed by the authors and is presented in Appendix A."}, {"section_title": "The Shuffle Method", "text": "The \"Schaake Shuffle,\" or Shuffle method for this paper, is a method of multivariate BC that Clark et al. [2004] developed as a method for correcting both spatial and temporal covariability in forecast models. Vrac and Friderichs [2014] showed that the method could further be used for multivariate BC. The Shuffle method takes advantage of the fact that the CDF of a time series is not dependent upon the order of the time series. This means it is possible to univariately bias correct the CDF of each variable in a dataset to fix the individual variable's bias and then shuffle each variable's time series to fix the intervariable correlation. Correlation is, simply put, a relationship between two variables and this can be created by properly organizing The Shuffle method is very powerful in that temporal, spatial, and intervariable relationships can be corrected simultaneously. This is only possible because the relationship in time, space, and location (as preserved by the rank) of the target time series is being \"stolen\" for the model time series. This differentiates the Shuffle method from other BC techniques as it produces a scaled version of the target time series rather than a scaled version of the model output. This further requires that the target time series and model time series have the same sampling rate and size which can be a significant limitation."}, {"section_title": "Validation Methods", "text": "The primary method of assessing the effectiveness of a BC methodology is to look at how well the method is able to change the statistical properties of the model to that of the target. Because our validation methodology involves an ensemble, these statistics were normalized as the relative difference (RD), defined as: in which the variables T and B follow the notation from above but represent some metric of performance (mean, standard deviation, etc.). A comparison of statistical moments from individual variables ignores the relationship between variables. Since preservation of this relationship is the primary goal of a multivariate BC, we also looked at a comparison of correlation coefficients (a measure of the statistical dependence between two variables). Three common correlation coefficients were examined, A further metric used as a quantification of BC effectiveness was the shape of the bivariate PDF and CDF. Correlation coefficients would be expected to capture some of the relationships shown by the shape of these distribution functions but not all. Piani and Haerter [2012] noted this when they found that the shape of the bias corrected bivariate CDF was one of the primary means of seeing the difference between a bivariate and univariate correction. Their study noted this qualitatively but did not propose a quantitative metric of this result. We expand upon this by considering the root mean square error (RMSE) between the target's bivariate CDF and the bias corrected model's CDF as a way of quantifying how well correlation is corrected. PDF RMSE was also investigated as an additional perspective into changes of overall distribution shape. Oftentimes wave modeling studies are more concerned with extreme events than the mean of a wave climate. High return level events are drivers of coastal hazards and can dominate related processes signals (such as geomorphology) [Anderson et al., 2010;Zhang et al., 2002]. This importance brings an interest in the ability of BC to address incorrect model prediction of high quantile events. For this reason the 95 th and 99 th quantiles were compared between the target and BC time series as a further metric for comparison. As mentioned earlier, previous studies have brought into question the robustness of BC techniques [Li et al., 2010;Lafon et al., 2013] and in particular empirical methods due to overfitting and the sensitivity of BC to the training period. We decided to address this issue by considering measures of performance using a modified jack-knife (also known as a k-fold) crossvalidation technique following the procedure of Bissell and Ferguson [1975] and Lafon et al. For each of these iterations the RD is calculated, resulting in an ensemble of normalized \"errors\" that will quantify the robustness of the BC technique to the calibration period. Wave parameters often exhibit a strong seasonal signal (See Figure 2) that is important to capture in a BC methodology. For this reason it was decided to additionally break the total record up into seasons. As each of these seasons has a characteristically different distribution shape, this can be considered a further test of robustness in how well the BC method performs on different shaped distributions. Each of these seasons was run through the crossvalidation procedure. With 4 seasons, each with a 10 bin jack-knife test, the resulting ensemble for each measure of performance was 40 data points. As the RD is normalized, the data were not regrouped by season but treated as a single sample."}, {"section_title": "Results", "text": "Although BC was tested on 4 locations, only results from buoy 42001 will be presented graphically in this section. This location was chosen for presentation as being a characteristic case with all other locations exhibiting the same general behavior. It was additionally chosen as the most illustrative with model output being generally the most biased across all metrics. Any cases where individual results vary from the displayed data will be discussed within the text. Furthermore, additional plots for all locations have been provided in the supplementary materials. Figure 8 presents results of the BC procedure in terms of summary statistics for Hs and\nModel output was saved at a subset of model nodes (stations) in order to keep output files manageable in size. Output stations were spaced evenly across the bay in order to capture spatial variability ( Figure 15). While ADCSWAN has the ability to write out numerous variables (including wave heights, periods, etc.) the focus of this study is on WLs so discussion here will be limited to that variable. The model was run in 3-month long segments with a 2 week overlap to avoid discontinuities in dynamic processes. Output data from these segments were then recombined into a single continuous time series. \n"}, {"section_title": "Statistical Bias", "text": "Tp. The ensemble, as described above, provides important information about the sample spread, leading to the presentation of the data as box plots. All presented box plots have the centerline of the box at the median value, the edges of the box at the 25 th and 75 th percentiles, and the whiskers extending to the most extreme data points not considered outliers (as defined using the Tukey boxplot convention [Frigge et al., 1989] where the end of the whiskers are +/-1.5 times the interquartile range from the box edges). Outliers are plotted as black dots. Each methodology has both historic and future results represented by the blue and red colored boxes in each column, respectively. A heavy line is used to connect the means to provide a visual cue as to the change from historic to future runs. When examining a condensed information plot such as Figure 8, it useful to consider what an \"ideal\" result would be. The y-axis is a RD, meaning that the values are both normalized (unitless) and comparative. A value of 0 on this axis (plotted in the figure as a dotted line) for the historic case would mean a perfect match between the target and the bias corrected data. The second aspect to consider is the width of the boxes (and their corresponding whiskers). This is the sample spread and represents the variance in RD, or how well the BC performed for a variety of test cases (the ensemble). An ideal BC technique would have a width approaching zero, meaning that it is robust to both calibration period (k-fold validation) and distribution shape (seasons). The first column in each panel of Figure 8 shows the raw model output. The y-axis value in this case indicates the performance of the wave model itself, when compared to the observational data. For example, in looking at RD for the mean values, it is clear that the model under-predicts Hs (negative value for RD) and as well as Tp (also see Figure 2). The spread in the raw model results is due to the jack-knife procedure, where only a portion of the buoy data is considered the target at a time. The displayed sample spread therefore indicates the natural variability of the target dataset over time. Since there is no observation (target) dataset for the future, the future result metric is calculated using the historic observational data for normalization. The differences between the historic and the future run results therefore is an indicator of changes in the wave climate. An ideal BC technique would preserve the relationship between the historic and future runs. Conceptually this means that despite biases in the simulated wave data, the future period is assumed to have a useful signal if interpreted relative to the historic period rather than that of the observed [Wood et al., 2002]. If the amount of change is the same for the model column and the bias corrected column, then the BC has successfully maintained the relationship between the historic and future runs. Looking at the Hs column of Figure 8, all BC techniques perform consistently across the metrics. Furthermore all methods perform very well with the RD of metrics being close to zero with low variance. Noting the change of y-axis scale, it is seen that all techniques perform decreasingly well (both with matching the target and with sample spread) for increasingly higher moments. This is an expected result as higher moments are more variable depending on calibration period. It may be initially counterintuitive that all BC methods perform so uniformly for Hs but recall that all tested multivariate BC methods require one of the variables to be corrected with a univariate BC as a step in the process. For this study this initial variable was chosen to be Hs so all methods are essentially treating this variable the same. The Tp column of Figure 8 indicates a similar general level of success in correcting the moments of the Tp distribution. This result is contrary to our hypothesis that the choice of primary BC variable was significant and that the second variable would be less accurately corrected. The only outlier to this generally good performance is the Direct method which did not perform as well for higher statistical moments. It should be noted that the Direct method additionally showed poor performance for mean and standard deviation in some of the other buoys tested (buoy #44025 and #46029). A sensitivity analysis was performed to explore the significance of Hs being arbitrarily chosen as the primary variable by rerunning a single buoy (#46029) with TP as the primary variable. This test corroborated the above result with no difference found between the means of boxplots run with Hs or Tp as a primary variable.  "}, {"section_title": "Correlation", "text": ""}, {"section_title": "Bivariate PDF/CDF Shape and RMSE", "text": "While bulk statistics can quickly show the properties of a dataset, analysis of the full distribution provides a more visual and intuitive understanding of changes. First considering the bivariate PDFs (displayed as the 2 left columns), Figure 10 shows that both univariate quantile mapping methods (empirical and parametric) still have the same general shape of the raw model. The distribution has been scaled by the BC (a shrinking or expansion along the x or y axes) but the characteristics of the shape have not been significantly altered. This is contrasted with the bivariate BC methods that not only scale the distribution but also transform the shape into closer alignment with the target. In this case, both the Binning and Shuffle method closely match the target while the Direct method appears to be somewhere in the middle, performing better than the univariate methods but worse than the other bivariate methods. This result is in alignment with what was found by comparing statistical moments and correlation coefficients. The CDF plots tell a similar story although the removing of the marginal distribution allows us to see that the univariate BC methods are entirely unable to fix correlation. The shape of the model's CDF remains unchanged in each of the univariate methods CDF plots. This is contrasted by the bivariate methods, which for the Binning and Shuffle methods, match the target to the point that it is difficult to see the dotted line representing the target. The CDF plots show again the direct method is somewhere in between the univariate and bivariate methods. These plots qualitatively show how the BC process is working but by calculating the RMSE we can attempt to quantitatively compare the methods. Figure 11    valid comparison for this metric. This forced methodology change means that displayed performance is not directly comparable to that seen in Figure 8 and Figure 9 as the entire record length is used both for bias correction and validation. It would be expected that uncertainty and robustness in this comparison is significantly higher so results should be viewed with caution."}, {"section_title": "Extreme Quantiles", "text": "Proceeding with this caveat, Figure 12 shows an improvement of the 95 th and 99 th percentile for both wave height and wave period. The only outlier to this general improvement is the Parametric method which shows little to no improving for the 99 th percentile of wave height and a decrease in accuracy from the model for both the 95 th and 99 th percentile of wave period. "}, {"section_title": "Discussion", "text": "This study is intended as an investigation of bias correction applied to wave modeling applications and as a comparative guide to which methods are most applicable in this context. In particular, consideration was given to the relative performances of (1) empirical vs. parametric quantile mapping and 2 With current scientific focus on quantifying uncertainty, many wave modeling efforts have evolved into ensemble based frameworks requiring many simulations. This approach brings additional scrutiny to the computational burden of each step in the modeling process. A full computational analysis of the methodologies explored in this paper is beyond the scope of this study but a broad comparative investigation was explored. Looking at the time to correct one season of 30 years of data, Parametric corrections are very fast (order of 10's of seconds). Empirical methods take longer (order of 10's of minutes) but still not to the point of being significant in relation to the computational time of model runs. Of Empirical methods, the Direct method is slowest and the Binning method is fastest but with the difference being in the order of minutes. These results are presented cautiously as the magnitude of this comparison is undoubtedly variable dependent on the implementation and system the code is being run on. Overall the computational time of BC was found to be minimal and not a driver of the choice between methodologies or the use of BC in general.\n"}, {"section_title": "Conclusions", "text": "In summary, wave forcing is a key component in the accurate modeling of ocean and coastal systems. Oftentimes a direct wave model application will result in biased output due to a variety of factors ranging from errors within the wave model itself to biases within the input meteorological fields (especially with AOGCM derived products). Biases in wave model output can affect the entire system being considered making BC oftentimes requisite for bringing results back into agreement with reality. This said, BC should be used only after noting the various limitations and assumptions ranging from a masking of uncertainty to the potential introduction of physical inconsistencies between model output fields. While BC of model input data is widely accepted, this study investigates the alternative approach of a direct BC to the wave model output itself. In this context, a subset of BC techniques are compared using a statistically robust crossvalidation technique in order to investigate which methodologies are most effective. Key findings of this study were that: (i) the BC of wave model output is an effective tool for bringing wave model output into alignment with observations; (ii) no statistical difference was found between univariate empirical and parametric methods; (iii) bivariate methods were shown to be superior to univariate methods due to improvement in matching correlation; and, (iv) the empirical binning method of Piani and Haerter [2012] (extended in this study to apply to future projection model runs) proved to be the best overall approach across the various metrics considered.\nThis paper introduced a process-based modeling framework for analyzing climate change impacts to various controls on estuarine flooding. In particular this study focused on extremes and changes to RI events at two PNW estuaries. This study described the difficulty of using RIs in the context of nonstationarity and showed how \"effective RIs\" can be an intuitive way of understanding changing flood hazards. Effective RIs showed that predicted changes to forcing results in a decrease in extreme event magnitude moving into the future. This buffers the full effect of SLR on extreme events. This buffering effect was shown to be smaller for low RI events than high RI events suggesting that increasing extremes will be felt first for low RI events. This study used multiple study sites and climatological periods to explore drivers of extreme events. It was found that extreme events for both locations were not controlled by a single forcing but rather by compound events. Tides were shown to be a key control on extremes and that the concurrence of strong forcing and high tides is required for extreme WLs. It is unlikely that maximum annual forcing coincides with a high tide suggesting that the common procedure of adding an uncoupled high tide and high storm surge will result in a significant overprediction of flooding magnitude. An additional outcome of this study was the demonstration that extreme WLs are spatially variable in an estuary. The results showed that WLs varied by more than 25 cm across each estuary domain. Relying only on predictions at the tide gauge and the assumption of a horizontal water surface will therefore mischaracterize flood risk. waves varying significantly across the estuary. The approach is shown to be effective in reproducing other variables in addition to water levels such as non-tidal residuals and significant wave height suggesting that emulation may play a key role in improving our ability to probabilistically assess flood risk in complex hydrodynamic environments.\nThis paper has presented an application of emulation, or surrogate modeling, to the problem of rapidly simulating hydrodynamic variables within the Grays Harbor, WA estuary. This methodology is targeted towards a variety of computationally constrained problems including probabilistic modeling, uncertainty quantification, model optimization, and nonparametric extreme event analysis. To facilitate efficiently achieving these goals, this study has focused on validating and quantifying the error induced by emulation. Additionally, a variety of simplifications to the simulator have been suggested for reducing input dimensionality, and therefore the size of the emulator training dataset. The results from this study suggest that the Gaussian process regression derived emulator is skillful for calculating a variety of model output variables (WL, NTR, and Hs). A decadal scale comparison of emulated WLs to tide gauge data showed the emulator having a RMSE of 15 cm. Emulator performance is evaluated at multiple observation points across the estuary domain providing confidence that emulation is skillful across spatial extents. Decomposing the error from different emulator construction simplification levels shows that the largest source of unexplained variance in emulator hindcasts is from ADCSWAN. Of particular interest, strong simulator simplifications (including that of stationarity) are a relatively low contributor to loss in emulator performance (average increase in RMSE of 1 cm). Therefore, future efforts to improve emulator performance should focus on improving the full simulator before reducing simplifications or optimizing the emulator. Emulation is found to be very efficient after the construction of the training dataset. Using a LHS experimental design, analysis shows that the training dataset size guidance of 10 times the number of input dimensions [Loeppky et al., 2009] is optimal. Finally, the emulator was applied to investigate the relative contributions of different forcing variables to annual maxima WLs at the study site. Results show the importance of compound events to flooding in the PNW with a diverse mix of forcing contributing to annual extreme WLs. Streamflow was found to be, on average, a relatively minor contributor to extremes except near the river's mouth. Wave contributions were found to be significant but only at locations near the edges of the bay, suggesting strong wave penetration during extreme WL events. Further investigation, however, showed that this effect is because annual maxima WL events are not necessarily always concurrent with large wave events due to the compound nature of estuarine flooding. While only a single example application of emulation to estuary hydrodynamics questions was explored, results signal the significant potential of emulation to a broad range of applications. "}, {"section_title": "Chapter 3 -The Effects of Changing Climate on Estuarine Water Levels: A Pacific Northwest Case Study", "text": "Kai Parker, David Hill, Gabriel Garc\u00eda-Medina, Jordan Beamer Abstract: Climate change impacts to extreme water levels at two U.S. Pacific Northwest estuaries is investigated using a multi-component process-based modeling framework. The integrated impact of climate change on estuarine forcing is considered using a series of sub-models that track changes to oceanic, atmospheric, and hydrologic controls on hydrodynamics. This modeling framework is run at decadal scales for historic and future periods with changes to flooding quantified across the two study sites. It is found that there is spatial variability in extreme water levels at both study sites with all recurrence interval events increasing with further distance into the estuary. This spatial variability is found to increase for the 100-year event moving into the future. It is found that sea level rise is buffered by a decrease in forcing. Small recurrence interval events are less buffered and therefore more impacted by sea level rise than higher return interval events. Finally, results show that annual extremes at the study sites are defined by compound events with a variety of forcing contributing to high water levels."}, {"section_title": "Study Sites", "text": "This study focuses on two PNW estuaries, Coos and Tillamook Bays ( Figure 13). These estuaries were selected for two reasons. First, each has an active watershed / estuary organization (Coos Watershed Association / Tillamook Estuaries Partnership) which allowed for data sharing and project collaboration. Secondly, the estuaries have similar forcing profiles (Table 1)    In terms of physical layout, Coos has a unique hook shape while Tillamook has a more classical bay form with an enclosing sand spit defining the western edge ( Figure 13). Both estuaries have a jettied inlet and U.S. Army Corps of Engineers maintained channel. Tillamook's entrance is maintained at 5.5 meters deep, 60 meters wide while Coos has a significantly larger deep-draft channel at 18 meters deep and 210 meters wide. Coos has a larger average depth and surface area giving it a larger estuary volume than Tillamook by around 130 million cubic meters. Coos is also skinnier with a deep channel along the majority of its length. Tillamook, on the other hand, has a channel only near the entrance with the bulk of its area being defined by broad shallow tidal flats. In terms of forcing, the estuaries are quite similar although with Tillamook experiencing slightly higher environmental forcing. Tillamook's tidal range is approximately 20 cm larger than Coos and mean wave, wind and streamflow forcing are all modestly more intensive."}, {"section_title": "Climate Model", "text": "This study uses data from the North American Regional Climate Change Assessment Program (NARCCAP) [Mearns et al., 2009[Mearns et al., , 2013. NARCCAP provides an ensemble of AOGCMs paired with Regional Climate Models (RCMs) focused on the North American continent. The project's future runs are forced by the Special Report on Emissions Scenarios (SRES) A2 emissions scenario which represents one of the higher emission, anthropologically controlled climate scenarios for the fourth IPCC assessments report [Nakicenovic et al., 2000].  [Mesinger et al., 2006] dataset interpolated to the location of the CRCM grid nodes. AOGCM data was also found to be biased but was not bias corrected as there is no target dataset that spans the full global climate model extent. Instead this bias correction was handled within the relevant sub-model."}, {"section_title": "Wave Model", "text": "A basin-scale Wavewatch III v3.14 (WW3) [Tolman, 2009] simulation was performed in order to characterize wave climate and provide offshore wave boundary conditions for the two hydrodynamic model domains. WW3 has seen significant success in the PNW for reproducing wave conditions [Garcia-Medina et al., 2013] and is well suited to application at large scales [Hanson et al., 2009]. The model was run with two nested grids based on the National Centers for Environmental Prediction (NCEP)'s operational Global and North East Pacific models with a resolution of (1\u00b0 by 1.25\u00b0) and (0.25\u00b0 by 0.25\u00b0) respectively. Wind forcing for the WW3 model was provided by the CCSM global model. Wave model predictions were found to exhibit a significant bias in comparison to observed wave parameters in the PNW study areas. This bias is likely a result of the low-resolution wind fields [Holthuijsen et al., 1996] or the AOGCM's inability to reproduce marine winds [Hemer et al., 2011]. This transfer of bias from wind fields to wave model output has been similarly reproduced in other studies [Feng et al., 2006, Hemer et al., 2011. Sensitivity analysis showed that running the hydrodynamic model with over-predicted wave heights produced unrealistic flooding values and overwhelmed the influence of other signals contributing to extreme WLs. We therefore utilized a bivariate statistical bias correction technique that corrects both the marginal distribution of wave height and period as well as maintains the correlation structure [Parker and Hill, 2017]. The models were calibrated and validated using the NARR dataset as forcing. Both study sites have multiple USGS stream gauges for comparison and Nash-Sutcliffe efficiencies (NSE; Nash and Sutcliffe, [1970]) as well as coefficients of determination (r 2 ) were found to be high in all cases. This indicates that the hydrological model adequately captures the hydrology of the Coos and Tillamook watersheds and provides confidence moving on to simulations using CRCM input. For consistency with other aspects of the model framework, the CRCM variables were all bias corrected using quantile mapping with the NARR dataset as the target."}, {"section_title": "Hydrological Model", "text": "The gridded hydrologic model produces runoff values at every model cell along the coastline. However, most cells produce very low flows so only significant points were selected for inclusion in the hydrodynamic model. For Tillamook, four rivers were included (the Kilchis, Wilson, Miami, and Trask) which were found to capture 95% of the basin streamflow. For Coos, seven points were chosen for inclusion representing 90% of the basin streamflow. The streamflow from these points was aggregated into three inputs into the hydrodynamic model at the location of the Coos River, Palouse Slough and Noble Creek (Figure 15). "}, {"section_title": "Monthly Anomaly Model", "text": "where is monthly mean sea level, 0 is the regression y-intercept term, eq is the equatorial wind stress, ls is the local alongshore wind stress, xy is the wind stress curl, is the residual error, and ( , , ) are regression coefficients. The reader is directed to the original source publication for additional information regarding the specifics of the wind stress coefficients as well as the scientific basis. Figure 16 shows the result of this regression for the Tillamook study site (Coos is not shown). While this formulation does not capture all variability in MMSLA (R 2 \u2248 .6), it does qualitatively capture the MMSLA signal and is based entirely on variables that are readily available from the NARCCAP dataset. MMSLA WLs are added to the hydrodynamic model time series as a post-processing step (see Figure 14). Not including MMSLA within the hydrodynamic model does exclude some potential non-linear interactions between MMSLA, WLs and other forcings. Fitted contributions to MMSLA from predictor variables eq, ls, and xy are shown as thin black lines (full, dotted and dashed respectively). The bold black line is the observed MMSLA signal while the bold red line is the total fitted MMSLA signal."}, {"section_title": "Sea Level Rise", "text": "SLR was included in the modeling framework as a change to mean sea level (MSL) within the hydrodynamic model. Projections were taken from the National Resource Council (NRC) report [NRC, 2012] which developed local estimates for SLR along the Pacific coast. These estimates include contributions from steric/dynamic ocean modifications, glaciers and ice caps, sea-level fingerprint effects, and vertical land motion (e.g., isostatic adjustments). In calculating local SLR estimates, the NRC used a combination of IPCC 4 th assessment projections (mid-range scenario) and an extrapolation methodology for the cryosphere components. This produced values larger than either the IPCC 4 th or 5 th assessments but still below some estimates for mean 2100 global SLR [Vermeer and Rahmstorf, 2009]. SLR data were taken from the nearest reported location to Coos and Tillamook Bay, Newport Oregon, which is situated approximately between the two study sites. The NRC report provides projection values (and ranges) for 2030, 2050, and 2100. A cubic spline was fitted to these values to allow a smooth interpolation to intermediate years. Multi-decade model runs of the hydrodynamic model were broken into smaller 3-month periods for model stability reasons and MSL was updated accordingly for each of these simulation blocks. This allowed for changes in MSL in a step-wise but nearly continuous fashion."}, {"section_title": "Local Hydrodynamic/Wave Model", "text": "The coupled ADCIRC-SWAN (ADCSWAN) model  was used for this study. ADCSWAN is a highly configurable in terms of implemented physics and readers are directed to the source publication and model manuals for a full description of options and parameters. ADCIRC [Luettich and Westerink, 1992]  DEMs [NOAA, 2018].Wetting and drying were enabled due to the significant intertidal areas present in both bays. Non-linear bottom friction was used with a spatially variable friction factor set based on general land use classes Homer et al., 2011]. ADCIRC was run in the 2D depth-integrated mode. ADCIRC has 3D barotropic/baroclinic modes which would capture additional physics not included with this study's methodology. However, this comes at a substantial additional computational cost that is prohibitive for the climatological timescale of this study. Additionally, our primary focus is on WLs (not full water column properties), and research has shown that storm surge and tidal signals can be accurately resolved by 2D barotropic models. Specifically, Resio and Westerink [2008] point out that 3D effects are readily absorbed by model calibration coefficients. Additionally, a sensitivity study by Weaver and Luettich [2010] found that differences in predicted WLs between 3D and 2D models were on the order of 5% over most of the domain. These modest differences suggest that a 2D model can be an effective and efficient choice for studies of this type. SWAN is a third-generation spectral model that solves the spectral action balance equation to compute the spectral evolution of wind waves. The unstructured format of SWAN [Zijlema, 2010] was utilized to allow tight coupling (on the same grid) with ADCIRC. SWAN was run in a nonstationary mode with offshore forcing provided by a temporally varying JONSWAP spectrum fitted to bulk wave parameters. ADCSWAN was run with atmospheric forcing provided as gridded horizontal wind components and surface pressure; wave forcing using SWAN's nonstationary TPAR parametric spectrum file; hydrologic input as a normal flux into the domain, and tidal forcing at the ocean boundaries. Tidal forcing was defined as the eight locally dominant constituents (K1, O1, P1, Q1, M2, S2, N2, and K2) with location dependent amplitude and phase defined by the ENPAC tidal database [Mark et al., 2004] and the simulation time dependent nodal factor and equilibrium argument defined by the T_tide harmonic analysis package [Pawlowicz et al., 2002]. The ADCSWAN model was first validated at each location using a tidal simulation compared against NOAA tide gauge predictions. Using a one month simulation, both the Tillamook and Coos grids were found to have R 2 values greater than 0.97. The Tillamook model was additionally validated against the largest storm of record (the Great Coastal Gale of 2007; Crout et al., [2008]) with good agreements to extreme WLs [Cheng et al., 2015b]."}, {"section_title": "Nonstationary Recurrence Intervals", "text": "This study primarily considers extremes which will be quantified in terms of RI events, since engineering design and community planning often rely on this concept. The traditional definition of an RI is built on an assumption of stationarity, or time-invariance. This assumption makes the definition of a RI simultaneously the inverse of the probability that an event of a given magnitude will be exceeded in a given year and the expected recurrence period of that event. This definition breaks down under nonstationary conditions, which can be experienced due to climate change and SLR. Reconciling a nonstationary environment with traditional design methods based on stationary assumptions is an ongoing challenge. Proposed alternatives include effective design value [Katz et al., 2002], expected waiting time [Olsen et al., 1998;Sala and Obeysekera, 2014], expected number of events [Parey et al., 2007[Parey et al., , 2010, design exceedance probability, and design life level [Rootzen, 2013]. Each of these definitions represents a unique projection of the stationary case for nonstationary conditions. Problematically, the chosen metric can result in significantly different calculated RIs while most users simply interpret the result as comparable to the stationary case. This highlights the importance of rigorously defining utilized nonstationary RI formulation as well as considering if the utilized metric fits design conceptions. This study uses the effective design value [Katz et al., 2002] interpretation of nonstationary RIs. This defines a temporally varying RI (termed an effective RI, or design value, by Katz et al. [2002]) which holds the probability of occurrence for an event constant through time. This preserves an intuitive definition of RIs as well as how nonstationarity impacts extremes. While this definition does add an additional dimension of time to RIs, this does not significantly impact usability as the user can simply choose the RI value from a standard RI curve plot and then add the amount of nonstationarity (for this study, SLR) corresponding to the year of interest. "}, {"section_title": "Recurrence Intervals (Tide Gauge Locations)", "text": "RIs were calculated using a GEV distribution fitted to annual block maxima events.     Figure 19 shows the same RI behavior as above with historic RIs being higher than future RIs for the current period (year 2000). Moving forward in time, this plot demonstrates that SLR eventually overtakes this effect to result in a higher future flood. The point of overtake (where nonstationary RIs begin to predict a larger flood) is plotted in Figure 19 as colored dots on the x-axis. This intersection is significantly earlier for lower RIs (within 20 years for a 2-year event) than for higher RI events (over 60 years for a 100-year event). This result is similarly shown for Tillamook Bay, although with the spread in overtake time being smaller."}, {"section_title": "Recurrence Intervals (Spatially Variable)", "text": "Section 3.5.1 discusses RIs at a single location (the tide gauge), but a key feature of this study's methodology is the ability to explore spatial variability in RIs across the study sites. Figure 20 demonstrates this by plotting the 100-year RI WL calculated at each output station. Analysis was limited to stations that were wet over 75% of the record length. This was implemented to limit uncertainty in GEV analysis due to insufficient record lengths at only periodically wet stations.  It is apparent in Figure 20 that there is a significant gradient in extreme WLs across both study site estuaries. For Tillamook, WLs differ by approximately 25 cm and for Coos by around 35 cm. The gradient in WLs is orientated such that that the minimum WL is located near the estuary entrance with extreme WL height increasing with further distance into the estuary. Of particular interest, this trend is the same for both study sites suggesting that this pattern may be more generally applicable. While Figure 20 only plots the 100-year event, this gradient is maintained across various RI periods. The spatially variable WLs produced from this analysis provide the information necessary for building flood maps. This was accomplished through fitting a smooth surface to the scattered stations using spatial spline interpolation [ESRI, 2016]. The 100-year RI WL surface was then intersected with the estuary DEM with all locations below the extreme WL surface defined as \"flooded\". This methodology makes some assumptions, such as extrapolation at the edge of the surface where station output was not available. The calculated flood surface is shown in Figure 20 as a light blue surface. It is useful to compare these results to other flooding products.  "}, {"section_title": "Changes to RI spatial variability", "text": "In order to investigate how RIs may change into the future, an additional RI analysis was performed for the future simulation period. With an identical configuration (both for the processbased and statistical models) to the historic period simulation, the only free variable is the AOGCM forcing under climate change. Therefore, a comparison shows how extreme events can be expected to change (in a relative manner) over time.  Figure 22 shows the difference between future effective 100-year RI WLs (calculated at year 2050) and the historic 100-year RI WLs. If the RI difference plot (Figure 22) were to show no spatial variability, then the same pattern seen in Figure 20 would be replicated in the future (only the magnitude would change). This would be an important result signifying that the spatial pattern of current extreme flooding events will remain unchanged into the future with only an estuary-wide SLR adjustment being required to update hazard assessments. Instead, Figure 22 shows a spatial pattern that is qualitatively similar to that seen in the historic RI plots. Specifically, results show the change in WLs increases with further distance into the estuary, a result that signals an increase in the spatial gradient of RIs as the study sites move into the future. Looking at other RI event periods (not shown), results show a gradual decrease in changes to spatial variability from around 10 cm for the 100-year RI event to approximately no change for the 2-year RI event. 3.6. Discussion  The results show that, for the simulated events, pressure and MMSLA are the largest components of non-tidal residual. Streamflow and wave forcing were found to be an order of magnitude less important than pressure and MMSLA. This is especially true for the two events at the Tillamook study site (subplot c and d) where streamflow and wave forcing were found to be of negligible importance to extreme event WLs. The wind contribution was found to be variable across stations and events. This is expected as wind setup is very dependent on the estuary geometry and the wind direction of the specific event."}, {"section_title": "Drivers of Extreme WLs", "text": "However, this analysis is only for the maximum annual event and others events likely have different compositions in terms of forcing contributions. Further investigation shows that, for both study sites and both climatological periods, extreme event magnitude is not correlated with any individual forcing other than MMSLA and tides (p-value > 0.05 when calculating correlation between WL magnitude and forcing magnitude at annual maximum events). This is reinforced by the fact that annual maximum events are found to often occur during below average non-tidal forcing conditions (e.g. below average wind or waves). This is even true for MMSLA with a few annual events occurring during negative MMSLA forcing. This supports the conclusion of variable WL composition, in terms of forcing contributions, and the predominance of compound events in the PNW. A full analysis of WL contributions would therefore require simulating many events to understand this variability. Tides are additionally a key driver of extreme events in the Pacific Northwest. Tides were not plotted in Figure 23 for scale reasons but were found to be the largest fraction of WLs (an average of 185 cm for Till and 145 cm for Coos). Tides are the dominant source of WL variability in the Pacific Northwest [Allan and Komar, 2002a] Figure 23). While waves have been shown to be important drivers of NTR in PNW estuaries [Cheng et al., 2015b;Olabarrieta et al., 2011], tidal modulation means extreme WLs do not necessarily occur during maximum wave energy events. Finally, this provides evidence that the common methodology of simply adding the largest storm surge to a high tide may result in significant overestimations of event magnitude."}, {"section_title": "Spatial variability in RIs", "text": "It is common practice to calculate WLs at a convenient location (such as a tide gauge) and then apply this value across the entire study domain. While larger estuaries (e.g., Delaware Bay) may have multiple tide gauges, smaller estuaries typical of the PNW tend to have one or none. A spatially constant assumption represents a major simplification as even tides can produce significant spatial WL variability in semi-enclosed basins [Holleman and Stacey, 2014]. Additionally, spatial variability is particularly important for estuaries as they are often regions of low-gradient topography where a modest change in water elevation can correspond to a large change in inundated area. Results (Figure 20) show variability in WLs for both locations in excess of 25 cm. Furthermore, the minimum of this gradient is at the estuary mouth where, in the PNW, the tide gauge is generally located. This means that estimating flooding from a tide gauge will result in under-predictions for flooding with errors increasing with upstream distance into the estuary. This result strongly supports the importance of considering spatial variability in WLs within flood hazard assessments. Between the two study sites, Coos is found to have a larger WL gradients from the mouth to the interior bay by about 10 cm. This is contrary to the expected result that Tillamook, with its proportionally larger forcing, would have a larger gradient. Streamflow was expected to have significant impact on WLs but the results for Tillamook found a minimal contribution. Figure 23 shows that, while the streamflow component does increase moving shoreward for Coos bay, the majority of the WL differential for both sites is driven by pressure. An additional component of the WL gradient is from tidal forcing which produces around a 10 cm differential between the estuary mouth and the inner bay for both locations. Results show that spatial variability is predicted to change into the future. However, this result is primarily shown for high RI events with lower RI events not showing any change in spatial variability for the future scenario. Lower RI period events are better constrained statistically than higher RI period events so it is possible that some proportion of the predicted spatial variability is a function of GEV analysis on a temporally limited record. A modeled record longer than the period used here (20 years for historic, 30 years for future) could help to illuminate if this conclusion is a physical result."}, {"section_title": "Changing Extreme Events", "text": "With analysis showing RIs evolving through time, a natural next question is how climate change is modifying the estuarine system. To help illustrate this,  Figure 18 is caused by a broad scale reduction of forcing on the estuary for the GCM scenario considered. Unfortunately, with all modeled forcing shown to be reduced for the future period, it is difficult to conclusively differentiate which drivers are controlling changing RIs. Furthermore, as GEV analysis is based on a parametric fit of multiple annual maximum events, it is difficult to characterize the cause of changing RIs without considering the aggregate behavior of all the events to which the GEV is fitted (rather than a single event as shown in Figure 23).  (Figure 19). This result found that the reduction in forcing is eventually overcome by SLR, although with the timing being controlled by the size of the RI event. The idea of an \"overtake point\" is a simplification based on the assumptions within the model, specifically that of stationarity/nonstationarity (see section 3.6.4.3) and projecting the future period RI's backward in time (e.g. RIs calculated for the future period applied to 2000). Another way of viewing this result is that SLR represents a constant nonstationarity across all RIs. By year the 2050, both 2year and 100-year RI events increase by 17 cm due to SLR. On the other hand, the change in RI magnitude from forcing is variable across return periods. From forcing, the change in 2-year RI is only 3 cm while for the 100 year RI it is much larger at 32 cm. This means that low RI events are less buffered by a change in forcing than high RI events. The conclusion is the same from this interpretation in that small RI events will be more impacted by SLR than high RI events."}, {"section_title": "Modeling limitations", "text": ""}, {"section_title": "Excluded Processes", "text": "In this study bathymetry/topography was held constant through time. Bathymetry is a first order control on flooding and so an ideal future projection would include morphological evolution of the estuary. This said, morphological projections at climate change scales are extremely uncertain. The combination of high uncertainty and high dependence would leave resulting flood predications dominated by an uncertain morphology projection with all other signals obscured. This study therefore does not consider morphological evolution in order to specifically highlight how changing forcing impacts extreme WLs. Both estuaries have significant anthropological modifications ranging from coastal infrastructure to dredged channels. Coos in particular has an engineered coastline along the majority of its southern boundary. A key factor in future extreme events is the interaction between human intervention and the estuarine system. For example, estuary WL characteristics under tidal forcing show high sensitivity to anthropological changes [Gallien et al., 2011;Wang et al., 2017] and modifications to land use have been shown, in certain cases, to be of the same order of importance to WLs as SLR [Bilskie et al., 2014]. Dredging, for the same reason as morphological evolution, can cause drastic changes to estuarine hydrodynamics. However, similarly to morphological predictions, anthropological controls are highly uncertain. This is often handled in climate modeling through multiple scenarios but this was not possible for computational reasons."}, {"section_title": "Climate Model Variability", "text": "The results shown in Figure 18 provide a unique validation tool via a comparison of modeled and observed extremes. However, it is pertinent that the modeled results and observed results are not based on the exact same forcing time series, but rather two different iterations of the historic climate. There is inherent variability in the climate so there is correspondingly variability in RIs depending on the random climate iteration experienced. Since both the Coos and Tillamook RI curves have a straighter shape, this could be a result of the specific climate that was used for the simulation. This can be generalized to other conclusions made in this paper in that there is an unquantified uncertainty as a function of the specific climate model used. The common solution to this problem is the usage of ensembles rather than a single AOGCM [Murphey et al., 2004]. Unfortunately running ensembles is computationally expensive given the level of complexity included in this study. Conceptually this study make the compromise of including more physical complexity at the cost of uncertainty quantification. Results from this project should therefore not be viewed as a probabilistic or a \"most-likely\" timing (especially tides) at these study sites. While forcing was bias corrected using a methodology that has been shown to perform well for extreme quantiles [Parker and Hill, 2017], timing of forcing occurring on a high tide or during a high MMSLA is additionally critical. This study examines only one possible combination of forcing timings that may or may not be representative of the overall extreme behavior of the system. This could once again be addressed through usage of ensembles or multiple iterations of the current climate."}, {"section_title": "Assumptions of Stationarity", "text": "Another important assumption in this study is that of stationarity when SLR is removed. The reality is somewhat more complicated as climate change will result in a forcing driven nonstationarity in addition to that seen from SLR. For our case, each time series segment is statistically stationary (Augmented Dickey-Fuller test, p-value < 0.001) but the overall time series (from 2000 to 2070) must be nonstationary. This is because the two segments (historic and future) show distinctly different calculated RI curves ( Figure 18) so WL behavior, as controlled by forcing, must be changing. Simply put, this study analyzes two segments that are not long enough to reveal the longer term nonstationary behavior of the overall time series as controlled by changing forcing. This is not problematic for this study, which compares two snapshots, but a comprehensive analysis would need to resolve the overall nonstationarity from forcing as well as from SLR. This is a factor in the calculation of the overlap timing from effective RIs (Figure 19) since the overall nonstationarity from forcing is not included in the analysis."}, {"section_title": "Study Site", "text": "Grays Harbor, WA ( Figure 25) is an excellent candidate for testing emulation as it exhibits many of the complexities that make estuarine modeling difficult. Grays Harbor is predominantly shallow, dominated by depths averaging less than 5 meters, but also contains a maintained (United States Army Corps of Engineers; USACE) deep-water navigation channel giving it significant depth variability ( Figure 25). The bay exhibits spatial variability in WLs [Cialone et al., 2001] as a result of its size [approximately 235.3 km 2 , Engle et al., 2007], shape, and gradients in forcing. Grays Harbor is located in the U.S. Pacific Northwest (PNW, Figure   25) and is subject to an energetic storm and wave climate. The Global Ocean Wave 2 (GOW2) reanalysis [Perez, et al., 2017] node nearest to the study site (see Figure 25) shows a mean offshore significant wave height (Hs) of 2.5 meters with events exceeding 7.5 meters annually. Extreme storm events are generally associated with extratropical cyclones that can produce strong winds, pressure differentials, and precipitation [Allan and Komar, 2002a;Mass and Dotson, 2010]. These events are often associated with significant non-tidal residuals (NTR) Komar, 2002a, 2006;Serafin et al., in revision] although of a smaller magnitude than locations impacted by tropical cyclones or with broader continental shelves [Zhang et al., 1999]. Grays Harbor has significant hydrological input from the Chehalis, Humptulips, Hoquiam, Elk and Johns Rivers which collectively drain a watershed of over 7,000 km 2 for an average monthly river flow of over 22 million m 3 /month [Engle et al., 2007]. Figure 25: Grays Harbor, WA study site and locations of observational datasets. Circles and triangles represent USACE deployments with co-located instruments labeled with a single number representing both WL and Hs stations. The main panel shows the bathymetry and topography of the local estuary in the NAVD88 Datum. The inset panel shows the larger geographical context of the estuary with the thin black line delineating the domain of the hydrodynamic model."}, {"section_title": "Observational Data", "text": "This study utilizes a variety of observational datasets ranging from instrument deployments to reanalysis products. Forcing and model development datasets are explained in the following section (4.2.2.1) while section 4.2.2.2 details observations specifically used for model validation."}, {"section_title": "Forcing and Model Development Datasets", "text": "Wave forcing for the model was obtained from the GOW2 reanalysis of Perez et al. [2017] with output selected from a node located at (Lat: 47\u00b0, Lon: -125\u00b0; Figure 25). Atmospheric forcing was provided by the North American Regional Reanalysis (NARR) [Mesinger et al., 2006]. NARR provides a wide range of gridded atmospheric variables from which the 3-hourly 10m wind fields and 3-hourly surface pressure fields were utilized. Streamflow was obtained from USGS river gauges with total estuary inflow constructed as the sum of three gauged rivers, the Chehalis, Satsop, and Wynoochee (USGS stations 12031000, 12035000, and 12037400 respectively). The bathymetry data for the simulator grid were developed by blending two National "}, {"section_title": "Validation Datasets", "text": "In addition to forcing, a series of observational datasets were used to validate simulated and emulated variables within the study site. The first dataset is the Westport, WA tide gauge (NOAA station ID # 9441102) which provides continuous hourly WL data from 2006 until the present. Water level observations were decomposed into constituent components (e.g., deterministic tide, monthly mean water level anomalies, storm surge etc.) using the approach described in Serafin and Ruggiero [2014]. The five largest NTR events on record were extracted to test simulator and emulator skill. A brief summary of these storm events is provided in Table   3. Water level observations at the tide gage were supplemented by a field campaign carried out by the U.S. Army Corps of Engineering (USACE) from September-December, 1999 [ Figure   25, Cialone et al., 2001;2002]. This dataset includes seven locations near the inlet with bottom mounted tripods measuring wave characteristics, WLs, tidal currents, and suspended sediment. Additionally, five surface stations were distributed throughout the bay measuring WLs, conductivity and temperature. The USACE field campaign was broken up into two deployments (with a small maintenance/data collection break between the two deployments). Instruments were replaced in approximately the same location except for Hs station 0 which was moved to location 7 for the second deployment [Cialone et al., 2002]. Figure 25 illustrates the spatial distribution of the various observation stations which have been renamed in this paper for clarity."}, {"section_title": "Simulator Configuration", "text": "This study utilizes the coupled Advanced Circulation [ADCIRC; Luettich and Westerink, 1992] and unstructured Simulating Waves Nearshore [SWAN, Zijlema, 2010] simulator [ADCSWAN;. The coupled ADCSWAN simulator has seen extensive validation and success in predicting WLs and NTR at various estuaries around the world [Bhaskaran et al., 2013;Dietrich et al., 2012;Krien et al., 2015]. Recently, ADCSWAN has been successfully implemented in the PNW with good agreement between simulator output and observations of WLs, NTR, and currents [Cheng et al., 2015b;Cialone et al., 2002]. ADCSWAN was chosen for this study due to its balance of physical complexity for the variables of interest and computational efficiency. ADCSWAN is implemented in the 2D depth-integrated barotropic mode which has been shown to perform with acceptable error for computing WLs and depth integrated currents in embayed regions [Resio and Westerink, 2008;Weaver and Luettich, 2010]. ADCIRC is run in the fully 2-way coupled implementation with SWAN, which is important for resolving the interacting physics of wave fields and nearshore hydrodynamics [Cialone et. al., 2002;Dietrich et al., 2010Funakoshi et al., 2008]. ADCSWAN is run on an unstructured mesh which, in this study, extends beyond the continental shelf (approximately 115 kilometers offshore; Figure 25). Unstructured meshes provide flexibility in simulator resolution with the model grid having element sizes ranging from around 7,000 meters offshore to under 20 meters within the inner Grays Harbor channel."}, {"section_title": "Dimensional Reduction and Levels of Simplifications", "text": "Emulator construction requires sampling the full input forcing parameter space. This constraint dictates the number of times the simulator must be run (to create the training dataset; section 4.2.5) to be proportional to the number of dimensions included as inputs. In general, process-based hydrodynamic simulators are based on many inputs making some form of dimensional reduction necessary. An important part of emulator construction is finding a balance between minimizing the number of inputs and maintaining sufficient complexity to acceptably resolve variables of interest. Figure 26 provides a conceptual model of each simplification that went into transforming the full process based simulator (ADCSWAN) into an emulator for this study. Each of these simplifications theoretically introduces some amount of error into the output, defined in Figure   26 as a \"level\" of error. This paper also refers to simplification levels which correspond to the simplification that induced the error (right side of Figure 26). Finally, this paper will discuss discrete error (error from a single simplification) and cumulative error which refers to the sum of all preceding discrete errors. For example, a comparison of model output from the level 3 simplification (stationary simulator) to observations (no simplification) quantifies the cumulative level 3 error. The following methods sections (4.2.4.1 -4.2.4.3) explain each simplification in this hierarchy while corresponding error is later quantified in the Results section (section 4.3)."}, {"section_title": "Simulator Simplifications", "text": "The first level of simplification is simply that of using a process-based simulator. Simulators are unable to exactly reproduce observations for a variety of reasons ranging from incorrect or unresolved physics (e.g., assumptions, parameterizations, etc.) to numerical approximations (truncation errors, etc.). The error induced by this simplification is primarily a function of the chosen model, the implemented physics, and model tuning. Therefore the specific configuration and choice of ADCSWAN (section 4.2.3) is the primary control on the strength of this simplification. This study considers emulation of a specific implementation of the ADCSWAN model and therefore the model grid (bathymetry, resolution, etc.) is held constant. Additionally, simulator configuration parameters are not included in the emulation framework. ADCSWAN contains a large number of input switches, tuning parameters, forcing options, numerical configurations, and other choices [Westerink et al., 1992]. This study holds all general model configuration parameters constant leaving the various forcing components of water level variability as the sole input dimensionality of the emulator."}, {"section_title": "Forcing Simplifications", "text": "Even with the simplification of holding the model configuration fixed, the input dimensionality remains high, due to the numerous physical forcing mechanisms. Here the simulator is eventually simplified to 16 forcing dimensions in order to investigate how well emulation can work at a relatively basic level. This approach is desirable as less input dimensionality corresponds to a smaller training dataset and therefore a more efficient emulator construction."}, {"section_title": "Wave simplification", "text": "It is well known that offshore wave energy can impact water levels within bays such as Grays Harbor [Cheng et al., 2015b, Olabarrieta et al., 2011. This forcing is implemented in the simulator using a JONSWAP spectrum fitted to a peak wave period (Tp), Hs, mean wave direction (MWD), and directional spread parameters. While research has shown the importance of forcing with full directional spectra for reproducing wave observations [Montoya et al. 2013;Rogers et al., 2007], most studies accounting for wave influence on WLs use simpler bulk parameter based formulations. Therefore a fitted JONSWAP spectrum is used for both the full (level 1) and simplified (level 2) simulator comparisons. Based on previous research in the PNW [Cheng et al., 2015a], directional spread is held constant at 20 degrees, and wave forcing is applied uniformly along the full simulator open boundary. With these simplifications, wave forcing is included in the emulator as three dimensions: Hs, Tp, and MWD."}, {"section_title": "Atmospheric simplification", "text": "Atmospheric forcing represents a unique challenge for emulation due to spatially variability. Gridded inputs represent a high degree of dimensionality, with every node potentially representing an input dimension. For this reason, a sensitivity study was undertaken to see if spatially constant atmospheric forcing could be used as an approximation of the full forcing fields. WL output from simulator runs with full gridded forcing were compared to runs with spatially constant forcing. Results indicated that the error introduced in predicted WLs by the spatially constant assumption was acceptable in comparison to the corresponding reduction in dimensionality. This error is quantified in the results section (along with other simulator simplifications) as level 2 error. Adopting the spatially constant assumption, atmospheric forcing is reduced in the emulator framework to three dimensions: wind speed, wind direction, and sea level atmospheric pressure."}, {"section_title": "Tidal Simplification", "text": "Tidal forcing in hydrodynamic models is generally represented through harmonic constituents. NOAA tidal predictions at gauge locations (which are based on harmonic analysis) typically contain around 40 individual constituents, each of which is specified by an amplitude and phase. From an emulation perspective, including all constituents would be computationally prohibitive so some form of simplification is desirable. Many studies using ADCIRC are forced with eight or fewer constituents, mainly because global databases of tidal constituents (e.g., TPXO [Dushaw et al., 1997], or LeProvost [Le Provost et al., 1994) are typically limited to that number. Despite this, these simulations are found to agree well with both harmonic analysis derived and observed tidal elevations [Blain et al., 1998;Blain et al., 2001;Westerink et al., 1992]. ADCIRC simulates tidal forcing as a boundary elevation time series [Luettich et al., 1992] determined by a spatially variable, temporally constant phase and amplitude and a temporally variable, spatially constant equilibrium argument and nodal factor. Amplitudes and phases are determined by the simulator boundary location, and are therefore not an emulator input dimension when considering a fixed study site. The nodal factor represents adjustments of the amplitude/phase of each constituent that results from the nodal tide cycle. The equilibrium argument (deterministic based on date and time) controls the timing of the harmonic. While tides are deterministic, they are included within the emulator as forcing for a variety of reasons which will be described in section 4.2.3.3. In approaching simplifications, a sensitivity test was performed to determine the tidal dimensionality required for accurately reproducing maximum WLs during storm events (Table 3). It was found that removing the nodal factor did not significantly change simulated WLs. After this simplification, results showed that eight harmonics (without nodal factors) were sufficient for accurately producing WLs. This allows tides to be included in the emulator as eight input dimensions: 8 harmonic equilibrium arguments, each ranging from 0 to 360 degrees."}, {"section_title": "Streamflow Simplification", "text": "Streamflow is represented in ADCIRC as a flux of water into the domain (specified as a normal flow per unit width of boundary). This allows the simulation of large rivers that have significant cross-channel velocity profiles and for calibration where data on these cross-channel profiles are available. For this study, we instead specify a laterally constant velocity profile across the Chehalis River boundary. This simplification is common [McKay and Blain, 2010;Bunya et al., 2010], especially if the boundary is far enough away from the area of interest that a natural flow profile can develop. This allows streamflow to be represented as a single input dimension (the total volumetric flow rate) for each river inlet. Grays Harbor has multiple river inlets but the majority of the input flow is concentrated at the Chehalis River which captures around 80% of the watershed area. For simplicity, only the Chehalis input is included in the simulator as a single input dimension. All other stream inputs are assumed to be minimal with only local influences on WLs."}, {"section_title": "Base Water Level Simplifications", "text": "A final (16 th ) input dimension is considered within the emulator framework as a \"Base WL\" parameter. This is included to account for large scale changes to estuary sea level, as is experienced through monthly mean sea level anomalies (MMSLA), seasonality, and sea level rise (SLR) [Serafin et al., in revision]. These forcing dimensions are defined in the simulator simply as a static change to mean sea level and are therefore included in the emulator as a single input dimension. Shorter time scale variations in WLs are computed by the simulator as controlled by forcing."}, {"section_title": "Simulator Stationary Simplification", "text": "ADCSWAN and other process-based hydrodynamic simulators are dynamic in that both inputs and outputs are functions of time and the instantaneous simulator state is determined, in part, by previous states. Seeking simplicity, this study makes the assumption that the dynamic system can be approximated using a series of stationary simulations. Precedents for such an assumption exist in less complex systems, including spectral evolution in wave modeling (SWAN) approximated using a series of steady-state simulations [Rogers et al., 2007;Rusu et al., 2008]. Simplifying tidal forcing with stationary simulations is difficult since there is no tidal equilibrium in WLs. One approach is to consider tides as a series of horizontal water surfaces of different elevations (corresponding to tidal phases). This would reduce tidal forcing dimensionality to a single value (tidal WL), but at the cost of losing spatial variability. Testing showed that, for the Grays Harbor study site, tidal wave evolution and propagation across the estuary results in significant spatial variability in tidally forced WLs. A second approach is to decouple NTR and tidal WLs and add the two as a linear summation. However, further testing confirmed that this simplification results in significant error. Therefore, a hybrid solution was developed in which all non-tidal forcing is stationary but tides are computed dynamically with model output recorded only at a specific moment of interest. This approach is appropriate since tides are deterministic and, for a specific set of equilibrium arguments, the previous state of tide induced WLs will always be the same. This approach allows tidal forcing to be simplified but retains the spatial variability in tidal WLs and the nonlinear interactions with other processes. Figure 27 illustrates how stationary runs compare to the full dynamic simulator. Figure   27a compares NTR from the fully forced ADCSWAN (simplification level 1; black line) and seven stationary ADCSWAN runs (simplification level 3; black dots) during Storm 2. NTRs are computed for both cases by subtracting a 'tides only' simulation from the fully forced model. Figure 27b demonstrates how the stationary NTR is computed for the peak of Storm 2. This NTR value is plotted in Figure 27a as a red outlined dot. The agreement between the fully dynamic run and the seven stationary runs is overall good, with an RMSE error for Storm 2 of 11 cm."}, {"section_title": "Experimental Design", "text": "Following the framework outlined in Figure 24, the first step in building an emulator is the selection of design points (experimental design) to create the training dataset. This study implements a design from the commonly utilized Latin Hypercube sampling (LHS) family of schemes first explored by McKay et al. [1979]. LHS is one of the older and more popular experimental designs and has been found to perform well for complex simulators [Jones and Johnson, 2009]. The specific experimental design for this study was created using a \"maximin\" LHS design [Johnson, 1990;Morris and Mitchell, 1992] from the LHS package in R [Carnell, 2017]. Parameters required for a LHS design are the number of dimensions to be included, the range of each dimension, and the number of design points. As detailed in section 4.2.4, this study used an input parameter dimensionality of 16, including wind speed and direction, sea surface pressure, Hs, Tp, MWD, river flow, base WL, and eight tidal equilibrium arguments. LHS considers only the maximum and minimum values of each dimension with design points spaced approximately uniformly across dimensions. Ranges were chosen for each parameter in an attempt to span all plausible forcing scenarios. This was determined by looking at 100 year return level events as calculated from the observational records. The size of the training dataset is typically controlled by the cost of running the simulator but Loeppky et al. [2009] provide the general guidance that the training dataset should be approximately 10 times the number of dimensions of the input space. Given the 16 input dimensions of this study, this suggests a theoretical training dataset size of 160 runs. To explore the relationship between training dataset size and emulator skill and to validate the emulator's overall effectiveness, this study conservatively developed a larger training dataset consisting of 480 ADCSWAN runs."}, {"section_title": "Emulator Configuration", "text": "A variety of formulations have previously been used in an emulation context, including support vector machines, artificial neural networks, and many others [Jin et al., 2001;Gano et al., 2006;Razavi et al., 2012]. This study uses Gaussian process regression (GPR, also referred to as Kriging), a Bayesian statistical nonlinear regression model well suited to this particular application as it scales well to high-dimensional input and intrinsically considers model uncertainty [Levy and Steinberg, 2010;O'Hagan, 2006]. Furthermore, GPR is a general and flexible framework that can be optimized for a variety of modeling problems [Rasmussen and Williams, 2006]. For example, many other common emulator formulations, such as neural networks [Rasmussen and Williams, 2006] and radial basis functions [Anjuo and Lewis, 2011], can be shown to be a form of GPR under certain conditions. The foundational definition of a Gaussian process is that of an infinite collection of variables for which any finite subset is described by a multivariate Gaussian distribution. Every point in the input space can be modeled as a random variable (due to uncertainty about the functional response to uncertain input) and the Gaussian process governs how these variables are related. A common way of thinking about GPR is as a distribution over functions [Rasmussen and Williams, 2006]. This is mathematically tractable as a GPR can be completely defined by a mean and covariance function (due to being modeled as a multivariate Gaussian distribution). From a Bayesian perspective, this means a GPR is specified as a prior mean and covariance function. The data then updates this prior with information about the true form of the function to develop the posterior. This process is conceptualized for a one-dimensional case in Figure 28. The effect of the Bayesian conditioning on the emulator can be seen as an \"anchoring\" of the posterior sample functions (and uncertainty) at locations of observations. This limits the possible functions to those that go through these observed points. Uncertainty is quantified by the possible functions that pass through these training points. shows 3 random sample functions drawn from the posterior distribution after 4 training observations (dark black points). The effect of training is to constrain possible functions to only those that go through observation points. In panel (b), the shaded region represents the 95% Bayesian credible interval. Figure after Rasmussen and Williams [2006]. The first component of a Gaussian process is the mean function. A common choice is to set the prior mean to zero and this is demonstrated in Figure 28 (as shown by the approximate mean of the sample prior functions being zero). As an alternative, this study follows the methodology of Timmermans, [2015] who used a simple linear regression to obtain information about the mean function's form. Residual analysis of our data showed a cubic relationship for the tidal equilibrium argument terms, a somewhat expected result due to the cyclic nature of tides. Based on this result, the prior mean function was defined with a cubic term for all tidal equilibrium argument inputs and a linear term for all other inputs. A comparison of emulators with and without the modified mean function showed a significant gain in skill by using the modified mean function. The covariance function of a Gaussian process is the second necessary component for defining the emulator. The covariance function can be thought of as describing the relationship between points in the process. Practically this describes the smoothness of the resulting GPR. A comparison of 4 commonly used covariance functions, the Gaussian, squared exponential, Matern (\u03bd =1.5) and Matern (\u03bd =2.5) [Rasmussen and Williams, 2006] was performed using kfold cross-validation and comparing model RMSE values procedure [Arlot and Celisse, 2010;Kohavi, 1995]. K-fold cross-validation breaks the total training dataset into k segments and cycles through every possible combination of withholding one segment for validation and training the emulator with the remaining segments. This results in an ensemble of skill metrics for which the mean is less biased and more robust to the training period than a standard validation methodology [Arlot and Celisse, 2010]. It was found that the Matern (\u03bd =2.5) was the best performing and this was utilized for all results found in the following section. The training of the emulator was performed using the Managing Uncertainty in Complex Models (MUCM) package in R ."}, {"section_title": "Error Introduced by Model Simplifications", "text": "With the construction of the emulator being hierarchical (Figure 26 Figure 29 shows the difference between the simulators (level 1 simplification minus level 3 simplification) at two locations: the tide gauge ( Figure 29a) and WL station 7 (Figure 29b). This difference is denoted here as the \"error\" resulting from simplifying the simulator. The error computed via this test was found to have a max of 25 cm with a RMSE of 6 cm at the gauge location. The maximum RMSE for approximately 100 stations randomly scattered across the estuary domain was found to be 9 cm. The simplified simulator was found to be only lightly biased with a mean approximately 2 cm lower than the full simulator (represented as a  Table 3). The bold line represents zero error while the dotted line is the mean error of the ensemble. Subplot (a) is computed at the tide gauge location while subplot (b) is at WL station 7 (see Figure 25). positive mean error in Figure 29). Sensitivity testing for non-storm conditions (not shown) confirmed that these storm results are likely conservative and that the simplified simulator generally performs better for non-storm conditions."}, {"section_title": "Emulator Validation", "text": "The ability of the emulator to replicate the fully simplified simulator (level 4 error) was quantified using a k-fold cross-validation. Figure 30 shows the results of the cross-validation with 5 segments comparing emulated WLs and simplified simulator WLs. Overall the emulator was found to perform well at this comparison level with a high level of skill. The emulator shows no bias (mean of the residuals less than 1 cm), and relatively even variance in residuals across WL magnitude. This said, the width of the histogram in Figure 30b suggests that this step introduces more error than simulator simplifications. The RMSE was found to be around 13 cm (level 4 error) which is significantly larger than the calculated simulator simplification error ( Figure 29, sum of level 2 and level 3 error) of approximately 6 cm. However, this RMSE comparison is likely biased as the level 4 error assessment is based on a larger sample size and more rigorous k-fold validation and the level 2 and level 3 error assessment only examines performance during storm events."}, {"section_title": "Emulator Performance: Westport, WA Tide Gauge", "text": "The next stage of quantifying the skill of the emulator is to compare emulated WLs to observations at the tide gauge. This test provides a measure of the cumulative level 4 error, or the total integrated error from predicting observed WLs using emulation. This analysis was performed by using the emulator to predict hourly WLs at the location of the tide gauge for the entire period of record (2006)(2007)(2008)(2009)(2010)(2011)(2012)(2013)(2014)(2015)(2016). Comparison between tide gage observations and hourly emulated WLs for a randomly chosen month long segment are shown in Figure 31. Overall, hourly emulated WLs (for the over 10-year record) compare favorably to the tide gauge with an R 2 value of greater than 0.96, RMSE of approximately 15 cm, and a bias of less than 1 cm. As with tide gauge records, WL output from an emulator can be considered as the sum between two components: tides and NTRs. In the PNW, tides are the dominant source of WL variability [Allan and Komar, 2002b] [Haigh et al., 2014;Pugh, 1996;2014]. Therefore, for this study NTRs were calculated from tide gauge data using the  (Table 3). Due to windowing for spectral filtering, storm 5's observed NTR is calculated using the subtraction method rather than the Bromirski method. All panels have the same y-axis scaling. procedure of Bromirski et al. [2003]. This method uses spectral filtering to remove energy from tidal bands for a cleaner estimate of NTRs. For spectral analysis, a windowing function is used making data at the edge of the record unusable. In addition the Bromirski et al., [2003] "}, {"section_title": "Emulator Performance: USACE field campaign", "text": "The tide gauge provides a rich dataset for validating the emulator due to its record length, but is spatially limited to a single comparison point within the study area. One key strength of using an emulator is the ability to provide WL information across study sitesas an emulator can be constructed at any grid node within the ADCSWAN model.  Cialone et al., 2001;2002]. Figure 33 shows good performance between observed and modeled WLs across most locations. The main exception to this is WL station 6 which has comparatively poor agreement between the observed and emulated WLs. This lack of skill is equally shown by the full ADCSWAN simulator and is therefore not a result of the emulation procedure. Table 4 gives     . To determine the relative contribution of different forcings on extremes, seven versions of the 31 year time series were emulated under different forcing scenarios. As a baseline \"full forcing\" case, the time series was emulated with the observed forcing at Grays Harbor (comparable to the results in Figure 34). 6 additional forcing scenarios were emulated with one forcing contribution excluded (e.g., waves, wind, pressure, MMSLA, streamflow, and tides) to isolate the contribution of each particular forcing on WLs.  The diverse mix of contributions for each bar in Figure 34 shows that extremes events are compound in nature and controlled by multiple forcings. The exception to this is streamflow which is negligible at all locations except the easternmost station (EW5, Figure 34). While some contributions are approximately constant across the domain (e.g. pressure and base WL), others are spatially variable leading to both an East-West and North-South gradient in water levels. This is partly from wave influence which is found to have a significant contribution to the annual maxima but only at stations in the northern and eastern reaches of the bay. The influence of wind increases to the north, most likely due to the mean wind direction emanating from the south during storm events. The influence of pressure anomalies on extreme WLs is found to be uniform but this result is likely from the spatial simplification of sea level pressure fields. Not shown in Figure 34 is the contribution from tidal forcing. This is primarily for scale reasons as the tidal component is an order of magnitude larger than that from other forcing (average of 140 cm). Tides do show a gradient across the estuary although with the opposite pattern as that shown in Figure 34. The tidal component of annual maxima WLs decreases by about 30 cm moving from the center of the estuary moving North or East. As WLs are the sum of these two components (tides and forcing driven NTR), the calculated gradient in total WLs is less than that observed in Figure 34 (under 20 cm across the two transects)."}, {"section_title": "Effect of simulator simplifications", "text": "The hierarchical validation used in this study provides a unique approach to quantifying the error budget as sourced from multiple simplification levels. A comparison of model performance at various simplification levels (Table 4) found that the primary source of lost skill is from the fully dynamic simulator rather than from simulator simplifications or from emulation. Averaging across stations and deployment periods, all simplifications and emulation only increased RMSE by 1 cm relative to the level 1 error. This result is of particular interest when compared to the quantification of discrete error from only emulation (see the histogram in Figure 30) which shows that emulation introduces comparatively significant error into WL estimates. Acknowledging the larger sample size of this analysis and more rigorous k-fold validation, it is still relevant that the RMSE of discrete level 4 error is double that of the combined level 2 and level 3 errors. Furthermore, a cumulative level 4 comparison at the tide gauge ( Figure 31) found a RMSE of 15 cm while Level 4 itself ( Figure   30) had a RMSE of 13 cm. This result suggests that the error from the emulator construction is not necessarily additive nor linear. In other words, the cumulative error is not the sum of the discrete errors. For this case, the dominance of level 1 error is found to mask that of the other levels. This may not be true if level 1 error was able to be significantly reduced by improvements in process-based modeling at which point other simplifications may become relevant to the error budget. In terms of quantifying model skill, the RMSE for comparisons to the 1999 USACE field data is overall larger than the RMSE in comparison to the tide gauge. A close examination of the USACE WL time series shows significant high frequency noise that is likely the cause of the overall larger RMSE values. The source of this noise is unclear but is most likely measurement noise or local disturbances (e.g. ship wakes). It is expected that the tide gauge, located within a stilling well with controlled calibration and validation procedures, would present a more accurate representation of the modeling error. This study suggests that the most effective action to improve emulated WL predictions is to reduce level 1 error. One option would be optimized tuning, a process which can be accomplished by including tuning parameters within the emulator framework [Hall et al., 2009;Kennedy et al., 2006]. ADCSWAN could also be replaced with a different simulator or physics implementation that could better resolve processes at the location of interest. For example, ADCSWAN could have been run in 3D baroclinic mode. This would come at the cost of drastically increasing computation time and requiring additional input dimensionality in the form of density, temperature, and salinity fields. An additional source of error for hydrodynamic simulators in general is bathymetry. Bathymetry is a first order control on WLs and unresolved or incorrect topographic features are an important fraction of the error budget. This is likely the source of errors for WL station 6. Figure 35 shows WLs at this station having an asymmetric tidal signal indicative of shallow water while the observations have less asymmetry. This suggests that the water depth at the time of deployment was greater than the depth in the compiled bathymetric dataset used to generate the ADCSWAN grid. Therefore investment in more accurate or more recent bathymetry is most likely a viable step for decreasing level 1 error. Level 2 error could likely be reduced by making less aggressive simplifications of forcing inputs. It is conceptually straight forward to include other input dimensionality such as spatially variable atmospheric forcing or full spectral wave forcing. A promising strategy is the inclusion of field variables through decomposing the field into principal components [Higdon et al., 2008;Liu and Guillas, 2016]. There are additionally a range of options for avoiding the stationarity assumption made in this study, which would eliminate or reduce level 3 error. The incorporation of temporal variability in emulators is reviewed by Reichert et al. [2011] who suggests the following strategies: 1) Apply a standard emulation methodology but with time as an additional degree of dimensionality. [Conti et al., 2005] 2) Describe the time series using basis functions and then apply emulation to the basis function coefficients. [Bayarri et al., 2007;Higdon et al., 2008] 3) Emulate the difference from one time point to the next. [Bhattacharya, 2007;Conti et al., 2009] 4) Use a Gaussian stochastic process as a Bayesian prior. [Liu and West, 2009] 5) Develop a hybrid dynamic/emulated model, or a \"Mechanistic dynamic emulator.\" [Albert C., 2012;Reichert et al., 2011] In the context of this study, strategy 1 is conceptually the simplest but it is not clear a priori how far into the past the system's memory extends and each included time step multiplies the dimensionality of the input space. Strategy 2 is complicated by identifying basis functions that adequately capture the various contributing signals. For example, a Fourier transformation is a natural solution except that storm surge is non-periodic and an important contributor to estuarine WLs. Strategies 3-5 all have potential advantages, but bring additional complexity to an already complex system so were not explored further. For reducing level 4 error, GPR is a flexible framework and there are likely gains to be made through a more exhaustive approach for emulator specification. In particular, handling of the periodic nature of tides within the covariance function [Roberts et al., 2013] is a promising direction for reducing error."}, {"section_title": "Computational Cost Considerations", "text": "Emulation is an approach to dramatically reduce simulation times and is particularly valuable in situations where the simulator must be run for very long periods or repeatedly for many iterations (i.e., probabilistic risk assessment). While emulation requires an upfront cost, through the running of multiple simulations to construct a training dataset (see section 4.2.5), after this initial computational cost is paid the emulator is comparatively instantaneous. As the nature of the trade-off is computation time, it is useful to review the costs of building the training dataset. The first control on computational cost for the training set is the number of input dimensions. The simplifications implemented in this study managed to reduce the input space to 16 dimensions. Each design point took approximately 66 core hours to run so a full experimental design of 160 points would require over 10.5 thousand core hours (although with parallelization the actual time was much less). This study developed a larger experimental design (over 400 points) but this was primarily for validation rather than emulator skill (see discussion below). Full ADCSWAN required approximately 18 core hours per day of simulation time. This means that the comparison in Figure 34 (simulating 31 years of tide gauge data across 7 scenarios) would have required around 80 thousand core hours to complete. Once the emulators had been constructed, this analysis was accomplished in just hours. In this specific case, emulation becomes an efficient option if approximately one and a half years of simulation are required. This limit is highly situationally dependent and is controlled by computer, simulator, emulator, etc. and is intended only as an order of magnitude reference. Furthermore, emulation is primarily targeted at probabilistic methodologies rather than hindcasting for which multiple iterations of time series quickly sum to very large total simulation times. The above analysis is based on a LHS design and the Loeppky et al. [2009] guideline that a training dataset should be around 10 times the number of input dimensions. However, LHS is one of many possible experimental designs [Levy and Steinberg, 2010]. Significant research has focused on optimizing experimental designs beyond LHS and it is possible that a more complex design could reduce the size of the training dataset. For example, LHS does not consider the probability that a particular combination of input parameters may occur. Therefore, some design points are likely poorly utilized exploring space that is physically impossible or highly improbable (for example, high wave heights associated with low wave periods). Finally, the above analysis did not consider the effect of training dataset size on skill. This relationship was tested by quantifying emulator performance at a variety of training dataset sizes. For this analysis, the total body of simulations (480) was partitioned into smaller dataset sizes ranging from 50 to 450 simulations for testing. For each smaller dataset, a k-fold validation with 5 segments was performed ( Figure 35) to quantify emulator skill at this smaller training dataset size. This analysis is identical to that described in section 4.3.2 but with an artificially decreased training dataset size. Our results are in good agreement with the guidance of Loeppky et al. [2009] in that ten times the number of input dimensions is sufficient for building a skillful emulator (Figure 35). Beyond this limit, only very small gains in skill are realized, suggesting that it is not efficient to over build the training dataset."}, {"section_title": "Emulation Beyond WLs", "text": "Although this study has primarily focused on WLs, emulation can easily be extended to other variables in a coastal hazards framework. Hs was emulated at the observational Hs stations from the 1999 USACE field campaign ( Figure 25) [Cialone et al., 2001;2002] for validation. The Hs emulators were developed identically to that of WLs except that Hs emulation was found to not need cubic terms for the prior mean function. A comparison to observations (Figure 36) shows that the emulator performs well for Hs with the peak (around 26 September, 1999) being well reproduced by the emulator at stations 0, 1, 2, 3, and 4 ( Figure 36). Performance is comparatively poor at stations 5 and 6, which are further within the estuary and less dominated by offshore waves. This result is quantified in : Comparison of observations (USACE deployments), full simulator and emulated Hs time series. Symbols are used for the observations for the sake of visual clarity. All panels have the same y-axis scaling but cannot resolve these instantaneous jumps. For this reason, it is important to carefully consider the form of the output variable being emulated and it's relation to the input parameters. "}, {"section_title": "Extreme WL Event Decomposition", "text": "The decomposition of annual maximum WLs throughout the bay into its constituent components supports the findings in Chapter 3 that extremes in PNW estuaries are controlled by compound events. This conclusion is further supported by the variance in emulated extreme WL contributions (not shown) which demonstrate that the composition of each annual maximum event varies widely across the 31 yearly events. The mean contribution of each forcing is found to be significant providing evidence that all included forcing processes are important for properly quantifying extreme water levels. The only exception to this is streamflow which is found to be of nominal importance for all regions other than very near the streamflow boundary. This result is likely specific to the Grays Harbor estuary and would be different for a more hydrologically dominated estuary system [Chen et al., 2014, Lavery andDo novan, 2005;Svensson and Jones, 2004]. While most components (other than streamflow) contribute to extreme WLs across the estuary, significant wave contributions are only found at locations away from the center of the bay. For comparison, Olabarrieta et al. [2011] investigated the influence of wave setup in Willapa Bay, WA using a 3-D fully coupled ocean-wave model. They found that wave induced setup is relatively consistent across the estuary domain with only a small decay of magnitude with distance into the estuary. Willapa Bay has a significantly shallower mouth than Grays Harbor, so wave breaking (and therefore radiation stress induced wave setup), is likely occurring immediately upon entering the estuary. Grays Harbor has a deeper channel allowing waves to penetrate further into the estuary. However, Figure 36 does show some wave attenuation at both station 3 and 4 so it is somewhat unexpected to see no wave contribution at these locations. Further investigation, however, shows that the choice of investigating annual maximum  WL components based on annual maxima NTR events reveal waves contributing to WLs at all station transects. This suggests that the missing wave contribution for interior stations in Figure 34 may be because maxima WLs are not concurrent with the largest wave heights. The relatively smaller waves during some of these events are able to penetrate into the estuary without breaking and therefore do not contribute to WLs. An analysis of forcing agrees with this conclusion, showing that waves are, on average, 1.8 meters smaller for max annual WL events than for max annual NTR events."}, {"section_title": "Acknowledgments", "text": ""}, {"section_title": "Chapter 5 -General Conclusion", "text": "This dissertation has explored challenges, approaches, and conclusions in regards to climate change impacts to flooding hazards in the Pacific Northwest (PNW). Using multiple study sites and modeling perspectives, this dissertation has pushed the current state of understanding for these systems as well as developed tools and methodologies for further progress. The following sections provide a brief synthesis of the preceding chapters."}, {"section_title": "Chapter Conclusions", "text": "Chapter 2 of this dissertation focused on the problem of bias in global climate models (GCMs). When using GCM output as forcing, bias can propagate leaving hazard assessments out of alignment with reality. Therefore, some form of bias correction is often necessary. This chapter specifically addressed biased wave model data but the developed procedures and conclusions are more broadly applicable and were used for a variety of GCM output variables in other chapters. It was found that statistical bias correction is an effective tool for realigning biased model output data with observations. A robust testing of a variety of different methodologies highlighted the importance of inter-variable correlation structure and that this structure can be distorted without proper consideration. For this reason, bivariate methods that explicitly correct correlation structure were shown to be superior to univariate methods. This chapter additionally explored BC performance under nonstationary conditions through evaluating or extending tested methodologies functionality for a climate change context. Chapter 3 built upon the tools developed in Chapter 2 to consider climate change impacts to two PNW estuary study sites. A multi-component modeling framework was developed to consider the combined impact of climate change on multiple estuary forcing components. Using decadal scale simulations, water level (WL) time series were analyzed to quantify changes to return interval (RI) event magnitude. Key findings were the importance of spatial variability in flooding characteristics with both study sites exhibiting increasing extreme WL magnitude with further distance into the estuary (a gradient of around 25 cm). As controlled by climate model forcing, this gradient is predicted to increase for 100-year RI events. An investigation of contributions to WLs found that extremes in the Pacific Northwest are defined by compound events. Tides are the largest component of extreme WLs and an important modulator of extreme event occurrence. Finally this study explored the meaning of RIs in a nonstationary context. Using effective RIs it was found that the full effect of SLR is predicted to be offset by a decrease in forcing. Smaller RI events are buffered less by this effect than large RI events suggesting that low RI flooding events will be more impacted by SLR. Chapter 4 proposed an alternative methodology to that employed in Chapter 3. An \"emulation\" based approach was investigated which allows for a fast statistical approximation of complex process-based models. This methodology was found to predict WLs skillfully across a range of observational datasets. A hierarchical validation showed that the largest loss of skill in emulator construction was from the process-based model approximating observations. The emulator was additionally found to predict other estuarine variables (significant wave height and non-tidal residual) skillfully. Finally, an example application of the emulator explored forcing contributions to extreme WLs. All forcings were found to contribute to extremes with streamflow being the smallest in magnitude and wave contributions being confined to the edges of the estuary."}, {"section_title": "Conclusions on Methodologies", "text": "This thesis provides a unique perspective on climate change impacts to estuarine flooding through approaching the problem with multiple methodologies. Research into climate change impacts to hazards can be broadly categorized into two distinct approaches: (1) observationalbased and (2) process-based modeling. While both Chapter 3 and 4 propose distinct perspectives, they are both foundationally process-based modeling approaches. Emulation is best defined as a \"hybrid\" methodology, being a statistical model of a process-based model. However, emulator predictions are still reliant on process-based model accuracy rather than a function of observed environmental variables (other than through validation). A fully distinct approach would follow an observational-based methodology (most commonly through statistical analysis). Observational studies have seen great success in investigating flooding [Kirshen et al., 2008, Serafin and Ruggiero 2014, Wahl et al., 2015 and the benefits of using in-situ data are many, including the avoidance of approximations and simplifications inherent to numerical modeling. This dissertation's reliance on process-based methodologies represents both a strength and a weakness. One limitation from adopting a process-based modeling approach is visible in the problem of bias in GCM data. Specifically, in the case of GCMs, process-based models are currently not able to reproduce observations due to a variety of insufficiencies ranging from course model resolution to incorrect physics. The BC methodologies presented in Chapter 3 are essentially a post-processing fix to realign incorrect model output with observations. As is mentioned in Chapter 2, it is a fix that comes with a wide range of caveots, theoretical issues, and likely incorrect assumptions [Ehret et al., 2012]. It is expected that, with increasing computation power and better GCM physics, the problem of bias will be removed and BC will no longer be necessary. This conclusion can be extended to process-based models in general: one of their main weaknesses is an inability to reproduce observations. This is for a variety of reasons ranging from incorrect physics, assumptions, numerical approximations, resolution, and many other sources of error. Constantly improving scientific understanding, models, and computing power continue to reduce these sources of model error making process-based modeling more viable through time. A strength of process-based modeling, and the primary reason why this dissertation does not explore an observation-based statistical model, is the ability to extend information beyond observational sampling points. Most studies using observational information for coastal flood hazard analysis make use of tide gauge records and a large proportion of estuaries, particularly in the PNW, are ungauged or poorly sampled. Early engagement with communities and stakeholders placed a strong emphasis on producing spatial hazard information, in particular flood maps. Flooding maps require either process-based modeling or strong assumptions in regards to extending spatially limited observations. For example, point water level estimates are often extrapolated using a static \"bath-tub\" approximation (i.e., the assumption of a horizontal water surface). Previous research [Gallien et al., 2014] as well as this dissertation (Chapter 3) has shown that this can result in significant errors due to spatial variability in extreme WLs. An additional reason why process-based models were chosen for this dissertation is the focus on climate change and predictions into the future. An ideal process-based model would be as accurate for the future period as it is for the current period. With a perfect understanding of future forcing, a validated process-based model which captures all of the relevant physics should produce a correct forecast for the future. This is, of course, impossible as neither process-based models, nor our understanding of future forcing are perfect. Nevertheless this is a major attraction for using a process-based approach for understanding climate change. Statistical observation-based models can be used for future predictions but only very carefully. Some form of trend extrapolation or other model of change is required. More problematically, it must be assumed that the statistical relationships on which the model are built will not change into the future (or will change in a way specified by the model) [Wilby et al., 2004]. However, these issues can be minimized and the current imperfection of GCM models bring to question how well GCMs are able to predict the future as well. While not exploring a fully observational perspective, this thesis does develop a contrast between process-based models and emulation. The main differentiation between these two methodologies is that of simulation time. Long simulation time is the second major weakness of process-based modeling for which emulation is a direct response. Reducing simulation times is beneficial in multiple ways, both as allowing analysis which requires long or multiple simulations (e.g. probabilistic assessment, optimization, etc.) as well as allowing more complex simulations. Simulation time and simulator accuracy are, in general, linked as the major sources of error in simulators are often sourced from simplifications to reduce simulation time, for example coarse spatial or temporal resolution, reduced physics, etc. Therefore a method that can reduce simulation times potentially alleviates both main weaknesses of a process-based methodology: both by allowing long/multiple simulations and a more accurate simulator. However, it is important to remember that emulation is an approximation of a processbased model. This is especially true if, as detailed in Chapter 4, a reduced form of the simulator is used. Results from Chapter 4 showed nominal increases in error from emulation and simulator simplifications but this result is not general. It is expected that, if a more accurate simulator were used, the error from emulation and simulator simplifications would become more predominant. An example of this can be seen in the importance of pressure in producing a cross-estuary gradient in WLs in Chapter 3 results. This effect is lost in Chapter 4 due to simplified pressure fields. Therefore, while emulation can be extremely effective, dynamic models will always be preferable if computational time is not a factor. With drastic increases to processing power, the benefits of emulation will be reduced."}, {"section_title": "Future Work", "text": "Future work will be aimed at extending conclusions drawn at specific study sites to the broader PNW and beyond. Of particular interest, these studies show important results in regards to WL contributions across estuary domains. However, the 3 main estuaries investigated in this dissertation are all characteristically similar in terms of size and forcing profiles. Any attempts to generalize conclusions would require a larger distribution of estuary types. This is especially true in regards to streamflow which was found to be a negligible factor in extreme WLs. Preliminary work at a smaller estuary (La Push, Washington) showed a much larger proportional contribution from streamflow. It is expected that there is a gradient in the importance of streamflow ranging from minimal (as seen in Grays, Tillamook, and Coos) to dominant in very small river inlets. Other forcings may also change across this gradient, for example wind which, for small estuaries, would not have a large fetch to build cross-estuary WL differentials. Exploring different size and types of estuaries could provide important conclusions in regards to what processes are important across the full range of estuary systems. Even without exploring multiple estuary types, this dissertation lays a strong foundation for understanding the processes that effect flooding within estuaries in the PNW. Both Chapter 3 and 4 show the importance of compound events with tides modulating when extreme events occur. The long datasets of extremes developed in this dissertation could provide rich information for further exploring the joint probability of occurrence between the various contributors to extreme WLs. It is clear, from examining annual maximum WLs, that adding maximum NTR to maximum tidal WLs results in significant over-predictions of extreme events. It is also clear that just looking at tidal WLs (the dominant component of extreme WLs) results is a significant under-prediction. Developing a better understanding of where flooding estimates fall between these two extremes could have important implications for flooding hazard assessment. For example, it could result in the ability to predict what conditions will result in an annual flooding event in the PNW. Future work will re-analyze these datasets with a focus on characterizing the nature of PNW compound events. This dissertation discusses at length the importance of uncertainty in future hazard predictions. This is argued as a major limitation to the modeling framework in Chapter 3 and a primary motivating objective for emulation (Chapter 4). Future research will build on the tools developed in this dissertation towards realizing a fully probabilistic flood hazard projection. In terms of a future forecast, there are multiple sources of uncertainty [Uusitalo et al., 2015]. The first is uncertainty as a function of model error. While not explicitly detailed in this dissertation, GPR was partly chosen due to a natural handling of uncertainty in model predictions. The GPR posterior distribution gives detailed probabilistic information in regards to uncertainty in the GPR's ability to approximate the simulator. This information can be supplemented with information on uncertainty from the simulator trying to reproduce observations [Higdon et al., 2004;Kennedy et al., 2001]. Additional sources of uncertainty come from climate variability which can be assessed through ensembles [Kay, 2015;Murphy et al., 2007] or statistical simulation models [Serafin et al., in revision]. Finally there is uncertainty in future projections a function of the future climate, especially as controlled by uncertain anthropological inputs [Nakicenovic et al., 2000]. This is generally handled through GCM ensembles under various climate scenarios. Future work will attempt to consolidate these multiple uncertainties into an integrated probabilistic future hazard assessment. The Direct Method (Future) The future extension for the Direct method is detailed in Figure 39. The overarching goal of the methodology is to find the components necessary for an equidistant quantile mapping: the quantile of the model historic and target datasets (associated by correlation) so that we can apply the difference to the model future quantile of interest. These two quantiles are found using two standard Direct methods applied to their respective datasets. The first step is to bias correct the first variable (X) using a standard equidistant quantile mapping. This step is not shown in Figure 39. The second step (2) is to fit a copula to all three of the model future, model historic, and target datasets. This step is performed only once while all following steps are repeated for each value in the model future dataset. The next step is to find the bivariate CDF value for the model future copula corresponding the CDF value for model future variable X and Y (3). This is done by finding the quantiles corresponding to the model future value pair and evaluating the copula at this point. At this point the two Direct methods diverge with the first branch using the target copula and the second branch using the model historic copula. This divergence corresponds to the goal of finding the quantile of model historic and target datasets. With these two copulas, the next step is to take a slice of the bivariate CDF at the known bias corrected value (variable X). As with the normal direct method (see Figure 6), the value of CDFX is already known since it is the bias corrected variable X (step 1). If we slice the copula at this value (4) and use our knowledge of the bivariate copula CDF value of the model (evaluated in step (3)), we can find CDFY (5). At this point we found the quantiles of the model historic and target datasets associated with the model future Copula value. With these values, a normal equidistant quantile mapping method is performed by finding the ValueY corresponding to these quantiles and quantifying the difference between these values (6). Finally, we find the quantile of the model future value (7) and apply this difference to find the bias corrected value (8). "}]