[{"section_title": "Contents", "text": "The AIR/NCES Policy Fellowship Program .................................................................................. Executive Summary ........................................................................................................................ Why Focus on Transfers-out of an Institution? .............................................................................. State or system student unit record systems. ........................................................................ National Student Clearinghouse. .......................................................................................... Changes for 2009 IPEDS GRS. ............................................................................................ Part B: Institutions that did not report the number of transfers in 2007 ....................................... Survey respondents. .............................................................................................................. Institutional mission. ............................................................................................................. Internal reporting ................................................................................................................ "}, {"section_title": "Executive Summary", "text": "Key findings include: Trends in Transfer-out Reporting Study (2002)(2003)(2004)(2005)(2006)(2007) \u2022 Comparing the total number of transfers-out from IPEDS with the 1995-96 Beginning Postsecondary Students (BPS) Survey suggests that transfers-out are significantly underreported on the IPEDS Graduation Rate Survey. \u2022 Institutions in the lowest total cohort graduation rate quintile are more likely than institutions in the highest quintile to report the number of transfers-out. \u2022 While the percentage of institutions reporting the number of transfers-out declined between 2002 and 2007 (41% to 35%), the percentage of the adjusted cohort enrolled in institutions reporting the number of transfers-out has remained steady (~49%). \u2022 Institutions located in states with state and/or system student unit record systems are more likely to report the number of transfers-out. \u2022 Larger institutions are more likely to report the number of transfers-out than smaller institutions."}, {"section_title": "Online Survey of Institutions", "text": "\u2022 The most commonly used source of data on transfers-out of an institution on the 2008 IPEDS GRS component was the National Student Clearinghouse (46.6%). However, state and system student unit record systems were used by 40.9% of institutions. \u2022 Thirty-one respondents at institutions that did not report transfers-out on the IPEDS GRS reported having a transfer mission. The most common reasons for not reporting included lack of data, staff, and time. \u2022 Approximately one-fifth of the institutions that did not report the number of transfers-out and do not have a transfer mission reported tracking transfers-out internally. Slightly over 80 percent of these institutions track transfers-out internally using the National Student Clearinghouse. \u2022 While many institutions conduct exit and follow-up surveys of non-returning students (40.4%), very few of them use the data for IPEDS reporting (4.3% and 4.2%, respectively). \u2022 Less than 2 percent of the institutions surveyed reported using transcript and/or withdrawal requests for IPEDS reporting."}, {"section_title": "Data Sources", "text": "The above findings were based on two studies that were developed and implemented between September 2008 and August 2009. These studies included: 1. The Trends in Transfer-out Reporting Study (TTRS). The TTRS is based on data collected on the IPEDS Graduation Rate Survey between 2002 and 2007. 1 The purpose of the study is to examine the usefulness of existing data on the number of transfers-out. It includes an analysis of institutions by sector, state, and size. It also includes a descriptive analysis to evaluate the magnitude of the transfer-out phenomenon. 2. The Online Survey of Institutions (OSI). The OSI was a web-based survey of 1,500 potential 2 IPEDS keyholders at four-and two-year, degree granting institutions across the United States. Over 800 (n = 814, 54.3% response rate) invited participants responded. Participants received one of two surveys based on whether or not their institution reported the number of transfers-out on the 2007 IPEDS GRS survey. The OSI primarily asked questions on transfer mission and data sources."}, {"section_title": "Why Focus on Transfers-out of an Institution?", "text": "This study examines the different processes that institutional researchers use to calculate the number of transfers-out for the Integrated Postsecondary Education Data System (IPEDS) Graduation Rate Survey (GRS), with a particular focus on how the process is influenced by institutional mission and data availability. Policy makers and the public have been very interested in what colleges are doing to improve degree attainment for over seventy years (e.g., McNeeley, 1937). In the 1970s, research on dropping out, stopping out, and transferring began to increase (e.g., Astin, 1975;Cope & Hannah, 1975;Tinto, 1975). Pantages & Creedon reported that only five out of ten students who enter college in the United States will eventually graduate from the same college and that, \"Of the five students who dropped out of the college altogether, four will reenroll at a different college, and of those four reenrollees, only two will graduate\" (1978, p. 49). In 1989, a study of graduation rates by the National Institute of Independent Colleges and Universities found that just 42.7% of students at 4-year public institutions and 54.2% of students at private institutions graduated within six years (Porter, 1989). Given the wide-spread interest in improving degree attainment, understanding what happens to a student after he or she leaves an institution is essential; unfortunately, very little is known about the different processes that institutional researchers use to track transfers-out of an institution. This study provides an important first look at what the data in IPEDS can tell us about transfers-out."}, {"section_title": "A Brief History of Transfer-out Reporting and the IPEDS GRS", "text": "When Congress enacted the Student Right-To-Know and Campus Security Act of 1990 (SRK), which required institutions to disclose graduation rates within 150 percent of normal time for graduation from the program, the stated intention of Congress was that, \"Knowledge of graduation rates would help prospective students and prospective student athletes make an informed judgment about the educational benefits available at a given institution of higher education\" (Public Law 101-542). The Secretary of Education determined that completing the IPEDS Graduation Rate Survey satisfies the disclosure requirements of the SRK Act. In the final regulations implementing the SRK Act, reporting the number of transfers-out of an institution became part of the GRS; because IPEDS is required, reporting the number of transfers-out is also required. Four types of documentation that a student had transferred to another institution were enumerated: (a) certification letter or document from the receiving institution; (b) electronic certification or secure e-mail message; (c) confirmation of enrollment data from a legallymandated, statewide or regional tracking system; and (d) other documentation of enrollment at the receiving institution. After the Higher Education Amendments of 1998 (Public Law 105-244), only those institutions that considered \"substantial preparation\" for transfer to another institution as part of their mission were required to report counts of transfers-out. The proposed rules generated after HEA-98 recognized that, \"Institutions with substantial numbers of transfers-out may have a lower graduation and completion rate than other institutions and thus may find it desirable to report a transfer-out rate\" (64 Fed. Reg. 59062). It appears that many institutions, even those without transfer missions, have found it desirable to report transfer-out counts. Based on the 2007 IPEDS Graduation Rate Survey (GRS) data, 58% of public and 27% of private, four-year institutions and 74% of public and 30% of private, two-year institutions reported the number of transfers-out. In the commentary on the final SRK Act regulations, published in the Federal Register on December 1, 1995, several commentators \"suggested that the Secretary encourage institutions to supply additional information to place their graduation rate reports in context, as a way of providing greater comparability and usefulness\" (60 Fed. Reg. 61779). While the Secretary strongly encouraged providing contextual information, it was determined that there was no statutory authority to require institutions to provide it. Given the current increased interest in the IPEDS Graduation Rate Survey in general and the number of transfers-out in particular, it is an ideal time to examine the processes and data sources institutions use to report the number of transfers-out on the IPEDS GRS."}, {"section_title": "Sources of Transfer-out Tracking Data", "text": "The Higher Education Opportunity Act of 2008 prohibits the Department of Education from developing, implementing, or maintaining a Federal database of personally identifiable information on students except for systems necessary for the operation of programs authorized by Titles II, IV, or VII and that were in use by the Secretary, directly or through a contractor (e.g., the National Student Loan Data System (NSLDS)), as of the day before the date of enactment of the legislation. However, four other broad classes of sources exist that enable institutions to report the number of transfers-out of the institution: 1. State student unit record databases. Forty of the fifty states have student unit record databases, which cover 77 percent of national FTE enrollments. Pennsylvania and Michigan are the largest states without a student unit record system. The remaining 8 states are Delaware,Idaho,Iowa,Montana,Nebraska,New Hampshire,Rhode Island,and Vermont. 3 2. System student unit record databases. System student unit record databases are similar to state student unit record databases, except that they only cover one system. Seven states use multiple databases: California (UC, CSU, and CCC), North Carolina, Oregon, Washington, Wyoming, and New York (SUNY and CUNY). 3. The National Student Clearinghouse. The National Student Clearinghouse serves as a nexus between educational institutions, the federal government, guaranty agencies, lenders, and student loan servicers. It is a non-profit organization whose programs are designed to reduce administrative burden on colleges. Colleges participating in the core service, EnrollmentVerify, DegreeVerify, and that provide 12 additional data elements beyond those required for the core service (e.g., middle name, class level, college student ID) receive free access to the StudentTracker service. There is a fee schedule for schools that do not participate in all programs. The StudentTracker service allows institutions to receive subsequent enrollment data for an unlimited number of cohorts. StudentTracker contains enrollment data for over 92% of U.S. higher education students. 4 4. Institutional surveys, exit interviews, and administrative records. Many institutions administer surveys and exit interviews to departing students to better understand the reasons for departure. Transcript request forms and withdrawal forms are also commonly used. The major problem with these methods of tracking transfer is that they often collect data on intention to enroll and not actual enrollment at another institution."}, {"section_title": "Questions for Research", "text": "The purpose of this research was three-fold and closely tied with the objectives of the Policy Fellowship Program-to improve the quality, comparability, and usefulness of transferout reporting on the IPEDS GRS. Quality -In educational research, there is often a lot of discussion about reliability and validity. There was no attempt made to \"audit\" data previously reported in IPEDS for this study. However, potential data sources were evaluated for reliability and validity. For example, can the data source produce an estimate of the number of transfers-out that complies with the current IPEDS reporting requirements? Can the data source or process produce reliable results? Comparability -Two kinds of comparability were examined in this study. The first looks at comparability among institutions. Two institutions may report the same transfer-out rate, but one may be a within-system transfer-out rate, while the other is a national transfer-out rate based on the National Student Clearinghouse. Both are estimates of the \"true\" transfer-out rate, but clearly one is more comprehensive than the other. The other kind looks at comparability among NCES surveys, in this case the IPEDS GRS and the Beginning Postsecondary Student (BPS) Longitudinal Study. Usefulness -For a data element to be useful, it must be able to make some contribution to public policy and research. Usefulness requires more than just quality and comparability. The data element needs to be disseminated in such a way that policy makers and research can use it appropriately in their own studies. This research report was developed using the same data tools available to the public (i.e., IPEDS Data Center, BPS DAS). Many of the suggestions for improving usefulness generated during the past year are not included in this report. To accomplish the above goals, the following research questions are addressed: 1. How has institutional reporting of the number of transfers-out changed over time? "}, {"section_title": "Brief Methodology", "text": "To address this project's five research questions, two studies were developed and implemented between September 2008 and August 2009. They included: 1. The Trends in Transfer-out Reporting Study (TTRS). The TTRS is based on data collected on the IPEDS Graduation Rate Survey between 2002 and 2007. 5 The purpose of the study is to examine the usefulness of existing data on the number of transfers-out. It includes an analysis of institutions by sector, state, and size. It also includes a descriptive analysis to evaluate the magnitude of the transfer-out phenomenon. 2. The Online Survey of Institutions (OSI). The OSI was a web-based survey of 1,500 potential 6 IPEDS keyholders at four-and two-year, degree granting institutions across the United States. Over 800 (n = 814, 54.3% response rate) invited participants responded. Participants received one of two surveys based on whether or not their institution reported the number of transfers-out on the 2007 IPEDS GRS survey. The OSI primarily asked questions on transfer mission and data sources. The OSI study was supplemented by a qualitative component. The diverse nature of higher education institutions and institutional practice meant that some respondents had difficulty selecting a single \"best\" answer on the survey. When appropriate, interview data is summarized in this report to augment quantitative results. Transfer-out Reporting Study (2002-2007 The goals established for the TTRS included:"}, {"section_title": "Findings: The Trends in", "text": "(a) determining how many and which types of institutions report the numbers of transfers-out, (b) exploring the impact of state data systems, (c) examining differences in reporting by institution size, (d) estimating the magnitude of the transfer-out phenomenon, and (e) comparing the total reported number of transfers-out in IPEDS against the Beginning Postsecondary Student (BPS) Longitudinal Study. 7 An important caveat: Parent/child relationships A parent/child relationship exists when one institution shares a common Program Participation Agreement (PPA) with multiple campuses. Each of the multiple campuses has its own unique unitid number. The ways that NCES has dealt with these relationships has varied greatly across surveys and IPEDS years. The most recent IPEDS GRS surveys include allocation factors. For the TTRS study, an institution was considered as reporting transfers-out if the parent institution reported transfers-out. No attempt was made to allocate parent responses to children since the study focuses on totals. Other research questions may require closer examination of the changing nature of parent/child relationships on the IPEDS GRS survey over time. Number of institutions in the TTRS. Calculating the percentages in the tables that follow requires an accurate denominator. The TTRS begins with the First Look Universe, which consists of institutions that are currently in the IPEDS universe, are open to the public, participate in HEA Title IV federal financial aid programs, and are primarily postsecondary. Institutions in the TTRS must have had a cohort of first-time, full-time undergraduate students entering the institution in the cohort year. 1,283 1 A significant revision to the parent/child reporting process was implemented in fall 2004, which accounts for the majority of the reduction in the number of institutions. 2 Prior to 2005, the First Look Universe was referred to as the E.D. Tab Universe. Both refer to institutions that are (1) currently in the IPEDS universe, (2) open to the public, (3) a participant in federal financial aid programs, and (4) are a primarily postsecondary institution. For this study, the universe is restricted to institutions in the 50 states and the District of Columbia (U.S. Only). 3 TTRS-Eligible Institutions are (1) currently in the IPEDS universe, (2) open to the public, (3) a participant in federal financial aid programs, (4) are a primarily postsecondary institution, (5) U.S. only, and (6) enrolled full-time, first-time students in the cohort year. Consistency of reporting by institutions. Approximately 78% of institutions consistently reported graduation rate data during the six-year study period. 8 An additional 10.8% of institutions appear to be newly-reporting institutions 9 , while 7.6% of institutions appear to have \"closed\" or otherwise stopped reporting 10 . The remaining 3.8% reflect various patterns of missing data. These patterns of missing data could be due to the relative volatility of the private, for profit sector.  2open to the public, (3) a participant in federal financial aid programs, (4) are a primarily postsecondary institution, (5) U.S. only, and (6) enrolled full-time, first-time students in the cohort year. 2 The response pattern reflects the patterns of missing data across UNITIDs over the six year study period: \"1\" represents that data was available and \".\" represents that data was not available. 3 An additional 29 response patterns, each with less than 6 cases, were collapsed into \"other\". \"Does the mission of your institution include providing substantial preparation for students to enroll in another eligible institution without having completed their programs? If you answer Yes to this question, you will be expected to report transfer-out data. If you answer No to this question, you may report transfer-out data if you wish.\" The responses to the screening question indicate large differences between control and level. It also raises some questions. For example, if the sole mission of a 2-year college is to provide technical training resulting in an Associate in Applied Science-and does not offer a formal transfer program-but participates in an articulation agreement with a 4-year university by which some of those credits automatically count towards the general education requirements of a bachelor's degree, should the 2-year institution report those students as transfers? For an institution seeking to avoid reporting, it might say that such agreements are not substantial; however, by entering into such agreements, the institution is acknowledging the importance of offering transferable credits even in the most highly specialized technical and career training programs. Currently, there is no standard definition of what counts as substantial preparation. The current regulations leave it up to each institution to decide. Additionally, there is no standard definition of what counts as enrollment. If a student leaves an institution and then takes a single course within the reporting period at another institution, but did not enroll in a program at the receiving institution, does that count as a transfer? A consistent and reliable transfer-out rate that is useful for public policy and research requires greater standardization-even at the risk of reducing the number of reported transfers. Public (n = 1,962) 39.6% 4-year or above 11.6% 2-year 61.0% Less than 2-year 3.3% Private, Not-for-Profit (n = 1,546) 8.7% 4-Year 8.5% 2-Year 11.5% Less than 2-year 4.1% Private, For-Profit (n = 2,089) 4.3% 4-Year 1.1% 2-Year 2.6% Less than 2-year 5.9% 1 TTRS-Eligible Institutions are (1) currently in the IPEDS universe, (2) open to the public, (3) a participant in federal financial aid programs, (4) are a primarily postsecondary institution, (5) U.S. only, and (6) enrolled full-time, first-time students in the cohort year. Percentage of institutions reporting transfers-out. Two different methods of calculating the percentage of institutions reporting transfers-out are useful for policy discussion. The first is the unweighted percentage of institutions reporting the number of transfers-out. The unweighted percentage gives a general indication of the effects of institutional mission and data availability. The second is weighted using the size of the adjusted cohort. The weighted percentage indicates the coverage of the real population of interest-first-time, full-time students. While the percentage of institutions reporting has declined (41% to 35%) the overall coverage has remained approximately the same (~49%). Total adjusted cohort. The size of the IPEDS GRS adjusted cohort reflects the total number of first-time, full-time degree-or certificate-seeking students minus any permissible exclusions 11 . Over the study period, the size of the adjusted cohort enrolled in TTRS-eligible institutions increased by 23.8% from 1,973,484 to 2,442,998.  1 Total number of reported transfers-out. Since institutions are not required to report the number of transfers-out of the institution the figures in Table 7 reflect the minimum number of transfers-out nationally. As noted in Table 5, approximately half of the total adjusted cohort is located at institutions that do not report the total number of transfers-out, which implies that these figures could be significantly underestimated, especially for private institutions, which account for the smallest percentages of reported transfers-out (Table 8).  1   Reported transfers-out as a percentage of adjusted cohort. Dividing the number of transfers-out by the total adjusted cohort provides an estimate of the \"national\" transfer-out rate (Table 9). However, as noted above, the number of transfers-out may be greatly underreported. For comparison, the six-year \"transfer rates\" for full-time students by sector of first institution from the 1995-96 Beginning Postsecondary Students (BPS) Survey are listed in Table 10. When those percentages are applied to the total adjusted cohort figures in Table 6, the estimated number of transfers-out is three times the total in Table 7. This suggests that researchers should be cautious about making inferences based on aggregate number of transfers-out beyond the institutional level.    State effects on transfer-out reporting. The Higher Education Opportunity Act of 2008 (HEOA) prohibited the Department of Education from \"developing, implementing, or maintaining\" any new Federal databases of personally identifiable information on students. However, HEOA permits the maintenance of current systems necessary for the operation of programs authorized by Titles II, IV, or VII (e.g., the National Student Loan Data System); it also permits states or groups of states to develop student unit record systems. The American Recovery and Reinvestment Act of 2009 provided funding to enable state educational agencies to design, develop, and implement statewide, longitudinal data systems to efficiently and accurately manage, analyze, disaggregate and use individual student data. For many states, this means expanding current postsecondary databases to include K-12 student data. Of the 47 postsecondary databases studied in Ewell and Boeke (2007), 9 are from the 1970s and 14 are from the 1980s. Only 8 of the databases were less than 10 years old. 12 The two figures and five tables in this section examine the percentage of institutions reporting transfers-out (unweighted and weighted) in relation to the state of the institution. Not surprisingly, those states with active state student unit record databases (e.g., New Jersey) and system student unit record databases (e.g., California) have the highest reporting percentages in terms of both the total number of institutions as well as the size of the adjusted cohort. States without student unit record databases tend to have lower reporting percentages in terms of both the total number of institutions as well as the size of the adjusted cohort. One benefit of participation in state and system student unit record systems is the possibility of centralized reporting. Some institutions commented that the institution's Graduation Rate Survey is pre populated by the state/system coordinators, which helps ensure uniform and consistent reporting. In addition to knowing that a state has one or more active student unit record systems, it is important to know its capacity for tracking transfer students. Unfortunately, it is impossible to tell from previously collected IPEDS data if the list of non-returning students was checked against a national database (e.g., the National Student Clearinghouse) or other local databases. Institutions that receive transfer-out data from a student unit record system were asked about whether the system links to other data sources on the Online Survey of Institutions; however, institutions may not be fully aware of these linkages. The study of state student unit record systems being undertaken by SHEEO for NCES to update the Ewell and Boeke (2007) report will likely provide a more complete picture of the linkages between systems. Additional questions on tracking transfers were added to that survey as a result of this research; however, the results are not available at this time. 0 % 1 -2 5 % 2 -5 % 1 -7 5 % 7 6 -9 9 % 1 0 0 % The map below (Figure 1) shows the coverage of public, 2-year enrollment at institutions that report the number of transfers-out. For example, 100% of New Jersey's public, 2-year institutions reported the number of transfers-out; the New Jersey Commission of Higher Education is the state's SHEEO and coordinates data collection through the Student Unit Record (SURE) system. 13 Only 45% of Idaho's public, 2-year enrollment is covered; it does not have a student unit record system at the SHEEO or system level.  0 % 1 -2 5 % 2 -0 % 1 -7 5 % 7 6 -9 9 % 1 0 0 % The map below (Figure 2) shows the coverage of public, 4-year enrollment at institutions that report the number of transfers-out. As in the above figure, 100% of New Jersey's public, 4 year institutions reported the number of transfers-out. In general, the coverage of public, 4-year enrollments is lower than the coverage of public, 2-year enrollments. In a few states (e.g., Mississippi, New Mexico, Kentucky), none of the public, 4-year institutions reported the number of transfers-out.       Transfer-out reporting by graduation rate. The final factor considered in the TTRS was the relationship between institutional size and total cohort graduation rate. Total cohort graduation rate is calculated as the total number of completers within 150% of normal time divided by the revised cohort minus any allowable exclusions. Institutions were grouped into quintiles based on the total cohort graduation rate. Institutions in the lowest fifth have the lowest total cohort graduation rates. If total cohort graduation rates had no effect on reporting the number of transfers-out then the percentage reporting should be approximately the same across quintiles; however, in general, institutions with higher total cohort graduation rates are less likely to report the number of transfers-out than institutions with lower total cohort graduation rates.  Response rates. Response rates varied among sectors, ranging from 35.6% to 70.8%. The initial sample size was based on a \u00b13% confidence interval, a 95% confidence level, and a 50% nonresponse rate. Since surveys were sent by email to unverified email addresses, the majority of the nonresponse is likely due to email delivery problems. A personalized memo was faxed to nonresponding institutions announcing the launch of the study and asking participants to either check their inbox and spam folder for the survey link or request a resend of the original email. The email/fax combination proved to be an extremely efficient way to identify replacement and temporary IPEDS keyholders. Poststratification was used to mitigate differences in response rates by sector. 100.0% 1 Institutions in the sample universe are (1) currently in the IPEDS universe, (2) open to the public, (3) a participant in federal financial aid programs, (4) are a primarily postsecondary institution, (5) U.S. only, (6) control is public or private, notfor-profit, (7) institutional level is at least 2 years, and (8) enrolled full-time, first-time students in the cohort year. Institutional mission. Reporting the number of transfers-out of an institution is required for institutions whose mission includes, \"Providing substantial preparation for students to enroll in another institution without completing a program.\" For all other institutions, reporting the number of transfers-out is optional. Respondents were asked the same screening question which last appeared on the GRS in 2002. Of those institutions that reported on transfer mission in 2002 (n = 797), 77% reported having the same transfer mission on this survey, 7.7% added a transfer mission, 4.5% no longer have a transfer mission, and 10.9% reported being unsure of their transfer mission. Part A: Institutions that reported the number of transfers-out in 2007. Institutions that reported a non-zero, non-missing number of the 2007 IPEDS GRS were asked to complete Part A of the OSI. Part A consisted of questions on previously used data sources and plans to change data sources in the 2008 GRS collection cycle. Survey respondents. Approximately 50% of the sample universe reported the number of transfers-out on the 2007 IPEDS GRS. Reporters represent 48.8% of the sample, of which 48.4% responded to the survey.  2open to the public, (3) a participant in federal financial aid programs, (4) are a primarily postsecondary institution, (5) U.S. only, (6) control is public or private, notfor-profit, (7) institutional level is at least 2 years, and (8) enrolled full-time, first-time students in the cohort year. Institutional mission. Comparing the transfer mission reported in 2002 with the mission reported on the 2009 OSI showed that 67.1% of institutions reported having the same transfer mission, 14.2% added a transfer mission, 6.4% no longer have a transfer mission, and 12.3% reported being unsure of their transfer mission. Primary source of 2008 IPEDS GRS data. The most commonly used source of data on transfers-out of an institution on the 2008 IPEDS GRS component was the National Student Clearinghouse (46.6%). However, state and system student unit record systems were used by 40.9% of institutions. Interestingly, 1% reported using transcript requests, which is not acceptable evidence of transfer under the current regulations. Exit and/or follow-up surveys of departing students are used by about 6.3% of institutions. 3.6% 1 The original survey question did not include transcript requests as an option; however, it was the most common answer in the \"other\" category. 2 When appropriate, \"other\" responses were recoded to fit with the standard options. The remaining responses were either too idiosyncratic to be analytically useful or could not be accurately recoded."}, {"section_title": "SOURCE: Author's calculations based on the OSI.", "text": "Changes in primary source from 2007 IPEDS GRS data. Overwhelmingly (90.4%), respondents reported using the same source of data in 2008 that they had used in 2007. Among the institutions that reported a change, 9 out of 12 had not reported transfers-out in 2007. Responses to individual data source questions. After selecting a primary source of data, respondents were asked if they used specific data sources. Over 60% of institutions reported using the National Student Clearinghouse, which is a 17 percentage point increase over the percentage using it as their primary source of data. There is a strong overlap (26.8%) between institutions that use the National Student Clearinghouse and a Student Unit Record System. There are two possible explanations for this phenomenon: (1) respondents reported using the National Student Clearinghouse if they knew a particular student unit record system matched the records against the Clearinghouse (e.g., California) or (2) respondents used the National Student Clearinghouse to supplement transfer-out data from a state or system student unit record system to eliminate gaps in tracking. The percent reporting using student unit record systems increased by 7 percentage points. The large increase in the use of non-returning student surveys from 6.3% reporting using an exit or follow-up survey as the primary source to 40.4% reporting using an exit or follow-up survey as a source is largely due to question wording. The screening question asked, \"Does your institution conduct surveys of non-returning students?\" It is reasonable for institutions to survey former students for many reasons. However, when asked if these surveys were used for IPEDS reporting only 4.3% reported using exit surveys and 4.2% reported using follow-up surveys. Transcript and withdrawal requests are used by 1.7% of institutions; however, a transcript or withdrawal request is not verifiable proof of having transferred. An interesting use of administrative records was cross-checking the GRS cohort against applications for graduate school. One institution reported that because of the small size of the institution, transfers were tracked by word of mouth. 1 \"Transcript and/or withdrawal requests\" was created based on fill-in responses to the \"other\" category. 2 \"Administrative records\" was created based on fill-in responses to the \"other\" category. For example, two institutions reported using graduate admissions records. These institutions assumed the student must have transferred and received a bachelor's degree at another institution if they were applying for graduate school. SOURCE: Author's calculations based on the OSI. State or system student unit record systems. Respondents indicating that a state or system student unit record database was used were asked about the kinds on institutions included in the database. The majority (57.3%) of student unit record systems, as reported by respondents, focus exclusively on public institutions at the state or system level. Approximately one-quarter report on public institutions and some private institutions. Almost 9% were unsure which kinds of institutions were included. It is likely that the results reflect the amount of keyholder knowledge about the purpose and scope of state databases than accurate descriptions of the actual purpose and scope. At the time of this report's writing, SHEEO (State Higher Education Executive Officers) is conducting a study of state student unit record systems for NCES, which will update the Ewell and Boeke (2007) report on student unit record databases. Additional questions on tracking transfers were added to that survey as a result of this research; however, the results are not available at this time. National Student Clearinghouse. Respondents that indicated using the National Student Clearinghouse were asked two additional follow-up questions. The first question asked about using a Cohort Query to determine the number of transfers-out. Institutional representatives can access Clearinghouse data in one of two ways: (1) by looking up individual student records or (2) by uploading a list of all students in a cohort and receiving a matched list back. In telephone conversations, many smaller schools indicated looking up individual student records using the Clearinghouse's online system-an error-prone and time-consuming process. While it may take some time to developing a query to properly create the Cohort Query submission file, it only needs to be done once. Approximately 40% of institutions reported using the file upload option for matching. The second question asked about tracking international students. The design of National Student Clearinghouse database makes it impossible to track international students. Only 6.1% of respondents reported using alternative methods to track international students. Changes for 2009 IPEDS GRS. The final question asked if there were any planned changes to the method of reporting transfers-out during the next collection cycle. Only 5.4% indicated planning to change data sources. Of the 20 write-in responses, 16 (80%) planned on using or expanding the use of National Student Clearinghouse data. "}, {"section_title": "Part B: Institutions that did not report the number of transfers in 2007", "text": "Institutions that did not report the number transfers-out on the 2007 IPEDS GRS were asked to complete Part B of the OSI. Part B consisted of questions on institutional mission, reasons for not reporting transfers-out if transfer was part of the institution's mission, and the tracking of transfers-out for internal reporting. Survey respondents. Approximately 50% of the sample universe did not report the number of transfers-out on the 2007 IPEDS GRS. Reporters represent 51.1% of the sample, of which 59.8% responded to the survey. This is in large part because Part B was significantly shorter than Part A. It was easy to convert nonrespondents when they asked to be removed from the study because the questions could be easily asked on the phone or in the email response. 100.0% 1 Institutions in the sample universe are (1) currently in the IPEDS universe, (2) open to the public, (3) a participant in federal financial aid programs, (4) are a primarily postsecondary institution, (5) U.S. only, (6) control is public or private, notfor-profit, (7) institutional level is at least 2 years, and (8) enrolled full-time, first-time students in the cohort year. Institutional mission. Comparing the transfer mission reported in 2002 with the mission reported on the 2009 OSI showed that 77.8% of institutions reported having the same transfer mission, 6.7% added a transfer mission, 2.8% no longer have a transfer mission, and 12.8% reported being unsure of their transfer mission. The majority (71.9%) of institutions reported not having a transfer mission in both years. Internal reporting. Respondents that stated their institution did not have a transfer mission or they were unsure about their mission were asked about the tracking of transfer-out students for internal reporting. Approximately one-fifth of the institutions reported tracking transfers-out internally. Slightly over 80 percent of institutions tracking transfers-out internally used the National Student Clearinghouse. Reasons for not reporting number of transfers-out. Thirty-one respondents at institutions that did not report transfers-out on the 2007 IPEDS GRS reported having a transfer mission. As would be expected, the most common responses included lack of data, staff, and time. Two respondents mentioned FERPA as a barrier to sharing data on transfers. One respondent mentioned that the institution did not have an institutional researcher on staff during the data collection. Four institutions mentioned the National Student Clearinghouse. One has access, but no time to use it; the other three are planning on using it next year. Given the limitations expressed, institutions appear to want a simple, timely, standard method for tracking transfersout."}, {"section_title": "Conclusion", "text": "The current regulations for the Student Right to Know Act were developed at a time when IPEDS was still being collected by paper surveys and institutions did not have access to national enrollment and degree databases. While the currently available databases do not have perfect coverage of the postsecondary universe, results will still be the most comprehensive data available in most cases. Assuming ongoing institutional confidence in the benefit, security, and convenience of national enrollment and degree databases, enrollment and degree coverage should only increase. However, it must be noted, that as institutions become more sensitive to the risks associated with the release of personally identifiable information, a data breach by a national enrollment and degree databases could undermine confidence in the entire system. These fears are bolstered by news reports of data breaches and overly-cautious interpretation of privacy legislation like the Family Educational Rights and Privacy Act (FERPA). 15 While data is essential for good public policy, the presence of data itself does not on its own lead to good public policy. The data must be comprehensive, consistent, and reliable if it is to inform public policy and research. Of course, the desire for accuracy is always tempered by the desire to reduce burden. The currently collected data on the number of transfers-out lacks these three essential qualities. The optional nature of reporting combined with imprecise definitions of terms, like \"substantial\" and \"subsequent enrollment\", result in the inability to make clear sense of what the data represent. The results of the two studies described in this report, personal experiences using the data, and conversations with data providers and users have led to a number of observations with implications for data collection, data dissemination, and public policy and research. These observations are as follows:"}, {"section_title": "Implications for Data Collection", "text": "\u2022 Require institutions to select from one of predefined set of reporting methods and collect which one was used. Currently, IPEDS training recommends certain sources of data (e.g., a student unit record system) and recommends against the use of others (e.g., transcript requests). Furthermore, even though two institutions use the same source, they might use the same source differently. The standard methods would provide a clear, step-by-step procedure for going from initial cohort selection through reporting outcomes. Ideally, the number of methods should be the minimum number needed to allow institutions of different sizes and institutional research capacity to report without being too burdensome. Sample standard methods could include matching the cohort against a state student unit record system or the National Student Clearinghouse using established best processes. \u2022 Encourage state and system databases to link with other data sources. Linking with other databases at the state and system level reduced burden for institutions and ensures consistency of reporting. This could be implemented through funding mechanisms for data systems. \u2022 Rephrase mission language. Approximately 10% of OSI survey respondents were unsure if their institution's mission included transfer preparation. It would be clearer and less ambiguous if this language was replaced with clear characteristics of transfer programs. For example, participation in articulation agreements where other institutions agree to accept the origin institution's courses."}, {"section_title": "Implications for Data Dissemination", "text": "\u2022 Pre-allocate data for institutions in parent/child relationships. Parent/child relationships are probably one of the least understood aspects of IPEDS. Whenever possible, publically-released IPEDS data should provide pre-allocated data for children. \u2022 Storage of Zero Values. Preallocation would also help with zero values in IPEDS data. To save space, IPEDS does not store observations without data unless the institution explicitly enters a zero. In many cases, it is acceptable to fill-in these missing rows with zero. However, for institutions with a parent/child relationship, automatically filling in zeros for child institutions would usually be incorrect \u2022 IPEDS Value Coding. For indicator variables, coding \"no\" as zero and \"yes\" as one would make it easier to use the data. As an added benefit, taking the mean of the variable would give the percentage for that variable."}, {"section_title": "Implications for Research and Public Policy", "text": "\u2022 Refrain from aggregating the number of transfers-out beyond the institutional level. Optional reporting combined with different methods of calculation result in the total number of transfers-out being significantly underestimated in IPEDS when compared with BPS. #Q09. An intended outcome of this project is to identify \"best practices\" for reporting the number of transfer out students. Does AIR have your permission to associate your organization's name with the responses you have just provided? R1. No (47.7%) R2. Yes (52.3%)"}, {"section_title": "Closing Text", "text": "To submit your responses, click \"Done\" below. Thank you for taking the time to respond to this survey. Results will be made available to all respondents in late July. For more information, contact this study's Principal Investigator, Allan Joseph Medwick, at amedwick@airweb2.org."}]