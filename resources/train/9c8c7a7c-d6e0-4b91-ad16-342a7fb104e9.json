[{"section_title": "TABLE OF CONTENTS (continued)", "text": "List of Tables (continued) "}, {"section_title": "5-18", "text": "6-1 ECLS-K, spring-third grade 2002 xiii   School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 1-3 Instruments used in the ECLS-K, by round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001 1-4 Direct child assessments, by round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001  xvi This page intentionally left blank. 1-1\n"}, {"section_title": "INTRODUCTION", "text": "This methodology report provides technical information about the development, design, and conduct of the third grade data 1 collection of the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). It begins with an overview of the ECLS-K study. Subsequent chapters provide information on the development of the instruments, sample design, data collection methods, data preparation and editing, response rates, and weighting and variance estimation. The ECLS-K focuses on children's early school experiences beginning with kindergarten. It is a multisource, multimethod study that includes interviews with parents; the collection of data from principals, teachers, and student records abstracts; and direct child assessments. The ECLS-K was developed under the sponsorship of the U.S. Department of Education, National Center for Education Statistics (NCES). Westat is conducting this study with assistance provided by Educational Testing Service (ETS) in Princeton, New Jersey. The Survey Research Center and the School of Education at the University of Michigan assisted Westat in conducting the base year and first grade study. The ECLS-K follows a nationally representative cohort of children from kindergarten through fifth grade. The base year data were collected in the fall and spring of the 1998-99 school year when the sampled children were in kindergarten. A total of 21,260 kindergartners throughout the nation participated. Two more waves of data were collected in the fall and spring of the 1999-2000 school year when most, but not all, of the base year children were in first grade. 2 The fall-first grade data collection was limited to a 30 percent subsample of schools. Approximately 27 percent of the base year students who were eligible to participate in year 2 attended the 30 percent subsample of schools (see exhibit 1-1). The fall-first grade data collection was a design enhancement to enable researchers to measure the extent of summer learning loss and the factors that contributed to such loss and to better disentangle school and home effects on children's learning. The spring-first grade data collection, on the full sample, was part of the original study design and can be used to measure annual school progress and to describe the first grade learning environment of children in the study. All children assessed during the base year were 1-2 eligible to be assessed in the spring-first grade data collection regardless of whether they repeated kindergarten, were promoted to first grade, or were promoted to second grade. In addition, children who were not in kindergarten in the United States during the 1998-99 school year and, therefore, did not have a chance to be selected to participate in the base year of the ECLS-K were added to the spring-first grade sample. 3 Such children included immigrants to the United States who arrived after fall 1998 sampling, children living abroad during the 1998-99 school year, children who were in first grade in 1998-99 and repeated it in 1999-2000, and children who did not attend kindergarten. Their addition allows researchers to make estimates for all first graders in the United States rather than just for those who attended kindergarten in the United States in the previous year. Exhibit 1-1. ECLS-K waves of data collection: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Data A fifth wave of data was collected in the spring of the 2001-02 school year when most, but not all, of the sampled children were in third grade. Approximately 89 percent of the children interviewed were in third grade during the 2001-02 school year, 9 percent were in second grade, and less than 1 percent were in fourth grade or higher. In addition to the school, teacher, parent, and child assessment data collection components, children were asked to complete a short self-description questionnaire, which included questions about what they were like and their likes and dislikes. The spring-third grade data collection can be used to measure school progress and to describe the third grade learning environment of children in the study. 3 Their addition is referred to as \"freshening\" the sample. See chapter 3 for more detail on the freshening process."}, {"section_title": "1-3", "text": "The sample of children in the third grade round of data collection of the ECLS-K represents the cohort of children who were in kindergarten in 1998-99 or in first grade in 1999-2000. Since the sample of children fielded in 2001-02 was not freshened with third graders who did not have a chance to be sampled in kindergarten or first grade (for example, because they were out of the country during their kindergarten and first grade years), this sample of children does not represent all third graders in 2001-02. The vast majority of children in third grade in the 2001-02 school year are members of the cohort. However, third graders who repeated second or third grade and recent immigrants were not covered. Data were collected from teachers and schools to provide important contextual information about the environment for the sampled children. The teachers and schools are not representative of third grade teachers and schools in the country in 2001-02 but representative only of teachers of the cohort of children sampled in kindergarten in 1998-99 and freshened in spring-first grade in 2000. For this reason, the only weights produced from the study are for making statements about children, including statements about the teachers and schools of those children. The final wave of data collection as currently planned is scheduled for spring 2004 when most of the sampled children will be in the fifth grade. The ECLS-K has several major objectives and numerous potential applications. The ECLS-K combines (1) a study of achievement in the elementary years; (2) an assessment of the developmental status of children in the United States at the start of their formal schooling and at key points during the elementary school years; (3) cross-sectional studies of the nature and quality of kindergarten programs in the United States; and (4) a study of the relationship of family, preschool, and school experiences to children's developmental status at school entry and their progress during the kindergarten and early elementary school years. The ECLS-K is part of a longitudinal studies program comprising two cohorts-a kindergarten cohort and a birth cohort. The birth cohort (ECLS-B) is following a national sample of children born in the year 2001 from birth through first grade. The ECLS-B focuses on the characteristics of children and their families that influence children's first experiences with the demands of formal school, as well as children's early health care and in-and out-of-home experiences. Together these cohorts will provide the depth and breadth of data required to more fully describe and understand children's health and early learning, development, and education experiences."}, {"section_title": "1-4", "text": "The ECLS-K has both descriptive and analytic purposes. It provides descriptive data on children's status at school entry, their transition into school, and their progress through fifth grade. The ECLS-K also provides a rich data set that enables researchers to analyze how a wide range of family, school, community, and individual variables affect children's early success in school; explore school readiness and the relationship between the kindergarten experience and later elementary school performance; and record children's cognitive and academic growth as they move through elementary school."}, {"section_title": "Background", "text": "National policymakers and the public at large have increasingly recognized that the prosperity of the United States depends on the successful functioning of the American education system. There is also growing awareness that school reform efforts cannot focus solely on the secondary and postsecondary years but must pay attention to the elementary and preschool years as well. Increased policy interest in the early grades and the early childhood period is reflected in an intensified recent national policy aimed at ensuring that children are capable of reading by the third grade, providing college student and adult volunteer tutors for children who are having difficulty learning to read, and preparing children to succeed in school with improved Head Start and early childhood development programs. Efforts to expand and improve early education will benefit from insights gained through analyses of data from the large scale, nationally representative ECLS-K data and the study's longitudinal design. The ECLS-K database contains information about the types of school programs in which children participated, the services they received, and repeated measures of the children's cognitive skills and knowledge. The ECLS-K database also contains measures of children's physical health and growth, social development, and emotional well-being, along with information on family background and the educational quality of their home environments. As a study of early achievement, the ECLS-K allows researchers to examine how children's progress is affected by such factors as placement in high or low ability groups, receipt of special services or remedial instruction, grade retention, and frequent changes in schools attended because of family moves. Data on these early school experiences are collected as they occur, with the exception of their experiences before kindergarten, which were collected retrospectively. This produces a more accurate 1-5 measurement of these antecedent factors and enables stronger causal inferences to be made about their relationship to later academic progress. The ECLS-K enables educational researchers and policy analysts to use a variety of perspectives on early childhood education, using techniques such as multilevel modeling to study how school and classroom factors affect the progress of individual children. The data collected enable analysts to examine how children's status at school entry and performance in school are determined by an interaction of child characteristics and school and family environments. Data collected during the kindergarten year serve as baseline measures to examine how schooling shapes later individual development and achievement. The longitudinal nature of the study enables researchers to study children's cognitive, social, and emotional growth and to relate trajectories of change to variations in children's experiences in kindergarten and the early grades. The spring-third grade data collection can be used to describe the diversity of the study children and the classrooms and schools they attend. It can also be used to study children's academic gains in the years following kindergarten and first grade. The ECLS-K sample includes substantial numbers of children from various minority groups. Thus, the ECLS-K data present many possibilities for studying cultural and ethnic differences in the educational preferences and literacy practices of families, the developmental patterns and learning styles of children, and the educational resources and opportunities that different groups are afforded in the United States."}, {"section_title": "Conceptual Model", "text": "The design of the ECLS-K has been guided by a framework of children's development and schooling that emphasizes the interrelationships between the child and family, the child and school, the family and school, and the family, school, and community. The ECLS-K recognizes the importance of factors that represent the child's health status and socioemotional and intellectual development and incorporates factors from the child's family, community, and school-classroom environments. The ECLS-K conceptual model is depicted in exhibit 1-2. The study has paid particular attention to the role that parents and families play in helping children adjust to formal school and in supporting their education through the primary grades. It has also gathered information on how schools prepare for and respond to the diverse backgrounds and experiences of the children and families they serve."}, {"section_title": "Study Components", "text": "The emphasis placed on measuring children's environments and development broadly has critical implications for the design of the ECLS-K. The design of the study includes the collection of data from the child, the child's parents/guardians, teachers, and schools. Children participate in various activities to measure the extent to which they exhibit those abilities and skills deemed important for success in school. They are asked to participate in activities designed to measure important cognitive (general knowledge, science, literacy, and quantitative) skills and noncognitive (fine and gross motor coordination [in kindergarten] and socioemotional development) skills and knowledge. All measures of a child's cognitive skills are obtained through an untimed one-on-one assessment of the child. Beginning with the third grade data collection, children report on their own perceptions of their abilities and achievement as well as their interest in and enjoyment of reading, math, and other school subjects. Children are assessed in each round of data collection. Children's height and weight are also measured in each round of data collection. Parents/guardians are an important source of information about the families of the children selected for the study and about themselves. Parents provide information about children's development at school entry and at various points in time and about their experiences both with family members and others. Information is collected from parents each time children are assessed using computer-assisted interviews (CAIs)."}, {"section_title": "1-7", "text": "Teachers, like parents, represent a valuable source of information on themselves, the children in their classrooms, and the children's learning environment (i.e., the classroom). Teachers are not only asked to provide information about their own backgrounds, teaching practices, and experience, they are also called on to provide information on the classroom setting for the sampled children they teach and to evaluate each sampled child on a number of critical cognitive and noncognitive dimensions. Special education teachers and service providers of sampled children with disabilities are also asked to provide information on the nature and types of services provided to the child. With the exception of the fall-first grade data collection, teachers complete self-administered questionnaires each time children are assessed. School administrators, or their designees, are asked to provide information on the physical, organizational, and fiscal characteristics of their schools, and on the schools' learning environment and programs. Special attention is paid to the instructional philosophy of the school and its expectations for students. Information is collected from school administrators via self-administered questionnaires during each spring data collection. With the exception of first grade, the administrators of all ECLS-K schools use a single questionnaire. In the first grade two different questionnaires were used, one for original sampled schools and one for new or transfer schools. More information can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), User's Manual for the ECLS-K First Grade Public-Use Data Files and Electronic Code Book (NCES 2002-135;U.S. Department of Education, National Center for Education Statistics, 2002b). School office staff are asked to complete a student records abstract form and a school fact sheet. The student records abstract form includes questions about an individual child's enrollment and attendance at the school, transfer to another school (if applicable), and verifies whether the child has an Individual Education Plan (IEP) on record. A student records abstract form is completed for each child in the study during each spring data collection. During the third grade data collection, school office staff are also asked to complete a school fact sheet. This form supplements the school administrator questionnaire with basic information about the school, including grade level, school type (public or private), length of school year, and attendance recordkeeping practices. This school fact sheet is only filled out once for each school in the study. Prior to the third grade data collection, the questions were part of the school administrator questionnaire. Exhibit 1-3 summarizes the instruments that were used in each of the data collection periods from kindergarten through spring-third grade. Exhibit 1-4 provides additional detail about the direct child assessments conducted during each of the data collection periods. Separate "}, {"section_title": "Springfirst grade", "text": "Springthird grade Parent interview X X X X X Child assessments X X X X X Teacher questionnaire part A X X X X 2 X Teacher questionnaire part B X X X X 2 X Teacher questionnaire part C X X X X 2 X Special education teacher questionnaire part A X X X Special education teacher questionnaire part B X X X Adaptive Behavior Scale X X Self-Description Questionnaire X School administrator questionnaire X X 3 X Student records abstract X X X School fact sheet X School facilities checklist X X X Salary and benefits questionnaire 4 X Head Start verification 5 X X Round that included the instrument. 1 The fall-first grade data collection consisted of a 30 percent subsample of the study schools enrolling 27 percent of the eligible children. See the User's Manual for the ECLS-K First Grade Public-Use Data Files and Electronic Code Book (NCES 2002-135) for information about the purposes and methods of the fall-first grade data collection."}, {"section_title": "1-9", "text": "Exhibit 1-4. Direct child assessments, by round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001School years -02 1998School years -1999  (science and social studies) X X X X Science X 2 Psychomotor X Height and weight X X X X X X Round that included the instrument. / The OLDS was administered to language minority students who were new to the study in the spring or did not pass the cut score in the English version during the previous OLDS administration. 1 The OLDS was given to children with a non-English language to determine if the children understood English well enough to receive the direct child assessments in English. For further information on the OLDS, please refer to the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), Base Year Public-Use Data Files andElectronic Code Book: User's Manual (NCES 2001-029, U.S. Department of Education, National Center for Education Statistics, 2000) or the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), Restricted-Use Base Year Child File, Teacher File, andSchool File (NCES 2000-097, U.S. Department of Education, National Center for Education Statistics, 2001). The OLDS was not used in third grade because the vast majority of children passed it by spring-first grade. 2 In spring-third grade, general knowledge was replaced with a science assessment. Children received a science assessment that measured their understanding of science concepts and scientific investigation skills. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02. Additional information about the ECLS program can be found on the NCES web site: http://www.nces.ed.gov/ecls."}, {"section_title": "Second Grade Bridge Study", "text": "One of the critical goals of the ECLS-K is to measure children's growth in cognitive achievement through the early elementary school years. Due to budgetary constraints, data were not collected during the 2000-01 school year, when most of the sampled children were in second grade. The absence of second grade data presented a challenge for establishing longitudinal scales to link the first 1-10 grade to third grade scores. Very few children answered the most difficult items in the spring-first grade data collection correctly. Third grade field test results indicated that these same items would be too easy for the vast majority of third graders. The ability levels of first graders overlapped with those of third graders only in the tails of the distributions. For this reason, a bridge study was conducted to fill in the data points that lie between the preponderance of the first and third grade ability levels so that stable item parameter estimates could be calculated that would support the measurement of gain. Reading and mathematics assessment data collected from a relatively small sample of second graders served as a bridge to link the first and third grade rounds. This bridge sample did not consist of ECLS-K longitudinal sample members, nor was it nationally representative. For more information about the bridge study, see ECLS-K Psychometric Report for the Third Grade."}, {"section_title": "Contents of Report", "text": "This report provides detailed technical information about the development, design, and conduct of the third grade data collection. Chapter 2 provides an overview of process used to develop the computer-assisted (CAI) and hard-copy survey instruments. Approximately 89 percent of the children interviewed were in third grade during the 2001-02 school year, 9 percent were in second grade, and less than 1 percent were in fourth grade or higher. Chapter 3 describes the sample design and implementation. Chapter 4 describes the data collection methods, including information about the training of field staff and quality control procedures. Chapter 5 details the preparation and editing of the data as it is receipted from the field. Chapter 6 provides information on unit and item response rates. Chapter 7 discusses weighting and variance information. A separate nonresponse bias analysis report examining whether survey nonresponse is adversely affecting the quality of the data will be available in fall 2003. Because both this report and the ECLS-K Psychometric Report for the Third Grade focus on the third grade data collection, minimal information is provided about the base year or first grade data. Users who wish to learn more about these data collections should refer to the following documents:  Regional Education Laboratories and organizations with expertise in school reform were reviewed in order to identify important topics to cover."}, {"section_title": "Review Panels", "text": "Studies with the scope, complexity, and importance of the ECLS-K require input from a number of individuals and organizations to address the data needs of policymakers and of those performing policy studies and educational research. In addition, consultations with practitioners, content area experts, and researchers are necessary to ensure that instruments accurately reflected curricular standards and practices. The ECLS-K project staff established and sought guidance from the Technical Review Panel, the Content Review Panel, and individual consultants. 2-4"}, {"section_title": "Technical Review Panel", "text": "The Technical Review Panel was assembled to provide review and comment on such matters as the technical design and implementation of the ECLS-K and policy and research topics that are appropriate for the ECLS-K third and fifth grade data collections. The membership of the Technical Review Panel represents a broad range of nonfederal and federal experts in elementary education, educational, and family research and policy issues. 2-6"}, {"section_title": "Timing Study", "text": "As with any study instrument, questionnaire length and respondent burden were issues of concern. A timing study was conducted for the draft parent questionnaire. Five Westat staff members conducted nine interviews with respondents who had previously volunteered to participate in studies being conducted by Westat. No attempt was made to recruit respondents representative of either racial or economic groups as the objective was to obtain an estimate of the length of the questionnaire rather than to examine how individuals interpreted the questions. Westat did attempt to select people who would go through the various questionnaire paths (e.g., married couples, single parents, one household with a nonresident father). All of the respondents were parents of third grade children. All interviews were conducted over the telephone using a paper version of the questionnaire. Interviewers used stopwatches to time the individual sections and to get an overall time for the interview. The interviewers stopped the watches for extended interruptions, such as a respondent having to take care of the needs of a family member. In most cases, the respondents were asked to answer questions in sections that required knowledge of data collected from an earlier wave of the data collection as if they had provided the information in a previous round of the survey. In only two interviews were respondents asked to complete sections, as would be the case with a new respondent. The revised paper version of the questionnaire took an average of 59 minutes and 27 seconds to complete. Table 2-1 summarizes the overall and section timings for each interviewer and across interviewers. The initials denote the five interviewers. Two interviewers completed more than one interview (e.g., interviewer JW completed three interviews, JW1, JW2, and JW3). Total 0:56:54 1:06:53 0:58:40 0:48:40 0:54:04 0:54:18 0:52:02 1:05:00 1:18:33 0:59:27 INQ 0:02:01 0:01:39 0:02:19 0:01:39 0:02:12 0:02:25 0:02:05 0:01:36 0:02:05 0:02:00 PIQ 0:09:29 0:08:21 0:10:37 0:07:12 0:08:03 0:09:21 0:07:50 0:09:34 0:11:15 0:09:05 FSQ 0:01:40 0:02:27 0:05:24 0:01:28 0:03:15 0:03:41 0:01:01 0:02:22 0:01:00 0:02:29 HEQ 0:15:53 0:14:25 0:13:53 0:11:10 0:13:48 0:13:02 0:13:55 0:16:55 0:20:05 0:14:47 SSQ 0:02:57 0:03:28 0:03:00 0:02:47 0:02:53 0:02:37 0:03:19 0:03:13 0:03:00 0:03:02 CFQ 0:01:01 0:01:10 0:01:01 0:01:10 0:01:18 0:01:37 0:02:02 0:01:36 0:02:00 0:01:26 CCQ 0:04:01 0:06:14 0:01:04 0:04:10 0:01:30 0:01:24 0:01:09 0:01:34 0:02:00 0:02:34 DWQ 0:05:09 0:05:02 0:03:27 0:05:09 0:04:23 0:04:08 0:03:53 0:05:38 0:07:07 0:04:53 NRQ 0:00:00 0:05:23 0:00:00 0:00:00 0:00:00 0:00:00 0:00:00 0:00:00 0:00:00 0:00:36 CHQ 0:07:35 0:05:26 0:09:05 0:06:26 0:07:48 0:07:46 0:09:00 0:08:39 0:11:37 0:08:09 PPQ 0:01:49 0:01:56 0:01:49 0:01:48 0:01:51 0:01:53 0:02:33 0:02:20 0:03:29 0:02:10 FDQ 0:02:17 0:03:31 0:01:49 0:01:44 0:01:35 0:01:29 0:01:38 0:02:14 0:02:15 0:02:04 PEQ 0:00:39 0:00:25 0:00:27 0:00:20 0:00:23 0:00:25 0:00:39 0:01:20 0:02:00 0:00:44 EMQ 0:00:28 0:00:11 0:00:15 0:00:40 0:00:23 0:00:15 0:00:25 0:02:05 0:03:00 0:00:51 WPQ 0:00:39 0:01:35 0:01:25 0:01:23 0:01:05 0:01:09 0:00:55 0:00:54 0:03:42 0:01:25 PAQ 0:00:38 0:01:20 0:00:41 0:01:08 0:01:04 0:01:13 0:00:50 0:02:00 0:02:00 0:01:13 CMQ 0:00:38 0:04:20 0:02:24 0:00:26 0:02:33 0:01:53 0:00:48 0:03:00 0:01:58 0:02:00 SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten class of 1998-99 third grade data collection, school year 2001-02 timing study."}, {"section_title": "2-7", "text": "Overall, the parent interview required just under an hour to complete. The timings ranged from a low of 48 minutes to a high of 78 minutes. The approaches used to capture the information (update versus obtain new data) and the characteristics of the child and household contributed to the variations in the length of interviews. It was estimated that the time to complete an interview for twins would extend the length of the interview, on average, by a conservative 30 minutes. This was determined by doubling the time for the three sections that required the most replication of data for each child (PIQ, HEQ and CHQ)."}, {"section_title": "2-8", "text": "No individual section was unduly long. The section that required the most time to administer was the Home Environment, Activities and Cognitive Stimulation (HEQ) section. It took between 11 minutes and 20 minutes, depending upon the interviewer. In most of the timing interviews the questions on education and occupation were updated (as if the information had been obtained in an earlier interview), which would be the more common approach in the national study; however, in two interviews the sections were asked as if the information was not previously known. The timing differences with those two strategies were as follows: updating the education and occupation questions took an average of 25 seconds while collection of new information required an average of 2 minutes and 6 seconds. Only one respondent completed the Nonresident Parent section of the questionnaire, which includes items about parents who do not live with their child, such as a question about the frequency of their contact with the child. Also, only one of the sample respondents included a parent of twins. The timing, however, was only done for the first interview because this represented the most common approach. The results of the timing study suggested that the parent questionnaire could be administered within 45 minutes. 3-1"}, {"section_title": "SAMPLE DESIGN AND IMPLEMENTATION", "text": "The Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) employed a multistage probability sample design to select a nationally representative sample of children attending kindergarten in 1998-99. In the base year the primary sampling units (PSUs) were geographic areas consisting of counties or groups of counties. The second-stage units were schools within sampled PSUs. The third and final stage units were students within schools. During the base year data were collected in both the fall and the spring. The first grade data collection included only base year respondents. A case was considered responding for the base year if there was a completed child assessment or parent interview in fall-or spring-kindergarten. A child with a disability who could not be assessed was also considered a base year respondent whether or not this child had a complete parent interview. Background characteristics such as gender, race/ethnicity, age, height, and weight are available for children with disabilities who could not be assessed. While all base year respondents were eligible for the spring-first grade data collection, the effort for fall-first grade was limited to a 30 percent subsample. The spring-first grade student sample was freshened to include current first graders who had not been enrolled in kindergarten in 1998-99 and, therefore, had no chance of being included in the ECLS-K base year kindergarten sample. For both falland spring-first grade, a 50 percent subsample of students who had transferred from their kindergarten schools was followed. The third grade data collection included base year respondents and children sampled in first grade through the freshening operation. As in the first grade data collection, only a subsample of students who had transferred from their kindergarten schools was followed. In third grade, however, the subsampling rate applied to movers was slightly higher: children whose home language was non-English (also known as children belonging to the language minority group) and who moved for the first time in third grade were followed with certainty. In other words, 100 percent of the children belonging to the language minority group who did not move between kindergarten and first grade but moved between first grade and third grade were followed into their new third grade schools. Language minority children who had moved between kindergarten and first grade and were not subsampled for followup in first grade did not reenter the third grade sample; those who were subsampled for followup in first grade were followed with certainty into their third grade schools if they moved again between first grade and third grade. The higher subsampling rate allowed for the preservation of this group in the sample for analytic reasons. Children not in the language minority group continued to be subsampled for followup at a 50 percent rate if they moved out of the original sample schools."}, {"section_title": "3-2", "text": "The precision requirements and achieved sample sizes for the different waves of data collection are discussed in section 3.1. The base year, fall-first grade, and spring-first grade samples are discussed in section 3.2, 3.3 and 3.4, respectively. Sampling issues that were considered prior to the third grade data collection are discussed in section 3.5. Section 3.6 discusses the characteristics of the third grade sample with an emphasis on children in the language minority group and children who moved out of their original sample schools since these two groups of children were the focus of the sampling issues discussed in section 3.5."}, {"section_title": "Precision Requirements and Achieved Sample Sizes", "text": "The ECLS-K is a nationally representative longitudinal survey of children who attended kindergarten in 1998-1999, supplemented with children who were in first grade in spring 2000, but were not in kindergarten the previous year. Data on these children were collected from a variety of sources at two points in the base year (kindergarten in 1998-1999), two points in the 1999-2000 school year (as noted earlier, the fall collection was limited to a subsample of children) when most of the children were in first grade, and again in spring of 2002 when most of the children were in third grade. The plan calls for collecting data on children again in spring of 2004 when most of the children are in fifth grade. The overall design for the survey evolved over time. The initial design study recommended sampling 23,500 children in approximately 1,000 kindergarten programs sampled from 100 PSUs. The initial plans also called for sampling children in private schools at a higher rate than children in public schools, as well as sampling minorities (children of Black, Hispanic, or Asian or Pacific Island [API] race or ethnicity) at higher rates than nonminorities. The design study assumed that because of nonresponse and losses due to children moving, the final number of completed interviews at the end of the survey would be about 10,300. While the design study was useful in providing overall direction, the final framework for the sample design differed in many ways from its recommendations. The sample design implemented through the third grade in the ECLS-K is described in this chapter. The remainder of this section gives an overview of the sampling objectives and how the design was revised to accommodate changes in those objectives over the course of the study. Subsequent sections of the chapter give the details of the procedures used to implement the sample in the various rounds or waves of data collection, beginning with the base year in 1998-1999."}, {"section_title": "3-3", "text": "Four precision requirements for the survey were identified and formed the basis for the base year sample design and plans for the followups in subsequent rounds. These requirements are the ability to do the following: Measure a change of 20 percent in proportions across waves; Measure a change of 5 percent in a mean score across waves; Estimate a proportion for each wave with a coefficient of variation (CV) of 10 percent or less; and Estimate a mean score for each wave with a CV of 2.5 percent or less. The goals were interpreted as being objectives not only for all children, but for subgroups of analytic interest including children attending public and private schools and children from different race and ethnic groups. After the spring-first grade data collection, language minority (LM) children were a newly identified subgroup of analytic interest for sample design purposes. A large number of assumptions had to be made to estimate sample sizes sufficient to meet the precision requirements. The key assumptions included projections of the losses due to nonresponse and attrition due to children moving, the design effects 1 associated with the sample design, the element mean and standard deviations of the scores, and the correlation of the statistics across waves. Since the ECLS-K is the first study of this population using this methodology, many of the assumptions had to be based on judgments without much empirical data to support them. The precision requirements that drive the sample design (those demanding the largest sample size) have to deal with estimating changes over time and estimating the precision of estimates in the fifth grade data collection. Based on assumptions described above, it was determined that a sample in fifth grade of about 10,000 children would be adequate to meet the precision requirements overall and for most subgroups. A sample of about 800 to 1,000 children in a subgroup would be achieved for most of the subgroups with an overall sample of 10,000 children and these would approximately meet the precision goals. Children in private schools and APIs were the two subgroups that were expected to fall short of the goals if higher sampling rates were not applied. As noted in the following sections, sampling procedures were implemented to increase the sample size for these two groups. After the spring-first grade data collection was completed, the assumptions were reviewed and the ability of the sample to meet the survey goals was re-examined. At that time, language minority 3-4 children were included as being a subgroup of analytic interest. The evaluation showed that the sample sizes were adequate for most subgroups, but special efforts were needed to retain API children and language minority children in subsequent rounds. Table 3-11 in section 3.5 shows the outcome of the spring-first grade data collection by type of children. Since funding was made available to support these efforts, sampling procedures for retaining movers were modified. In the first grade data collection, half of the movers were subsampled and included for followup, without taking any characteristics of the children into account. To increase the sample of API and language minority children, the sampling procedures were revised for the third grade followup to retain as many of these children as possible. The evaluation also showed that the assumed design effects for test scores (reading, math and general knowledge) were larger than originally expected, ranging from 4.5 to 9.5. The larger than expected design effects for scores were first identified after the base year. The design effects for percentages, ranging from 1.6 to 6.9 for proportions greater than 30 percent, were close to those originally anticipated (3.8 on average). 2 The design effects for scores affect the ability to meet the two precision goals for scores. The evaluation also showed the correlation over time of the scores was higher than expected, having the opposite effect and making estimates of change in scores over time more precise. Thus, only one precision objective is adversely affected by the higher than expected design effects for scores. It may mean that estimates of mean score for the last wave of data collection (i.e., fifth grade) for smaller subgroups will have CVs of more than 2.5 percent. Table 3-1 tracks the ECLS-K sample from the base year through third grade. The table shows that the large initial sample of children has been reduced over time due to subsampling movers and nonresponse. While the initial assumptions that drove the sample design were not always accurate separately, the overall effect of the losses has been very close to what was expected. The overall number of eligible children at the end of the third grade wave was nearly 17,000 children compared to the expected value of 17,000 to 18,000 using assumed mover rates as discussed later in table 3-12. Projecting the losses forward (i.e., 45 percent of children who are still in the original school by the end of third grade will move to a new school during fifth grade and 90 percent of the eligible children will have completed assessment in fifth grade), it is likely that the final sample size for the fifth grade sample will exceed the 10,000 children in the initial projections. In fact, depending on the response and mover rates associated with the transition between third and fifth grade, the overall sample size will be approximately 11,000 children. Table 3-1. ECLS-K sample size from the base year through third grade, by selected  characteristics: School years 1998-99, 1999-2000, and 2001-02   Characteristic  Fallkindergarten   Springkindergarten   Fallfirst grade 1   Springfirst grade   Spring-Third grade   Beginning wave sample size  21,387  22,772 2  6,507  21,357 3,4  21,357  Fielded after subsampling movers  5,728  18,507  17,240  Fielded after locating movers  22,088  5,691  17,708  16,951  Number of eligibles  21,356  21,941  5,652  17,652  16,829  Child-complete 5  19,173  19,967  5,291  16, 1 Only 30 percent of base year schools were included in the fall-first grade sample. 2 Including 1,426 children from refusal converted schools and excluding 41 children in schools that cooperated in fall-kindergarten and refused in spring-kindergarten. 3 Only children who have at least one of the four base year data points (fall-kindergarten assessment or parent data, or springkindergarten assessment or parent data). 4 Including 165 children sampled in first grade through sample freshening. 5 Child-complete if the child had assessment data or was not assessed due to a disability. 6 Parent-complete if the child had parent data. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02. The details on the sample sizes for subgroups at the end of the third grade are provided later in this chapter (see tables 3-14, 3-15, and 3-16). Those tabulations show that the third grade sample sizes for all of the identified subgroups of interest exceed 1,100. For many of the groups the sample sizes are much larger, with the lowest sample size for API children. Thus, the sampling and data collection procedures developed in the initial stages and modified throughout the course of the study are producing samples that are meeting or exceeding the requirements. The final outcomes in fifth grade depend on the success of the operations and procedures proposed for that year, but these results suggest the prospects of achieving the sample size goals are excellent."}, {"section_title": "Base Year Sample", "text": "In the base year, the ECLS-K selected a nationally representative sample of children attending kindergarten in 1998-99 using a dual-frame multistage probability sample design. Counties and groups of counties constituted the first stage sampling units or PSUs, schools or kindergarten programs within PSUs were the second-stage units, and children were the third-and final-stage units. 3-6"}, {"section_title": "Selecting the Area Sample", "text": "The point of departure for the ECLS-K area sample frame development was an existing multipurpose frame of PSUs created using 1990 county-level population data and 1988 per capita income data from the U.S. Department of Commerce, Bureau of Economic Analysis. This frame contained 1,404 PSUs that were counties or groups of contiguous counties. PSUs did not cut across census regional 3 boundaries, but were allowed to cross state boundaries. Each 1990 metropolitan statistical area (MSA) 4 constituted a single PSU except where an MSA crossed census regions, and it was split into two PSUs. The minimum size of a PSU in the multipurpose frame was 15,000 persons. Since the focus of the ECLS-K is kindergarten students, the existing PSU frame was updated with 1994 population estimates of 5-year-olds by race/ethnicity, the most up-to-date estimates available from the U.S. Bureau of the Census at the time. The counts of 5-year-olds by race/ethnicity were used to revise PSU definitions relative to a different minimum PSU size and to construct a measure of size (MOS) that facilitated the oversampling of APIs. Each PSU in the frame that did not have at least 320 5year-olds was collapsed with an adjacent PSU. This minimum PSU size was developed based on assumptions concerning anticipated school response rates, the average number of schools that would be selected per PSU, and the target number of students to be sampled per school. After this collapsing, the final ECLS-K PSU frame contained 1,335 records. The MOS used for selecting PSUs took into account the amount of oversampling of APIs required to meet the ECLS-K precision goals. The weighted MOS was calculated as follows: where 2.5 is the oversampling rate for APIs, and n API and n other are the counts of 5-year-old APIs and all others, respectively. The oversampling rate for APIs was calculated as the target number of completed API cases divided by the expected number of completed API cases without oversampling. In all, 100 PSUs were selected for the ECLS-K. The 24 PSUs with the largest measures of size were designated as certainty selections or self-representing (SR) 5 and were set aside. They were included in the sample with certainty. Once the self-representing PSUs were removed, the remaining PSUs, called non-selfrepresenting (non-SR) 6 , were partitioned into 38 strata of roughly equal MOS. The frame of non-selfrepresenting PSUs was first sorted into eight superstrata by MSA status and by census region. Within the 3 A census region is a geographic region defined by the U.S. Bureau of the Census. 4 A metropolitan statistical area (MSA) is a geographic entity designated as one or more counties in a metropolitan area, except in New England, where MSA is defined in terms of county subdivisions. MSAs generally have under 1 million in population. 5 A self-representing PSU is selected into the sample with certainty (i.e., with probability 1). 6 A non-self-representing PSU is selected into the sample with probability proportional to its measure of size (MOS)."}, {"section_title": "3-7", "text": "four MSA superstrata, the variables used for further stratification were race/ethnicity (high concentration of API, Black, or Hispanic), size class (MOS \u2265 13,000 and MOS < 13,000), and 1988 per capita income. Within the four non-MSA superstrata, the stratification variables were race/ethnicity and per capita income. Table 3-2 describes how the 38 non-self-representing strata were created. A census region is a geographic region defined by the U.S. Bureau of the Census. 2 Primary sampling unit. NOTE: In this table, \"Any\" means any value of the column variable. For example, stratum 1 includes PSUs that have MSA status, are located in the Northeast region, with a MOS greater than or equal to 13,000 and per capita income ranging between $22,062 and $25,424, and can have any value of the race/ethnicity percentage. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten data collection, school year 1998-99."}, {"section_title": "3-8", "text": "Two PSUs were selected from each non-self-representing stratum using Durbin's Method (Durbin, 1967). This method selects two first-stage units per stratum without replacement, with probability proportional to size and a known joint probability of inclusion. The Durbin method was used because it has statistical properties that make it easier to compute variances. Table 3-3 summarizes the characteristics of the ECLS-K PSU sample. The Durbin method required two passes of the frame with a different selection probability at each pass to obtain the desired probabilities of inclusion and joint probabilities of inclusion. In the first pass, one PSU was selected in the stratum with probability p 1 . In the second pass, the selected PSU was excluded and another PSU was selected with probability proportional to where p 1 = M 1 /M and p 2 = M 2 /M, M 1 is the MOS of the first unit selected, M 2 the MOS of the second unit selected, and M the MOS of the stratum. The overall selection probability of non-self-representing unit i is 2 , 1 , 2 The joint probability of inclusion of the first and second units is"}, {"section_title": "Selecting the School Sample", "text": "In the second stage of sampling, public and private schools offering kindergarten programs were selected. For each ECLS-K PSU, a frame of public and private schools offering kindergarten programs was constructed using existing school universe files: the 1995-96 Common Core of Data (CCD) and the 1995-96 Private School Universe Survey (PSS). The school frame was freshened in the spring of 1998 to include newly opened schools that were not included in the CCD and PSS and schools that were in the CCD and PSS, but did not offer kindergarten according to those sources. A school sample supplement was selected from the supplemental frame."}, {"section_title": "School Frame Construction", "text": "The 1995-96 CCD Public School Universe File was the primary source for the ECLS-K public school sampling frame. Most schools run by the U.S. Department of the Interior, Bureau of Indian Affairs (BIA) and the schools run by the U.S. Department of Defense (DOD) are not included on the CCD. The 1995-96 Office of Indian Education Programs Education Directory was consulted in order to complete the list of BIA schools in the CCD file. For the DOD schools, a 1996 list of schools obtained directly from the DOD was used. The 1995-96 PSS Universe File was used as the primary source of the private school sampling frame. The first step in frame construction involved subsetting the file to schools located in counties that constituted the ECLS-K PSU sample. Further subsetting retained only those schools that offered transitional kindergarten, kindergarten, or transitional first grade, or which were strictly ungraded, as indicated by the school's grade span. The constructed ECLS-K school frame included 18,911 public-school records and 12,412 private-school records. The school frame was supplemented in the spring of 1998 to include schools that would be operational in fall 1998 but that were not included in the frame just described. The procedures used to supplement the frame are given later in this section. Table 3-4 gives the estimated the number of schools offering kindergarten programs and the number of kindergarten students from the ECLS-K school frame. These are the numbers of schools and students in the sampled PSUs in the frame weighted by the inverse of the PSU selection probabilities. "}, {"section_title": "3-10", "text": ""}, {"section_title": "School Measure of Size", "text": "Within each PSU, schools with fewer than a predetermined minimum number of kindergarten students were clustered together before sampling in order to obtain a sample that is closer to self-weighting. The minimum number of kindergartners was 24 for public schools and 12 for private schools. Schools were selected with probability proportional to size. As with the PSU sample, a weighted MOS was constructed taking into account the oversampling of APIs: where 2.5 is the oversampling rate for APIs, and n API,ij and n other,ij are the counts of API kindergarten students and all other kindergarten students, respectively, in school j of PSU i."}, {"section_title": "School Allocation", "text": "Schools were sampled at rates designed to result in an approximately self-weighting sample of students within public and private school strata. The target number of sampled schools per PSU was calculated separately for public schools and private schools, and for self-representing and non-selfrepresenting PSUs. The number of schools selected was the target number of schools adjusted upward by the estimated school response and eligibility rate. 3-11"}, {"section_title": "Public Schools", "text": "The total MOS for public schools was partitioned into the self-representing and non-selfrepresenting strata. There are 100 PSUs in the ECLS-K sample, of which 24 are in the self-representing strata. The number of public schools selected from the self-representing strata was calculated as where n is the total number of public schools to be selected, w i is the weight of PSU i, and The value for n is 800/.85 = 941 where .85 is the expected eligibility and response rate for public schools. The supplement of public schools was expected to add relatively few schools to the frame and thus the 85 percent rate was not modified. The distribution of sampled schools was approximately 291 for self-representing strata and 650 for non-self-representing strata. For self-representing and nonself-representing strata alike, the number of schools allocated to each PSU was proportional to the weighted MOS of the PSU (w i \u00d7PSUMOS i ). In the ECLS-K public school frame, 4 percent of public schools had fewer than 24 kindergarten students. These schools were combined with other schools in the same PSU to form clusters with at least 24 students prior to sampling. Schools with 24 students or more were not grouped, but were also referred to as clusters (of one school each). To sample approximately 941 public schools, around 915 clusters (single schools or groups of schools) have to be selected. As a general rule, if a sampled school or cluster of schools had 24 or more students, 24 students were selected. However, for practical reasons, all students in the sampled school or cluster were selected if there were fewer than 27 students. More details on the clustering of schools are found in the next section. The number of clusters was allocated to each PSU proportionally to the weighted MOS of the PSU (w i \u00d7PSUMOS i ). When the 915 clusters were allocated to PSUs, it was discovered that in 5 PSUs there were not enough clusters in the frame to select the required number of clusters. This resulted in only 900 clusters being selected. Table 3-5 shows the expected distributions of clusters, schools, and students. \nSchools with fewer than 24 kindergarten students were clustered. Within each PSU, the list of small schools was sorted in ascending order of kindergarten enrollment; it was then split in half, with 3-14 the second half re-sorted in descending order. The two halves were then put together in an interleaving fashion. Beginning at the top of the list, clusters of schools with at least 24 kindergarten students were formed. If the last cluster on the list still did not have the required 24 minimum, then it was put together with the next to the last cluster on the list. This clustering scheme resulted in 18 clusters with 5 or more schools, which were considered problematic as far as fieldwork was concerned. The worst case was one cluster with 13 schools and only 41 students. In order to minimize the number of clusters having 5 or more schools, each problematic cluster was broken into groups of 2 or 3 schools, and each group was combined with the smallest of the \"large\" schools having 25 or more kindergarten students. Since enrollment in schools with missing kindergarten enrollment was imputed to be equal to 24, grouping any of these imputed schools with another school was avoided, lest they might turn out not to have kindergarten students. In addition to the 18 problematic clusters above, there were 12 PSUs with only 1 small school (with fewer than 24 kindergarten students) and there were 2 PSUs with only 2 small schools that, when grouped together, still had fewer than 24 kindergarten students. These small schools or groups of small schools were manually combined with the smallest school in the PSU having 25 or more students (see table 3-7). \nWithin each PSU, the clusters were sorted by the MOS and separated into three size classes of roughly equal size (high, medium, and low). Within each size class, clusters were sorted by the proportion of APIs in a serpentine manner (alternating the sort order from one size class to the next).\nEach public school district having one or more schools sampled was sent a sampling framebased list of all schools offering kindergarten. Districts were asked whether any school expected to offer kindergarten in academic year 1998-1999 was missing from the list. For each school identified by the district, school name, address, telephone number, grade span, and kindergarten enrollment were obtained. Districts were also contacted that fell within the boundaries of the ECLS-K PSUs, but for which the CCD file listed no schools offering kindergarten, unless it was clear from their name that they were strictly secondary school districts (e.g., Middlebury Union High School District). The information obtained from the school districts was checked against the ECLS-K public school frame to confirm that these schools were truly new or newly eligible. Bona fide new schools were given a chance of being sampled. A new school's chance of selection was conditioned on the school district's probability of selection. Overall, 252 new public schools were identified. Of these, 19 were selected using systematic sampling with probability proportional to size where the MOS was the same as it was for schools sampled from the main sample. Thus, a total of 953 public schools were included in the sample (934 + 19)."}, {"section_title": "3-12", "text": ""}, {"section_title": "Private Schools", "text": "The same procedure was used to determine the allocation of the private schools. The private school target samples are labeled n SR \u2032 and n NSR \u2032 for self-representing and non-self-representing PSUs respectively, and n\u2032 is the sum of n SR \u2032 and n NSR \u2032. The value of n\u2032 is 200/.60=333, where .60 is the expected eligibility and response rate. The supplement to the frame was expected to add some private schools with kindergarten programs. The rate 60 percent was used because of the uncertainties associated with the estimate of the eligibility and response rate for private schools. The percentage of schools with fewer than 24 kindergarten students was large for private schools. Approximately 56 percent of private schools offered a kindergarten program that had fewer than 24 students. Schools having fewer than 12 kindergarten students (according to the frame) were grouped into clusters of schools with at least 12 students in each cluster, following the clustering rules discussed in the next section. Schools with 12 students or more were not grouped. As a general rule, if a sampled school or cluster of schools had 24 or more students, 24 students were selected; if a sampled school or cluster had fewer than 24, all students were sampled. However, for practical reasons, all students in the sampled school or cluster were selected if there were fewer than 27 students. In order to sample approximately 333 private schools, 278 clusters were selected (single schools or groups of schools). Table 3-6 shows the expected distributions of clusters, schools, and students.\nSchools with fewer than 12 kindergarten students were clustered. Within each PSU, the list of private schools was first sorted by religious and nonreligious affiliation. If the number of religious schools and nonreligious schools in the PSU differed by no more than a factor of 3, the smaller of the two 3-15 lists (religious or nonreligious) was sorted in descending order while the larger of the two lists was sorted in ascending order of kindergarten enrollment. The two lists were then put together in an interleaving fashion, so that the records that were at the bottom of the longer list were records with larger kindergarten enrollment, and did not have to be grouped together. Beginning at the top of the entire list, clusters of schools of at least 12 kindergarten students were formed. If the last cluster on the list still did not have the required minimum size, it was put together with the next to last cluster on the list. If the number of religious schools and nonreligious schools in the PSU differed by a factor greater than 3, schools were not separated into religious and nonreligious lists. Instead, the entire list of schools was sorted in ascending order of kindergarten enrollment; it was then split in half, with the second half re-sorted in descending order. The two halves were then put together in an interleaving fashion. Clusters of schools were formed as above. There were 3 PSUs where the clustering of small schools as specified above did not work well. Two of the 3 PSUs had only 1 small school each and the third one had 2 small schools that, when grouped together, still had fewer than 12 kindergarten students. These small schools or groups of small schools were manually combined with other large schools in the PSU (table 3-18). \nWithin each PSU, each cluster was identified as religious, mixed, or nonreligious. 7 The list of clusters was then sorted by these three categories. Within each category, the clusters were sorted in a serpentine manner by the MOS prior to selection. However, for the self-representing PSUs, all clusters were sorted as if they were from the same PSU, i.e., the aggregated list of clusters from the 24 selfrepresenting PSUs was sorted by religious affiliation (religious/mixed/nonreligious). This procedure provided better control of the sample distribution of religious/mixed/nonreligious clusters. Across nonself-representing PSUs, clusters were sorted by religious affiliation, and within each category of religious affiliation, by the MOS in a serpentine manner.\nThe procedure for obtaining new school information from Catholic dioceses was exactly the same as for public schools. Since a diocese could cut across county or even state lines, each school identified by a diocese had to be associated with the correct county, and hence the correct PSU, before checking to see whether it was truly new. Since dioceses might cross PSU boundaries, a new Catholic school's chance of being sampled had to be conditioned on the diocese's probability of selection within the PSU where the new school was located. There were 126 new Catholic schools identified, and 6 were selected using systematic sampling with probability proportional to size. When combined with the main sample, the final Catholic school sample size was 123 (117 + 6). The supplemental frame contained 11,405 private schools. A sample of 279 schools was selected using systematic sampling with a probability proportional to these imputed enrollments. Each sampled school was contacted by telephone and screened to ascertain whether the school was public or private, whether it would be open in academic year 1998-1999; and if it would offer kindergarten. If the school met all of these conditions and was not Catholic, the school was eligible and released for data collection. A second supplemental procedure involved contacting local education agencies (LEAs) and local government offices for information on non-Catholic private schools. This procedure was done only in the smallest ECLS-K PSUs, on the theory that if these PSUs had coverage problems their large weights were likely to introduce a larger bias in the estimates. All LEAs within these PSUs were contacted by telephone. For each city/town within the PSU, a list of local government offices was compiled using the blue pages. Successive government offices were called within a city or town until one was found that could provide information on private schools. As with the yellow pages, new schools identified by LEAs and local government offices were unduplicated against the PSS file before being added to the new school frame. Since kindergarten enrollment was unknown, it was imputed as described in the previous paragraph and sampling was performed using systematic sampling with probability proportional to size. The LEA search resulted in the identification of 30 new private schools after unduplication, of which 14 were sampled. The local government search yielded 19 new schools, of which 8 were sampled. Finally, 3-20 three additional new private schools were reported by field staff based on personal knowledge. Of these, two schools were sampled. The same screening procedures to ascertain whether the school was public or private; whether it would be open in academic year 1998-1999; and if it would offer kindergarten were then applied to these sampled schools. The total number of non-Catholic private schools that were sampled was 303. After the screening procedures were applied, only 109 of these schools were eligible. These 109 schools are referred to as the supplemental sample of non-Catholic private schools. "}, {"section_title": "3-13", "text": "The number of clusters was not allocated separately to each self-representing PSU, since sampling was done on the aggregated list of schools in the self-representing PSUs. For the non-selfrepresenting PSUs, the sample was allocated to each PSU proportionally to the weighted MOS of the PSU , with a minimum of one cluster per PSU imposed if the PSU was so small that it was not allocated any clusters. "}, {"section_title": "Clustering of Small Schools", "text": "As noted above, schools with fewer than 24 students (public) or 12 students (private) were clustered together in order to obtain a sample that was closer to self-weighting. For example, if a school with 12 students was not clustered, the students from that school would be sampled at about half the probability as students in larger schools. The goal of the clustering of small schools was to form school clusters with a small number of schools that have close to 24 students and are heterogeneous. This goal was set so that if a cluster was selected, it would not be necessary to recruit many small schools; furthermore, the heterogeneity of schools improves the reliability of the estimates. Heterogeneity was defined by school size for public schools, and by religious affiliation and school size for private schools. Within each PSU, schools with fewer than a predetermined minimum number of kindergarten students were separated from the frame and clustered together. A few exceptions to this general rule did occur and are discussed later. The procedures for clustering of schools are described below."}, {"section_title": "Implicit Stratification of Schools/Clusters of Schools", "text": "Public schools with more than 24 kindergarten students and private schools with more than 12 kindergarten students were not clustered. However, they are referred to as clusters (of one school each) for simplicity. 3-16"}, {"section_title": "School Selection", "text": "Selection of the clusters of schools was systematic, with probability proportional to the MOS. Sampling of public schools was done independently within PSU (i.e., each PSU forms a separate sampling stratum) after the clusters of schools were sorted by MOS and proportion of API. Sampling of private schools was done separately for self-representing PSUs and for non-self-representing PSUs. All self-representing PSUs were placed in one sampling stratum and all non-self-representing PSUs were placed in a second stratum. In the self-representing stratum, sampling was done with one random start after sorting clusters of schools by religious affiliation and MOS. In the non-self-representing stratum, sampling was done with one random start after sorting clusters of schools by PSU, religious affiliation, and MOS. 7 A cluster is \"religious\" if all schools in the cluster are Catholic schools or non-Catholic religious schools; \"nonreligious\" if all schools in the clusters have no religious affiliation; \"mixed\" if it has a combination of schools with or without religious affiliation. 3-17"}, {"section_title": "The ECLS-K Main School Sample", "text": "A total of 1,280 schools were selected for the ECLS-K, of which 934 were public and 346 were private schools. The characteristics of the school sample are presented in table 3-9. "}, {"section_title": "3-18", "text": ""}, {"section_title": "Supplemental School Sample", "text": "As mentioned earlier, the public and private school frames were supplemented in the spring of 1998. The procedures for supplementing the frames were different for public schools, Catholic schools and non-Catholic private schools. These procedures are discussed below separately."}, {"section_title": "Sampling Children, Parents, and Teachers Within Schools", "text": "The goal of the student sample design was to obtain an approximately self-weighting sample of students to the extent possible while achieving the minimum required sample size for APIs (the only subgroup that needed to be oversampled to meet the study's precision goals). Two independent sampling strata were formed within each school, one containing API students and the second, all other students. Within each stratum, students were selected using equal probability systematic sampling, using a higher rate for the API stratum. In general, the target number of children sampled at any one school was 24. The actual sample size per school ranged from 1 to 28. If one twin was selected into the sample then both twins were included, raising the maximum number of children to sample from 24 to 28 in a small number of schools. Once the sampled children were identified, parent contact information was obtained from the school and was used to identify a parent or guardian for the parent interview. During the fall-kindergarten data collection, a census of kindergarten teachers was taken at each school. In spring-kindergarten, new teachers who had joined the schools and teachers in schools participating after the fall were added to the census of teachers. In the spring-first and spring-third grade data collections, the only teachers included were the teachers of the sampled children. For every data 3-21 collection, each sampled child was linked to his or her teacher. A child could be linked to only one general education teacher. In cases where a child had more than one general education teacher, a 'primary' teacher was identified for the child. In addition, special education teachers and service providers were linked to sample cases who received such services. As with the general education teachers, a child would be linked to only one special education teacher or service provider. Details on the linking of teachers to the children is found in Chapter 4."}, {"section_title": "Fall-First Grade Subsample", "text": "The fall data collection consisted of a 30 percent sample of schools containing approximately 25 percent of the base year students eligible to participate in the second year. The goal of this subsample was to measure the extent of summer learning loss and the factors that contribute to such loss and to better disentangle school and home effects on children's learning"}, {"section_title": "PSU Sample", "text": "A subsample of ECLS-K PSUs was selected for the fall-first grade data collection. All 24 of the self-representing PSUs were retained. Of the 76 non-self-representing PSUs, 38 were retained by sampling one PSU per stratum with equal probability."}, {"section_title": "School Sample", "text": "Base year schools in the 62 fall-first grade sampled PSUs were stratified by frame source  "}, {"section_title": "Child Sample", "text": "Fall-first grade data collection consisted of the direct child assessment and the parent interview. Data collection was attempted for every eligible child found still attending the school in which he or she had been sampled during kindergarten. \"Eligible\" was defined as a base year respondent (i.e., a child who had either a fall-or spring-kindergarten child assessment or parent interview, or a child who could not be assessed because of a disability). Base year nonrespondents were not sampled and were handled by adjusting the weights (see chapter 7 for more details). Because of the additional burden of school recruiting, the cost of collecting data for a child who transferred from the school in which he or she was originally sampled exceeds that for a child who 3-23 stayed enrolled. To contain these costs, a random 50 percent of children were subsampled to be followed for fall-first grade data collection in the event that they had transferred. Prior to subsampling with equal probability, children were stratified into groups of nonmovers, movers with information identifying their new schools, and movers without such identifying information. A flag was created for each child indicating whether the child had been sampled to be followed. Except for children who were repeating kindergarten, all base year children sampled in schools with a high grade of kindergarten are de facto movers. Since many of these movers might move en masse to the same first grade school, steps were taken to follow these children at a higher rate. Using the information collected during spring-kindergarten, a list of destination schools was compiled for each such school. The destination school having the most movers was designated as primary, unless no such school had more than three movers. Children who moved en masse into a primary destination school in fall-first grade were treated as \"nonmovers\" and were not subsampled. All other movers were sampled at the rate of 50 percent."}, {"section_title": "Spring-First Grade Sample", "text": "The ECLS-K spring-first grade data collection targeted all base year respondents and not just the fall-first grade subsample. Hence, the sample includes children who were assessed and whose parents were interviewed in fall-or spring-kindergarten, as well as children who were excluded from the direct assessments because of a disability. In addition the spring student sample was freshened to include current first graders who had not been enrolled in kindergarten in 1998-99 and, therefore, had no chance of being included in the ECLS-K base year kindergarten sample. This group includes children who skipped kindergarten altogether in 1998-99, children who attended a kindergarten program outside of the U.S. in 1998-99, and children who were in first grade in 1998-99 and repeating it in 1999-2000. While all students still enrolled in their base year schools were recontacted, only a 50 percent subsample of base year sampled students who had transferred from their kindergarten school was followed for data collection. 3-24"}, {"section_title": "Subsampling Movers", "text": "In spring-first grade all children in a random 50 percent subsample of base year schools were flagged to be followed for data collection if they transferred from their base year school. (This is in contrast to fall-first grade where a random 50 percent of children in each of the 30 percent of schools subsampled were flagged.) In order to maximize the amount of longitudinal data, care was taken during spring-first grade sampling to ensure that any child who had been flagged to be followed in fall-first grade would continue to be followed. In selecting the spring-first grade 50 percent subsample of schools where movers would be flagged for followup, the three primary strata were self-representing PSUs, non-self-representing PSUs that had been selected for fall-first grade, and non-self-representing PSUs that had not been selected for fall-first grade. Within these major strata, schools were grouped by frame source (original public, original private, new from Catholic dioceses, new from local governments, etc.). Finally within each frame source, schools were stratified by cooperation status (i.e., cooperating in spring-kindergarten or not cooperating in spring-kindergarten), and arranged in this original selection order. Schools that had been part of the 30 percent fall-first grade sample were automatically retained. Then equal probability sampling methods were employed to augment the sample to the desired 50 percent of schools. The net result of these procedures was that every base year selected school had a 50 percent chance of having its ECLS-K movers followed during spring-first grade, and any mover who had been followed in fall-first grade would still be followed in spring-first grade."}, {"section_title": "Sample Freshening", "text": "As noted earlier, a sample freshening procedure was used to make it possible to produce estimates of all children enrolled in first grade in the spring of 2000. The spring-first grade student freshening used a half-open interval sampling procedure (Kish, 1965). The procedure was implemented in the same 50 percent subsample of ECLS-K base year schools where movers were flagged for followup. Each of these schools was asked to prepare an alphabetic roster of students enrolled in first grade and the names of ECLS-K kindergarten-sampled students were identified on this list. Beginning with the name of the ECLS-K first kindergarten-sampled child, school records were checked to see whether the student directly below in the sorted list attended kindergarten in the United States in fall 1998. If not, (1) that child was considered to be part of the freshened sample and was linked to the base year sampled student (i.e., was assigned that student's probability of selection), and (2) the record search procedure was repeated for the next listed child, and so forth. When the record search revealed that a child had been 3-25 enrolled in kindergarten the previous year, that child was not considered part of the freshened sample and the procedure was started all over again with the second base year ECLS-K sampled student name, and so on. 8 Student freshening brought 165 first graders into the ECLS-K sample, which increased the weighted survey estimate of the number of first graders in the United States by about 2.6 percent. The student freshening procedure was not entirely free of bias. A first grader would have no chance of being in the ECLS-K first grade sample if he or she was enrolled in a school where neither the child nor any of his or her classmates had attended kindergarten in the United States in fall 1998. This would be a rare circumstance and is not thought to be an important source of bias. A more significant source of potential bias is nonresponse. One source of nonresponse inherent to the freshening plan was that the procedure only involved students who had not transferred from the school in which they had been sampled during the base year. Another source of nonresponse that also affected the freshening procedure was schools that refused to provide or could not provide the necessary information such as alphabetic roster of students enrolled in first grade, or whether students attended kindergarten the previous year."}, {"section_title": "Spring-Third Grade Sample Design Issues", "text": "The procedures used in spring-first grade to subsample movers reduced the loss in sample size and reduced data collection costs since movers cost considerably more to interview than nonmovers. These procedures were also used for the ECLS-K third grade data collection with some modifications. One reason for modifying the procedures was that some children had already moved out of their original school, and some of the movers were sampled and some were not. In addition, there were concerns about special domains of interest and methods that might be used to increase the sample size for the children in these groups. Results from the first grade collection were used to address these third grade sample design issues. "}, {"section_title": "Estimates from Spring-First Grade", "text": ""}, {"section_title": "3-28", "text": "The mover rates show the types of variation that had been expected, with higher mover rates for Black and Hispanic children, for example. A total of 39 percent of the children in non-Catholic private moved to public schools when they advanced from kindergarten to first grade. Seventy-six percent of children who moved from kindergarten in private schools to first grade in public schools attended non-Catholic private schools in kindergarten, about three times the number of children who moved from Catholic to public schools. One of the concerns in using the kindergarten to first grade mover rates to make estimates for future transitions was whether the mover rates for the 1-year time period between kindergarten and first grade were reasonable when applied to the transition between first and third grade. One might argue that a 2-year period should result in a higher mover rate than the 1-year rate. However, parents may be more reluctant to change the school for a child between first and third grade than between kindergarten and first grade. Kindergarten is also special for other reasons, for example, the availability of full-and part-day classes may be an important factor in the choice of the kindergarten. There are no other data sources that could be used to examine differential mover rates between years. As a result, the 1-year moving rates in table 3-11 were applied to the 2-year period between first and third grade after adding another 5 percent to the rates to account for the 2-year period. An exception was made for children who attended non-Catholic private schools in the base year and had the highest rates of moving among all the domains examined. This was assumed to be a special case for kindergarten and the average mover percentage was applied to these children for the third grade. The other main concern was whether the extremely high response rate for nonmovers (97 percent) could be duplicated in future years. To be more conservative and to account for the fact that nonrespondents from earlier rounds (i.e., base year respondents were included in the third grade sample whether or not they responded in first grade) were included in subsequent rounds of data collection, it was assumed that a 95 percent response rate would be achieved for nonmovers in third grade."}, {"section_title": "Third Grade Sample Design", "text": "The basic plan for third grade was the plan adopted for first grade where only 50 percent of the children who moved from the original sample schools were followed into their new school. This plan was modified for third grade as described below. The basic plan was to include all eligible children in the third grade sample if they remained in the school they attended previously and to treat movers according to the following specifications:"}, {"section_title": "3-29", "text": "To be eligible for the sample, a child had to have been a base year respondent or sampled in first grade. Children who moved out of the country or died were excluded (i.e., ineligible). All the respondents in the base year who remained in their original schools, where the original schools also included destination schools (described later). All the children who moved from an original school in a previous wave of data collection and were retained in the subsample of movers for that wave. For example, if a child moved between kindergarten and first grade and was part of the 50 percent subsample that was followed, then the child would be retained for future rounds without subsampling as long as the child remained eligible. A subsample of 50 percent of the children who moved from their original school at any time after the base year. For example, a child who moved between first grade and third grade would be subject to subsampling and had a 50 percent chance of being included in the third grade followup. In alternatives discussed later, differential subsampling rates were introduced. To prevent an accumulation of nonresponse, the ECLS-K design does not use the approach of many longitudinal studies that excludes sampled units from future rounds if they did not respond in a particular wave. Instead, the basic plan was modified so that all eligible base-year respondents who were sampled in the first grade followup would be eligible for the third grade followup even if they did not respond in the first grade. Even though the participation rate for first grade nonrespondents might be lower compared with first grade respondents in the subsequent followups, the effort was an attempt to increase overall response rates by including first grade nonrespondents in third grade. The approach is also consistent with the analytic use of the data for the ECLS-K, since many analyses may include less than complete wave responses. For example, a change in scores from kindergarten to third grade for subgroups is an important analytic objective, and it can be estimated without complete data at each wave. A second procedure that was part of the modification of the basic plan for the third grade followup was an extension of a procedure that was used in the first grade followup to deal with schools that ended with kindergarten (i.e., kindergarten was the highest grade offered). A school was called a destination school if at least 4 students from a school ending in kindergarten attended this school in first grade. For the third grade 28 original schools ended in second grade, and 3 of the destination schools identified in first grade ended in second grade. In total, 3 percent of all eligible first graders in the ECLS-K sample attended schools ending in second grade. As was done for the first grade sample, children in the destination schools were treated as nonmovers for the third grade sample. As nonmovers, they were all followed into their new schools resulting in a 2 percent increase of the third grade sample size over that which would result if 50 percent of these children were subsampled out as movers. 3-30 "}, {"section_title": "Expected Sample Size", "text": ""}, {"section_title": "Protecting the Language Minority Children", "text": "Special attention was paid to language minority and API children to ensure that the sample sizes would be large enough to support analytic goals in developing the sampling plans for the third grade. Children in the language minority group are children whose home language was non-English or who were screened using the Oral Language Development Scale (OLDS) prior to assessments during the base year (or first grade for freshened children). 9 Two classifications of APIs are shown in table 3-11. The first classification was identified using a strict definition of API, i.e., if the child was identified only as API by the composite race variable (RACE = 5-Asian or 6-Native Hawaiian or other Pacific Islander). The second classification was identified using a broader definition, i.e., if a child was identified only as API as in the strict definition (RACE = 5-Asian or 6-Native Hawaiian or other Pacific Islander) or if a child has positive answers to the API race identification variables (WKASIAN = 1-Child is Asian or WKPACISL = 1-Child is Native Hawaiian or other Pacific Islander). The variables RACE, WKASIAN and WKPACISL are in the base year data file. The broader definition of API yields a larger population of children. 9 Information about home language came from the parent interview and whether or not children were screened with the OLDS was based on information provided by their schools (see the ECLS-K Base Year Public-Use Data Files and Electronic Code Book: User's Manual). "}, {"section_title": "3-31", "text": ""}, {"section_title": "3-32", "text": "After reviewing the expected yields without oversampling, it was decided to increase only the sample size for children belonging to the language minority group. Beginning in third grade, these children would not be subsampled for followup if they moved from their original school. Instead, data collection would be attempted for all language minority children. Table 3-13 is analogous to table 3-12 but is adjusted for this approach of retaining all movers in the language minority group (in practice the subsampling rates are shown as 95 percent because some children became ineligible). One consequence of protecting this subgroup is to increase the sample size and precision for the subgroup. The design effect due to subsampling is slightly lower under this plan because a smaller proportion of the movers were subsampled than under the basic plan (only the movers that were already subsampled in first grade are subsampled). Another consequence is that the number of schools that the sampled children attended increased. Because all language minority children were followed , table 3-13 shows an expected increase of 395 schools in third grade (1,918 -1,523 = 395)."}, {"section_title": "Cost Savings Options", "text": "As mentioned earlier, the cost of collecting data for a child who transferred from the school in which he or she was originally sampled exceeds that for a child who stayed enrolled at the same school. The additional costs include recruiting the schools and collecting data within a school for one or very few children. This differential cost is the primary reason that all movers were not followed in the ECLS-K. For example, base year respondents attended 1,014 schools in the base year. The number of schools with at least one sampled child after subsampling movers increased to 2,131 by spring-first grade and to 3,225 by spring-third grade. The following options were considered to reduce the cost of the data collection, but none were implemented in third grade. Collect only child assessment and parent interview data for movers. This approach would eliminate some of the costs of contacting school districts and schools for a child in a school where there were no other sampled children Drop movers from the sample if they did not respond in a previous wave (defined by no child or parent data from the wave). Subsample all movers or some set of movers (such as those that moved between waves 1 and 4) at a lower rate than 50 percent. The sampling rate for movers is set at 47 percent (instead of 50 percent) to account for ineligibility of students in future rounds. If the language minority group is preserved, it is set at 95 percent. 2 The number of new third grade schools is estimated as 1.5 schools per sampled new mover. 3 2,621 schools from kindergarten/first grade plus the new third grade schools. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02. 3-34 Specify a minimum response set of data for children across rounds of data collection and subsample or eliminate children who do not meet this minimum."}, {"section_title": "3-33", "text": "The main disadvantage of the last three options is that the sample size would be reduced, thus jeopardizing analyses of data for smaller subgroups. Dropping data collection of schools and teachers as described in the first option would eliminate any longitudinal analyses using school and teacher data."}, {"section_title": "Precision Requirements", "text": "When the precision estimates were computed from the kindergarten sample at the end of the base year, higher than expected design effects for assessment scores were observed. The design effects for most other statistics, such as proportions of children with a particular characteristic, were moderate and within the range expected (1.6 to 6.9 for proportions greater than 30 percent for an average of 4.0). The design effects for assessment scores (4.5 to 9.5 for an average of 6.9) were investigated and found to be correct and unrelated to data collection artifacts. For example, interviewer effects were found to be negligible and did not bias assessment scores. The design effects for test scores were much larger than the average of 3.8 that was expected at the design stage. For all students, the design effects for math and reading scores were about 6.5, while for general knowledge the design effects were even larger at 7.7. The spring-third grade estimates of design effects are similar to those in the earlier rounds and are larger than had been predicted prior to any data collection. The longitudinal estimates have design effects that are not as large as might be expected given the larger cross-sectional design effects. In fact, the correlations for mean test scores seem to be running as high as .8 to .9. The higher than expected correlations make it possible to meet the precision requirements for estimates of change with smaller sample sizes.  3-36"}, {"section_title": "Spring-Third Grade Sample", "text": "To summarize, the sample of children for spring-third grade consists of all children who were base year respondents and children who were brought into the sample in spring-first grade through the sample freshening procedure. Sample freshening was not implemented in third grade; hence no new students entered the sample. While all students still enrolled in their base year schools were recontacted, slightly more than 50 percent of the base year sampled students who had transferred from their kindergarten school were followed for data collection. This subsample of students was the same 50 percent subsample of base year movers followed in spring-first grade, including the movers whose home language was not English (language minority students). Children who were followed in spring-first grade were retained in the sample (i.e., the mover followup still targeted the same 50 percent subsample of children in the base year schools). In addition, children whose home language was not English and who moved between springfirst grade and spring-third grade were all retained rather than being subsampled at the 50 percent rate. If they had moved before first grade, they were not to be followed. This modification to the mover followup procedure provided a larger sample of children whose home language is not English for analytic purposes. The mover followup activities that originally targeted a 50 percent subsample of children in base year schools resulted in a 54 percent subsample with the addition of language minority children. Tables 3-14 (count) and 3-15 (percent) show the characteristics of the achieved third grade sample compared with the expected third grade sample. The total number of children in the language minority group is virtually the same as the expected number while the total number of children in the other group is about 5 percent larger than the expected number. In computing the expected sample size, the same mover rate was assumed for both groups of children. The third grade sample shows that the nonlanguage minority children moved at a lower rate (42 percent) than the language minority children (44 percent) resulting in a slightly larger sample of non-language minority children. The agreement between the expected and achieved sample sizes is rather remarkable given the numerous assumptions required. The actual percent distribution of third graders within each subgroup is as expected with the exception of the Catholic and non-Catholic private schools where the percent of children in Catholic schools is higher than that of children in non-Catholic private schools. This may be due to the lower completion rate of children in non-Catholic private schools compared with children in Catholic private schools (93 percent and 97 percent, respectively). Elsewhere among the children in the language minority group, the 3-37 difference between the expected distribution and the actual distribution is less than 1 percent. Elsewhere among the children not in the language minority group, the difference is less than 3 percent.   As seen in table 3-1, the achieved sample for third grade is consistent with the goal to achieve a fifth grade sample of 10,300 children with completed data. By the end of third grade, there were 14,470 children with complete assessment data or not assessed due to a disability; 13,489 children with completed parent data; and 15,305 children in either group. There were 12,654 children with the more restricted condition, i.e., with complete assessment data (or not assessed due to a disability) and parent data. These sample sizes suggest that the fifth grade target sample size can be achieved. 4-1"}, {"section_title": "DATA COLLECTION METHODS", "text": "The following sections discuss the data collection procedures in the third grade data collection phase of the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). Section 4.1 gives an overview of the data collection methods. Detailed information is provided on roles and responsibilities in the study (section 4.2), study training procedures (section 4.3), fall preassessment school contacts (section 4.4), spring-third grade data collection (section 4.5), and data collection quality control procedures (section 4.6)."}, {"section_title": "Overview of Data Collection Methods", "text": "The ECLS-K third grade data collection was conducted in the fall and spring of the 2001-02 school year. Fall data collection included contacting sampled schools to schedule appointments to conduct the child assessments in the spring of the school year, verify the parent consent procedures, link children to their teachers, identify children who had withdrawn from the school, and obtain locating information about their new schools. Spring data collection included the administration of direct child assessments and parent interviews and the collection of teacher and school questionnaires, student records abstracts, and facilities checklists. The activities to locate children and gain cooperation of the schools into which they transferred began in the fall and continued during the spring data collection. The content and timeline of the third grade data collection is shown in exhibit 4-1. Computer-assisted personal interviewing (CAPI) was the mode of data collection for the child assessments; telephone and in-person computer-assisted interviewing (CAI) was the mode of data collection for the parent interview; and self-administered questionnaires were used to gather information from teachers, school administrators, and student records. Field staff completed the facilities checklist."}, {"section_title": "Roles and Responsibilities in the ECLS-K Study", "text": ""}, {"section_title": "School's Role", "text": "During school recruitment, the schools were asked to designate a staff member to be the school coordinator to assist the ECLS-K staff with all school arrangements. Since the child assessments were administered at the schools, schools needed to provide appropriate space for conducting the assessments."}, {"section_title": "School Coordinator's Role", "text": "A school coordinator was designated by the principal to facilitate the ECLS-K activities in the school. The school coordinator played a significant role in the smooth functioning and successful completion of the ECLS-K child assessments in each cooperating school. He or she knew the personality of the school, the most opportune times to schedule the assessments, the available locations where the 4-3 one-on-one assessments could be conducted, and the best way to notify students, their parents, and their teachers of the assessment. The coordinator was asked to assist the ECLS-K in four ways: 1. Notify selected students, their teachers, and their parents of the study; 2. Arrange for suitable space for the assessment activities; 3. Provide information on sampled children, such as their grade and teacher's name; and 4. Distribute teacher and school questionnaires."}, {"section_title": "Supervisor's Role", "text": "There were a total of 85 supervisors during the third grade data collection. Eight of the supervisors oversaw interviewers conducting the parent telephone interview. The remaining 77 oversaw field staff conducting both the parent interviews and child assessments. The supervisors' responsibilities were as follows: Contact each school assigned to them to Follow up and track receipt of parental consent forms, as necessary; Update the Field Management System (FMS) regularly and report to their field manager; Transmit updated FMS data to the home office; Pick up e-mail regularly; and Return all materials at the end of the field period. 4-4"}, {"section_title": "Assessor's Role", "text": "A team of three to four assessors worked with each supervisor in a work area. The primary responsibilities of the ECLS-K assessors were to conduct the computer-assisted one-on-one child assessments and parent interviews. There were a total of 319 assessors, with 61 having conducted the parent telephone interview only (these 61 assessors are referred to as interviewers rather than assessors in the remainder of this chapter). The remaining 258 conducted both the parent interview and the child assessments. In addition to these responsibilities, some assessors were asked by their supervisor to assist with various other activities that took place in the school. These included, but were not limited to, assisting with sampling activities, preparing parental consent forms (if required), collecting teacher questionnaires, and assisting with various other recordkeeping tasks."}, {"section_title": "Field Manager's Role", "text": "Eight experienced regional field managers were assigned to oversee the work of the 85 supervisors. The field managers received regular reports from supervisors. They held weekly telephone conference calls with each supervisor assigned to them. If a supervisor had an immediate problem, he or she was encouraged to call the field manager at any time. Depending on the stage of the field period, the telephone conference calls between supervisors and field managers reviewed those activities that were in the planning stage, in progress, or in the process of being completed. These discussions included the following topics: Status of telephone contacts with original and transfer schools; Status of assessments scheduled in original schools by work area; Status of parent consent and followup in original and transfer schools; Status of linking children to teachers; Status of children who were withdrawn from the school;"}, {"section_title": "4-5", "text": "Any refusal cases; Receipt of all school materials; and Overall and individual costs in the work area."}, {"section_title": "Field Staff Training", "text": "Several in-person training sessions were conducted to prepare staff for the third grade data collection. In the fall of 2001, supervisors were trained to contact original schools and recruit transfer schools. In the spring of 2002, four training sessions were held: one to train trainers, one for staff who only conducted parent interviews, one for field supervisors, and one for assessors. All training sessions were conducted using scripted training manuals to ensure that all trainees received the same information. Center for Education Statistics, forthcoming) for a description of the child assessment materials. The following sections discuss the fall and spring trainings."}, {"section_title": "Advance Contact and Recruitment Training", "text": "During the fall 2001, advance contact was made with the schools in order to remind them about the study and to collect information that would be helpful in the spring 2002 national data collection. The advance effort not only reduced the burden on supervisors in the spring but also reacquainted schools with the study's procedures and gave supervisors a chance to encourage the schools' participation. The major fall tasks were to contact schools to set appointments for the child assessments in 4-6 the spring, to verify the parent consent procedures, to link children to teachers, and to identify children who had withdrawn from the school and obtain locating information about their new schools. Field supervisors were trained for 3 days in September 2001 to contact original sampled schools and transfer schools to set up the data collection in the spring. A total of 50 field supervisors and 2 field managers completed training. Topics included an overview of study activities to date, verifying parent consent procedures, identifying and locating children who moved from the schools they attended in the first grade, identifying the teachers of ECLS-K children and linking them to those children, and exercises on scheduling schools efficiently within a work area (see appendix A1: Advance Contact Training Agenda). As in the first grade training, advance contact and recruitment training was conducted using the automated FMS. The FMS is a database that is used throughout the data collection period to enter information about the sampled children, parents, teachers, and school and to monitor production on all data collection activities. The FMS contains information essential to conducting and monitoring the progress of the data collection. For example, it contains the names and addresses of each school, the principal's name and telephone number, the name and telephone number of the school coordinator, the first and last day of classes, the school hours, and the names of the sampled children in the school. For children, it contains information on their name and their parent's name, whether they have any accommodations or need to use a hearing aid or glasses, as well as other information. Information of the same sort is also collected about the teachers and parents. For example, the FMS contains the name, address, and telephone number of each parent. For each parent, the name of the sampled child (or children) is also listed. For teachers, the FMS contains their name, school, and the sampled children in their classroom. It also indicates whether they are a regular or a special education teacher. To monitor production, the FMS contains case disposition codes and case assignment information. During training presentations, the field supervisors entered information into the FMS, thus acquiring hands-on experience with the FMS and all field procedures prior to beginning data collection. The field supervisors also completed role-play exercises that involved entering information into the FMS."}, {"section_title": "Spring-Third Grade Training", "text": "Field supervisors, interviewers, and assessors were trained for the spring-third grade data collection in three sessions in February and March 2002. The first session was trainers' training and certification. During the second session, staff assigned to conduct only the parent interviews were trained. The last and largest training, held in March, involved the training of the supervisors and assessors. Before"}, {"section_title": "4-7", "text": "the March in-person training session, supervisors and assessors completed 8 hours of home study training on the study design, field procedures, and computer keyboard skills. Staff conducting only the parent interviews did not have a pre-session home study to complete."}, {"section_title": "Trainers' Training and Certification", "text": "The purpose of trainers' training was to (1) introduce lead and co-trainers to the training materials; (2) evaluate the flow, language, exercises, and time allotment of the training sessions; and (3) certify trainers on the child assessment. Some, but not all, co-trainers and runners (staff who assisted trainees who experienced difficulties with the CAI application and helped with the management and distribution of training materials) were also certified on the child assessment. Experienced trainers with in-depth knowledge of the ECLS-K conducted the trainer training sessions. Not only had these trainers developed the CAI specs, but they had also worked with expert consultants to develop the child assessments and the assessment materials, In addition, they conducted 9 assessments on nonsampled children between January and February 2002 in order to become certified on the assessments of the ECLS-K prior to the launch of the national data collection. Approximately 15 lead trainers, 15 co-trainers, and 15 runners were trained at trainers' training in a single room. As noted earlier, experienced trainers conducted the parent interview and the child assessment training sessions. In addition, a data display person responsible for running the electronic data display and two runners assisted in the training. Trainers' training was conducted for 5 days in February 2002 in Rockville, Maryland. The trainers' training agenda (see appendix A2: Trainer Training Agenda) covered many of the same topics that were presented during the assessment and parent interview training for the national data collection, as well as some additional topics on the trainers' tasks and responsibilities. In addition, trainers were certified on the child assessment following the procedures described in section 4.3.2.2. The only difference between trainer certification and field staff certification was that trainers were certified on an entire child assessment and the field staff were certified on approximately half of a child assessment. Assessor Training. The assessor training sessions were conducted in Los Angeles, California. Assessor training lasted for 5 days; field supervisors were also trained to perform all assessor activities. Two hundred sixty-six assessors 1 and 77 field supervisors completed training. Assessor training included an overview of study activities to date, interactive lectures on the direct child assessments and the parent interview, role-play scripts to practice parent interviews and direct child assessments, direct child assessment precertification exercises on each form of the direct child domain assessments, techniques for parent refusal avoidance, and strategies for building rapport with children (see appendix A5: Assessor Training Agenda). A major goal of the assessor training was to train field staff in the proper procedures to conduct the direct child assessments. The sessions provided trainees with practical experience with all the direct child assessment materials and procedures and the CAI programs before data collection. Trainees practiced entering information into the CAI system on laptop computers during training sessions on conducting the direct child assessments and parent interview role-play scripts. 1 Eight of the 266 trainees chose to leave the study prior to the start of data collection, leaving 258 assessors working in the field. 4-9"}, {"section_title": "Certification of the Child Assessors", "text": "Assessors and field supervisors conducted child assessments. Training to administer the assessment battery included exercises that took into account the different ways children may answer questions. Ambiguous answers were included in both training interactives and role plays to provide practice in coding a variety of responses. Particular attention was paid to the question-by-question specifications (QxQs) for constructed response/open-ended questions so that a clear understanding of coding guidelines was established. These guidelines were reviewed frequently during training so that trainees were prepared for answers that were not phrased exactly as found in the scoring rubrics. Training on the specific subdomains was implemented in four parts. The first two parts Supervisors and assessors were certified on the child assessment by administering approximately half of a cognitive assessment to a child while being observed by certified evaluators. As evaluators observed the assessment, they completed the appropriate sections of the Assessment"}, {"section_title": "4-11", "text": "Certification Form. This form had both the trainee and the evaluator's names recorded on the cover as well as the date the evaluation was conducted. The Assessment Certification Form had two sections: section 1 (exhibit 4-2) rated the trainee on key skill areas, such as building rapport, using neutral praise, responding to behaviors presented by the child, appropriate pacing, and avoiding coaching. The evaluator marked each skill area that the trainee did not demonstrate appropriately. Exhibit 4-2. Section 1 of the Assessment Certification Form: School year 2001-02 Evaluator: As the assessment is administered, record whether or not the assessor successfully performed the following behaviors. Check \"No\" if the assessor makes 3 or 4 errors and needs to make improvements. Section 2 listed specific questions from each routing and subdomain form. The instructions for completing section 2 are shown in exhibit 4-3. For each of the listed questions observed, the evaluator recorded both the child's response and noted if the trainee did not demonstrate the specified required administration skills for that question. The required administration skills included reading questions verbatim, using appropriate probes, and using appropriate hand motions (gesturing). For each question on which the evaluator observed that the trainee did not demonstrate the required administration skill(s), he or she checked a box, indicating which skill was not performed. As mentioned earlier, during the practice child assessment, the evaluator simultaneously coded the child's response to the open-ended assessment items listed in the Assessment Certification Form. These open-ended items were flagged in the CAI program for quality control review and a screen showing how the trainee coded each of those answers to the questions was accessed at the end of the assessment for review with the evaluator as shown in exhibit 4-4."}, {"section_title": "4-12", "text": "Exhibit 4-3. Instructions for Section 2 of the Assessment Certification Form: School year 2001-02 SECTION 2: Specific Assessment Activities Supervisor/Evaluator: Code the items as the assessor administers the assessment. Code the child's response as the item is administered. If the item requires probing, check the box if the assessor does not use the appropriate probe. Check the box in the \"Verbatim\" column if the assessor does not read the item exactly as worded on the screen. Check the box in the \"Gesturing\" column if the assessor does not use appropriate hand motions. "}, {"section_title": "4-14", "text": "At the end of the \"live\" child assessment, after the child had been escorted from the room, the evaluator and the trainee reviewed each trainee's overall performance on the half of the cognitive assessment that he or she conducted. After discussing the ratings in section I, the trainee accessed the quality control screen. The trainee and evaluator reviewed their codes for each open-ended question asked in section II of the Assessment Certification Form. The evaluator then scored the certification assessment using the scoring form shown in exhibit 4-6. The evaluator first counted the number of boxes checked in section I: Rapport Building of the Assessment Certification Form (see exhibit 4-2), and recorded that number in the appropriate row of Form A of the Certification Scoring Form (exhibit 4-6). The evaluator then counted the number of check marks for each section of the assessment that was observed and recorded those numbers in the appropriate boxes of Form A. Exhibit 4-7 presents an example of a completed Form A. In this example, the evaluator observed one trainee who administered Reading Routing, Reading Yellow and Math Routing. The evaluator recorded zero (0) check marks for section I, zero (0) check marks for Reading Routing, one 1check mark for Reading Yellow, and zero (0) check marks for Math Routing. The total check marks recorded was one (1). The evaluator then calculated the number of possible points for the sections of the assessment observed using the Total Possible Points Chart displayed in exhibit 4-8. This chart was necessary for the assessment certification because each trainee only conducted half of the cognitive assessment with a child respondent; this chart helped the evaluator use the correct denominator for calculating the score. For each section of the assessment observed, the evaluator circled the last question recorded in the Assessment Certification Form and wrote the number of possible points in the appropriate box in the second section of the scoring form (Form B) (see exhibit 4-9). Continuing with the example started earlier, the evaluator observed all of Reading Routing (6 possible points), all of Reading Yellow (24 possible points), and all of Math Routing (7 possible points). In addition, section 1 contributes 5 possible points. Based on these sections of the assessment, the total number of possible points for this trainee is 42."}, {"section_title": "4-15", "text": "Exhibit 4-6. Certification Scoring Form: School year 2001-02"}, {"section_title": "Certification Scoring", "text": "Step 1: Record the number of check marks from section 1: Rapport Building and section 2: Specific Assessment Activities in the appropriate boxes of Form A. Step 2: Sum each row and record total in Row Totals column of Form A. Step Step 6: Use Proportion Correct Chart to determine the proportion correct and write that proportion in this box: "}, {"section_title": "Number of Check Marks", "text": ""}, {"section_title": "4-20", "text": "Exhibit 4-11. Example of Proportion Correct Chart: School year 2001-02 Step 6: Use Proportion Correct Chart to determine the proportion correct and write that proportion in this box:   "}, {"section_title": "Parent Interview Training and Certification", "text": "Training assessors to conduct the parent interview also included interactive lectures and role plays that were designed not only to review the intent of the questions but also to demonstrate the different ways that parents may answer questions. Ambiguous answers were included in both training interactive lectures and role-play scripts to provide practice in probing and handling a variety of responses. Parent interview QxQs were carefully reviewed throughout training so that assessors/interviewers would be prepared for a variety of responses from respondents. The culmination of training on the parent interview was a final certification role play that was conducted with the assessor/interviewer's supervisor. This certification role play was designed to test all the protocols and techniques that were reviewed during training. After training, interviewers and assessors who would be conducting parent interviews completed a final role play with their field supervisor (see appendix B5: Parent Interview Certification Role Play). These role plays occurred in the first week after training before any interviews were done. areas, such as contacting and selecting the respondent, asking the questions verbatim, probing properly, and following the correct question path. Two points were given for each item performed correctly. Trainees had to score at least 30 out of a possible 38 points to pass. All of the interviewers (61) and the majority of assessors (233) conducted parent interviews. The majority of interviewers/assessors (94.8 percent or 244 staff) were successfully certified on the parent interview. Nine of the 244 staff certified on the parent interview were bilingual staff who were certified in Spanish. Supervisors also conducted some parent interviews. All 77 supervisors passed the certification. The majority of the final role plays were conducted in English (217 interviewers/assessors)."}, {"section_title": "Fall Preassessment School Contact", "text": "Beginning in September 2001, all participating ECLS-K schools (i.e., schools that participated in fall or spring of kindergarten or first grade) were contacted by telephone to prepare for the spring data collection. When children were identified as transferring to another school, the child's new school (and district, if necessary) was recruited. As noted in section 4.3.1, the advance contact served several purposes. It reminded schools about the study and reacquainted them with the study's procedures; it provided supervisors the opportunity to persuade the schools to participate; and it allowed the collection of information necessary for the spring 2002 national data collection. There were four primary tasks to be accomplished during the fall contact. These were to schedule appointments to conduct the child assessments in the spring; to verify parent consent procedures; to identify the children's teachers; and to identify children who had withdrawn from the school and obtain locating information about their new schools. The fall contact activities are described below."}, {"section_title": "Advance Mailings", "text": "In September 2001, an advance package was mailed via Federal Express to all participating ECLS-K schools asking them to prepare for the preassessment contact telephone call. The schools were asked to identify a school staff coordinator to serve as a liaison with the study (in original sampled schools, this person was usually the coordinator from the previous rounds of data collection). A package 4-23 containing study materials was sent to the schools. The package contained the following materials (see appendix C: Fall Preassessment Advance Contact Materials): Appendix C1: Advance Letter (original schools)-a letter printed on ECLS-K letterhead reminding school staff about the study, describing the third grade data collection, and alerting the school coordinator of the advance contact in the fall; Appendix C2: Advance letter (transfer schools)-a letter printed on ECLS-K letterhead introducing school staff to the study, describing the third grade data collection, and alerting the school coordinator of the advance contact in the fall; Appendix C3: School Summary Sheet (original schools)-a two-page document providing a brief review of the study to date and the third grade data collection activities; Appendix C4: School Summary Sheet (transfer schools)-a two-page document providing a brief overview of the study and the third grade data collection activities; Appendix C5: Study Findings Sheet-a summary of findings about children from the previous rounds of data collection; Appendix C6: ECLS-K Study Children Form and Instructions-a listing of all the sampled children and instructions for completing the form with specific information such as the children's continued attendance at the school, their grade, their teachers' names, classroom numbers, receipt of special education services, and receipt of assessment accommodations/exclusions before the preassessment call, and: Appendix C7: Class Organization Form-a form for requesting information on departmentalized instruction in the schools"}, {"section_title": "Fall Preassessment School Coordinator Contact", "text": "The preassessment contacts were made by telephone between September and November 2001. The preassessment school contacts were successful in meeting all four tasks described above. Contacting original sampled schools to set up the spring assessment and identifying children who withdrew from their spring-first grade school and moved into their third grade transfer school, enabled the identification of schools that were ineligible for third grade data collection. Schools were determined to be ineligible for third grade data collection if no ECLS-K sampled children were currently enrolled. Original sampled schools became ineligible because second grade was the highest grade in the school or because the school had closed, that is, was no longer operational. More transfer schools were determined to be ineligible as children transferred out of them into other schools."}, {"section_title": "4-24", "text": "During the preassessment contact, the field supervisor contacted the school coordinator to collect some basic information about the school and some detailed information about each ECLS-K sampled child. The field supervisor used the School Information Form (see appendix D1 for the School Information form) to collect basic information about the school, such as school start and end dates, vacation and holiday schedules, and parking directions. The form was also used to determine if the school was a year-round school, taught third grade, or required new parent consent, and to obtain information on class organization. The supervisor used the Child Work Grid (see appendix D5 for the supervisor version of this form) to collect basic information about the child such as his or her grade, the name and classroom number of the child's primary teacher to link the child to a teacher, and whether the child had an Individualized Education Plan (IEP) or its equivalent. In addition, in original sample schools, the assessment date was scheduled; assessment dates for new transfer schools were scheduled in the spring."}, {"section_title": "Reviewing Parent Consent With the School", "text": "Although parental consent was obtained in the base year (and, in some schools, in the first grade year), field supervisors asked the school coordinator whether the base year or first grade parental consent was acceptable for third grade. If the schools required current consent forms or changed the type of consent that was required (e.g., from implicit to explicit), parent letters and consent forms were either mailed to the school for distribution to parents or sent directly to parents by Westat depending on the schools' preference (see appendix D2: Consent Process Flowchart, which describes this process). Parent cover letters and consent forms were available in English and Spanish (see appendix D3: Parent Cover Letters and Consent Forms). Overall, 16 percent of all schools contacted required an updated parental consent. Of the schools requiring updated parental consent, approximately 50 percent required explicit consent from the parents."}, {"section_title": "Class Organization in the School", "text": "As children move into the later elementary grades, they may have different teachers for different subjects. This practice is usually referred to as departmentalized instruction. For the ECLS-K data collection, knowing how classrooms are organized is important so that the children's correct teacher for a given subject can be identified. In an effort to more fully understand the teaching approaches used in 4-25 schools and to begin the process of crafting an approach for the fifth grade data collection, the school coordinator was asked to complete the Class Organization Form (see appendix D4: Class Organization Form [Supervisor Version]). To minimize disruption to the school during spring data collection, details about how the school organized instruction for language arts, mathematics, science, and social studies were collected by the school coordinator prior to the preassessment call. The field supervisor then collected the class organization information during the preassessment call with the school coordinator. In schools that indicated they had some type of departmentalized instruction for third grade, as opposed to self-contained classrooms, additional information was collected about the subjects that the teachers of the sampled ECLS-K children taught. Table 4-2 presents the percentages of different instructional practices by grade for the 2,422 ECLS-K schools that reported about instructional practices. There appeared to be a tendency to increase the use of departmentalized instruction in the later grades with only 4.4 percent of schools reporting departmentalized instruction for third grade but 11.3 percent by fifth grade; only 39 of the original sample schools identified departmentalized instruction in their third grade classes. "}, {"section_title": "Collecting Information About ECLS-K Sampled Children", "text": "Field supervisors primarily used the Child Work Grid (see appendix D5 for the supervisor were the same as those for the kindergarten and first grade direct cognitive assessment (see section 4.5.2). Field supervisors contacted the teachers of the ECLS-K children as necessary for any of this information. Field supervisors also identified the respondent for the student records abstract for each child (see appendix D7: Student Records Abstract Linking Form). If a child was identified as having transferred out of the school, the field supervisor asked the school coordinator to provide the names, addresses, and telephone numbers of these transfer schools. Of those children who transferred, only a subset were followed to their new school (see section 3.4.2 in chapter 3 for more detail on how mover children were subsampled). If the new school belonged to a district that was new to the study, the district was contacted and recruited before any contact was made with the school (see appendix D8: New District Recruitment Letter). If the district was already cooperating, the district was notified and the new school was contacted and recruited directly (see appendix D9: Cooperating District Notification Letter). Field supervisors also verified with the school that no child who had previously transferred had returned to the school (see appendix D10: Returning Student Form)."}, {"section_title": "Contacting Families of Homeschooled Children", "text": "As part of the fall preassessment contact, children in the ECLS-K sample who were homeschooled in previous rounds were identified. The status of home-schooled children who were identified in rounds 1 through 4 was verified with their parents and updated as necessary. In addition, during the preassesment contact some schools identified homeschooled children. Their status was also verified with their parents during data collection. Parents of these children were contacted by telephone in September through November 2001 to determine if the child was still homeschooled or had enrolled in a school (see appendix D11: Homeschooled Children Form). If the child had enrolled in a school, the new 4-27 school was contacted and recruited into the study. Parents of children who were still schooled at home were notified about the next round of data collection in the spring."}, {"section_title": "Fall Preassessment Contact Results", "text": "The goals for the fall preassessment contact with schools were the following: (1) set appointments for the spring assessment in original sample schools, (2) identify schools that ended at second grade to determine the school to which the sample children transferred, (3) identify children who changed schools since first grade, (4) link children to teachers for the advance school and teacher questionnaire mailings, and (5) contact as many transfer schools as possible within the field period to ascertain whether the child was still there and recruit the school into the study. It was not expected that every transfer school identified within the fall contact could be contacted within the fall field period because of the numbers of children that were expected to move. It was also expected that additional schools would be contacted during the spring round because children were expected to continue to move between fall and spring of the school year. Approximately 30 percent of the ECLS-K sampled children transferred to other schools between the spring of first grade and beginning of third grade (i.e., fall 2001). At the start of the fall 2001 field period, there were 1,410 transfer schools, but 506 were deemed out-of-scope resulting in 904 transfer schools identified prior to the fall contact. An additional 858 transfer schools were identified during the fall contact field period (September-November) for a total of 1,762 transfer schools at the end of the field period. All of the transfer schools identified during the fall were contacted by mail in the spring with an advance package that included the name of the child who transferred into the school. In addition, attempts were made to follow up with the schools via telephone. However, telephone contact could be made with only 52 percent of the schools before the fall field period ended. During the fall preassessment contact, 4,679 children were identified as movers and processed in preparation for the spring data collection. Table 4-3 presents the status of these 4,679 mover cases. End of field period 507 11 NOTE: Movers who had a status of \"Fielded for assessment in spring\" were treated like the nonmover cases during the spring data collection."}, {"section_title": "4-28", "text": "Those with a status of \"Unlocatable\" could not be located during the fall preassessment contact. Attempts to locate them continued into January and February 2001. The cases were fielded in the spring if the location efforts were successful (see section 4.4.4) Cases identified as \"Moved to nonsampled PSU\" or \"Moved to outside of U.S.\" were not fielded for the spring data collection because they had moved out of the designated data collection area. Cases with a status of \"Subsample not followed\" were not fielded in the spring because the sampling plan called for collecting data from only a subsample of movers (see chapter 3, section 3.4.2). These cases were not in the subsample to be followed. The 507 cases with a status of \"End of field period\" were not contacted in the fall because the schools were either new to the school sample frame with limited information available to contact them or in school districts new to the sample that required an additional contact at the district level before the schools could be contacted for spring data collection. These cases were closed out and rolled over for contact in the spring. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02. The fall preassessment contact protocol was completed for 98 percent of the original ECLS-K schools and 76 percent of the transfer schools (both those identified in the fall field period and those identified during the first grade data collection) within the fall field period. Seventeen percent of schools were identified as out of scope, since they did not contain any sampled students (6 percent of original sampled schools and 22 percent of transfer schools). All of the schools that children transferred to as a result of the school ending at second grade, closing, or merging with another school were identified within the field period. Tables 4-4 through 4-6 present the production reports for the fall preassessment contact for original sample and transfer schools.   4-31"}, {"section_title": "4-29", "text": ""}, {"section_title": "4-30", "text": ""}, {"section_title": "Tracing Activities Between Fall-and Spring-Third Grade Data Collection", "text": "In order to ensure that as many of the sampled children as possible were contacted in the  \"Unlocatable\" means that the children and their households could not be found using the available tracing and locating strategies; Children were \"ineligible\" for this round of data collection because they were deceased, had moved out of the data collection area, or were not in the sample of movers to be followed; \"final refusal\" means that the child's family indicated that they did not want to participate; \"partially located\" means that the tracing and locating effort yielded some information about the child, but not enough to definitively locate the child; \"unable to locate due to language barriers\" means that the household language was not English and no staff were available who were bilingual in that language to communicate with the householder. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02. A mailing to the post office requesting change of address information for sampled households was also conducted in November 2001. Westat mailed the requests for 17,472 eligible households. Table 4-8 presents the results of that mailing. "}, {"section_title": "4-32", "text": ""}, {"section_title": "Spring-Third Grade Data Collection", "text": "All children who were assessed during the base year or for whom a parent interview was completed in the base year were eligible to be assessed in the spring-third grade data collection. Eligibility for the study was not dependent on the child's current grade; that is, children were eligible whether they were promoted to third grade, retained in an earlier grade, or promoted to a higher grade (e.g., fourth grade). As in previous rounds of data collection, the field staff were organized into work areas, each with a data collection team consisting of one field supervisor and three or more assessors/interviewers. The data collection teams were responsible for all data collection activities in their work areas; they conducted the direct child assessments and the parent interviews, distributed and collected all school and teacher questionnaires, and completed school facilities checklists. The majority of field staff members in third grade were continuing from previous rounds of data collection; a few new staff were hired in areas where no experienced ECLS-K staff lived. questionnaires and to arrange for space to conduct the assessments. This follow-up call to the schools was essentially to confirm the logistics for the assessments."}, {"section_title": "Spring Preassessment Activities", "text": ""}, {"section_title": "Conducting the Direct Child Assessments", "text": "The direct child assessments were conducted from late March through June 2002, the same time of year as in prior spring data collections. The school coordinator set the assessement date with a supervisor based on the school's schedule. Approximately 91 percent of the assessments were completed in April and May and 9 percent were completed in June. There was no evidence that certain types of children or schools were assessed early or late in the field period. Table 4-9 presents the weekly completion rates for the child assessments. In year-round schools, assessment teams made multiple visits to the school, visiting when each track was in session to assess the sampled children. There were 617 (2.6 percent) sampled children in year-round schools. The direct child assessments were usually conducted in a school classroom or library. Before conducting the assessments, field supervisors and assessors set up the room for the assessments. They followed procedures for meeting children that were agreed upon during the preassessment contact with the school. Each child was signed out of his or her classroom prior to the assessment and signed back into the classroom upon the conclusion of the assessment. When scheduling schools in the fall, an attempt was made to conduct the direct child assessments at about the same point in time from the beginning of school year and the end of the year to increase the chances that exposure to instruction was about the same for all children. The third grade direct child assessments averaged 94 minutes in length.   "}, {"section_title": "4-34", "text": ""}, {"section_title": "Conducting the Parent Interview", "text": "Parent interview procedures mirrored those of the base year and first grade. The parent interview was administered primarily by telephone interview using CAI from March to July 2002. (See appendixes E2 and E3 for the English and Spanish versions of the interview.) Sixteen percent of the parent interviews were completed in March, 54 percent were completed in April and May, and 30 percent were completed in June or later. Table 4-12 presents the weekly completion of parent interviews. The parent interview averaged 62 minutes. As in previous rounds of data collection, the parent interview was conducted in person if the respondent did not have a telephone and in other languages as needed. "}, {"section_title": "4-37", "text": ""}, {"section_title": "4-39", "text": "In spring third grade data, 21 percent of the parent cases were classified as final nonresponse. As in most field studies, the primary reasons for final nonresponse were parents who could not be located and parents who refused to complete the interview; 31.4 percent of the parent cases were unable to be located, 54.1 percent were refusals and 14.5 percent were other nonresponse (e.g. language barrier). Because the ECLS-K is a longitudinal study, nonresponse builds over time. Spring third grade data collection began with 6.4 percent of the parent cases classified as either could not be located (66 percent of initial nonresponse cases) or refusal cases (34 percent of initial nonresponse cases). During data collection additional cases could not be located (6 percent of parent cases) and refused (3 percent of parent cases). Field staff were somewhat successful with completing the initial nonresponse cases; 21.5 percent of initial could not be located were found and interviewed and 30 percent of initial refusal cases were convinced to participate. A special effort to build parent interview response rates was conducted from July 5 to 31,2002, and yielded an additional 7.3 percentage points to the response rate. Almost 8 percent (7.7 percent) of the parent interviews were not completed because of locating problems. Almost 7 percent (6.6 percent) of the parent interviews were not completed because of refusal, either regular (2.9 percent) or hard (3.7 percent). Hard refusals from previous rounds were not fielded for data collection during third grade."}, {"section_title": "Conducting Data Collection on Children Who Withdrew From Their Previous Round", "text": ""}, {"section_title": "School", "text": "While contacting schools, field supervisors asked school coordinators to identify children who had withdrawn from the school since the spring of first grade. School staff were asked whether they knew the name and address of the school to which the child transferred, as well as any new information about the child's household address. For the children who had moved from their spring-first grade school and were not part of the sample to be followed, information was collected only from the school personnel and not parents. For children who had withdrawn from their spring-first grade school and were identified to be followed (i.e., were part of the sample of movers), supervisors also consulted parents and other contacts for information on the children's new school. This information was entered into the FMS and processed at Westat for data collection. As previously mentioned, 4,679 movers were identified during the fall preassessment contact. During spring-third grade, an additional 1,628 movers were identified (3,582 movers were 4-40 identified since kindergarten and flagged to be followed for data collection). The identification of 75 percent of the third grade movers during the fall contact accomplished two important goals: (1) allowed additional time to trace movers; and (2) reduced the burden of tracing a large number of movers during the spring-third grade data collection. Table 4-14 presents the final status of the children who were identified as movers in third grade; a total of 9,889 children were identified as having transferred from the school in which they were enrolled when they were sampled in kindergarten. Of the 9,889 children identified as movers in springthird grade, 5,668 children were selected to be followed and were followed (57.3 percent of total movers). The remaining 4,221 mover children were ineligible for this round of data collection because they moved out of the country, were deceased, or were subsampled out; no child assessments or parent interviews were conducted for these children. "}, {"section_title": "4-41", "text": "Different data collection strategies were followed for children who moved, depending on where they moved and the status of their new school. Data collection was attempted for children who moved and were flagged as \"follow\" in spring-third grade in the following ways: Children moving into cooperating schools were treated as nonmovers. Data collected for children moving into cooperating base year sampled schools included the child assessments in the school, school administrator questionnaire, school fact sheet, regular and/or special education teacher questionnaires, facilities checklist, and student records abstract forms, and parent interview. Data collected for children moving into nonsampled schools in base year cooperating districts included the child assessments in the school, school administrator questionnaires, school fact sheet, regular and/or special education teacher questionnaires, and student records abstract forms, if the school agreed to participate. If school permission was not obtained, the assessments were conducted in the home and no school or teacher data were collected. Parent interviews were attempted for all children. For children moving into transfer schools that refused, schools in sampled districts that refused, or originally sampled schools that were ineligible when sampled because they did not have kindergarten classes, the direct child assessments were conducted in the home. No school or teacher data were collected. Parent interviews were attempted for all children. For children moving into schools in nonsampled districts or dioceses: -If the school was within the primary sampling unit (PSU), data collected included the child assessments in the school, school administrator questionnaire, school fact sheet, regular and/or special education teacher questionnaires, facilities checklist, and student records abstract forms, if school permission was obtained. If school permission was not obtained, the assessments were conducted in the home and no school or teacher data were collected. Parent interviews were attempted for all children. -If the school was outside the PSU, no child, school, or teacher data were collected. The parent interview was still attempted. For children who were not enrolled in school in the spring (including children who were home schooled), data collected included the child assessments in the home if the child was in the sampled PSU. If the child was outside the sampled PSU, no child assessment or school or teacher data were collected. Parent interviews were attempted for all children. Of the children who moved in third grade and were selected to be followed, 15.4 percent moved into a school outside the PSU and 10.7 percent of the movers could not be located. Assessments were completed for 65 percent of the movers who were followed in the spring-third grade data collection 4-42 and parent interviews were completed for 68 percent of these children. Table 4-15 presents the third grade movers by school and district status. "}, {"section_title": "Teacher and School Data Collection", "text": "Data were collected from school administrators, regular classroom teachers, and special education teachers from March through June 2002. The school and teacher questionnaires were mailed to the school coordinators in February 2002. This schedule allowed 2 months of additional time for these respondents to complete and return the instruments to Westat. Using the child-teacher linkage information collected in the fall, a packet of questionnaires was assembled for each regular and special education teacher. The regular teacher packet included a cover letter, a sheet explaining the study and its goals, teacher questionnaire A, teacher questionnaire B, and teacher questionnaire C for each student who had been linked to the teacher in the fall. The special education teacher packet contained a cover letter and summary sheet, special education teacher questionnaire A, and special education teacher questionnaire B for each sampled student linked to the teacher. Packets were bundled together by school and mailed to the school coordinator for distribution. If the school and/or teacher and school administrator were not identified in the fall advance contact, then the supervisor gathered the relevant information during the preassessment call in the spring and mailed the packets at that time. During their visits to the schools, field supervisors also completed a 4-43 facilities checklist for each sampled school. (See appendix E4: Cover Letters to School Coordinators and appendix E5: Hard-Copy School and Teacher Questionnaires.) Field supervisors began prompting for the return of questionnaires when they contacted schools to confirm the assessment schedule. During the field period, field supervisors followed up with school administrators and teachers by telephone and visits to the schools to prompt for the return of the questionnaires and collected completed questionnaires to return to Westat. In May 2002, the field supervisors were instructed to conduct intensive followup for missing school administrator questionnaires for schools with high minority student populations. There were 99 such schools identified. As a result of the followup efforts by the field supervisors, 75 of the 99 schools (76 percent) completed and returned the school administrator questionnaires."}, {"section_title": "Hard-Copy Data Retrieval", "text": "Data retrieval involved collecting missing items for questionnaires that were otherwise "}, {"section_title": "4-44", "text": "As they collected complete questionnaires, field supervisors reviewed the critical items for each questionnaire to ensure that such items were completed before sending the questionnaires to Westat. Field supervisors attempted two types of data retrieval: Scanning for missing critical items in questionnaires that they collected directly from the school and teachers, and attempting to retrieve the missing items; Completing Data Retrieval Forms for questionnaires the schools and teachers returned directly to Westat, and attempting to retrieve missing items."}, {"section_title": "Scan for Missing Critical Items", "text": "Field supervisors were asked to scan questionnaires for missing critical items, that is, items identified as especially important to analysts. The critical items were packaged for the field supervisors in a comprehensive list (see appendix E6: Third Grade Questionnaires-Critical Item List). In the Third Grade Questionnaire-Critical Item List the procedures were listed as follows: For each questionnaire, the critical items were listed along with the rule for attempting data retrieval for each item. The color of the questionnaire cover was included with the name of the questionnaire. As field supervisors collected completed questionnaires, they scanned the questionnaires for missing critical items using the Critical Item List. They only scanned questionnaires for the critical items listed in the Critical Item List. They were instructed not to attempt to edit the questionnaire or change responses. As field supervisors identified missing critical items, they affixed a tape flag to the questionnaire page so they could quickly flip to the item when they reviewed the questions with the respondent. The student records abstract (SRA) was to be completed after the school year ended. If the field supervisor collected the completed SRAs, they were instructed to scan them for completeness and attempt to retrieve any missing data."}, {"section_title": "Retrieving Missing Critical Items from Questionnaires Received in the Home Office", "text": "As described earlier, questionnaires were mailed to schools and teachers in February 2002. School staff were given the option of either returning the questionnaires via an enclosed Federal Express mailer or holding the questionnaires until the field supervisors visited the schools. By the time the critical 4-45 item list was finalized 6 weeks into the field period, Westat had received more than 11,000 questionnaires directly from the school respondents. About 11 percent of the receipted questionnaires required retrieval of missing critical items, with the majority of the data retrieval required for the school administrator questionnaire and teacher questionnaire A. Receipt control staff scanned the questionnaires for critical items upon receipt (see chapter 5 for a more detailed discussion of this process). When a critical item was identified as missing a response, The Data Retrieval Forms were mailed to field supervisors to attempt to retrieve missing critical items. Each form had a label on the cover that identified: The questionnaire type; The supervisor name, region, and work area; The school name, ID, address, and phone number; The school coordinator name (only on the school administrator questionnaire and the school fact sheet); The teacher name and ID number (only on teacher questionnaires A, B, and C and special education teacher questionnaires A and B); The child name and ID number (only on teacher questionnaire C, special education teacher questionnaire B and student records abstracts); and The respondent name and ID number (only on student records abstracts). Field supervisors were instructed to review each Data Retrieval Form to determine which teachers and/or school staff they should contact, and for which critical items they were to attempt data retrieval."}, {"section_title": "4-46", "text": ""}, {"section_title": "In-Field Data Retrieval Attempts", "text": "Field supervisors attempted to retrieve missing critical items and missing questionnaires in the schools in their assignments. They scheduled their retrieval efforts for the day the assessments were scheduled and attempted to find the respondents in person. Otherwise, they attempted these retrieval attempts by telephone. Field supervisors recorded any changes to missing critical items in blue pencil in the questionnaire. If the respondent did not know the answer, they recorded \"DK\" by the item; if the respondent refused to answer, they recorded \"RF\" by the item. Field supervisors recorded the status of the questionnaire as one of the following: Questionnaire complete with no missing critical items: no data retrieval required. Questionnaire is missing one or more critical items: data retrieval required; one or more critical items collected. Questionnaire is missing one or more critical items: data retrieval required; no critical items were collected. Questionnaire refused: unit nonresponse. Table 4-16 presents the final results of the critical item retrieval."}, {"section_title": "School Fact Sheet and Student Records Abstract Followup", "text": "To improve response rates on the school fact sheet and student records abstract, a telephone followup effort was conducted in fall 2002. The school fact sheet and student records abstracts included items that were not time-sensitive that school staff could abstract from records. A package was mailed in early September 2002 to all nonrefusing schools with outstanding school fact sheets or student records abstracts with a request to complete and return these questionnaires. The package included a customized letter and questionnaires labeled with the student name and ID number (see appendix F: Followup Letters and Questionnaires [School Fact Sheets and Student Records Abstracts]). The experience in prior rounds of data collection was that prompting schools to return the questionnaires in the summer was ineffective. The students records abstracts were particularly problematic with respect to retrieval because the school staff could not complete the form until the school year ends. In the first grade, mail prompting for unit nonresponse began in early June. Rarely were school staff physically in the school at that time. Sometimes the principal was present, but there were typically 4-47 "}, {"section_title": "Incentives in the ECLS-K", "text": "In order to gain respondent cooperation and ensure participation throughout the various data collection phases of the ECLS-K study, various incentives were offered. Child Incentives. Children were given a small token at the end of the assessment to thank them for their cooperation in completing the assessment. In the spring-third grade, they were given a pen with the phrase \"I love ECLS-K\" and the toll-free number for the respondent to call. The pen was blue 4-49 and red with white lettering. In addition, each month Westat mailed birthday cards to children whose birthdays fell within that month. Children were sent birthday cards throughout the calendar year, not just during the school year. By mailing these cards, children were not only thanked again for their participation, but parents are also reminded about the study. These periodic reminders are important in a longitudinal study, in which respondents may become apathetic toward the study during later rounds. Not only do these reminders encourage respondent participation, but they help the home office update addresses of families that have moved. Parent Incentives. In the spring of 2000, shortly before the beginning of first grade data collection, a newsletter about the study was published and mailed to parents. The newsletter served to update respondents on the initial findings from the fall of kindergarten year and inform them about the future rounds of data collection. Reading and mathematics skills, social skills, and child care were a few of the topics discussed. Not only did the newsletter update parents on the findings of the study and highlight its importance, but it also was an incentive for future rounds of participation. Respondents were able to see the results of their participation in the study. Parents received an incentive for participating in the parent interview. At the end of the field period, thank-you letters were generated for every respondent who completed a parent interview. The letters, along with an ECLS-K Post-It notepad, were mailed to the respondent. Letters translated into Spanish were mailed to those respondents who completed the interview in Spanish. (See appendix G1:"}, {"section_title": "Parent Thank You Letters [English and Spanish])", "text": "Teacher Incentives. Teachers in original ECLS-K schools were asked to complete individual ratings for the sampled children in their classrooms, and they were reimbursed $7 for each child rating (teacher questionnaire C and special education teacher questionnaire B) they completed. Teachers in transfer schools were also asked to complete individual ratings for the sampled children in their classrooms, and they were reimbursed $20 for up to 3 child ratings and $7 for each additional completed child rating. School Incentives. Original schools were also paid a monetary incentive for participating in the ECLS-K. Because school staff are often very busy and may not be aware of the benefits of cooperating, the cooperating original sample schools were remunerated $200 for participating. School staff who completed the student records abstract received $7 for each questionnaire completed."}, {"section_title": "4-50", "text": "All checks to schools, teachers, and school staff completing the SRAs were mailed weekly during the field period and were sent with thank-you letters. The checks to teachers and to staff completing the SRA were made out the individual who completed the forms. (See appendix G2: Thank You Letters for the School, Teacher, and Student Records Abstract Respondents.)"}, {"section_title": "Data Collection Quality Control", "text": "The ECLS-K data are used by researchers to study children's school experience and its relation to student outcomes, and by educators and policymakers to inform policy decisions. It is important that the information used by these groups is based on sound research practice and that considerable attention be paid to identifying potential sources of error, quantifying this error, and designing techniques to either reduce the error or minimize its impact on survey estimates. The work carried out in support of the ECLS-K includes a variety of activities that are directed toward ensuring that the data are of high quality."}, {"section_title": "Maintaining Reliability on the Child Assessment", "text": "To ensure that assessors maintained the standard that they achieved at training, assessors were observed by their supervisor in the field at 2 different points in time. The first observation was to be conducted within the first 2 weeks of the field period and the second observation 2 to 3 weeks after the first. The supervisor completed the Assessment Observation Form (see appendix H1: Assessment Observation Form), which rated the assessor on key areas of the assessment protocol. In the Assessment Observation Form, the supervisor simultaneously coded with the assessor those open-ended assessment items that required judgment by the assessor to determine whether the child's answer was correct. At the end of the assessment period, after the child was escorted from the room, the supervisor and the assessor reviewed the assessor's overall performance. The two also compared the way that they each handled the open-ended questions. If there were large discrepancies, they reviewed the QxQs for these items carefully. 4-51"}, {"section_title": "Assessment Observation Form", "text": "The Assessment Observation Form had the names of the assessor and the supervisor, the case ID observed, the observation number, as well as the date the observation was conducted. The form had two sections: section 1 (shown in exhibit 4-15) was use by supervisors to rate the assessor on key overall skill areas, such as building rapport, using neutral praise, responding to behaviors presented by the child, pacing appropriately, and coaching. In section 1 the supervisor checked \"No\" for each skill area that the assessor did not demonstrate appropriately. Exhibit 4-15. Section 1 of the Assessment Observation Form: School year 2001-02 Evaluator: As the assessment is administered, record whether or not the assessor successfully performed the following behaviors. Check \"No\" if the assessor makes 3 or 4 errors and needs to make improvements. Section 2 listed specific questions from each routing and subdomain (e.g., mathematics) form. The instructions for completing section 2 are shown in exhibit 4-16. For each of the listed questions observed, the supervisor recorded both the child's response and if the assessor did not demonstrate the specified required administration skills for that question. The required administration skills included reading questions verbatim, using appropriate probes, and using appropriate hand motions. For each question in which the supervisor observed that the assessor did not demonstrate the required administration skill(s), he or she checked a box, indicating which skill was not performed. Code the child's response as the item is administered."}, {"section_title": "4-52", "text": "If the item requires probing, check the box if the assessor does not use the appropriate probe. Check the box in the \"Verbatim\" column if the assessor does not read the item exactly as worded on the screen. Check the box in the \"Gesturing\" column if the assessor does not use appropriate hand motions.  How many of each different shape of block did Arnold use to make his star shape? IF STUDENT RESPONDS WITH \"5\", or \"1 AND 4\" PROMPT: \"5/ 1 and 4 of which type/shape of block?\" IF STUDENT RESPONDS ONLY WITH \"SQUARE(S) AND TRIANGLE(S),\" PROMPT: \"How many squares and how many triangles?\" CORRECT ( At the end of the child assessment, after the child was escorted from the room, the supervisor and the assessor reviewed the assessor's overall performance. After discussing the ratings on section 1, the assessor accessed the quality control screen. The assessor and supervisor reviewed their codes for each open-ended question asked in section 2 of the Assessment Observation Form. The supervisor then scored the assessment observation using the scoring form shown in exhibit 4-18. In the first part of the form (Form A), the supervisor counted the number of check marks recorded in section 1: Rapport Building and recorded that number in the appropriate row. "}, {"section_title": "4-56", "text": "Finally, the supervisor determined the proportion correct by using the Proportion Correct Chart displayed in exhibit 4-21. Continuing the example, the supervisor found the column on the Proportion Correct Chart displaying the total possible points for the assessor based on the portions of the assessment observed (74) and the row on the Proportion Correct Chart displaying the total check marks the supervisor recorded for the trainee 3, and recorded the proportion from the corresponding box on the Proportion Correct Chart in the box under step 6 (exhibit 4-22) on the scoring form (.98). Once the supervisor completed the scoring, the assessor was rated as Passed, Remedial Action, or Failed. The field supervisors recorded their observations on the form and then reviewed the form with the assessor. The most frequent problems observed were not reading the items verbatim and inappropriate gesturing. Feedback was provided to the assessors on the strengths and weaknesses of their performance and, when necessary, remedial training was provided in areas of weakness.  "}, {"section_title": "Assessor Interrater Reliability", "text": "As part of the child assessment observation described above, field supervisors completed an Assessment Certification Form for each observation they conducted. An important element of this form was the \"validation items.\" With the exception of the reading routing test, all of the assessments included at least one item that both the observer and the assessor scored. These items had open-ended responses that called for interpretation on the part of the assessor to determine whether a child's response was correct. By comparing the extent to which assessors and observers agreed on scoring these validation items, a measure of interrater reliability was obtained. Interrater reliability provided a measure of the accuracy of the assessor's scoring compared with the standard, the observer's. Supervisors had been certified at training by experienced Westat staff. New supervisors were first observed by field managers before observing any assessments. 97.9 \u2020 Not applicable. No reading routing items were selected for validation. NOTE: Percent agreement for specific domain levels (e.g., Reading Red) was calculated as follows: number of validation items observed in which observer agreed with the assessor divided by the total number of validation items observed. Percent agreement for domains (e.g., Science) is an average of the percent agreements for each level within that domain. The number of observations at the domain level (Reading, Mathematics, Science) is the sum of the observations at each specific domain level (Red, Yellow, Blue) to reflect the typical number of children assessed within a given domain. Due to missing data, the sum of observations for the specific domain levels do not sum to the number of observations at the routing level. The missing data occurred because not every child was able to answer all the questions in the assessment. All children, however, were able to complete the routing forms for the domains. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02."}, {"section_title": "Validation of Parent Interviews", "text": "Approximately 10 percent of the completed parent interviews were called back by a field supervisors (i.e., validated). The first parent interview completed by an assessor was always validated. Over the course of the field period, a running count of an assessor's completed parent interviews was maintained, and each tenth completed parent interview was selected for validation, thus ensuring that 10 percent of each assessor's cases were selected for validation. The parent validation was approximately 5 meaning responses to the original interview and the validation interview were identical; (2) \"Minor changes\" meaning there was a minor discrepancy (e.g., the ZIP code was different) between the responses to the original interview and the validation interview; and (3) \"Major changes\" meaning there was a discrepancy between the response to the original interview and the response in the validation interview (e.g., the school breakfast program was reported differently between the two contacts). One interviewer's parent validation determined that the interview was falsified. The date when the problem occurred was identified and all cases the interviewer completed after that date was validated. Random interviews before the date when the problem occurred for that interviewer were also selected for validation. In total, 20 cases were validated. The five cases that were found to be falsified were re-set to pending, sent out to the field, and completed by another interviewer. The interviewer who falsified data was released immediately from the project. Table 4-19 presents the results of the parent validations. 4-61"}, {"section_title": "Validations of School Visits", "text": "To ensure that assessments proceeded smoothly, a validation call was completed with the school principal in at least two of each supervisor's assigned schools in the spring-third grade data collection. Field managers conducted the school validations. The first school that each team completed was called to ascertain how well the preassessment and assessment activities went. If the feedback from the school was positive, the fifth school that each team completed was called. If any problems were indicated in the first validation call, immediate action was taken with the field supervisor. The validation feedback was discussed with the supervisor and remedial action was taken, including in-person observation of the supervisor's next school, if necessary. Field managers used a standardized telephone script, the School Validation Script (see appendix H3: School Validation Form), to call the school principals. The script covered the following topics: An overall rating of how the assessments went; Feedback the principal had received about the study from children and teachers; Suggestions for improving procedures and making it easier for a school to participate; and General comments and suggestions. By the end of the field period in June, field managers had called 155 original sample schools (100 percent of the schools to be validated) to ascertain whether the conduct of the child assessments in the school was satisfactory. The most common feedback from school administrators was on the burden of the hard-copy questionnaires. About 15 percent of school administrators reported that the school and teacher questionnaires were a \"real burden\" or \"too long\" and that there was too much redundancy between the school and teacher questionnaires. School administrators recommended shortening the questionnaires considerably to make it less stressful for the staff and make it easier for teachers to complete questionnaires. Some school administrators suggested that the study provide the data that was collected from them in previous years so that they could update the items. Although teachers were not included in the validations, some of them sent unsolicited e-mails, letters, and telephone calls indicating that they were disappointed with the remuneration, thinking it was insufficient compensation for the 4-62 amount of work they were asked to do. Almost three quarters (72 percent) of the teachers in original sample schools were linked to three or fewer children; more than half (58 percent) were linked to only one or two children. Given that the remuneration was $7 per child, some teachers perceived the incentive as an insult. Table 4-20 presents the results of the school validation calls. 5-1"}, {"section_title": "DATA PREPARATION AND EDITING", "text": "As described in chapter 4, two types of data collection instruments were used for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) data collection in the springthird grade: computer-assisted and self-administered paper forms (hard copy). The data preparation approach differed with the mode of data collection. The direct child assessments and parent interview were conducted using computer-assisted interviewing (CAI) techniques. Editing specifications were built into the computer programs used by assessors to collect these data. The teacher, school administrator, and student records abstract forms were self-administered. When the field supervisors and respondents returned these forms, coders recorded the receipt of these forms into a project-specific forms tracking system. Prior to mailing the questionnaires and forms to Westat, the field supervisors reviewed them to ensure that critical items had been completed (see appendix E6 for a list of the critical items). When forms and questionnaires were received at Westat, coders reviewed them to ensure data readability for transfer into an electronic format and for the completion of critical items. The visual review included changing (upcoding) any \"Other, specify\" responses that actually fit within the available response categories of the question. For example, if the parent said \"skiing\" in response to the question on the types of exercise or physical activity the child participated in, this answer was upcoded to the existing category, \"individual sports.\" (See appendix I: \"Other, specify\" Items and Coding Instructions\" for more information on the coding of \"other\" responses.) There were some items for which upcoding was conducted after the data were keyed due to the large volume of \"Other\" responses. Once they finished this review, the coders sent the instruments to data entry to be manually transferred to an electronic format and reviewed for range and logic consistency. The following sections describe the data preparation activities for both modes of data collection in more detail."}, {"section_title": "Coding and Editing Specifications for Computer-Assisted Interviews (CAI)", "text": "The very nature of designing a computer-assisted interview requires decisions about edit specifications to be made at the development stage. Both acceptable ranges and logic consistency checks were preprogrammed into the electronic questionnaires (see appendix R: Range Specifications for Each Questionnaire and appendix L: Parent Interview Household Roster Data Edits for ranges and logic consistency checks). The next few sections describe the coding and editing of the data that were conducted during and after CAI parent interview. 5-2"}, {"section_title": "Range Specifications", "text": "Within the CAI parent interview instruments, respondent answers were subjected to both \"hard\" and \"soft\" range edits during the interviewing process. A \"soft range\" is one that represents the \"Hard ranges\" have a finite set of parameters for the values that can be entered into the computer, for example, \"0-5 times\" for the number of times the child, in the previous 5 days, ate a breakfast that was not school provided. Out-of-range values for closed-ended questions were not accepted. If the respondent insisted that a response outside the hard range was correct, the interviewer could enter the response in a comments data file. Data preparation and project staff reviewed these comments. Out-of-range values were accepted and entered into the data file if the comments supported the response. The child assessments did not employ hard and soft ranges. Children's answers were recorded verbatim.\nHard-copy range specifications set the parameters for high and low acceptable values for a question. Where values were printed on the forms, these were used as the range parameters. For openended questions, such as, \"Counting this school year, how many years have you taught in your current school including part-time teaching?,\" high and low ranges were established as acceptable values (see appendix R for range specifications for each questionnaire). Data frequencies were run on the range of values to identify any errors. Values outside the range were identified as errors and were printed for a data editor to review. Cases with range errors were identified, and the original response was updated. In some cases, range violations were retained in the data because the value was checked and found to be the value 5-17 reported by the teacher or school. These were marked as \"KeepAsIs\" cases. Data frequencies were then rerun and reviewed. This iterative process was repeated until no further range errors were found."}, {"section_title": "Logic Consistency Checks (Logical Edits)", "text": "Logic consistency checks, or logical edits, examined the relationship between responses to ensure that they did not conflict with one another or that the response to one item did not make the response to another item unlikely. For example, in the household roster, one could not be recorded as a mother and male. When a logical error such as this occurred during an interviewing session, the interviewer was presented with a message requesting verification of the last response and a resolution of the discrepancy. In some instances, if the verified response still resulted in a logical error, the interviewer recorded the problem either in a comment field or on a problem report. Consistency checks were not applicable to the child assessments. 5-3"}, {"section_title": "Coding", "text": "Additional coding was required for some of the items collected in the CAI instruments by data preparation and project staff after an interview was completed. These items included \"Other, specify\" text responses, occupation, and race/ethnicity. Interviewers entered verbatim responses to these items. Data preparation staff were trained to code these data using coding manuals designed by Westat and the National Center for Education Statistics (NCES) to support the coding process. This section describes the coding activities for the CAI instruments.\nThe hard-copy questionnaires required a quick visual review of particular questions in each questionnaire, coding of race/ethnicity for teachers, and review of \"Other, specify\" text responses. The quick visual review was to ensure that the questionnaire values accurately reflected existing categories, were complete and consistent across variables, and that the numbers were converted to the appropriate unit of measurement prior to converting data to an electronic format. Once the hard-copy questionnaires had been visually reviewed, they were coded. The coding staff was trained on the procedures and had manuals to support the coding process (see appendix O: Coding Instructions and Items To Be Coded). Senior coders verified coding. The verification rate was set at 100 percent for each coder until accuracy of less than 1 percent error rate was established. After that point, work was reviewed at a rate of 10 percent. The \"Other, specify\" text responses were reviewed by the data editing staff and, where appropriate, upcoded into one of the existing response categories. The specify responses that remained after upcoding were reviewed to evaluate whether the addition of any new response categories would be appropriate. There was no need for the addition of new response categories in third grade (see appendix P: Codes From Previous Rounds of Data Collection)."}, {"section_title": "Review of \"Other, Specify\" Items", "text": "There were sixteen \"Other, specify\" open-ended responses in the parent interview (see appendix I for the complete set of \"Other, specify\" items and coding instructions). All of these items were reviewed to determine if they should be coded into one of the existing response categories. During data collection, when a respondent selected an \"other\" response in the parent interview, the interviewer entered the text into a \"specify\" overlay that appeared on the screen. The data preparation staff reviewed these text \"specify\" responses and, where appropriate, coded them into one of the existing response categories. If a response did not fit into one of the existing categories, it remained in \"other.\" If there were numerous responses that were essentially the same, a new category was added. The addition of new categories was not needed during the third grade data collection, but it did occur in earlier rounds (see appendix J: Added Response Codes From Previous Rounds). The parent \"Other, specify\" coding system was revised from previous rounds of data collection and in production testing in April 2002. The revisions consisted of adding new \"Other, specify\" items that had not been part of the previous rounds. Parent \"Other, specify\" coding was completed in batches of cases during June and July 2002; the first batch of 3,333 text strings was extracted in June, 2002, and the last batch of 2,320 text strings was extracted in July. A total of 5,653 \"Other, specify\" text strings were processed through the parent \"Other, specify\" coding system. All possible upcodes were applied to the 4,204 cases that had at least one \"Other, specify\" text string. As noted above, whenever appropriate, responses were upcoded to existing categories. There were no \"Other, specify\" items in the child assessments. Table 5-1 presents the number of text strings for each \"Other, specify\" item. "}, {"section_title": "5-4", "text": ""}, {"section_title": "Parent Occupation Coding", "text": "As in the base year and first grade data collections, occupations were coded using the numbers of cases and similar categories had similar participation rates, suggesting that the separate codes could be collapsed without significant loss of information. The NHES industry and occupation code categories use a two-digit code, the highest level of aggregation, to have sufficient numbers of cases to support analysis without collapsing categories. There are 13 industry codes and 22 occupation codes in the NHES coding scheme. If an industry or occupation could not be coded using this manual, the Index of Industries andOccupations-1980 (U.S. Department of Commerce, 1982) and Standard Occupational Classification Manual-1980(U.S. Department of Commerce, 1980 were used. Both of these manuals use an expanded coding system and at the same time are directly related to the much more condensed NHES coding scheme. These manuals were used for reference in cases where the NHES manual did not adequately cover a particular situation. (See appendix K for an expanded description of the industry and occupation codes.) Occupation coding began with an autocoding procedure using a computer string match program developed for the NHES. The program searched the responses for strings of text for each record/case and assigned an appropriate code. A little over half the cases were autocoded (50.4 percent). Cases that could not be coded using the autocoding system were coded manually using a customized coding utility program designed for coding occupations. The customized coding utility program brought up each case for coders to assign the most appropriate codes. In addition to the text strings, other information, such as main duties, highest level of education, and name of the employer, was available for the coders. The coders used this information to ensure that the occupation code assigned to each case was appropriate. After the cases were coded (either manually or via autocoding), they were reviewed and verified. One hundred percent of the cases were verified. Verification of coding is an important tool for ensuring quality control and extending coder training. As a verification step, a second coder independently assigned codes (i.e., double-blind coding) to industry and occupation cases that had been initially coded either by the autocoding system or manually by a coder. A coding supervisor adjudicated disagreements between the initial code and the verification code. In the early stages, 100 percent of each coder's work was reviewed. Once the coder's error rate had dropped to 1 percent or less, 10 percent of the coder's work was reviewed. Almost 20 percent (19.9 percent) of the cases that were autocoded required adjudication because the verifier disagreed with the autocoding. About the same percent (21.2 percent) of the manually coded cases required adjudication because the manual coder and the verifier disagreed."}, {"section_title": "5-6", "text": "Table 5-2 summarizes the results of the coding and verification process for occupation coding. In the table, manually coded indicates that occupation was initially coded by a coder as opposed to using the autocoding system. Discrepancies are the count of disagreements between the autocoder and the verifier or between the manual coder and the verifier: the discrepant cases required adjudication. The percentage of times in which the coding supervisor disagreed with the coder's (or the autocoding system's) initial coding is referred to as the coder error rate. The percentage of times in which the coding supervisor disagreed with the verifier's coding is referred to as the verifier error rate. The denominator used in calculating these error rates is the number of cases verified. The error rate for manually coded cases was essentially the same for coders and verifiers (11.7 percent and 12.0 percent). The autocoded cases had a slightly higher coder error rate, 13.7 percent, compared to the manually coded rate, 11.7 percent. 5-7"}, {"section_title": "Editing the Household Roster in the Parent Interview", "text": "The parent data edit system was modified from spring-first grade in April and May 2002 to reflect changes to the third grade parent interview. The parent interview data were edited in batches as the interviews were completed (see table 5-3). This was done to make the process more efficient. The first batch consisted of all cases received from the beginning of the round through June 13, 2002. The second batch consisted of cases completed between June 13 and July 9. The last batch consisted of all the remaining cases through the end of the round. The first step in the editing process was to extract the household roster data and run the data edits. The second step was to apply the programmatic updates to the cases failing the edits to correct any errors programmatically. The third step was for an expert reviewer to manually review the cases, conduct as-needed discussions with NCES for resolution, and resolve and correct data errors. Four cases required a discussion with NCES. Only one case was lost from the data file after the expert review and discussion with NCES because the respondent was a nonresident father and thus was not eligible to participate. Several tests were run on the household roster to look for missing or inaccurate information. There were essentially three general types of roster tests performed to determine which cases required editing (see appendix L: Parent Interview Household Roster Data Edits). First, the relationship of an individual to the focal child was compared to the individual's listed age and sex. Problems found were corrected on the basis of data from prior data collections wherever possible. Second, households with more than one mother or more than one father were scrutinized for errors. While it was possible to have more than one mother in a household-for example, a household could contain one biological and one 5-8 foster mother of the focal child-such cases warranted closer inspection. Corrections were made whenever clear errors and a clear resolution existed. Last, the relationship of an individual to both the focal child and the respondent was examined, as there were cases in which the relationship of an individual to the focal child conflicted with his status as the spouse/partner of the reference person. For example, in a household containing a child's grandparents but not his or her parents, the grandmother may be designated the \"mother\" figure, and the grandfather thus becomes the \"father\" (for the purposes of some questions in the interview) by virtue of his marriage to the grandmother. These cases were examined but left unchanged. Both the original-and correct (grandfather)-relationship data and the new \"parent-figure\" designation (father) that had been constructed were kept. Updates to the household roster were required when one or more of the edit checks described above failed. The data for the case would be inspected, resolved, and, if necessary, corrected. There were 644 parent interview cases requiring edits in third grade (identified by P5EDIT=1). There were 164 cases in which the interviewer noted in FSQ that they had entered a person incorrectly in the household roster. These cases can be identified by the flag \"P5ERRFLG\" on the final data files. These \"error\" cases may or may not have also been edit cases, depending on whether or not the error triggered one of the edit checks."}, {"section_title": "5.2", "text": "Coding and Editing Specifications for Hard-Copy Questionnaires"}, {"section_title": "Receipt Control", "text": "In order to monitor the more than 50,000 documents that were to be received in the third grade year, the project-specific receipt and document control system developed in the base year was used, with some modifications. The electronic receipt and document control system was initially loaded or filled with identifying information, such as identification numbers for schools, teachers, and children; the identification numbers linking teachers and children; and the questionnaires that were expected from each school and teacher for each cooperating school in the sample. As data were collected in the field, field supervisors completed transmittal forms for each school to indicate which questionnaires were being mailed to the home office. Teachers did not complete transmittal forms. They mailed their questionnaires to Westat in the Federal Express mailers that were provided to them. Once data collection started, receipt control clerks reviewed the questionnaires sent in from the field for accuracy and completeness. The identification number on each form was matched against the 5-9 identification numbers in the tracking system to verify that the appropriate number of forms for each school was returned. Questionnaires that matched identification numbers in the tracking system were receipted in the system and assigned a batch number. All receipted questionnaires matched identification numbers in the tracking system. Each questionnaire type had a different batch number so that questionnaires of the same type would be processed together. Processing data in batches was more efficient because it allowed staff to process in volume one type of questionnaire rather than switching from one type to another. Questionnaires with the same batch numbers were then compiled into batches of 25 instruments. The batch sheets with all the questionnaire identification numbers were printed from the system; the questionnaires in the batches were verified against the batch sheets. Verified batches were then sent for data entry. Once the batches had completed data entry, that is, all the questionnaires in a given batch had been keyed, the batches, accompanied by the electronic data, were returned to the data preparation department. The electronic data were loaded into the editing system and edited. At each point in the process, a flag was set in the receipt and document control system, which indicated the status of the instrument and the batch to which it was assigned. These statuses were the following: Batches logged from keypunch; Cases loaded to BES (editing system); Cases cleaned (in edit system); and Cases pending (in edit system)."}, {"section_title": "Scan Edit Procedures", "text": "Prior to receipting returned questionnaires, trained clerks scanned each instrument for missing critical items (see appendix M1: Receipt and Scan Edit Procedures Manual). Critical items were identified for each hard-copy questionnaire, except for the facilities checklist. Questionnaires with incomplete or missing data for critical items were not considered complete and were processed for return to the field. Clerks completed scan edit sheets listing the missing critical items. The questionnaires were 5-10 receipted in the system as \"needs data retrieval\" and were forwarded to the data preparation department for data entry, coding, and editing. Using the scan edit sheets, clerks identified the missing or incomplete items on data retrieval forms that were then sent to the appropriate field supervisor (see appendix M2: Scan Edit Rules for copies of the forms used to record missing or incomplete critical items). The supervisor was instructed to contact the school to try to obtain the missing information. See section 4.5.5.1 for further discussion of the data retrieval data collection procedures. Questionnaires that were scanned and needed no data retrieval were logged into the receipt and document control system as \"complete, no data retrieval.\" Once forms were logged in, the data were first keypunched into electronic format and than coded and edited. Questionnaires that contained no data due to refusal by the respondents were logged into the receipt and document control system as \"refusal.\" Table 5-4 presents data on the number of questionnaires receipted by week. The following sections describe the data entry, coding, and editing processes for hard-copy questionnaires. 5-11"}, {"section_title": "Data Entry", "text": "Data entry consisted of two steps: (1) entering the data and (2) verifying that the data had been entered accurately. Westat data entry staff keyed the forms in each batch. A set of keying rules was established for each questionnaire that the data entry staff followed (see appendix N: Keying Instructions For Questionnaires). To verify the accuracy of the data entry, more senior data entry operators then rekeyed all the data. The results of the two data entry sessions were compared and differences identified. When differences were found, the hard-copy form was pulled and examined to determine what corrections, if any, had to be made to the keyed data. These corrections were rekeyed, resulting in an accuracy rate exceeding 99 percent. The verified batches were then transmitted electronically to Westat's computer system for data editing."}, {"section_title": "5-12", "text": ""}, {"section_title": "Data Editing Management Process", "text": "The management of the data editing process involved the creation of several data files, including the Collection Database, Holding Database, Editing Database, and Delivery Database. Exhibit 5-1 provides a diagram of the process described below."}, {"section_title": "Collection Database", "text": "This database contained the keyed records for hard-copy questionnaires. One Collection Database was created for each instrument and as additional data were keyed, the cases were added to the database. The Collection Databases were Blaise databases. The ASCII file resulting from the key entry process was converted to Blaise data in the Collection Database so that they could be merged with the parent interview data and so that they could undergo additional data review (see section 5.2.6) Records in the Collection Databases were assigned status codes reflecting their current status. All new records were assigned a status of CollectionNew. When cases were copied to the Holding Database, the status was updated to CollectionCopied. The data in the Collection Database were retained in their original form, that is, they were not modified based upon later steps."}, {"section_title": "Holding Database", "text": "Data were copied from the Collection Database to the Holding Database for the editing process. The Holding Database for each instrument was also a Blaise database. The copied cases were assigned a status code of New. Cases that had already been involved in a prior editing cycle and had been returned to the Holding Database were assigned a status of CheckEdit or KeepAsIs."}, {"section_title": "5-13", "text": "Exhibit 5-1. ECLS-K hard-copy data editing, third grade data collection: School year 2001-02 "}, {"section_title": "5-14", "text": "As the data were copied from the Collection Database to the Holding Database, a number of processes were run. Code-all-that-apply (COTA) recoding and \"yes/no\" recoding were applied. COTA recoding involved changing the multiple-response values of 0/1, 0/2, 0/3, etc., to a series of yes/no (1/2) responses. Yes/no recoding provided a means to resolve questions left unanswered in a series of yes/no items. If all marked answers were \"Yes,\" then the unanswered items were converted to \"No.\" However, if any item was \"No,\" \"Don't know,\" or \"Refused,\" all unanswered items were converted to -9 (Not ascertained). All blanks were converted to -9 (Not ascertained) and don't know and refused responses were converted to -7 and -8 as appropriate. It was at this stage that skip patterns were enforced using the Blaise CheckRules function and legitimate skips were assigned the standard code of -1 (see appendix Q: General Rules for Data Recoding Specifications During Hard-Copy Editing). Edit programs (range and logical checks) were run against all cases contained in the Holding Database. As the editing process continued, the Holding Database contained both new cases copied from the Collection Database and edited cases returned from the Editing Database (discussed later). Each case was assigned a status code that reflected its current status. For cases that were new to the Holding Database, the CheckRules function assigned one of two codes. The status CleanNew was assigned to new cases that contained no edit (range or logical) errors. The status DirtyNew was assigned to new cases that failed one or more edit checks. Those cases that had undergone edit updating were also subjected to edit checks to identify any errors that remained or were inadvertently introduced during edit updating. The CheckRules function assigned the status CleanEdit to cases with no remaining errors. The status DirtyEdit was assigned to cases returned from edit updating that had remaining or new errors. Those cases that were assigned a status of KeepAsIs in a previous editing round were considered clean. Cases that were found to have edit errors (DirtyNew and DirtyEdit) were copied to the Editing Database for review and updating. At that time, their status in the Holding Database was set to InEdit. A face sheet was generated for each case with editing errors, giving the batch number, case ID, and edit rules that had been violated."}, {"section_title": "Editing Database", "text": "Cases in the Holding Database that failed edit checks were copied to the Editing Database for the correction of errors. As cases were copied to the Editing Database, they were assigned a status of WaitingForEdit in the Editing Database. Editing staff worked from face sheets produced during the edit 5-15 checks conducted on the Holding Database to retrieve and correct case records. Using the batch number and case ID number, editors retrieved and reviewed hard-copy instruments as necessary to resolve editing errors. Once the editor had reviewed and updated each case as necessary, he or she assigned one of two outcome codes. The status code of WasEdited was assigned when all edit errors had been corrected. A status of KeepAsIs was assigned when the editor's review indicated that data that violated an edit check should be retained, for example, when the hard-copy instrument indicated that an out-of-range value was correct. Cases with the statuses of WasEdited and KeepAsIs were moved back to the Holding Database. Cases that had a status of WasEdited in the Editing Database were assigned the status CheckEdit in the Holding Database. The edit rules were applied to these cases to ensure that they were clean. As noted earlier, cases assigned a status of KeepAsIs in the editing process were considered clean."}, {"section_title": "Delivery Database", "text": "The main purpose of the Delivery Database was to store the instrument data at the school, teacher, or child level in a \"rectangular\" format consistent with downstream activities in preparation for data delivery. Cases for which editing and coding activities were completed were copied from the Holding Database to the Delivery Database. These were cases with status codes of CleanNew, CleanEdit, or KeepAsIs. When the data were copied to the Delivery Database, the \"Other, specify\" upcodes and parent interview occupation codes were applied. See table 5-5 for a summary of the status codes assigned for data management databases."}, {"section_title": "Data Editing", "text": "The data editing process consisted of running range edits for soft and hard ranges, running consistency edits in Blaise, and then manually reviewing frequencies of the results. "}, {"section_title": "5-16", "text": ""}, {"section_title": "Consistency Checks (Logical Edits)", "text": "By programming logical edits between variables, consistency between variables not involved in a skip pattern was confirmed. For example, in the school administrator questionnaire, the number of children eligible for free breakfast could not exceed the total number of children enrolled in the school. These logical edits were run on the whole database after all data entry and range edits were complete (see appendix S: Data Edit Specifications for Each Questionnaire). The logical edits were run separately for each form. All batches of data were combined into one large data file, and data frequencies were produced. The frequencies were reviewed to ensure the data remained logically consistent within the form. When an inconsistency was found, the case was identified and the inconsistency was printed for an editor to review. The original value was corrected (or checked and \"kept as is\" if the data item was confirmed, and the case was again run through the consistency edits. Once the case passed the consistency edits, it was returned to the main data set. The frequencies were then rerun and reviewed. This was an iterative process; it was repeated until no further inconsistencies were found. Table 5-6 shows hard-copy questionnaire data preparation production."}, {"section_title": "Applying Data Retrieval in the Editing Process", "text": "As described in section 4.5.5.3, if critical items were missing from the hard-copy questionnaires, field supervisors were asked to contact the schools and collect this information using a data retrieval form. As completed critical item forms were received and receipted in the document control system, the data retrieval data were reviewed to see how it could be applied to the data collected in the original instruments. In general, the purpose of collecting data retrieval was to improve or complete the data collected in the original instrument. However, that was not always the case. If, for example, data collected during data retrieval contradicted data already received, we would not use it. Appendix T: Decision Rules for Data Retrieval presents the decision rules that were used to apply the data retrieval data to each instrument. "}, {"section_title": "Frequency and Cross-Tabulation Review", "text": "As a final review, frequencies and cross-tabulations were run to determine consistency and accuracy within the various forms. If discrepancies could not be explained, no changes were made to the data. For example, in teacher questionnaire A, an item asking about languages other than English spoken in the classroom included a response option of \"No language other than English.\" If a respondent circled that response, but also answered (in subsequent items) that other languages besides English were spoken in the classroom, then the response was left as recorded by the respondent because the discrepancy could not be resolved."}, {"section_title": "RESPONSE RATES", "text": "This chapter describes the computation of unit completion rates for the spring-third grade data collection of the ECLS-K, and unit overall response rates for the base year respondents. Weighted and unweighted unit completion rates are presented for different groups of children. Item response rates for selected items from the ECLS-K third grade instruments are also presented."}, {"section_title": "Definition of Response and Completion Rates", "text": "Response rates and completion rates are two ways to describe the outcomes of data collection activities. A response rate is the ratio of the number of units with completed interviews (for example, the units could be children, parents, schools or teachers) to the number of units sampled and eligible for the interview. The response rate indicates the percentage of possible interviews completed taking all survey stages into account. On the other hand, the completion rate measures the percentage of interviews completed for a specific stage of the survey. For example, in the base year of the ECLS-K children were identified for assessment in a two-stage process. The first stage involved the recruitment of schools to participate in the study. Preassessment visits were made to schools that agreed to participate. During the preassessment visit, field supervisors met with the participating school's school coordinator to enumerate and sample the kindergartners. Assessments were then conducted for the sampled children whose parents consented. If the school refused to participate in the study, no children were sampled for assessments. Under this design, the completion rate for the child assessment is the percentage of sampled children who completed the assessment. The response rate is the product of the school participation or cooperation rate and the child assessment completion rate. Response and completion rates can be either unweighted or weighted. The unweighted rate, computed using the raw number of cases, provides a useful description of the success of the operational aspects of the survey. The weighted rate, computed by summing the weights (usually the reciprocals of the probability of selecting the units) for both the numerator and denominator, gives a better description of the success of the survey with respect to the population sampled since the weights allow for inference of the sample data (including response status) to the population. Both rates are usually not very different unless the probabilities of selection and the response rates in the categories with different selection probabilities vary considerably."}, {"section_title": "6-2", "text": "For example, the completion rate of the ECLS-K child assessment (CA) is computed as where W i is the weight (inverse of the probability of selection of the child) for child i, and ER CA denotes eligible child assessment respondent and ENR CA eligible child assessment nonrespondent. To compute the unweighted rates, W i is set to 1 for each child. The response rate of the child assessment can be computed as where r S is the school cooperation rate and r CA is the child assessment completion rate. After the base year, only completion rates were computed for the different ECLS-K instruments, since the response rates of the schools where the children were sampled remained the same for each subsequent round. Data users can compute the third grade response rate for each ECLS-K instrument by multiplying the school response rate from the base year and the third grade completion rate for each instrument."}, {"section_title": "Completion Rates", "text": "For the ECLS-K third grade data collection, there were 11 survey instruments: child assessment; parent interview; teacher questionnaire part A, part B, and part C; special education teacher questionnaire part A and part B; school administrator questionnaire; school fact sheet; facilities checklist; and student records abstract. Except for the child assessment and parent interview, all other instruments were paper-and-pencil instruments. For each instrument, completion rates were computed separately for children who were sampled as part of the kindergarten cohort in the base year and for children who were sampled in first grade through the student sample freshening procedure. These rates were then combined to obtain the completion rates for all children in first grade. 6-3"}, {"section_title": "Children Sampled in Kindergarten", "text": "For the ECLS-K, a completion rate is a response rate conditioned on the results of an earlier stage of data collection. In the case of the ECLS-K third grade data collection, the condition is that children who were sampled in kindergarten were base year respondents since only base year respondents were eligible for subsequent data collection efforts. Children sampled in first grade were exempt from this condition in the computation of completion rates. They are discussed in section 6.2.2. For each instrument, the unweighted completion rate is the proportion of base year respondents with completed data for the instrument to the base year respondents who remain eligible to have the third grade instrument administered. Base year respondents who were subsampled out because they moved from their base year original sample schools and base year respondents who died or moved out of the country were not included in the denominator. For the weighted completion rates, the weight used is the product of the school base weight, the within-school child weight, and the factor that was used to adjust for movers between base year and third grade who were subsampled out for data collection. For a description of these weights and adjustment factor, see chapter 7. Tables 6-1 to 6-4 present weighted and unweighted child-level completion rates for springthird grade data collection, broken out by school characteristics. These rates pertain to children who were sampled as part of the kindergarten cohort in the base year. (Rates for students sampled in first grade through the student sample freshening procedure can be found in table 6-14.) Relative to spring-first grade the overall completion rates for the child assessments (80.8 percent) and the parent interview (77.8 percent), shown in for a complete set of spring-first grade completion rates. The decrease is almost completely due to the increase in the number of children who moved outside of the sampled primary sampling units (PSUs) or moved within the sampled PSUs but could not be located (the numbers slightly more than doubled in both cases). 1 These children are included in the category labeled \"Unknown\" for each of the different school characteristics (tables 6-1 to 6-4). The category \"unknown\" also includes 49 children who were homeschooled and thus had no information concerning schools. As shown in table 6.1, the completion rates for the child assessments are quite high and uniform across school characteristics (ranging from 97.1 percent in schools with 750 or more enrolled to 100 percent in schools in large towns), except for the \"unknown\" category for the different school characteristics. Similarly, the completion rates for the parent interviews were fairly consistent across school characteristics (ranging from 78.5 percent for children in schools with more than 90 percent non-White enrolled to 93.4 percent for children in large towns, with an overall completion rate of 78.5 percent), except for the \"unknown\" category. The \"unknown\" category aside, both the child assessments and the parent interview completion rates increased between spring-first grade and spring-third grade for all school characteristics. The completion rates by mover status are discussed later, but the rates of completing all the instruments are much lower for children who moved than for those who did not move. In all tables, the unweighted completion rates are almost always higher than the weighted completion rates, by as much as 7 percent at the overall level. This difference is due to movers who have larger weights and higher nonresponse rates than nonmovers. The weights of the movers were increased to account for the subsampling of movers. They also responded at a much lower rate than nonmovers, as shown in tables 6-5 to 6-8 and discussed later. Table 6-2 shows that the overall weighted completion rate is 66.1 percent for the school administrator questionnaire and 76.5 percent for the school fact sheet. The completion rate for the school fact sheet is about 10 percent higher than the school administrator questionnaire due to the extension of data collection into fall 2002 that affected the school fact sheet and the student records abstract. The completion rate for the school administrator questionnaire, 66.1 percent, is about 10 percentage points lower than the corresponding rate in spring-first grade, 76.3 percent (see chapter 5 of the User's Manual for the ECLS-K First Grade Public-Use Data Files and Electronic Code Book (NCES 2002-135) for spring-first grade completion rates). Note that there was no school fact sheet in spring-first grade. Once again, the increase in the movers is largely responsible for the lower rates, as discussed below. In third grade, 9,889 children no longer attended the schools where they were sampled in kindergarten. Of these, 41 percent were not subsampled. By comparison, 5,612 children in the spring-first grade sample no longer attended the original sample schools, and 49 percent of them were not subsampled. The subsampling rate is higher in third grade than in first grade because language minority children were followed at a much 6-5 higher rate, as discussed in chapter 3. The mover rate was 26 percent in spring-first grade and 46 percent in spring-third grade. 2 Table 6-2 shows that the completion rates for the school administrator questionnaire range from 69.0 percent for children in large cities to 99.4 percent for those in large towns (ignoring the \"unknown\" category). Rates for the school fact sheet follow the same pattern. Excluding the movers, the completion rate for the school administrator questionnaire is much higher, with an overall rate of 86.7 percent as shown in table 6-6, only slightly lower than the spring-first grade rate of 88.7 percent. In the case of the school fact sheet, the rate for nonmovers is 95.8 percent. It is worth noting that the completion rates for the school administrator questionnaire are lower for schools with higher percentages of minorities, a phenomenon also observed in previous rounds for the school administrator questionnaire. However, this disparity decreased considerably in the first-grade year and in spring-third grade compared to the base year, reflecting the success of increased data collection efforts targeted toward these schools. See chapter 4 for a discussion of the followup for missing school administrative questionnaires.The facilities checklist has a 77.5 percent completion rate, which is about 11 points higher than that for the school administrator questionnaire, but only 1 point higher than the rate for the school fact sheet. The student records abstract, which was to have been completed for all students except for those who moved and could not be located, had a 67.0 percent completion rate. Excluding children whose school characteristics are unknown, the completion rates for the facilities checklist in table 6-3 range from 89.1 percent for children in the suburbs of mid-size cities to 100 percent for children in large towns. For the student records abstract, the completion rates are lower, ranging from 72 percent for children in large cities to 95.8 percent for children in large towns. Table 6-7 shows the completion rates for these two instruments, separately for movers and nonmovers. Excluding the movers, the completion rates for both instruments are much higher. The rate for the facilities checklists is 97.2 percent for nonmovers compared with 50 percent for movers. The rate for the student records abstracts is 85.2 percent for nonmovers compared with 41.6 percent for movers. All three of the teacher questionnaires were completed at an overall rate of 62 to 63 percent. The completion rates have substantial variation when broken out by school characteristics, even when the \"unknown\" category for the school characteristics is ignored. The completion rates are 90 percent or more for Catholic schools and for schools in large and small towns. Schools in large cities, schools with 750 students or more, and schools with 90 percent or more minority enrollment have completion rates in the 6-6 60s. The \"unknown\" categories have, by far, the lowest completion rates. For a discussion of how data for the different instruments were collected for children who moved from their previous round schools, see chapter 4. 6-7 As noted earlier, the rate at which the survey instruments were completed varies markedly by mover status and within movers, by whether or not the child was located and followed. As shown in table 6-5, the completion rate for the child assessments was 94.5 percent for children still enrolled in their base year school. For movers it dropped by close to 9 points to 85.6 percent for those who were located and followed, and for those not located or followed due to a move to a non-ECLS-K PSU, it was zero. The parent interview completion rates varied from 84.5 percent for nonmovers to 74.8 percent for movers who were located and followed for the purposes of the child assessments, to 51.2 percent for movers who could either not be located or were not followed for the purposes of the child assessments. Even though children who had moved to a non-ECLS-K PSU were not administered the child assessments, a parent interview was conducted by telephone wherever possible leading to the 51.2 percent response rate for this category. Table 6-6 shows that the school administrator questionnaire completion rate is about 30 points lower for movers compared to nonmovers, even when the children who had moved were located and followed. For the school fact sheet, it is about 20 points lower for movers than for nonmovers. The facilities checklist (see table 6-7) behaves very much like the school fact sheet while the student records abstract has rates that are closer to those for the school administrative questionnaire. There are several reasons for the differences in rates between movers and nonmovers: located movers were not always assessed in schools; new schools in which movers enrolled had a lower level of commitment to the ECLS-K and often refused to complete the school administrator questionnaire; 3 and, some of these schools were contacted too late in the school year for them to consider completing it. For example, a child was identified as moving into another school during the last weeks of the field period; since the school year was over, the school and teacher questionnaires could not be collected for the child in that new school. The completion rate for nonmovers was 86.7 percent for the school administrator questionnaire and 95.8 percent for the school fact sheet. For located and followed movers it was 56.0 and 74.5 percent for the school administrator questionnaire and the school fact sheet, respectively. For all three teacher questionnaires the completion rates were approximately 82 percent if the child had not moved; about 54 percent if the child moved, was located, and followed; and just about 0 if not located or followed (table 6-8). There was a handful of children who could not be located but for whom teacher data were available. These data were collected from teachers in the schools from which the children transferred, facilitated by the teacher-child link identified in the fall of the school year. The 3 Of the 3,228 schools in the third grade sample, 41 percent refused to complete this instrument."}, {"section_title": "6-16", "text": "reasons for lower completion rates from teachers if the child moved are similar to the reasons that affected the school administrator questionnaire and school fact sheet completion rates for movers. 6-17 Data were also collected during spring-third grade from the special education teachers for children in special education programs (fewer than 1,200). Because this sample of children is small, completion rates were not computed for the different subgroups. Table 6-13 presents counts of completes and weighted and unweighted completion rates at the overall student level for the special education teacher questionnaires A and B. The completion rates for these instruments are higher than for the regular teacher questionnaires, 73.0 percent for part A, which captures teacher information, and 72.8 percent for part B, which relates to individual students who receive special education services. "}, {"section_title": "Children Sampled in First Grade", "text": "In spring-first grade, the student sample was freshened to include first graders who had no chance of selection in the base year because they did not attend kindergarten in the United States or were in first grade in the fall of 1998. For a detailed description of the freshening procedure see chapter 3, section 3.4.2. This same sample of students was followed into spring-third grade. Nonresponse in the freshened student sample could occur at two stages: during the procedure for sampling schools for freshening and identifying children to be used as freshening links in spring-first grade (first component) and then during data collection from the freshened students in spring-third grade (second component). The first component of the completion rate is the proportion of students sampled from the base year and subsampled for freshening that the study was able to do freshening on. The numerator includes all children who could be freshened from (i.e., those who did not move and in schools that cooperated with the freshening in first grade); the denominator includes children sampled for freshening (excluding movers not subsampled). For the weighted first component of the completion rate, both 6-27 numerator and denominator were weighted by the product of the school base weight, the within-school child weight, and the freshening adjustment factor. The school base weight and the within-school child level weight reflect the multi-stage sampling of the ECLS-K design, while the freshening adjustment factor is necessary because schools were subsampled for freshening in first grade as described above. These weights and adjustment factor are discussed in more detail in chapter 7. The first component alone can further be decomposed into two sources: attrition due to entire schools refusing to implement the freshening procedure (the school term), and attrition due to ECLS-K sampled children moving to other schools (the child term). To contain costs, students who transferred from schools targeted for freshening were not used as links to identify freshened students, even when they were otherwise followed for data collection. These movers were considered freshening nonrespondents in the child term. The second component is the proportion of freshened children with completed data for the instrument from within the population brought into the sample by freshening. The weight for this component is the product of the school base weight, the within-school child weight, the school freshening subsample adjustment factor and the third grade mover subsampling adjustment factor (which is equal to 1 for children sampled in first grade since none of them were subsampled out for followup). The final completion rate is the product of the two components. For example, the final completion rate for child assessment is computed as follows: where A is the set of children who could be freshened from as described above, B is the set of children sampled for freshening, W 1i is the weight for child i for the first component as described above, W 2i is the weight for child i for the second component as described above, ER CA denotes eligible child assessment respondent, and ENR CA eligible child assessment nonrespondent. To compute the unweighted rates, W 1i and W 2u are set to 1 for each child. for freshening (the school term) is 65.4 percent. As part of the freshening process, schools were asked to prepare an alphabetic roster of students enrolled in first grade. These schools were also requested to identify which students did not attend kindergarten the previous year. Schools did not participate in the freshening process because they either refused or were unable to provide the requested information. Within the schools that agreed to freshen, the freshening completion rate is 98.3 percent, the slight loss due to students who transferred to other schools (the child term). Multiplying these two terms together gives a first component completion rate of 64.3 percent. Note that the first component rate for spring-third grade is identical to the first component rate for spring-first grade. The second component varies by survey instrument. The rates for the paper-and-pencil instruments range from 50.9 percent for the child-level teacher questionnaire to 80.3 percent for the facilities checklist and are uniformly lower than for the kindergarten sample. The child assessment at 78.3 percent is 3 points lower than for the kindergarten sample, and the parent interview at 63.7 percent is 14 points lower. The larger difference in the parent interview rates, as compared with the difference in the rates for child assessment, is due to the different number of followups the two groups of parents had. For parents of children sampled in first grade, the third grade data collection was the first followup while most parents of children sampled in kindergarten had at least three followups (spring-kindergarten, spring-first grade and spring-third grade). The large parent nonresponse rate is consistent with what is found in most longitudinal studies where the biggest sample attrition is in the first followup. The number of followups does not affect the completion rate of child assessment in the same way since child assessment was done in the school for most children and hence could be perceived as part of school activities. The final completion rate for each instrument is the product of the two components. Because of the low rates at the first stage, these range from a high of 51.7 percent for the facilities checklist to a low of 32.7 percent for the child-level teacher questionnaire. In the first component, this is the completion rate for freshening. In the second component, this is the completion rate for the survey instruments. The product of the two components is the overall completion rate for the survey instruments. The freshening completes and completion rates for children in schools targeted for freshening. The freshening completes and completion rates for children in schools that agreed to the freshening procedure. Reading, math, or science assessment was scorable, or child was disabled and could not be assessed. Family structure portion of parent interview was completed."}, {"section_title": "6-29", "text": "A completed questionnaire was defined as one that was not completely left blank. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02. 6-30"}, {"section_title": "Spring-Third Grade Completion Rates for All Children", "text": "To compute the completion rate for the combined set of children sampled in the base year and children sampled in first grade, the completion rate for each group is weighted by the proportion of all children in that group, and the two weighted completion rates were added together. For example, the weighted completion rate for the child assessment (CA) was computed as where BY denotes base year, 1ST denotes first grade, r CA,BY is the child assessment completion rate for children sampled in the base year, r CA,1ST is the child assessment completion rate for children sampled in first grade, and W i is the final weight (C5CW0 for child assessment) for child i. To get the weighted completion rate for the child assessment (which is 80.8 percent for children sampled in the base year and 50.4 percent for children sampled in first grade), the weighted proportion of children who were sampled in the base year was 0.9766; the weighted proportion of children who were sampled in first grade was 0.0234. The weighted completion rate for the child assessment was 0.808\u00d70.9766+0.504\u00d70.0234=0.801, or 80.1 percent. grade represent such a small fraction of the total population of children, their inclusion in the computation of the completion rate brings down the rates for all children by less than one percent relative to the rates for children sampled in kindergarten, even though the completion rates for children sampled in first grade are lower than the kindergarten rates. Table 6-16 shows the completion rates for the child assessments, the parent interviews, and the school and teacher instruments for children who have nonzero child weights (C5CW0>0). These are children whose spring-third grade reading, math, or science assessments were scorable, or children who could not be assessed because of disabilities. These conditioned completion rates are useful to analysts who want to assess the relationship between the different instruments in term of participation. The completion rates from the different instruments are dependent in that if data from one instrument are 6-31 missing (e.g., parent instrument) it is likely that data from other instruments are also missing. (e.g., school administrator questionnaire). The conditioned completion rate for the child assessments are by definition 100 percent. The less than 100 percent rate shown when children sampled in kindergarten are combined with children sampled in first grade is due to the school freshening nonresponse for children sampled in first grade. When the completion rates are conditioned on the presence of the child assessment data, they are at least 9 points higher than the unconditioned completion rates for all instruments except for the two special education questionnaires. For these last two instruments, the difference between the number of completes for the conditioned and unconditioned rates is very small; hence the conditioned rates are not affected as much as for the other instruments. For all the other instruments, the conditioned completion rates are higher by 9 points for the parent interviews and as high as 17 points for the student records abstract. Since data were collected from schools, parents, teachers, and children, there were many opportunities for sources to contribute differentially to nonresponse, and this is reflected in the varying completion rates. These completion rates differ not only by survey instruments, but within each survey instrument they are also different by school and child characters. A separate report examines the potential for bias in estimates produced from the ECLS-K third grade data. Since analysis of the third grade data is conditioned on the base year-only base year respondents were included in the collection of first grade and third grade data-the analysis of nonresponse bias is built on the base year nonresponse bias analysis "}, {"section_title": "Item Response Rates", "text": "In the ECLS-K, as in most surveys, the responses to some data items are not obtained for all interviews. There are numerous reasons for item nonresponse. Some respondents do not know the answer for the item or do not wish to respond for other reasons. Some item nonresponse arises when an interview is interrupted and not continued later, leaving items at the end of the interview blank. Item nonresponse may also be encountered because responses provided by the respondent are not internally consistent, and this inconsistency is not discovered until after the interview is completed. In these cases, the items that were not internally consistent were set to missing. Every item in the ECLS-K data file has values that indicate whether the respondent did not know the answer to the item (-8), or refused to give an answer (-7). The value -9 is used in all other cases where the answer is left blank or set to missing due to reasons mentioned above (described in the data file as \"Not ascertained\"). However, where an item is left blank due to a valid skip pattern, this is indicated by the value -1. Chapter 7 of the ECLS-K User's Manual for the Third Grade Public-Use Data File and Electronic Code Book. (NCES 2004-001) discusses in detail these special values. For each survey item, the response rate was computed as the number of responses not equal to any of the special values (-1, -7, -8, or -9) divided by the number of responses not equal to -1. Of all the ECLS-K instruments, only the parent interview has sizable number of items with special values -7 (\"Refused\") or -8 (\"Don't know\"). Table 6-17 shows the distribution of the nonresponse values for each instrument. For most of the data items collected in the ECLS-K, the item response rate was very high."}, {"section_title": "6-35", "text": "Overall, the median item response rate is 98.3 percent. The median item response rate for each of the instruments ranges from 87.8 for the school fact sheet to 99.7 for the child assessment. Table 6-18 shows the number of items, the median item response rate, the lowest item response rate, the highest item response rate, and the number of items with response rates of less than 85 percent for all instruments. Items with less than 85 percent response rates are listed in table 6-19 by instrument and in ascending order of item response rate. The number of cases for which each item was attempted is also shown in this table. The tables in this chapter show the item response rates for items on the restricted-use file. 6-37  6-39 6-40 6-41 "}, {"section_title": "6-36", "text": ""}, {"section_title": "WEIGHTING AND VARIANCE ESTIMATION", "text": "The ECLS-K data were weighted to compensate for differential probabilities of selection at each sampling stage and to adjust for the effects of nonresponse. As in the first grade year, only childlevel weights were computed for third grade. The use of these weights was essential to produce estimates "}, {"section_title": "Types of Weights", "text": "Two sets of weights were computed for third grade, cross-sectional and longitudinal. Crosssectional weights were used to produce estimates that are representative of the cohort of children who were in kindergarten in 1998-99 or in first grade in 1999-2000. 1 Longitudinal weights were used to analyze data in a longitudinal file created by merging base year, first grade, and third grade data. 1 Since the third grade sample was not freshened with third graders who did not have a chance to be sampled in kindergarten or first grade (as was done in first grade), estimates from the ECLS-K third grade data are representative of the population cohort rather than all third graders in 2001-02. The estimated number of children from the ECLS-K is approximately 96 percent of all third graders. While the vast majority of children in third grade in the 2001-02 school year are members of the cohort, third graders who repeated second or third grade and recent immigrants are not covered."}, {"section_title": "7-2", "text": "As in previous years, there were several survey instruments administered to sampled children and their parents, teachers, and schools: cognitive and physical assessments for children; parent interviews; several types of teacher and school questionnaires, and observation instruments (see the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) Base Year Public-Use Data Files and Electronic Code Book: User's Manual (NCES 2001-029;U.S. Department of Education, National Center for Education Statistics, 2001) for a description of the instruments). The multiple stages of base year sampling in conjunction with differential nonresponse at each stage and the diversity of survey instruments require that multiple cross-sectional sampling weights be computed for use in analyzing the ECLS-K data. Exhibit 7-1 summarizes the different types of cross-sectional weights. Exhibit 7-1. ECLS-K third grade cross-sectional weights: School year 2001-02 Weight to be used for analysis of ..."}, {"section_title": "C5CW0", "text": "third grade direct child assessment data, alone or in conjunction with any combination of (a) a limited set of child characteristics (e.g., age, sex, race/ethnicity), (b) any third grade teacher questionnaire A, B or C data, and (c) data from the school administrator questionnaire or school fact sheet."}, {"section_title": "C5PW0", "text": "third grade parent interview data alone or in combination with (a) third grade child assessment data, (b) third grade teacher questionnaire A, B, or C data, and (c) data from the school administrator questionnaire or school fact sheet. Exception: If data from the parent AND child assessment AND teacher questionnaire A or B (not C) are used, then C5CPTW0 should be used."}, {"section_title": "C5CPTW0", "text": "third grade direct child assessment data combined with third grade parent interview data AND third grade teacher data alone or in conjunction with data from the school administrator or school fact sheet or facilities checklist. The ECLS-K longitudinal files are created by merging data from the base year, first grade, and third grade. Longitudinal weights were created to use in analyzing data in these longitudinal files. These weights are described in exhibit 7-2. All longitudinal weights are child-level weights. As mentioned in the introduction, teachers and schools are not representative of third grade teachers and schools in 2001-02. For this reason, there are no cross-sectional weights computed to provide estimates at the school or teacher level. Consequently, there are no longitudinal weights computed at the school or teacher level."}, {"section_title": "7-3", "text": "Exhibit 7-2. ECLS-K: K-3 (panel) longitudinal weights: School year 2001-02 Weights to be used for analysis of C45CW0 direct child assessment data from BOTH spring-first grade and spring-third grade, alone or in conjunction with any combination of a limited set of child characteristics (e.g., age, sex, race/ethnicity)."}, {"section_title": "C45PW0", "text": "parent interview data from BOTH spring-first grade and spring-third grade."}, {"section_title": "C245CW0", "text": "direct child assessment data from spring-kindergarten AND spring-first grade AND spring-third grade, alone or in conjunction with any combination of a limited set of child characteristics (e.g., age, sex, race/ethnicity)."}, {"section_title": "C245PW0", "text": "parent interview data from spring-kindergarten AND spring-first grade AND spring-third grade."}, {"section_title": "C1_5FC0", "text": "direct child assessment data from four rounds of data collections involving the full sample of children (fall-kindergarten, spring-kindergarten, spring-first grade, spring-third grade), alone or in conjunction with any combination of a limited set of child characteristics (e.g., age, sex, race/ethnicity)."}, {"section_title": "C1_5FP0", "text": "parent interview data from four rounds of data collections involving the full sample of children (fall-kindergarten, spring-kindergarten, spring-first grade, spring-third grade)."}, {"section_title": "C1_5SC0", "text": "direct child assessment data from all five rounds of data collection, alone or in conjunction with any combination of a limited set of child characteristics (e.g., age, sex, and race/ethnicity). Each set of weights created to be used with the ECLS-K data consists of a full sample weight that is used in computing survey estimates and replicate weights that are used in variance estimation with a jackknife replication method. First-stage stratum and primary sampling unit (PSU) identifiers are also created so that variance estimation using the Taylor series approximation method can be produced using the full sample weights. See section 7.2.5 for a description of how replicate weights were created. Section 7.3 discusses variance estimation methods. The data file includes the final full sample weight (described in section 7.2.4) and the final replicate weights (described in section 7.2.5) but not the intermediate weights leading to the final weights. The names of the full sample weights in the file are as described in exhibits 7-1 and 7-2 (e.g., C5CW0). The names of the replicate weights have the same prefix as the full sample weight with the last digit indicating the replicate (e.g., C5CW1 to C5CW90 are the 90 replicate weights to be used with the full sample weight C5CW0). 7-4"}, {"section_title": "Computation of the Spring-Third Grade Weights", "text": "The third grade sample included only base year respondents (see chapter 3) and a supplemental sample of first graders brought in through a sample freshening procedure implemented in spring-first grade. Only a subsample of children who moved from the schools they were attending when they were sampled originally was followed into their new schools. However, children who moved into a destination school 2 because they had completed the highest grade at the originally sampled school were all followed. Also, children who belong to the language minority group and who had not moved out of the original sample schools at any time during the first grade year were all followed into their new third grade schools if they moved from the original sample school after spring-first grade. The weighting procedures for both cross-sectional and longitudinal weights are similar, although weighting cells were defined differently for each type of weight. For example, any longitudinal weight that contains data from fall-first grade may have used different cells due to sample size constraints. The weighting procedures for the third grade were divided into three main stages. These procedures were followed for creating each weight shown in exhibit 7-1 and exhibit 7-2. Only the groups of eligible children to whom the weight applies changed. For example, weight C5CW0 pertains to children with completed assessments in spring-third grade; weight C45PW0 pertains to children with completed parent interview in both spring-first grade and spring-third grade. In the base year, children who were not assessed because of a disability or because they were language minority children had positive C1CW0 and C2CW0 weights because they had data such as age, gender, race/ethnicity, height and weight, and characteristics of parents, teachers, and classrooms. In subsequent rounds of data collection, they continued to be treated the same. Weights that include any fall-first grade data (such as C1_5SC0, which is the weight for children for whom child assessments were obtained in all five rounds) were computed using the same procedures, but the cells for the weighting adjustments were more restricted because only the fall-first grade subsample was included. The replication scheme for data that include the fall-first grade panel is also different as described in section 7.2.5. 2 A destination school is a school that received at least four students from the school where they had just completed the highest grade."}, {"section_title": "7-5", "text": "The first stage of weighting was to compute an initial child weight that reflects the following: Adjustment of the school base weight for base year school-level nonresponse; Adjustment of the base year child weight for base year child-level nonresponse; and Adjustment of the base year child weight for subsampling of schools for freshening in first grade (for children sampled in first grade only). The first-stage weights are exactly the same as those used for the first grade year. They were used in spring-third grade as the starting weights for the adjustments in the second stage. The second stage of weighting was to adjust the initial child weight computed in the first stage for the following: Subsampling of movers and Child-level nonresponse. The third and last stage was to rake the weights adjusted in the second stage to sample-based control totals. The procedures used in the four stages of weighting are described in more detail in the following sections of this chapter."}, {"section_title": "Initial Child Weights", "text": "As mentioned earlier, the first stage of weighting was to compute an initial child weight that reflects: (1) the adjustment of the school base weight for base year school-level nonresponse (school-level weights), (2) the adjustment of the base year child weight for base year child-level nonresponse (childlevel weights), and (3) the adjustment of the base year child weight for subsampling of schools for freshening in first grade (child-level weights, for children sampled in first grade only). These weights were already computed for spring-first grade. For completeness, they are described below, in section 7.2.1.1 for the school-level weights and in section 7.2.1.2 for the child-level weights. Base year school characteristics used for constructing nonresponse cells were the school type (public, Catholic private, non-Catholic religious private, or nonsectarian private), the school locale (large city, midsize city, suburb of large city, suburb of midsize city, large town, small town, or rural area), the region where the school is located (Northeast, Midwest, South, or West), and the size classification of the school in terms of school enrollment as described in table 7-1. "}, {"section_title": "7-9", "text": ""}, {"section_title": "Base Year Child Weights", "text": "Two groups of children were fielded in spring-third grade: base year respondents, and eligible children who were sampled in first grade as part of the sample freshening procedure. The base year child weights for the two groups were the same as those computed for the first-grade year. A description of the two weights follows."}, {"section_title": "Base Year Child Weights for Base Year Respondents", "text": "A base year respondent is a sampled child with a positive C1CW0, C2CW0, C1PW0, or C2PW0 weight. The C1CW0 weights are positive for language minority (not Spanish), disabled, and assessed children in fall-kindergarten. The C1PW0 weights are positive for children whose parents 7-10 completed the family structure questions of the parent interview in fall-kindergarten. The C2CW0 and C2PW0 weights are positive under similar circumstances, but apply to data from spring-kindergarten. is the base year nonresponse-adjusted school weight described in section 7.2.1.1, and i CHLD_PROB is the probability of selection of the child within a school. To account for base year nonresponse-children who were not assessed in the base year and whose parent interviews were not completed (i.e., children who did not have at least one positive weight among C1CW0, C2CW0, C1PW0 and C2PW0)-the base year child weight was adjusted for nonresponse. The child weight adjusted for base year child-level nonresponse, The base year child weights were adjusted using weighting classes similar to those developed for the cross-sectional spring-kindergarten child weights. These classes were created with CHAID (chi-square automatic interaction detection), using the school characteristics from the school nonresponse adjustments (i.e., school type, locale, region, school enrollment classified into size category), 7-11 and a set of child characteristics (i.e., year of birth, sex, and race/ethnicity). Data on year of birth were obtained from the parent interviews, while data on sex and race/ethnicity were from the child sampling information, which was provided by the schools. If year of birth was missing from the parent interview, then it was taken from the child sampling information. If sex or race/ethnicity was missing from the child sampling information, then they were obtained from the parent interview data. Any remaining missing data were imputed with the modal value from the school from which the child was sampled for this purpose."}, {"section_title": "Base Year Child Weights for Children Sampled in First Grade", "text": "In spring-first grade the student sample was freshened to include first graders who had not been enrolled in kindergarten in 1998-99 and, therefore, had no chance of being included in the ECLS-K base year kindergarten sample. For this group of children who entered the sample in first grade, their weights need to have additional adjustments to account for the freshening procedure. See chapter 3 for a discussion of the student freshening in spring-first grade. Since each child sampled in first grade was directly linked to a child sampled in kindergarten, the first step was to compute a weight for the children who were sampled in kindergarten that reflects the school freshening subsampling and the school freshening nonresponse (some schools refused to provide the complete alphabetical roster of all students enrolled in first grade needed for freshening). This weight was then linked back to the child sampled in first grade and further adjusted for nonresponse due to not obtaining the data (e.g., assessment data, parent interview data) from the sample of freshened children. The procedures for computing the base year child weights for children sampled in first grade are described next. This adjustment was done within cells defined by school type (public, Catholic private, non-Catholic religious private, or nonsectarian private) and census region (Northeast, Midwest, South, or West). School Weight Adjusted for Freshening Nonresponse. The freshening procedure could not be applied in all designated schools because some schools did not provide the information needed for freshening (see chapter 3 for more details on the freshening procedures). These schools were considered nonrespondents. The school weight adjusted for freshening school-level nonresponse, "}, {"section_title": "7-13", "text": "In both the numerator and denominator of this factor, the school measure of size (i.e., the count of students in the school as described in section 3.2.2.2) was incorporated; the school measure of size is relevant because the weights will be used for child-level estimates, not school-level estimates. The nonresponse cells for this adjustment were created using school type (public, Catholic private, non-Catholic religious private, or nonsectarian private) and urbanicity (large city, midsize city, suburb of large city, suburb of midsize city, large town, small town, or rural area). Base Year Child Weight. Next, the school-adjusted weight was multiplied by the inverse of the probability of sampling the child in the base year to obtain a base year child weight for freshening. The base year child weight was is the within-school child selection probability. The base year child weight was then adjusted for base year child nonresponse because children who did not respond in the base year could not be linked to children in first grade in spring 2000. The adjusted weight, The nonresponse cells were created using the school characteristics school type, locale, region, and school enrollment size, and the child characteristics age, sex, and race/ethnicity."}, {"section_title": "7-14", "text": "Base Year Child Weights Adjusted for Movers. Only children who did not move from their original school were designated as links to children in the freshening procedure. The children who moved and were followed into their new schools were not identified to participate in the freshening process in their new schools. As a result, all children who moved were considered nonrespondents for the freshening process. Additionally, nonmovers and movers who were not in first grade were not eligible for freshening (e.g., if the child was in kindergarten in spring 2000, he or she would be linked only to other kindergarten children and thus was not eligible for the freshening of first graders). An adjustment was necessary to account for these two groups of children and was done in two steps. In the first step, an adjustment was made for movers whose grade was unknown. A portion of the movers was assumed to be in first grade. In the second step, the weights were adjusted for children who were in first grade and who were not identified to participate in the freshening process because they moved into a new school. For this two-step adjustment, each child was classified as in table 7-2. "}, {"section_title": "\u2211 \u2211", "text": "This two-step adjustment was done within cells defined by school type and census region. The weights thus created for children sampled in kindergarten were then linked to the children that they brought into the sample in first grade through sample freshening. In other words, the 7-16 weight of the child sampled in first grade was defined at this point to be the weight computed for the child sampled in kindergarten that was responsible for bringing the first grader into the sample. For the next step in the computation of the spring-third grade child weights, the two groups of children-base year respondents and children sampled in first grade through sample freshening-were put together, and a common variable and label were used to designate the initial child weight. This is the base year child weight as computed above for each group of children: if base year respondent, 4 2 if sampled in first grade The initial child weights ICHLDW were adjusted for movers between the base year and third grade and for nonresponse in third grade, and raked to sampled-based control totals to obtain the final spring-third grade child weights. These adjustments and raking procedures are described below.\nIn both steps of the adjustment, separate nonresponse classes were created for movers and nonmovers using various combinations of response status of child assessments and/or parent interviews in the base year 7 as well as whether children belong to the language minority group, the type of family (i.e., one-parent family, two-parent family, or other type of family) as identified during the spring-third grade parent interviews (C5PW0 only), and the school type including whether the child was homeschooled (C5CPTW0 only). These cells vary according to each type of weights as shown in appendix 7B."}, {"section_title": "Adjustment for Movers Between the Base Year and Third Grade", "text": "In the ECLS-K, a child could move more than once and at different times. For example, a child could move out of his or her original sample school because the school did not have grades higher than kindergarten. Then he could move again between first and third grade. Once a child was identified as a mover, he stayed a mover (unless he or she moved back to the original sample school). For example, a child who moved between kindergarten and first grade, but stayed in the same school from second and third grade would be considered a mover for the third grade. The spring-first grade follow flags indicating that a child should be followed if he moved were maintained for all children in the spring-third grade sample except for children whose home language was not English. For these language minority children, their spring-first grade follow flags were switched to 1 (i.e., to be followed if moved) if they were not already equal to 1 and if they had not already been subsampled out because they were identified as movers in spring-first grade. 5 Thus, children who moved out of their original sample school were followed in the random 50 percent of schools where the follow flag was set to 1, and language minority 5 In order to maximize the number of children with longitudinal data, care was taken during spring-first grade sampling to ensure that any child who had been flagged to be followed in fall-first grade continued to be followed in spring-first grade. In third grade, these children continued to be followed. 7-17 children were followed at 100 percent if they had not moved previously. 6 The child weight adjusted for subsampling movers 5 1 i R CHLDW was computed as For the cross-sectional weights, the mover adjustment factor was computed within cells created using the following characteristics: whether children were sampled in kindergarten or first grade, whether they were movers in spring-first grade, whether they were language minority children, the school type of their original sample school, and the region where their original sample school was located. For the longitudinal weights, a longitudinal mover follow status was created. This status took into account whether the child moved from his or her original school in fall-first grade, spring-first grade, or springthird grade (for longitudinal weights involving the fall-first grade data) or whether the child moved from his or her original school in spring-first grade or spring-third grade (for the other longitudinal weights). If a child moved in either round, he or she was considered a mover. Adjustment cells were created in the same way as for cross-sectional weights. However, for longitudinal weights involving all five rounds of data, only mover status and language minority status were used to create adjustment cells. Appendix 7A gives the cell definitions for the mover adjustment for cross-sectional and longitudinal weights. A few children with large cross-sectional child weights (C5CW0) and a few children with large spring-first grade/spring-third grade longitudinal child weights (C45CW0) had their weights 6 Language minority children who moved between kindergarten and first grade were followed to their first grade new schools at a rate of approximately 50 percent. If these movers moved again between first grade and third grade, then they were followed with certainty. The only language minority children who left the sample are those who moved between kindergarten and first grade and were not followed into their first grade schools."}, {"section_title": "7-18", "text": "trimmed by half. However, the weights were not redistributed because the total sum of weights was reestablished in the raking procedure that came later."}, {"section_title": "Adjustment for Third Grade Nonresponse", "text": "After the adjustment for subsampling movers, the child weights were adjusted for nonresponse. As in spring-first grade, the nonresponse adjustment was done in two steps. In the first step, the adjustment was for children whose eligibility was not determined (unknown eligibility). A portion of children of unknown eligibility was assumed to be ineligible. In the second step, the adjustment was for eligible nonrespondents. To carry out these adjustments, each child was classified as in table 7-3. where 5 2 c R ADF , the adjustment factor, was computed as , , "}, {"section_title": "Raking to Sample-Based Control Totals", "text": "To reduce the variability due to the subsampling of schools and movers, the child weights were then raked (i.e., calibrated) to sample-based control totals computed using the initial child weights described in section 7.2.1. A file was created with the initial child weights and school and child characteristics collected in the base year or first grade year (such as school type, region, urbanicity, sex, age, race/ethnicity, SES, language minority status, whether sampled in kindergarten or first grade, and if sampled in kindergarten, mover status in spring-first grade) to be used in the computation of the control totals. The child records included in this file are records of base year respondents and records of eligible children sampled in first grade, including records of children who became ineligible in spring-third grade. The sum of the initial weights thus calculated is the estimated number of children who were in kindergarten in 1998-99 or first grade in 1999-2000. In the previous steps, the weights of the nonresponding children were distributed to the responding children while the weights of the ineligible children were not affected by this weighting step. The weights of the ineligible children were set to zero at the end of this process because these children are not meant to be included in the analysis of the springthird grade data. The reason for including the ineligible children in the raking step is that these children were included in the sampled-based control totals. Before raking the C5CPT weights, a small number of responding movers had their nonresponse-adjusted weights trimmed and the excess weight redistributed among the remaining responding movers so that the sum of weights before trimming was equal to the sum of weights after trimming. The raked child weight or spring-third grade final child weight, 5 4 i R CHLDW , was This raking procedure is essentially a multivariate poststratification. Raking cells (also known as raking dimensions because they typically involve more than one variable, for example, gender by age) were created using school and child characteristics collected in the base year or first grade year: school type, region, urbanicity, sex, age, race/ethnicity, SES, language minority status, whether sampled in kindergarten or first grade, and if sampled in kindergarten, mover status in spring-first grade. Appendix 7C gives the raking dimensions used for spring-third grade."}, {"section_title": "7-21", "text": "There was no restriction set in the number of iterations during the raking procedure. The procedure was allowed to run until complete convergence was achieved within a control total. Typically, this occurred with 12 to 16 iterations."}, {"section_title": "Replicate Weights", "text": "For each set of cross-sectional and longitudinal weights included in the third grade data file, a set of replicate weights was computed. All adjustments to the full sample weights were repeated for the replicate weights. The replication scheme used for the base year was used for all of the spring-third grade weights that did not contain any fall-first grade component. If a fall-first grade component was included in the definition of the respondents for the weight, then the replication scheme used for fall-first grade estimates was used. Replicate weights are needed to estimate the standard errors of survey estimates. A total of 90 replicate weights were computed using the paired jackknife method (denoted as JK2) for the springthird grade weights if no fall-first grade component was included. These replicates take into account the Durbin method of PSU selection. A total of 40 replicates using the paired jackknife method were created for the weights that contain a fall-first grade component. The smaller number of replicates is due to the fact that only 30 percent of the full sample of schools was included in the fall-first grade subsample. Only one of the two sampled PSUs in the non-self-representing strata was kept in the sample. Consequently, the fall-first grade weights do not account for the Durbin PSU sampling method, which required two PSUs per stratum. The procedures used to compute the replicate weights took into account each step of the weighting process. One feature that is somewhat uncommon in practice is the use of sample-based raking as described in section 7.2.4. The control totals ( c CNT SMP _ ) used for raking are estimates calculated using the initial child weights ( i ICHLDW ). When population-based raking is used, these totals are assumed to be numbers that are known and without sampling error. To reflect the variability of the control totals in the sample-based raking, a set of replicate control totals was calculated rather than having a constant set of totals. Each replicate weight was then raked to the corresponding replicate-based control total. The result of this process was that each replicate retained the variability associated with the original sample estimates of the control totals. As with the full sample weight, the raking procedure was allowed 7-22 to run until complete convergence. For spring-third grade, full convergence was achieved after 12 to 16 iterations for each replicate weight."}, {"section_title": "Replicate Weights for Samples not Involving Fall-First Grade", "text": "For the original ECLS-K design in the base year, replicate weights were created taking into account the Durbin method of PSU selection. The Durbin method selects two first-stage units per stratum without replacement, with probability proportional to size and a known joint probability of inclusion. In the ECLS-K PSU sample design, there were 24 self-representing (SR) strata and 38 nonself-representing (NSR) strata. Among the 38 NSR strata, 11 strata were identified as Durbin strata and were treated as SR strata for variance estimation. The purpose of the Durbin strata is to allow variances to be estimated as if the first-stage units were selected with replacement. This brings the number of SR PSUs to 46 (24 original SR PSUs and 22 Durbin PSUs from the 11 Durbin strata). The remaining 54 NSR PSUs are in 27 NSR strata; thus 27 replicates were formed, each corresponding to one NSR stratum. For the SR strata, 63 replicates were formed. The 90 replicates will yield about 76 degrees of freedom for calculating confidence intervals for many survey estimates. As stated earlier, the sample of PSUs was divided into 90 replicates or variance strata. The 27 NSR strata formed 27 variance strata of two PSUs each; each PSU formed a variance unit within a variance stratum. All schools within an NSR PSU were assigned to the same variance unit and variance stratum. Sampled schools in the 46 SR PSUs were grouped into 63 variance strata. In the SR PSUs, schools were directly sampled and constituted PSUs. Public schools were sampled from within PSU while private schools were pooled into one sampling stratum and selected systematically (except in the SR PSUs identified through the Durbin method where private schools were treated as if they were sampled from within PSU). Schools were sorted by sampling stratum, type of school (from the original sample or newly selected as part of freshening), type of frame (for new schools only), and their original order of selection (within stratum). From this sorted list, they were grouped into pairs within each sampling stratum; the last pair in the stratum may be a triplet if the number of schools in the stratum is odd. This operation resulted in a number of ordered preliminary variance strata of two or three units each. The first ordered 63 strata were then numbered sequentially from 1 to 63; the next ordered 63 strata were also numbered sequentially from 1 to 63, and so on until the list was exhausted, thus forming the desired 63 variance strata."}, {"section_title": "7-23", "text": "In strata with two units, a unit being a PSU in the case of NSR PSUs and a school in the case of SR PSUs, the base weight of the first unit was doubled to form the replicate weight, while the base weight of the second unit was multiplied by zero. In strata with three units, two variance strata were created: in the first variance stratum, the base weight of two of the three units was multiplied by 1.5 to form the replicate weight, and the base weight of the last unit was multiplied by zero; in the second variance stratum, the base weight of a different group of two units was multiplied by 1.5, and the base weight of the third unit was multiplied by zero. Multiplying the base weight in a unit by zero is equivalent to dropping one unit as required by the jackknife method. All adjustments to the full sample weights were repeated for the replicate weights. For each full sample weight, there are 90 replicate weights with the same weight prefix. A child sampled in first grade through the freshening process was assigned to the same replicate as the originally sampled child to whom the child was linked. When the child sampled in first grade was assigned a full sample weight, he or she was assigned the replicate weights in the same manner."}, {"section_title": "Replicate Weights for Samples Involving Fall-First Grade", "text": "For the two longitudinal weights involving fall-first grade (C1_5SC0 and C1_5SP0), there are 40 replicate weights. The smaller number of replicates was due to the fact that only a subsample of schools was included in the fall-first grade sample. The weights associated with the fall-first grade data do not account for the Durbin method of selecting PSUs, since it no longer applied. Rather, they reflect the fact that only one of the two sampled PSUs in the NSR strata was kept in the subsample. To account for this feature, pairs of similar NSR PSUs were collapsed into 19 variance strata. The SR PSUs account for the remaining 21 variance strata."}, {"section_title": "Variance Estimation", "text": ""}, {"section_title": "Jackknife Method", "text": "The final full sample and the adjusted replicate weights can be used to compute estimates of variance for survey estimates using WesVar, AM, or other software that handles replicate weights. The"}, {"section_title": "Taylor Series Method", "text": "Variance stratum and variance unit (first-stage sample unit) identifiers were also created to be used in statistical software that computes variance estimates based on the Taylor series method (e.g., SUDAAN, Stata, SAS, and AM). In this method, a linear approximation to a statistic is formed and then substituted into the formula for calculating the variance of a linear estimate appropriate for the sample design. The Taylor series method relies on a simplified procedure for estimating the variance for a linear statistic even with a complex sample design and is valid in large samples in which the first stage units are sampled with replacement. For the ECLS-K, this simplified method does not capture the variance related to the Durbin sampling method, the effects of the adjustments of the weights for nonresponse, or the sample-based raking procedures. These effects are not captured in the Taylor series variance estimates mainly because each adjustment corresponds to a different estimator that the variance estimation software does not support. In some cases these adjustments may have only a minor effect on the variance estimates, but in other cases the effects could be more substantial. For software that uses the Taylor series method, the variance strata and PSUs must be defined. For the spring-third grade ECLS-K, the Taylor variance strata were assigned by sequentially numbering the sampling strata and collapsing any stratum with one PSU with an adjacent stratum. In theory, any variance stratum with fewer than two responding units would be combined with an adjacent stratum, but this did not happen in the ECLS-K. The variance units were assigned by sequentially numbering the first-stage sampling units within sampling strata. For example, for C5CW0, Taylor variance strata were numbered sequentially from 1 to 90. Within each Taylor stratum, Taylor units were numbered sequentially from 1 to the total number of units in the stratum. This procedure was done separately for each cross-sectional and longitudinal weight."}]