[{"section_title": "Abstract", "text": "Neuroimaging genomics is an emerging field that provides exciting opportunities to understand the genetic basis of brain structure and function. The unprecedented scale and complexity of the imaging and genomics data, however, have presented critical computational bottlenecks. In this work we present our initial efforts towards building an interactive visual exploratory system for mining big data in neuroimaging genomics. A GPU accelerated browsing tool for neuroimaging genomics is created that implements the ANOVA algorithm for single nucleotide polymorphism (SNP) based analysis and the VEGAS algorithm for gene-based analysis, and executes them at interactive rates. The ANOVA algorithm is 110 times faster than the 4-core OpenMP version, while the VEGAS algorithm is 375 times faster than its 4-core OpenMP counter part. This approach lays a solid foundation for researchers to address the challenges of mining large-scale imaging genomics datasets via interactive visual exploration."}, {"section_title": "Introduction", "text": "Recent advances in multimodal brain imaging and high throughput genotyping and sequencing techniques provide exciting new opportunities to study the influence of genetic variation on brain structure and function. Research in this emerging field, known as neuroimaging genomics (Glahn et al. 2007; Seshadri et al. 2007; Shen and Cooper 2017) , holds great promise to better understand complex neurobiological systems, as well as brain structure, function and cognition Saykin et al. 2015; Shen et al. 2010 Shen et al. , 2014 Viding et al. 2006; Yao et al. 2017) .\nThe unprecedented scale and complexity of these data sets, however, have presented critical computational bottlenecks requiring new concepts and enabling tools. On one hand, it remains a major challenge to develop systematic data mining approaches for revealing complex relationships between the brain (e.g., 1 million voxels) and genome (e.g., 3 billion base pairs). Additional challenges include how to seamlessly integrate the data mining methods with prior knowledge to produce biologically meaningful findings, and how to translate the methods into user-friendly, interactive software tools that optimally combine human expertise and machine intelligence to enable novel contextually meaningful discoveries (Kim et al. 2009a) .\nIn order to address these challenges, using the study of Alzheimer's disease (AD) as a test bed, we've developed a GPU (graphics processing unit) accelerated computational method that enables visual exploratory browsing for interactive mining of complex neuroimaging genomics data. In this framework, a user-friendly heat map interface, coupled with a brain explorer and a genome explorer, is used to visualize high-dimensional research results while focusing on effective integration of techniques from data mining, interactive visualization, and big data analysis. Interactive data mining techniques have the potential to help people gain significant insights into a wide range of problems by means of iterative machine computation and visual exploration.\nHowever, as datasets (e.g., those in neuroimaging genomics) are being generated in larger volumes, higher velocity, and greater variety, creating effective interactive data mining techniques becomes a more difficult task. The use of a GPU can potentially enable the user to freely 'wander' around the data and interactively analyze datasets at scale. In this work, we emphasize the value of the GPU by implementing a browsing system (or browser in short) using both a GPU approach and a multi-threaded, CPU approach. The 375 fold improvement of the GPU over the 4-CPU version in this study clearly demonstrates the value of a GPU based implementation. This paper presents the initial efforts for creating such an application. Currently, the focus is on mining imaging genetic associations between structural magnetic resonance imaging (MRI) scans and SNP (single nucleotide polymorphism) genotyping data. The goals include (1) the creation of an application that helps the user better understand the relationships between image and genomic data at interactive rates, and (2) to provide a user friendly evaluation scheme for further investigation of potential patterns. An ANOVA (analysis of variance) method was initially prototyped as an example strategy to identify imaging genetic associations at the SNP level. Additionally, the VEGAS (Versatile Gene Based Association Study) algorithm Mishra and Macgregor 2015) was implemented to identify imaging genetic associations at the gene level. The team's contributions include: (1) an application that is accelerated by the GPU; (2) an ANOVA that runs at interactive rates; and, (3) the underlying MonteCarlo simulation for the VEGAS algorithm runs in less than 1,500 milliseconds for 10,000 iterations."}, {"section_title": "Materials and Methods", "text": "The voxel-based imaging genetic analysis was applied to demonstrate the interactive application for efficient and effective discovery of imaging genetic associations in the study of Alzheimer's disease. We first describe the imaging and genotyping data, and then present the method for interactively calculating imaging genetic associations and visualizing these statistics from various perspectives."}, {"section_title": "Data and Materials", "text": "The proposed application was emiprically evaluated using the baseline structural magnetic resonance imaging (MRI) data and genotyping data obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu) . One goal of ADNI has been to test whether serial magnetic resonance imaging, positron emission tomography, other biological markers and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early AD. For up-to-date information, see www.adni-info.org.\nBaseline structural MRI scans of both ADNI-1 and ADNI-GO/2 cohorts were downloaded from LONI (adni.loni.usc.edu) and processed with voxel-based morphometry (VBM) in SPM8 (Risacher et al. 2009 ). Briefly, scans were aligned to a T1-weighted template image, segmented into gray matter, white matter and cerebrospinal fluid maps, and then normalized to the Montreal Neurological Institute space. The gray matter density (GMD) maps were extracted and smoothed with an 8mm FWHM kernel. The resulting GMD images have a dimension of 182 \u00d7 218 \u00d7 182 (i.e., containing 7,221,032 voxels with 1 \u00d7 1 \u00d7 1 mm 3 voxel size).\nGenotyping data of both ADNI-1 and ADNI-GO/2 (Saykin et al. 2010 Shen et al. 2014) were also obtained from LONI, quality controlled, imputed and combined as described in Kim et al. (2013) . In the experiment, 100 non-Hispanic Caucasian subjects were randomly selected. In addition, all of the SNPs located within 253 genes, which were reported in publications using the ADNI genetics data, were extracted. As a result, there were a total of 22,888 SNPs included in the experiment.\nTo summarize, the test data set contains 100 subjects, 22,888 SNPs (from 253 genes), and 7.2 million voxels. These values are selected so that all the data can be loaded onto the GPU in this initial implementation. A future topic is to study whether these size restrictions can be removed by designing an effective memory allocation strategy for larger scale data sets."}, {"section_title": "Methodological Background", "text": "The use of the GPU for computing in the neuroimaging genomics domain appears to be unique. Earlier researchers typically used tools like Matlab or R that executed on several CPUs. As a result, they frequently limited the number of variables involved due to computational challenges. For example, Kim et al. (2009b) built a browser for neuroimaging genomic data that implemented the ANOVA (analysis of variance) and ANCOVA (analysis of covariance) algorithms in Matlab. They performed analyses in targeted regions including the hippocampus, amygdala, and the entire temporal lobe. They also allowed for the processing of 137 SNPs. Even with these restrictions it still took 2 to 4 seconds to calculate one statistical map of p-values.\nIn a similar vein, though genome-wide association studies (GWAS) (Hirschhorn and Daly 2005; Purcell et al. 2007; Zondervan and Cardon 2007) have been actively performed, it remains a challenging issue to relate high throughput genotyping data to large scale image data. The reduction in the size of these data types limits a researcher's ability to identify important relationships in an unbiased manner.\nIt is important to note that GPUs have been applied successfully to various forms of MRI data. Gembris et al. (2011) produced the first work involving fMRI analysis on the GPU with the purpose of accelerating the calculation of correlations between voxel time series, a technique used for identifying functional brain networks. used the GPU to accelerate correlation analysis. Eklund et al. (2011) used the GPU to create an interactive interface, with 3D visualization, for exploratory functional connectivity analysis."}, {"section_title": "Our Approach", "text": "To begin, the upper bound on the number of brain regions to consider is increased. For example, the Automated Anatomical Labeling (AAL) atlas subdivides the brain into 116 regions of interest (ROIs) (see the left panel of Fig. 1) , and many prior studies focus on analyzing the summary statistics of these ROIs instead of measures on millions of voxels. These boundaries have been eliminated and now permit the user to investigate up to several million voxels generated by the MRI (e.g., our test data containing 7.2 million voxels; see the right panel of Fig. 1 ).\nOn the genomic end, in this study, we allow the user to explore tens of thousands of SNPs (e.g., our test data set contains 22,888 SNPs). In these experiments, the number of subjects is limited to 100 so that all of this data can be loaded into the RAM of the GPU. In the future, strategies will be explored on how to remove this constraint via effective memory management. Figure 2 shows the primary components of the user interface called BECA (Brain Explorer for Connectomic Analysis). In the lower left hand corner, a 3-dimensional model of a reference brain is displayed with 7.2 million voxels, color-mapped with the p-value of the association between each voxel and the current SNP or gene. At the top of the user interface is the SNP or gene explorer. This region displays the \u2212log 10 (p-value) of the association between each SNP or gene and the current voxel.\nThe focal point of the user interface is the heat map. The user first selects either the ANOVA or VEGAS algorithm. If the ANOVA is selected, then the y-axis corresponds to the voxels and the x-axis corresponds to the SNPs. The intersection of a given voxel row and SNP column contains the ANOVA p-value of the association between the corresponding voxel and SNP.\nIf, on the other hand, the user selected the VEGAS algorithm, the y-axis corresponds to the voxels and the x-axis corresponds to the genes. The intersection of a given voxel row and gene column contains the VEGAS pvalue (the details will be explained later) of the association between the corresponding voxel and gene.\nIn both cases, the heat map is a window of 80 voxels by 80 SNPs (or 80 genes) into the larger matrix with at most 7.2 million voxels along the y-axis and 22,888 SNPs across the x-axis for the ANOVA, or 253 genes in the case of the VEGAS algorithm. Using a technique similar to Google Earth, the cells of the heat map are computed as the mouse moves about. The mouse wheel can be used to zoom in or zoom out. While the mouse is hovering over a given voxel-SNP (or voxel-gene) location, all of the p-values associated with column are mapped onto the brain explorer. At the same time, the p-values associated with the row are mapped to the genome explorer. The GPU performs approximately two TFLOPS (trillion floating point operations per second) to achieve this level of interactivity. If we had naively instanced the entire voxel-snp matrix for the ANOVA, it would have required 22, 888 \u00d7 7, 200, 000\u00d74 = 628 gigabytes of RAM and approximately 85 hours to compute on the GPU. Instead, we used our window based approach and computed everything required to be displayed, in 0.012 seconds. In this case, the RAM requirements were merely 22, 888 \u00d7 100 X 1 byte = 2.1 megabytes for the snp-subject matrix and 7, 200, 000 \u00d7 100 \u00d7 4 bytes = 2,746 megabytes for the voxel-subject matrix. It is unreasonable to expect a researcher to wait 85 hours to compute the entire matrix when they are only interested in examining small subsets that are relevant to their work."}, {"section_title": "ANOVA Functionality", "text": "The purpose of the ANOVA functionality in the application is to understand the relationship between the gray matter density (GMD) value from the MRI data and the SNP value from the genotyping data. The resulting p-value helps answer the question 'Given a voxel and a SNP, are the mean GMD values at the voxel location different across the SNP genotype groups?' If the group means are significantly different, that indicates the genetic variation at the SNP location has an effect on the phenotypical GMD value at the voxel location.\nThe reported p-value for a given voxel-SNP pair across all the subjects is\nwhere MS stands for Mean Square, the bg subscript refers to 'between group', the wg subscript refers to 'within group', and F \u22121 is the inverse F distribution function. Table 1 summarizes the intermediate computations required to compute the MS bg and MS wg , where k represents the number of SNP genotype groups, N is the total number of subjects, n j is the number of subjects within each genotype group, X is an individual observation,X j is the sample mean of the j th group, andX is the overall sample mean."}, {"section_title": "VEGAS Functionality", "text": "The VEGAS algorithm was first described by . This gene-based approach considers the association between a trait and all the markers (usually SNPs) within a gene rather than each marker individually. By combining the effects of all SNPs in a gene into a test statistic and correcting for linkage disequilibrium (LD), the gene-based test might be able to detect a stronger collective effect from multiple SNPs than that from each individual SNP. \nLike the ANOVA, the resulting p-value helps answer a similar question 'Given a gene and a voxel, do the genetic variations within the gene have a collective effect on the GMD measure at the voxel?' On the surface it might appear that this approach would require fewer computations than the ANOVA, but the underlying Monte-Carlo simulation quickly negates any of those benefits. The algorithm assumes that for a given gene with n SNPs, an n-element multivariate normally distributed vector with mean \u03bc = 0 and covariance is simulated. The n \u00d7 n covariance matrix representing the pairwise LD values is formed by computing all the pairwise Pearson correlation coefficients. The simulation (as described by ) is achieved by drawing n values from N (0, 1) and then multiplying the vector by the Cholesky decomposition of . The result, C = Chol( ), is a lower triangular matrix such that CC t = .\nThe trouble with this approach is that the Cholesky decomposition of is only defined if is symmetric, positive definite. However, covariance matrices are classified as symmetric, positive semi-definite, which means the Cholesky decomposition can fail due to singularity of . A more robust solution is to extract [U, S, V t ] = SV D( ), where SVD is the singular value decomposition of its argument. With U orthogonal, S a diagonal matrix of singular values, and V t orthogonal, we have\nBut is symmetric, so we will have\nwhich means that\nwhere C is now typically dense. With CC t as an approximation to the covariance matrix (due to rounding and truncation error caused by numerical computation), the vector Y n \u2208 R n can be generated from a distribution with covariance , by "}, {"section_title": "OpenMP Implementation", "text": "In order to understand the benefit of the GPU acceleration, the ANOVA and VEGAS algorithms were first implemented on the CPU using OpenMP. OpenMP is an application programming interface that supports shared memory multiprocessing in C, C++ and Fortran (our code was written in C++). In short, it uses a portable, scalable model that gives programmers a simple and flexible interface for developing parallel programming applications.\nFirst, the OpenMP implementation of the ANOVA algorithm was generated. The pseudo code is shown in Algorithm 1, along with Eq. (1) and Table 1 . Normally each voxel would be processed sequentially, but, by prefixing the first F orEach loop with the #pragma omp parallel for preprocessor directive, the compiler will generate C++ code that executes in parallel across all of the available CPU cores. The VoxelSNP matrix is the matrix of pvalues that are computed from the matrix of VoxelSubject values (derived from the MRI) and the SNPSubject values (supplied by the genotyping data). Now that the ANOVA algorithm has been implemented on the CPU, the focus is turned to the VEGAS algorithm. The pseudo code is described in Algorithm 2 and consists of 3 components. First, lines 2-8 perform an ANOVA calculation for all the SNPs associated with a given gene. Then, lines 11-15 compute the observed p-values for those voxel-gene pairs. Finally, lines 18-31 perform a MonteCarlo simulation to generate the empirical p-values. It is important to note that Algorithm 2 is called every time the user moves the mouse over the heat map. It is also important to point out that by inserting the OpenMP pragma on lines 1,10, and 17, the algorithm is effortlessly parallelized across the cores in a workstation."}, {"section_title": "GPU Implementation", "text": "With the CPU implementations complete, the GPU logic is now reviewed. The GPU in use is the GeForce GTX Titan X. It has 12gb of RAM and 3,072 cores. Given the embarrassingly parallel nature of the ANOVA algorithm, Algorithm 3 demonstrates that the p-value can be computed for each voxel V and each SNP S simultaneously on the GPU.\nThe VEGAS algorithm, however, is more difficult to implement efficiently. The first attempt executed in the same amount of time as the CPU version. The final attempt was hundreds of times faster. The following paragraphs describe why this occurred.\nAssume for a moment that you are asked to implement a 1 dimensional Monte-Carlo simulation. Algorithm 4 demonstrates a typical approach. First a random value X is drawn from an N (0, 1) distribution. X is then used as the argument of a function C that returns some value Y . Finally, in line 4, some sort of decision is made with regard to Y and the process continues for K iterations.\nIn the case of the VEGAS algorithm, the variable Z n is drawn from a normal distribution. This variable Z n corresponds to the same variable in Eq. (5). Algorithm 5 demonstrates a reasonable approach to the generation of Z n . Line 2 draws n values from N (0, 1) and assembles them into Z n . Line 3 shows the function C operating on Z n to yield the response variable Y n . Finally, a decision is made with regard to Y n and the process continues for K iterations. The first attempt at implementing the VEGAS algorithm on the GPU followed this pattern. The pseudo code is shown in Algorithm 6. You can see how the Monte-Carlo simulation is executed in parallel for every (voxel, gene) pair.\nAt this point it's necessary to understand how the performance of logic on a GPU is measured. The rate at which memory is accessed and the number of floating point operations per second (FLOPS) are two critical parameters. On the GeForce GTX Titan X, memory can be accessed at a rate of 336 gb/sec, and the maximum number of giga FLOPS (GFLOPS) is 6144. Algorithm 5 was accessing memory at a rate of 20 gb/sec and the computation rate was 20 GFLOPS. The solution to the poor performance problem is described in Algorithm 7.\nOn line 1 a stream of random numbers of length n \u00d7 K is generated in parallel by the GPU. This corresponds to the number of n-dimensional values that would be generated by K iterations of the Monte-Carlo loop. Line 2 then takes advantage of the structure of our problem. Recall that Eq. (5) transforms the normally distributed variable Z n into a new variable Y n \u223c C n\u00d7n \u00d7 Z n through multiplication by the C matrix. On line 2 a matrix multiplication between the matrix C n\u00d7n and X n\u00d7K is performed. Finally, line 4 is where some sort of decision is made about the matrix Y n\u00d7K . The approach in Algorithm 7 leverages the massive memory bandwidth and computational bandwidth of the GPU in an optimal fashion. Algorithm 8 shows the final implementation and Table 2 shows the performance improvement between Algorithm 5 and Algorithm 7.\nThe counter intuitive aspect of Algorithm 8 lies in the structure of the two F orEach loops. The voxel-gene pairs are processed sequentially, but the Monte-Carlo simulation is performed in parallel. An 800 fold improvement in the Monte-Carlo simulation is observed when the architecture of the GPU is considered while implementing the logic."}, {"section_title": "Results", "text": "The user interface for the application was written in C++ using QT 5.7. The interface logic between the QT code and GPU code was written in C++ using Visual Studio Professional 2015. The GPU code utilized CUDA 8.0 and device driver version number 382.33. CUDA (Compute Unified Device Architecture) is the parallel computing platform and application programming interface that was created by NVIDIA to give application programmers access to the massively parallel hardware of the GPU. All of the host and GPU code is 64 bit. The accuracy of our VEGAS algorithm for both the CPU and GPU was validated against a targeted genetic association analysis containing 24 genes and 1 phenotype. Our implementation yielded the same result when running the same analysis on the VEGAS website http://vegas2.qimrberghofer.edu.au/. Figure 3 shows the timing results for the ANOVA implementation. The GeForce Titan X is approximately 110 times faster than the 4-core OpenMP version, and approximately 440 times faster than the 1-core CPU version. It's interesting to note that the 1-core version is faster than the 4-core version up to a window size of 100 2 . This suggests that the overhead associated with OpenMP can play a dominant role in execution time for 'small problems'. Figure 4 shows the timing results for the VEGAS implementation. It can be seen that the GeForce Titan X is nearly 375 times faster than the 4-core OpenMP version. The 4-core version is nearly 4 times faster than the 1-core version. All of this suggests that the GPU version is nearly 1,500 times faster than the 1-core CPU version.\nWe have also made a video (see Supplemental Materials) to demonstrate the functionality and real-time interaction performance of the GPU-accelerated imaging genomics browser. The video was recorded on a PC running Windows 10/64 with one Intel i7-7700k CPU, one GTX 1080 GPU and 32GB RAM."}, {"section_title": "Conclusion", "text": "This paper has demonstrated how a GPU can be used to accelerate a big data mining application for neuroimaging genomics. With response times on the order of 1 second during interactive calculation of ANOVA and VEGAS heat map p-values, researchers are given unprecedented insight into basic characteristics their data. The 375 fold acceleration of the GPU based VEGAS algorithm is important because it enables the computation of MonteCarlo simulations at interactive rates. It has been shown how the obvious implementation of the simulation leads to poor response times. Then, an effective arithmetic transformation (that is better suited to the architecture of the GPU) is demonstrated to enable a huge performance increase.\nAlthough this work focused primarily on execution time, there are opportunities for further exploration with regards to implementation. For example, all of the computations were performed using single precision floating point format. We believe double precision should be investigated.\nIn addition, the user interface communicates to the computing engine using TCP-IP based sockets in a clientserver architecture. The overhead of the network bandwidth was never explored because the computing engine was never deployed to a server class machine. It could be that those tests would reveal further opportunities for improvements.\nThis work implemented the ANOVA and VEGAS algorithms. There are many other algorithms used in imaging genomics analyses such as those implemented in PLINK (http://www.cog-genomics.org/plink2). There are few changes that need to be made to the user interface to include them. The bulk of the work requires implementation of the computations in OpenMP on the CPU and on the GPU, which is time-consuming and non-trivial. However, the two existing algorithms can serve as a template when implementing new algorithms. We will gradually expand the functionality of this tool by implementing these additional algorithms.\nFinally, work is in progress to implement a userfriendly and dynamic graphical interface containing the heat map panel, the brain explorer, and the genome explorer; as well as functionalities to enable visual analytics via interactive exploration. Given that the critical computational challenge is successfully addressed by the GPU-based approach presented in this work, we aim to expand this prototype imaging genomic browser into an integrative system that optimally combines machine intelligence, human intelligence and domain knowledge to advance the scientific discovery in high dimensional brain imaging genomics."}, {"section_title": "Information Sharing Statement", "text": "Both the source code and documentation are available at https://github.com/lheric/GPU-IGB."}]