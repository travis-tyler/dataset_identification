[{"section_title": "Abstract", "text": "We introduce Bayesian QuickNAT for the automated quality control of whole-brain segmentation on MRI T1 scans. Next to the Bayesian fully convolutional neural network, we also present inherent measures of segmentation uncertainty that allow for quality control per brain structure. For estimating model uncertainty, we follow a Bayesian approach, wherein, Monte Carlo (MC) samples from the posterior distribution are generated by keeping the dropout layers active at test time. Entropy over the MC samples provides a voxel-wise model uncertainty map, whereas expectation over the MC predictions provides the final segmentation. Next to voxel-wise uncertainty, we introduce four metrics to quantify structure-wise uncertainty in segmentation for quality control. We report experiments on four out-of-sample datasets comprising of diverse age range, pathology and imaging artifacts. The proposed structure-wise uncertainty metrics are highly correlated with the Dice score estimated with manual annotation and therefore present an inherent measure of segmentation quality. In particular, the intersection over union over all the MC samples is a suitable proxy for the Dice score. In addition to quality control at scan-level, we propose to incorporate the structure-wise uncertainty as a measure of confidence to do reliable group analysis on large data repositories. We envisage that the introduced uncertainty metrics would help assess the fidelity of automated deep learning based segmentation methods for large-scale population studies, as they enable automated quality control and group analyses in processing large data repositories."}, {"section_title": "Introduction", "text": "Automated brain segmentation is a basic tool for processing magnetic resonance imaging (MRI) and provides imaging biomarkers of neuroanatomy like volume, thickness, and shape. Despite efforts to deliver robust segmentation results across scans from different age groups, diseases, field strengths, and manufacturers, inaccuracies in the segmentation outcome are inevitable (Keshavan et al., 2018) . A fundamental limitation of existing methods for whole-brain segmentation is that they do not estimate segmentation quality. Hence, manual quality control (QC) is advised before continuing with the analysis, but it has several shortcomings: (i) time consuming, (ii) subject to intra-and inter-rater variability, (iii) binary (pass/fail), and (iv) global for the entire scan. In particular when operating on large datasets, manual QC is very time consuming so that cohort-level summary statistics on biomarkers have, for instance, been used for identifying outliers (Sabuncu et al., 2016) . A shortcoming of such Bayesian approaches for image segmentation are an alternative because they do not only provide the mode (i.e., the most likely segmentation) but also the posterior distribution of the segmentation. Most of such Bayesian approaches use point estimates in the inference, whereas marginalizing over parameters has only been proposed in combination with Markov Chain Monte Carlo sampling (Iglesias et al., 2013) or the Laplace approximation (Wachinger et al., 2015) . Although sampling-based approaches incorporate fewer assumptions, they are computationally intense, especially when used in conjunction with atlas-based segmentation, and thus, have only been used for segmenting substructures but not the wholebrain (Iglesias et al., 2013) .\nFully convolutional neural networks (F-CNNs) have become the tool of choice for semantic segmentation in computer vision Long et al., 2015) and medical imaging (Ronneberger et al., 2015) . In prior work, we introduced QuickNAT (Roy et al., 2017 (Roy et al., , 2018b , an F-CNN for whole-brain segmentation of MRI T1 scans that has not only outperformed existing atlas-based approaches, but also accomplished the segmentation orders of magnitude faster. QuickNAT is also much faster than DeepNAT, a previous patch-based approach for brain segmentation with neural networks (Wachinger et al., 2018) . Although F-CNNs provide high accuracy, they are often poorly calibrated and fail to estimate a confidence margin with the output (Guo et al., 2017) . The predictive probability at the end of the network, i.e., the output of the softmax layer, does not capture the model uncertainty (Gal and Ghahramani, 2016) .\nRecent progress in Bayesian deep learning utilized the concept of Monte Carlo (MC) sampling via dropout to approximate samples from the posterior distribution (Gal and Ghahramani, 2016) . Dropout has originally been proposed to prevent overfitting during training (Srivastava et al., 2014) . Dropout at test time approximates sampling from a Bernoulli distribution over network weights. As dropout layers do not have learnable parameters, adding them to the network does not increase model complexity or decrease performance. Thanks to fast inference with CNNs, multiple MC samples can be generated to reliably approximate the posterior distribution in acceptable time. MC dropout for estimating uncertainty in deep learning was originally proposed for classification (Gal and Ghahramani, 2016 ) and later applied to semantic segmentation with F-CNNs in computer vision , providing a pixel-wise model uncertainty estimate.\nIn this article, we propose to inherently measure the quality of whole-brain segmentation with a Bayesian extension of QuickNAT. For this purpose, we add dropout layers to the QuickNAT architecture, which enables highly efficient Monte Carlo sampling. Thus, for a given input brain scan, multiple possible segmentations are generated by MC sampling. Next to estimating voxel-wise segmentation uncertainty, we propose four metrics for quantifying the segmentation uncertainty for each brain structure. We show that these metrics are highly correlated with the segmentation accuracy (Dice score) and can therefore be used to predict segmentation accuracy in absence of reference manual annotation. Finally, we propose to effectively use the uncertainty estimates as quality control measures in large-scale group analysis to estimate reliable effect sizes.\nThe automated QC proposed in this article offers advantages with regards to manual QC. Most importantly, it does not require manual interactions so that an objective measure of quality control is available at the same time with the segmentation, particularly important for processing large neuroimaging repositories. Furthermore, we obtain a continuous measure of segmentation quality, which may be a more faithful representation than dichotomizing into pass and fail. Finally, the segmentation quality is estimated for each brain structure, instead of a global assessment for the entire brain in manual QC, which better reflects variation in segmentation quality within a scan.\nThe main contributions of the work are as follows:\n1. First approach for whole-brain segmentation with inherent quality estimation 2. Monte Carlo dropout for uncertainty estimation in brain segmentation with F-CNN 3. Four metrics to quantify structure-wise uncertainty in contrast to voxel-wise uncertainty 4. Comprehensive experiments on four unseen datasets (variation in quality, scanner, pathology) to substantiate the high correlation of structure-wise uncertainty with Dice score 5. Integration of segmentation uncertainty in group analysis for estimating more reliable effect sizes.\nWhile end-to-end learning approaches achieve high segmentation accuracy, the 'black box' nature of complex neural networks may impede their wider adoption in clinical application. The lack of transparency of such models makes it difficult to trust the outcome. In addition, the performance of learning-based approaches is closely tied to the scans used during training. If scans are presented to the network during testing that are very different to those that it has seen during training, a lower segmentation accuracy is to be expected. With the uncertainty measures proposed in this work, we address these points by also estimating a quality or confidence measure of the segmentation. This will allow to identify scans with low segmentation accuracy, potentially due to low image quality or variation from the training set. While the contributions in this work do not increase the segmentation accuracy, we believe that assigning a meaningful confidence estimate will be as important for its practical use."}, {"section_title": "Prior Art", "text": "Prior work exists in medical image computing for evaluating segmentation performance in absence of manual annotation. In one of the earliest work, the common agreement strategy (STAPLE) was used to evaluate classifier performance for the task of segmenting brain scans into WM, GM and CSF (Bouix et al., 2007) . In another approach, the output segmentation map was used, from which features were extracted to train a separate regressor for predicting the Dice score (Kohlberger et al., 2012) . More recent work proposed the reverse classification accuracy (RCA), whose pipeline involves training a separate classifier on the segmentation output of the method to evaluate, serving as pseudo ground truth . Similar to previous approaches, it also tries to estimate Dice score. The idea of RCA was extended for segmentation quality control in large-scale cardiac MRI scans (Robinson et al., 2017) .\nIn contrast to the approaches detailed above, our approach provides a quality measure or prediction confidence that is inherently computed (i.e. derived from the same model, in contrast to using a separate model for estimating quality) within the segmentation framework, derived from model uncertainty. Thus, it does not require to train a second, independent second classifier for evaluation, which itself might be subject to prediction errors. An earlier version of this work was presented at a conference (Roy et al., 2018a) and has here been extended with methodological improvements and more experimental evaluation. To the best of our knowledge, this is the first work to provide an uncertainty measure for each structure in whole-brain segmentation and its downstream application in group analysis for reliable estimation. "}, {"section_title": "Method", "text": "We propose a fully convolutional neural network that produces next to the segmentation also an estimate of the confidence or quality of the segmentation for each brain structure. To this end, we use a Bayesian approach detailed in the following sections."}, {"section_title": "Background on Bayesian Inference", "text": "Given a set of training scans I = {\u0128 1 , \u00b7 \u00b7 \u00b7 ,\u0128 m } with its corresponding manual segmentations S = {S 1 , \u00b7 \u00b7 \u00b7 ,S m }, we aim at learning a probabilistic function F seg : I \u2192 S. This function generates the most likely segmentation S given a test scan I . The probability of the predicted segmentation is\nwhere W are the weight parameters of the function F seg (\u00b7). The posterior distribution over weights in Eq. (1) is generally intractable, where we use variational inference to approximate it. Thus, a variational distribution over network's weights q(W) is learned by minimizing the Kullback-Leibler divergence KL(q(W)||p(W|I, S)), yielding the approximate predictive distribution\nIn Bayesian neural networks, the stochastic weights W are composed of\nHere z i,j are Bernoulli distributed random variables with probabilities p i , and M i are variational parameters to be optimized. The diag(\u00b7) operator maps vectors to diagonal matrices whose diagonals are the elements of the vectors. Also, K i represents the number of nodes in the i th layer. The integral in Eq. (2) is estimated by summing over Monte-Carlo samples drawn from W \u223c q(W). Note that sampling from q(W i ) can be approximated by performing dropout on layer i in a network whose weights are (Gal and Ghahramani, 2016 ). The binary variable z i,j = 0 corresponds to unit j in layer i \u2212 1 being dropped out as an input to the i th layer. Each sample of W provides a different segmentation for the same input image. The mean of all the segmentations provides the final segmentation, whereas the variance among segmentations provides model uncertainty for the prediction."}, {"section_title": "QuickNAT architecture", "text": "As the base architecture, we use our recently proposed QuickNAT (Roy et al., 2018b) . QuickNAT consists of three 2D F-CNN models, segmenting an input scan slicewise along coronal, axial and sagittal axes. This is followed by a view aggregation stage where the three generated segmentations are combined to provide a final segmentation. Each 2D F-CNN model has an encoder-decoder based architecture, four encoder blocks and four decoder blocks separated by a bottleneck block. Dense connections are added within each encoder and decoder block to promote feature re-usability and promote learning of better representations (Huang et al., 2017) . Skip connections exist between each encoder and decoder block similar to UNet (Ronneberger et al., 2015) . The network is trained by optimizing the combined loss function of weighted Logistic loss and Dice loss. Median frequency balancing is employed to compensate for class imbalance (Roy et al., 2018b) ."}, {"section_title": "Bayesian QuickNAT", "text": "We use dropout layers (Srivastava et al., 2014) to introduce stochasticity during inference with the QuickNAT architecture. A dropout mask generated from a Bernoulli distribution z i,j generates a probabilistic weight W i , see Eq. (3), with random neuron connectivity similar to a Bayesian neural network (Gal and Ghahramani, 2016) . For Bayesian QuickNAT, we insert dropout layers after every encoder and decoder block with a dropout rate r, as illustrated in Fig. 1 . Dropout is commonly used during training of neural networks to prevent over-fitting, but deactivated during testing. Here, we keep dropout active in the testing phase and generate multiple segmentations from the posterior distribution of the model. To this end, the input scan I is feed-forwarded N times through QuickNAT, each time with a different and random dropout mask. This process simulates the sampling from a space of sub-models with different connectivity among the neurons. This MC sampling of the models generates N samples of predicted probability maps {P 1 , \u00b7 \u00b7 \u00b7 P N }, from which hard segmentation maps {S 1 , \u00b7 \u00b7 \u00b7 S N } can be inferred by the 'arg max' operator across the channels c. This approximates the process of variational inference as in Bayesian neural networks (Gal and Ghahramani, 2016) . The final segmentation S is estimated by computing the average over all the MC probability maps, followed by a 'arg max' operator as\nThe probability map P i consists of c channels {P\ni }, representing probability maps for each individual class, which includes the addressed brain structures and background."}, {"section_title": "Uncertainty Measures", "text": ""}, {"section_title": "Voxel-wise Uncertainty", "text": "The model uncertainty U s for a given voxel x, for a specific structure s is estimated as entropy over all N MC probability maps {P\nThe global voxel-wise uncertainty is the sum over all structures, U = s U s . Voxels with low uncertainty (i.e. low entropy) receive the same predictions, with different random neurons being dropped out from the network. An intuitive explanation for this is that the network is highly confident about the decision and that the result does not change much when the neuron connectivity is partially changed by using dropouts. In contrast, the prediction confidence is low, if predictions change a lot with altering neuron connectivity."}, {"section_title": "Structure-wise Uncertainty", "text": "As most quantitative measures extracted from segmentation maps (e.g., Hippocampus volume) relate to specific brain structures, it is helpful to have an uncertainty measure corresponding to each brain structure, rather than each voxel. Here, we propose four different metrics for computing structure-wise uncertainty from MC segmentations, illustrated in Fig. 2 for N = 3 MC samples. Type-1: We measure the variation of the volume across the MC samples. As volume estimates are commonly used for neuroanatomical analysis, this type of uncertainty provides a confidence margin with the estimate. We compute the coefficient of variation,\nwith mean \u00b5 s and standard deviation \u03c3 s of structure s for MC volume estimates. Note that this estimate is agnostic to the size of the structure."}, {"section_title": "Type-2:", "text": "We use the overlap between samples as a measure of uncertainty. To this end, we compute the average Dice score over all possible pairs of N MC samples,\nThis measures the agreement in area overlap between all the MC samples in a pair-wise fashion. Type-3: We use the intersection over overlap (IoU s ) metric, over all the N MC samples for a specific structure s as measure of its uncertainty. The value of IoU s is constraint between [0, 1] and it is computed as\nType-4: We define the uncertainty for a structure s as mean global voxel-wise uncertainty over the voxels which were labeled as s,\nIt must be noted that d"}, {"section_title": "M C s", "text": "and IoU s are directly related to segmentation accuracy, while U s and CV s are inversely related to accuracy. Also, it is worth mentioning that computing voxel-wise uncertainty maps requires all N segmentation probability maps P i (each one having a size around 2 GB), which can be computationally demanding. In contrast, our proposed metrics (except U s ) use label maps S i (size around 200 KB), which are much smaller in size and can be computed faster.\nand IoU s ) and set the weight as,\nIncluding weights in the regression increases its robustness as scans with reliable segmentation are emphasized. Setting all weights to a constant results in standard regression. In our experiments, we use\nwith age A i , sex S i and diagnosis D i for subject i. Of particular interest is the regression coefficient \u03b2 D , which estimates the effect of diagnosis on the volume of a brain structure.\nand IoU s show compact point clouds, whereas U s is more dispersed indicating lower correlation. It must be noted that each of the three unseen datasets has unique characteristics, which are not present in the training MALC scans. IBSR consists of scans with low resolution and thick slices. ADNI contains subjects exhibiting neurodegenerative pathologies, whereas training was done on healthy subjects. CANDI consists of children scans, whereas none of the training subjects was from that particular age range. So, we believe our experiments cover a wide variability of out of sample data (resolution, pathology, age range), which the model might encounter in a more uncontrolled setting. This is shown in Fig. 4 and explained in detail in Sec. 5.5. \nand IoU s ) to normal regression in Table 7 . Strikingly, uncertainty weighted regression results in significant associations to autism, identical to (Van Rooij et al., 2017) , whereas normal regression is only significant for amygdala.\nStandard approaches for group analysis on large cohorts involves detection of outlier volume estimates and removing the corresponding subjects from the regression process. This sometimes also requires a manual inspection of the segmentation quality. In contrast to these approaches, we propose to use all the scans and associated a continuous weight for all, providing their relative importance is estimating the regression coefficients without the need for any outlier detection or manual inspection."}, {"section_title": "Segmentation Uncertainty in Group Analyses", "text": "Commonly, image segmentation is only a means to an end, where image-derived measures are used in follow-up statistical analyses. We are interested in propagating the uncertainty from the segmentation to the follow-up analyses. The rationale is that segmentations with high uncertainty potentially corresponds to scans with poor quality whose inclusion would confound the true effect sizes and limit the statistical significance of observed group differences. We demonstrate the integration of uncertainty for generalized linear models (GLMs) in the following, but it can also be generalized to other statistical models. GLMs are frequently used in neuroimaging studies for identifying significant associations between image measures and variables of interest. For instance, in numerous group analyses studies Hippocampus volume was shown to be an important imaging biomarker with significant associations to Alzheimer's disease.\nIn solving the regression model, each equation, i.e., each subject, has equal importance in the optimization routine (i.e. \u03c9 i = 1, \u2200i). In contrast, we propose to integrate the structure-wise uncertainty in the analysis. This is achieved by solving a weighted linear regression model with an unique weight \u03c9 i \u2265 0 for subject i, with design matrix X, vector of coefficients \u03b2, and normalized brain structure volume V i (normalized by intra cranial volume). We use the proposed structure-wise uncertainties (CV s , d"}, {"section_title": "Experimental Setup", "text": ""}, {"section_title": "Architecture and Training Procedure", "text": "We set the dropout rate to r = 0.2 (other values of r decreased the segmentation performance compared to not using droupouts) and produce N = 15 MC samples (< 2 minutes), after which performance saturates (shown in Sec. 5.1). For training the neural network with limited data, we use the pre-training strategy with auxiliary labels proposed earlier (Roy et al., 2017) . To this end, we pretrain the network on 581 volumes of the IXI dataset 1 with segmentations produced by FreeSurfer (Fischl et al., 2002) and subsequently fine-tune on 15 of the 30 manually annotated volumes from the Multi-Atlas Labelling Challenge (MALC) dataset (Landman and Warfield, 2012) . The remaining 15 volumes were used for testing. The split is consistent to challenge instructions. This trained model is used for all our experiments. In this work, we segment 33 brain structures (listed in the appendix)."}, {"section_title": "Test Datasets", "text": "We test of four datasets, where three of the datasets have not be seen during training.\n1. MALC-15: 15 of the 30 volumes from the MALC dataset that were not used for training are used for testing. MALC is a subset of the OASIS repository (Marcus et al., 2007 (Kennedy et al., 2012) . The objective is to observe changes in uncertainty for data with age range not included in training. 4. IBSR-18: The dataset consist of 18 scans publicly available at https://www.nitrc.org/projects/ ibsr. The objective is to see the sensitivity of uncertainty with low resolution and poor contrast scans.\nNote that the training set (MALC) did not contain scans with AD or scans from children. Manual segmentations for MALC, ADNI-29, and CANDI-13 were provided by Neuromorphometrics, Inc. "}, {"section_title": "Experimental Results and Discussion", "text": ""}, {"section_title": "Number of MC Samples", "text": "First, we examine the choice of number of MC samples (N ) needed for our task. This choice is mainly dependent on two factors: (i) the segmentation accuracy by averaging all the MC predictions needs to be similar to the segmentation accuracy not using dropouts at test time, and (ii) the estimated uncertainty map needs to be stable, i.e., addition of more MC samples should not effect the computed entropy values. We use the CANDI-13 dataset for this experiment as it represents an out of sample dataset, i.e., data not used in training the model. It therefore provides a realistic test case on unseen data. We performed experiments with N = {3, 6, 9, 12, 15, 18}.\nThe mean global Dice scores for different values of N are reported in Tab. 1. We observe that the Dice score remains more or less constant as N increases from 3 to 18, which is very close to the Dice performance with no dropouts at test time. This is in contrast to prior work that reported a performance increase with more MC samples . A potential reason for this is that the QuickNAT framework aggregates segmentations across the three principal axes (coronal, axial and sagittal) (Roy et al., 2018b) . Hence, N MC samples actually represents aggregating 3\u00b7N segmentations in our framework. Furthermore, the view aggregation step compensates from the slight decrease in segmentation performance due to dropout at test time.\nNext, we investigate the number of MC samples needed to reliably estimate the model uncertainty. The voxel-wise uncertainty can be considered stable if the estimated entropy values do not change substantially with larger N . Let the uncertainty maps for i and j MC samples be U i and U j , respectively. We estimate the mean absolute difference between them, E[|U i \u2212U j |] to quantify the stability. It is worth mentioning that as N increases, not only does the segmentation time per scan increase, but also the required computational resources and complexity. This is due to the fact that all the N intermediate 3D segmentation probability maps (4D tensors) need to be loaded in the RAM for estimating the voxel-wise uncertainty map. We set N = 15 for all the following experiments, which provides high segmentation accuracy and reliable uncertainty estimates, while keeping the computational complexity within acceptable margins."}, {"section_title": "Uncertainty based quality control across different", "text": "datasets In this section, we conduct experiments to explore the ability of the proposed structure-wise uncertainty metrics in predicting the segmentation quality across different seen and un-seen datasets. Towards this end, we compute the correlation coefficient between the four uncertainty metrics and the Dice scores to quantify its efficacy in providing quality control. We report the mean Dice score, the correlation coefficients and mean IoU in Table 3 for all four datasets described in Sec. 4.2. Firstly, we observed that the segmentation Dice score is the highest on MALC dataset (88%), while the performance drops by 5\u22127% Dice points for other datasets (ADNI, CANDI and IBSR). The reason for this is that part of the MALC dataset was used for training, whereas the other datasets are un-seen scans resembling more realistic scenarios with training and testing scans coming from different datasets. This decrease in Dice score is accompanied by decrease in mean IoU (i.e. increase in structure-wise uncertainty). We also observe IoU has the highest correlation across all four datasets. Next to reporting correlations, we show the scatter plots for the four uncertainty measures with respect to actual Dice score on CANDI-13 dataset in Fig. 3 . In the scatter plots, we represent one dot per structure per scan, with unique colors for each of the classes. For the sake of clarity, structures from the left hemisphere of the brain are only displayed. We note that d"}, {"section_title": "IoU as a proxy for the Dice score", "text": "The Dice coefficient is the most widely used metric for evaluating segmentation accuracy and provides an intuitive 'goodness' measure to the user. This has motivated earlier works to directly regress the Dice score for segmentation quality control (Kohlberger et al., 2012; Valindria et al., 2017) . Our approach is different because we provide inherent measures of uncertainty of the segmentation model. While we have demonstrated that our measures are highly correlated to Dice scores (Sec. 5.2), the actual structure-wise uncertainty values may be challenging to interpret because it is not immediately clear which values indicate a good or bad segmentation. When looking at the scatterplot in Fig. 3 , we see that the uncertainty measures on the x-axis and the Dice score on the y-axis are in different ranges, with the only exception of IoU. Indeed, the values of IoU closely resembles the Dice score and we will demonstrate in the following paragraph that it is a suitable proxy for the Dice score.\nWe estimated the mean absolute error (MAE) between IoU s and Dice score and reported the results in Table 4 . Also, similar to Valindria et al. (2017) , we define three categories, i.e., Dice range [0.0, 0.6) as 'bad', [0.6, 0.8) as 'medium' and [0.8, 1 .0] as 'good'. We categorize the segmentations with actual Dice score and IoU s , and report the per-class classification accuracy in Table 4 . MAE varies between 2 \u2212 7%, while accuracy between 80 \u2212 88% as reported in Table 4 . All the similarity metrics (Correlation, MAE and 3-class classification accuracy) between IoU s and Dice score have values very similar or better to the ones reported in over 4 different datasets. This is remarkable because Valindria et al. (2017) trained a model to dedicatedly predict the Dice score, while we are simply computing the intersection over overlap of the MC samples without any supervision.\nWe also presented a structure-wise analysis to investigate similarity between Dice score and IoU s in Fig. 5 . Again, only structures on the left hemisphere of the brain are shown for clarity. In the boxplot, we observe that for most of structures IoU s is very close to actual Dice score. The worst similarity is observed for the inferior lateral ventricles, where there is about 15% difference between the two metrics. A potential reason could the small size of the structure. With all these experiments, we substantiate the fact that IoU s can be effectively used as a proxy for actual Dice score, without any reference manual annotations."}, {"section_title": "Sensitivity of Uncertainty to scan Quality", "text": "MRI scans of poor quality can lead to a degradation of the segmentation performance. Such poor quality scans can occur due to various reasons like noise, motion artifacts, and poor contrast. Model uncertainty is expected to be sensitive to the scan quality and should increase whenever segmentation accuracy decreases due to poor data quality. In this section, we investigate whether this property holds for our proposed model. Towards this end, we performed an experiment where we artificially degraded the quality of the input brain MRI scan with Rician noise. Here we use the MALC test dataset for evaluation pur- poses. We corrupt the scans with dB levels {3, 5, 7, 9} and reported the mean global Dice score and mean IoU s at each noise level in Tab. 5. We observe that the mean Dice score reduces as the dB level of the added Rician noise increases, whereas mean IoU s also decreases (indicating an increase in uncertainty). This confirms our hypothesis than our model is sensitive to scan quality. We also observe that mean IoU s falls at a faster rate than mean Dice score, indicating that uncertainty is more sensitive to noise than segmentation accuracy. It must be noted that in all our experiments with real scans, we did not encounter any scenario where segmentation failed (Dice score < 0.5). The experiment with Rician noise with dB = 9 resembles an artificially induced failure case."}, {"section_title": "Qualitative Analysis", "text": "We present qualitative results of Bayesian QuickNAT in Fig. 4 . From left to right, the input MRI scan, its corresponding segmentation, voxel-wise uncertainty map and structure-wise uncertainty (IoU s ) heat map are illustrated. The scale of the heat map replicates the Dice score [0, 1], where red corresponds to 1, indicating higher reliability in segmentation. Each row shows an example from the four different datasets, where we selected the scan with the worst segmentation accuracy for each dataset. The first row shows results on a test sample from the MALC dataset, where segmentation is overall of high quality. This is reflected by the thin lines in the voxel-wise uncertainty (anatomical boundaries) and redness in the structure-wise uncertainty heat map. Since the same dataset was used for training, we obtain high segmentation accuracy on MALC. The second row presents the scan with worst performance on the IBSR-18 dataset. Careful inspection of the MRI scan shows poor contrast with prominent ringing artifacts. The mean Dice score of the scan is 0.79, which is 3% below the mean score for the overall dataset. An increase in voxel-wise uncertainty can be observed visually by the thickening of the lines along anatomical boundaries (in contrast to MALC). The structure-wise uncertainty maps shows lighter shades of red in some sub-cortical structures, indicating a lesser reliable segmentation, in comparison to MALC. The third row presents the scan with worst performance in ADNI-29, which belongs to a subject of age 95 with severe AD pathology. Prominent atrophy in cortex along with enlarged ventricles can be visually observed in the scan. In addition to the pathology, ringing artifacts at the top of the scan can be observed. The mean Dice score is 0.78, which is 5% below the mean Dice score for the dataset. Its IoU s heat map shows higher uncertainty in some subcortical structures with brighter shades, whereas the reliability of cortex and lateral ventricles segmentation is good. It must be noted that training scans did not consist of any subjects with AD, and this example illustrates the performance of our framework for un-seen pathology. The last row presents the MRI scan with the worst performance on CANDI-13. The mean Dice score of the scan is 0.73, which is 8% below the mean Dice performance of the dataset. This scan can be considered as an outlier in the dataset. The scan belongs to a subject of age 5 with strong motion artifacts together and poor contrast. Scans of such age range and such poor quality were not used in the training pipeline, which explain the degradation of the segmentation performance. Its voxel-wise uncertainty is higher in comparison to others, with some prominent dark highly uncertain patches in subcortical regions. The heat map shows the lowest confidence for this scan, in comparison to other results. The cortical regions show shades of yellow, whereas some sub-cortical structures show shades of blue, which is towards the lower end of the reliability scale."}, {"section_title": "Uncertainty for Group Analysis", "text": "In the following section, we integrate structure-wise uncertainty in regression models for robust group analyses."}, {"section_title": "Group analysis on ADNI-29", "text": "ADNI-29 is a small subset of the ADNI dataset with 15 control and 14 Alzheimer's patients. We perform a group analysis as per Eq. (10) with age, sex, and diagnosis as independent variables and the volume of a brain structure as independent variable. Since we have manual annotations for ADNI-29, we can compute the actual volumes and accordingly estimate the ground truth regression coefficients. Table 6 reports the regression coefficients for diagnosis \u03b2 D for twelve brain structures. The coefficients are estimated based on manual annotations, segmentation with FreeSurfer and with Bayesian QuickNAT. Further, we use the uncertainty-based weighting on the volume measures from Bayesian QuickNAT. Weighting was done using three of the proposed structure-wise uncertainty as presented in Eq. (11). Our hypothesis is that weighting will result in regression coefficients \u03b2 D that are numerically equal or closer to the estimates from the manual annotation than those without weighting. We observe that out of the selected 12 structures, more reliable estimation of \u03b2 D is achieved with weighting and five structures using d M C s based weighting. Also for all structures, any weighting resulted in \u03b2 D estimation, which is closer to its actual value, thus substantiating our hypothesis. These results demonstrate that integrating segmentation quality in the statistical analysis leads to more reliable estimates. "}, {"section_title": "ABIDE-I", "text": "We perform group analysis on the ABIDE-I dataset (Di Martino et al., 2014) consisting of 1, 112 scans, with 573 normal subjects and 539 subjects with autism. The dataset is collected from 20 different sites with a high variability in scan quality. To factor out changes due to site, we added site as a covariate in Eq. 10. We report \u03b2 D with corresponding p-values for the volume of brain structures that have recently been associated to autism in a large ENIGMA study (Van Rooij et al., 2017) . We compare uncertainty weighted regression (weighted by CV s , d"}, {"section_title": "General Discussion", "text": "We introduced an approach to not only estimate the segmentation but also the uncertainty in the segmentation. The uncertainty is directly estimated from the segmentation model. Consequently, the uncertainty increases if a test scan is presented to the network that is different to the scans that it has seen during training. On the one hand, this holds for individuals that have different demographic characteristics or pathologies. On the other hand, this holds for image quality, which is related to the image acquisition process. Learning-based approaches can produce staggering segmentation accuracy, but there is strong dependence on the scans used during training. Since it will be impossible to have all scans that can potentially occur in practice represented in the training set, uncertainty is a key concept to mark scans with lower segmentation accuracy. Uncertainty could therefore be used to decide if scans have to acquired again due to insufficient quality. Further, it could be used to guide the inclusion of particular types of scans in training. Our experiments have demonstrated that structure-wise uncertainty measures are highly correlated to the Dice score. They can therefore be used for automated quality control. In particular, the intersection over union of the Monte Carlo samples has the same range as the Dice score and is demonstrated to be highly correlated with Dice in unseen datasets. Consequently, it can be interpreted as a proxy for the Dice score when manual annotations are not available to compute the actual Dice score. This can be beneficial for judging the segmentation quality of single scans.\nFor the analysis of groups of images, we then went one step further and integrated uncertainty measures in the follow-up analysis. We have demonstrated the impact of such an integration for regression analysis, but the general concept of weighting instances by their uncertainty can be used for many approaches, although it may require some adaptation. Such an approach offers particular advantage for the analysis of large repositories, where a manual quality control is very time consuming. Our results for the regression models have shown that weighting samples according to the segmentation quality yields estimates that are more similar to those from the manual annotation."}, {"section_title": "Conclusion", "text": "In this article, we introduced Bayesian QuickNAT, an F-CNN for whole brain segmentation with a structurewise uncertainty estimate. Dropout is used at test time to produce multiple Monte Carlo samples of the segmentation, which are used in estimating uncertainty. We introduced four different metrics to quantify structure-wise uncertainty. We extensively validated on multiple unseen datasets and demonstrate that the proposed metrics have high correlation with segmentation accuracy and provide effective quality control in absence of reference manual annotation. The datasets used in the experiments include unseen data from a wide variety with scans from children, with pathologies, with low resolution and with low contrast. Strikingly, one of our proposed metrics, intersection over union of MC samples, closely approximates the Dice score. In addition to this, we proposed to integrate the uncertainty metrics as confidence in the observation into group analysis, yielding reliable effect sizes. Although, all the experiments are performed on neuroimaging applications, the basic idea is generic and can easily to extended to other segmentation applications. We believe our framework will aid in translating automated frameworks for adoption in large scale neuroimaging studies as it comes with a fail-safe mechanism to indicate the user whenever the system is not sure about a decision for manual intervention. the Alzheimer's Disease Cooperative Study at the University of California, San Diego. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California."}]