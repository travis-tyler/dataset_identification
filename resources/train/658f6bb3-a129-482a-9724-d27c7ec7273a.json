[{"section_title": "Abstract", "text": "Do students perform better on statewide assessments in years in which they have more school days to prepare? We explore this question using data on math and reading assessments taken by students in the third, fifth, and eighth grades since \uf731994 in Maryland. Our identification strategy is rooted in the fact that tests are administered on the same day(s) statewide in late winter or early spring, so any unscheduled closings due to snow reduce instruction time and are not made up until after the exams are over. We estimate that in academic years with an average number of unscheduled closures (five), the number of third graders performing satisfactorily on state reading and math assessments within a school is nearly 3 percent lower than in years with no school closings. The impacts of closure are smaller for students in fifth and eighth grades. Combining our estimates with actual patterns of unscheduled closings in the last three years, we find that more than half of schools failing to make adequate yearly progress ( \nThe amount of research on the relationship between various aspects of schooling and student performance is vast. This includes work by economists on the impact of teachers and other inputs, and work on the effects of curricula or teaching methods. While questions about what happens inside schools and classrooms and how this affects student learning and performance are surely important, almost no work has addressed the fundamental question: What is the impact of having no school at all? Each winter, administrators are regularly forced to cancel school days because of bad weather. Each day a school is subject to an unscheduled closure, teachers, curricula, and school resources, no matter how effective, can have no real impact on student learning.\nIn this article, we examine the impact of school closures on student performance using data from Maryland public schools. We begin by describing the context within which unscheduled school closure decisions occur and how these might affect student performance. We then describe relevant research by economists and our empirical approach. Next we discuss our results and then consider their implications."}, {"section_title": "INTRODUCTION", "text": "The amount of research on the relationship between various aspects of schooling and student performance is vast. This includes work by economists on the impact of teachers and other inputs, and work on the effects of curricula or teaching methods. While questions about what happens inside schools and classrooms and how this affects student learning and performance are surely important, almost no work has addressed the fundamental question: What is the impact of having no school at all? Each winter, administrators are regularly forced to cancel school days because of bad weather. Each day a school is subject to an unscheduled closure, teachers, curricula, and school resources, no matter how effective, can have no real impact on student learning.\nIn this article, we examine the impact of school closures on student performance using data from Maryland public schools. We begin by describing the context within which unscheduled school closure decisions occur and how these might affect student performance. We then describe relevant research by economists and our empirical approach. Next we discuss our results and then consider their implications."}, {"section_title": "BACKGROUND", "text": "Annually across the country, students begin a school year that includes highstakes tests in selected grades. The results of these tests are part of states' efforts to improve accountability in public schools. They have been used to provide information to parents, to pressure administrators, and in some cases to trigger reconstitution of individual schools. They are currently being used to track adequate yearly progress for the federal No Child Left Behind Act (NCLB).\nIn Maryland, students in the third, fifth, and eighth grades take math and reading assessments and have done so since \uf731994. Initially, the testing regime was called the Maryland State Performance Assessment Program (MSPAP), which was then replaced in 2003 by the Maryland State Assessments (MSAs)."}, {"section_title": "\uf731", "text": "The test dates are set going into the school year so that teachers and principals can accommodate the tests. For security reasons, the tests are administered on the same day(s) statewide. So in all twenty-four districts across the state, teachers and administrators plan curricula and instruction in preparation for the exam date.\nOne important variable for which it is harder to plan is the number of unscheduled school closings before the test date arrives. Each year districts schedule days over and above the \uf73180-day minimum mandated by the U.S.\n\uf731. The MSPAPs were first administered in \uf731993, but the Maryland State Department of Education (MSDE) has not released results for all grades in that year. Since 2002, students in fourth, sixth, and seventh grades have also been tested as part of the MSA. However, because the MSPAP tested only students in third, fifth, and eighth grades, we restrict our analysis to the smaller, common set of grades.\nDepartment of Education so that schools can be closed in the event of bad weather while still meeting the minimum. If the number of days cancelled due to severe weather exceeds the excess number of days scheduled, the school year is extended. If the number of days cancelled is less than the cushion, schools can be dismissed before the scheduled closing. Regardless of whether the year is extended or shortened, this occurs at the end of the year-after the MSAs have been administered in the first week of March or the MSPAPs in the third week of April.\nThere are three additional features of school calendars in Maryland that are important to recognize. First, most districts start on the same day, and all start within approximately one week of one another.\n2 Though there are small differences in schedules, such as the timing of holiday breaks, there is little difference across districts in the number of instructional days going into the middle of spring. Further, there is no systematic variation across the state in start dates. For example, in the current year, the westernmost and easternmost districts started on the same day, and of the two districts to start the week before August 27, one was in the central part of the state and suburban and the other was in the western part of the state and rural. Finally, although we do not have school calendars for all districts over time, districts appear to hardly change their school calendars from year to year. Within districts the number of scheduled instructional days at the start of the testing period changes little from year to year. 3 As we will illustrate below, there is substantial year-to-year variation in the number of unscheduled closings. Within a school, there can be large differences in the number of actual days of instruction students have been provided as they sit for state assessments in the spring. The empirical question we explore in this article is straightforward: Do students perform better on statewide assessments in years in which they have more school days to prepare for the tests? For obvious reasons, more days in school ought to help students better prepare for state assessments. Of course, it is possible that schools and teachers could alter curricula or forgo activities less useful in preparing for assessments if days are unexpectedly lost to closure. But teachers may not be able to completely make up for days lost to closure. Indeed, the very notion of a mandatory \uf73180-day school year rests on a premise that a certain amount of time is necessary for teachers to cover and students to comprehend material. Work by economists on the relationship between schooling inputs and student performance has focused largely on the impact of higher quality inputs, not on marginal changes in input quantity. This includes a substantial amount of work on changes in class size and on increased expenditures on education. 4 Little empirical work has been done on the impact of more or less school.\nRecent work on teacher absences provides some insight because days when teachers are absent provide fewer days of exposure to the treatment as intended. Clotfelter, Ladd, and Vigdor (2006) and Miller, Murnane, and Willett (2006) have examined the impact of teacher absences on student performance. Both studies find evidence that students learn less when teachers are absent. Of course, when teachers are absent, students are supervised and likely even taught by substitute teachers, so characterizing this as a quantity change is questionable.\nA different piece of evidence comes from Pischke (2003) , who examines the impact of shortened school years in Germany. He exploits the fact that in the mid-\uf731960s, West German states switched from the practice of beginning school years in January to beginning in the autumn, with the exception of Bavaria, which already started in September. To accommodate this switch, the school year beginning in January was abbreviated so that the next year could begin in September. Pischke finds evidence that students fared more poorly immediately following abbreviated school years but that there were no persistent, longer-term effects on schooling or labor market outcomes.\nBoth Card and Krueger (\uf731992) and Grogger (\uf731996) examine the relationship between length of school year across states within the United States and subsequent labor market earnings. This approach is much less direct and relies on fairly small levels of variation in length of the academic year. Indeed, during the past several decades there has been no real variation in term length within the United States because districts widely adhere to a \uf73180-day calendar. 5 Eren and Millimet (2007) examine the effect of length of school year on student performance using data from the \uf731988 National Education Longitudinal Study. They use a dichotomous measure of length of school year (\uf73180 days or fewer versus more than \uf73180 days) and find that high-performing students do better with longer school years, while low-performing students fare worse. Marcotte (2007) estimates the impact of snowfall on student performance in Maryland. He finds that students who took exams in years with heavy snowfall performed significantly worse on the MSPAP assessments than did their peers in the same school who took the exams in other years. However, this 4. See Card and Krueger (\uf731992, \uf731996) and Hanushek (2002) -2005. 6 In the models below, we use measures of the percentage of students in a school who met MSDE guidelines for satisfactory performance on the MSPAP or MSA reading and math assessments. The MSDE data also provide information about average student characteristics in the school and resources in the district. The measures of student characteristics include the number of students with English as a second language (ESL) barriers, the number of students with special education needs, the number of students receiving free or reduced price lunches, and the number of students who are Title I eligible. We include these in the models estimated below to control for differences in student characteristics across schools that may be expected to affect performance.\nTo the panel constructed from MSDE data, we merge in data on the number of unscheduled closing days in an academic year, collected directly from school districts. Not all districts were able to provide data on closures for each year back to the \uf731993-94 school year, so the panel is a bit unbalanced with districts entering the estimation panel in the first year these data are available.\n7 On average, we have just more than seven years of data for each district. We have data on total snowfall for all academic years leading up to the test dates. Data on snow accumulation are provided by the National Oceanic and Atmospheric Administration's National Climatic Data Center (NCDC). From the NCDC, we obtain the accumulation recorded at the principal weather 6. The MSPAP also included subject tests in science, social studies, writing, and language usage, while the MSA includes only reading and math assessments. 7. In Baltimore City, administrators were able to provide these data for only the three most recent years, and in Calvert County they were unable to provide any data. In both cases, administrators recommended that we use closure dates from neighboring districts, which they suggested nearly always made the same decisions about closings. Our main results come from adopting this strategy and using data on school closings for Baltimore City from surrounding Baltimore County and from St. Mary's County and Charles County for Calvert County. We obtain similar results when we drop years with missing data from Baltimore City and Calvert County.\nreporting station within each county during a winter. In Maryland each county constitutes its own local education authority (district), as does Baltimore City. Using this panel, we estimate models of the relationship between unscheduled closures in a year and schools' performance on the math and reading tests for students. Separately for each grade and each subject, the basic setup is:\nwhere P st is the percentage of students in the relevant grade at school s at time t who perform at least satisfactorily on a given subject test (math or reading). The MSDE did not provide scores for individual students or the means for students within schools/grades. Nonetheless, the percentage of students performing satisfactorily is the fundamental policy variable of interest, since it is the yardstick with which performance under NCLB is measured. X st is a vector of resources and student characteristics for school s at time t known to affect achievement, including the percentage of children receiving free or reduced price meals (FARM), 8 the percentage of Title I students in the school, the percentage of ESL students, and the percentage of students in special education. C st is a measure of the number of scheduled school days that school s was closed in year t. We include unscheduled closing days directly and in a specification to check for possible nonlinear effects. We also estimate models in which we examine whether unscheduled closing days have different effects at schools with more poor students (as defined by the percentage of students receiving free/reduced price meals).\nWe include school fixed effects (\u03b1 s ) and school-specific linear trends to control for unmeasured student characteristics within schools and any gradual changes in those characteristics. The empirical question of interest then becomes: Do students in a given school perform better than their peers at the same school who, in different academic years, received fewer days of instruction prior to the state assessments, over and above any preexisting trend in the school's scores?\nWe also include a vector of three dummy variables measuring year effects for the two years prior and the year of the change in the test regime. It was widely known that the MSPAP tests were being phased out and that test results in these final two years were less consequential. We do not include fixed year effects for all years. More than 70 percent of the variation in school closings 8. In our main models we include the percent of children receiving free/reduced price meals as a quadratic.\nduring the panel is across time and is common across districts. 9 Thus year effects would absorb most of the variation of interest. All models are estimated using weighted least squares to adjust for heteroskedasticity in the error term, using each school's enrollment in the relevant grade as a weight. We account for the possibility of serial correlation in the error term by using Huber-White robust standard errors, clustering on school district, because superintendents make decisions about closure for all schools in the district.\nWe estimate separate models for performance on the reading and math assessments in turn for students in grades 3, 5, and 8. If instruction time improves test performance, we expect that the proportion of students performing at a level the MSDE considers satisfactory declines with the number of unscheduled closings in a given year."}, {"section_title": "RESULTS", "text": ""}, {"section_title": "Descriptives", "text": "We begin by considering descriptive statistics on test performance, unscheduled closing days, district resources, and student characteristics for our sample, presented in table \uf731. Over the course of the panel, between about one-third and one-half of students performed satisfactorily on reading and math tests in various grades. For example, on average, districts saw 43.5 percent of third graders perform satisfactorily on reading assessments and 45.4 percent perform satisfactorily on math assessments. However, the range for both of these scores is large. The percentage of students whose scores were satisfactory was comparable among fifth-grade students but performance was relatively poor among students in the eighth grade, with 37.6 percent and 43.5 percent performing satisfactorily on the reading and math assessments, respectively. On average, districts reported that about 42 percent of third-grade students were eligible for free or reduced price meals, less than 5 percent were ESL students, and about \uf7312 percent were in special education programs.\nTo illustrate the potential impact of unscheduled closures, in figure \uf731 we report variation in the number of unscheduled closing days over the period, by district. The diamond within each box represents the median number of unscheduled closures over the period within a district and the top and bottom of each box represent the 75th and 25th percentiles, respectively. It is important to note that the minimum value for many districts was zero. That is, during at least one school year in the period there were no unscheduled closings. Nonetheless, even in such districts there were other years with many unscheduled closings. For example, Baltimore County also had a year in which twelve 9. The R 2 from a regression of total closing days on year fixed effects only is .7\uf731\uf731. days were lost to unscheduled closings. An interesting outlier is the twentyone unscheduled closing days in Prince George's County. This was during the Washington area sniper's reign of terror, which included the shooting of a student at a Prince George's middle school. \uf7310 This outlier notwithstanding, over the course of the panel, many districts had some years in which there were no unscheduled closures and some years when more than two weeks of instruction time was lost to snow.\n\uf7310. Because that event may have had other effects on learning, we experimented with dropping all observations from Prince Georges County in this year. Doing so had no effect on the results reported below. As a way to begin understanding the relationship between unscheduled school closures and performance on state assessments, in figure 2 we present time series of the average number of school closures in the state and de-trended average performance on third-grade reading and math tests, net of school fixed effects. The solid line is the time series of a student-weighted average number of unscheduled closing days across the state of Maryland between \uf731994 and 2005. The dashed and dotted lines report the average number of third-grade students passing state assessments in math and reading (respectively) net of trends and school fixed effects. So in a year in which students in a school perform better on math/reading tests than average over the panel, the series is positive; in years below average, it is negative.\nThe series are interesting and suggestive. Both math and reading scores vary inversely with unscheduled school closings. Years in which students score above their school means were years with relatively few unscheduled closures, and years of below mean performance were years with frequent closures.\nTo more fully consider the relationship between days lost to unscheduled school closures and the percent of students performing satisfactorily on the reading and math assessments, we present results on key variables from our main models in table 2. In the first panel we present results for performance on the reading and math assessments for third-grade students. In the second and third panels, we present results for fifth and eighth grades, respectively. These models include controls for student characteristics, school fixed effects, and district-specific time trends. In the first column in panel \uf731, we find that each day lost to unscheduled closure reduces the percent of third-grade children in an elementary school that pass the reading exam by 0.485. The pass rates on the math exam fall by 0.528 percent with each day third graders lose to unscheduled closing. Since on average there are a bit more than five unscheduled closings each winter, nearly 3 percent fewer third graders pass reading and math tests in a typical winter than would have if there were no unscheduled closings at all. In winters with ten unscheduled closings, more than 5 percent fewer third graders would pass reading and math assessments.\nOf course, it may be that each day lost to unscheduled closure may not have the same impact on performance. Losing one or two days over the course of a winter may not have much impact on student performance, and many teachers may even find it restorative. But losing many days may be more seriously disruptive, perhaps because lessons are forgotten and time needs to be spent reteaching material covered before a long unplanned intermission. To test this, we include a series of indicators for the total number of days lost in a school year, ranging from \uf731-2 days to more than \uf7312. These results, presented in columns 2 and 5, do suggest that test scores fall as unscheduled closures mount. There is some evidence that the impact of closures is nonlinear, but not substantially. For reading, a bit more than 0.40 percent fewer students will perform satisfactorily for each day lost in winters with \uf731\uf731-\uf7312 closures, while this daily loss rises in the heaviest winters. For math, in years with 3-5 days of closure, about 0.25 percent fewer students will pass state assessments for each day lost, while in years with 8-\uf7310 closings, about 0.33 percent fewer students will pass for each day lost. This rises to a bit more than 0.50 percent per day in winters with more than \uf7312 unscheduled closings. These patterns suggest that long layoffs may be especially disruptive to learning or teaching mathematics. Notes: Standard errors adjusted for clustering at the LEA level are reported in parentheses. All models also control for school and student characteristics, district-level wealth and resources, school fixed effects, school-specific linear trends, and dummies for change in test regime. FARM students are those eligible for free and reduced price meals. * significant at 10%; * * significant at 5%; * * * significant at 1%\nIn the next columns, we allow the impact of unscheduled closings on performance to vary with the percent of students eligible for free/reduced price meals. We distinguish among schools where the percent of students eligible was less than \uf7315 percent, between \uf7315 and 35 percent, between 35 and 67 percent, and more than 67 percent. These are approximate cut points for statewide unconditional quartiles.\nWe estimate that days lost to unscheduled closure decrease reading pass rates by about two-fifths of a percent (.403) at schools with the fewest poor students and by nearly three-fifths of a percent (.592) at schools with the most poor students. These patterns are consistent with the interpretation that lost instruction time has a larger negative impact on reading performance for the marginal student in schools with more poor students. This pattern does not hold for performance on the math assessments. Indeed, we estimate that the impact of days lost on the percent of students passing math tests is at least as large at wealthier schools as at poor schools.\nIn the next panels, we present results for fifth and eighth grades. In both cases, unscheduled school closings negatively affect performance on math and reading assessments, but the impact is smaller than observed for third graders. For fifth-grade students, the impact on math performance is smaller, though comparable to the third-grade results, but the effect on reading performance is quite a bit smaller. For eighth graders, the unscheduled school closings have similar effects on the percentage of students passing the reading and math tests, with the effect about half as large as that observed among third graders.\nThat the largest effects are observed for students in lower grades is not surprising. Since districts can and do make up for snow days at the end of all school years, unscheduled closings during the year result in a relatively larger loss in cumulative instruction at the time of the assessments for students in lower grades. That is, going into the third grade, students had already received at least 360 instructional days in first and second grades. Going into the eighth grade, students have received a minimum of \uf731,260 instructional days. So any closures in the testing year would result in a larger fraction of lost instructional time for students in third grade."}, {"section_title": "Specification Checks", "text": "The results in table 2 provide evidence that days lost to unscheduled closure have negative and nontrivial consequences for student performance on state assessments. Whether we can interpret these effects as causal depends on whether unscheduled closing days provide exogenous information on variation in school instruction and resources. The decision to close schools in a local education authority (LEA) is made by the district superintendent. Naturally, superintendents are also aware of past and expected performance on state assessments within their districts. One concern is that in districts where schools are underperforming, superintendents may be less willing to close school in the face of bad weather than in districts where schools are expected to be safely performing above state standards. Of course, the weather has no regard for superintendents' worries about test scores, and it seems likely that the decisions about last-minute closures are highly constrained. One way to shed light on whether closing decisions are the product of natural fluctuations in weather or of something else is to examine variation in school closings and snow within districts. In the first panel of figure 3 we plot average snowfall and closing days for the state. In the three other panels, we present closings and snowfall for a district in the western part of the state (Garrett), a central district (Baltimore County), and one on the Eastern Shore (Talbot). Each panel presents the snowfall (in inches) on the primary y-axis and the number of unscheduled closing days in an LEA on the secondary y-axis. Both on average and within counties, unscheduled school closings substantially track snowfall. In each case, closing days track above snowfall in some years and below in others. This pattern is to be expected due to variation in the timing of snow or in how it is distributed across storms, rather than substantial differences in how districts use unscheduled closings. \uf731\uf731 \uf731\uf731. Snow on a school day would have a different effect than during a holiday. Winters with several average storms may have a larger effect than winters with one large storm, even if snow totals are Notes: Robust standard errors are in parentheses. All models also control for school and student characteristics, school fixed effects, school-specific linear trends, and dummies to measure change in test regime. * * significant at 5%; * * * significant at 1%\nA better way to assess the possibility that unscheduled closures are endogenous is to make use of data on snowfall as an instrument for unscheduled closings. In table 3 we present a summary of the results of two-stage least squares (2SLS) estimates of the impact of total closings on the percentage of third, fifth, and eighth graders passing state reading and math assessments. These results are from models that include the same specification used in table 2. Here, however, we use total snow accumulation during the winter in a district to instrument for the number of unscheduled closing days."}, {"section_title": "\uf7312", "text": "As expected, snow substantially predicts the number of unscheduled closings in a school year. For each grade, the coefficient on snowfall in the firststage model to predict the number of closings is about 0.\uf73125: On average, schools close a day for each eight inches of additional snow accumulation during the winter. The smaller t-statistic for the eighth grade is the result of a larger standard error, since there are fewer middle schools, not the result of a smaller first-stage point estimate.\nTo test whether the number of closings in a district is endogenous, we include the residual from the first-stage model of total closings in models of similar. For example, in \uf731996, much of the state faced a storm that dumped between twenty and thirty inches of snow in one day. \uf7312. There is some chance that snowfall, though purely a natural event, may not satisfy the exclusion restriction. If snowfall affects childrens' ability to focus on work while in school or spend time on homework outside school, we may be attributing some of the direct effect of snow to the policy variable, school closure. Unfortunately we have no way of testing this.\nreading and math scores. This sets up a test of endogeneity as a test of significance on this coefficient (Hausman \uf731978) . Any evidence that unmeasured factors associated with a superintendent's propensity to use unscheduled closing days are positively related to test scores is modest, at best. In four of six cases there is some evidence consistent with the concern described above, that superintendents are more likely to close school for the day if they are less worried about poor test performance.\nIn the last two columns of table 3, we present the 2SLS estimates of the effects of unscheduled closings on reading and math scores. The standard errors of the IV estimates are much larger than those reported in table 2, while the estimates are on the order of twice as large (in absolute value). Nonetheless, the same patterns emerge: Unscheduled school closings appear to negatively affect test performance, especially in lower grades, and more so for mathematics.\nA second way to check the specification is to examine whether exam performance suffers more if school is interrupted closer to the exam date and not at all if it is interrupted after the exam date. Unfortunately, data on the timing of school closures are available only for an unrepresentative subset of LEAs. Only thirteen of the twenty-four LEAs were able to provide at least some historical data on the dates of unscheduled closings. Worse, these were largely rural districts on the Eastern Shore and in western and southern Maryland. Only two populous counties in central Maryland provided these data.\n\uf7313 However, while we do not have data on the timing of unscheduled closures themselves in all LEAs, we do have data on snowfall by month for all LEAs."}, {"section_title": "\uf7314", "text": "We use the incomplete information on the timing of unscheduled closures, along with the complete information on the timing of snowfall, to carry out two separate checks of the relationship between timing of interruptions and test performance. First, we estimate models for the limited sample that provided information on the timing of school closures to identify timing effects. Second, for the full sample we estimate reduced form models of the impact of snowfall on test performance.\nIn both cases, we estimate the impact of the interruption (closing/snowfall) by month. We include separate measures of snow late in the season (March and April) for MSA versus MSPAP years. The MSA has been administered in the first week of March. Closures and snow after the test date should have no effect on MSA performance. But closures and snow in March and April should have an effect on performance on the MSPAP, which was administered in late April.\n\uf7313. The LEAs providing data on the timing of closure are Baltimore County, Caroline, Charles, Dorchester, Frederick, Garrett, Howard, Kent, Queen Anne's, St. Mary's, Talbot, Washington, and Worcester. \uf7314. We were not able to acquire information on the day of the week on which snowfall occurred, though this would obviously be helpful for identifying effects on school closures.\nIn table 4, we present results on the effects of unscheduled closings, by month, for the subsample providing these data. By way of assessing the characteristics of this subsample, we first present the effects of unscheduled closings on test performance, regardless of the timing. This specification is identical to the first columns in each subject/grade in table 2. We then differentiate between unscheduled closures in the fall (in or before November), December, January, February, and then March and April during MSA years separate from March and April during MSPAP years.\nIt is clear from the patterns across grades that unscheduled closures in the early part of the academic year do not have a consistent, significant impact on test scores. Closures in the fall, December, and January have somewhat mixed effects on performance. However, unscheduled closures as the exams loom have a negative and significant effect on test performance. In February, unscheduled closures have a consistent, substantial, and significant negative effect on performance. Closings in March and April are also negatively related to test performance in MSPAP years, when the exams were given in mid to late April. However, closings late in the year have no negative impact on performance in MSA years, when the exams were administered earlier.\n\uf7315 For this subsample, at least, the relationship between timing of closures and test scores is consistent with a causal link between unscheduled closures and performance.\nIn table 5, we present results from the reduced form approach for the whole sample. Snow early in the season has a negative effect only in the case of performance on third-grade reading exams. Snow in December, but not in January, has a consistently negative effect on pass rates across subjects and grades. As was the case for the models of unscheduled closure, it is disruption in February that appears to have the most consistent, negative impact on test performance. For both subject tests and all grades, snow in February is substantially and negatively related to test performance. Also, like the results in table 4, snow after March is generally negatively associated with MSPAP scores but not with MSA scores. As with the model of timing on test performance on the limited sample, this reduced form evidence from the full sample is largely consistent with the interpretation that interruptions to class schedules negatively affect test performance."}, {"section_title": "CONCLUSIONS AND IMPLICATIONS", "text": "The experiences of school districts in Maryland provide evidence that losing school days to unscheduled closings has negative effects on performance on state assessments. We find that school closings have larger effects on \uf7315. This should be interpreted with some caution. Only one district (Frederick) reported a closing this late in the year during the three years of MSA testing. Notes: Robust standard errors are in parentheses. All models also control for school and student characteristics, district-level wealth and resources, school-specific fixed effects and linear trends, and dummies to measure change in test regime. * significant at 10%; * * significant at 5%; * * * significant at 1% (all two-tailed tests) Notes: Robust standard errors are in parentheses. Models also control for school and student characteristics, school-specific fixed effects and linear trends, and dummies to measure change in test regime. * significant at 10%; * * significant at 5%; * * * significant at 1%\nperformance for students in lower grades. We estimate that the pass rate for third-grade math and reading assessments will fall by more than one-half percent for each school day lost to an unscheduled closure. This means that in years with a high level of unscheduled closures (ten days is a common value in heavy snow winters), more than 5 percent fewer students will pass third-grade reading and math tests than in winters with no unscheduled closures. That unscheduled closings affect the percent of students meeting state standards raises troubling questions about whether such schools can make adequate yearly progress (AYP) in meeting mandates for \uf73100 percent proficiency by 20\uf7313-\uf7314 under NCLB. Between 2003 and 2006, Maryland AYP was set so that the percentage of students performing at least satisfactorily (annual measurable objectives, or AMOs) increased at an average rate of 5 percent for reading and 6 percent for math assessments. For schools at the margin of meeting AMO for a year, unscheduled closures could impose a substantial handicap. Our estimates imply that in academic years with an average number of unscheduled closures, about 3 percent fewer third graders will perform satisfactorily on these tests. It is easy to see that in a bad winter, a real increase in performance necessary to make AMO could be swamped by the consequences of school closures.\nIn table 6 we illustrate the potential importance of unscheduled closures on the likelihood that schools fail to make AYP. We present the actual number of elementary schools in the state failing to make AYP in third-grade reading and Similarly, we estimate that thirty-four of the fifty-six schools failing to meet AYP in math would have exceeded the standard had it not been for unscheduled closings. Only in the final year, with relatively few unscheduled closings, does the number of failing schools far enough below their AMO that they would not have met AYP even if school had been in session for all scheduled days exceed the number of schools that would have met AYP with the extra days. We find some evidence that superintendents may be aware of the marginal impact of a lost school day and that those with lower average test scores are somewhat less likely to make use of an unscheduled closing when faced with the same weather conditions. This underutilization of unscheduled closings in poor performing districts (or excessive use in districts scoring better than expected) biases the impact of closings on performance toward zero. So the causal effects of unscheduled closings on student performance on MSA math and reading assessments are, if anything, larger than the estimates presented here.\nA further implication of this work bears on the question of whether the academic year should be extended. At least as far back as the \uf731983 report to the president, A Nation at Risk, extending the school year has received some \uf7316. AYP was first measured in Maryland in 2003. attention as a mechanism to improve performance. Currently, state or district superintendents in Massachusetts, Indiana, and Minnesota are pushing for longer school years, and public charter schools are experimenting with lengthening school days and school years. For example, the Knowledge Is Power Program (KIPP) association of charter schools has extended the length of the school day and year. Our results suggest that additional days of instruction do improve achievement on standardized tests. However, whether lost school days in the middle of the academic year have the same effect on achievement as additional days in late spring or early summer is an open question. Besides the distractions inherent with warm weather, the possibility of diminishing returns to the marginal day beyond \uf73180 makes it difficult to use the current results to forecast achievement gains that might result from adding days to the school calendar. Credible evidence on this question can best be had by evaluating experiences of districts that have made these changes."}]