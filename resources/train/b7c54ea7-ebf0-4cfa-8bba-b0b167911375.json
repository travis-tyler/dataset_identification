[{"section_title": "Abstract", "text": "Identification of regions of interest (ROI) associated with certain disease has a great impact on public health. Imposing sparsity of pixel values and extracting active regions simultaneously greatly complicate the image analysis. We address these challenges by introducing a novel region-selection penalty in the framework of image-on-scalar regression. Our penalty combines the Smoothly Clipped Absolute Deviation (SCAD) regularization, enforcing sparsity, and the SCAD of total variation (TV) regularization, enforcing spatial contiguity, into one group, which segments contiguous spatial regions against zero-valued background. Efficient algorithm is based on the alternative direction method of multipliers (ADMM) which decomposes the non-convex problem into two iterative optimization problems with explicit solutions. Another virtue of the proposed method is that a divide and conquer learning algorithm is developed, thereby allowing scaling to large images. Several examples are presented and the experimental results are compared with other state-of-the-art approaches."}, {"section_title": "Introduction", "text": "There has been significant research activity aimed at the association between image data and other scalar variables (e.g. cognitive score, diagnostic status) in the study of neurodegenerative and neuropsychiatric diseases, such as Alzheimer's disease (AD) [10] . The growing public threat of AD has raised the urgency to discover ROI of magnetic resonance images (MRI) that may identify subjects at greatest risk for future cognitive decline and accelerate the testing of preventive strategies. Machine learning methods have been developed and the penalized optimization is popular in the framework of the empirical risk minimization plus a penalty. However, spatially heterogeneous smoothness and local region selection greatly complicates the image analysis. To address these challenges, several regularization methods have been proposed to impose sparsity on both pixel values and their spatial derivatives. For instance, GraphNet [5] combines the Lasso penalty and an 2 penalty of image gradients, and TV-1 [4, 9] uses a weighted combination of the Lasso penalty and the TV penalty.\nIt is well-known that both Lasso and TV models have the inherent bias and often lead to less stable predictions [16] . For example, the spatially adaptive TV model [15] was proposed to remove the inherent bias in the TV model by utilizing a spatially varying weight function that is inversely proportional to the magnitude of image derivatives. It is a two-step procedure where the weight function obtained from the first step using standard TV is then used to guide smoothing in the second step. It is of interest to note that, in the statistical literature, the Smoothly Clipped Absolute Deviation (SCAD) penalty [6] has been proposed in the context of high-dimensional linear regression to address the shortcomings of Lasso (which is not consistent in variable selection). SCAD has some desired properties of the estimator such as continuity, asymptotic unbiasedness, sparsity, and the so-called oracle property (which behaves the same as when the zero coefficients are known in advance). There are a few papers on the use of SCAD for image analysis [3, 9] . None of them consider the local region learning. We will adapt the SCAD penalty for our local region selection problem in the framework of image-on-scalar regression.\nIn this paper, we propose a novel regularization method called SCAD2TV, which combines the SCAD regularization, enforcing sparsity, and the SCAD of TV regularization, enforcing spatial contiguity, into one group, which segments contiguous spatial regions against zerovalued background. This paper makes three main contributions:\n\u2022 The new penalty, SCAD2TV, forces zeros on coordinates and spatial derivative jointly, which makes it easy to identify ROI for the image-on-scalar regression model. It solves the bias issue inherent in LASSO or TV methods.\n\u2022 Our proposed algorithms are based on ADMM, which decomposes a non-convex problem with the non-convex penalty into two iterative optimization problems with explicit solutions. The divide and conquer learning algorithm is also developed, thereby allowing scaling to large images.\n\u2022 Compared with GraphNet and TV-1 , SCAD2TV has better or competitive performance in either prediction or selection errors.\n2 Image-on-Scalar Regression and SCAD2TV"}, {"section_title": "Image-on-Scalar Regression", "text": "Regression models with image responses and scalar predictors are routinely encountered in many applications [2, 7] . Consider an image-on-scalar regression model with varying coefficients: Y (s) = X T \u03b2(s) + \u03b7(s) + (s), where X \u2208 R p is the covariate, Y (s) \u2208 R is the image response at pixel s \u2208 S (a 2D or 3D domain), and \u03b2(s) = (\u03b2 1 (s), . . . , \u03b2 p (s))\nT \u2208 R p is the coefficient image vector. Here \u03b7(\u00b7) is a zero-mean spatial field which characterizes the spatial correlation, and (\u00b7) is the white noise with mean zero and variance \u03c3 2 . In this paper, we focus on Y \u2208 R N \u00d7N a 2D image. Extension to 3D images is straightforward. The objective is to identify ROI in the response image which are associated with the corresponding covariate by estimating the coefficient images \u03b2 1 , . . . , \u03b2 p . The available data are image and covariate pairs for n subjects, (X i , Y i (\u00b7)), i = 1, . . . , n. We obtain the estimator by minimizing\nwhere pen(\u00b7) is a penalty function which favors estimators according to certain criteria. Our purpose is to recover nonzero active regions of \u03b2 1 , . . . , \u03b2 p . The main challenges are that we need to impose sparsity of pixel values and extract active regions simultaneously."}, {"section_title": "Existing Regularizers", "text": "TV and SCAD. The TV analysis plays a fundamental role in various image analyses since the path-breaking works [13, 12] . We focus on the anisotropic version of TV. For \u03b2 \u2208 R N \u00d7N , define the discrete gradient \u2207 :\nThe TV norm \u03b2 T V is just \u03b2 T V = j,k (\u2207\u03b2) jk 1 . The isotropic induced TV norm is j,k (\u2207\u03b2) jk 2 , which is equivalent to the anisotropic induced TV norms up to a factor of \u221a 2.\nThe SCAD penalty \u03c1 \u03bb (\u00b7) is more conveniently defined its derivative\nand \u03c1 \u03bb (0) = 0. We use a = 3.7 by convention. Consider a penalized least squares problem:\nThe solution is unique, explicit, and\u03b8 = S ,\u03bb (z), where S ,\u03bb is the thresholding function. Figure 1 displays the thresholding function for SCAD and the soft thresholding function for Lasso with = 1 and \u03bb = 2. The SCAD penalty shrinks small coefficients to zero while keeping the large coefficients without shrinkage. GraphNet and TV-1 . GraphNet and TV-1 have been successful applied to medical images. GraphNet is the weighted average of an 1 penalty on all coordinates and a squared 2 penalty on the discrete gradient, while TV-1 is the weighted average of an 1 penalty and a TV penalty:\nFor both penalties, \u03bb > 0 is the smoothing parameter which controls the strength of regularization and \u03b3 \u2208 [0, 1] is another smoothing parameter controlling the trade-off between pixel sparsity and spatial regularity."}, {"section_title": "A New Penalty: SCAD2TV", "text": "For each coordinate \u03b2 jk , the discrete gradient (\u2207\u03b2) jk \u2208 R 2 involves three coordinate\nwhere \u03bb > 0 and \u03b3 \u2208 [0, 1] are two tuning parameters, and \u03c1 \u03bb is the SCAD function. The first term in the penalty allows adaptive estimation of the coefficient image and the second one enforces sparsity on coordinate values. One may also consider the functional version of (2). After some rescaling, (2) is equivalent to\nThe SCAD2TV solves the bias problem inherent in the TV and Lasso models. Note that this penalty function, unlike the L 1 penalty used in Lasso, is not convex, so that (1) is a non-convex objective function. We solve this problem based on the ADMM and convert it into two sub-problems with closed-form solutions. In general, ADMM has successful applications to convex problems. The behavior of ADMM applied to nonconvex problems has been a mystery. Recently, the global convergence of ADMM in non-convex optimization is discussed in [17] , which shows that several ADMM algorithms including SCAD are guaranteed to converge.\n3 Local Region Learning by SCAD2TV"}, {"section_title": "Algorithm based on ADMM", "text": "Our proposed algorithm is based on ADMM [1] . We may write (1) as the matrix form by an abuse of notation:\nwhere\n2 is the fixed extended design matrix related to the covariate for subject i, and \u03b2 \u2208 R pN 2 is the concatenated vectorized unknown coefficient image. Furthermore, one of the advantages of SCAD2TV in (2) is that we can write\n2 + 6(N \u2212 1)) by pN 2 matrix D depending only on \u03b3, which greatly facilitates the efficiency of our algorithm. This fact can be easily seen since the elements involved in the (j, k) th term in (2) are\nWe form the augmented Lagrangian as\nThe ADMM consists of the iterations\nIt should be emphasized that both (5) and (6) have the explicit solutions. Specifically, (5) is a ridge regression problem and (6) is a penalized least squares problem with the identity design matrix. The closed-form solutions for (5) and 6 are, respectively,\n.\nThe details of the algorithm is summarized in Algorithm 1. Update \u03b1 from (6):\nUpdate \u03b2 from (5): For given \u03b1 = \u03b1 (k+1) and \u03b7 = \u03b7 (k) ,\nUpdate \u03b7 by (7): For given \u03b1 = \u03b1 (k+1) and\n7 end Output : \u03b2 and \u03b1\nThe output is either \u03b2 or \u03b1. Note that \u03b1 is a sparse solution and \u03b2 may not be sparse. In practice, we extract the coefficient image estimator from the output \u03b1 to obtain the sparse estimator."}, {"section_title": "Divide and Conquer Learning Algorithm for Large Image Size", "text": "To address the big data issue, a divide and conquer (D&C) algorithm is a solution by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem. In the above discussion, we assume that, for each subject i, all image coordinate values Y i \u2208 R N 2 are used together to infer the coefficient images \u03b2. However, in many applications N may be large and such a \"batch\" procedure is undesirable. In order to solve this issue, we develop a D&C algorithm for large image size. Image data have their intrinsic structure and we need proceed the divide step with extra caution. For example, we may just partition each image as non-overlap sub-images\nwith the data processed sequentially. Due to the TV term in SCAD2TV, we lose the boundary information for all sub-images and this will give poor estimates of the boundary for all sub-images. We propose to partition Y i with overlapped sub-images Y i =\u1ef8 i1 \u222a\u1ef8 i2 \u222a \u00b7 \u00b7 \u00b7 \u222a\u1ef8 iJ . For instance, Figure 2 displays a 24 \u00d7 24 image, and it is straightforward to partition them into nine non-overlapped 8 \u00d7 8 sub-images. For our purpose, we extend each sub-image in four directions. Specifically, on the left up corner, we include both the light blue part and the light yellow part, which makes our first sub-image a 9 \u00d7 9 image. In the center, we take the inside 8 \u00d7 8 orange part together with the brown part, which makes it a 10 \u00d7 10 image. After this partition, we obtain nine overlapped sub-images. We perform Algorithm 1 on each sub-image and update the coefficient images. For each update we only keep the estimate for the original 8 \u00d7 8 sub-image.\nThe above D&C algorithm can be executed sequentially in a single machine. This algorithm is naturally adapted for execution in multi-processor machines, especially sharedmemory systems where the communication of data between processors does not need to be planned in advance, because distinct sub-problems can be executed on different processors."}, {"section_title": "Empirical Results", "text": ""}, {"section_title": "Synthetic data", "text": "We design a synthetic data example to compare the performance among three approaches: SCAD2TV, GraphNet, and TV-1 in terms of both prediction and selection errors.\nData Generation. In our setting, \u03b2 = (\u03b2 0 , \u03b2 1 , \u03b2 2 ) where each \u03b2 j is a 64 \u00d7 64 image (See the left panel of Figure 3 ). The covariate is X = (1, X 1 , X 2 )\nT and each X j is generated from a uniform distribution between 0 and 2. The spatial field \u03b7(\u00b7) is generated from a zero mean Gaussian random field. The error process (\u00b7) is the white noise with mean zero and variance \u03c3 2 . Two noise levels are adopted at \u03c3 = 1, 0.1. The sample size is n = 100. Applying SCAD2TV. In order to examine the performances of three methods, SCAD2TV, GraphNet, and TV-1 , we have generated 100 datasets for each setting. For each dataset, we obtain the coefficient image estimates\u03b2 from these three methods. The selection rate is define as SR = 1 |S| \nand the mean squared error is defined as\nwhere |S| = 4096 is the total number of pixels and the\u0176 i are the predicted images. Practical Consideration. Smoothing parameters \u03bb, \u03b3 can be selected by using the K-fold cross-validation (CV). However, its computational time can be long even under current computing facilities. In our experiment, we have tested a few different values for the tuning parameters such as \u03bb = 1, 2, . . . , 10 and \u03b3 = 0.1, 0.2, . . . , 0.9. We find \u03b3 = 0.5 is a good balance for the estimation. The value of \u03bb is related to our expectation of ROI. If the ROI has a sharp boundary and the values do not change much inside ROI, we can use a large \u03bb. Otherwise, a smaller \u03bb would be preferred. We choose \u03bb = 5, \u03b3 = 0.5 and \u03c1 = 1.\nResults. For each setting, the experiments using SCAD2TV, GraphNet, TV-1 are repeated 100 times. Figure 3 displays the estimates of coefficient images from one realization. We note that SCAD2TV provides the solution almost exact the same as the truth. TV-1 can keep the sharp boundary but provide biased estimates inside the active zone. GraphNet displays blurred estimates for both active zone and zero sub-regions. The average of the selection rates and the MSEs are reported in Table 4 .1. It is noted that, in terms of the selection rate, SCAD2TV performs better consistently than the other two methods. On the other hand, in terms of the prediction error, SCAD2TV and TV-1 are similar to each other, and GraphNet gives the highest MSE. "}, {"section_title": "Hippocampus Data", "text": "Dataset. To illustrate the usefulness of our proposed model, consider anatomical MRI data collected at the baseline by the Alzheimers Disease Neuroimaging Initiative (ADNI) study, which is a large scale multi-site study collecting clinical, imaging, and laboratory data at multiple time points from healthy controls, individuals with amnestic mild cognitive impairment, and subjects with Alzheimers disease. Given the MRI scans, hippocampal substructures were segmented with FSL FIRST [11] and hippocampal surfaces were automatically reconstructed with the marching cube method [8] . We adopted a surface fluid registration based hippocampal subregional analysis package [14] , which uses isothermal coodinates and uid registration to generate one-to-one hippocampal surface registration for surface statistics computation.\nIn the dataset, we have total 403 observations. For each subject, it includes a 150 \u00d7 100 2D representation of left hippocampus and 4 covariates: gender (female=0 and male=1), age (55-92), disease status (control=0 and AD=1), and behavior score . The goal is to identify local regions of the response image associated with each covariate.\nApplying SCAD2TV. We have applied our D&C learning algorithm to this dataset. We divide each response image into 150 overlapped sub-images. We execute the algorithm sequentially in a single machine. The algorithm spends about 10 secs for each partition, and takes 25 minutes to get the final estimation of \u03b2's. The estimated coefficient images are presented in the top panel of Figure 4 . Results. Our purpose is to identify the local regions where the response hippocampus image is associated with each individual covariate. Among all of the covariates, we are particularly interested in the association between the response hippocampus image and the disease status (Control vs AD). From the top panel of Figure 4 , it is interesting to notice that gender has no effect on the response image, and for other three covariates SCAD2TV has been successfully identify the local active regions. We take a close investigation on the coefficient image corresponding to the disease status. The left panel of Figure 5 displays the estimate of \u03b2 3 . The sub-regions in red indicate the active zone and the region in orange is the zero sub-region. In general, the AD patients have lower pixel values in the response hippocampus image. The right two panels are the mean response images for both health control and AD. The mean difference is consistent with our estimation of \u03b2 3 .\nWe extract the pixels within the ROI for health controls and AD, and apply hypothesis testing to test if their difference is significant. By applying hypothesis testing on each pixel in the ROI, all of them are different between health controls and AD at the significance level 5%, and 99.85% of the pixels in the ROI are different at the significance level 1%. The result justifies our ROI selection is indeed the region to differentiate between health controls and AD.\nComparison with TV-1 and GraphNet. We obtain the estimates of the coefficient images for TV-1 and GraphNet, which are presented in the middle and bottom panels of Figure 4 . These three methods overall detects similar regions. Both TV-1 and GraphNet display more blocky active regions, whereas SCAD2TV keep the active zone with sharp boundaries. We also divide our dataset into 5 parts to compare the prediction performance where each dataset contains around 80 observations. Every time we use 4 of them as the training data, and make prediction on the remaining testing data. The averages of the MSEs are computed for each methods, which are reported in Table 2 . The MSEs are similar to each other and SCAD2TV displays a slightly better prediction power.\nSCAD2TV TV-1 Graphnet MSE 0.5476 0.5482 0.5491 Table 2 : The MSEs for three methods"}, {"section_title": "Conclusion", "text": "We have introduced a new region-selecting sparse non-convex penalty, SCAD2TV, which enforces large regions of zero sub-images and extracts non-zero active zones simultaneously. Efficient algorithm and the distributed algorithm have been developed. Numerical examples are presented and the experimental results are superior or competitive with other state-ofthe-art approaches such as GraphNet and TV-1 . We have so-far focused on 2D images. It should be noted that our method works for 3D images as well. We are currently implementing our algorithm in the distributed platform such as Apache Spark. We have discussed the application for image-on-scalar regression models. This new framework may also be applied to the image clustering and image classification problems, which assume that only small regions of the images have significant effects on clustering and classification."}]