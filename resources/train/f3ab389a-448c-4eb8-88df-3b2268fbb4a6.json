[{"section_title": "Table Of Contents", "text": "List of Figures ............................................................................................................vi List of Tables ...........................................................................................................viii List of Abbreviations ..................................................................................................ix Abstract.......................................................................................................................x Chapter 1 Introduction................................................................................................ Figure SST versus Power Dissipation Index from (Emanuel, 2005) Webster et. al. 2005..........................93 Figure 45 Storm decay approaching landfall from Resio et al. 2007...............................96 Figure 46 Surge differences between Storm 870 and 1000 year return levels (feet)...... 102  "}, {"section_title": "List of Figures", "text": ""}, {"section_title": "List of Tables", "text": ""}, {"section_title": "Abstract", "text": "The most critical hazards impacting the world today are the affects of climate change and global warming. Scientists have been studying the Earth's climate for centuries and have come to agreement that our climate is changing, and has changed, many times abruptly over the history of our planet. This research focuses on the impacts of global warming related to increased hurricane intensities and their surge responses along the coast of the State of Louisiana. Surge responses are quantified for storms that could potentially occur under present climate but 50 years into the future on a coast subjected to current erosion and local subsidence effects. Analyses of projected hurricane intensities influenced by an increase in sea surface temperatures (SSTs) are performed. Intensities of these storms are projected to increase by 5% per degree of increase in SSTs. A small suite of these storms influenced by global warming and potentially realized by abrupt climate changes are modeled. Simulations of these storms are executed using a storm surge model. The surges produced by these storms are significantly higher than surges produced by presentday storms. These surges are then compared to existing surge frequency distributions along the Louisiana coast. KEYWORDS: abrupt climate change, global warming, hurricanes, storm surge Chapter 1 Introduction"}, {"section_title": "Background", "text": "On the 29 th August 2005 the world changed. Katrina was by far \"the storm of the century\" for the Gulf Coast. Then on the 24th September, less than a month later, Rita struck western Louisiana and eastern Texas. Not one, but two immense storms struck in relatively close proximity within less than one month. Katrina caused well over an estimated $100 billion in economic damages alone. But these storms were only two of a total of twenty six named storms of the year 2005. This was the very first year the alphabetical list of names was not long enough (the list has only 21 names since it excludes those beginning in Q, U, X, Y or Z). There were three Category 5 storms (Katrina, Rita, and Wilma) in one year. The last year to record 3 Category 5 storms was 1851. In 2005 Wilma was the most intense storm ever recorded in Gulf of Mexico. Katrina devastated the city of New Orleans and the surrounding metropolitan area. Levees were breached in many locations in areas of St. Bernard and Plaquemines Parish, but the major New Orleans city flooding was due to the overtopping and breaks in the now infamous 17 th Street and London Avenue Canal floodwalls. This catastrophe was not supposed to happen. Levees were breached in many of the surrounding areas including St. Bernard and New Orleans East. The floodwalls along the 17 th Street Canal were estimated to have failed about 9:45 am on the 29 th August, and the water could not be stopped. The storm and flooding caused so much destruction and the tragedy that it became imperative to resolve the in detail the entire reasons and sequence of events that enabled such calamity to occur. fundamental questions about the performance of the hurricane protection and flood damage reduction system in the New Orleans metropolitan area. This information is being used as it is developed to assist in the reconstitution of hurricane protection in New Orleans in the ongoing repair phase and to form a foundation for more effective hurricane protection in the future in New Orleans and in other parts of the nation that face similar threats."}, {"section_title": "IPET", "text": "The IPET was composed of 10 tasks, of which Task 4 was the Storm Surge Modeling and Wave Analyses. Task 4 scientists and engineers used the Advanced Circulation (ADCIRC) model for storm surge computations. (Luettich et. al. 1992;Westerink et al. 1992) ADCIRC is a state of the art program for solving the equations of motion of moving fluid on a rotating earth. Essentially, ADCIRC computes water surface elevations and currents in coastal oceans, estuaries, lakes, and rivers. Information on the ADCIRC model is provided in Chapter 2. Congress also directed the Corps of Engineers, New Orleans District, in partnership with the State of Louisiana, to initiate a 24-month endeavor, the Louisiana Coastal Protection and Restoration (LACPR) Project. The LACPR project was mandated to identify, describe and propose a full range of flood control, coastal restoration, and hurricane protection measures for South Louisiana. Components of this project include characterization of a Saffir-Simpson Category 5 storm which is a 1-5 rating based on the hurricane's wind speed and intensity. This dissertation work will complement LACPR efforts and perform simulations some of the storm characteristics defined by LACPR project. In addition to the impact to the people of the coast, the storms of 2005 caused great injury to the already disappearing Louisiana coastal wetlands. These wetlands and barrier islands reduce storm surge and help protect coastal cities and infrastructure. (Suhayda, 1997). The Louisiana Coastal Area (LCA) Study Report (USACE, Nov 2004) documents much of the history of the causes and disappearance of the wetlands. LCA Study Report, Appendix B, is the Historic and Projected Coastal Louisiana Land Changes 1978 -2050. The USGS documents in Appendix B, the land loss (and some gain) from 1956 through 2000. Additionally, the USGS projected land loss rates through 2050. According to this report, the projected land loss from 2000 to 2050 is 674 sq miles (1,746 sq km) and land gain of 161 sq miles (417 sq km). All land loss and gain features, existing and projections, were encoded as layers in a Geographic Information System (GIS). This study was completed prior to Katrina and Rita, and did not consider such devastation as caused by the record storms of 2005."}, {"section_title": "Global Climate Change", "text": "Katrina increased the debate over the global climate change caused by man made \"green house gases\" (TIME, August 2005). People wanted to know if Katrina and the other very powerful storms could have been caused by human induced global climate warming. Ultimately, it is fundamentally impossible to prove an individual storm was caused by human (CO 2 ) induced global climate change. Our weather each day is one realization of an infinite number of possibilities created by the many forces that form and create our climate. The weather is semi-random by nature and conversely, it would not be defensible to say a specific single event as Katrina was caused by the long-term natural climate cycle alone. There is much evidence that the scientific community has agreed that natural cycles alone cannot explain recent ocean warming (IPCC AR4, 2007). Due to human activities such as burning fossil fuels, clearing forests, etc., today's carbon dioxide (CO 2 ) levels in the atmosphere are significantly higher than at any time during the past 400,000 years. CO 2 and other heat-trapping emissions act like insulation in the lower atmosphere, warming land and ocean surface temperatures (Houghton et al. 2001 (Barnett et al., 2001) The PCM results provide credible weight for claims that an anthropogenic signal has been detected in the global climate model system. Although scientists agree that global warming is underway, there is significant controversy over the relation between hurricanes and climate change. Research over the past few years has indicated potential links between global warming and hurricane size and energy (Emanuel, (2005(Emanuel, ( , 2005a; Mann and Emanuel, 2006). Hurricanes are a complex phenomena and their formation is affected by sea surface temperature, wind speeds and directions, humidity, and other ocean and atmospheric conditions. A 2004 study analyzed the relationships of today's storms with simulated storms for increased concentrations of CO 2 (the primary greenhouse gas) (Knutson and Tuleya, 2004). A total of nine different global climate models were executed, all with the same amount of increased CO 2, a +1% yr -1 for 80 years. These GCM results formed boundary conditions for an idealized hurricane model, the Geophysical Fluid Dynamics Laboratory (GFDL) R30 coupled model. Approximately 1300 5-day simulations were performed with a high resolution GFDL R30 model. Results from all of the simulations were aggregated and averaged. They indicate a 14% increase in central pressure fall, a 6% increase in maximum surface wind speed, and an 18% increase in average precipitation rate within 100 km of the storm center. Current hurricane potential intensity theories were also applied to the climate model environments in this study. These theories show an average increase in intensity (pressure fall) of 8% (Emanuel convective parameterization model) to 16% (Holland model) or the high-CO 2 environments. Convective available potential energy (CAPE) is 21% higher on average in the high-CO 2 environments. One implication of these results is that if the frequency of tropical cyclones remains the same over the coming century, a greenhouse gas-induced warming may lead to a gradually increasing risk in the occurrence of highly destructive category-5 storms. Even more recently, a 2005 study demonstrated a statistical link of global warming to an increase in storm intensity and duration. (Emanuel, 2005) The study suggests intensity and duration relationships to increased sea surface temperatures associated with global warming, specifically during the past 10 years. During this time global average sea surface temperatures were at record levels (Online at http://www.ncdc.noaa.gov accessed April, 2006). The destructive power of storms can be measured by the total power dissipation (PD). (Emanuel, 1998). The PD is defined as where C D is the surface drag coefficient, \u03c1 the surface air density, |V| is the magnitude of the surface wind, and the integral is over radius to an outer storm limit given by r 0 and over \u03c4 , the lifetime of the storm. Due to the difficulty in evaluating these integrals and other reasons, Emanuel defined a simplified Power Dissipation Index (PDI) as where V max is the maximum sustained wind speed at the 10 meter altitude. The PDI was computed for all storms since 1950. It is a combination of each storm's maximum wind speeds and storm duration. It was found that during the last 30 years, the destructive power of storms, the PDI, has doubled in the Atlantic and Pacific. But Emanuel did not just find the upward trend of storm intensity. What he also found startled the climate community. In the area of the Atlantic where most hurricanes start, the power released during the lifetimes of the storms is \"spectacularly well correlated with sea surface temperatures\" (Kerr, 2006). Figure 1 displays Emanuel (2005) results for the North Atlantic. Emanuel (2005) has shown that the hurricane intensity and sea surface temperatures have risen over the last 45 years. Many scientists now believe that the warming of the Atlantic may be driven at least in part by rising greenhouse gases."}, {"section_title": "Figure 1 SST versus Power Dissipation Index from (Emanuel, 2005)", "text": "Both SST and PDI have been scaled using a constant offset for ease of comparison. SST Scaled August-October temperatures and PDI is meters 3 / seconds 2 However, to date, science has not been able to acceptably link worldwide storm frequency with global warming (Webster et.al, 2005). Each ocean basin has its multiyear cycles of storm activity. While the total number of storms in the tropics remained similar through time, one study suggests that the percentage of category 4 and 5 hurricanes have increased over the past 30 years (Trenberth, 2005). Trenberth states that despite the enhanced activity, there is still no sound theoretical basis for defining if and/or how anthropogenic climate change affects hurricane numbers or tracks, and if they hit land. This issue was so intense, that the leading US meteorologist Chris Landsea resigned from the IPCC, complaining that Trenberth had supported a link between warming and storms in a previous press conference (Schiermeier, 2005). Landsea (2005) in review of Emanuel (2005) results presented three critical issues. The first issue was the plotting of unfiltered end points of the PDI time series which should have been deleted. Emanuel plotted these end points which suggested the strong increase in PDI over the last few years. If the unfiltered end points were removed the indexes are similar to those previous to the 1950s. The second issued concerned the method of bias removal which reduced the tropical cyclone winds for the Atlantic by 2.5 -5.0 m/s for the 1940s-1960s. Landsea (2005) argued that the hurricane winds should be used as is with no adjustments. The third issue was the difficulty to interpret an anthropogenic signal in the Atlantic storms due to the large natural multi-decadal oscillations and the short time period of the reliable data record. He presented a PDI for 1901-2004 for only US land falling tropical cyclones which showed no evidence of long term trends ( Figure 2). Given the rapidly changing topology and land characteristics in Southern Louisiana, as well as the possible fundamental changes in climate, this research proposes to investigate the impact on Southern Louisiana through detailed hydrodynamic model studies. In light of the above background, research tasks and goals are outlined in the following sections."}, {"section_title": "Abrupt Climate Change", "text": "What is \"abrupt\" climate change? Alley defines abrupt as a change that occurs when the climate system is forced to cross some threshold, triggering a transition to a new state at a rate determined by the climate system itself and faster than the cause (Alley, 2002). Although this is accurate for a scientific definition, most people see abrupt change as any type of climate change that lasts for years or longer and has significant impact on their lives. These include change in intensity, duration, or frequency of extreme events which persist. Thus, for example, a persistent change, may be seen as an increase in the number of floods or the number and intensity of storms. \"Large, abrupt climate change have repeatedly affected much or all of the earth, locally reaching as much as 10\u00baC change in 10 years. Available evidence suggests that abrupt climate changes are not only possible but likely in the future, potentially with large impacts on ecosystems and societies.\" (Alley, 2002). Our climate changes rapidly when being forced either naturally, or, as being postulated today, by human induced greenhouse warming which may increase the possibility of abrupt, regional or global events. Before the 1990s, most scientists believed the climate changed very slowly, with gradual swings of the ice ages over tens of thousands of years or longer. But over the last few decades, geologic evidence has undisputedly shown that the climate can and has changed abruptly (NRC, 1998). Changes of up to 16 o C and a factor of 2 in precipitation have occurred in some places in periods as short as a few years or decades (Alley, Clark, 1999). Sedimentary and other \"proxy\" methods have demonstrated widespread abrupt climate changes over the last 100,000 year and beyond. The period called the \"Younger Dryas\" is the best known cold interval. The Younger Dryas (YD) began about 12,800 years ago as an interruption to the gradual global warming trend following the last ice age. The YD event ended abruptly about 11,600 years ago. Analysis of the Greenland ice cores (Alley, 2000) showed that cooling occurred in a few decade long steps, but warming at the end, happened in one large step of about 8 o C in about 10 years. Even so, additional studies of the Greenland ice core data have indicated the YD was only one of many large, abrupt, widespread climate changes. Scientists around the world give recognition to these abrupt state changes but only a small amount of knowledge and information is understood about the reasons for the abrupt state changes. We do not fully know under what conditions state changes are possible either under modern or nearfuture climate conditions. But from the evidence, we know they can be abrupt. No one has yet identified the exact cause of the YD but many scientists have associated drastic changes in the Atlantic thermohaline circulation (THC) as playing a central role in this event as well as many other past abrupt climate changes. Some theories suggest large influxes of fresh water into the North Atlantic reduced the ability of its waters to sink. This reduction in heat transport to the north allowed heat to remain in the south, thus a reduction in the THC. The implications of fluctuations in the Atlantic THC have received much attention and many efforts are being conducted to more fully understand the THC and how the ocean heat transport affects the climate. Most GCM models suggest that for the THC, or sometimes called the \"conveyor belt\" to shut down, a 4 -5 o C global warming is required. But the current warming trend is at 0.3\u00baC per decade for the period 1990 through 2005 (IPCC AR4, 2007). The current rate is approximately 0.2\u00baC per decade and is remain the same for the next two decades. GCM models show that the THC flow may be expected to slow by an average of 25% by the end of the twenty-first century, but not to shut down completely. An inter-model comparison analysis (Gregory et al. 2005), specifically on the THC, also confirmed these results under various IPCC increased CO 2 concentration scenarios. However, recent observations (Bryden et al. 2002) have estimated a 30% weakening in the overturning circulation within the last few decades. These results came as a surprise to many scientists because of the disparity with other factors such as changes in salinity, deep convection, or lack of observed cooling in the North Atlantic.  (Emanual, 2005). These results indicate the very real possibilities of increased tropical cyclone destructive capability."}, {"section_title": "Scope and Objectives of Research", "text": "The overall objective of this research is to quantify potential future storm surges, as well as storm frequency, taking into consideration global warming and in particular abrupt climate change. This research will have 3 primary objectives: (1) Quantify storm surges for the Louisiana Coast with wetland loss as projected by the USGS in the year 2050. (2) Project the impacts of abrupt climate change through creation and modeling of storms of increasing maximum intensities possible under such change to estimate future surge levels. (3) Estimate frequencies of future storms based upon minimum central pressure and radius to maximum wind, and compare surge results obtained from these future storms, to published storm surge return period levels."}, {"section_title": "2050 Storm Surge Simulations.", "text": "In order to quantify storm surges for the Louisiana Coast with wetland loss as projected by the USGS in the year 2050, the Advanced CIRCulation (ADCIRC) model will be used to perform all storm surge computations. The ADCIRC mesh representing 2050 future degraded conditions of the Louisiana zone will be used. This mesh is the same mesh created for use in the USACE Louisiana Coastal Protection and Restoration (LACPR) study. The Coastal Louisiana Ecosystem And Restoration (CLEAR) group projected land loss/gain rates and future topography based upon historic relative regional and local subsidence rates (Twilley, et al. 2008). This topography represents the future topography and bathymetry in 2050 and was transferred onto an ADCIRC mesh. The pre-Katrina topography and levee system is represented in the LACPR 2007 ADCIRC mesh. A post-Katrina, ADCIRC mesh with topography representing the Louisiana coast of 2050, will be encoded with all levee heights set to the published Corps of Engineers 100year level of protection elevations. ADCIRC executions will be performed to simulate Katrina, as well as the Category 5 storm characteristics defined by the USACE Louisiana Coastal Protection Restoration Study. Simulations of these storms will be performed on these grids which represent the Louisiana coast in 2050, to quantify surges that could occur with the predicted future land loss. The final results of these simulations will provide a quantitative assessment of future storm surges in the 2050 projected land configuration by the USGS. Areas of inundation can be compared to 2005 Katrina flooded locations. This will demonstrate the locations of more severe flooding and those with less flooding, based on an event similar to Katrina, and a potential Category 5 storm striking the Louisiana coast in 2050."}, {"section_title": "Abrupt Climate Change.", "text": "The second objective of this research is to project the impacts of abrupt climate change through modeling of storms of increasing maximum intensities. Future physically realistic storms with maximum potential intensity (Holland, 1997) and Emanuel (1987) that can be attributed to abrupt climate change will be created. Simulations of these storms will be performed and the results analyzed for impacts to the Gulf Coast, specifically the state of Louisiana and the New Orleans area.  (Dowdeswell, 2006). These results indicate a contribution from the Greenland Ice Sheet of more than 0.5 mm year -1 to the global sealevel rise. These more than double previous estimates of losses from the ice sheet to the oceans. If these numbers are correct, large influxes of fresh water will decrease the salinities and may trigger a slow down in the thermohaline circulation. Even without the affects of a THC slow down, a major consequence of abrupt climate change could be a significant rise in sea level due to glacier and ice sheet melting. For the purposes of evaluating this objective, \"abrupt\" change will be interpreted as a rise in sea surface temperature (SST) of 6\u00baC during the period from the present to 2050. For comparison if the current trend of SST persists (0.2\u00baC for 2 decades until 2027, then 0.4 \u00baC per decade) without an abrupt change the SST would increase by approximately (2 * 0.2) + (2 * 0.4) = 1.2\u00baC by 2047. For the \"A1F1\" high emissions scenario (IPCC 2001a), global mean temperature increase is estimated to be 4\u00baC by 2100, or 2\u00baC by 2050."}, {"section_title": "Frequency Analyses.", "text": "Storms projected to form under abrupt climate change conditions will have different characteristics than those of the current climate. These most likely include higher intensities in terms of stronger wind speeds and lower central pressures. Consequently these storms will produce higher surges. Analyses will be conducted to determine how these storms and surges fit into relations published for future climate conditions.  Where \u03b7 max is the maximum surge, p a is the ambient pressure (millibars), and p 0 is the minimum central pressure (millibars). Conner et. al. (1957) Jelesnianski et. al. (1972) improved computer based modeling with advances in computer hardware and software that enabled the creation complex computer code which was called \"SPLASH\". SPLASH had three basic steps to compute the maximum surge. The first step calculated a maximum surge based on the empirical formulas derived by Conner et. al. (1957) and Harris (1963). There were five constraints: (1) radius of maximum winds is constant; (2) for each set of storms, only the pressure drop is varied; (3) all storms have steady motions, 15 mph and the track is normal to the coast; (4) a standard simplified coast is used for all storms; (5) all storms make landfall at 30\u00baN. The second step was an adjustment to the first computation by introducing a shoaling factor to correct for local coastal bathymetry. The third step was to correct for storm direction and storm speed. The final equations established were: where S p is the preliminary maximum surge estimate, F G and F E are the Gulf and East coast shoaling factors, and F M is the correction factor for storm motion. During the mid 70's, the FEMA SURGE model was created by Tetra Tech, Inc. and called TTSURGE (Camp Dresser et. al. 1985).  (Massey et. al. 2007). The late 1980's was the last time the FEMA SURGE model was used in new or updated flood insurance studies to revise FIRMs (Massey et. al. 2007). Although in the early 1990's coastal engineering firms were updating the model to run on desktop computer platforms. The 1990's saw the development of the SLOSH model by Techniques Development Laboratory of the National Weather Service (Jelesnianski et. al., 1992). SLOSH is a 2D numerical-dynamical tropical storm surge model created for real time forecasting of hurricane storm surges. Since its creation it has been extensively used by  (Luettich, et. al. 1992). The purpose was: (a) to provide a means of generating a database of harmonic constituents for tidal elevation and current at discrete locations along the east, west, and Gulf of Mexico coasts, and (b) to utilize tropical and extratropical global boundary conditions to compute frequency indexed storm surge hydrographs along the US coasts. The database was to provide site-specific hydrodynamic boundary conditions for use in analyzing the longterm stability of existing or proposed dredge material disposal sites. ADCIRC was developed to simulate hydrodynamic circulation along shelves, coasts, and within estuaries. To allow long numerical simulations (over one year) over very large computational domains (for example the entire east coast of the United States, including the Gulf of Mexico), ADCIRC was designed to have high computational efficiency and has been extensively tested for both hydrodynamic accuracy and numerical stability (USACE New Orleans District, ITR, 2003); Westerink, et. al. 2004). Since its inception in the early 1990s, ADCIRC has evolved in many ways that range from its modeling capabilities, numerical algorithms, and computational efficiencies. Today it is a state of the art finite element model that enables the use of highly flexible, unstructured, and large domain meshes (or grids). Many agencies and organizations are using ADCIRC for a wide range of applications including modeling tides and wind driven circulation, analyzing hurricane storm surge and flooding, dredging and material disposal studies, estuarine hydrodynamic studies, as well as larval transport studies, and near shore marine operations. An important aspect of the model is that it can simulate tidal circulation and storm-surge propagation over very large computational domains while simultaneously providing high resolution in areas of complex shoreline configuration and bathymetry.   "}, {"section_title": "Model Formulation", "text": "In two dimensions, the model is formulated using the depth-averaged shallow water equations for conservation of mass and momentum. Furthermore, the formulation assumes that the water is incompressible, hydrostatic pressure conditions exist, and that the Boussinesq approximation is valid. Using the standard quadratic parameterization for bottom stress and neglecting baroclinic terms, the following equations are implemented in primitive, non-conservative form, and expressed in a spherical coordinate system, and incorporated into the model (Flather 1988;Kolar et al. 1993;Luettich and Westerink, 2004). \u03c4 s\u03bb and \u03c4 s\u03d5 = applied free surface stresses in the longitudinal and latitudinal directions, respectively, and \u03c4 = bottom shear stress and is given by the expression is the bottom friction coefficient. The momentum equations (Equations 2.4 and 2.5) are differentiated with respect to \u03bb and \u03c4 and substituted into the time differentiated continuity equation (Equation 2.6) to develop the following Generalized Wave Continuity Equation (GWCE): The ADCIRC-2DDI model solves the GWCE in conjunction with the primitive momentum equations given in Equations 2.4 and 2.5. The GWCE-based solution scheme eliminates several problems associated with finite-element programs that solve the primitive forms of the continuity and momentum equations, including spurious modes of oscillation and artificial damping of the tidal signal. Forcing functions include time-varying water-surface elevations, wind shear stresses, atmospheric pressure gradients, and the Coriolis affect. The ADCIRC model uses a finite-element algorithm in solving the defined governing equations over complicated bathymetry encompassed by irregular sea/ shore boundaries. This algorithm allows for extremely flexible spatial discretizations over the entire computational domain and has demonstrated excellent stability characteristics. The advantage of this flexibility in developing a computational grid is that larger elements can be used in open-ocean regions where less resolution is needed, whereas smaller elements can be applied in the near shore and estuary areas where finer resolution is required to resolve hydrodynamic details."}, {"section_title": "ADCIRC Louisiana mesh validation", "text": "The Louisiana ADCIRC mesh has approximately 80% of the nodes and elements along the Louisiana coast and highly resolved topographic and bathymetry definition extends from portions of Texas, through Louisiana, Mississippi, and into Mobile Bay. All ridge features such as levees, roads, railways, and raised linear features which can impede flow are either specifically represented in the finite elements or by internal boundaries. Internal boundaries were modeled as weirs and represent a non-hydrostatic flow scenario. Once water levels overtopped these barrier heights, the flow across the crest is computed using the standard weir formula. All Federal levees are captured, as well as numerous state, local, and private levees. The computational grid (Figures 3,4, and 5 ) has been constructed to provide sufficient resolution for the tidal, wind, atmospheric pressure, and riverine flow forcing functions from the ocean basins to the coastal floodplain. A minimum spacing of 100 feet was used since a 0.5 Courant, Fredrichs, Levy parameter is best for the ADCIRC model. An optimum time step equal to 1 second was used for all storm simulations using the Louisiana mesh. The mesh was validated using 3 historic storms: Katrina, Rita, and Andrew. Water levels and time series results for each of these storms were compared against measured high water marks (HWMs) and observed hydrographs captured for these events. Tidal validation simulations were also performed and consistently produced R 2 values greater than 0.9 for the Louisiana, Mississippi-Alabama coast (FEMA and USACE 2008). HWMs for Katrina and comparison results are documented in the IPET report (USACE, 2007). A straight line fit through all of the USACE HWMs produced a slope of 1.0007. Thus, the model was over predicting surge by only 0.07%. The average error is -0.14 foot while the average absolute error is 1.31 feet. A correlation coefficient (R 2 ) equal to 0.9317 was achieved. A group of additional HWMs were collected by URS, Inc. for FEMA and when using all of these HWMs the slope of the line through all data points was equal to 1.0315. This indicates the model is on average over-predicting surge by 3.15 percent. The average error was 0.45 foot while the average absolute error was 1.24 feet. The correlation coefficient (R 2 ) was equal to 0.9460. Results for Rita and Andrew are also well correlated with HWMs and hydrographs from these storms. Detailed results for Katrina and Rita simulations are in \"Flood Insurance Study: Southeastern Parishes, Louisiana, Intermediate Submission 2, FEMA and the U.S. Army Corps of Engineers (USACE) New Orleans District, 2008."}, {"section_title": "ADCIRC Numerical Implementation", "text": "The ADCIRC model and Louisiana mesh validation results (Section 2.2.2) demonstrate the local accuracy of the ADCIRC model even though it is globally but not locally mass conservative. But, there is no standard or consensus on the best way to check local mass conservation. ADCIRC implements a rigorous method that directly integrates the continuity equation over a control volume that coincides with a finite element or an entire cluster of finite elements. This method provides diagnostics and ability to determine if transport computations driven with these flows will result in localized mass losses/gains that result in artificial oscillations. With this tool, local mass balance errors have been effectively minimized and hence local truncation errors (FEMA and the USACE, 2008). Local mass conservation is minimized by use of a local \"\u03c4 0 \" or Generalized Wave-Continuity Equation (GWCE) weighting factor that weights the relative contribution of the primitive and wave portions of the GWCE. Local mass conservation was checked at the element level for the Louisiana mesh and it was found that over 93% of the domain had a relative error of less than 0.01% in magnitude and 72% of the domain had a relative error of less than 0.001% in magnitude (FEMA and the USACE, 2008). These errors are within tolerances normally associated with modeling of these complex systems. The GWCE in ADCIRC is implemented by combining the differentiated momentum equation in its conservative form with the temporally differentiated continuity equation multiplied by a numerical parameter \u03c4 0 (Kolar and Westerink, 2000). The GWCE and the momentum equations are solved sequentially. The finite element solution is implemented using Lagrange linear finite elements in space and 3 and 2 level schemes in time for the GWCE and momentum equations respectively. Details of the discretization and solution techniques used in ADCIRC are given in Luettich and Westerink, 1992;Westerink, 1993;Luettich and Westerink, 2004. Wetting and drying processes are implemented based on a combination of nodal and elemental criteria (Luettich and Westerink, 1999;Dietrich et. al. 2005). The following is a brief overview of the wetting and drying algorithm following from (FEMA and USACE, 2008). \"All nodes within an element must be wet in order for that element to be included in the hydrodynamic computations. Two parameters are used to define the wetting/drying criteria. The first parameter, H0 defines the nominal water depth for a node to be considered wet. The second parameter is a minimum velocity, Umin, that must be exceeded for water to propagate from a wet node to a dry node. Nodes are defined as initially dry if they lie above the defined starting water level or if they are below the starting water level but are within pre-defined regions, such as ring levees as in protected areas of New Orleans. Wetting is accomplished by examining each dry element with at least two wet nodes with depth greater than 1.2 H0 (ensuring sufficient water depth to sustain flow to the adjacent node). The velocity of the flow from the wet nodes toward the dry node along each element edge is computed based on a force balance between the free surface gradient and the bottom friction. If this velocity exceeds Umin, then the third node and the element are wetted. Finally, a check is made for elements that are surrounded by wet elements to ensure sufficient water column height (greater than 1.2 H0 at all flow originating nodes) to allow flow to occur through these elements. While a purely nodal wetting scheme will allow these elements to wet, the elemental check may prevent this from occurring. For hurricane storm surge inundation, wet/dry parameters that are relatively unrestrictive have been found to be most effective: H0 = 0.10 m, and Umin = 0.01 m/s. It is critical that all wet/dry checks be done at a small enough time interval so that the wetting/drying algorithm is not Courant surpassing. This latter condition artificially retards the wetting front as the surge progresses inland and the surge height will excessively build up behind the wetting front. Practically, this implies performing wet/dry checks at each model time step.\" (FEMA and USACE, 2008) (Thompson and Cardone, 1996). Additional input requirements are wind surface reduction factors, Manning's n roughness coefficients, and sea surface submergence state. These parameter definitions and formats can be found at http://www.adcirc.org."}, {"section_title": "ADCIRC Input and Forcing Functions", "text": "Other primary ADCIRC parameters such as time step and tidal factors are specified and defined in a \"control\" file which is also described on the ADCIRC web site."}, {"section_title": "Chapter 3 Global Climate Change", "text": ""}, {"section_title": "Introduction", "text": "Only within the past century or so have we come to fully realize how much the earth's climate has changed over the long time scales of our planet's evolution. Many scientists and people of all walks of life have labored and endeavored to understand our climate. A complete history of these efforts and results would take volumes. This chapter will focus primarily on the summaries and results of the reports produced by the International Panel on Climate Change (IPCC) of which the first was published in 1990. A very brief outline of major advances in this effort are covered within the next few paragraphs. Throughout history some people believed that humans could affect climate in some way or another but had no way of actually proving any theories. In the early 19 th century many Americans believed cutting down forests brought more rain to a region. Discovery of the ice ages opened the eyes of many scientists and demonstrated that the earth's climate did change and very drastically. However, it was thought that the time scales were over tens to thousands of years, and no one knew why or what forced these changes. It occurred to several scientists that one of these could be the composition of the atmosphere. Joseph Fourier was one of the first to realize that energy in the form of light from the sun penetrates the atmosphere to heat up the surface but could not as easily escape back into space. (Online at http://www.aip.org/history/climate/index.html) He theorized the air absorbed invisible heat rays (infrared radiation) from the surface. Once heated, the warmer air radiates heat back down to the surface thus keeping it warm. This effect later came to be called the \"greenhouse effect\". The question of the explanation of the ice ages intrigued other scientists such as John Tyndall who later (1850's) identified several gases (water vapor, carbon dioxide) that he proved in his laboratory did trap heat rays. Following Tyndall, Svante Arrhenius also sought to solve the mystery of the ice ages and calculated that by cutting the amount of CO 2 in the atmosphere in half, could lower the temperature in Europe by 5-7 O C or approximately 7-9 O F, which was the temperature of the ice ages. (Online at http://www.aip.org/history/climate/index.html) But this answer was a solution to the mystery only if the large changes in the atmospheric composition were actually possible. In looking for this answer, Arrhenius brought up the possibility that as humans burned coal that added carbon to the atmosphere, the atmosphere would heat up and raise the average temperature of the earth. But scientists found many reasons to doubt that human emissions could actually raise the temperature of the earth and if possible, it would take thousands of years. During the 1930's scientists and most people realized that the U.S. and North America had warmed significantly during the last 50 years. Most thought is was a natural cycle except for Guy Stewart Callendar who insisted that is was the greenhouse warming effect and the warming would continue. In the 1950's scientists performed more detailed research into the questions raised by Callendar, and did so with better techniques and calculations (and funding from the military). Results showed that accumulation of carbon dioxide in the atmosphere could result in increased global warming. Then in 1960, exact and careful measurements were performed by Charles David Keeling both in Antarctica and on top the Mauna Loa volcano in Hawaii. Continued measurements and subsequent publications proved inexorably that carbon levels were becoming noticeably higher each year. The now famous \"Keeling curve\" has become one of the most iconic symbols of the greenhouse effect. Over the next decade scientists created simple mathematical models of the climate that showed feedbacks which could make the climate very variable. The field of paleoclimatology was born with people finding ways to retrieve ancient temperatures using ancient pollens and fossils. They found evidence the climate had changed in a small a span as a few centuries. The new general circulation climate models were even able to reproduce these climates and the models themselves were the results of significant efforts to predict the weather. However, altogether scientists saw no need for political or policy actions but just the need for more understanding and research efforts. Environmentalism of the 1970's brought ideas on the tremendous negative effects humans had on the earth and our climate. The media seized upon reports by scientists that showed the dust and smog particles being put into the atmosphere could block sunlight and cause general cooling. There became confusion with some reports predicting large scale warming with the ice caps melting, and others which pointed to a doomsday view to a new ice age. All scientists agreed that much more research was needed because we clearly did not fully understand the climate system. Research began a great pace to obtain detailed measurements and observations of all aspects of the climate and our environment. The world soon began to realize the climate was an amazingly complex system which was influenced by very many factors. These included volcanic eruptions, solar flares, and subtle changes in the Earth's orbit. It became apparent that even small changes might initiate large and severe shifts over to new climate regimes. The now famous \"chaos\" theory, developed by Lorenz (1963) explained that the most insignificant change of initial conditions might randomly bring a huge change in the future climate. \"Climate may or may not be deterministic,\" he concluded. \"We shall probably never know for sure.\" (Lorenz, 1963) These theories were later supported by analyzes of the Greenland and Antarctica ice cores which showed large and abrupt temperature changes throughout the history of our climate. The 1980's and 1990's brought about significant advancements in computer hardware and software technology. This enabled tremendous improvements in numerical climate models. These models showed just how fast changes could occur in the atmosphere and ocean and also predicted storms, droughts, sea level rise and other disasters. However, the models did not capture all climate aspects equally. Assumptions and parameterizations had to be made about clouds and other factors which prominent scientists pointed out to dispute the reliability of the results. There was a need for more coherent and organized approach but the research remained disparate and unorganized. During this time the unexpected discovery of other gases levels in the atmosphere were rising and were related to depletion of the ozone layer. Also, in the 1980's global temperatures were being observed to be on the rise again. It was 1988 that scientist's claim first caught high public attention as 1988 was claimed the hottest summer on record (now since exceeded by several years in the 1990's and 21 st century). There was a constant and highly aggressive debate on what actions to take and how much governments should intervene. Eventually, the world's governments created a panel to provide the most reliable possible advice obtained from thousands of climates experts and scientists. This became the International Panel on Climate Change who established a consensus that it was much more likely than not that our world in fact is in a global warming state.\nHurricane frequency is now one of the most highly visible and debatable aspects of climate change. The issue of frequency is compounded by the fact that the time series of reliable tropical cyclone databases are simply too short for detecting trends in the frequency of extreme events (Landsea et al., 2006). "}, {"section_title": "International Panel on Climate Change (IPCC) 1 st through 3 rd", "text": ""}, {"section_title": "Assessments", "text": "The World Meteorological Organization (WMO) and the United Nations Consider the example of global cooling resulting from the eruption of Mt. Pinatubo which provided important tests of specific aspects of some climate models (Hansen et al., 1992). Another example is the subsequent measurements and observations of temperatures compared to the projections of the FAR, SAR, and TAR. Figure 6 shows that FAR model projections were higher than the SAR and TAR, and also higher than actual observations. The actual observations were above the SAR but within and near the upper range of the TAR (IPCC, 2001a). Over the years IPCC efforts were required to increase dramatically to keep pace with the ever increasing amount of climate related research. Between 1965 and 1995, the number of articles published each year in atmospheric science journals tripled (Geerts, 1995). Stanhill 2001 Temperatures are relative to the mean 1961 to 1990 values, and as projected in the FAR (IPCC, 1990), SAR (IPCC, 1996) and TAR (IPCC, 2001a). The \"best estimate\" model projections from the FAR and SAR are in solid lines with their range of estimated projections shown by shaded areas. The TAR did not have \"best estimate\" model projections but rather a range of projections. Annual mean observations are shown in black circles and the thick black line shows decadal variations obtained by smoothing the time series using a 13-point filter. Over the years, collection of observed temperature data and analyses techniques have changed; however they all show a high degree of consistency.   (Mercer, 1978). With developments in the understanding of the oceanic and atmospheric circulations, scientists now better understand the strength and variability of global ocean circulation but there is still debate to its complete role in the climate. To better understand the climate and these factors, climate scientists now rely heavily upon numerical models. Due to the speed and power of today's supercomputers, model complexity has also increased by including more and more components, increasing the length of simulations, as well as spatial resolutions.  This chapter presents a brief summary of AR4 findings. However the AR4 provides new uncertainty guidance and the likelihood of a result or outcome. The standard terms used for levels of confidence are: Standard terms to define the likelihood of an event are: The following are listing several of the primary findings of the IPCC AR4: \u2022 Greenhouse gas concentrations of carbon dioxide, methane, and nitrous oxide have increased significantly due to human activities and now far exceed values obtained from ice cores spanning thousands of years. Carbon dioxide is the most important gas (Figure 9) and now exceeds a pre industrial level of 280 ppm to 379 ppm in 2005. Natural levels derived from ice cores range from 180 to 300 ppm over the last 650,000 years. \u2022 Global warming is now undisputable as evident in measured increases in the average global air and ocean temperature as well as the world wide melting of glaciers and ice sheets, and the rise in average global sea level ( Figure N.2). \u2022 Numerous long term climate changes have occurred across continental and regional scales. These include widespread changes in precipitation, ocean salinity, wind patterns, and extreme weather including droughts, heavy precipitation, heat waves, and the intensity of tropical cyclones. (Figure 9 and Table 4 and 5). \u2022 Since the TAR, major advancements have been realized in the assessment of climate change projections due to the significant improvements in numerical climate models, the broader range of models, and the larger number of simulations available. Model simulations cover a range of future scenarios including idealized emissions or concentrations. These include SRES 14 illustrative marker scenarios for the 2000 to 2100 year period, as well as concentrations held constant after year 2000 or 2100. \u2022 For the next 20 years, a warming of about 0.2 O C per decade, or 0.4 O C by around 2027, for the range of SRES 14 scenarios. Continued greenhouse gas emissions at or above the current rates would cause further warming and induce many changes in the 21 st century that would very likely (>90% probability) be larger than those of the 20 th century. (Table 6 and Figure 10) \u2022 Very likely of slow down of meridional overturning current or THC \u2022 Anthropogenic warming and sea level rise would continue for centuries due to the time scales associated with climate processes and feedbacks, even if green gas concentrations were stabilized to those of today.      Greenland (Dansgaard et al., 1984) and Antarctic ice cores, these efforts have become even more important. Within these contexts, 'abrupt' designates regional events of large amplitude, typically a few degrees Celsius, which occurred within several decades -much shorter than the thousand-year time scales that characterize changes in astronomical forcing (IPCC 2007). Further analyses of ice cores during the 1990's identified numerous changes (Dansgaard et al., 1993), that were abrupt (Alley et al., 1993) and of large magnitude. These changes are now called the Dansgaard-Oeschger events. Now even more stunning evidence of really fast large climatic shifts within 1 to 3 These enabled a high resolution record which allowed them to precisely define the shifts. Normally, ice core records are 'fuzzy' at these time scales and this is the first publication with such high resolution results. In  (Frappier et al. 2007). From this brief summary above, one can begin to see as we continue our efforts to retrieve prehistoric climate information, large scale abrupt climate changes abound throughout the earth's history. Accumulation of evidence points continues to point to abrupt changes that are possible within less than 10 years rather than on a scale of 100 to 1000 years as previously surmised.  (Britsch, Dunbar, 1993). With the era of the satellites, the U.S. Geological Survey, the U.S. Army Corps of Engineers and Louisiana state agencies successfully used LANDSAT imagery to quantify land loss and gain from the 1970's through 2002 and used these historic rates to project coastal wetland areas in 2050 (Barras et al, 2007). This work was performed in conjunction with the Louisiana Coastal Area (LCA) project. The Corps of Engineers, USGS, and State of Louisiana quantified land loss rates for delineated sub domains, based on historical photos and satellite data to produce a projected land loss/gain for the year 2050. In support of LCA, the Coastal Louisiana Ecosystem and Restoration (CLEAR) Program performed additional analyses and produced datasets quantifying ecological and wetland changes for several future scenarios. The future landscapes were modeled based upon with state, federal, and local protection and restoration projects in place, as well as with \"no increased actions\" (Twilley et al., 2008). A 2050 landscape was created based on a \"degraded\" coastal zone. Using this degraded landscape, ADCIRC 2050 geometry was created as well as a friction layer based on the Manning's n values of the wetland features. Figure 12 shows the difference between ADCIRC mesh which represents 2050 topography and the ADCIRC mesh which represents 2007 conditions in the south eastern Louisiana area. The \"2007\" denotes the configuration of the levee heights surrounding the New Orleans area and not specifically the topography. Topography and bathymetry encoded in the  "}, {"section_title": "Analyses", "text": "What is potentially the most intense storm capable of striking the Louisiana coast? Much work has been done to answer this question. Emanuel (1987) was the first to introduce the concept of the maximum potential intensity for a tropical cyclone and link greenhouse gas-induced warming to potential intensity increases. The maximum potential intensity (MPI) can be defined as the upper limit of intensity that a TC can achieve based on conditions such as sea surface temperature, regional surface temperatures, and moisture content. But the MPI does not include dynamic effects such as wind shear. Given the today's climatology, a theoretical maximum potential intensity (MPI) was estimated as 880 mb (Resio, 2007). This MPI was obtained by combining the results of Tonkin et al. (2000) and Schade (2000). Tonkin et al. performed a comparison of the Emanuel (1986Emanuel ( ,1991 and Holland (1997) theoretical MPI models. Storms were examined within three areas: 1) the Australian/southwest Pacific region, 2) the northwest Pacific region, and 3) the North Atlantic region.   Schade, 2000). The solid and the dashed lines correspond to ambient relative humidity of 75% and 85%, respectively. The heavy lines mark the maximum possible intensity that is realized neglecting (negative) SST feedback. The thin lines connect points with the same ambient SST.\nEmanuel's (1987) theory predicts roughly a 5 percent increase in potential intensity per degree Celsius SST warming (e.g., Emanuel 2005a). His conclusions were later supported by a similar potential intensity theory proposed by Holland (1997) as applied to several climate models of greenhouse warming scenarios (Tonkin et al. 1997). Holland (1997) concluded a rapid increase in MPI of 30 hPa per degree Celsius SST warming up to 30 \u00baC. For SST > 30 \u00baC, a slower rate of increase in MPI occurs which may suggest a physical limit. Global average temperature increase projected by the IPCC TAR ranged from 1.4 to 5.8 \u00baC for the period from the present to 2100. This increase is projected to occur over the 100 years but nevertheless SST's could increase by 5 to 6 \u00baC. IPCC AR4 now has even broadened the range of temperature increases for the next 100 years. The AR4 range starts at 1.4 \u00baC for the lowest emissions scenario to an even higher 6.8 \u00baC for the highest emissions scenario. Emanuel (1988) created the theoretical basis for the maximum intensity of hurricanes and demonstrated the existence of critical conditions for which no solution for the minimum central pressure exists and defined storms within this supercritical regime as \"hypercanes\". Emanuel (1988) showed these hypercanes would extend high into the stratosphere and either have very large outer radii or very small eyes. These hypercanes require higher sea surface temperatures and other environmental conditions. It is possible that abrupt climate change may bring about some of these conditions, specifically higher SSTs. Emanual (1988) calculated several possible minimum central pressures, radius to max winds, and max wind speeds as shown in Table   9. These values are computed and shown in Table 9 along with radius to maximum winds (r m ) and forward velocity (Vm). Results are for two different sea surface temperatures and for p a = 1013 mb, relative humidity of 80%, and outflow temperature = -73 \u00baC.   Using the results which are the stations most relevant to the Gulf of Mexico, one can compute the average MPI attained based on each theoretical model, H1 and E1. Table 11 shows these results. Emanuel (1988) (E1) values underestimated observed values for both stations, however generally agreed with observations at other stations. Holland (1997) (H1) results were more intense than observed values which was the case for other stations as concluded by Tonkin et al., 2000. The average percentage MPI attained by the H1 model for these two stations is 84.5%. Thus, a value of approximately 80% theoretical MPI was used for the design of the storms."}, {"section_title": "Results", "text": "Using the theoretical maximum MPI of 880 mb, five storms (numbered 191 through 195) with a radius to max winds of 25 nautical miles, were simulated along five different tracks across southeast Louisiana shown in Figure 15. These tracks are labeled T1 through T5 and are the primary tracks used in the Corps of Engineers Louisiana Coastal Protection and Restoration (LACPR) storm surge study. Figures 16 through 19 show the peak surge levels for 2007 conditions for storms 192 through 195. Table 8 displays the parameters for each of these storms. These five storms were also simulated on the 2050 ADCIRC geometry and Figures 20 through 24 show the peak surge levels for these 2050 degraded coast conditions.     Table 9 shows the comparison of peak surge for each storm for each condition. As can been seen, the peaks are all larger for the 2050 conditions. The peak maps show storm 193 with the highest surges and tremendous flooding occurs in St. Bernard, Orleans East, West Jefferson on south of Lake Pontchartrain, as well as inundation well inland on the north shore of the lake. One can see the impacts of a degraded coast where peak surges could range from a 1 to over 3 feet higher than existing conditions.   Katrina simulations for 2050 conditions result in somewhat higher surges; however there is essentially no overtopping of the USACE 100 year level of protection. Some overtopping occurred in the New Orleans East area due to a low levee height set in the mesh near the I-10 and levee junction. The low height was derived from levee height data that did not include the in-place structure height of 19 feet. This structure height would prevent overtopping in this location. The storm simulations for 2050 conditions show a significant increase in surge heights for extremely intense storms. However, these storms represent the most intense During ascent, total heat is approximately conserved, with little frictional loss of energy. The air eventually flows outward at the top of the storm and loses heat to long wave radiation to space. This simple model was later modified Emanuel (1988) to form exact equations governing pressure fall in steady tropical cyclones.\nConsidering the analyses above, these storms were designed with MPI values and a radius to maximum winds as shown in Table 12. Storms were simulated along one track to reduce the total number of variations and enable effective comparisons between storm results. An ADCIRC grid of the Louisiana coast in 2050 with the USACE 100 year levee design elevations in place. Figure 29 shows 100 year level of protection levee heights as set in the ADCIRC mesh.   (Thompson and Cardone, 1996) developed by the Corps of Engineers. This model was updated and enhanced by Ocean Weather Inc. (OWI) for modeling hurricanes and produced wind speed, wind direction, and atmospheric pressures to drive ocean response models. Winds produced by this PBL model are what are called 30-minute average wind speeds at the 10 meter level. These need to be converted to 1-minute average wind speeds in order to be categorized according to the Saffir-Simpson scale. A value of 1.24 was used as the conversion factor which is the approximate value most accepted in practice (Westerink, J., 2007, personnel communication). Additionally, a value of 1.09 was used to convert the PBL model winds to 10-minute average wind speeds required by ADCIRC. Storm 800 with an MPI of 800mb and a radius to max winds of 25.8 nautical miles produced the most extreme wind speed of 194 miles per hour. This is the most extreme 'hypercane' postulated to form influenced by sea level temperature as high as 36 \u00baC which is one degree higher than shown in Table 5.2. Peak surge plots from these storms are shown below in Figures 30 to 37. These storms all follow Track 3 (T3 of Figure 15) to enable direct comparison of surge results to storms simulated given the present climate. The storms shown in Table   10 and 13 and the peak surges produced by those storms, are shown in Figures 30 through 37 and were simulated given abrupt climate change and increased sea surface temperatures. One can progressively increasing storm surge heights corresponding with decreasing central pressures or increasing storm intensities. Table 14 provides a summary of maximum peak surges for the New Orleans area for selected modeled storms. Note that the surge values shown in Table 13 are for the entire storm simulation and are not located in the exact same location. To quantify the increase in surge influenced by 1\u00baC of SST warming, Storm 027 peak surges were subtracted from Storm 193 peak surges and the results are shown in Figure 38. These surge differences show the increases from a very intense storm (027) reasonably possible given today's climate from surges produced by a storm influenced by 1\u00baC of SST warming (Storm 193). A mean difference of 2.1 feet was obtained for the study area. To obtain the increase in surges produced from storms influenced by 2\u00baC SST increase, peak surges from Storm 027 are subtracted from Storm 870 and shown in Figure 39. A mean of 2.0 feet was obtained for the study area. To obtain the increase in surges produced from storms influenced by 3\u00baC SST increase, peak surges from Storm 027 are subtracted from Storm 850 and shown in Figure 40. A mean of 3.2 feet was obtained for the study area. To obtain the increase in surges produced from storms influenced by 4\u00baC SST increase, peak surges from Storm 027 are subtracted from Storm 830 and shown in Figure 41. A mean of 4.2 feet was obtained for the study area. To obtain the increase in surges produced from storms influenced by 5 to 6\u00baC SST increase, peak surges from Storm 027 are subtracted from Storm 800 and shown in Figure 42. A mean of 6.1 feet was obtained for the study area. It is evident from the above simulations that these extreme storms generate even more extreme surges. With just a 1\u00baC increase in SST, peak differences range to 34 feet. The largest increases are for the 5 to 6\u00baC increase in SST up to 44 feet. However, the mean values start at 2 feet for the 1\u00baC increase in SST up to a maximum mean value of 6.1 feet. Table 14 tabulates percent increase in surges. Chapter 6 Frequency Analyses"}, {"section_title": "Figure 27 Carnot Engine Model for Tropical Cyclones", "text": "Carnot Engine Model for Tropical Cyclones from (Emanuel, 1987). Air movement begins inward at radius r 0 and M o represents absolute angular momentum per unit mass, \u03b8* er is moist entropy at radius r, T B is temperature of inward airflow and within a thin boundary layer, M is angular momentum, \u03b8* e is moist entropy, and T out is mean outflow temperature. Emanuel's was the first work which proposed a link between greenhouse gas induced warming to a possible future increase in potential tropical cyclone intensities. Taking a different approach, Holland (1997) "}, {"section_title": "Review of Existing Work", "text": "This section provides a very brief synopsis of previous efforts to quantify hurricane frequencies, both past and future. Following this synopsis, an overview of two storm surge frequency methodologies is discussed. The previous methodology employed by FEMA to produce FIRMS will be discussed along with the new methodology created by the Corps of Engineers along with FEMA and other agencies. Frequency of extreme hurricanes has become so important not only because of the significant amount of potential damages and loss of life, but also because we are compelled to know if increases in intensities and/or frequencies are the result of anthropogenic forcing, natural variability, or both.  state that most research suggests future changes in hurricane frequency will be regionally dependent and there is no consistency among these studies on even the sign of the change in the total global number of storms (Henderson-Sellers et al. 1998;Royer et al. 1998;Sugi et al. 2002). One of the conclusions of the authors is \". . . peer-reviewed literature reflects that a scientific consensus exists that any future changes in hurricane intensities will likely be small in the context of observed variability (Henderson-Sellers et al. 1998;Knutson and Tuleya 2004), while the scientific problem of tropical cyclogenesis is so far from being solved that little can be said about possible changes in frequency . . .\" .  contended that if hurricanes were becoming more destructive over the years then these trends would also be evident in damage statistics. However, he could find no such trends and postulated that Emanuel's PDI although realistic, was perhaps a weak indicator of hurricane destructiveness. But Emanuel (2005b) stood by his conclusions and stated that the trends were large and in all ocean basins, despite measurement techniques, and well correlated with SST which was a reliable well-observed data set. Emanuel accepted Landsea's (2005) corrections (see Chapter 1) to his bias-removal scheme for a portion of the historic Atlantic wind observations, but he still emphasized caution due to the high correlation of hurricane activity and SSTs, especially since the SST record is long enough to capture the influence of global warming. Emanuel (2005b) concluded that even with the arguments of Landsea and Pielke, the current levels of tropical storminess are unprecedented in the historical record and that a global-warming signal is now emerging in records of hurricane activity. Emanuel (2005a) also noted that his rate of increase of hurricane intensities (per degree Celsius of SST warming) is much greater than results from simulation projections by Knutson and Tuleya (2004). Webster et al. (2005) published results from another study analyzing the changes in tropical cyclone number, duration, and intensity which caused more intense debate among scientists. The authors first completed a statistical assessment of tropical ocean SST demonstrating SSTs have increased by about 0.5\u00baC between 1970 and 2004. An increase in SST should correspond to an increase in the intensity (wind speed is used as the metric of intensity in this analysis) of tropical storms. Webster et al. (2005) concluded that the total global number of hurricanes has remained the same but the number of category 4 and 5 hurricanes has almost doubled globally over the past 30 years ( Figure   44). The authors show this is about an 80% increase in these intense storms due to the  There are still many other efforts and approaches to assessing hurricane frequency and projection of future storm characteristics. These range from a combination of a statistical-deterministic approach (Emanuel et al. 2006;Vickery and Twisdale, 1995) to the entire field of paleotempestology. Proxy records from coastal lake and marsh sediments from four sites in Louisiana, Mississippi, Alabama, and Florida suggest that intense hurricane activity along the Gulf Coast varies significantly at the millennial timescale (Liu, 2004). In light of the preceding discussion on increased frequency of intense storms, focus turns to past efforts to capture the frequency (in terms of return period) of storm surges produced by these intense hurricanes. As shown above, increases in more intense radius to maximum winds, forward speed, and landfall direction were analyzed. The probability distribution of each factor was plotted and analyzed for each 50 nautical miles of the coast. The authors did perform some limited efforts to investigate joint probabilities and interrelations amongst the parameters, but ultimately concluded that a much larger data set was required for any meaningful or reliable result. But the results of this report and additional efforts were used in deriving storm surge return periods for the FIA and then FEMA. These early efforts primarily used the Joint Probability Method (JPM) in which probabilities of certain parameters are obtained before hand and then conditional probabilities are derived to ultimately compute final surge probabilities. The JPM was developed in the 1970's (Myers, 1975;Ho and Meyers, 1975) and subsequently extended by a number of investigators (Schwerdt et al., 1979;Ho et al., 1987) in an attempt to overcome problems related to limited historical records. In this approach, information characterizing a small set of storm parameters was analyzed from a relatively broad geographic area such as the study mentioned above for the Gulf and entire East U.S. coast. In applications of this method in the 1970's and 1980's, the JPM assumed that storm characteristics were constant along the entire section of coast from which the storm data were obtained. Recent analyses suggest that this assumption is inconsistent with the actual distribution of hurricanes along the east coast and within the Gulf of Mexico. The JPM used a set of parameters, including 1) central pressure, 2) radius of maximum wind speed, 3) storm forward speed, 4) storm landfall location, and 5) the angle of the storm track relative to the coast, to generate parametric wind fields. Initial applications of the JPM assumed that the values of these five parameters varied only slowly in storms approaching the coast and thus, the values of these parameters at landfall could be used to estimate the surge at the coast. Recent data show that this is not a good assumption (Figure 45) due to the decay of intensity as the storm encounters the coastal area. Kimball (2006) has shown that such decay is consistent with the intrusion of dry air into a hurricane during its approach to land. Other mechanisms for decay might include lack of energy production from parts of the hurricane already over land and increased drag in these areas. In any event, the evidence appears rather convincing that major hurricanes begin to decay before they make landfall, rather than only after landfall as previously assumed. In this method discrete increments of probability are assigned to the value of the simulated surge. Recently with the development of risk based applications, the concept of a \"response surface\" has been developed. In this approach and development of a response surface, it is assumed that response is a continuous function of the parameters used to discretize the probabilities. Thus, for this application, surge is assumed to be a continuous function of central pressure, radius to maximum wind (Rmax), angle of approach, forward storm velocity, and distance from location of maximum surge for a specific combination of the other parameters. The integration is now more aligned with a Gauss Quadrature integration method. Essentially a set of functional relations between surge and each of the 5 primary probability parameters is defined. Once defined, an interpolation can be performed without the need to perform additional simulations. Its accuracy is predicated on the ability to fit the response surface with an accurate set of functional relationships from the actual sampled storms. These functional relationships and their development are detailed by Resio et. al. (2007). Additionally, the estimation of the surge cumulative distribution function includes a \"random\" (\u03b5) deviation term added to the modeled values. This term includes the variations to all the neglected parameters which ultimately affect surge heights and include both surge-independent terms (tide and model error) and surge-dependent terms (Holland B parameter), etc. All surge computations for this work were computed using a subset of the JPM-OS Louisiana synthetic storms and the physics-based numerical models used in the Joint FEMA and Corps of Engineers Coastal Storm Surge Study. In view of the above, it is worth mentioning another methodology for modeling hurricane surge risk which uses the \"Empirical Track Model\". Vickery et al. (2000) presents this method which has been adopted for the development of wind speed maps within the U.S. ((American National Standards Institute (ANSI), ASCE 1990ASCE , 1996. This method uses a Monte Carlo approach to sample from empirically derived probability and joint probability distributions. The central pressure is modeled stochastically as a function of sea surface temperature along with storm heading, storm size, storm speed, and the Holland B parameter. This method has been validated for several regions along U.S. coastlines and provides a rational means for examining hurricane risks associated with geographically distributed systems such as transmission lines and insurance portfolios (Resio et. al., 2007). However, the Empirical Track Model is applied within a Monte Carlo framework and has the ability to efficiently execute storms over many, many years (20,000 years in the Vickery et al. (2000) application). This number of simulations can be readily performed using this PBL wind model. However, there is simply not enough time and power even using supercomputers today to run this many storms using large, high-resolution ocean and coastal response models (wave models and surge models). For this reason, the Empirical Track Model was not considered for application to coastal storm surge probability computations. But this method does provide an excellent source for validating the statistical characteristics of the winds used for inundation modeling (Resio et al. 2007)."}, {"section_title": "Surge Frequency Analyses", "text": "Most frequency analysis methods require a large data set and normally to obtain results for storm surge probabilities, a significant number of simulations are required. The emphasis of this work was not to specifically compute future storm surge probabilities but to determine the impacts of future storms. Only a small selection of storms were simulated which limits computations of storm surge probabilities from this data set alone. However, these results can be compared to existing storm surge return periods, specifically those computed using the JPM-OS methodology for the Joint FEMA These projects produced a set of five levels of storm surge elevations which include the 50, 100, 500, 1000, and 2000 year return periods. Comparing future storm peak surges to these elevations quantifies where these results fit onto existing surge probabilities. . These values were attained approximately 90 miles off the coast and then as the storm moved closer to the coast and inland, decay rates were applied according to (Resio et al. 2007). Storm 870 produced a maximum wind speed of 172 miles per hour (1 minute average at the 10 meter level). Storm 870 represents a potential storm influenced by 2\u00baC increase in SST. The surges from Storm 870 are both higher than today's 1000 year levels and in some locations lower. The highest surge is 18 feet above the 1000 year level. This is due to a small protected area located on the north shore of Lake Pontchartrain which had very low return elevations and there was significant inundation in this area by Storm 870. The lowest values of around -20 feet because Storm 870 produced either little or no surge response, or drawdown occurred. Drawdown, or lowering of the water surface occurred primarily on the western side of the storm track and thus was evident on the west bank of the Mississippi River. The counterclockwise rotation of the winds resulted in winds from a northern direction driving inland water outward towards the coast. These locations were subsequently much lower than the 1000 year return levels. The mean difference between Storm 870 surges and the 1000 year return levels is -1.17 feet and thus below the 1000 year return period surge levels. However, as can be seen in Figure 43, Storm 870 surge levels are generally 1 to 3 feet higher than 1000 year levels at mid lake and then along the south shore of Lake Pontchartrain. These gradually increase to around 5 feet and higher near the area where Lake Pontchartrain eventually connects to the Gulf of Mexico which is called Rigolets Pass. Surges were also approximately 5 to 6 feet higher along the southwestern shore of Lake Pontchartrain near the Bonnet Carr\u00e9 spillway. Surge differences near the Louisiana and Mississippi State boundary, into the Pearl River Basin, range from 5 feet up to 9 feet along the Mississippi coast. These are primarily due to the track of Storm 870 for which the greatest surges were produced on the eastern side of the storm. Below New Orleans and on the west bank of the Mississippi River, Storm 870 surge levels were lower than the 1000 year levels primarily due to the storm track. The 1000 values were produced using the JPM-OS method which incorporated surges produced in this area by a large suite of storms along many tracks and directions.  (Table 12)  Storm 850 are a couple feet higher than those from Storm 870 near the south shore and the middle of Lake Pontchartrain. These surges are 10 to 11 feet higher than the 1000 year levels along the north shore of Lake Pontchartrain. The surges from Storm 850 are both higher than today's 1000 year levels and in some locations lower. The mean difference is -0.2 feet and the areas lower than the 1000 year levels are again below New Orleans on the west side of the Mississippi River. Generally both storms 870 and 850 produce surges near the current 1000 year return levels but as can be seen, one specific storm produces a wide range of surges, each with a different return period based on its specific location. Note that the 1000 year levels are chosen for comparison because the FEMA and LACPR projects did not produce 2000 year return period surge level surfaces. Although it is difficult to ascertain surge probabilities, some estimates can be performed to compute probabilities of the storms. Using the results of  This chart shows that the frequency of a central pressure reduction > 100 is about 10 times higher for the high CO 2 climate compared to the present-day climate. This implies that a 1:2000 event could become a 1:200 event. The \u2206CP for the low frequency events is about 12 hPa which is about \u00bd of a Category which agrees with Knutson and Tuleya (2004). The results from a recent study computed specific return levels for    Table 16 shows the shifts in frequency for each storm possible in a warmer high carbon climate. However, to compute the best probability estimate of a storm, all storm parameters should be used and a conditional probability distribution. For these future storms the other primary factor readily available in this study is the radius to maximum winds. Thus, a better estimate of the probability of Storm 193 and the others is given by P(storm) = P(Cp) * P(Rm | Cp) (7.1) where Cp = minimum central pressure and Rm = radius to maximum winds. Following (Resio et al. 2007) (Resio et al., 2007). Using this formulation for the conditional probability of a storm based on both its central pressure and it's radius to maximum winds, Table 17 shows the probabilities computed for storms under present climate and also for future climate conditions. Considering the conditional probability of both minimum central pressure and radius to maximum wind, the return period of Storm 870 is 38,000 years for present day climate and estimated to be 3,800 years in a future climate where SSTs are 2\u00baC higher than today. Of course these are only estimates and the total probability estimate should include other factors such as storm wind speed, and atmospheric influences of vertical wind sheer, and El Nino events."}, {"section_title": "Chapter 7 Discussion", "text": ""}, {"section_title": "Effect of Relative Sea Level Rise and Degradation of Coast on Surge Impacts", "text": "Results of simulations of high category 5 storms on the Louisiana coast for 50 years into the future show increased surge levels. The projected increases are on the magnitude of 3 to 5 feet and primarily in regions of projected high erosion and/or local subsidence. Storms were simulated on five different storm tracks and produced very large surges ranging to 30 feet or more. These storms characteristics were designed to model the most intense storm possible under today's climate and sea surface temperatures. Surges from these storms overtopped the New Orleans 100 year level protection. However, simulations of Katrina on a degraded 50 year projected coast did NOT overtop the New Orleans 100 year levee system. As local subsidence, erosion, and sea level rise occur over specific regions along the coast, it is these regions which will feel the most impact and can expect higher future surges. Projections have been made (Barras et. al, 2004); however these are based on historic rates and capture results of recent extreme events (i.e. Hurricane Andrew) but do not take into consideration a changing climate and geomorphologoic response to climate change. Figure 50 shows the 2050 projected coast which includes land gain from state and federal protection and restoration projects. Land loss areas are shown in red and these are the locations which will most likely see higher changes in surge levels. These 2050 projections were produced before the catastrophic hurricane season of 2005. As can be seen in Figure 50 barrier islands are projected to still exist. However, Katrina had a severe impact on the Chandeleur Islands along the east coast of Louisiana. The USGS has been actively surveying the islands for 3 years after Katrina and all surveys indicate a continued rate of erosion. The Chandeleur Islands in the Louisiana portion of the Gulf may erode all together in the near future (Sallenger, 2008). This has implications for other Louisiana barrier islands and the Mississippi River bird's foot delta. Extreme events such as Katrina and the storms possible with abrupt climate change may severely erode Louisiana barrier islands and initiate their degradation. The Mississippi delta may also feel the impacts of these storms but this depends on the amount of sediment that will be allowed to continue to flow into the Gulf. There are many large scale diversions alternatives currently being studied to divert this sediment away from the bird's foot delta. AR4 projects global sea level rise as shown in Figure 51. Twilley and Doyle, (2007) have incorporated these projections into estimates of local relative sea level rise projections specifically for the Louisiana coast ( Figure 52). These estimates also include local subsidence and the resultant range is 24 to 76 inches (2 to 6 feet) change in Relative Sea Level Rise by 2100 (Twilley and Doyle, 2007). Of critical importance in these computations and the future is how well the wetlands and marshes can keep pace with the changes in sea level. There are ongoing studies to further investigate these issues. However, the storms and simulations in this study did not account for sea level rise. Thus, if sea levels do continue to rise, impacts will most likely be even higher surges depending on the amount of relative sea level rise. "}, {"section_title": "Effect of Abrupt Climate Change on Coast and Surge Impacts", "text": "Storms designed to represent plausible intense storms driven by abrupt climate change were simulated on the projected future coastline. These storms produced significant surges on the order of 30 to 40 feet in some locations. When comparing these surges to existing storm surge frequencies, the results indicate return periods ranging from current day 500 to over 1000 year return levels. In light of these results there still could be many questions and much discussion on whether these storms and surges are really possible. The minimum central pressure and radius to maximum winds were adjusted, but how will future climatic affect the wind field distributions in storms? How will vertical wind shear change and reduce formation of new storms? How will ocean heat content and circulation patterns affect storm intensities? A tremendous amount of effort has been expended over the years to advance understanding of the Earth's climate. We now understand more fully the physics of atmospheric and oceanic circulation and also better understand their interactions and complexities. We know many of the primary and secondary factors that affect storm surge and these include storm direction of approach, forward speed, minimum central pressure, distance from the eye-wall, radius to maximum wind, tides, slope of the coast, etc. However there are still many questions to answer and issues to address. In regards specifically to climate change and the influence on hurricanes, questions still remain. Several unresolved questions include quality and reliability of tropical cyclone databases. Also, the relative importance of thermodynamic state (e.g., potential intensity, SST, atmospheric temperature and moisture, ocean heat content, etc.) versus the role of dynamic factors such as vertical wind shear in affecting frequency and formation of tropical cyclones."}, {"section_title": "Hurricane Frequencies and Surge Impacts in 2050", "text": "The results from hurricane frequency analyses indicate the possible increase in frequency and intensity under a warmer climate with higher carbon concentrations. The more extreme storms will produce higher storm surges and as shown in Figures 46 and   47. The highest surge increases will be in the regions susceptible to the most erosion and relative sea level rise. Along the Louisiana coast these are wetland areas but not necessarily developed urban areas. Damages will increase and be dependent on the level of protection, both manmade (levees) and natural (wetlands and barrier islands). What is the likelihood of an increase or decrease in the frequency of these extreme events? Some research indicates very real possibilities (Knutson and Tulelya, 2004, Webster et al. 2005, Mann and Emanuel, 2006. The conclusions of AR4 (IPCC, 2007) answer an emphatic: \"Yes, the type, frequency and intensity of extreme events are expected to change as the Earth's climate changes, and these changes could occur even with relatively small mean climate changes. Changes in some types of extreme events have already been observed, for example, increases in the frequency and intensity of heat waves and heavy precipitation events.\" AR4 identifies research efforts of Knutson and Tuleya, (2004) and others which show evidence that tropical cyclones can become more severe with greater wind speeds and more intense precipitation. AR4 also specifically addresses the question of \"How likely are Major or Abrupt climate changes such as Loss of Ice Sheets or Changes in Global Ocean Circulation?\" The authors state that these changes are \"not likely to occur in the 21 st century\" (IPCC, 2007). However, increased evidence of significant changes in glacier melting and sea formation since the publication of IPCC, 2007 have many scientists debating on the real possibilities of major changes in effect today. To address the likelihood of climate change impacts on tropical storms, IPCC, 2007 specifically concludes that \"increased tropical cyclone activity\" is \"Likely\" (IPCC, 2007). This translates into a >66% probability of occurrence (Table 3)."}, {"section_title": "Uncertainties", "text": "The importance of uncertainty cannot be overstated. Issues of data uncertainties are paramount in regards to the historic hurricane record and the need to be consistent for our current and future observations. Climate models are becoming extremely complex and we are building into these models more of the fundamental physics. But there are uncertainties in many of the parameterizations of specific components such as clouds and water vapor content. There are uncertainties in the numerical storm surge and wave models, as well as the PBL wind models. All have to be validated against reliable, consistent, and valid measurements and observations. The ADCIRC storm surge model has been validated over the years performing hindcasts of many storms. Overall results from Katrina simulations are on the order of less than 1 foot. But can this be improved? And how much do these results depend upon the wind fields? How (un)certain are the \"best\" winds? One of the hardest pieces of data to collect are the wind speeds (averaged for how many seconds? minutes?) at the level of 10 meters during the lifetime an extremely intense storm. These data are now mainly collected using dropsondes and remote devices. What are the uncertainties associated with this equipment? In order to begin to assess uncertainties one must first identify the factors that contribute to uncertainty in the results. These can be: \u2022 Inputs: solar energy, atmospheric composition -carbon, water vapor , methane, clouds, aerosols, etc., ocean -temperatures, salinity, etc.; landscape -deserts, forests, urban areas, etc.; cryosphere -ice sheets, glaciers, sea ice, snow, permafrost, etc. There are uncertainties in the measurements of each of the inputs to varying degrees. \u2022 Drivers: solar energy, atmospheric circulation -jet streams, El Nino, La Nina, etc.; oceanic heat content and circulation patterns and meridional overturning current. \u2022 Bathymetry, topography, vegetation -these uncertainties are also in the measurements, data accuracy, and how well the data represents actual conditions. Surveys have their inherent uncertainty and the physical geomorphology is constantly changing. \u2022 Numerical models -uncertainties range from model formulation which depends not only on how well we understand the physics, but also how well the mathematical formulation and implementation can represent the physics. Other factors then include calibration and parametric implementation of some components, i.e., barotropic assumptions. Additional uncertainties arise when models are coupled together. Factors come into play such as transformation and re-griding of one model output to another model input formats as well as time step issues such as between a storm surge model running at a 1 sec time step and a wave model running at a 30 minute time step. Additionally, uncertainty arises in scenarios where non-hydrostatic conditions affect flow and surge. ADCIRC represents ridges, roads, and levees as sub grid scale features and employs the standard weir equation to compute flows over these features. However, uncertainty can be reduced by using more accurate 3D Boussinesq models that better capture the physics of the conditions. Boussinesq models are required to capture local wave setup at critical structures and levee reachs. Additionally, the 2D Shallow Water Equations have limitations in that they do not allow bidirectional flow in the vertical and ignore density effects such as changes in salinity. These could be important in the near shore and also in the Gulf. AR4 (IPCC, 2007) defines several classes of uncertainties. The two primary types are \"value\" and \"structural\". Value uncertainties are associated with incomplete determination of the values or results when data are inaccurate or do not fully represent the component of interest. For example, these are the uncertainties of measured observations and the quality of historic databases. Structural uncertainties are those associated with the incomplete understanding of the processes that control a particular value or result, or when the model used for a particular analysis does not fully capture the relevant processes or relationships. AR4 strives to be particularly transparent on all uncertainties and separates uncertainty from likelihood. Definitions for levels of confidence (uncertainties) are shown in Table 2 and likelihood levels (probabilities) are shown in Table 3. These definitions are very helpful to quantify issues such as uncertainty in climate models due to parameterization of such components as clouds and water vapor processes. Skeptics have used the uncertainties in these parameterizations to formulate arguments which essentially say one can place no confidence in any of the climate models results. The above is merely scratching the surface of the total uncertainties inherent in modeling our Earth system. But there are ongoing research efforts which are beginning to help and lay the ground work for assessing the total uncertainty in modeling complex systems. These efforts will eventually help us to further quantify uncertainty in our systems and quantify results with standard and accepted levels of confidence."}, {"section_title": "Recommendations", "text": "Results of this work can be used as a starting point for further research and study into hurricanes influenced by warming, potential abrupt climate change, and both storm and overall climate impacts. Work can be extended to further design time a suite of future possible time varying storms modeled to produce a large data set of storms, storm characteristics, and the resultant surges. This database can then be used for statistical analyses such as the JPM-OS. The JPM-OS would have to be modified using climate model projection results of future storm parameter probability distributions. These future distributions of factors such as minimum central pressure, forward speed, radius to maximum winds, etc. can then be incorporated into the JPM-OS to the produce probability distributions of future storm surges. Uncertainties can then be compiled for both the storm parameters as well as the resultant storm surges. There are many possible metrics of intensity (maximum potential intensity, average intensity, average storm lifetime, maximum storm lifetime, average wind speed, maximum sustained wind speed, maximum wind gust, accumulated cyclone energy, power dissipation, etc.), and not all of these have been closely studied. This has been due to data limitations and other reasons. Additionally, most of the debate has tended to focus on SSTs although other environmental factors should be considered. For example, the tropical cyclone heat potential (a measure of the oceanic heat content from the sea surface to the depth of the 26 \u00b0C isotherm) may be a better indicator of the potential for hurricane intensification than SST (Scharroo et al. 2005). With increasing temperatures and exacerbating affects of sea level rise, the coast of Louisiana is more vulnerable than ever to climate change impacts and especially extreme storm events. Along with the coastal zone, the people who populate the southern coast of Louisiana are also in peril. If storm power and frequency increase, so will damages and potentially loss of life within the coastal zone. These implications are relevant not only to Louisiana, but to all coastal areas surrounding the U.S. which are susceptible to the impacts of climate change."}, {"section_title": "Figure 53 Global Surface Warming Projections", "text": "Solid lines are multi-model global averages of surface warming (relative to 1980-1999) for the scenarios A2, A1B and B1 shown as continuations of the 20th century simulations. Shading denotes the \u00b11 standard deviation range of individual model annual averages. The orange line is for the experiment where concentrations were held constant at year 2000 values. The grey bars at right indicate the best estimate (solid line within each bar) and the likely range assessed for the six SRES marker scenarios. The assessment of the best estimate and likely ranges in the grey bars includes the AOGCMs in the left part of the figure, as well as results from a hierarchy of independent models and observational constraints (IPCC, 2007). Long-term studies and re-analysis of atmospheric and oceanic data sets will continue to be needed to address issues climate change and hurricanes. World wide efforts are ongoing and re-analysis for both historic hurricanes and for reconstruction of climate data for ready incorporation into new climate models are being performed. Additionally, improvements in modeling technology should provide new insights and advancements as well. For example, a model at NASA Goddard Space Flight Center called the Finite Volume General Circulation Model (fvGCM) that represents hurricanes and their behavior at unprecedented spatial resolutions for a GCM (Atlas et al. 2004). New satellite and enhanced in situ observing capabilities can provide new observational capabilities for hurricane internal and external environments that will increase our understanding of past and current events. This will also help in assessing the likelihood of future projections. However, it seems for the Atlantic and Gulf of Mexico, regardless of whether the underlying cause of increased hurricane activity is a natural cycle, or by anthropogenic forcing, or a combination of the two, it appears likely that continued high levels of hurricane activity will continue as long as increased SSTs persist. More research is needed from observations, theory, and modeling to address issues of the effect of global warming and abrupt climate changes on tropical storms."}, {"section_title": "Chapter 8 Summary and Conclusions", "text": "This pressure and radius to maximum wind, and compare surge results obtained from these future storms, to published storm surge return period levels. The first objective was met by simulating the most intense storm possible given today's climate using a topography representative of the landscape 50 years from the present. The conclusion drawn from this objective is that if the Louisiana coast continues to degrade through erosion processes and subsidence, the results will be higher surges which will depend on the storm intensity, direction, minimum central pressure and other atmospheric parameters as previously discussed. Surge heights ranged from 1 to 3 feet higher for the storms simulated and specific future surges will additionally depend on the degree of local subsidence and relative sea level rise. Additionally, the New Orleans 100 year level of protection will protect against a Katrina-like storm 50 years form now. However, overtopping will occur for more intense storms that are possible given our present climate. To meet the second objective, results form current research was used to create possible storms characteristic of future conditions influenced from 1 to 6\u00baC of average global warming. Storm surges produced by these storms were quantified and differences between surges of these future storms were compared to surges of comparable present day storms. Surges can be significantly higher along many areas of the coast. Lake Pontchartrain surges increases ranged from 2 feet to over 10 feet higher for each degree increase in SST. IPCC 'A1F1' high emission scenario results in an estimated 2\u00baC increase in global mean temperature by 2050, and 4\u00baC by 2100. Realization of these temperatures will result in 2 feet to 7 feet higher surges depending on storm intensity, direction, forward speed, and other atmospheric conditions. New Orleans levees will offer some protection from more intense storms that may potentially form influenced by global warming and increased sea surface temperatures, but overtopping will occur. The third objective was met by comparing future storm surges against storm surge return levels, specifically for the 1000 year return period, produced from current present coastal storm surge analyses. Additionally, estimates of the probabilities of these future storms were computed along with shifts in the frequencies of these events given abrupt climate change realization. There is no doubt our climate is warming and this warming is having impacts regionally and globally. Whether the cause of this warming is human induced or not is still open for debate. However, based on the results of these storm simulations, for each 1\u00baC rise in average SSTs, surges from extreme storms will increase on the order of at least 1 to 3 feet or more depending on other atmospheric and oceanic conditions. Table 16 summarizes probabilities of future extreme events and the potential shifts in frequencies from present climate conditions. A storm with a probability of 1:10000 years may become a storm of 1:1000 years return period. Figure 53 shows IPCC projections for global average temperatures for six SRES scenarios with the highest estimate of 4\u00baC with the likely range of up to 6 \u00baC by 2100. If temperatures reach these limits, future storms may potentially be very similar to the storms designed in this study along with the high surges produced by these powerful events. These are extreme storms and from basis statistical reasoning, a small shift in the mean of a primary variable (i.e. average temperature) can result in substantial changes in the frequency of the extremes. Extremes are the infrequent events at the high and low end of the range of possible values for a particular variable. An increase in the frequency of one extreme (e.g. the number of hot days) can be accompanied by a decline in the opposite extreme (in this case the number of cold days). Ultimately, the result is an increase in the number of extremes (hot days or severe storms). Within the next 50 years, the amount of increased warming will determine the realization of the exact number of these extreme events."}]