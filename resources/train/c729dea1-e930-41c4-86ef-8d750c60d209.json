[{"section_title": "INTRODUCTION", "text": "The following report of the Second NOAA Data Quality and Continuity Workshop contains summaries of issues and problems presented at the first workshop as well as papers on additional topics that have come to our attention since the the initial workshop. Material for some presentations that appear on the workshop agenda has been excluded based on overlap with other topics. Other papers that have recently come to our attention, but not presented at either workshop, have been included to highlight additional issues within NOAA. Inevitably, changes and improvements in observing systems become part of mature environmental monitoring programs. These include changes in \u2022spatial and temporal sampling, processing algorithms, quality control procedures; instrumentation, accuracy, precision, and representativeness of the measurements. Sometimes these changes can seriously alter the interpretation of the data and products derived from the measurements. The DQ&C project focuses on the scientific integrity and continuity of the data from all of our observing systems as we change, improve, or replace existing observing systems \u2022or algorithms used to derive data a..11d products from these observi.l.l~g systems. The issues of interest to the DQ&C project are much broader than traditional areas of interest in Data Management. The DQ&C project must also be concerned with the purpose of various Observation Networks and Programs, including such fundamental issues as sampling theory, instrument and network design, and operating environments. Furthermore, based on knowledge \u2022of the expected applications of the data derived from these networks, the DQ&C project seeks to set up a rational process to decide whether changes of data quality or continuity are of serious concern to users of NOAA data, and if so, how we may go about resolving these issues.\nOperational environmental satellites, such as NOAA's Polar-orbiting Operational Environmental Satellite (POES) series, have the potential to provide critical information on global environmental change. Table 1 is a list of global change parameters currently accessible from operational satellites. The importance (three stars is highest) and the present knowledge (D represents least knowledge) of each variable is also shown.\nMuch has been learned over the past decade about the deficiencies in NOAA's marine data management practices through work associated with the Comprehensive Ocean-Atmosphere Data Set (COADS)*. Data covering the period 1854-1991 have been assembled, edited, and smnmarized from a wide range of historical and contemporary global surface marine reports in COADS. The products include individual marine reports, imd monthly statistics for a variety of observed and derived climate variables using 2\u00b0 latitude x 2\u00b0 longitude boxes. Over 200 groups worldwide have obtained COADS products, and improved 1947-92 data are being provided for Global Reanalysis. With the record extending so far back in time, it spans many different coding and observing practices, and because of the scope of the data included (varying platforms and originating sources), COADS encompasses a large number of data quality and continuity issues. Development and enhancement of COADS therefore provides a valuable pe~spective on these issues. Integrating data from ships and automated platforms has revealed data discontinuities and quality differences that may not be apparent in examination of the data from a single platform type. Combining real-time and delayed-mode data has also helped identify additional problems in t.lj_e data and metadata (h-rformation about the data) of a similar nature. IDSTORICAL SHIP DATA ISSUE. Wide variations in temporal and spatial coverage exist throughout the historical marine record .. Annual totals of global marine reports show a fairly steady increase from 1854, when guidelines for standardizing observing and reporting methods were first organized internationally. Spatial coverage also varies with commercial and political events, such as the opening of the Panama canal, openings and closing of the Suez canal, and the historical impact of World War I and II. Efforts to address this data continuity problem rely on gathering and digitization of available retrospective ship data. Due to the expense and lack of readily available data prior to the last four or five decades, international involvement is being actively sought. Merchant ship data provide the bulk of the data in CO ADS throughout the 19th and 20th centuries. Shipboard observing techniques and instrumentation have changed d.fan:laticlu.ly during the period. These factors, plus effects involving the size, construction, shipping routes, and propulsion of ships, have lead to relatively well-known biases in variables such as sea surface temperature, air temperature, and wind. Biases are not as well understood in variables such as waves, clouds, and humidity.\nSince the mid 1960's the expendable bathythermograph (XBT) has been the principal tool for measuring water temperature in the global upper ocean. During this period of time there have been several variables introduced into the XBT instrument system. If these system variables are not properly accounted for they could seriously impact the quality and continuity of the XBT temperature data. The variables include the entry of a second vendor into the XBT market, the availability and use of more than one XBT controller or data acquisition box each with different performance characteristics, and the recent determination that the standard \"fall-rate equation\" used to relate descent time to XBT probe depth in the water colunm is systematically in error. Fortunately, early recognition of these variables has helped to minimize the potential for data continuity problems. For each of the these issues, action has either been taken or is underway to insure that the highest possible quality and continuity of the data collected by the XBT's."}, {"section_title": "RECENT DATA QUALITY AND CONTINUITY EFFORTS", "text": "Reliable data with adequate quality to address the multitude of environmental problems and issues is fundamental to understanding the earth system as well as to predicting its future state. Over the past two years, Drs. Vern Derr \u2022and Robert Reeves have led a NOAA-wide effort to renew the focus on DQ&C. Numerous DQ&C issues were identified in a DQ&C FY91 workshop (being mailed separately). Varying degrees of progress have been made in solving each of these problems. It has become clear, however, both to those within and outside NOAA, that DQ&C is a persistent issue and will continue to be so for the foreseeable future, especially as new observing systems, \u2022 processing systems, and new environmental problems continue to arise."}, {"section_title": "A CONTINUITY OF EFFORT", "text": "It is critical that NOAA also develop the ability to routinely identify DQ&C issues as early as\u2022 possible and to provide a sense of priority to these issues. Without such a "}, {"section_title": "THE PROBLEM", "text": "The near-surface temperature of the earth, a few meters above the surface where we live, work, and grow our food, is critical to life on this planet as we know it. Temperature measurements are among the longest physical records of the earth's history, and because of the characteristics of the near-surface temperature, it is the key climate quantity related to long-term global change. Anticipated changes in the climate system, as projected from the most complex coupled ocean/atmosphere climate models to the simplest !-dimensional climate models, all indicate the most consistent change expected with increases of greenhouse gases is an increase of near-surface temperature. As such, long-term temperature records of near-surface temperature are critical to: ( 1) Detecting the greenhouse effect, (2) Quantifying the sensitivity of the climate system to all types of climate forcings and feedbacks (e.g., anthropogenic aerosols, volcanic aerosols, and changes in solar irradiance ), and (3) Adapting to or mitigating expected changes. In the course of compiling long-term temperature records, we havefmmd many biases that have affected the record in the past. In NOAA today, we are repeating and magnifying many of these mistakes. Figure 1 provides a measure of the magnitude of the adjustments scientists have been required to make to the temperature record. The adjustments are greater than the signal they believe exists in the record, a warming of about 0.5\u00b0C over the past 100 years. The uncertainty of this record is large relative to the magnitude of the warming. Although the problems of the past cannot be changed, we can prevent similar problems in the future. Unfortunately, at the present time NOAA does not have an observing system to. detect longterm near-surface temperature change. Such a requirement is neither part of its modernized automated surface observing system (ASOS) nor its 100-year old cooperative observing system. Moreover, even its long-term climate reference network of 23 stations is no longer supported. NOAA currently supports nearly 6000 thermometric stations without a long-term temperature observing requirement.\nIt is likely that greenhouse wanning will be accompanied by increased snowfall at high latitudes and by increased snow melting during spring and sununer. However, the role snow cover can play in a decadal-scale cliinatic change is complicated by a number of feedback mechanisms that have not been quantified and are poorly understood. Longterm records of snow cover variability are essential for: detecting the greenhouse signal quantifying the sensitivity of the climate system to internal feedbacks adapting to expected changes. A major underlying problem with many of these data sets, some of which span nearly a century, is the nearly total lack of integration. Many different data sets are available at varying temporal and spatial intervals resulting from numerous measurement techniques .. Areal coverage extends from surface point measurements to largescale area integrations from satellite sensors. Measurement frequency varies from hourly (some automated stations) to monthly (manual observations). Time series and climatologies of snow extent, water equivalent, and surface albedo of snow covered regions need to be compiled. In addition, there is concern that the snow depth measurement technique proposed for NOAA's automated surface observing system (ASOS) may not provide a level of accuracy comparable to that of the daily snow depth measurements currently provided by the conventional (manual) observing method.\nThe acquisition of an accurate and credible data set of global stratospheric ozone vertical profiles is a major undertaking for long-term monitoring and other climatic purposes. Such a data set w'Jl of necessity inco1 porate data from the suite of instrw-nents on several platforms. These include SBUV/2 on NOAA operational satellites, ozone-sondes on high-altitude balloons and Umkehr observations using Dobson spectrophotometers. This list is restricted to NOAA systems, but is goes without saying that other sources of ozone information will be incorporated into the overall ozone data set, such as the overalllidar programs underway. Satellite data offer the broadest coverage and the program is planned to continue to do so into the next century. Like any measurement system, the SBUV/2 has faults that affect the accuracy of the derived ozone products. The major ones are the pre-launch calibration and the later in-orbit performance, usually a degradation of the instrument. The level of ground calibration of these systems are state-of-the art for ultraviolet instruments; ie, 1-2%. When instruments are launched on successive satellites, it is expected that a jump will occur in the data record. And as time goes on, the trend in the data set of a singleinstn1m_entwill hav~ some uncertainty due to instrument performance. Onboard calibration systems provide an operational performance and are strong tools in characterizing the data products. External measurements are invaluable in helping to understand the overall data set. A major set of data for validation is that obtained through Umkehr observations by Dobson spectrophotometers. The current Umkehr data record extends back in time 30 or so years and is in itself a valuable data set for climate change studies. These data, however can be contaminated by atmospheric variations especially volcanic debris. These must be accounted for in generating a portion of the Umkehr data. This has been a major NOAA project under the direction of J. DeLuisi ofNOAAIERL. A recent report of the Global Atmospheric Watch of the World Meteorological Organization (WMO) Global Ozone Research and Monitoring Project noted that the WMO Executive council called for, as a priority, a thorough review and reevaluation of the existing Umkelrr data set. The WMO/EC recognized the unique information contained in this extensive data set.\nA long-term climate monitoring system has never been established for the worlds oceans although strides in this direction are being undertaken with programs such as the Global Ocean Observing System and the Global Climate Observing System sponsored by IOC and WMO. In the past, data for studying climate trends and general ocean climatology and oceanography have come from basically operational programs (i.e. Voluntary Observing Ships, buoys, satellites) and~ limited research voyages. The collection, dissemination, and archiving of these data are often not well coordinated among data users/collectors introducing numerous shortfalls and biases. With the oceans covering such a large portion of the globe and impacting so greatly upon the weather and climate, a high quality marine database is essential to climate research and to the possibility of predicting climate trends with good precision. The basic problem is that users need access to a high quality and easily usable marine database, with periodic updates. However, the requirements for management and docuniehtation of marine data have become more and\u2022 more complex with the brisk introduction of new types of in situ observing platforms, and related advances in automation and satellite communications. Careful data management methods and complete metadata are critical to help minimize data processing errors and to provide adequate documentation of data formats, observing practices, and processing techniques, with an important goal of being able to recreate the original basic observational data. In addition, many marine data quality and continuity problems have arisen because of inadequate coordination among governmental or international bodies."}, {"section_title": "-A", "text": ""}, {"section_title": "THE SOLUTION", "text": "A long-term temperature observing system is required. Such a network may consist of as few as 50 to 100 well placed stations. The frrst priority of such a network would be to observe decadal to century-scale temperature changes. NOAA's Automated Surface Observing System (ASOS), needs climate support. As NOAA introduces new hygrothermometers into its frrst order automated surface observing system (ASOS), a 1-year period of overlap between old and new instruments can help to correct for any bias between old and new instruments. Complete records of equipment change and calibrations must be maintained and archived with the data. Instruments in our cooperative, mostly-volunteer observing network are not up to task of monitoring long-term temperature change. The 100-year old technology, and even recent new electronic instruments used to upgrade a portion of the network during the 1980s continues to propagate the so-called \"time of observations bias\" that introduces as much as a zoe bias into the record. The new maximum/minimum thermometers have been introduced into the network without adequate information regarding the bias introduced into the record. Evidence suggests the bias is several tenths of degrees C. The temperature measurements in the cooperative observing system need to be modernized. Plans have been developed over the past few years to accomplish a modernization of the cooperative observing network which would help monitor the details of decadal-to-Century scale temperature change. These plans need to be implemented.\nComplete and accelerate implementation of a proposal to test thoroughly the year-round simultaneous performance of the several established and candidate uew gages along with the gage initially planned for the modernized Automated Surface Observing System (ASOS).\nIt is necessary to recognize that a single array of standardized instrumentation will not satisfy all needs within likely budget constraints. Moreover, a large number of complicated sites is not necessary, and a small number of simple sites is not adequate. A nested network is required, with SURFRAD sites (or sites like them) constituting a specialized core network, and with a much larger number of simpler sites (like those of SOLRAD) operating in conjunction with them. Funding for the SURFRAD \"core\" now exists. There is no continuing source of support for the historic SOLRAD operation. A third level of observation is needed, in addition to the new SURFRAD and historic SOLRAD activities. There is need for a denser array of even simpler instruments than are used in SOLRAD, so as to reveal spatial differences that are presently not being observed and to provide local input to meso-scale numerical models that are now being refmed. In simple concept, this need could be met by the simple expedient of fitting each Automated Surface Observing Station (ASOS) of the modernized National Weather Service with a simple silicon-cell radiation sensor. Data quality and the number of variables monitored would then follow a sequence of increasing accuracy and complexity from the modified ASOS array, through SOLRAD, and ending with the new SURFRAD sites. Data acquisition systems, calibration procedures, quality control, and archiving of data need to be modernized and expanded in order to satisfy the present growing demand for surface radiation data of all kinds, and over all time scales. Existing data systems need to be refurbished, so as to yield ready access to yesterday's data from a central archive, while at the same time providing decadal scale continuity and quality assurance.\nA global snow cover data set (snow extent, depth, and water equivalent) should be produced through the integration of data from numerous sources, including surface measurements, both manual and automated, as well as satellite remote sensing. The primary data sets which should receive immediate attention include, for station data, snow depth measurements available from the GTS network through NOAA's Climate Analysis Center (CAC), data from the Former Soviet Union (FSU), USDA Soil Conservation Service manual snow course and SNOTEL data, and for satellite data, the NOAA-A VHRR digital satellite data (1972present), and NASA-SMMR (1978-1987 and DMSP-SSMII (1987-present). In particular, the continuation of the weekly NOAA/NESDIS snow charts and digital archives should be ensured, along with support to check quality and homogeneity of those records. In spite of the importance of snow cover, surface measurement data sets are often lacking in high-latitude and other remote areas due to the sparse surface observation network. During the past two decades, important information on snow extent has been provided by satellite remote sensing in the visible wavelengths. However, with visible spectrum data, spatial and temporal continuity are limited by the effect of cloud cover and polar darkness. The application of passive microwave remote sensing allows data collection in nearly all weather conditions and during darkness, and provides a means to compute snow mass or water equivalent and to detect melt. Thus the passive microwave data set will provide important input to energy budget, hydrologic, and global circulation models.\nTo achieve optimum benefits to the scientific community from the NCDC archive of WSR-88D Level III products, a comprehensive, long-term program to make numerical copies of these data (along with associated metadata) available to all requesters is required. This solution would undoubtedly involve a substantial investment in personnel and workstation hardware.\nThe solution adopted by the C0 2 measurement community was to establish a Central C0 2 Laboratory (at the Scripps Institution of Oceanography) sanctioned by the World Meteorological Organization. This laboratory maintained the primary calibration scale, and measurements by most international laboratories have been directly tied to this scale. The solution has only been partially successful because the funding has been intermittent. Despite these and other difficulties, the calibration of C0 2 measurements has been much better than for other trace gases such as CO. Ad hoc intercomparisons have taken place 1 -E occasionally for other trace gases and for the isotopic ratio measurements. Experience has proven that such a level of standardization is insufficient. An approach similar to that for C0 2 should be adopted for the measurement of other trace gases and for isotopic ratios.\nThe Umkelrr data set, past and future, must continue to be corrected and reevaluated in order to provide an independent data set as well as to provide a source of satellite data validation for global purposes. To data NESDIS has provided significant funding support for this work. It is appropriate that the overall Umkelrr work be institutionalized and assigned the priority within NOM that is consistent with its ultimate contributions to many programs both within and outside of NOM. In order to maintain scientific continuity, this work should remain under the direction of J. DeLuisi. Full-time dedication to this Umkelrr work would require the addition of one FTE scientist at the GS 12/13 level and its identification as a distinct program in an ERL laboratory. New funding is required for the additional staffmg and other resources as needed. In addition to providing a better Umkelrr data set, considerations must also be given to improvements in the SBUV/2 profile retrieval algorithm itself. This will insure that the most credible satellite data set is available at any given time. Several scientific groups are or will shortly be using SBUV/2 data for a variety of reasons. The more users there are of the satellite data, the more the data will be assessed for accuracy. This will ultimately lead to improved algorithms. In fact the next generation algorithm is under study now. These \u2022groups include NASA/GSFC, NCAR (with support from the NOM C&GC program), University of Maryland (also with G&GC funding), and potentially George Tech (with DOE support). NESDIS must be in an operational readiness state to rapidly implement such algorithm or product changes in the NESDIS system. Past history indicates that two FTEs \u2022 at the GS-12/13 level are required for this. New funding is needed for the additional staffing and other resources as required.\n1. Complete digitization of the long term tide records that are expected to be useful for long term climate studies, check them and enter them into a computer data base that will be easily available to researchers. This will require significant data base and communications software development and improved computer capability. 2. Perform an intensive check and adjustment of about 10 long term U.S. coastal tide stations for vertical benclunark consistency, location movement effects, etc. Make these \"clean series\" available to the research community. 3. Estimate and document the uncertainties in the records for each of these ten stations as a function of the year of recording. This will require study of outdated systems and their operations as well as oceanographic studies.\nIncreased on-site presence of data center liaison/acquisition personnel at major marine research institutions and programs will result in much more systematic acquisition of recently collected data and recovery of older data and documentation. NODC has had successful placement of liaison/acquisition scientific staff at five strategic locations (Woods Hole, MA; Miami, FL; La Jolla, CA; Seattle, WA; Honolulu, HI). These scientists, besides acting as foci for data acquisition in their regions, have introduced, to varying degrees of success, data management practices and expertise in their NOAA and academic host institutions. Their success, however, has its down side. Some have become so important to data management of major projects in their home institution (e.g., JEDA, JASL, NECOP, JGOFS, etc.) that data collection efforts in other institutions have suffered. It is proposed to place additional NODC liaison primary staff in new major centers of marine research, and to augment personnel resources, where needed, at present sites. Initially we have targeted as new liaison sites the University of Rhode Island's Graduate School of Oceanography and the Naval Oceanographic Office, Stennis Space Flight Center, MS. Both sites are very close to several marine research laboratories. We propose also to augment the Woods Hole and La Jolla sites each with an assistant. This is necessary because of the present demands by JEDA (La Jolla) and JGOFS (Woods Hole) on the incumbent liaison offices. Other data management activities, which have suffered, will benefit greatly."}, {"section_title": "THE IMPACT OF IGNORING THE PROBLEM", "text": "Much is at stake. Decisions related to taking actions to curb the emissions of greenhouse gases are critically dependent on our ability to measure the near-surface temperature. The cost of incorrect decisions is staggering. Many argue that the economic and environmental stability of the planet are at risk, while others contend that the cost of stabilizing world \u2022 greenhouse emissions are too great. As Secretary Brown indicated recently, we caunot tolerate the false choice between the environment and the economy. We will not be able to address the many ramifications of complex environmental/economic issues that confront us without an unbiased temperature record.\nThe staggering costs and human suffering of aperiodic floods in the U.S. are well documented. At the other end of the scale, extreme droughts cause major impacts in agriculture, water resources, transportation, recreation/tourism, wildlife, and other elements of the nation's environmental and economic infrastructure. Costs and losses of the 1988 drought amounted to nearly $40 billion, of which losses in agriculture alone during the last three quarters of 1988 were $13 billion of direct GDP. Second and third quarter GDP growth was reduced to 0.9% and 0.6%, respectively, due mostly to reduced agricultural production. This increased retail food prices in the U.S. by 0.5%. 1 Improved precipitation gauges are critical to eliminating the quantitative impact of faulty precipitation data on climate records and global change research. This matter will have a direct impact on national environmental policy decisions.\nThe climate observing system offue nation is degrading. Data continuity problems at many sites now threaten fuese records wifu biases fuat are greater fuan fue signal researchers are looking for to detect climate change. These problems have begun to impede fue Nation's ability to assess its own surface climate and changing environment. Allowing the degradation to continue will result in an extension in our inability to confidently detect climate change by perhaps decades. This situation will greatly increase the risk of making poor and/or extremely expensive (multi-billion) decisions regarding investments in and management of the nation's resources, infrastructure, and economy.\nIt is only within the last decade or so that the potential of the current NWS radar network (started in 1957) has begun to be realized. Objective, numerical use of reflectivity data has been coupled with a better understanding of properties of severe storms. Many of these advances were a direct result of the scientific investigation of archived data from these radars. With these mature developmental methodologies in place, the pace of improvements in the use of the WSR-88Ds should be much quicker. Archiving and dissemination of the WSR-88Ds should be much quicker. Inadequate archiving and dissemination of the WSR-88D data could raise the spectre of a 20-to 30-year delay in realizing the full potential of NEXRAD.\nVariations, both in space and on an interannual timescale, are taking place in the mixing ratios of the greenhouse gases. We cannot take advantage of these to learn more about the cycles and feedback processes driving the variations because we are not sufficiently sure of what we are observing. For example, we don't even know whether the trend of CO during the last decade has been up or down due to calibration uncertainties; any trend in CO carries important implications for possible changes in the concentration of OH radicals and the oxidative capacity of the troposphere. We see large interannual variation in the buildup of atmospheric CO, but we are so largely missing the accompanying isotopic signature, which could pinpoint the oceans or the terrestrial ecosystems as the dominating cause. As a result J the development of a better understanding of the relevant biogeochemical cycles is J hampered, and our predictive capability is impaired.\nSea level has been theorized to be one of the first indicators of Greenhouse Warming. As such, clean records are needed to assure that t.h.e effect is distinguishable from the background \"noise\" as quickly I;IS possible. Without this, confirmation of the theorized Greenhouse warming will be significantly delayed --perhaps until it is too late to take effective and efficient policy action."}, {"section_title": "WHY NOW?", "text": "NOAA lacks a network designed to monitor decadal and longer temperature changes near the surface of the earth where we live, work, and grow our food. NOAA is in the process of modernizing its surface observing system, but will not be prepared to measure long term temperature changes free of bias unless action is taken now. Recent efforts in NOAA to modernize its cooperative observing system have exacerbated the situation as new instruments and observing procedures were not focused on measuring decade to centuryscale temperature change. NOAA is the agency responsible for monitoring, assessing, and predicting the state of the atmosphere and oceans. NOAA is introducing even greater biases in the temperature record than had ever before existed in the historical temperature record, without an adequate means to correct for these biases. Meanwhile, these records are seriously affecting our ability to measure decadal and longer term temperature change.  \nVery recent history has demonstrated the major impacts of climatic extremes, particularly precipitation extremes. Record high levels of the Great Lakes and its accompanying effects in 1985 and 1986 were followed in 1987 and 1988 by the drought mentioned above which represented the nation's most costly natural disaster to affect the nation up to that time. Basic to any progress in investigating future flood and drought disasters is better precipitation measurements using technology now available, not the inadequate subsystem now a part of ASOS.\nIt is now that the networks are slowly dying, and it is also now that the need for their data is becoming widely recognized; new models are being refmed using the latest high performance computers, and these new models need radiation as an input. If we take action NOW, we can rescue the operation before it fmally dies and the long-standing expertise is lost. The current debate about UV-B and its possible health consequences underlines the importance of immediate action. resulting from a wanning trend results in decreased surface albedo, increased absorption of solar radiation and additional heat to melt increased amounts of snow. This results in the classic positive feedback mechanism which is included in nearly all climate models. In addition, one of the primary factors controlling how heat is consumed at the surface during spring and sununer is the mass of snow, due to the large amount of latent heat required for phase change. Snow is a major component of the surface hydrologic regime, affecting the freshwater runoff and the local storage of water. Thus, snow cover is both an indicator and an agent of climate change and is important to the study of global change with respect to surface energy budgets and the hydrologic cycle.\nDelay in being able to provide the information needed for policy decisions could result in delay until it is too late to take policy actions in an effective and efficient way.\nIn order to be useful in providing the data for NOAA responsibilities and initiatives, we must begin now to locate, acquire, process, and archive data for long-term to interannual/decadalscale envirol1Il1ental assessment and prediction. The longer we delay aggressive action, the larger the problem.  Fisheries habitat refers to the ecologically significant aspects of the environment that support the reproduction, development, growth, and survival of a species. This may include physical factors (temperature, salinity, depth), chemical factors (dissolved oxygen concentration, pH, toxic chemicals, biotoxins ), biological factors (parasites, pathogens, symbionts, competitors, predators), and geomorphic factors (coastal wetlands, sea grass beds, hard substrates, soft sediment substrates). Common mechanisms of habitat degradation that can adversely impact LMRs include: physical loss of habitat, low oxygen levels due to massive die-offs of phytoplankton blooms caused by coastal nutrient enrichment, changes in freshwater availability to estuaries due to dams/water diversion projects, dredge spoil disposal in the ocean, gravel mining/mineral extraction activities, and chemical contamination by toxic chemicals or biotoxins. Since NOAA/NMFS has legislative mandates to protect LMRs and their supporting habitat, it is imperative to have available to our user community accurate, up-to-date sets of data on the factors responsible for the habitat support value of LMRs and in a pro-active mode the ability to assess habitat quality of an area proposed for anthropogenic modification. This user community includes the Fishery Management Councils (FMCs) and their habitat committees; National Ocean Service (NOS) Damage Assessment Center (DAC) and NMFS Restoration Center (RC), which carry out NOAA's Damage Assessment & Restoration Program under CERCLA and OP A; regional NMFS offices of habitat and protected resources, which perform the agency's consultative role; and the scientists (both internal to NOAA and externally) which use habitat quality information in their research projects. Given the diversity of factors that defme habitat support value/habitat quality and the corresponding impacts of habitat degradation in LMRs, it is not possible to address all of the data quality and continuity issues for the broad range of legislative mandates that NMFS faces in the habitat protection arena. Some areas which seem to be requ:iri..ng greater attention include: damage assessment and restoration, NPDES permits, Endangered Species Act concerns, marine mammal/fisheries interactions, and the impact of aquaculture on coastal environmental quality and wild fish populations. I have contacted two of our major user groups (FMCs and their habitat committees plus the Northeast Region's Division of Habitat and Protected Resources) to obtain their ideas regarding the major problems and their potential solutions. A major data continuity problem, for example, is the long-term follow-up of permits issued by NOAA/NMFS in order to check on permit compliance by responsible parties and the effectiveness of the recommended mitigation/restoration actions. It is difficult to have successful management without an adequate scientific base or a follow-up of past management actions. The stock assessment advice that NOAAINMFS provides to the FMCs is based upon much sounder, broader science and better management action follow-up than is-the -case for-habitat support value. Thus, we h!lve _a better handle on the impact of harvesting on LMRs that we do the impact of habitat degradation. Given the perceived deficiencies in NOAA's habitat support value research and management activities (conducted primarily by NOS and NMFS), a number of groups have made recommendations on the level of resources required to redress these problems. Data Quality and Continuity (DQ&C) concerns are only a small subset of these habitat support value deficiencies. However, any solutions to the DQ&C issues must be carried out within the context of a program that is stretched out thinly and presently lacks adequate resources to accomplish its mission. An FY92 Fisheries Habitat Management proposal recommended a budget of $15.6M to address all of the issues in the NMFS Habitat Policy (more limited programs cost $3. 6M and $10.2M). The National Fish and Wildlife Foundation (NFWF) in its FY94 Fisheries and Wildlife Assessment pointed out that the FY94 base NMFS budget for habitat management was $5.8M, which was less than the FY93 base budget of $6.8M. The FY93 base budget had the following deficiencies: zero funding for the NMFS Habitat Office, only 50 scientists were available nationwide (1.2 per coastal state) to conduct project reviews (10,000 annually), and there was no comprehensive NOAA program to assess either the status or to monitor the habitat requirements of estuarine and marine LMR species. The NFWF recommended in FY92 a budget of $33.7M to adequately fund the NMFS Habitat Management Program. A group of 30 environmental and fishing organizations prepared an FY93 white paper entitled: \"Restoring America's Coastal Environmental Quality-Protecting Marine Fisheries and Their Essential Habitat\". This white paper recommended a budget of $50M for FY93 (a program which would support 300 scientists/technicians). The Friends of the Earth (FOE) FY94 Earth Budget recommended $26M to support the NMFS Habitat Management Program. For the NOS roughly half the budget is used to support environmental programs. This environmental budget was $100M in FY92 and $89M in FY93. The FOE Earth Budget recommends $106.3M for FY94.\nIf we fail to protect endangered species and marine mammals by adopting an approach to restore critical habitat, then they may go extinct and the agency will fail in its stewardship role. Current emphasis in preserving biodiversity focuses on preserving existing habitats that support critical species, rather than waiting until a species is threatened or endangered and then trying to preserve its critical habitat. This is another example of the precautionary principle approach in action. In the case of commercial LMRs the new focus is on ecosystem management of fisheries, which involves a holistic perspective involving commercial species and their competitors and predators, key habitat support elements, and interactions with other components of concern in the ecosystem, such as marine mammals and apex predators (large sharks, tuna, billfish, etc.). One cannot manage commercial fish and shellfish species in isolation from the rest of the ecosystem. As NOAA/NMFS moves toward an ecosystem-based management perspective for LMRs, it will provide the agency with a means to develop effective stock rebuilding strategies, resolve marine marmnal/fishery interactions, and provide effective management strategies for apex predators. .... J- The Reanalysis Project will use a frozen state-of-the-art analysis/forecast system and perform data assimilation using past data, from 1958 to the present (Reanalysis). Moreover, the same frozen analysis/forecast system will be used to perform data assimilation into the future (Climate Data .A..ssimilation System or CDA..S), so that climate researchers can assess whether current climate anomalies are significant compared to a long reanalysis without changes in the system. We are planning to perform the 35 years of reanalysis within 2 or 3 years, so that we need to do one month of reanalysis per day. This has forced us to design a system which is robust and has many advanced features not presently included in operational systems. Some of the characteristics of the first phase of the NMC/NCAR CDAS/Reanalysis are: 0 T62 model (equivalent to a horizontal resolution of about 210 Km) with 28 vertical levels. The model is identical to the NMC global model operational as of late 1993, except for the horizontal resolution, which is T126 for the operational model (Kanamitsu, 1990); 0 SSI (3-D variational) analysis (Parrish andDerber, 1992, Derber et al, 1991); 0 Complex QC (with corrections)ofrawinsondesheights and winds, including time interpolation checks; OI-based complex QC of all other data; 0 Optimal Averaging of a number of parameters over a number of areas (better averages, estimates of error of the average); 0 OI SST reanalysis Qatest reanalysis from R. Reynolds, starting from 1982); UKMO SSTR and ice reanalysis for earlier periods; 0 One-way coupled ocean model4-D assimilation for 1982-onwards (from A. Leetmaa); .. 0 The data input will be pre-processed, and all the analysis output fields will be post-processed with a \"complex QC\" monitoring system, in which the statistics of the data, time tendencies, etc, are compared to climatological statistics in order to detect errors. These statistics include tendency checks. (These monitoring systems will be also implemented operationally and their development constitutes a major spin-off from this project for NMC)."}, {"section_title": "NORMAN CANFIELD UNIVERSITY OF MARYLAND", "text": "The frequency, duration, and amount of rain and snow falling upon the earth is critical to life on this planet as we know it. The great spatial and temporal variability of precipitation and the inherent difficulties of its measurement are basic to our current inability to project global change with regard to floods and droughts, as well as to project the significant impacts of variability of precipitation on water supply, agriculture, and hydroelectric systems. Higher quality, continuous and consistent precipitation records are critical to: No precipitation gage is perfect. There is a great variety in gages operated by meteorological and hydrologic services around the world. However, official NOAA precipitation records generally have come from no more than four types of gages over the past century; and the priority gage (all-season weighing gage with analog recorder) has been consistently used for more than four decades. But now, this satisfactory recording gage is scheduled for replacement by a gage with serious deficiencies, some potentially correctable, but some genenc."}, {"section_title": "CLIMATE DATA CONTINUITY PROJECT OBJECTIVE MICHAEL UHART NWS/OFFICE OF METEOROLOGY", "text": "The objectives of the Climate Data Continuity Project are to determine any systematic differences (biases) that exist between the climate record as recorded by new National Weather Service's (NWS) observing systems and the historical climate record from those systems that are being replaced or upgraded and to document those differences. Such documentation will help minimize the effects of discontinuities and heip ensure ihe integrity of published climatological data and of continuous data sets required for climate research and applications as well as NWS operations."}, {"section_title": "SUMMARY", "text": "The NWS is now commissioning a major new surface weather observing system, the Automated Surface Observing System (ASOS). This new system has been designed to automate and replace, for the mo\u2022st part, observations made by the human observer. The collection of these observations during this century has become a valuable part of the climatology of the U.S. This project will document the differences between the old and new observing systems so that a continuous time series of important climate elements can be built using the discontinuous time series of observations. The scheduling and implementation of this system is a dynamic process with many procedures not yet defined. It is essential that resources be available during initial stages of implementation, when original equipment and observers still remain at the observation site."}, {"section_title": "BACKGROUND", "text": "The National Weather Service's (NWS) proposal for a climate data continuity project was funded under the ESDIM Program for FY 91 and FY 92. The project's objective is to determine any systematic differences that may exist between the climate record as recorded by the Automated Surface Observing System (ASOS) and the historical climate record at NWS First Order stations and to document those differences. The climate variables to be studied in the original project are 24-hour maximum and minimum temperatures and accumulated precipitation. Performance characteristics of the H0-83 temperature/dew point instrument revealed in tests conducted at Tucson, Arizona led to a design review of the ASOS hygrothermometer. A final version of the H0-83 will not be 0 0 available at commissioned ASOS sites until early in FY 94. Investigations of the heated tipping bucket precipitation gauge are continuing. A geographic expansion of the project in FY 94 is prudent, as well as an extension of the period of study at a subset of stations. Although the original project was designed to investigate only the basic climate variables described above, other climate data continuity concerns have been expressed by the climate conununity both directly to NOAA and through the ASOS Climate Working Group (CWG). These can be grouped into three areas: (1) climate research variables that are no longer available through ASOS and the new NWS surface weather observing system (e.g., cloud type, snow on the ground, etc.); (2) variables that are continued but which, because of automation, are observed in a different manner (e.g., wind, wind gusts, total cloud cover, visibility, present weather, etc.) and (3) published climate variables that are observed by new systems.\nChanges, including improvements, in observing systems become part of the \"monitoring system\" upon which we rely for detecting environmental change, understanding human impacts on the environment, and developing models for prediction. NOAA's Earth System Data Information Management (ESDIM) Data Quality and Continuity (DQ&C) Project recognizes that \"sometimes these changes can seriously alter the interpretation of the data and products derived from the measurements\". The ESDIM DQ&C Project is concerned with the scientific integrity and continuity of observing systems, with management of the data from them, and with their purpose. The NOAA is responsible for assembling, processing, archiving, and distributing environmental data. Every line office within NOAA takes measurement of the environment, often with a long-term mission in mind. And, data and environmental observation policies developed by NOAA often serve as models abroad. A workshop was organized by the ESDIM DQ&C Project (NOAA, 1991) to focus the Agency's problems in this area and to articulate policies to address them. When observing platforms and sensors change, when field research projects amass data under different protocols, when data management priorities slip, and when international research programs come and go, the potential for interrupting data quality and continuity increases. An update of the subject at a follow-up workshop on April19-20, 1993, concluded with a call to the NAS/NRC/BASC to help develop \" .. a national policy related to the responsibility of scientists, data managers, and observing system operators and managers to deliver ... highquality and homogeneous data\"\u2022; identify \"specific issues of concern related to critical data bases and observing systems\"; and recommend \"actions to rectify and/or prevent existing and future problems. In the context of data quality and continuity, the BASC is well situated within the Academy to: 0 0 0 know of Academy studies that may impact NOAA's data management systems and be asked by other Academy groups to assist; this is true for example in: NWS modernization (with the CETSY establishing criteria for long-term data retention m the National Archives (with the CPSMA) development of autonomous underwater vehicles for monitoring the ocean and air-sea boundary (with the Marine Board) developing a strategy for monitoring global change (with the Board on Global Change, BGC) a review of the NCDC (with the Board on Earth Sciences and Resources) advising on the formation, content and structure of a \"National Biological Survey\" within the DOl (with the CGER) understand the demands on data management systems due to ongoing and proposed operations (including observations and monitoring for global environmental change) and research raise credibility of monitoring (and data quality and continuity) as a worthy and challenging task, a task that if done well is more cerebral then clerical 1 CETS is the NRC Commission on Engineering and Technical Systems; CPSMR is the Commission on Physical Sciences, Mathematics, and Resources; CGER is the Commission on Geosciences, Environment, and Resources. The Ocean Studies Board, the Water Science and Technology Board, the Polar Research Board, the Board on Global Change, and BASC are among the boards under CGER."}, {"section_title": "ACCOMPLISHMENTS", "text": "Approximately 1-112 years of parallel (ASOS and conventional) observations have been collected by Tom McKee of Colorado State University. A final report on precommissioned ASOS comparisons has been published and a quarterly report on commissioned data has been submitted. Dr. McKee also presented a paper at the 1993 AMS Conference in Anaheim. Data collection continues at 13 commissioned sites. A time extension of Dr. McKee's project was evaluated and granted. His investigation was expanded to look at relative humidity and dew point. A list of 18 climatologically varied ASOS sites at which other continuity studies can be undertaken was coordinated with the CWG and forwarded to NWS management for approval. David Easterling of the National Climatic Data Center documented the changes that occurred in the temperature record due to earlier changes in the NWS observing systems, namely the H0-60 and H0-83 hygrothermometers. The results of his investigation were presented at the 1993 AMS Conference in Anaheim. Briefmgs on the satellite total cloud cover project were attended. The project investigators concluded that the satellite cloud cover products (total cloud cover and cloud tops of douds above 400 rob) provide useful information to the NWS for operational activities and reconunended that the products be implemented for selected ASOS locations to complement their cloud observations. The satellite cloud cover information will be archived by NCDC and combined with ASOS cloud cover observations (clouds below 12,000 feet) for their LCD publication. The satellite cloud cover product will become operational in 1993. Investigations will begin to compare this new amalgamation of cloud observations with current synoptic cloud observations (for research) and the LCD publication. A proposal from NCDC to study ASOS cloud observations and the NWS satellite cloud cover product and has been received and will be awarded this FY. Contacts were made concerning FY 93 funding of act!Vlttes to document ASOS measurements and algorithm evaluation in high precipitation rate events. A proposal from Tom Lockhart of the Meteorological Standards Institute has been reviewed and comments provided. The ASOS wind study will include participation by the NWS Testing and Evaluation Center in Sterling, Virginia and NCDC. A revised proposal is expected shortly with an award this FY."}, {"section_title": "RATIONALE", "text": "Up to this point, only three climate variables measured by ASOS are under investigation-maximum temperature, minimum temperature, and precipitation accumulation. As the ASOS Program develops and other NWS observing systems are modernized, the way in which we observe additional climate variables will change. Many of these will continue a climate record, necessitating an investigation of continuity of the data record similar to the project underway. This project must be responsive to the commissioning of new NWS observing .systems. Several possible candidates for study are currently being considered:"}, {"section_title": "TOTAL CLOUD COVER", "text": "One candidate for investigation in FY 93 and FY 94 will be the satellite derived total cloud cover observations. The ASOS Climate Working Group recommended that total cloud cover observations be available at 6-hourly intervals from 100 staffed NWS locations primarily to continue total cloud cover climatology in support of climate research. NESDIS has been testing a cloud product, using a GOES sounder carbon dioxide technique developed by Dr. Paul Menzel, which estimates the cloud cover and cloud top heights above 12,000 feet MSL. An operational test of this product for specified locations took place in the spring 1992. Results of this test were positive and a recommendation was made that the product be made operational. At this writing, the product is not yet operational. Impacts to cloud clirnatologies and sky cover in the LCD have not been investigated. Data are still being collected on a non-operational basis. It is imperative that an investigation begin as soon as possible to document the continuity of the new cloud observing system (a combination of the ASOS observation below 12,000 feet and the satellite cloud cover product) for LCD users, as well as climate researchers. Many sectors of the economy use wind and wind gust information for planning and postanalysis (forensic) purposes. Concern has been expressed as to the continuity of the record because of the difference in the averaging periods and instrument siting. The Office of the Federal Coordinator for Meteorological Services and Supporting Research (OFCM) is currently developing standards for wind observation and archival of wind information."}, {"section_title": "IDGH RESOLUTION DATA", "text": "In addition, these new observing systems will provide new data that could be important in documenting the climate. Examples of such data bases are high frequency wind and rainfall observations. Data collection, arcPjving and distribution procedures as well as design of a responsive climate data base need to be developed. The NWS's Office of Hydrology and Office of Systems Operations are investigating the continuity issues with regards to high resolution rainfall data, the effects of high rainfall rates on the accuracy of the heated tipping bucket gage,.and the performance of the gage during frozen precipitation events. This project will. provide additional support of activities started in FY 92. The OFCM is consolidating requirements for ASOS wind speed data to support engineering applications and hurricane research. High resolution ASOS wind data may be needed to support those requirements real-time and/or retrospectively."}, {"section_title": "SUNSHINE/SOLAR RADIATION", "text": "Minutes of sunshine are now observed and reported by the NWS observer, used as a forecast variable in the NWS agricultural forecast, and published in the LCD. There is a sunshine switch under development by the ASOS program office and it will be tested in Brownsville this summer. An analysis of the discontinuity between the current, semi-automated observing system and the totally automated ASOS system will be required."}, {"section_title": "LONG RANGE FORECASTS", "text": "Routine, operational, surface weather observations provide a rich source of data for climate monitoring, research and impact assessment. These data remain useful only to the extent that they represent a stable and documented record of surface observations, instrumentation and observing practices. These routine observations (both surface aviation and cooperative) also provide the historical data base and verification data for long range forecasts (the Monthly and Seasonal Weather Outlook) routinely produced by the Climate Analysis Center. The skill levels of these NWS operational forecasts is measurable but modest. Changes in the observations systems could complicate, or even completely invalidate, the usefulness of these forecasts through the introduction of bias in the verification data base. Research and development efforts to improve these forecasts could also be hampered through the introduction of undocumented biases to the data base. Since the development of ASOS by the NWS represents fundamental changes in the surface weather observations there is considerable concern about the impact of these changes on the accuracy and reliability of the NWS long range forecasts. Studies must be conducted to determine the affect of the changes in the observing system. The NWS is the largest contributor to climate information in the U.S. The management of this information and the coordination among \u2022NWS offices and external groups is a full tiTile job. The NWS requires a full-time permanent position to be the focal point for coordination of data management issues within NWS and with other Federal agencies, to support the NWS member of the ESDIM team, and to assist the project administrator for the Climate Data Continuity Project and other funded data management related projects. The position is at the GM-13 level and is administratively responsible to the Chief, Systems Requirements Branch of the Office of Meteorology, Programs, Plans, and Development Division. The NWS began commissioning ASOS in 1992 and doppler radar systems will begin commissioning in 1993. At the moment, plans and procedures have not been firm enough on which to develop additional climate data continuity studies. However, as more details are known, solicitations can be made for such studies. Even though principal investigators and budgets cannot be specified at this time, it is imperative that funds be available for such studies when the systems are commissioned and studies must begin. Climate data continuity cannot wait for next year's fimds. The first ASOS was commissioned in September 1992. As of May 1993, 19 ASOS sites had been commissioned and 220 \"accepted.\" Eleven more ASOSs will be commissioned through August 1993. A satellite-derived total cloud cover product will become operational in FY 93. NCDC is archiving ASOS high resolution data. A Program Development Plan and NOAA FY 95 funding request for the modernization of the cooperative observer network was submitted and is being reviewed. FY 94 will continue to be an important year in the study of climate data continuity because of additional ASOSs being commissioned and the conversion of the LCD to the new ASOS format. The high resolution data and the ASOS \u2022 daily and monthly sununary statistics must be captured now, even before there are approved projects to study continuity and quality control, because if we don't, the opportunity to use them in comparison studies will be lost forever. The temperature/precipitation continuity study will begin official data collection on or about November 1993 (the final H0-83 configuration should be approved and in place by then) and continue through FY 94 and into FY 95 to accumulate enough comparative data. Relative humidity and dew point studies will be added to the investigation. A final report and peer reviewed publications will be presented in FY 94. Papers will be presented at the annual AMS conference on tliis project. The final report on precommissioned\u2022 ASOS data will be distributed. Quarterly reports will continue to be submitted by the investigators. A project will begin to investigate the continuity of cloud observations, based on preliminary investigations done in FY 92 and FY 93. It will be a cooperative project involving the University of Wisconsin, Roy Jenne of the Nation~l Corporation for .A . . . ~ospheric Research (NCAR), and NCDC. The Principal Investigator will be selected and a project plan written (January 1994). The NWS will be testing an ASOS sunshine switch in Brownsville this summer. A proposal ~ will be solicited to study the continuity of sunshine data and document the change in the sunshine observing system. A Principal Investigator will be selected and a project plan written. The Climate Data Continuity Project Plan will be promulgated (August 1993)."}, {"section_title": "LINKAGES", "text": "ASOS is a multiagency program. ASOS generated climate data sets will be assembled and stored at national data centers for maintenance and distribution to retrospective users. The collection of temperature, precipitation, and many other parameters is important to the understanding of the water balance and the energy processes encompassing the water cycle. For a thorough understanding of these processes it is important that ASOS support the data continuity of atmospheric parameters so that basic and applied research in atmospheric dynamics and the hydrologic cycle can be advanced to predict the variations of the atmosphere and water resources caused by global and regional influences. For example, data continuity is important for the GEWEX Continental-scale International Project (GCIP), which is an activity of the Global Energy and Water Cycle Experiment (GEWEX). A major field component of this experiment is to be centered in the Mississippi River basin. The NWS CDCP was presented and discussed at the NOAA Data Quality and Continuity Workshop in April1993 and the OFCM's Interdepartmental Committee for Meteorological"}, {"section_title": "11-A", "text": "Services and Supporting Research Task Group for Surface Instrumentation Standards meeting in May 1993 .."}, {"section_title": "PERSONNEL", "text": "Mr. Andy Horvitz was hired by the Systems Requirements Branch of the NWS to be the data management focal point and to assist the project manager of the Climate Data Continuity Project and other funded data management related projects."}, {"section_title": "BUDGET", "text": "The NWS proposes that the Climate Data Continuity Project be authorized and funded at the $232K level in FY 94, inciuding $6T!C flillding of the GM-13 cli.:nate data manager position. long-term climate change research (data set of choice by researchers) weekly NWS climate anomalies and drought assessments NWS forecasts and warnings (including flooding) tens of thousands of requests armually for data to support a wide variety of economic activities (Figure 1) ground truth for precipitation calibration of next-generation weather radar (NEXRAD) monthly national agricultural production estimates by USDA daily public service reports (media) THE PROBLEM 1. Inferior Data/Missing Data: Despite the whole-hearted dedication of the great majority of residential observers, there are still a substantial number who fail to collect accurate and complete data. This situation reduces our ability to describe and assess climate anomalies and detect climate. change."}, {"section_title": "12-", "text": ""}, {"section_title": "Network Instability:", "text": "Although there are a select number of cooperative stations around the country that have remained in one place with little change in . the surrounding environment for about 100 years, these sites are the exception rather than the rule. The majority of stations last no more than 15 years ( Figure 2). Additionally, the number of stations is steadily declining (Figure 3). Since 80 percent of stations are located at private residences, as people move, age, or die, their property changes hands and the new owners might not want to take over the station because of the responsibilities associated with it. Thus, the station closes or a new observer is found in a different location. This situation reduces our ability to describe and assess climate anomalies and detect climate change."}, {"section_title": "13-A", "text": ""}, {"section_title": "Recreation", "text": ""}, {"section_title": "Different/Changing Observing Times:", "text": "Manual instrumentation for observing temperature and precipitation in the network makes the network dependent on whatever 24hour time of observation is convenient for each volunteer observer. Thus, many different times of observation exist. Comparison of data between stations with different (but consistent) observation periods can lead to inaccurate conclusions about climate variability. Changes in the time-of-observation at given stations are also a common occurrence. This situation also introduces as much as a 3 o Fahrenheit bias ( Figure 4) into monthly average temperatures (as compared to a midnight-to-midnight calendar day period). . . 4. Instrumentation Changes: Although the cooperative network instrumentation has been relatively stable compared to the NWS staffed (first order) stations, there are still problems. There have been several generations of liquid-in-glass thermometers introduced into the network with no bias measurements. In addition, totally new electronic instruments for measuring temperature (the Maximum-Mininuun Temperature System or MMTS) have been introduced in recent years without functional, bias, and comparability tests having been conducted. These tests are critical to \"adjusting\" individual station temperature records for purposes oflong-term data continuity. In addition, overlapping observations at sites, many of which also saw the new instruments installed at a different location (still considered the same station) have not occurred. Instrument biases can account for changes of about 1 o Celsius in monthly temperature averages. Small moves alone can also introduce a enviromnental bias of several degrees Celsius in monthly averages."}, {"section_title": "14-", "text": ""}, {"section_title": "Lack of adequate station\u2022 documentation:", "text": "More emphasis needs to be placed on maintaining and coordinating station histories. Much is lacking in the current system. Some of the inadequacies result from weak or incomplete policies. Others result from a conflict between requirements for real-time data versus long-term data continuity. In any case, complete investigations oflong-term changes or trends in temperature, precipitation, the   frequency of destructive phenomena, or snow may be seriously weakened or flawed without proper station histories, including changes in observation times, routine maintenance visits, changes in exposure, location, and condition (cleaning etc.) of instruments, and changes in observers. 6. Changing Observers: Since observations are exclusively manual, the system is destined to non-systematic errors that occur when a new observer begins observations at an already existing station with a record.from a previous observer. Although a bias may occur in temperature and precipitation observations when a new observer takes over observations at an existing station, snowfall observations are especially prone to observer bias. Again, longterm data analysis of the station's data set can be significantly weakened when this situation occurs without proper consideration. 17-A"}, {"section_title": "SOLUTIONS AND COSTS", "text": "Plans have been developed over fue past few years to accomplish a modernization of the cooperative observing network which would help minimize many of the data continuity problems described. Total cost offue modernization is estimated at about $61 million. This proposal needs to be funded so that implementation of solutions to the identified problems can begin. In addition, NOAA needs to delineate policies regarding long-term data continuity and provide program managers with guidelines. These policies must consider the long standing conflicts that frequently exist between real-time requirements for observations and requirements for data in the decadal-to-century and longer time frames. Policies must also consider resources available to accomplish the tasks. DATA QUALITY AND CONTINUITY ISSUE"}, {"section_title": "THE MEASUREMENT OF RADIATION AT THE SURFACE BRUCEIDCKS OAR, AIR RESOURCES LABORATORY THE PROBLEM", "text": "There are many purposes for measuring radiation components at the surface, few of which impose common demands on the sensor system. For example, solar energy requirements are usually for data on the flux of radiation falling on a tilted surface, for both shortwave and longwave components. Surface energy (and hydrological) applications typically require data on the net radiation. Radiation measurements intended for revealing changes in atmospheric opacity extend the requirements to specific radiation components --the diffuse or direct beam. In each such application, there are different requirements for wavelength specificity that can lead to considerable frustration when sensor systems that are well suited for one particular purpose are then found to be unsuitable for another. Furthermore, there are newly emerging issues that are of current importance, such as the need to provide ground-truth for satellite systems, the need to relate radiation characteristics to cloud cover, and the increasing awareness that there may be a trend in UV-B radiation (due to decreasing stratospheric ozone) that would have health consequences. There are long-standing networks that measure different radiation variables at the surface. Principal among these is the NOAA Solar Radiation (SOLRAD) network, that has been reporting total visible radiation to a horizontal surface and the direct solar beam for several decades. This array of about 20 presently-operating sites is currently being dismantled, as a result of the modernization and site automation efforts of the National Weather Service. In this respect, a key consideration is that accurate measurement of all surface radiation variables requires frequent attention by a qualified technician, to ensure that all optical components are kept clean. At the same time as this existing network operation is being disrupted, demands for its information are rapidly expanding because of (a) growmg interest in solar energy, (b) expanding modeling capabilities that make use of the radiation data, and (c) the current great demand for information on radiative effects of clouds and aerosols."}, {"section_title": "19-A", "text": "The SOLRAD network provides information that is relevant to such interests, but which is of limited utility in the context of the surface energy balance. For this latter purpose, a more complicated set of measurements is required --especially including infrared radiative exchange. A new network of SURface RADiation (SURFRAD) sites is now being set up, as a component of studies focussing on the surface water balance of the central USA. SURFRAD promises more extensive information than has ever previously been provided by a radiation monitoring network, but at far fewer sites. Development of the \"hole\" in stratospheric ozone over the Antarctic continent and the reported subsequent increase in ultraviolet (UV) radiation has prompted concern over the potential for increases in W radiation worldwide. In the northern hemisphere at mid-latitudes, for example, ozone reductions of 3-5% per decade have been projected from satellite arid other data. This has prompted the development of programs to measure UV irradiance in Canada, New Zealand, Australia, Europe and the United States. Principal support for the U.S. effort is provided by the U.S. Department of Agriculture (USDA), the U.S. Environmental Protection Agency (EPA), and the National Oceanic and Atmospheric \u2022Administration (NOAA). Of particular concern is radiation in the W spectral region commonly referred to as the W -B (290-320 nanometers), which has been shown to pose a threat to human health and to plants and animals. Unfortunately, irradiance measurement in this spectral region presents a very difficult problem with respect to instrumentation and places stringent demands on instrument characterization and calibration in order to provide data of unquestioned quality."}, {"section_title": "THE IMPACT OF THE PROBLEM", "text": "NOAA is well known for its claims to be the earth systems data agency, yet is widely recognized for its failure to assure continuity of the single record most basic to climate concern-the record of how much solar energy is available at the surface to drive the global atmospheric heat engine. The temperature of air near the ground is governed by the solar radiation received at the surface and by the nature of the surface. If a correct interpretation of changing air temperatures is to be made, then surface radiation measurements are critical. NOAA scientists are at the lead of international programs that are constructed to produce reliable surface radiation data around the world, yet within the USA these same scientists have been powerless to prevent their own agency from disregarding and now dismantling the observational array on which the rest of the world is building. This situation borders on ludicrous, and is one from which we must escape."}, {"section_title": "21-", "text": ""}, {"section_title": "IMPACT OF NOT TAKING ACTION", "text": "Continuing inability to quantitatively assess the interactions between snow cover and climate and to accurately model the positive feedback mechanism between global extent of snow cover and various energy balances parameters. Restricted advances in models which rely on the improved and integrated snow cover data sets for input, parameterization, and validation.\nThe current state of ignorance of polar ocean ice balance sensitivity to greenhouse induced clh+nate perturbations and sea ice effects on ocean deepwater formation rates and identified episodes of freshening in the North Atlantic surface waters, will continue."}, {"section_title": "WHY NOW", "text": "The data sets, both surface and remote sensing, required to implement the data ~tegration described here are currently available, although locations are scattered and data types and formats are highly variable. At this time operational links are in place between NSIDC and the other agencies and institutions holding snow cover data necessa..ty to facilitate tbis plan. Thus the consolidation and standardization of these data could be greatly facilitated if such a plan were undertaken immediately.\nThe satellite systems with required sensors and partial arrays of moored sonars are essentially in place. The development of a prototype scheme can be implemented in readiness for EOS and Mission to Planet Earth and NOAA's Climate and Global Change Program. In the interim, until new systems are developed, it is essential that existing JIC weekly ice chart products be maintained and that 1-2 years of overlap be provided in order not to break the temporal continuity of time series ice data."}, {"section_title": "-B", "text": "DATA QUALITY AND CONTINUITY ISSUE"}, {"section_title": "POLAR SEA ICE ROGER G. BARRY UNIVERSITY OF COLORADO/CIRES", "text": "Polar sea ice varies in extent, percent cover and thickness on annual decadal and long time scales. These variations affect climate in middle, as well as high, latitudes through the effect of the ice cover on the surface energy balance and weather systems. Ice that is transported to lower latitudes by ocean currents and wind aiso determines the deep water formation in the world ocean. Changes in polar sea ice projected by global climate models indicate substantial shrinkage of Arctic sea ice and lesser changes in the Antarctic. Records of ice conditions are required to: 1) detect the greenhouse signal; 2) quantify the sensitivity of the climate and ocean system to internal feedbacks; 3) adapt to expected changes. Weekly maps (TIC) of sea ice extent on a global scale are accurate only for the last 40 years in the Arctic and 20 years in the Antarctic Reliable maps showing fractional coverage (concentration) and ice type span only 20 years. Ice thickness is poorly known, even for mean values. All three are required to monitor changes in ice mass. Extent, concentration and ice type data are obtained by satellite passive and active (radar) microwave observations and thickness moored by upward-looking sonar or from submarines. Currently the latter data are not routinely available and radar data are only available for limited areas and times (from ESA and Japan). Moored sonars are just being deployed on a research basis."}, {"section_title": "SOLUTION", "text": "An integrated ice mapping and data archival system is required, with inter-agency cooperation. Passive microwave data are collected operationally by DMSP satellites and processed for the research community using validated algorithms; new algorithms for thin ice types, snow on ice, and melt puddling are under development. Ice type and small open water areas can also be mapped by combining passive microwave and radar data. ERS-1 and JERS-1 radar data and geophysical products are processed at Alaska SAR facility. Sonar data from moored buoys are being collected at the Norwegian P<Jlar Institute and University of Washington. A coordinated system of data acquisition is required to include: 2) maps of ice extent, ice concentration, and type based on consistently merged (visible, PMR, and radar data from the present suite of satellite sensors; 3) modeling of ice mass in both polar oceans and of ice transport/melt using operational sea ice models and NMC winds with assimilated ice data."}, {"section_title": "UPPER AIR SYSTEM CHANGES AND DATA CONTINUITY CARL A. BOWER, JR. NWS/OFFICE OF SYSTEMS OPERATIONS INTRODUCTION", "text": "Upper air system changes can invariably impact data continuity as a result of system improvements and operational changes. These improvements and operational changes do impact the quality of real-time data collection and ultimately long-term data continuity. Data continuity is continuously being addressed in future upper air operational and policy decisions."}, {"section_title": "DISCUSSION", "text": "The results of data quality changes on data continuity are well documented, NOAA/ARL (Gaffen) has addressed the use of different radiosondes over time, practices, and the associated impacts to data continuity. Change-over to different types of radiosondes, types of sensors, or processor software do result in changes to climatological time series data sets. The Naval Research Laboratory, Marine Meteorology Division (Baker) has studied the United States upper air network and has found height anomalies in the intermountain areas of the United States. These anomalies are associated with one type of radiosonde. The WMO has also documented concerns with the upper air data in Alaska using this identical radiosonde. Many of the data quality anomalies have rational explanations and can be mitigated if the explanations are documented as part of the data archive. Data quality and continuity audit trails, however, are difficult to reconstruct because of insufficient documentation."}, {"section_title": "SOFTWARE UPGRADE", "text": "The National Weather Service is in the process of validating and implementing a new operational MicroART software load to be used with the Automatic Radiotheodolite systems. This new load designed as version 1.52 will replace the current 1.45 version. Several changes in the software proposed by the research community will improve data quality and at the same time, .cause perturbations in data continuity for all locations. Three changes that will impact data continuity are: a. The change of relative humidity reporting practices to allow relative humidity to be reported below 20 percent and to report relative humidity throughout the sounding 1 -c regardless of the previous restriction to temperatures warmer than -40 degrees Celsius. b. Correct the gravity constant to 28.27095. c. Correct the low relative hmnidity bias for high relative hmnidities by changing the value of the parallel resistor in the circuitry from 1.2 megohm to 1.0 megohm. The change in the gravity constant will have a cumulative effect on geopotential height calculations from the surface to flight termination of 20 geopotential meters or more. Heights previously reported up to 30 Km altitude will be lower than those reported with earlier MicroArt software versions. Software changes to incorporate the proper resistor in the hmnidity circuitry used for the VIZ radiosonde will increase relative hmnidities reported in near saturated conditions by around 3 percent. The impact for relative hmnidities below 70 percent is negligible. Comparative differences between the current software and the new software are shown in Figure 3. The changes in high relative hmnidity from the two software loads are shown in Figure 4. Two other changes in the software will be reflected in the hmnidity values. One change will be to report relative hmnidities below 20 percent as contrasted to past practices when relative humidities below 20 percent were reported as 19 percent in the National Climatic Data Center archive. The other change will eliminate the minus 40 degree Celsius threshold for reporting relative hmnidity. The previous practice reported hmnidity as 777.77 when a temperature less than minus 40 degrees Celsius was encountered."}, {"section_title": "RADIOSONDE CHANGES", "text": "The National Weather Service recently completed the establishment of a new radiosonde Qualified Products List. Radiosondes on this list are eligible for procurement. The new list has two new radiosondes that are significantly different from radiosondes we have used in the past. If these radiosondes are procured, their performance characteristics will be different from currently used radiosondes. Although these radiosondes have different characteristics, they fall within standards used to evaluate their performance at the National Weather Service Sterling Test Center. These new radiosondes were evaluated in environmental changes, and in\u2022 the atmosphere through functional precision and functional comparison testing. In addition to the evaluation at Sterling, Virginia, the radiosondes were tested at NASA Wallops facility to determine true heights and to determine radiation corrections necessary for the thermistors. Although these radiosondes meet standards, their possible operational use could create discontinuities in long-term data sets. Operational policy decisions may impact data quality and data continuity. During the most of 1992, the National Weather Service was operating under a no-second release policy for failed radiosonde flights because of budgetary constraints. The Senior Duty Meteorologist at the National Meteorological Center approved requests for second releases if their data were deemed necessary for global models. About one-half of the requests for second releases were approved. The no-second release policy was discontinued in October 1992. Sensor corrections (except for pressure offsets) are not currently applied to radiosonde data transmitted over the Global Telecommunications System. The National Weather Service position has been that adjustments to observed variables to account for radiation effects, or other sonde compatibility considerations, are the responsibility of t..lte user of t.l].e sounill11g information. This is based on the fact that such adjustments are likely to change as more data on radiosonde response and compatibility tests are aggregated. This policy is currently under review. Applying data corrections by the user is more easily accomplished than trying to back corrections out of historical data bases by users trying to construct long-term continuous data sets."}, {"section_title": "MODERNIZATION", "text": "Upper air station site location changes, while not impacting data quality, certainly impact long-term data continuity. As part of the National Weather Service Modernization Program, a number of stations in the United States network are being relocated. In the Southern Region alone, 7 of 22 stations will be moved (see Figure 5). These station changes should have minimal impact on global models, but will present challenges to the data community using station specific long-term data bases. The National Weather Service is planning for a replacement upper air system in the late 1990s. As the first step in the replacement system effort, a contract was placed with the National Center for Atmospheric Research to build a \"proof-of-principle\" experimental prototype. This system is nearing completion and should enter acceptance testing this summer and undergo four-season environmental testing in all National Weather Service regions. The system will use state-of-the-art technology and will be based on a navigationbased LORAN-C and OMEGA wind fmding capability. Wind determination using the navigation-based wind fmding capability will be more accurate in time and space. The replacement upper air system will be part of the NOAA Office of Atmospheric Research FY 95 Budget Initiative. Plans call for having the meteorological research community involved in developing high resolution data transmittal and archive requirements."}, {"section_title": "CONCLUSION", "text": "The NWS upper air program is an ever changing program driven by numerous requirements. Under the best of circumstances, a high quality data gathering operational program is not always in harmony with long-term data continuity considerations. The planned upper air replacement system data output will not be transparent to a continuous data base. Improved meteorological element sensing and time and spatial accuracy will be noticed. One critical missing element between the operational data base and the long term data base lies in documentation. Documentation required for constructing audit trails on long-term station specific data continuity is often insufficient to make appropriate adjustments to data and to ensure long term continuity. Better documentation and records will improve continuity. The National Weather Service is committed to managing changes to upper air systems, radiosondes, and their associated data sets.  . Relative humidity differences between MicroART Versions 1.45 and 1.52. The MicroART Version 1.52 yields higher humidities than the Version 1.45. The difference in the two Versions was the replacement of the 1.2 megohm resistor value in the VIZ relative humidity equation with the correct 1.0 megohm value.     DATA QUALITY AND CONTINUITY ISSUE\nSuch changes as these may well mask a real underlying climate shift or falsely suggest one that is not occurring. Those who find themselves threatened by proposals to reduce emissions can cite these data continuity issues to argue that it is premature to take any action. Data integrity should not be an issue in policy decisions. These problems arise not because of incompetence in the weather services but because of their desire to improve observations for the daily forecasts. Difficulties. evaluating climate records come from a lack of commitment to ensure their integrity. This may be because real concern over climate change is a recent phenomenon. Nevertheless, we will not be able to answer the serious questions about the climate until such a commitment is made."}, {"section_title": "MEASUREMENT OF UPPER AIR TEMPERATURE, MOISTURE AND WIND THE PROBLEM WILLIAM ELLIOTT AND DIAN GAFFEN OAR, AIR RESOURCES LABORATORY", "text": "The v1orries about global vt&Yir...ing are based on model projections v:l'..ich need observational support. Upper-air temperatures and moisture patterns are expected to change in response to increased greenhouse gases, but their changes will not necessarily be the same as at the surface. For instance, temperatures in the stratosphere are expected to cool as the surface and troposphere warm. At present, radiosonde observations provide the only extensive record of changes in the upper air. The radiosonde record has several limitations, however. The reliability of humidity data is low in the upper troposphere and lower stratosphere where even small changes in moisture can have significant effects on climate. Coverage is not uniform over the globe; there are large gaps, particularly over the oceans. In addition, there have been and continue to \u2022be-frequent changes in radiosonde instrumentation and data reporting practices. These changes can produce apparent changes in temperature and moisture as large or larger than those expected from greenhouse gas in.creases and so corifound the true signal.\u2022 Unfortunately, there is no systematic record of changes which can be relied upon by researchers. Also, there is no standard against which all radiosondes can be measured. (There are a number of different types in use around the world: even the NWS has two suppliers.) The upper-air observing system is not designed or operated for the detection of climate change."}, {"section_title": "SOLUTIONS", "text": "(I) NOAA should maintain a record of all changes in instruments, reporting practices, station locations, and data reduction algorithms. It must be accessible; future researchers must know where to fmd the information. Putting together a history of the U.S. radiosonde system now leads one to all sorts of docmnents, memories, etc. but not to a systematic metadata source. It is not just sensor changes that must be docmnented. Recording practices and algorithms for treating signals from the sondes and for converting observed quantities into other variables can change. Corrections or adjustments, such as radiation corrections of the temperature, are sometimes applied to raw data. An effort to redo all station histories should be undertaken soon. All this information must be widely available both inside and outside NOAA. Publication in the open literature is strongly recommended. (2) NOAA should join other agencies in the development of a Reference Radiosonde. Such a reference may incorporate advanced technology and could cost more than conventional sondes. Other sondes would be compared to it, including those from other nations and possibly ones in use during earlier times. Such a system could also be referenced in specifications for procuring operational sondes. Several groups are currently working toward such a reference system: NCAR with DOE support; NASA and Air Force scientists are working on a sensors that could also be used in such a sonde. Strong consideration should be given to bringing the NIST into such a program. (3) NOAA should develop a long-term climate monitoring system complementing the daily weather observations. Many of the problems raised above come about because weather observing networks have a function different from monitoring climate, so no one is held accountable for the quality of the climate record. The system need not rely solely on radiosondes, even improved ones. It could incorporate remote sensing, both ground-based and from space. NASA scientists have already given thought to small, relatively inexpensive satellites for monitoring climate. (Note that NASA's EOS program is not designed to be a climate monitoring system.)"}, {"section_title": "IMPACT OF IGNORING THE PROBLEM", "text": "Before the public will support drastic actions to slow possible greenhouse warming, they must believe the threat is credible. This will only come about when observations support to the models. There are scientists now who believe the threat has been overstated and they often cite inhomogeneities in the climate record to support their contention. Conversely, should the record suggest the apparent warming trend has ceased, there will be attacks on the data from a different quarter. It could be as costly to institute unnecessary controls as it would be to be unprepared for significant climate changes. Whatever happens, the integrity of the climate record should not be at issu:e. NOAA intends to make a number of changes in its present radiosonde system, including station moves and possibly new vendors. It is also planning an entirely new system with different sensors and positioning techniques. The reference radiosonde, and careful documentation, will allow these changes to be assimilated by those trying to understand climate changes. It will take a number of years to develop a true climate monitoring system. If work on this is not started now and soon put in place, NOAA observations may not be reliable enough to diagnose upper-air climate change.\nMany data, critical for solving enviromnental problems vital to the well being of the Planet are not readily available to the scientific community. The impact on the decision making process could be critical to major economic and ecological issues. These issues include, inter alia, global warming, pollution monitoring, energy-related enviromnental assessments, renewable resources utilization, and predictive modelling."}, {"section_title": "UPPER-AIR DATA CONTINUITY AND THE DETECTION OF CLIMATE CHANGE WILLIAM ELLIOTT OAR, AIR RESOURCES LABORATORY", "text": "Lower-middle tropospheric moisture and free air temperatures have been identified as being among the most useful variables to monitor for detecting effects of greenhouse gas (GHG) increases. The latter are measured routinely in the radiosonde network, but how satisfactorily is an open question. Problems with homogeneity of existing data preclude giving definitive answers to the question of whether the atmosphere's behavior has been consistent with model estimates."}, {"section_title": "PREDICTED CHANGES", "text": "Surface temperature changes by GCMs have received the most attention. Present time-dependent models suggest a greenhouse-gasinduced increase of about 0.3 oc/decade in global average surface temperature during the next few decades. Aloft, the rate of increase should average somewhat more, since most equilibrium models show a bit more warming in the midtroposphere. In the stratosphere, these models suggest cooling rates, increasing with height, that are greater than the warming rates. Thus we are trying to detect long-term warming or cooling rates on the order of0.5\u00b0C/decade."}, {"section_title": "NATURAL CHANGES IN UPPER-AIR TEMPERATURES", "text": "The functional precision of U.S. radiosondes is about 0.3\u00b0C, well within the WMO recommendation of 0.5\u00b0C. The intramonthly midtroposphere temperature standard deviations, reflecting day-to-day changes in \"weather\", vary from 7-8\u00b0C (or even greater) over mid-latitude continents in winter to occasionally less than 1 oc in the tropics. On longer time scales, El Ninos produce season-to-season variations on the order of 0.5\u00b0C in the troposphere, and volcanoes can cool the troposphere by 0.3-0.5\u00b0C, while warming the stratosphere up to l 0 C. Also, the QBO can contribute fluctuations of 1 oc. There may well be other interannual temperature changes not related to changes in greenhouse gas warming. Detection of long-term climate changes of0.3 to O.SOC per decade in the presence of these other effects requires substantial continuity in the basic data set."}, {"section_title": "INHOMOGENEITY OF TEMPERATURE RECORD", "text": "Several decades of observations will be necessary to detect a trend of this magnitude with statistical confidence. We do not now possess an upper-air data set of the necessary Planned improvements in U.S. upper-air measurements will put more inhomogeneities into the record as will up-coming changes in Vaisala radiosondes. It is most unlikely that there will ever be a multi-decade period without substantial changes in the basic upper-air measuring systems. ."}, {"section_title": "\u2022 \u2022", "text": "The effects of past changes have not been small. For instance, Gaffen, iri a fmihcoming study of radiosonde data, finds temperature \"dropped\" about 0.1 oc at 850mb, 0.8\u00b0C at 500mb and almost 3.0\u00b0C at 100mb when Israel switched from a French to a U.S. sonde. She finds drops of nearly 2 oc at high altitudes upon the introduction of radiation corrections for British instruments, and fmiher notes that improvements in lag characteristics of the Finnish sonde could have given an apparent cooling of about 0.5\u00b0C at 500 mb. Even changing the length of the train beneath the ascending balloon can produce changes in reported temperatures of up to 1 oc. These inhomogeneities have to be considered before any assessment of climate change can be carried out. The U.S. has introduced a new sonde, produced by Space Data Division, into its network. Although purportedly procured under the same specifications as the VIZ instrument, the differences between them are large enough so that NMC adjusts the SDD values to agree with the rest of the network. Comparisons of SDD and VIZ radiosondes show differences of over 1 oc about 25% of the time at low pressures and temperatures and nearly 0.5\u00b0 C at somewhat higher temperatures. To complicate matters more, the U.S. may introduce Vaisala instruments into the network, and phase out the SDD instrument. The introduction of new sondes into the U.S. network will certainly suggest a climate shift to the unwary."}, {"section_title": "UPPER-AIR HUMIDITY", "text": "The upper-air humidity record is even more variable as there have been more changes in humidity instrumentation than in temperature sensors. For instance, apparent changes on the order of 20% RH were caused by changes in the design of the housing of U.S. sondes. Changes of several degrees in the dewpoint can be found in other nations's records associated with instrument changes. Even algorithms to convert measured RH to reported dewpoint differ around the globe, giving lise to situations where identical measurements of temperature and RH result in different reported dewpoints."}, {"section_title": "DATA QUALITY AND CONTINUITY ISSUES IN PROCUREMENT POLICIES WILLIAM ELLIOTT AND DIAN GAFFEN OAR, AIR RESOURCES LABORATORY", "text": "This issue is relevant to all NOAA global change programs, not just upper-air data, as evinced by similar problems with XBT program. We discuss here the procurement of radiosondes, as we understand it, as an example of ignoring data continuity issues in the procurement process. NWS now has a Qualified Products List (QPL) which allows those on the list to bid on supplying NWS with radiosondes. (The sondes in current use, from two separate vendors, have been \"grandfathered\" into the system: whether this means they will always be on the QPL, regardless of improvements in technology, is not clear.) To get a sonde on the QPL a vendor submits samples for testing against some previously issued specifications. If the product meets the specifications, it goes on the list. Subsequently, when radiosondes are to be procured, vendors on the QPL submit bids and cost becomes the only criterion for selection. There is a counter-incentive for a vendor to produce an instrument better than the specifications called for. A much better product, only slightly more expensive, would not be procured, even if it were a better value. In fact, NWS could reduce the accuracy requirements, as it has apparently done for ASOS temperatures, and so degrade the system. There were spatial and temporal inhomogeneities introduced into U.S. upper-air data in 1988 by the addition of a new vendor's product into the network. Even though the specifications were presumably the same for two vendors, their radiosondes produce different readings. It is plauned to have bidding on a two-year cycle so there may be an apparent two-year cycle in the upper-air record. There is the possibility of losing the vendor who has supplied the sondes in use over the last 3 decades. Because NWS observations are the main source of U.S. climatological data, NWS should take the requirements of the climate record into consideration when procuring observing systems. The entire procurement process for obtaining radiosondes should be reviewed."}, {"section_title": "11-c", "text": "Those concerned with the integrity of the climate record should be involved in the procurement process before specifications before are issued. Note that a Reference Radiosonde could help the procurement process by providing a standard for comparison. Upper-air temperature trends, based on radiosonde data, have been reported to be a 0.1 to 0.3 C degree increase per decade in the troposphere and about a 0.6 C decrease per decade in the lower stratosphere. The magnitude of these changes has been shown to be consistent with temperature changes expected from increasing carbon dioxide and decreasing stratospheric ozone. It is important for any changes in radiosonde instruments and data derivation procedures to be closely controlled and documented to assure the capability of using the long-term radiosonde data record to detect atmospheric changes of the order of 0.1 to 0.6 degrees per decade. This note highlights areas of possible concern, as they pertain to the record of NOAA radiosonde data. First, implications of NWS purchasing of radiosonde instruments from multiple vendors will be discussed. Second, recent information on radiation corrections to U. S. VIZ radiosonde data will be reviewed. Finally, issues involved in applying corrections to radiosonde data at the observing sites will be discussed."}, {"section_title": "RADIOSONDE INSTRUMENTS", "text": "The National Weather Service used radiosonde instruments purchased exclusively from VIZ Co. from the 1950's to 1988. Since 1988 about one third of the radiosondes used are from Space Data Corporation (SDC), and two thirds are from VIZ. A careful evaluation of differences among radiosondes is made by the NWS as part of test and evaluation procedures. Average differences of more than 1 C degree were found between VIZ and SDC temperatures. Temperature differences of this magnitude would potentially have significant impact on numerical weather analyses, and perhaps forecasts. On the basis of these findings, messages were sent to the international user community, notifying them of these findings and the schedule as the new sondes were introduced at each NWS station. Average temperature differences of the order of 1 C degree also have potentially serious implications for determining climate change at stations using different radiosonde instruments."}, {"section_title": "RECOMMENDATION: Publish", "text": ", in the open literature, results of instrument evaluation tests, and changes in instruments used at each NWS station."}, {"section_title": "RADIATION CORRECTIONS TO VIZ TEMPERATURES", "text": "Solar heating of radiosonde thermistors has been recognized to cause reported temperatures which are too high during daytime. In the 1950's, corrections were applied to operational radiosonde temperatures to compensate for this observed effect. In the late 1950's VIZ introduced a white-coated thermistor which substantially reduced the anomalous daytime temperature readings. When the Weather Bureau phased the new sonde into operations in the early 1960's, temperature corrections were no longer applied at observing sites. However, further studies, using differences between nighttime and daytime observations, found significant day-night differences, which increased from small values in the troposphere to 3 C degrees or more at stratospheric levels, and varied with solar elevation angle. These results were used in the numerical analyses produced at the National Meteorological Center to adjust daytime radiosonde values to the level of nighttime temperatures. New adjustments to radiosonde data used for NMC analyses are based on results of international radiosonde intercomparisons and other instrument studies. Recent work has shown that VIZ white thermistors are also subject to long-wave errors, which affect both daytime and nighttime radiosonde measurements. Results of modelling such errors show that errors increase from about 1 C degree in the troposphere to 2 C degrees or more at stratospheric levels. Errors vary with atmospheric temperature and also ground surface and cloud radiation characteristics. These modelling results agree with studies using satellite data as a reference standard and also instrument studies using a reference radiosonde. NESDIS plans to implement radiosonde adjustments, based on these studies, for the next generation of satellite temperature retrievals, which. depend_ on radiosonde data. There are implications, for operational numerical analyses as well as for climate studies using radiosonde data, of radiation errors of the magnitudes and with variabilities that have been reported. RECOMMENDATION: Support studies of radiation errors of all operational radiosonde instruments."}, {"section_title": "OPERATIONAL CORRECTIONS OF RADIOSONDE RADIATION ERRORS", "text": "Corrections to radiosonde temperatures have not been applied at NWS observing sites since the early 1960's, as discussed above. Therefore, temperature trends for the last three decades, calculated with the uniformly uncorrected VIZ temperatures, may be used with confidence. Vaisala instruments are used at many observing sites throughout the world and at some U. S. military sites. Most radiosonde stations using Vaisala instruments also use Vaisala data processing equipment in which temperature corrections are usually applied. There have been changes in the magnitude and details of the corrections applied over the years, as new instruments have been developed, new computers introduced, and additional data studies completed. Differences in temperature corrections of up to 0.8 C appear in successive tables of corrections used by Vaisala for the Vaisala RS80 radiosonde. With current computer capabilities, radiation corrections to radiosonde temperatures, based on current studies, could be implemented at all NWS observing sites, or temperature adjustments could continue to be done at NMC. Introducing a corrections to VIZ temperatures at the observing site may result in operational data more directly useful to other NWP centers, but would change the climate record of temperatures disseminated throughout the world. If corrections were made by stations, presumably the uncorrected temperatures could also be archived, along with the corrected temperatures, for climate use. If Vaisala radiosondes were to be used by NWS stations, a decision must be made about whether to apply corrections to the data, and which corrections to apply, to make Vaisala temperatures consistent with data taken by other countries throughout the world. Consistency between data procedures for Vaisala temperatures (which perhaps should be corrected) and those for VIZ and SDC data (which perhaps should not be corrected) also needs to be considered. RECOMMENDATION: There should be careful consideration of any corrections to radiosonde data, based on necessities of producing the best operational data, but also with clear understanding of implications for climate data users."}, {"section_title": "SATELLITE DATA QUALITY AND CONTINUITY ISSUES GEORGE OHRING AND ARNOLD GRUBER NESDIS/OFFICE OF RESEARCH AND APPLICATIONS", "text": ""}, {"section_title": "CALIBRATION OF ADVANCED VERY HIGH RESOLUTION RADIOMETER (AVHRR)", "text": "Problem: The visible and near-infrared channels of the A VHRR on the NOAA POES satellites have no on-board calibration. Their degradation with time is estimated by assuming that THE EARTH IS NOT CHANGING WITH TIME. To measure changes of the Earth's surface and cloudiness, on-board calibration devices are required."}, {"section_title": "Solution:", "text": "Install on-board calibration devices for solar reflectance channels Impact of not doing: We would not have the foggiest idea of whether global changes of these quantities are occurring, which could lead to false economic choices.\nInstall station-keeping motors on the satellites to maintain a constant observing time. Impact of not doing: Delay in the detection of global greenhouse warming.\nUpgrade the NOAA surface networks from ozone, solar radiation and atmospheric turbidity. Impact of not doing: Lack of confidence in satellite observed trends, with poorer information for policy makers to make decisions related to global warming and ozone depletion.\nProvide for reprocessing of radiances and/or updating products as new algorithms are developed. Provide complete documentation of improvements. Impact of not doing: Inability to monitor long term changes in important climate parameters, which could lead to false economic choices.\nFly a new series of small NOAA Climsat satellites. Supplement the planned NOAA core satellite program with those key measurements necessary to monitor precisely the most important climate forcing and response variables. These include the solar irradiance, Earth radiation budget, total ozone amount, upper tropospheric and stratospheric vertical profiles of ozone, aerosols, and water vapor, and all weather sea surface temperature. Transition from research to operations those space proven instruments that NASA has developed for making the above measurements. Impact of not doing: An additionallO to 20 years to detect global wanning and to check out global climate models; and uncertain information on global ozone trends. False choices will not be avoided."}, {"section_title": "SATELLITE DRIFT", "text": "Problem: The orbits of the NOAA afternoon POES satellites drift with time at a rate of about 1/2 hour per year. With such a drift of observation time, long-term measurements are influenced by diurnal variations of the phenomenon being studied, for example, surface temperature, as well as the effects of changing solar zenith angles on the observations. As a result, the small long term trends associated with global warming, for example, cannot be isolated from U1e records."}, {"section_title": "SATELLITE OVERLAP", "text": "Problem: Instrument changes from one satellite to tb,e next, either as a result of advanced instruments to measure the same quantity or small variations in the same instrument family, can lead to artificial discontinuities in the record. One example is the planned replacement of the Microwave Sounding Unit (MSU) by the Advanced Microwave Sounding Unit 1 -D (AMSU). The MSU has provided a precise measurement of global atmospheric temperature variations since 1979. Solution: Plan for and provide an overlap period of one year to intercalibrate instruments on successive satellites.\u2022 Impact of not doing: Loss of continuity in the record of, for example, global warming or ozone trends, possibly leading to false economic choices. Problem: Satellite measurements must be calibrated/validated against ground truth. For example, Dobson ozone measurements have been crucial for analyzing spurious ozone trends observed by the NOAA-9 Solar Backscatter Ultraviolet Radiometer (SBUV). NOAA's surface networks of solar radiation observations, sun photometer observations of atmospheric turbidity, and Dobson total ozone and Umkehr observations are deteriorating at a time when the information they provide is most needed."}, {"section_title": "IMPROVING ALGORITHMS", "text": "Problem: The algorithms that convert radiances to geophysical parameters usually evolve to provide more accurate data. This typically results in discontinuities of the data record making it difficult to maintain a continuous record for long term assessments. This problem \u2022 is often compounded by incomplete doclllhentation of the improvements that have been implemented into the data processing stream."}, {"section_title": "ADDITIONAL SATELLITE MEASUREMENTS", "text": "Problem: Current and planned NOAA satellites, and the NASA Earth Observing System (EOS), will not provide the long term, continuing, precise measurements required for early detection of greenhouse wanning and ozone depletion as well as the inputs needed by climate modelers to evaluate climate prediction capabilities."}, {"section_title": "ADVANTAGES AND DISADVANTAGES OF OPERATIONAL SATELLITES", "text": "Operational satellites have a nmnber of attractive features for the monitoring problem. Since they are operational systems, when one satellite fails, a replacement is launched. Thus, long time series of similar instrmnent observations can be accmnulated. Furthermore, some of the instrmnents may provide a dual function, serving the weather/ocean short-term and interannual forecasting community, as well as the climate monitoring scientists. As opposed to the ground-based observing system, which is confined largely to the populated land areas, satellites obtain global coverage, thus insuring adequate sampling of the entire Earth. The satellite obtains its measurements with single instrmnents; the ground-based system must integrate the measurements of many observing sites and must grapple with the problems of different instrmuents, observing practices, and local effects. But the operational satellites also suffer from a nmnber of deficiencies. As with all remote sensing observations, they can provide only indirect measurements of the geophysical variables of interest and, for the most part, must rely on ground-based measurements for \"tuning.\" The current operational satellites were not designed for monitoring global change, but rather for weather observations. Problems arise from poor calibration, instrmnent drifts, changes in instrmnents or spectral response functions of the same instrmnent type on successive satellites, satellite orbit drifts, artifacts introduced into the time series of retrieved products as a result of episodic events (e.g., effect of volcanic aerosols on sea surface temperature and other data records), and changes in processing algorithms."}, {"section_title": "OUALITY ISSUES AND SOLUTIONS", "text": "The visible and near-infrared channels of the Advanced High Resolution Radiometer (A VHRR) have no onboard calibration and must rely on the pre-flight laboratory calibrations. In the past, instruments were flown that had been calibrated many years prior to flight. Because of aging and degradation of the instrument during storage, its calibration on launch was different from the measurements. NOAA now requires annual recalibrations and a fmal calibration as close to launch as possible. Once in flight, these sensors degrade with time, and, with no onboard calibration, they cannot be used for long-term monitoring. A partial solution is to calibrate these instruments by assuming that the Earth is not changing with time and with aircraft instrument underflights of the satellite. This approach has been used by the International Satellite Cloud Climatology Project (ISSCP) and the NOAA/NASA Pathfinder Project. Figure 1, based on the Pathfinder results, shows the degradation rates for the NOAA-7, -9, and -11 AVHRRs. Inter-satellite changes must be evaluated by analysis of satellite overlap data. The long-term solution to this problem is onboard calibration, and this is planned for the post NOAA-K,L,M series of satellites of the next decade. The afternoon polar satellites drift with time. For example, upon launch in 1985, NOAA-9 observed the Earth at 2:20 p.m.; four years later, when replaced by NOAA-11, its observation time was two hours later. Orbital drift introduces spurious trends into the time series, either due to diurnal variations or effects of solar zenith angle on retrieved physical quantities. A near-term .solution recommended for study by a NOAA Working Group is launching into a slightly earlier orbit. Such an approach risks spacecraft over heating, if there is a launch error. The long-term solution is to install satellite station keeping motors. Although costly and not implementable until post NOAA-K,L,M, it is absolutely necessary for monitoring global change over long time scales. -Instrument changes from one satellite to the next, either as a result of advanced instruments to measure the same quantity or small variations in the same instrument family, must be analyzed during a suitable overlap period of observation. A current example of such a problem is the replacement of the Microwave Sounding Unit (MSU) by the Advanced Microwave Sounding Unit (AMSU) on NOAA-Kin 1996. The MSU channel2 has proved to be a precise monitor of global tropospheric temperature, as shown in Figure 2, which compares MSU and radiosonde temperature variations from 1979 to 1990 (Oort and Liu, 1993). The AMSU does not have a channel with the same atmospheric weighing function as MSU-2. However, by an appropriate linear combination of the AMSU channel measurements, one can simulate the MSU-2 with a high degree of accuracy. The difference in brightness temperatures is of the order of 0.1 o C, which is smaller than variations from instrument to instrument in the same family ofMSUs. In general, an overlap period of about a year is needed to intercalibrate instruments on successive satellites. A break-through in the satellite overlap/continuity problem occurred recently at NOAA. For the first time, the science community (external and NOAA) was consulted on a satellite launch decision. As a result, NOAA-I was launched in August 1993, rather than waiting for the demise of the still operational NOAA-11. The Report of the NOAA Working Group on Data Continuity/Science Issues for NOAA-I and Future Launch Decisions also recommended that a NOAA Working Group be established to advise on future launch decisions. Episodic events such as volcanic eruptions distort the time series of several surface variables, for example, sea surface temperatures. In the short term, procedures can be developed for eliminating much of these effects by, for example, correcting observed radiances using \u2022information from the satellite measurements of volcanic aerosols, but in the long-term, advanced instruments, such as the Along Track Scanning Radiometer (ATSR) or all-weather microwave radiometers are needed. The importance of ground truth cannot be over emphasized. For example, Dobson measurements have been crucial to analyzing spurious ozone trends measured from NOAA-9's Solar Backscatter Ultraviolet Radiometer/2 (SBUV/2). NOAA must make a long-term commitment to maintaining its U.S. Dobson network, whose continuity is currently in doubt. Changes in processing algorithms are more easily dealt with. Simple reprocess the entire data set when algorithm improvements warrant such an effort. The first large scale reprocessing of operational satellite data is currently taking place in the NOAA/NASA Pathfmder Project."}, {"section_title": "ACHIEVING A LONG-TERM MONITORING CAPABILITY", "text": "While the current operational system makes some contribution to global environmental . monitoring, its real potential will only be realized after significant upgrades and the addition of 'critical new measurements. The necessary continuity/quality upgrades have been discussed above and are summarized here: Achieve one-year satellite overlap periods Reprocess periodically The observing system must attempt to meet the accuracy goals for long-term monitoring of the type shown in Table 2. Clearly, this is a difficult challenge. Furthermore, some critical variables, in particular the climate forcing variables, are not currently measured from operational satellites. NOAA and the science community must decide which variables are the most important for long-term enviromnental monitoring. Table 3 is a preliminary list of critical new measurements and possible instruments. The instruments have all proven themselves in NASA research missions and can easily be transmitted to operational flight. The Nation's operational enviromnental satellite system has focussed on weather forecasting needs. It must now also meet requirements for continuous monitoring of interannual and decadal enviromnental change. NOAA is the U.S. civil agency responsible for such observations and it must take the responsibility for the enviromnental monitoring system.   "}, {"section_title": "~", "text": ""}, {"section_title": "Degradation of A VHRR", "text": "Degradation rqtes of AVHRR visible (Ch. 1) and near infrared (Ch. 2) channels for the NOAA-7, -9, and -11 satellites. To date, Li'le NOAA budget proposals support opJy winimal capabilities at the National Climatic Data Center (NCDC) concerning NEXRAD (WSR-88D) Level III data. These data comprise the WSR-88D products that are routinely generated and recorded at each National Weather Service (NWS) Office's Radar Product Generator (RPG). The Level III data will satisfy the requirement for \"official\" records ofNWS radar data. The Level III media (optical disks) will be shipped approximately monthly from the NWS sites to NCDC; there they will be copied, catalogued, and stored (i.e., archived). The only dissemination of these data that will be supported at NCDC, however, will be color hard copies of individual products. This level of dissemination is intended to support the need for such data for legal purposes such as aircraft accident investigations. There are currently no plans to fund the NCDC to support the use of WSR-88D product data for scientific purposes. These products are valuable for basic research, algorithm development, climatology studies, \"quick look\" indexes to WSR-88D Level II data (much more voluminous, base data, rather than products), etc. Additionally, much of the \"metadata\" associated with the Level III products (e.g:, radar calibration data, changes in system and algorithm adaptable parameters, new versions of the algorithms producing the products) is not recorded with the products themselves. These data are available at the NEXRAD Operational Support Facility (OSF), but there are no explicit plans for coupling them with the Level III products at NCDC."}, {"section_title": "DATA QUALITY AND CONTINUITY ISSUE REFERENCE GAS STANDARDS FOR TRACE GAS MEASUREMENTS PIETERTANS OAR, OBSERVATORY OPERATIONS THE PROBLEM", "text": "In order to better predict, and possibly manage, future concentration of greenhouse gases we need an improved quantitative understanding on large regional scales of the global atmospheric budgets (sources, sinks) of C0 2 , CH 4 , CO, and N 2 0. The observational basis for such understanding is provided partly by detailed \"process\" studies and partly by high accuracy global measurements that reveal relatively weak temporal and spatial gradients. The gradients result from the presence of sources and sinks on large regional scales. Isotopic ratios ( 13 C/ 12 C, 18 0f1 6 0, D/H, 15 Nf1 4 N) are playing an increasingly important role in deciphering the budgets. Different processes can each have a characteristic isotopic signature. In the case of C0 2 , exchange with the biosphere is characterized by a lower 13 C/ 12 C ratio than air-sea exchange. In the case of methane, thermogenic CH 4 has a higher 13 Cfl~ ratio than ambient atmospheric CH\"' whereas biogenic CH 4 has a lower 13 Cf12C ratio. The measurements demand very careful standardization over a long time and between different laboratories because atmospheric mixing is vigorous, leaving only relatively small signatures in background air. This rc;:quires the establishment and maintenance of both very accurate gas standards and isotopic standards. Similarly, in the case of the direct measurement of the uptake of fossil C0 2 by the oceans very accurate sea water standards are needed because the C0 2 added from fossil fuel combustion is only a small fraction of the carbon already present in the oceans. Today these requirements are being met only partially. Especially, the isotopic standards are inadequate."}, {"section_title": "REQUIRED ACCURACY AND PRECISION FOR AIR STANDARDS C0 2", "text": "The C0 2 growth rate varies between 0.5 and 2.5 ppm y\u2022 1 and the global north-south gradient varies from 2.5 to 4.5 ppm. The level of accuracy recommended by the WMO C0 2 Experts Meetings is 0.1% (0.35 ppm), but the recommended level of precision (repeatability, intercomparability) is three to five times more stringent, especially with an eye towards deciphering sources and sinks in the Southern Oceans. This is achievable, with considerable effort, and a mechanism is in place to reach the goal."}, {"section_title": "C0 2 isotopes", "text": "The global north-south 13 Cf'2C gradient varies between 0.2 and 0.3 permil, and the annual increase is between 0.00 and 0.03 permil per year. A difference of0.05 permil in the north-south gradient means that 1 *10 15 g C y\u2022 1 is either being absorbed by the oceans or by the terrestrial biosphere. A terrestrial or oceanic cause of observed recent interannual variations in the C0 2 growth rate (correlated with ENSO) cannot be established at this time because of calibration differences between different laboratories in the range of 0. 05 permil. The interlaboratory accuracy and precision should be 0. 01-0.02 permil. Two kinds of isotopic standards (also for CH 4 , N 2 0, etc.) are required. Standards of the pure gas with isotopic ratios in the ambient range, and standards consisting of the trace gas in air at ambient concentrations. This allows separate calibration/standardization of the mass spectrometers and of the sample preparation steps. Demand for 18 0/ 15 0 standards can be expected in the future. The oxygen isotopic ratios are just beginning to be used for carbon cycle research. From what is known thus far about the 18 0/ 16 0 picture, the required accuracy would be less than for carbon ~sotopes, 0.05 to 0.1 permil would probably suffice. The isotopic signatures present in the background air are considerably larger for oxygen than for carbon. No serious interlaboratory comparisons have been done for 18 0/ 16 0. CH 4 Methane appears to be a stable trace gas in many different kinds of containers. Relative precision of the standards has been achieved of better than 1 ppb (parts per billion), with ambient concentrations about 1700 ppb. The CH 4 growth rate varies between 5-15 ppb y\u2022 1 , and spatial differences in background air can be up to 200 ppb. The absolute accuracy of the calibration scale is no better than 1%, but accuracies of a few ppb seem achievable with present technology. The calibrations of different laboratories may differ from each other by up to one percent, but this is not a serious problem because of the stability. We expect that this will be straightened out in the near future."}, {"section_title": "CH 4 isotopes", "text": "The north-south 13 C/ 12 C gradient is about 0.6 permil, and the C isotopic ratio is increasing at a rate of0.5-l.O permil per decade due to changes in the mix Interlaboratory precision is currently about 0.5 perrnil. Needed are pure CO, pure CH 4 , and ambient CH 4 -in-air standards. The 13 C/ 12 C isotopic ratios would become a very strong constraint on the global CH 4 budget if the precision were to improve to 0.05 perrnil, which appears to be feasible with effort. No mechanism is in place to push in that direction. D/H isotopic ratios have thus far not been used much, so that it is hard to give reliable \"specifications\" for the required accuracy. CO NOAA/CMDL has made an accurate (about 1 ppb) gravimetric CO calibration scale. Comparison with older measurements remains difficult because trace CO in reference tanks is not very stable, and calibration differences between laboratories are known to be up to 25%. There is not yet a mechanism in place to systematically propagate the NOAA scale to other laboratories. CO isotopes Little is known at this time about the isotopic signatures of the various CO sources, so it is not clear whether it would be useful to measure these ratios in background air. It is not being done at the moment. A gravimetric calibration scale (accuracy about 1 ppb) has been established for N 2 0 by NOAA/CMDL. This trace gas appears to be stable in high pressure reference gas cylinders. The rate of increase is about 0.6 ppb y\u2022 1 , and the north-south gradient is about 1 ppb. Measurement precision considerably better than 1 ppb is possible, so that the same measurement strategy as for C0 2 and CH 4 can be fruitful. Measurements made by different laboratories need to be on the same, very precise, calibration scale. No mechanism is in place to assure this."}, {"section_title": "N 2 0 isotopes", "text": "Nitrification and denitrification appear to impart substantially different isotopic signatures to N 2 0, but too little is known at this time to determine whether isotopic measurements would be useful. The isotopic\u2022 approach appears to-be promising."}, {"section_title": "WHYNOW? ]", "text": "Every year that we delay is lost to the record. We do not have a second opportunity to J improve our measurements of what is happening today."}, {"section_title": "MEASUREMENTS AND VALIDATION OF OZONE VERTICAL PROFILES WALTER PLANET NESDIS, OFFICE OF RESEARCH AND APPLICATIONS", "text": ""}, {"section_title": "IMPACT", "text": "If the assembling of an accurate and credible data set of Umkelrr ozone profiles such has been provided by NOM in the pastis curtailed or abolished within NOM, NOM would abdicate its scientific responsibility to provide valuable data that it had been providing. This data set is of value in many scientific areas such as long-term monitoring and in global ozone trend determinations. To this must be added the need to provide global profile distributions based on NOM satellite measurements. During real time processing, the data from NDBC stations undergo automated data quality control, including range and time continuity checks, redundant sensor intercomparison, and engineering parameter checks prior to their.release over the GTS. More stringent quality control procedures carried out using a man-machine mix (manual examination of automated data quality flags, time-series graphics, scatterplots, etc.) are performed on these data within 24 hours of GTS release. This higher-level check results in a second data set that is specially prepared at NDBC and submitted monthly to the National Climatic Data Center (NCDC) and the National Oceanographic Data Center (NODC). \u2022\u2022 The data forinats of the latter subinissions have beeii established through extensive advance coordination with both centers; in addition to the enhanced qualitY checks, measurements in these data sets are of higher resolution. Since NDBC was established in 1970, data have been archived from more than 180 moored buoy and C-MAN stations. The difference between this number of stations and the current network represents stations that have been disestablished due to termination of reimbursable projects, or relocated to better support user requirements. In addition, data from over 300 TOGA drifting buoys have undergone data quality control and been specially prepared for NCDC and NODC. A substantial amount of data has been supplied by NDBC stations. The establishment of a long term 50 to 100 station temperature observing system proposed as a solution to climate problems should include some existing deep ocean and coastal marine stations. There are several reasons for including these stations. First, a temperature record of nearly two decades already exis~s in the archives for several deep ocean stations. Although established in support of operational forecasting, the entire data set from such stations has been quality controlled, and hardware configuration and sampling strategy is well documented. Second, use of such ocean stations in climate monitoring, even though originally established to support operational forecasting, should meet research accuracy standards and is fiscally wise. Third, other data types in addition to temperature, such as wind and barometric pressure, are also archived and can be useful in climatological studies. Fourth, data from ocean stations are not subject to local, anthropogenic change caused by urbanization, deforestation, etc. Changes noted at sea will more accurately represent global-scale, climatological trends. "}, {"section_title": "THE ERA OF AUTOMATED PLATFORM DATA", "text": "Oceanographic measurements taken from bathythermographs date back to about 1900, but it was not until the 1970's that. the modem mix of automated ocean observing equipment started to take shape. Moored data buoys now provide frequent and relatively accurate data from a number of near-shore or climatically important regions, and drifting buoys have been dispersed widely with the advent of international climate observing programs. Buoy data follow-on from early mauned \"fixed\" ocean stations, such as the Ocean Weather Ship program. (mostly mid 1940's-mid to late 1970's), except that regrettably the measurements were not continued in the same locations due to practical considerations or insufficient consideration of the importance of data continuitY. Locations occupied by lightships around the coastal US (starting as early as \u20221915) have generally been continued with moored buoys or C-MAN stations, although with some breaks in the period of record. Unfortunately even these locations are threatened by a loss of data continuity due to budget constraints. \u2022 Climatically siguificant differences may exist between ship and buoy data. However, ship data remain critically important due to wider coverage and as a data continuity baseline. Efforts to more effectively combine modem and historical data sources remain important, with the goal to develop improved \"value added\" data sets incorporating adjustments and corrections for instrumental and observational heterogeneities."}, {"section_title": "OBSERVING, PROCESSING, AND ARCHIVAL PROBLEMS", "text": "The advent of computer and telecommunications technology has provided the pathways and tools to quickly and effectively exchange, process, and store large amounts of data. However, technological change has also lead to unexpected problems in the quality and continuity of data. The Global Telecommunication System, while enabling revolutionary changes in the transmission of weather observations, appears also to have significantly increased the noise in the marine datastream. Code changes aimed at reducing GTS message traffic have in some cases not been properly implemented, resulting in data discontinuities that appear only partially correctable (Figure 1 ). Subtle data continuity effects may arise from the use of different units for data transmission, and serious conversion errors and other data processing problems have impacted the U.S. as well as many other countries (Figure 2). Itshonld be noted that simple, periodic checking of data inputs and intercomparison between data sources have proven to be powerful methods of identifying and correcting su:ch data errors. Improvements in software quality assurance and metadata retention will help reduce data problems. Metadata problems impact both the historical record, and the modem marine observing system. It is perhaps not surprising that difficulty was encountered in trying to locate adequate documentation of 19th Century observing methods. However, it is of siguificant concern that the authors, at least, have failed to locate any U.S. library that has retained each and every revision of the WMO observing and supplemental code handbooks (since 1955)."}, {"section_title": "RECOMMENDATIONS", "text": "Marine data quality and continuity problems have arisen in some cases because of inadequate coordination among governmental or international bodies. A troubling example of a lack of governmental coordination is the apparent destruction in 1974 of the U.S. merchant marine logbooks for World War II. At times the differing concerns and goals of.real-time versus archival centers, and of the operational versus research conununities, have not been fully considered in the design and implementation of changes in the marine observing system. A long term commitment to improving the total marine database system is required without the irregularities of the past. It is strongly reconunended that increased coordination among all interested parties be inunediately initiated by establishing marine data focal points. EXAMPLES ofNOAA organizations where permanent marine focal points are desperately needed include NCDC, ERL, OSO, NMC, NDBC, NODC, and NWS PMO's. It is our view that such a Working Group needs to be spearheaded by the climate research conununity in order to provide sufficient attention to data continuity. Additionally, improvements of all facets of data documentation (data biases, observing and processing procedures, platform characteristics, instrumentation, etc.) is urgently needed. The Working Group of marine focal points could be a vehicle to help accomplish these important goals if staffed at the appropriate level (members who are technically qualified and administratively capable of accomplishing the necessary tasks at their organization). * CO ADS is the result of a continuing cooperative project between the National Oceanic and Atmospheric Administration (NOAA)--its Environmental Research Laboratories (ERL), National Climatic Data Center (NCDC), and Cooperative Institute for Research in Environmental Sd.ences (CIRES; joint with the University of Colorado )--and the National Science Foundation's National Center for Atmospheric Research (NCAR). CO ADS products are available from NCAR or NCDC. 1970 1975 1980 1985 1990 Figure 1. Precipitation frequency anomalies (60\"N-70\"Nl. A spurious increase in precipitation events occurs because the station/weather indicator, which was introduced into the WMO ship code in 1982, was not used. The indicator can be used to adjust part of the curve downward. unfortunately, it was not made part of the international exchange logbook code until 1985, and was not archived in US telecommunications data until 9 May 1984."}, {"section_title": "25---", "text": "22 __________________ \" _______ _ 12 ____ :.:,~~2s 1970 1975 1980 1985 1990 Figure 2. Ship wind speeds based on selected 10\u00b0 boxes (Marsden Squares 79,80,122,123,141,142,184,185,199,200,217,& 252) in the North Atlantic. curves shown for USSR and all other data are displaced possibly due to biases from reporting wind in whole meters per second versus knots. The effect of a us conversion software error is also strongly evident."}, {"section_title": "DATA CONTINUITY AND QUALITY CONTROL PROCEDURES FOR WATER LEVELS STEPHEN GILL NOS, OFFICE OF OCEAN AND EARTH SCIENCES INTRODUCTION", "text": "Data processing and quality control have been a major focus of water level measurement program activities of the National Ocean Service since systematic measurements began in the mid 1800s. This has resulted ill one of the most unique and valuable continuous geophysical data sets available. One of the primary missions of the Ocean and Lake Levels Division (OLLD) and predecessor organizations has been the management of the National Water Level Observation Network (NWLON). The NWLON now consists of 189 long-term continuously operating water level stations in the U.S. Coastal Ocean, the Great Lakes, and ocean island possessions and territories. The longest operating NWLON stations include San Francisco (Presidio) for which continuous data start in 1854 and several stations in the Great Lakes, for which continuous data start in 1860. OLLD activities have been extended through participation in the Climate and Global Change Program and the Global Sea Level Network (GSLN). NOAA's Next Generation Water Level Measurement System (NGWLMS) field units have been installed at 75 percent of the NWLON stations and also at 25 additional foreign and U.S. locations in support of the GSLN with several others planned over the next few years.. The NGWLMS program includes the implementation of a new data processing and analysis system in Headquarters. The operational procedures required to maintain the NWLON and to provide derived \u2022 products and services have been carefully developed over the years and are continually updated based on application of new knowledge and technology. These operational procedures include: 1) the field operation and surveying procedures, 2) system engineering and development procedures, 3) analyst standard operating procedures, 4) automated data processing procedures and software, and 5) computational algorithms."}, {"section_title": "DATA CONTINUITY", "text": "Data continuity has been and continues to be maintained using a combination of policies, procedures, and techniques that ensure that data time series, datums, and information derived from the water level observation program have virtually no gaps and that there are no unexplained discontinuities in the historical record. These include: 1) maintenance of station datums through routine differential level ties from the measurement systems to local bench mark networks, 2) use of systematic simultaneous measurements and data comparisons with older systems when new technology systems are developed and deployed, 3) implementation of ]Jackup data measurement, data collection, and on site data storage systems, and 4) careful implementation and testing of new data processing and data management systems."}, {"section_title": "DATA QUALITY CONTROL", "text": "Procedures for ensuring data continuity are an integral part of the OLLD operations and are pa..rt and parcel of the policies and procedures for data quality. The water level measurement program operates under the umbrella of a documented operational data quality assurance program with detailed standard operating procedures documented for each functional area. These functional areas include: 1) field operations for gauge preventive anq corrective maintenance and for monitoring of vertical stability, 2) automated and manual procedures for data quality monitoring with appropriate feedback to field parties for quick response to problems, 3) data processing, analysis, and verification procedures, and 4) independent \u2022 checks of the entire cycle from data collection to production of output products."}, {"section_title": "PROBLEMS FOUND IN THE IMPLEMENTATION OF NEW SYSTEMS", "text": "The NGWLMS field units and data processing system have been designed to improve on the kuown deficiencies of the older technology systems. The new system improves data accuracy, improves timeliness of data collection, improves the ability to remotely monitor the stations, allows for the automated measurement of ancillary parameters, decreases the need for corrective maintenance, eliminates manual data collection and manual data processing steps, and increases data availability to all users in a timely fashion. However, with the elimination of the problems of the older systems, the implementation of a new technology has presented the program with a set of new and challenging technical and managerial problems. The technical problems include: 1) a dramatic increase in operational telecommunications costs and office automation costs, 2) the occurrence of simultaneous system-wide failure during NESDIS downlink failure or GOES satellite communications changes and failures, 3) the transition from a single processor (mini-computer) centralized computer system running FORTRAN application programs to a client-server enviromnent featuring VAX servers and PC-based personal computers on a local area network running 3GL and 4GL applications. The managerial problems include: 1) managing a multi-year development and implementation of new teclmology through periods of reorganization, realignment, and diminishing resources, 2) shifling of field personnel skill levels to be more oriented towards electronics and computers, shifling of processing personnel skill levels to be more analytical and computer oriented with higher skill levels in math and oceanography, and 3) obtaining the different skills required of in-house ADP personnel in making the transition described above."}, {"section_title": "XBT MANUFACTURERS", "text": "Until recently, Sippican was the ouly manufacturer of XBT's. There are now two companies that manufacturer XBT's, Sippican Inc. and Sparton of Canada. Adding a second source of XBT data into the XBT archive adds another variable that must be considered. Sparton, whose XBT is virtually identical to the Sippican XBT, has expressed an interest in competing for U.S. Navy and worldwide XBT purchases. It was widely agreed upon by both the Navy and the scientific community that the Sparton probe be tested extensively to determine its quality and reliability. An Integrated Global Ocean Services System (IGOSS) task team is conducting a series of at-sea tests that compare the Sparton probe with both a Sippican probe and a CTD. Results from these tests will be forthcoming within the next year. The Navy has conducted a three phase qualification evaluation of 2 of the Sparton probes, the T-4 and T-7. This evaluation included an at-seas test, a series of in plant performance tests, and a drawing/docmnentation review. The Navy conducted a first at-sea test in January 1992 and found the Sparton probe had two outstanding problems. The first was wire breaks attributed to the XBT wire unspooling too quickly. The second was that the probe was descending too quickly. From Sparton modified it's probe and a second \"dry run\" test was conducted in August 1992. Results from this test showed that Sparton had successfully solved both problems and were ready for the qualification test. The at-sea qualification test was conducted in September 1992, in which 144 Sparton and 144 Navy (or Sippican) T-7 probes were launched. A comparison was done between the two probes and with a CTD. Averages were taken of the \"good\" drops and then compared. \"Bad\" drops or drops with excessive temperature deviation from the norm were scored to get a reliability percentage. Results from the test showed Sparton and Sippican probes were nearly identical in temperature measurement performance and descent rate. The drawing/documentation review was also completed with successful results. The drawing was considered to be consistent with the as-built T -7 probe. A similar review of the T -4 probe in currently underway and has not yet been completed. Based on the results of the evaluation process, the Navy recommended the approval of Sparton of Canada, LTD as an alternate source for XBT type T -7. The qualification of the T-4 probe is pending the at-sea phase of the qualification testing. Adding a second source of XBT data into the XBT archive adds another variable that must be considered. The tests that have been and are being conducted show that careful consideration is being given to this potential new source ofXBT probes to insure XBT data continuity and quality."}, {"section_title": "XBT CONTROLLERS", "text": "When XBT's first came out in the mid 1960's the Sippican MK-2 XBT analog signal recorder was used to record data. This device directly records the XBT's analog signal onto strip chart. The strip chart then must be manually encoded for later use and archiving. In the late 1970's Bathy Systems Inc. developed an analog to digital controller that made it possible to use computers to accepted and store data for later use. However the Bathy system had some problems with bowing or anomalous increases in temperature with depth and noise or small temperature fluctuations. It was determined that the Bathy 810 controller was runuing too low a current through the XBT. Bowing and noise errors are directly related to electrical current leakage through imperfection in the XBT wire insulation. The maguitude of these errors are directly related to the current used. By increasing the current from microamperes to the level for which the probe was designed, 200 microamperes, these errors were reduced. The Sippican MK.-9 AID controller was introduced shortly after the Bathy controller and is now widely used. The MK-9 has been a solid performer for over ten years now. However, a timing problem was discovered on one of the ROM updates but the problem was found early and fixed. "}, {"section_title": "FALL RATE", "text": "The third areal would like to cover is the descent rate of the XBT. The depth of the XBT is calculated using a fall rate equation. The equation is a second order equation and is a function of time. In recent XBT fall rate studies, it has been shown that XBT's fall at a faster rate than what the equation states for the T -4's, T -6's and T -7's. In CTD-XBT comparisons, the depth offsets as the drop progresses. Field tests are now being conducted in different ocean regions by an IGOSS task team to study this problem. To ensure a controlled test, participants were requested to coordinate the experiment using similar test procedures. Data collected from these tests are being combined and processed together using an agreed upon procedure that eliminates temperature errors. From these studies new coefficients for the fall rate equation will be derived by the IGOSS task team to more accurately represent the true fall rate. Implementation of these new equations into real time messages is expected by 1995 through a new World Meteorological Organization (WMO) JJ:XX message that will identify the equations used."}, {"section_title": "FINAL COMMENTS", "text": "A proposal has been submitted to the WMO to include another five character field in the real time JJXX message that will incorporate information like probe manufacturer, XBT controller used, and fall rate equation used. Having this information stored with the data will enable data to be singled out and corrected if future problems\u2022 arise in any of these areas. 14-F Sea Level is one of the most used of the parameters for studying climate change because it has one of the few relatively long, high accuracy measurement records and because modelers expect the Greenhouse Warming Effect to show up strongly and early in it. However, all of these sea level time series need much work to optimize them for such studies. \u2022 Sea Level is a derived property based on averages of hourly elevations of the water levels of gauges which are designed primarily to measure tidal fluctuations. They are related to vertical elevations of local individual benchmarks on land, which themselves are not stable. Further, the computation of the datum for long term records must adjust for small changes in the locations of the gauges, variations in the surveys between the tide staffs and benchmarks, observer uncertainties in relating the tide staffs to the gauges, effects of filling gaps with data from nearby sites or back up gauges, wind wave and tidal current effects, etc. Consequently, most of the sea level records that have been used to date have not been \"clean.\" There are also real, shorter term (seasonal to decadal), fluctuations in the values of sea level that are of much greater magnitude than the expected change of sea level over a few decades. These obfuscate attempts to get accurate long term trends and accelerations. Finally, some of the older records are not yet in a computer compatible format and many are not yet in a data base that will make them easily available to the outside scientist."}, {"section_title": "RESOURCES NEEDED", "text": "Approximately $700K/yr and 2 FTE. Not all important marine data and associated descriptive information are available to the data user community, including the NOAA Distributed Active Archive Centers (DAAC). Some estimate that only approximately 50 percent of marine data collected over the years are archived in the oceanographic data centers. The actual percentage of data \"out there\" cannot be estimated with a high degree of reliability, but we believe it is very substantial. For data in data center archives and for data yet to be archived, there is additional need to obtain and process documentation and other metadata associated with digital observations. Proper data management should begin with the data originator and the data collecting activity with regard to data standards, inventories, formats, transfer media, etc. Most often, data management within the collecting institution is inadequate or non-existent. On-site or close NODC data management expertise would expedite the journey of data from data collector to data base (NODC) to data user."}, {"section_title": "THE SOLUTIONS:", "text": "Mention has already been made of the need for a long term data set to follow-up on compliance by responsible parties to NOAA/NMFS permits and the success of recomniended mitigation/restoration activities. Given the magnitude of the number of permits that the NOAAINMFS habitat and protected resources offices receive each year (estimated 10,000), it is not possible to track the compliance and mitigation/restoration success for each permit on which we\u2022 comment or issue. The agency should design a statistically sound sampling studyofthese permits; based upon the nature of the proposed activity, the potential impact on LMRs and their critical supporting habitat, politicaVeconomic importance, and amount ofNOAA/NMFS resources to conduct the follow-up, in order to develop the appropriate long term data set to deal with this problem. One needs to follow up on both megaprojects and minor projects since the cumulative impact of the latter can induce major responses. For example, a National Research Council Report on the sources of marine pollution from oil reported that relative inputs were: 1.5% offshore oil production, 8% natural sources, 37% municipal and industrial wastes/runoff, 33% other forms of transportation; 12.5% tanker accidents, and 9% from the atmosphere. \"Megaproject\" sources (oil platforms and tanker accidents) only accounted for 14% of the total sources of oil pollution into the marine environment. Our habitat support value user \u2022community expressed a need for information products as opposed to access to the raw databases which are the focus of the data quality and continuity initiative. For example, the NPDES permits list the permissible concentrations of a variety of chemicals inside the mixing zone for a point pollution source, while our user community needs to know the impact of these chemical levels on LMRs and the ecosystem that supports fish and shellfish: Many monitoring programs measure the levels of toxic chemicals in water, sediment, and organisms. Unfortunately there is no generally agreed upon list of safe levels for these toxic chemicals, based either upon direct research or risk assessment procedures (usually focused on impact on human health and not natural resources impacts). Another example is a need for a synthesis of life history information for key LMR species and their different life stages (eggs, larvae, juveniles, and adults) throughout their geographic range of distribution. Key coastal species in New England requiring such syntheses includes lobsters, winter flounder, summer flounder, softshell clams, striped bass, bluefish, river herrings, Atlantic salmon (special interest species), and shortnose sturgeon (endangered). The USFWS has prepared species profiles for many of these forms as part of their Biological Report series. Also NOS, National Estuarine Program (NEP), and Coastal Ocean Program (COP) have conducted habitat mapping programs for selected LMR species and their supporting habitats (wetlands and sea grass beds). One needs to continue supporting these types of data synthesis efforts. In order to help facilitate this effort it would be good to develop habitat quality/habitat support value workshops within each major NMFS region every 2 to 3 years to address gaps in our knowledge that are critical to NMFS managers/user community and to update/improve existing syntheses on the life history of LMRs and their supporting habitat. This would perform in the habitat arena the same function as the existing stock assessment workshops do in supporting the FMCs in providing advice on the status of stocks. Technical evaluations of the relative impacts of specific anthropogenic activities on LMR species or their critical habitats is needed. This would include such activities as: dredging, dredge material disposal, ocean outfalls for sewage/industrial wastes/storm discharge, marinas, raised platforms, toxic chemicals, biohazards (bacteria, viruses, and biotoxins ), and nutrient enrichment. This is another case of requiring an information product synthesized from our existing data bases or from those of other state/Federal agencies. There is a wealth of information in these areas buried in the gray literature, but some of this information is based upon data of variable quality (DQ&C issue). The other constraint is that a technical evaluation provides a generic answer to a problem, while most permits are site specific. The appropriate response to an anthropogenic activity also includes the past developmental history of the coastal system and the cumulative effect of other anthropogenic activities in the system."}, {"section_title": "THE IMPACT OF IGNORING THE PROBLEM:", "text": "Current information indicates that 28% of the Nation's LMR stocks are overfished. As a response, NOAAINMFS in conjunction with the FMCs is developing stock rebnilding strategies (sometimes under legal court orders). Even though it is thought that excessive harvesting played a greater role in stock declines than did habitat degradation, the environment in which the rebuilding occurs differs from that in which the stocks were originally predominant. For example on Georges Bank between 1970s and the present, cod and haddock have been displaced by dogfish and skates. The role of the dogfish and skates as predators may inhibit the growth of stocks of cod, haddock, and yellow tail pollock, even if fishing pressure is greatly reduced. Many endangered salmon species in California and Oregon have lost their spawning habitats as a result of dams/water diversion projects. These inland waters also suffer from nutrient enrichment and toxic chemical pollution, so that a sound rebuilding strategy has to go beyond fish hatcheries. Additional requests for development permits can exacerbate this situation unless NOAAfNMFS managers have up-to-date information on habitat quality of different regions and know the habitat support values required for different LMR species. It is not possible to rebuild either depleted commercial fisheries or endangered species/marine mammals without good information on habitat quality/habitat support values. Since many of the permits are on a site specific basis, one needs to evaluate cnmulative impacts of other permitted activities/anthropogenic development endeavors (non-point pollution sources) in the watershed or coastal system on the LMRs and their critical habitat. We need to move from a reactive mode (action based upon evidence of damage to LMRs) to a proactive mode to prevent damage before it occurs (adoption of the precautionary principle). If our depressed commercial LMR stocks recovered, then we could harvest near the sustainable yield level rather than at the 30% of that level which often occurs at present (cod and haddock in New England). Increased yields would aid coastal economies (both commercial and recreational fishing activities) and decrease our balance of payments deficit for fish products ($3.27B for 1991). Successful recovery plans for endangered species and marine mammals would aid NOAAINMFS in fulfilling its stewardship role for natural living resources."}, {"section_title": "-H", "text": "The Reanalysis Output includes: * Level-2 observational data in BUFR with QC information. *Analysis fields presented in \"synoptic\" form (every 6 hours) including basic variables and comprehensive diagnostic fields such as precipitation, surface fluxes and diabatic heating in sigma coordinates in GRIB format. A restarffile will be included once a month to allow rerunning shorter periods with enhanced diagnostics. * \"Time series\" output ordered by day including standard pressure level fields, precipitation and other diagnostic fiefds. This format will be the most useful for most users. *Possibly a special GRIB file for the North American region. Period covered by the Reanalysis: 1958Reanalysis: -1993, with a continuation into' the future through the CDAS. The reanalysis for the period 1985-1993 will be performed first, followed by 1979-1985, 1962-1979, 1958-1962. Reanalysis for the period 1946-1958, when the NH upper air network was established has been requested by several atmospheric researchers (e.g., J. M. Wallace), and will probably be also carried out at the end of this project. 5-day forecasts every 5 days if the computer resources allow it. Expected completion of the first phase (3 8 years): 1997. "}, {"section_title": "PREFACE", "text": "This, along with other workshop presentations, most notably the Monday morning keynote address by Jerry Mahlman, triggered considerable debate as to the root of prevailing inadequate attention to data qThe following prospectus is based on the presentation made at the Second NOAA Data Quality and Continuity Workshop. uality and continuity. The prospectus which follows grew out of that debate and the original presentation. The prospectus is now the basis of an NRC study supported by the NESDIS. William A. Sprigg January 6, 1994 The Board of Atmospheric Sciences and Climate (BASC) proposes to review and assess the policy and conduct of research in the context of data quality and continuity and prepare a report of findings. This study would present a recommended policy to guide source-to-end data management and observing and research practices to improve longterm data quality and continuity. The report would include a discussion of the important issues and guiding principles."}, {"section_title": "EXECUTIVE SUMMARY", "text": "Research to understand atmosphere and climate processes, to develop forecast techniques and to monitor and assess long-term changes in the climate system is hampered by inadequate attention to observation and data policies. The problem is international. In the broadest interpretation of these research aims, virtually all NOAA's data mandate may apply. A study on this subject would best address generic issues and refrain from focussing on specific data sets except where examples provide useful illustrations. Thus, conclusions of the study should apply to most environmental data management policies and apply world wide."}, {"section_title": "-", "text": "establish a firm scientific underpinning to the task 0 articulate and promote a national policy on the topic 0 begin development of an army of leading scientists who understand the data quality and continuity issue, help to form solutions to problems within the issue, practice research that follows preferred data policy guidelines and promote these practices world-wide in their research, consulting and teaching. Other ongoing BASC activities that provide opportunities to promote the conclusions of the study proposed, or contribute to it, include: Agencies are considering a BASC proposal to conduct a two-year study of the \"opportunities, chailenges and imperatives for the atmospheric sciences entering the 21st century\". The Board has also had preliminary discussions with the Administration concerning formation of a department of the environment (or other restructuring) and convergence of DOD, NASA, and NOAA satellite observing systems. The BASC's Climate Research Committee (CRC) has specifically highlighted observations and data management as an issue needing attention. The first report in its series of \"critical issues in climate research\" addresses this topic. The report is in peer revrew. The CRC is developing research strategies and reviewing plans for several major international programs (CLIVAR, GEWEX, GOALS, and DEC-CENf These programs will involve a large component of the international research community, continue for fifteen years, and gencrutc large amounts of data wh.iclt every one\u2022 will want free, immediately, totally, unconditionally and entirely devoid of error or suspicion. Data must not, as John Dutton (BASC chair) has said, be stored on \"Write-Only-Memory\" media. This research offers an opportunity to implement new policies and introduce a new way of thinking about data quality and continuity to the lasting benefit of future research. 2 CLIVAR is the World Climate Research Program's (WCRP) future thrust in understanding \"climate variability' GEWEX is the WCRP Global Energy and Water Cycle Experiment; GOALS (Global Ocean-Atmosphere-Land System) is the proposed U.S. contribution addressing the seasonal-to-interannual part of CLIVAR; DEC-CEN may become the abbreviated title of the U.S. proposed contribution to CLIVAR covering decade-to-century scales of climate-variability."}, {"section_title": "-I", "text": "The BASC's Committee on Atmospheric Chemistry (CAC) is the U.S. scientific interface to the International Global Atmospheric Chemistry program and has a report \"in press\" on the status of the program. It also interfaces with the International Geosphere-Biosphere Program. The BASC's Committee on Meteorological Analysis, Prediction, and Research has been asked by several agencies to advise on the U.S. Weather Research Program progress and plans."}, {"section_title": "APPROACH", "text": "The BASC will assign responsibility for the study to the Climate Research Committee, chaired by Eric Barron. A panel of experts will be formed that will include senior scientists whose statements on the subject are likely to motivate cooperation within the scientific community and whose reputations are acknowledged world wide. Others on the panel will include experts recently engaged in developing observation and data management strategies, with international representation. The CRC will draft terms of reference and recommend panel membership for the study at its May 10-12, 1993 meeting. Assuming receipt of funding in July, the NRC will form the panel in August and schedule a late Fall meeting. That first meeting will bring everyone up to date on relevant issues, and outline a study strategy and timetable. The panel will meet two more times to draft its report, with a possible fourth meeting of final editing and approval. The panel will brief the CRC, BASC, and the BGC at least once during the year. The report will undergo peer review and meet NRC approval, with publication and dissemination of the report in late 1994. Upon completion of the report, the panel chair and members ofCRC and BASC will be itvailable foibiieffugs Within NOAA and other agencies, national and international, as required, in order to assist in implementation of the panel's recommendations."}]