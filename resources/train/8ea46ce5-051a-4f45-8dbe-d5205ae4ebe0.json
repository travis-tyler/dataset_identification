[{"section_title": "Abstract", "text": "Abstract. In this study, we present a system for Alzheimer's disease classification on the ADNI dataset [1]. Our system is able to learn/fuse registration-based (matching) and overlap-based similarity measures, which are enhanced using a self-smoothing operator (SSO). From a matrix of pair-wise affinities between data points, our system uses a diffusion process to output an enhanced matrix. The diffusion propagates the affinity mass along the intrinsic data space without the need to explicitly learn the manifold. Using the enhanced metric in nearest neighborhood classification, we show significantly improved accuracy for Alzheimer's Disease over Diffusion Maps [2] and a popular metric learning approach [3] . Stateof-the-art results are obtained in the classification of 120 brain MRIs from ADNI as normal, mild cognitive impairment, and Alzheimer's."}, {"section_title": "Introduction", "text": "Alzheimers Disease (AD) and its preclinical stage, mild cognitive impairment (MCI), are the most common form of dementia in elders. Magnetic resonance imaging (MRI) can provide insight into the relation between AD and the structure of the brain: AD is known to be connected with gray matter loss [4] and with the shape of subcortical structures (especially the hippocampus) [5] . There have been several attempts in the literature to automatically classify a brain MRI as AD, MCI or normal (typically represented by older control subjects, OC). Chupin et al. [6] automatically segment the hippocampus and use its volume for the classification. Vemuri et al. [7] use support vector machines (SVM) based on tissue densities and a number of covariates (demographics, genotype). Kl\u00f6ppel et al. [8] feed a SVM directly with image data after registration (i.e. spatial alignment). Zhang et al. [9] use a SVM with cerebrospinal fluid, positron emission tomography and MRI data as features. Davatzikos et al. [10] use the distribution of gray matter, white matter and cerebrospinal fluid in registered space. Desikan et al. [11] feed the entorhinal cortex thickness, hippocampal volume and supramarginal gyrus thickness to a logistic regression analysis. These works are summarized in Table 1 .\nIn this paper, we approach the OC/MCI/AD classification problem from the perspective of metric learning. Given a number of heterogeneous affinity measures between the data points, the task is to find an enhanced metric which will [6] 605 OC (24%), MCI (49%), AD(27%) 60-80% Vemuri et al. [7] 100 OC (50%), AD (50%) 89% Kl\u00f6ppel et al. [8] 68 OC (50%), AD (50%) 94% Zhang et al. [9] 103 OC(50%), AD(50%) 93% Zhang et al. [9] 150 OC(34%), MCI(66%) 76% Davatzikos et al. [10] 30 OC (50%), MCI (50%) 90% Desikan et al. [11] 151 OC (62%), MCI (38%) 90%\nultimately improve the classification rate in a k-nearest neighbor (kNN) framework. Popular distance metric learning methods [12, 3] , which are mostly supervised, learn a Mahalanobis distance parametrized by a positive semi-definite matrix. However, the performance gain is rather limited because a global linear transform does not suffice to discriminate the data. Nonlinear versions exist, but it is difficult to find a kernel that provides good results. Non-parametric manifold learning techniques such as Isomap [13] do not necessarily provide a better metric, which limits their use in classification. They also have the disadvantage that explicitly estimating the manifold can be difficult and time consuming. Their application to medical image analysis has also been limited [14] .\nHere we adopt an unsupervised metric learning algorithm: self-smoothing operator (SSO). SSO enhances an input pair-wise affinity matrix similar to a Gram matrix. A smoothing kernel is built from the matrix and used to iteratively propagate the affinity mass between strongly connected neighbors, following the structure of the manifold without having to compute it explicitly. The framework can accommodate semi-supervise learning (i.e. taking advantage of not only labeled but also unlabeled examples to build a classifier [15] ): even if unlabeled examples cannot be used in the kNN classification, they can still be considered in the prior diffusion, often bridging gaps between points with the same label. A feature selection method is incorporated into the design of the affinity matrix to improve the results. We apply the proposed framework to the AD classification problem with registration-based and overlap-based similarity measures, comparing the results with metric learning [3] and Diffusion Maps [2] ."}, {"section_title": "Materials", "text": "Brain MRI from 120 subject Brain MRI scans from 120 subjects (age 76.7\u00b16.4 years) are used in this study. The subjects were randomly selected from the ADNI dataset [1] under two constraints: 1) the scans are from the same cross section (12 months after the start of the study); and 2) the three classes (OC,MCI,AD) and the two genders are equally represented. The scans were acquired with T 1 -weighted MPRAGE sequences, skull-stripped with BET [16] and fed to BrainParser [17] to automatically extract 56 cortical and subcortical structures."}, {"section_title": "Methods", "text": ""}, {"section_title": "Self-Smoothing Operator", "text": "SSO is closely related to the Diffusion Maps algorithm [2] , which defines diffusion distances between data samples to improve an input pair-wise affinity matrix. It introduces a global diffusion distance metric over data samples. Given the transition kernel H (a row-wise-normalized version of the pair-wise affinity matrix), the diffusion distance between data samples x i and x j at step t is defined as:\nwhere h t (i, \u00b7) is the i-th row of H t , and \u03c6 0 is the equilibrium distribution. Instead of using an alien notion of diffusion distances between data samples as in Diffusion Maps, we work on the affinity matrix directly, using a self-induced smoothing kernel. Given data samples {x i , . . . , x n } and a symmetric affinity function \u03d1(\n. SSO diffuses the pair-wise affinities of W along the geometry of the manifold without having to construct it explicitly:\nIn\nStep 1, the smoothing kernel P that governs the diffusion process in Step 2 is induced from the input similarity matrix. W (p) is not a proper Gram matrix (since \u03d1(x i , x j ) is not an inner product), so it is in general neither symmetric nor positive semi-definite (PSD), which is not a problem in this application: we simply take the k minimal non-diagonal values of each row as the k nearest neighbors for classification. The only parameter in the algorithm is the step p, which determines the scale at which the data are analyzed. The output W (p) is an updated weight matrix that represents similarity more faithfully than W (as experimentally shown below) and that can be used directly in classification."}, {"section_title": "Similarity/Divergence Measures", "text": "The affinities W (i, j) can be built from a similarity or divergence function, \u03b3, \nA divergence function complementing the Dice coefficient should consider nonlinear deformations. Here we use a diffeomorphic registration algorithm [18] to estimate the degree of warping that is required to deform a shape into another.\nTo compare brains i and j, we first register j to i. Then, for structure of interest s, we compute the irregularity of the obtained deformation field u ji (r) within the mask \u03a9 s,i corresponding to s in i (r is the location vector). We use the curvature and diffusion of u ji (r) as measures of irregularity:\nwhere the index d loops along the three spatial dimensions. The deformation field for a sample case is shown in Fig. 1a . The integrands \u03b3 curv and \u03b3 dif f are displayed in Fig. 1c and 1d . Finally, the corresponding weight matrices can be computed using a Gaussian function as follows:\nwhere [\u00b7] refers to curvature or diffusion, and var(\u03b3 [\u00b7] ) is the variance of the divergence \u03b3 across the dataset. The weights W [\u00b7] are explicitly symmetrized."}, {"section_title": "Feature Selection", "text": "Assuming that the global weight matrixW is a linear combination of matrices based on single features (divergences or similarities), the question is which combination of weights w = {w m } to use. Specifically, we seek to maximize the leave-one-out (LOO) classification rate \u03a8 (w) under the constraints: 0 \u2264 w \u2264 1 and 1 t w = 1. This problem is difficult to solve because \u03a8 is neither smooth nor convex, and has multiple local maxima. Instead, we further constrain the problem by assuming that only M \u2264 M weight matrices are used with equal weights w m = M \u22121 , \u2200m. Then the problem becomes analogous to that of feature selection in machine learning. This is still a hard combinatorial problem, but good approximate solutions can be achieved using a proper selection strategy. Here we use \"plus 2 -take away 1\" [19] : from an initial empty set, features are greedily added / removed one at the time following the pattern +, +, \u2212, +, +, \u2212, . . .. The final set of features is the one that maximizes \u03a8 (w) throughout the process."}, {"section_title": "Experiments and Results", "text": ""}, {"section_title": "Experimental Setup", "text": "The feature selection was cross-validated (10 folds) to obtain an unbiased estimate of the performance; otherwise features are selected upon the test data. For each fold, a set of features is selected with LOO on the training data. For each candidate set, the scale of the diffusion p is tuned individually using exhaustive search. The selected features and p are used to classify the test data in the fold. The number of neighbors was kept constant (k = 10) to limit the computational load of training. Ties are broken by examining subsequent neighbors. Rather than using all the 56 segmented structures in the selection process, only the (left and right) caudate nucleus, hippocampus and putamen are considered (18 features in total). These structures are well-known to be related to AD, and using this reduced set decreases the risk of overfitting.\nIn testing, an augmented W is created by adding to the original a new row and column for each test sample. We assume that all the test data are simultaneously available, which enables semi-supervised learning: during the diffusion process, the unlabeled test data can increase the performance of the system by making the structure of data easier to follow (only the labeled training data are considered during the kNN classification).\nFor the sake of comparison, analogous experiments were run using Diffusion Maps and the metric learning approach from [3] , which attempts to find the positive definite matrix A that parameterizes the Mahalanobis distance best separating the training data into the different classes. Cross validation was again performed with 10 folds using the training data to select features (same selection strategy) and tune parameters: the matrix A for metric learning and the step t for Diffusion Maps (i.e. the scale of the diffusion). As for SSO, the number of nearest neighbors was fixed throughout the experiments (k = 10)."}, {"section_title": "Results", "text": "The impact of feature selection on the performance is illustrated in Fig. 2a . The three most frequently selected features were: 1) diffusion -left hippocampus; 2) curvature -left putamen; and 3) overlap -left caudate. It is not surprising that the top feature is related to the hippocampus, which is known to be strongly connected with AD [5] . The curve in Fig. 2b shows the impact of the diffusion on the classification. At first, increasing the scale of the diffusion p has a positive influence on the accuracy, which is boosted from \u223c 55% at p = 0 (no diffusion) to \u223c 90% at p \u2248 0.6. This is illustrated with a sample subject and its nearest neighbors before and after diffusion in Fig. 3 . When p becomes too large, data samples start to come too close to one another and the accuracy begins to decrease. Fortunately, the location of the peak is quite stable and the method generalizes well, as shown by the cross validation experiment below.\nTables 2a through 2c display the confusion matrices for metric learning, Diffusion Maps and our approach, respectively. Metric learning performs poorly because the structure of the data is too complex to discriminate the classes using a global linear transform. Diffusion Maps provides decent results: 78% accuracy with no mistakes between OC and AD. Our SSO-based approach makes no OC-AD mistakes either, but preserves the structure of the input similarity better than Diffusion Maps, increasing the accuracy to 89%. There is no noticeable drop in accuracy from the training data (Fig. 2) because cross-validation (LOO) was already used within the feature selection process.\nEven though the results reported in Table 1 were achieved on other datasets, it is illustrative to compare them to ours. Chupin et al.'s study, the only one considering the three-class problem, reports considerably lower accuracy than this work. To compare our results with the methods which classify OC vs. AD, we assume that only OC / AD are fed to the classifier and that the samples classified as MCI are relabeled to either OC or AD. Another option would be to remove the MCI cases from the training data, but that would have a negative impact on the results (the diffusion would be guided by less data). Our approach provides 96.25% or 97.5% accuracy (depending on the relabeling criterion), slightly higher than the best reported results in the literature (Kl\u00f6ppel et al., 94%) . In order to compare our approach with methods that discriminate OC from MCI, we assume that only OC and MCI cases are fed to the classifier, and the cases for which the estimated class is AD are relabeled as MCI. In that case, the accuracy is 91.25%, comparable to Davatzikos et al. "}, {"section_title": "Discussion and Future Work", "text": "A nearest neighbor classifier based on registration and overlap features and enhanced by a self-smoothing operator has been presented in this study. SSO propagates the similarity between data samples along the manifold in which the data lie. The updated affinity measure can be used in a nearest neighbor framework to classify brains as AD, MCI or OC, achieving state-of-the-art results. The main disadvantage of the method is that, when a new case is presented to the system, computing the corresponding new row in the affinity matrix requires nonrigid registration to all the training cases, which is very time consuming (the SSO algorithm itself only takes a fraction of a second). Exploring its application to other disease patterns, testing features that are faster to compute and improving the design and combination of features remain as future work."}]