[{"section_title": "Abstract", "text": "Abstract-Deformable image registration is a fundamental task in medical image analysis, aiming to establish a dense and non-linear correspondence between a pair of images. Previous deep-learning studies usually employ supervised neural networks to directly learn the spatial transformation from one image to another, requiring task-specific ground-truth registration for model training. Due to the difficulty in collecting precise ground-truth registration, implementation of these supervised methods is practically challenging. Although several unsupervised networks have been recently developed, these methods usually ignore the inherent inverse-consistent property (essential for diffeomorphic mapping) of transformations between a pair of images. Also, existing approaches usually encourage the to-be-estimated transformation to be locally smooth via a smoothness constraint only, which could not completely avoid folding (typically means registration errors) in the resulting transformation. To this end, we propose an Inverse-Consistent deep Network (ICNet) for unsupervised deformable image registration. Specifically, we develop an inverse-consistent constraint to encourage that a pair of images are symmetrically deformed toward one another, until both warped images are matched. Besides using the conventional smoothness constraint, we also propose an anti-folding constraint to further avoid folding in the transformation. The proposed method does not require any supervision information, while encouraging the diffeomoprhic property of the transformation via the proposed inverse-consistent and anti-folding constraints. We evaluate our method on T1-weighted brain magnetic resonance imaging (MRI) scans for tissue segmentation and anatomical landmark detection, with results demonstrating the superior performance of our ICNet over several state-of-the-art approaches for deformable image registration. Our code will be made publicly available."}, {"section_title": "INTRODUCTION", "text": "As a fundamental task in medical image analysis, deformable image registration aims to establish dense, nonlinear spatial correspondences between a pair of images (denoted as the source/moving image and the target/fixed image) [1] . For example, while it is difficult to compare brain magnetic resonance imaging (MRI) scans of different subjects due to significant anatomical variability [2] , [3] , deformable registration enables direct comparison of anatomical structures across scans, and thus, is crucial for understanding variability across populations and the longitudinal evolution of brain anatomy for individuals with brain diseases [4] - [6] .\nOver the past few decades, a variety of non-learningbased deformable registration methods have been proposed for medical image analysis [6] - [9] . Typically, these methods iteratively optimize a similarity function for each pair of images to non-linearly align voxels with the similar appearance, while encouraging local smoothness on the registration mapping [1] . Since the similarity function needs to be optimized from scratch for each pair of unseen images, i.e., the inherent registration patterns shared across different images are ignored, these methods are usually slow to perform deformable registration in practical applications. To address these issues, supervised learning-based and deep-learningbased approaches have been developed for deformable image registration [10] - [12] . These methods typically rely on task-specific ground-truth registration to train a regression model for image registration. However, the difficulty of J. Zhang is with the Tencent AI Medical Center, China. (Email: xdzhangjun@gmail.com).\ncollecting ground-truth information often limits their utility in real-world applications. In addition, the mapping learned by these supervised methods might be biased by the selected ground-truth registration.\nRecently, several unsupervised deep-learning methods have been proposed for medical image registration [13] - [17] without using any pre-defined supervision information for network training, thus maintaining the unsupervised nature of deformable registration. Although these methods achieve better registration performance in comparison to traditional supervised-learning-based methods, the output transformations (e.g., displacement field or flow) are usually asymmetric, i.e., the inherent inverse-consistent property of transformations between a pair of images is ignored. Here, the inverse-consistent property means that to-be-learned optimal transformations would encourage that a pair of images are symmetrically deformed toward each another, and the two bidirectionally deformed images are finally matched. Unfortunately, previous studies usually independently estimate the transformation from an image A to an image B or from B to A, thus failing to ensure these transformations be inverse mappings for each another. Besides, most of the existing (supervised or unsupervised) algorithms utilize solely a spatial smoothness penalty to constrain the transformation, which could not completely avoid foldings (typically indicating errors) in the registration mapping. As an illustration, in Fig. 1 , we show two flows generated by a state-of-the-art deep-learning method [17] trained with different (i.e., strong vs. weak) contributions from the smoothness constraint. As shown in Fig. 1 (a) , if we excessively encourage local smoothness of the to-be-estimated flow by using a large weight for the smoothness constraint, the [17] using (a) a large weight for the smoothness constraint and (b) a small weight for the smoothness constraint, respectively. obtained registration will be inaccurate due to global errors. Otherwise, there will be a lot of foldings in the learned flow (see Fig. 1 (b) ), thus generating wrong registration due to local defects. That is, it is challenging to properly tune the contribution of the smoothness constraint to simultaneously avoid foldings in the estimated flow and maintain high registration accuracy.\nTo address these two issues, in this paper, we propose an Inverse-Consistent deep neural Network (ICNet) for unsupervised deformable image registration. Specifically, in ICNet, we develop an inverse-consistent constraint to encourage that a pair of images are symmetrically deformed toward each another in multiple passes, until the bidirectionally deformed images are matched to achieve correct registration. Besides using the conventional smoothness constraint, we also develop an anti-folding constraint to avoid foldings in the to-be-estimated flow. The proposed ICNet method does not require any supervision information, which also encourages the diffeomoprhic property of the transformations via the proposed inverse-consistent and anti-folding constraints. We evaluate our method in both tasks of tissue segmentation and anatomical landmark detection with 3D T1-weighted brain MRI scans. The experimental results demonstrate the superior performance of the proposed method over several state-of-the-art methods in deformable image registration.\nThe rest of this paper is organized as follows. We briefly introduce relevant studies in Section 2. In Section 3, we present the proposed network, inverse-consistent constraint, and anti-folding constraint in detail. We then describe studied materials, competing methods, experimental settings and results in Section 4. We further analyze the influence of several essential strategies used in the proposed method in Section 5. We finally conclude this paper in Section 6."}, {"section_title": "RELATED WORK", "text": ""}, {"section_title": "Deformable Image Registration", "text": "Deformable image registration refers to an non-linear process of revealing the voxel-wise spatial correspondence between source and target images. Let S denotes the source image, and T represents the target image. We assume F ST is the to-be-learned flow (i.e., displacement field) that warps S to T. The optimization problem is typically defined as\nwhere \u03a6 denotes the transformation operator that warps the source image S to the target image T using the flow F ST . The first term in Eq. 1 is a similarity/matching/distance criterion, which is used to quantify the level of alignment between the warped source image \u03a6(S, F ST ) and the target image T. The second term is a regularizer that constrains the transformation to favor a specific property, such as encouraging the estimated flow to be locally smooth. The optimization problem consists of either maximizing or minimizing the objective function, depending on how the first term is defined. Different algorithms for deformable registration mainly differ in deformable models, similarity criteria, and numerical optimization [1] , [18] - [23] . In the literature, many types of similarity metrics have been proposed for image registration, such as mean squared distance (MSD) [24] , [25] , sum-of-squares distance (SSD) [26] - [28] , normalized cross correlation (CC) [6] , [11] , [29] , [30] , and normalized mutual information (MI) [31] - [33] . Besides, there are various regularization terms developed to penalize undesired deformations, such as topology preservation [34] - [36] , [36] , [37] , volume preservation [38] - [43] , and rigidity constraints [18] , [44] , [45] . In general, the existing registration algorithms can be roughly divided into two categories, including 1) nonlearning-based methods, and 2) learning-based methods. We now introduce relevant studies in these two categories."}, {"section_title": "Non-learning-based Registration Methods", "text": "Non-learning-based registration algorithms typically optimize a transformation iteratively based on an energy function in the form of Eq. 1. Based on how to compute the similarity between the warped source image and the target image, there are two types of registration approaches, including 1) volume-based methods where the voxel intensities in the whole volume are used to drive the registration process, and 2) landmark-based methods where features extracted at anatomical landmarks are employed to guide the matching of the local correspondence during registration.\nThe most popular non-learning-based methods for deformable registration include automatic image registration (AIR) [24] , automatic registration toolbox (ART) [29] , HAM-MER [6] , Demons [26] , diffeomorphic Demons [27] , [46] , statistical parametric mapping (SPM) [25] , deformable registration via attribute matching and mutual-saliency weighting (DRAMMS) [47] , DROP [48] , CC/MI/SSD-FFD [8] , FNIRT [28] , and symmetric normalization (SyN) [5] . Most of these methods require iterative optimization algorithms for parameter tuning [1] . Also, the registration performance of these methods would degrade when the source and target images have large variations in anatomical appearance. Therefore, robust and tuning-free deformable registration methods are highly desired for dealing with different data and registration tasks."}, {"section_title": "Learning-based Registration Methods", "text": "Many learning-based methods have been developed for deformable image registration [49] , such as those based on random forest [50] , support vector regression [51] , sparse representation [52] , and deep neural networks [12] , [16] , [17] . In these methods, deformable registration is often formulated as a learning problem to estimate the registration parameters. Compared with non-learning methods, learning-based approaches can predict the transformation efficiently for unseen testing images based on pre-trained models. According to whether supervision information is needed, existing learning-based methods for image registration can be further categorized into two types: 1) supervised learning methods, and 2) unsupervised learning methods."}, {"section_title": "Supervised Methods", "text": "In supervised methods for deformable image registration, task-specific supervision information (e.g., ground-truth registration) is usually required for model training. For instance, random forest has been applied for infant brain MRI registration and multi-modal image registration [50] , [53] , [54] , based on hand-engineered imaging features and predefined ground-truth registration. However, the registration performance of such traditional learning-based methods could be sub-optimal, since the process of feature extraction is independent of the model training.\nSeveral deep-learning-based methods have been recently developed, by incorporating feature extraction and registration model learning into a unified framework. Cao et al. [12] proposed a convolutional neural network (CNN) based regression model to directly learn the mapping from the input image pair (i.e., target and source images) to their corresponding deformation fields. Yang et al. [55] developed a fully convolutional network (FCN) to predict 3D deformable registration, followed by a correction network to further refine the predicted transformation. Roh\u00e9 [56] proposed an FCN model to learn the stationary velocity field, which consists of a contracting path to extract taskrelevant features and a symmetric expanding path to output the transformation parameters. Sokooti et al. [57] proposed to predict displacement vectors by CNN models. Krebs et al. [58] adopted a deep reinforcement learning framework to estimate deformation fields. These methods usually require task-specific ground-truth registration for model learning. Since ground-truth information is difficult to collect, supervised methods usually have limited utility in practice. Also, the performance of supervised methods is largely determined by the quality of predefined ground-truth registration."}, {"section_title": "Unsupervised Methods", "text": "By maintaining the unsupervised nature of deformable registration, unsupervised deep-learning methods have also been applied for medical image registration [13] - [17] . That is, these methods do not rely on any pre-defined supervision information for network training. For example, Miao et al. [14] developed a CNN model to learn transformation parameters for 2D/3D images, where imaging features for parameter regression should be predefined, which means this network cannot be trained in an end-to-end manner. Wu et al. [13] proposed an unsupervised deep-learning algorithm for image registration based on image patches. Although this method can automatically extract features from images, it requires additional post-processing that cannot be handled inside CNNs. Shan et al. [15] developed an unsupervised end-to-end deep-learning model for 2D tissue registration, by directly predicting deformation field via a CNN. In [16] , an end-to-end unsupervised deep-learning model, consisting of a CNN-based regressor, a spatial transformer, and a re-sampler, was developed for deformable registration. Then, Guha et al. [17] proposed an unsupervised CNN model, in which a spatial transformer network (STN) is also used to reconstruct one image from another while imposing smoothness constraints on the registration field. This method has achieved superior accuracy for 3D image registration in comparison to previous methods.\nIt is worth noting that most of the existing deep-learning methods ignore the inherent inverse-consistent property of transformations between a pair of images [15] - [17] . That is, by independently estimating the transformation from an image A to an image B and that from B to A, these methods are unable to ensure that these transforms are inverse mappings of one another. Note that several studies tackle this shortcoming by jointly estimating the transformations from both A to B and B to A, under the consistency constraint that these transformations are inverses of one another [34] , [59] , [60] . However, these methods are not learning-based methods and require human-engineered feature representations (e.g., image intensity with Fourier series parameterization) of input images, which may not extracted in a task-oriented manner. Motivated by these studies, we develop an unsupervised deep network with an inverseconsistent constraint to encourage the inverse-consistent property of transformations between a pair of input images. Besides, considering using the smoothness constraint alone (as previous studies did) is not sufficient to guarantee that there is no folding in the estimated transformations, we further develop an anti-folding constraint to avoid foldings in the learned transformations."}, {"section_title": "PROPOSED METHOD", "text": "In this section, we first introduce the notations used in this paper. Then, we describe the proposed inverse-consistent deep neural network, as well as the objective function (with both the proposed inverse-consistent and anti-folding constraints) for network training. We finally introduce the implementation details for the proposed method."}, {"section_title": "Notations", "text": "In image registration, a pair of images are usually referred to as the source image and the target image. Since we do not rely on particular target images in our method, in this paper, we denote one input image as A and the other as B. These two images are defined in the image domain \u2126. The transformation is a mapping function of the image domain \u2126 to itself, which spatially deforms any point locations to other locations. Also, we assume that the image A is deformed to match the image B according to a dense flow (i.e., discrete displacement field) F AB defined in the A space, while the image B is deformed to match the image A via another dense flow F BA defined in the B space. Note that each element in a flow is a 3-dimensional vector (corresponding to three axes, i.e., x, y, z), indicating the displacement of a particular voxel from its original location to a new location. In addition, the deformed/warped images of A and B are denoted as A and B, respectively. \nC o n tr a c ti n g P a th E x tr a c ti n g P a th Figure 2 illustrates the proposed unsupervised deep network for deformable image registration. As shown in Fig. 2 (a), we employ a fully convolutional network (FCN) to model two dense, non-linear transformations (i.e., F AB and F BA ) from a pair of input images (i.e., A and B) to their warped images (i.e., A and B). There are two FCN modules in our proposed network. The first one is used to align the image A to B (as the target image) using the flow F AB , generating the warped image A. In contrast, the second FCN is designed to model the registration mapping from the image B to A (as the target image) via the flow F BA , yielding the warped image B. It is worth noting that these two FNCs share network structure and parameters. As shown in Fig. 2 (b), the FCN we used here follows a U-Net architecture [61] to capture and combine both global and local structural information of input images. Specifically, the input data contain two channels (with each channel corresponds to a particular input image), and n is the number of starting filter channels of the FCN. The FCN contains a contracting path for image down-sampling and an expanding path for image up-sampling. Every step in the contracting path contains a 3\u00d73\u00d73 convolution with a stride of 1, and a 3 \u00d7 3 \u00d7 3 convolution with a stride of 2 for downsampling. Besides, each step in the expanding path consists of a 2\u00d72\u00d72 deconvolution with a stride of 2 for up-sampling, followed by a concatenation process to combine up-sampled feature maps with the corresponding feature maps from the contracting path, and then a 3 \u00d7 3 \u00d7 3 convolution with a stride of 1. In this network, each convolution is followed by a rectified linear unit (ReLU) activation, while the output of the last layer (having 3 filter channels that are corresponding to the x, y, and z axis) is constrained into [\u2212\u03c4, \u03c4 ]. That is, we first use a tanh function to normalize the output of the last layer to [\u22121, 1], followed by multiplying by a constant \u03c4 (i.e., the maximum displacement magnitude). Since both input images are treated equally in the proposed ICNet framework, the two FCNs in Fig. 2 (b) share the same parameters."}, {"section_title": "Inverse-consistent Unsupervised Neural Network", "text": "Besides, we can see from Fig. 2 (a) that a grid sampling module is utilized to generate the warped image (e.g., A), based on the input image (e.g., A) and the learned flow (e.g., F AB ). Specifically, such grid sampling is implemented via the fully-differentiable spatial transformer network (STN) [62] , containing a regular spatial grid generator and a sampler. The flow (displacement field) predicted by our image registration network is used to transform the regular spatial grid into a sampling grid. Then, the sampler uses the sampling grid to warp the input image. Bilinear interpolation is used during the sampling process, making STN fully differentiable for back propagation.\nFurthermore, an inverse network is developed to generate an estimated inverse flow (e.g., F BA ) of each transformation learned by the FCN (e.g., F AB ), based on which an inverse-consistent loss (i.e., Loss inv 1 and Loss inv 2 ) is further adopted to encourage the inverse-consistent property of two transformations. As shown in Fig. 2 (c) , we utilize the grid sampling strategy to generate the estimated inverse flow F BA , based on both F AB and \u2212F AB ."}, {"section_title": "Proposed Objective Function", "text": ""}, {"section_title": "Inverse-consistent Constraint", "text": "Existing deep-learning methods typically ignore the inverseconsistent property of transformations between a pair of images [15] - [17] . Motivated by previous non-learning-based inverse-consistent methods [34] , [59] , [60] , we propose to simultaneously estimate the transformation from A to B (i.e., F AB ) and the transformation from B to A (i.e., F AB ), and enforce the consistency constraint that these bidirectional transformations are inverse mappings of one another.\nSpecifically, we propose an inverse-consistent regularization term to penalize the difference between two transformations from the respective inverse mappings. As shown in Fig. 2 (c) , we rely on an inverse network to generate the inverse mapping (e.g., F BA ) of each transformation (e.g., F AB ). Specifically, for the flow F AB , we first obtain its negative flow (i.e., \u2212F AB ) in the A space. We then feed both F AB and \u2212F AB to the grid sampling module (via a STN) to obtain the estimated inverse flow (i.e., F BA in the B space) of F AB . Similarly, we feed both F BA and its negative flow \u2212F BA to the grid sampling module, and hence can obtain the estimated inverse flow (i.e., F AB ) of F BA . Mathematically, the proposed inverse-consistent constraint can be defined as follows\nwhere G is the mapping generated by the grid sampling module (via a STN), and \u00b7 F represents the Frobenius norm of a matrix. The two terms in Eq. 2 correspond to the notations L inv 1 and L inv 2 in Fig. 2 (a) . By minimizing Eq. 2, we concurrently encourage both the difference between the flows F AB and F AB (i.e., the inverse of F BA ) and that between F BA and F BA (i.e., the inverse of F AB ) to be small. In this way, the inverse-consistent property of the to-beestimated transformations can be explicitly modeled in the proposed network."}, {"section_title": "Anti-folding Constraint", "text": "As mentioned in Section 1 (e.g., Fig. 1 ), if we excessively encourage local smoothness of the to-be-estimated flow by using a large weight for the smoothness constraint, the registration results will be inaccurate. Otherwise, there will be possible foldings in the flow, thus yielding unreasonable registration. To deal with this issue, besides using the conventional smoothness constraint, we also develop an antifolding constraint as\nwhere \u2207F i AB (p) is the gradient of the flow F AB along the i-th (i \u2208 {x, y, z}) axis at the location of the voxel p. Besides, the term \u03b4(Q) is an index function used to penalize the gradient of the flow at the locations with foldings. That is, if Q \u2264 0, \u03b4(Q) = |Q|; and \u03b4(Q) = 0, otherwise.\nThe purpose of Eq. 4 can be explained as follows. If there is a folding at the location of p along the i-th axis (i.e., \u2207F i AB (p) + 1 \u2264 0), we enforce the penalty |\u2207F i AB (p) + 1| on the gradient at this location, requiring this gradient to be small. In contrast, if \u2207F i AB (p) + 1 > 0 (i.e., no folding at the location of p along the i-th axis), we do not penalize the gradient at this location. More detailed explanation can be found in the Appendix."}, {"section_title": "Smoothness Constraint", "text": "In previous studies, the to-be-estimated deformation field is generally to be locally smoothed via a smoothness constraint on its spatial gradients [16] , [17] . Here, we also use such smoothness constraint in the objective function as\nwhere \u2207F AB (p) is the gradient of the flow F AB at the voxel p, while \u2207F BA (p) denotes the gradient of the flow F BA at the voxel p. The operation \u00b7 2 represents the l 2 norm of a vector. Here, we approximate the spatial gradients using the differences between neighboring voxels."}, {"section_title": "Objective Function", "text": "In this work, we utilize the mean squared distance (MSD) as the similarity metric to compare the alignment between the warped image and its corresponding target image. Specifically, the MSD-based symmetric similarity is employed to measure the shape differences between the deformed image A and image B and the differences between the deformed image B and image A, which is defined as\nwhere A = G (F AB , A) and B = G (F BA , B) , and G is the mapping function learned in the grid sampling module.\nAccordingly, the loss function of our proposed ICNet for deformable registration is formulated as follows\nwhere the parameters \u03b1, \u03b2 and \u03b3 are used to balance the contributions of the smoothness, inverse-consistent and anti-folding regularizers, respectively."}, {"section_title": "Implementation", "text": "We implement the proposed deep network with Pytorch [63] . The objective function in Eq. 7 is optimized by the Adam algorithm [64] combined with a back-propagation algorithm for computing gradients as well as updating network parameters. The learning rate for Adam is empirically set to 5 \u00d7 10 \u22124 . In Fig. 3 , we report the change curves of both training (red) and validation (green) losses on the public Alzheimer's Disease Neuroimaging Initiative (ADNI-1) database [65] , where 10% subjects are randomly selected as the validation data, and the remaining subjects are used as the training data. This figure indicates that the proposed ICNet method generalizes well with almost no over-fitting issue, and the proposed objective functions converges quickly within 40, 000 iterations. For readers' convenience, our code and trained model will be made publicly available online."}, {"section_title": "EXPERIMENTS", "text": "In this section, we first introduce the studied materials, competing methods, and experimental settings. Then, we present results of brain tissue segmentation and anatomical landmark detection based on the warped MR images achieved by different registration methods. We finally analyze the computational costs of different methods. "}, {"section_title": "Materials and Image Pre-processing", "text": "We perform experiments on 860 subjects from two subsets of the public ADNI database 1 [65] , i.e., 1) ADNI-1, and 2) ADNI-2. To be specific, there are 805 subjects with the baseline structural brain MRI scans in ADNI-1, while the remaining 55 subjects with the baseline structural MRI data are randomly selected from ADNI-2. Since several subjects participated in both ADNI-1 and ADNI-2, we simply remove these subjects from ADNI-2, to ensure that ADNI-1 and ADNI-2 are independent datasets in the experiments. Notably, the studied subjects from ADNI-1 have 1.5 T T1-weighted MRI scans, while those in ADNI-2 have 3.0 T T1-weighted MRI data.\nFor all structural brain MR images, we perform both spatial normalization and intensity normalization for image normalization. For spatial normalization, we first perform skull stripping and cerebellar removal for all brain MRIs, and then linearly align them to a common Colin27 [66] template. We further resample all linearly aligned images to have the same spatial resolution (i.e., 1 mm\u00d71 mm\u00d71 mm), followed by cropping them to have the same image size (i.e., 144 \u00d7 192 \u00d7 160). For intensity normalization, we first match the intensity histogram of each brain MRI to that of the Colin27 template by using a histogram matching algorithm. Then, we also perform the z-score normalization to make the mean intensity of each image is zero and the standard deviation is one.\nIn the experiments, we perform two tasks to evaluate the registration performance of different methods, including 1) brain tissue segmentation and 2) anatomical landmark detection. In the task of brain tissue segmentation, the ground-truth segmentation is generated by using first FAST in FSL [67] to obtain the tissue segmentation map, followed by further manual correction. As illustrated in Fig. 4 (b) , three tissues are segmented from each brain MR image, including cerebrospinal fluid (CSF), gray matter (GM) and white matter (WM). In the task of anatomical landmark detection, the ground-truth landmarks are manually annotated by an experienced radiologist. As shown in Fig. 4 (c) , five anatomical landmarks are annotated in each brain MR image, which are mainly located in ventricles. 1 . http://adni.loni.usc.edu "}, {"section_title": "Competing Methods", "text": "We compare the proposed ICNet method with three stateof-the-art methods for deformable image registration, including 1) Demons with the symmetric local correlation coefficient used as the similarity metric [46] , 2) symmetric normalization (denoted as SyN) [5] , and 3) a unsupervised deep-learning (denoted as DL) method with a minimum mean squared error (MMSE) loss and a smoothness constraint [17] . Among them, Demons and SyN are nonlearning-based methods, while DL is an unsupervised learning method. For the fair comparison, the network architecture of the DL method is similar to our ICNet, while its objective function is different from ours. Specifically, only the first two terms in Eq. 7 is included in the objective function of DL, and hence, DL can be regarded as a degenerated variant of ICNet.\nTo evaluate the effectiveness of the proposed two constraints (i.e., the inverse-consistent constraint and the antifolding constraint), we further compare ICNet with its two variants. The first variant is denoted as ICNet-1, in which the proposed inverse-consistent constraint is removed. Similarly, the second variant is denoted as ICNet-2, in which the proposed anti-folding constraint is not used."}, {"section_title": "Experimental Settings", "text": "Since the six methods for comparison are all unsupervised, we do not need to generate any ground-truth registration for them. For the non-learning-based methods (i.e., Demons and SyN), we utilize their recommended parameter settings in the experiments. For the learning-based methods (i.e., DL, ICNet-1, ICNet-2, and ICNet), we treat the ADNI-1 and ADNI-2 as the training set and testing set, respectively. We randomly select 10% subjects from the ADNI-1 as the validation data, while the remaining subjects are used as the training data.\nIn the training stage, we randomly select a pair of MR images from the ADNI-1 as the input for each learningbased method. In the testing stage for all competing methods, to perform deformable registration, we first select 5 MR images from the ADNI-2 as atlas images, while the remaining 50 MR images are used as testing images. By using different deformable registration algorithms, we first warp each of the 5 atlases to a particular testing image, and thus generating 5 warped atlas images based on this testing image. Then, we employ a multi-atlas based segmentation algorithm with a majority voting strategy [68] to perform brain tissue segmentation in each testing image. Similarly, for landmark detection, each landmark in the atlases is first mapped onto a particular testing image via the corresponding deformable transformation [69] . Thus, given a testing image, we obtain 5 warped landmark positions for each landmark, followed by averaging these positions to generate the final location of this landmark. It is worth noting that, to evaluate the performance of the proposed ICNet for deformable image registration, we only utilize atlas-based methods for tissue segmentation and landmark detection, while other supervised learning methods are beyond the scope of this paper.\nIn ICNet, the parameter \u03b3 is empirically set to 10 5 to avoid folding in the flow as much as possible, and the other two parameters (i.e., \u03b1 and \u03b2) are determined via grid search within the range of [10 \u22125 , 10 \u22124 , \u00b7 \u00b7 \u00b7 , 10 5 ] on the validation set. Similarly, parameters in the three competing methods (i.e., \u03b1 in DL, \u03b1 in ICNet-1, and \u03b1, \u03b2 in ICNet-2) are also selected from the same range through cross-validation. Besides, we set \u03b2 = 0, \u03b3 = 10 5 in ICNet-1, and \u03b3 = 0 in ICNet-2. The number of starting filter channels n (see Fig. 2 (b) ) for ICNet and its two variants (i.e., ICNet-1 and ICNet-2) is empirically set to 8. In each of the four deeplearning methods (i.e., DL, ICNet-1, ICNet-2, and ICNet), the output of the last layer (with 3 filter channels corresponding to the x, y, and z axis) is constrained into the range [\u2212\u03c4, \u03c4 ]. Following [70] , we empirically set \u03c4 = 7 in this work, considering the displacement magnitude is usually less than 7 in the deformable registration of brain MRI scans.\nIn the experiments of brain tissue segmentation, five complementary metrics are used for quantitative evaluation of segmentation performance, including 1) dice similarity coefficient (DSC), 2) sensitivity (SEN), 3) positive predictive value (PPV), 4) average symmetric surface distance (ASD), and 5) Hausdorff distance (HD). In the experiments of anatomical landmark detection, for each landmark, we report the landmark detection error by computing the Euclidean distance between the estimated landmark location (achieved by a specific method) and its ground-truth location. For the evaluation metrics of ACC, SEN and PPV, higher values indicate better performance. For the remaining three metrics (i.e., ASD, HD, and detection error), lower values denote better performance."}, {"section_title": "Results of Brain Tissue Segmentation", "text": "In the first group of experiments, we perform the segmentation of three types of brain tissues (i.e., CSF, GM and WM), based on the warped atlas images generated by six different methods. The experimental results are shown in Table 1 . From Table 1 , one could have the following observations. First, in most cases, the proposed methods (i.e., ICNet-1, ICNet-2, and ICNet) achieve the overall best performance (regarding DSC, SEN, PPV, ASD, and HD) for segmenting all the three types of tissues. For instance, the DSC value achieved by ICNet for CSF segmentation is 83.58%, while the DSC produced by the conventional deep-learning method (i.e., DL) is only 79.76%. Second, even though no supervision information is required, four unsupervised deeplearning-based methods (i.e., DL, ICNet-1, ICNet-2, and ICNet) usually outperform two non-learning-based methods (i.e., Demons and SyN). The underlying reason may be that deep-learning-based methods can extract task-oriented features via neural networks, while conventional methods simply employ hand-engineered features of brain MRIs. Besides, we can see that our ICNet method usually outperform its two variants (i.e., ICNet-1 and ICNet-2). Note that ICNet-1 does not use the proposed inverse-consistent constraint, and ICNet-2 does not utilize the proposed anti-folding constraint. This implies that including both the inverseconsistent and anti-folding constraints to train our ICNet could boost the deformable image registration performance.\nGiven a testing image, we further visually compare the registration results for a source brain MR image achieved by different methods in Fig. 5. From Fig. 5 , we can see that the proposed ICNet method brings impressive improvement for the registration results, compared with the competing methods. For instance, it is obvious that the regions of the left and right planum temporale are more accurately registered to the target image using ICNet, as indicated by the red arrow (left planum temporale) and the yellow arrow (right planum temporale) in Fig. 5. "}, {"section_title": "Results of Anatomical Landmark Detection", "text": "In the second group of experiments, we perform landmark detection based on the deformed atlas images generated by different registration methods, with the results reported in Table 2 . From Table 2 , we can see that the overall performance of our ICNet method is superior to the five competing methods. For instance, the average landmark detection error achieved by ICNet is 2.61, which is lower than the result of Demons (i.e., 3.16).\nIt is worth noting that our methods (i.e., ICNet-1, ICNet-2, and ICNet) are unsupervised and do not need to generate the ground-truth flow for each to-be-registered image during network training. This is a particularly useful property for the deformable registration algorithm, which not only maintains the unsupervised nature of deformable registration, but also avoids the challenge of collecting accurate ground-real registration."}, {"section_title": "Computational Cost", "text": "We now analyze the computational costs of the proposed ICNet method and those competing methods. For the four deep-learning-based methods (i.e., DL, ICNet-1, ICNet-2, and ICNet), the training process is performed off-line, while the non-leanding-based methods (i.e., Demons and SyN) do not need any training process. Hence, we only analyze the on-line computational cost for nonlinearly registering a new brain MRI in the application/testing stage. Table 3 reports the computational costs of different methods. Note that Demons and SyN are implemented using a CPU (i7-7700, 3.6GHz), while the remaining four methods are implemented using a GPU (GTX 1080ti). We can observe from Table 3 that the computational costs of the four deep-learningbased methods require only \u223c 0.25 second for deformable registration of one MRI, which is much faster than Demons (\u223c 1 minutes) and SyN (\u223c 2 hours). These results further demonstrate the potential utility of our method in practical applications. "}, {"section_title": "DISCUSSION", "text": "In this section, we first investigate the effect of two essential components (i.e., the inverse-consistent and anti-folding constraints) in the proposed ICNet method. We then analyze the influence of network architectures and a network refining strategy used in the application stage."}, {"section_title": "Influence of Inverse-consistent Constraint", "text": "To evaluate the influence of the proposed inverse-consistent constraint in Eq. 2, we visually illustrate the flows estimated by ICNet with different contributions from the proposed inverse-consistent constraint. Fig. 6 shows a pair of input images, as well as the flows and estimated inverse flows achieved by ICNet-1 and ICNet with different parameter settings. Here, we fix the parameter \u03b3 = 10 5 for the antifolding constraint, while the inverse flows are generated by linear interpolation via the proposed inverse network (see Validation losses achieved by ICNet-1 and ICNet using different weights for the smoothing and inverse-consistent constraints, regarding four terms (from left to right) in Eq. 7. Red line denotes the validation loss of ICNet-1 using a small weight for the smoothness constraint and without the inverse-consistent constraint (i.e., \u03b1 = 1, \u03b2 = 0, and \u03b3 = 10 5 ), green line represents the validation loss of ICNet-1 with a large weight for the smoothness constraint and without the inverse-consistent constraint (i.e., \u03b1 = 10, \u03b2 = 0, and \u03b3 = 10 5 ), and blue line denotes the validation loss of ICNet using a small weight for the smoothness constraint and the inverse-consistent constraint (i.e., \u03b1 = 1, \u03b2 = 0, and \u03b3 = 10 5 ). Fig. 6 (b) -(c) are generated by ICNet-1 without using the inverse-consistent constraint (i.e., \u03b2 = 0) but having different weights for the smoothness constraint, while those in Fig. 6 (d) are yielded by ICNet with the inverse-consistent constraint."}, {"section_title": "Fig. 2 (c)). Results in", "text": "From Fig. 6 (b) , we can observe that using a small weight (i.e., \u03b1 = 1) for the smoothness term in ICNet-1 cannot generate good results (with foldings in the estimated inverse flow), and also the flow between two images is not inverseconsistent. For instance, the estimated inverse flow F AB is not consistent with F AB , while F BA looks different from F BA . Fig. 6 (c) suggests that using a large weight (i.e., \u03b1 = 10) for the smoothness constraint in ICNet-1 will generate over smooth flow, which may degrade the registration accuracy. Fig. 6 (d) shows that ICNet (\u03b1 = 1, \u03b2 = 0.1) can generate flows with a reasonable smoothness degree. Also, it can be seen from Fig. 6 (d) that F AB is similar to F AB , and F BA also looks similar to F BA . This suggests that ICNet can well preserve the inverse-consistent property of Furthermore, we show the validation loss achieved by ICNet-1 and ICNet with different contributions from the smoothing and inverse-consistent constraints in Fig. 7 . For ICNet-1 with \u03b2 = 0, the inverse-consistent term in Eq. 7 is not used for network optimization and we only record the corresponding loss here. In Fig. 7 , red line denotes the validation loss of ICNet-1 with \u03b1 = 1, green line represents the loss of ICNet-1 with \u03b1 = 10 for the smoothness constraint, and blue line denotes the loss of ICNet with \u03b1 = 1 and \u03b2 = 0.1. As shown in the figure, using a large weight for the smoothing term (green lines) can have relatively small loss L smo , but the L inv is pretty large. It implies the warped source image is largely different from the target image. Besides, using a small weight for the smoothness regularizer (red lines) can yield relatively large loss L inv , but a good loss L sim concerning the similarity between the warped source image and the target image. In contrast, the losses of ICNet with \u03b1 = 1 and \u03b2 = 0.1 (blue lines) suggest that ICNet can not only produce inverse-consistent registration, but also keep the warped source image as similar as possible to the target image."}, {"section_title": "Influence of Anti-folding Constraint", "text": "We then study the influence of the proposed anti-folding constraint, by comparing ICNet with ICNet-2 (without using the anti-fold constraint). In this group of experiments, we fix the parameters for the smoothing and inverseconsistent constraints (i.e., \u03b1 = 1 and \u03b2 = 0.1). Fig. 8 shows the flows generated by ICNet-2 (left) and ICNet (right). It can be observed from Fig. 8 (a) that the flow generated by ICNet-2 without using the anti-folding regularizer includes many folding (see the black rectangle) that would degrade the registration accuracy. In contrast, Fig. 8 (b) suggests that ICNet using the anti-folding regularizer can effectively avoid the folding in the flow. These results demonstrate the effectiveness of the anti-folding constraint in preventing foldings in the learned transformation."}, {"section_title": "Influence of Network Architecture", "text": "We also investigate the influence of the network architecture on the performance of ICNet, where the number of starting filter channels in FCN (see n in Fig. 2 (b) ) is the essential component. In the above-mentioned experiments, we empirically set n = 8. In this group of experiments, we compare ICNet with its variant denoted as ICNet 16 with n = 16, and report the results of tissue segmentation achieved by these two methods in Fig. 9 . It can be seen from Fig. 9 that ICNet 16 achieves slightly better results in segmenting the three types of tissues, compared with ICNet. This implies that using more filter channels in the FCN within the proposed ICNet framework helps boost the registration accuracy, thus improving the tissue segmentation performance. Note that ICNet 16 requires much large memory (\u223c 20GB) for network training, while the ICNet with 8 starting filter channels only need \u223c 10GB memory. Also, the training time of ICNet 16 will double that of ICNet. Considering the marginal improvement of the registration performance shown in Fig. 9 , we can flexibly choose the number of starting filter channels in practice, based on the memory capacity at hand."}, {"section_title": "Influence of Network Refining Strategy", "text": "The proposed ICNet is unsupervised, without using any ground-truth registration results. Therefore, given a pair of new testing images, we can feed them to ICNet (trained on the training data) to further refine the network, thus adapting the network to the testing images. Here, we denote ICNet with such a network refining process as ICNet R. In ICNet R, we first optimize the network parameters using the training data, and then refine the network (with the learned parameters as initialization) for the new pair of testing images. In the refining stage, we use a small learning rate (i.e., 1 \u00d7 10 \u22125 ) for optimization, and the number of iteration is empirically set to 100. After refinement, we can use the newly learned network parameters to produce the final registration results for the testing images. The experimental results on tissue segmentation achieved by ICNet and its refined variant (i.e., ICNet R) are shown in Fig. 10 .\nIt can be seen from Fig. 10 that ICNet R consistently outperforms ICNet in segmenting three types of tissues, regarding five evaluation metrics. For instance, the PPV value of ICNet R in segmenting CSF is 0.87, while that of ICNet is only 0.84. The possible reason could be that the refining strategy makes the network to be better coordinated with the new input images, thus reducing the negative influence of distribution differences between training and test data. It is worth noting that such network refining strategy is a general approach, which can also be applied to improving other unsupervised algorithms for image registration."}, {"section_title": "CONCLUSION AND FUTURE WORK", "text": "In this paper, we propose an inverse-consistent deep network (ICNet) for unsupervised deformable image registration. Specifically, we develop an inverse-consistent constraint to encourage that a pair of images are symmetrically deformed toward one another, and then propose an antifolding constraint to minimize foldings in the estimated flow. The proposed method is evaluated on registration of T1-weighted brain MR images for tissue segmentation and anatomical landmark detection. Experimental results demonstrate that our ICNet outperforms several state-ofthe-art algorithms for deformable image registration. In the current work, we utilize the mean squared distance (MSD) to measure the similarity between a warped source image and a target image, while many other similarity measures (such as correlation or mutual information) can be employed in the proposed deep-learning framework. Besides, only a simple network refining strategy is proposed in this work to handle the challenge of data heterogeneity (e.g., in image contrast, resolution, and noise level), while more advanced data adaptation algorithms [71] , [72] could be used to further boost the registration performance."}, {"section_title": "APPENDIX", "text": "As mentioned in the main text, besides using the conventional smoothness constraint [16] , we also develop an anti-folding constraint to further prevent the folding in the learned discrete displacement field (i.e., flow). Note that each element in a flow is a 3-dimensional vector (corresponding to three axes, i.e., x, y, z), indicating the displacement of a particular voxel from its original location to a new location. We now explain the details of this anti-folding constraint defined in Eq. 5 in the main text.\nWe denote the to-be-estimated flow from an image A to an image B as F AB , and F AB is defined in the A space. For simplicity, we denote the 1-dimensional displacement in F AB along the i-th axis as F i AB . As shown in Fig. 11 , for the point m and its nearest neighbor point m + 1 along the i-th axis in the space of A, we denote their displacements as To avoid folding at the location of m, it is required that the new locations of these two points (after displacement via Combining Eq. 9 to Eq. 8, we can get the following "}]