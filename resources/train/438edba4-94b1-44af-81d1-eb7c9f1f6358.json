[{"section_title": "Abstract", "text": "Positron emission tomography (PET) imaging is an imaging modality for diagnosing a number of neurological diseases. In contrast to Magnetic Resonance Imaging (MRI), PET is costly and involves injecting a radioactive substance into the patient. Motivated by developments in modality transfer in vision, we study the generation of certain types of PET images from MRI data. We derive new flow-based generative models which we show perform well in this small sample size regime (much smaller than dataset sizes available in standard vision tasks). Our formulation, DUAL-GLOW, is based on two invertible networks and a relation network that maps the latent spaces to each other. We discuss how given the prior distribution, learning the conditional distribution of PET given the MRI image reduces to obtaining the conditional distribution between the two latent codes w.r.t. the two image types. We also extend our framework to leverage \"side\" information (or attributes) when available. By controlling the PET generation through \"conditioning\" on age, our model is also able to capture brain FDG-PET (hypometabolism) changes, as a function of age. We present experiments on the Alzheimers Disease Neuroimaging Initiative (ADNI) dataset with 826 subjects, and obtain good performance in PET image synthesis, qualitatively and quantitatively better than recent works."}, {"section_title": "Introduction", "text": "Positron Emission Tomography (PET) images provide a three-dimensional image volume reflecting metabolic activity in the tissues, e.g., brain regions, which is a key imaging modality for a number of diseases (e.g., Dementia, Epilepsy, Head and Neck Cancer). Compared with Magnetic Resonance (MR) imaging, the typical PET imaging procedure usually involves radiotracer injection and a high cost associated with specialized hardware and tools, logistics, and expertise. Due to these factors, Magnetic Resonance (MR) imaging is much more ubiquitous than PET imaging in both clinical and research settings. Clinically, PET imaging is often only considered much further down the pipeline, after information from other non-invasive approaches has been collected. It is not uncommon for many research studies to include MR images for all subjects, and acquire specialized PET images only for a smaller subset of participants.\nOther use cases. Leaving aside the issue of disparity in costs between MR and PET, it is not uncommon to find that due to a variety of reasons other than cost, a (small or large) subset of individuals in a study have one or more image scans unavailable. Finding ways to \"generate\" one type of imaging modality given another is attracting a fair bit of interest in the community and a number of ideas have been presented [34] . Such a strategy, if effective, can increase the sample sizes available for statistical analysis and possibly, even for training downstream learning models for diagnosis.\nRelated Work. Modality transfer can be thought of \"style transfer\" [6, 11, 15, 16, 19, 22, 24, 25, 27, 30, 31, 42, 43, 49, 50] in the context of medical images and a number of interesting results in this area have appeared [13, 17, 23, 28, 32, 34, 44] . Existing methods, mostly based on deep learning for modality transfer, can be roughly divided into two categories: Auto-encoders and Generative Adversarial Networks (GANs) [3, 12, 18] . Recall that autoencoders are composed of two modules, encoder and decoder. The encoder maps the input to a hidden code h, and the decoder maps the hidden code to the output. The model Figure 1 : The DUAL-GLOW framework. For the conditional module, the dashed and dotted pieces are added and removed respectively. The colored circle represents the latent code whereas the gray one is the image or the intermediate output.\nis trained by minimizing the loss in the output Euclidean space with standard norms ( 1 , 2 ). A U-Net structure, introduced in [36] , is typically used for leveraging local and hierarchical information to achieve an accurate reconstruction. Although the structure in auto-encoders is elegant with reasonable efficiency and a number of authors have reported good performance [32, 37] , constructions based on minimizing the 2 loss often produce blurry outputs, as has been observed in [34] . Partly due to these reasons, more recent works have investigated other generative models. Recently, one of the prominent generative models in use today, GANs [12] , has seen much success in natural image synthesis [3] , estimating the generative model via an adversarial process. Despite their success in generating sharp realistic images, GANs usually suffer from \"mode collapse\", that tends to produce limited sample variety [1, 4] . This issue is only compounded in medical images, where the maximal mode may simply be attributed to anatomical structure shared by most subjects. Further, sample sizes are often much smaller in medical imaging compared to computer vision, which necessitates additional adjustments to the architecture and parameters, as we found in our experiments as well.\nFlow-based generative models. Another family of methods, flow-based generative models [7, 8, 21] , has been proposed for variational inference and natural image generation and have only recently begun to gain attention in the computer vision community. A (normalizing) flow, proposed in [35] , uses a sequence of invertible mappings to build the transformation of a probability density to approximate a posterior distribution. The flow starts with an initial variable and maps it to a variable with a simple distribution (e.g., isotropic Gaussian) by repeatedly applying the change of variable rule, similar to the inference procedure in an encoder network. For the image generation task, the initial variable is the real image with some unknown probability function. Designating a well-designed inference network, the flow will learn an accurate mapping after training. Because the flow-based model is invertible, the generation of synthetic images is straightforward by sampling from the simple distribution and \"flowing\" through the map in reverse. Compared with other generative models and Autoregressive Models [33] , flow-based methods allow tractable and accurate log-likelihood evaluation during the training process, while also providing an efficient and exact sampling from the simple prior distribution at test time.\nWhere is the gap? While flow-based generative models have been successful in image synthesis, it is challenging to leverage them directly for modality transfer. It is difficult to apply existing flow-based methods to our task due to the invertibility constraint in the inference network. Apart from various technical issues, consider an intuitive example. Given an MRI, we should expect that there would be many solutions of corresponding PET images, and vice versa. Ideally, we prefer the model to provide a conditional distribution of the PET given an MRI -such a conditional distribution can also be meaningfully used when additional information about the subject is available.\nThis work. Motivated by the above considerations, we propose a novel flow-based generative model, DUAL-GLOW, for MRI-to-PET image generation. The value of our model includes explicit latent variable representations, exact and efficient latent-variable inference, and the potential for memory and computation savings through constant network size. Utilizing recent developments in flow-based generative models by [21] , DUAL-GLOW is composed of two invertible inference networks and a relation CNN network, as pictured in Figure 1 . We adopt the multi-scale architecture with spliting technique in [8] , which can significantly reduce the computational cost and memory. The two inference networks are built to project MRI and PET into two semantically meaningful latent spaces, respectively. The relation network is constructed to estimate the conditional distribution between paired latent codes. The foregoing properties of the DUAL-GLOW framework enable specific improvements in modality transfer from MRI to PET images. Sampling efficiency allows us to process and generate full 3D brain volumes.\nConditioning based on additional information. While the direct generation of PET from MRI has much practical utility, it is often also the case that a single MRI could correspond to a very different PET image -and which images are far more likely can be resolved based on additional information, such as age or disease status. However, a challenge arises due to the high correlation between the input MR image and side information: traditional conditional frameworks [21, 29] cannot effectively generate meaningful images in this setting. To accurately account for this correlation, we propose a new conditional framework, see Figure  1 , where two small discriminators (Multiple Layer Perceptron, MLP) are concatenated at the end of the top inference networks to faithfully extract the side information contained in the images. The remaining two discriminators concatenated at the left invertible inference network are combined with Gradient Reverse Layers (GRL), proposed in [10] , to exclude the side information which exists in the latent codes except at the top-most layer. After training, sampling from the conditional distribution allows the generation of diverse and meaningful PET images. Extensive experiments show the efficiency of this exclusion architecture in the conditional framework for side information manipulation.\nContributions. This paper provides: (1) A novel flow-based generative model for modality transfer, DUAL-GLOW. (2) A complete end-to-end PET image generation from MRI for full three-dimensional volumes. (3) A simple extension that enables side condition manipulation -a practically useful property that allows assessing change as a function of age, disease status, or other covariates. (4) Extensive experimental analysis of the quality of PET images generated by DUAL-GLOW, indicating the potential for direct application in practice to help in the clinical evaluation of Alzheimer's disease (AD)."}, {"section_title": "Flow-based Generative Models", "text": "We first briefly review flow-based generative models to help motivate and present our algorithm. Flow based generative models, e.g., GLOW [21] , typically deal with single image generation. At a high level, these approaches set up the task as calculating the log-likelihood of an input image with an unknown distribution. Because maximizing this log-likelihood is intractable, a flow is set up to project the data into a new space where it is easy to compute, as summarized below.\nLet x be an image represented as a high-dimensional random vector in the image space with an unknown true dis-\nand choose a model class p \u03b8 (x) with parameters \u03b8. Our goal is to find parameters\u03b8 that produces p\u03b8(x) to best approximate p * (x). This is achieved through maximization of the log-likelihood:\nIn typical flow-based generative models [7, 8, 21] , the generative process for x is defined in the following way:\nwhere z is the latent variable and p \u03b8 (z) has a (typically simple) tractable density, such as a spherical multivariate Gaussian distribution: p \u03b8 (z) = N (z; 0, I). The function g \u03b8 (\u00b7) may correspond to a rich function class, but is invertible such that given a sample x, latent-variable inference is done by z = f \u03b8 (x) = g \u03b8 \u22121 (x). For brevity, we will omit subscript \u03b8 from f \u03b8 and g \u03b8 .\nWe focus on functions where f is composed of a sequence of invertible transformations:\nwhere the relationship between x and z can be written as:\nSuch a sequence of invertible transformations is also called a (normalizing) flow [35] . Under the change of variables rule through (2), the log probability density function of the model (1) given a sample x can be written as:\nwhere we define h 0 = x and h k = z for conciseness. The scalar value log |det(dh i /dh i\u22121 )| is the logarithm of the absolute value of the determinant of the Jacobian matrix (dh i /dh i\u22121 ), also called the log-determinant. While it may look difficult, this value can be simple to compute for certain choices of transformations, as previous explored in [7] . For the transformations {f i } k i=1 which characterizes the flow, there are several typical settings that result in invertible functions, including actnorms, invertible 1 \u00d7 1 convolutions, and affine coupling layers [21] . Here we use affine coupling layers, discussed in further detail shortly. For more, details regarding these mappings we refer the reader to existing literature on flow-based models, including GLOW [21] ."}, {"section_title": "Deriving DUAL-GLOW", "text": "In this section, we present our DUAL-GLOW framework for inter-modality transfer. We first discuss the derivation of the conditional distribution of a PET image given an MR image and then provide strategies for efficient calculation of its log-likelihood. Then, we introduce the construction of the invertible flow and show the calculation for the Jacobian matrix. Next, we build the hierarchical architecture for our DUAL-GLOW framework, which greatly reduces the computational cost compared to a flat structure. Finally, the conditional structure for side information manipulation is derived with additional discriminators. Log-Likelihood of the conditional Distribution. Let the data corresponding to the MR and PET images be denoted\n, we are interested in generating images which have the same properties as images in the dataset\n. In our DUAL-GLOW model, we assume that there exists a flowbased invertible function f p which maps the PET image x p to z p = f p (x p ) and a flow-based invertible function f m which maps the MR image x m to z m = f m (x m ). The latent variables z p and z m help set up a conditional probability p \u03b8 (z p |z m ), given by\nThe full mapping composed of f p , f m , \u00b5 \u03b8 and \u03c3 \u03b8 formulates our DUAL-GLOW framework:\nsee Figure 2 . The invertible functions f p and f m are designed as flow-based invertible functions. The mean function \u00b5 \u03b8 and the covariance function \u03c3 \u03b8 for p \u03b8 (z p |z m ) are assumed to be specified by neural networks. In this generating process, our goal is to maximize the log conditional probability p \u03b8 (x p |x m ). By the change of variable rule, we have that\nNote that the Jacobian\nRecall that calculating the deteriminant of such a matrix is straightforward (see [38] ), which leads directly to (10) . Without any regularization, maximizing such a conditional probability can make the optimization hard. Therefore, we may add a regularizer by controlling the marginal distribution p \u03b8 (z m ), which leads to our objective function\nwhere \u03bb is a hyperparameter, p \u03b8 (z m ) = N (z m ; 0, I) and\nInterestingly, compared to GLOW, our model does not introduce much additional complexity in computation. Let us see why. First, the marginal distribution p \u03b8 (z) in GLOW is replaced by p \u03b8 (z p |z m ) and p \u03b8 (z m ), which still has a simple and tractable density. Second, instead of one flow-based invertible function in GLOW, our DUAL-GLOW has two flow-based invertible functions f p , f m . Those functions are setup in parallel based on (12) , extending the model size by a constant factor. Flow-based Invertible Functions. In our work, we use an affine coupling layer to design the flows for the invertible functions f p and f m . Before proceeding to the details, we omit subscripts p and m to simplify notations in this subsection. The invertible function f is composed of a sequence of\nare designed by using the affine coupling layer [8] following these equations:\nwhere denotes element-wise multiplication, h i \u2208 R d1+d2 , h i;1:d1 the first d 1 dimensions of h i , and h i;d1+1:d1+d2 the remaining d 2 dimensions of h i . The functions s(\u00b7) and t(\u00b7) are nonlinear transformations where it makes sense to use deep convolutional neural networks (DCNNs). This construction makes the function f invertible. To see this, we can easily write the inverse function \nIn addition to invertibility, this structure also tells us that the log(|det(dz p /dx p )|) term in our objective (12) has a simple and tractable form. Computing the Jacobian, we have:\nwhere I 1:d1 \u2208 R d1\u00d7d1 is an identity matrix. Therefore,\nwhich can be computed easily and efficiently, requiring no on-the-fly matrix inversions [21] .\nEfficiency from Hierarchical Structure. The flow f = f k \u2022 \u00b7 \u00b7 \u00b7 f 2 \u2022 f 1 can be viewed as a hierarchical structure. For the two datasets\n, it is computationally expensive to make all features of all samples go through the entire flow. Following implementation strategies in previous flow-based models, we use the splitting technique to speed up DUAL-GLOW in practice, see Figure 3 . When a sample x reaches the i-th transformation f i in the flow as h i\u22121 , we split h i\u22121 in two parts h i\u22121,1\ni=1 and the top-most h k are concatenated together to form z. By using this splitting technique in the flow hierarchy, the part leaving the flow \"early\" goes through fewer transformations. As discussed in GLOW and previous flow-based models, each transformation f i is usually rich enough that splitting saves computation without losing much quality in practice. We provide the computational complexity in the appendix. Additionally, this hierarchical representation enables a more succinct extension to allow side information manipulation.\nHow to condition based on side information? As stated above, additional covariates should influence the PET image we generate, even with a very similar MRI. A key assumption in many conditional side information frameworks is that these two inputs (the input MR and the covariate) are independent of each other. Clearly, however, there exists a high correlation between MRI and side information such as age or gender or disease status. In order to effectively incorporate this into our model, it is necessary to disentangle the side information from the intrinsic properties encoded in the latent representation z m of the MR image.\nLet c denote the side information, typically a high-level semantic label (age, sex, disease status, genotype). In this case, we expect that the effect of this side information would be at a high level in relation to individual image voxels. As such, we expect that only the highest level of DUAL-GLOW should be affected by this. The latent variables z p should be conditioned on side variable c and z m = {h i,2 } k i=1 except h k . Thus, we can rewrite the con-ditional probability in (12) by adding c:\nis independent on c, and\nTo disentangle the latent representation z m and exclude the side information in z m , we leverage the well-designed conditional framework composed of both flow and discriminators. Specifically, the condition framework tries to exclude the side information from {h i,2 } k\u22121 i=1 and keep it in h k at the top level during training time. To achieve this, we concatenate a simple discriminator for each {f i } k\u22121 i=1 and add a Gradient Reversal Layer (GRL), introduced in [10] , at the beginning of the network. These classifiers are used for distinguishing the side information in a supervised way. The GRL acts as the identity function during the forwardpropagation and reverses the gradient in back-propagation. Therefore, minimizing the classification loss in these classifiers is equivalent to pushing the model to exclude the information gained by this side information, leading to the exclusive representation z m = {h i,2 } k\u22121 i=1 . We also add a classifier without GRL at the top level of f m , f p that explicitly preserves this side information at the highest level.\nFinally, the objective is the log-likelihood loss in (16) plus the classification losses, which can be jointly optimized by the popular optimizer AdaMax [20] . The gradient is calculated in a memory efficient way inspired by [5] . After training the conditional framework, we achieve PET image generation influenced both by MRI and side information."}, {"section_title": "Experiments", "text": "We evaluate the model's efficacy on the ADNI dataset both against ground truth images and for downstream applications. We conduct extensive quantitative experiments which show that DUAL-GLOW outperforms the baseline method consistently. Our generated PET images show desirable clinically meaningful properties which is relevant for their potential use in Alzheimer's Disease diagnosis. The conditional framework also shows promise in tracking hypometabolism as a function of age."}, {"section_title": "ADNI Dataset", "text": "Data. The Alzheimer's Disease Neuroimaging Initiative (ADNI) provides a large database of studies directly aimed at understanding the development and pathology of Alzheimer's Disease. Subjects are diagnosed as cognitively normal (CN), significant memory concern (SMC), early mild cognitive impairment (EMCI), mild cognitive impairment (MCI), late mild cognitive impairment (LMCI) or having Alzheimer's Disease (AD). FDG-PET and T1-weighted MRIs were obtained from ADNI, and pairs were constructed by matching images with the same subject ID and similar acquisition dates.\nPreprocessing. Images were processed using SPM12 [2] . First, PET images were aligned to the paired MRI using coregistration. Next, MR images were nonlinearly mapped to the MNI152 template. Finally, PET images were mapped to the standard MNI space using the same forward warping identified in the MR segmentation step. Voxel size was fixed for all volumes to 1.5\u00d71.5\u00d71.5mm 3 , and the final volume size obtained for both MR and PET images was 64 \u00d7 96 \u00d7 64. Through this workflow, we finally obtain 806 MRI/PET clean pairs. The demographics of the dataset are provided in the appendix. In the following experiments, we randomly select 726 subjects as the training data and the remaining 80 as testing within a 10-fold evaluation scheme.\nFramework Details. The DUAL-GLOW architecture outlined above was trained using Nvidia V100 GPUs with Tensorflow. There are 4 \"levels\" in our invertible network, each containing 16 affine coupling layers. The nonlinear operators s(\u00b7) and t(\u00b7) are small networks with three 3D convolutional layers. For the hierarchical correction learning network, we split the hidden codes of the output of the first three modules in the invertible network and design four 3D convolutional networks for all latent codes. For the conditional framework case, we concatenate the five discriminators to the tail of all four levels of the MRI inference network and the top-most level of the PET inference network. The GRL is added between the inference network and the first three discriminators. The hyperparameter \u03bb is the regularizer and set to 0.001. For all classification losses, we set the weight to 0.01. The model was trained using the AdamMax optimizer with an initial learning rate set to 0.001 and exponential decay rates 0.9 for the moment estimates. We train the model for 90 epochs. Our implementation is available at https://github.com/haolsun/ dual-glow. "}, {"section_title": "Generated versus Ground Truth consistency", "text": "We begin our model evaluation by comparing outputs from our model to 4 state-of-the-art methods used previously for similar image generation tasks, conditional GANs (cGANs) [32] , cGANs with U-Net architecture (UcGAN) [36] , Conditional VAE (C-VAE) [9, 39] , pix2pix [16] . Additional experimental setup details are in the appendix. We compare using commonly-used quantitative measures computed over the held out testing data. These include Mean Absolute Error (MAE), Correlation Coefficients (CorCoef), Peak Signal-to-Noise Ratio (PSNR), and Structure Similarity Index (SSIM). For Cor Coef, PSNR and SSIM, higher values indicate better generation of PET images. For MAE, the lower the value, the better is the generation. As seen in Table 1 and Figure 4 , our model competes favorably against other methods. Figure 9 shows test images generated after 90 epochs for Cognitively Normal and Alzheimer's Disease individuals. Qualitatively, not only is the model able to accurately reconstruct large scale anatomical structures but it is also able to identify minute, sharp boundaries between gray matter and white matter. While here we focus on data from individuals with a clear progression of Alzheimer's disease from those who are clearly cognitively healthy, in preclinical cohorts where disease signal may be weak, accurately constructing finer-grained details may be critical in identifying those who may be undergoing neurodegeneration due to dementia. More results are shown in the appendix."}, {"section_title": "Scientific Evaluation of Generation", "text": "As we saw above, our method is able to learn the modality mapping from MRI to PET. However, often image acquisition is used as a means to an end: typically towards disease diagnosis or informed preventative care. While the generated images may seem computationally and visually coherent, it is important that the images generated add some value towards these downstream analyses.\nWe also evaluate the generated PET images for disease prediction and classification. Using the AAL atlas, we obtain all 116 ROIs via atlas-based segmentation [45] and use the mean intensity of each as image features. A support vector machine (SVM) is trained with the standard RBF kernel (e.g., see [14] ) to predict binary disease status (Normal, EMCI, SMC vs. MCI, LMCI, AD) for both the ground truth and the generated images. The SVM trained on generated images achieves comparable accuracy and false positive/negative rates ( Table 2 ), suggesting that the generated images contain sufficient discriminative signal for disease diagnosis. Adjusting for Age with Conditioning. The conditional framework naturally allows us to evaluate potential developing pathology as an individual ages. Training the full conditional DUAL-GLOW model, we use ground truth \"side\" information (age) as the conditioning variable described above. Figure 13 shows the continuous change in the 3 generated images given various age labels for the same MRI. The original image (left) is at age 50, and as we increase age from 60 to 100, increased hypometabolism becomes clear. To quantitatively evaluate our conditional framework, we plot the mean intensity value of a few key ROIs. As we see in Figure 7 , the mean intensity values show a downward trend with age, as expected. While there is a clear 'shift' between AD, MCI, and CN subjects (blue dots lie above red dots, etc.), the wide variance bands indicate a larger sample size may be necessary to derive statistically sound conclusions (e.g., regarding group differences). Additional results and details can be found in the appendix."}, {"section_title": "Other Potential Applications", "text": "While not a key focus of our work, to show the model's generality on visually familiar images we directly test DUAL-GLOW's ability to generate images on a standard computer vision modality transfer task. Using the UTZap50K dataset [47, 48] of shoe images, we construct HED [46] edge images as \"sketches\", similar to [16] . We aim to learn a mapping from sketch to shoe. We also create a cartoon face dataset based on CelebA [26] and train our model to generate a realistic image from the cartoon face. Fig. 8 shows the results of applying our model (and ground truth). Clearly, more specialized networks designed for such a task will yield more striking results, but these experiments suggest that the framework is general and applicable in additional settings. These results are available on the project homepage and in the appendix."}, {"section_title": "Conclusions", "text": "We propose a flow-based generative model, DUAL-GLOW, for inter-modality transformation in medical imaging. The model allows for end-to-end PET image generation from MRI for full three-dimensional volumes, and takes advantage of explicitly characterizing the conditional distribution of one modality given the other. While inter-modality transfer has been reported using GANs, we present improved results along with the ability to condition the output easily. Applied to the ADNI dataset, we are able to generate sharp synthetic PET images that are scientifically meaningful. Standard correlation and classification analysis demonstrates the potential of generated PET in diagnosing Alzheimer's Disease, and the conditional side information framework is promising for assessing the change of spatial metabolism with age."}, {"section_title": "Appendix", "text": "In this supplement, We provide extensive additional details regarding the neuroimaging experiments and some theoretical analysis."}, {"section_title": "The ADNI dataset", "text": "Data used in the experiments for this work were obtained directly from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/ wp-content/uploads/how_to_apply/ADNI_ Acknowledgement_List.pdf.\nThe ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biologicalmarkers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimers disease (AD).For up-to-date information, see www.adni-info.org.\nPre-processing details are described in the main paper. MR images were segmented such that the skull and other bone matter were masked, leaving only grey matter, white matter, and cerebrospinal fluid (CSF). After pre-processing, we obtain 806 clean MRI/PET pairs. The demographics of the final dataset are shown in Table 3 . As voxel size was fixed for all volumes to 1.5mm 3 , processed images were of dimension 121 \u00d7 145 \u00d7 121. Images were cropped and downsampled slightly after skull extraction to allow for faster training."}, {"section_title": "Architecture Details", "text": "ADNI Brain Imaging Experiments. There are 4 \"levels\" in our two invertible networks, each \"level\" containing 16 affine coupling layers. The small network with three 3D convolutional layers is shared by two nonlinear operators s(\u00b7) and t(\u00b7). There are 512 channels in the intermediate layers. For the hierarchical correction learning network, we split the hidden codes of the output of the first three modules in the invertible network and design four 3D CNNs (relation networks) with 1 convolutional layer for all latent codes. For the conditional framework case, we concatenate the five discriminators with the adaptive number of layers to the tail of all four levels of the MRI inference network and the top-most level of the PET inference network.\nFor other compasion methods, we have 13 resBlocks of 34 convs with the U-net architecture for C-VAE, 12 convs in G and 8 resBlocks in D for the cGAN, 15 convs in G and 8 resBlocks in D for UcGAN. Flow-based methods for image translation do not currently exist. But we implemented an iterative Glow (iGlow: 4 levels, 4 \u00d7 10 coupling layers), concatenating paired MRI and PET as input. After training, we fix networks and set input PET as trainable variables and obtain PET by iteratively optimizing the log-likelihood w.r.t. these variables. But the unstable gradient ascent gives bad results.\nNatural Image Experiments. For natural image experiments, the settings of the hypersparameters are shown in Table 4. The depth is equal to the number of coupling layers in each \"level\". Since the resolution of images in UT-Zap50K is 128 \u00d7 128, we use the \"level\" of 5. For the celebA dataset (resolution: 256 \u00d7 256), the number of the \"level\" is 6. The relation network is a CNNs with 8 conv layers in both two experiments."}, {"section_title": "Generated Samples", "text": "Brain Imaging Experiments. The images that follow are additional representative samples from our framework. Figures 9, 10, 11 show ground truth and reconstructions of cognitively healthy, mildly cognitively impaired, and Alzheimer's diseased patients within our test group. Figure 12 shows the comparison visualization results, which shows that DUAL-GLOW outperforms other methods in most regions. Figure 13 shows additional age conditioning results, again for each of the three disease groups (CN, MCI, and AD). We also plot the mean intensity of other 30 ROIs of 3 subjects given 6 age labels (from 50 to 100) in Fig 14, which shows a clear decreasing trend, i.e., decreased metabolism with aging.\nOther Experiments. While not the focus of our work, we show some additional results on two standard imaging datasets: the cartoon-to-face translation in Figure 15 ; the sketch-to-shoe snynthesis in Figure 16 . The input of the CelebA face dataset is the cartoon image processed by using the technique in opencv. For UT-Zap50K shoe dataset, we extract edge images as sketches by using HED (Holisticallynested edge detection) and learn a mapping from sketch to shoe."}, {"section_title": "Theoretical Analysis", "text": "Hierarchical architecture. In Fig 3/ paper, half of the feature map is used as input to the next level. The computational complexity is mainly dependent on the input of each coupling layer. Let denote the number of levels. Supposing the input size is 2 and the time complexity for each level is O(N ), we have the time complexity of O(2 N ) for the flat architecture and O((2 +1 \u22122)N ) for the hierarchical one. A larger leads to further reduction.\nThe choice of \u03bb. \u03bb regularizes networks for MRI. Our (Fig 17) .\nGT 0.001 1.0 There are 3 subjects, AD, MCI, and CN, each subject provides 6 results w.r.t. a variant of age labels. As we scan left to right, we indeed see a decrease trend in metabolism (less red, more yellow) which is completely consistent with what we would expect in aging. Figure 15 : Input cartoon images and generated/reconstructed faces applying our DUAL-GLOW framework to the CelebA dataset. Figure 16 : Input \"sketch\" images and generated/reconstructed shoes applying our DUAL-GLOW framework to the UTZap50K dataset."}]