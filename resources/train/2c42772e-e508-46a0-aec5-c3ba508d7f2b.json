[{"section_title": "T", "text": "here is increasing evidence that modest financial and informational interventions can affect important educational decisions (e.g., Bettinger et al. 2012; Hoxby and Turner 2013; and Pallais 2015) . Inducing a student to take a college admissions exam may be a particularly desirable nudge in this context. 1 The exam is taken early in the application process, is required for admission to most four-year colleges, and provides a personalized measure of academic potential that may induce the most promising students to matriculate. 2 Further, approximately half of all high school students do not take a college admissions exam and there is less access to the exams at schools that serve families of low socioeconomic status. I test if college outcomes are affected by access using two policy relevant sources of variation: the opening and closing of test center locations, and the introduction of district policies that offer free in-school testing and default registration. The long-run college outcomes of 1 Historically, policies to promote college matriculation have focused on reducing tuition or easing financial constraints. In 2010, the federal government provided approximately $36 billion in Pell Grants, $85 billion in Direct and Perkins loans, and $15 billion to tax payers through the American Opportunity and Lifetime Learning Credits. The magnitudes of the effects of such programs have been evaluated by, for example, McPherson and Schapiro (1991) ; Dynarski (2002) ; Cornwell, Mustard, and Sridhar (2006) ; Abraham and Clark (2006); and Kane (2007) .\n2 Admissions considerations reported in the National Center for Education Statistics Integrated Postsecondary Education Data System indicate that the SAT or ACT is recommended or required for admission to more than 85 percent of 4-year colleges.\nexplain the difference in outcomes for always-takers, marginal takers, and inframarginal takers. These findings are helpful for understanding the implications of targeting and scaling policy.\nThe analysis reveals sharp, within-school increases in SAT taking and college outcomes across cohorts in the exact year a policy is introduced or a center opens or closes. These results are biased if changes in test center access and policy adoption are correlated with changes in students' propensities to attend college or confounding policies that affect college enrollment. For example, if a strong (weak) cohort of students causes the school to open (close) a center or causes a district to adopt an opt-out policy, then the results will favor finding a positive effect. Several factors suggest that such bias may not be significant. The change in the demographic composition of cohorts before and after treatment is consistent with state and national trends. Falsification tests reveal no evidence of a change in average PSAT or AP scores for students at affected schools, suggesting that changes in student aptitude or school quality are not driving the results. In contrast, the change in college outcomes occurs suddenly in the year of the change in access, and is persistent (i.e., the estimated change cannot be attributed to a single outlier cohort). Centers that open or close also appear to have an effect on students at neighboring schools-schools that are unlikely to have adopted concurrent policies. Likewise, the size of the response to a center is correlated with the distance to an alternative center. Importantly, the estimated distribution of scores for new takers suggests that some college-caliber students are induced to take the exam. 6 District policies increase the fraction of students taking the SAT by about 73 percent (31 percentage points). New testing centers also generate a response, with an average increase in taking of 8.5 percent (4 percentage points) for students attending the host school and 2.6 percent (1.4 percentage points) for students attending a school in close proximity. Center closures are associated with a decrease in SAT participation of 4.9 percent. 7 These responses suggest that a modest nudge can cause students to take the first step in the college matriculation process. Examining college outcomes reveals the probability with which new takers progress to each stage of college enrollment and completion. These patterns are compared to those of always-takers from the same high schools and, with caution, across policies. I estimate that a significant fraction, approximately 40 percent, of students induced by a center opening subsequently attend a 4-year college. A smaller fraction, about 10 percent, of students compelled to take the exam by an opt-out district policy go on to attend a 4-year college. The estimates suggest that opening testing centers would increase the rate of 4-year college enrollment by about 4 percent (1.6 percentage points) at each school without a center. Implementing free in-school administration and opt-out registration is estimated to increase 4-year matriculation by about 3 percentage points. This is significant in light of the fact that fewer than half of schools host a center, with lower rates of hosting at schools that serve low-income communities.\nInterestingly, while the rate of matriculation is significantly higher among always-takers, there is no evidence that new takers complete fewer semesters of college or are less likely to graduate than always-takers conditional on attending. To examine why the rate of college attendance differs between always-takers and new takers but the rate of persistence in college does not, I differentiate the effects by student aptitude. To do this, I exploit the fact that some schools require all students to take the PSAT to construct a proxy for the aptitude of all students in the sample. This approach yields some interesting insights. First, college matriculation for new takers is highly correlated with aptitude. The positive selection into college is even more pronounced than for always-takers, which may explain why new takers are not more likely to drop out. Second, new takers are less likely to attend college than always-takers of similar aptitude, revealing that other important factors deter college attendance. Nonetheless, under conservative assumptions about the returns to college, back-of-the-envelope calculations suggest that the option value of taking the SAT for marginal takers from the bottom, middle, and top terciles of the ability distribution is about $50,000, $125,000, and $200,000, respectively. Taken together, these findings suggest that there may be high returns to targeting policies to those students with the highest aptitude and to identifying additional barriers to college enrollment.\nThe finding in this paper that students are highly sensitive to SAT access is consistent with growing evidence of the potential of nudges in economics. 8 Most recently, Pallais (2015) exploits a policy change in the number of free ACT score reports that exam takers can send to a college and finds that 20 percent send an additional report as a result. Though she does not measure takers' college outcomes, she projects that the expanded choice set could significantly change the composition of colleges attended. The finding in this paper that students' college decisions change after the SAT requirement is satisfied and college options are revealed is consistent with the results of Bettinger et al. (2012) . In that study, college attendance was 8 percentage points higher when H&R Block filed federal aid forms on the student's behalf and provided information about tuition at local public universities. It is important to note that these percentages apply to lower income tax filers who expressed interest in participating in a college enrollment study and thus may not be comparable to the estimates presented in this paper. Several other studies have found that college attendance decisions are influenced by the quality of information available. For example, Cunha, Miller, and Weisburst (2009); Avery (2010) ; Carrell and Sacerdote (2012) ; and Hoxby and Turner (2013) generally find positive effects of initiatives that provide information about college to high school students through resource materials or counseling services. 9 The observed heterogeneity in response to taking the SAT by aptitude has a natural analogue in studies by Manski (1989) , Altonji (1993) , Arcidiacono (2004 Arcidiacono ( , 2005 , and Stinebrickner (2008, 2012) , who find that students update their perceived college options after they matriculate and their ability is revealed.\nResearch examining the effect of state-level admissions exam policies is relevant to the opt-out district policy component of this study. Goodman (2012) and Klasik (2013) both examine ACT policies in Colorado and Illinois using matched control states and aggregate enrollment counts at colleges in these states. Klasik finds effects ranging from \u22127 to 18 percentage points, while Goodman (2012) finds a statistically insignificant increase of 5 percent but a shift toward selective colleges. The wide range of estimates in these studies of the same states is likely due to the aggregate nature of the data used and the assumption that matched states accurately capture year-to-year variation in college enrollment. Researchers from the College Board (Hurwitz et al. 2015) examine an SAT policy in Maine using matched control states and find an increase in 4-year college enrollment of 2 to 5 percentage points. However, students in the first treated cohort in Maine were also affected by several concurrent initiatives. 10 In a subsequent study, Hyman (2014) uses a single-difference design to evaluate an ACT policy in Michigan and finds a 1.9 percentage point increase in 4-year college enrollment. 11 The estimates found by Goodman (2012) and Hurwitz et al. (2015) are similar in magnitude to the district policy effects in this study. However, a direct comparison is misguided if there are significant general equilibrium effects or if states promote college enrollment by adopting both mandatory admissions exams and expanding access to public universities.\nSection I provides background on the SAT and the college matriculation process. Section II describes the new dataset of test centers, policies, and linked student outcomes. Section III details the identification strategy. Section IV presents the results and Section V examines how the effects vary with student aptitude. Section VI concludes. 9 In related literature, Dominitz and Manski (1996) , Attanasio and Kaufmann (2009), Jensen (2010) , and Abramitzky and Lavy (2014) find that educational attainment is sensitive to perceived returns. 10 Hurwitz et al. (2015) and Klasik (2013) present difference-in-differences style estimates of the SAT initiative in Maine with 2007 being the first treated cohort. Hurwitz et al. (2015) note that the introduction of the policy was accompanied by the use of online courses to help students prepare for the SAT. In addition, though not noted, is that Maine offered the PSAT in school to sophomores beginning in the 2004-2005 school year, more than tripling participation for the 2007 cohort relative to the prior cohort. The same cohort was also the first to participate in Maine's one-to-one laptop initiative (starting as seventh graders in 2002). Additionally, several Maine high schools, such as Poland Regional High School, independently implemented policies requiring students to take the SAT and to apply to at least one college during this period, inspiring Maine House Speaker Glenn Cummings to propose a bill in 2007 that would require all students to apply to college.\n11 Hyman (2014) also attempts to estimate the policy effect using a difference-in-differences design that interacts the post-policy period with a dummy for a high school not having an ACT testing center. He states that the \"policy increases statewide enrollment at 4-year institutions by 0.6 percentage points (2 percent).\" While this specification reveals the differential effect of the policy for schools that do and do not have centers, it is unlikely to produce an estimate of the effect of the policy. Specifically, the design cancels out any effect of the policy that is common across centers and noncenters. The resulting estimate is determined by some combination of the differential increase in ACT taking between centers and noncenters and the differential response to taking the ACT for the large number of new takers at both centers and noncenters. It is not clear what sign the latter effect will have and which effect will dominate. Thus, I do not attempt to compare the college enrollment results of this design with those from other studies."}, {"section_title": "I. Background", "text": "As of 2014, more than three million students take the SAT or ACT each year, both of which are accepted for admission consideration by almost every four-year college and university in the country.\n12 As shown in Table 1 , which exam a student takes is primarily a function of the state in which he or she lives. In states where the SAT is the preferred exam, 52.7 percent of students take it; whereas only 6.2 percent take it in predominantly ACT states. 13 The fee to take the SAT is $52.50 and waivers are available for eligible low-income students. 14 12 All 489 colleges and universities using the Universal College Application in 2011 accepted both the SAT and ACT. The SAT has historically consisted of mathematics and reading sections, each scored on a scale from 200-800 points for a maximum of 1,600 points. A writing section was added in 2005, but for continuity it is not included in this analysis.\n13 I use the fall count of high school seniors reported by the NCES Common Core of Data to represent the size of a cohort rather than the count of graduates. The number of graduates in a cohort is potentially endogenous to outcomes of interest such as attending college.\n14 A student is eligible for an SAT fee waiver if he qualifies for a free or reduced price lunch, lives in federally subsidized housing, or his family receives some other form of public assistance. Testing center locations, openings, and closings are identified using a dataset constructed by the author. SAT taking is based on the universe of College Board testing records. Alternative high schools, such as night schools, juvenile detention centers, and vocational schools are not included. The fraction of students taking the SAT is relative to fall grade 12 enrollment, and not the number of graduates, which is potentially endogenous to the outcomes of interest in this paper.\nDespite the widespread use of the SAT and ACT by colleges, there is variation in access to these exams across high schools. Two factors in particular affect this: the locations of testing centers, and district and state policies that eliminate the exam fee, offer in-school administration, and have opt-out or mandatory registration. Half of public high school students attend a campus that does not host a testing center and therefore must register to take the exam at another school in the area. Test center locations can be searched online by town name, though only centers in that town are presented. That is, if there is no center in a town, then no result is presented."}, {"section_title": "15", "text": "As shown in Table 2 , high school campuses that host the SAT have approximately 70 percent higher enrollment than campuses that do not, 5.7 percentage points fewer students who qualify for a free or reduced lunch, and 2.7 percentage points fewer Hispanic students than schools that do not."}, {"section_title": "16", "text": "Centers open when a teacher, counselor, principal, or local government employee registers to become a test center supervisor by applying online with the College Board and taking an online training. 17 Based on a survey of 50 counselors at schools where new testing centers opened, schools typically announce test dates for on-campus centers on school websites and newsletters, making the exam more salient to students. 18 The tests may also be advertised at neighboring high schools. State-and district-level policies are a relatively recent phenomenon, as the first was implemented in 2001 and the majority have been adopted since 2009. Most of these policies entail waiving the exam fee and having the exam offered on a school day rather than on a Saturday. They also typically require that students opt out of registering rather than opting in. This paper considers the three earliest adopting districts: Stockton, California; Irving, Texas; and Palm Beach, Florida.\nThe role of admissions exams in the college decision process can be thought of in two stages. 19 In the first stage, a student chooses whether or not to take the exam. In the second, he or she chooses whether or not to attend college. In a traditional human capital model, access should only affect the college enrollment decision by reducing the cost of taking a college entrance exam. The cost of taking the exam is reduced modestly and in measurable ways by a testing center, as students must still pay the exam fee, travel to a testing center, and take the exam on a Saturday. Thus, the change in cost is likely to be small in magnitude relative to factors such as the wage return to college, foregone earnings, and college tuition. Thus, an increase in exam taking in response to a center is likely to be due to increased salience, a perceived implicit or explicit recommendation by the school, or a high level of sensitivity to search and travel costs. These explanations are typically considered \"nudges\" in the behavioral economics literature. By contrast, students affected by 15 In the \"SAT Registration Bulletin,\" which lists all testing centers, towns and cities are listed alphabetically, so identifying the closest center might require a student to look up several neighboring communities. Thus, neither the online search tool nor the published bulletin is especially conducive to finding a center. 16 These statistics are based on high school demographic average from the NCES CCD from 2004 to 2011. 17 Test center supervisors, room supervisors, and proctors are compensated by the College Board for each exam day offered. While the amounts that they are compensated are not large, this may generate a financial incentive to ensure that the exam is well advertised to students.\n18 Notably none of the counselors mentions new centers as tangential to broader college enrollment initiatives. The details of this survey and responses are presented in the online Appendix.\n19 Altonji, Blom, and Meghir (2012) develop a model of an analogous two-stage decision in the context of choosing a college major. district policies face lower costs, as the exam no longer requires a weekend commitment, travel arrangements, paying the fee, or independent registration. The subsequent response to taking an admissions exam is easier to rationalize in a model with uncertainty. Specifically, a student who takes the exam has cleared a primary hurdle in the application process and may have a clearer understanding of his or her college options. This is analogous to the findings in several studies that students face uncertainty about college and update their perceptions after matriculating. 20 20 Altonji (1993) and Arcidiacono (2004 Arcidiacono ( , 2005 introduce uncertainty into estimates of the returns to college and major, while Manski (1989) and Stinebrickner (2008, 2012) identify determinants of college drop-out under uncertainty. Dominitz and Manski (1996) find survey evidence that students are aware of uncertainty in the returns to college. Categories. The status of each high school as a testing center is determined using an original dataset constructed by the author. SAT taking totals are based on the universe of College Board testing records. Alternative high schools, such as night schools, juvenile detention centers, and vocational schools are not included. The fraction of students attaining each outcome is relative to fall grade 12 enrollment, and not the number of graduates, which is potentially endogenous to the outcomes of interest in this paper. *** Significant at the 1 percent level. ** Significant at the 5 percent level. * Significant at the 10 percent level."}, {"section_title": "II. Data", "text": "The dataset draws on five sources: high school data from the National Center for Education Statistics Common Core of Data (NCES CCD), the universe of individual testing records maintained by the College Board, college enrollment records purchased from the National Student Clearinghouse (NSC), college characteristics from the Integrated Postsecondary Education Data System (IPEDS), and a history of SAT and ACT test center locations constructed by the author.\nI start with the list of every public high school cohort in the US for the ten year period from 2001 to 2011. Each cohort is linked to a complete history of district SAT policies, testing centers at the high school attended, and testing centers at neighboring locations. 21 This history of high school cohorts is merged with the universe of individual student data on SAT taking and college score reports sent to colleges."}, {"section_title": "22", "text": "Many students do not take the SAT. However, the number of missing students in each high school cohort and their race and gender can be inferred using the difference in student counts by race and gender between the CCD and College Board student records. I link these records to college enrollment and graduation records from the NSC for a large sample of schools affected by district policies and test centers opening or closing. Students are linked to district policies based on the cohort the policy was intended to target (e.g., seniors in the 2008-2009 school year) and to test centers according to what months students are most likely to take the SAT. 23 The result is an individual-level dataset of students, SAT access at the high school for each cohort, and testing and college outcomes.\ncollege Board records.-The College Board, a nonprofit organization, provided the author with access to the population of test taking records for the years of interest. 24 College Board records have been used previously for economic research. For example, Card and Krueger (2005) use them to evaluate the effect of affirmative action policies, while Hoxby and Turner (2013) use them to identify a pool of high performing students from low-income communities. Each cohort includes approximately three million student records of PSAT and SAT results and test dates. The PSAT is used to construct the underlying distribution of student aptitude as explained in Section V and to test if there is a significant advantage to taking the SAT at one's own school. Each student record includes the codes of college where score reports were sent. 21 Test centers are almost exclusively located on high school or college campuses. However, the codes used by the College Board and ACT to identify test centers are not linked to state or national institution codes. I develop a crosswalk between every test center code and the associated high school or college code used by the NCES. 22 The codes used by the College Board to identify the recipients of score reports are matched with institution codes maintained in the NCES IPEDS using a crosswalk originally developed by Caroline Hoxby (Stanford University), Sarah Turner (University of Virginia), the Mellon Foundation, the College Board, and the ACT. I update the crosswalk to reflect changes in the set of college and universities since 2007. 23 Based on College Board records from 2000 and 2011, approximately 95 percent of students who took the SAT did so during the following months: March (4.4 percent), May (9.9 percent), and June (12.2 percent) of junior year, and October (24.6 percent), November (16.7 percent), December (15.4 percent), January (8.6 percent), and March (4.0 percent) of senior year. A student is considered to have had access to a test center at their high school campus if it was open during this period of their education.\n24 Additional information about the College Board can be found at www.collegeboard.org. ocToBEr 2015\nNational center for Education Statistics ccd and IpEdS.-The NCES Common Core of Data includes enrollment counts by race and gender and free lunch status for every public high school in the United States. For individual-level analysis, these counts are expanded into a separate record for each student as described above. Individual-level data is merged to each of these records using the high school code, cohort, race, and gender reported by the College Board. The NCES Integrated Postsecondary Education Data System provides information on the characteristics of every college and university in the United States, including Carnegie Foundation classifications of college type and selectivity. Results are presented for three categories of selectivity: \"more selective\" colleges whose average test scores place them in approximately the top two-fifths of four-year colleges, \"selective colleges\" whose test scores place them in the middle two-fifths, and \"inclusive colleges\" that generally do not limit admission based on exam score."}, {"section_title": "25", "text": "SAT and AcT Test centers.-The College Board publishes the \"SAT Registration Bulletin,\" which lists every test center that is expected to host the exam in the upcoming academic year. After switching to an online format, the College Board continued to publish paper copies, making it possible to collect a ten year history of bulletins. These bulletins were digitized by the author for use in the analysis. 26 The result is a panel history of test centers where the SAT has been offered in the last ten years, including the subset of Saturdays when the center was open (each year it is offered on one Saturday in October, November, December, January, March, May, and June). Approximately 80 percent of centers are located at public high school campuses, 11 percent at private high schools, and 9 percent at colleges and universities. Newly opened or closed centers are identified as they appear or disappear from the bulletins. Each high school is also linked to all testing centers within 15 miles and an indicator for whether or not the neighboring center was open at the time that each cohort would be most likely to take the SAT.\nNational Student clearinghouse records.-The NSC is a nonprofit organization that maintains records of student enrollment and degree status reported by more than 96 percent of colleges and universities. 27 Records are collected each semester and make it possible to observe each student matriculate, withdraw, transfer, or earn a degree. Unfortunately, the choice of major is unavailable for the majority of enrolled students and academic performance data are not collected. The focus of this analysis is four-year colleges.\n28 I draw NSC records for all students who attended high schools with district policies and students at a random sample of high schools 25 See http://classifications.carnegiefoundation.org for details about the Carnegie Classification. 26 Details of the digitization process are provided in the online Appendix. An analogous history of test centers is constructed for American College Testing (ACT), which publishes \"Registering for the ACT.\" 27 A complete list of institutions that report enrollment to the NSC can be found at www.studentclearinghouse. org. NSC data is used by student lenders, employers, universities and colleges, high schools, and government agencies. 28 Inclusive four-year colleges that do not require a college assessment for admission are grouped with two-year colleges. These schools are primarily for-profit colleges (e.g., the University of Phoenix, Devry, and ITT Tech), nursing colleges, and bible colleges, and the average attainment of matriculates is 2.65 semesters, which is almost identical to the 2.66 semester average at two-year colleges.\nwhere a center opened, closed, or a neighboring center opened within 5 miles. The sampling procedure for schools affected by testing centers was a stratified random sample. Smaller, poorer schools are less likely to have a center open and were given greater weight in order to more accurately reflect the national student population."}, {"section_title": "III. Empirical Strategy", "text": "This section details the identification strategy for evaluating the effect of district policies and access to testing centers. I consider four sources of variation in access: centers opening, centers closing, neighboring centers opening, and the adoption of district policies that provide free in-school administration and default registration.\nHundreds of centers open and close during the period of the analysis. Table 1 presents the number of public high schools, testing centers, and centers that opened or closed between 2004 and 2011 in each state where the SAT is the dominant exam."}, {"section_title": "29", "text": "Each source of variation has relative strengths in terms of what it revealed about student behavior. The opening of a testing center is likely to generate a modest nudge by reducing travel and search costs and increasing awareness. Conversely, a closure may reduce awareness and increase costs. Students attending schools that neighbor testing centers experience an even smaller nudge. Thus, a change in a centers status may induce or deter those closest to the margin of taking the exam. District policies compel more inframarginal students to take the SAT and reveal the test performance and college outcome effects for a larger fraction of the student body. Thus, these alternative strategies shed light on the differences between marginal and more inframarginal test takers and allow us to compare outcomes across policies. However, caution should be used when making direct comparisons as schools in each of the four treated groups are not comparable in terms of observable characteristics.\nEach source of variation also differs in terms of the challenges to credible causal identification. Centers typically open or close due to individual initiative, reducing concerns of the change coinciding with a broader college-going initiative. Further, the effect on SAT taking is strongest for high schools farther from alternative centers, which suggests that search and travel costs are a determining factor. Identification using neighboring centers is perhaps least likely to be biased by changes in student composition or school policies, as the affected school plays no direct part in generating the variation in access. District policies that cover the exam fee for all students and allocate a school day for administration represent a larger time and financial commitment for schools than testing centers. Thus, they are perhaps most likely to coincide with other policy changes. However, because the districts examined are very large, the policies are least likely to coincide with a significant change in student composition.\nI estimate the effects of district policies and testing centers using within-school variation over time. A within-school identification strategy controls for all time-invariant student, school, and community characteristics. Students in adjacent cohorts at the same school are likely to have been taught by essentially the same set of teachers over the course of their schooling, have parents with similar backgrounds and expectations, and have the same access to local colleges and universities. Student assignment to high schools is largely determined by where families live, so it is unlikely that there would be significant changes in composition in adjacent cohorts. Empirically, there is little change in student characteristics at schools affected by district policies and testing centers, and the change that does occur is similar to that observed at other schools in the same state (see Tables D5 to D8 of the online Appendix). Importantly, falsification tests are conducted that apply the primary reduced form specification for each of the four treatments to alternative measures of student performance. The results reveal that the treatment had no significant effect on the average PSAT or AP scores of students (see Table C3 of the online Appendix). This provides evidence that estimated changes in college outcomes are unlikely to stem from changes in students aptitude or school quality, both of which are likely to lead to higher scores on other standardized high school exams.\nEquation (1) is used to estimate the first-stage effect of centers and policies on the probability that a student takes the SAT. Whether or not a student takes the SAT is assumed to depend on a school fixed effect, \u03b1 s ; a cohort effect, \u03b1 c ; school and individual characteristics that may differ across cohorts, X s, c and X i ,; and whether or not the cohort is affected by a policy or center, p s, c :\nThe coefficient of interest \u03b2 is the estimated effect on the probability of taking the SAT. If students take the SAT in response to increased awareness and reduced search and travel cost, then the effect at a neighboring center should be decreasing in distance (and smaller than equation (2) \nIf the effect of new testing centers exclusively increases SAT taking at the host school through some unobserved shift in school quality or policy (rather than increased awareness and reduced cost), then there should not be an effect at neighboring schools. Conversely, a significant change when a center opens nearby is evidence of an access effect. Finding smaller effects for centers that open farther from the school of interest supports this conclusion."}, {"section_title": "32", "text": "If the returns to college are uncertain, then the information contained in a college assessment or the subsequent recruitment by colleges could change a student's enrollment decision. Taking the SAT also eliminates one of the key barriers in the process of applying to most four-year colleges. Estimating the effect of taking the SAT on college outcomes is difficult in the cross section as the decision to take a college assessment is endogenous to a student's propensity to attend. District policies and testing centers are used as instruments to estimate the effect of taking the SAT on short-and long-run college outcomes:\nCollege outcome y i, s, c of student i at high school s in cohort c is assumed to depend on school and cohort fixed effects and whether or not the student takes the SAT. The parameter \u03b4 is the estimated effect of taking the SAT among compliers who are compelled or induced by a district policy or a testing center. In the case of centers, \u03b4 is the effect for marginal SAT takers, while in the case of district policies it is the average affect for a significant fraction of students who would typically not take the exam. The college outcomes considered include score reports sent to colleges, college matriculation, semesters completed, and graduation. Thus, it is possible to observe (with varying levels of precision) the probability with which new takers proceed to each stage of college completion. These rates can be compared to those of always-takers. It is not possible to observe the counterfactual of what would have happened had an SAT center not opened or closed or a district policy not been adopted. The identification strategy relies primarily on within-school, cohort-to-cohort variation, which is evident graphically and in population estimates (see Tables A1 to A3 of the online  Appendix) . Nonetheless, there are two potential benefits of including within-state matched control schools. First, controls can help to adjust estimates for national and state variation in the rate of SAT taking and college attendance, which can be significant. Such variation can stem from, for example, changes in economic conditions that make college more or less appealing, changes in state investment in primary or secondary education, or changes in tuition or admissions policies at state colleges and universities. Second, some variation in outcomes from year-to-year is due to data limitations. For example, while the majority of students who apply to four-year colleges do so in the years immediately following high school, some apply in each subsequent year. Thus, all else equal, a higher fraction of students in older cohorts are observed to have sent a score report to a college. This would generate a negative bias for centers that open and district policies, and a positive bias for centers that close.\nIn order to account for year-to-year variation that is common across schools, each treated high school is matched using characteristics and trends in characteristics in the years prior to being treated. Specifically, schools are matched using racial composition, fraction of students eligible for a free or reduced lunch, enrollment totals, fraction of students taking the SAT, and five year trends in both enrollment and SAT taking. Various methods of matching are detailed in Dehejia and Wahba (2003) ; Imbens (2004) ; Abadie and Imbens (2006) ; Caliendo and Kopeinig (2008); and Abadie, Diamond, and Hainmuller (2010) . The matches presented in this analysis are nearest-neighbor propensity score matches with replacement, estimated using a probit model. 33 The online Appendix presents the details of the matching procedure and the quality of the resulting control group. The matching for each of the four treated groups results in pairs of schools that are balanced in the characteristics used for matching as well as characteristics not used in the matching process. Importantly, treated schools exhibit little change in characteristics before and after treatment, and the changes that do occur are similar to those of matched controls."}, {"section_title": "IV. The Effect of Centers and Policies on College Outcomes", "text": "This section examines how changes in access to admissions exams affect shortand long-run college outcomes. Testing centers are shown to induce a significant number of students on the margin to take the SAT, while district policies that provide free in-school administration and default registration nearly double participation. I estimate if new takers send score reports to four-year colleges and subsequently attend, progress through, and graduate from college. I highlight the stages at which new takers appear to no longer continue their educations and compare these to the outcomes of always-takers. Section V extends the analysis to examine the aptitude of students induced to take the SAT and how this affects college outcomes. Identifying heterogeneity in this dimension is important for comparing the outcomes of marginal and inframarginal takers, explaining the pattern of college outcomes, and for targeting policy.\nTo ensure comparable groups across outcomes of interest, I present estimates for the sample of more than 800,000 students for whom records were pulled from the National Student Clearinghouse. The online Appendix presents population estimates for SAT taking and score reports, which are observed in College Board records, with and without control schools, student characteristics, and school characteristics (see Tables A1 to A3 ). Estimates are also presented for specifications that control for school-specific time trends and interactions of treatment with baseline levels of access (see Tables C1 and C2). 34 I also consider the extent to which a student's 33 Because nearly all of the identifying variation is within school, I find no evidence that the estimates of SAT taking and score reports sent are sensitive to using multiple matched control schools, matching without replacement, or omitting matched controls altogether. Because data on college enrollment and attainment must be purchased from the National Student Clearinghouse, I present college outcome results for the nearest-neighbor propensity score match with replacement, which requires only one control school. 34 Omitting matched control schools is appealing if treated schools share trends in outcomes and therefore act as good controls for each other (see, for example, Abramitzky and Lavy 2014) . This is a practical approach if there is significant variation in the timing of when schools are treated and if these schools are similar in terms of, for example, geographic location and demographic composition. This is the case for center openings, which have taken score is likely to benefit from taking the exam at his or her own school, an alternative mechanism by which centers and policies could change college outcomes. Table 3 presents the estimated effect of a testing center opening at a host school on SAT taking, score reports sent, four-year college attendance, semesters completed, and four-year college graduation. When a new testing center opens, students at the host school have reduced travel and search costs and may receive additional reminders that increase salience. The results indicate that these nudges compel some students to take the SAT and ultimately result in them attending a four-year college."}, {"section_title": "A. Test centers opening", "text": ""}, {"section_title": "35", "text": "Having a test center on campus increases the rate of SAT taking by 3.8 percentage points, or 8.5 percent. 36 Estimates that control for student demographics (gender, race, and free lunch status) are nearly identical to those that do not. 37 College Board records do not indicate the specific center where a student took the SAT, so I develop an alternative method of testing if students are responsive to new centers. The data does include the month and year when each student took the exam. Each test center is, on average, open for 2.2 of 7 possible test dates in the first year it is open. Thus, I link the date a student took the test to the dates when the new center was open. This approach reveals an immediate increase of more than 30 percent (6 percentage points) in students taking the SAT on the dates when the center is open-a change that is sufficient in magnitude to explain the total increase in taking across all dates.\n38 Figure 1 presents the change in SAT taking relative to the date when the first cohort had access to a test center on campus. The rate of taking is stable prior to the opening of the center, increases in the year the center opens, and remains elevated. The suddenness of the change and the lack of change in prior years is inconsistent with explanations based on changes in student or teacher composition (which are unlikely to be sudden), or the implementation of related policies (that are unlikely to coincide precisely with the timing of new centers at all schools). The sustained nature of the effect in the after period is not consistent with a center opening in response to an outlier cohort. 39 place over time and in a variety of schools in each state. This is not, however, a useful approach for evaluating district policies as two of the three policies were implemented in the same year and the districts are in different states, have dissimilar baseline rates of college enrollment, and have dissimilar characteristics. 35 The results indicate that both types of nudges matter. Specifically, there is evidence of larger effects when alternative centers are farther away and indirect effects of centers on students at neighboring schools. This suggests that travel and search costs matter. However, the largest effects are for students at host schools, even when there are many alternative centers in the immediate vicinity. This suggests that salience is important. 36 The estimated effects are larger when the distance to an alternative center is greater. Specifically, at schools classified as rural, the effect size increase by 3 percentage points per 10 miles to the next closest center. The estimates generated by this identification strategy differ from those using a naive cross-sectional approach that compares schools with and without centers. A cross-sectional approach indicates that a test center increases SAT taking by 11.3 percent and thus appears to suffer from upward bias due to omitted variables. 37 Population estimates in SAT dominant states are similar in magnitude to the estimates for the NSC sample (see Table A1 of the online Appendix). 38 The net increase in SAT taking of approximately 4 percentage points is reflected in a 6 percentage point increase on dates when the center is open and a 2 percentage point decrease on dates when the center is closed. 39 Estimates that control explicitly for school-level time trends indicate a 3.4 percentage point increase in SAT taking, further supporting the hypothesis that the results are not due to existing trends (see Table C2 of the online After being induced to take the exam, students become eligible to apply to nearly all four-year colleges. Column 2 of Table 3 presents the instrumental variables Appendix). Note that a specification with a linear time trend may understate the true effect as there is an upward trend after a center opens (perhaps due to the fact that additional dates are typically added in subsequent years). Notes: Estimates in the first column reflect the first stage effect of a new test center opening on SAT taking. The estimates are the percent change relative to the mean prior to the center opening. Columns 2 to 5 present the instrumental variables estimates of the effect of taking the SAT on sending a score report to a college or university, attending a four-year college, semesters completed at a four-year college, and graduating from a four-year college. In the case of college graduation, attention is restricted to students in cohorts that could feasibly have graduated by the date of the data pull (five years after the expected date of high school graduation). Testing center locations and openings are identified using a dataset constructed by the author. SAT taking is based on the universe of College Board testing records. College outcomes are derived from linked individual records from the NSC. Each specification includes school and cohort fixed effects. Panel B presents results with controls for student demographic characteristics (the gender and race of each student) and school characteristics (fraction of students receiving a free or reduced lunch, pupil-teacher ratio). Standard errors are clustered at the school level. *** Significant at the 1 percent level. ** Significant at the 5 percent level. * Significant at the 10 percent level.\nestimates of the effect of taking the SAT on sending a score report to a college or university. The estimated fraction of new takers that send a report is approximately 64 percent. 40 Once the score reports have been sent, colleges can target recruitment to students who have expressed an interest and have the scores necessary for admission. On average, students induced to take the SAT attend a four-year college at a rate of 37 percent in the preferred specification. By comparison, the rate of 4-year college attendance among always-takers at the same schools is 60 percent. This differential is likely due in part to marginal takers having lower average scores and 40 Estimates for the population of schools with new centers in SAT dominated states indicate a rate of sending score reports of about 55 percent, which is robust to the choice of specification (see Table A1 of the online Appendix). Score reports are a proxy for applications, as examined in Card and Krueger (2005) . Students who register to take the SAT independently have revealed an interest in attending college and have taken a first step in the college application process. Thus, it is more likely that score reports correspond to applications for these students than those who must opt-out of registration due to a district policy. Notes: These panels present changes in college outcomes before and after a testing center opens for students attending a host high school. Panel A presents the change in the fraction of students who took the SAT on a date when the exam was offered at that school. Panels B, C, and D present the change in the fraction of students who sent score reports to a four-year college, attended a four-year college, and graduated from a four-year college. Year 0 corresponds to the year when a center opens for the first time at the host school during the sample period. The mean baseline rate for treatment and control groups is set to 0. Testing center locations and openings are identified using a dataset constructed by the author. SAT taking is based on the universe of College Board testing records. College outcomes are derived from linked individual records from the NSC.\nin part to having a lower propensity to attend due to other factors, such as financial constraints. This distinction is examined explicitly in Section V. Estimates reveal that while students induced to take the SAT by a testing center seem to send score reports quite broadly, they are most likely to attend a college or university that is selective, and not more selective (see Tables B1 and B2 of the online Appendix). The instrumented effect of taking the SAT on semesters completed at a 4-year college is 2.9 semesters per newly induced taker, or more than 6 semesters per matriculate. 41 This is comparable to the number of semesters completed by the average matriculate from the same high school. That is, students induced to take the SAT by a new center appear less likely to attend college than always-takers, but conditional on attending there is no evidence that they complete fewer semesters. This result implies that the students being induced to attend a four-year college are not dropping out at higher than typical rates, which may be a primary benefit of a policy that induces attendance by revealing aptitude rather than by reducing cost.\nRestricting attention to students who were expected to graduate in the 2007 cohort or earlier, and thus could feasibly have graduated, I estimate the effect of taking the SAT on graduating from college within five years. Column 5 of Table 3 indicates that approximately 26 percent of those induced to take the SAT go on to graduate from college within 5 years. Among inframarginal takers, the rate of graduation is approximately 42 percent.\n42 Figure 1 presents the change in score reports sent, college attendance, and the fraction graduating from college. Each graph reveals a sharp change in the year a center opens that mirrors the change in SAT taking. No lagged effect is observed in the years prior to the change in access."}, {"section_title": "B. Test centers closing", "text": "When a test center closes at a high school campus, students must identify an alternative center, face increased travel costs, and are less likely to receive reminders that increase the salience of registration deadlines. Thus, a test center closure may reduce SAT taking and, in turn, decrease the probability that a student attends a four-year college. Table 4 presents estimates of the effect of a testing center closing that are analogous to those for centers opening. These indicate that having a test center close decreases the rate of SAT taking by 2.5 percentage points, or 4.9 percent. 43 The estimates are essentially unchanged by the inclusion of student and school characteristics. An estimated 44 percent of students who do not take the SAT due to the closure would have sent a score report to a college had they taken it, and an estimated 39 percent would have attended a 4-year college. That is, a significant fraction of the students deterred from taking the exam by a lack of access to a testing center are students who otherwise would have attended a four-year college. Of 41 Note that the estimate of semesters completed, while statistically significant, is measured with substantial error. Thus, comparisons of the college outcomes of marginal and inframarginal takers should be made with caution. 42 For these cohorts, approximately 62 percent of always-takers at schools where a center opens attend a 4-year college and 51 percent are observed enrolling for at least 4 semesters. By comparison, an estimated 37 percent of new takers matriculate and 30 percent are observed enrolling for at least 4 semesters. Thus, the estimated rate of persistence conditional on matriculation is similar. 43 Population estimates for test centers closing show a reduction in SAT taking of between 4.6 and 4.9 percent across specifications (see Table B2 of the online Appendix). Notes: Estimates in the first column reflect the first stage effect of a test center closing on SAT taking. The estimates are presented as a percent change relative to the mean prior to the center closing. Columns 2 to 5 present the instrumental variables estimates of the effect of taking the SAT on sending a score report to a college or university, attending a four-year college, semesters completed at a four-year college, and graduating from a four-year college. In the case of college graduation, attention is restricted to students in early enough cohorts that their graduation cohorts could have been realized by the date of the data pull (five years after the expected date of high school graduation). Each specification includes school and cohort fixed effects. Panel B presents results with controls for student demographic characteristics (the gender and race of each student) and school characteristics (fraction of students receiving a free or reduced lunch, pupil-teacher ratio). Testing center locations and closings are identified using a dataset constructed by the author. SAT taking is based on the universe of College Board testing records. College outcomes are derived from linked individual records from the NSC. Standard errors are clustered at the school level. *** Significant at the 1 percent level. ** Significant at the 5 percent level. * Significant at the 10 percent level. ocToBEr 2015 note is that marginal takers at centers that close are estimated to attend college at the same rate as marginal takers at centers that open. Semesters completed are (imprecisely) estimated to increase by approximately 3.5 per new taker. Restricting the sample to those who could feasibly have graduated from college reveals a positive and statistically significant graduation effect."}, {"section_title": "44", "text": "Figure 2 presents the change in college outcomes graphically. Because the firststage effect of a center closing on SAT taking is only half that of centers opening, the changes in college outcomes are smaller in magnitude. Nonetheless, there appears to be a clear shift in each outcome at the time that a center closes. The 44 For cohorts that could feasibly have graduated, approximately 61 percent of always-takers at schools where a center opens attend a 4-year college and 52 percent are observed enrolling for at least 4 semesters. The estimates suggest a matriculation rate of about 45 percent for marginal takers deterred by the closure and a nearly identical increase in the fraction completing at least 4 semesters. This suggests very high rates of persistence in college. However, each of these estimates is measured with enough error that caution should be exercised in making refined comparisons. Notes: These panels present changes in college outcomes before and after a testing center closes for students attending a host high school. Panel A presents the change in the fraction of students who took the SAT. Panels B, C, and D present the change in the fraction of students who sent score reports to a four-year college, attended a four-year college, and graduated from a four-year college. Year 0 corresponds to the year when a center closes for the first time at the host school during the sample period. The mean baseline rate for treatment and control groups is set to 0. Testing center locations and closings are identified using a dataset constructed by the author. SAT taking is based on the universe of College Board testing records. College outcomes are derived from linked individual records from the NSC.\nfigure supports the conclusion that the reduction in these outcomes was not due to a preexisting trend relative to similar schools in the same state. There are several reasons to believe that the effect of a center closure may not mirror the effect of a center opening. First, centers may open in areas where there has been consistently high demand for centers relative to the supply, while centers may close in areas where there is low demand relative to supply. This would predict smaller first-stage effects of center closures. Likewise, if there are low levels of announcements at high schools in the years prior to a center closing, then closures may be associated with a weaker treatment than when a center opens."}, {"section_title": "C. Test center Neighbors", "text": "When centers open (or close) it affects access to the SAT for students at neighboring schools. The reduced search and travel costs generate a nudge that is likely to be weaker than for students at a host school. The effect should also decrease in the distance to the center. This provides a valuable opportunity to check if there are indirect effects of centers and to credibly identify the role of proximity to a center on college outcomes. If the effect of new testing centers exclusively increases SAT taking at the host school through some unobserved shift in student or teacher composition or an unobserved policy change, then there should not be an effect at neighboring schools. Population estimates may be of particular interest in this context due to the fact that more than 60 percent of high schools without a center have one open or close within 15 miles during the 10 year period of this study. The linked NSC sample used in this section is for schools that had a center open within five miles.\nThe results in Table 5 indicate that students who attend a high school in the vicinity of a center that opens are more likely to take the SAT. Each time a test center opens within 5 miles of a high school that does not have a center, the probability that a student from that school takes the SAT increases by approximately 1.4 percentage points, or 2.6 percent. 45 It is important to note that it is common for a school to be treated by more than one center opening during the sample period. The effect of a center opening or closing diminishes in distance, with little evidence of a positive effect of a center 10 to 15 miles away. 46 The lack of an effect of centers farther away suggests that the change in SAT taking is due to the presence of a neighboring center and not to state and regional trends or shocks. Some combination of increased awareness of the new test center, ease of identifying a local center, and reduced travel distance results in more students taking the exam. Figure 3 is centered for the first cohort that has a center open within five miles. The graphs indicate an increase when a center opens and a slight upward trend in future years.\nIn the preferred specification, I find that approximately 51 percent of students induced to take the SAT by a neighboring center send a score report to a 4-year Notes: Estimates in the first column reflect the first stage effect of a test center opening within 5, 5-10, and 10-15 miles of the school a student attends. They reflect the percent change relative to the mean prior to the center opening. Columns 2 to 5 present the estimated effect of taking the SAT on sending a score report, attending college, semesters completed, and graduating. For graduating, attention is restricted to students who completed high school at least five years prior to the data pull. Each specification includes school and cohort fixed effects. Panel B presents results with controls for student demographic characteristics (the gender and race of each student) and school characteristics (fraction of students receiving a free or reduced lunch, pupil-teacher ratio). Testing center locations are identified using a dataset constructed by the author. SAT taking is based on the universe of College Board records. College outcomes use linked individual records from the NSC. Standard errors are clustered at the school level. *** Significant at the 1 percent level. ** Significant at the 5 percent level. * Significant at the 10 percent level."}, {"section_title": "AmErIcAN EcoNomIc JoUrNAL: AppLIEd EcoNomIcS ocToBEr 2015", "text": "college and an estimated 32 percent attend a 4-year college. 47 Conditional on attendance, new takers complete an estimated 5.3 to 5.8 semesters, which is similar to the outcome of students at host schools. Estimates indicate that 23 to 26 percent of these students graduate from college. As with students at host schools, the graphical evidence is compelling. Figure 3 indicates that the magnitudes are smaller than for host schools, but that college outcomes are elevated for cohorts that graduate after a center opens within five miles."}, {"section_title": "D. district policies", "text": "Extending the analysis to district policies serves several purposes. First, it is relevant in light of the large number of districts and states that are currently implementing or considering such policies. Second, it is possible to evaluate how taking the 47 Population estimates show that 43 to 50 percent of marginal takers who are induced or deterred by a neighboring center send a score report to a college (see Table A3 of the online Appendix). "}, {"section_title": ". The Effect of a Test Center Opening on College Outcomes at Neighboring Schools", "text": "Notes: These panels present changes in college outcomes before and after a testing center opens for students attending a neighboring high school. Panel A presents the change in the fraction of students who took the SAT. Panels B, C, and D present the change in the fraction of students who sent score reports to a four-year college, attended a four-year college, and graduated from a four-year college. Year 0 corresponds to the year when a center opens at a neighboring center within five miles. The mean baseline rate for treatment and control groups is set to 0. Testing center locations and openings at neighboring schools are identified using a dataset constructed by the author. SAT taking is based on the universe of College Board testing records. College outcomes are derived from linked individual records from the NSC.\nSAT differentially affects more inframarginal takers who are likely to differ from the marginal takers induced by testing centers. I examine three districts that were the earliest adopters of SAT policies, thus allowing an examination of college outcomes. Stockton Unified School District in California offered the SAT for free to all students and provided transportation to the test starting with the graduating cohort of 2009. The School District of Palm Beach County in Florida and Irving Independent School District in Texas adopted \"SAT School Day,\" which results in the exam being offered for free on a school day with opt-out registration starting with the graduating cohort of 2011. In total, the three districts contain 33 high schools. In terms of population, Palm Beach County is similar in size to the states of Maine and New Hampshire. Because I exploit district policies rather than state policies, matched control schools can be selected from within the state, thus controlling for state-specific year effects in college outcomes, such as those driven by regional economic conditions or changes in tuition and enrollment targets at state colleges and universities. District policies dramatically increase the rate at which students take the SAT. Students are compelled, rather than nudged, to take the exam. School day administration eliminates time, search, and transportation costs, as well as exam fees. The estimates in Table 6 suggest that district policies increase SAT taking by 31 percentage points, or 73 percent on a baseline rate of 42 percent. The inclusion of individual school time trends has only modest effects on the estimates. 48 Instrumenting for SAT taking with the district policy, I find that approximately 55 percent of students compelled to take the SAT send a score report to a four-year college. Students may identify up to four colleges that will receive a score report at the time they take the exam for no charge, so sending a report does not imply that a completed application followed. This is especially likely to be the case with a district policy because students did not select into taking the exam based on having an interest in college (in contrast to marginal takers near centers who independently register and pay the exam fee).\nApproximately 11 percent of new takers attend a 4-year college, a 3 percentage point increase. Estimates reveal that students compelled to take the SAT by a district policy are more likely to send a score report to a more selective college than a selective college, but almost all that matriculate attend a selective college (see Table  B2 of the online Appendix). New takers complete a statistically significant number of semesters in their first year and an estimated 8 percent of new takers complete at least 2 semesters of college within 2 years of completing high school. 49 The high loss rate of students at each step between taking the SAT and finishing a year of college is evident in Figure 4 . The estimated fraction of students compelled to take the SAT by a district policy who ultimately attend college is significantly lower than the analogous estimate for the average student and for students nudged to take the exam by a testing center. 48 Tables C1 and C2 of the online Appendix present estimates while controlling for individual school trends and interactions between the policy and each school's status as an SAT center. The rate of SAT taking increases somewhat less at schools that already host an SAT center at the time the policy is introduced. However, schools that do and do not host centers in the cross section are fundamentally different so the differential effects of the policy may not reflect the effect of differential access in the baseline period.\n49 Table C4 of the online Appendix presents the standard errors for a variety of methods of clustering, including the wild cluster bootstrap procedure recommended by Cameron, Gelbach, and Miller (2008) for cases with few clusters."}, {"section_title": "V. College Outcomes by Aptitude", "text": "The previous section highlighted the differences in college outcomes for marginal takers, inframarginal takers, and always-takers. This section examines the extent to which the aptitude of new takers explains the observed pattern of SAT Notes: Estimates in the first column reflects the first stage effect on SAT taking of district policies that provide free in-school administration and default registration. The estimates are presented as a percent change relative to the mean prior to policy implementation. Columns 2 to 5 present the instrumental variables estimates of the effect of taking the SAT on sending a score report to a college or university, attending a four-year college, semesters completed at a four-year college, and completing the first year of college. Because the policies were implemented recently, it is not possible to estimate their effect on college graduation. Each specification includes school and cohort fixed effects. Panel B presents results with controls for student demographic characteristics (the gender and race of each student) and school characteristics (fraction of students receiving a free or reduced lunch, pupil-teacher ratio). SAT taking is based on the universe of College Board testing records. College outcomes are derived from linked individual records from the NSC. Standard errors are clustered at the district level. Alternatives to districtlevel clustering are presented in the online Appendix. *** Significant at the 1 percent level. ** Significant at the 5 percent level. * Significant at the 10 percent level.\ntaking and college matriculation. The information revealed to a student by the exam and subsequent recruitment by colleges may be a primary mechanism for determining his or her college enrollment decision. Highly heterogeneous effects by aptitude could also have important implications for determining how to efficiently target policy. Likewise, lower enrollment among new takers relative to always-takers of similar aptitude indicates that other barriers to matriculation may be important. This section estimates the aptitude of students induced to take the SAT and develops a method to test for heterogeneity in college outcomes when a uniform measure of aptitude is not observed."}, {"section_title": "A. Student Aptitude", "text": "District policies and new testing centers do not necessarily target students that are most likely to benefit from taking a college assessment. I estimate the distribution of scores of new SAT takers by defining new outcome variables for scores in 100 point bins ranging from 400 (the lowest) to 1,600 (the highest) and estimating equation ( Notes: These panels present changes in college outcomes before and after a district policy is implemented that provides free in-school administration and default registration. Panel A presents the change in the fraction of students who took the SAT. Panels B, C, and D present the change in the fraction of students who sent score reports to a four-year college, attended a four-year college, and graduated from a four-year college. Year 0 corresponds to the year when the policy is implemented. The mean baseline rate for treatment and control groups is set to 0. SAT taking is based on the universe of College Board testing records. College outcomes are derived from linked individual records from the NSC.\neach score bin. The results are presented graphically in Figures 5 and 6 , with the estimated distribution normalized to one to facilitate comparison. In the case of new centers, the distribution is shifted left relative to the distribution for always-takers, indicating that new takers score lower than the average. However, there is a common support. Though few new takers score above a 1,200, it appears that a significant fraction earn scores near 1,000, which is the national average. For district policies, the majority of scores are well below the pre-policy mean. An estimated 44 percent of new takers earn a score that would place them among the bottom decile of pre-policy takers at the same schools. This is likely due to the fact that a large fraction of non-takers are compelled to take the exam by free in-school administration and opt-out registration regardless of their academic aptitude or interest in college. A host school advantage on the SAT would be an alternative explanation for an increase in the number of students earning high scores after a new center opens or a policy is adopted. That is, the perceived common support of new takers and always-takers could be spurious if centers increase the scores of always-takers. However, there is no empirical evidence of such an advantage. Specifically, as shown in the online Appendix, the estimated PSAT scores for students induced to take the SAT is nearly identical to the estimated SAT scores, though the PSAT is almost always taken at a student's own school. This supports the hypothesis that some new takers are college-ready. Likewise, controlling for baseline PSAT scores, there is no evidence of higher SAT scores for students with access to a newly opened center. "}, {"section_title": "B. college outcomes by Ability", "text": "I estimate how college attendance patterns vary with student aptitude. In conjunction with the distribution of scores of new takers estimated in the previous section, this may help to explain the heterogeneous outcomes of always-takers, marginal takers, and inframarginal takers.\nNote that the instrumental variables Wald estimator is the ratio of the change in the fraction of students attaining an outcome relative to the change in the fraction of students who take the SAT. In the case of average effects, this requires a measure of the number of students at each school, which is observed in the NCES CCD. In the case of differential effects by aptitude, it requires a measure of the number of students at each school with aptitude in the range of interest. Unfortunately, no uniform measure of aptitude for all students in the United States exists. As an alternative, I exploit the fact that some schools require all students to take the PSAT.\n50 At these schools, I can use the distribution of PSAT scores to estimate the distribution of SAT scores were all students to take it. This requires 50 Hundreds of schools make the PSAT mandatory for all sophomores or juniors, often as a result of a state or district policy. The states of Arkansas, Connecticut, Delaware, Florida, Georgia, Indiana, Maine, New Mexico, North Carolina, Oregon, South Carolina, and Texas have all enacted PSAT policies at some point since 2000. Notes: This figure presents the estimated distribution of SAT scores for students who are compelled to take the SAT by a district policy and the distribution of all SAT scores at the same high schools prior to the policy. District policies provide free in-school administration and default registration. Each distribution has been normalized to sum to one for ease of comparison. The distribution of new scores is estimated by defining new outcome variables for scores in 100 point bins ranging from 400 (the lowest) to 1,600 (the highest) and estimating equation (1) for each. This specification includes school and cohort fixed effects. SAT taking is based on the universe of College Board testing records.\na mapping of PSAT scores to expected SAT scores, which I compute using all students nationally who take both exams:\np i, j is a 121 by 121 matrix containing the probabilities that a student with PSAT score i earns SAT score j . This transition matrix is used to predict the \"expected\" distribution of SAT were all students to take it."}, {"section_title": "51", "text": "At schools where the PSAT is administered to all students, I have both an estimate of the full distribution of SAT scores and the actual SAT scores of students who took it. This is sufficient to determine the number of students in each aptitude range and to compute the desired instrumental variables estimates. At schools that do not require the PSAT, I make the assumption that the underlying distribution of SAT scores is the same as at similar schools that do require the PSAT. Specifically, I pair non-PSAT schools to PSAT schools on the basis of having the same enrollment and the same realized SAT score distribution (mean and variance). When differentiating the effects of taking the SAT by ability, the important assumption is not that the distribution has been precisely estimated, but that the change in the underlying distribution does not diverge between treatment and control schools after treatment."}, {"section_title": "52", "text": "The resulting prediction of the aptitude distribution suggests that a substantial fraction of students who are not taking the SAT would perform well if they did, as shown in Figure 7 . This supports the finding that some induced takers perform well on the exam. Table 7 presents outcomes for testing centers by student ability and Table 8 presents analogous results for district policies. The first-stage effect for each tercile of the ability distribution when a new center opens indicates that the top, middle, and bottom of the ability distribution are estimated to have increases of 3.2, 4.2, and 4.0 percentage points in SAT taking. This translates to 10.9, 7.0, and 5.4 percent of non-takers, respectively. 53 An estimated 65.4 percent of students in the top tercile of the ability distribution who are induced to take the SAT go on to attend a four-year college as a result. For students induced from the middle and bottom terciles, 37.0 and 15.1 percent go on to attend a 4-year college, respectively. Thus, among marginal takers who would not have taken a college assessment had 51 Equation (4) assumes that the mapping from PSAT to SAT scores, using students who take both exams, is not biased by students systematically opting out of taking the SAT who would have performed less well than other students with the same PSAT score. If this occurs, the estimated distribution will be shifted to the right of the true distribution and would bias the estimates downward for high-ability students and upward for low-ability students (due to over and understating the number of students in each aptitude bin). But, in practice, the estimates indicate much larger effects for high-ability students. 52 For example, if the cutoff score for the top third of the distribution was incorrectly placed at the thirtieth percentile score, instead of the thirty-third, then the estimate will reflect the average treatment effect for a slightly different group than intended. Significant bias is only likely if the unobserved distribution of aptitude changes significantly at the time of treatment. This is not surprising, as an unobserved shift in student aptitude at the time of treatment would invalidate all of the results in this study. 53 For example, 70.4 percent of students in the top tercile of the ability distribution take the SAT, so only 29.6 percent are non-takers. Thus, a 3.2 percentage point increase in taking implies that 10.9 percent of non-takers are induced by the new center.\nthere been no test center, the effect of taking the exam on college attendance is highly correlated with aptitude. Also of interest is that students induced to take the exam are somewhat less likely to attend college than always-takers of similar ability. The rate of 4-year college attendance among always-takers at the same schools is 74.2, 52.6, and 29.3 percent for students from the top, middle, and bottom of the distribution. This comparison also reveals that a higher fraction of marginal takers than always-takers who attend college are drawn from the top third of the ability distribution. That is, positive selection is stronger for new takers than always-takers.\nPrior to district policies, 42 percent of all seniors took the SAT in the affected districts. I estimate that this rate was 20.8, 39.1, and 66.4 percent among students in the lowest to the highest ability terciles. The policy increased the rate by 56.5, 31.3, and 11.4 percentage points in the bottom, middle, and top terciles of the distribution, making the rates approximately equal across the ability distribution. This is consistent with the highly skewed distribution of new scores. Differentiating college outcomes by student ability reveals that 3.5 percent of students from the bottom tercile, 9.0 percent from the middle tercile, and 38.7 percent from the top tercile ultimately matriculate at a 4-year college. Thus, it appears that the average rate at which compelled takers go on to college is largely determined by aptitude. The rates of attendance for new takers are significantly lower than for always-takers of similar ability at the same high schools. For example, the rate of college attendance among students from the top tercile of ability who are compelled to take the SAT is half that of always-takers from the top tercile and nearly identical to that of the middle tercile. Fraction of students in score bin 400 500 600 700 800 900 1,000 1,100 1,200 1,300 1,400 1,500 1,600\nSAT score\nObserved scores Latent scores"}, {"section_title": "Figure 7. SAT Taking by Latent Ability", "text": "Notes: This figure presents the observed distribution of SAT scores and an estimate of the distribution of SAT scores for all students if they were to take the exam. The distribution for all students is derived using schools where all students are required to take the PSAT. PSAT scores can be mapped to SAT scores using the universe of students who take both exams. The distributions are derived using the universe of College Board records from 2000 to 2011.\nStudents induced or compelled to take the SAT have lower aptitude than always-takers. This results in relatively low rates of college attendance because aptitude is a primary determinant of whether new takers attend a four-year college. Further, there is consistent evidence that even after conditioning on aptitude marginal takers are less likely to attend college. This pattern is more pronounced among students compelled to take the exam by a district policy than those induced by a testing center. This suggests that the same factors that deter students from taking the SAT may deter them from completing the subsequent steps in the matriculation process. However, as a Notes: Panels A, B, and C present college outcomes for students in the bottom, middle, and top terciles of the ability distribution, respectively. The first column presents the effect of a new center opening on SAT taking. The results are presented as a percentage point increase relative to the baseline rate. Columns 2 and 3 present the instrumental variables estimates of the effect of being induced to take the SAT by a new center on college attendance and semesters completed. The specifications include school and cohort fixed effects and school demographic characteristics (gender and race). Testing center locations and openings are identified using a dataset constructed by the author. SAT taking is based on the universe of College Board testing records. College outcomes are derived from linked individual records from the NSC. Standard errors are clustered at the school level. *** Significant at the 1 percent level. ** Significant at the 5 percent level. * Significant at the 10 percent level.\nresult, the average aptitude of new matriculates is relatively high, which may explain why they appear to complete as many semesters as the average matriculate."}, {"section_title": "C. Aptitude and option Value", "text": "Students who are induced to take the SAT as a result of a district policy or new testing center may accrue additional lifetime earnings. I combine the estimates of Notes: Panels A, B, and C present college outcomes for students in the bottom, middle, and top terciles of the ability distribution, respectively. The first column presents the effect on SAT taking of district policies that provide free in-school administration and default registration. The results are presented as a percentage point increase relative to the baseline rate. Columns 2 and 3 present the instrumental variables estimates of the effect of being compelled to take the SAT by a district policy on college attendance and completing one year of college. The specifications include school and cohort fixed effects and school demographic characteristics (gender and race). SAT taking is based on the universe of College Board testing records. College outcomes are derived from linked individual records from the NSC. Testing longer term college outcomes is restricted by the fact that the policies were recently implemented. Standard errors are clustered at the school level. Alternatives to district-level clustering are presented in the online Appendix. *** Significant at the 1 percent level. ** Significant at the 5 percent level. * Significant at the 10 percent level.\ncollege attendance effect in this study with estimates from the returns to college literature to compute a back-of-the-envelope estimate of the option value of taking the SAT. Interestingly, the literature suggests that the returns to college for marginal students are similar to those for inframarginal students. Carneiro, Hansen, and Heckman (2003) estimate that the returns to attending college for students on the margin are 51 percent, relative to 56 percent for nonmarginal attendees. These estimates are similar in magnitude to those in Card (2001) , which finds returns of 9.7 to 13.2 percent per year of 4-year college, and in the middle range of the literature."}, {"section_title": "54", "text": "The benefit of attending a college that is 1 standard deviation higher in quality (in terms of SAT score) is estimated to be 4.8 percent (Long 2010 ) and 3.7 percent (Black and Smith 2006) . I compute as the probability of attaining each outcome (entering the labor market, completing two-year college, or completing four-year college) multiplied by the discounted lifetime return of each. I assume, conservatively, that students who are induced to attend a four-year college would otherwise have attended a two-year college, and use the more conservative estimates of the returns for each level of educational attainment. The average high school graduate is predicted to earn approximately $1.3 million in 2009 dollars during their lifetime. 55 Under these assumptions, the average student in the top tercile of the distribution who is induced to take the SAT by a new center would have an option value of taking the exam of more than $200,000. Those induced from the middle and bottom would have an option value of about $125,000 and $50,000, respectively. Clearly these values require a number of very strong assumptions and are highly sensitive to the choice of returns and lifetime income. However, they highlight two important points. First, the potential benefits of policies that induce high aptitude students to take an admissions exam are quite large relative to the likely cost of making college entrance exams more accessible. Second, switching from not taking to taking the exam in response to center access requires a nudge-based explanation rather than one based on traditional models of the human capital decision."}, {"section_title": "56", "text": ""}, {"section_title": "VI. Conclusion", "text": "There are significant differences in access to college admissions exams across high schools. This study finds compelling evidence that variation in access due to the locations of testing centers and in-school administration policies affect students' college outcomes. The results are consistent with a growing literature that documents the potential of educational nudges. This nudge, in particular, has two desirable characteristics: it is low cost, and it appears to induce the \"right\" students to attend college. Specifically, the results suggest that the marginal students induced to 54 Monk-Turner (1994); Kane and Rouse (1995); and Behrman, Rosenzweig, and Taubman (1996) present alternative estimates and approaches for determining the returns to college attainment. 55 See, Julian and Kominski (2011) for census estimates and Carnevale, Rose, and Cheah (2011) for estimates from the Georgetown University Center on Education and the Workforce.\n56 Thaler and Sunstein (2003) argue that providing a nudge, or libertarian paternalism, is \"an approach that preserves freedom of choice but that authorizes both private and public institutions to steer people in directions that will promote their welfare.\" attend college are overwhelmingly those with the highest aptitude. Thus, admissions exams policies stand in contrast to more traditional policies that reduce the price of attendance equally for all potential students (e.g., grants, loans, and tax credits)."}, {"section_title": "57", "text": "A natural question is whether it is more desirable to improve access by opening more testing centers or implementing more policies with free in-school administration and default or mandatory registration. The cost of offering the exam for free in school is relatively high, as many of the students who take it will be inframarginal (either because they are always-takers or because they will not attend college even if they do take the exam). The cost of hosting a testing center is comparatively low, as students pay their own exam fees. Further, this paper finds that the benefits of expanding access rapidly diminish as more inframarginal takers are induced. Thus, the return on investment is certainly higher for testing centers than for in-school policies. However, given the high estimates of the lifetime returns to attending college in the literature, almost any measure that gives weight to these returns will favor the policy that induces the most students to attend (i.e., mandatory in-school administration). In the face of limited resources, the results suggest that hosting a center and inducing the highest ability students to take it may generate gains close to those of more comprehensive policies."}]