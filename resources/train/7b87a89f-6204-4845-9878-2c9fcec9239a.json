[{"section_title": "Semiparametric Small Domain Estimation", "text": "\n12 5 Application to Salary Estimation for the NSRCG In this section, we illustrate our DPM approach by analyzing salary data from the 2008 National Survey of Recent College Graduates (NSRCG). We cannot, of course, know the \"ground truth\", but our results suggest a mixed (multi-modal) distribution of the domain-level means, implying that higher credence could be placed in the results from the DPM model as demonstrated in Section 4. "}, {"section_title": "1 Introduction", "text": "Many government agencies administer large-scale sample surveys to study various population attributes of interest, for which they typically represent large geographical areas, such as the entire country, or large domains, such as country's male population. Subject to cost constraints, these agencies construct survey designs such that the allocated survey samples yield the estimates with desired accuracies. Often, however, when the focus is on small geographical regions or population subdomains, these direct design-based estimates become unreliable, primarily due to small sample sizes. When those instances occur, survey analysts rely on small area estimation methodologies that \"borrow strength\" via explicit or implicit model-based approaches to optimally estimate the desired population attributes, such as the area means and the corresponding uncertainties. Furthermore, those model-based methods often incorporate supplementary data, like administrative records and other surveys, (for example, the American Community Survey (ACS)) to increase the reliability of the estimators. For a comprehensive review of the small area estimation literature, see Jiang and Lahiri (2001) or Rao and Molina (2015). As illustrated in Jiang (2007), consider the estimation of true domain-level means, \u03b8 i , where i is a domain index, for a continuous characteristic of population members-for instance, salary, as in Section 5. In this paper, we use the terms \"domain\" and \"area\" interchangeably. As discussed in more detail in Section 2, \u03b8 i can be estimated directly using model-based methods even though they are defined from individual-level information. Within this context, one widely used model is the Fay-Herriot (FH) model (Fay and Herriot 1979;You 2008), which assumes normal distributions for both the domain-level direct estimates,\u03b8 i , and their corresponding true means, \u03b8 i . While the central limit theorem is often used as a justification for the distribution of\u03b8 i (Rao 2003), making the same assertion for \u03b8 i is often problematic and difficult to verify (Sinharay and Stern 2003). Moreover, misspecification of their distribution can lead to undesired consequences for the estimators, such as incorrect posterior distribution convergence, bias in estimates, and poor posterior variance estimates. When the assumption of normality for \u03b8 i is not tenable, other parametric models have been proposed. For example, to capture the outliers, Bell and Huang (2006) considered the t-distribution, Semiparametric Small Domain Estimation 3 while Farrell et al. (1994) considered Laplace priors for their model. More recently, Liu (2009) considered the use of the exponential power distribution. To use asymmetric distributions for \u03b8 i , Fabrizi and Trivisano (2010) used a skewed exponential power distribution and Diallo and Rao (2014) used skewed normal distributions. In this paper we propose a semiparametric model with nonparametric specification, based on a Dirichlet process prior, for the distribution of the \u03b8 i . As demonstrated in Section 4, this model can successfully be applied when the assumed distribution for \u03b8 i deviates from the normal distribution, for instance, by being heavy-tailed, asymmetric, or mixed. Moreover, the model performs comparably to the FH model even when \u03b8 i are normally distributed, illustrating the robustness of our estimators with respect to the distribution. We pay special attention to not only the point estimates from the models but also their empirical distribution over the domains. To this end, we compare the empirical distributions between the ensemble of posterior estimates from our method to the corresponding estimates from the standard FH model (Fabrizi and Trivisano 2010). The paper is organized as follows. Section 2 introduces the FH model and discusses its previous extensions. Section 3 describes our proposed approach with the Dirichlet process mixture (DPM) model. In Section 4, we use simulations to compare between the DPM model with the standard FH model under a variety of distributions for \u03b8 i . In Section 5, we apply both the FH model and our DPM model to the 2008 National Survey of Recent College Graduates (NSRCG), which was conducted by the National Center for Science and Engineering Statistics at the U.S. National Science Foundation. We compare the domain-level salaries between the direct estimates, posterior means from the FH model, and those from our approach. Among our findings, the NSRCG salary data exhibit a multimodality captured by the DPM model but not by the FH model. Section 6 contains conclusions and some paths for further research."}, {"section_title": "Previous Model-based Estimation Methods", "text": "In this section, we describe the FH model, which was originally developed to obtain modelbased county-level income estimates where the housing data and the tax records were used as the Semiparametric Small Domain Estimation 4 area-level supplementary information. Recently, the U.S. Census Bureau used the FH model that incorporates data from the ACS and the Current Population Survey (CPS) to produce the estimates for the numbers of school-age children in poverty in each state and its counties. See http:// www.census.gov/did/www/saipe/. Let y i j be the response, assumed numerical (in Section 5, the response is salary), of unit/person j in domain i, i = 1, . . . , m. Domains can be defined in terms of geography, demography, or even a cross-classification of several variables. We wish to estimate the true domain means where N i is the population size in domain i. The direct design-based (or Horvitz-Thompson) estimator is defined as\u03b8 i = where w i j is the survey weight for the unit j, s i denotes the set of sampled units, and n i is the number of sampled units in corresponding domain i. For inference regarding \u03b8 i , Fay and Herriot (1979) introduced the following two-level hierarchical model: Level 2 (Prior model) : where the \u03bd i represent domain-level random effects that account for heterogeneity among domains. The variance component in (1), \u03c8 i , is called sampling variance and assumed to be known. The domain-level covariates are denoted as x i where they are typically drawn from external sources. The hyperparameters \u03b2 and \u03c3 2 \u03c4 are called model parameters, which are treated as unknowns and they are usually assigned non-informative or weak prior distributions in a hierarchical Bayesian setting. In practice, when the domain-level covariates are not available (Sinharay and Stern 2003), the Prior model in (2) reduces to where it is often referred to as an unbalanced one-way analysis of variance (Jones and Spiegelhalter 2011). In a Bayesian setting, once we appropriately specify the distributions for model parameters , it becomes relatively straightforward to obtain the posterior distribution of \u03b8 i , f (\u03b8 i |\u03b8), via Gibbs sampling in a Markov chain Monte Carlo (MCMC) algorithm (You et al. 2003;Gelman et al. 2004). Unlike the Sampling model in equation 1, where appeal to the central limit theorem can be warranted from the superpopulation perspective, the Prior model in (2) or in (3) is challenging to justify. To this end, there have been some efforts, especially for the Prior model to guard against using a single parametric distribution or having a normality assumption. For example, Maiti (2001) applied a finite mixture of normal distributions via hierarchical modeling and Articus and Burgard (2014) used the EM algorithm to obtain the inference for \u03b8 i . 3 Dirichlet Process Extension of the Fay-Herriot Model"}, {"section_title": "The Dirichlet Process", "text": "A random probability distribution G is called a Dirichlet process (DP) with base distribution G 0 and concentration parameter \u03b1 on a space S if for every partition where Dir denotes the Dirichlet distribution. We denote this by G \u223c DP(\u03b1, G 0 ). The larger \u03b1, the more G resembles G 0 ; in all cases, E(G) = G 0 . In a Bayesian setting, we can model uncertainty in \u03b8 by assuming \u03b8|G iid \u223c G, where G \u223c DP(\u03b1, G 0 ), which we call the DP prior. For computational efficiency, one can use a constructive representation of the DP, referred to as the stick-breaking representation (Sethuramen 1994), given by where \u03b4 \u03b8 (\u2022) is the point mass at \u03b8 . In practice, the infinite sum in (5) is replaced by the finite sum, with a large value of K such that \u2211 \u221e k=K+1 \u03c0 k has a distribution concentrated near zero. Ishwaran and James (2001) suggest the truncation approximation to DP such that The truncation is helpful to decrease the computational burden of the MCMC implemented for inference."}, {"section_title": "Our Approach Using the DP prior", "text": "Our model extends the FH model by relaxing the parametric distributional assumption for \u03b8 i . Specifically, for the Prior model in (3), we use a Dirichlet process mixture (DPM), which has been widely used in Bayesian analysis (Escobar 1994;Muller et al. 1996). See Escobar and West (1995), Kim et al. (2014), and Kim et al. (2015) for more details. To recapitulate notation, let (\u03b8 1 , . . . , \u03b8 m ) be the true domain-level means and (\u03b8 1 , . . . ,\u03b8 m ) be the direct, design-based estimates, where m is the total number of domains. In our approach, we assume that each domain i belongs to one of K latent mixture components where z i \u2208 {1, . . . , K} denote the component index for domain i. The DPM model, then, comprises into three levels, such that: Level 2 (Prior model I) : Level 3 (Prior model II) : Semiparametric Small Domain Estimation 7 where (\u03c0 1 , . . . , \u03c0 K ) are interpreted as component weights for the latent class k and follow the truncated DP in (6). Marginalized over z i , the prior model of \u03b8 i reduces to We use the conjugate prior for \u00b5, \u03c4 2 given by where IG denotes the inverse Gamma distribution. Following Wang and Dunson (2011) and Kim et al. (2014), we simply chose the value of K large enough (K = 25 for simulations in Section 4 and for data analysis in Section 5) to include all relevant possibilities. In contrast to Ohlssen et al. (2007), where a prior distribution is assigned, we use a fixed value for \u03b1 = 1. For the other hyperparameters, we set fixed values as a \u03c4 = 2 and b \u03c4 = 1, which can be interpreted as small prior sample sizes and h 0 = 0.1. With these specifications, we can explicitly derive the posterior distributions for \u03b8 i by the MCMC using a Gibbs sampler. See Appendix A for detail."}, {"section_title": "Simulation Study", "text": ""}, {"section_title": "Simulation Structure", "text": "We compare the performance of our proposed DPM model and the FH model via simulation studies in which the direct estimates,\u03b8 i , follow the Sampling model in (1), but the true domain-level means, \u03b8 i , may deviate from the normal distribution in (3). Specifically, we consider the following four cases in which the distributions for \u03b8 i are defined as: Case 2: Heavy-tailed distribution. \u03b8 i iid \u223c T 2 , a t-distribution with 2 degrees of freedom. We separately generate a set of sampling variances \u03c8 i \u223c \u0393(shape = 6, rate = 2) to apply in each case. The direct estimates,\u03b8 i , are then generated from\u03b8 i ind \u223c N(\u03b8 i , \u03c8 i ), i = 1, . . . , m, and m = 500. We simulated a total of 100 sets of replicated samples for this study. To implement the DPM approach, we follow the model and the prior distributions described in Section 3.2 and Appendix A. On the other hand, for the FH model we use weak priors for the hyperparameters, \u00b5 \u223c N(0, 1000), \u03c3 \u03c4 \u223c Uni f (0, \u221e) suggested by Gelman (2006). For both methods, we used 10,000-iteration MCMC runs with 5,000-iteration burn-in periods with 4 thinning steps."}, {"section_title": "Simulation Results: One Replication", "text": "We first compare between our proposed DPM approach and the FH model based on one replicate sample. Specifically, by plotting them, we examine samples from each of their posterior distributions, f (\u03b8 i |\u03b8), against the true distribution for the four cases listed in Section 4.1. In Figure 1, the results from the FH models are in the left-hand panels, and those from the DPM models are in the right-hand panels, where we use kernel density estimates to represent the distributions from each posterior sample with gray curves and the true distribution with a black curve. In Case 1, estimated densities of the posterior samples from the FH model are, unsurprisingly, closer to the true density than those from the DPM approach. However, those from the DPM approach also locate the true normal distribution of \u03b8 i , albeit with larger variation. In Case 2, the estimated densities from the FH model are overly dispersed and thus fail to capture the true underlying density. On the other hand, those densities from the DPM approach capture the true density reasonably well, although they may seem too concentrated around the mean. At first glance for Case 3, the FH may seem to perform slightly better than the DPM approach since the density estimates capture the true density reasonably well. However, while the posterior draws from the FH model seem symmetric and lack the skewness, those from the DPM model appear to be more skewed to the left but yet only just capture the true distribution. In Case 4, the estimated densities from the FH model overly smooth the variation of the true distribution. Moreover, they perform very poorly due to the substantial distributional deviation from the true distribution for \u03b8 i . On the other hand, "}, {"section_title": "Simulation Results: Multiple Replications", "text": "Based on our analysis in the previous section, we have shown that our DPM approach is more effective when underlying true distribution deviates from a normal distribution. In this section, we extend our analysis by comparing the domain-level estimates based on the posterior means,\u03b8 * i = E(\u03b8 i |\u03b8), from each model and their ensembles {\u03b8 * i } m i=1 by using the following summary measures on multiple replicates."}, {"section_title": "Root Average Squared Bias, (RASB)", "text": "which is an aggregate of differences between the posterior mean,\u03b8 * i , from each model of each simulated replicate and the true domain-level mean \u03b8 i ."}, {"section_title": "Root Integrated Squared Error Loss, (RISEL)", "text": "Louis 1998), where F n (t) is the empirical distribution function (EDF) of the en- andF * n (t) is the corresponding EDF for the estimators,\u03b8 * = {\u03b8 * i } m i=1 , from each replicate. RISEL measures the average difference between two empirical distributions.  which measures the maximum distance between the EDFs from the estimators and the true domain-level means. Table 1 shows the averages of each summary statistic; the numbers in the parenthesis represent the 5% and 95% quantiles over 100 replicates. For Case 1, the DPM and FH results are essentially identical, showing that the DPM model suffers no performance degradation when \u03b8 i are in fact normally distributed, i.e., when the assumptions for underlying model of the FH are satisfied. For Case 2, the DPM approach outperforms the FH model rather dramatically for all three measures, especially for RISEL and K-S. For Case 3, DPM appears to be marginally superior to FH in terms of means, but it is more so for RISEL and K-S than for RASB. This is expected from the result in the previous section since the DPM approach is more successful at capturing the skewness of the distribution. Additionally, the DPM approach yields shorter inter-quantile distances than FH, suggesting that its estimates show less variation. Lastly in Case 4, there is a factor-of-two difference in RASB, a 50% difference in RISEL and approximately a 20% difference in K-S, all in favor of the DPM although the DPM produced wider inter-quantile distances."}, {"section_title": "The NSRCG", "text": ""}, {"section_title": "Analysis of Salaries", "text": "Building on the work of Carrillo and Karr (2013) regarding estimation of salaries of Ph.D. recipients using data from NCSES Survey of Doctorate Recipients, we restrict the data to respondents who are employed full time, work more than 35 weeks, and report salary income exceeding $5,000 per year. The reduced data set contains approximately 8,300 respondents. With these criteria, we estimate mean salaries for small domains constituting a cross-classification of five variables-gender, race, degree level, field of degree, and Carnegie code of the degreegranting institution. Although more detail is available in the data, these variables were re-coded to the levels shown in Table 2. When fully crossed, the total number from the five variables resulted in 190 domains since two of these domains are empty in our case. The distribution of the sample sizes, on the square-root scale, is shown in Figure 2, which shows that more than one-half of the domains have sample sizes less than 19.  The population domain-level means are given by where N i is the population size in domain i. The direct design-based, or Horvitz-Thompson estima- where s i is the set of sampled individuals and w i j is the survey weight for person j in domain i. The weights reflect the complex sample design in the NSRCG. Direct use of design-based sampling variance, \u03c8 i , for all domains is not a viable option due to the small sample sizes. Thus, we apply an adjustment method similar to that in Ha et al. (2014). For a detailed description of our method, see Appendix B."}, {"section_title": "Result for Domain-level Salaries", "text": "The complete set of domain-level direct design-based, FH, and DPM estimates appears in the table in Appendix D. Here we compare the results in terms of five domains with the highest and  lowest estimated salaries, which appear in Table 3. This table shows that the posterior means from the FH and the DPM models are quite similar for most domains, which is expected from the posterior model checking with the Bayesian p-value in the Appendix C. Evidently, the choice of prior distribution did not make a meaningful difference when we examined only the posterior means from both models. Figure 3 shows the estimated densities of\u03b8 i 's from all three (DPM, FH, and design-based) estimates. As expected, there is a large difference between the design-based estimates and the FH and DPM estimates. However, there is much less difference between posterior estimates from the FH and the DPM models overall while the distributional difference of posterior means for the FH and the DPM model are more apparent when the salary estimates are between $35,000 and $45,000. Table 4 shows the groups and posterior means for each model. However, when we examined estimates that fall within that range, we found no systematic patterns of differences for the domains. "}, {"section_title": "Distribution of Subdomain Salaries", "text": "Finally, we compare the models in terms of how they differentiate salaries by type of degree.   We observe substantial differences between distributions from the DPM and FH models when their shapes are compared. For bachelor's degrees (upper plots in Figure 5), the posterior distribution from the DPM model clearly shows a multi-modality, whereas the posterior distribution from the FH model does not, i.e., it is flatter and with no clear multi-modality. For graduate degrees (lower plots in Figure 5), the results are similar: the posterior density estimates of the DPM model demonstrate a mixture of distributions with different modes, whereas those of the FH model appears smoother in comparison. Based on our simulation results in Section 4, we believe that the flexible distributional assumption of the DPM approach allows it to conform better to the true underlying distribution, which appears to be multi-modal."}, {"section_title": "Concluding Remarks", "text": "In this paper, we have introduced an extension of the classical FH model by employing a DP prior for the distribution of the true domain-level means. Using simulation studies, we demonstrated that our approach with the DPM model outperforms the FH model when the true distribution deviates from normality-heavy tails, skewness and mixtures, and with no loss of performance when the normality assumption is satisfied. For the 2008 NSRCG, the DPM model and the FH model were both applied to estimate the salaries for 190 domains comprising from a full cross-classification of gender, race, degree-level, field of degree, and Carnegie classification. The differences between the estimates produced by two models are subtle but meaningful. Based on our findings from the simulations studies, we claim that the underlying distribution for domain-level salary averages may exhibit a multi-modality and Similar to other model-based estimation methods, our approach is still subject to possible model failure. One possible solution could be using a benchmarking method similar to that in Datta et al. (2009). Benchmarking is a popular method in small area estimation because the method provides techniques such that the sum of the estimates for small domains becomes equivalent to that of the corresponding larger domains. Our analysis focused on the intercept-only model for the prior distribution since the NSRCG does not contain relevant covariates for the domains of our interest. However, for many cases in small area estimation problems, there are available domain/area-level covariates, and our DPM Semiparametric Small Domain Estimation 20 approach can be extended to those general cases. Step 2. For each k = 1, . . . , K, draw \u03c3 2 k \u223c IG(a * k , b * k ), and then draw \u00b5 k \u223c N \u00b5 * k , \u03c3 2 k /h * k , where . Step 3. For each k = 1, . . . , K \u2212 1, draw v k \u223c Beta 1 + n k , \u03b1 + \u2211 g>k n g and let v K = 1. Then calculate the mixture component weights Step 4. , where for k = 1, . . . , K."}, {"section_title": "Appendix B Sampling Variance Estimation", "text": "In the Fay-Herriot model, the sampling variances \u03c8 i are assumed to be known; however, in practice their estimated values are used. There are two commonly used methods to obtain estimated sampling variances. The first method uses a jackknife replication method to develop replicate weights (Fay and Train 1995), and this method requires construction of replicate subsamples by using the survey design information, such as strata or the primary sampling units, (Shao 1996). The second method employs the Generalized Variance Function (GVF, Wolter 1985), for approximating variances. The GVF method is a model-based method in which the model describes the relationship between the relative variance of a survey estimator and its expectation. For our variance estimates, we primarily used the jackknife method, based on replicate weights available in the NSRCG datasets. However, due to the small sample sizes in many domains, some of the variance estimates were either undefined or small. Especially, the design effects (ratios of the variance under the survey design and variance under simple random sampling) for those domains were less than one, which is uncommon under complex survey designs. Thus, we have made an adjustment in the following manner, with assumptions similar to those in Ha et al. 2014. First, we examined the variance estimates at larger domains defined by gender \u00d7 race \u00d7 degree level. We made a conservative assumption, for those subdomains with design effect less than one, that the variation for all subdomains would be similar to that of the corresponding larger domains. Let \u03c8 i be a variance estimate for small domain i and \u03c8 j be the corresponding variance estimate where the domain i is a subdomain within the domain j. Then, we assume that where n i and n j are sample sizes for domains i and j. Finally, we replaced the \u03c8 i with \u03c8 * i as defined by for those domains in which the design effect was less than one. "}, {"section_title": "Appendix C Model Fit via Posterior Predictive Model Checking", "text": "where \u03c8 i denotes the sampling variance for domain i. By using this discrepancy measure, D, the posterior predictive p\u2212value is defined as Using the posterior simulated samples from the Gibbs sampling, the posterior predictive p\u2212value REFERENCES 26 (14) can be approximated easily. For each iteration and simulated value\u03b8 , we can draw\u03b8 * , , and consequently compute D(\u03b8 obs ,\u03b8 ) and D(\u03b8 * , ,\u03b8 ). Then equation 14can be approximated as: "}]