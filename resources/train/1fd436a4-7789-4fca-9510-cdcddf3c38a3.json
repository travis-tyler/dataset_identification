[{"section_title": "M3d-CAM Overview", "text": "M3d-CAM is an easy to use library for generating attention maps with any CNN-based Pytorch [1] model both for 2D and 3D data as well as with classification and segmentation tasks. M3d-CAM works by injecting itself into a given model appending and even replacing certain functions of the model. The model itself will work as usual and its predictions remain untouched ensuring that no code is broken. M3d-CAM itself will work behind the scenes and generate attention maps every time model.forward is called. Examples of these attention maps are shown in figure  1 . The most important functions of M3d-CAM are explained in the following subsections. "}, {"section_title": "Injection", "text": "To inject a model with M3d-CAM one simply needs to insert the line model = medcam.inject(model) after model initialization as shown in code example 1. This will add all the necessary functionality to the model. Additionally inject offers multiple parameters that can be adjusted. As an example one can define an output_dir and set save_maps=True to save every generated attention map. One can also set a desired backend which is used for generating the attention maps such as Grad-CAM. These backends are explained in more detail in section 2. Furthermore, it is possible to choose the layer of interest with layer . Hereby one can specifically define a single layer, a set of layers, every layer with full or the highest CNN-layer with auto for the most comfort. 1 # Import M3d-CAM 2 from medcam import medcam "}, {"section_title": "Layer retrieval", "text": "As the layer names of a model are often unknown to the user, M3d-CAM offers the method medcam.get_layers(model) for quickly acquiring every layer name of a model. However it needs to be noted that attention maps can not be generated for every type of layer. This is true for layer types such as fully connected, bounding box or other special types of layers. The attention for theses layers can be computed but it is not possible to project them back to the original input data, hence no attention maps can be generated."}, {"section_title": "Evaluation", "text": "M3d-CAM also supports the evaluation of attention maps with given ground truth masks by simply calling model.forward(input, mask) including the mask in the forward call. The attention map is then internally evaluated by the medcam.Evaluator class with a predefined metric by the user. Alternatively one can call the medcam.Evaluator class directly. By calling model.dump() or respectively medcam.Evaluator.dump() the evaluation results are saved as an excel table.\nM3d-CAM supports multiple methods for generating the attention maps. For simplicity we will refer to them as backends. For a better understanding of how these attention maps look like we included examples for every backend. The original input images are shown in figure 2. The first image displays a chest X-Ray used on the task of classification by employing a CovidNet [2] , the second a lung CT slice on the task of 2D segmentation by employing an Inf-Net [3] and the third a 3D prostate CT image on the task of 3D segmentation by employing a nnUNet [4] . Figure 2 : From left to right: A chest X-Ray from the COVID-19 image data collection [5] , a lung CT slice also from [5] and 3D prostate CT image from the Medical Decathlon dataset [6] 2.1 Grad-CAM Grad-CAM [7] works by first propagating the input through the entire model. In a second step a desired class in the output is isolated by setting every other class to zero. The output of this isolated class is then backpropagated through the model up to the desired layer. Here the layer gradients are extracted and together with the feature maps of the same layer the attention map is computed. The result is a heatmap-like image of the attention at the desired layer as shown in figure 3 . The approach of generating an attention map from a specific preferably high layer gives a good compromise between high-level semantics and detailed spatial information. Furthermore, by isolating a specific class Grad-CAM becomes class discriminant. "}, {"section_title": "Guided Backpropagation", "text": "Guided Backpropagation was first introduced in [8] and works by first propagating the input through the entire model similar to Grad-CAM. In a second step the output is then backpropagated through the entire model. However only the non-negative gradients are passed to the next layer as negative gradients correspond to suppressed pixels deemed not relevant by the authors. The result is a noise-like image depicting the model attention as shown in figure 4. The advantage of Guided Backpropagation is that the attention is pixel-precise. The downsides are that it is neither class nor layer discriminant. "}, {"section_title": "Guided Grad-CAM", "text": "Another backend presented in [7] is Guided Grad-CAM which is a combination of Guided Backpropagation and Grad-CAM in an effort to combine the best of both approaches. When generating attention maps with both backends the resulting attention maps can be combined through simply multiplying them element-wise. The result is a noise-like class and layer discriminant pixel-precise attention map as shown in figure 5 . The only downside of Guided Grad-CAM is the need of performing backpropagation two times. "}, {"section_title": "Grad-CAM++", "text": "Grad-CAM++ is an extension of Grad-CAM introduced in [9] . It differs to vanilla Grad-CAM in that it weights the gradients before combining them with the feature maps resulting in more precise attention maps, especially when dealing with multiple instances of the same class in an image according to the authors. Examples of these attention maps are shown in figure 6. "}]