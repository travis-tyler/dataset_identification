[{"section_title": "Abstract", "text": "Abstract Recent advances in high-performance computing technologies are enabling multiple climate modeling groups to perform global multi-decadal simulations at tropical cyclone-permitting resolutions. This chapter discusses the developing state of the art of such high-resolution modeling. These global atmospheric models, with horizontal resolutions in the 10-50 km range, simulate strong gradients in temperature and moisture far more realistically than contemporary mainstream climate models at coarser resolution. With these models, simulated tropical cyclones exhibit a surprising degree of realism in terms of both the physical characteristics of individual storms and their long-term statistical behavior. Experience with the Community Atmospheric Model version 5 is used as an example to demonstrate the strengths and weaknesses of this new class of climate models."}, {"section_title": "Introduction", "text": "The numerical simulation of tropical cyclones has a rich history, and its roots can be traced back to some of the earliest studies in atmospheric models with the successful implementation of convective parameterizations (Kuo 1965; Ooyama 1969) . The use of atmospheric models for decadal simulations of tropical cyclones (TCs) has been in practice for well over two decades. The first study to utilize a general circulation model (GCM) to perform decade-long simulations of tropical cyclones was produced by Broccoli and Manabe (1990) and consisted of experiments at two horizontal resolutions of roughly 500 and 200 km. The simulations resulted in a reasonable global climatology of tropical storm-like features compared to the observed climatology; however, the regional distribution lacked skill. The Broccoli and Manabe (1990) study was also the first to perform simulations with increased greenhouse gas concentrations to quantify how tropical cyclones may change in an altered climate. This work was expanded upon at finer grid spacings of approximately 125 km with noted improvement in storm climatology (Bengtsson et al. 1995 (Bengtsson et al. , 1996 . Numerous studies with various models at grid spacing greater than 100 km followed these initial studies and a more complete summary can be found in Walsh (2008) .\nIn addition to these early GCM simulations, limited-area models have also been used in recent decades for decadal projections of tropical cyclone statistics in present-day and future climates for specific ocean basins. These downscaling studies have often utilized grid spacings as fine as approximately 20 km. Examples of experiments that use limited-area models with prescribed sea surface temperatures to investigate cyclone statistics for the North Atlantic are Knutson et al. (2008) and Bender et al. (2010) . For such studies, the large-scale atmospheric conditions at the lateral domain boundaries, such as temperature, water vapor, and wind velocities, are often derived from low-resolution coupled GCM climatologies or reanalysis data. Furthermore, coupled-atmosphere-ocean configurations of such models (e.g., Knutson et al. 2001 ) have been utilized to explore the impact of ocean coupling on tropical cyclones.\nWhile the use of limited-area models to simulate tropical cyclone statistics is reasonably well established, the forcing from lateral boundary conditions can have a strong impact on the climatology of the atmospheric state within limited-area models. Errors in these boundary conditions can have profound effects on tropical cyclogenesis due to biases in wind shear, steering flow, sea surface temperature, and atmospheric thermodynamics which can lead to errors in storm track density and pattern. Hence, direct downscaling of current generation coupled climate models such as those in the database of the Coupled Model Intercomparison Project version 5 (CMIP5) is subject to larger errors in tropical cyclone statistics that are downscalings of observationally constrained reanalysis products.\nThe usage of GCMs for simulating tropical cyclones has been limited by insufficient horizontal and vertical grid spacings, which are limited by computational resources. Despite these limitations, conventional GCMs have demonstrated the ability to produce tropical cyclones, even at coarse horizontal resolutions on the order of 100 km Wehner et al. 2014) . However, tropical cyclones simulated in GCMs at these resolutions are of much weaker intensity and larger size than observed storms (Walsh 2008) . Because of the reduced intensities, the number of trackable storms in coarse resolution GCMs is generally substantially lower than observed.\nRecent advancements in computer architectures currently permit multi-decadal high-resolution GCM simulations with grid spacings in the 10-50 km range. However, such models still face many challenges in accurately representing both the physical and statistical behavior of tropical cyclones. The storm size and count, the representation of the intense convection, and the interplay of large-scale and small-scale processes have not been demonstrated to have converged. Subgrid-scale parameterizations have generally been adopted from the tuned coarser models and are usually not aware of the change in scale. Nonetheless, comparison of the tropical cyclone statistics between this class of global simulations with observations of the last two to three decades show a remarkable degree of realism Reed et al. 2015; Roberts et al. 2015; Strachan et al. 2013; Wehner et al. 2014; Zhao et al. 2009 ). For climate change experiments, the impact of increased carbon dioxide (and other greenhouse gases) and/or increased, prescribed sea surface temperatures (SSTs) has been investigated. Recent examples of such studies include Oouchi et al. (2006) , Zhao et al. (2009 ), Sugi et al. (2009 ), Wehner et al. (2010 , Murakami and Sugi (2010) , Zhao (2011), Murakami et al. (2012) , Villarini et al. (2014) , Scoccimarro et al. (2014) , Lin et al. (2015) , and Wehner et al. (2015) .\nIn addition to advances in uniform high-resolution GCMs, recent progress has also been made in techniques permitting local mesh refinement within GCMs. Mesh refinement techniques allow for limited regions of high resolution within the global domain. These refinement areas can be tailored to particular research questions and, therefore, offer an attractive approach to simulating regional tropical cyclone activity in GCMs. There are two main types of mesh refinement used in models. Static mesh refinement techniques, where a multi-resolution grid is fixed at initialization and remains the same for the entirety of the model run, have been included in GCM frameworks for tropical cyclone investigations (Caron et al. 2011; Chauvin et al. 2006) . A more recent example of such an approach is Zarzycki and Jablonowski (2014) which utilized an unstructured variable resolution mesh of approximately 25 km over the North Atlantic basin. An alternative technique is adaptive mesh refinement, where a grid dynamically refines/coarsens based on a particular features being present in the model. For example, a certain vorticity threshold in tropical regions within a 100 km simulation may trigger an automatic refinement to 25 km to better resolve cyclogenesis and the corresponding storm lifetime. Challenges such as multi-scale subgrid physical parameterizations, dynamic computational load balancing in massively parallel simulations, and the development of suitable refinement threshold criteria for cyclogenesis are areas of ongoing research before adaptive techniques could be a viable option for long-term simulations of tropical cyclones in climate models."}, {"section_title": "Uniform High-Resolution Global Atmospheric Modeling", "text": "The Community Atmospheric Model version 5 (CAM5) is typical of the global atmospheric component sub-models in the broader class of coupled general circulation models found in the CMIP5 database. The public-release version of the model is supported at a horizontal resolution of approximately 100 km at the equator and 30 vertical levels extending into the lower stratosphere Neale et al. 2012) . Increases in the horizontal resolution to an approximate grid spacing of 25 km using the finite volume (FV) dynamical core with minimal changes to the physical parameterizations have shown marked improvements to the realism of individual storms, including tropical cyclones Wehner et al. 2014) . These improvements are most apparent in the sharper gradients of moisture and temperature in the higher-resolution version of the model when compared to lower-resolution models. As a result, many characteristics of extreme storms compare favorably with available observations. Depending on the details of the storm-tracking algorithm, the global number of simulated tropical cyclones in multi-decadal integrations can match present-day climatology. This high-resolution version of CAM5-FV even produces some intense hurricanes (Categories 4 and 5), although not all other current models at this resolution can also do so. The lowresolution (100 km) public-release configuration is typical of CMIP5-class climate models and is revealed to produce far too few tropical cyclones. Relaxation of tracking algorithm thresholds can increase the storm counts, albeit with less physical realism, but not to a significant fraction of the observed values .\nDespite the good performance in simulating the total number of tropical cyclones with the 25 km version of CAM5-FV, the simulated spatial distribution has some significant biases. Figure 8 .1 shows the tropical cyclone track density \u0131 radius of a given point per year. Note that the color scale differs for each row of the high-resolution version for CAM5-FV compared to observations. Note that the observations, the International Best Track Archive for Climate Stewardship (IBTrACS, Knapp et al. 2010) , are shown over the same time period as the CAM5 simulations. The largest discrepancy in the high-resolution model is in the central Pacific with the model generating far too many storms there than are observed, as well as the western Pacific where CAM5-FV produced too few storms, although the total number of simulated Pacific storms is reasonable. Such errors are likely the result of biases in the large-scale circulation simulated by the climate model. Each tropical cyclone-permitting global climate model has its own unique set of large-scale errors. Increases in horizontal resolution alone do not generally reduce these errors in regions without significant local orography Wehner et al. 2014) . Rather, such errors are more likely the results of deficiencies in subgrid-scale parameterizations. As most modeling groups have tuned these parameterizations of unresolved processes to coarser resolutions, errors in large-scale circulation features can actually be greater at the higher resolution necessary for tropical cyclogenesis.\nRecent work by Reed et al. (2015) has suggested that the simulation of tropical cyclone climatology in CAM5 is also sensitive to the choice of dynamical core. The dynamical core is the main fluid flow component of an atmospheric model and is a discretization of the Navier-Stokes equations, the relevant equations of motion for the atmosphere. As the largest errors in simulated large-scale climatological fields are often traceable to errors arising from the subgrid-scale parameterizations, it is not obvious that the dynamical core should have such a large effect on simulated tropical cyclone properties. CAM5 permits the selection of three different dynamical core approaches and two different sets of physical parameterizations. Reed et al. (2015) showed that a spectral-element (SE) dynamical core produces stronger storms, resulting in more hurricanes and major hurricanes over a 26-year simulation than does the FV approach when using nearly identical physical parameterization packages. This is despite the fact that the CAM5-FV simulation produces a slightly more favorable environment for intense storms based on analysis of the large-scale climatology. Figure 8 .1 also displays the track density for the CAM5-SE simulation from Reed et al. (2015) and Bacmeister et al. (2016) . When comparing CAM5-SE to CAM5-FV, there is a reduction in the bias of track density in the west Pacific, but the bias in the central Pacific is worse. CAM5-SE also shows decreased tropical cyclone activity in the North Atlantic. This work demonstrates that internal uncertainties due to model design choices are not fully understood in the current generation of high-resolution simulations. It is worth noting that many previous studies have investigated the more commonly understood sensitivity of tropical cyclones to the choice of physical parameterizations (e.g., Bacmeister et al. 2014; Kim et al. 2012; Reed and Jablonowski 2011a,b; , which is not discussed here.\nTropical cyclone-permitting climate models are a critical tool toward developing a theoretical understanding of the effect of climate change on tropical cyclone statistics (Walsh et al. 2015) . The Hurricane Working Group (HWG) of the US Climate Variability and Predictability Research Program (CLIVAR) developed four idealized test configurations to explore the effects of increased sea surface temperature (SST) and increased atmospheric carbon dioxide concentrations both separately and jointly on future tropical storm behavior in warmer climates (http:// www.usclivar.org/working-groups/hurricane; Held and Zhao 2011). The warmer configurations imposed a uniform 2K increase to a base case 1990 SST climatology. Model performance in simulating the 1990 baseline case varied considerably. However, most of the models that contributed to the study, including the highresolution version of CAM5-FV, produced stronger storms in the tail of their tropical storm distributions as measured by wind speed and minimum central pressures. A likely mechanism for such behavior is that when an intense tropical cyclone occurs, the large-scale conditions of low wind shear, high sea surface temperature, and high humidity are ideal. In the warmer simulations, the average wind shear conditions in tropical cyclogenesis regions do not change much, but average values of sensible and latent heat are larger due to increases in temperature and humidity. Hence, when the conditions are ripe for an intense storm to occur, it becomes stronger due to the increase in available energy. The models were less conclusive in regard to the number of lower intensity storms (Walsh et al. 2015) . Most of the models, including the high-resolution version of CAM5 , produced significantly fewer storms in the uniform 2K warmer configurations. This reduction was demonstrated to be a result of both the warmer surface and elevated air temperature aloft (driven by the increased greenhouse gas concentration), with the latter being the larger contributor.\nThe US CLIVAR HWG test problems present an opportunity to test conceptual models of the response of tropical cyclones to changes in the relevant largescale climatological fields. The maximum potential intensity (MPI) index gives a bulk measure estimate of the highest possible wind speed and lowest possible central pressures of a \"perfect\" tropical cyclone modeled as a Carnot engine transporting energy from the ocean surface to the stratosphere (Emanuel 1987) . Using changes in the monthly averaged surface temperatures and pressures as well as the vertical profiles of air temperature and humidity from the high-resolution version of CAM5.1, changes in MPI correctly predicted that the most intense simulated storms had higher winds and lower central pressures in the warmer test problems . The magnitude of the MPI changes in maximum wind speed was also reasonable when compared to the changes in the ten most intense simulated storms in each configuration. The genesis potential index (GPI) uses the MPI together with vorticity, humidity, and wind shear to estimate the cyclogenesis density and has been tuned to observe values through reanalyses (Camargo et al. 2007; Emanuel and Nolan 2004) . However, this bulk measure of tropical storm activity fails to correctly predict the decrease in the number of storms in the high-resolution CAM5.1 US CLIVAR HWG test problems as SST is uniformly increased . A more sophisticated approach follows a downscaling approach designed by (Emanuel 2013) which \"seeds\" small-scale vorticity disturbances into the large-scale climatological conditions. This approach also predicts an increase in tropical cyclone activity (Walsh et al. 2015) . Similarly, the low-resolution version of CAM5.1 also fails to predict the sign of the response of its high-resolution counterpart. Since neither bulk measures of cyclogenesis nor direct tracking results in low-resolution models are guaranteed to faithfully replicate the behavior of models capable of actually producing hurricane category winds, confidence in projected future changes in tropical cyclone statistics derived from the low-resolution models in the CMIP5 database is undermined.\nFixed SST numerical experiments imply that the ocean is of infinite heat capacity. This is of course not true, and a cold wake of reduced SST is often observed behind large tropical cyclones due to the mixing of colder water at depth to the surface (Mei and Pasquero 2013; Price 1981) . This serves as a negative feedback on storm intensity (Cione and Uhlhorn 2003) . In addition to the effect on the tropical cyclone itself, this mixing also transports warmer surface waters to the subsurface thermocline (Li et al. 2015; McClean et al. 2011) . Fully coupled global oceanatmosphere models at tropical cyclone-permitting resolutions are at the very limit of existing computational technologies. The requisite multi-century spin-ups of such models demanded by the long time scales in the ocean are currently prohibitively computationally intensive, and experience is limited. Long-term biases in ocean sea surface temperatures may also deleteriously impact tropical cyclone climatology, as demonstrated by Small et al. (2014) using a 25 km version of CAM coupled to a prognostic ocean model. However, preliminary simulations reveal that the cold wake phenomena can be reproduced (Li et al. 2015; McClean et al. 2011) . Furthermore, the relaxation time of the cold wake back to \"normal\" temperatures was recently found to be sensitive to ocean model resolution, with mesoscale eddy-permitting configurations responding more quickly (Li et al. 2015) . As an alternative, long spin-up simulations can be avoided through the use of slab or mixed-layer ocean models, and early attempts to develop such configurations are ongoing (e.g., Hirons et al. 2015) ."}, {"section_title": "Variable-Resolution Global Modeling", "text": "Recent developments in numerical techniques have allowed for global simulations to be performed with regional refinement. Variable-resolution GCMs (VRGCMs) allow for targeted use of computing resources, as in regional climate or limited-area models, but do so within a global framework, allowing for a more physically and mathematically consistent treatment of the atmosphere in addition to eliminating the need for lateral boundary conditions to drive nested domains. Tropical cyclones are a natural fit for these frameworks as their spatial distribution is well defined, leading to obvious choices for refinement location. Storms are also generally restricted based on their genesis location, allowing for individual ocean basins to be easily isolated for regional studies, provided any nonlocal TC genesis precursor features remain adequately resolved in unrefined regions of the mesh (such as North Atlantic tropical waves and El Nino-Southern Oscillation (ENSO) teleconnections in Zarzycki and Jablonowski (2014) and ). Recently, a variable-resolution option (Zarzycki et al. 2014b ) has been implemented into the SE dynamical core of CAM5 (Dennis et al. 2012a; Taylor et al. 1997; Taylor 2011) , allowing for regionally refined simulations using the same framework as the uniform simulations discussed above. An example of tracked tropical cyclones from Zarzycki and Jablonowski (2014) is shown in Fig. 8.2 . The top panels show results from a multi-decadal Atmospheric Model Intercomparison Project (AMIP, Gates 1992) historical simulation on an unrefined, CMIP-class, 100 km grid. The bottom panels show results from an identical model setup except using a grid with 100 km grid spacing everywhere except over the North Atlantic, where the horizontal resolution is increased to 25 km. The impact of finer grid spacing is clearly highlighted in the trajectories shown on the right in Fig. 8.2 , with the regionally refined simulations able to achieve a significantly more realistic representation of both the spatial distribution and intensity of Atlantic storms. The fact that the bottom right panel is well matched to the results seen with the globally uniform simulations seen in Fig. 8 .1 (albeit a simulation using a different dynamical core, but with the same set of subgrid parameterizations), lends further credence to the use of VRGCMs as a tool for regional tropical cyclone assessments. While VRGCM development remains in the early stages, other studies using different models have found similar promising results with respect to tropical cyclones (Caron et al. 2011; Hashimoto et al. 2015) .\nThe variable-resolution mesh in Fig. 8 .2 (bottom) contains 13,340 elements, compared to 86,400 for a globally uniform mesh of the same resolution (25 km). Assuming a variable-resolution model that is able to scale with the number of grid cells (as CAM-SE has demonstrated (Zarzycki et al. 2014a )), regional tropical cyclone studies can be dramatically improved by decreasing the computational cost to simulate at a particular horizontal resolution. In the case above, the variableresolution simulation only cost one-sixth of the CPU hours of an equivalent simulation utilizing high-resolution over the entire global domain. For a fixed computing load, this opens up additional simulation enhancements, such as increased resolution or longer run-time. It also may allow for the addition of ensemble members, which has been shown to improve the interannual correlation of tropical cyclones in highresolution hindcasts forced with historical observed SSTs (LaRow et al. 2008 ). However, we note that while variable-resolution runs can be substantially less computationally expensive than uniform resolution calculations, they are by their very nature more targeted simulations and of less general applicability in a spatial sense. Hence, the decision to use variable resolution methods is a trade-off between reduced computational cost and limitations on the variety of analyses that can be performed on the model output."}, {"section_title": "Tracking", "text": "A wealth of automated detection algorithms has been developed to objectively find and quantify tropical cyclones in gridded climate data. The majority of published techniques employ a similar strategy. First, cyclone centers are defined by either a near-surface (generally 850 hPa) vorticity maximum or sea-level pressure minimum. Following this, warm-core criteria are typically applied to exclude mid-latitude cyclones. A surface wind speed threshold must also be surpassed, and all of these criteria need to be met for a minimum period of time, typically 1-3 days.\nWhile the general formulation is similar among popular detection mechanisms, a great deal of variety exists in the particular criteria used. The choice of vorticity maximum or sea-level pressure minimum as the tropical cyclone center, as well as the applicable threshold value for either, differs between schemes. The warm-core detection criteria are also variable, with some schemes seeking a particular air temperature anomaly at one or multiple pressure surfaces (e.g., Murakami et al. 2012; Vitart et al. 1997; Zhao et al. 2009 ), others utilizing geopotential thicknesses (e.g., Tsutsui and Kasahara 1996), and others focusing on a negative gradient in vorticity with height (indicating a warm core via the thermal wind relation (e.g., Bengtsson et al. 2007; Strachan et al. 2013) . Additionally, some apply basin- (Camargo and Zebiak 2002) or resolution-specific (Murakami and Sugi 2010; Walsh et al. 2007) thresholds to produce results which more closely match observed tropical cyclones, others implement additional exclusionary criteria based on geographic location (such as latitude restrictions in many trackers or the removal of monsoon lows in Murakami et al. 2012) , and others tune based on PDFs of relevant variables generated within a particular climate simulation (Camargo and Zebiak 2002) .\nAs the horizontal grid spacings of climate models increase, analyzing data sets becomes more cumbersome due to growing file size. Recently, MapReduce-style (Dean and Ghemawat 2008) techniques have been used to parallelize existing code (e.g., Prabhat et al. 2012; Zarzycki and Jablonowski 2014) to leverage highperformance computing capabilities for tropical cyclone-tracking purposes. In these cases, candidate cyclones are first detected in a time-independent fashion, allowing for data files (generally ranging from three hourly to daily) to be spread out among many processors. Following the completion of this processing, candidate cyclones are sorted and then connected to other nearby points in space and time to merge tropical cyclones into their particular trajectories.\nOf particular concern is whether the wide range of algorithms used contributes to uncertainty in tropical cyclone results. Extremely intense tropical cyclones generally have very well-defined features (vorticity maximum, warm core, etc.) that are picked up universally across algorithms. However, weaker storms are likely more difficult to track, particularly since the observational record of less intense, shortlived storms is questionable itself . Some techniques may miss these storms altogether, although schemes that track large numbers of weaker storms may be more susceptible to false alarms. Using CLIVAR HWG data, Horn et al. (2014) found broad agreement in projected changes in future tropical cyclone count, particularly when homogenizing for certain thresholds which are common among tracking algorithms, such as wind speed and storm duration. However, other differences still existed, likely due to fundamental differences across the mechanics of the algorithms, implying that additional work is necessary to close the gap between published schemes. Recently, tracking on reanalysis products provided a potential avenue for this assessment (e.g., Murakami 2014; Strachan et al. 2013 ). The utility of using the current class of reanalysis products (at or coarser than approximately 50 km grid spacing) may be questionable due to a systematic weak bias when comparing reanalysis TCs to observations (Schenkel and Hart 2012) , but as higher-resolution reanalysis products become available, it is likely their usefulness in developing and tuning tracking algorithms will increase."}, {"section_title": "Assessing Model Quality of Tropical Cyclone Statistics Simulation", "text": "Model intercomparison studies of CMIP5 class are often constrained to largescale metrics of seasonally or annually averaged quantities (Myhre et al. 2013 ).\nMulti-model assessment of extreme weather in these models has been confined mostly to daily or longer indices averaged over large regions (Grotjahn et al. 2015; Sillmann et al. 2013 ). The improved realism of simulated storms from high-resolution climate models offers additional opportunities to assess model quality. Short-duration hindcast experiments of actual individual or idealized storms can provide valuable insight into errors arising from dynamical cores and the physical parameterizations of the climate model Jablonowski 2011a,b, 2012; . However, the present discussion will be confined to performance metrics describing the statistical behavior of simulated tropical cyclones in multi-decadal simulations. The simulated annual average distribution of tropical cyclones is perhaps the most fundamental measure of a model's ability to characterize the statistics of these intense storms. However, as noted in Sect. 4, identifying tropical cyclones in long simulations is subject to uncertainties. Furthermore, even during the satellite era, observations do not provide a fully complete record of the global climatology of tropical cyclones. In particular, short-duration weak storms were not uniformly tracked by the hurricane centers as observational technologies advanced, resulting in spurious trends . As most of the errors in tracking simulated storms also arise from weak storms, comparison of simulated and observed storm statistics at wind speeds above a critical threshold, say 33 m/s (or category 1 or greater on the Saffir-Simpson scale), is less dependent on the choice of tracking scheme. However, this can be problematic for some models, even in the less than 30 km resolution class, if their distributions of wind speeds are biased low. In fact, the number and intensity distribution of simulated tropical cyclones is highly dependent on a large number of factors as discussed in Sect. 2 and has not yet been demonstrated to converge in any model as resolution is increased. Nonetheless, given its importance, modeling groups often compare to the global number of tropical cyclones and in some cases tune their models to that number, although this may bias climate change studies. The spatial distribution of simulated tropical cyclones is also of fundamental importance if high-resolution climate models are to provide useful information to the impact community. The simplest model performance metric in this regard is to divide the ocean basins into somewhat arbitrary regions and count the simulated storms and/or their fraction of the total in each of the regions. Table 8 .1 shows the annual global and selected basin number of storms for several high-resolution models under contemporary forcing factors and an observational estimates as for three different instantaneous maximum wind speeds defined by the Saffir-Simpson scale. The observations are from the International Best Track Archive for Climate Stewardship (IBTrACS) observed track database (Knapp et al. 2010) .\nComparison of tracked storms from simulations to observational datasets such as IBTraCS should be performed with caution. The observations are a handcrafted product constructed in an entirely different manner than a model storm tracker. Likewise, as discussed in Sect. 4, comparison between model results that are obtained by different tracking methods can also be problematic. In Table 8 .1, two different sets of storm counts are provided for CAM5-FV. The set labeled CAM5-FV a comes from Wehner et al. (2014) using the GFDL method and threshold values published in (Knutson et al. 2007 ). The set labeled CAM5-FV b comes from the same tracker used with CAM5 in Reed et al. (2015) which is based on a more recent method from GFDL (Zhao et al. 2009 ) with substantially lower counts than the earlier analysis despite being applied to exactly the same model output. However, the comparison of CAM5-FV b and CAM5-SE used the same tracker , revealing a true difference in tropical cyclone statistics between a few models.\nThe basins in Table 8 .1 are perhaps the most natural division of the world's oceans, but more detailed comparison of simulated and observed track and cyclogenesis densities are also revealing. Also, models typically produce a few storms in the South Atlantic. Although only a few tropical cyclones have been observed there, it is not a well-observed region. Another interesting metric is the fraction of Atlantic storms that originate in the Caribbean Roberts et al. 2015) . This detail has proven to be difficult for contemporary 25 km models to reproduce. A further refinement on basin-wide storm counts is to separately account for land-falling storms Vecchi and Villarini 2014) . While many simulated storms recurve poleward at some point in their evolution, in some models with cyclogenesis biases too far to the east, the fraction of storms that make landfall is too low even if their simulation of the basin-wide number of storms is correct. Such errors can limit the usefulness of a model for tropical cyclone impact studies.\nThe simulated tropical cyclone frequencies in Table 8 .1 can be further augmented by considering the number of storm days per year as a measure of basin-wide cyclonic activity as typically used by the hurricane seasonal forecast community (Gray 1979; Wehner et al. 2010 ). However, measures based on Saffir-Simpson categories are defined by point-wise maximum wind speeds only. Integrated measures of tropical cyclone properties such as accumulated cyclonic energy, ACE (Bell et al. 2000) , Power Dissipation Index, PDI (Emanuel 2005 (Emanuel , 2007 , and integrated kinetic energy, IKE (Powell and Reinhold 2007) provide a more holistic description of overall storm intensities. Wind speeds alone provide an incomplete measure of the impacts of tropical storms (Wendel 2015) . The Cyclone Damage Potential (Holland et al. 2016 ) and the Hurricane Hazard Index (Wendel 2006 ) also incorporate the spatial extent of high winds to assess the potential for impacts from tropical storms. Application of model evaluation metrics based on observational estimates of these quantities to high-resolution atmospheric models will likely prove more informative than simple counting based on the Saffir-Simpson scale.\nEach ocean basin has its own seasonal cycle of tropical cyclogenesis. The timing of the beginning and end of these stormy periods provides another model performance metric. Depending on the basin, the seasonality is a function of the magnitude of sea surface temperature and wind shear in the cyclogenesis regions. Figure 8 .3 shows seasonal cycle of North Atlantic tropical cyclones as simulated by the variable-resolution model described in Sect. 3, compared to the IBTraCS observations. The model (25 km CAM-SE, red) reproduces the correct observed (IBTraCS, blue) annual peak for all TCs (Fig. 8.3a) with September being the most active month, followed by August, and then October. When only hurricanes and major hurricanes are considered (Fig. 8.3b-c) , the peak is shifted 1 month too early in the simulations, although the model does an adequate job reproducing the peridocity of storm formation. When absolute values are plotted (Fig. 8.3d-f) , it is (From Zarzycki and Jablonowski 2014) clear that the model is biased slightly low in the overall number of TCs produced (similar to the CAM-SE North Atlantic results in Table 8 .1), but that the cycle is significantly more realistic than that demonstrated by a coarser 100 km model (dark green) (Zarzycki and Jablonowski 2014) .\nImprovement in the seasonal cycle of TCs as a function of resolution is not limited to the North Atlantic. In the North Indian, the observed tropical cyclone seasonality is bimodal with a peak in March and a second larger peak in November before and after the monsoon season (Li et al. 2013) . In prescribed SST experiments, models can reproduce this bimodality . Other studies utilizing high-resolution configurations have shown similar reproductability in other ocean basins as well (e.g., Zhao et al. 2009 ).\nThe interannual variations in tropical cyclone number in some basins are strongly controlled by the state of the oceans. For instance, in the North Atlantic, both the El Nino-Southern Oscillation (ENSO) and the Atlantic Meridional Mode (AMM) modulate tropical cyclogenesis (Gray 1984; Patricola et al. 2014) . Several analyses of prescribed SST experiments examined the relationships between simulated and observed time series of annual tropical cyclone counts finding relatively high correlations in some basins Zhao et al. 2009 ). Performance metrics describing the physical characteristics of simulated tropical cyclones are also an important part of a complete model assessment. A scatterplot of the instantaneous maximum wind speed versus minimum pressure for each tracked storm provides a picture of how well the dynamical core responds to high vorticity flow. Figure 8 .4 shows this pressure-wind relationship for North Atlantic and West Pacific basins as simulated the CAM5-FV model at a resolution of approximately 25 km compared to the IBTrACS observations. Quadratic fits to each scatterplot are shown in the background of Fig. 8.4 . Further quantitative analysis could also be performed when comparing such scatterplots. Other scatterplots, such as storm radii versus maximum wind speed or minimum pressure would also reveal much about how resolution impacts the structure of simulated tropical cyclones.\nDirect usage of simulated paths and statistics for studies of tropical cyclone impacts must be made with caution. Systematic errors in the position of simulated tracks may under-or overestimate landfall frequency although corrections could be imposed. Errors in the total basin-wide storm count and/or seasonality must also be carefully considered. Furthermore, cumulus convection parameterizations are still necessary for models in the 10-50 km range. As these parameterizations were not developed for these resolutions, errors in total precipitation may also need to be corrected for usage in tropical cyclone-related inland flooding analyses."}, {"section_title": "Computational Performance Issues, Analysis, and Scalability", "text": "Since the seminal study by Oouchi et al. (2006) , computers have continued to increase in peak speed enabling other modeling groups to extend their models to tropical cyclone-permitting horizontal resolutions. Although Moore's law continues to increase peak speeds of the largest available supercomputers, the doubling of transistor density approximately every 18 months is no longer used to increase processor clock speeds because of energy consumption and cooling considerations (Donofrio et al. 2009 ). Rather, machine designers have used these additional components to increase the number of on-chip processors. This has important implications for the software design of climate models. Scalability to large number of processors, at least tens of thousands, is necessary to fully utilize the most recent hardware designs (Dennis et al. 2012b ). Scalability to 100,000 processor cores or more will likely be indispensable in the not too distant future to fully exploit manycore chip based machines (Wehner et al. 2011) .\nAs spatial resolution increases, sub-daily output becomes ever more interesting due to the increased realism of the simulated weather. For some tropical cyclonetracking schemes, eight surface variables are required at six hourly intervals. For other studies, three hourly or even hourly outputs can be informative about the diurnal cycle. Furthermore, the sub-daily output of variables at multiple vertical levels can inform about the structure of storms. Tens to hundreds of terabytes of model output can justifiably be saved from multi-decadal simulations at order 25 km resolution. However, overall throughput of model simulations can be adversely affected by this volume and frequency of data output. For instance, the CAM-SE will scale without output to well over 30,000 processor cores through a combination of MPI tasks and OpenMP threads (Dennis et al. 2012b; Mirin and Worley 2011) . At such processor counts, careful usage of parallel i/o libraries is required in order to avoid serial bottlenecks. Attention must be paid to the specifics of the parallel file systems (e.g., Mizielinski et al. 2014) . However, available high-performance machines such as edison.nersc.gov, a Cray XC30, have a limited number of i/o nodes. Hence, parallel output does not scale past more than 256 pio tasks (pio is a parallel i/o library using pnetcdf) for the 25 km version of CAM-SE on that machine when a large three hourly output dataset is specified. As a practical matter, overall performance is degraded by this poor i/o scaling such that using more than about 8000 processor cores is ineffective. Such limitations have been found on other, very different architectures such as the Blue Gene Q (mira.alcf.gov). There are a number of yet to be implemented fixes to this problem, including more effective usage of the parallel file systems (in this case, LUSTRE) by pnetcdf/pio as well as asynchronous i/o protocols to overlap output with computation on a separate set of i/o processors.\nSome might argue that such large volumes of output data are unnecessary as some analyses, such as parts of the tracking algorithm, can be performed in-core during the model integration. In fact, this statement is true if the analysis to be performed is independent of time or can be simply accumulated as model time advances. However, multi-decadal, tropical cyclone-permitting climate model simulations are so computationally intensive that only a few groups can perform them. We argue here that this class of integration is most effectively used as a community-shared resource. Since it is impossible for a modeling group to anticipate how external users may creatively use the model output, distribution of large datasets is unavoidable if the model output is to be used by the wider scientific community. Fortunately, advanced file transfer protocols such as GLOBUS (www.globus.org) fully utilize network bandwidth to facilitate the movement of large amounts of data between computing centers.\nSuch large datasets present significant challenges for offline analyses. In order to reduce the workflow turnaround time, analysts must restructure their algorithms to use parallel, multiprocessor programs. Many common analyses can indeed be structured so that at least one dimension is embarrassingly parallel, that is not requiring any interprocessor communications. In many cases, as noted in the previous paragraph, this dimension can be time. For example, the interpolation from an irregular model mesh to a regular latitude-longitude mesh or the construction of spatial averages falls into this category. By contrast, calculating time-dependent quantities such as monthly averages or values above a threshold may be cast as embarrassingly parallel in space. Such parallel processing tasks are easily programmed by dividing the entire problem up at the start of the analysis calculation and reassembling at the end. Bottlenecks often occur during the i/o phase of analysis calculations. This can be particularly acute when trying to read many files simultaneously to initialize the analysis as the parallel tasks can be in contention for the available i/o resources. This reinforces the notion that well-designed climate modeling supercomputer centers must invest in special purpose analysis machines (e.g., Lawrence et al. 2013) , with large memories and fast i/o in addition to the large number crunchers necessary to run the actual climate models."}, {"section_title": "Future Directions", "text": "Direct simulation of the multi-decadal statistics of tropical cyclones has only recently become possible. This new area of climate modeling offers many new challenges and opportunities as available computational resources continue to increase. Certainly, coupling of tropical cyclone-permitting atmospheric models to mesoscale eddy-permitting ocean models is among the most difficult of current research efforts. Consortia in both Europe (the PRIMAVERA project 1 ) and in the USA (the ACME program 2 ) has been formed to specifically develop new climate models of this class. These efforts, in addition to existing ones, recognize that the increased realism in simulated weather systems, of which tropical cyclones are just one class, leads to more credible local and regional simulation of present and future climate. Public and private decision-makers require much more localized information than current CMIP5-class models can easily provide. The significant resources currently being devoted to developing global atmospheric models resolved in the range of 10-25 km is motivated in part by these policy needs.\nInternationally, the endorsement by the CMIP panel of the HighResMIP protocol (https://dev.knmi.nl/projects/highresmip) for the next climate model intercomparison, CMIP6, is an important step in advancing both stand-alone and coupled tropical cyclone-permitting climate models as well as quantifying the structural uncertainty in future projected changes in tropical cyclone properties. The protocol defines multiple century-scale numerical experiments from the mid-twentieth century to the mid-twenty-first century under two different future external climate-forcing scenarios. In order to permit research on storm statistics, a variety of output fields at six hourly intervals are prescribed which will more than likely result in a multipetabyte data set.\nThe need for ensemble simulations is as critical for tropical cyclone-permitting models as it is for more conventional resolution climate models (Deser et al. 2014) . Analysis of variance (ANOVA) techniques can be applied to such integrations to explore forced and unforced climate variability. As was the case with conventional resolution models, early ensemble simulations will be small (the HighResMIP protocol is only three realizations), but will grow in size as computational resources expand.\nAs the amount of data available to study tropical cyclones increases due to both higher resolution and larger ensembles, further efforts in developing flexible, highly parallelized tracking algorithms (such as those previously discussed in this chapter) will be required. Techniques where TCs are tracked \"online\" during the actual model run may become popular as a way of alleviating post-simulation data analysis bottlenecks. This would allow users to continue to output high-resolution, high-frequency TC data, but without the need for this data to be first generated at the global scale for initial detection purposes, significantly reducing the data storage required for investigating TCs. As the structure of simulated TCs grows closer to those observed in nature, machine-learning algorithms may also be viable future detection and tracking paths (e.g., Liu et al. 2016 ). An example would be using observations of historical TCs (such as satellite images or wind fields) to be used as training data sets, which can then be applied to find storms in climate model data.\nAs mentioned earlier, the subgrid-scale parameterizations in current tropical cyclone-permitting models were not specifically developed for usage at such high resolutions. Research in \"scale aware\" formulations, particularly of cumulus convective processes, will likely improve the mean climatology of models down to approximately 10 km resolution. As computational resources permit resolutions finer than about 10 km, more dramatic changes in model formulation are dictated. Below this resolution, non-hydrostatic effects in the fluid motion equations must be accounted for, requiring new formulations of the dynamical cores of some models. Furthermore, for resolutions below 10 km, deep cumulus convection cannot be rigorously parameterized and some portion of convection is actually directly simulated. The range of resolutions from about 1 km to about 10 km has been called a \"gray zone\" where some combination of explicit and parameterized convection is required. One option is to remove the convective parameterization altogether (Satoh et al. 2014) . Another approach to unify this combination (Arakawa and Wu 2013; Wu and Arakawa 2014 ) defines a scaling factor between 0 and 1 to multiply a parameterized convective contribution to temperature and moisture tendencies; this is still an active area of research."}, {"section_title": "Conclusions", "text": "High-resolution multi-decadal simulation of tropical cyclones can now be performed by a few groups using the largest available supercomputers. This new class of models, with horizontal resolution in the range of 10-50 km, enables more realistic simulation of many classes of extreme storms in addition to tropical cyclones. However, increased resolution alone will not decrease the large-scale model errors that result from deficiencies in subgrid-scale physical parameterizations. Many aspects of simulated tropical cyclones in these models are surprisingly realistic, including their frequency, seasonality, and interannual variability. Wind speeds up to category 5 on the Saffir-Simpson scale are simulated by some models with correspondingly realistic central low-pressure values. Fixed SST models are the most practical, in terms of computational burden, at the present time. Carefully constructed climate change experiments with these models are beginning to confirm some theoretical expectations of the behavior of tropical cyclones in future warmer climates. Fully coupled global ocean atmosphere models are more challenging to integrate due to the need to spin up or initialize the ocean state, and multi-decadal simulations are just beginning to be made. Such models can offer insight into the effect of tropical cyclones on the climate through the poleward transport of energy and moisture as well as more realistically simulate the interaction of storms with the upper ocean.\nNearly every tropical cyclone-permitting model produces stronger storms in the tail of their simulated wind speeds under warmer ocean conditions. What the total number of tropical cyclones in a warmer world will be is less clear as results vary between models and are dependent on both the magnitude and pattern of ocean temperature change. As more modeling groups are enabled to perform multi-decadal tropical cyclone-permitting simulations, structural uncertainties in the behavior of future storm statistics will be better quantified. This is a critical part of the development of a climate change theory of tropical cyclones.\nby its trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the US government or any agency thereof, or the Regents of the University of California. The views and opinions of authors expressed herein do not necessarily state or reflect those of the US government or any agency thereof or the Regents of the University of California."}]