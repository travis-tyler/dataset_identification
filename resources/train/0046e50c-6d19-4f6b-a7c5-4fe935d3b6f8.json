[{"section_title": "Introduction", "text": "The emergence of high-dimensional data in genomics and neuroimaging, among other areas, has presented us with a large number of predictors as well as many response variables, which may have strong correlations. For instance, in imaging genetics as an emerging field, such problems frequently arise when multivariate imaging measures, such as volumes of cortical and subcortical regions of interest (ROIs), are predicted by high-dimensional covariate vectors, such as gene expressions or single nucleotide polymorphisms (SNPs). The joint analysis of imaging and genetic data may ultimately lead to discoveries of genes for some complex mental and neurological disorders, such as autism and schizophrenia (Cannon and Keller, 2006; Turner et al., 2006; Scharinger et al., 2010; Paus, 2010; Peper et al., 2007; Chiang et al., 2011a,b) . This motivates us to develop low rank regression models (GLRR) for the analysis of high-dimensional responses and covariates under the high-dimensionlow-sample-size setting.\nDeveloping models for high-dimensional responses and covariates poses at least four major challenges including (i) a large number of regression parameters, (ii) a large covariance matrix, (iii) correlations among responses, and (iv) multicollinearity among predictors. When the number of responses and the number of covariates, which are denoted by d and p, respectively, are even moderately high, fitting conventional multivariate response regression models (MRRM) usually requires estimating a d \u00d7 p matrix of regression coefficients, whose number pd can be much larger than the sample size. Although accounting for complicated correlation among multiple responses is important for improving the overall prediction accuracy of multivariate analysis (Breiman and Friedman, 1997) , it requires estimating d(d + 1)/2 unknown parameters in a d \u00d7 d unstructured covariance matrix. Another notorious difficulty is that the collinearity among a large number of predictors can cause issues of over-fitting and model misidentification (Fan and Lv, 2010) .\nThere is a great interest in developing new statistical methods to handle these challenges for MRRMs. The early developments involve a separation approach-variable selection to reduce dimension and then parameter estimation, when both p and d are moderate compared to the sample size (Breiman and Friedman, 1997) . For instance, Brown et al. (2002) introduced Bayesian model averaging incorporating variable selection for prediction, which allows for fast computation for dimensions up to several hundred. Recently, much attention has been given to shrinkage methods for achieving better stability and improving performance (Tibshirani, 1996) . Notably, the most popular ones are the L 1 and L 2 penalties. The L 2 penalty forces the coefficients of highly correlated covariates towards each other, whereas the L 1 penalty usually selects only one predictor from a highly correlated group while ignoring the others. L 1 priors can be seen as sparse priors since they create a singularity at the origin whose gravity pulls the smaller coefficients to zero under maximum a posteriori (MAP) estimation. There are fully Bayesian approaches with sparse priors for univariate responses like the Bayesian LASSO (Park and Casella, 2008) , a generalization of the LASSO (Kyung et al., 2010) , and the double Pareto (Armagan et al., 2011) , among many others. These methods, however, are primarily developed under the univariate-responsehigh-dimensional-covariate setting.\nThere have been several attempts in developing new methods under the high-dimensionalresponse-and-covariate setting. When both p and d are moderate compared to the sample size, Breiman and Friedman (1997) introduced a Curds and Whey (C&W) method to improve prediction error by accounting for correlations among the response variables. Peng et al. (2010) proposed a variant of the elastic net to enforce sparsity in the high-dimensional regression coefficient matrix, but they did not account for correlations among responses. Rothman et al. (2010) proposed a simultaneous estimation of a sparse coefficient matrix and sparse covariance matrix to improve on estimation error under the L 1 penalty. Similarly, Yin and Li (2011) presented a sparse conditional Gaussian graphical model in order to study the conditional independent relationships among a set of gene expressions adjusting for possible genetic effects. Furthermore, several authors have explored the low rank decomposition of the regression coefficient matrix and then use sparsity-inducing regularization techniques to reduce the number of parameters (Izenman, 1975; Reinsel and Velu, 1998; Tibshirani, 1996; Turlach et al., 2005; Chen et al., 2012; Vounou et al., 2010) . For instance, Chen et al. (2012) and Vounou et al. (2010) considered the singular value decomposition of the coefficient matrix and used the LASSO-type penalty on both the left and right singular vectors to ensure its sparse structure. Since all variable selection methods require a selection of a proper amount of regularization for consistent variable selection, some methods, such as stability selection and cross validation, are needed for such selection (Meinshausen and Buhlmann, 2010) . They, however, do not provide a standard inference tool (e.g., standard deviation) on the nonzero components of the left and right singular vectors or the coefficient matrix. Moreover, frequentist inference is the primary approach for making statistical inferences in the high-dimensional-response-and-covariate setting.\nIn this paper, we propose a new Bayesian GLRR to model the association between genetic variants and brain imaging phenotypes. A low rank regression model is introduced to characterize associations between genetic variants and brain imaging phenotypes, while accounting for the impact of other covariates. We assume shrinkage priors on the singular values of the regression coefficient matrix, while not explicitly requiring orthonormality of left and right singular vectors. This facilitates fast computation of the regression coefficient matrix. We consider a sparse latent factor model to more flexibly capture the within-subject correlation structure and assume a multiplicative gamma process shrinkage priors on the factor loadings, which allow for the introduction of infinitely many factors (Bhattacharya and Dunson, 2011) . We propose Bayesian local hypothesis testing to identify significant effects of genetic markers on imaging phenotypes, while controlling for multiple comparisons. Posterior computation proceeds via an efficient Markov chain Monte Carlo (MCMC) algorithm.\nIn Section 2, we introduce the NIH Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. In Section 3, we introduce GLRR and its associated Bayesian estimation procedure. In Section 4, we conduct simulation studies with a known ground truth to examine the finite sample performance of GLRR and compare it with the conventional LASSO method. Section 5 illustrates an application of GLRR in the joint analysis of imaging, genetic, and clinical data from ADNI. Section 6 presents concluding remarks."}, {"section_title": "Generalized Low Rank Regression Models", "text": ""}, {"section_title": "Model Setup", "text": "Consider imaging genetic data from n independent subjects in ADNI. For each subject, we observe a d \u00d7 1 vector of imaging measures, denoted by Y i = (y i1 , \u2026, y id ) T , and a p \u00d7 1 vector of clinical and genetic predictors, denoted by X i = (x i1 , \u2026, x ip ) T , for i = 1, \u2026, n. Let Y = (y ik ) be an n \u00d7 d matrix of mean centered responses, X = (x ij ) be an n \u00d7 p matrix of standardized predictors, B = (\u03b2 jk ) be a p \u00d7 d matrix of regression coefficients, and E = (\u220a ik ) be an n \u00d7 d matrix of residuals. We consider a multivariate response regression model given by (1) where\nThere are several statistical challenges in fitting model (1) to real data. When both p and d are relatively large compared to n, the number of parameters in B equals p \u00d7 d and can be much larger than n. Furthermore, the number of unknown parameters in \u03a3 equals d(d+1)/2. In addition to the number of unknown parameters, there are some additional complexities arising from practical applications, including different scales for different response variables and collinearity among the predictors.\nIn model (1), multiple responses are measured from the same subject and share a set of common predictors. Therefore, the regression coefficient matrix B can have two-way linear dependence coming from both the correlated responses and covariates. This shared mean structure can lead to a low rank mean parameter matrix B. We exploit this shared structure of B by decomposing it as (2) where r is the rank of B,\nIn (2), it is assumed that genetic variates that are associated with phenotypes may be relatively sparse and each column of U may group informative SNPs with similar association into clusters. Thus, under such assumption, a small rank of B may capture the major dependence structure between Y i and X i .\nGiven the large number of parameters in \u03a3, we consider a Bayesian factor model to relate the random effects \u220a i to the latent factors \u03b7 i as (3) where \u039b is a d \u00d7 \u221e factor loading matrix, \u03b7 i ~ N \u221e (0, I \u221e ), and \u03be i ~ N(0, \u03a3 \u03be ) with . As shown in Hyun et al. (2014) , the factor model (3) is useful for delineating the medium-to-long-range (or global) spatial dependence of neuroimaging data. Another advantage of (3) is that it bypasses the challenging issue of selecting the number of factors through a delicate prior setting. To achieve dimensionality reduction, one would typically restrict the dimension of the latent factor vector \u03b7 i to be orders of magnitude less than that of \u220a i . By following Bhattacharya and Dunson (2011) , we choose a prior that shrinks the elements of to zero as the column index increases. Thus, it bypasses the challenging issue of selecting the number of factors. Finally, our GLRR integrates the low rank model (2) and the Bayesian factor model (3). Specifically, our GLRR can be written as (4) Other than genetic markers, such as SNP's, it is common that X i has a subvector, denoted by X Pi , consisting of several prognostic variables, such as age, gender, and disease status in real applications. There are two different methods to deal with prognostic factors in the presence of genetic markers. The first method is a two-step approach. The first step is to fit the MRRM solely with these prognostics factors as covariates and then calculate the fitted residuals as adjusted responses. The final step is to fit model (4) to the adjusted responses with genetic markers as X. The second method is to fit model (4) with both prognostic factors and genetic markers as covariates. Let B P be the p P \u00d7 d matrix of coefficients associated with the prognostic factors and X Si and B S be, respectively, the subvector of X i and the submatrix of B associated with genetic markers. It may be reasonable to assume that B P may be unstructured and B S admits the decomposition given by . In this case, the model can be written as (5) We take the second approach and fit model (5) in real data analysis."}, {"section_title": "Low Rank Approximation", "text": "The decomposition (2) is similar to the standard singular value decomposition (SVD), but it differs from SVD. Specifically, it is unnecessary that the columns of U and V in (2) are orthonormal and this allows that u jl and v jl can take any value in (\u2212\u221e, \u221e), since identifiability is not critical for making inference on B. Thus, the decomposition (2) can be regarded as a generalization of SVD in Chen et al. (2012) . Moreover, compared to SVD, this decomposition leads to better computational efficiency, since sampling a unit vector in a high-dimensional sphere is computationally difficult. Nevertheless, each layer B l is a factorization with unit rank, which amounts to estimating a common p \u00d7 1 vector of distinct regression coefficients and making the rest of the coefficients some linear combinations of this vector with d additional parameters. Within the l-th layer, each column of B l shares the same u l and \u03b4 l , which facilitates the exploitation of a common dependence structure among the covariates collected from the same set of subjects. Similarly, each row of B l shares the same v l and \u03b4 l facilitating the exploitation of a common dependence structure among the responses from the same set of subjects. The number of parameters at each layer is p + d and the total number of parameters equals r \u00d7 (p + d). Since r << min (p, d) , the use of the decomposition (2) leads to a huge dimension reduction.\nThe decomposition (2) differs from two other popular methods including multivariate response models and stepwise unit rank regression models. Multivariate response models estimate a separate p\u00d71 vector of coefficients for each response totaling p\u00d7d parameters. In frequentist analysis (Chen et al., 2012) , it is common to sequentially explore each layer of B based on the ordering of \u0394, which leads to stepwise unit rank regressions (SURR). Specifically, one first fits the unit rank (r = 1) regression with the observed Y as the response to estimate the first layer and . Subsequently, one fits another unit rank regression with as the response to estimate the second layer . One can continue this process until the r-th rank. Thus, SURR can be viewed as a special case of GLRR."}, {"section_title": "Covariance Structure", "text": "The covariance structure for Y i is given by (6) It is common to impose a constraint on \u039b to define a unique model free from identification problems, since \u03a3 is invariant under the transformation \u039b* = \u039bP for any semi-orthogonal matrix P with P P T = I. For instance, for identifiability purposes, one may impose a full rank lower triangular constraint, which implicitly specifies an order dependence among the responses (Geweke and Zhou, 1996) . However, it is unnecessary to impose such a constraint on \u039b if our primary interest is on covariance matrix estimation. Specifically, we will specify a multiplicative gamma process shrinkage prior in (7) on a parameter expanded loading matrix with redundant parameters. The induced prior on \u03a3 is invariant to the ordering of the responses. This shrinkage prior adaptively selects a truncation of the infinite loadings to one having finite columns. Thus, it facilitates the posterior computation and provides an accurate approximation to the infinite factor model."}, {"section_title": "Priors", "text": "We first consider the priors on the elements of all layers B l . When dealing with two highly correlated covariates, the L 1 prior tends to pick one and drop the other since it is typically a least angle selection approach to force some coefficients to zero, whereas the L 2 prior tends to force the coefficients towards each other to produce two highly correlated coefficients. In GLRR, since our primary interest is to exploit the potential two-way correlations among the estimated coefficients, we choose the L 2 prior. Let Ga(a, b) be a gamma distribution with scale a and shape b. Specifically, we choose where a 0 , b 0 , c 0 , d 0 , e 0 , and f 0 are prefixed hyper-parameters. The number of predictors p is included in the hyperprior of \u03c4 u to have a positive-definite covariance matrix of high dimensional u l and fix the scale of u l . Similarly, we add the dimension d to all hyper-priors for \u03c4 v,l . Moreover, we standardize all predictors to have zero mean and unit variance, and thus a single prior is sensible for all elements of u l . The varying dispersions \u03c4 v,1 , \u2026, \u03c4 v,d are chosen to account for different scales of different responses. For example, the volumes of different ROIs vary dramatically across ROIs, so it is more sensible to use separate dispersions for different ROIs.\nWe place the multiplicative gamma process shrinkage prior (Bhattacharya and Dunson, 2011) on \u039b in order to increasingly shrink the factor loadings towards zero with the column index. Such shrinkage priors avoid the drawback of order dependence from the lower triangular constraint on \u039b for identifiability. We use inverse gamma priors on the diagonal elements of \u03a3 \u03be . Specifically, these priors are given as follows: (7) where \u03c8 g for g = 1, \u2026, \u221e are independent random variables, \u03c4 \u03bbh is a global shrinkage parameter for the h-th column, and the \u03d5 kh s are local shrinkage parameters for the elements in the h-th column. Moreover, v, a 1 ,, a 2 , a \u03c3k and b \u03c3k are prefixed hyper-parameters. When a 2 > 1, the \u03c4 \u03bbh 's increase stochastically with the column index h, which indicates more shrinkage favored over the columns of higher indices. The loading component specific prior precision allows shrinking the components of \u039b. Straightforward Gibbs sampler can be applied for posterior computation"}, {"section_title": "Posterior Computation", "text": "We propose a straightforward Gibbs sampler for posterior computation after truncating the loadings matrix to have k * << d columns. An adaptive strategy for inference on the truncation level k * has been described in (Bhattacharya and Dunson, 2011) . The Gibbs sampler is computationally efficient and mixes rapidly. Starting from the initiation step, the Gibbs sampler at the truncated level k * proceeds as follows:\n1. Update (u l , \u03c4 u ) according to their conditional distributions where .\n3. Update (\u03b4 l , \u03c4 \u03b4 ) according to their conditional distributions where ."}, {"section_title": "4.", "text": "Update the kth row of \u039b k* , denoted by \u03c4 k , from its conditional distribution\nT is the kth column of E = Y \u2212 XB, and"}, {"section_title": "Update \u03d5 kh from its conditional distribution", "text": "6. Update \u03c8 1 from its conditional distribution and update \u03c8 h , h \u2265 2 from its conditional distribution where for h = 1, \u2026, k * .\nwhere \u220a i is the ith row of E."}, {"section_title": "Determining the Rank of B", "text": "We consider different methods for determining the rank of B. For frequentist inference, many regularization methods have been developed to recover the low rank structure of a matrix, such as B, by shrinking \u03b4\u2113's to zero in (2) (Chen et al., 2012) . For Bayesian inference, it may be tempting to use Bayesian model averaging and allow varying number of layers in order to improve prediction performance, but it limits us on making statistical inference on each layer of B, U, and V . We take a fixed-rank approach and use some selection criteria to choose an optimal value of r. Specifically, we consider five different selection criteria including the Akaike information criterion (AIC), the Bayesian information criterion (BIC), the normalized prediction error (PEN), the multivariate R 2 , and the normalized model error (MEN) for GLRR. Let , where is the posterior estimate of B based on the MCMC samples. Let be the error sum of squares and p * = r(p + d) be the number of parameters in B. The five evaluation criteria are, respectively, given by (8) The numerator and denominator of the MEN are, respectively, the model error and measurement error of model (4) (Yuan et al., 2007) . Thus, the MEN is the ratio of the model error over the measurement error as a percentage of the total magnitude of all parameters. Similarly, the PEN and R 2 are defined as percentages, which makes comparisons more meaningful and readily comparable across studies.\nTo illustrate the effectiveness of all five criteria, we independently simulated 100 data sets from model (4) with (n, p, d) = (100, 200, 100) and a rank 5 matrix B. For each simulated data set, we used the Gibbs sampler to draw posterior samples to estimate B and then calculated the five selection criteria in (8) as the rank varied from 1 to 10. Finally, based on all 100 simulated data sets, we calculated the mean and standard deviation of each selection criterion as the rank varied from 1 to 10. As shown in Figure 1 , PEN, MEN, R 2 , and AIC stabilize around the true rank, whereas BIC reaches the minimum at the true rank. This may indicate that BIC outperforms other selection criteria for determining the true rank of B. This result also agrees with the findings in Bozdogan (1987) such that in parametric settings, AIC tended to select a larger model than the true model even for large sample sizes, whereas BIC is asymptotically consistent in estimating the true model."}, {"section_title": "Thresholding", "text": "Based on the MCMC samples obtained from the Gibbs sampler, we are able to identify three different sets of information including (i) SNPs that significantly contribute to a large portion of imaging phenotypes, (ii) imaging phenotypes that are associated with those SNPs in (i), and (iii) important individual SNP effects on individual imaging phenotypes.\nStatistically, (i), (ii), and (iii) can be formulated as testing significant elements in U, V , and B, respectively. For the sake of space, we focus on (i). Suppose that we draw a set of MCMC samples for m = 1, \u2026, M. Due to the magnitude ambiguity of U, we normalize each column of U = (u jl ) to calculate . Moreover, we develop a specific strategy to deal with the sign ambiguity of U*. For the l-th column of U*, we use the normalized MCMC samples to empirically determine the j 0 -th row such that for all j\u2032 \u2260 j 0 . Then, we fix to be positive for l = 1, \u2026, r and m = 1, \u2026, M.\nTo detect SNPs in (i), we suggest to calculate the median and median absolute deviation (MAD) of , denoted by and s u,jl , respectively, since the MCMC samples may oscillate dramatically between the positive solution and the negative solution due to the sign ambiguity for all j, l. Then, one may formulate it as testing the local null and alternative hypotheses for relative to s u,jl given by where T* is a specific threshold for each . One may calculate the probability of given the observed data and then adjust for multiple comparisons (M\u00fcller et al., 2004; Wang and Dunson, 2010) . Another approach is to directly calculate t u,jl and apply standard multiple comparison methods, such as the false discovery rate, to determine T* (Benjamini and Hochberg, 1995) . We have found that these two methods lead to similar results, and thus we take the second approach. Moreover, this Bayesian thresholding method works well even when different responses are not on the same scale. Compared to the `hard' thresholding methods used in shrinkage methods (Chen et al., 2012; Peng et al., 2010; Rothman et al., 2010; Yin and Li, 2011) , this Bayesian thresholding method accounts for the variation of each and has a probabilistic interpretation."}, {"section_title": "Simulation Study", "text": ""}, {"section_title": "Simulation Setup", "text": "We carried out some simulation studies to examine the finite-sample performance of the GLRR and its posterior computation. We generated all simulated data according to model (4). The simulation studies were designed to establish the association between a relatively high-dimensional phenotype vector with a set of continuous covariates or a set of commonly used genetic markers (e.g., SNP). For each case, 100 simulated data sets were generated.\nWe simulated \u220a i ~ N d (0, \u03a3) and used two types of covariates including (i) continuous covariates generated from X i ~ N p (0, \u03a3 X ) and (ii) actual SNPs from ADNI data set. We determined \u03a3 and \u03a3 X as follows. Let p 0 be the binomial probability, which controls the sparsity of the precision matrix. We first generated a p\u00d7p matrix A = (a jj\u2032 ) with a jj = 1 and a jj\u2032 = uniform(0, 1) \u00d7 binomial(1, p 0 ) for j \u2260 j\u2032, set \u03a3 X = AA T , and standardized \u03a3 X into a correlation matrix such that \u03a3 X,jj = 1 for j = 1, \u2026, d. Similarly, we used the same method to generate \u03a3, the covariance matrix of \u220a i . For both \u03a3 and \u03a3 X , we set about 20% of the elements of \u03a3 \u22121 and to be zero, yielding that the means of the absolute correlations of \u03a3 and \u03a3 X are close to 0.40, respectively. We chose actual SNPs from the ADNI data set. Specifically, we only considered the 10,479 SNPs collected on chromosome 19, screened out all SNPs with more than 5% missing data and minor allele frequency (MAF)< 0.05, and randomly selected 400 SNPs from the remaining SNPs. For n = 1, 000 case, 500 subjects were randomly chosen and then replicated twice, whereas for the n=100 case, 100 subjects were randomly chosen from ADNI data set.\nWe considered five structures of B in order to examine the finite-sample performance of GLRR under different scenarios.\n\u2022 Case 1: X i ~ N p (0, \u03a3 X ) and a \"+\" structure was preset for B with (p, d) = (100, 100) with the elements of B being set as either 0 or 1.\n\u2022 Case 2: X i \u00d7 N p (0, \u03a3 X ) and B was set as a 200 \u00d7 100 matrix with the true rank r 0 = 5. Specifically, we set B = U\u0394V with U = (u jl ), \u0394 = diag(\u03b4 ll ) = diag (100, 80, 60, 40, 20) , and V = (v lk ) being 200 \u00d7 5, 5 \u00d7 5, and 5 \u00d7 100 matrices, respectively. Moreover, we generated all elements u jl and v lk independently from a N(0, 1) generator and then orthonormalized U and V.\n\u2022 Case 3: Covariates are actual SNPs and B has the same structure as that in Case 2 but with (p, d) = (400, 100).\n\u2022 Case 4: X i ~ N p (0, \u03a3 X ) and B was set as a 200 \u00d7 100 matrix with high degrees of correlation among elements with an average absolute correlation of 0.8, and then 20% of the elements of B were randomly forced to 0. After enforcing zeros, the true rank is 100 and the average absolute correlation is close to 0.7.\n\u2022 Case 5: Covariates are actual SNPs and B is the same as that in Case 4 with (p, d) = (400, 100).\nWe chose noninformative priors for the hyperparameters of B and set \u03b1 0 = \u03b2 0 = a 0 = b 0 = c 0 = d 0 = e 0 = f 0 = 10 \u22126 . Since shrinkage is achieved through dimension reduction by choosing r << min(d, p), these noninformative choices of the hyperparameters suit well. For the hyperparameters of \u03a3, we chose somewhat informative priors in order to impose the positive-definiteness constraint and set \u03bd = a 1 = b 2 = a \u03c3k = b \u03c3k = 1 for k = 1, \u2026, d. The induced prior of \u03a3 ensure that \u03a3 is positive definite, while the prior variances are large enough to allow \u03a3 to be primarily learned from the data. For each simulated dataset, we ran the Gibbs sampler for 10,000 iterations with 5, 000 burn-in iterations.\nAs a comparison, we considered a multivariate version of LASSO (Peng et al., 2010) , Bayesian LASSO (BLASSO) (Park and Casella, 2008) , and group-sparse multitask regression and feature selection (G-SMuRFS) (Wang et al., 2012) for all simulated data. For LASSO, we fitted d separate LASSO regressions to each response with a single tuning parameter across all responses by using a 5-fold cross validation. Since variances of all columns X and E are relatively equal, the variances of all columns of Y should be close to each other. In this case, a single tuning parameter is sensible. For BLASSO, we chose single priors for each column of the response matrix by setting all hyperparameters to unity. For GSMuRFS, we used single group and selected the optimal values of the penalty parameters by using a 5-fold cross validation.\nTo compare different methods, we calculated their sensitivity and specificity scores under each scenario. For all regularization methods, since we choose all possible values of the tuning parameters for calculating their sensitivity and specificity scores, it is unnecessary to use the cross validation method to select the tuning parameters. Let I(\u00b7) be an indicator function of an event and , where and s \u03b2,jk denote the posterior mean and standard deviation of \u03b2 jk , respectively. Specifically, for a given threshold T 0 , sensitivity and specificity scores are, respectively, given by where TP(T 0 ), TP(T 0 ), TP(T 0 ), and TP(T 0 ) are, respectively, the numbers of true positives, false positives, true negatives, and false negatives, given by Varying T 0 gives different sensitivity and specificity scores, which allow us to create receiver operating characteristic (ROC) curves. In each ROC curve, sensitivity is plotted against 1-specificity. The larger the area under the ROC curve, the better a method in identifying the true positives while controlling for the false positives."}, {"section_title": "Results", "text": "We first performed a preliminary analysis by using five data sets simulated according to the five structures of B and n = 1, 000. See Figure 2 for the true B and estimated by using GLRR3 (GLRR with r = 3), GLRR5 (GLRR with r = 5), BLASSO, G-SMuRFS, and LASSO under Case 1-Case 5. Inspecting Figure 2 reveals that for relatively large sample sizes, the fitted GLRR with r close to the true rank does a better job in recovering the underlying structure of B, while BLASSO and G-SMuRFS perform reasonably well for all cases. For the \"+\" structure of B with the true rank r 0 = 2 in Case 1, GLRR3 performs the best, whereas LASSO does a poor job. For B with the true rank r 0 = 5 in Cases 2 and 3, GLRR5 performs the best. The LASSO method performs reasonably well in recovering B for continuous X, when B is a 200 \u00d7 100 matrix, whereas it performs poorly when X is the SNP matrix. For the high-rank B in Cases 4 and 5, LASSO performs the best in recovering B, while GLRR3 and GLRR5 perform reasonably well.\nSecondly, we examined the finite sample performance of LASSO, BLASSO, G-SMuRFS, GLRR3, and GLRR5 under Cases 1-5 for n = 100. In each case, 100 simulated data sets were used and the mean and standard deviation of each of the five selection criteria were calculated. The results are presented in Table 1 . Inspecting Table 1 reveals that GLRRs outperform LASSO in most cases. As p increases, GLRRs outperform LASSO in terms of MEN, PEN, and R 2 . Under Cases 3 and 5, GLRRs outperform LASSO with much smaller errors as well as lower standard deviations for MEN and PEN. LASSO performs much better for continuous covariates than for discrete SNPs, but such patterns do not appear for GLRRs. The results of GLRRs and BLASSO are comparable in terms of both AIC and BIC, but the number of parameters under GLRRs is much smaller than that under BLASSO. BLASSO and G-SMuRFS perform well in terms of both model error and prediction. The high R 2 and low prediction error of BLASSO and G-SMuRFS in the high dimension cases may be caused by over-fitting and model misidentification (Fan and Lv, 2010) .\nThirdly, we used the ROC curve to compare LASSO, BLASSO, G-SMuRFS, GLRR3, and GLRR5 under Cases 1-5. See Figure 3 for details. For Case 1, BLASSO demonstrates consistently the best power for almost every level of specificity, while G-SMuRFS is the second best. GLRR3 and GLRR5 fall in the middle. For Case 4, all the methods appear to be comparable with GLRR3 and GLRR5. For Cases 2, 3, and 5, GLRRs consistently outperform all other methods.\nWe also compared the timing of each method in a personal laptop with Intel Core i5 1.7 GHz processor and 4 GB memory. It takes LASSO and G-SMuRFS roughly 5 minutes to choose the optimal penalty and calculate estimates for a single sample of Case 5. All Bayesian methods take much longer since one has to sample many MCMC samples. Specifically, BLASSO takes about 2.75 hours to generate 10,000 samples plus 10,000 thousand burn-ins. For the same number of samples, GLRR3 takes about 30 minutes and GLRR5 takes about 40 minutes.\nWe fitted GLRR (5) with all the baseline volumes of 93 ROIs in 749 subjects as a multivariate response vector, the 1,072 selected SNPs as X matrix, and age, intracerebroventricular volume (ICV), gender, education and handedness as prognostic related covariates. To determine the rank of B, GLRR was fitted for up to r = 10 layers. By comparing the five different selection criteria, we chose r = 3 layers for the final data analysis. We ran the Gibbs sampler for 20, 000 iterations after 20, 000 burn-in iterations. Based on the MCMC samples, we calculated the posterior median and maximum absolute deviation (MAD) of the normalized U and V, and B, and then we used the standard normal approximation to calculate the p\u2212values of each component of U, V, and B. The upper left panel of Figure 4 presents the estimated posterior median map of B, in which the elements with their p\u2212values greater than 0.01 were set to zero, which reveals sparsely distributed points along the horizontal and vertical directions in the estimated B, indicating that the lowrank model would fit the ADNI data reasonably well.\nWe used 1.426 \u00d7 MAD to compute robust standard errors from the posterior median based MAD for each element of B and used a normal approximation to compute its \u2212log 10 (p). Specifically, we created two new matrices based on the estimated B in order to detect important ROIs and SNPs. We first applied this thresholding method to B in order to compute a new matrix B bin , in which \u03b2 jk was set at zero if its \u2212log 10 (p) is less than 10, and We selected the top ROIs corresponding to the largest diagonal elementsof which are listed in the first column of Table 2 . We also picked the top ROIs based on the \u2212log 10 (p) \u2212values in each column of V, which are shown in the second, third, and fourth columns in Table 2 . The locations of these ROIs are shown in Figure 5 . Among these ROIs, the left and right sides rank close to each other, which may indicate structural brain symmetry.\nWe ranked the SNPs in the B bin B bin T according to the sum of the columns, and in the first three columns of the U matrix by their \u2212log 10 (p)\u2212values. The top 20 most significant SNPs and their corresponding genes are listed in Table 3 under columns B bin B bin T , U 1 , U 2 , and U 3 , respectively. To investigate the top SNPs and their relationship with ROI volumes in the coefficient matrix. we retained SNPs, which are correlated with at least one ROI at a significant level smaller than 10 \u22126.3 . For each SNP, we highlighted the locations of ROIs with correlation at a significant level smaller than 10 \u22126.3 , which are shown in Figure 6 . There are different patterns of SNPs' effects on ROIs: i) rs10792821 (PICALM), rs9791189 (NEDD9), rs9376660 (LOC651924), and rs17310467 (PRNP) are significantly correlated with a small number of ROIs with relative large coefficients; ii) rs4933497 (CH25H) and rs1927976 (DAPK1) are significantly correlated with a small number of ROIs with relative small coefficients; iii) rs1411290 (SORCS1), rs406322 (IL33), and rs1018374 (NEDD9) are significantly correlated with a large number of ROIs with medium coefficients; iv) rs1411290 (SORCS1), rs406322 (IL33) is significantly correlated with a large number of ROIs with small coefficients. Figure 7 shows the heatmap of coefficients among these 10 SNPs and the ROIs on the left and right hemispheres, respectively. The ROIs are chosen such that each ROI is significantly correlated to at least one of the 10 SNPs at a significance level small than 10 \u22126.3 . We were able to detect some SNPs, such as rs439401 (gene APOE), among others. The use of imaging measures as endophenotype may dramatically increase statistical power in detecting much more informative SNPs and genes, which deserve further investigation in Alzheimer's research. In contrast, most GWA studies often use case-control status as the response variable, which leads to substantially power loss.\nThe correlation structure among imaging phenotypes and that among SNPs are characterized by the columns in U and V, respectively. Each column of U represents a group of SNPs that are similarly associated with a group of ROIs determined by the corresponding column of V. The identified correlated phenotypes and genotypes largely agree with the spatial and LD structure, which are shown in Table 3 and Figure 5 . Multiple SNPs from the same gene appear in the same columns of U. However, due to different objectives, the structure captured by U and V may not be exactly identical to the correlation structure of imaging phenotypes only based on the phenotypes, and the LD structure of genotypes, respectively."}, {"section_title": "The Alzheimer's Disease Neuroimaging Initiative", "text": ""}, {"section_title": "Imaging Genetic Data", "text": "Imaging genetics is an emergent trans-disciplinary research field to primarily evaluate the association between genetic variation and imaging measures as continuous phenotypes. Compared to traditional case control status, since imaging phenotypes may be closer to the underlying biological etiology of many neurodegenerative and neuropsychiatric diseases (e.g., Alzheimer), it may be easier to identify underlying genes of those diseases (Cannon and Keller, 2006; Turner et al., 2006; Scharinger et al., 2010; Paus, 2010; Peper et al., 2007; Chiang et al., 2011b,a) . A challenging analytical issue of imaging genetics is that the numbers of imaging phenotypes and genetic markers can be relatively high. The aim of this data analysis is to use GLRR to specifically identify strong associations between imaging phenotypes and SNP genotypes in imaging genetic studies.\nThe development of GLRR is motivated by the analysis of imaging, genetic, and clinical data collected by ADNI. \"Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). The ADNI was launched in 2003 by the National Institute on Aging (NIA), the National Institute of Biomedical Imaging and Bioengineering (NIBIB), the Food and Drug Administration (FDA), private pharmaceutical companies and non-profit organizations, as a $60 million, 5-year publicprivate partnership. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD). Determination of sensitive and specific markers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as lessen the time and cost of clinical trials. The Principal Investigator of this initiative is Michael W. Weiner, MD, VA Medical Center and University of California, San Francisco. ADNI is the result of efforts of many coinvestigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 subjects but ADNI has been followed by ADNI-GO and ADNI-2. To date these three protocols have recruited over 1500 adults, ages 55 to 90, to participate in the research, consisting of cognitively normal older individuals, people with early or late MCI, and people with early AD. The follow up duration of each group is specified in the protocols for ADNI-1, ADNI-2 and ADNI-GO. Subjects originally recruited for ADNI-1 and ADNI-GO had the option to be followed in ADNI-2. For up-to-date information, see www.adni-info.org. \"\nOur problem of interest is to establish the association between SNPs on the top 40 AD candidate genes as listed on the AlzGene database (www.alzgene.org) as of June 10, 2010 and the brain volumes of 93 regions of interest, whose names and abbreviation are given in the supplementary document, while accounting for other covariates, such as age and gender. By using the Bayesian GLRR, we can easily carry out formal statistical inferences, such as the identification of significant SNPs on the differences among all 93 ROI volumes.\nThe MRI data, collected across a variety of 1.5 Tesla MRI scanners with protocols individualized for each scanner, included standard T1-weighted images obtained using volumetric 3-dimensional sagittal MPRAGE or equivalent protocols with varying resolutions. The typical protocol included: repetition time (TR) = 2400 ms, inversion time (TI) = 1000 ms, flip angle = 8\u00b0, and field of view (FOV) = 24 cm with a 256\u00d7256\u00d7170 acquisition matrix in the x-, y-, and z-dimensions yielding a voxel size of 1.25\u00d71.26\u00d71.2 mm 3 . The MRI data were preprocessed by standard steps including anterior commissure and posterior commissure correction, skull-stripping, cerebellum removing, intensity inhomogeneity correction, segmentation, and registration (Shen and Davatzikos, 2004) . Subsequently, we carried out automatic regional labeling by labeling the template and by transferring the labels following the deformable registration of subject images. After labeling 93 ROIs, we were able to compute volumes for each of these ROIs for each subject.\nThe Human 610-Quad BeadChip (Illumina, Inc., San Diego, CA) was used to geno-type 818 subjects with 228 Normal Controls (NC), 397 MCI, and 193 AD in the ADNI 1 database, which resulted in a set of 620,901 SNP and copy number variation (CNV) markers. Since the Apolipoprotein E (APOE) SNPs, rs429358 and rs7412, are not on the Human 610-Quad Bead-Chip, they were genotyped separately. These two SNPs together define a 3 allele haplotype, namely the \u220a2, \u220a3, and \u220a4 variants and the presence of each of these variants was available in the ADNI database for all the individuals. The software EIGENSRAIT in the package of EIGENSOFT 3.0 was used to calculate the population stratification coefficients of all subjects. To reduce population stratification effects, we only used 761 Caucasians from all 818 subjects. We used the baseline T1 MRI scans and genetic data from all 742 Caucasians.\nBy following Wang et al. (2012) , we selected SNPs belonging to the top 40 AD candidate genes by using quality control methods. The first line quality control steps include (i) call rate check per subject and per SNP marker, (ii) gender check, (iii) sibling pair identification, (iv) the Hardy-Weinberg equilibrium test, (v) marker removal by the minor allele frequency, and (vi) population stratification. The second line preprocessing steps include removal of SNPs with (i) more than 5% missing values, (ii) minor allele frequency smaller than 10%, and (iii) Hardy-Weinberg equilibrium p\u2212value < 10 \u22126 . This left us with 1,071 SNPs on 37 genes. We used the 1071 SNP and APOE-\u220a4 to form X, that gives p = 1, 072."}, {"section_title": "Discussion", "text": "We have developed a Bayesian analysis GLRR to model the association between highdimensional responses and high-dimensional covariates with an novel application in imaging genetic data. We have introduced a low rank regression model to approximate the large association matrix through the standard SVD. We have used a sparse latent factor model to more flexibly capture the complex spatial correlation structure among highdimensional responses. We have proposed Bayesian local hypothesis testing to identify significant effects of genetic markers on imaging phenotypes, while controlling for multiple comparisons. GLRR dramatically reduces the number of parameters to be sampled and tested leading to a remarkably faster sampling scheme and effcient inference. We have shown good finite-sample performance of GLRR in both the simulation studies and ADNI data analysis. Our data analysis results have confirmed the important role of well-known genes such as APOE-\u220a4 in the pathology of ADNI, while highlighting other potential candidates that warrant further investigation.\nMany issues still merit further research. First, it is interesting to incorporate common variant and rare variant genetic markers in GLRR (Bansal et al., 2010) . Second, external biological knowledge, e.g., gene pathways, may be the incorporated in the model through the use of more delicate priors to further regularize the solution (Silver et al., 2012) . Third, it is important to consider the joint of genetic markers and environmental factors on highdimensional imaging phenotypes (Thomas, 2010) . Fourthly, the key features of GLRR can be adapted to more complex data structures (e.g., longitudinal, twin and family) and other parametric and semiparametric models. For instance, for longitudinal neuroimaging data, we may develop a GLRR to explicitly model the temporal association between highdimensional responses and high-dimensional covariates, while accounting for complex temporal and spatial correction structures. Finally, it is important to combine different imaging phenotypes calculated from other imaging modalities, such as diffusion tensor imaging, functional magnetic resonance imaging (fMRI), and electroencephalography (EEG), in imaging genetic studies."}, {"section_title": "Supplementary Material", "text": "Refer to Web version on PubMed Central for supplementary material. Simulation results: the box plots of five selection criteria including , , , AIC, and BIC against rank r from the left to the right based on 100 simulated data sets simulated from model (4) Simulation results: comparisons of true B image and estimated true B images by using LASSO, BLASSO, G-SMuRFS, GLRR3, and GLRR5 under five different scenarios.\n) and BIC were calculated for each estimated . The sample size is n = 1000. Columns 1-5 correspond to Cases 1-5, respectively. The true ranks of B under Cases 1-5 are, respectively, 2, 5, 5, 100 and 100. The top row contains true B maps under Cases 1-5 and rows 2-6 correspond to the estimated under LASSO, Bayesian LASSO, G-SMuRFS, GLRR3, and GLRR5, respectively. For simplicity, only the first 100 rows and 100 columns of B were presented. Moreover, all plots in the same column are on the same scale. Comparisons of GLRR3, GLRR5, and LASSO under Cases 1-5: mean ROC curves based on GLRR3 (red line), GLRR5 (blue line), LASSO (black line), G-SMuRFS (dottedd line) and BLASSO (dashed line). For each case, 100 simulated data sets of size n = 100 each were used. Results of ADNI data: at a \u2212log 10 (p) significance level greater than 6.3, the top row depicts the locations of ROIs that are correlated with SNPs rs10792821 (PICALM), rs9791189 (NEDD9), rs9376660 (LOC651924), rs17310467 (PRNP), rs4933497 (CH25H), respectively; the bottom row shows the ROIs correlated with SNPs rs1927976 (DAPK1), rs1411290 (SORCS1), rs406322 (IL33), rs1018374 (NEDD9), and rs439401 (APOE). The sizes of the dots represent the absolute magnitudes of the regression coefficients. Ranked top SNPs based on the diagonal of and columns of U. "}]