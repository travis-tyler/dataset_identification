[{"section_title": "Abstract", "text": "Multi-modality data are widely used in clinical applications, such as tumor detection and brain disease diagnosis. Different modalities can usually provide complementary information, which commonly leads to improved performance. However, some modalities are commonly missing for some subjects due to various technical and practical reasons. As a result, multi-modality data are usually incomplete, raising the multi-modality missing data completion problem. In this work, we formulate the problem as a conditional image generation task and propose an encoder-decoder deep neural network to tackle this problem. Specifically, the model takes the existing modality as input and generates the missing modality. By employing an auxiliary adversarial loss, our model is able to generate high-quality missing modality images. At the same time, we propose to incorporate the available category information of subjects in training to enable the model to generate more informative images. We evaluate our method on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database, where positron emission tomography (PET) modalities are missing. Experimental results show that the trained network can generate high-quality PET modalities based on existing magnetic resonance imaging (MRI) modalities, and provide complementary information to improve the detection and tracking of the Alzheimer's disease. Our results also show that the proposed methods generate higher quality images than baseline methods as measured by various image quality statistics."}, {"section_title": "INTRODUCTION", "text": "Many clinical applications [35, 37] , such as tumor detection and brain disease diagnosis [1, 21, 30, 32] , require high-quality multimodality data in order to achieve good diagnostic results, since different modalities of a subject provide complementary information. While standardized methods for clinical tests have been developed to collect multi-modality data, there are some practical concerns in the process of obtaining some important and informative modalities. For example, the positron emission tomography (PET) modality is often used to reveal metabolic information, in addition to anatomical details provided by other common modalities like magnetic resonance imaging (MRI) modality. However, to obtain PET images of diagnostic quality, a living subject needs to take an injection of a radioactive tracer. It raises the risk of radioactive exposure, resulting in potential harm to one's health. Therefore, the PET scan is rejected in some cases, where the data of a subject are incomplete with missing PET modality. While completely safe methods have not been developed, it is desired to perform the multi-modality missing data completion, where one can generate missing modalities based on available modalities. In this work, we explore a deep learning [18] solution to this problem.\nSince multi-modality data are in a 3D imaging format, we formulate the missing data completion [33, 34] problem as a conditional image generation task; that is, we aim at generating missing modality images conditioned on existing modality images. As multimodality data are collected from the same subject, there must be some underlying relationships between modalities, although they focus on different information. Therefore, the task is feasible if one can capture the relationships and estimate a mapping from existing modalities to missing ones. Deep learning has achieved great success in such conditional image generation tasks, like image super-resolution [20, 25] , image-to-image translation [7, 14, 38] , and video prediction [23] . In addition, because both the inputs and outputs of such tasks are images, usually of the same spatial size, other image tasks of structured outputs such as image segmentation [2, 4, 6, 8, 9, 22, 27] are also related.\nWith the fast development of deep models on these tasks, we propose an efficient and effective model for multi-modality missing data completion. However, it is worth noting that there is a major difference in our problem. For all of the relevant tasks above, the models are trained on two types of images, used as inputs and outputs, respectively. During inference, one type of images is predicted given the other. In the multi-modality missing data completion task, another kind of information, the category labels of subjects, is available during training. Such labels cannot be used as inputs to a model, as they are not accessible when performing prediction. Nor should they be outputs, because they are not what we aim to predict. Yet they provide useful information to guide the missing data completion process. How to take an advantage of these labels during training without affecting the inference phase remains challenging. We propose an elegant approach to utilize them in our model, based on our understanding of generative adversarial networks [10] .\nWe evaluate our model on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database, where the MRI modality data are given and the PET modality data are missing. Experimental results show that the proposed network can generate high-quality PET modality data based on the MRI modality data, and provides complementary information to improve the detection and tracking of the Alzheimer's disease. Our results also show that the proposed methods generate higher quality images than baseline methods as measured by various image quality statistics."}, {"section_title": "RELATED WORK", "text": "The multi-modality missing data completion problem was previously addressed in [21] . The authors employed a 3D convolutional neural network (CNN) architecture with only two hidden layers. There are two main drawbacks in this approach. First, the model only has two hidden layers without any downsampling layers, resulting in that any voxel in the final feature maps only incorporates information from a receptive field of a limited size in the inputs. In practice, such behavior commonly leads to poor performance when the spatial size of inputs is large. To overcome this, cropped patches of a small size (15 \u00d7 15 \u00d7 15 \u00d7 1) were applied as inputs to their model, which leads to the second problem. Since the CNN architecture uses convolutional operations without padding and performs no upsampling, the outputs become even smaller, with a size of 3 \u00d7 3 \u00d7 3 \u00d7 1. However, the required outputs usually have a much larger spatial size. Therefore, during inference, they had to scan the model on the full inputs and then concatenated all the outputs together to form a final prediction. For instance, the spatial size of the missing PET modalities in the ADNI database is 64 \u00d7 64 \u00d7 64 \u00d7 1. Nearly ten thousand times of scanning is needed at least, which results in excessively slow inference. In addition, inconsistency happens near the edges of different output patches, hindering the performance of their model. Similar problems were observed in image tasks of structured outputs such as image segmentation. To address this, an encoderdecoder architecture, with different upsampling layers, was developed [2, 4, 6, 8, 9, 22, 27] . Basically, by adding upsampling layers, downsampling layers that expand the receptive fields are enabled in the encoder, allowing inputs of a larger size. Moreover, the outputs can have the same size as the inputs, making the inference efficient.\nIn terms of image generation tasks, generative adversarial networks (GANs) were proposed by [10] and achieved impressive results. The GAN framework consists of a generator network and a discriminator network. The generator maps from latent representations to images while the discriminator is used to distinguish generated images from images from the dataset. By incorporating conditional information in the latent representations, GANs can be easily generalized to conditional GANs [3, 24, 36] . Most recent studies on conditional image generation tasks [7, 14, 20, 38] employed the encoder-decoder architecture as the generator network, which encodes the conditional information to latent representations.\nTo understand the GAN framework, we prefer thinking of it as a generator network with an adversarial loss function instead of two networks. Note that the discriminator network is only involved during training. When performing inference, the generator network alone is applied. Thus, the discriminator can be considered as a loss function, which is trainable and differs from a regular fixed one in regular deep learning models. With such an interpretation of GANs, we are free to use extra available information in the discriminator during training to provide a better loss function, without any influence on the inference phase.\nIn recent studies, training the generator network with extra loss functions in addition to the adversarial loss was found beneficial [5, 7, 14, 20, 23, 25, 38] . It was pointed out that the adversarial loss function encourages generating sharp images while the generated images can be significantly different from true ones. In contrast, regular content loss functions, such as L 1 and L 2 loss, are able to force generating images of similar appearances to those in the dataset, but suffer from the blurring problem. As a result, by finding an appropriate tradeoff among various loss functions, it is possible to train the same model but achieve improved performance.\nIn this work, we propose a 3D encoder-decoder model with multiple loss functions of different functionalities for the multimodality missing data completion problem. Our model incorporates all available information during training and generates high-quality modality data in an efficient way."}, {"section_title": "METHODS", "text": ""}, {"section_title": "Problem Formulation", "text": "In multi-modality missing data completion problems, we are given a dataset of subjects, in which a subject I is composed of two modalities {x, y} and a corresponding category label \u2113. We assume the modality x is available for all subjects, while modality y is available for only a portion of the subjects. The training set consists of subjects with both x and y as {x i , y i , \u2113 i } N i=1 while the test set consists of subjects with only x. The multi-modality missing data completion task aims to predict y for subjects in the test set. In order to achieve this, we attempt to capture the mapping from x to y by developing a model on the training set. With the development of deep learning [17, 19] , we propose to model this relationship using a deep neural network. Specifically, our goal is to train a generator network G (x; \u03b8 ) to estimate the missing modality y where G is parameterized by \u03b8 . From a probabilistic perspective, suppose y is drawn from an underlying distribution p Y (y) and x is drawn from p X (x ). The model G estimates a conditional probability p G (y|x; \u03b8 ) to approximate p Y (y). Note that in this setting, the category label \u2113 is not used for prediction. We will discuss an approach to incorporate the category information in prediction in Section 3.4."}, {"section_title": "Content Loss", "text": "In order to train the generator network G, a content loss function is employed to encourage G (x; \u03b8 ) to be close to y. The most straightforward way to achieve this is to minimize the Euclidean distance between them, resulting in the mean squared error (MSE) loss L MS E defined as\n(\nGiven a training set {I i } N i=1 where I i = {x i , y i , \u2113 i }, the generator network G is trained by minimizing L M S E . To be specific, the optimal \u03b8 is obtained by solving the following optimization problem:\nThe ideal case is to have y = G (x;\u03b8 ), i.e., the trained generator network G works perfectly and outputs the true modality. However, in practice, minimizing the content loss function above can hardly achieve this, and the reason is clear in the probabilistic perspective. Minimizing L MS E can be interpreted as maximizing the likelihood of p G (y|x; \u03b8 ) by assuming that p G (y|x ) follows a Gaussian distribution. It is equivalent to minimizing the Kullback-\nThis training strategy suffers from the so-called \"blurry\" problem when p Y follows a complex distribution, such as a multimodal distribution, which is usually the case in multi-modality missing data completion tasks. For example, in Figure 1 , suppose the underlying distribution p Y is a mixture of two Gaussian distributions with two equally probable modes \u0434 1 and \u0434 2 . The optimized p G will be a unimodal Gaussian with a single mode (\nConsequently, the modalities predicted by G tend to be blurry due to the average of two modes.\nAnother problem of the content loss function L M S E is that it only minimizes element-wise differences between the predicted modalities and the true ones. It does not take any global similarity, like structural similarity between modalities, into consideration. To generate high-quality and informative modalities, we need to address the blurry problem and incorporate global information."}, {"section_title": "Adversarial Loss", "text": "In order to address the limitations suffered by the content loss L M S E , we propose to use the adversarial loss, based on generative adversarial networks (GANs) [10, 26] . In addition to the generator network G, a discriminator network D, parameterized by \u03b2, is employed. To be concrete, given the modality x and y of a subject, the discriminator D ((x, y); \u03b2 ) distinguishes whether the pair (x, y) is real or fake. Hence, D is a binary classification network. The corresponding training data is (x, y) from the subject of given dataset with label 1 and (x, G (x; \u03b8 )) with label 0, where G is a generator network.\nThe objective function of adversarial learning in our model can be expressed as follows:\nwhere the generator G is parameterized by \u03b8 and the discriminator D is parameterized by \u03b2. We optimize the adversarial loss based on minimax game theory. In this process, we first train the discriminator D to distinguish the true modality pairs from predicted modality pairs. Hence, the discriminator is a binary classifier. We give the true modality pair (x, y) a label of 1 and the predicted modality pair (x, G (x )) a label of 0. We minimize the following cross-entropy loss to train the classifier:\nwhere k is the number of classes, c \u2208 R is the true label,\u0109 \u2208 R k \u00d71 , c i is the probability that the sample belongs to category i, and 1{\u00b7} is the indicator function. In this work, the discriminator is a binary classifier. Hence, k = 2 in Eq. (4). The loss function of the discriminator D can be expressed as:\nGiven the same dataset in Section 3.2, the optimal \u03b2 can be obtained by solving the following optimization problem:\nResearch Track Paper KDD 2018, August 19-23, 2018 , London, United Kingdom\nThe generator is optimized to estimate the missing data so that the generated data is hard to be distinguished from the true data by the discriminator. Therefore, we train the generator with the following objective function L G by fixing the parameter \u03b2 in discriminator:\nGiven the same dataset in Section 3.2, the optimal \u03b8 can be obtained by solving the following optimization problem:\nBy optimizing the generator and the discriminator iteratively, the generator can generate the missing modality data that is hard to be distinguished from the true data by the discriminator. To see why the adversarial loss can alleviate the blurry effect caused by optimizing L M S E , we again use the probabilistic perspective. Given the same example in Section 3.2, the optimized generator network may not predict an averaged-mode data (\u0434 1 + \u0434 2 )/2, since this data can be distinguished from the true data by the discriminator network. To fool the discriminator, the generator must predict the modality which is close to the true distribution. Therefore, employing the adversarial loss can alleviate the blurry effect of L M S E .\nIn addition, the discriminator takes the modality pair as input and distinguishes whether the pair is true or predicted. It does not consider the distance between each position in the true and predicted modalities. Therefore, optimizing the adversarial loss also encourages the predicted modality to be close to the true modality in a global view."}, {"section_title": "Classification Loss", "text": "The category label is of great importance in multi-modality missing data completion tasks. The input data has different categories. For subjects from different categories, the relationship between the input modality and the missing modality can be different. Therefore, it is necessary to take category label into consideration for the missing data completion task. The challenge is that the category label is not available when completing the missing modality on test data. To overcome this problem, we propose to employ an auxiliary classification loss in the discriminator to distinguish the different categories of inputs.\nIn the proposed model, the discriminator produces not only the real/fake probability distribution but also the category probability distribution of the input pairs. Therefore, the classification loss consists of two parts; those are, the cross entropy loss for true pairs and predicted pairs. The definition of L D has been discussed in Section 3.3. The classification loss function L C LS can be described as follows:\nThe discriminator is trained to minimize L D +L C LS and the generator is trained to minimize L G + L C LS . The predicted modality is not only close to the true modality in order to fool the discriminator but also takes the category information into consideration to optimize L C LS . Given the same dataset in Section 3.2, the optimal \u03b2 can be obtained by solving the following optimization problem:\nThe optimal \u03b8 can be obtained by solving the following optimization problem:\u03b8\nAnother reason why we employ an auxiliary classification loss in the discriminator is that it can make the training procedure stable. The unstable training procedure makes adversarial networks difficult to train. It has been shown in prior work [25] that the category label information can make the training stable, leading to improved quality of the predicted sample."}, {"section_title": "The Proposed Optimization Problem", "text": "Training the generator with L G +L C LS encourages the generator to produce data which confuse the discriminator. The adversarial loss distinguishes the predicted and true data in a global perspective. The auxiliary classifier can make the training procedure stable. However, if we only employ the adversarial loss and classification loss to optimize the generator, the generator can generate data that has a similar contour to the true modality. This predicted data can confuse the discriminator, but it loses many details. To overcome this problem, we combine the L M S E , L D and L C LS in our optimization. L M S E encourages the learning of detailed information for completing the missing modality. The adversarial loss is employed to alleviate the blurry effect of L M S E loss and improve the quality of predicted data. The classification loss is used to make the training procedure stable and take category label into consideration to improve the completion performance. Therefore, the overall loss function can be described as follows:\nwhere \u03bb M S E ,\u03bb G ,\u03bb C LS are tradeoff parameters for each loss. The algorithm for optimizing the problem in Eq. (12) is given in Algorithm (1)."}, {"section_title": "Algorithm 1:", "text": "Training the proposed deep adversarial networks for missing modality completion. Set the batch size m, learning rates \u03c1 D and \u03c1 G , and weights\nUpdate the discriminator D:\n\u2022 Get m pairs of modality and category label\n\u2022 Update the generator by:\nResearch Track Paper KDD 2018, August 19-23, 2018, London, United Kingdom"}, {"section_title": "Brain Data Completion", "text": "In this work, we focus on the brain multi-modality [29] missing data completion problem as illustrated in Figure 3 . Specifically, x is the MRI modality which is available for all subjects, and y is the PET modality which is missing for some subjects. In addition, we are also given the category labels in the dataset, which refer to the prodromal stage including Normal controls (NC), Alzheimer's disease (AD), sMCI (the patient's symptom is stable and will not progress to AD in 18 months) , pMCI (the patient will progress to AD in 18 months). In order to predict the PET modality from MRI modality, we need to construct a generator which can capture the relationship between the MRI modality and PET modality. Since both modalities are 3D data and the PET modality has the same spatial size as the MRI modality, we construct a 3D encoder-decoder network with skip connection as the generator [2, 4, 27] . The encoder network consists of 3D convolutional layers [15] . The decoder network consists of 3D convolutional layers and 3D deconvolutional layers. The 3D convolutional layers are used to extract features and predict the values of voxels in the PET modality. We employ the 3D convolutional layers with stride 2 \u00d7 2 \u00d7 2 in the network to reduce the size of feature maps and increase the receptive fields of output voxels. The encoder-decoder architecture can extract features from the MRI modality with different scales. Because the MRI modality and the PET modality have the same size, 3D deconvolutional layers [9] are used to restore the spatial information. We add skip connections [11, 12] in the network between the corresponding encoder and decoder layers to allow the information to transmit directly in each level of the network.\nThe whole pipeline of brain modality completion is illustrated in Figure 2 . The 3D encoder-decoder network takes the MRI modality as input and generates the PET modality. The predicted PET modality is concatenated with the MRI modality and used as an input pair for the discriminator network. The true modality pair is also fed as input to the discriminator network. The discriminator network is trained with the combined loss of L D and L C LS . The generator network is trained with the combined loss of L M S E , L G and L C LS . When completing the PET modality, only the generator network is required to take the MRI modality as input."}, {"section_title": "EXPERIMENTS", "text": "In this section, we evaluate our proposed model on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database [32] and compare the quality of predicted data using different loss functions. Our code is publicly available 1 ."}, {"section_title": "Data Preprocessing", "text": "In this work, we use the data for 398 subjects in the ADNI dataset. Each subject has an MRI modality and a corresponding PET modality [1, 30] . These subjects cover four different prodromal stages (NC, sMCI, pMCI, AD). There are 93 AD subjects, 101 NC subjects, 76 pMCI subjects, and 128 sMCI subjects in the dataset.\nFor each subject in the dataset, we correct the intensity inhomogeneity and remove the cerebellum in the T1-weighted MRI modality. The MRI modality is segmented into gray matter, white 1 https://github.com/divelab/completion/ matter and cerebrospinal fluid. In the experiments, we use the gray matter tissue density maps as the input modality and predict the output PET modality. The PET modality obtained from the ADNI dataset is also rigidly aligned to the corresponding MRI modality. We employ a unit standard deviation Gaussian kernel to smooth both the MRI and PET modalities in order to improve the signal-tonoise ratio. We downsampled both the input and output modalities to 64 \u00d7 64 \u00d7 64 \u00d7 1 voxels to reduce the computational cost. The network architecture is shown in Table 1 . The depth of the generator network is 5. Each layer contains a downsampling block and a corresponding upsampling block. In the downsampling block, we employ two 3D convolutional layers to extract features. To facilitate the training, we use batch normalization [13] after each convolutional layer. The stride of the second convolutional layer in the downsampling block is set to 2 \u00d7 2 \u00d7 2 to increase the receptive fields of voxels in output. The number of output feature maps is 16 in the first downsampling block. For the following blocks, the number of output feature maps is twice of the number in the previous block. In terms of the upsampling block, we employ 3D deconvolutional layer to restore the spatial size. We use skip connections between the upsampling block and the corresponding downsampling block to directly transmit information. The concatenated feature maps are fed into a 3D convolutional layer. The learning rate [16] of the generator network is set to 1e-3, and the batch size is set to 5."}, {"section_title": "Experimental Setup", "text": "We employ a 3D convolutional network for the discriminator. The architecture of the network is shown in Table 2 . The network consists of four 3D convolutional layers and one fully connected layer. Since the adversarial loss is difficult to train, we employ several techniques in training. Specifically, we use the batch normalization layer after each convolutional layer. Instead of using \nmax-pooling layers, we employ 3D convolutional layers with stride 2 \u00d7 2 \u00d7 2 to implement downsampling. We use dropout layers with probability 0.5 after each batch normalization layer. In tradition adversarial networks, the number of output nodes for fully connected layers is set to one. To make the training procedure stable, we add a classification loss in the discriminator network. Since the PET modality covers four different categories, the number of output nodes for the fully connected layer is set to five. One node represents whether the pair of MRI and PET modalities is real or fake. The remaining four nodes represent the probabilities of each category for the pair of modalities. The learning rate of the discriminator is set to 2e-4. In this work, we focus on evaluating our proposed method for missing data completion. The ADNI dataset contains 398 subjects and each subject has a pair of MRI and PET modalities. We randomly select 200 subjects in the dataset as the training set to train the model. We evaluate the performance of the generators with different losses on the remaining 198 subjects."}, {"section_title": "Results and Analysis", "text": "In the experiments, we train the 3D encoder-decoder network using different loss functions and evaluate the predicted modality on the test dataset. To evaluate the quality of predicted modality and the similarity between the predicted modality and the true modality, we employ different quantitative evaluation metrics. The results are summarized in Table 3 .\nWe employ the peak signal-to-noise ratio (PSNR) [28] to measure the quality of the predicted modality. PSNR is generally used to measure the quality of predicted data in video prediction tasks. The\nResearch Track Paper KDD 2018, August 19-23, 2018 , London, United Kingdom definition of PSNR between the predicted modality\u0177 and the true modality y is defined as follows:\nPSNR(\u0177, y) = 10 log 10 max 2\u0177\nwhere max\u0177 represents the maximum possible voxel value of the modality. Larger PSNR values imply larger similarity between the predicted modality and the true modality. We can observe from the results in Table 3 that the generator using the combination of L M S E , L G and L C LS achieves the highest PSNR value. The definition of PSNR value is given based on the MSE value. The generator that only uses L M S E as a loss function obtains better PSNR value than the generator using L M S E and L G . Compared with the generator using L M S E , the model using L MS E and L G has to balance L M S E and L G . Therefore, the PSNR value of the generator with L M S E is better than the model with the combination of L G and L M S E . After using L C LS , the generator achieves the best PSNR value on the test dataset. This also demonstrates that our model has a better generalization ability.\nWe also use the structural similarity index measure (SSIM) [31] to measure the similarity between the predicted and the true modalities. Different from the MSE and PSNR measurements, SSIM does not measure the absolute difference. It focuses on the difference in terms of structural information. The SSIM score is calculated using the following equation:\nwhere \u00b5 x is the mean of x, \u00b5 y is the mean of y, \u03c3 2 x is the variance of x, \u03c3 2 y is the variance of y, \u03c3 xy is the covariance of x and y, c 1 and c 2 are two constant variables. The value of SSIM score ranges between -1 and 1.\nThe PSNR score measures the voxel-wise absolute error between the two modalities. The SSIM score evaluates the similarity between two modalities based on the structure information. The model using the combination of L M S E and L G obtains a lower PSNR value compared with the generator employing L M S E . But it achieves significant improvement on the SSIM score. Since the discriminator distinguishes the real/fake modality from a global perspective, the Research Track Paper KDD 2018, August 19-23, 2018 , London, United Kingdom modality predicted using the combination loss has better performance on maintaining structure information and thus achieves a higher SSIM score. We can observe from the results that employing L C LS in the discriminator improves the performance in terms of both PSNR and SSIM scores. This demonstrates that the category information is important for the PET modality completion task. Our model can predict the missing PET modality without providing the category label as the input. One key challenge in modality completion tasks is to produce sharp images. To measure the sharpness of predicted modality and show the effects of adversarial loss. We employ the sharpness score proposed in [23] to measure the sharpness between the predicted modality and the true modality. The definition of sharpness is described as follows:\nSharp(x, y) = 10 log 10 max 2\nThe sharpness is calculated based on the gradient between the voxels in the original modality. A higher score is obtained if the gradient in the predicted modality is close to the gradient in the original modality.\nWe can observe from the results that our proposed method outperforms the generator using L M S E . L M S E is employed based on the assumption that the data follow a single Gaussian distribution. When the subjects in dataset follow a more complex distribution, the generator using L M S E predicts blurry modality. The sharpness values in the table also verify this observation. The generator model using L MS E obtains the lowest sharpness value. When we employ an additional adversarial loss in the model, the effect of L M S E is alleviated and the PET modality predicted using the combined loss is sharper. This demonstrates that the discriminator can successfully distinguish the averaged modality from the true modality. By introducing the adversarial loss, the model can produce sharper images. The adversarial loss is difficult to train. We employ a classification loss in discriminator network to make the training procedure stable. Thus, the generator with the combination of three losses achieves the best sharpness value.\nIn Figure 4 , we provide the true PET slices and predicted PET slices using different loss functions. We can observe from the results that the images obtained from the generator using the combination of three losses is closer to the true PET modality as compared with the results of other models. In the true PET modality, there exists a clear contour in the image. The images generated by the model with the MSE loss are blurry and the contour of the components is also not clear. By introducing the adversarial and classification loss, the effect of L M S E is alleviated.\nWe also compare the classification results of the true PET modality and the predicted PET modalities using different losses. If the predicted modality is close to the corresponding true PET modality, the category label should be the same. Therefore, we employ a logistic regression model as a classifier to evaluate the predicted modality using different losses.\nWe train a logistic regression classifier based on the PET modality in the training dataset and test the classification accuracy based on the predicted modality using different losses in the test dataset. The results of classification accuracy are shown in Table 4 . The PET modality covers four different categories. In Table 4 , we provide both multi-class classification accuracy and binary classification accuracy. We can observe from the results that the model with three losses achieves higher multi-class classification accuracy than the other models, including that of the true PET modality. The development of Alzheimer's disease includes four stages (NC, sMCI, pMCI, AD). We extract subjects from each category and construct a binary classifier. The challenge in the diagnosis of Alzheimer's disease is to distinguish the neighboring stages. We can also observe from the results that the classification accuracy of neighboring stages is lower than that of other stages. For example, the classification accuracies of NC/sMCI, sMCI/pMCI, pMCI/AD are lower than that of others. The classification accuracies of our model for AD/NC and NC/pMCI tasks are lower than the model with L M S E . However, these two tasks can be successfully completed by only using the MRI modality. In terms of neighboring stages classification tasks, our predicted data outperforms other modalities, including that of the true PET modality. This demonstrates that our predicted PET modality can be used to improve the accuracy of disease diagnosis."}, {"section_title": "CONCLUSION AND DISCUSSION", "text": "In this work, we propose a deep learning model for completing the missing modality and apply it to Alzheimer's disease diagnosis. Our model takes the MRI modality as input and predicts the corresponding PET modality, which can improve the accuracy of Alzheimer's disease diagnosis. Our model employs a 3D encoder-decoder network to capture the relationship between the MRI modality and PET modality. To alleviate the blurry effect of L M S E and generate high quality data, we employ an adversarial loss in our model. Since the category label information is of key importance in data completion, we add an additional classification loss in the discriminator. In this way, we can complete the missing modality without the category label information as an input. The classification loss can also make the training procedure stable. Experiment results on the ADNI dataset demonstrate that our predicted PET modality achieves higher quality both in similarity and sharpness compared to the predicted modality with L M S E . In addition, our predicted PET modality outperforms other predicted modalities in classification tasks. This also demonstrates our model can improve the accuracy of disease diagnosis.\nIn this work, we focus on completing one modality data based on data from another modality. Our method can be applied to complete multiple modalities simultaneously based on data from another set of modalities. For example, another application in disease diagnosis is to predict high-dose PET data based on MRI and low-dose PET data [1, 30] , as high-dose PET data are not available for many patients. We plan to explore those applications in the future."}]