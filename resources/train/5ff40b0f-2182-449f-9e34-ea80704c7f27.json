[{"section_title": "Introduction", "text": "Deep Learning (DL) has emerged as the most popular machine learning technique in recent years, especially in computer vision. It has brought transformative impact in industries and quantum leaps in the quality of a wide range of everyday technologies including face recognition (Schroff et al., 2015; Taigman et al., 2014; Parkhi et al., 2015; Wen et al., 2016; Liu et al., 2019) , speech recognition Chung et al., 2014; Kim, 2014; Graves, 2013; Ravanelli et al., 2018) and machine translation Sutskever et al., 2014; Luong et al., 2015; Vaswani et al., 2018) . Most of these systems are built based on learning a deep neural network (DNN) model from a huge amount of data. However, it has been observed that these deep learning systems could fail in some underrepresented cases (Johnson and Khoshgoftaar, 2019; Lin et al., 2017; Chan et al., 2019; Fern\u00e1ndez et al., 2018; Huang et al., 2019) . For example, Apple's FaceID (a face recognition system) is much less accurate for recognizing a child than an adult (Bud, 2018) , and an autonomous driving car might fail at night under the same road condition (Wakabayashi, 2018) .\nThe key factors that cause these problems are (i) the training data sets collected from the real-world are usually highly imbalanced (e.g., the number of facial images of children are much less than that of adults); (ii) Figure 1 : A two-dimensional linear binary classification toy example with logistic loss. We optimize the model for one iteration using SGD with different weighting schemes for the same stochastic minibatch. The classifier generated in iteration 1 vividly demonstrates the effectiveness of attentional biased SGD, compared to not doing it, on handling data imbalance. current deep learning systems lack the capability to handle imbalanced data distribution well. Most modern deep learning techniques in the literature are crafted and evaluated on benchmark datasets (e.g., ImageNet data for image classification), which are created by humans intentionally with balanced distributions among different classes.\nTwo categories of approaches have been well studied for tackling data imbalance, i.e., under/over-sampling based approaches (Chawla et al., 2002; Han et al., 2005; Chawla, 2009 ) and cost-sensitive loss minimization (Zhou and Liu, 2005; Sun et al., 2007; Scott, 2011; Parambath et al., 2014; Narasimhan et al., 2015; Elkan, 2001; Busa-Fekete et al., 2015; Yan et al., 2017) . These two categories of approaches have been investigated for deep learning (Johnson and Khoshgoftaar, 2019; Masko and Hensman, 2015; Lee et al., 2016; Khan et al., 2017) . But existing studies in these directions are not very successful for deep learning with big data. For example, several studies have found that over-sampling yields better performance than using undersampling (Johnson and Khoshgoftaar, 2019) . But over-sampling will add more examples to the training data, which will lead to increased training time. The idea of cost-sensitive loss minimization is to assign classlevel weights to data belonging to different classes aiming to balance the contributions from different classes. However, this type of methods neglect the differences between different examples in the same class despite it might require much parameter-tuning of class-level weights. Beyond the aforementioned two categories of methods, some recent works tackle the data imbalance issue from different perspectives, including crafting new individual loss functions (Cao et al., 2019) , and learning individual-level weights (Jamal et al., 2020; Ren et al., 2018) . For example, Jamal et al. (2020) ; Ren et al. (2018) proposed meta-learning methods to learn the individual weights along with updating the model parameters. These methods require maintaining a balanced validation data to update the individual weights for sampled data and require multiple backward propagations at each iteration, which incurs additional significant burden for training an neural network on big data.\nIn this paper, we propose a simple yet systematic attentional biased stochasic gradient descent (AB-SGD) method for addressing the data imbalance issue. Our method is a simple modification of the popular momentum SGD method for deep learning by injecting individual-level importance weights to stochastic gradients in the mini-batch. These importance weights allow our method to focus on hard examples using an attentional mechanism. Unlike existing attention models that learn attentional weights on more fine-grained level (e.g., words in language (Vaswani et al., 2017) , and regions in images (Posner et al., 1980) ), our method is using the attention on the level of samples in a mini-batch. This idea is illustrated in Figure 1 on a toy imbalanced dataset by comparing it with the standard momentum SGD method for deep learning. Unlike existing meta-learning methods, our individual-level weights are self-adaptive that are computed based on the loss value of each individual data. In particular, the weight for each example is proportional to exponential of a scaled loss value on that example. The weighting scheme is grounded in the theoretically justified distributionally robust optimization (DRO) framework for learning with imbalanced data. Specifically, our method can be considered as a stochastic momentum method for solving an information-regularized distributionally robust optimization (IR-DRO) problem (Zhu et al., 2019) defined on all possible data. From this perspective, our method has several unique features. (i) The weights for all examples in the mini-batch have a proper normalization term to ensure the method optimizes the IR-DRO problem, which is updated online. We prove a theorem to show that our method converges to a stationary solution of the non-convex IR-DRO problem (with a certain convergence rate). (ii) The scaling factor before the loss value in the exponential function is interpreted as the regularization parameter in the DRO framework. In order to balance between the learning of feature extraction layers and the learning of the classifier layer of a DNN, we employ a step damping strategy for the scaling factor in our method. In addition, our method has two benefits: (i) it is applicable in online learning, where the data is received sequentially; (ii) it is loss independent, and can be combined with all existing loss functions crafted for tackling data imbalance. Finally, we summarize our contributions below:\n\u2022 We propose a simple attentional biased stochasic gradient descent method with momentum and selfadaptive importance weighting to tackle deep learning with imbalanced data, which is named as AB-SGD.\n\u2022 We prove that ABSGD finds a stationary solution of a non-convex information-regularized distributionally robust optimization problem for learning a deep neural network, and establish its convergence rate.\n\u2022 We compare ABSGD with a variety of existing methods for addressing the data imbalance issue, including crafted loss functions, class-balancing weighting methods, meta-learning methods for individuallevel weighting, and demonstrate superb performance of ABSGD."}, {"section_title": "Related Work", "text": "In this section, we provide a review on related work for addressing the issue of data imbalance. We will focus on four categories of methods.\nClass-level Weighting. The idea of class-level weighting is to introduce weights to examples at the class level to balance the contributions from different classes. This idea is rooted in cost-sensitive classification in machine learning (Ling and Sheng, 2008) . Typically, the class-wise weights are proportional to the inverse of class sizes (Huang et al., 2016; Yin et al., 2018) . Cui et al. (2019) proposed an improved class-level weighting scheme according to inverse of the \"effective number\" of examples per class. It is also notable that over/under-sampling methods have the same effect of introducing the class-level weighting to the training algorithm. We can see that these class-level weighting schemes usually require certain knowledge about the size (distribution) of each class, which makes them not suitable to online learning where the size of each class is not known beforehand. These methods also neglect the differences between different examples from the same class (cf. Figure 1 ). Crafted Individual Loss Functions. Some crafted individual loss functions have been proposed for tackling data imbalance. A popular loss function is known as the focal loss (Lin et al., 2017) , which is a modification of the standard cross-entropy loss. Specifically, it is defined as \u2212(1 \u2212 p t ) \u03b3 log(p t ) where \u03b3 > 0 is a tuning parameter p t is the estimated probability for the ground-truth class. The focal loss has been observed to be effective for dense object detection and is also widely used for classification with imbalanced data due to its simplicity (Goyal and Kaiming, 2018) . However, the focal loss lacks theoretical foundation. To complement this, Cao et al. (2019) proposed a theoretically-principled label-distribution-aware margin loss, which injects uneven margins into the cross-entropy loss, where the margin for each class is proportional to inverse of each class size to the power of 1/4. Our method is loss independent and hence can be combined with these existing crafted individual loss functions. Meta-Learning Methods. Meta-learning methods have been proposed for improving the performance on imbalanced datasets. The idea is to learn individual-level weights by solving a two-level optimization problem. In particular,\nwhere D denotes the training dataset, C denotes a balanced validation dataset, w denotes the model parameter, z i denotes a data, L(w; z) denotes the loss value of model w on data z, and \u03b8 = (\u03b8 1 , . . . , \u03b8 |D| ) denotes the weights on the training examples. Ren et al. (2018) directly optimized the individual weights in the framework of meta-learning with a heuristic trick by normalizing the weights of all examples in a training batch so that they sum up to one. Jamal et al. (2020) considered the problem from the perspective of domain adaptation and decomposed the individual weight into sum of a non-learnable class-level weight and a learnable individuallevel weight. One issue of these meta-learning methods is that they require three back-propagations at each iteration, which is computationally more expensive than our method that is about the same cost of standard SGD for DL. Optimization of DRO. DRO is a useful technique for domain adaptation, which has been shown both theoretically and empirically promising for learning with imbalanced data (Shalev-Shwartz and Wexler, 2016; Namkoong and Duchi, 2017; Qi et al., 2019) . However, most existing optimization algorithms for DRO are not practical for deep learning, which dims the usefulness of DRO. In the literature, DRO is formulated as (Rahimian and Mehrotra, 2019; Namkoong and Duchi, 2017) :\nwhere \u2206 n = {p \u2208 R n : i p i = 1, p i \u2265 0} denotes an n-dimensional simplex, h(p, 1/n) is a divergence measure or constraint between p and uniform probabilities 1/n, r(w) is a standard regularizer on w. We can see DRO aims to minimize the worst-case loss over all the underlying distribution p in an uncertainty set specified by h(p, 1/n). Many primal-dual optimization algorithms have been designed for solving the above problem for DL (Rafique et al., 2018; Yan et al., 2020) . However, the dual variable p in the above min-max form is an n-dimensional variable restricted to a simplex, which makes existing primal-dual optimization algorithms computationally expensive and not applicable for the online setting where the data is coming sequentially. Our method can be considered as a solution to addressing these issues by considering a specific information-oriented regularizer h(p, 1/n) = \u03bb i p i log(np i ) that is the KL divergence between p and uniform probabilities 1/n, which allows us to transform the min-max formulation into an equivalent minimization formulation with a compositional objective. From this perspective, our method resembles a recently proposed dual-free algorithm RECOVER (Qi et al., 2020) . However, RECOVER requires computing stochastic gradients at two different points in each iteration, which causes their GPU cost to double ours. In addition, RECOVER is a variance-reduction method, which might have poor generalization performance."}, {"section_title": "Attentional Biased SGD with Momentum (ABSGD)", "text": "In this section, we present the proposed method ABSGD and its analysis. We first describe the algorithm and then connect it to the DRO framework. Then we present the convergence result of our method for solving IR-DRO. Throughout this paper, we let z = (x, y) denote a random sample that includes an input x \u2208 R d and a ground truth label y \u2208 {1, . . . , K}, w \u2208 R d denote the weight of the underlying DNN to be learned. Let f (x) \u2208 R K be the prediction score of the DNN on data x, and (f ; y) denote a loss function. For simplicity, we let L(w; z) = (f (x); y). A standard loss function is the cross-entropy loss where (f ; y) = \u2212 log exp(fy(x)) K k=1 exp(f k (x)) . We emphasize that our method is loss independent, and can be applied with any loss functions (f ; y). More importantly, we can employ the class-level weighted loss functions such as the class-balanced loss (Cui et al., 2019) , crafted individual loss functions such as label-distribution aware margin loss (Cao et al., 2019) "}, {"section_title": "Algorithm", "text": "The proposed algorithm ABSGD is presented in Algorithm 1. The key steps are described in Step 4 to Step 8, and the key updating step for w t+1 is given by\nwhere r(w) \u221d 1/2 w 2 2 denotes a standard 2 norm regularization (i.e., for contributing weight decay in the update). The above update is a simple modification of the standard momentum method (Polyak, 1964) , where the last term \u03b2(w t \u2212 w t\u22121 ) is a momentum term. The modification lies at the introduced weight p i for each data z i in the mini-batch. The individual weight p i is computed in Step 7 and is proportional to exp(L(w t ; z i )/\u03bb), where \u03bb is a scaling parameter. Intuitively, we can see that a sample with a large loss value tends to get a higher weight. It makes sense for learning with imbalanced data since the model leans towards fitting the data from the majority class well rendering the loss value on the minority class larger. Hence, the data from the minority class tends to get a larger weight p i . This phenomenon is demonstrated on a toy dataset in Figure 1 .\nThere are several other features of Algorithm 1 that deserves further discussion. First, the weight p i is properly normalized dividing a quantity s t+1 that is updated online. In particular, s t+1 maintains a moving average of exponential of the scaled loss value on the sampled data (Step 6). It is notable that the normalization does not make the sum of p i in the mini-batch equal to 1. We emphasize that this normalization is essential in twofold: (i) it stabilizes the update without causing any numerical issue; (ii) it ensures the algorithm converge to a meaningful solution as presented in next subsection. Second, the algorithm has two stages divided by the value changing of \u03bb. This has a connection to the regularization impact of \u03bb. With larger \u03bb, our method is getting close to the standard momentum SGD method without weighting. In particular, when \u03bb = \u221e, the update becomes the standard momentum SGD method. When \u03bb becomes smaller, the update will focus more on data with larger loss values (e.g., from the minority class). This uneven weighting is helpful to learn a robust classifier. However, it harms the learning of feature extraction layers. This phenomenon has been also observed in previous works (Cao et al., 2019) . Hence, to address this issue we design a step damping strategy for \u03bb by dividing the learning into two. For example, in the first stage, we use a larger value of \u03bb 0 to facilitate the learning of feature extraction layers and in the second stage we decrease the value of \u03bb to a smaller value \u03bb 1 < \u03bb 0 to facilitate the learning of classifier. It is notable that in the second stage, we also need to reset s T0 = 0 due to the value change of \u03bb. Also note that this two-stage scheme can be extended to multiple stages with a decreasing sequence of \u03bb. Finally, in Algorithm 1 we allow the learning rate \u03b7 to be annealed after some iterations following the standard learning rate strategy for the momentum SGD method (Krizhevsky et al., 2017) ."}, {"section_title": "Connection with DRO", "text": "In this subsection, we explain ABSGD by connecting it to the framework of DRO. In particular, given n training samples {z 1 , . . . , z n } we consider the following information-regularized DRO formulation\nAlgorithm 1 ABSGD 1: Require: \u03bb 0 > \u03bb 1 , T 0 , T 1 , \u03b3 \u2208 (0, 1), \u03b2 \u2208 (0, 1), \u03b7 2: Initialize w 1 randomly, 3: Set \u03bb = \u03bb 0 , and s 0 = 0 Stage I 4: for t = 1, . . . , T 0 do 5:\nUpdate w t+1 by Equation (2) 10: end for 11: Set \u03bb = \u03bb 1 , s T0 = 0\nReset \u03bb, s for Stage II 12: Optional: \u03b7 = \u03b7/\u03c4 : anneal \u03b7 by a factor \u03c4 if necessary 13: for t = T 0 + 1, . . . , T 0 + T 1 do 14:\nRepeat Steps 4 \u223c 10 as in the Stage I 15:\nOptional: anneal \u03b7 after some iterations if necessary 16: end for 17: Optional: Add more stages with smaller \u03bb if necessary Note that unlike many existing studies of DRO that regularizes p by a constraint, we use the KL divergence between p and 1/n as regularization function, i.e., use \u03bb n i p i ln(np i ) to regularize p, where \u03bb is the regularizer parameter. Robustness of DRO. The DRO framework can be understood as a way of addressing domain adaptation. In particular, the empirical uniform distribution over training examples {z 1 , . . . , z n } does not necessarily represent the distribution in testing data. Hence by introducing the non-uniform probabilities p we can capture the domain shift between training data and the testing data. There is an interesting observation regarding \u03bb. When \u03bb = \u221e, then p i = 1/n and the above problem becomes the standard empirical risk minimization problem:\nWhen \u03bb = 0, then p has only one component equal to 1 that corresponds to the data with largest loss value, and the problem becomes the maximal loss minimization:\nThe above maximal loss minimization has been studied for learning with imbalanced data (Shalev-Shwartz and Wexler, 2016) . It was shown theoretically to yield better generalization performance than empirical risk minimization. However, the maximal loss minimization is sensitive to outliers. Hence, by varying the value of \u03bb we can enjoy balanced robustness to the imbalanced data and outliers. Optimization of DRO. It is nice that the DRO formulation is robust to imbalanced data. However, the minmax formulation of DRO is not friendly to the design of efficient optimization algorithms, especially given the constraint p \u2208 \u2206 n . To this end, we follow Qi et al. (2020) to transform the min-max formulation into an equivalent minimization formulation. In particular, we first compute the optimal solution of dual variable p * for the inner maximization problem given w. By computing the first-derivation of equation (3) in terms of p and setting it to zero, i.e. \u2207 p F (w, p) = 0, we have p * :\nBy substituting p * into the min-max problem of DRO, we obtain the following equivalent minimization formulation:\nIn an online learning setting, we can further generalize the above formulation as\nGiven the above minimization formulations, our method ABSGD can be considered as a stochastic algorithm for optimizing (5) in offline learning or optimizing (6) in online learning. Our method is rooted in stochastic optimization for compositional optimization that has been studied in the literature (Wang et al., 2017; Chen et al., 2020; Qi et al., 2020) . Intuitively, we can understand our weighting scheme p as following. In offline learning with a big data size where n is huge, it is impossible to calculate the p * as in (4) at every iteration due to computation and memory limits. As a result, we need to approximate p * in a systematic way. In our method, we use moving average estimate s t+1 to approximate the denominator in p * , i.e., 1 n n i=1 exp( L(w;zi) \u03bb ), and use it to compute a scaled weight of data in the mini-batch by Step 7, i.e.,\nMore rigorously, our method ABSGD is a stochastic momentum method for solving a compositional problem in the form f (g(w)) + r(w). To this end, we write the objective in (6) as f (g(w)) + r(w), where f (g) = \u03bb log(g) and g(w) = E z [exp(L(w; z)/\u03bb)]. The difficulty of stochastic optimization for the compositional function f (g(w)) lies on computing an approximate gradient at w t . By the chain rule, its gradient is given by \u2207f (g(w t )) T \u2207g(w t ), where \u2207g(w t ) can be estimated by mini-batch stochastic gradient, i.e.,\nTo approximate \u2207f (g(w t )) = \u03bb g(wt) , we use a moving average to estimate g(w t ) inspired by (Wang et al., 2017) , which is updated in Step 6, i.e.,\nHence, the true gradient \u2207f (g(w t )) T \u2207g(w t ) is able to be approximated by\nwhich is exactly the approximate gradient used in the update of w t+1 as in equation (2). Note that, different from (Wang et al., 2017; Chen et al., 2020; Qi et al., 2020) on stochastic optimization for a compositional function, we employ the momentum term to improve the convergence and generalization performance."}, {"section_title": "Convergence Analysis", "text": "In this subsection, we provide a convergence result of ABSGD for solving the DRO objective under some standard assumptions in non-convex optimization. In particular, we focus on the online formulation in (6). For presentation simplicity, we use the notations g(w) = E z [exp(L(w; z)/\u03bb)] and g(w; z) = exp(L(w; z)/\u03bb). We first state a standard assumption (Qi et al., 2020; Wang et al., 2017) and then present our main theorem.\nAssumption 1 For a fixed \u03bb = \u03bb 1 , there exists V g > 0 such that\nand L(w; z) for any z is an L-smooth function, i.e.,\nTheorem 1 Let assume assumption 1 holds. Also assume there exists C, G,\nwhere we exhibit the constant in the big O in appendix."}, {"section_title": "Remark:", "text": "The above theorem implies that a randomly chosen solution in Stage II will converge to the stationary point of the objective F \u03bb1 (w) when T 1 \u2192 \u221e. The convergence rate in the order of 1/T 1/4 1 is the same as the stochastic method for solving compositional optimization problem (Wang et al., 2017) . It is notable that we can also use an advanced technique to update s t+1 according to (Chen et al., 2020) \nand we are able to prove a convergence rate in the order of 1/T 1/2 1 similar to the convergence rate of standard SGD for empirical risk minimization. In practice, we observe that the two methods have similar performance and since the convergence analysis is not the focus of this paper, we defer the improved convergence rate analysis to the appendix for the interested readers.\nThe proof is the same as Lemma1 of Yan et al. (2018) , we omitteed the proof here. Before we move on, we derive some inequalities that are helpful for later proofs. By ABSGD(Algorithm 1), \u2207F (q t ) = \u2207f (g(q t )) T \u2207g(q t ) for q t = w t + p t . Then for any iteration t = 1 \u00b7 \u00b7 \u00b7 T in ABSGD, the following inequality holds for \u03b4 t = \u2207f (y t+1 ) T \u2207g \u03be (w t ) \u2212 \u2207f (g(w t )) T \u2207g(w t )\nProof:\n(27) Where the first inequality applies the smoothness property of the objective f . The first equality is due to Lemma 1. Then for the first cross term 1 on the right hand side above equation, we have\n(28) Plugging it back, we have\nCombining equation (29) and (22), we finish the proof of the lemma.\nLemma 3 By running algorithm 1, for \u2200, 1 \u00b7 \u00b7 \u00b7 T ,\nIn addition,"}, {"section_title": "Experiments", "text": "We conduct experiments on imbalanced benchmark datasets, including CIFAR-10 (LT), CIFAR-10 (ST), CIFAR-100 (LT), CIFAR-100 (ST), ImagetNet-LT (Liu et al., 2019) , Places-LT (Zhou et al., 2017) , which are manually created for evaluating different methods for tackling data imbalance. We provide details of these datasets shortly. We also compare with state-of-the-art methods on inherently imbalanced dataset iNa-turelist2018 (iNatrualist, 2018). Finally, we provide some ablation studies on the proposed method.\nFor our method ABSGD, we tune \u03bb 1 in {100, 200}, and tune \u03bb 2 in {0.1, 1, 5, 10}. The momentum parameter is set to 0.9 in all of our experiments. We use a weight decay parameter 2e-4 for CIFAR10, CIFAR100, 5e-4 for Imagenet-LT and Places-LT, 1e-4 for iNaturalist2018. ResNets (He et al., 2016) networks are the main backbone in our experiments. For baseline methods that are implemented by us, we use the momentum SGD method with the same momentum parameter, weight decay and step size. For baselines that are not implemented by us, we directly cite the results from the original paper. The original CIFAR-10 and CIFAR-100 data contain 50,000 training images and 10,000 validation with 10 and 100 classes, respectively. We construct the imbalanced version of training set of CIFAR10, CIFAR100 following the two strategies: Long-Tailed (LT) imbalance (Cao et al., 2019) and\nStep (ST) imbalance (Buda et al., 2018) with two different imbalance ratio \u03c1 = 10, \u03c1 = 100, and keep the testing set unchanged. The imbalance ratio \u03c1 is defined as the ratio between sample sizes of the most frequent and least frequent classes. The LT imbalance follows the exponentially decayed sample size between different categories. In ST imbalance, the number of examples is equal within minority classes and equal within majority classes but differs between the majority and minority classes. We denote the imbalanced versions of CIFAR10, CIFAR100 as CIFAR10-LT/ST, CIFAR100-LT/ST according the imbalanced strategies. We adopt a stagewise training strategy with initial learning rate 0.1 and decays by a factor 10 to 160-th, 180-th epoch, respectively. We decay the value of \u03bb at 160 epoch. The batch size is 100.\nResults of using Label-Distribution Independent Losses. We first compare the effectiveness of our method compared with standard DL methods. In particular, we consider two loss functions, cross-entropy (CE) loss and focal loss. The baseline method is the momentum SGD optimizing these losses, denoted by SGD (CE) and SGD (Focal). Our methods are denoted by ABSGD (CE) and ABSGD (Focal) that employ the two losses in our framework. This comparison is meaningful as in the online learning setting the prior knowledge of class-frequency is not known. The results are reported in Table 1 . We can see that ABSGD consistently outperforms SGD with a noticeable margin regardless of imbalance strategies and imbalance ration \u03c1. In particular, we have improvements of more than 2%, 3% on the CIFAR10-LT, and CIFAR10-ST, respectively. Results of using Label-distribution Dependent Losses. Next, we compare ABSGD with baseline methods that use label-distribution dependent losses. In particular, we consider class-balanced versions of three individual losses, including CE loss, focal loss, label-distribution-aware margin (LDAM) loss (Cao et al., 2019) . The class-balanced weighing strategy is from (Cui et al., 2019) , which uses the effective number of samples to define the weight. As a result, there are three categories of class-balanced (CB) losses, i.e., CB-CE, CB-Focal, CB-LDAM. For each of these losses, we consider two optimization methods. The first method is the standard momentum SGD method with a practical trick that defers adding the class-level weighting after a number of pre-training steps with no class-level weights. This trick has been found useful to improve the performance of imbalanced data (Cao et al., 2019) . We denote the first method by SGD (XX), where XX denotes the loss function. The second method is the meta learning method (Jamal et al., 2020) that uses meta-learning on a separate validation data to learn individual weights and combines them with class-balanced weights. The meta learning method has been observed with state-of-the-art results on these benchmark imbalanced datasets. We let META (XX) denote the second method. Our method is denoted by ABSGD (XX).\nThe results are reported in Table 2 . We have the following observations regarding the results. (i) Comparing ABSGD with SGD, we can see that our method that incorporates the self-adaptive robust weighting scheme performs consistently better in almost all imbalanced settings. This verifies that the proposed self- adaptive weighting scheme is effective. (ii) Comparing ABSGD with META, we can see that our method performs better on CIFAR100-LT and performs slightly worse on CIFAR10-LT. It is notable that META requires a separate validation data and is more computationally expensive than our method. Hence, our method is a strong choice even compared with the state-of-the-art meta learning method."}, {"section_title": "Experiemental Results on ImageNet-LT and Places-LT", "text": "ImageNet-LT (Liu et al., 2019 ) is a long-tailed subset of the original ImageNet-2012 by sampling a subset following the Pareto distribution with the power value 6. It has 115.8K images from 1000 categories, which include 4980 for the head class and 5 images for the tail class. The Places-LT dataset was also created by sampling from Places-2 (Zhou et al., 2017) using the same strategy as ImageNet-LT. It contains 62.5K training images from 365 classes with an imbalance ration \u03c1 = 4980/5. We compare our method with the meta learning method (META) (Jamal et al., 2020) , and also include two other baseline methods reported in (Jamal et al., 2020) . Similar to (Jamal et al., 2020) , we train ResNet32 on ImageNet-LT from scratch and train ResNet152 on Places-LT from a pretrained net trained on ImageNet data. The initial learning rate is 0.2 for ImageNet-LT and 0.01 for Places-LT, using the cosine annealing schedule (Loshchilov and Hutter, 2016) with 90 and 30 epochs, respectively. \u03bb is decayed at 60th epoch for ImageNet-LT and 20th epoch for Places-LT. The experimental results are reported in Table 3 . From Table 3 we can see that, our ABSGD outperforms the META method and other baselines in both ImageNet-LT and Places-LT by more than 2%, which implies that our self-adaptive weighting strategy is effective. It is notable that the results in Table 3 are not comparable to the state-of-the-art results reported on these datasets. In Appendix, we combine our method with a useful trick for learning with imbalanced data (Kang et al., 2019) , i.e., split the training into two stages and learn different parameters in different stages, and observe competitive results as the state-of-the-art results. (Cui et al., 2019) , FC represents fully connected layer, LB represents the last block in the backbone. \u2020 represents an additional balanced data set is required in the second stage. All represents standard training process that optimizing all the parameters of the backbone."}, {"section_title": "Methods", "text": "Stage-2 TV Results (90) \nSampling Loss Stage-1 TV Stage-1 TV Results Vanilla Model (Jamal et al., 2020) None CE All -41.0 CB (Cui et al., 2019) None CE All -41.8 Joint (Kang et al., "}, {"section_title": "Experimental Results on iNaturalist 2018", "text": "iNaturalist 2018 is a real world dataset whose class-frequency follows a heavy-tail distribution (iNatrualist, 2018) . It contains 437K images from 8142 classes. All methods train ResNet50, ResNet152 from the corresponding pretrained models on the ImageNet. In this section, we compare ABSGD with STOA methods, which includes single-stage, two-stage and meta-learning method. For baselines, we include momentum SGD for optimizing LDAM loss, CB-CE loss and CB-Focal loss. They are all single-stage methods. NCM (CE) and cRT (CB-CE), \u03c4 -normalized (CB-CE), LWS (CB-CE) are two-stage methods (Kang et al., 2019) , where the first stage uses the standard uniform sampling to train the model with the CE loss, and the second stage fine tunes some parameters as indicated in the second column of Table 4 . META is the meta-learning method (Jamal et al., 2020) , which also combines the two-stage strategy for improving the performance.\nSimilar to the two-stage methods (Kang et al., 2019) , we can optimize different set of network parameters in our method to boost the performance. We set \u03bb to a large value \u03bb 1 \u2192 \u221e in the first stage. Then we decrease it to \u03bb 2 \u2208 {100, 200}, and to \u03bb 3 \u2208 {10, 20, 30} in the second and third stages, in which we only fine-tune the FC classifiers and the last block of the neural network. For fair comparison, we follow the setting in (Kang et al., 2019; Jamal et al., 2020) that we optimize ResNet50 backbone by ABSGD with 90, 20, 10 epochs and ResNet152 backbone by ABSGD with 200, 20, 10 epochs in the three stages respectively. The batch size is 512. We also adopt the cosine annealing schedule (Loshchilov and Hutter, 2016) and the initial learning rate is 0.2. Table 4 reports the results for all methods, which indicate that our result outperforms all the baselines. Our method also achieves the SOTA results of 73.1% with ResNet152 as the backbone. To show the generality of ABSGD, we provide additional experiments on the sentiment analysis tasks in Natural Language Process (NLP). Internet Movie DataBase (IMDB) (Maas et al., 2011) is a large dataset of informal movie reviews with a collection of 50,000 movie reviews with 50% positive reviews and 50% negative reviews. It is equally divided into the training and testing set. To construct an imbalanced set, we remove 90% negative reviews from the training set and keep the testing set unchanged. We train the bidirectional LSTM network for 90 epochs. The initial learning rates is set 0.01, and is divided by 100, 10000 at 30th and 60th epoch, respectively. The \u03bb is decayed at 60th epoch."}, {"section_title": "Experimental results on NLP dataset IMDB", "text": "We report the top-1 testing accuracy, mean and standard deviation (std) over 5 runs, on both online and offline settings with CE and focal loss in Table 5 . The improvements of ABSGD over SGD verify the effectiveness of our ABSGD method for NLP tasks."}, {"section_title": "COVID-LT: Constructed Long-Tailed COVID-19 Dataset", "text": "In this subsection, we conduct the experiments on a medical image classification task, i.e., using X-ray images to classify COVID-19. Similar tasks have been investigated in the literature (Minaee et al., 2020; Zhao et al., 2020; Cohen et al., 2020) , but they mostly focus on binary classification tasks. Here, we demonstrate the effectiveness of our method for recognizing COVID-19 among other similar diseases. To this end, we construct a dataset named COVID-LT 2 , which contains four classes of radiographs: Other Findings, Normal (Irvin et al., 2019) . The images labeled as \"other findings\" include many other lifethreatening diseases, e.g., Cardiomegaly, Edema. The class of (non-covid) pneumonia is included to make the task more difficult. The label distribution follows Cui et al. (2019) with the following formulation:\nwhere C refers to the number of classes in the dataset, \u03c1 = n 0 /n C\u22121 , and n 0 , n C\u22121 denotes the # of samples in the most and least frequent classes, respectively. Other Findings is the most frequent class and COVID-19 is the least frequent class. The Other Findings, Normal, Pneumonia radios are randomly samples from downsampled version of CheXpert (Irvin et al., 2019) . There are a total of 14 observations in CheXpert, and we only keep the positive observations of Pneumonia and No Findings, while categorizing all other radiographs into Other Findings. The COVID-19 radiographs have 478 images that are collected from the positive cases of COVID-19 image data collection (Cohen et al., 2020) . 378 of them are used to construct the training dataset and 100 of them are used for testing. As a result, n C\u22121 = 378. Then n 0 = 37800 when \u03c1 = 100, n 0 = 3780 when \u03c1 = 10. The label distribution of COVID-LT with different \u03c1 is illustrated in Figure 2 . Therefore COVID-LT with \u03c1 = 100 contains 12875 images, and COVID-LT with \u03c1 = 10 contains 4516 images. The testing set contains 100 images for each class.\nWe compare with baselines using momentum SGD with CE, CB-CE, Focal, CB-Focal loss for training ResNet50. In the training process, we fine tune the last block and the FC classifier layers on the Ima-geNet pertained ResNet50 network using SGD and ABSGD with 90 epochs. The initial learning rate is 0.1, Table 6 verify the effectiveness of our proposed ABSGD on COVID-LT dataset."}, {"section_title": "Ablation Studies", "text": "In this section, we provide some ablation studies to verify our algorithmic choice. The Benefit of Momentum. First, we verify that adding the momentum term can dramatically improve the performance. The results are plotted in the first column of Figure ( 3) on CIFAR10-LT (\u03c1 = 100) and CIFAR100-LT (\u03c1 = 100) datasets, where we plot the testing accuracy vs the epochs of optimization with average of 3 runs. The results clearly show that including an momentum term helps improve the performance and stabilize the training process. The Benefit of Damping \u03bb. Second, we verify that using the damping strategy on \u03bb is important for improving the performance. To this end, we compare our damping strategy with that using a fixed value of \u03bb. For the damping strategy, we use \u03bb 1 = 100 in the first stage and \u03bb 2 = 1 in the second stage. For fixed values of \u03bb, we compare with \u03bb = 100 and \u03bb = 1. The results are plotted in the second column of Figure ( 3) on CIFAR10-LT and CIFAR100-LT datasets. We can see that the damping strategy is better than using a fixed value of \u03bb, which verifies our algorithmic choice. The Effect of Damping \u03bb on Learned Features. Lastly, we compare the learned feature representation of using a damping strategy and a fixed value of \u03bb. Figure 4 shows the t-SNE visualization of the learned representations of training and testing data at the end of training on the CIFAR10-LT (\u03c1 = 100) with different \u03bb strategies, where each color represents each class. Figure (4a) shows that with a fixed value \u03bb = 1, while Figure (4b) shows that with \u03bb 1 = 100 and \u03bb 2 = 1 in the first and second training stage of ABSGD, respectively. It is clear to see using the damping strategy on \u03bb yields much better feature representations that are well separated between different classes. In contrast, the learned feature representations with a fixed value \u03bb = 1 are more cluttered."}, {"section_title": "Conclusion", "text": "In this paper, we have proposed a simple momentum SGD method with self-adaptive robust weighting to tackle the data imbalance problem in deep learning. Theoretically, ABSGD is guaranteed to converge to a stationary point of a regularized distributionally robust optimization problem for learning a deep neural network. Empirically, ABSGD outperforms other weighting methods by a large margin on CIAR100-LT, Places-LT and ImageNet-LT, and matches the SOTA results when applying it with a two-stage training method on ImageNet-LT, iNaturalist18, and Places-LT."}, {"section_title": "Appendix A. Proof of Theoretical Analysis", "text": "According to the conditions of the loss function L, in Assumption 1 and Theorem 1, we first derived some properties for function f (s) = \u03bb log(s) and g(w) = exp( L(w;z) \u03bb ).\nProposition 1 g(w) is an C 2 L \u03bb 2 -smooth and C 2 G \u03bb 2 -Lipschitz continuous function.\nProof By Assumption 1 and Theorem 1, \u2207L(w; z) \u2212 \u2207L(w ; z) 2 \u2264 L w \u2212 w 2 , \u2200w, w . g(w; z) = exp( L(w;z) \u03bb ) \u2264 C, L(w; z) \u2265 0 and , thus 1 \u2264 g(w; z) \u2264 C. Therefore,\nIn addition, with E[ \u2207L(w; z) 2 ] \u2264 G in Theorem 1,\nProposition 2 f (s) = \u03bb log(s) is an \u03bb-smooth and \u03bb-Lipschitz continuous function.\nProof \u2207f (s) = \u03bb s . As s = g(w; z) \u2208 (1, C], \u2207f (s) \u2264 \u03bb, which implies \u2207f (s) \u2264 \u03bb. In addition,\nIn the following proof, we denote,\nwhere G(w t ) = \u2207g \u03be (w t )\u2207f (y t+1 ), \u03be represents a random sample from {z 1 , \u00b7 \u00b7 \u00b7 , z n }."}, {"section_title": "Proof:", "text": "Denote e t = (1 \u2212 \u03b3)(g(w t ) \u2212 g(w t\u22121 )).By Updates of ABSGD (Algorithm 1)\nTherefore,\nwhere the first equality holds due to E[ y t \u2212 g(w t\u22121 ), g \u03be (w t ) \u2212 g(w t ) ] = 0. In addition, by a + b 2 \u2264 (1 + ) a 2 + (1 + 1/ ) b 2 , we have\nCombining equation (34) and (35):\nWe show by induction that E[ y t+1 \u2212 g(w t ) 2 ] \u2264 D y for all t = 1, \u00b7 \u00b7 \u00b7 , T . Suppose that y t \u2212 g(w t\u22121 ) 2 \u2264 D y , then\nWith \u03b7 = 1 T 3/4 , \u03b3 = 1 T 1/2 , \u03b7 2 \u03b3 = 1 T . Assume that 4C g C f \u2264 T 1/2 /2, and T \u2265 16, we have \u03b7 2\nA.1 Proof of Theorem 1\nBy Lemma 2 and Lemma 3, multiply (1 + \u03b3 t ) for equation (31) and adding together with equation (26):\nRearranging terms and summation from 1, \u00b7 \u00b7 \u00b7 , T ,\nNext we bound the terms in the right hand size of the equation (43). Taking the summation of equation (24) from 1, \u00b7 \u00b7 \u00b7 , T ,\nwhere the last inequality is due to Lemma 3.\nTaking the summation of equation (23) from 1, \u00b7 \u00b7 \u00b7 , T ,\nPlugging above equations (45) and (44) to equation (43), "}, {"section_title": "Appendix B. Two-Stage Experimental Results on Places-LT and ImageNet-LT", "text": "Following similar setting as section 4.3, we compare ABSGD with SOTA methods at this section by combine the two-stage training trick proposed in Kang et al. (2019) .i.e, we train the (last block of) feature representation in the first stage and fine-tune the classifier in the second and third stages of ABSGD. In particular, we train the last block of the convolutions layer and fully connected layer for 90 epochs in the first stage with \u03bb 1 = 1000 from the pretrained model on ImageNet for Places-LT, and retrain the fully connected layer for 20 and 10 epochs in second and third stage of ABSGD with \u03bb 2 \u2208 {100, 200} and \u03bb 3 \u2208 {1, 5, 10}, respectively. For ImageNet-LT, we jointly train the feature representation and classifier in the first stage for 90 epochs from scratch, and retrain the fully connected layer for 60 and 30 epochs of ABSGD in the second and third stages with \u03bb 2 = {100, 200} and \u03bb 3 = {1, 5, 10}, respectively. The initial learning rate is 0.2 for ImageNet-LT and 0.01 for Places-LT. We apply the cosine annealing schedule (Loshchilov and Hutter, 2016) in the training process, and reset the the learning rate at the start of second stages. The mini-batch size is 512. Experimental results are reported in Table 7 and Table 8 . (Cui et al., 2019) , FC represents fully connected layer, LB represents the last block in the backbone. * denotes an additional memory is required in the second stage and \u2020 represents an additional balanced data set is required in the second stage. Table 7 reports the experimental results on Places-LT. \u03c4 -normalization method proposed in (Kang et al., 2019) has the best reported SOTA result. However, it is obvious to see that ABSGD outperforms the META methods by a relatively large margin of 0.6% with standard CE loss on Places-LT after applying the twostage training trick on both methods. The results is consistency with the results in Table 3 , which verifies that ABSGD is more effective than META methods on Places-LT with CE loss. In Table 8 , ABSGD has the same results as the SOTA results achieved by the META method on ImageNet-LT and has no requirements on the additional balanced validation datasets. (Cui et al., 2019) , FC represents fully connected layer, LB represents the last block in the backbone. \u2020 represents an additional balanced data set is required in the second stage."}]