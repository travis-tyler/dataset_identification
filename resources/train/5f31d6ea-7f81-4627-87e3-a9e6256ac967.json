[{"section_title": "Preface", "text": "The genius of American higher education is often said to be in the close association of training and research -that is, in the nation's research-doctorate programs. Consequently, we are not surprised at the amount of worried talk about the quality of the research doctorate, for deterioration at that level will inevitably spread to wherever research skills are needed-and that indeed is a far-flung network of laboratories, institutes, firms, agencies, bureaus, and departments. What might surprise us, however, is the imbalance between the putative national importance of research-doctorate programs and the amount of sustained evaluative attention they themselves receive. The present assessment, sponsored by the Conference Board of Associated Research Councils-comprised of the American Council of Learned Societies, the American Council on Education, the National Research Council (NRC), and the Social Science Research Council-seeks to correct the imbalance between worried talk and systematic study. In this effort the Conference Board continues a tradition pioneered by the American Council on Education, which in 1966 published An Assessment of Quality in Graduate Education, the report of a study conducted by Allan M.Cartter, and in 1970 published A Rating of Graduate Programs, by Kenneth D.Roose and Charles J.Andersen. The Cartter and Roose-Andersen reports have been widely used and frequently cited. Some years after the release of the Roose-Andersen report, it was decided that the effort to assess the quality of research-doctorate programs should be renewed, and the Conference Board of Associated Research Councils agreed to sponsor an assessment. The Board of Directors of the American Council on Education concurred with the notion that the next study should be issued under these broader auspices. The NRC agreed to serve as secretariat for a new study. The responsible staff of the NRC earned the appreciation of the Conference Board for the skill and dedication shown during the course of securing funding and implementing the study. Special mention should also be made of the financial contribution of the National Academy of Sciences which, by supplementing funds available from external sources, made it possible for the study to get under way. To sponsor a study comparing the quality of programs in 32 disciplines and from more than 200 doctorate-granting universities is to invite critics, friendly and otherwise. Such was the fate of the previous studies; such has been the fate of the present study. Scholarship, fortunately, can put criticism to creative use and has done so in this project. The study committee appointed by the Conference Board reviewed the criticisms of earlier efforts to assess research-doctorate programs, and it actively solicited criticisms and suggestions for improvements of its own design. Although constrained by limited funds, the committee applied state-of-the-art methodology in a design that incorporated the lessons learned from previous studies as well as attending to many critics of the present effort. Not all criticism has thus been stilled; nor could it ever be. Additional criticisms will be voiced by as many persons as begin to use the results of this effort in ways not anticipated by its authors. These criticisms will be welcome. The Conference Board believes that the present study, building on earlier criticisms and adopting a multidimensional approach to the assessment of researchdoctorate programs, represents a substantial improvement over past reports. Nevertheless, each of the diverse measures used here has its own limitations, and none provides a precise index of the quality of a program for educating students for careers in research. No doubt a future study, taking into account the weaknesses as well as strengths of this effort, will represent still further improvement. One mark of success for the present study would be for it to take its place in a continuing series, thereby contributing to the indicator base necessary for informed policies that will maintain and perhaps enhance the quality of the nation's research-doctorate programs. For the more immediate future the purposes of this assessment are to assist students and student advisers seeking the best match possible between individual career goals and the choice of an advanced degree program; to serve scholars whose study site is higher education and the nation's research enterprise; and to inform the practical judgment of the administrators, funders, and policymakers responsible for protecting the quality of scholarly education in the United States. A remarkably hard-working and competent group, whose names appear on p. vii, oversaw the long process by which this study moved from the planning stage to the completion of these reports. The Conference Board expresses its warmest thanks to the members of its committee and especially to their co-chairmen, Lyle V.Jones and Gardner Lindzey. Conference "}, {"section_title": "I Origins of Study and Selection of Programs", "text": "Each year more than 22,000 candidates are awarded doctorates in engineering, the humanities, and the sciences from approximately U.S. universities. They have spent, on the average, five-and-a-half years in intensive education in preparation for research careers either in universities or in settings outside the academic sector, and many will make significant contributions to research. Yet we are poorly informed concerning the quality of the programs producing these graduates. This study is intended to provide information pertinent to this complex and controversial subject. The charge to the study committee directed it to build upon the planning that preceded it. The planning stages included a detailed review of the methodologies and the results of past studies that had focused on the assessment of doctoral-level programs. The committee has taken into consideration the reactions of various groups and individuals to those studies. The present assessment draws upon previous experience with program evaluation, with the aim of improving what was useful and avoiding some of the difficulties encountered in past studies. The present study, nevertheless, is not purely reactive: it has its own distinctive features. First, it focuses only on programs awarding research doctorates and their effectiveness in preparing students for careers in research. Although other purposes of graduate education are acknowledged to be important, they are outside the scope of this assessment. Second, the study examines a variety of different indices that may be relevant to the program quality. This multidimensional approach represents an explicit recognition of the limitations of studies that rely entirely on peer ratings of perceived quality-the so-called reputational ratings. Finally, in the compilation of reputational ratings in this study, evaluators were provided the names of faculty members involved with each program to be rated and the number of research doctorates awarded in the last five years. In previous reputational studies evaluators were not supplied such information. During the past two decades increasing attention has been given to describing and measuring the quality of programs in graduate education. It is evident that the assessment of graduate programs is highly important for university administrators and faculty, for employers in industrial and government laboratories, for graduate students and prospective graduate students, for policymakers in state and national organizations, and for private and public funding agencies. Past experience, however, has demonstrated the difficulties with such assessments and their potentially controversial nature. As one critic has asserted: \u2026the overall effect of these reports seems quite clear. They tend, first, to make the rich richer and the poor poorer; second, the example of the highly ranked clearly imposes constraints on those institutions lower down the scale (the \"Hertz-Avis\" effect). And the effect of such constraints is to reduce diversity, to reward conformity or respectability, to penalize genuine experiment or risk. There is, also, I believe, an obvious tendency to promote the prevalence of disciplinary dogma and orthodoxy. All of this might be tolerable if the reports were tolerably accurate and judicious, if they were less prescriptive and more descriptive; if they did not pretend to \"objectivity\" and if the very fact of ranking were not pernicious and invidious; if they genuinely promoted a meaningful \"meritocracy\" (instead of simply perpetuating the status quo ante and an establishment mentality). But this is precisely what they cannot claim to be or do. 1 The widespread criticisms of ratings in graduate education were carefully considered in the planning of this study. At the outset consideration was given to whether a national assessment of graduate programs should be undertaken at this time and, if so, what methods should be employed. The next two sections in this chapter examine the background and rationale for the decision by the Conference Board of Associated Research Councils 2 to embark on such a study. The remainder of the chapter describes the selection of disciplines and programs to be covered in the assessment. The overall study encompasses a total of 2,699 graduate programs in 32 disciplines. In this report-the first of five reports issuing from the study-we examine 596 programs in six disciplines in the mathematical and physical sciences: chemistry, computer sciences, geosciences, mathematics, physics, and statistics/biostatistics. These programs account for more than 90 percent of the research doctorates awarded in these six disciplines. It should be emphasized that the selection of disciplines to be covered was determined on the basis of total doctoral awards during the FY1976-78 period (as described later in this chapter), and the exclusion of a particular discipline was in no way based on a judgment of the importance of graduate education or research in that discipline. Also, although the assessment is limited to programs leading to the research-doctorate (Ph.D. or equivalent) degree, the Conference Board and study committee recognize that graduate schools provide many other forms of valuable and needed education."}, {"section_title": "PRIOR ATTEMPTS TO ASSESS QUALITY IN GRADUATE EDUCATION", "text": "Universities and affiliated organizations have taken the lead in the review of programs in graduate education. At most institutions program reviews are carried out on a regular basis and include a comprehensive examination of the curriculum and educational resources as well as the qualifications of faculty and students. One special form of evaluation is that associated with institutional accreditation: The process begins with the institutional or programmatic self-study, a comprehensive effort to measure progress according to previously accepted objectives. The self-study considers the interest of a broad cross-section of constituencies-students, faculty, administrators, alumni, trustees, and in some circumstances the local community. The resulting report is reviewed by the appropriate accrediting commission and serves as the basis for evaluation by a site-visit team from the accrediting group\u2026. Public as well as educational needs must be served simultaneously in determining and fostering standards of quality and integrity in the institutions and such specialized programs as they offer. Accreditation, conducted through non-governmental institutional and specialized agencies, provides a major means for meeting those needs. 3 Although formal accreditation procedures play an important role in higher education, many university administrators do not view such procedures as an adequate means of assessing program quality. Other efforts are being made by universities to evaluate their programs in graduate education. The Educational Testing Service, with the sponsorship of the Council of Graduate Schools in the United States and the Graduate Record Examinations Board, has recently developed a set of procedures to assist institutions in evaluating their own graduate programs. 4 While reviews at the institutional (or state) level have proven useful in assessing the relative strengths and weaknesses of individual programs, they have not provided the information required for making national comparisons of graduate programs. Several attempts have been made at such comparisons. The most widely used of these have been the studies by Keniston (1959), Cartter (1966), and Roose and Andersen (1970). All three studies covered a broad range of disciplines in engineering, the humanities, and the sciences and were based on the opinions of knowledgeable individuals in the program areas covered. Keniston 5 surveyed the department chairmen at 25 leading institutions. The Cartter 6 and Roose-Andersen 7 studies compiled ratings from much larger groups of faculty peers. The stated motivation for these studies was to increase knowledge concerning the quality of graduate education: A number of reasons can be advanced for undertaking such a study. The diversity of the American system of higher education has properly been regarded by both the professional educator and the layman as a great source of strength, since it permits flexibility and adaptability and encourages experimentation and competing solutions to common problems. Yet diversity also poses problems\u2026. Diversity can be a costly luxury if it is accompanied by ignorance\u2026. Just as consumer knowledge and honest advertising are requisite if a competitive economy is to work satisfactorily, so an improved knowledge of opportunities and of quality is desirable if a diverse educational system is to work effectively. 8 Although the program ratings from the Cartter and Roose-Andersen studies are highly correlated, some substantial differences in successive ratings can be detected for a small number of programs-suggesting changes in the programs or in the perception of the programs. For the past decade the Roose-Andersen ratings have generally been regarded as the best available source of information on the quality of doctoral programs. Although the ratings are now more than 10 years out of date and have been criticized on a variety of grounds, they are still used extensively by individuals within the academic community and by those in federal and state agencies. A frequently cited criticism of the Cartter and Roose-Andersen studies is their exclusive reliance upon reputational measurement. The ACE rankings are but a small part of all the evaluative processes, but they are also the most public, and they are clearly based on the narrow assumptions and elitist structures that so dominate the present direction of higher education in the United States. As long as our most prestigious source of information about post-secondary education is a vague popularity contest, the resultant ignorance will continue to provide a cover for the repetitious aping of a single model\u2026. All the attempts to change higher education will ultimately be strangled by the \"legitimate\" evaluative processes that have already programmed a single set of responses from the start. 9 A number of other criticisms have been leveled at reputational rankings of graduate programs. 10 First, such studies inherently reflect perceptions that may be several years out of date and do not take into account recent changes in a program. Second, the ratings of individual programs are likely to be influenced by the overall reputation of the university-i.e., an institutional \"halo effect.\" Also, a disproportionately large fraction of the evaluators are graduates of and/or faculty members in the largest programs, which may bias the survey results. Finally, on the basis of such studies it may not be possible to differentiate among many of the lesser known programs in which relatively few faculty members have established national reputations in research. Despite such criticisms several studies based on methodologies similar to that employed by Cartter and Roose and Andersen have been carried out during the past 10 years. Some of these studies evaluated post-baccalaureate programs in areas not covered in the two earlier reports-including business, religion, educational administration, and medicine. Others have focused exclusively on programs in particular disciplines within the sciences and humanities. A few attempts have been made to assess graduate programs in a broad range of disciplines, many of which were covered in the Roose-Andersen and Cartter ratings, but in the opinion of many each has serious deficiencies in the methods and procedures employed. In addition to such studies, a myriad of articles have been written on the assessment of graduate programs since the release of the Roose-Andersen report. With the heightening interest in these evaluations, many in the academic community have recognized the need to assess graduate programs, using other criteria in addition to peer judgment. Though carefully done and useful in a number of ways, these ratings (Cartter and Roose-Andersen) have been criticized for their failure to reflect the complexity of graduate programs, their tendency to emphasize the traditional values that are highly related to program size and wealth, and their lack of timeliness or currency. Rather than repeat such ratings, many members of the graduate community have voiced a preference for developing ways to assess the quality of graduate programs that would be more comprehensive, sensitive to the different program purposes, and appropriate for use at any time by individual departments or universities. 11 Several attempts have been made to go beyond the reputational assessment. Clark, Harnett, and Baird, in a pilot study 12 of graduate programs in chemistry, history, and psychology, identified as many as 30 possible measures significant for assessing the quality of graduate education. Glower 13 has ranked engineering schools according to the total amount of research spending and the number of graduates listed in Who's Who in Engineering. House and Yeager 14 rated economics departments on the basis of the total number of pages published by full professors in 45 leading journals in this discipline. Other ratings based on faculty publication records have been compiled for graduate programs in a variety of disciplines, including political science, psychology, and sociology. These and other studies demonstrate the feasibility of a national assessment of graduate programs that is founded on more than reputational standing among faculty peers."}, {"section_title": "DEVELOPMENT OF STUDY PLANS", "text": "In September 1976 the Conference Board, with support from the Carnegie Corporation of New York and the Andrew W.Mellon Foundation, convened a three-day meeting to consider whether a study of programs in graduate education should be undertaken. The 40 invited participants at this meeting included academic administrators, faculty members, and agency and foundation officials, 15 who represented a variety of institutions, disciplines, and convictions. In these discussions there was considerable debate concerning whether the potential benefits of such a study outweighed the possible misrepresentations of the results. On the one hand, \"a substantial majority of the Conference [participants believed] that the earlier assessments of graduate education have received wide and important use: by students and their advisors, by the institutions of higher education as aids to planning and the allocation of educational functions, as a check on unwarranted claims of excellence, and in social science research.\" 16 On the other hand, the conference participants recognized that a new study assessing the quality of graduate education \"would be conducted and received in a very different atmosphere than were the earlier Cartter and Roose-Andersen reports\u2026. Where ratings were previously used in deciding where to increase funds and how to balance expanding programs, they might now be used in deciding where to cut off funds and programs.\" After an extended debate of these issues, it was the recommendation of this conference that a study with particular emphasis on the effectiveness of doctoral programs in educating research personnel be undertaken. The recommendation was based principally on four considerations: (1) the importance of the study results to national and state bodies, (2) the desire to stimulate continuing emphasis on quality in graduate education, (3) the need for current evaluations that take into account the many changes that have occurred in programs since the Roose-Andersen study, and (4) the value of extending the range of measures used in evaluative studies of graduate programs. Although many participants expressed interest in an assessment of master's degree and professional degree programs, insurmountable problems prohibited the inclusion of these types of programs in this study. Following this meeting a 13-member committee, 17 co-chaired by Gardner Lindzey and Harriet A.Zuckerman, was formed to develop a detailed plan for a study limited to researchdoctorate programs and designed to improve upon the methodologies utilized in earlier studies. In its deliberations the planning committee carefully considered the criticisms of the Roose-Andersen study and other national assessments. Particular attention was paid to the feasibility of compiling a variety of specific measures (e.g., faculty publication records, quality of students, program resources) that were judged to be related to the quality of research-doctorate programs. Attention was also given to making improvements in the survey instrument and procedures used in the Cartter and Roose-Andersen studies. In September 1978 the planning group submitted a comprehensive report describing alternative strategies for an evaluation of the quality and effectiveness of research-doctorate programs. The proposed study has its own distinctive features. It is characterized by a sharp focus and a multidimensional approach. 1It will focus only on programs awarding research doctorates; other purposes of doctoral training are acknowledged to be important, but they are outside the scope of the work contemplated. 2The multidimensional approach represents an explicit recognition of the limitations of studies that make assessments solely in terms of ratings of perceived quality provided by peers-the so-called reputational ratings. Consequently, a variety of quality-related measures will be employed in the proposed study and will be incorporated in the presentation of the results of the study. 18 This report formed the basis for the decision by the Conference Board to embark on a national assessment of doctorate-level programs in the sciences, engineering, and the humanities. In June 1980 an 18-member committee was appointed to oversee the study. The committee, 19 made up of individuals from a diverse set of disciplines within the sciences, engineering, and the humanities, includes seven members who had been involved in the planning phase and several members who presently serve or have served as graduate deans at either public or private universities. During the first eight months the committee met three times to review plans for the study activities, make decisions on the selection of disciplines and programs to be covered, and design the survey instruments to be used. Early in the study an effort was made to solicit the views of presidents and graduate deans at more than 250 universities. Their suggestions were most helpful to the committee in drawing up final (2) more than one-third of that specified number of doctorates awarded in FY1979, or (3) an average rating of 2.0 or higher in the Roose-Andersen rating of the scholarly quality of departmental faculty. In each discipline the specified number of doctorates required for inclusion in the study was determined in such a way that the programs meeting this criterion accounted for at least 90 percent of the doctorates awarded in that discipline during the FY1976-78 period. In the mathematical and physical science disciplines, the following numbers of FY1976-78 doctoral awards were required to satisfy the first criterion (above): Chemistry-13 or more doctorates Computer Sciences-5 or more doctorates Geosciences-7 or more doctorates Mathematics-7 or more doctorates Physics-10 or more doctorates Statistics/Biostatistics-5 or more doctorates A list of the nominated programs at each institution was then sent to a designated individual (usually the graduate dean) who had been appointed by the university president to serve as study coordinator for the institution. The coordinator was asked to review the list and eliminate any programs no longer offering research doctorates or not belonging in the designated discipline. The coordinator also was given an opportunity to nominate additional programs that he or she believed should be included in the study. 21 Coordinators were asked to restrict their nominations to programs that they considered to be \"of uncommon distinction\" and that had awarded no fewer than two research doctorates during the past two years. In order to be eligible for inclusion, of course, programs had to belong in one of the disciplines covered in the study. If the university offered more than one research-doctorate program in a discipline, the coordinator was instructed to provide information on each of them so that these programs could be evaluated separately. The committee received excellent cooperation from the study coordinators at the universities. Of the 243 institutions that were identified as having one or more research-doctorate programs satisfying the criteria (listed earlier) for inclusion in the study, only 7 declined to participate in the study and another 8 failed to provide the program information requested within the three-month period allotted (despite several reminders). None of these 15 institutions had doctoral programs that had received strong or distinguished reputational ratings in prior national studies. Since the information requested had not been provided, the committee decided not to include programs from these institutions in any aspect of the assessment. In each of the six chapters that follows, a list is given of the universities that met the criteria for inclusion in a particular discipline but that are not represented in the study. As a result of nominations by institutional coordinators, some programs were added to the original list and others dropped. Table 1.2 reports the final coverage in each of the six mathematical and physical science disciplines. The number of programs evaluated varies *The data on doctoral awards were provided by the study coordinator at each of the universities covered in the assessment. considerably by discipline. A total of 145 chemistry programs have been included in the study; in computer sciences and statistics/ biostatistics fewer than half this number have been included. Although the final determination of whether a program should be included in the assessment was left in the hands of the institutional coordinator, it is entirely possible that a few programs meeting the criteria for inclusion in the assessment were overlooked by the coordinators. During the course of the study only two such programs in the mathematical and physical sciences-one in mathematics and one in biostatistics-have been called to the attention of the committee. In the chapter that follows, a detailed description is given of each of the measures used in the evaluation of research-doctorate programs in the mathematical and physical sciences. The description includes a discussion of the rationale for using the measure, the source from which data for that measure were derived, and any known limitations that would affect the interpretation of the data reported. The committee wishes to emphasize that there are limitations associated with each of the measures and that none of the measures should be regarded as a precise indicator of the quality of a program in educating scientists for careers in research. The reader is strongly urged to consider the descriptive material presented in Chapter II before attempting to interpret the program evaluations reported in subsequent chapters. In presenting a frank discussion of any shortcomings of each measure, the committee's intent is to reduce the possibility of misuse of the results from this assessment of research-doctorate programs."}, {"section_title": "II Methodology", "text": "Quality\u2026you know what it is, yet you don't know what it is. But that's self-contradictory. But some things are better than others, that is, they have more quality. But when you try to say what the quality is, apart from the things that have it, it all goes poof! There's nothing to talk about. But if you can't say what Quality is, how do you know what it is, or how do you know that it even exists? If no one knows what it is, then for all practical purposes it doesn't exist at all. But for all practical purposes it really does exist. What else are the grades based on? Why else would people pay fortunes for some things and throw others in the trash pile? Obviously some things are better than others\u2026but what's the \"betterness\"? \u2026So round and round you go, spinning mental wheels and nowhere finding anyplace to get traction. What the hell is Quality? What is it? Robert M.Pirsig Zen and the Art of Motorcycle Maintenance Both the planning committee and our own study committee have given careful consideration to the types of measures to be employed in the assessment of research-doctorate programs. 1 The committees recognized that any of the measures that might be used is open to criticism and that no single measure could be expected to provide an entirely satisfactory index of the quality of graduate education. With respect to the use of multiple criteria in educational assessment, one critic has commented: At best each is a partial measure encompassing a fraction of the large concept. On occasion its link to the real [world] is problematic and tenuous. Moreover, each measure [may contain] a load of irrelevant superfluities, \"extra baggage\" unrelated the outcomes under study. By the use of a number of such measures, each contributing a different facet of information, we can limit the effect of irrelevancies and develop a more rounded and truer picture of program outcomes. 2 Although the use of multiple measures alleviates the criticisms directed at a single dimension or measure, it certainly will not satisfy those who believe that the quality of graduate programs cannot be represented by quantitative estimates no matter how many dimensions they may be intended to represent. Furthermore, the usefulness of the assessment is dependent on the validity and reliability of the criteria on which programs are evaluated. The decision concerning which measures to adopt in the study was made primarily on the basis of two factors: (1) the extent to which a measure was judged to be related to the quality of research-doctorate programs, and (2) the feasibility of compiling reliable data for making national comparisons of programs in particular disciplines. Only measures that were applicable to a majority of the disciplines to be covered were considered. In reaching a final decision the study committee found the ETS study, 3 in which 27 separate variables were examined, especially helpful, even though it was recognized that many of the measures feasible in institutional self-studies would not be available in a national study. The committee was aided by the many suggestions received from university administrators and others within the academic community. Although the initial design called for an assessment based on approximately six measures, the committee concluded that it would be highly desirable to expand this effort. A total of 16 measures (listed in Table 2.1) have been utilized in the assessment of research-doctorate programs in chemistry, computer sciences, geosciences, mathematics, and physics; 15 of these were used in evaluating programs in statistics/biostatistics. (Data on research expenditures are unavailable in the latter discipline.) For nine of the measures  1 Reported number of faculty members in the program, December 1980. 02 Reported number of program graduates in last five years (July 1975through June 1980."}, {"section_title": "03", "text": "Reported total number of full-time and part-time graduate students enrolled in the program who intend to earn doctorates, December 1980."}, {"section_title": "Characteristics of Graduates 2 04", "text": "Fraction of FY1975-79 program graduates who had received some national fellowship or training grant support during their graduate education."}, {"section_title": "05", "text": "Median number of years from first enrollment in graduate school to receipt of the doctorate-FY1975-79 program graduates. 3 "}, {"section_title": "06", "text": "Fraction of FY1975-79 program graduates who at the time they completed requirements for the doctorate reported that they had made definite commitments for postgraduation employment. 07 Fraction of FY1975-79 program graduates who at the time they completed requirements for the doctorate reported that they had made definite commitments for postgraduation employment in Ph.D.-granting universities."}, {"section_title": "Reputational Survey Results 4 08", "text": "Mean rating of the scholarly quality of program faculty. 09 Mean rating of the effectiveness of the program in educating research scholars/scientists 10 Mean rating of the improvement in program quality in the last five years. 11 Mean rating of the evaluators' familiarity with the work of program faculty. University Library Size 5  12Composite index describing the library size in the university in which the program is located, 1979-80."}, {"section_title": "Research Support 13", "text": "Fraction of program faculty members holding research grants from the National Science Foundation; National Institutes of Health; or the Alcohol, Drug Abuse, and Mental Health Administration at any time during the FY1978-80 period. 6 14 Total expenditures (in thousands of dollars) reported by the university for research and development activities in a specified field, FY1979. 7 Publication Records 8 15 Number of published articles attributed to the program, 1978-79. 16 Estimated \"overall influence\" of published articles attributed to the program, 1978-79. 1 Based on information provided to the committee by the participating universities. 2 Based on data compiled in the NRC's Survey of Earned Doctorates. 3 In reporting standardized scores and correlations with other variables, a shorter time-to-Ph.D. is assigned a higher score. 4 Based on responses to the committee's survey conducted in April 1981. 5 Based on data compiled by the Association of Research Libraries. 6 Based on matching faculty names provided by institutional coordinators with the names of research grant awardees from the three federal agencies. 7 Based on data provided to the National Science Foundation by universities. 8 Based on data compiled by the Institute for Scientific Information and developed by Computer Horizons, Inc. data are available describing most, if not all, of the mathematical and physical science programs included in the assessment. For seven measures the coverage is less complete but encompasses at least a majority of the programs in every discipline. The actual number of programs evaluated on every measure is reported in the second table in each of the next six chapters. The 16 measures describe a variety of aspects important to the operation and function of research-doctorate programs-and thus are relevant to the quality and effectiveness of programs in educating scientists for careers in research. However, not all of the measures may be viewed as \"global indices of quality.\" Some, such as those relating to program size, are best characterized as \"program descriptors\" which, although not dimensions of quality per se, are thought to have a significant influence on the effectiveness of programs. Other measures, such as those relating to university library size and support for research and training, describe some of the resources generally recognized as being important in maintaining a vibrant program in graduate education. Measures derived from surveys of faculty peers or from the publication records of faculty members, on the other hand, have traditionally been regarded as indices of the overall quality of graduate programs. Yet these too are not true measures of quality. We often settle for an easy-to-gather statistic, perfectly legitimate for its own limited purposes, and then forget that we haven't measured what we want to talk about. Consider, for instance, the reputation approach of ranking graduate departments: We ask a sample of physics professors (say) which the best physics departments are and then tabulate and report the results. The \"best\" departments are those that our respondents say are the best. Clearly it is useful to know which are the highly regarded departments in a given field, but prestige (which is what we are measuring here) isn't exactly the same as quality. 4 To be sure, each of the 16 measures reported in this assessment has its own set of limitations. In the sections that follow an explanation is provided of how each measure has been derived and its particular limitations as a descriptor of research-doctorate programs."}, {"section_title": "PROGRAM SIZE", "text": "Information was collected from the study coordinators at each university on the names and ranks of program faculty, doctoral student enrollment, and number of Ph.D. graduates in each of the past five years (FY1976-80). Each coordinator was instructed to include on the faculty list those individuals who, as of December 1, 1980, held academic appointments (typically at the rank of assistant, associate, and full professor) and who participated significantly in doctoral education. Emeritus and adjunct members generally were not to be included. Measure 01 represents the number of faculty identified in a program. Measure 02 is the reported number of graduates who earned Ph.D. or equivalent research doctorates in a program during the period from July 1, 1975, through June 30, 1980. Measure 03 represents the total number of full-time and part-time students reported to be enrolled in a program in the fall of 1980, who intended to earn research doctorates. All three of these measures describe different aspects of program size. In previous studies program size has been shown to be highly correlated with the reputational ratings of a program, and this relationship is examined in detail in this report. It should be noted that since the information was provided by the institutions participating in the study, the data may be influenced by the subjective decisions made by the individuals completing the forms. For example, some institutional coordinators may be far less restrictive than others in deciding who should be included on the list of program faculty. To minimize variation in interpretation, detailed instructions were provided to those filling out the forms. 5 Measure 03 is of particular concern in this regard since the coordinators at some institutions may not have known how many of the students currently enrolled in graduate study intended to earn doctoral degrees."}, {"section_title": "CHARACTERISTICS OF GRADUATES", "text": "One of the most meaningful measures of the success of a research-doctorate program is the performance of its graduates. How many go on to lead productive careers in research and/or teaching? Unfortunately, reliable information on the subsequent employment and career achievements of the graduates of individual programs is not available. In the absence of this directly relevant information, the committee has relied on four indirect measures derived from data compiled in the NRC's Survey of Earned Doctorates. 6 Although each measure has serious limitations (described below), the committee believes it more desirable to include this information than not to include data about program graduates. In identifying program graduates who had received their doctorates in the previous five years (FY1975-79), 7 the faculty lists furnished by the study coordinators at universities were compared with the names of dissertation advisers (available from the NRC survey). The latter source contains records for virtually all individuals who have earned research doctorates from U.S. universities since 1920. The institution, year, and specialty field of Ph.D. recipients were also used in determining the identity of program graduates. It is estimated that this matching process provided information on the graduate training and employment plans of more than 90 percent of the FY1975-79 graduates from the mathematical and physical science programs. In the calculation of each of the four measures derived from the NRC survey, program data are reported only if the survey information is available on at least 10 graduates. Consequently, in the disciplines with smaller programs-computer sciences and statistics/biostatistics-only slightly more than half the programs are included in these measures, whereas more than 90 percent of the chemistry and physics programs are included. Measure 04 constitutes the fraction of FY1975-79 graduates of a program who had received at least some national fellowship support, including National Institutes of Health (NIH) fellowships or traineeships, National Science Foundation (NSF) fellowships, other federal fellowships, Woodrow Wilson fellowships, or fellowships/ traineeships from other U.S. national organizations. One might expect the more selective programs to have a greater proportion of students with national fellowship support-especially \"portable fellowships.\" Although the committee considered alternative measures of student ability (e.g., Graduate Record Examination scores, undergraduate grade point averages), reliable information of this sort was unavailable for a national assessment. It should be noted that the relevance of the fellowship measure varies considerably among disciplines. In the biomedical sciences a substantial fraction of the graduate students are supported by training grants and fellowships; in the mathematical and physical sciences the majority are supported by research assistantships and teaching assistantships. Measure 05 is the median number of years elapsed from the time program graduates first enrolled in graduate school to the time they received their doctoral degrees. For purposes of analysis the committee has adopted the conventional wisdom that the most talented students are likely to earn their doctoral degrees in the shortest periods of time-hence, the shorter the median time-to-Ph.D., the higher the standardized score that is assigned. Although this measure has frequently been employed in social science research as a proxy for student ability, one must regard its use here with some skepticism. It is quite possible that the length of time it takes a student to complete requirements for a doctorate may be significantly affected by the explicit or implicit policies of a university or department. For example, in certain cases a short time-to-Ph.D. may be indicative of less stringent requirements for the degree. Furthermore, previous studies have demonstrated that women and members of minority groups, for reasons having nothing to do with their abilities, are more likely than male Caucasians to interrupt their graduate education or to be enrolled on a part-time basis. 8 As a consequence, the median time-to-Ph.D. may be longer for programs with larger fractions of women and minority students. Measure represents the fraction of FY 1975-79 program graduates who reported at the time they had completed requirements for the doctorate that they had signed contracts or made firm commitments for postgraduation employment (including postdoctoral appointments as well as other positions in the academic or nonacademic sectors) and who provided the names of their prospective employers. Although this measure is likely to vary by discipline according to the availability of employment opportunities, a program's standing relative to other programs in the same discipline should not be affected by this variation. In theory, the graduates with the greatest promise should have the easiest time finding jobs. However, the measure is also influenced by a variety of other factors, such as personal job preferences and restrictions in geographic mobility, that are unrelated to the ability of the individual. It also should be noted parenthetically that unemployment rates for doctoral recipients are quite low and that nearly all of the graduates seeking jobs find positions soon after completing their doctoral programs. 9 Furthermore, first employment after graduation is by no means a measure of career achievement, which is what one would like to have if reliable data were available. Measure 07, a variant of measure 06, constitutes the fraction of FY1975-79 program graduates who indicated that they had made firm commitments for employment in Ph.D.-granting universities and who provided the names of their prospective employers. This measure may be presumed to be an indication of the fraction of graduates likely to pursue careers in academic research, although there is no evidence concerning how many of them remain in academic research in the long term. In many science disciplines the path from Ph.D. to postdoctoral apprenticeship to junior faculty has traditionally been regarded as the road of success for the growth and development of research talent. The committee is well aware, of course, that other paths, such as employment in the major laboratories of industry and government, provide equally attractive opportunities for growth. Indeed, in recent years increasing numbers of graduates are entering the nonacademic sectors. Unfortunately, the data compiled from the NRC's Survey of Earned Doctorates do not enable one to distinguish between employment in the top-flight laboratories of industry and *Percentages are based on responses to the NRC's Survey of Earned Doctorates by those who indicated that they had made firm commitments for postgraduation employment and who provided the names of their prospective employers. These percentages may be considered lowerbound estimates of the actual percentages of doctoral recipients employed outside the academic sector. government and employment in other areas of the nonacademic sectors. Accordingly, the committee has relied on a measure that reflects only the academic side and views this measure as a useful and interesting program characteristic rather than a dimension of quality. In disciplines such as geosciences, chemistry, physics, and computer sciences, in which more than one-third of the graduates take jobs outside the academic environs (see Table 2.2), this limitation is of particular concern. The inclusion of measures 06 and 07 in this assessment has been an issue much debated by members of the committee; the strenuous objections of three committee members regarding the use of these measures are expressed in the Minority Statement that follows Chapter IX."}, {"section_title": "REPUTATIONAL SURVEY RESULTS", "text": "In April 1981, survey forms were mailed to a total of 1,788 faculty members in chemistry, computer sciences, geosciences, mathematics, physics, and statistics/biostatistics. The evaluators were selected from the faculty lists furnished by the study coordinators at the 228 universities covered in the assessment. These evaluators constituted approximately 13 percent of the total faculty population-13,661 faculty members-in the mathematical and physical science programs being evaluated (see Table 2.3). The survey sample was chosen on the basis of the number of faculty in a particular program and the number of doctorates awarded in the previous five years (FY1976-80)-with the stipulation that at least one evaluator was selected from every program covered in the assessment. In selecting the sample each faculty rank was represented in proportion to the total number of individuals holding that rank, and preference was given to those faculty members whom the study coordinators had nominated to serve as evaluators. As shown in Table 2.3, 1,461 individuals, 82 percent of the survey sample in the mathematical and physical sciences, had been recommended by study coordinators. 10 Each evaluator was asked to consider a stratified random sample of 50 research-doctorate programs in his or her discipline-with programs stratified by the number of faculty members associated with each program. Every program was included on 150 survey forms. The 50 programs to be evaluated appeared on each survey form in random sequence, preceded by an alphabetized list of all programs in that discipline that were being included in the study. No evaluator was asked to consider a program at his or her own institution. Ninety percent of the survey sample group were provided the names of faculty members in each of the 50 programs to be evaluated, along with data on the total number of doctorates awarded in the last five years. 11 The inclusion of this information represents a significant departure from the procedures used in earlier reputational assessments. For purposes of comparison with previous studies, 10 percent (randomly selected in each discipline) were not furnished any information other than the names of the programs. The survey items were adapted from the form used in the Roose-Andersen study. Prior to mailing, the instrument was pretested using a small sample of faculty members in chemistry and psychology. As a result, two significant improvements were made in the original survey design. A question was added on the extent to which the evaluator was familiar with the work of the faculty in each program. Responses to this question, reported as measure 11, provide some insight into the relationship between faculty recognition and the reputational standing of a program. 12 Also added was a question on the evaluator's field of specialization-thereby making it possible to compare program evaluations in different specialty areas within a particular discipline. A total of 1,155 faculty members in the mathematical and physical sciences-65 percent of those asked to participate-completed and returned survey forms (see Table 2.3). Two factors probably have contributed to this response rate being approximately 14 percentage points below the rates reported in the Cartter and Roose-Andersen studies. First, because of the considerable expense of printing individualized survey forms (each 25-30 pages), second copies were not sent to sample members not responding to the first mailing 13 -as was done in the Cartter and Roose-Andersen efforts. Second, it is quite apparent that within the academic community there has been a growing dissatisfaction in recent years with educational assessments based on reputational measures. Indeed, this dissatisfaction was an important factor in the Conference Board's decision to undertake a multidimensional assessment, and some faculty members included in the sample made known to the committee their strong objections to the reputational survey. As can be seen in Table 2.3, there is some variation in the response rates in the six mathematical and physical science disciplines. Of particular interest is the relatively high rate of response from chemists and the low rate from physicists-a result consistent with the findings in the Cartter and Roose-Andersen surveys. 14 It is not surprising to find that the evaluators nominated by study coordinators responded more often than did those who had been selected at random. No appreciable differences were found among the response rates of assistant, associate, and full professors or between the rates of those evaluators who were furnished the abbreviated survey form (without lists of program faculty) and those who were given the longer version. Each program was considered by an average of approximately 90 survey respondents from other programs in the same discipline. The evaluators were asked to judge programs in terms of scholarly quality of program faculty, effectiveness of the program in educating research scholars/scientists, and change in program quality in the last five years. 15 The mean ratings of a program on these three survey items constitute measures 08, 09, and 10. Evaluators were also asked to indicate the extent to which they were familiar with the work of the program faculty. The average of responses to this item constitutes measure 11. In making judgments about the quality of faculty, evaluators were instructed to consider the scholarly competence and achievements of the individuals. The ratings were furnished on the following scale: In assessing the effectiveness of a program, evaluators were asked to consider the accessibility of faculty, the curricula, the instructional and research facilities, the quality of the graduate students, the performance of graduates, and other factors that contribute to a program's effectiveness. This measure was rated accordingly:  14 To compare the response rates obtained in the earlier surveys, see Roose and Andersen, Table 28, p. 29. 15 A copy of the survey instrument and accompanying instructions are included in Appendix C. Evaluators were instructed to assess change in program quality on the basis of whether there was an improvement in the last five years in both the scholarly quality of the faculty and the effectiveness in educating research scholars/scientists. The following alternatives were provided: 2 Better than five years ago 1 Little or no change in last five years 0 Poorer than five years ago X Don't know well enough to evaluate Evaluators were asked to indicate their familiarity with the work of the program faculty according to the following scale: In the computation of mean ratings on measures 08, 09, and 10, the \"don't know\" responses were ignored. An average program rating based on fewer than 15 responses (excluding the \"don't know\" responses) is not reported. Measures 08, 09, and 10 are subject to many of the same criticisms that have been directed at previous reputational surveys. Although care has been taken to improve the sampling design and to provide evaluators with some essential information about each program, the survey results merely reflect a consensus of faculty opinions. As discussed in Chapter I, these opinions may well be based on out-of-date information or be influenced by a variety of factors unrelated to the quality of the program. In Chapter IX a number of factors that may possibly affect the survey results are examined. In addition to these limitations, it should be pointed out that the evaluators, on the average, were unfamiliar with almost one-third of the programs they were asked to consider. 16 As might be expected, the smaller and less prestigious programs were not as well known, and for this reason one might have less confidence in the average ratings of these programs. For all four survey measures, standard errors of the mean ratings are reported; they tend to be larger for the lesser known programs. The frequency of response to each of the survey items is discussed in Chapter IX. Two additional comments should be made regarding the survey activity. First, it should be emphasized that the ratings derived from the survey reflect a program's standing relative to other programs in the same discipline and provide no basis for making cross-disciplinary comparisons. For example, the fact that a much larger number of chemistry programs received \"distinguished\" ratings on measure 08 than did computer science programs indicates nothing about the relative quality of faculty in these two disciplines. It may depend, in part, on the total numbers of programs evaluated in these disciplines; in the survey instructions it was suggested to evaluators that no more than 10 percent of the programs listed be designated as \"distinguished.\" Nor is it advisable to compare the rating of a program in one discipline with that of a program in another discipline because the ratings are based on the opinions of different groups of evaluators who were asked to judge entirely different sets of programs. Second, early in the committee's deliberations a decision was made to supplement the ratings obtained from faculty members with ratings from evaluators who hold research-oriented positions in institutions outside the academic sector. These institutions include industrial research laboratories, government research laboratories, and a variety of other research establishments. Over the past 10 years increasing numbers of doctoral recipients have taken positions outside the academic setting. The extensive involvement of these graduates in nonacademic employment is reflected in the percentages reported in Table 2.2: An average of 40 percent of the recent graduates in the mathematical and physical science disciplines who had definite employment plans indicated that they planned to take positions in nonacademic settings. Data from another NRC survey suggest that the actual fraction of scientists employed outside academia may be significantly higher. The committee recognized that the inclusion of nonacademic evaluators would furnish information valuable for assessing nontraditional dimensions of doctoral education and would provide an important new measure not assessed in earlier studies. Results from a survey of this group would provide an interesting comparison with the results obtained from the survey of faculty members. A concentrated effort was made to obtain supplemental funding for adding nonacademic evaluators in selected disciplines to the survey sample, but this effort was unsuccessful. The committee nevertheless remains convinced of the importance of including evaluators from nonacademic research institutions. These institutions are likely to employ increasing fractions of graduates in many disciplines, and it is urged that this group not be overlooked in future assessments of graduate programs."}, {"section_title": "UNIVERSITY LIBRARY SIZE", "text": "The university library holdings are generally regarded as an important resource for students in graduate (and undergraduate) education. The Association of Research Libraries (ARL) has compiled data from its academic member institutions and developed a composite measure of a university library's size relative to those of other ARL members. The ARL Library Index, as it is called, is based on 10 characteristics: volumes held, volumes added (gross), microform units held, current serials received, expenditures for library materials, expenditures for binding, total salary and wage expenditures, other operating expenditures, number of professional staff, and number of nonprofessional staff. 17 The 1979-80 index, which constitutes measure 12, is available for 89 of the universities included in the assessment. (These 89 tend to be among the largest institutions.) The limited coverage of this measure is a major shortcoming. It should be noted that the ARL index is a composite description of library size and not a qualitative evaluation of the collections, services, or operations of the library. Also, it is a measure of aggregate size and does not take into account the library holdings in a particular department or discipline. Finally, although universities with more than one campus were instructed to include figures for the main campus only, some in fact may have reported library size for the entire university system. Whether this misreporting occurred is not known."}, {"section_title": "RESEARCH SUPPORT", "text": "Using computerized data files 18 provided by the National Science Foundation (NSF) and the National Institutes of Health (NIH), it was possible to identify which faculty members in each program had been awarded research grants during the FY1978-80 period by either of these agencies or by the Alcohol, Drug Abuse, and Mental Health Administration (ADAMHA). The fraction of faculty members in a program who had received any research grants from these agencies during this three-year period constitutes measure 13. Since these awards have been made on the basis of peer judgment, this measure is considered to reflect the perceived research competence of program faculty. However, it should be noted that significant amounts of support for research in the mathematical and physical sciences come from other federal agencies as well, but it was not feasible to compile data from these other sources. It is estimated 19 that 55 percent of the university faculty members in these disciplines who received federal R&D funding obtained their support from NSF and another 19 percent from NIH. The remaining 26 percent received support from the Department of Energy, Department of Defense, National Aeronautics and Space Administration, and other federal agencies. It also should be pointed out that only those faculty members who served as principal investigators or co-investigators are counted in the computation of this measure. Measure 14 describes the total FY1979 expenditures by a university for R&D in a particular discipline. These data have been furnished to the NSF 20 by universities and include expenditures of funds from both federal and nonfederal sources. If an institution has more than one program being evaluated in the same discipline, the aggregate university expenditures for research in that discipline are reported 17 See Appendix D for a description of the calculation of this index. 18 A description of these files is provided in Appendix E. 19 Based on special tabulations of data from the NRC's Survey of Doctorate Recipients, 1979. 20 A copy of the survey instrument used to collect these data appears in Appendix E. for each of the programs. In each discipline data are recorded for the 100 universities with the largest R&D expenditures. As already mentioned, these data are not available for statistics and biostatistics programs. This measure has several limitations related to the procedures by which the data have been collected. The committee notes that there is evidence within the source document 21 that universities employ varying practices for categorizing and reporting expenditures. Apparently, institutional support of research, industrial support of research, and expenditure of indirect costs are reported by different institutions in different categories (or not reported at all). Since measure 14 is based on total expenditures from all sources, the data used here are perturbed only when these types of expenditures are not subsumed under any reporting category. Also, it should be noted that the data being attributed to geosciences programs include university expenditures in all areas of the environmental sciences (geological sciences, atmospheric sciences, and oceanography), and the data for mathematics programs include expenditures in statistics as well as mathematics. In contrast with measure 13, measure 14 is not reported on a scale relative to the number of faculty members and thus reflects the overall level of research activity at an institution in a particular discipline. Although research grants in the sciences and engineering provide some support for graduate students as well, these measures should not be confused with measure 04, which pertains to fellowships and training grants."}, {"section_title": "PUBLICATION RECORDS", "text": "Data from the 1978 and the 1979 Science Citation Index have been compiled 22 on published articles associated with research-doctorate programs. Publication counts were associated with programs on the basis of the discipline of the journal in which an article appeared and the institution with which the author was affiliated. Coauthored articles were proportionately attributed to the institutions of the individual authors. Articles appearing in multidisciplinary journals (e.g., Science, Nature) were apportioned according to the characteristic mix of subject matter in those journals. For the purposes of assigning publication counts, this mix can be estimated with reasonable accuracy. 23 Two measures have been derived from the publication records: measure 15-the total number of articles published in the 1978-79 period that have been associated with a research-doctorate program and measure 16-an estimation of the \"influence\" of these articles. The latter is a product of the number of articles attributed to a program and the estimated influence of the journals in which these articles appeared. The influence of a journal is determined from the weighted number of times, on the average, an article in that journal is cited-with references from frequently cited journals counting more heavily. A more detailed explanation of the derivation of these measures is given in Appendix F. Neither measure 15 nor measure 16 is based on actual counts of articles written only by program faculty. However, extensive analysis of the \"influence\" index in the fields of physics, chemistry, and biochemistry has demonstrated the stability of this index and the reliability associated with its use. 24 Of course, this does not imply that the measure captures subtle aspects of publication \"influence.\" It is of interest to note that indices similar to measures 15 and 16 have been shown to be highly correlated with the peer ratings of graduate departments compiled in the Roose-Andersen study. 25 It must be emphasized that these measures encompass articles (published in selected journals) by all authors affiliated with a given university. Included therefore are articles by program faculty members, students and research personnel, and even members of other departments in that university who publish in those journals. Moreover, these measures do not take into account the differing sizes of programs, and the measures clearly do depend on faculty size. Although consideration was given to reporting the number of published articles per faculty member, the committee concluded that since the measure included articles by other individuals besides program faculty members, the aggregate number of articles would be a more reliable measure of overall program quality. It should be noted that if a university had more than one program being evaluated in the same discipline, it is not possible to distinguish the relative contribution of each program. In such cases the aggregate university data in that discipline were assigned to each program. Since the data are confined to 1978-79, they do not take into account institutional mobility of authors after that period. Thus, articles by authors who have moved from one institution to another since 1979 are credited to the former institution. Also, the publication counts fail to include the contributions of faculty members' publications in journals outside their primary discipline. This point may be especially important for those programs with faculty members whose research is at the intersection of several different disciplines. The reader should be aware of two additional caveats with regard to the interpretation of measures 15 and 16. First, both measures are based on counts of published articles and do not include books. Since in the mathematical and physical sciences most scholarly contributions are published as journal articles, this may not be a serious limitation. Second, the \"influence\" measure should not be interpreted as an indicator of the impact of articles by individual authors. Rather it is a measure of the impact of the journals in which articles associated with a particular program have been published. Citation counts, with all their difficulties, would have been preferable since they are attributable to individual authors and they register the impact of books as well as journal articles. However, the difficulty and cost of assembling reliable counts of articles by individual author made their use infeasible."}, {"section_title": "ANALYSIS AND PRESENTATION OF THE DATA", "text": "The next six chapters present all of the information that has been compiled on individual research-doctorate programs in chemistry, computer sciences, geosciences, mathematics, physics, and statistics/ biostatistics. Each chapter follows a similar format, designed to assist the reader in the interpretation of program data. The first table in each chapter provides a list of the programs evaluated in a discipline-including the names of the universities and departments or academic units in which programs reside-along with the full set of data compiled for individual programs. Programs are listed alphabetically according to name of institution, and both raw and standardized values are given for all but one measure. 26 For the reader's convenience an insert of information from Table 2.1 is provided that identifies each of the 16 measures reported in the table and indicates the raw scale used in reporting values for a particular measure. Standardized values, converted from raw values to have a mean of 50 and a standard deviation of 10, are computed for every measure so that comparisons can easily be made of a program's relative standing on different measures. Thus, a standardized value of 30 corresponds with a raw value that is two standard deviations below the mean for that measure, and a standardized value of 70 represents a raw value two standard deviations above the mean. While the reporting of values in standardized form is convenient for comparing a particular program's standing on different measures, it may be misleading in interpreting actual differences in the values reported for two or more programs-especially when the distribution of the measure being examined is highly skewed. For example, the numbers of published articles (measure 15) associated with four chemistry programs are reported in Table 3 Although programs C and D have many times the number of articles as programs A and B, the differences reported on a standardized scale appear to be small. Thus, the reader is urged to take note of the raw values before attempting to interpret differences in the standardized values given for two or more programs. The initial table in each chapter also presents estimated standard errors of mean ratings derived from the four survey items (measures 08-11). A standard error is an estimated standard deviation of the sample mean rating and may be used to assess the stability of a mean rating reported for a particular program. 27 For example, one may assert (with .95 confidence) that the population mean rating would lie within two standard errors of the sample mean rating reported in this assessment. No attempt has been made to establish a composite ranking of programs in a discipline. Indeed, the committee is convinced that no single measure adequately reflects the quality of a research-doctorate program and wishes to emphasize the importance of viewing individual programs from the perspective of multiple indices or dimensions. The second table in each chapter presents summary statistics (i.e., number of programs evaluated, mean, standard deviation, and decile values) for each of the program measures. 28 The reader should find these statistics helpful in interpreting the data reported on individual programs. Next is a table of the intercorrelations among the various measures for that discipline. This table should be of particular interest to those desiring information about the interrelations of the various measures. 27 The standard error estimate has been computed by dividing the standard deviation of a program's ratings by the square root of the number of ratings. For a more extensive discussion of this topic, see Fred N.Kerlinger, Foundations of Behavioral Research, Holt, Reinhart, and Winston, Inc., New York, 1973, Chapter 12. Readers should note that the estimate is a measure of the variation in response and by no means includes all possible sources of error. 28 Standardized scores have been computed from precise values of the mean and standard deviation of each measure and not the rounded values reported in the second table of each chapter. The remainder of each chapter is devoted to an examination of results from the reputational survey. Included are an analysis of the characteristics of survey participants and graphical portrayals of the relationship of mean rating of scholarly quality of faculty (measure 08) with number of faculty (measure 01) and the relationship of mean rating of program effectiveness (measure 09) with number of graduates (measure 02). A frequently mentioned criticism of the Roose-Andersen and Cartter studies is that small but distinguished programs have been penalized in the reputational ratings because they are not as highly visible as larger programs of comparable quality. The comparisons of survey ratings with measures of program size are presented as the first two figures in each chapter and provide evidence about the number of small programs in each discipline that have received high reputational ratings. Since in each case the reputational rating is more highly correlated with the square root of program size than with the size measure itself, measures 01 and 02 are plotted on a square root scale. 29 To assist the reader in interpreting results of the survey evaluations, each chapter concludes with a graphical presentation of the mean rating for every program of the scholarly quality of faculty (measure 08) and an associated \"confidence interval\" of 1.5 standard errors. In comparing the mean ratings of two programs, if their reported confidence intervals of 1.5 standard errors do not overlap, one may safely conclude that the program ratings are significantly different (at the .05 level of significance)-i.e., the observed difference in mean ratings is too large to be plausibly attributable to sampling error. 30 The final chapter of this report gives an overview of the evaluation process in the six mathematical and physical science disciplines and includes a summary of general findings. Particular attention is given to some of the extraneous factors that may influence program ratings of individual evaluators and thereby distort the survey results. The chapter concludes with a number of specific suggestions for improving future assessments of research-doctorate programs. 29 For a general discussion of transforming variables to achieve linear fits, see John W.Tukey, Exploring Data Analysis, Addision Wesley, Reading, Massachusetts, 1977. 30 This rule for comparing nonoverlapping intervals is valid as long as the ratio of the two estimated standard errors does not exceed 2.41. (The exact statistical significance of this criterion then lies between .050 and .034.) Inspection of the standard errors reported in each discipline shows that for programs with mean ratings differing by less than 1.0 (on measure 08), the standard error of one mean very rarely exceeds twice the standard error of another."}, {"section_title": "III", "text": ""}, {"section_title": "Chemistry Programs", "text": "In this chapter 145 research-doctorate programs in chemistry are assessed. These programs, according to the information supplied by their universities, have accounted for 7,304 doctoral degrees awarded during the FY1976-80 period-approximately 93 percent of the aggregate number of chemistry doctorates earned from U.S. universities in this five-year span. 1 On the average, 75 full-time and part-time students intending to earn doctorates were enrolled in a program in December 1980, with an average faculty size of 23 members. 2 The 145 programs, listed in Table 3.1, represent 143 different universities-the University of Akron and the University of Kansas each have two chemistry programs included in the assessment. All but three of the programs were initiated prior to 1970. In addition to the 143 universities represented in this discipline, another 4 were initially identified as meeting the criteria 3 for inclusion in the assessment: Chemistry programs at these four institutions have not been included in the evaluations in this discipline since in each case the study coordinator either indicated that the institution did not at that time have a research-doctorate program in chemistry or failed to provide the information requested by the committee. Before examining individual program results presented in Table 3.1, the reader is urged to refer to Chapter II, in which each of the 16 measures used in the assessment is discussed. Summary statistics describing every measure are given in Table 3.2. For all but two measures, data are reported for at least 139 of the 145 chemistry programs. For measure 12, a composite index of the size of a university library, data are available for 87 programs; for measure 14, total university expenditures for research in this discipline, data are available for 95 programs. The programs not evaluated on these two measures are typically smaller-in terms of faculty size and graduate student enrollment -than other chemistry programs. Were data on measures 12 and 14 available for all 145 programs, it is likely that the reported means for these two measures would be appreciably lower (and that some of the correlations of these measures with others would be higher). Intercorrelations among the 16 measures (Pearson product-moment coefficients) are given in Table 3.3. Of particular note are the high positive correlations of the measures of program size (01-03) with measures of publication records (15, 16) and reputational survey ratings (08, 09, and 11). Figure 3.1 illustrates the relation between the mean rating of the scholarly quality of faculty (measure 08) and the number of faculty members (measure 01) for each of the 145 programs in chemistry. Figure 3.2 plots the mean rating of program effectiveness (measure 09) against the total number of FY1976-80 program graduates (measure 02). Although in both figures there is a significant positive correlation between program size and reputational rating, it is quite apparent that some of the smaller programs received high mean ratings and that some of the larger programs received low mean ratings. Table 3.4 describes the 301 faculty members who participated in the evaluation of chemistry programs. These individuals constituted 69 percent of those asked to respond to the survey in this discipline and 9 percent of the faculty population in the 145 research-doctorate programs being evaluated. 4 A majority of the survey participants were organic or physical chemists and held the rank of full professor. Almost three-fourths of them had earned their highest degree prior to 1970. To assist the reader in interpreting results of the survey evaluations, estimated standard errors have been computed for mean ratings of the scholarly quality of the faculty in 145 chemistry programs (and are given in Table 3.1). For each program the mean rating and an associated \"confidence interval\" of 1.5 standard errors are illustrated in Figure 3.3 (listed in order of highest to lowest mean rating). In comparing two programs, if their confidence intervals do not overlap, one may safely conclude that there is a significant difference in their mean ratings at a .05 level of significance. 5 From this figure it is also apparent that one should have somewhat more confidence in the accuracy of the mean ratings of higher-rated programs than lower-rated programs. This generalization results primarily from the fact that evaluators are not as likely to be familiar with the less prestigious programs, and consequently the mean ratings of these programs are usually based on fewer survey responses.                  "}, {"section_title": "IV Computer Science Programs", "text": "In this chapter 58 research-doctorate programs in computer sciences are assessed. These programs, according to the information supplied by their universities, have accounted for 1,154 doctoral degrees awarded during the FY1976-80 period-approximately 86 percent of the aggregate number of computer science and computer engineering doctorates earned from U.S. universities in this five-year span. 1 Because computer sciences is a younger discipline than the other five mathematical and physical sciences covered in this assessment and because computer science programs may be found in a variety of settings within universities, the committee encountered some difficulty in identifying research-doctorate programs that have produced graduates in this discipline. On the average, 41 full-time and part-time students intending to earn doctorates were enrolled in a program in December 1980, with an average faculty size of 16 members. 2 Most of the 58 programs, listed in Table 4.1, are located in computer science or computer and information science departments. Approximately 20 percent are found in departments of electrical engineering. Fifteen programs were initiated since 1970, and no two programs are located in the same university. In addition to the 58 institutions represented in this discipline, another 7 were initially identified as meeting the criteria 3 for inclusion in the assessment: University of Chicago"}, {"section_title": "George Washington University", "text": "Harvard University Northeastern University 1 Data from the NRC's Survey of Earned Doctorates indicate that 889 research doctorates in computer sciences and another 458 research doctorates in computer engineering were awarded by U.S. universities between FY1976 and FY1980. 2 See the reported means for measures 03 and 01 in Table 4.2. 3 As mentioned in Chapter I, the primary criterion for inclusion was that a university had awarded at least 5 doctorates in computer sciences during the FY1976-78 period. The latter two institutions chose not to participate in the assessment in any discipline. Computer science programs at the other five institutions have not been included in the evaluations in this discipline, since in each case the study coordinator either indicated that the institution did not at that time have a research-doctorate program in computer sciences or failed to provide the information requested by the committee. Before examining individual program results presented in Table 4.1, the reader is urged to refer to Chapter II, in which each of the 16 measures used in the assessment is discussed. Summary statistics describing every measure are given in Table 4.2. For nine of the measures, data are reported for at least 56 of the 58 computer science programs. For measures 04-07, which pertain to characteristics of the program graduates, data are presented for only approximately half of the programs; the other half had too few graduates on which to base statistics. 4 For measure 12, a composite index of the size of a university library, data are available for 49 programs; for measure 14, total university expenditures for research in this discipline, data are available for 44 programs. The programs not evaluated on measures 12 and 14 are typically smaller-in terms of faculty size and graduate student enrollment-than other computer science programs. Were data on these two measures available for all 58 programs, it is likely that their reported means would be appreciably lower (and that some of the correlations of these measures with others would be higher). With respect to measure 13, the fraction of faculty with research support from the National Science Foundation, the National Institutes of Health, and the Alcohol, Drug Abuse, and Mental Health Administration, data are reported for 45 programs that had at least 10 faculty members. Intercorrelations among the 16 measures (Pearson product-moment coefficients) are given in Table 4.3. Of particular note are the high positive correlations of the measures of program size (01-03) with measures of publication records (15, 16) and reputational survey ratings (08 and 09). Figure 4.1 illustrates the relation between the mean rating of the scholarly quality of faculty (measure 08) and the number of faculty members (measure 01) for each of 57 programs in computer sciences. Figure 4.2 plots the mean rating of program effectiveness (measure 09) against the total number of FY1976-80 program graduates (measure 02). Although in both figures there is a significant positive correlation between program size and reputational rating, it is quite apparent that some of the smaller programs received high mean ratings and that some of the larger programs received low mean ratings. Table 4.4 describes the 108 faculty members who participated in the evaluation of computer science programs. These individuals constituted 62 percent of those asked to respond to the survey in this discipline and 12 percent of the faculty population in the 58 research-doctorate programs being evaluated. 5 A majority of the survey participants had earned their highest degree since 1970, and almost one-third held the rank of assistant professor. Two exceptions should be noted with regard to the survey evaluations in this discipline. Regretably, ratings are unavailable for the program in the Department of Computer and Communications Sciences at the University of Michigan since an entirely inaccurate list of its faculty members was included on the survey form. Also, it has been called to the attention of the committee that the faculty list (used in the survey) for the Department of Computer Science at Columbia University was missing the names of four members. The committee has decided to report the survey results for this program but cautions that the reputational ratings may have been influenced by the omission of these names. To assist the reader in interpreting results of the survey evaluations, estimated standard errors have been computed for mean ratings of the scholarly quality of faculty in 57 computer science programs (and are given in Table 4.1). For each program the mean rating and an associated \"confidence interval\" of 1.5 standard errors are illustrated in Figure 4.3 (listed in order of highest to lowest mean rating). In comparing two programs, if their confidence intervals do not overlap, one may conclude that there is a significant difference in their mean ratings at a .05 level of significance. 6 From this figure it is also apparent that one should have somewhat more confidence in the accuracy of the mean ratings of higher-rated programs than lower-rated programs. This generalization results primarily from the fact that evaluators are not as likely to be familiar with the less prestigious programs, and consequently the mean ratings of these programs are usually based on fewer survey responses.           institutions have not been included in the evaluations in this discipline, since in each case the study coordinator either indicated that the institution did not at that time have a research-doctorate program in geosciences or failed to provide the information requested by the committee. Before examining individual program results presented in Table 5.1, the reader is urged to refer to Chapter II, in which each of the 16 measures used in the assessment is discussed. Summary statistics describing every measure are given in Table 5.2. For nine of the measures, data are reported for at least 89 of the 91 geoscience programs. For measures 04-07, which pertain to characteristics of the program graduates, data are presented for only two-thirds of the programs; the other one-third had too few graduates on which to base statistics. 4 For measure 12, a composite index of the size of a university library, data are available for 69 programs; for measure 14, total university expenditures for research in this discipline, data are available for 73 programs. With respect to the measure 14, it should be noted that the reported data include expenditures for research in atmospheric sciences and oceanography as well as in the geosciences. The programs not evaluated on measures 12 and 14 are typically smaller-in terms of faculty size and graduate student enrollment-than other geoscience programs. Were data on measures 12 and 14 available for all 91 programs, it is likely that the reported means for these two measures would be appreciably lower (and that some of the correlations of these measures with others would be higher). With respect to measure 13, the fraction of faculty with research support from the National Science Foundation, the National Institutes of Health, and the Alcohol, Drug Abuse, and Mental Health Adminstration, data are reported for 72 programs that had at least 10 faculty members. Intercorrelations among the 16 measures (Pearson product-moment coefficients) are given in Table 5.3. Of particular note are the high positive correlations of the measures of the numbers of doctoral graduates and students (02, 03) with measures of publication records (15-16) and reputational survey ratings (08, 09, and 11). Figure 5.1 illustrates the relationship between the mean rating of the scholarly quality of faculty (measure 08) and the number of faculty members (measure 01) for each of the 91 geoscience programs. Figure 5.2 plots the mean rating of program effectiveness (measure 09) against the total number of FY1976-80 program graduates (measure 02). Although in both figures there is a significant positive correlation between program size and reputational rating, it is quite apparent that some of the smaller programs received high mean ratings and some of the larger programs received low mean ratings. Table 5.4 describes the 177 faculty members who participated in the evaluation of geoscience programs. These individuals constituted 65 percent of those asked to respond to the survey in this discipline and 12 percent of the faculty population in the 91 research-doctorate programs being evaluated. 5 More than one-third of the survey participants were geologists, and approximately two-thirds held the rank of full professor. Almost three-fourths of them had earned their highest degree prior to 1970. To assist the reader in interpreting results of the survey evaluations, estimated standard errors have been computed for mean ratings of the scholarly quality of faculty in 91 geoscience programs (and are given in Table 5.1). For each program the mean rating and an associated \"confidence interval\" of 1.5 standard errors are illustrated in Figure 5.3 (listed in order of highest to lowest mean rating). In comparing two programs, if their confidence intervals do not overlap, one may conclude that there is a significant difference in their mean ratings at a .05 level of significance. 6 From this figure it is also apparent that one should have somewhat more confidence in the accuracy of the mean ratings of higher-rated programs than lower-rated programs. This generalization results primarily from the fact that evaluators are not as likely to be familiar with the less prestigious programs, and consequently the mean ratings of these programs are usually based on fewer survey responses.               "}, {"section_title": "VI Mathematics Programs", "text": "In this chapter 115 research-doctorate programs in mathematics are assessed. These programs, according to the information supplied by their universities, have accounted for 2,731 doctoral degrees awarded during the FY1976-80 period-approximately 92 percent of the aggregate number of mathematics doctorates earned from U.S. universities in this five-year span. 1 On the average, 35 full-time and part-time students intending to earn doctorates were enrolled in a program in December 1980, with an average faculty size of 33 members. 2 The 115 programs, listed in Table 6.1, represent 114 different universities-two programs are included from the University of Maryland (College Park). All but six of the programs were initiated prior to 1970. In addition to the 114 universities represented in this discipline, another 3 were initially identified as meeting the criteria 3  Mathematics programs at these three institutions have not been included in the evaluations in this discipline, since in each case the study coordinator either indicated that the institution did not at that time have a researchdoctorate program in mathematics or failed to provide the information requested by the committee. Before examining the individual program results presented in Table 6.1, the reader is urged to refer to Chapter II, in which each of the 16 measures used in the assessment is discussed. Summary 1 Data from the NRC's Survey of Earned Doctorates indicate that 2,958 research doctorates in mathematics were awarded by U.S. universities between FY1976 and FY1980. 2 See the reported means for measures 03 and 01 in Table 6.2. 3 As mentioned in Chapter I, the primary criterion for inclusion was that a university had awarded at least 7 doctorates in mathematics during the FY1976-78 period. statistics describing every measure are given in Table 6.2. For 10 of the measures, data are reported for at least 108 of the 115 mathematics programs. For measures 04-07, which pertain to characteristics of the program graduates, data are presented for 95 (or more) of the programs; the other 20 programs had too few graduates on which to base statistics. 4 For measure 12, a composite index of the size of a university library, data are available for 82 programs; for measure 14, total university expenditures for research in this discipline, data are available for 83 programs. With respect to the latter measure, it should be noted that reported data include expenditures for research in statistics as well as in mathematics. The programs not evaluated on measures 12 and 14 are typically smaller-in terms of faculty size and graduate student enrollment-than other mathematics programs. Were data on measures 12 and 14 available for all 91 programs, it is likely that the reported means for these measures would be appreciably lower (and that some of the correlations of these measures with others would be higher). Intercorrelations among the 16 measures (Pearson product-moment coefficients) are given in Table 6.3. Of particular note are the high positive correlations of the measures of the numbers of doctoral graduates and students (02, 03) with measures of publication records (15, 16) and reputational survey ratings (08, 09, and 11). Figure 6.1 illustrates the relation between the mean rating of the scholarly quality of faculty (measure 08) and the number of faculty members (measure 01) for each of the 114 mathematics programs. Figure 6.2 plots the mean rating of program effectiveness (measure 09) against the total number of FY1976-80 program graduates (measure 02). Although in both figures there is a significant positive correlation between program size and reputational rating, it is quite apparent that some of the smaller programs received high mean ratings and some of the larger programs received low mean ratings. Table 6.4 describes the 223 faculty members who participated in the evaluation of mathematics programs. These individuals constituted 64 percent of those asked to respond to the survey in this discipline and 6 percent of the faculty population in the 115 research-doctorate programs being evaluated. 5 More than one-third of the survey participants were in the specialty area of analysis/functional analysis, and almost two-thirds were full professors. More than two-thirds had earned their highest degree prior to 1970. Several exceptions should be noted with regard to the survey evaluations in this discipline. In the initial survey mailing the list of faculty in the Brown University program included only applied mathematicians. At the request of the study coordinator at this institution and a member of the study committee, the names of another 24 mathematics faculty members were added to the list, and revised survey forms that included the Brown program along with 11 other (randomly selected) mathematics programs were sent to 178 evaluators in this discipline. 6 The responses to the second survey were used to compute mean ratings for the Brown program. Another problem with the survey evaluations in mathematics involved the mislabeling of the location of an institution. In the program listing on the survey form, New Mexico State University at Las Cruces was identified as being located in Alamogordo, which has a junior college branch of the same institution. Since a large majority of faculty evaluators indicated that they were unfamiliar with this program and it is quite possible that some of them were misled by the inaccurate identification of this institution, the committee has decided not to report the survey results for this program. Two other instances of mislabeling were called to the attention of the committee. The program at the Courant Institute was identified as \"New York University-Mathematics,\" and the Wesleyan University program in the Department of Mathematics was called \"Physical Sciences.\" The committee has decided in both instances to report the survey results but cautions that the reputational ratings may have been influenced by the use of inaccurate program titles on the survey form. To assist the reader in interpreting results of the survey evaluations, estimated standard errors have been computed for mean ratings of the scholarly quality of faculty in 114 mathematics programs (and are given in Table 6.1). For each program the mean rating and an associated \"confidence interval\" of 1.5 standard errors are illustrated in Figure 6.3 (listed in order of highest to lowest mean rating). In comparing two programs, if their confidence intervals do not overlap, one may conclude that there is a significant difference in their mean ratings at a .05 level of significance. 7 From this figure it is also apparent that one should have somewhat more confidence in the accuracy of the mean ratings of higher-rated programs than lower-rated programs. This generalization results primarily from the fact that evaluators are not as likely to be familiar with the less prestigious programs, and consequently the mean ratings of these programs are usually based on fewer survey responses.                 "}, {"section_title": "VII Physics Programs", "text": "In this chapter 123 research-doctorate programs in physics are assessed. These programs, according to the information supplied by their universities, have accounted for 4,271 doctoral degrees awarded during the FY1976-80 period-approximately 87 percent of the aggregate number of physics doctorates earned from U.S. universities in this five-year span. 1 On the average, 56 full-time and part-time students intending to earn doctorates were enrolled in a program in December 1980, with an average faculty size of 28 members. 2 The 123 programs, listed in Table 7.1, represent 122 different universities-only Stanford University has two physics programs included in the assessment. All but two of the programs were initiated prior to 1970. In addition to the 122 universities represented in this discipline, only one other institution-Purdue University-was initially identified as meeting the criteria 3 for inclusion in the assessment. Since no information was received (in response to the committee's request) on a physics program at this institution, it has not been included in the evaluations in this discipline. Before examining the individual program results presented in Table 7.1, the reader is urged to refer to Chapter II, in which each of the 16 measures used in the assessment is discussed. Summary statistics describing every measure are given in Table 7.2. For all but two of the measures, data are reported for at least 109 of the 123 physics programs. For measure 12, a composite index of the size of a university library, data are available for 83 programs; for measure 14, total university expenditures for research in this discipline, data are available for 88 programs. The programs not evaluated on measures 12 and 14 are typically smaller-in terms of faculty size and graduate student enrollment-than other physics programs. Were data on measures 12 and 14 available for all 123 programs, it is likely that the reported means for these two measures would be appreciably lower (and that some of the correlations of these measures with others would be higher). Intercorrelations among the 16 measures (Pearson product-moment coefficients) are given in Table 7.3. Of particular note are the high positive correlations of the measures of program size (01-03) with measures of publication records (15, 16), research expenditures 14, and reputational survey ratings (08, 09, and 11). Figure 7.1 illustrates the relation between the mean rating of the scholarly quality of faculty (measure 08) and the number of faculty members (measure 01) for each of the 121 physics programs. Figure 7.2 plots the mean rating of program effectiveness (measure 09) against the total number of FY1976-80 program graduates (measure 02). Although in both figures there is a significant positive correlation between program size and reputational rating, it is quite apparent that some of the smaller programs received high mean ratings and some of the larger programs received low mean ratings. Table 7.4 describes the 211 faculty members who participated in the evaluation of physics programs. These individuals constituted 57 percent of those asked to respond to the survey in this discipline and 6 percent of the faculty population in the 123 research-doctorate programs being evaluated. 4 A majority of the survey participants specialized in elementary particles or solid state physics, and more than two-thirds held the rank of full professor. Approximately 85 percent had earned their highest degree prior to 1970. One exception should be noted with regard to the survey evaluations in this discipline. In the program listing on the survey form, New Mexico State University at Las Cruces was identified as being located in Alamogordo, which has a junior college branch of the same institution. Since a large majority of faculty evaluators indicated that they were unfamiliar with this program and it is quite possible that some of them were misled by the inaccurate identification of this institution, the committee has decided not to report the survey results for this program. To assist the reader in interpreting results of the survey evaluations, estimated standard errors have been computed for mean ratings of the scholarly quality of faculty in 121 physics programs (and are given in Table 7.1). For each program the mean rating and an associated \"confidence interval\" of 1.5 standard errors are illustrated in Figure 7.3 (listed in order of highest to lowest mean rating). In comparing two programs, if their confidence intervals do not overlap, one may safely conclude that there is a significant difference in their mean ratings at a .05 level of significance. 5 From this figure it is also apparent that one should have somewhat more confidence in the accuracy of the mean ratings of higher-rated programs than lower-rated programs. This generalization results primarily from the fact that evaluators are not as likely to be familiar with the less prestigious programs, and consequently the mean ratings of these programs are usually based on fewer survey responses.            NOTE: On the first line of data for every program, raw values for each measure are reported; on the second line values are reported in standardized form, with mean=50 and standard deviation=10. \"NA\" indicates that the value for a measure is not available. Since the scale used to compute measure (16) is entirely arbitrary, only values in standardized form are reported for this measure."}, {"section_title": "PHYSICS PROGRAMS 131", "text": "About this PDF file: This new digital representation of the original work has been recomposed from XML files created from the original paper book, not from the original typesetting files. Page breaks are true to the original; line lengths, word breaks, heading styles, and other typesetting-specific formatting, however, cannot be retained, and some typographic errors may have been accidentally       case the study coordinator either indicated that the institution did not at that time have a research-doctorate program in statistics/ biostatistics or failed to provide the information requested by the committee. Before examining the individual program results presented in Table 8.1, the reader is urged to refer to Chapter II, in which each of the 16 measures used in the assessment is discussed. Summary statistics describing every measure are given in Table 8.2. For nine of the measures, data are reported for at least 61 of the 64 statistics/biostatistics programs. For measures 04-07, which pertain to characteristics of the program graduates, data are presented for only 36 of the programs; the other 28 had too few graduates on which to base statistics. 4 For measure 12, a composite index of the size of a university library, data are available for 50 programs; for measure 13, the fraction of faculty with research support from the National Science Foundation, the National Institutes of Health, or the Alcohol, Drug Abuse, and Mental Health Administration, data are reported for 37 programs that had at least 10 faculty members. As mentioned in Chapter II, data are not available on total university expenditures for research in the area of statistics and biostatistics-measure 14. Intercorrelations among the 15 measures (Pearson product-moment coefficients) are given in Table 8.3. Of particular note are the high positive correlations of the measures of the numbers of faculty and doctoral graduates (01, 02) with measures of publication records (15, 16) and reputational survey ratings (08, 09, and 11). Figure 8.1 illustrates the relation between the mean rating of the scholarly quality of faculty (measure 08) and the number of faculty members (measure 01) for each of the 63 statistics/biostatistics programs. Figure 8.2 plots the mean rating of program effectiveness (measure 09) against the total number of FY1976-80 program graduates (measure 02). Although in both figures there is a significant positive correlation between program size and reputational rating, it is quite apparent that some of the smaller programs received high mean ratings and some of the larger programs received low mean ratings. Table 8.4 describes the 135 faculty members who participated in the evaluation of statistics/biostatistics programs. These individuals constituted 71 percent of those asked to respond to the survey in this discipline and 17 percent of the faculty population in the 64 research-doctorate programs being evaluated. 5 More than one-third of the survey participants were mathematical statisticians and 16 percent were biostatisticians. More than half of them held the rank of full professor; almost two-thirds had earned their highest degree prior to 1970. Two exceptions should be noted with regard to the survey evaluations in this discipline. The biostatistics program in the School of Public Health at the University of Michigan (Ann Arbor) was omitted on the survey form because at the time of the survey mailing no information on this program had been provided by the institution. At the request of the study coordinator at this university, the program has been included in all other aspects of the assessment. Shortly after the survey mailing it was called to the committee's attention that the name of a faculty member in the statistics program at the University of Rochester was not included on the survey form. The department chairman at this university contacted other department chairmen in the discipline informing them of this omission. To assist the reader in interpreting results of the survey evaluations, estimated standard errors have been computed for mean ratings of the scholarly quality of faculty in 63 statistics/ biostatistics programs (and are given in Table 8.1). For each program the mean rating and an associated \"confidence interval\" of 1.5 standard errors are illustrated in Figure 8.3 (listed in order of highest to lowest mean rating). In comparing two programs, if their confidence intervals do not overlap, one may conclude that there is a significant difference in their mean ratings at a .05 level of significance. 6 From this figure it is also apparent that one should have somewhat more confidence in the accuracy of the mean ratings of higher-rated programs than lower-rated programs. This generalization results primarily from the fact that evaluators are not as likely to be familiar with the less prestigious programs, and consequently the mean ratings of these programs are usually based on fewer survey responses. 55 009. Colorado          "}, {"section_title": "IX", "text": ""}, {"section_title": "Summary and Discussion", "text": "In the six preceding chapters results are presented of the assessment of 596 research-doctorate programs in chemistry, computer sciences, geosciences, mathematics, physics, and statistics/biostatistics. Included in each chapter are summary data describing the means and intercorrelations of the program measures in a particular discipline. In this chapter a comparison is made of the summary data reported for the six disciplines. Also presented are an analysis of the reliability (consistency) of the reputational survey ratings and an examination of some factors that might possibly have influenced the survey results. This chapter concludes with suggestions for improving studies of this kind-with particular attention given to the types of measures one would like to have available for an assessment of research-doctorate programs. This chapter necessarily involves a detailed discussion of various statistics (means, standard deviations, correlation coefficients) describing the measures. Throughout, the reader should bear in mind that all these statistics and measures are necessarily imperfect attempts to describe the real quality of research-doctorate programs. Quality and some differences in quality are real, but these differences cannot be subsumed completely under any one quantitative measure. For example, no single numerical ranking-by measure 08 or by any weighted average of measures-can rank the quality of different programs with precision. However, the evidence for reliability indicates considerable stability in the assessment of quality. For instance, a program that comes out in the first decile of a ranking is quite unlikely to \"really\" belong in the third decile, or vice versa. If numerical ranks of programs were replaced by groupings (distinguished, strong, etc.), these groupings again would not fully capture actual differences in quality since there would likely be substantial ambiguity about the borderline between adjacent groups. Furthermore, any attempt at linear ordering (best, next best,\u2026) also may be inaccurate. Programs of roughly comparable quality may be better in different ways, so that there simply is no one best program-as will also be indicated in some of the numerical analyses. However, these difficulties of formulating ranks should not hide the underlying reality of differences in quality or the importance of high quality for effective doctoral education. Table 9.1 are the numbers of programs evaluated (bottom line) and the mean values for each measure in the six mathematical and physical science disciplines. 1 As can be seen, the mean values reported for individual measures vary considerably among disciplines. The pattern of means on each measure is summarized below, but the reader interested in a detailed comparison of the distribution of a measure should refer to the second table in each of the preceding six chapters. 2 Program Size (Measures 01-03). Based on the information provided to the committee by the study coordinator at each university, mathematics programs had, on the average, the largest number of faculty members (33 in December 1980), followed by physics (28) and chemistry (23). Chemistry programs graduated the most students (51 Ph.D. recipients in the FY1975-79 period) and had the largest enrollment (75 doctoral students in December 1980). In contrast, statistics and biostatistics programs were reported to have an average of only 12 faculty members, 15 graduates, and 22 doctoral students."}, {"section_title": "SUMMARY OF THE RESULTS", "text": ""}, {"section_title": "Displayed in", "text": "Program Graduates (Measures 04-07). The mean fraction of FY1975-79 doctoral recipients who as graduate students had received some national fellowship or training grant support (measure 04) ranges from .17 for graduates of computer science programs to .32 for graduates in statistics/biostatistics. (The relatively high figure for the latter group may be explained by the availability of National Institutes of Health (NIH) training grant support for students in biostatistics.) With respect to the median number of years from first enrollment in a graduate program to receipt of the doctorate (measure 05), chemistry graduates typically earned their degrees more than half a year sooner than graduates in any of the other disciplines. Graduates in physics and geosciences report the longest median times to the Ph.D. In terms of employment status at graduation (measure 06), an average of 80 percent of the Ph.D. recipients from computer science programs reported that they had made firm job commitments by the time they had completed the requirements for their degrees, contrasted with 61 percent of the program graduates in mathematics. A mean of 43 percent of the statistics/biostatistics graduates reported that they had made firm commitments to take positions in Ph.D.-granting institutions (measure 07), while only 22 percent of those in the geosciences had made such plans. This difference may be due, to a great extent, to the availability of employment opportunities for geoscientists outside the academic sector. Survey . Differences in the mean ratings derived from the reputational survey are small. In all six disciplines the mean rating of scholarly quality of program faculty (measure 08) is slightly below 3.0 (\"good\"), and programs were judged to be, on the average, a bit below \"moderately\" effective (2.0) in educating research scholars/scientists (measure 09). In the opinions of the survey respondents, there has been \"little or no change\" (approximately 1.0 on measure 10) in the last five years in the overall average quality of programs. The mean rating of an evaluator's familiarity with the work of program faculty (measure 11) is close to 1.0 (\"some familiarity\") in every discipline-about which more will be said later in this chapter. University Library (Measure 12). Measure 12, based on a composite index of the size 3 of the library at the university in which a program resides, is calculated on a scale from `2.0 to 3.0, with means ranging from .1 in chemistry, mathematics, and physics to .4 in computer sciences and geosciences, and .5 in statistics/biostatistics. These differences may be explained, in large part, by the number of programs evaluated in each discipline. In the disciplines with the fewest doctoral programs (statistics/biostatistics, computer sciences, and geosciences), programs included are typically found in the larger institutions, which are likely to have high scores on the library size index. Ph.D. programs in chemistry, physics, and mathematics are found in a much broader spectrum of universities that includes the smaller institutions as well as the larger ones. Research Support . Measure (13), the proportion of program faculty who had received NSF, NIH, or ADAMHA 4 research grant awards during the FY1978-80 period, has mean values ranging from as high as .48 and .47 in chemistry and geosciences, respectively, to .25 in statistics/biostatistics. It should be emphasized that this measure does not take into account research support that faculty members have received from sources other than these three federal agencies. In terms of total university expenditures for R&D in a particular discipline (measure 14), the mean values are reported to range from $616,000 in mathematics to $3,996,000 in the geosciences. (R&D expenditure data are not available for statistics/biostatistics.) The large differences in reported expenditures are likely to be related to three factors: the differential availability of research support in the six disciplines, the differential average cost of doing research, and the differing numbers of individuals involved in a research effort. Publication Records (Measures 15 and 16). Considerable diversity is found in the mean number of articles associated with a research-doctorate program (measure 15). An average of 106 articles published in the 1978-79 period is reported for programs in physics and 75 articles for programs in chemistry; in each of the other four disciplines the mean number of articles is fewer than 40. These large differences reflect both the program size in a particular discipline (i.e., the total number of faculty and other staff members involved in research) and the frequency with which scientists in that discipline publish; it may also depend on the length of a typical paper in a discipline. Mean scores are not reported on measure 16, the estimated \"overall influence\" of the articles attributed to a program. Since this measure is calculated from an average of journal influence weights, 5 normalized for the journals covered in a particular discipline, mean differences among disciplines are uninterpretable. Correlations with Measure 02. Relations among the program measures are of intrinsic interest and are relevant to the issue of validity of the measures as indices of the quality of a research-doctorate program. Measures that are logically related to program quality are expected to be related to each other. To the extent that they are, a stronger case might be made for the validity of each as a quality measure. A reasonable index of the relationship between any two measures is the Pearson product-moment correlation coefficient. A table of correlation coefficients between all possible pairs of measures has been presented in each of the six preceding chapters. In this chapter selected correlations to determine the extent to which coefficients are comparable in the six disciplines are presented. Special attention is given to the correlations involving the number of FY1975-79 program graduates (measure 02), the survey rating of the scholarly quality of program faculty (measure 08), university R&D expenditures in a particular discipline (measure 14), and the influence-weighted number of publications (measure 16). Table 9.2 presents the correlations of measure 02 with each of the other measures used in the assessment. As might be expected, correlations of this measure with the other two measures of program size-number of faculty (01) and doctoral student enrollment (03)-are quite high in all six disciplines. Of greater interest are the strong positive correlations between measure 02 and measures derived from either reputational survey ratings or publication records. The coefficients describing the relationship of measure 02 with measures 15 and 16 are greater than .70 in all disciplines except statistics/ biostatistics. This result is not surprising, of course, since both of the publication measures reflect total productivity and have not been adjusted for program size. The correlations of measure 02 with measures 08, 09, and 11 are almost as strong. It is quite apparent that the programs that received high survey ratings and with which evaluators were more likely to be familiar were also ones that had larger numbers of graduates. Although the committee gave serious consideration to presenting an alternative set of survey measures that were adjusted for program size, a satisfactory algorithm for making such an adjustment was not found. In attempting such an adjustment on the basis of the regression of survey ratings on measures of program size, it was found that some exceptionally large programs appeared to be unfairly penalized and that some very small programs received unjustifiably high adjusted scores. Measure 02 also has positive correlations in most disciplines with measure 12, an index of university library size, and with measures 13 and 14, which pertain to the level of support for research in a program. Of particular note are the moderately large coefficients-in disciplines other than statistics/biostatistics and physics-for measure 13, the fraction of faculty members receiving federal research grants. Unlike measure 14, this measure has been adjusted for the number of program faculty. The correlations of measure 02 with measures 05, 06, and 07 are smaller but still positive in most of the disciplines. From this analysis it is apparent that the number of program graduates tends to be positively correlated with all other variables except measure 04-the fraction of students with national fellowship support. It is also apparent that the relationship of measure 02 with the other variables tends to be weakest for programs in statistics/biostatistics. Correlations with Measure 08. Table 9.3 shows the correlation coefficients for measure 08, the mean rating of the scholarly quality of program faculty, with each of the other variables. The correlations of measure 08 with measures of program size (01, 02, and 03) are .40 or greater for all six disciplines. Not surprisingly, the larger the program, the more likely its faculty is to be rated high in quality. However, it is interesting to note that in all disciplines except statistics/biostatistics the correlation with the number of program graduates (measure 02) is larger than that with the number of faculty or the number of enrolled students. Correlations of measure 08 with measure 04, the fraction of students with national fellowship awards, are positive but close to zero in all disciplines except computer sciences and mathematics. For programs in the biological and social sciences, the corresponding coefficients (not reported in this volume) are found to be greater, typically in the range of .40 to .70. Perhaps in the mathematical and physical sciences, the departments with highly regarded faculty are more likely to provide support to doctoral students as teaching assistants or research assistants on faculty research grants-thereby reducing dependency on national fellowships. (The low correlation of rated faculty quality with the fraction of students with national fellowships is not, of course, inconsistent with the thesis that programs with large numbers of students are programs with large numbers of fellowship holders.) Correlations of rated faculty quality with measure 05, shortness of time from matriculation in graduate school to award of the doctorate, are notably high for programs in mathematics, geosciences, and chemistry and still sizeable for physics and statistics/biostatistics programs. Thus, those programs producing graduates in shorter periods of time tended to receive higher survey ratings. This finding is surprising in view of the smaller correlations in these disciplines between measures of program size and shortness of time-to-Ph.D. It seems there is a tendency for programs that produce doctoral graduates in a shorter time to have more highly rated faculty, and this tendency is relatively independent of the number of faculty members. Correlations of ratings of faculty quality with measure 06, the fraction of program graduates with definite employment plans, are moderately high in physics and somewhat lower, but still positive, in the other disciplines. In every discipline except computer sciences the correlation of measure 08 is higher with measure 07, the fraction of graduates having agreed to employment at a Ph.D.-granting institution. These coefficients are greater than .50 in mathematics, geosciences, and physics. The correlations of measure 08 with measure 09, the rated effectiveness of doctoral education, are uniformly very high, at or above .95 in every discipline. This finding is consistent with results from the Cartter and Roose-Andersen studies. 6 The coefficients describing the relationship between measure 08 and measure 11, familiarity with the work of program faculty, are also very high, ranging from .87 to .97. In general, evaluators were more likely to have high regard for the quality of faculty in those programs with which they were most familiar. That the correlation coefficients are as large as observed may simply reflect the fact that \"known\" programs tend to be those that have earned strong reputations. Correlations of ratings of faculty quality with measure 10, the ratings of perceived improvement in program quality, are near zero for mathematics and physics programs and range from .29 to .35 in other disciplines. One might have expected that a program judged to have improved in quality would have been somewhat more likely to receive high ratings on measure 08 than would a program judged to have declined-thereby imposing a small positive correlation between these two variables. Moderately high correlations are observed in most disciplines between measure 08 and university library size (measure 12), support for research (measures 13 and 14), and publication records (measures 15 and 16). With few exceptions these coefficients are .50 or greater in all disciplines. Of particular note are the strong correlations with the two publication measures-ranging from .70 to .86. In all disciplines except statistics/biostatistics the correlations with measure 16 are higher than those with measure 15; the \"weighted influence\" of journals in which articles are published yields an index that tends to relate more closely to faculty reputation than does an unadjusted count of the number of articles published. Although the observed differences between the coefficients for measures 15 and 16 are not large, this result is consistent with earlier findings of Anderson et al. 7 Correlations with Measure 14. Correlations of measure 14, reported dollars of support for R&D, with other measures are shown in Table 9.4. (Data on research expenditures in statistics/biostatistics are not available.) The pattern of relations is quite similar for programs in chemistry, computer sciences, and physics: moderately high correlations with measures of program size and somewhat higher correlations with both reputational survey results (except measure 10) and publication measures. For programs in mathematics many of these relations are positive but not as strong. For geoscience programs, measure 14 is related more closely to faculty size (measure 01) than to any other measure, and the correlations with rated quality of faculty and program effectiveness are lower than in any other discipline. In interpreting these relationships one must keep in mind the fact that the research expenditure data have not been adjusted for the number of faculty and other staff members involved in research in a program. Correlations with Measure 16. Measure 16 is the number of published articles attributed to a program and adjusted for the \"average influence\" of the journals in which the articles appear. The correlations of this measure with all others appear in Table 9.5. Of particular interest are the high correlations with all three measures of program size and with the reputational survey results (excluding measure 10). Most of those coefficients exceed .70, although for programs in statistics/biostatistics they are below this level. Moderately high correlations are also observed between measure 16 and measures 12, 13, and 14. With the exception of computer science programs, the correlations between the adjusted publication measure and measure 05, time-to-Ph.D., range from .31 to .41. It should be pointed out that the exceptionally large coefficients reported for measure 15 result from the fact that the two publication measures are empirically as well as logically interdependent. Despite the appreciable correlations between reputational ratings of quality and program size measures, the functional relations between the two probably are complex. If there is a minimum size for a high quality program, this size is likely to vary from discipline to discipline. Increases in size beyond the minimum may represent more high quality faculty, or a greater proportion of inactive faculty, or a faculty with heavy teaching responsibilities. In attempting to select among these alternative interpretations, a single correlation coefficient provides insufficient guidance. Nonetheless, certain similarities may be seen in the pattern of correlations among the measures. High correlations consistently appear among measures 08, 09, and 11 from the reputational survey, and these measures also are prominently related to program size (measures 01, 02, and 03), to publication productivity (measures 15 and 16), to R&D expenditures (measure 14), and to library size (measure 12). These results show that for all disciplines the reputational rating measures (08, 09, and 11) tend to be associated with program size and with other correlates of size-publication volume, R&D expenditures, and library size. Furthermore, for most disciplines the reputational measures 08, 09, and 11 tend to be positively related to shortness of time-to-Ph.D. (measure 05), to employment prospects of program graduates (measures 06 and 07), and to fraction of faculty holding research grants (measure 13). These latter measures are not consistently correlated highly with the size measures or with any other measures besides reputational ratings."}, {"section_title": "ANALYSIS OF THE SURVEY RESPONSE", "text": "Measures 08-11, derived from the reputational survey, may be of particular interest to many readers since measures of this type have been the most widely used (and frequently criticized) indices of quality of graduate programs. In designing the survey instrument for this assessment the committee made several changes in the form that had been used in the Roose-Andersen study. The modifications served two purposes: to provide the evaluators with a clearer understanding of the programs that they were asked to judge and to provide the committee with supplemental information for the analysis of the survey response. One change was to restrict to 50 the number of programs that any individual was asked to evaluate. Probably the most important change was the inclusion of lists of names and ranks of individual faculty members involved in the research-doctorate programs to be evaluated on the survey form, together with the number of doctoral degrees awarded in the previous five years. Ninety percent of the evaluators were sent forms with faculty names and numbers of degrees awarded; the remaining ten percent were given forms without this information so that an analysis could be made of the effect of this modification on survey results. Another change was the addition of a question concerning an evaluator's familiarity with each of the programs. In addition to providing an index of program recognition (measure 11), the inclusion of this question permits a comparison of the ratings furnished by individuals who had considerable familiarity NOTE: For survey measures 08, 09, and 10 the \"don't know\" category includes a small number of cases for which the respondents provided no response to the survey item. with a particular program and the ratings by those not as familiar with the program. Each evaluator was also asked to identify his or her own institution of highest degree and current field of specialization. This information enables the committee to compare, for each program, the ratings furnished by alumni of a particular institution with the ratings by other evaluators as well as to examine differences in the ratings supplied by evaluators in certain specialty fields. Before examining factors that may have influenced the survey results, some mention should be made of the distributions of responses to the four survey items and the reliability (consistency) of the ratings. As Table 9.6 shows, the response distribution for each survey item does not vary greatly from discipline to discipline. For example, in judging the scholarly quality of faculty (measure 08), survey respondents in each discipline rated between 6 and 8 percent of the programs as being \"distinguished\" and between 1 and 3 percent as \"not sufficient for doctoral education.\" In evaluating the effectiveness in educating research scholars/scientists, 7 to 9 percent of the programs were rated as being \"extremely effective\" and approximatey 2 to 5 percent as \"not effective.\" Of particular interest in this table are the frequencies with which evaluators failed to provide responses on survey measures 08, 09, and 10. Approximately 30 percent of the total number of evaluations requested for measure 08 were not furnished because survey respondents in the mathematical and physical sciences felt that they were not familiar enough with a particular program to evaluate it. The corresponding percentages of \"don't know\" responses for measures 09 and 10 are considerably larger-47 and 53 percent, respectively-suggesting that survey respondents found it more difficult (or were less willing) to judge program effectiveness and change than to judge the scholarly quality of program faculty. The large fractions of \"don't know\" responses are a matter of some concern. However, given the broad coverage of research-doctorate programs, it is not surprising that faculty members would be unfamiliar with many of the less distinguished programs. As shown in Table 9.7, survey respondents in each discipline were much more likely to furnish evaluations for programs with high reputational standings than they were for programs of lesser distinction. For example, for mathematical and physical science programs that received mean ratings of 4.0 or higher on measure 08, almost 95 percent of the evaluations requested on measure 08 were provided; 85 and 77 percent were provided on measures 09 and 10. In contrast, the corresponding response rates for programs with mean ratings below 2.0 are much lower-52, 35, and 28 percent response on measures 08, 09, and 10, respectively. Of great importance to the interpretation of the survey results is the reliability of the response. How much confidence can one have in the reliability of a mean rating reported for a particular program? In the first table in each of the preceding six chapters, estimated standard errors associated with the mean ratings of every program are presented for all four survey items (measures 08-11). While there is some variation in the magnitude of the standard errors reported in every discipline, they rarely exceed .15 for any of the four measures and typically range from .05 to .10. For programs with higher mean ratings the estimated errors associated with these means are generally smaller-a finding consistent with the fact that survey respondents were more likely to furnish evaluations for programs with high reputational standing. The \"split-half correlations 8 presented in Table 9.8 give an indication of the overall reliability of the survey results in each discipline and for each measure. In the derivation of these correlations, individual ratings of each program were randomly divided into two groups (A and B), and a separate mean rating was computed for each group. The last column in Table 9.8 reports the correlations between the mean program ratings of the two groups and is not corrected for the fact that the mean ratings of each group are based on only half rather than a full set of the responses. 9 As the reader will note, the coefficients reported for measure 08, the scholarly quality of program faculty, are in the range of .96 to .98-indicating a high degree of consistency in evaluators' judgments. The correlations reported for measures 09 and 11, the rated effectiveness of a program and evaluators' familiarity with a program, are somewhat lower but still at a level of .92 or higher in each discipline. Not surprisingly, the reliability coefficients for ratings of change in program quality in the last five years (measure 10) are considerably lower, ranging from .67 to .88 in the six mathematical and physical science disciplines. While these coefficients represent tolerable reliability, it is quite evident that the responses to measure 10 are not as reliable as the responses to the other three items. Further evidence of the reliability of the survey responses is presented in Table 9.9. As mentioned in Chapter VI, 11 mathematics programs, selected at random, were included on a second form sent to 178 survey respondents in this discipline, and 116 individuals (65 percent) furnished responses to the second survey. A comparison of the overall results of the two survey administrations (columns 2 and 4 in Table 9.9) demonstrates the consistency of the ratings provided for each of the 11 programs. The average, absolute observed difference in the two sets of mean ratings is less than 0.1 for each measure. Columns 6 and 8 in this table report the results based on the responses of only those evaluators who had been asked to consider a particular program in both administrations of the survey. (For a given program approximately 40-45 percent of the 116 respondents to the second survey were asked to evaluate that program in the prior survey.) It is not surprising to find comparable small differences in the mean ratings provided by this subgroup of evaluators. Critics of past reputational studies have expressed concern about the credibility of reputational assessments when evaluators provide judgments of programs about which they may know very little. As already mentioned, survey participants in this study were offered the explicit alternative, \"Don't know well enough to evaluate.\" This response option was quite liberally used for measures 08, 09, and 10, as shown in Table 9.6. In addition, evaluators were asked to indicate their degree of familiarity with each program. Respondents reported \"considerable\" familiarity with an average of only one program in every five. While this finding supports the conjecture that many program ratings are based on limited information, the availability of reported familiarity permits us to analyze how ratings vary as a function of familiarity. This issue can be addressed in more than one way. It is evident from the data reported in Table 9.10 that mean ratings of the scholarly quality of program faculty tend to be higher if the evaluator has considerable familiarity with the program. There is nothing surprising or, for that matter, disconcerting about such an association. When a particular program fails to provoke more than vague images in the evaluator's mind, he or she is likely to take this as some indication that the program is not an extremely lustrous one on the national scene. While visibility and quality are scarcely the same, the world of research in higher education is structured to encourage high quality to achieve high visibility, so that any association of the two is far from spurious. From the data presented in Table 9.10 it is evident that if mean ratings were computed on the basis of the responses of only those most familiar with programs, the values reported for individual programs would be increased. A largely independent question is whether a restriction of this kind would substantially change our sense of the relative standings of programs on this measure. Quite naturally, the answer depends to some degree on the nature of the restriction imposed. For example, if we exclude evaluations provided by those who confessed \"little or no\" familiarity with particular programs, then the revised mean ratings would be correlated at a level of at least .99 with the mean ratings computed using all of the data. 10 (This similarity arises, in part, because only a small fraction of evaluations are given on the basis of no more than \"little\" familiarity with the program.) The third column in Table 9.10 presents the correlation in each discipline between the array of mean ratings supplied by respondents claiming \"considerable\" familiarity and the mean ratings of those indicating \"some\" or \"little or no\" familiarity with particular programs. This coefficient is a rather conservative estimate of agreement since there is not a sufficient number of ratings from those with \"considerable\" familiarity to provide highly stable means. Were more such ratings available, one might expect the correlations to be higher. However, even in the form presented, the correlations, which are at least .92 in all six disciplines, are high enough to suggest that the relative standing of programs on measure 08 is not greatly affected by the admixtures of ratings from evaluators who recognize that their knowledge of a given program is limited. As mentioned previously, 90 percent of the survey sample members were supplied the names of faculty members associated with each program to be evaluated, along with the reported number of program NOTE: The item response rate is the percentage of the total ratings requested from survey participants that included a response other than \"don't know.\" graduates (Ph.D. or equivalent degrees) in the previous five years. Since earlier reputational surveys had not provided such information, 10 percent of the sample members, randomly selected, were given forms without faculty names or doctoral data, as a \"control group.\" Although one might expect that those given faculty names would have been more likely than other survey respondents to provide evaluations of the scholarly quality of program faculty, no appreciable differences were found (Table 9.11) between the two groups in their frequency of response to this survey item. (The reader may recall that the provision of faculty names apparently had little effect on survey sample members' willingness to complete and return their questionnaires. 11 ) The mean ratings provided by the group furnished faculty names are lower than the mean ratings supplied by other respondents (Table 9.12). Although the differences are small, they attract attention because they are reasonably consistent from discipline to discipline and because the direction of the differences was not anticipated. After all, those programs more familiar to evaluators tended to receive higher ratings, yet when steps were taken to enhance the evaluator's familiarity, the resulting ratings are somewhat lower. One post hoc interpretation of this finding is that a program may be considered to have distinguished faculty if even only a few of its members are considered by the evaluator to be outstanding in their field. However, when a full list of program faculty is provided, the evaluator may be influenced by the number of individuals whom he or she could not consider to be distinguished. Thus, the presentation of these additional, unfamiliar names may occasionally result in a lower rating of program faculty. However interesting these effects may be, one should not lose sight of the fact that they are small at best and that their existence does not necessarily imply that a program's relative standing on measure 08 would differ much whichever type of survey form were used. Since only about 1 in 10 ratings was supplied without the benefit of faculty names, it is hard to establish any very stable picture of relative mean ratings of individual programs. However, the correlations between the mean ratings supplied by the two groups are reasonably high-ranging from .85 to .94 in the six disciplines (Table 9.12). Were these coefficients adjusted for the fact that the group furnished forms without names constituted only about 10 percent of the survey respondents, they would be substantially larger. From this result it seems reasonable to conclude that differences in the alternative survey forms used are not likely to be responsible for any large-scale reshuffling in the reputational ranking of programs on measure 08. It also suggests that the inclusion of faculty names in the committee's assessment need not prevent comparisons of the results with those obtained from the Roose-Andersen survey. Another factor that might be thought to influence an evaluator's judgment about a particular program is the geographic proximity of that program to the evaluator. There is enough regional traffic in academic life that one might expect proximate programs to be better known than those in distant regions of the country. This hypothesis may apply especially to the smaller and less visible programs and is  confirmed by the survey results. For purposes of analysis, programs were assigned to one of nine geographic regions 12 in the United States, and ratings of programs within an evaluator's own region are categorized in Table 9.13 as \"nearby.\" Ratings of programs in any of the other eight regions were put in the \"outside\" group. Findings reported elsewhere in this chapter confirm that evaluators were more likely to provide ratings if a program was within their own region of the country, 13 and it is reasonable to imagine that the smaller and the less visible programs received a disproportionate share of their ratings either from evaluators within their own region or from others who for one reason or another were particularly familiar with programs in that region. Although the data in Table 9.13 suggest that \"nearby\" programs were given higher ratings than those outside the evaluator's region, the differences in reported means are quite small and probably represent no more than a secondary effect that might be expected because, as we have already seen, evaluators tended to rate higher those programs with which they were more familiar. Furthermore, the high correlations found between the mean ratings of the two groups indicate that the relative standings of programs are not dramatically influenced by the geographic proximity of those evaluating it. Another consideration that troubles some critics is that large programs may be unfairly favored in a faculty survey because they are likely to have more alumni contributing to their ratings who, it would stand to reason, would be generous in the evaluations of their alma The pairs of means reported in each discipline are computed for a subset of programs with a rating from at least one alumnus and are substantially greater than the mean ratings for the full set of programs in each discipline. maters. Information collected in the survey on each evaluator's institution of highest degree enables us to investigate this concern. The findings presented in Table 9.14 support the hypothesis that alumni provided generous ratings-with differences in the mean ratings (for measure 08) of alumni and nonalumni ranging from .24 to .58 in the six disciplines. It is interesting to note that the largest differences are found in statistics/ biostatistics and computer sciences, the disciplines with the fewest programs. Given the appreciable differences between the ratings furnished by program alumni and other evaluators, one might ask how much effect this has had on the overall results of the survey. The answer is \"very little.\" As shown in the table, in chemistry and physics only one program in every four received ratings from any alumnus; in statistics/biostatistics slightly more than half of the programs were evaluated by one or more alumni. 14 Even in the latter discipline, however, the fraction of alumni providing ratings of a program is always quite small and should have had minimal impact on the overall mean rating of any program. To be certain that this was the case, mean ratings of the scholarly quality of faculty were recalculated for every mathematical and physical science program-with the evaluations provided by alumni excluded. The results were compared with the mean scores based on a full set of evaluations. Out of the 592 mathematical and physical science programs evaluated in the survey, only 1 program (in geosciences) had an observed difference as large as 0.2, and for 562 programs (95 percent) their mean ratings remain unchanged (to the nearest tenth of a unit). On the basis of these findings the committee saw no reason to exclude alumni ratings in the calculation of program means. Another concern that some critics have is that a survey evaluation may be affected by the interaction of the research interests of the evaluator and the area(s) of focus of the research-doctorate program to be rated. It is said, for example, that some narrowly focused programs may be strong in a particular area of research but that this strength may not be recognized by a large fraction of evaluators who happen to be unknowledgeable in this area. This is a concern more difficult to address than those discussed in the preceding pages since little or no information is available about the areas of focus of the programs being evaluated (although in certain disciplines the title of a department or academic unit may provide a clue). To obtain a better understanding of the extent to which an evaluator's field of specialty may have influenced the ratings he or she has provided, evaluators in physics and in statistics/biostatistics were separated into groups according to their specialty fields (as reported on the survey questionnaire). In physics, Group A includes those specializing in elementary particles and nuclear structure, and Group B is made up of those in all other areas of physics. In statistics/biostatistics, Group A consists of evaluators who designated biostatistics or biomathematics as their specialty and Group B of those in all other specialty areas of statistics. The mean ratings of the two groups in each discipline are reported in Table 9.15. The program ratings supplied by evaluators in elementary particles and nuclear structure are, on the average, slightly below those provided by other physicists. The mean ratings of the biostatistics group are typically higher than those of other statisticians. Despite these differences there is a high degree of correlation in the mean ratings provided by the two groups in each discipline. Although the differences in the mean ratings of biostatisticians (Group A) and other statisticians (Group B) are comparatively large, a detailed inspection of the individual ratings reveals that biomedical evaluators rated programs appreciably higher regardless of whether a program was located in a department of biostatistics (and related fields) or in a department outside the biomedical area. Although one cannot conclude from these findings that an evaluator's specialty field has no bearing on how he or she rates a program, these findings do suggest that the relative standings of programs in physics and statistics/biostatistics would not be greatly altered if the ratings by either group were discarded."}, {"section_title": "INTERPRETATION OF REPUTATIONAL SURVEY RATINGS", "text": "It is not hard to foresee that results from this survey will receive considerable attention through enthusiastic and uncritical reporting in some quarters and sharp castigation in others. The study committee understands the grounds for both sides of this polarized response but finds that both tend to be excessive. It is important to make clear how we view these ratings as fitting into the larger study of which they are a part. The reputational results are likely to receive a disproportionate degree of attention for several reasons, including the fact that they reflect the opinions of a large group of faculty colleagues and that they form a bridge with earlier studies of graduate programs. But the results will also receive emphasis because they alone, among all of the measures, seem to address quality in an overall or global fashion. While most recognize that \"objective\" program characteristics (i.e., publication productivity, research funding, or library size) have some bearing on program quality, probably no one would contend that a single one of these measures encompasses all that need be known about the quality of research-doctorate programs. Each is obviously no more than an indicator of some aspect of program quality. In contrast, the reputational ratings are global from the start because the respondents are asked to take into account many objective characteristics and to arrive at a general assessment of the quality of the faculty and effectiveness of the program. This generality has self-evident appeal. On the other hand, it is wise to keep in mind that these reputational ratings are measures of perceived program quality rather than of \"quality\" in some ideal or absolute sense. What this means is that, just as for all of the more objective measures, the reputational ratings represent only a partial view of what most of us would consider quality to be; hence, they must be kept in careful perspective. Some critics may argue that such ratings are positively misleading because of a variety of methodological artifacts or because they are supplied by \"judges\" who often know very little about the programs they are rating. The committee has conducted the survey in a way that permits the empirical examination of a number of the alleged artifacts and, although our analysis is by no means exhaustive, the general conclusion is that their effects are slight. Among the criticisms of reputational ratings from prior studies are some that represent a perspective that may be misguided. This perspective assumes that one asks for ratings in order to find out what quality really is and that to the degree that the ratings miss the mark of \"quintessential quality,\" they are unrealr although the quality that they attempt to measure is real. What this perspective misses is the reality of quality and the fact that impressions of quality, if widely shared, have an imposing reality of their own and therefore are worth knowing about in their own right. After all, these perceptions govern a large-scale system of traffic around the nation's graduate institutions-for example, when undergraduate students seek the advice of professors concerning graduate programs that they might attend. It is possible that some professors put in this position disqualify themselves on grounds that they are not well informed about the relative merits of the programs being considered. Most faculty members, however, surely attempt to be helpful on the basis of impressions gleaned from their professional experience, and these assessments are likely to have major impact on student decision-making. In short, the impressions are real and have very real effects not only on students shopping for graduate schools but also on other flows, such as job-seeking young faculty and the distribution of research resources. At the very least, the survey results provide a snapshot of these impressions from discipline to discipline. Although these impressions may be far from ideally informed, they certainly show a strong degree of consensus within each discipline, and it seems safe to assume that they are more than passingly related to what a majority of keen observers might agree program quality is all about."}, {"section_title": "COMPARISON WITH RESULTS OF THE ROOSE-ANDERSEN STUDY", "text": "An analysis of the response to the committee's survey would not be complete without comparing the results with those obtained in the survey by Roose and Andersen 12 years earlier. Although there are obvious similarities in the two surveys, there are also some important differences that should be kept in mind in examining individual program ratings of the scholarly quality of faculty. Already mentioned in this chapter is the inclusion, on the form sent to 90 percent of the sample members in the committee's survey, of the names and academic ranks of faculty and the numbers of doctoral graduates in the previous five years. Other significant changes in the committee's form are the identification of the university department or academic unit in which each program may be found, the restriction of requesting evaluators to make judgments about no more than 50 research-doctorate programs in their discipline, and the presentation of these programs in random sequence on each survey form. The sampling frames used in the two surveys also differ. The sample selected in the earlier study included only individuals who had been nominated by the participating universities, while more than one-fourth of the sample in the committee's survey were chosen at random from full faculty lists. (Except for this difference the samples were quite similar-i.e., in terms of number of evaluators in each discipline and the fraction of senior scholars. 15 ) Several dissimilarities in the coverage of the Roose-Andersen and this committee's reputational assessments should be mentioned. The former included a total of 130 institutions that had awarded at least 100 doctoral degrees in two or more disciplines during the FY1958-67 period. The institutional coverage in the committee's assessment was based on the number of doctorates awarded in each discipline (as described in Chapter I) and covered a total population of 228 universities. Most of the universities represented in the present study but not the earlier one are institutions that offered research-doctorate programs in a limited set of disciplines. In the Roose-Andersen study, programs in five mathematical and physical science disciplines were rated: astronomy, chemistry, geology, mathematics, and physics. In the committee's assessment, two disciplines were added to this list 16 -computer sciences and statistics/biostatistics-and programs in astronomy were not evaluated (for reasons explained in Chapter I). Finally, in the Roose-Andersen study only one set of ratings was compiled from each institution represented in a discipline, whereas in the committee's survey, separate ratings were requested if a university offered more than one research-doctorate program in a given discipline. The consequences of these differences in survey coverage are quite apparent: in the committee's survey, evaluations were requested for a total of 593 research-doctorate programs in the mathematical and physical sciences, compared with 444 programs in the Roose-Andersen study. Figures 9.1-9.4 plot the mean ratings of scholarly quality of faculty in programs included in both surveys; sets of ratings are graphed for 103 programs in chemistry, 57 in geosciences, 86 in mathematics, and 90 in physics. Since in the Roose-Andersen study programs were identified by institution and discipline (but not by department), the matching of results from this survey with those from FIGURE 9.1 Mean rating of scholarly quality of faculty (measure 08) versus mean rating of faculty in the Roose-Andersen study-103 programs in chemistry. FIGURE 9.2 Mean rating of scholarly quality of faculty (measure 08) versus mean rating of faculty in the Roose-Andersen study-57 programs in geosciences. FIGURE 9.3 Mean rating of scholarly quality of faculty (measure 08) versus mean rating of faculty in the Roose-Andersen study-86 programs in mathematics. FIGURE 9.4 Mean rating of scholarly quality of faculty (measure 08) versus mean rating of faculty in the Roose-Andersen study-90 programs in physics. the committee's survey is not precise. For universities represented in the latter survey by more than one program in a particular discipline, the mean rating for the program with the largest number of graduates (measure 02) is the only one plotted here. Although the results of both surveys are reported on identical scales, some caution must be taken in interpreting differences in the mean ratings a program received in the two evaluations. It is impossible to estimate what effect all of the differences described above may have had on the results of the two surveys. Furthermore, one must remember that the reported scores are based on the opinions of different groups of faculty members and were provided at different time periods. In 1969, when the Roose-Andersen survey was conducted, graduate departments in most universities were still expanding and not facing the enrollment and budget reductions that many departments have had to deal with in recent years. Consequently, a comparison of the overall findings from the two surveys reveals nothing about how much the quality of graduate education has improved (or declined) in the past decade. Nor should the reader place much stock in any small differences in the mean ratings that a particular program may have received in the two surveys. On the other hand, it is of particular interest to note the high correlations between the results of the evaluations. For programs in chemistry, mathematics, and physics the correlation coefficients range between .93 and .96; in the geosciences the coefficient is .85. The lower coefficient in geosciences may be explained, in part, by the difference, described in footnote 16, in the field coverage of the two surveys. The extraordinarily high correlations found in chemistry, mathematics, and physics may suggest to some readers that reputational standings of programs in these disciplines have changed very little in the last decade. However, differences are apparent for some institutions. Also, one must keep in mind that the correlations are based on the reputational ratings of only three-fourths of the programs evaluated in this assessment in these disciplines and do not take into account the emergence of many new programs that did not exist or were too small to be rated in the Roose-Andersen study."}, {"section_title": "FUTURE STUDIES", "text": "One of the most important objectives in undertaking this assessment was to test new measures not used extensively in past evaluations of graduate programs. Although the committee believes that it has been successful in this effort, much more needs to be done. First and foremost, studies of this kind should be extended to cover other types of programs and other disciplines not included in this effort. As a consequence of budgeting limitations, the committee had to restrict its study to 32 disciplines, selected on the basis of the number of doctorates awarded in each. Among those omitted were programs in astronomy, which was included in the Roose-Andersen study; a multidimensional assessment of research-doctorate programs in this and many other important disciplines would be of value. Consideration should also be given to embarking on evaluations of programs offering other types of graduate and professional degrees. As a matter of fact, plans for including masters-degree programs in this assessment were originally contemplated, but because of a lack of available information about the resources and graduates of programs at the master's level, it was decided to focus on programs leading to the research doctorate. Perhaps the most debated issue the committee has had to address concerned which measures should be reported in this assessment. In fact, there is still disagreement among some of its members about the relative merits of certain measures, and the committee fully recognizes a need for more reliable and valid indices of the quality of graduate programs. First on a list of needs is more precise and meaningful information about the product of research-doctorate programs-the graduates. For example, what fraction of the program graduates have gone on to be productive investigators-either in the academic setting or in government and industrial laboratories? What fraction have gone on to become outstanding investigators-as measured by receipt of major prizes, membership in academies, and other such distinctions? How do program graduates compare with regard to their publication records? Also desired might be measures of the quality of the students applying for admittance to a graduate program (e.g., Graduate Record Examination scores, undergraduate grade point averages). If reliable data of this sort were made available, they might provide a useful index of the reputational standings of programs, from the perspective of graduate students. A number of alternative measures relevant to the quality of program faculty were considered by the committee but not included in the assessment because of the associated difficulties and costs of compiling the necessary data. For example, what fraction of the program faculty were invited to present papers at national meetings? What fraction had been elected to prestigious organizations/groups in their field? What fraction had received senior fellowships and other awards of distinction? In addition, it would be highly desirable to supplement the data presented on NSF, NIH, and ADAMHA research grant awards (measure 13) with data on awards from other federal agencies (e.g., Department of Defense, Department of Energy, National Aeronautics and Space Administration) as well as from major private foundations. As described in the preceding pages, the committee was able to make several changes in the survey design and procedures, but further improvements could be made. Of highest priority in this regard is the expansion of the survey sample to include evaluators from outside the academic setting (in particular, those in government and industrial laboratories who regularly employ graduates of the programs to be evaluated). To add evaluators from these sectors would require a major effort in identifying the survey population from which a sample could be selected. Although such an effort is likely to involve considerable costs in both time and financial resources, the committee believes that the addition of evaluators from the government and industrial settings would be of value in providing a different perspective to the reputational assessment and that comparisons between the ratings supplied by academic and nonacademic evaluators would be of particular interest. National Research Council. Their inclusion in this report might be explained by bureaucratic inertia, but this inclusion adds nothing to the report."}, {"section_title": "SAUNDERS MAC LANE", "text": "C.K.N.PATEL ERNEST S.KUH been compiled on research-doctorate programs within your institution will be made available to you for a nominal cost. These data should prove to be quite valuable for an assessment of the particular strengths and weaknesses of individual programs at your institution. For the past three months the committee has deliberated over what fields are to be covered in the study and which programs within each field are to be evaluated. The financial resources available limit us to an assessment of approximately 2,600 programs in 31 fields. The fields to be included have been determined on the basis of the total number of doctorates awarded by U.S. universities during the FY1976-78 period and the feasibility of identifying and evaluating comparable programs in a particular field. Within each of the 31 fields, programs which awarded more than a specified number of doctorates during the period have been designated for inclusion in the study. For each of the programs at your institution that are to be evaluated, we ask that you furnish the names and ranks of all faculty members who participate significantly in education toward the research doctorate, along with some basic information (as indicated) about the program itself. A set of instructions and a computer-printed roster (organized by field) are enclosed. In addition, you are given an opportunity to nominate other programs at your institution that are not on the roster, but that you believe have significant distinction and should be included in our evaluation. Any program you nominate must belong in one of the 31 fields covered by the study. The information supplied by your institution will be used for two purposes. First, a sample of the faculty members identified with each program will be selected to evaluate research-doctorate programs in their fields at other universities. The selection will be made in such a way as to ensure that all institutional programs and faculty ranks are adequately represented in each field category. Secondly, a list of names of faculty and some of the program information you supply will be provided to evaluators selected from other institutions. Thus, it is important that you provide accurate and up-to-date information. You may wish to ask department chairmen or other appropriate persons at your institution to assist in providing the information requested. If you do so, we ask that your office coordinate the effort by collecting the information on each program and sending a single package to us in the envelope provided. We hope that you will be able to complete this request by December 15. Should you have any questions regarding our request, please call (collect) Porter Coggeshall, the study director, at (202)389-6552. Thank you for your help in this effort. Sincerely, Lyle V.Jones Co-Chairman"}, {"section_title": "Gardner Lindzey", "text": "Co-Chairman \u2022 In many instances the list of faculty for a program may be identical to an institutional list of graduate faculty. \u2022 Faculty names should be provided in the form in which they are most likely to be recognized by colleagues in the field. We prefer that, within each academic rank, you list faculty alphabetically by last name."}, {"section_title": "Nomination of Faculty to Serve as Program Evaluators (Column 3 of Faculty Roster)", "text": "\u2022 Please check the names of at least two faculty members in each academic rank within each program who would be available and, in your opinion, well-qualified to evaluate research-doctorate programs in their field. \u2022 A sample of evaluators will be selected from the list of faculty you provide for each program. In selecting evaluators preference will be given to those whose names you have checked. If no names are checked, a random sample will be selected from the faculty list."}, {"section_title": "Faculty Who Do Not Hold Ph.D. Degrees From U.S. Universities (Column 4 of Faculty Roster)", "text": "\u2022 In order to help us match the faculty names you provide with records in the Doctorate Records File (maintained by the National Research Council), we ask that you identify those faculty members who do not hold a Ph.D. or equivalent research-doctorate from a university in the United States. \u2022 This information will be used only for the purposes of collating records and will not be released to those who are selected to evaluate your institution's programs. Nor will this information affect in any way the selection of program evaluators from your institution's faculty."}, {"section_title": "Nomination of Additional Programs", "text": "\u2022 We recognize the possibility that we may have omitted one or more research-doctorate programs at your institution that belong to (non-asterisked) fields listed on the first page of the roster and that you believe should be included in this study. \u2022 The last two pages of the accompanying roster are provided for the nomination of an additional program. You are asked to provide the names of faculty and other information about each program you nominate. Should you decide to nominate more than one program, it will be necessary to make additional copies of these two pages of the roster. \u2022 Please restrict your nominations to programs in your institution that you consider to be of uncommon distinction and that have awarded no fewer than two doctorates during the past two years. \u2022 Only programs which fall under one of the 31 field categories listed on the first page of the accompanying roster will be considered for inclusion in the study. At the top of the next page please provide the Information requested on the highest degree you hold and your current field of specialization. You may be assured that all Information you furnish on the survey form is to be used for purposes of statistical description only and that the confidentiality of your responses will be protected. On the pages that follow you are asked to judge 50 programs (presented in random sequence) that offer the research-doctorate. Each program is to be evaluated in terms of: (1) scholarly quality of program faculty; (2) effectiveness of program in educating research scholars/scientists; and (3) change in program quality in the last five years (see below). Although the assessment is limited to these factors. our committee recognizes that other factors are relevant to the quality of doctoral programs, and that graduate programs serve Important purposes in addition to that of educating doctoral candidates. A list of the faculty members signficantly involved in each program, the name of the academic unit in which the program is offered, and the number of doctorates awarded in that program during the last five years have been printed on the survey form (whenever available). Although this Information has been furnished to us by the institution and is believed to be accurate, it has not been verified by our study committee and may have a few omissions, misspellings, or other errors. Before marking your responses on the survey form, you may find it helpful to look over the full set of programs you are being asked to evaluate. In making your judgments about each program, please keep in mind the following instructions: (1) Scholarly Quality of Program Faculty. Check the box next to the term that most closely corresponds to your judgment of the quality of faculty in the research-doctorate program described. Consider only the scholarly competence and achievements of the faculty. It is suggested that no more than five programs be designated \"distinguished.\" (2) Effectiveness of Program in Educating Research Scholars/Scientists. Check the box next to the term that most closely corresponds to your judgment of the doctoral program's effectiveness in educating research scholars/scientists. Consider the accessibility of the faculty, the curricula, the Instructional and research facilities, the quality of graduate students, the performance of the graduates, and other factors that contribute to the effectiveness of the research-doctorate program. (3) Change in Program Quality in Last Five Years. Check the box next to the term that most closely corresponds to your estimate of the change that has taken place in the research-doctorate program in the last five years. Consider both the scholarly quality of the program faculty and the effectiveness of the program in educating research scholars/scientists. Compare the quality of the program today with its quality five years ago-not the change in the program's relative standing among other programs in the field. In assessing each of these factors, mark the category \"Don't know well enough to evaluate\" if you are unfamiliar with that aspect of the program. It is quite possible that for some programs you may be knowledgeable about the scholarly quality of the faculty, but not about the effectiveness of the program or change in program quality. For each of the programs identified, you are also asked to indicate the extent to which you are familiar with the work of members of the program faculty. For example, if you recognize only a very small fraction of the faculty, you should mark the category \"Little or no familiarity.\" Please be certain that you have provided a set of responses for each of the programs identified on the following pages. The fully completed survey form should be returned in the enclosed envelope to:    "}, {"section_title": "VI. OPERATIONAL CONSIDERATIONS", "text": ""}, {"section_title": "A. Basics of Publication and Citation Analysis", "text": "The first section of this chapter discusses the major stages of publication and citation analysis techniques in evaluative bibliometrics. Later sections of the chapter consider publication and citation count parameters in further detail, including discussions of data bases, of field-dependent characteristics of the literature, and of some cautions and hazards in performing citation analyses for individual scientists. The basic stages which must be kept in mind when doing a publication or citation analysis are briefly summarized in Figure 6-1."}, {"section_title": "Type of Publication", "text": "For a publication analysis the fundamental decision is which type of publication to count. A basic count will include all regular scientific articles. However, notes are often counted since some engineering and other journals often contain notes with significant technical content. Reviews may be included. Letters-to-the-editor must also be considered as a possible category for inclusion, since some important journals are sometimes classified as letter journals. For example, publications in Physical Review Letters were classified as letters by the Science Citation Index prior to 1970, although they are now classified as articles. For most counts in the central core of the scientific literature, articles, notes and reviews are used as a measure of scientific output. When dealing with engineering fields, where many papers are presented at meetings accompanied by reprints and published proceedings, meeting presentations must also be considered. In some applied fields, i.e., agriculture, aerospace and nuclear engineering, where government support has been particularly comprehensive, the report literature may also be important. Unfortunately, reports generally contain few references, and citations to them are limited so they are not amenable to the normal citation analyses. Books, of course, are a major type of publication, especially in the social sciences where they are often used instead of a series of journal articles. In bibliometrics a weighting of n articles equal to one book is frequently used; no uniformly acceptable value of n is available. A few of the papers discussed in Chapter V contain such measures. "}, {"section_title": "Time Spans", "text": "A second important decision in making a publication count is to select the time span of interest. In the analysis of the publications of an institution a fixed time span, usually one year or more, is most appropriate. In comparing publication histories of groups of scientists, their professional ages (normally defined as years since attaining the PhD degree) must be comparable so that the build-up of publications at the beginning of a career or the decline at the end will not complicate the results. A typical scientist's first publication appears soon after his dissertation; if he continued working as a scientist, his publications may continue for thirty or more years. The accurate control of the time span of a count is not as trivial as it might seem. Normally, the publication count is made from secondary sources (abstracting or indexing services) rather than from scanning the publications individually. Since most abstracting and indexing sources have been expanding their coverage over time, any publication count covering more than a few years must give careful consideration to changes in coverage. Furthermore, the timeliness of the secondary sources varies widely, with sources dependent on outside abstractors lagging months or even years behind. Since these abstracting lags may depend upon language, field and country of origin, they are a particular problem in international publication counts. The Science Citation Index is one of the most current secondary sources, with some 80% to 90% of a given year's publications in the SCI for that year. Of course, no abstracting or indexing service can be perfect, since some journals are actually published months after their listed publication dates. Nevertheless, variations in timeliness are large from one service to another."}, {"section_title": "Comprehensiveness of Source Coverage", "text": "An important consideration in making a publication count is the comprehensiveness of the source coverage. Most abstracting and indexing sources cover some journals completely, cover other journals selectively, and omit some journals in their field of interest. The Science Citation Index is an exception in that it indexes each and every important entry from any journal it covers. This is one of the major advantages in using the SCI as a data base. Chemical Abstracts and Biological Abstracts have a group of journals which they abstract completely, coupled with a much larger set of journals from which they abstract selectively, based upon the appropriateness of the article to the subject coverage. In some cases the abstractor or indexer may make a quality judgment, based on his estimate of the importance or the quality of the article or upon his knowledge of whether similar information has appeared elsewhere; Excerpta Medica is a comprehensive abstracting service for which articles are included only if they meet the indexers' quality criteria. Some data on the extent of coverage of the major secondary sources is presented in Section D of this chapter."}, {"section_title": "Multiple Authorships and Affiliations", "text": "Attributing credits for multiple authorships and affiliations is a significant problem in publication and citation analysis. In some scientific papers the authors are listed alphabetically; in others the first author is the primary author; still others use different conventions. These conventions have been been discussed by Crane 1 and by other social scientists. 2 There does not seem to be any reasonable way to deal with the attribution problem, except to attribute a fraction of a publication to each of the authors. For example, an article which has three authors would have one-third of an article attributed to each author. The amount of multiple authorship unfortunately differs from country to country and from field to field. Several studies have investigated the problem, but no comprehensive data exists. 3 Multiple authorship takes on particular importance when counting an individual's publications since membership on a large research team may lead to a single scientist being a co-author of ten or more publications per year. This number of publications is far in excess of the normal publication rate of one to two articles per year per scientist. Multiple authorship problems arise less often in institutional publication counts since there are seldom more than one or two institutions involved in one publication. A particularly vexing aspect of multiple authorship is the first author citation problem: almost all citations are to the first author in a multi-authored publication. As a result, a researcher who is second author of five papers may receive no citations under his own name, even though the papers he co-authored may be highly cited. Because of this, a citation count for a person must account for the citations which appear under the names of the first authors of publications for which the author of interest was a secondary author. This can lead to a substantial amount of tedious additional work, since a list of first authors must be generated for all of the subjects' multi-authored papers. Citations to each of these first authors must then be found, the citations of interest noted, and these citations fractionally attributed to the original author. Since multiple years of the Citation Index are often involved, the amount of clerical work searching from volume to volume and from author to author, and citation to citation can be quite large. A note of caution about the handling of multiple authorship in the Corporate Index of the Science Citation Index: SCI lists a publication giving all the corporate affiliations, but always with the first author's name. Thus a publication by Jones and Smith where Jones is at Harvard and Smith is at Yale would be listed in the Corporate Index under Harvard with the name Jones and also under Yale with the name Jones. To find the organization with which the various authors are affiliated, the original article must be obtained. Although the publisher of the Science Citation Index, the Institute for Scientific Information, tries to maintain a consistent policy in attributing institutional affiliations, when authors have multiple affiliations the number of possible variants is large. In the SCI data base on magnetic tape, sufficient information is included to assign a publication with authors from a number of different institutions in a reasonably fair way to those institutions; however, in the printed Corporate Index, one has to refer to the Source Index to find the actual number of authors, or to the paper itself to find the affiliations of each of the authors."}, {"section_title": "Completeness of Available Data", "text": "Another consideration in a publication analysis is the completeness of data available in the secondary source, since looking up hundreds or thousands of publications individually is tedious and expensive. One difficulty here is that most of the abstracting and indexing sources are designed for retrieval and not for analysis. As a result, some of the parameters which are of greatest analytical importance, such as the affiliation of the author and his source of financial support, are often omitted. Furthermore, some of the abstracting sources are cross-indexed in complex ways, so that a publication may only be partially described at any one point, and reference must be made to a companion volume to find even such essential data as the author's name. While intellectually trivial, these searches can be exceedingly time consuming when analyzing large numbers of publications. The specific data which are consistently available in the secondary sources are the basic bibliographic information: i.e., authors' name, journal or report title, volume, page, etc. This information is the basic data used for retrieval, and since the abstracting and indexing services are retrieval oriented, this bibliographic information is always included. Data which are less consistently available in the secondary source are the authors' affiliation and the authors' rank or title. Both of these are of interest in analysis. For example, the ranking of universities based on publication in a given subject area is often of interest. This ranking can be tabulated only from a secondary source which gives the authors' university affiliation."}, {"section_title": "Support Acknowledgements", "text": "The source of the authors' financial support is seldom given in any secondary source, although it is now being added to the MEDLARS data base. Since this financial data can be used to define the fraction of a subject literature which is being supported by a particular corporate body such as a governmental agency, the data are of substantial evaluative interest. The amount of acknowledgement of agency support in the scientific literature has changed over time. In a Computer Horizons study completed in 1973 the amount of agency support acknowledgement was tabulated in twenty major journals from five different fields. 4 Table 6-1 summarizes those support acknowledgements for 1969 and 1972. In 1969, only 67% of the articles in 20 major journals acknowledged financial support. By 1972, the percentage of articles acknowledging financial support had risen to approximately 85%. The table shows that the sources of support differ from one field to another and also shows that the fields of interest to these sources differ as well. For example, the National Science Foundation is the major source of acknowledged support in mathematics, while the National Institutes of Health clearly dominate the support of biology. Chemistry is the field with the largest amount of non-government (private sector) support in the U.S. Note also that the 20 journals used were major journals in their fields; as less prestigious journals are examined, the amount of support acknowledgement generally decreases.  1969 1972 1969 1972 1969 1972 1969 1972 1969 1972 1969 1972 NSF 18% 37% 14% 19% 18% 21% 8% 8% 8% 8% 13% 16% N I H A E C 1 1 2 1 1 5 1 0 8 3 2 3 2 1   1 8D O D 1 5 7 1 9 1 5 1 0 1 0 1 1 2 3 1 0 9 N A S A 1 1 7 9 2 2 1 1 1   2 3 4O t h e r U . S . G o v e r n m e n t 1 2 1   2 2 2 1 1 1   3 1 2O t h e r U . S . 3 1 0 3 1 4 8 2   1 1 0 1 0 9 1 3 7   1 4F o r e i g n 5 4 5 1 5 7 8 1   6 2 5 1 0 2 4 8   1 6Unacknowledged 55 37 31 11 32 18 25 13 42 14 33 15 sources of support responded to the questionnaire. Of the authors who responded, over two-thirds were supported by their institutions as part of their regular duties; approximately 20% of the respondents cited specific governmental agencies as sources of support, even though they had not acknowledged these in the article itself. Twelve percent of the respondents listed no agency or institutional support; research done as fulfillment of graduate studies was included in this category. Overall, the 1972 tabulation and survey showed that 88% of the research reported in these prestigious journals was externally supported, and that 97% of the externally supported work was acknowledged as such."}, {"section_title": "Subject Classification", "text": "Having constructed a basic list of publications, the next step in analysis is normally to subject classify the publications. Either the journals or the papers themselves may be classified. When a large number of papers is to be analyzed, classification of the papers by the field of the journal can be very convenient. Such a classification implies, of course, a degree of homogeneity of publication which is normally adequate when analyzing hundreds of papers. Such a classification may not be sufficient for the analysis of the scientific publications of one or a few individuals. Subject classification schemes differ from one abstracting and indexing service to another. Therefore, a comparison of a collection of papers based on the classification schemes of more than one abstracting and indexing service is almost hopeless. A classification of papers at the journal level has been used in the influence methodology discussed in Chapters VII through X."}, {"section_title": "Citation Counts", "text": "Citation counts are a tool in evaluative bibliometrics second in importance only to the counting and classification of publications. Citation counts may be used directly as a measure of the utilization or influence of a single publication or of all the publications of an individual, a grant, contract, department, university, funding agency or country. Citation counts may be used to link individuals, institutions, and programs, since they show how one publication relates to another publication."}, {"section_title": "APPENDIX F 228", "text": "About this PDF file: This new digital representation of the original work has been recomposed from XML files created from the original paper book, not from the original typesetting files. Page breaks are true to the original; line lengths, word breaks, heading styles, and other typesetting-specific formatting, however, cannot be retained, and some typographic errors may have been accidentally inserted. Please use the print version of this publication as the authoritative version for attribution. In an attempt to account for the 15% of unacknowledged papers, a questionnaire was sent to all U.S. authors in the 1972 sample who did not acknowledge agency support. Almost 70% of the authors who had not listed In addition to these evaluative uses, citations also have important bibliometric uses, since the references from one paper to another define the structure of the scientific literature. Chapter III discusses how this type of analysis may be carried out at a detailed, micro-level to define closely related papers through bibliographic coupling and co-citation. That chapter also describes how citation analysis may be used at a macro-level to link fields and subfields through journal-to-journal mapping. The bibliometric characteristics of the literature also provide a numeric base against which evaluative parameters may be normalized. Some of the characteristics of the literature which are revealed by citation analysis are noted on Figure 6-1. These characteristics include: The dispersion of references: a measure of scientific \"hardness\", since in fields that are structured and have a central core of accepted knowledge, literature references tend to be quite concentrated. The concentration of papers and influence: another measure of centrality in a field, dependent upon whether or not a field has a core journal structure. The hierarchic dependency relationships between field, subfield and journals, including the comparison of numbers of references from field A to field B, compared with number of references from field B to field A: this comparison provides a major justification for the pursuit of basic research as a foundation of knowledge utilized by more applied areas. The linkages between fields, subfields and journals: a measure of the flow of information, and of the importance of one sector of the scientific mosaic to another."}, {"section_title": "B. Development of the Weighting Scheme", "text": ""}, {"section_title": "The Citation Matrix", "text": "A citation matrix may be used to describe the interactions among members of a set of publishing entities. These entities may, for example, be journals, institutions, individuals, fields of research, geographical subdivisions or levels of research methodology. The formalism to be developed is completely general in that it may be applied to any such set. To emphasize this generality, a member of a set will be referred to as a unit rather than as a specific type of unit such as a journal. The citation matrix is the fundamental entity which contains the information describing the flow of influence among units. The matrix has the form A distinction is made between the use of the terms \"reference\" and \"citation\" depending on whether the issuing or receiving unit is being discussed. Thus, a term C ij in the citation matrix indicates both the number of references unit i gives to unit j and the number of citations unit j receives from unit i. The time frame of a citation matrix must be clearly understood in order that a measure derived from it be given its proper interpretation. Suppose that the citation data are based on references issued in 1973. The citations received may be to papers in any year up through 1973. In general, the papers issuing the references will not be the same as those receiving the citations. Thus, any conclusions drawn from such a matrix assume an on-going, relatively constant nature for each of the units. For instance, if the units of study are journals, it is assumed that they have not changed in size relative to each other and represent a constant subject area. Journals in rapidly changing fields and new journals would therefore have to be treated with caution. A citation matrix for a specific time lag may also be formulated. This would link publications in one time period with publications in some specified earlier time period."}]