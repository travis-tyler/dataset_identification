[{"section_title": "List of Figures", "text": "Joint distribution of polychoric correlations and mean differences for second follow-up and earlier student responses to personal and family background items Figure 5.2: Joint distribution of polychoric correlations and mean differences for second follow-up and earlier student responses, to school-related items Figure 6.1: Comparison of polychoric correlations between parent and student and between 2nd follow-up and earlier responses Comparison between second follow-up and base year parent-student polychoric correlations Figure 6.3: Comparison between base year to second follow-up and first to second follow-up polychoric correlations Figure 6.4: Polychoric correlations for high and low 8th grade reading ability students . . . Tables   Table 2.1A: First and second follow-up status of NELS:88 cases with positive second follow-up panel weights 14 Table 2.1B: First and second follow-up status of NELS:88, cases with positive base year weights and zero second follow-up panel weights.  Table 2.4: Unweighted frequencies of subpopulations on which items were compared . . .  Table 3.1A: Comparison of parent and student responses to items about numbers of siblings Table 3.1B: Comparison of parent and student nonresponse rates to items about numbers of siblings Table 3.1C: Response characteristics on items about numbers of siblings, for population subgroups 23 Table 3.1D: Statistically significant associations of sibling counts, based on parent and student responses, with selected NELS:88 outcomes Table 3.2A: Comparison of parent and student responses to items about student jobs 26 Table 3.2B: Comparison of parent and student nonresponse rates to items about student jobs 27 Table 3.2C: Response characteristics on items about student jobs, for population subgroups 28 Table 3.2D: Statistically significant associations of working on a job, based on parent and student responses, with selected NELS:88 outcomes 28 viii Table 3.3A: Comparison of parent and student responses to items about use of non-English languages 30 Table 3.3B: Comparison of parent and student nonresponse rates to items about use of non-English languages 31 Table 3.3C: Response characteristics on items about use of non-English languages, for population subgroups 32 Table 3.3D: Statistically significant associations of English use in the home, based on parent and student responses, with selected NELS:88 outcomes 33 Table 3.4A: Comparison of parent and student responses to items about parents' education 34 Table 3.4B:"}, {"section_title": "vii", "text": ""}, {"section_title": "List of", "text": "Comparison of parent and student nonresponse rates to items about parents' education 34 Table 3.4C: Response characteristics on items about parents' education, for population subgroups 35 Table 3.4D: Statistically significant associations of parents' education, based on parent and student responses, with selected NELS:88 outcomes 35 Table 3.5A: Comparison of parent and student responses to items about people at home after school 36 Table 3.5B: Comparison of parent and student nonresponse rates to items about people at home after school 37 Table 3.5C: Response characteristics on items about people at home after school, for population subgroups Table 3.5D: Statistically significant associations of people at home after school, based on parent and student responses, with selected NELS:88 outcomes Table 3.6A: Comparison of parent and student responses to items about ratings of the school Table 3.6B: Comparison of parent and student nonresponse rates to items about ratings of the school ix Table 3.6C: Table 3.6D: Table 3.7A: Table 3.7B: Table 3.7C: Table 3.7D: Table 3.8A: Table 3.8B: Table 3.8C: Table 3.8D: Table 3.9A: Table 3.9B: Table 3.9C: Table 3.9D: Response characteristics on items about ratings of the school, for population subgroups Statistically significant associations of school factors, based on parent and student ratings, with selected NELS:88 outcomes Comparison of parent and student responses to items about parent-student discussion of issues 43 Comparison of parent and student nonresponse rates to items about parent-student discussion of issues 44 Response characteristics on items about parent-student discussions, for population subgroups 44 Statistically significant associations of parent-student discussions, based on parent and student ratings, with selected NELS:88 outcomes Comparison of parent and student responses to items about substance use decisionmaking Comparison of student and parent and student nonresponse rates to items about substance use decisionmaking Response characteristics on items about substance use decisionmaking, for population subgroups Statistically significant associations of substance use decisionmaking, based on parent and student responses, with selected NELS:88 outcomes 49 Comparison of parent and student responses to items about suspensions and dropping out 51 Comparison of parent and student nonresponse rates to items about suspensions and dropping out 51 Response characteristics on items about suspensions and dropping out, for population subgroups"}, {"section_title": "51", "text": "Statistically significant associations of suspensions and enrollment status, based on parent and student responses, with selected NELS:88 outcomes Table 3.10A: Table 3.10B: Table 3.10C: Table 3.10D: Comparison of parent and student responses to items about educational aspirations and expectations"}, {"section_title": "53", "text": "Comparison of parent and student nonresponse rates to items about educational aspirations and expectations\n"}, {"section_title": "54", "text": "Response characteristics on items about educational aspirations and expectations, for population subgroups 54 Statistically significant associations of educational aspirations and expectations, based on parent and student responses, with selected NELS:88 outcomes 55 Table 3.11A: Comparison of parent and student responses to items about college choice factors 56 Table 3.11B: Comparison of parent and student nonresponse rates to items about college choice factors 57 Table 3.11C: Response characteristics on items about college choice factors, for population subgroups 58 Table 3.11D: Statistically significant associations of college choice factors, based on parent and student responses, with selected NELS:88 outcomes 60 Table 3.12A: Table 3.12B: Table 3.12C: Table 3.12D: Comparison of parent and student responses to items about occupational expectations\nResearch in educational psychology suggests that educational expectations may influence students' educational achievement. Table 3.10D displays the significance of educational expectations in relation to select NELS:88 outcomes. Uniformly, educational expectations at the eighth grade level are positively and significantly related to the outcome measures. This relation holds true regardless of whether one uses educational expectation information from the student or aspiration information from parent surveys. Educational expectations at the twelfth grade level show similar (positive and significant) relations to the outcome measures. Again, either source of information produces these results. Note: \"+\" and \"-\" refer to the direction of relation, P and S indicate that the relation is significant only for data from that source (parent or student), and \"..\" indicates that no significant relation was found. \" --\" indicates that the comparison would not be appropriate because the predictor and outcome measures are the same. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88) Base Year, First Follow-up, and Second Follow-up Parent and student Surveys."}, {"section_title": "61", "text": "Comparison of parent and student nonresponse rates to items about occupational expectations 62 Response characteristics on items about occupational expectations, for population subgroups 62 Statistically significant associations of occupational expectations, based on parent and student responses, with selected NELS:88 outcomes Table 3.13A: Comparison of parent and student responses to NELS:88 items. Table 3.13B: Comparison of parent and student nonresponse rates to NELS:88 items Table 3.14: Polychoric correlations between parent and student responses, for population subgroups xi Table 3.15: Average parent-student response differences, for population subgroups Table 3.16: Percentage of student nonresponse for student-parent item pairs, for population subgroups Table 3.17: Statistically significant associations of parent and student measures with selected NELS:88 outcomes Table 4.1A: Comparison of teacher and student responses to items about students' native language and English language proficiency Table 4.1B: Comparison of teacher and student nonresponse rates to items reflecting students' English proficiency Table 4.1C: Response characteristics on items about students' English proficiency, for population subgroups Table 4.1D: Statistically significant associations of students' English proficiency, based on teacher and student reports, with selected NELS:88 outcomes 81 Table 4.2A: Comparison of teacher and student responses to items about instructional practices in math and science classrooms Table 4.2B: Comparison of teacher and student nonresponse rates to items about classroom practices Table 4.2C: Response characteristics on items about classroom practices, for population subgroups Table 4.2D: Statistically significant associations of classroom practices, based on teacher and student reports, with selected NELS:88 outcomes Table 4.3A: Comparison of teacher and student responses to items about students' high school track 89 Table 4.3B: Comparison of teacher and student nonresponse rates to items about students' high school track 89 Table 4.3C: Response characteristics on items about students' high school track, for population subgroups xii 13  Table 4.8: Table 5.1A: Table 5.1B: Table 5.1C: Table 5.1D: Table 5.2A: Table 5.2B: Table 5.2C: Table 5.2D: Statistically significant associations of students' high school track, based on teacher and student reports, with selected NELS:88 outcomes 90 Comparison of student and teacher responses to NELS:88 items Comparison of teacher and student nonresponse rates to NELS:88 items Polychoric correlations between student and teacher responses, for population subgroups Average teacher-student response differences, for population subgroups 95 Percentage of student nonresponse for student-teacher item pairs, for population subgroups 96 Comparison of significant associations of teacher and student measures with selected NELS:88 outcomes 96 Comparison of second follow-up and base year student responses to items about television viewing 99 Comparison of second follow-up and base year student nonresponse rates to items about television viewing 100 Response characteristics on items about television viewing, for population subgroups"}, {"section_title": "100", "text": "Statistically significant associations of base year and second follow-up student responses about television viewing with selected NELS:88 outcomes . 101 Comparison of second follow-up and base year student responses to items about English language competence 103 Comparison of second follow-up and base year student nonresponse rates to items about English language competence 104 Response characteristics on items about English language competence, for population subgroups Statistically significant associations of base year and second follow-up student responses about English language competence with selected NELS:88 outcomes Table 5.3A: Comparison of first and second follow-up student responses to items about religious attitudes 106 Table 5.3B: Comparison of first and second follow-up student nonresponse rates to items about religious attitudes 106 Table 5.3C: Response characteristics on items about religious attitudes, for population subgroups 107 Table 5.3D: Statistically significant associations of first and second follow-up student responses about religious attitudes with selected NELS:88 outcomes 107 Table 5.4A: Comparison of second follow-up and base year student responses to items about views on parental trust 109 Table 5.4B: Comparison of second follow-up and base year student nonresponse rates to items about views on parental trust 109 Table 5.4C: Response characteristics on items about views on parental trust, for population subgroups 110 Table 5.4D: Statistically significant associations of base year and second follow-up student responses about views on parental trust with selected NELS:88 outcomes 110 Table 5.5A: Comparison of second follow-up and earlier student responses to items about discussions with parents 112 Table 5.5B: Comparison of second follow-up and earlier student nonresponse rates to items about discussions with parents 112 Table 5.5C: Response characteristics on items about discussions with parents, for population subgroups Table 5.5D: Statistically significant associations of earlier and later student responses about discussions with parents with selected NELS:88 outcomes Table 5.6A: Comparison of second follow-up and earlier student responses to items about self-esteem and locus of control Table 5.6B: Comparison of second follow-up and earlier student nonresponse rates to items about self-esteem and locus of control Table 5.6C: Response characteristics on items about self-esteem and locus of control, for population subgroups 118 Table 5.6D: Statistically significant associations of earlier and later student responses about self-esteem and locus of control with selected NELS:88 outcomes 120 Table 5.7A: Comparison of second follow-up and base year student responses to items about educational expectations 122 Table 5.7B: Comparison of second follow-up and base year student nonresponse rates to items about eddcational expectations 122 Table 5.7C: Response characteristics on items about educational expectations, for population subgroups 123 Table 5.7D: Statistically significant associations of base year and second follow-up student responses about educational expectations with selected NELS:88 outcomes 123 Table 5.8A: Comparison of second follow-up and earlier student responses to items about expected occupation at 30 125 Table 5.8B: Comparison of second follow-up and earlier student nonresponse rates to items about expected occupation at 30 125 Table 5.8C: Response characteristics on items about expected occupation at 30, for population subgroups 125 Table 5.8D: Statistically significant associations of base year and second follow-up student responses about expected occupation at 30 with selected NELS:88 outcomes 126 Table 5.9A: Comparison of second follow-up and earlier student responses to items about quality of life Table 5.9B: Comparison of second follow-up and earlier student nonresponse rates to items about quality of life Table 5.9C: Response characteristics on items about quality of life, for population subgroups xv Table 5.9D: Statistically significant associations of earlier and later student responses about quality of life with selected NELS:88 outcomes 130 Table 5.10A: Comparison of first and second follow-up student responses to items about importance of life attainments 132 Table 5.10B: Comparison of first and second follow-up student nonresponse rates to items about importance of life attainments 132 Table 5.10C: Response characteristics on items about importance of life attainments, for population subgroups 134 Table 5.10D: Statistically significant associations of first and second follow-up student responses about importance of life attainments with selected NELS:88 outcomes 135 Table 5.11A: Comparison of first and second follow-up student responses to items about students' high school program 136 Table 5.11B: Comparison of first and second follow-up student nonresponse rates to items about students' high school program Table 5.11C: Response characteristics on items about students' high school program, for population subgroups Table 5.11D: Statistically significant associations of first and second follow-up student responses about students' high school program with selected NELS:88 outcomes Table 5.12A: Comparison of second follow-up and earlier student responses to items about the school climate. Table 5.12B: Comparison of second follow-up and earlier student nonresponse rates to items about the school climate. Table 5.12C: Response characteristics on items about the school climate, for population subgroups Table 5.12D: Statistically significant associations of earlier and later student responses about the school climate with selected NELS:88 outcomes 142 Table 5.13A: Comparison of second follow-up and base year student responses to items about crime at school. Table 5.13B: Comparison of second follow-up and base year student nonresponse rates to items about crime at school 143 Table 5.13C: Response characteristics on items about crime at school, for population subgroups 144 Table 5.13D: Statistically significant associations of base year and second follow-up student responses about crime at school with selected NELS:88 outcomes . . . 144 Table 5.14A: Comparison of second follow-up and base year student responses to items about special educational programs. 146 Table 5.14B: Table 5.14C: Table 5.14D: Comparison of second follow-up and base year student nonresponse rates to items about special educational programs."}, {"section_title": "146", "text": "Response characteristics on items about special educational programs, for population subgroups 147 Statistically significant associations of base year and second follow-up student responses about special educational programs with selected NELS:88 outcomes 147 Table 5.15A: Comparison of second follow-up and base year student responses to items about homework. Table 5.15B: Comparison of second follow-up and base year student nonresponse rates to items about homework. Statistically significant associations of base year and second follow-up Comparison of second follow-up and base year student nonresponse rates to xvii Table 5.16D: Statistically significant associations of base year and second follow-up student responses about cutting class with selected NELS:88 outcomes 153 Table 5.17A: Comparison of second follow-up and base year student responses to items about preparing for class. 154 Table 5.17B: Table 5.17C: Table 5.17D: Table 5.18A: Comparison of second follow-up and base year student nonresponse rates to items about preparing for class."}, {"section_title": "155", "text": "Response characteristics on items about preparing for class, for population subgroups 155 Statistically significant associations of base year and second follow-up student responses about preparing for class with selected NELS:88 outcomes.. Comparison of second follow-up and base year student responses to items about extracurricular activities. Table 5.18B: Comparison of second follow-up and base year student nonresponse rates to items about extracurricular activities. Statistically significant associations of base year and second follow-up student responses about extracurricular activities with selected NELS:88 outcomes Table 5.21: Subpopulation comparisons of polychoric correlations between second follow-up and earlier years for items about personal and family attitudes and behaviors 171 Table 5.22: Subpopulation comparisons of polychoric correlations between second follow-up and earlier responses for items about school events 173 Table 5.23: Subpopulation comparisons of average response differences between second follow-up and earlier years for items about personal and family attitudes and behaviors 174 Table 5.24: Subpopulation comparisons of average second follow-up and earlier student responses differences for school-related items 176 Table 5.25: Subpopulation comparisons of percentage of student nonresponses at second follow-up for items about personal and family attitudes and behavior Table 5.26: Subpopulation comparisons of student nonresponse at second follow-up on items about school events Table 5.27: Statistically significant associations of second follow-up and earlier student responses to items about personal and family attitudes and behavior with selected NELS:88 outcomes Table 5.28: Statistically significant associations of earlier and later student responses about the school events with selected NELS:88 outcomes Introduction The National Education Longitudinal Study of In accordance with its congressional mandate to collect and disseminate statistics and statistical analyses, and in response to the need for policy-relevant longitudinal data on nationally representative samples of elementary and secondary students, the National Center for Education Statistics (NCES) initiated a continuing, long-term program called the National Education Longitudinal Studies (NELS). The overall goal of this program is \"to study the educational, vocational, and personal development of students at various grade levels, and the personal, familial, social, institutional, and cultural factors that may affect that development\" (NCES 1994a). NELS:88 represents the third major study in the NELS program, and follows the National Longitudinal Study of the High School Class of 1972 (NLS-72) and the High School and Beyond Study (HS&B) started in 1980. NELS:88 began with a base year survey of eighth grade students in 1988, followed up at 2year intervals in 1990, 1992, and 1994 (when most sample members would be in college or working).' NELS:88 collected data from more than 20,000 students, as well as their parents, teachers, school principals, and high school transcripts. The study entails a complex sampling design, which includes such features as augmentation (through \"freshening\") to provide a representative sample of students at each phase of the survey through high school (i.e., to represent the tenth grade population in 1990 and the twelfth grade population in 1992); the follow-up and subsequent inclusion of students who were not eligible to participate during the base year (base year ineligible, or BYI students) or who dropped out of school; and a complex set of case weights that support longitudinal analyses and combinations of data from student, parent, and teacher surveys. Because of its broad scope and longitudinal design, the NELS:88 data set allows for comprehensive examination of change in young people's lives and the roles that school and home environments play in promoting growth and positive (or negative) outcomes. It also enables researchers to classify and describe students according to various characteristics, such as sex, race, socioeconomic status (SES), and disability status; and provides an extensive set of user manuals, technical reports, and CD-ROM data files to help researchers access and use the data effectively. In short, NELS:88 provides a unique and rich source of data by which to examine the status and experiences of students as they progress from middle school through the high school years. A fourth follow-up is tentatively scheduled for 1998. 1 One interpretation of discrepancies, or lack of convergence of responses to the same item (about the same student) from two sources, is in terms of reliability. If we assume that responses from two \"judges\" are \"measuring\" the same \"construct,\" then the correlation of responses between judges indicates the extent to which the construct is being reliably measured. It is often called the inter-judge reliability. If the two responses are from the same judge at two different points in time, the correlation indicates the test-retest reliability. Underlying this interpretation is the assumption that the two responses are each measuring the same construct, each with some \"measurement error\" that leads to discrepancies. A second interpretation of convergence is in terms of validity. If we assume that one of the measures is an \"accurate\" (or criterion) measure of a construct and the other is an indicator or a predictor of that construct, then the correlation of responses indicates the validity of the predictor. The concept of validity is employed when carrying out research in which one measure (the predictor) is available but the other (the construct) is not. For example, eighth grade achievement scores might 2 To provide a uniform context for comparing responses to a variety of base year and follow-up items, NELS:88 participants included in this report are limited to cases with positive weights for the base year to second follow-up panel (F2PNLWT > 0).\nAll subgroups were more likely to consider their high school programs to be academic in their senior year than in their sophomore year. However, the subgroup analysis reveals that high SES and high base year reading score students were more likely than others to change their response from \"general\" to \"academic\" than others [t=-6.10, +7.54 for SES, and t=-11.29, +5.20 for reading]. Table 5.11D illustrates that few differences are likely to emerge from analyses of educational outcomes that depend on the time at which the program indicator was gathered. Students' first follow-up and second follow-up reports of academic or general programs are both significantly correlated with all of the outcomes tested. The vocational program indicator, on the other hand, was not correlated with the measure of students' self-concept at either time point, and its correlation with enrollment status was only significant at the earlier follow-up. "}, {"section_title": "22", "text": "be considered predictors of twelfth grade achievement scores, and high school grades and test scores might be considered predictors of success in college. Underlying this interpretation is the assumption that variation in predictor values is a sum of (a) variation on the underlying construct and (b) measurement error. The square of the correlation coefficient indicates the proportion of the variance in the predictor that reflects variation in the underlying construct; however, the assumption that an observed \"criterion\" is identical to (or a perfect measure of) the construct of interest is often relaxed in practice, and validity coefficients are adjusted (upward) for criterion unreliability, or attenuation. The third interpretation of discrepancies is in terms of communality between separate constructs, each measured by a questionnaire response. Two discrepant responses to the same item about the same subject may both be reliable and validbut they are measuring different things, and the phenomenon of their discrepancy can be a topic for substantive psychological or sociological research. In fact, lack of communality between apparently similar measures can provide new insights into processes under study. For example, in NELS:88, indicators of student disability were obtained from students, parents, teachers, and school officials. Each indicator was worded differently, but all were ostensibly related to an underlying construct of student disability. Results of comparisons of these measures showed that there was very little overlap (far less than 50 percent) in the population of students identified as disabled by these separate sources (Rossi and Wolman 1996). Rather than interpret this phenomenon as an indicator of unreliability or lack of validity, Rossi et al. interpreted the results in terms of different item wordings and the different perspectives that students, teachers, and parents have on a student's disability, developing a multidimensional picture of disability of high school students. Other examples abound. To the extent that teenagers and their parents provide different reports on the frequency of non-English language use in the home, both may be accurately viewing the same language use, but from different contexts. To the extent that teenagers and teachers differ on the extent to which they report that students do experiments in class, both may be accurately viewing the same classroom experiences, but from different contexts. To the extent that a student reports different expectations for college graduation between base year and second follow-up surveys, the base year measure may capture more variation related to choices the student makes in ninth and tenth grades (e.g., course selection), while the second follow-up measure captures more variation related to student achievement during high school (e.g., GPA, test scores). To decide among these interpretations requires an independent source of informationeither a separate, accurate measure of the reliability of the items or a logical argument that one of the measures can be treated as identical to the construct (i.e., as a criterion) for a researcher's purposes. This information is generally not available for measures in NELS:88 or any other survey. However, that is not critical for the value of information about convergence: researchers who might use NELS:88 for substantive research on educational policy and practice can take lack of convergence Appendix B. Recodings of Measures for Comparisons specifies the item recodings used for the purpose of comparing different items in this report. Items with similar content but different response options, such as students' and parents' expectations for the student's occupation, required recoding to construct meaningful comparisons. Appendix C. Statistical Measures of Association and Omission Bias provides detailed specifications of the measures of convergence and bias reported in this study.\nIf either nonmatch or nonresponse were located in some particular subpopulation, it could affect research results focusing on that group. The relative frequencies of these differential responses among different types of student-parent combinations are shown in Table 3.1C, which displays three  of the measures shown in Tables 3.1A and 3.1B, broken out by subgroup. The high correlations between parent and student counts of numbers of siblings appear to be fairly uniform, with the smallest polychoric correlation being .85. However, the correlations were somewhat higher for the following subpopulations compared to their counterparts in Table 3.1C: high SES students [t=6.06], students with high reading scores [t=7.20], and private school students [t=4.99]. The only noticeable variation in response among subpopulations was that the mean count of older siblings by the parent respondent was slightly lower than by the student in the low reading [t=3.62] and low SES [t=3.73] subpopulations. Student nonresponse rates to these items were low in all groups, the highest frequency of nonresponse being 2.2 percent, for counts of older siblings by students who did not live with the parent respondent all of the time [t=2.51]. (Each Student's t test is for the comparison with the complementary subsample.) 1.0 .9 1.5 MINI (a) A few high correlations are based on a Pearson correlation approximation. In those few cases in which the Pearson correlation was close to 1.0, the polychoric correlation computation in SAS failed to converge for a subpopulation, although it did converge for the total. In those cases, the approximation (1 -R2 (pm...) aulvopuhalon) x ((1 -R2 (polychoric) total) / (1 -R2 (pc.,n) ,w)) was used for (1 -R2 (poych,,,,c),brop1,in) The Pearson approximation was accurate for cases for which the computation converged. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year Parent and Student Surveys. Number of siblings and number of older siblings have been hypothesized as important factors in various models of educational achievement. Table 3.1D displays the significance of differences between students with either fewer than three siblings, or with no older siblings, and other students on twelfth grade mathematics scores and other outcome measures. (The NELS:88 variables used for outcome measures are specified in Chapter 2.) There are nine potential entries in the table:   P+ parent item shows a significant positive relation Pparent item shows a significant negative relation S+ student item shows a significant positive relation S student item shows a significant negative relation 23 both items show a significant positive relation both items show a significant negative relation P -S+ parent item shows a significant negative relation, student item shows a positive relation P+S parent item shows a significant positive relation, student item shows a negative relationship neither source shows a significant relation The significance of the relation was based, in each case, on a Student's t-test, between \"high\" and \"low\" on the number of siblings (or number of older siblings), using the indicated NELS:88 measure as the dependent variable. In testing a hypothesis, \"significance\" refers to the unlikelihood (p < .05) of the observed differences in means between \"high\" and \"low\" groups if the null hypothesis that the two groups have the same mean were true. For these t-tests, appropriate adjustments for design effects were implemented and differential case weights were taken into account, using SUDAAN; however, each was treated as a single test, with no adjustment for multiple tests, because the entries in the table were to be considered predictive of what researchers might find if they were studying a particular phenomenon, not as the basis for substantive inferences based on their inclusion in this report on survey item response quality. The number of siblings was dichotomized between 2 and 3, and the number of older siblings was dichotomized between 0 and 1. The negative signs in Table 3.1D indicate that the relations between having more than two siblings or at least one older sibling and various outcomes were significantly negative. Note: \"+\" and \"-\" refer to the direction of relation, P and S indicate that the relation is significant only for data from that source (parent or student), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year, First Follow-up, and Second Follow-up Parent and Student Surveys. As can be seen in the table, the effects of having at least three siblings are negative for the same 9 of 10 dependent measures whether counts are from parents or students. The results were more mixed when the measure was the number of older siblings. For three of the measurescurrent enrollment status (i.e., not being a dropout), educational attainment expectations, and prestige rating of the expected occupationthe student-based indicator that there was an older sibling was significant (the three Student's t-values for differences between the groups with one or more versus no older siblings ranged from -2.0 to -3.0), while the parent-based indicator was not (Student's t-values ranged from 24 42 -1.0 to -1.5). Because the sample sizes for parent and student data were nearly the same, it appears that whatever the difference in parent and student responses, the student's perception is more closely related to aspects of educational achievement. This does not indicate that the parent-based indicator has \"error,\" only that its variation includes factors unrelated to the student's progress (e.g., inclusion of children from other marriages with whom the student has not lived). In sum, researchers can be confident about using the responses provided by either parents or students to NELS:88 items related to sibling counts. The measures of convergence and match are high, the levels of nonresponse are low, and relations with outcomes appear to be generally consistent."}, {"section_title": "Chapter 2 Methodology", "text": ""}, {"section_title": "Overview", "text": "In most surveys, there is only one source of information for each measure, and as a result, there are severe limits on the evaluation of the items as measures. NELS:88, in contrast, has collected multiple sources of information on many important constructs. Thus, NELS:88 provides a unique resource for evaluating the performance of survey items. Four main research questions about NELS:88 base year and follow-up survey measures are addressed in this report: 1. To what extent do responses to the same items vary by (a) different respondents or (b) the same respondent over time? 2. How do individuals who omit an item differ from those who respond to it? 3. To what extent does variation in responses to the same item occur in particular subpopulations of students? 4. How much of an impact does variation between sources in responses to the same item have on estimated relations with outcome measures? Each of these questions can be addressed for a wide variety of items included in the NELS:88 surveys. A series of uniform displays of particular measures was selected in order to provide the basis for readers to compare the quality of responses across a variety of different items included in the surveys. The following sections describe the measures used to address these four research questions in chapter 3 of this report (for items in common to parent and student surveys), in chapter 4 (for items in common to teacher and student surveys), and in chapter 5 (for items presented to students in more than one of the surveys). 1. To what extent do responses to the same items vary by (a) different respondents or (b) the same respondent over time? The empirical basis for addressing this question is a cross-tabulation of responses from two sources. The simplest measure of convergence is the percentage of individuals for whom the responses constitute an exact match. That measure has flaws, however, in that aspects of the response distribution that do not conceptually represent convergence can dramatically affect the percent match. For example, an item in which a single response alternative is selected by a large percentage of respondents, such as a question about language usually used in the home, to which most American parents and teenagers would respond \"English,\" or a question about whether the student had ever been suspended from school, to which most parents and teenagers would respond \"no,\" would exhibit a high percentage matching even if the responses from the two sources were unrelated. Also, breaking a variable into finer distinctions changes the measurethe larger the number of response categories, the smaller will be the expected percentage of exact matches. For example, the percentage match on expected education level would be lower if there were seven response alternatives than if the responses were collapsed to three levels, even though level of agreement on the construct was unchanged. Therefore, although the percentage match is a meaningful concept for readers, another measure that is relatively impervious to distortions caused by skewness and fineness of breakdowns is needed. The most commonly used quantitative measure of association between responses to two items is the Pearson product-moment correlation coefficient. If the responses can be coded numerically (e.g., assigned values such as 1, 2, 3, and 4, for never, rarely, frequently, and always) in such a way that the ordering of the numbers matches the ordering of the response category labels, then this coefficient gives an index that is not severely distorted by skewness (if the skewness is similar for both items) or by fineness of breakdowns. Values of a correlation coefficient greater than .80 might be said to indicate a high level of convergence between items; a coefficient between .40 and .80 might be said to indicate a moderate level of convergence; and values less than .40 indicate low levels of convergencethe pair of items are primarily tapping different sources of variation (either \"measurement error\" or different constructs). For most items in NELS:88, response options are ordered in a natural way. In cases where they are not, an ordering can be developed. For example, occupations expected at age 30 can be ordered on level of professional training required or on prestige; and language usually used in the home can be collapsed into two categories: English or other. Therefore, a correlation coefficient is an appropriate measure of convergence for use in this examination of NELS:88 items. The product-moment correlation coefficient is, however, distorted by differential skewness. Two respondents, parents and students for example, might tend to \"use a different part of the scale,\" because they have a different criterion for the construct (e.g., whether teaching in the school is good), but otherwise might agree perfectly (e.g., that teaching at one school is better than at another). That is, their responses represent the same underlying construct, but the response scale (i.e., the dividing line or lines between high and low categories) is shifted for one of the respondents relative to the other. The product-moment correlation coefficient is affected (i.e., reduced) by this shift of thresholds, as well as by differences in agreement about the construct3. To separate the effects of such a shift, or differential skewness, from the effects of lack of convergence on the construct, another measure is needed. One such measure is the polychoric correlation coefficient, which is the correlation associated with the bivariate normal distribution that matches the two observed marginal distributions and best fits the observed bivariate response frequencies (Drasgow 1984). In the special case in which the two measures are both dichotomies, the polychoric correlation is the wellknown tetrachoric correlation. The procedure for estimating the polychoric correlation is described in Appendix C. Two item pairs with the same underlying relation but different skewness can have very different percentage matches but the same polychoric correlation. The polychoric correlation coefficient, it should be noted, does not depend on the assignment of (interval-scale) numbers to the response alternatives, merely on their ordinality. To complement the polychoric correlation coefficient, a measure is needed of the extent to which responses to an item from two sources are shifted, relative to each other. In the present report, the mean score is computed for each measure, and differences are presented for mean scores; that is, differences in the mean (ordinal) position of response alternatives. For example, for a three-alternative item with relative response frequencies of .2, .3, and .5 for parents and .5, .3, .2 for students, the mean score for parents would be .2(1)+.3(2)+.5(3) = 2.3 and for students would be .5(1)+.3(2)+.2(3) = 1.7, for a difference of 2.3-1.7 = 0.6. If a researcher were to consider the measure from one source (e.g., a parent) to be the construct of interest, this difference would be equivalent to the bias incurred by using the measure from the other source (e.g., a student) as the indicator. When a measure is a dichotomy, such as whether English is the teenager's native language, the responses are scored as 1 (yes) and 0 (no), or the reverse. This means, for example, that if 70 percent of the respondents reply \"yes\" and 30 percent respond \"no,\" then the mean response score is .70. Tabular presentations in this report uniformly display mean scores and differences in means; but when appropriate, these are discussed in the text as differences in percentages of responses of particular types. Another measure of convergence that would be appropriate if the measures from two sources were considered only as two separate imperfect measures of the same underlying construct is the root mean squared error of measurement, or measurement error, sometimes referred to as the standard error of measurement.' If two measures of the same construct are observed and measured on the same scale, measurement error can be estimated from the correlation between the two separate measures. If both measures are assumed to share equally the measurement error, the ratio of the standard error of measurement, se, to the overall standard deviation of the distribution of responses, sd, is equal to the square root of 1 minus the correlation coefficient: (se I sd) = \\11 r. For example, if the correlation coefficient is 0.75, then the standard error of measurement is one half of the overall standard deviation ( 0.5 = VI 0.75 ). In this terminology, r is the reliability of the measure(s). On the other hand, if one measure is assumed to be the underlying construct (i.e., the criterion measure), the relation is (se / sd) = \\11 r2. The concept of measurement error carries with if the notion of an equal interval scale, which is not appropriate for many ordinal survey items. Furthermore, the concept is not appropriate for two measures that are considered as measuring different but related constructs. Therefore, this report presents correlations, rather than measurement errors. In summary, the three statistics polychoric correlation, percentage match, and mean differenceprovide the basic summary data to address the first research question. 2. How do individuals who omit an item differ from those who respond to it? Next, this report examines whether there is substantial nonresponse bias in variables on the NELS:88 file. To the extent that individuals fail to respond to items on the survey, there is potential for bias in population estimates based on the sample survey: individuals who fail to respond may differ from respondents on the construct measured by the item. One check on the potential for nonresponse bias is the percentage of omissions. If the percentage of omissions is very low (e.g., less than 5 percent), then nonresponse bias is probably not severe. However, if the percentage of omissions is moderate or higher, estimates based only on respondents may or may not be biased. A simple examination of the percentage of missing data is not sufficient, because the impact of missing data depends on whether nonrespondents would have responded differently from respondents. If nonrespondents have the same distribution of characteristics as respondents, then there is no nonresponse bias. Normally, it is very difficult to estimate how nonrespondents might have responded to a survey item they omit. However, NELS:88 possesses data that are unusually effective for addressing the omission bias issuethose items whose convergence is studied in this report. To the extent that there is convergence, responses from one source can be used to provide insights into the response tendencies of the other source, whether or not the other source responded. For example, if parents of students who leave an item blank about drug use in school indicate that they think their teenager may have a drug problem more frequently than parents of students who respond to the item do, one can infer that omission by students is at least a weak indicator of drug use in school. The assumption still must be made, of course, that nonrespondents are similar to respondents who have matched responses from another source; but that assumption is easier to accept than an assumption that they are similar based on arbitrary other variables, such as race/ethnicity, sex, and socioeconomic status (SES). Missing data on an item can arise either because a selected individual fails to return a questionnaire or because he or she returns a questionnaire leaving that item blank (or otherwise unscorable). To adjust for failure to return a questionnaire, NELS:88 identifies similar cases in the sample and reweights these cases to represent nonrespondents, thus reducing potential nonresponse bias to the extent that the characteristics of matched respondents and nonrespondents are similar. Because the data analyzed in this report include only cases with a positive second follow-up panel weight (F2PNLWT), students' failure to return a follow-up survey is not reflected in percentages of missing data. Specific item omission remains, however, and \"omission bias\" can occur when selective item omission occurs. Moreover, parent and teacher failures to return a questionnaire are included in missing data estimates in this report, because the corresponding student panel weights remain greater than zeroother weights included in the NELS:88 database adjust for parent and teacher nonresponse. In order to base all results in this report on the same set of cases, analyses of parent and teacher item omissions have not been restricted to parents and teachers who returned questionnaires. Even when a survey instrument is available, data may be missing on some items because the respondent left an item out, refused to answer an item, marked multiple responses on a singleresponse item, or selected an \"I don't know\" option. In all of these cases, one can assume that the information that would have been conveyed in the response could have been valid. On the other hand, some items may be missing because the item was inappropriate, as indicated by an explicit or implicit skip pattern in the survey instrument. For this report, all of the cases except for legitimate skips are combined into a single category of \"omission\" because they all have the effect of potentially biasing research that ignores them. In this report, the percentages of missing data are computed for selected variables, and the differences in distributions between nonrespondents and respondents on the same item from a second source are presented. These differences are referred to in the report as \"omission bias.\" For example, in comparing student and parent response, \"parent omission bias\" is estimated as the mean difference between responses of students whose parents omitted the item and responses of students whose parents responded to the item. Conversely, \"student omission bias\" is estimated as the difference between responses of parents whose students omitted the item and responses of parents whose students responded to the item. The sizes of these differences, or estimates of \"omission bias,\" can be compared to the sizes of differences found in other analyses of the quality of item responses, either between different sources or between population subgroups. 3. To what extent does variation in responses to the same item occur in particular subpopulations of students? Many research studies focus on a particular category of students, such as one sex, or one level of SES, or students with particular proficiency levels. It may well be that the convergence of an item differs between population groups. To take an obvious example, students with lower reading scores can be expected to make more errors interpreting the survey items than students who obtain higher reading scores. The number of potential comparisons is virtually unlimited. In order to provide a broad picture of the amount of variation between population subgroups, a sample of critical dimensions 11 U0 that would be likely to show such variation was selected. The major groupings of students and dropouts were dichotomizations based on (a) the student's eighth grade reading score, (b) family SES, and (c) student sex. For comparisons with parent responses, two additional dichotomizations that might be expected to affect family response differences on a survey about education were included: (d) whether the student attended a public or private school at the time of the survey and (e) whether the parent who responded was living in the home with the student all of the time. Similarly, for comparisons with teacher responses, one additional dichotomization was included: (f) the subject of the course in which the teacher taught the student (mathematics or science). Differences between population subgroups might be found on any of the measures used to address research questions 1 and 2, but to reduce the volume of statistics included in this report, variation in the performance of items between population subgroups was measured using three of the measures used to address the first two questions: the polychoric correlation coefficient, mean difference, and percentage of missing student data. For each item analyzed, a three-part table is included, whose columns refer to the particular subpopulations. Each part of the table presents results for one of the three measures."}, {"section_title": "How much of an impact does variation in responses to the same items have on estimated relations with outcome measures?", "text": "The exceptional value of a multifaceted longitudinal survey like NELS:88 lies in the richness of the datathe simultaneous availability of background data, experiential data, and outcome data on the same representative sample of individuals. Policy questions can be sharply addressed by estimating the relations between background factors and experiences and between experiences and outcomes. In this context, it is important for researchers to know the extent to which relations observed in the data are a function of the particular ways in which data were gathered, as opposed to a function of underlying constructs measured by the items on the survey. Although students and parents may respond differently to an item, it may well be that both are responding in ways that measure an underlying factor that affects some outcomes. That is, even though convergence may be low, the same relations to outcomes are observed, no matter which source is used. To put it simplydo the differences matter, for a particular research problem? Or could one of the measures be substituted for the other if the need arose? To address this question, a series of seven to ten \"dependent\" variables were selected, and simple tests of hypotheses about the effects of variation on NELS:88 items on those dependent variables were carried out, first using the NELS:88 item from one source, then using the item from a second source. For example, a student's self-concept, measured in twelfth grade, might be expected to be related to whether \"teachers are interested in students,\" and this can be tested using either the student's or his/her parent's ratings of whether teachers are interested in students. To the extent that results of such a test vary by the source of the rating information, researchers must restrict their interpretations to \"students' [or parents' perceptions that teachers are interested in students\" and not translate measures of perceptions of interest into measures of actual interest. The dependent measures used in these analyses include the student's mathematics test scores in grade 12 (F22XMTH); the academic difficulty of his/her high school program (F2RTRPRG); average grades in core courses (F2RHENG2, F2RMAG2, F2RHSG2, F2RHSOG2); enrollment status (F2RTROUT); locus of control (F2LOCUS1) and self concept measures (F2CNCPT1); expected educational attainment (F2S43); and the socioeconomic index coding of the occupation expected at age 30 (F2S64). Average grade in core courses was used rather than the overall grade point average, because examination of the overall measure reveals that it is an undifferentiated aggregation of grades on a 4-point scale, a 100-point scale, and some other scales. For the comparisons between student and teacher responses, enrollment status was not used; and for comparisons between student and parent responses, mathematics scores in grades 8 and 10 (BY2XMTH and F12XMTH) were also examined. The statistical tests performed represent those that might occur in substantive research. The item whose convergence is of concern is used to create two groups (e.g., high vs. low ratings of the safety of the school), first based on one source (e.g., the student response), then based on the other source (e.g., the parent response). Two Student's t-tests are carried out, one for each of the response sources; and tables are constructed to show when it makes a difference which source is used and when it does not (i.e., when either both or neither of the definitions of the group is significantly related to the dependent variable). The t-tests make use of the appropriate sampling weights, in this case, F2PNLWT, and they take into account the complex sample design of NELS:88, as described in Appendix C."}, {"section_title": "Selection of the Study Sample", "text": "Because this report is aimed at researchers who would use NELS:88 to study the longitudinal progress of eighth graders through their next four years, all results presented in the report pertain to only those students who were included in the NELS:88 base year and first and second follow-ups; that is, the set of students for whom the longitudinal panel weight (F2PNLWT) is greater than zero. These are the only students whose data have any impact on appropriate longitudinal analyses across the high school years. This decision was made in order to limit the complexity of the reportreaders will find many questions to raise about the different results in this report without having to factor in variation in populations from one section to the next. The primary groups omitted from this report by this decision are the base year ineligible students (BYI), the freshened samples, students not followed-up, and students whose status becomes unknown in follow-ups. Because these groups are relatively small, it would be difficult to carry out separate studies of the NELS:88 data quality for them. In addition, including them in some analyses but not others would reduce the comparability of results presented in different sections of the report. However, it should be noted (1) that dropouts are included and (2) that use of the panel weight for analyses purely of base year results is conceptually accurate. Finally, it should be noted that NCES maintains both public-use and restricted access files for NELS:88. The latter files are available only for clearly specified and justified uses by organizations which maintain rigorous data security procedures and agree to severe sanctions for inappropriate release or reporting of the data. Because all students who appear on the restricted file but not on the public-use file have F2PNLWT equal to zero, the results in this report are equally applicable to both files. Student and Dropout Database. There are 16,489 NELS:88 cases (teenagers) on the 1995 NELS:88 CD-ROM for which the panel weight is nonzero. These cases are a subset of the 18,393 cases with positive base year weights that were included in the NELS:88 follow-ups. Another 6,206 cases with positive base year weights were not sampled for the follow-ups, as described in the NELS:88 Second Follow-up Student Component Data User's Manual (NCES 1994a). The 16,489 cases include 14,977 second follow-up student questionnaires and 1,512 second follow-up dropout questionnaires. As shown in Table 2.1A, which is based on the variable F2UNIV1, there were 1,766 dropouts at the time of the second follow-up; however, 254 of these returned student questionnaires. For most analyses, student and dropout data were combined.  The 1,904 cases with positive base year weights but zero panel weights were distributed as shown in Table 2.1B. Most of these cases were either known to be in school or identified as dropout, at both the first and second follow-ups. The extent to which these 1,904 students and the 6,206 students not included in the follow-ups represent the panel of American eighth grade students in 1988 as they passed through the high school years, the \"weight\" they would have contributed to population estimates for panel studies was redistributed to other, similar NELS:88 participants with more complete data. Thus, the total of base year weights and second follow-up panel weights are nearly the same, differing primarily to represent deaths and net outmigrations. The sum of base year weights, which is an estimate of the number of Americans in eighth grade in 1988, is 3,008,080; and the sum of NELS:88 second follow-up panel weights is 2,970,835. Parent Database. For the purpose of comparison of student responses to parent responses, the panel weight does not adjust for parent nonresponse (a different weight does). There were 1,370 students in the panel sample whose parents were not included in the second follow-up survey. Parents of both students and dropouts were included in the analyses. For most analyses, student and dropout data were combined; however, the parent-student comparisons excluded dropouts for four items: ratings of schools, college planning factors, occupational expectations, and an item on education-related family discussions. Teacher Database. The teacher sample included one mathematics or one science teacher for each student who was (a) in the context sample, the sub-sample of high schools from which NCES gathered contextual data; and (b) enrolled in a mathematics or science course at the time of the second follow-up. Therefore, the sample size for analyses involving teacher measures is substantially smaller than for other analyses in this report. The data file contains data only from teachers who were paired with a participating student, but because the data include records for students matched with teacher nonparticipants, teacher unit-level nonresponse can be identified along with item nonresponse. The sample of students who might have corresponding teacher responses included 9,853 records with teacher responses and 1,008 records with no teacher responses. Not all of the students represented in these 10,861 records participated in the second follow-up panel, however-9,832 of them are included. Table 2.2 summarizes the distribution of records. Because only slightly more than half of the students were included in the context sample for which teacher data were collected, there might be concern that the student-teacher response comparisons presented in this report do not represent the entire population of students in the cohort. To test this, values of key statistics were computed using both the panel weights (F2PNLWT) and the context weights (F2CXTWT). Table 2.3 tabulates some key statistics using the panel weights and context weights. For the most part, this table reveals only small differences in the key statistics using the panel weights and the context weights. Therefore, the results presented in this report, which are not weighted to represent students not included in the context sample in the student-teacher comparisons, are meaningful. Sample Sizes of Comparison Subpopulations. The numbers of cases in the subpopulation groups for which item comparisons were made are shown in Table 2.4. The numbers of cases were balanced except for (a) whether the parent respondent was living in the home with the student all of the time and (b) whether the student was enrolled in a public or private school. The sums are less 16 35 than 16,489 due to unavailable data on the variables (other than F2SEX) used to define the subpopulation dichotomies. "}, {"section_title": "Weighting of Cases", "text": "Four sets of analyses are included for each NELS:88 item examined in this report to address the four research questions about convergence of responses (i.e., accuracy, omission bias, subpopulation differences, and relations to outcomes). The real null hypothesis for the first three research questions is that the responses do not vary between sources; that is, for every parent (or teacher), not just parents (or teachers) on average, the student and parent (or teacher) responses measure the same construct. In a sense, this is a null hypothesis of \"respondent independence\"every response depends only on the item and the student it is about, not on the choice of who is making the response. Because this null hypothesis pertains to all cases individually, it is reasonable to study it assigning equal weight to all cases. Furthermore, since the purpose of this report is to provide a description of the performance of items, not of persons, the national estimates that would be provided by weighted statistics are not sought in this study. Therefore, for the most part, the statistics included in this report are unweighted. Specifically, measures of convergence (polychoric correlations, percentage matches, and mean differences), to address research question 1, measures of omission rates and omission bias, to address research question 2, and population subgroup comparisons, to address research question 3, are unweighted. One aspect of the report addresses the effects of nonconvergence on typical research hypothesis testing (see research question 4). For those analyses, panel weights are used and tests of statistical significance are carried out using SUDAAN (Shah, Barnwell, Hunt, and LaVange 1993), which takes the complex sampling design of NELS:88 into account in computing standard errors and significance tests. Lest there be concern that some subpopulations would be substantially over-represented in the unweighted analyses that address the first three research questions because weights were not used to adjust for differential sampling rates, sample means and standard deviations, calculated both unweighted and using F2PNLWT, are shown in Table 2.5. The weighted sample had slightly lower SES, percent female, twelfth grade math scores, and parental education, because larger weights for respondents in these categories were necessary to compensate for the fact that there were more nonrespondents in these categories. Nevertheless, the standard deviations were very similar, and the effect sizes (mean difference in standard deviation units) were no more than about 10 percent. Parent spouse's education (BYP31) 6.6 6.8 3.6 3.7 SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Parent and Student Surveys. Note: The weight used is F2PNLWT, and the sample consists of all cases with F2PNLWT>0."}, {"section_title": "Definition of Omission Rates", "text": "Because the panel weight was constructed to allow researchers to use the data on respondents to represent the entire population of teenagers, including those represented by nonrespondents, omissions by these students are limited to omissions of individual itemsa student who failed to respond at all to the second follow-up would not appear on the file, and his or her weight would be reassigned to individuals who did respond. Because parents of 1,370 teenagers with positive panel weights were not administered a follow-up questionnaire by design, computation of parent omission rates were adjusted to avoid counting their responses as omissions. Other cases with no parent questionnaire are counted as parent omissions. Therefore, since about 6 percent of the 16,489 cases studied here have no base year parent questionnaire due to nonresponse, each item on the parent survey has an omission percentage of at least 6 percent."}, {"section_title": "18", "text": "Teacher omission rates were based only on the 9,832 second follow-up panel students in the sample for whom teacher data were planned to be collected. Of these, 887 teachers failed to return a questionnaire at the second follow-up. As a result, omission rates for teachers were all at least 9 percent.\nWhat was the first language you learned to speak as a child? For this analysis, the two base year items, 17 and 18, were used together to indicate whether the student's native language was English. They were coded \"1\" for English and \"0\" for non-English for this analysis, to facilitate interpretation of means as percentages. The ability (\"how well?\") responses were scored 4, 3, 2, and 1, respectively. The special help items were scored \"2\" for receiving assistance and \"1\" for no assistance, but for this analysis they are rescored as 1 and 0 respectively. The questions on receiving services had different referent periods, which must be considered in evaluating differences in responses to the items. Finally, the ability and special help items were to be skipped, at base year, if English was the only language spoken in the home, and at second follow-up, if the student was a native English speaker. The results presented in Table 5.2A indicate that student reports of whether English was the native language are highly convergent, even though the specific wording of the items changed between the base year and second follow-up surveys. With a polychoric correlation of 0.95 and, as shown in Table 5.2B, a response rate greater than 99 percent, this information should be considered very accurate, whichever source is used. The ratings of English language competency, which were made only by those students who were not native English speakers, were reasonably convergent; but the average ratings by twelfth graders were much closer to the ceiling of 4.0 than the ratings of eighth graders [t=8.50, 9.17, 8.58, 6.12], possibly due to the differences in response labeling. As a result, the eighth grade ratings may provide more sensitive information on English language limitations for use in research than the twelfth grade ratings. Furthermore, a relatively large percentage, about 19 percent, of the second follow-up students who should have responded to this item omitted it. The omission bias figures in Table 5.3B indicate that those who omitted this item on second follow-up tend to have made lower self-ratings of English competence [t=-3.19, -3.82, -3.41, -3.95] as eighth graders. That is, those who had less proficiency in English in the base year were those who omitted the ratings in the second follow-up. Turning to the two special help items, which explicitly referred to nonoverlapping periods in the students' life (prior to grade 8 and after grade 8), the results in Table 5.2A exhibit very little convergence. Thus, researchers should avoid generalizing from data indicating that students had assistance in elementary or middle school to conclude that they were more likely to receive assistance in high school, or vice versa. More than three times as many students reported assistance prior to eighth grade as reported assistance after eighth grade [t = -14.39], and these were generally different students. Convergence of the self-ratings of English competence between base year and second followup was higher for (non-native speaking) students with higher eighth grade reading scores [t=4.04, 5.08, 4.52, 5.51], as shown in Table 5.2C. Generally, patterns of relations to eighth grade reading scores and to SES were similar to each other throughout this study. However, unlike the pattern for other items, the reading level effects and SES level effects on self-ratings of English competence were different from each other-although there were reading level effects, there was no tendency for higher convergence among higher SES students. It may be that SES and reading level are  different factors for the subpopulation of non-native English speakers than for students in general or that the self-ratings of language proficiency are especially sensitive to variations in reading levels. On the other hand, the differences in average levels (possibly indicating improvements in English competence from base year to second follow-up, according to self-ratings) were primarily among students with low eighth grade reading scores [t=5.72, 6.29, 4.81, 3.47];other students were already near the ceiling of the measure in eighth grade. Finally, as for other items, the percentages of omissions of self-estimates of English competence at second follow-up were noticeably higher for low SES [t=2.8, 2.9, 2.8, 2.6] and low eighth grade reading ability groups [t=4.4, 4.4, 4.4, 3.7]. English language competence is important for many educational outcomes. However, as shown in Table 5.2D, the patterns of significance found in NELS:88 depend on the time at which that competence is estimated. First, being a native English speaker was positively related to twelfth grade math scores and high school grade point average when assessed at both time points but significant relations to the difficulty rating of the high school program, locus of control, and dropping out depended on the time at which the response was obtained. Furthermore, self-ratings ofiEnglish competence were only correlated with educational and occupational aspirations and grade point average if measured at the eighth grade. Among the outcomes studied, only math scores, locus of control, and self-concept were clearly related to self-rated English competence at both grades. \nSummary tallies (e.g., 104 of 108) refer to comparisons of computed estimates, not to inferences of statistically different estimates. For information on the statistical significance of differences for individual measures, refer to the corresponding section of this chapter.  A similar pattern of more frequent omissions was observed for low SES teenagers, but it was a much smaller effect; and for one set of items, the extracurricular participation measures, high SES teenagers more frequently failed to respond.      "}, {"section_title": "Chapter 3", "text": ""}, {"section_title": "Comparison of Parent and Student Responses", "text": "A great deal of research has focused on the role of the family in a child's educational progress, and NELS:88 has supplemented this research with an extensive parent questionnaire. Both parents and students were asked for information about family factors that have been hypothesized to affect educational outcomes in order to provide a sound empirical basis for valid and reliable educational research and policy analysis. The items on the parent and student questionnaires were distinct but overlapping. They ranged from objective information, such as what language was spoken most frequently in the household; to perceptions about family interactions and school; to plans, aspirations, and expectations related to college and career. As shown in this chapter, which compares parent and student responses related to these topics, the convergence of item responses by parents and students varies from very high to very low."}, {"section_title": "Number of Siblings", "text": "In the base year survey, both parents and students were asked to indicate the number of siblings of the eighth grade student who was participating in NELS:88. Although the questions had similar meaning, they were not identical: We would like to know how many brothers and sisters your eighth grader has. Please consider all siblings, including half-and step-and adoptive brothers and sisters. Student Question 32A (BY) How many brothers and sisters do you have? Please include any stepbrothers and/or stepsisters if they live or have lived in your home. In both surveys, the respondent indicated None, One, Two, Three, Four, Five, or Six or more. Items 4 (parent) and 33 (student) also asked about the number of older siblings. The results from unweighted comparisons of parent and student responses for cases with positive second follow-up panel weights are shown in Table 3.1A. The polychoric correlations were high for total siblings and older siblings (.89 and .92, respectively); and the raw percentages of matched responses were 84.1 percent and 87.7 percent.5 Over half of the remainder (an additional 9.8 percent and 7.7 percent) differed by one in one direction or the other. Differences of one might 5 See Chapter 2 for an explanation of analysis and usage of polychoric correlations as a measure of convergence between paired items. 21 39 be due to careless responses, including the student in a count of the children in the family, by either the student or the parent. In addition, the item presented to students had an additional printed qualifierliving in the homeso students were not instructed to count step-siblings living in a separate home whereas parents were. Nevertheless, there was virtually no overall bias in source of response: the average number of siblings was 2.23 according to either source, the average number of older siblings was 1.21 according to parents, and 1.27 according to students; a small but significant difference [t=-8.53]. It should be noted that the two mean responses shown in Table 3.1A are unweighted and are based on the same cases (i.e., those for which both sources are nonmissing). As unweighted statistics in Table 3.1B indicate, 99 percent of students responded to these items, and 92 to 93 percent of parents responded to them. Since about 6 percent of parents failed to return a questionnaire, the percentage who omitted these specific items was small, similar to the student omission percentage. With 7 percent parent omissions, the question arises of whether students with missing parent responses had more or fewer siblings than students whose parents answered the item. As indicated in the column labeled \"parent omission bias,\" the average response of students whose parents failed to return a questionnaire or omitted the item on number of siblings was .253 greater than the response of other students. On average, according to data from the students, about one-quarter of the nonresponding parents had one more child than responding parents did, on average. Over the whole sample (unweighted), assuming that very few NELS:88 families had more than six children, this would create a nonresponse bias of .253 times 7 percent, or about .02 siblings per respondent, if the parent data were used. The \"parent omission bias\" for the second item, number of older siblings, is much smaller (.020) and suggests that parents who omitted this item probably had about as many children older than the NELS:88 student as those who responded. Finally, because student omissions were rare, student omission bias is not a critical problem. "}, {"section_title": "Student Jobs", "text": "A second objective question that was asked of both parents and students concerned whether, and when, the student had held a job. The items, which were asked on the second follow-up, were: Parent Questions"}, {"section_title": "(F2)", "text": "Has your teen ever worked for pay? For the parent, the first of these questions was simply \"yes-no,\" with a skip-pattern omitting the next two questions if the parent replied \"no\" to item 71. For the student, a skip pattern based on the response to item 86A omitted item 86B if the student indicated that he/she was currently working. For these analyses, \"yes\" was recoded as 1 and \"no\" as 0. The response alternatives for the time of starting and stopping the job were discrete years and months; the year and month responses are analyzed separately here."}, {"section_title": "43", "text": "For the question of whether the student had held a job, there was substantial, although not high, convergence: a polychoric correlation of .65 and an overall agreement of 88.1 percent as shown in Table 3.2A. Slightly more parents (90 percent) than students (87 percent) indicated that the student had held a job [t=11.44]. There was noticeably less agreement concerning the exact month and year that the student had last worked or had started the most recent job. First, students who were currently working left the \"last worked\" question blank, per instructions, while parents responded with the \"current month and year\" per instructions. Thus, fewer than half as many students were included in analyses of the \"when last worked\" question as in the analyses of the \"when started job\" question. With the exception of a polychoric correlation of .46 for the year started work, the convergence of these items was low (i.e., .31 for the last year worked, .32 for the month started, and .20 for the last month worked). The mean year started was estimated to be 1990.87, according to parents, versus 1990.656, according to students; and the mean year last worked was 1991.53, according to parents, versus 1991.01, according to students. The estimated year last worked, according to parents, included parent responses for students who were currently working at the time the survey was conducted, while the estimate for students did not. The modal month started (not shown in Table 3.2A) was June, according to both parents and students, and the modal month last worked was August, according to both students and those parents whose students indicated that they had stopped working. A substantial percentage of students omitted these items, as shown in Table 3.2B, and although few parents who returned a questionnaire omitted the item on whether the student had ever worked for pay, many omitted the items specifying dates. The omission rates for dates are probably sufficient to introduce noticeable bias, because parents' reports were of somewhat later years (.139 and .113) for students who omitted the items.\n"}, {"section_title": "6", "text": "The difference between 1990.65 and 1990.87 refers to the fact that more parents than students reported a later year (e.g., 1991) and does not indicate that they reported a later time in 1990. The responses to the \"month\" items indicate the average time of year given as a response. Agreement on the student's having ever worked for pay varied somewhat across population subgroups, as shown in Table 3.2C: students with higher reading scores matched parents better [t=14.62]; and girls matched parents better than boys (r=.70 vs. .59) [t=12.23]. In reporting which month of the year the student started his or her most recent job, there was noticeably more agreement when the student lived with the parent respondent all the time (.34 vs. .11) [t=6.79]. Average parent and student responses to that item also depended on whether the student lived with the parent all the time: the direction of the parent-student difference changed sign (-.21 vs. +.40) [t=5.04]. Finally, there were large differences in student omissions on having ever held a jobstudents in low SES MST COPY MAO fit LE 27 Finally, there were several significant positive relations between having worked outside the home and outcome measures. However, as shown in Table 3.2D, for grade point averages, twelfth grade mathematics scores, and locus of control, the significant effects were only found using the student's answer to the question of having worked. The parent's response was not as closely associated with these outcomes. S+ S+ Note: \"+\" and \"-\" refer to the direction of relation, P and S indicate that the relation is significant only for data from that source (parent or student), and \"..\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year, First Follow-up, and Second Follow-up Parent and Student Surveys."}, {"section_title": "44", "text": ""}, {"section_title": "4 6", "text": "Use of Non-English Languages A third objective item that was asked of both parents and students concerned the use of non-English languages in the home. Different aspects of this question were asked at the base year and second follow-up. The items analyzed were: Is any language other than English spoken in your home?"}, {"section_title": "(BY)", "text": "What is the main language people in your home usually speak? (12 specific options, plus \"other\") How often is the language you referred to in question 23' used with <your child/children> ? (Always or most of the time, About half of the time, Sometimes, or Never) Student Questions\nIs any language other than English spoken in your home?\nWhat language do people in your home USUALLY speak? (12 specific options, plus \"other\") 108A,B (F2) How often do you use your native language with <your mother (A), your father (B)> (Always or most of the time, About half of the time, Sometimes, or Never) Because the second follow-up parent question (F2P24B) only asked about language use by the respondent, not his or her spouse, the choice of which second follow-up student question (F2S108A or B) to match to the parent response depended on whether the respondent was father or stepfather, or mother or stepmother. Agreement was assessed separately for those students whose mother or stepmother was the parent respondent and for those whose father or stepfather was the respondent. Like the corresponding parent question (F2P24B), this question was only asked of those students who indicated that English was not their native language. 955 cases satisfied these constraints for the maternal respondent, and 311 for the paternal respondent. There was quite high agreement between parent and student on the base year question of whether a language other than English was spoken in the homea .96 polychoric correlation and a 93.4 percent response match. Twenty-one percent of students and 19 percent of parents indicated Question 23 refers to question 23 on the second follow-up parent survey (F2), not question 23 above, which is from the base year parent survey (BY). Parents' follow-up question 23 asks for the respondent's native language."}, {"section_title": "29", "text": "that another language was used,' and these respondents were asked to identify the language \"usually\" spoken in the home. That 13-alternative question was recoded for these analyses as 1 for English and 0 for any other language; the correlation was .74, and the percent matching on this dichotomy was 73.2 percent. That there was virtually complete agreement between parent and student as to which non-English language was used is indicated by the fact that among all 13 language alternatives, the percent match was 72.5 percentnearly all of the 27 or 28 percent disagreements concerned whether English was the language usually spoken. The variations in wording between the parent and student items (\"main\" and \"usually,\" versus \"USUALLY\") should be considered in interpreting the discrepancy. As indicated by the parent-student mean difference in Table 3.3A, in families in which English was not the only language used, parents were more likely than students to say that English was the main language usually spoken in the home [t =9.64] (as indicated by the comparison between the mean parent response, 0.54, indicating that 54 percent of parents thought that English was the main language usually spoken, and the mean student response, 0.37, indicating that 37 percent of teenagers thought that English was the language usually spoken at home). The second follow-up questions (24B for parent and 108 for student) called for more subtle judgments between four levels of frequency, and the wordings of the questions were not parallel, in that the parent was asked about the language spoken \"with children,\" not specifically \"with the [NELS:88 participant].\" As a result, the level of convergence between parent and student was lower. The polychoric correlation was .52 when the respondent was the mother and .61 when it was the father, and the percentage matches were 58 percent and 51 percent, respectively. Both parents and students who responded to this question indicated usual use of the other language: the mean values of 1.64, 1.78, 1.72, and 1.82 in Table 3.3A are on a 1-to-4 scale, in which 1 indicates use of the other language \"always or most of the time.\" (That is, a higher value indicates greater use of English.) There is a slight tendency for students to indicate use of English at home more than their mothers [t=2.41]. Examination of the unweighted frequency distributions (not shown in Table 3.3A) on which these means are based indicates that about 56 percent of the students and parents in families in which a non-English language is spoken use that language always or most of the time (response = 1, on the 1-to-4 scale). As shown in Table 3.3B, very few students omitted the first of the base year questions. However, 3.9 percent of those who indicated that another language was used in the home omitted the second question. Since most of the parents of students who omitted the item indicated that English was not usually spoken, there was a student omission bias estimate of -.138.9 It should be noted that the percentage nonresponse to this \"usual language\" item among parents is not as dramatically higher than for other items as would appear from Table 3.3B. Among the cases with positive second follow-up panel weights, there were 937 parents who failed to return a questionnaire in the base year. Out of over 16,000 cases, this represents a small percentagebetween 5 and 6 percent. However, because over 12,000 responding parents would appropriately skip this item (because English was the only language spoken in their home), the 937 nonresponding parents constitute a relatively large percentage of the remaining 4,000. In fact, most of the parents who omitted the second question were parents who failed to return a questionnaire. Although it was impossible to determine what percentage of those who failed to return a questionnaire might legitimately have skipped this item, the omission rate for parents who returned a questionnaire was about 6 percent, virtually the same as for the preceding item. As shown in Table 3.3C, parent-student convergence for the base year questions about non-English usage in the home was fairly uniform across subpopulations, although there was less agreement on the second item (which language was usually spoken in the home) in high SES families [t=-5.25] and in families where the student lives with the parent respondent all of the time [t=4.28]. The follow-up items on language usage behaved differently. When a father was the 9This means that 40 percent of the parents of those students who omitted this item indicated that English was usually spoken in the home. As indicated in Table 3.3A, 54 percent of parents who answered this question (and whose children answered the corresponding question) indicated that they usually spoke English in the home; and the student omission bias of -13.8 percent indicates that 13.8 percent fewer parents of the students who omitted the item usually spoke English. Subtracting the 13.8 percent omission bias from the base of 54 percent yields the overall estimate that 40 percent of parents of students who omitted this item indicated that they usually spoke English in the home. 31 respondent, there was greater convergence for high SES [t=2.80] and for private school students [t=2.26], when measured by the polychoric correlation. The largest base year discrepancies in the average differences occurred when the student did not live with the parent respondent all of the time [t=4.28]; however, the results at follow-up were inconsistent-fathers, but not mothers, tended to say they used English in communications 3.8 3.3 3.0 6.5 (a) A few high correlations are based on a Pearson correlation approximation. In those cases in which the Pearson correlation was close to 1.0, the polychoric correlation computation in SAS failed to converge for a subpopulation, although it did converge for the total. In those cases, the approximation (1 -R20,,,) subi,r,wion) x ((1 -R2(poychdo,,) / (1 -R20,..),\")) was used for (1 -R24yetorio subpopulafion) The Pearson approximation was accurate for cases for which the computation converged. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Parent and Student Surveys. with their teenager more than students did [t=2.70]. It appears that different information was being gathered from parents and students on these language usage questions. Finally, as shown in Table 3.3D, the base year question on whether English was the usual language in homes where it was not the only language spoken, and the follow-up question on native language usage in these homes, were differently related to some important educational outcomes, depending on the source of the information.10 First, in homes in which a non-English language was used in the base year, an indication that English was the usual language spoken was positively related to students' educational and occupational expectations, course difficulty level, grade point average, and enrollment statusbut only if based on the students' perceptions. Parents' perceptions of which language was usually used in interactions with the student were not significant predictors of these specific outcomes, although they were for other outcomes shown in Table 3.3D. Second, at follow-up, these parents' perceptions that they generally used English with the student, combined across mothers and fathers because of the small sample sizes, were positively related to most of the outcomes, whereas students' perceptions generally were not. For these measures, research using items on usual use of non-English language in the home from the two different sources would reach different conclusions. P+ P+ P+ P+ P+ Note: \"+\" and \"-\" refer to the direction of relation, P and S indicate that the relation is significant only for data from that source (parent or student), and \"..\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year, First Follow-up, and Second Follow-up Parent and Student Surveys."}, {"section_title": "Parents' Education", "text": "In the base year, NELS:88 asked both students and parents about the educational attainment of both parents. The questions were worded differently, and the number of education levels shown was 13 for the parent respondent and 7 for the student. The questions were: How far in school did your father (A), mother (B) go? The convergence of these items could only be analyzed for parent respondents who were mother or father of the student (BYP1A1=1 or 2) because the student question explicitly asked about the student's mother and father. For cases in which the items could be compared, a recoding of the parent responses onto the categories provided for student responses was necessary. Derived response categories on which this section's tables are based ranged from 1 to 7, where 2 indicates high school graduation and 5 indicates college graduation. The recodings are specified in Appendix B. Although these items are relatively objective, they call for information that might not have been discussed with an eighth grader, and many eighth graders have only a general idea about \"college.\" Nevertheless, the polychoric correlations of .87 for father's education, and .84 for mother's education, indicate a high degree of convergence that is expected for objective items. The percentage match, on the other hand, was only about 56 percent, because of the large number of response categories (7). On average, parents reported more education than students indicated, for both fathers (3.57 vs. 3.44 [1=13.07]) and mothers (3.20 vs. 3.13 [1=7.26]). The largest discrepancy, for both parents, was students' frequent failure to report \"some college\" when their parents reported it (see appendix Table A3.4).  There was a relatively high level of missing data from students on this item, however, as Table 3.4B indicates. Data from students were not available for 15 percent of the cases for father's education and 12 percent of the cases for mother's education; and data from the parents were missing for 20 percent of the cases for father's education and 8 percent of the cases for mother's education. Because this variable is an important factor in predicting educational outcomes, it is unfortunate that there are so many missing data. More than half the omissions of father's education were responses by mothers that their spouse's education \"does not apply.\" Finally, there were noticeable nonresponse biases parents of students who omitted the item indicated a lower level of educational attainment than did parents of responding students [t=-13.08,-11.58], and vice versa [t=-.8.81,-5.52]. Thus, while the level of agreement between students and parents on this item was high, research using these items should take nonresponse and nonresponse bias into account."}, {"section_title": "34", "text": "Comparisons of student-parent responses across subpopulations appear in Table 3.4C. The polychoric correlations between parent and student responses about the educational attainment of the parents are moderately high, mostly ranging from 0.75 to 0.9, the largest differences being between high and low SES respondents [t=16.6, 11.5]. The lower correlations for the SES subpopulations, it should be noted, reflect the restriction of range that is due to the inclusion of parental education in the calculation of the SES composite. Finally, the percentages of student nonresponse on fathers' and mothers' education levels were high in all groups, ranging from 9 percent to 19 percent; however, they were noticeably higher for students in low SES [t=-8.29, -4.04] and low reading groups [t=-9.59, -6.28] than for other students. Prior research has shown the educational attainment of parents to influence both the educational achievement and the occupational outcomes of their children. Table 3.4D shows the significance of the relationship between parents' educational attainment and students' mathematics achievement, occupational expectations, and other select NELS:88 outcomes. The table shows that mother's and father's education, as reported by either student or parent, are positively and significantly related to all of the student outcomes. Thus, even though the responses of students and parents differed somewhat, the positive relationship between parents' educational attainment and students' educational outcomes holds, regardless of the source of information used. "}, {"section_title": "Control cept Status", "text": "Father's education Mother's education + Note: \"+\" and \"-\" refer to the direction of relation, P and S indicate that the relation is significant only for data from that source (parent or student), and \"..\" indicates that no significant relation was found. Are any of the following people home when you return home from school? There were eight separate items for categories of people, and respondents indicated separately the frequency of each type of person being at home. Generally, as shown in Table 3.5A, the parents recalled mother and siblings at home more frequently than did students. On the scale from 1 (\"usually\") to 4 (\"never\"), the most frequent person \"usually\" at home was the mother or female guardian (an average of 1.78 according to the parent, compared to 1.93 according to the student) [t=22.19], while the average frequency for no one at home was 3.05 according to the parent, and 2.71 according to the student. More students than parents indicated that nobody was home when they returned from school [t=34.94]. Correlation between parent and student responses was greatest for what might be considered the traditional responsesmother and any siblings. The response that might be considered most controversialnobody at homehad a much lower correlation. Percentage matches bore little resemblance to the polychoric correlations because of different levels of skewness. Evidence that the \"percent match\" measure is distorted is provided by the \"sitter\" response, which had by far the highest percent match, but one of the lowest polychoric correlations: both parents and students almost always answered \"never\" to this item. \nReligious person Attending services F+ Note: \"+\" and \"-\" refer to the direction of relation, F and S indicate that the relation is significant only for data from that source(first follow-up or second follow-up), and \"..\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); First and Second Follow-up Student Surveys."}, {"section_title": "36", "text": "Since there was no response option for \"does not apply,\" nonresponse rates for some of these items are likely to include many cases in which there was nobody in the appropriate category (e.g., older sibling). In the case of mother and father, there were very few omissions by students (see Table 3.5B), although students who omitted the item had parents who were much more likely to indicate that they were not always at home [t=12.94, 11.06]. The pattern of omissions across these items is the same for both parents and students, suggesting that the omissions are not due to differential perceptions of parents and students, but to the problem with the item mentioned above (no \"does not apply\" response option). The greater omission rate for parents, it should be noted, as in other tables, includes about 7 percent of parents who failed to return a parent questionnaire. As shown in Table 3.5C, there were no systematic patterns of differential item performance across population subgroups. They appear to have similar levels of convergence, mean values, and nonresponse rates throughout the population. Furthermore, for the most part, they seem to have similar associations to educational outcomes, as shown in Table 3.5D. An important exception is the relation between \"nobody at home\" and both dropout status and career expectations. The parent's acknowledgment that nobody is at home when the student comes home from school, but not the student's, is related to negative outcomes in these two areas [t=3.16 and t=2.29 for parent response, and t=1.57 and t=1.02 for student response]. Another exception is the \"sitter\" item, for which the student response (that a sitter was at home), but not the parent response, was positively related to educational outcomes.  "}, {"section_title": "P-", "text": "Note: \"+\" and \"-\" refer to the direction of relation, P and S indicate that the relation is significant only for data from that source (parent or student), and \"..\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88) Base Year, First Follow-up, and Second Follow-up Parent and student Surveys.\nNote: \"+\" and \"-\" refer to the direction of relation, P and S indicate that the relation is significant only for data from that source (parent or student), and \". \" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88) Base Year, First Follow-up, and Second Follow-up Parent and student Surveys."}, {"section_title": "Ratings of the School", "text": "During the base year, parents were asked to indicate the degree to which they agreed with a series of statements about their students' schools, using a 4-point scale with each of a series of statements: (1) strongly agree, (2) agree, (3) disagree, or (4) strongly disagree. Eighth grade students were given a somewhat different series of items. At the second follow-up, these items were repeated and two additional items were included and worded identically for students and parents. The selected items used for analysis were: The school is a safe place. The teaching is good. Teachers are interested in students. I don't feel safe at this school.\" The teaching is good. Teachers are interested in students. Unlike the objective items discussed previously, these items call for evaluative judgments. As might be expected, the polychoric correlations were lower, as shown in Table 3.6A. For the item on school safety, they were .24 and .33 (at base year and follow-up, respectively); and for the items The scoring of the student item on safety was reversed to match the direction of the parent item on safety. 39 on teaching and teachers, they were .37 and .35. The corresponding percentage matches were 48 percent and 45 percent for the safety item and 59 percent and 53 percent for the other two items. For the items on teaching and teachers, the mean responses of parents and students were virtually identical, but for the \"safety\" items, there were substantial mean differences (of .16 at base year and .35 at follow-up, on a 1-to-4 scale), indicating that students believed their schools to be safer than parents did [t= 22.34, 44.97]. The different performance of the \"safety\" item, compared to the items on teaching and teachers, should not be interpreted as an indicator of discrepancies in parent and student views on safety, however. Another plausible explanation is that the relative lack of convergence is due to the wording change and reversal of the scale for studentsstrongly agreeing that one does not feel safe at school is not the same as strongly disagreeing that the school is a safe place. As can be seen from Table 3.6B, these items had low nonresponse rates, other than the 6 percent of parents who failed to return a questionnaire. Both students and parents were willing to provide ratings of their schools and teachers. There was somewhat more agreement between parents and students in the high SES [t=4.23,8.44,12.10,10.41] and high reading [1=8.96, 7.76, 10.90, 7.69] subpopulations, as indicated in Table   3.6C. As might be expected, the parent-student mean discrepancies in the safety measures were much greater among low SES [1=6.81,6.27] and public school students [t=16.01, 13.54] than among 40 58 high SES and private school students. Finally, the low student nonresponse rates on these items appeared to be similar throughout the population. Both parent and student ratings of schools were positively related to educational outcomes, almost without exception. Only the relations (a) between school safety ratings and student selfconcept and (b) between teacher interest in students and occupational expectations depended on the source of the ratings. In summary, the ratings of teachers and schools by parents and students, although not highly convergent, had few missing data and performed in a similar manner with respect to educational outcomes across the range of population subgroups. P+ + + + + + Note: \"+\" and \"-\" refer to the direction of relation, P and S indicate that the relation is significant only for data from that source (parent or student), and \"..\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88) Base Year, First Follow-up, and Second Follow-up Parent and student Surveys."}, {"section_title": "59", "text": "Parent-Student Discussions of Issues Several questions on the NELS:88 second follow-up were designed to create a picture of family interactions that might affect educational progress. Parents and students both responded to questions about the frequency of student-parent discussions on nine different topics. Each topic was scored 1 for \"never,\" 2 for \"sometimes,\" and 3 for \"often.\" The questions and topics analyzed were: Things that are troubling your teenager/you Parent and student responses to these questions were not very correlated, as shown in Table  3.7A. The polychoric correlations ranged from .13 for jobs after school, to .19 for grades, to .31 for school activities, to .46, for applying to colleges or other schools after high school. The percentages of matches were between 36 percent and 53 percent, low values for a three-point scale; and responses from parents, ranging from 2.37 to 2.74 on the 3-point scale, generally indicated more frequent discussions than did responses from students, which ranged from 1.88 to 2.33 [9 t's ranged from -51.08 to -83.91]. The lack of convergence should be attributed at least in part to the differences in itemwording. Parents were asked to consider the past two years, while students were asked to consider the past few months. For example, it is quite plausible that in many families, discussions about one of these topics might occur one to five times in a year, and that respondents would rate the frequency of discussions based on the ability to recall specific discussions, setting the dividing line between \"sometimes\" and \"often\" anywhere from 3 to 20 conversations. Since three to four times as many conversations would have been in the time scope of the parent question as in the time scope of the student question, it should not be surprising that students were less likely to report \"often\" and more likely to report \"never,\" even if they both remembered the same conversations and had the same dividing lines. Given these variations in time frame and frequency criteria, researchers should definitely not treat these parent and student items as interchangeable. In spite of these problems, one might expect to obtain a profile of discussions across topics. However-except for grades and applying to colleges, which on average were rated the most frequently discussed topics by both parents (2.74 and 2.66) and students (2.33 and 2.31)-there was little agreement, at the mean, about which topics were discussed most frequently. Missing data from students are more of a problem for these items than most of the others, as shown in Table 3.7B. Roughly one-eighth of the students omitted these items. Generally, for both sources, nonresponse was higher among those whose complementary source indicated less frequent discussions. The largest of these effects was for parents' responses to the item about discussing school activities: students whose parents did not respond to NELS:88 indicated fewer discussions of school activities with their parents: a mean difference of .18 units less on the 1-to-3 scale [ t= 2.92, -6.58, -3.94, -0.67, -2.62, -3.74, -2.70, -4.52, -1.78, respectively]. The pattern for student omission bias is less pronounced. The performance of these items was similar across population subgroups, as shown in Table  3.7C. There were no systematic patterns in the polychoric correlations, and the only consistent pattern involving average parent-student mean differences was the tendency for girls to agree with their parents, on average, more than boys, in recalling discussions on all nine topics [t=10.42, 9.12, 9.25, 12.07, 9.09, 11.72, 5.23, 4.98, 10.26]. Nonresponse was more prevalent in some population subgroups, however: low SES students [9 t's ranged from -6.09 to -6.65], males [9 t's ranged from -9.56 to -9.97], students with low reading scores [ 9 t's ranged from -14.26 to -15.29], and public school students [9 t's ranged from -2.80 to -3.11] tended to omit the items about discussions with their parents at rates ranging from 13 to 18 percent, compared to 9 to 11 percent for other students.  "}, {"section_title": "62", "text": "There was a consistent pattern of positive associations between reported frequency of discussions and educational outcomes except for discussion of job possibilities after high school or of things troubling the student, as shown in Table 3.7D. Negative associations between these latter two discussion topics and mathematics scores and grades may merely indicate that students with particularly high mathematics scores and high grades had few discussions with their parents about jobs after high school or about troubles. In most cases, the associations applied for both parent and student responses. The main exceptions were (a) discussions of grades and (b) associations with enrollment status. Students' reports of discussions of grades were significantly associated with greater locus of control and selfconcept, higher grades, and plans for professional careers; but their parents' reports were not. It appears that students and parents reflected different perceptions of discussions about grades in responding to this item. The other exception is an artifact of the fact that the item on discussions was not included on the dropout questionnaire, so that nearly all students who responded to the item were enrolled.12 As a result, the data base for testing hypotheses about the effects of these discussions, based on student reports, on enrollment status is very minimal. A researcher might consider the parent responses as proxies for the dropouts' reports of discussions for this analysis. However, the lack of convergence shown in Table 3.7A indicates against that strategy. P-P-S+ + Note: \"+\" and \"-\" refer to the direction of relation, P and S indicate that the relation is significant only for data from that source (parent or student), and \"..\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88) Base Year, First Follow-up, and Second Follow-up Parent and student Surveys."}, {"section_title": "12", "text": "A total of 254 NELS:88 participints with nonzero panel weights who were dropouts at the second follow-up received and completed student questionnaires, providing the possibility of testing these hypotheses for student reports also."}, {"section_title": "45", "text": ""}, {"section_title": "63", "text": "BEST COPY AVAILABLE Parent-Student Decisions on Drinking and Substance Problems Another NELS:88 second follow-up item aimed to create a picture of family decisionmaking, asking who made decisions on a variety of topics about the student's activities (parents alone, parents after discussion with student, parents and student together, student after discussion with parents, or student alone). Two were selected for analysis here; whether the student may drink alcohol when parents are present and whether the student may drink alcohol at social gatherings at which the parents are not present. The response alternatives ranged from (1) parents decide alone to (5) student decides alone and differed only in the use of pronouns (\"I\" refers to the respondent in each case). Also at the second follow-up, both parents and students were asked questions about student use of alcohol and drugs. The specific questions were: Parent Question 57 (F2) My teenager has a drinking problem. My teenager has a drug problem. (1) Strongly agree, (2) agree, (3) disagree, (4) strongly disagree, and (5) don't know) Student Question 85 (F2) Since the beginning of the school year, on how many occasions (if any) have you been under the influence of the following on school grounds? Alcohol"}, {"section_title": "Marijuana or hashish Cocaine (including crack)", "text": "The response alternatives for the student question were 0, 1-2, 3-19 and 20+ occasions, scaled from 0 to 3. The parent question was also placed on the same 4-point scale, treating \"don't know\" as an omission of information. Both parent and student questions are sensitive and have quite different response scales, but they address similar topics. Although the parent questions refer to general \"problems\" and the student questions refer to specific behaviors at school, they both address drugs and alcohol, and it is important to know the extent to which they might be substitutable for each other. The student question about \"alcohol\" was compared to the parent question on \"a drinking problem,\" and the student questions on \"marijuana\" and \"cocaine\" were separately compared to the parent question on \"a drug problem.\" The results shown in Table 3.8A suggest that the parent and student items on decisionmaking in the family are not measuring the same events. The polychoric correlations for the two items on drinking were between .14 and .15. The percentages of matches were 37 for drinking with parents, and 30 for drinking elsewhere. On a scale from 1 (parent decision alone) to 5 (student decision alone), the mean scores according to parents were 1.97 and 2.81, compared to 2.56 and 3.64 according to students [t= 31.23, 40.26]. Researchers who would include locus of family decisionmaking in educational achievement models should be cautious about interpreting these NELS:88 items as more than the individual opinions of students and parents about decisionmaking. The polychoric correlations of the items about student substance abuse were also low (.26 for alcohol, .31 for marijuana, and .19 for cocaine), but they are higher than for the subjective item on decisionmaking about drinking. Because the items are highly skewed (most students and parents acknowledge no drinking or drug abuse or problem), the percentages of matches (67 percent, 75 percent, and 77 percent) fail to convey the lack of convergence. Of course, the comparison of mean scores on these scales only serves to indicate the differences between the response options for the items. Many students omitted the item on family decisionmaking, possibly because it was item 98 on the questionnaire. By comparison, there was 8.0 to 9.5 percent nonresponse by parents on these items, due in part to overall follow-up attrition in parent responses. The nonresponse rate was similar for both the alcohol and drug problem items. Students, on the other hand, were more likely to leave the items on drugs unanswered than the item on drinking (13 percent vs. 9 percent). Students who omitted the items on drug and alcohol use at school were more likely than those who 47 answered them to have parents who said they thought their teenager had a drinking or drug problem. Taken together, these results suggest that student nonresponse to these items may be an indicator of a drinking or drug problem [t= 4.13, 5.00, 5.29, for alcohol, marijuana, cocaine]. As shown in Table 3.8C, the low convergence on these items was found in all of the population subgroups studied; however, the mean parent-student differences on who decides about drinking when parents are present varied. The mean discrepancy between parent and student responses was greater when the student did not live with the parent respondent all of the time [t=6.50], when SES [t=9.47] and reading scores were low [t=11.94], when the student was male [t=4.43], and when the student was in public school [t=2.46]. Finally, all but the smallest subpopulation difference in student nonresponse were significant, the largest being the differences on decisionmaking related to whether the student was living with the parent respondent all the time [t=17.24, 17.53] and the differences on substance use/problems between students with low and high reading scores [t=13.71, 16.42, 17.27]. The relations between these items and educational outcome measures follow distinctive patterns, as shown in Table 3.8D. For the three substance abuse items, the source of the information does not alter the significance of the negative effect (the lack of a significant student finding for enrollment status is an artifact of not including the items on the dropout questionnaire). The two decisionmaking items have different patterns. In both cases, student reports that the student makes the decisions are negatively associated with educational outcomes. For decisions about drinking when parents are present, however, the same reports from parents are positively associated with outcomes. The conclusion to be drawn is that these decisionmaking items must be considered as parents' and teenagers' perceptions of behavior, rather than actual behavior.  "}, {"section_title": "48", "text": ""}, {"section_title": "S-P-", "text": "Note: \"+\" and \"-\" refer to the direction of relation, P and S indicate that the relation is significant only for data from that source (parent or student), and \"..\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88) Base Year, First Follow-up, and Second Follow-up Parent and student Surveys."}, {"section_title": "67", "text": "Enrollment, Suspension, and Dropout Status The enrollment status of a NELS:88 participant is both a principal outcome measure and a component of the design: different questionnaires were distributed to students and dropouts. Therefore, the accuracy of the enrollment status on the computer file must be high if the data are to be useful. In the second follow-up, parents were asked whether their teenager was currently enrolled in school (Question 30), and the results indicate the validity of the enrollment status indicator on the file (F2UNIV1)the polychoric correlation was .97, with a 97 percent match, as shown in Table   3.9A. The mismatches indicated that slightly fewer parents reported their teenager had dropped out (9.5 percent vs. 10.5 percent) than the file indicator suggested. Questions about suspension from school were also included in the second follow-up questionnaires, but they were worded differently for parents and students. The questions were: Parent Question 35b (F2) Has your teenager ever been suspended from school? Student Question 9f (F2) How many times did the following things happen to you in the first semester or term of the current school year? I was suspended or put on probation from school. The parent item was one of three parallel items, the other two asking whether the student was a behavior problem at school or had been expelled, and the responses to all three were (1) \"yes\" or (2, recoded as 0 for this analysis) \"no.\" In contrast, the student question was one of nine parallel items ranging from \"I was late for school\" and \"I was put on an in-school suspension\" to \"I spent time in a juvenile home/detention center.\" The responses to each were in six categories: never, 1-2, 3-6, 7-9, 10-15, and 15+ times. Moreover, as the question wordings indicate, parents and students were to consider different time periods. For this analysis, the student item was dichotomized as (1) \"one or more times\" or (0) \"never,\" to match the parent item. In spite of the differences in the questions about suspensions, the polychoric correlation for this item pair was .72. Parents were twice as likely [t=-22.65], however, to indicate a suspension from school (14 percent vs. 7 percent), which might be expected since the parent was considering a longer time frame. About 2 percent of students left this item out, and their parents were twice as likely (28 percent) to indicate that their teenager had been suspended. Again, nonresponse appears to have been an indicator of a problem.  There was a sharp difference in parent-student agreement on suspensions depending on whether the student was in a public or private school [t=11.84]: students enrolled in private schools were less likely to be in agreement. Also, students with lower SES, with lower eighth grade reading scores, enrolled in public school, and not always living with the parent respondent, were more likely than their parents to indicate having been suspended [t=7.87, 7.26, 8.78, 5.49]. The information displayed in Table 3.9D indicates significant relations between enrollment status and educational outcomes for both parent and student reports. Furthermore, because the items 51 69 are so highly correlated, parent information on enrollment status appears to be an acceptable substitute for more complete information. Suspensions are significantly negatively related to outcome measures, with the exception that the association with student self-concept was only significant when the report was from parents. "}, {"section_title": "50", "text": ""}, {"section_title": "Educational Aspirations and Expectations", "text": "The second follow-up included questions to students about what level of education the student's mother and father wanted their teenager to attain and a question to parents about what level of education they wanted their teenager to attain. The base year and second follow-up student questionnaires also included a question asking about how far the student would go in school. For this analysis, students' expectations were compared to parents' aspirations for them. The base year questions were in different units, and it was necessary to collapse the parent response alternatives to match the eighth grade student item, which was scored from 1 to 6. The second follow-up items, on the other hand, were scored from 1 to 12. At the eighth grade (base year) there was moderate agreement in the educational aspirations of parents and expectations of students, with a polychoric correlation of .58. The agreement was about the same at the twelfth grade. The agreement between parents' desires and student expectations, however, was higher (a polychoric correlation of .58), than was the agreement between students' estimates of what their parents wanted and what the parents said they wanted (polychoric correlations of .51 for agreement with fathers and .50 for agreement with mothers). Note that for the question about the parent's aspiration, only data from fathers and stepfathers were used for the father's aspiration for the teenager's education and only data from mothers and stepmothers were used for the mother's aspiration. Ten to twenty percent of the students left these items blank at the second follow-up, and their parents differed from other parents in aspiring to somewhat less education for their teenagers. Likewise, parents who left the second follow-up item blank had teenagers who had lower educational expectations and thought their parents had lower educational aspirations for them. These results should be taken into account in carrying out research on the relations of educational aspirations and expectations to other factors in education. Note that the percentages of omissions by parents in Table 3.10B are inflated because for cases with no parent questionnaire, both the father and mother were counted as having omitted the item. The percentage was especially high for fathers, because most parent respondents were mothers. Table 3.10C shows the relative frequencies of student-parent responses across population subgroups. The polychoric correlations for expectations at the eighth grade level were similar across all subgroups, although the correlation was slightly lower for the low reading group [t=7.70]. At twelfth grade, the agreement with student expectations was less for the low SES [t=9.63] and low reading subgroups [t=10.26].  At the eighth grade level, average parent aspirations were lower than student expectations, and this discrepancy was larger for low SES students [t=-3.17] and for girls [t=4.55]. In contrast, at the twelfth grade the differences between parents' desires for their teenagers and student expectations were positive in all subgroups-in these cases, parents' desires exceeded students' personal expectations. The parent-student difference was especially large for the low SES [t=12.00] and low reading [1=11.84] groups (parent-student differences of more than .6 for both groups). Student nonresponse rates at the eighth grade level were low across all subgroups. Nonresponse rates at the twelfth grade level, however, were somewhat higher, and higher for students in the low reading group [t=-21.50, -21:02, -13.51] and low SES group [t=-20.12, -15.96, -8.66] than other students."}, {"section_title": "College Choice Factors", "text": "Both students and parents of students who were considering college were asked a question on the second follow-up about the importance of 18 different factors in selecting a college. The question was: Parent Question 66 (F2) How important is or was each of the following to you in your teenager's choice of a school to attend after high school? Student Question 59 (F2) How important is or was each of the following in choosing a school you would like to attend? A three-valued scale of (1) not important, (2) somewhat important, or (3) very important was used. Although these questions are similarly worded, it should be no surprise if students and parents 55 73 indicated different priorities for college choice. Convergence is not an issue of data quality so much as one of an evaluation of the possible substitutability of one source for the other in research. The convergence results shown in Table 3.11A vary from a high polychoric correlation of greater than .60 for \"availability of financial aid\" and \"ability to attend school while living at home\" to a low of less than .20 for \"active social life at the school,\" \"availability of a degree program that will allow me to get a job in my chosen field,\" and \"geographic location of the school.\" The low convergence of the latter two items might have been expected. The item about the fit of the degree program is the most complex of these items, and its convergence depends on a secondary convergence between the parent and student on the \"chosen field.\" The geographic location item may have been confusing to respondents because three other location items (about living at home and living away from home and being in a low crime area) appeared earlier on the list. Respondents may have found different interpretations of \"geographic location.\" Generally, parents were more interested than were students in having the student in a low-crime environment [t=57.65] and in a religious environment [t=42.74]; and students were more interested in an active social life at the school [t=-36.61]. The most noticeable nonresponse bias, for both parents and students, as shown in Table  3.11B, was that for respondents who omitted the items on \"ability to attend school while living at 56 home\" and \"easy admissions standards,.\" responses from the alternative source indicated that these factors were more important on average than in families in which the items were not omitted. Convergence of responses on college choice factors varied between population subgroups, as shown in Table 3.11C. Generally, the polychoric correlations were higher for high SES [t=11.5, 13.2, 3.6, 7.1, 5.0, 11.0, 4.9, 10.0, 1.9, 3.0, 4.0, 5.3, 4.9, 5.2, -0.2, 4.3, 0.3, 13.0, respectively] and high reading students [t=5 .7 , 7.9, 4.9, 6.2, 4.7, 14.1, 5.4, 7.8, 1.3, -0.6, 2.1, 2.5, 4.7, 6.1, -0.1, 6.1, 2.6, 11.6, respectively]. Agreement with parents was slightly higher for girls than for boys, especially on the importance of attending the same college as a parent [t= 6.99]. Differences in average level between parent item responses and student item responses also variedbetween groups. Overall, more than in other families, low SES [t =7.00, 7.58, 15.11], and public school parents [t= 4.15, 1.95, 7.79] were more likely than their children to view expenses, aid, and living at home as important factors in college choice. On the other hand, even more than other parents, parents of high SES, high reading, and private school students were less likely than their children to say that social life was an important factor [t=18.21, 4.13, 6.87] and more likely to say that academic reputation [t =8.46, 8.62, 6.82], a religious environment [t=16.28, 6.90, 3.43], and location [t=10,89, 9,32, 8.36] were important. For these measures, descriptions of population subgroups depend on the source of the information defining those groups. Finally, the pattern of nonresponse followed that for other 57 survey items, students omitting this item as a whole if at all, without noticeable variation among the factors.  Whether students or parents provide the ratings on college choice factors can make an important difference in the results of research on educational outcomes, as shown in Table 3.11D. The importance of course offerings, preparation for job, and college location in choosing a college are positively related to outcome measures, but for the most part only if based on student ratings. On the other hand, the importance of availability of college aid, college social life, and job placements are negatively related to these outcomes, but for the most part only if based on parent ratings. Because of this symmetry-each source is significantly related to some outcomes to which the other is not-it is not reasonable to assume that one source is merely a less reliable or valid measure of the other. Each source represents a different, although related, construct. Same school as parent S-S P+ P Note: \"+\" and \"-\" refer to the direction of relation, P and S indicate that the relation is significant only for data from that source (parent or student), and \"..\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88) Base Year, First Follow-up, and Second Follow-up Parent and student Surveys."}, {"section_title": "Occupational Expectations", "text": "At the second follow-up, both parents and students were asked about their expectations for the kind of job or occupation the student might pursue. The questions were: Parent Question 69 (F2) Which of the categories below comes closest to describing the job or occupation your teenager is interested in? The level of agreement between the parent and student items was high, when compared to other subjective items, as shown in Table 3.12Asufficiently high to warrant confidence that they are measuring roughly the same concept. The polychoric correlation was .65, and the percentage match of 55.6 percent was high for a 16-category response. The average responses were similar, with parents .27 units closer to the professional end of the scale (i.e., parents were somewhat more likely [t, 6.82] to think their teenager expects a professional career). There was a substantial nonresponse rate, however, as seen in Table 3.12B. Moreover, students who failed to respond to this item tended to have parents who indicated that their teenager was not interested in a career in one of the professions, and vice versa. The average of parent responses (on the scale from 1 to 16, with 1 and 2 indicating a professional career) was .85 units higher for students who failed to respond than for other parents [t=7.04]; and the average student responses were .71 units higher when parents did not respond [t=6.16]. The average differences between parent and student expectations for the student's occupation were greater among high SES households [t=2.63], high reading score students [t=1.57], males [t =-2.43], and students in private schools [t=2.49] than others, as seen in Table 3.12C. In these subpopulations, parents were especially likely to expect more of their students than the students indicated. Overall, the positive associations with educational outcomes did not depend on the source of the information, as, seen in Table 3.12D. For that table, occupation expected was distinguished as \"professional\" verus \"other\". Thus, the use of one of these sources when the other is not available is reasonable, if the biases shown in Table 3.12C can be taken into account. Note: \"+\" and \"-\" refer to the direction of relation, P and S indicate that the relation is significant only for data from that source (parent or student), and \". .\" indicates that no significant relation was found. -\" indicates that the comparison would be meaningless, with the same dependent and independent variables in the model. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88) Base Year, First Follow-up, and Second Follow-up Parent and student Surveys."}, {"section_title": "8 Summary", "text": "The combined results across the various topic areas discussed in this chapter are shown in the following series of tables. First, in Figure 3.1, the scatter plot of items in terms of two measures of convergence, the mean difference and the polychoric correlation, are shown. The numbers in the plot in Figure 3.1 correspond to the items as numbered in Table 3.13A. Item response scales described in this chapter, rather than the response standard deviations, are used for measuring the parent-student differences to facilitate interpretation of the differences. As can be seen, the polychoric correlations vary from .10 to .97, and they provide a different picture than would be obtained by comparing the absolute matches. As might be expected, the polychoric correlations were highest for objective items, for items that were worded similarly, and for nonsensitive items. The mean differences, even when statistically significant, are not great. In only a few cases do they constitute more than one half unit on the response scale, and in no case are they as much as a whole response unit. However, they must be compared in the context of the entire range of the response scale. On a dichotomous item, a difference of .10, for example, would correspond to a shift of 10 percent of the sample from one response to the other. 64  Next, Table 3.13B summarizes the impact of parent and student nonresponse on the distributions of item responses. In several cases, the evidence suggests that, especially for students, nonresponse was more likely if the response would have been a socially less desirable response or would have given sensitive information (e.g., drug use). Nearly all percentages of omissions by parents are greater than 6 percent because parent questionnaires were not returned for about 6 percent of the cases, while student or dropout questionnaires were available for 100 percent of the cases (with F2PNLWT greater than zero). The omission rates for parent and student items were generally comparable, although parents had less nonresponse to items about family discussions and about alcohol and drugs.  Table 3.14 summarizes the variations in polychoric correlations between various population subgroups. Each row corresponds to one of the item pairs considered in this chapter, and each column-pair corresponds to a population dichotomy. Overall, these results show greater convergence between parents and students when the student is a more proficient reader and when the family has a higher socioeconomic status. More often than not, there was also greater convergence when the student was female. On the other hand, there was no consistent pattern between convergence and whether the student lived with the parent respondent all of the time or attended public or private school.  (a) A few high correlations are based on a Pearson correlation approximation. In those few cases in which the Pearson correlation was close to 1.0, the polychoric correlation computation in SAS failed to converge for a subpopulation, although it did converge for the total. In those cases, the approximation (1 -R2 (Pearson) suhpopulation) x ((1 R2 (polychorie) total) / (1 -R2 (Pearson) tow)) was used for (1 -R2 "}, {"section_title": "66", "text": ""}, {"section_title": "88", "text": "The question of convergence can also be addressed in terms of the \"location\" of the responses, as indicated by the difference in the mean response by parents and students. If there is a greater difference between parents and students in some subpopulations, then research on those subpopulations must be especially cognizant of the source of the information used to develop indicators. As shown in Table 3.15, although there was substantial variation across NELS:88 items, the general pattern was for closer agreement between parents and students when the socioeconomic status measure was high, when the respondent was female, when the student was a better reader, and when the student was enrolled in a private school.  Differences in student nonresponse rates between subpopulations were more striking than the differences-in convergence, as shown in Table 3.16. When differences were observed, percentage omissions were generally from 2 to 5 times as great in one of the subpopulation categories as in the other-more in families in which the student did not live with the parent respondent all of the time, more in low SES homes, among males, among poorer readers, and among public school students. While item omission rates for the complementary groups were sufficiently high to support research on them without severe nonresponse bias, nonresponse bias in analyses of the groups listed above may substantially distort results.  Finally, Table 3.17 summarizes the comparisons of findings of statistically significant effects on outcome measures, when the predictive factors are based on parent or student information. Although the majority of the 570 entries indicate that research results would not depend on the source of the information, the outcomes of over 150 of these selected analyses depend on whether the information was collected from parents or students. The need for considering the parents' and students' processes involved in generating survey responses is clear.  Note: \"P\" signifies that a significant effect would be found in the specified direction only for the parent measure; \"S\" signifies a significant effect in the specified direction only for the student measure; \" + \" and \" \" signify that both are significant in the same direction; \"13-S+\" and \"P+S-\" signify significant parent and student effects in opposite directions, respectively; and \"..\" signifies no significant effects. The NELS:88 data offer an opportunity to compare student and teacher reports about a number of individual, classroom, and school characteristics. NELS:88 collected teacher data during the base year data collection and at each follow-up. Understanding the analyses of correspondence between teacher and student reports, but in particular the analyses of missing data presented here, requires understanding some aspects of the teacher sample design as described in Chapter 2. (A more complete discussion of the teacher sample is available in the Second Follow-Up: Teacher Component Data File User's Manual [NCES 1994b].)"}, {"section_title": "92", "text": "In this chapter, comparisons between second follow-up student and teacher responses are presented for three sets of items: ratings of the student's English proficiency, perceptions of teaching practices in the mathematics or science classroom, and specification of the student's academic program. For each set of items, four tables are displayed, containing summary data relevant to the four research questions stated in chapter 2 (relating to convergence, omissions, subpopulations, and impact on outcomes). The first three tables in each case are computed without weights; the fourth presents significance results that take into account both the survey design and differential weighting."}, {"section_title": "Student English Proficiency", "text": "Students and teachers were both asked to answer two items about students' English proficiency: whether or not the student's native language was English, and, for non-native speakers, whether the student's English skills contributed to classroom difficulties. Student Question 107 (F2) Is English your native language (the first language you learned to speak when you were a child)? Teacher Question 9(F2) Is this student's native language English? The teachers' question included a footnote indicating that the phrase \"native language\" referred to the first language that the student learned to speak as a child. For these analyses, a positive response was scored as \"1,\" a negative response as \"0.\" A follow-up item relied upon more subjective judgment, and this subjectivity shows clearly the extent of disagreement between teachers and students. Students who reported that they were not native speakers of English were asked: 77 95 Complete homework assignments? Teachers were asked a much less detailed question: Teacher Question 10 (F2) Is this student's academic performance in your class limited by his or her level of English language proficiency? Furthermore, this teacher item was asked about all students, while the corresponding student item was asked only of non-native speakers. For the current analysis, the comparison between teacher and student responses employs only those teacher responses that correspond to a student who indicated that he or she was not a native English speaker. This restriction allowed a comparison of the reports on the teacher and student items for the same subsample of students. To compare the teacher and student items, the student responses were collapsed into a binary variable that assumed a value of 1 if the student responded positivelS, to any of the items (a through g, above), and a value of 0 if he or she responded negatively to all of the items. This coding was comparable to the yes/no format of the teacher item. Table 4.1A summarizes key measures of correspondence between student and teacher reports on these items about students' English proficiency. The similarity in wording allowed a direct comparison between the teacher's and student's reports of the student's native language. As illustrated in Table 4.1A, students and teachers responded fairly consistently. The polychoric correlation between the items was 0.86; however, 78 96 noticeably more students (8 percent) than their teachers (5 percent) indicated that the student had a non-English native language [t=9.31]. Though high among student/teacher comparisons, the polychoric correlation also reveals some considerable disagreement, which is to be expected for an item such as this one. It is entirely possible that teachers simply would not know the status of high school students who are not native English speakers but who learned English sufficiently young to speak without an accent. As Table 4.1A makes clear, the communality of response was much lower for the evaluative items than for the items pertaining to being a native English speaker. Teachers were less likely than students to note the academic impact of limited English proficiency. This difference may be due to the differences in item construction. The more detailed question asked of students may be more likely to elicit positive responses than the simpler item asked of teachers; the longer list of detailed possibilities may jog the respondent's memory of an incident or feeling that he or she experienced. The examination of potential effects of nonresponse on these findings combined instrument nonresponse and item nonresponse to identify overall rates of missing data for the teacher data, and item nonresponse to identify rates of missing data for students. In addition, possible nonresponse biases were evaluated. To do so, differences were calculated between the average student reports for (a) students for whom teacher reports were missing and (b) students for whom teacher reports were available. This measure identifies, according to student reports, how missing teacher responses would differ from available teacher responses. Correspondingly, the difference in average teacher responses between those students who did not provide student reports and those who did was also calculated. Results of these analyses are presented in Table 4.1B. First, Table 4.1B indicates substantial missing data. Over 20 percent of expected teacher reports about student native-speaker status are missing. Much of this finding is attributable to teachers who reported that they did not know the students' status. Similarly, over 20 percent of students who were not native English speakers failed to report whether their English proficiency affected their schoolwork. (Student reports on this measure were considered not missing if the student responded either (a) positively to any one of the seven items included in the measure or (b) negatively to all of them.) The bias assessment also revealed one important nonresponse effect. The negative sign of the differences on the first item (i.e., students' native-speaker status) suggest that data are more likely to be missing for students who are not native English speakers [t=18.64, 7.53], and this bias is important in the case of teachers because of the substantial missing data. This result seems logical, especially with respect to the bias in the teacher reports. For example, it is plausible that teachers would report they \"don't know\" for high school students with Asian or Hispanic surnames who speak fluent English, many of whom learned English as a second language. The correspondence between student and teacher responses to these two item-pairs was examined separately for groups defined by the following: Below median and above median socioeconomic status Student sex Below median and above median base year reading scores Class subject (math or science) taught by teacher Missing data rates were comparable across subgroups. While the potential bias from missing data could be large for some groups, the limited amount of data made estimates too imprecise to report differences with confidence. This analysis revealed no cross-group differences in either mean differences between student and teacher reports or the percentage of nonresponses. However, the analysis does reveal substantial variations in polychoric correlations for the estimates of the impact of limited English proficiency on schoolwork. These results are presented in Table  4.1C. 18.2 (a) A few high correlations are based on a Pearson correlation approximation. In those cases the polychoric correlation computation in SAS failed to converge for a subpopulation, although it did converge for the total. In those cases, the approximation (1 -R2 (pc.) The Pearson approximation was accurate for cases for which the computation converged. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Second Follow-up Student and Teacher Surveys. First, there was very little convergence between the answers given by the students who were below the median SES and those given by their teachers about the extent to which their English skills contributed to any classroom difficulties [t=4.90]. There was also very little convergence between the responses given by female students and their teachers [t=.--5.10] concerning how great an effect the students' English proficiency had on their academic performance. Finally, there was less 80 convergence for math teachers than for science teachers [t= 3.14], possibly due to lower linguistic requirements in mathematics classes. Teacher and student reports of student English proficiency differ to some degree. Researchers interested in studying education-related outcomes must ask whether these differences will lead to different substantive conclusions when they rely on one report or the other. To illuminate this issue, six key outcome measures were examined, comparing differences between Students reported to be native English speakers and students reported to be non-native English speakers; and Non-native English speakers whose English proficiency is reported to affect their school work adversely and non-native English speakers whose proficiency is reported not to affect their school work adversely. The outcomes examined in Table 4.1D are the student's second follow-up mathematics test score; the level of education the student expects to complete; the prestige rating of the student's occupational expectation at second follow-up; a composite variable indicating the difficulty of the student's educational program as constructed from transcript data; high school grade point average in four areas; and the student's self concept score and locus of control score at second follow-up. There are nine potential entries in the table:   T+ teacher item shows a significant positive relation T teacher item shows a significant negative relation S+ student item shows a significant positive relation S student item shows a significant negative relation both items show a significant positive relation both items show a significant negative relation T -S+ teacher item shows a significant negative relation, student item shows a positive T+S teacher item shows a significant positive relation, student item shows a negative neither source shows a significant relation "}, {"section_title": "T-", "text": "Note: \"+\" and \"-\" refer to the direction of relation, T and S indicate that the relation is significant only for data from that source (teacher or student), and \"..\" indicates that no significant relation was found.  Table 4.1D indicates that there can be discrepancies in the conclusions that would be reached concerning the relations between English language use and outcome measures, depending on whether student or teacher reports were the basis for the assessment. Teacher reports of whether English is the student's native language are positively related to the difficulty of a student's program, grade point average, and level of the student's occupational expectations, whereas student reports are not. On the other hand, students' reports are positively associated with locus of control. Teachers' reports that students' school performance is limited by English language proficiency are related, negatively, to the students' educational aspirations and occupational expectations, curriculum difficulty, and grade point average, whereas students' perceptions are not. Clearly, different conclusions about the relations between English language proficiency and educational outcomes will be found, depending on whether the teacher or student is the source of the perception."}, {"section_title": "Practices in the Classroom", "text": "The NELS:88 second follow-up asked students and teachers about instructional practices in math and science classes. This section examines the correspondence between student and teacher reports about what happens in the classroom. Responses of math and science teachers and their students to the following items, which were on a 5-point scale ranging from (1) \"never/rarely\" to 5 Listen to the teacher lecture? g. Use computers in math class? i. Participate in student discussions? The items on student discussions are included in the analysis even though they refer to somewhat different activities, to assess how differently two items on classroom student discussions may operate in a research study. Demonstrate an experiment or lead students in systematic observations? a. Have students do an experiment or observation individually or in small groups? c. Require students to turn in written reports on experiments or observations? e. Have students use computers for data collection and analysis? Student Science Class Question 15B (F2) In your current or most recent science class, how often do/did you e. Watch the teacher demonstrate an experiment or lead you in observations? f. Do an experiment or observation individually or in small groups? h. Write up reports on experiments or observations? i. Use computers for collecting and/or analyzing data?. It is important to note that the questions asked of teachers and students about instructional practices in math and science classes are not as comparable as they appear on the surface. Teachers were asked about the frequency with which they employ specific instructional practices in their classrooms. Students, on the other hand, were asked not how often their teachers employed specific instructional practices but rather how often they themselves participated in those activities. The differences between the emphases of teacher and student items may lead to seemingly incomparable reports about what happens in the classroom. A teacher might, for example, use computers in the classroom once a week, but a student who watches another student work on a computer in this classroom but does not work on the computer himself might not report that he works with computers in that same classroom about once a week.. In short, NELS:88 asked teachers to report about their practices in the classroom, while students were asked about their own participation in the classroom. Neither report alone accurately captures what actually occurs in the classroom. Table 4.2A summarizes key measures of correspondence between student and teacher reports for these items. With the exception of the item about computer use, student and teacher responses about the instructional practices in math classes bear little resemblance to each other; the polychoric correlations are around 0.1. To some extent, this divergence reflects the different perspectives,that the items encourage. The teacher items ask about classroom activities, while the student items may encourage responses about the individual student's participation in those activities. Students and teachers may also have different referentsa student may consider what the teacher says in introducing an activity to be a \"lecture,\" while the teacher would not consider that to be a \"lecture.\" However, the item on computer usage stands in stark contrast to the other two, with a polychoric correlation of about 0.5. Examination of the full frequency table reveals that students and teachers almost always agree when they report that they \"never/rarely\" use computers in the class. Such agreement would be expected in classrooms in which no computer existed. It may be that, under certain circumstances (i.e., when no computer is physically present) the item has a physical referent in the classroom, and therefore relies less on student or teacher judgment.  The polychoric correlations between the four pairs of items about the instructional practices in science classes ranged from a low of about 0.17 to a high of 0.46. Once again, the highest communality was found in the responses to the item about computers, and student/teacher agreement was quite high when they reported that computers were \"never/rarely\" used. Again, a computer in a classroom is a concrete referent that requires little judgment to note its absence. The teacher and student responses to the question about student experiments were also more highly correlated than either the item about writing reports or the item about watching the teacher demonstrate experiments. It seems reasonable to believe that actually conducting experiments or observations may provide students with more concrete referents than would merely watching a teacher conduct an experiment. Determining whether or not a teacher's demonstration constitutes an experiment requires student judgment, and watching an experiment may simply be less memorable than conducting one. The examination of nonresponse combined instrument nonresponse and item nonresponse to identify overall rates of missing data. As in the previous section, likely nonresponse biases were examined by looking at the teacher reports for which student data were missing, and vice-versa. These results are tabulated in Table 4.2B. "}, {"section_title": "102", "text": "As Table 4.2B illustrates, about 3 percent of the student data and between 11 and 17 percent of the teacher data were missing. The examination of potential biases revealed no obvious biases. This finding, however, does not necessarily suggest that respondents were substantially similar to nonrespondents. The low communalities between the student and teacher reports could be taken as an indication of poor measurement of one or both items in each pair. If neither itemprovides a good measure, then observed differences would be rare, especially in small samples (for instance, the sample in which a report from one source is available and a report from another source is missing). The correspondence between student and teacher responses to each of the item-pairs was examined separately for groups defined by sex, SES, and base year reading scores. Analyses were done separately for math classes and science classes. Table 4.2C summarizes the results of these analyses. First, looking at the comparison of student responses with those of their math teachers, greater disagreement in the mean appears between females and their teachers than males and their teachers. In particular, in comparison to males, females seem to report more frequent lectures in their math classes. The mean difference between the teachers' and the girls' reports is 0.42; the mean difference between the teachers' and the boys' reports is 0.25 [t=4.64]. However, the difference in the wording of the teacher and student items may support the conclusion that girls were simply reporting spending more time listening to lectures than boys were. At the same time, the difference between girls' reports of the frequency of computer usage in math classes and their teachers' reports also tends to be greater, in the opposite direction, than the difference between boys' and teachers' reports [t= -5.26]. This difference suggests that girls may be using the computers less frequently than boys in the class. A similar pattern emerges in the comparison between math teacher reports on lecturing and the reports of students whose base year reading scores are above and below the median. The differences between student and teacher reports tend to be larger for students with high eighth grade reading scores (0.39) than for other students (0.26) [t=3.57]. Again, on the question about listening to lectures, higher eighth grade reading scores students may be reporting that they listen more, not necessarily that the teacher lectures more. Also, students with higher eighth grade reading scores reported less frequent computer usage in comparison with their teachers (-0.18) than other students did (-0.07) [t=-3.55]. Whether this difference is related to differences (a) in the math classes these students are taking, (b) in the experiences they are having with computers in the same classes, or (c) in their perceptions of the same experiences is not clear. With respect to teacher and student reports on science classroom practices, students with lower eighth grade reading scores tended to report more of each of the four types of activities, relative to their teachers, than did those with higher eighth grade reading scores [t=4.81,4.50, 2.46, 2.86]. Again, these may be differences in perception or differences in experience. Turning to relations to outcome measures, the associations between classroom practices and outcomes were assessed (a) when the practices were reported by teachers and (b) when the practices were reported by students. The frequencies of practices were dichotomized between \"rarely or never\" and the other four alternatives. Table 4.2D presents these results. Table 4.2D reveals that the teacher and student reports frequently do not identify the same significant differences between groups experiencing different levels of instructional practice usage on the outcome measures. At the same time, however, there is only one case in which teacher and student reports yield contradictory significant results (in more difficult curricula, teachers indicate having more student-led discussions but students indicate participating in fewer student discussions). This table clearly indicates that researchers should not choose lightly between student and teacher reports about classroom practices, and in particular, differences between \"participating in student discussions\" and \"having student-led discussions\" should not be ignored. Indeed, in light of the low levels of agreement between teachers and students about classroom practices, it is unclear whether either report is an accurate measure of what is occurring in the classroom. Note: \"+\" and \"-\" refer to the direction of relation, T and S indicate that the relation is significant only for data from that source (teacher or student), and \". .\" indicates that no significant relation was found. "}, {"section_title": "85", "text": ""}, {"section_title": "Student's High School Track", "text": "The final set of items on which teacher and student reports were compared indicated the student's high school track. The curriculum track items on the teacher and student questionnaires, like other items analyzed, were not perfectly matched: teachers were asked about the high school track of the course they taught, and students were asked in general about their high school track. However, since the teachers were teachers of mathematics and science, to the extent that a student's math and science courses are indicative of his or her curriculum track, one might be considered substitutable for the other if needed for a research analysis. This section assesses that substitutability. Teacher Question 2_3 (F2) Which of the following describes the \"track\" this class is considered to be? Alternative, Stay-in-School, or Dropout Prevention Program An additional problem to be addressed in this comparison was that the response categories were not identical for the student and teacher items. However, it was possible to collapse the responses into general, vocational, and college preparatory tracks and limit the analyses to these three tracks. Rather than utilize a single scale for these three items, the items were recoded to generate three binary variables (coded 1 if the course/student is in the track and 0 otherwise) for each set of respondents. Students in classes reported by teachers to be in a remedial track or by themselves to be in special education, an alternative program, or a specialized program like fine arts were not counted in these analyses. (That is, they were treated as legitimate skips for the trichotomy of main tracks). Table 4.3A presents key measures of correspondence between teacher and student reports for these three variables. The polychoric correlations for these items were in the moderate range; however, the convergence for the college preparatory track response (.59) was noticeably higher than the others [t=3.72, 7.16]. That is, there appeared to be greater overlap of students in one track in courses in another track for the vocational education and general tracks than for the academic. track. Most math and science teachers considered their courses to be college preparatory (75 percent of students had teachers who reported their class to be college preparatory) , although they were open to students not in those tracks (only 62 percent of students reported being in a college preparatory track) [t=24.07]. This finding may follow from the differences between the teacher and student items. For instance, many students in a general or vocational high school track may take math or science classes that teachers consider to be college preparatory. It is also possible that, depending upon how apparent or unapparent track levels are in a school, students simply do not know which 88 track they belong to and simply report \"general\" because they believe they are taking the same courses their peers are. The examination of nonresponse combined instrument nonresponse and item nonresponse to identify overall rates of missing data. As in earlier sections, likely nonresponse biases were examined by looking at the teacher reports where student data was missing, and vice-versa. These results are tabulated in Table 4.3B.  Table 4.3B reveals that about 4 percent of the student responses and about 10 percent of the teacher responses to the item on which these measures are based were missing. There is some evidence that the students for whom responses are missing are less likely to be enrolled in a college preparatory science or math class, according to the teacher report [t=10.82]. Also, teachers who omitted this item were less likely to teach students who reported being enrolled in college preparatory programs [t=2.23]. The match between student and teacher responses to this set of items was compared for the subgroups defined by students' sex, SES, and eighth grade reading scores, and whether the teacher was a math or science teacher for the student. These comparisons revealed only two substantial cross-group differences: student omission of this item was primarily among low SES students [t=9.94] and students with low eighth grade reading scores [t=13.82]. These figures are presented in Table 4.3C. The analysis of potential differences in findings on key outcome measures reveals no substantial difference between using teacher and student reports for this item. The comparison of the relationships between course and student track variables and the same outcomes investigated above yields no important differences between students who are reported to be in a particular track (student report) or in a math or science course in a particular track (teacher report). In all cases but one, both measures yield results that are significant and in the same direction; for the only discrepancy-vocational track students' self concept-only the student report was associated with a significant, negative effect. Table 4.3D presents the results of this analysis. Note: \"+\" and \"-\" refer to the direction of relation, T and S indicate that the relation is significant only for data from that source (teacher or student), and \"..\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Second Follow-up Student and Teacher Surveys."}, {"section_title": "89", "text": ""}, {"section_title": "90", "text": ""}, {"section_title": "Summary", "text": "The combined results across the various topic areas discussed in this chapter are shown in the following series of tables. First, in Figure 4.1 and Table 4.4A, the scatter of items in terms, of two measures of convergence, the teacher-student difference and the polychoric correlation, are shown. As can be seen, the polychoric correlations vary from .10 to .86. Next, Table 4.4B summarizes the impact of teacher and student nonresponse on the distribution of item responses. Table 4.5 summarizes the variations in polychoric correlations between various population subgroups. Table 4.6 shows the general pattern of agreement between teachers and students for population subgroups, and Table 4.7 shows the pattern of student nonresponse to these items. (Items pertaining to classroom practices cannot be compared in Tables 4.5, 4.6, and 4.7 between the subpopulations of students for whom math or science teacher responses are available because the items about math classes do not match the items about science classes. Therefore, parts of these tables are left blank.) Finally, Table 4.8 summarizes the comparisons of findings of statistically significant associations with outcome measures, when the predictive factors are based on teacher or student information. Teacher and student responses to the NELS:88 items under consideration tend to differ, sometimes quite dramatically. Even seemingly straightforward items, such as the question of whether or not English is a student's native language, yield reports from teachers and students that differ noticeably from one another. In some cases of difference between teacher and student reports, the differences may be attributed, at least in part, to differences in the wording of items or to the ways in which the student and teacher are likely to understand the items. Among such cases, instances with the highest correspondence tend to be those with a fairly concrete referent (e.g., the physical presence of a computer in a classroom; the student's recollection of conducting an experiment in science class). Based upon findings presented in this chapter, researchers employing data about classroom practices are likely to obtain different findings depending on specific item wordings and whether they choose to rely on student or teacher reports of the practices researched (e.g, are student or student-led discussions as effective as lectures in a high school math class?). Student and teacher data diverge so significantly on these items that one must carefully scrutinize conclusions that are based on reports by teachers and students about classroom activities to determine whether they are warranted. Student and teacher responses provide different perspectives on high school, and research to combine them into more effective pictures of school processes would be valuable. Given the data available, researchers should not make the choice between student and teacher reports of classroom practices lightly, and indeed, perhaps should not make the choice in that form at all. The incomparability of teacher and student data on these items shows clearly that researchers using these data must clearly define what they intend to measure and develop an explicit model of the relation that the data bear to that construct. Such models might include data from multiple sources, as well as specification of the error structure of the model.\nA total of 112 pairs of repeated NELS:88 measures, between base year and second follow-up or between first and second follow-ups, were examined in this chapter. These included 77 pairs of measures that are related to the teenager's family, home, and general attitudes and plans, displayed in Tables 5.1 19A and 5.19B. Note that the use of \"A\" and \"B\" in these table numbers differs from their use elsewhere in this report: other \"B\" tables contain information about omissions, not convergence. Generally, the polychoric correlations are of modest size, suggesting that the interpretations of NELS:88 items generally depend on the age level at which they were administered. The most notable pattern in these data is that the polychoric correlation coefficients, which measure the convergence of the pairs, are generally higher for pairs that include first and second follow-up measures. This is to be expected because the interval that separates those pairs is only half the length of the interval that separates the base year from the second follow-up. One might expect the difference to be even more striking for the measures relating to schools, because most teenagers attend the same school in tenth and twelfth grades, while nearly all change schools between eighth and twelfth grades. However, the median polychoric correlations do not reflect this interaction. Compared to second follow-up measures, the median polychoric correlations are .45 and .39 for first follow-up measures related to home and school, respectively, and .32 and .26 for base year measures related to home and school, respectively. Not all of the polychoric correlations were modest. Teenagers responded nearly identically at base year and second follow-up to the question about whether English was their native language; and they were fairly stable in indicating whether the curriculum they were engaged in was intended to prepare them for college and whether they planned to go to college. Their indications of religiosity were also much more stable than the other measures examined, whether asked as a selfperception or as an explicit behavioral indicator (frequency of attendance). While reported attendance at services dropped off, the correlation between years was high. For these analyses, all measures were resealed, if necessary, to a small number of discrete response categories. On that scale, a mean increase or decrease of 1.0 would indicate a large shift between the earlier and later response distribution. Most of the changes in mean values were much less than one half unit on the measure's response scale, but as shown in Figures 5.1 and 5.2, several were on the order of .5 or -.5. Most notable were reports of decreases in discussions about school with parents, in television watching, and in attending religious services and of increases in time spent on homework and remembering to bring pencil and paper to class. Whether these changes in reports reflect real changes in behavior or merely different interpretations of the same behavior by teenagers cannot be determined directly from these data. In either case, interpretations of the reported frequencies of these behaviors should take into account the age at which the reports were made. 160 /78 1 80162 BEST copy AMUR P7   Omission Rates. The information on omission rates, shown in Tables 5.20A and 5.20B, shows a general tendency for more omissions in the first follow-up than in the base year and most omissions.in the second follow-up. This tendency is not due to failure of teenagers to return followup questionnaires, because the analyses are limited to the subset of NELS:88 participants with nonzero second follow-up panel weights, and therefore to participants who returned a questionnaire for each of the surveys. There are noticeable exceptions to the general pattern, and in interpreting them, readers should note that for these analyses, noninformative responses, such as \"I don't know,\" were treated as omissions, because that is how they are generally treated by researchers who use NELS:88 for substantive educational research. On the other hand, legitimate skips were not treated as omissions. In some cases with large percentages of legitimate skips (such as items that assumed the father was home and items that were intended only for non-native English speakers), omission rates are not directly comparable because they are for different populations. At the base year, responses to questions about television watching, about parents' educational aspirations for them and their occupational expectations for the future, and about extracurricular activities had high nonresponse rates, as did the question for non-native English speakers on having received special help. These may be due to a combination of unwillingness to respond with sensitive information, lack of certainty about the correct response, lack of understanding of the item, or complex response patterns that may have been unflagged legitimate skips. At the first follow-up, a large percentage of respondents did not give a categorizable expected occupation, most responding with \"I don't know.\" On the other hand, omissions were very few for the item concerning plans for the future. For both first and second follow-ups, fewer than 2 percent of respondents omitted this item, although more than twice as many omitted items before and after it in the survey instruments. This item, which has 15 fairly easy judgmental responses (not important, somewhat important, or very important), is the first item at the top of the page entitled \"In YOUR PLANS FOR THE FUTURE,\" and as such, it may have attracted students who otherwise were quite willing to leave items that were difficult for them blank. At the second follow-up, response rates were generally higher for the items concerning home, family, and self than for the items about school and schoolwork. There were especially high omission rates, nearly 20 percent, for items about the relationship between teenager and parent (e.g., is the student aware of parents' reasons for rules?) and about teenagers' knowledge about what their parents' educational aspirations are for them, and among non-native English speakers, for selfestimates of English proficiency. Generally, in each year, there was a clear tendency to respond to or omit a multicomponent item as a whole. For example, at the second follow-up, the 3 items concerning views on parental trust all had omission rates between 19 and 20 percent, the 4 items concerning discussions with parents all had omission rates between 12 and 13 percent, the 12 items on chances for success in different areas all had omission rates between 10 and 11 percent, the 13 items on locus of control and self-concept all had omission rates between 9 and 10.2 percent, 6 of the 7 items on extracurricular activities (excluding cheerleading) all had omission rates between 6.9 and 7.7 percent, and the 13 items on importance of lifetime achievements all had omission rates between 0.9 and 1.1 percent. Thus, the major hurdle for the teenagers appears to be in beginning to respond to an item.   "}, {"section_title": "109", "text": "On the other hand, the NELS:88 items that point to students' and math and science class track appear to lead (at least in the examples considered here) to substantively similar conclusions. Despite the fact that these items set out to measure different things (i.e., the teacher measure identifies the track of the particular class while the student reports on his or her overall academic track), these reports appear to be close enough to yield substantially similar results. This finding, however, comes with limitations. The items' estimated mean differences, while generally significant and in the same direction, were almost always different from each other, as would be correlation and regression coefficients. Accurate estimates of effect size would almost certainly require explicit models of the measurement process.    Table 4.5-Pol choric correlations between student and teacher res ones, for o i ulation sub rou s s    Note: \"+\" and \"-\" refer to the direction of relation, T and S indicate that the relation is significant only for data from that source (teacher or student), and \"..\" indicates that no significant relation was found. In efforts to improve education through research, analysts have explored ever-increasingly complex models of relations among various measures of student attitudes, behaviors, choices, and performance. Students' homework behavior, television-watching, self-esteem, sense of what is important, and educational expectations interact in different ways for different students to affect performance in high school. In many studies, single-time measures of attitudes, behaviors, and choices are correlated with single-time measures of performance. Questions of whether attitudes and behaviors are specific to a particular grade level and whether the age at which they are measured determines whether they are correlated with high school performance are rarely considered because the data to address them are rarely available. NELS:88 offers a unique opportunity to address these questions, because similar and overlapping survey items were administered two or three times to students between eighth and twelfth grades. In this chapter, NELS:88 responses at the second follow-up, when most participants were in twelfth grade, are compared to base year (eighth grade) or first follow-up (tenth grade) responses on similar items. These items range from family interactions, including language use and discussions; to personal attitudes and expectations, including self-esteem, locus of control, and occupational and educational expectations; to behaviors that might be related to performance in school, such as television viewing, homework, course selection, and extracurricular activities; and finally, to feelings about the student's school. NELS:88 data are used to determine how stable these items are across the high school years, how response patterns change, and how these measures taken at different times have different relations to outcome variables. The stability of NELS:88 cognitive measures across years is discussed in a separate Psychometric Report for the NELS:88 Base Year Through Second Follow-up (Rock et al. 1995). Most of the measures examined in this chapter are expected to change between eighth grade and twelfth grade or between tenth grade and twelfth grade because the teenagers are growing and the environment around them is changing. Therefore, findings of lack of convergence are not necessarily a sign that the items were poorly designed or that students misunderstood the items. Discrepancies between the measures only constitute measurement error for analyses that make the assumption that the measures are constant across the four years. The results in this chapter indicate the extent to which it is reasonable to characterize individual differences among teenagers and their environments as constant traits from eighth to twelfth grades. Can a teenager be characterized as a television watcher, as a church goer, as a homework completer, or as having high educational and occupational expectations, or do different students have these characteristics at eighth, tenth, and twelfth grades? Do eighth graders who attend schools where they feel that they are unsafe, that teachers are not interested in students, and that crimes occur feel the same way about their schools in twelfth grade?"}, {"section_title": "97", "text": "If these characteristics change, and especially if they change only for important subpopulation groups, such as low SES students, and if those changes affect the relations between characteristics and outcomes, such as twelfth grade measures of performance and self-concept, then use of these characteristics in educational development models must acknowledge their mutability over time. If these characteristics remain consistent, on the other hand, then use of the measure at one time as an indicator of a teenager's status throughout the period from eighth to twelfth grade is appropriate. This chapter contains comparisons for 18 sets of items. The first 10 sections focus on behavior outside of the school context, interactions with parents, self-perceptions, and expectations. The final 8 sections focus on in-school behavior and attitudes. Comparisons are made either between base year and second follow-up responses or between first and second follow-up responses."}, {"section_title": "Television Viewing Habits", "text": "Television viewing has been linked by educational research to performance in schoolsstudents who report watching more television tend to obtain lower test scores.I3 Of some interest is the reliability and stability of student reports of television watching behavior: is television watching reported the same in twelfth grade as in eighth grade? The NELS:88 base year and second follow-up surveys both asked about television viewing with the same item, providing the basis for assessing whether the two items are equivalent. If they are equivalent, then researchers can make broader generalizations from one-time surveys; however, if the response distributions are not equivalent, then either television viewing habits change over the high school years or twelfth grade students interpret and respond to the item differently from eighth graders (e.g., students at one grade or the other may be more motivated to report watching less television). In the base year survey and in the second follow-up survey, students were asked to indicate how many hours per day, both on weekdays and on weekends, they spent watching television during the school year. The items analyzed were: Base Year, Second Follow-up 42A(BY), 35A (F2) During the school year, how many hours a day do you usually watch TV on weekdays? 42B (BY), 35B (F2) During the school year, how many hours a day do you usually watch TV on weekends? For the base year items, the response options were (a) don't watch TV, (b) less than 1 hour per day, (c) 1 2, (d) 2 3, (e) 3 4, (f) 4 -5 hours per day, or (g) over 5 hours per day of television watching. At second follow-up, response options (e) and (f) were combined (\"3-5 hours\"). 13 See, for example, Mullis, I.V.S., Campbell, J. R., and Farstrup, A.E., NAEP 1992: Reading report card for the nation and the States, pp. 171-174."}, {"section_title": "98", "text": ".1 1 6 Therefore, for these analyses, base year response options (e) and (f) were combined to produce the same scale, from 0 to 5, as in the second follow-up. The results from comparison of students' responses over the two waves of the survey are shown in Table 5.1A. The polychoric correlations were 0.42 and 0.37 for weekday hours and weekend hours, respectively, indicating only a moderate degree of convergence for students' responses between eighth and twelfth grades; and the raw percentages of matched responses were 29.2 percent and 29.8 percent. Students on average reported slightly more hours of TV watching on the base year survey than they did on the second follow-up survey (3.0 hours14 versus 2.5 hours on weekdays [t, 37.3] and 3.5 hours versus 3.0 hours on weekends [t=-33.1]). Whether the lack of convergence and change in mean values indicate changing behavior on the part of students or merely changing responses to the item, these data suggest that television watching is not a stable trait measured over time. There was a higher level of missing data (approximately 10 percent) from students in the base year than at the second follow-up (less than 5 percent) [t=-20.2, -22.0, for weekday and weekend hours], as shown in Table 5.1B. Moreover, the omission bias for the base year was noticeably positive for both items (0.38 and 0.19); that is, base year nonrespondents tended to report greater amounts of television viewing [t=9.63, 4.79] at the second follow-up than did other students, on average. At eighth grade, these nonrespondents may have been avoiding the response of more television watching by omitting the item. Thus, it is unlikely that the drop in average reported television watching (-0.46, in Table 5.1A) is due to differential nonresponse. Moreover, a similar omission bias was not found at the second follow-up, which suggests that if there was a tendency to avoid reporting a large amount of television watching, it was more noticeable at the eighth grade than at the twelfth grade. Taken together, the results on omissions indicate that the apparent drop in television viewing from the base year to the second follow-up was more likely a reflection of actual behavior, rather than a differential tendency of twelfth graders to avoid saying that they watched a great deal of television. 14 Average scaled responses are referred to as \"hours\" to provide meaning for the comparisons. Although a more accurate scaling might subtract .5 from the values, it may be that students perform that subtraction in generating their responses. 99 117 The characteristics of these differential responses among different population subgroups are shown in Table 5.1C. This table focuses on polychoric correlations, average changes in response from eighth to twelfth grade, and nonresponse rates in the second follow-up. The polychoric correlations were somewhat lower for lower SES students [t=7.57, 3.55] and students with relatively low eighth grade reading scores [t=8.68, 5.27] on both the weekday and weekend items. For example, with respect to weekday TV watching, the correlations were 0.34 versus 0.46 for lower and higher SES students, respectively; and 0.33 versus 0.46 for students with low and high eighth grade reading scores, respectively. Unlike the correlations, the average differences between base year and second follow-up responses were similar across groups. Thus, differences in television viewing that researchers might find between SES and eighth grade reading groups would not be greatly affected by the timing of the survey item. Finally, students from lower SES backgrounds [t=5.4, 6.8] and students with relatively low eighth grade reading scores [t=12.6, 15.2] were less likely to respond to these items at second follow-up. Excessive television watching has been hypothesized to be detrimental to students' academic achievement. In order to investigate the relations between television watching and educational outcomes, researchers might use the NELS:88 data to determine whether such a statistically significant relation exists, and the question arises as to whether the same conclusions would be reached by researchers who used the eighth and twelfth grade responses. Table 5.1D displays the relationship between television watching and twelfth grade mathematics scores and other outcome measures, as measured by weighted t-tests comparing two groups: those who reported watching three or more hours versus those who reported watching fewer than three hours. The entries in Table 5.1D and subsequent tables of relations to outcome measures have the following meanings: B+ base year or first follow-up item has a significant positive relation B base year or first follow-up item has a significant negative relation S+ second follow-up item has a significant positive relation S second follow-up item has a significant negative relation both items have a significant positive relation both items have a significant negative relation B -S+ base year item has a significant negative relation, second follow-up item has a positive relation B+S base year item has a significant positive relation, second follow-up item has a negative relation neither source shows a significant relation (--) (--) (--) Entry not appropriate because no second follow-up dropout questionnaire data were included for television watching. Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \"..\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Student Surveys. As Table 5.1D shows, the relations between student outcomes and relative high numbers of hours of television watching on weekdays (three or more hours per day) is consistently negative and significant for six of the eight outcome measures.15 This is true no matter which student response is used. However, the story for weekend television watching is different: only one of the outcome measures (locus of control) was negatively related to weekend television watching, as measured at the base year; and one was significantly positively related (curriculum program difficulty). On the other hand, three outcome measures (twelfth grade math scores, grade point average, and locus of control) were negatively related at the second follow-up. Researchers using the NELS:88 student reports of television watching would find different results if they measured weekend television viewing at eighth or twelfth grade."}, {"section_title": "English Language Competency", "text": "English language competency is an important goal of American schools, and its achievement is related both to many contextual factors and to many outcomes. Knowledge about changes in this measure between eighth grade and twelfth grade can help researchers interpret results of analyses that use the measure at one time or the other. In the base year and second follow-up NELS:88 surveys, students were asked questions about their native language; help received in learning English; how well they understood, spoke, read, and wrote English; and about their knowledge of other languages. The items analyzed were: Base Year"}, {"section_title": "17", "text": "Before you started going to school, did you speak any language other than English?"}, {"section_title": "S-", "text": "Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \". .\" indicates that no significant elation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Student Surveys."}, {"section_title": "Religious Attitude", "text": "For many students, religion plays an important role in shaping values which, in turn, guide behavior in and out of school that affects educational outcomes. An important question for researchers is whether measures of religious attitude and church attendance obtained in a paper-andpencil survey capture reliable variation in behavior between individuals. With NELS:88, it is possible to address this question because in the first and second follow-up surveys, students were asked to indicate how often they attended religious services and whether or not they viewed themselves as religious. The items analyzed were: First Follow-Up, Second Follow-up 82 (F1), 106 (F2) In the past year, about how often have you attended religious services? 83 (F1), 105(F2) Do you think of yourself as a religious person? In both surveys, students were given six choices regarding attendance at religious services, ranging from (1) \"more than once per week\" to (6) \"not at all\". With respect to being a religious person, students could respond (1) \"very,\" (2) \"somewhat,\" or (3) \"not at all.\" For both items, the scales were reversed before conducting the analyses. As shown in Table 5.3A, the polychoric correlations for both items were 0.71, which shows fairly strong agreement in students' responses between tenth grade and twelfth grade, whether the question is couched as a self-image rating or as a report of frequency of behavior. The rate of matches for the religious self-image question was higher (68.9 percent) than for the attendance question (46.9 percent), but this was due to the difference in number of response options (3 versus 6). Although the two items had similar convergence, the average response to the question about attendance changed between tenth and twelfth grades (from 3.53 to 3.22 [t=-24.5]), while the average response to the self-image question remained stable (1.83 and 1.82). A noticeable percentage of students in both surveys did not respond to either item (Table  5.3B). Data from students in the first follow-up were not available for about 7 percent of the cases for each item; and data from students in the second follow-up were not available for about 12 percent [t=17.26, 18.15]. Although there was very little omission bias in the twelfth grade items, those who omitted the question about attending services in tenth grade were likely at twelfth grade to indicate slightly less attendance, on average, than other students (a difference of 0.14) [t=-2.19]. Comparisons of the responses among different subgroups of students are shown in Table  5.3C. The polychoric correlations on both items are fairly high across subgroups, although they are highest for high SES students [t=10.4, 11.4] and students with relatively high eighth grade reading scores [t=13.8, 15.4]. Otherwise, there are no substantial differences in responses between the For all but one outcome measure, the relationship between having a religious self-image and attending religious services is positive and significant at both tenth and twelfth grades (Table 5.3D). In general, both of these measures appear to be reliable correlates of educational outcomes, no matter at which grade level the item is asked. "}, {"section_title": "Views on Parental Trust", "text": "A variety of questions about interactions with parents were asked in NELS:88, and several of them were repeated over time. Do these questions tap the same underlying trait, or does each repetition tap a new aspect of the teenager's personality as it develops? The stability of the responses to these items is an important consideration in planning their use in educational research, as well as in designing new surveys of members of this age group. Two examples of these items are examined in this and the following section."}, {"section_title": "A", "text": "In the base year survey and in the second follow-up survey, students were asked to indicate the extent to which three statements about their relationships with their parents were true. The statements were as follows: In the base year survey, students could respond in one of two ways, true or false. For this analysis, true is coded as 1, false as 0; and data for statement B are reversed, so that all three items are scored in a positive direction. In the second follow-up, students were given a choice based on a 6-point scale, ranging from \"true\" to \"false\". In order to make comparisons, the second follow-up 6-point scale was collapsed to a 2-point scale (1, 2, 3 vs. 4, 5, 6) to correspond to the base year scale. Comparisons of student responses over the two waves of the survey are shown in Table 5.4A. The polychoric correlations (0.30, 0.25, and 0.31 for statements A, B, and C, respectively) indicate a low level of convergence. It is of some interest that a substantial majority of students responded with a positive attitude about parents for the first two of the items (87 and 74 percent, at second follow-up) but disagreed with it for the other item (only 19 percent agreed). This is a positive indicator that students were responding to each item independently, not as repeated, redundant questions about their relations with their parents. Also, for two of the items, the percentage of students responding \"false\" changed little between eighth and twelfth grades, but for the question about parents \"checking up,\" the percentage of students responding \"false\" dropped from 20 percent to 13 percent (i.e., the mean response changed from .80 to .87) [t=17.64]. In the second follow-up, more students on average believed that their parents trusted them to do what they [their parents] expected, without checking up. One might suppose that this is an indicator of growth on the personal characteristic tapped by this item, but the low correlation (0.31) suggests that growth is not a reliable measure at the individual level: many students changed from \"false\" to \"true,\" but many also changed from \"true\" to \"false\". Although student nonresponse on these items in the base year was very low, student nonresponse in the second follow-up survey was very high [t=58.0, 58.1, 57.1]. Nearly 20 percent of teenagers omitted these items in the twelfth grade survey (Table 5.4B). Fortunately, for the use of these items in research, the data indicate that second follow-up respondents who omitted the item were not very different from others, as measured by their base year responses to these items [t.-1.69, -2.79, 4.88]. The tendency for nonrespondents to have indicated earlier that they counted on parents to solve their problems, although statistically significant, was only 4 percent greater than the corresponding likelihood for respondents. Levels of convergence between base year and second follow-ups differed somewhat across population subgroups, as shown in Table 5.4C. However, the convergence was low in all of the subgroups shown in the table. With respect to average differences in student responses between the two surveys, there was a small but noticeable trend for the item concerning awareness of parents' reasons for telling them what to do; increases from 3 to 4 percent occurred among high SES [t=3.71], high reading ability [t=3.63], and female respondents [t=4.99], compared to no increases for other respondents. Similar to other items with substantial numbers of omissions, omission rates for all three of these items at second follow-up were related to subgroups. High SES [t=9.7,9.8,9.8], high eighth grade reading level [t=18.4, 18.4, 18.4], and female teenagers [t=6.5, 6.6, 6.8] omitted the items less frequently than students in the other subgroups. As shown in Table 5.4D, responses at both base year and second follow-up were significantly correlated with outcome measures. Even though the outcome measures were selected from the second follow-up, there was no tendency for the measures to be more correlated with second followup responses than base year responses. The findings of negative correlations between \"counting on parents to solve problems\" and the outcome measures, plus the finding (in Table 5.4A) that relatively few respondents said this was true, suggest that this item is qualitatively different from the other two items. In fact, at the second follow-up, the correlation between responses to statements B and C (both coded to be positive, as indicated above) was significantly negative (r weighted =r unweighted = -0.24, p<.0001). Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Student Surveys."}, {"section_title": "128", "text": "Discussions with Parents The other item concerning interactions with parents asked for recall of the frequency of discussions on school-related topics. In the base year survey and in the first and second follow-up surveys, students were asked how often they had discussed certain topics with either or both of their parents or guardians. The question in the base year survey asked how often the students had discussed the topics \"since the beginning of the school year,\" with response options of \"not at all,\" \"once or twice,\" or \"three or more times.\" The question in the first follow-up survey asked the students to indicate how frequently they had discussed these topics \"in the first half of the school year;\" and the question in the second follow-up survey asked how frequently this had occurred \"in the first semester or term of the school year.\" Both follow-up items had response options of \"never,\" \"sometimes,\" and \"often.\" The topics were as follows: Base Year 36 A. Selecting courses or programs at school."}, {"section_title": "B.", "text": "School activities or events of particular interest to you.\nSchool activities or events of particular interest to you."}, {"section_title": "C.", "text": "Things you've studied in class. Selecting courses or programs at school.\nThings you've studied in class."}, {"section_title": "D.", "text": "Your grades. The polychoric correlations, shown in Table 5.5A, indicate a low level of convergence between the base year and second follow-up, similar to the levels seen in the preceding section. In this case, however, an additional potential source of divergence is the change of schools that took place for virtually all of the students between the two surveys (i.e., from middle school to high school). The convergence between tenth and twelfth grade measures was somewhat higher, although it can still only be called moderate. Unfortunately, because the response options were different for the base year, it is impossible to determine whether the difference in convergence levels is related to wording changes or to the fact that there was a change in the frequency of discussions with parents between eighth grade and tenth grade but not between tenth grade and twelfth grade. On the other hand, the large mean differences [t, 56.7, -76.0, -67.2] shown in Table 5.5A between base year and follow-up are probably due to the change in wording: many would interpret \"sometimes\" to include part of the region referred to by \"three or more times.\" The average changes between first and second follow-up were relatively small, although taken together they indicate slightly less frequent conversations with parents about school at the twelfth grade [t=-24.95, 11.12, -6.95, 16.40]. As shown in Table 5.5B, there were substantial percentages of omissions of these items at the first (9 percent) and second (12 percent) follow-ups. With the exception of discussions about grades, those who omitted the items on the second follow-up tended to have indicated fewer conversations with parents when they responded to the base year or other follow-up survey than other students did [t (second follow-up)=-5.44, -8.15, -5.23, -3.33,-3.04, 2.95, 0.76]. Therefore, estimates of frequency based only on respondents are probably slight overestimates. There were no substantial subpopulation differences in either convergence or mean level of response, as shown in Table 5.5C. There were, however, noticeable variations in tendency to omit the item. Fewer students from high SES households [seven t's ranged from 6.1 to 6.7], students with high eighth grade reading scores [seven t's ranged from 14.3 to 15.3], and female students [seven t's ranged from 9.7 to 10.0] omitted the item than other students.  Finally, as shown in Table 5.5D, students who reported having frequent discussions with their parents had significantly more positive outcomes, and these relations held whether the assessments of frequency were made at the twelfth grade or earlier. The exception to this, the enrollment status indicator, is artifactual, because this item was not included in the second follow-up dropout questionnaire. Although there were 376 student questionnaires (with F2PNLWT greater than zero) from individuals with a negative value on the enrollment status indicator (i.e., dropped out [370], aged out [2], or died [4]);most dropouts returned dropout questionnaires, not student questionnaires. Significant effects on enrollment status would thus be hard to detect based solely on those who completed a student questionnaire.  + + Note: \"+\" and \"-\" refer to the direction of relation, B, F, and S indicate that the relation is significant only for data from that source (base year, first follow-up, or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and First and Second Follow-up Student Surveys."}, {"section_title": "Self-Esteem and Locus of Control Items", "text": "In the base year survey and in each follow-up survey, students were asked to indicate the extent to which they agreed or disagreed with 13 statements about themselves. These statements were combined into two composites, \"locus of control\" and \"self-concept,\" which provide a bridge to other social research efforts, including the NLS-72 and HS&B surveys. The statements were as follows: Base Year; First Follow-up; Second Follow-up In each NELS:88 survey, students could respond \"strongly agree,\" \"agree,\" \"disagree,\" or \"strongly disagree\" (coded as 1, 2, 3, and 4, respectively, on the NELS:88 CD-ROM) to the statements regarding their opinions and attitudes about themselves. The coding on the CD-ROM yields higher scores for disagreement with the statements as presented on the questionnaire. For this analysis, the responses of positive items (A, D, E, H, and K) were reversed, so that all responses were scaled with higher numerical scores for internal locus of control and high self-esteem. Results are shown separately for all 13 items, rather than for the two composites, to provide the basis for comparing a set of parallel survey items that differ in wording complexity, in serial position in a list, in positive or negative orientation, and in the construct they are measuring (locus of control or selfesteem). Together, they shed light on how these factors are associated with convergence, omission, subpopulation differences, and relations to outcome measures, when responses of eighth-and tenthgraders are compared to their twelfth grade responses. The convergence between eighth grade and twelfth grade measures was low, as shown in Table 5.6A, and the convergence between tenth and twelfth grade measures was only slightly higher. Convergence was slightly higher for the first item on the list, which is also the most simply stated item; and as can be seen in Table 5.6B, slightly fewer respondents omitted this item than later items. This might reflect a tendency for respondents to consider the first of the list of items more attentively, thinking that the other items are essentially redundant. On the other hand, variation in the complexity of wordings of these items affects the consistency of responses over time. For example, items B and K, which have the lowest convergence, have among the most complex wordings. These results suggest that use of the composite self-esteem and locus of control measures for research is preferable to analyses based on the individual items that make up the scales. The average responses to these items were in the upper mid-range of the 1-to-4 scale, between 2.55 and 3.34. Changes in the measures between earlier and second follow-up administrations were generally small. However, for one of the items (M), \"chance and luck are very important for what happens in my life,\" the mean change from base year to second follow-up was 0.19; that is, on average, 19 percent of the respondents responded one unit more positively (i.e., disagreeing with the statement) at the second follow-up than at base year. Because the self-esteem items (A, D, E, H, I, J, and L) as a whole had more positive responses at second follow-up (3.33, 3.34, 3.33, 3.20, 2.67, 2.88, and 3.18) than at first follow-up (3.24, 3.25, 3.24, 3.11, 2.58, 2.79, and 3.14) [seven t's range from 7.19 to 15.59], interpretation of results from studies of teenagers using this scale should take into account their specific age levels. The locus of control items, on the other hand, did not exhibit a corresponding change from first to second follow-up. Convergence between first and second follow-up responses was somewhat greater (by about 0.10) for respondents with relatively high eighth grade reading scores, compared to respondents with low reading scores [t=6.0, 5.8, 5.4, 9.6, 6.5, 7.4, 5.8, 12.0, 8.3, 6.9, 7.2, 6.2, 4.6], as shown in Table   5.6C. Otherwise, convergence was fairly similar across population subgroups. Also, with the exception of the item concerning importance of luck, mean differences between earlier and second follow-up responses were nearly uniform across population subgroups. For perceptions of the importance of luck (item M), mean positive changes from base year to second follow-up were twice as great for low SES [1=5.25]   As shown in Table 5.6D, nearly all of these items were significantly and positively related to outcome measures at all three time periods. There were exceptions, however. In five cases, relations are significant when measured at base year or first follow-up but not significant at second follow-up: relations between item A (\"I feel good about myself') and grade point average and enrollment status, relations between item E (\"I can do things as well as others\") and occupational expectation and enrollment status, and the relation between item H (\"I am satisfied with myself\") and occupational expectation. In these cases, the eighth or tenth grade measures have more variance in common with the outcomes. On the other hand, in three cases, relations are significant when measured at the second follow-up but not at one of the other measurements. Item A (\"I feel good about myself') is significantly positively related to twelfth grade math scores, but not when item A is administered at the tenth grade; occupational expectations are significantly positively related to item K (\"I am almost certain I can make my plans work\") and I (\"I [don't] feel useless at times\") at the second follow-up but not at first follow-up for both K and I; and base year for item I. The major finding with respect to the 13 items making up the two composites is that, except for the first item, there are few systematic differences among them. The first item (I feel good about myself) stands out as having both (1) greater convergence across time and (2) relations with outcome measures that change in ways different from the other items.  Unimportance of luck + + + + + + + + Note: \"+\" and \"-\" refer to the direction of relation, B, F, and S indicate that the relation is significant only for data from that source (base year, first follow-up or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and First and Second Follow-up Student Surveys."}, {"section_title": "138 Educational Expectations", "text": "In this and the following three sections, the stability of teenagers' expectations for education, for occupation, and for various aspects of quality of life, as well as of the importance they place on aspects of quality of life, are examined. Because NELS:88 asked similar questions at multiple time points, these data provide new information on the extent of change in these important measures over a crucial developmental period. Educational expectations are both an indicator of a teenager's self-image and, to the extent that self-image reflects an objective assessment of the likelihood of future events, a predictor of educational outcomes. These expectations are based on many factors, and they may be related to teenagers' perceptions of their parents' expectations for them. Whether these expectations as measured in eighth grade are as valid as later assessments of expectations is of importance to researchers who would use them in models of educational development. In the base year and second follow-up surveys, students were asked how far in school they thought they would get. The item was: Base Year, Second Follow-up 45 (BY), 43 (F2) As things stand now, how far in school do you think you will get? In the base year survey, students were given six choices: \"won't finish high school\", \"will finish high school\", \"will attend vocational, trade, or business school after high school\", \"will attend a four-year college\", \"will finish college\", and \"will attend a higher level of school after graduating from college\", scored 1 to 6, respectively. In the second follow-up survey, students were given eleven choices. The first ten choices ranged from \"less than high school graduation\" to \"Ph.D., M.D., or other professional degree,\" and the last response alternative was \"don't know.\" The second follow-up item was recoded for this analysis onto the base year scale. In both surveys, students were also asked to indicate how far in school they thought their father and their mother would want them to reach. The statements were as follows: Base Year (48); Second   The word \"get\" was used in base year survey, whereas \"go\" was used in the second followup. In the base year survey, students were given the same six response alternatives as in the question about their own expectations, plus an \"I don't know\" alternative. In the second follow-up survey, the response options for the parental aspirations items were the same as for the expectations item, with the addition of a \"does not apply\" option, presumably for the case of absent parents. Second follow-up responses were also scaled to match the base year scale in this analysis. For this analysis, the responses \"I don't know\" were treated as omissions of information; and \"does not apply\" was deleted from analysis as a legitimate skip. As shown in Table 5.7A, a student's educational expectations were moderately convergent, with a polychoric correlation of 0.57; and they were stable, on average, slightly below the level of expecting to graduate from a four-year college. Furthermore, the rates of omissions were low (0.7 percent and 3.8 percent) for base year and second follow-up respectively, as shown in Table 5.7B. With the low omission rates, the omission bias figures are not of concern. The negative omission bias reflects the fact that those who omitted the item tended to have lower eighth grade reading scores [t=11.4] and lower SES [t=5.2], and so were less likely to attend and graduate from college, as shown in Table 5.7C. The finding that males omitted the item nearly twice as frequently as females is noticeable [t=7.5], but even among males, the omission rate was low. Teenagers' perceptions of their parents' aspirations for them were less stable. The convergence was lower [t=-8.63, -11.94], and the average perceptions decreased [t=-4.44, -4.70]; however, the major difference was the greater percentage of omissions. As shown in Table 5.7B, more than 12 percent of the responses were omitted at both times, and the omission bias indicates that those who omitted the item had lower perceptions of their parents' aspirations for them [t=-12.67, -11.78, -15.7, -14]. As with the self-expectations, the negative omission biases reflect the fact that those who omitted the item tended to have lower eighth grade reading scores [t=21.49, 21.01] and lower SES [t=20.14,15.97], and so were less likely to attend and graduate from college. Omission rates were also somewhat lower for girls than boys [t=4.25, t=7.22].  Finally, as shown in Table 5.7D, both base year and second follow-up expectations, dichotomized at the point of finishing college or not, near the midpoint of the distribution of expectations, were significantly related to outcome measures. Students' perceptions of their parents' aspirations for them are also significantly related to educational outcomes, whether measured at base year or second follow-up, with the exception that perceptions of fathers' aspirations at base year were not significantly related to self-concept measured at second follow-up. Overall, it appears that educational self-expectations can be assessed at either eighth or twelfth grade without distorting their use in research, but some caution is needed in generalizing about perceptions of parents' aspirations across years.  (--) indicates that the assessment of effects of second follow-up educational aspiration and expectation on itself would be meaningless. The corresponding effect for the base year measure was significantly positive, as indicated by the correlation of 0.57 between the two measures, silownn Table 5.7A. Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Student Surveys."}, {"section_title": "141 Expected Occupation", "text": "In the base year, first, and second follow-up surveys, students were asked questions about their current or most recent jobs and about the jobs they expected or planned to have in the future. The items analyzed were: Eleven separate occupational categories were provided in the base year item, and 16 were provided in the follow-up items. The categories were re-ordered to match the standard occupational prestige coding (Stevens and Cho 1985), and the follow-up categories were collapsed to match the base year categories for the base year comparisons. The resulting 11 categories were scored from 1 to 11 in the following increasing order: professions such as law; professions such as science or engineering; teacher or homemaker; technical; office work or sales; military or police; small business owner; farmer; craftsperson or machine operator; service worker; and laborer. See appendix Table  B4 for further details. Dropouts were not included in this analysis. As shown in Table 5.8A, the convergence between tenth and twelfth grade measures was moderate, higher than the convergence between eighth grade and twelfth grade measures. This difference may be due to either or both the change in response alternatives and developmental factors between eighth and tenth grades. As can be seen in Table 5.8B, there were more omissions of this item in the earlier surveys than in the second follow-up [t=-28.25, -2933]. Those who omitted the item tended to have low eighth grade reading scores [t=15.2] and low SES [t=7.4] and to be male [t=4.4], as shown by the second follow-up omission rates in Table 5.8C, and this is reflected in the direction of the omission bias (Table 5.8B): those who omitted the item at any point tended to select lower SES occupations and those requiring lower reading skills when they responded at another time point [t=,7.81, 9.03, 6.96, 8.51]. As shown in Table 5.8D, measurements of expected occupation at all three times were significantly positively related to the educational outcomes considered. For this purpose, the occupations were dichotomized into the professions (the first two categories) versus the rest of the 124 142 occupations. Those who reported that they were expecting to be in a profession had significantly higher outcomes.    Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \". .\" indicates that no significant relation was found. Association with expected occupation is not meaningful. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and First and Second Follow-up Student Surveys."}, {"section_title": "Quality of Life Expectations", "text": "During teenage years, individuals are developing expectations for many aspects of quality of life, and it is of interest to know how stable these expectations are. In the base year survey and follow-up surveys, students were asked to indicate how sure they were that they would graduate from high school, and in the follow-ups, they were asked to estimate the chances that they would have various positive aspects of quality of life in the future including high school graduation. The items analyzed were: Base Year"}, {"section_title": "46.", "text": "How sure are you that you will graduate from high school? First Follow-Up, Second Follow-Up Life will turn out better for you than it has for your parents? L. Your children will have a better life than you had? In the base year survey, students could respond \"very sure\", \"probably\", \"probably won't\", or \"very sure I won't\" with respect to graduating from high school. For this analysis, these were coded as 5, 4, 2, and 1, respectively. In the follow-up surveys, students were given five choices regarding the probability of events occurring in their futures: \"very low,\" \"low,\" \"fifty-fifty,\" \"high,\" and \"very high,\" coded as 1 to 5. The convergence of expectations of going to college (0.72) between tenth and twelfth grades was noticeably higher than the other ratings provided by teenagers, and convergence between tenth and twelfth grades for chances of graduating from high school were nearly as high (0.66). The other items were moderately convergent. The moderate convergence of most of these items suggests that they are at least partially tapping different or changing constructs between tenth and twelfth grades; however, separate factor analyses of these items at the two grades, both weighted and unweighted, indicate that the same two factors account for 57 to 58 percent of the variance in these 12 items at both grades.16 Although few eighth graders omitted the base year item, about 5 percent omitted these items at the first follow-up, and 10 percent omitted the item at the second follow-up, as shown in Table  5.9B. Except for the estimation of whether the student's children would have a better life than the student, there was a tendency for those who omitted an estimate to respond with a lower certainty at the other time period [t Omission Bias at First Follow-up =-21.23, -18.77, -8.94, -7.78, -1.85, -3.91, -4.49, -5.95, -5.81, -7.81, -3.78, +3.30, for the 12 outcomes, respectively; and t Omission Bias at Second Follow-up =-8.24, -8.16, -5.48, -3.77, -4.31, -3.76, -3.75, -1.70, -2.29, -3.62, -2.01, +1.38, for the 12 outcomes, respectively 1. The largest effect was for the college attendance item, in which those who omitted the item at first follow-up but responded at second follow-up had a mean score nearly one unit lower (-0.87) on the item; and those who omitted the item at the second follow-up had a mean score 0.27 units lower than others at the first follow-up. The polychoric correlations were noticeably lower for respondents with lower eighth grade reading scores [t=4.62, 7.48, 9.57, 9.23, 5.88, 6.65, 8.34, 6.42, 8.60, 4.74, 9.63, 6.26, for the 12 dimensions, respectively], as shown in Table 5.9C. Thus, for these respondents, the measurements at the two time periods are less likely to be measuring the same construct reliably. Furthermore, these respondents were more likely than others to omit the items at the second follow-up (13 percent vs. about 7 percent) [all twelve t's were significant, ranging narrowly between 10.6 and 12.1, because omission tended to be \"all or none \"]. Also, it should be noted that male respondents .were more likely to omit these items than female respondents (12 percent vs. 8 percent) [all twelve t's were significant, ranging between 7.6 and 8.3]. Among seven of the dimensions of expectation, there was very little difference in the relations of outcomes to expectations measured at second follow-up or first follow-up, as shown in Table 5.9D. All of these relations were significantly positive. For each of the other five expectations, there was at least one outcome significantly related to the second follow-up measure but not to the first follow-up measure. This may have been the result of growth in the ability to interpret and react to these other items, which involve somewhat more complex, relational expectations, or expectations which teenagers may not have considered. In particular, the comparisons of the expected quality of their own lives with those of their parents were significantly positively related to occupational expectations, curriculum choice, grade point average, locus of control, and avoiding dropping out, only when measured at the second follow-up. Finally, the estimation of whether one's children's lives would be better than one's own stood out as tapping a different dimension from the other items, positively related to some outcomes and negatively to others.   Child has better life S+ F-S+ + Note: \"+\" and \"-\" refer to the direction of relation, B, F, and S indicate that the relation is significant only for data from that source (base year, first follow-up , or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and First and Second Follow-up Student Surveys. Overall, these items give a picture of the kinds of expectations that teenagers have formed opinions about by the time they are sophomores (such as graduating from high school and attending college), indicated by moderately high convergence, versus expectations that are still emerging during the high school years (such as family, job, and place in the community). Measurement of most of them at the sophomore level yields the same relations with outcomes as measures two years later. In both surveys, the students were asked to choose either \"not important\", \"somewhat important\", or \"very important\", coded 1, 2, or 3, for each of the statements. The results presented in Table 5.10A indicated that these responses have the same range of convergence as the items on expectations, discussed in the preceding section. Although the average responses vary across a large part of the 1-to-3 scale, from 1.66 for the importance of getting away from parents to 2.88 for the importance of steady work, the polychoric correlations are all between .36 and .56. This low-tomoderate level of convergence indicates that these measures are tapping constructs that are changing over these two years. However, a factor analysis of the 13 measures, either weighted or unweighted, yields the same clustering of the items at both first and second follow-up.' The same four factors account for virtually the same percentage of the variance at both follow-ups: 52.4 percent at the first follow-up and 51.7 percent at the second follow-up. Thus, the change from first to second follow-up indicated by the low-to-moderate polychoric correlations in Table 5.10A appears to indicate changes at the factor level, rather than changes in individual measures. As shown in Table 5.10B, very few teenagers omitted these items. Therefore, even though some of the omission bias estimates are relatively large (e.g., students who omitted the item about the importance of strong friendships at the first follow-up averaged responses of .19 less than others when asked this question at the second follow-up [t=-6.22]), there should be little impact on estimates for the entire population.  The convergence of items differed across subpopulations, as shown in Table 5.10C, although the polychoric correlations remained in the low-to-moderate range for all of the subpopulations displayed. The responses of teenagers with high eighth grade reading scores were more convergent [t=2.58, 9.42, 8.37, 4.44, 6.20, 7.56, 12.1, 10.1, 9.65, 11.7, 12.0, 4.28, 6.01, respectively], as were the responses of high SES respondents [t=3.55, 7.38, 6.58, 1.65, 5.67, 6.60, 7.00, 6.19, 4.32, 6.57, 7.08, 3.86, 4.09, respectively] and of female respondents [t=5. 00, 6.21, 6.86, 8.58, 3.97, -.27, 5.09, 1.76, 2.95, 2.82, 8.01, 1.89, 4.84, respectively]. Stability of some of the ratings but not others varied between subpopulations. Between first to second follow-ups, respondents from low SES households and with low eighth grade reading scores were more likely than others to perceive increasing importance of success in work [t=7.72, 10.13], steady work [t=5.26, 7.52], and creating better opportunities for their children [t=9.11, 9.12]. Although the subpopulation differences in second follow-up omission rates were statistically significant, it should be noted, the largest rate was still very small, 1.6 percent. Finding significant relations between importance ratings and educational outcome measures frequently depends on whether the importance ratings are gathered at the tenth or twelfth grade level. As shown in Table 5.10D, for all but two of the importance dimensions, there is at least one outcome that is significant at one time point but not the other. Most notable is the rating of importance of being able to find steady work, which is positively related to second follow-up outcome measures if measured at the first follow-up, but not if measured at the second follow-up. Importance ratings at the two time periods therefore cannot be used interchangeably in analyses of relations of student values and achievement in school. Changes in some importance ratings vary systematically between population subgroups, and the relations between importance ratings and outcomes changes.   Note: \"+\" and \"-\" refer to the direction of relation, B, F, and S indicate that the relation is significant only for data from that source (base year, first follow-up, or second follow-up), and \". .\" indicates that no significant relation was found."}, {"section_title": "High School Program", "text": "Students were asked about their high school program on the first follow-up questionnaire and again on the second follow-up questionnaire. The statements were as follows: Which of the following best describes your present program? A. Alternative, Stay-in-School, or Dropout Prevention Program For the following comparisons of first and second follow-up reports of high school academic program, responses were collapsed into three dichotomous indicators: (C) vocational, (B) academic, and (A) general. Students who selected response alternatives D,E,F, or G were counted in this analysis as respondents indicating none of the main three tracks. Note that Fine Arts was not included in the vocational track on either follow-up for this analysis. Table 5.11A summarizes the correspondence between students' first follow-up and second follow-up reports of their high school programs. Note: The mean responses sum to less than 1.00 to the extent that students indicated one of the \"other\" programs (D, E, F, and G, above). SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); First and Second Follow-up Student Surveys. These results suggest that in both the first and second follow-up years, students most frequently identified their high school program as either academic or general; and more of them were likely to consider their program academic in the second follow-up [t=18.82]. The polychoric correlation was fairly high for the academic program indicator, but only moderate for the others. Whether that is an indication of different rates of changing programs or of different levels of survey item interpretation is not clear. Table 5.11B reports nonresponse rates and statistics assessing the likely bias from nonresponse. Very few data were missing for these two items-2 percent on the first follow-up and less than 1 percent on the second follow-up. However, despite such high response rates, the statistics suggest some nonresponse bias: at each follow-up, fewer of the nonrespondents than respondents (13 percent and 18 percent) indicated an academic track at the other follow-up [t=-4.30, -3.96]. The analysis was repeated for various subgroups to determine whether students in different groups demonstrated different response patterns on the two items. Table 5.11C reports the associations between students' first and second follow-up responses to this item pair by subgroup. For the vocational and general program indicators, there was greater convergence for high SES [t=4.21, 9.21] and high reading score students [t=5.85, 11.7]; however, this was not true for the academic program indicator, indirectly suggesting that the higher reading skills of academic program students are responsible for the greater convergence of the academic indicator. "}, {"section_title": "General track", "text": "Note: \"+\" and \"-\" refer to the direction of relation, B, F, and S indicate that the relation is significant only for data from that source (base year, first follow-up, or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); First and Second Follow-up Student Surveys."}, {"section_title": "Students' Evaluation of School Atmosphere", "text": "Students were asked questions about school atmosphere on the base year, first follow-up, and second follow-up questionnaires. It is important to note that, in most cases, students have changed schools between the base year and the first follow-up: most students attend the eighth grade in a middle school or junior high school, and therefore, by the time they participated in the first follow-up study, they would have left that school for a high school. Therefore, similar responses on base year and second follow-up questions about school atmosphere and interactions among students and between teachers and students probably reveal more about individual students than they do about schools. For this reason, one might expect students' reports to be more similar on first and second follow-up questionnaires than on base year and second follow-up questionnaires, for most students will have been attending the same school at the time of the first and second follow-ups. Base Year, First Follow-up, and Second Follow-up 59 (BY), 7 (F1), and 7 (F2) How much do you agree with each of the following statements about your school and teachers? There is a real school spirit. "}, {"section_title": "F.", "text": "Disruptions by other students get in the way of my learning. One of the items, \"discipline is fair\", was not included in the second follow-up dropout survey instrument. Otherwise, unlike subsequent sections on school-related measures in this chapter, this analysis combines responses of students and dropouts. In all three surveys, students responded on a four-point scale, with (1) Strongly agree, (2) Agree, (3) Disagree, and (4) Strongly disagree. For this review, the coding was reversed for the first four items so that more desirable reports are associated with greater numbers. Table 5.12A summarizes the correspondence between students' base year, first, and second follow-up reports on six statements about their schools. This table reveals that although the means are quite similar for base year and second follow-up reports, the polychoric correlations between the base year and second follow-up are very low, all below 0.3. Finding very little convergence is not surprising because different schools are being rated in eighth grade and twelfth grade in almost every case. The convergence improves when comparing the first and second follow-ups, but the correlations are modest, ranging from 0.30 to 0.44. Either the constructs being measured at different years do not have a great deal of overlap, or the schools may have changed, as perceived by the students. 139 157 Table 5.12B reports nonresponse rates and statistics assessing the likely bias from such nonresponse. Rates of nonresponse were low for these items; however, those who omitted the item on safety indicated less safety when they responded at another point in time [t (base year)=-3.66, t (first follow-up)=-5.84, t (second follow-up)= -2.76, -1.64]; and those who omitted the fairness item at second follow-up had indicated a lower rating of fairness at the first follow-up [t=-3.64]. Examining various groups separately reveals that some types of students provide more stable reports than others. Table 5.12C summarizes the reports of students from various groups. Polychoric correlations indicate that ratings of teaching, teachers, and safety were somewhat more convergent across survey years for high SES respondents [six t's ranged from 5.42 to 7.91] and for high eighth grade reading score respondents [six t's ranged from 5.12 to 8.22] than for others. High SES students also generated more convergent ratings of class disruptions [t=4.86, 7.09]. Changes in the average ratings from an earlier survey to the second follow-up survey did not vary greatly among population subgroups, with four exceptions: declining ratings of school spirit after eighth grade were primarily among female respondents [t=-6.18] and among high eighth grade readers [t=-4.27]; ratings that teachers were interested in students increased after eighth grade primarily for high SES respondents [t=5.88]; and ratings of safety increased after eighth grade primarily for male respondents [t=5.91]. Finally, second follow-up nonresponse was about twice as great among male respondents [six t's ranged from 3.9 to 4.7] and among low eighth grade readers [six t's ranged from 3.3 to 5.7] as among other respondents. Given the nature of these data, it is impossible to know the extent to which these differences stem from differences in the reliability with which students from various backgrounds report school characteristics, or differences in the stability of the environments in which these students find 140 themselves. In any case, research that makes use of these student ratings of school climate must take into account the changes in these measures that occur over the 4 years following eighth grade. Although these ratings tap different constructs in the different surveys, their relations to educational outcome variables are fairly independent of the time at which they are asked. Generally, they are positively related to outcomes, as shown in Table 5.12D. The most notable exception is that while base year ratings of school spirit were positively related to six of the eight twelfth grade outcomes, later ratings of school spirit are positively related only to the self-concept and locus of control scores. This, coupled with the low convergence of school spirit ratings between base year and second follow-up (.18) and subgroup differences in changes in the mean ratings, indicate that school spirit ratings are only related to other survey measures in very indirect paths.  + + S+ + Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and First and Second Follow-up Student Surveys."}, {"section_title": "Crime at School", "text": "NELS:88 collected data on crime at school in the base year and the second follow-up. Two items were reasonably consistent across years. On the base year questionnaire, students were asked: Base Year 57. During the first semester of the current school year, how many times have any of the following things happened to you? A. I had something stolen from me at school. B. Someone offered to sell me drugs at school. In the first semester or term of the current school year, how many times did any of the following things happen to you? 142 160 A. I had something stolen from me at school. B. Someone offered to sell me drugs at school. On both items, students selected from three alternative responses: 0 = Never, 1 = Once or twice, 2 = More than twice. Table 5.13A reports the correspondence between base year and second follow-up reports to the two crime items. As the table makes clear, these responses are not very stable over time. Students report being less likely to have had something stolen during twelfth grade [t=-33.07], but more likely to have been offered drugs [t=26.98]. In addition, fairly low polychoric correlations suggest that these mean differences do not tell the whole story. The polychoric correlation between the base year and second follow-up is 0.23 for the question about theft, and 0.41 for the question about drugs. Because responses on these two items in most cases reflect students' experiences at two different schools, the middle or junior high school and the high school, expectations of stability over time are hard to establish. As shown in Table 5.13B, nonresponse on these items was below 3 percent. With such high response, the opportunity for nonresponse bias is small. Table 5.13B reveals no evidence of nonresponse bias.  Table 5.13C reports the convergence statistics separately for several subgroups. Examination of the polychoric correlations and average differences reveals that the increase in reports of offers of drugs at second follow-up occurred primarily among male students [t=-15.4].  Table 5.13D presents the results of the analysis of the association between students' base year and second follow-up reports and several key outcome measures. The table reveals the general negative relationship between these events and educational outcomes, but the relations to selfconcept depend on when the reports were given. Having something stolen is negatively related to self-concept, but only when reported at the second follow-up, and being offered drugs is negatively related to self-concept, but only when reported at the base year. In both cases, at the time reports of the crime were more frequent, there was no significant relation to self-concept. "}, {"section_title": "S-B-B-", "text": "Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Student Surveys."}, {"section_title": "Student Participation in Special Programs", "text": "Students were asked during both the base year and the second follow-up about their participation in special academic programs. These items were not identical on the two questionnaires. Students were asked two separate questions during the base year: Base Year 68. Are you enrolled in any of the following special programs/services? Have you ever been in any of the following kinds of courses or programs in high school? D."}, {"section_title": "English as a Second Language program E.", "text": "Advanced placement program J."}, {"section_title": "A program for the gifted and talented", "text": "Responses are coded on the NELS:88 CD-ROM data file as 1, for reported participation, and 2, for reported nonparticipation, but for the present analyses, nonparticipation was recoded as 0. Analysis of correspondence between these items is problematic on several accounts. First of all, the items asking about enrollment in advanced courses are very dissimilar, and therefore do not necessarily indicate comparable academic programs during each of the two years: for purposes of the present analysis, a student's affirmative response to any of the four items in question 66 was considered a \"yes\" and compared to students' high school enrollment in an advanced placement program (item 13E). Second, the base year questionnaire asks students about their enrollment in special programs during the school year when the survey was administered. The second follow-up questionnaire, however, asks students to report their participation in special programs during any of their high school years. Finally, there may be real changes in participation, and the extent to which the instability follows from real change versus poor measurement is not clear. As with many of the other items in this chapter, this item, reported accurately, should change over time. As students make their way through high school, they encounter additional opportunities to enter a special program. Also, students need only to enroll in such a program during one year of high school to respond affirmatively to these items at the second follow-up; therefore, it would be expected that more students would respond affirmatively to the second follow-up items than to the similar base year items. Similarly, students enrolled in an English as a second language program during the base year may no longer require the services of such a program in subsequent years. (Also, it seems likely that, for the most part, students not enrolled in an ESL program during the base year will not be enrolled in such a program during high school.) Nevertheless, whether events are changing or there is error in measurement, lack of convergence of responses calls for different interpretations of the measures if taken at different times. Looking at Table 5.14A, there appears to be little convergence for reports of English as a second language instruction, but there is substantial stability concerning participation in a gifted and talented program. The high frequency of positive responses to the item about AP courses is curious, since about 7 percent of students nationally participate in such courses. In NELS:88, however, 41 percent of students in the second follow-up reported participation in AP courses (the weighted percentage was 37 percent). It is possible that some students did not understand the question, perhaps responding positively if they had been placed in any higher-level class, such as algebra. Clearly, researchers should be cautious in their use of this item.  Table 5.14B presents nonresponse statistics for these items. With the exception of the ESL items, nonresponse remained below 5 percent, leaving little opportunity for nonresponse bias. The ESL item had 5.6 percent nonresponse in the base year. Among these nonrespondents were a disproportionate number who later reported participation (in the second follow-up) [t=3.90]. Base year nonrespondents to the other two items were less likely to have reported participation advanced placement [t= -4.97] or a gifted and talented program [t=-3.77] at the second follow-up than were base year respondents.   t=12.5, 16.0] provided more consistent responses than other students to the accelerated course/advanced placement and gifted and talented items. On the other hand, low SES students provided more consistent responses to the ESL items. Generally, higher polychoric correlations were obtained for subpopulations more likely to participate in the program. Similarly, the decline from participation in accelerated courses in the base year to participation in advanced placement in high school was primarily among low SES [t=14.79] and low eighth grade reading students [t= 22.80]. These students, along with males, were also more likely to omit the item about advanced placement at the second follow-up [t=3.2, 4.4, 8.4]. Participation in advanced placement and gifted and talented programs is positively related to educational outcomes, and participation in an ESL program is negatively related to five or six of the outcomes. Table 5.14D reveals only one difference in the significance of the associations of program participation with educational outcomes that depends on the year of report. That exception is that reporting participation in an ESL program is not related to educational expectations at the base year but is at the second follow-up. This result makes sense, as students participating in ESL in the eighth grade may not have felt limited in their educational expectations, while twelfth grade ESL participants may have felt more limited.  Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Student Surveys."}, {"section_title": "147", "text": "165 Homework There can be no question that doing homework increases the impact of schooling on a student's achievement, but in modeling educational development, can one characterize a student as a homework completer, or is doing homework a characteristic that varies from course to course and year to year? Students were asked on both the base year and second follow-up questionnaires about the frequency with which they complete their homework and about the amount of time they spend working on homework each week. The items were similar on the two questionnaires. On the base year questionnaire, students were asked: Base Year 78. How often do you come to class and find yourself WITHOUT these things? C. Your homework done (when assigned) 79. In the following subjects, about how much time do you spend on homework How often do you come to class WITHOUT these things? C. Your homework done"}, {"section_title": "25.", "text": "In the following subjects and overall, about how much time do you spend on homework EACH WEEK, both in and out of school? A. Time spent on history/social studies each week The item about attending class without homework completed was scored from 1 to 4 for usually, often, seldom, and never. The questions about time on homework each week had different response options for base year and second follow-up, but they were collapsed to the same six categories for these analyses, scored 0 to 5: none, less than 1 hour, 1 to 3 hours, 4 to 6 hours, 7 to 9 hours, and 10 or more hours. Students who reported an additional category, \"not taking ... ,\" were omitted from the analysis for that type of course. It seems reasonable to expect that, in general, students who reported during the base year that they were regularly well-prepared for class (i.e., came to class with their homework completed) and that they spent a fair amount of time on their homework would continue to report these types of habits during the second follow-up. However, it is also reasonable to expect changes over time on these items, and in particular on the items asking about time spent completing homework. Students may simply report that they spend more time on homework during the second follow-up because they are asked to do more homework as high school students than they were asked to do in the eighth grade. Table 5.15A presents statistics about the correspondence between student reports at these two time points. These figures reveal little convergence between eighth grade reports and twelfth grade reports. Not only did the amount of time spent on homework increase substantially from base year to second follow-up [t=18.6, 30.4, 45.4, 22.6], but there was little communality between which students spent a great deal of time on homework at eighth grade and which spent a great deal of time at twelfth grade, as indicated by the range of polychoric correlations from .18 to .23. This lack of convergence was also apparent for the frequency with which students reported attending class without homework completed: the polychoric correlation was only .27. Also to be noted in Table  5,.16A is that although time spent on homework increased, the average response to the item about having completed homework before class declined from eighth grade to twelfth grade [t.-4.23], possibly because teachers' expectations for homework increased over the period. Again, it is hard to know what to make of the instability of this measure. Students' homework habits probably change as they progress through school. However, it is difficult to believe that the students who do the most mathematics homework in the eighth grade are not heavily represented among those who do the most homework in their senior year (among those taking mathematics or science). Cohen et al. (1996) reviewed the reliability of the homework measures and concluded that most of the variance in this item did not reflect actual time spent doing homework. As shown in Table 5.15B, about 5 percent of students omitted these items, with the exception that only 2 percent of twelfth graders omitted the item about attending class without completing homework. Generally, students who omitted the items tended to respond at the other survey point in time indicating fewer homework hours than other students [t (base year math)=-2.83, t(second follow-up).-4.10, -3.44, -2.80, 2.16]. Increases in amount of time spent on homework were also greatest for high SES students [t=3.79, 6.89, 5.95, 2.80], for female students [t=8.31, 4.11, 7.76, 2.38], and in science and English, for students with high eighth grade reading scores [t=5.11, 6.42]. Therefore, the finding that the decline in homework completion between eighth and twelfth grades was limited to high SES students [t= -7.74] and students with high eighth grade reading scores [t=-10.47] suggests that greater demands for homework are being made on these groups. Finally, most of the students who omitted the items on hours spent on homework each week at the second follow-up had low eighth grade reading scores [t's ranged from 13.1 to 13.7], were from low SES households [t's ranged from 8.0 to 9.0], and were male [t's ranged from 4.2 to 5.8]. "}, {"section_title": "150", "text": "As illustrated in Table 5.15D, the items asking about students' habits of coming to class with their homework complete is significantly associated, for both base year and second follow-up reports, with the key outcome measures: students who come to class relatively often without having completed their homework exhibit lower scores on the outcome measures than do students who come to class with their completed homework. In this sense, the base year and second follow-up items are in agreement. The four measures of time spent on homework, however, are not as consistently associated with the key outcomes for the two points in time. The amounts of time students report spending on math homework and English homework are consistently associated with the key outcomes (only one outcome measurestudent's self-conceptis significantly associated with time spent on math and English homework at second follow-up but not at the base year). For time spent on science homework, however, there is a significant association between students' second follow-up reports and all of the key outcomes, but only one significant association between base year reports and an outcome (the locus of control measure). On the other hand, time spent on social studies and history homework in eighth grade is associated with high school grade point average, curricular difficulty, and twelfth grade math scores, but time spent in twelfth grade is not. Clearly, the meanings of responses to the homework items are complex, and research to assess the impact of homework on differential educational attainment must take into account the specific courses the students are taking and teachers' indications of homework loads, as well as tendencies of students to over-or under-state their homework. Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Student Surveys."}, {"section_title": "Cutting Classes", "text": "Students were asked about the frequency with which they cut class on both the base year and second follow-up surveys. The two items were similar but not identical. On the base year survey, students were asked: How many times did the following things happen to you in the first semester or term of the current school year? B. I cut or skipped classes. On the base year survey, students could respond \"never/almost never\", \"less than once a week\", \"at least once a week\", and \"daily\". On the second follow-up survey, students were asked to report the number of times during the semester they had cut class: never, 1-2 times, 3-6 times, 7-9 times, 10-15 times, and over 15 times. For this analysis, the correspondence of students' base year and second follow-up reports was examined for those students who said that they \"never\" cut class and for those students who reported any other frequency of cutting class. Table 5.16A presents statistics about the correspondence between base year and second follow-up reports about whether students cut or skipped classes. As with many of the other items in this chapter, the meaning of stability or instability over time is not clear cut. We would expect that students who cut classes in the eighth grade would be more likely than others to skip school in their senior year, but we cannot specify a priori how much more likely we would expect them to be.  Table 5.16A reveals a polychoric correlation of about 0.37, with about 73 percent of students providing the same report in the base year and second follow-up. Students were much more likely to report skipping class at the second follow-up [t=51.5] than in the base year. Nonresponse does not pose a significant problem for this item, with overall nonresponse at 4 percent for the base year and 2 percent for the second follow-up. However, those who omitted the item at second follow-up were more likely to have reported skipping class in the eighth grade than were second follow-up respondents [t=3.59]. The second follow-up omission bias of .06 indicates that 13.3 percent of nonrespondents had indicated earlier that they had cut class frequently, compared to 7.8 percent of respondents. This is a case in which the differences in distribution between base year and second follow-up are so large that the measurement of omission bias based on the other year underestimates the likely bias.  Table 5.16C, which reports the correspondence statistics separately for subgroups, reveals only minor cross-group differences in the correspondence of reports over time. Convergence is slightly higher for female students [i=3.13], but the polychoric correlations were all in the low range. Reports of frequency of cutting class appear to be similarly distributed across subpopulations in eighth grade and twelfth grade.  Table 5.16D reveals no difference between the associations with students' base year and second follow-up reports of their class cutting habits and the key outcomes: for student reports at both points in time, there is a positive and significant association between reporting that one \"almost never\" cuts class and the outcome measures. Overall, although convergence is not high, both eighth and twelfth grade responses concerning cutting class have similar relations to other variables, including both subpopulation indicators and outcomes. Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Student Surveys."}, {"section_title": "153", "text": ""}, {"section_title": "171", "text": "BEST COPY AVM BLE"}, {"section_title": "Students' Preparedness for Class", "text": "Students were asked to report how often they come to class unprepared. The questions were similar on the base year and second follow-up questionnaires."}, {"section_title": "Base Year", "text": "\n"}, {"section_title": "78.", "text": "How often do you come to class and find yourself WITHOUT these things? A. Pencil or paper (when needed) B."}, {"section_title": "Books (when needed)", "text": "Second follow-up 24. How often do you come to class WITHOUT these things? A. Pencil or paper B. Books? (Note that these items were asked in conjunction with the items about homework completion, examined in an earlier section.) Students responded to these items on a four-point scale, ranging from (1) usually, (2) often, (3) seldom, and (4) never. Table 5.17A reports the correspondence between base year and second follow-up reports about the two items about preparedness for class. As with many of the other items examined in this chapter, these items might reasonably be expected to change over time. In the aggregate, students report attending class without a pencil or paper less frequently at the time of the second follow-up than in the base year [t=40.5]. This is not the case for attending class without books: the average responses in both the base year and second follow-up indicate that students almost never attended class without books. The low polychoric correlation for the item about books (0.22) suggests that this item may be more related to particular course requirements than habits of individual students. Base year nonresponse to these items was slightly greater than second-follow-up nonresponse [t=-12.00,-16.01] (see Table 5.17B). Even in the base year, nonresponse was just over 5 percent overall. Examination of the omission bias statistics reveals no likely substantial bias from nonresponse.  Table 5.17C presents the correspondence statistics separately for various subgroups. This table reveals a few cross-group differences. Although still low, the polychoric correlations for reports of attending class without paper and pencil were somewhat higher for high SES students [t=5.10] and students with high eighth grade reading scores [t=4.33]. The decrease that occurred between eighth and twelfth grades in reporting attending class without paper and pencils was much more pronounced for female students than for male students [t=7.67]. Finally, there were sex [t=3.9, 4.3] and reading level [t=5.1, 5.5] differences in the tendency to omit these items at the second follow-up.  Table 5.17D illustrates that, for the most part, student reports on the two class preparedness items are consistently associated with key outcomes across the two points in time. A few differences appear, however. For example, students' propensity to come to class during the base year period with pencil and paper is not significantly associated with their math score on the second follow-up, while their second follow-up preparedness on this item is. On the other hand, the base year preparedness item is associated with students' occupational expectations, curricular difficulty, and enrollment status while the second follow-up item is not. It is unclear why this would be the case. Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center' for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Student Surveys."}, {"section_title": "Participation in Extracurricular Activities", "text": "Students were asked about their participation in extracurricular activities on both the base year and second follow-up questionnaires. Many of the items were very similar on the two surveys."}, {"section_title": "82.", "text": "Have you or will you have participated in any of the following school activities during the current school year, either as a member, or as an officer (for example, vice-president, coordinator, team captain)? C. A team sport (baseball, basketball, football, soccer, hockey, etc.) C. Cheerleading, pompon, drill team C."}, {"section_title": "Student government D.", "text": "National Honor Society, other academic honor society I. Future Teachers of America, Future Homemakers of America, Future Farmers of America or other vocational education or professional clubs On both questionnaires, students were asked to identify if they (1) participated, (2) participated as an officer, captain, etc., or (3) did not participate. For the present analysis, these were scored on a 0-to-2 scale: (not participate, participate, lead). Again, as with most of the topics considered in this section, one might expect students' participation in extracurricular activities to change over time, especially between the eighth and twelfth grades, and especially when they move from junior high school to high school where, in many cases, more activities will be available to them. Stability of student reports on these items may better reflect students' continued interest or lack of interest in particular types of activities than it reflects the reliability or unreliability of students' reports on these items. (In some cases it may also reflect the unavailability of certain extracurricular activities. However, as only the second follow-up questionnaire allowed students to respond that their school did not offer a particular activity, this factor was not incorporated into the analysis.) Table 5.18A presents the correspondence statistics for these items. A couple of items stand out here. First, participation in cheerleading has a high polychoric correlation relative to the other items-0.53. Second, students' participation in almost all of the activities under consideration, with the exception of sports, increased between the base year and the second follow-up [t=-22.72, -17.12, 14.79, 22.55, 10.19, 16.15, 38.18, respectively]. Again, this may result from the availability of activities at the high school level that were not available at the junior high school level.  Table 5.18B reports nonresponse rates and statistics for these items. Overall nonresponse rates on these items were similar at the two time points; nonresponse ranged from 7.4 percent to 9.2 percent on the base year items and from 6.9 to 9.4 percent on the second follow-up items, levels that are somewhat higher than for other items concerning school-related behavior. Nonresponse on the base year item on participation in academic honor societies seems to introduce significant bias [t=-5.94] in that those who omitted the item tended to report not participating at the time of the second follow-up.   Table 5.18C presents the correspondence statistics for key subgroups. For the most part, the polychoric correlations are not appreciably different between subgroups. However, the polychoric correlation for cheerleading is substantially higher for female students than for male students [t=10.2], indicating that for females more than males, participation in cheerleading is likely to be characteristic of the entire period if at all. Increases in participation in drama, student government, and honor societies from eighth to twelfth grade were greater for high SES students [t=6.0, 4.3, 9.0] and high eighth grade readers [t=7.2, 6.8, 6.4]; but increases in participation in career clubs were lower for these groups [t, 9.2, -4.7]. Table 5.18D illustrates significant associations between base year and second follow-up reports of participation in extracurricular activities and key outcome measures. With some exceptions, such as academic honor societies, base year and second follow-up reports of participation in particular extracurricular activities are not consistently associated with the same outcome measures. For example, participation in drama is a positive indicator, but only at the twelfth grade level; participation in intramural sports in eighth grade, but not in twelfth grade, is a positive indicator of twelfth grade math scores, grade point average, and locus of control; and participation in career clubs is associated with lower educational and occupational aspirations, but only when measured at the second follow-up. "}, {"section_title": "S-S-B-B-", "text": "Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \". .\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Student Surveys."}, {"section_title": "169", "text": "Subpopulation Comparisons. Throughout the comparison of responses across the teenagers' ages, there was a prevailing tendency for teenagers who had obtained high reading scores in eighth grade, teenagers who had high SES scores, and girls to provide more \"reliable\" responses over time. Among 108 comparisons (excluding ties) between high and low readers in Tables 5.21 and  5.22, for all but four measures, the computed estimate of the polychoric correlation was greater for high readers than for low readers.18 For most of the items involved in the locus of control and selfconcept scales, as well as for most of the quality of life \"importance\" and \"chances\" measures, the differences in polychoric correlations between high and low ability readers were greater than .10. Whether the lives of students with low eighth grade reading scores were changing more during these years, or whether more of them were learning new ways of interpreting the questionnaire items during high school, researchers should consider these items and scales to be more stable for high ability readers in this age range than for low ability readers. Similarly, of 104 comparisons of estimated polychoric correlations between male and female respondents and 111 comparisons between high and low SES respondents (again, excluding ties), all but 12 and 19, respectively, were greater for girls' responses and for high SES teenagers. Because a multivariate analysis was not run, it is impossible to rule out that these are merely results of the correlation of reading ability with sex and SES in this age range. Subpopulation differences in the mean response changes from base year (or first follow-up) to second follow-up are shown in Tables 5.23 and 5.24. Overall, there is no consistent pattern of larger changes for one subpopulation or another. The homework items exhibited the most differential changestudents from high SES households, with high eighth grade reading scores, and girls tended to increase their reports of homework time from base year to second follow-up more than other respondents. One other item exhibited a large difference that was probably associated with a misinterpretation of the item in the eighth grade: a substantial percentage of eighth graders with low reading scores responded that they were in an advanced placement program in eighth grade but changed their response at twelfth grade. There were consistent subpopulation differences in the tendency to omit items, as shown in Tables 5.25 and 5.26. At the second follow-up, for every measure, low eighth grade readers more frequently failed to respond and boys more frequently failed to respond. The sex difference was small, but the reading ability difference was substantial. The median omission rate across measures for high readers was between 6 and 7 percent, compared to a median between 10 and 11 percent for low readers. The differential was greatest for measures that had a high omission rate even for high readers, a pattern that suggests that measures might be characterized by a \"difficulty of responding,\" similar to the difficulty parameter of open-ended cognitive items."}, {"section_title": "BEST COPY MAILABLE", "text": ""}, {"section_title": "197", "text": "Relations to Outcomes. The results of significance tests of the relations between the measures at two different times and outcome measures are summarized in Tables 5.27 and 5.28. Although there were several dozen specific discrepancies in results that would be obtained using earlier and later responses, the major finding is that in 85 percent of the cases (760 out of 888), the conclusions would be the same. In 700 of those cases, the results would be significant, if no Bonferroni adjustment to the two-tailed .05 significance level were required, given the researcher's rationale for conducting the test. There were 69 cases in which the relationship based on the earlier measure would be significant but based on the later measure would not. Of these, nearly one-third (21) were associated with the enrollment status indicator, which has a very limited range at the second follow-up, especially for items pertaining to activities in school during the year of the second follow-up. Thirteen of the remaining 48 involve the four self-ratings of English language ability by teenagers who are not native English speakers, suggesting that these items have greater construct validity when measured at eighth grade than later. Conversely, there were 56 cases in which the relationship based on the later (second followup) measure would be found significant but the relationship based on the earlier measure would not. This is a notably small number of cases because the outcome measures are all based on second follow-up dataa simple model would suggest that they would therefore be more likely to be significantly related to second follow-up measures than earlier measures would be. Of these 56, 13 are with the prestige of the expected occupation and 11 are with the self-concept measure. Thus, it may be that these two outcomes are slightly more closely related to other measures at the time of the second follow-up than to measures from earlier survey waves. To summarize, the convergence of the NELS:88 noncognitive measures across two points in time is generally modest, suggesting that valuable information was obtained by asking these items at two time points. Item omissions increased from wave to wave and followed distinct patterns. Teenagers with low eighth grade reading scores, and to a lesser extent low SES teenagers and boys, responded with lower levels of convergence and higher omission rates than other NELS:88 participants. Finally, in most cases, the choice of time point for measurement would not change conclusions about the significance of relations of these measures to educational outcome measures.    "}, {"section_title": "F-", "text": "Note: \"+\" and \"-\" refer to the direction of relation, B,F, and S indicate that the relation is significant only for data from that source (base year, first follow-up, or second follow-up), and \"..\" indicates that no significant relation was found. (--) refers to comparisons that are generally meaningless, because the predictor and outcome are the same measure. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and First and Second Follow-up Student Surveys. Note: \"+\" and \"-\" refer to the direction of relation, B and S indicate that the relation is significant only for data from that source (base year or second follow-up), and \"..\" indicates that no significant relation was found. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and First and Second Follow-up Student Surveys."}, {"section_title": "200", "text": ""}, {"section_title": "201", "text": "Chapter 6 Summary and Conclusions Data from the base year (eighth grade), first follow-up (two years later), and second followup (two more years later) of the NELS:88 cohort of eighth graders in 1988 can provide a rich picture of the cognitive and affective growth of teenagers from eighth grade to twelfth grade. Testing research hypotheses depends, however, on interpretations of descriptions and ratings by teenagers, their parents, and their teachers. This report addressed questions about discrepancies of answers between different sources of information about a teenager and about differential tendencies to omit survey items. This report focuses on 64 pairs of measures from parents and teenagers, 12 from teachers and teenagers, and 112 from teenagers taken in two separate survey waves (base year and second followup or first and second follow-ups). It addresses four research questions: 1. How similar are the response distributions for an item from different sources? 2. How extensive is item omission and what nonresponse bias is there? 3. How do pair-convergence and item omission differ among subpopulations?"}, {"section_title": "4.", "text": "What difference does the source of information make on conclusions about impact on student outcomes? Results relevant to these research questions were presented separately for the various pairs of measures. However, the set of measures is sufficiently large to examine overall patterns among types of items, to determine which types of items are most sensitive to variations in the source of the information. This chapter briefly explores these patterns. Although the 188 measure pairs studied do not represent a random sample from a well-defined domain, they represent a broad cross-section of the kinds of survey items that might be asked in studies of teenagers and high school students. Therefore, the patterns found in this uniquely rich dataset are presented as guidance to researchers. The pairs of measures differed on a variety of dimensions. Those considered in accounting for variations in convergence and percentage omissions in this chapter are the following: Source-pair: Parent versus student, teacher versus student, or student in earlier wave versus student at second follow-up Content: School-centered items versus home, family, and personal centered items Wave: For parent versus student, base year or second follow-up; and for earlier student versus second follow-up student, whether the earlier wave is base year or first follow-up; teacher-student comparisons were all at second follow-up 185 202 A few items asked about behaviors and events that might be embarrassing or frightening for some participants to acknowledge (e.g., drug use, suspensions) The definitions of the dimensions of objectivity and sensitivity are themselves subjective, based on the first author's judgment, but the other dimensions are apparent from the items. Although a thorough cognitive analysis of the processes involved in responding to the items would add to this analysis, that is beyond the scope of this report. Is convergence across time greater or less than convergence between parent and teenager? The distributions of polychoric correlations for parent-teenager item pairs (see Chapter 3, Table 3.14) and teenager item pairs over time (see Chapter 5, Tables 5.21 and 5.22) are shown in Figure 6.1. There is a great deal of overlap between these distributions, but possibly with greater variability (standard deviation of .24 versus .14) for the parent-teenager pairs. The sets of items included in the two sets of pairs differ, of course, so interpretation of this comparison is not simple. For example, the seven parent-teenager correlations greater than .8 include base year objective items, such as the number of siblings, which were not repeated at the second follow-up. Items asked at both base year and second follow-up undoubtedly include many on which change was anticipated and few for which no change was expected. The general pattern is that for the kinds of survey items used by NELS:88, discrepancies between parent and student responses are in the same range as discrepancies between teenagers' responses from eighth grade to twelfth grade. Is parent-teenager convergence greater at the base year or second follow-up? The distributions of polychoric correlations shown in Figure 6.2 suggest either (a) that there is a dramatic dropoff in parent-teenager convergence from eighth grade to twelfth grade (from a mean of .68 to .37) or (b) that NELS:88 included more items at the second follow-up on which parent-teenager disagreement was likely. Only three parent-teenager item pairs were very similar across the surveys. On these items, there was a much smaller dropoff: from .45 to .38 for expected education level (see discussion of Table 3.10A in Chapter 3) , from .74 to about .55 for ratings of how often English was used in multilingual homes (see Table 3.3A), and from .48 to .45 for ratings of the teenager's safety 186  at school (see Table 3.6A). Of these three items, only the safety item was virtually unchanged in wording between base year and second follow-up surveys. Does the length of time interval affect convergence of items? Second follow-up items were compared to both base year and first follow-up items (see Chapter 5), and as shown in Figure 6.3, the convergence from base year to second follow-up tended to be much lower than for first follow-up to second follow-up. This is partially confounded by the fact that many of the items address aspects of school and school activities: nearly all students changed schools between eighth and twelfth grades, but most did not change schools between tenth and twelfth grades. Thus, questions about school were generally about the same school in tenth to twelfth grade comparisons and about different schools in eighth to twelfth grade comparisons. (In Chapter 5, items about home, family, and personal characteristics are considered in discussions of Tables 5.1A 5.10A, and items about school are presented in Tables 5.11A 5.18A.) The mean polychoric correlations were: for base year/second follow-up pairs concerning school, .33, and not concerning school, .40; and for first follow-up/second follow-up pairs concerning school, .38, and not concerning school, .45. That is, doubling the time interval reduced the polychoric correlations on average by the same amount, .05, for both school-related item pairs and home, family, and personal item pairs. Does reading ability contribute to convergence? The subpopulation comparison tables in Chapters 3,4,and 5 (see Tables 3.14,4.5,5.21,and 5.22) all showed differences between teenagers with high and low eighth grade reading scores. As shown in the plot of polychoric correlations for high and low reading eighth graders in Figure 6.4, the effect of reading ability on convergence occurred for nearly every item, although in some cases the effect was more substantial than others. The average difference in polychoric correlations between high and low eighth grade readers was .08, with values ranging from -.09 to .23 across all items. There are at least two explanations for variations in the sizes of differences between high and low readers. It may be that some items are more difficult to read, so that poorer readers tend to make discrepant responses because they don't understand the items. If so, then this difference can be used as an indirect measure of the reading difficulty of items, because one of the primary reasons for lack of convergence is misinterpretation of the item by either or both of the sources of information. Of course, another cause of the reading ability association, at least for some items, can be real differences in the sources of information: low reading students may change more on some measures between eighth and twelfth grades than other students and they may have greater differences of perspective relative to parents and teachers than other students do. How often does the choice of items make a difference in the conclusions reached from research analyses? The significance tables in Chapters 3,4,and 5 (see Tables 3.17,4.8,5.27,and 5.28) show quite a number of comparisons that would be significant based on one source but not on another. Overall, about 19 percent of the hypothetical significance tests had different outcomes . depending on the source of the information about the comparison factor. The lowest percentage for a particular outcome (13 percent) was for twelfth grade mathematics scores, because these were most strongly related to the factors being tested of any of the outcomes analyzed. That is, for this outcome, the attenuation of the relationship from neither source was sufficient to hide the overall significant relation. The highest percentage (26 percent) was for the prestige coding of the expected occupation at age 30,19 a measure more indirectly related to the comparison factors and therefore more likely hidden by any attenuation. In parent-student comparisons, other than for the enrollment status outcome (see footnote 19), there was a noticeable tendency for more frequent significant results based on student rather than on parent information (78 comparisons were significant only when based on the student information; only 36 comparisons were significant only when based on parent information; while 319 comparisons were significant based on either source)."}, {"section_title": "Meta-analysis of Polychoric Correlations", "text": "In order to sort out the various factors that affect convergence and tendency to omit an item, one can evaluate the fit of multifactor item models to the item data through multiple linear regression, using predictors based on item dimensions. The results of these meta-analyses indicate which factors are effective in changing an item's convergence and which are merely correlated through association with other factors. Moreover, examination of items that are most deviant from the model can provide clearer pictures of which items are either more or less convergent than would be predicted by a simple model. The purpose of this meta-analysis is to provide insights into the reasons that some items have greater stability than others or lower omission rates than others. The multiple regressions that are reported below are not intended to be used to predict the convergence or omission rates of other items, only to summarize and clarify information on 187 different item pairs. For 187 polychoric correlations, a model with six predictors was fit, with an adjusted r2 of .39. That model was reduced from an initial model with eleven predictors, due to elimination of insignificant measures. After omitting 22 poorly fitting items, that is, outliers from the regression line, which are discussed below, the following model was fit, with an adjusted r2 of .68. The mean, standard deviation, and correlation of each predictor with the dependent variable are shown in parentheses. All of the regression weights included in the model were statistically significant. 19 The percentage of changes for enrollment (dropout) status, 29 percent, must be considered artifactual because of restriction of its range for many second follow-up comparison factors which assumed the teenager to be in school.  (mean=.14, sdev=.16, r=-.38) Based on the representations of predictors in the model, the excluded group of item-pairs, whose average polychoric correlation is indicated by the constant intercept term (.722), consists of base year home-related parent-student items that are not subjective ratings and for which the average response was the same for parents and students. The regression weights indicate the effect of the corresponding predictors on the average polychoric correlation. The first three factors reflect the differences in correlations between the over-time pairs and the pairs of sources at the second follow-up (teacher and student or parent and student). These negative coefficients can be summarized as indicating that the base year parent-student item pairs had the greatest convergence. The next two factors indicate that there was greater agreement on objective reports, as opposed to subjective ratings and on items about home, as compared to school. Separating items involving sensitive behaviors, such as drug use, as a factor in the model did not contribute significantly to the prediction of polychoric correlations. The final measure is indirect. The negative coefficient indicates that the polychoric correlations were lower for item pairs in which the different sources of information also differed in their overall perceptions. For example, in parent-student comparisons, there were lower polychoric correlations when the parents and students had larger mean differences. Such a relation would be expected to occur as an artifact if the simple product moment correlations were compared. However, because convergence was measured in terms of polychoric correlations, this represents a substantive relation. The value of the coefficient of this factor must be interpreted in the context of typical values for the mean change. The average absolute difference (by which the coefficient would be multiplied) was about .14, across all items. Notably, several factors were not included in this equation, because they had no strong relation to the polychoric correlations. One was the item number of the item in the student (or second follow-up student) questionnaire. Also, separate explicit indicators of teacher-student pairs and of parent-student pairs did not contribute significantly. Finally, the difference in convergence between students with high and low eighth grade reading scores, was not included, even though it would have made a significant contribution to the prediction [t=5.14], because the sign of the coefficient (positive) was only explainable as a statistical artifact. An examination of the items with large differences in polychoric correlations between high and low ability readers did not reveal any surface readability difficulties; and in some cases it became apparent that the reasons for differences probably reflected different characteristics of the respondents. For example, the item on discussions of 18 college choice factors had some of the highest and lowest differences between high and low ability readers, and the pattern of differences does not appear to be related to reading difficulties of the factor descriptions (compare tables 3.11A and 3.11C). In any case, it does not now appear appropriate to use subpopulation differences in polychoric correlations to predict the overall polychoric correlation. Of most interest are the items that did not fit this model. Thirteen items were from the parent-student comparisons. Seven items had higher convergence than expected from the model: whether the student was currently enrolled and/or had been suspended, which apparently were not \"sensitive,\" as they had been denoted in defining the factors for the meta-analysis (see the discussion of Table 3.9A in Chapter 3); expected education level in the base year (see Table 3.10A); expected occupation (prestige) in the second follow-up (see Table 3.11A); and three college choice factors (see Table 3.12A). The latter five items had been denoted as subjective ratings for the meta-analysis, but there appears to have been more parent-student agreement on these items than was typically the case for subjective ratings. Ability to live at home, availability of financial aid, and a religious environment are likely to have been topics for family discussion for teenagers planning to go to college by the time of the second follow-up. Six other parent-student items had lower convergence than expected from the model: the rating of whether the school was safe (see Table 3.6), the indicators that an adult neighbor, a sitter, or nobody was home when the teenager returned from school (see Table 3.5), the estimate of when the student stopped his or her most recent job (see Table 3.2), and the perceived importance of location in choosing a college (see Table 3.11). Each of these items had a unique problem. The safety-at-school item was stated as a negative rating for the teenager (\"Do you agree that you don't feel safe at school?\"). The question about an adult neighbor or a sitter at home could have been construed as only the adult neighbor or sitter by some respondents and as an adult neighbor visiting the parent by others, or the adult neighbor could have been considered by some to have been a sitter. The report of nobody at home may have appeared to be a socially undesirable response. The estimate of when the student stopped his or her most recent job presumed that the second follow-up questionnaires were completed by parents and students in the same month (because parents were instructed to respond with the \"current\" month if the student had a job) and that they had the same frame of reference for what was a job. Finally, the meaning of \"location\" as a college choice factor was ambiguous, especially as the 17th factor in a list in which earlier items included \"able to live at home\", \"chance to live away from home\", and \"low crime environment.\" These items should be used only with great caution."}, {"section_title": "194", "text": "Teachers' reports of whether a student's native language was English (see the discussion of Table 4.1A in Chapter 4) was noticeably more convergent with students' reports than predicted by the model, possibly because of cues available to the teacher, such as a student's accent, or surname, or direct conversations with parents, not represented in the model. Also, math and science teachers' reports that the student was in a college preparatory track math or science course were more convergent than predicted from the model, possibly because tracking limited opportunities for divergent responses. Six items had greater stability across student surveys than predicted by the model: whether the teenager was a non-native English speaker (base year to second follow-up; see the discussion of Table 5.2A in Chapter 5) , whether he or she was in a college prep program (base year to second follow-up; see Table 5.11A), enrolled in a gifted and talented program (base year to second followup; see Table 5.14), and expected to go to college (first to second follow-up; see Table 5.7A), and whether he or she was a religiously oriented person who attended religious services frequently (base year to second follow-up; see Table 5.3A). These are characteristics of teenagers that may have been determined or planned for by the time he or she reached eighth or tenth grade. Finally, one item had notably less stability between base year and second follow-up than predicted by the model: whether the student received special help for English language proficiency (see Table 5.2A). In the base year, it referred to ever having been enrolled in a program, and at second follow-up, it referred to having help with reading and writing since 1989. Overall, this analysis leads in some cases to recommendations concerning factors to take into account in the future wording of survey items and in other cases to a greater understanding of the meaning of the responses made by parents, teachers, and students."}, {"section_title": "Meta-analysis of Percentages of Omissions", "text": "A similar analysis was carried out for percentage of omissions by students for comparison with parents or teachers, or by students at the second follow-up. Although the same variables were used in the equation, different factors were found to be significant. Initially, the equation was fit to the percent missing, with an r2 of .54. However, after deleting 10 items with large deviations from the regression line, the r2 squared for the following equation was .76. Means, standard deviations, and correlations with the dependent variable are shown in parentheses. percent missing = 0.43 +1.46, for every 10 serial positions in the survey instrument, (mean=52.5/10, sdev=30.6/10, r=.86) 1.25, for second follow-up items asked of both students and parents, (mean=.29, r=.46, r=.05 (n.s.)) -1.66, for second follow-up items also asked at first follow-up (mean=.28, sdev=.45, r=-.06 (n.s.))"}, {"section_title": "212", "text": "The serial position of the item in the questionnaire, which for the items analyzed varied from 2 to 111, was highly significantly related to tendency to omit. Other things equal, a typical item at the end of the questionnaire would have a likelihood of omission of more than 15 percent, compared to less than 5 percent in the early parts of the questionnaire. Also, other things equal, there were fewer omissions at second follow-up than predicted by the model, for items also asked (a) of parents or (b) at the first follow-up. Other factors, including the reading difficulty estimate, the absolute mean change estimate, the school/home content dichotomy, and the objective/subjective indicators, which were significant in the prediction of polychoric correlations, were not included in this final equation because the regression coefficients associated with these factors were all nonsignificant. The 10 items deleted from the final analysis because they did not fit the model were associated with either implicit or explicit skip patterns. Three second follow-up items presented near the end of the questionnaire had lower than predicted omission rates: indicating one was not a native English speaker, indicating which other language was used in the home, and rating how frequently that language was used in the home (see Table 3.3A). On the other hand, seven items had higher than predicted omission rates. These included four items relating to parents' perspectives on education and three items that were to be answered only by a relatively small subset of the NELS:88 participants. Specifically, reports of father's and mother's education levels (see Table 3.4A) and perceptions of father's and mother's educational aspirations for the teenager's education (see Table 3.10A) had higher than predicted omissions; and the year and month the teenager stopped the most recent job (see Table 3.2A), which were only asked for teenagers who had had a job but were not employed at the time they completed the second follow-up survey, and self-ratings of school performance limits due to English language proficiency problems, which were answered only by non-native english speakers (see Table 5.2A). In conclusion, the evidence from comparing measures from different sources supports the usefulness of gathering multiple measures. In nearly every case, the alternative sources provided either slightly different or very different information. On the other hand, in some cases the comparisons were confounded by changes in wording between items presented to different sources. Matching wordings (e.g., response alternatives for levels of education) more closely would allow researchers to make further inferences about differences that are not due to wording variations. The analyses conducted for this report provide the basis for recommending to researchers that they not simply use the NELS:88 survey items at face value but draw inferences about students from the differences in responses over time and from different sources. Although an equation for convergence (i.e., for sample polychoric correlations) with six predictors accounted for 40 percent of the variance among items, convergence is item-specific. The interpretation of each item pair benefits from a consideration of the processing that respondents engage in when deciding how (or whether) to respond. Finally, the serial position effect on item omissions was substantial. If some items that are sufficiently important to be included in the survey must be placed after a large number of other items, a mechanism to recapture the interest of students repeatedly throughout the response period is essential. One item that had an unusually low omission rate was the first item on the first page of a section on YOUR PLANS FOR THE FUTURE (item 40 on the second follow-up student questionnaire, see the discussion of Table 5.10A in Chapter 5). Perhaps a mechanism for increasing response rates can be developed from a comparison of this item's context to that of other items.   Table A3.7 Comparison of parent and student responses to items about parent-student discussion of issues VALUE  F2P49A  F2S99A  F2P49B  F2S99B  F2P49C  F2S99C  F2P49D  F2S99D  F2P49E   missing   1018   1026   1875  1043  1897  1067  1935  1042  NA  1370  1512  1370  1512  1370  1512  1370  1512  1370   1   786  3398  783  3035  629  2714  342  1037  1599  2  5522  7674  5010  6869  6108  7727  3176  6722   5651   3   7793 (F2P I A) is father or stepfather. (c) when the parent (F2P1A) is mother or stepmother. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year and Second Follow-up Parent Survey, Base Year and Second Follow-up Student Survey.      Table A5.3 Comparison of first and second follow-up student responses to items about religious attitudes       Table A5.9 Comparison of second follow-up and earlier student responses to items about quality of life      Statistics, National Education Longitudinal Study, 1988 (NELS:88); Base Year, First and Second Follow-up Student Surveys."}, {"section_title": "221", "text": "The standard deviation of the mean difference was computed as the (unweighted) sample standard deviation of the differences, divided by the number of degrees of freedom, or n-1. To test whether the difference between means for different groups of students was significant, the standard (unweighted) two-group Student's t-test was used."}, {"section_title": "Percentage Omission", "text": "The percentage omission is computed as 100 times the ratio of (a) the count of cases with missing values of any kind other than legitimate skips to (b) the count of those cases plus cases providing valid non-missing responses. Only cases with non-zero values for F2PNLWT were included in the computation. When an item was included on the dropout questionnaire, student and dropout counts were aggregated for the computation, unless otherwise indicated in the text. The exclusion of cases with zero values for F2PNLWT means that second follow-up omissions by students and dropouts were limited to item omissions the case weights of students and dropouts who failed to return a second follow-up questionnaire were reassigned to other individuals who did respond. For parent, teacher, and first follow-up questionnaires, on the other hand, omission rates include the cases in which the parent, teacher, or first follow-up teenager failed to return a questionnaire (but a second follow-up questionnaire was returned). Whether the percent missing varied significantly between two groups was tested by the standard normal approximation for the the test of significance between two proportions: Omission bias is the estimate of the mean difference between nonrespondents' responses (if they had been available) and respondents' responses. The estimate of the difference was obtained by using an alternative source (e.g., student responses, for parent responses) and computing the difference between corresponding responses from the alternative source. For example, treating students as the alternative source for parents' responses, parent omission bias is the difference between (a) the mean for students whose parents failed to respond and (b) the mean for students whose parents responded. The meaningfulness of this estimate depends on the assumption that the responses from the two sources are interchangeable. Data presented for each measure in each case indicate how well that assumption is met. In many cases, of course, it is not."}, {"section_title": "251", "text": "The significance of omission bias was tested using the standard two-group Student's t-test, comparing the mean responses from the alternative source, for respondents and nonrespondents to the particular NELS:88 component."}, {"section_title": "Significance of Relations to Outcome Measures", "text": "Relations to outcome measures were estimated using SUDAAN, taking into account the complex sampling design of NELS:88 and the differential case weights (NCES 1994a). All tests were made at the two-tailed .05 level; that is, the values of Student's t were compared to 1.96. The significance levels were not controlled for multiple comparisons through a Bonferroni adjustment because they are intended to be descriptivethey denote the findings that researchers might obtain if they were using the NELS:88 measures to test theoretical hypotheses. For this report, the important question concerned whether such findings of significant results depended on the source of the measure."}]