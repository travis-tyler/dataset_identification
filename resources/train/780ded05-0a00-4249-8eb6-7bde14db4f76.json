[{"section_title": "Introduction", "text": "This report documents the findings from a nonresponse bias analysis of the 2003 National Survey of College Graduates (NSCG). The NSCG is one of three surveys that are combined to create the Scientists and Engineers Statistical Data System (SESTAT). The other two surveys are the National Survey of Recent College Graduates (NSRCG) and the Survey of Doctorate Recipients (SDR) 2 . The 2003 NSCG sampling frame consisted of eligible cases from the 2000 decennial census long form. In order to be included in the 2003 NSCG sampling frame, cases had to meet U.S. residence, age, and science and engineering (S&E) affiliation requirements as set forth for inclusion in the SESTAT population. At the completion of the sample selection processing, 177,320 cases were selected for the sample. At the completion of interviewing, 100,402 cases responded to the survey. After removing the ineligible cases the result was a weighted response rate of 73%. In planning for the 2010 NSCG, the National Science Foundation (NSF) stated the desire to have a nonresponse bias study conducted on the 2003 NSCG. The purpose of the study is two-fold: \u2022 To evaluate the quality of the current NSCG estimates by investigating the amount of nonresponse bias introduced into the NSCG sample in 2003. \u2022 To use the findings from this nonresponse bias study to assist in sample design planning for the 2010 decade of the NSCG. Nonresponse bias can be defined as the product of the nonresponse rate and the difference between respondents and nonrespondents on the estimates of interest. Low response rates may indicate the potential for nonresponse bias, but the nonresponse rate itself is not a good predictor of nonresponse bias for any given estimate. Instead, investigation is required on each estimate of interest to examine whether nonresponse bias is present [ Reference 1]. This report will analyze nonresponse bias with respect to the following estimates: Analysis in this report focuses on the first three of the above listed methods. The following sections describe the evaluation plans for each of these phases of the nonresponse bias study.\nNonresponse bias can be defined as the product of the nonresponse rate and the difference between respondents and nonrespondents on the estimate of interest [Reference 2]. While a high nonresponse rate, in other words a low response rate, is not a direct indicator of nonresponse bias, it does suggest the potential for nonresponse bias. With this idea in mind, this section evaluates 2003 NSCG response rates across demographic subgroups. Investigation of differing response rates focused on variables that were available on the 2003 NSCG frame and are of interest in terms of estimation. When there is a response rate difference, it provides insight into potential nonresponse bias to the extent the attribute variables are correlated with survey variables [Reference 1]. An evaluation was conducted on the response rates of key demographic variables available for all sample cases. The response rates were calculated using the formula documented in the SESTAT response rate guidelines [Reference 2]. If the response rates are not substantially different among subgroups, it reduces the potential for nonresponse bias in estimates for these subgroups.\nVariables that appear on the sampling frame are useful because the values for these variables are available for both respondents and nonrespondents. To the extent that frame variables are used in nonresponse weighting adjustments, nonresponse bias can be minimized for these variables. The previous section analyzed response rates by subgroups and highlighted the potential for nonresponse bias where the response rate varied. This section will examine estimates of the frame variables using the base weight 7 as well as the nonresponse adjusted weight to determine if the nonresponse weighting adjustment properly accounted for differential response. If the differential response is properly accounted for then nonresponse bias should be small. Table 5 compares estimates of frame variables of all 2003 NSCG sample cases using the base weight to estimates of frame variables for respondents using the base weight and the nonresponse adjusted weight. The estimates of frame variables using the base weight on all 2003 NSCG sample cases reflects the population distribution as determined by the 2000 decennial long form. Therefore, the estimates of frame variables using base weights are considered as accurately reflecting the population measured by the 2003 NSCG and thus serve as an accurate benchmark for comparisons."}, {"section_title": "Nonresponse Bias Benchmarks Study", "text": "Comparable benchmark surveys or administrative records were located that cover similar topics on the same target population as the 2003 NSCG. The benchmark sources located for this study include the following: \u2022 National Center for Educational Statistic's Integrated Postsecondary Education Data System (IPEDS) \u2022 Occupational Employment Statistics program (OES) Once the comparable sources were located, estimates from both the benchmark and the 2003 NSCG were compared to evaluate the amount of bias (including nonresponse bias) present in the NSCG."}, {"section_title": "Comparing Response Rates across Subgroups", "text": "An evaluation was conducted on the response rates of key demographic variables available for all sample cases. The response rates were calculated using the response rate formula documented in the SESTAT response rate guidelines Since nonresponse bias can result when subgroups with different characteristics have different response rates, this approach examined the response rate component. If the response rates are not substantially different, there should not be a large nonresponse bias in statistics for the groups. When there is a response rate difference, it provides insight into possible nonresponse bias to the extent the attribute variables are correlated with survey variables [Reference 1]. It should be noted that the differential response rates by subgroup only highlight the potential for nonresponse bias on the subgroup estimates and that nonresponse bias for these subgroup estimates and other estimates correlated with the subgroup can be reduced through appropriate weighting adjustments.\n"}, {"section_title": "Nonresponse Bias Evaluation of Frame Variables", "text": "An evaluation was conducted on estimating nonresponse bias by comparing estimates of respondents with estimates of the full sample on substantive variables available on the sampling frame. When estimates between the full sample and the respondents are compared, the difference is an estimate of nonresponse bias.\n"}, {"section_title": "Nonresponse Bias Evaluation of Estimates by Level of Effort", "text": "To evaluate the effect of additional data collection efforts, an analysis can be conducted on the sensitivity of survey estimates to successive data collection efforts. In some instances, this type of evaluation may provide a reasonable indicator of the magnitude and direction of nonresponse bias. This paper will discuss the findings from analysis conducted using the first three methods. Analysis of nonresponse bias on estimates by level of effort may be explored in a future paper."}, {"section_title": "Benchmarking", "text": ""}, {"section_title": "Purpose of Benchmarking", "text": "Benchmarking is a useful tool in nonresponse bias analysis as it uses comparisons between the survey estimates of interest and another data source considered as a 'gold standard.' This 'gold standard' data source could be administrative records or another survey that has better precision. Differences found between the survey estimates and the 'gold standard' are an indicator of nonresponse bias (as well as other biases). It should be noted that although 'gold standards' are considered to be the best data source for a particular estimate, there is still error associated with any data source. The limitations associated with the 'gold standards' used as benchmarks in this paper's analysis are noted in the limitation sections."}, {"section_title": "Evaluation of Possible Benchmarks", "text": "Two benchmarks were used for the 2003 NSCG nonresponse bias analysis. The first benchmark was the National Center for Education Statistics (NCES) Integrated Postsecondary Education Data System (IPEDS). IPEDS was used as a comparison for degree completion estimates from the 2003 NSCG. The second benchmark was the Occupational Employment Study (OES). The OES data were used as a comparison for occupation employment estimates from the 2003 NSCG. Other benchmarks were considered for analysis but it was determined that limitations made them less useful as a comparison point. As an example, the October 2002 School Enrollment Supplement to the Current Population Survey was considered as a benchmark because it collected information that enables one to identify and estimate the population of college graduates working in an S&E occupation. However, despite the ability to estimate a portion of the NSCG target population, the October 2002 Supplement to the CPS was not used as a benchmark because it had a different reference date than the 2003 NSCG and because numerous other limitations were identified when researchers tried to use it as a benchmark as part of a comparison with the 1997 SESTAT data. For more information on the comparison between the October 2002 Supplement to the CPS and the 1997 SESTAT, see reference 4. Another data set that was considered as a benchmark was the American Community Survey (ACS). Unfortunately, the ACS was in a developmental design phase in 2003 and was not forming nationally representative estimates based on the full production sample size of three million households that began being used in 2005. As a result, the ACS was not used as a benchmark for this study. The sections that follow will describe in detail the two benchmarks determined appropriate for evaluation."}, {"section_title": "Integrated Postsecondary Education Data System (IPEDS) -Degree Comparison", "text": ""}, {"section_title": "Description", "text": "IPEDS is a system of interrelated surveys conducted annually by the NCES that gathers information from every accredited college and university within the United States that participates in the federal student financial aid programs. Since IPEDS collects information from every college and university participating in the federal student financial aid programs it provides comprehensive coverage of postsecondary degree completion information with the United States. These institutions report data on enrollments, program completions, and some demographic information. For more information about the IPEDS go to http://nces.ed.gov/ipeds/about/. The IPEDS data collected from July 1991 to June 1998 have degree field, degree type, and gender information while the IPEDS data from July 1995 to June 1998 also contains race and ethnicity information. Degree field will be the only information used in this report. For comparisons using the degree type, gender, and race information please see reference 5.\nThe Occupational Employment Statistics (OES) survey is a semiannual mail survey sent to a sample of nonfarm establishments. The Bureau of Labor Statistics (BLS) conducts this survey. The OES survey produces estimates of the number of people employed in certain occupations and estimates of the wages paid to them. Self-employed persons are not included in the estimates. For more information on the OES survey, see http://www.bls.gov/OES/. In order for the OES to produce estimates for a given reference period, employment and wages are collected from establishments in six semiannual panels for three consecutive years. Every six months, a new panel of data is added, and the oldest panel is dropped, resulting in a moving average staffing pattern. The three years of employment data are benchmarked to represent the total employment for the reference period. This methodology assumes that industry staffing patterns change slowly. The use of six data panels to create a set of estimates means that sudden changes in occupational employment or wages in the population or changes in methodology show up in the OES estimates gradually. The OES survey used as a benchmark for analysis had a November 2003 reference date. "}, {"section_title": "Data Manipulation and Comparison", "text": "IPEDS was used as a benchmark for degree-based estimates from the 2003 NSCG for degrees earned in the U.S. between July 1991 and June 1998. Comparisons were made within each academic year by degree field and degree type. Comparisons were also made within each academic year on cross-classifications of degree field by gender and race/ethnicity when available 3 . This paper only discusses the comparisons broken out by degree field. For the full evaluation, please see reference 5. In an effort to evaluate the accuracy of the 2003 NSCG degree completion estimates, statistical comparisons were made between the NSCG and 'gold standard' IPEDS estimates. To conduct these comparisons, variances for the NSCG estimates were calculated using replicate weights derived through the successive difference replication methodology. Since the IPEDS estimates came from information directly provided by the colleges and universities, zero variance was assumed for IPEDS estimates. The main comparison focused on the absolute difference between the 2003 NSCG and IPEDS estimates. Relative difference comparisons were also made so that when a statistical difference was noted, the size and direction of the difference could be evaluated. In other words, were the statistical differences large or small (size) and were the 2003 NSCG estimates greater than or less than the IPEDS estimates (direction)? Benchmark counts from IPEDS were created based on completions data for degrees earned between July 1991 and June 1998. Since a focus of these comparisons was the evaluation of degree completion by degree field, July 1991 was chosen as the beginning date because a crosswalk that recoded the IPEDS degree field Classification of Instructional Program (CIP) codes 4 to SESTAT codes 5 was only available beginning in 1991. The degree-based estimates for the 2003 NSCG cases come from the 100,402 respondents to the 2003 NSCG. Each respondent listed up to three earned degrees on the NSCG questionnaire, resulting in a total of 159,406 degrees. A file was created with each degree included as one record (as opposed to a person-based data file). These degrees were then classified by the degree field they were earned in (based on SESTAT codes) to produce counts to allow comparisons with the IPEDS data. In addition, only U.S.-earned degrees were included in the NSCG data since the IPEDS data only covered U.S.-earned degrees. The counts were weighted using the 2003 NSCG final weight.\nThe OES survey does not collect demographic information from the sampled establishments and, as a result, occupation employment data cannot be broken out by demographic groups. Therefore, all comparisons between OES data and NSCG data will only be by field of occupation. Estimates included in the comparison of OES and NSCG data will be the number of people employed broken out by S&E occupation status as well as by major S&E occupation fields. Since the OES estimates are derived from sample data there are variances on these estimates. Variances for the NSCG occupation estimates were calculated using replicate weights derived through the successive difference replication methodology. Comparisons between OES and NSCG data will take both of these variances into account. As with the degree completion comparisons, comparisons of both the absolute differences and relative differences will be made for the occupation estimates. Benchmark counts from the OES of the number of people employed by occupation field were created using a OES data file that contained detailed employment estimates by standard occupation classification (SOC) codes (about 700 SOC codes were listed with the associated employment estimate and variance). The SOC codes were converted to SESTAT occupation codes to allow comparison with NSCG data. Then for both the OES and NSCG data, the SESTAT codes were collapsed into the occupation fields of interest for each set of comparisons: S&E occupation status and major S&E occupation fields."}, {"section_title": "Evaluation of Degree Completions", "text": ""}, {"section_title": "Overall evaluation", "text": "As noted above, two types of comparisons were made between the degrees earned estimates from the 2003 NSCG and the IPEDS: absolute differences and relative differences. Using these two metrics, the main result of the benchmark analysis is that the 2003 NSCG estimate for S&E degrees earned between 1991 and 1998 was higher than the IPEDS S&E degree estimate by 10% while the 2003 NSCG estimate for non-S&E degrees earned between 1991 and 1998 was lower than the IPEDS non-S&E degree estimate by 7% (see Table 1). Additionally, the 2003 NSCG overestimation of S&E-Related (S&E-R) health and S&E-R non-health degrees earned between 1991 and 1998 was 11% and 13%, respectively 6 . Encouragingly, the 2003 NSCG estimate for total number of U.S. degrees earned between 1991 and 1998 was not statistically different from the IPEDS estimate. This is an indication that the 2003 NSCG accurately measures the number of degrees earned in the U.S. On the downside, the 2003 NSCG overestimates the number of S&E degrees earned and underestimates the number of non-S&E degrees earned. "}, {"section_title": "Conclusions", "text": "The main conclusion found is that the 2003 NSCG estimated the total number of U.S. earned degrees well but its estimate of S&E degrees earned was high by 10%. One possible reason for this overestimation might be nonresponse bias. Those with S&E degrees might have had higher inclination to respond than those with non-S&E degrees. Another possible reason for the S&E degree overestimation is that it could be due to the degree categories listed on the 2003 NSCG questionnaire. If there is an emphasis on S&E majors then this might have inclined a respondent with a non-S&E degree to choose an S&E field of study. Related to this possible explanation is that the difference in the 2003 NSCG and IPEDS estimate of S&E degrees earned might be due to the self-reporting in the 2003 NSCG versus administrative reporting in the IPEDS.\nIt was expected that there would be large differences between the NSCG and OES employment estimates as the NSCG frame of the college educated population is a small subset of the OES frame. However, it was also hoped that the S&E occupation estimates would match closely under the assumption that most S&E occupations require a bachelor's degree and therefore the NSCG and OES frames would mostly overlap for this population. This was found to be the case with a relatively small underestimation of 8% for the S&E occupation estimates. A large portion of this underestimation seems to stem from the computer occupations where a bachelor's degree may not be required for employment. Within the S&E major occupation fields there was variation in the amount of over and underestimation. It was not expected to find overestimation in the NSCG since the frame is a subset of the OES frame. This overestimation in some major occupation fields may be due to differences between the self-reporting of occupation in the NSCG and administrative reporting of occupation in the OES survey. Another possibility is nonresponse bias in either the NSCG or OES estimates, but further research would be needed to evaluate that claim. Ignoring the limitations associated with the OES frame, the NSCG appears to do an adequate job estimating S&E occupations. However, numerous limitations associated with comparing the NSCG to the OES survey data requires caution when making claims Section on Survey Research Methods -JSM 2010 about the impact of nonresponse bias on NSCG occupation estimates. These limitations are discussed in more detail in the next section.\nAs mentioned in the introduction, nonresponse bias can vary between estimates in a survey. Because of this, an investigation was conducted on each estimate of interest to determine the level of nonresponse bias present on the individual estimates. It was found that with respect to the number of degrees earned between 1991 and 1998, the 2003 NSCG overestimated the number of S&E degrees earned by 10% while estimating the total number of degrees earned accurately. The overestimation of S&E degrees earned might be due to nonresponse bias if S&E degree holders were more likely to respond to the 2003 NSCG than non-S&E degree holders. An alternative or possibly congruent explanation is that the overestimation might be due to differences between the selfreporting of degree information in the NSCG versus administrative reporting of degree information in IPEDS. Analysis of employment estimates by occupation was a little murkier due to frame differences between the NSCG and the OES. However, there is some evidence that the 2003 NSCG estimates of employment in S&E occupations were reasonable with an underestimation of 8%. A large portion of this underestimation seems to have been driven by the underestimation of employment in computer occupations where a bachelor's degree may not have been required. Due to the limitation associated with comparing the 2003 NSCG to the OES, caution is required when making claims about the presence of nonresponse bias in the employment estimates by occupation. Comparisons of response rates by frame variables revealed variations in response rates among demographic variables such as age, race and marital status. This differential response rate had the potential for nonresponse bias if not properly addressed. It was seen in Table 5 that for those frame variables used in creating nonresponse adjustment cells, bias was reduced substantially through the nonresponse weighting adjustment. However, this substantial reduction in bias did not occur for estimates by marital status since this variable was not used in the nonresponse weighting adjustment. Still, bias was reduced somewhat for marital status since marital status is correlated with the frame variables used in the weighting adjustment. This finding provides evidence that nonresponse bias is likely present for other frame variables and key survey variables not accounted for in the nonresponse weighting adjustment, but that bias was likely reduced for those variables that are correlated with the nonresponse adjustment variables. Further research is needed to investigate these issues. These results display varying degrees of bias in the 2003 NSCG estimates, from an overestimation of S&E degrees and a possible underestimation of S&E occupations to little or no bias associated with some frame variable estimates such as gender and demographic group. A portion of the bias present in some estimates may be due to the difference between the nonrespondents and the respondents while other portions of the bias may be due to measurement error (e.g., self-reporting of responses versus administrative reporting of responses) and other sources of nonsampling error. Future investigation should attempt to evaluate the specific source of the S&E degree overestimation, whether it is due to nonresponse bias, measurement error, other nonsampling error or some combination, in an effort to correct this issue for future NSCG survey cycles."}, {"section_title": "Limitations", "text": "A limitation to the degree benchmark analysis is the scope of the comparisons between the 2003 NSCG and IPEDS. The 2003 NSCG target population was U.S. residents holding a U.S. or foreign earned bachelor's degree as of April 1, 2000. However, the comparisons made for this analysis were only for the years 1991 to 1998 and only applied to U.S. earned degrees. Therefore, it is unclear whether the overestimation of S&E degrees might occur in years earlier than 1991. Although the IPEDS survey was mandatory for college institutions receiving federal aid, a nonresponse rate of three to five percent existed for the colleges reporting to the 1991 to 1998 IPEDS. For these nonresponding institutions, imputation was implemented using prior year's data when available or similar institutions data to fill in degree completion information. Though nonresponse was small for IPEDS, the imputation may impact the results of the preceding analysis. Another limitation is the possibility of error in the information provided from the colleges and institutions to IPEDS. To the extent this is true, IPEDS may not live up to its assumed 'gold standard' status. This might imply that the overestimation and underestimation seen in the 2003 NSCG estimates may overstate the quality of NSCG degree estimates.\nAs discussed in the previous sections, there are limitations to using the OES as a benchmark for comparisons with NSCG occupation estimates. This section discusses some of the major limitations to keep in mind when interpreting the preceding comparisons. The biggest limitation to using the OES data as a benchmark for NSCG estimates is the difference in coverage between the OES and NSCG frames. The OES frame covers employees regardless of educational attainment while the NSCG only covers employees with at least a bachelor's degree. Another slight difference is that the OES does not cover the self-employed while the NSCG does. The OES survey used for the benchmark analysis had a reference period of November 2003 while the NSCG's reference period was October 1, 2003. The differences in frames should be kept in mind when interpreting the comparisons between occupation employment estimates. The OES data provided from the BLS website for November 2003 data listed total employment of about 127 million. However, summing up across the detailed breakout by SOC code, the employment estimate was only about 120 million. After follow-up with BLS staff, it was discovered that there are many 'residual' occupations that are not included in the detailed breakout as they are not typically available to the public. These residual occupations do not have a SOC code. Upon request, occupation estimates for 1.6 million more employees in specific requested S&E occupation fields were obtained. A judgment call was used to place these residual occupations in the appropriate occupation field. To the extent that the judgment was incorrect or that BLS did not provide complete occupation estimate information for the requested S&E occupations, caution should be used when interpreting the results. Also, residual occupation information was not provided for the non-S&E occupation fields thus lessening the validity of comparisons for this group. In benchmark studies, the preference is that the 'gold standard' have no variances on the estimates. However, that is not the case here as the OES estimates are derived from sample data and have variances associated with them. Also, the OES variance estimates for the analysis groups were created by summing variances for the individual groups that make up the analysis group. This assumes that the subgroups are independent. If this assumption does not hold then the variances would be smaller than the ones created. This does not significantly alter the analysis since most the differences were significant and would remain significant if the assumption of independence did not hold.\nWhile considering the information presented in this section, it should be kept in mind that the potential for nonresponse bias on a subgroup estimate only occurs when there is a differential response rate by that subgroup. Therefore, for example, nonresponse bias on gender estimates should be small, if present at all, due to similar response rates between men and women. It should be pointed out that a differential response rate by subgroup does not necessarily mean that nonresponse bias exists. As noted earlier, nonresponse bias can be defined as the product of the nonresponse rate and the difference between respondents and nonrespondents on the variable of interest. As a result, the presence of a low responding subgroup would lead to nonresponse bias only if the respondents were different from the nonrespondents on the variables of interest. Since information on the variables of interest was not collected from nonrespondents, the presence of nonresponse bias cannot be completely evaluated.\n"}, {"section_title": "Occupational Employment Statistics (OES) -Occupation Comparison", "text": ""}, {"section_title": "Comparison of Occupation Estimates", "text": "Differences between the NSCG and OES frames should be kept in mind when comparing estimates. The main substantive difference between frames is that the NSCG estimates employment for people who have obtained at least a bachelor's degree while the OES estimates employment for all people regardless of educational background. To the extent that certain occupations require at least a bachelor's degree, it is expected that the NSCG and OES employment estimates for these occupations should match well. Likewise, to the extent that certain occupations do not require at least a bachelor's degree, it is expected that the NSCG and OES employment estimates for these occupations will not match well. The assumption that an occupation requires at least a bachelor's degree seems a safe one for most S&E occupations since these occupations generally require higher education. Therefore, it is expected that the NSCG and OES employment estimates for S&E occupations should match well. The assumption of a bachelor's degree is not as safe for S&E related or non-S&E occupations leading to expectations that the employment estimates between NSCG and OES will not match well for these occupations."}, {"section_title": "S&E Status Evaluation", "text": "As with the degree estimate evaluation, two types of comparisons were made between the 2003 NSCG and OES occupation estimates: statistical differences and relative differences. Table 2 shows that the NSCG underestimates employment for all S&E status occupation fields. However, the smallest underestimation of 8% occurs for the S&E occupation. This is likely due to the assumption that bachelor's degrees are required for most S&E occupations and therefore the NSCG and OES frames are covering similar populations. The S&E related and non-S&E occupation fields have large underestimations ranging between 49% and 78%. This underestimation is expected, as the assumption of bachelor's degree for S&E related and non-S&E occupations is not strong. "}, {"section_title": "S&E Major Field Evaluation", "text": "Further comparison between the NSCG and OES employment estimates by S&E major occupation subgroups leads to some interesting insights. The employment estimate in the social science and engineering occupations are not statistically different between the comparison frames. This makes sense particularly for the engineering occupations where advanced degrees are usually required. The computer and math occupations are underestimated by 23% in the NSCG. Again, this may make sense as many computer jobs may only require certifications and not a degree. Therefore the NSCG would undercount the computer jobs that do not require a bachelor's degree. The life sciences and the physical sciences are overestimated by 57% and 8%, respectively. It is not immediately clear why the NSCG would overestimate any occupation, as the NSCG only covers a subset of the OES frame. In particular it is unclear why there is such a large overestimation in the life science occupations. A possible explanation for this inconsistency is that the OES collects occupation category information from establishments where as the NSCG collects the information directly from the sample respondents. However, this possible explanation needs further evaluation. "}, {"section_title": "Comparisons", "text": "The table below displays that there were several differences in response rates among the demographic subgroups. Examining by demographic group reveals that non-White citizens had lower than the overall response rate of 73% while White citizens had a higher than overall response rate. Non-U.S. citizens at birth had particularly low response rates at 60%. Only small differences in response rate were seen by gender. Age group had a significant impact on response rates with older people more likely to respond than younger people. Those aged 55 and up responded at 81% while those less than 30 responded at 61%. Marital status also affected response rates with those separated least likely to respond at 58%. The differential response rates by subgroup highlight the potential for nonresponse bias on the subgroup estimates if not appropriately addressed through the weighting methodology. Section 5 explores whether the 2003 NSCG nonresponse weighting adjustment adequately addresses the potential for nonresponse bias identified in this section. "}, {"section_title": "Results", "text": "The relative difference column in Table 5 shows that using the base weight for respondents to estimate the frame variables results in significant nonresponse biases. For example, Blacks are underestimated by 27% and non-U.S. citizens at birth are underestimated by 30% while those aged 55 and up are overestimated by 13%. (The pattern is that underestimation occurs for those groups who responded at low rates while overestimation occurs for those subgroups who responded at high rates.) However, using the nonresponse adjusted weight the bias for Blacks, non-U.S. citizens, and those aged 55 and up drops to less than 1%. Similarly, the bias for gender, occupation, degree type, age group, and marital status estimates drops significantly when using the nonresponse adjusted weight as opposed to the base weight. This demonstrates that the nonresponse weighting adjustment reduced nonresponse bias for these frame variables. It should be noted that some large biases do remain for the marital status estimates. This is because the marital status variable was not used to create the nonresponse weighting cells. Nonetheless, for four out of five levels of the marital status variable bias was reduced. This reduction occurred because marital status is correlated with the nonresponse adjustment variables such as age and demographic group. What is occurring for marital status is also likely occurring for key survey variables. Since the key survey variables are not used in the nonresponse adjustment, biases remain on estimates for these variables. However, to the extent that the key survey variables are correlated with the frame variables used in the nonresponse adjustment, biases are still reduced.  Table 5 shows that nonresponse bias was reduced by adjusting the base weight to account for nonresponse. However, it also shows that large biases remain for the marital status variable, though the biases were reduced for four out of the five levels. The biases remain since marital status was not used in the creation of the nonresponse weighting adjustment. However, the biases were still reduced because marital status is correlated with the nonresponse adjustment variables. This shows that other variables that were not used in the nonresponse weighting adjustment, including variables that did not exist on the frame, may also display large nonresponse biases. To the extent these other variables are correlated with the nonresponse adjustment variables, it is expected that the bias will still be reduced somewhat."}]