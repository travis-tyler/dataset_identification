[{"section_title": "Abstract", "text": "High resolution Magnetic Resonance (MR) images are desired for accurate diagnostics. In practice, image resolution is restricted by factors like hardware and processing constraints. Recently, deep learning methods have been shown to produce compelling state-of-the-art results for image enhancement/superresolution. Paying particular attention to desired hi-resolution MR image structure, we propose a new regularized network that exploits image priors, namely a low-rank structure and a sharpness prior to enhance deep MR image super-resolution (SR). Our contributions are then incorporating these priors in an analytically tractable fashion as well as towards a novel prior guided network architecture that accomplishes the superresolution task. This is particularly challenging for the low rank prior since the rank is not a differentiable function of the image matrix (and hence the network parameters), an issue we address by pursuing differentiable approximations of the rank. Sharpness is emphasized by the variance of the Laplacian which we show can be implemented by a fixed feedback layer at the output of the network. As a key extension, we modify the fixed feedback (Laplacian) layer by learning a new set of training data driven filters that are optimized for enhanced sharpness. Experiments performed on publicly available MR brain image databases and comparisons against existing stateof-the-art methods show that the proposed prior guided network offers significant practical gains in terms of improved SNR/image quality measures. Because our priors are on output images, the proposed method is versatile and can be combined with a wide variety of existing network architectures to further enhance their performance."}, {"section_title": "I. INTRODUCTION", "text": "H IGH Resolution (HR) MR images provide rich structural information about bodily organs which is critical in analyzing any given medical condition. Often, the quality of these images is restricted by factors like imaging hardware, sensor noise, budget, and time constraints. In such scenarios, the spatial resolution of these images can be enhanced by a well-designed mathematical algorithm. Simple and fast interpolation methods like bilinear and bicubic [1] have been widely used for increasing the size of low-resolution (LR) medical images. In many cases, these methods are known to introduce blurring, blocking artifacts, ringing and are thus unable to recover sharp details of an image. To alleviate this problem, an alternative approach known as super-resolution (SR) was introduced in [2] . Current literature on SR can be classified into two categories: multi-image SR and single-image SR.\nIn multi-image SR [2] , [3] , an HR image is generated by exploiting the information from multiple LR images which are acquired from the same scene with a slightly shifted field of view. However, these methods are likely to fail if an adequate amount of LR images from the same scene are not available. As an alternative approach, single image SR was introduced wherein multiple LR images from the same scene are not required to obtain an HR image. In this approach, a mapping between LR and HR images is learned by constructing examples from a given database [4] - [9] .\nRecently, deep learning methods have been shown to produce compelling state-of-the-art results [10] - [23] for single image SR. Invariably though, the training requirement of deep networks, i.e. the number of example LR and HR image (or patch) pairs, is quite significant. In some medical diagnosis problems, generous LR and HR pairs is not a problem but there are compelling real-world problems such as enhancing 3T MR to 7T MR images [8] , [24] , where the paucity of training has been recognized. There has been encouraging recent application of deep networks for MR image SR [25] - [29] but the methods remain training intensive. An outstanding open challenge for deep MR image super-resolution is the development of methods that exhibit a graceful degradation with respect to (w.r.t.) the number of training LR and HR image pairs.\nOur approach to improve deep MR image superresolution for all training regimes is via the exploitation of suitable structural prior information pertinent to MR images. In [30] , a model based SR approach is presented that uses low-rank (approximated by nuclear norm) and total variation regularizers. Despite the promise shown by a low rank prior, incorporating a low rank constraint or even its nuclear norm relaxation in a deep network for SR presents a stiff analytical challenge since neither is a differentiable function of the image matrix (and hence the network parameters). Our contribution includes incorporating a suitable approximation to the rank, which is smooth, differentiable and amenable for learning in a deep CNN framework. Additionally, recognizing the need for well formed sharp edges in diagnosis, we propose a sharpness prior realized via a variance of the Laplacian measure which adds to the network structure at the output as a fixed feedback layer. As we bring prior information into a deep network, we call our method Deep Network with Spatio-Structural Priors (DNSP). As a key extension, we modify the fixed feedback (Laplacian) layer by learning a new set of training data driven filters that are optimized for enhanced sharpness.\nAnother class of approaches for single image SR called self-image SR [31] , [32] has been developed recently. Self image SR has been adapted to enhance resolution of MR images [33] , [34] . These approaches exploit the fact that MR images are inherently anisotropic to learn a regression between LR and HR images. Thus, they generate new additional images, each of which is LR along a certain direction, but is HR in the plane normal to it. Further, these approaches improve the resolution across the z axis by assuming that the images in the axial plane are HR. But, in our approach, we focus on improving the resolution of images in the axial plane by exploiting suitable structural information of MR images thereby differentiating from the aforementioned methods.\nContributions: While most of the existing deep learning methods for MR image SR focus on learning an end to end relationship between LR and HR image, our goal is to enrich this deep learning framework by bringing in suitable structural information via informative priors and incorporating them in an analytically tractable fashion. Specifically, our key contributions are as follows:\n\u2022 Novel Prior-Guided Network Structure: We propose a new network structure that consists of two components: 1) A regression network that maps LR to HR images 2.) a prior information network component that guides the learning of the regression network during the training phase. Note that only, the regression network is used in the test (inference) stage to map LR to HR images. \u2022 Incorporating Spatio-Structural Priors: We impose a low rank constraint on the output of the deep network. Evidence for brain MR images being low-rank has been provided recently [30] . However, incorporating a low rank constraint or even its nuclear norm relaxation into a deep learning framework is not straightforward as neither of the functions are differentiable. We provide a solution to integrate low rank constraint into a deep network by approximating the rank function with a smooth and differentiable function. We further incorporate a spatially based sharpness prior defined as the variance of Laplacian computed on the network output (image). The laplacian operation can be implemented via a linear convolution with a filter (fixed) and subsequently, the variance is computed to yield a regularization term. \u2022 Data Adaptive Filters to Enhance Sharpness: We further extend the aforementioned two contributions by learning a series of filters that are aimed at enhancing the sharpness of the output image. We develop new data adaptive regularizers which ensure that the learned sharpness filters are physically meaningful. \u2022 Novel Regularized Loss Function: Analytically, to integrate the proposed priors, we introduce three new regularization terms in the loss function along with the standard reconstruction loss term. The first regularization term poses a low rank prior, the second is a sharpness prior, while the third constrains the filters that replace the Laplacian and are aimed at enhancing sharpness. Further, back-propagation equations for optimizing network parameters w.r.t the regularized loss function are derived in a form that is implementation friendly. \u2022 Experimental validation and reproducibility: Experimental validation of our method is carried out on two publicly available data bases: 1.) Brainweb (BW) 1 and 2.) Alzheimer's Disease Neuroimaging Initiative (ADNI). 2 We compare DNSP against several state of the art methods that are used for MR image SR. We also provide the entire code of our experiments for the purpose of reproducibility at https://scholarsphere. psu.edu/concern/generic_works/9s4655g25h. A preliminary version of this work was presented as a 4 page conference paper at 2018 IEEE International Conference on Image Processing [35] . This present draft involves both substantial conceptual and experimental extensions including: 1.) We evolve the fixed Laplacian layer into a learnable one that computes an enhanced sharpness measure, 2.) more detailed analytical development is presented including back-propogation derivations under the new regularizers, 3) we integrate the aforementioned priors with the most competitive deep network architectures that are used for image SR thereby demonstrating the versatility of our method, and 4) we significantly expand experiments by comparing against many new state-of-the-art methods. Results are also presented for several variants of DNSP and in new scenarios (training and test selection).\nThe rest of the paper is organized as follows. The proposed prior guided deep network for MR image SR is explained in Section II. Extensions of DNSP to include learnable sharpness filters is subsequently presented in Section III. Detailed experimental validation against the state of the art methods is reported in Section IV. Finally, our work is summarized and concluded in Section V."}, {"section_title": "II. SPATIO-STRUCTURAL PRIORS FOR DEEP MR IMAGE SUPER-RESOLUTION", "text": "We first introduce the notation that is followed through rest of the paper and then give a brief introduction for deep networks for image SR before describing our DNSP method."}, {"section_title": "A. Notation", "text": "Let X \u2208 R M\u00d7N represent the LR image where M and N are the width and height of the image respectively. Let Y \u2208 R s M\u00d7s N be the output HR image and s is the desired scale to which X needs to be upscaled and Y g \u2208 R s M\u00d7s N is the ground truth HR image for X. Let W l k \u2208 R m\u00d7n\u00d7d be the k th convolutional filter in layer l where m, n and d represent the width, height and depth of the filter respectively. Similarly, let b l k \u2208 R be the k th bias coefficient of layer l. The objective of the network is to learn W l k and b l k so that the output of the network Y is a close representation of the ground truth Y g . So, let = {W l k , b l k }\u2200l, k. To make the size of input and output of the network the same, we first upscale X by a factor of s using bicubic interpolation and use this upscaled X s \u2208 R s M\u00d7s N as input to the network. Finally, let the mapping function of the network be represented by F where F(X s , ) = Y ."}, {"section_title": "B. Deep CNNs For SR", "text": "Deep learning methods are a class of machine learning methods which are inspired by biological neural networks. In general, a cascade of many nonlinear processing units are used to learn features to represent data effectively for a given task. In particular, a deep CNN for image SR usually consists of two or more convolutional layers (each layer essentially is a combination of filters followed by an activation function) which are used to learn an end-to-end mapping between sample HR and LR image pairs. For example, Fig. 1 illustrates the SRCNN network [11] , [27] for super-resolution which is known to be the most widely used deep SR network. Following these footsteps, many new architectures have been designed for image SR which showed considerable gains in terms of performance [10] , [12] , [14] - [18] , [21] - [23] . Each convolutional layer in the network consists of several learnable filters, which are convolved with the output from the previous layer. For a given layer, outputs obtained by convoluting with each filter are combined to form a data cube which is passed through a nonlinear activation function and then forwarded as an input to next layer [36] . The most commonly used activation function in deep networks is the Rectified linear unit (Relu) [37] . The input to the first layer is the image obtained after bicubic interpolation and the output of the last layer is the expected HR image. The filters are learned to minimize the loss function given by:\nwhere \u00b7 F represents the Frobenius norm."}, {"section_title": "C. Deep Network With Spatio-Structural Priors (DNSP)", "text": "As discussed in Section I, we integrate two priors into the learning of the CNN. Note that both the priors are to be applied on Y as it represents the desired output HR image. The two priors are as follows:\n1) Low Rank Prior: It has been demonstrated recently [9] , [30] , [38] , [39] that MR images are naturally rank deficient. For example, Figure 2 shows several low rank images of an MR image reconstructed from partial singular value decomposition (SVD) approximation. We can observe that the recovered image with a rank of 90, which is approximately half of full rank (170) of the image matrix; still An example that demonstrates that MR brain images are naturally rank deficient. The low rank images are obtained by zeroing out the smallest singular values (from the SVD). This example reveals that the image has an effective rank in the range 115-120. exhibits a Peak Signal to Noise Ratio (PSNR) of about 45dB. Further, the reconstruction is visually indistinguishable from the original image. We wish to emphasize that an image being low-rank implies that the effective rank of the matrix is low. For example, it can be observed from Figure 2 that the change in PSNR value in the range of 110-120 rank is relatively negligible compared to that of the PSNR change in the range of 90-110 rank. Hence the effective rank of this particular image can be argued to be in between 115 and 120 which is much smaller than the full rank of 170. Rank of an image captures the global structure of a given image. An effective low-rank implies that the image adheres to some structural properties like near symmetry which can be observed in brain images. Hence, a low-rank constraint is effective in recovering the global structure of a given brain image.\nHowever, the rank of a matrix is a non-differentiable function w.r.t. its input and therefore cannot be used as regularizer in a CNN. Most of the optimization problems with a low-rank constraint are solved by minimizing the nuclear norm of the matrix which is a convex relaxation of the low-rank constraint. However, this relaxation also cannot be used in a CNN as the nuclear norm is also a non-differentiable function. To address this, we pursue smooth and differentiable approximations of the rank. In particular, in recent work [40] an estimate of the number of singular values of a matrix that are zero was proposed as:\nwhere \u03b4 is a tunable parameter that affects the measure of approximation error in finding the rank. Intuitively, for small \u03b4, differentiable and its gradient w.r.t. Y is given by:\n2) Sharpness Prior: HR images look much sharper compared to LR images. The main reason can be attributed to blurriness of the LR images. The pursuit of quantifying sharpness begins by computing the Laplacian (\u2207 2 Y ) of the image [41] . The laplacian of a smooth/blurred image is more uniform compared to the laplacian of a sharp image. The variance of the Laplacian is hence an indicator of sharpness. As shown in Figure 3 , an MR brain image is degraded by a gaussian filter with different blur parameters \u03b6 and plotted against the variance of laplacian. It can be observed that the variance of laplacian decreases as the blur parameter increases. Therefore, we propose to use V (Y ) = var (\u2207 2 Y ) as a regularizer to encourage the CNN to yield sharper HR images. V (Y ) is a quadratic function in Y and therefore a differentiable function which can be easily integrated into the CNN learning. Note that the laplacian of an image can be implemented by well-known linear filters [41] , which are also easily integrated into the CNN via a filtering layer at the output as shown in Fig. 4 .\nRemark: The two priors are chosen carefully so that they perform a complementary job to each other. For example, the low-rank constraint captures the global structure of the brain image and the sharpness prior aids in recovering the finer local structure thereby complementing the low-rank prior.\n3) Network Structure: A key advantage of using priors on the output HR image is that they can be incorporated into any network architecture. In Figure 4 , we show an example where the aforementioned two priors are incorporated into the basic SRCNN [11] framework. In Section IV, we demonstrate the versatility of our approach by incorporating the priors with more advanced networks. As observed from the Figure 4 , to obtain the variance of laplacian, we use a 3 \u00d7 3 filter\nafter the final layer to compute the Laplacian and subsequently find the variance of Laplacian. The loss function of DNSP to be minimized is given by:\nwhere, Y = F(X s , ), \u03b1 and \u03b2 are positive regularization parameters, note that negative sign before V (Y ) is to increase the variance of Laplacian. Note that the loss function in Eq (1) is a special case of Eq. (5) . We learn by minimizing E( ) using a stochastic gradient descent method [42] , [43] . In particular, weights are updated by the following equation:\nwhere, t represents the iteration number, \u03b7 represents the learning rate, and t represents the values of weights at previous iteration. As = {W l k , b l k }\u2200l, k, following gradients are to be computed:\n, where w l k denotes an arbitrary scalar entry in filter W l k . For simplicity, let output image Y be of dimension N \u00d7 N. The equation for computing the gradient of weight w l k in layer l is given by\nwhere between two matrices A and B is defined\nwhere P = [p i, j ], and P is obtained by convolving Y with a 3 \u00d7 3 laplacian operator L. Expression for p i, j is given by:\nDetailed derivations for the above equations are reported in the Appendix. Note that the gradient for bias terms are also updated in a similar fashion. The partial derivative \u2202Y \u2202w k l is obtained by a standard back propagation rule [42] , [43] ."}, {"section_title": "III. DNSP WITH DATA ADAPTIVE SHARPNESS", "text": "ENHANCING FILTERS A fixed Lapalacian filter can be sensitive to noise, enhance spurious components or might not be the best choice for a particular given data. Further, in the literature there exists a variety of sharpness enhancing filters [44] for different applications. Recall in Fig. 4 that the Laplacian is computed through a fixed 3\u00d73 convolutional filter. To develop data-adaptive filters, we intend to learn a set of sharpness enhancing filters jointly with the network parameters instead of using a fixed Laplacian filter. For this purpose, the fixed laplacian layer in Figure 4 is replaced by a set of filters that are initialized with the standard \nwith additive minor perturbations generated by a normal random variable with 0 mean and a small variance of = .0001. The output of the SR network is passed through these filters which is followed by computing the average variance obtained from outputs of all these learnable sharpness filters. The extended architecture of our network that implements a learnable sharpness layer is shown in Fig. 6 .\nWe represent these filters by\nwhere N L is the total number of learnable sharpness filters. The modified equation to compute the variance (sharpness prior) is:\nA first step towards learning data-adaptive filters is the selection of sharp and smooth training patches which we carry out as follows:\n\u2022 From each training image, we extract two patches of size P \u00d7 P of which one is sharpest and the other is smoothest.\nwe pass all the patches of size P \u00d7 P through a standard laplacian filter and select the patch that gives the maximum response in the sense of Frobenious norm \u2022 2 F of the patches. \u2022 Similarly, to find the smoothest patch {Sm i } Q i=1 , select the patch that gives the minimum response in the sense of Frobenious norm \u2022 2 F of the patches. \u2022 A visual inspection of all the candidate Q smooth and sharp patches is performed to arrive at N T \u2264 Q selected smooth and sharp patches. Note that a sharpness enhancing filter W i L is expected to give the maximum response for the sharpest patches and the minimum response for the smoothest patches. This behavior is captured by formulating the following regularization term:\nA negative sign before the response of sharp patches indicates that we intend to maximize it. Figure 5 shows three representative examples of sharp and smooth patches extracted via the procedure we discussed above. The new regularized loss function to learn the parameters of the SR network and data-adaptive sharpness filters is given by: Fig. 6 . The proposed DNSP architecture with learnable, data-adaptive sharpness. Bottom part of the network may be a typical state of the art LR to HR mapping, i.e. the SR network. As in Fig. 4 , the parameters of the SR network are guided by prior information, except that in this case the sharpness filters are jointly learned with the network parameters . Post learning, i.e. during inference, the SR network carries out the LR to HR mapping.\nModified Back-Propagation Equations: Extending Eq. (7), we obtain:\nwhere w z k is an arbitrary network parameter in layer z. Note that w z k does not depend on S( L ), which is not reflected in back-propagation equation. However, its influence is felt on learnable sharpness filters. The term D R \u03b4 remains the same as described in Section II-C. The complete expression for D V mod is given by:\nwhere P l = [p l i, j ], and P l is obtained by convolving Y with the l th 3 \u00d7 3 learnable sharpness filter W l L . The expression for p l i, j is given by:\nThe back-propagation equations for the sharpness filter parameter W l L k,m are given by:\nwhere W l L k,m is the (k, m) coefficient in l th learnable filter W l L and (k, m) \u2208 {\u22121, 0, 1}. Note that gradient of W l L k,m is not dependent on the first two terms of the loss function in Eq. (10) . (14) where Sm a = [s a i\u2212k, j \u2212m ], s a i\u2212k, j \u2212m is the (i \u2212 k, j \u2212 m) coefficient of Sm. Sh a can also defined in the similar fashion.\nwhere Y = [y i\u2212k, j \u2212m ] and D V mod is the same matrix as defined previously. From equations (11) and (13), we can observe that weights of the learnable sharpness filter influence the SR network parameters and vice-versa."}, {"section_title": "IV. EXPERIMENTAL EVALUATION", "text": ""}, {"section_title": "A. Experimental Setup 1) Databases, Training and Test Set-Up:", "text": "We evaluate the proposed DNSP on two publicly available MR brain image databases. The first database is 20 simulated T1 brain image stacks from Brainweb (BW). 3 Axial slices of these 20 stacks are distributed evenly for training and evaluation purposes. From each stack, we extract 40 slices making a total of 400 images for training and 400 images for evaluation. The second database we work with is from the Alzheimer's Disease Neuroimaging Initiative (ADNI). 4 The same training and test configuration is employed as that of the BW database.\n2) LR Image Simulation: Consistent with [4] , [30] , we simulate training LR images by applying a gaussian blur and factor of 2 downsampling. These LR images are then upscaled by bicubic interpolation. To speed up the training process, we further extract patches of size 40 \u00d7 40 from these bicubic enlarged LR training images. Note that this is also a standard procedure used for training a typical deep SR network [11] , [12] , [14] .\n3) Parameter Choices: To obtain an accurate rank surrogate of Y , we chose \u03b4 = .01 based on guidelines mentioned in [40] . We determine regularization weights in Eq. (10) as \u03b1 = 1\u00d710 \u22125 , \u03b2 = 5\u00d710 \u22123 , and \u03b3 = 1\u00d710 \u22127 based on cross validation. More details can be found in the supplementary document. The number of learnable sharpness filters 5 N L is chosen as 8. The size of smooth and sharp patches P is chosen as 40 with the help of a domain expert. Batch size and number of epochs are chosen to be 64 and 50 for all the experiments. A total of around 6400 patches are extracted for ADNI dataset and 8000 patches are extracted for BW dataset. Therefore, 100 iterations are required to complete an epoch in the ADNI dataset and 125 iterations are required to complete an epoch in the BW dataset. For optimization, an Adam optimizer [45] with a learning rate of 1 \u00d7 10 \u22124 is used. These values are consistent with other deep learning based SR methods [11] , [14] , [25] ."}, {"section_title": "4) Methods and Metrics for Comparison:", "text": "Two standard metrics PSNR and structural similarity index (SSIM) [46] are used for evaluation. We compare against following six methods:\n\u2022 Bicubic interpolation (BC) [ developed specifically for MR image super-resolution, published in 2018. 5) Network Architecture: As mentioned previously, the two proposed priors can be integrated into any deep SR network. Two deep SR networks we used are SRCNN and EDSR. SRCNN architecture is illustrated in Fig. 4 . Figure 7 illustrates the architecture of EDSR. It is composed of total 32 residual blocks wherein a convolutional layer of a residual block consists of 256 3 \u00d7 3 \u00d7 256 filters. First layer is composed of 256 3\u00d73\u00d71 filters and last layer consists of one 3\u00d73\u00d7256 filter. A detailed description of network architectures for both the methods can be found in [10] , [11] . Note that in the original EDSR architecture, the interpolation block is placed at the end of the residual network. In this work, to be consistent with SRCNN, we perform a bicubic interpolation prior to sending the input to EDSR network thereby removing the interpolation block after the residual network. Hence the size of the input send to the EDSR network is same as the size of the desired output. We did not observe noticeable performance difference by shifting the interpolation block, hence we chose the configuration that is consistent with other deep learning frameworks. More details can be found in the supplementary document. Unless otherwise stated, note that our priors are integrated with the EDSR network. Remark: Note that our choice of SRCNN and EDSR as base DNSP networks is because SRCNN is widely used and EDSR has recently been shown to be one of the best performing methods (winner of the 2017 NTIRE contest at IEEE conference on Computer Vision and Pattern recognition (CVPR)). The goal in this work is to demonstrate the value of priors in enhancing performance and not to perform an exhaustive comparison of deep SR architectures [17] , [20] ."}, {"section_title": "B. Significance of Priors: DNSP Variants", "text": "In this section, we report the results for different variants of the proposed DNSP to bring out the value added by each prior. We name the variants as follows: 1) DNSP-NP, network with no priors which is same as EDSR, 2) DNSP-LR, network with only low rank prior, 3) DNSP-FS, network with only sharpness prior with a fixed Laplacian layer, 4) DNSP-LS, network with only sharpness prior but with learnable sharpness filters and finally 5) DNSP-AP, network with both the priors included along with learnable sharpness filters. Table I shows the PSNR and SSIM on both the datasets. We can observe that priors improve the performance of the network. Among the individual priors, we observe the best performance for DNSP-LS, which is expected as the sharpness is enhanced via a data adaptive procedure that exploits available training. Fig. 8 .\nResponse of the 8 learned sharpness filters (along with the corresponding filter coefficients) vs. the Laplacian filter on a sample image from the ADNI dataset, as well as the coefficients of the learned filters. Figure 8 shows a comparison of the response from fixed laplacian filter and N L = 8 filters that are learned via DNSP-LS method. We can observe that spurious (undesirable/noise-like) edges that are present in the fixed Laplacian response are minimally seen in the response of the 8 filters learned based on data, which on the other hand lead to sharper images overall. It can be observed that the responses of learned sharpness filter depart from that of the Laplacian ( for example in exhibiting some directional orientation), which is a result of training image data adaptation.\nFurther, to provide more insights about the performance of different priors, validation curves for PSNR vs EPOCH on test sets are illustrated in Fig. 9 . It can be observed that the network with priors always outperforms the one without priors. As expected DNSP-AP is the best performing method. We also observe that DNSP-LS does better than DNSP-FS and DNSP-LR. For all the subsequent experiments, unless otherwise stated, we report results of DNSP-AP. Table II shows PSNR and SSIM values for all competing methods. Note that we used two different base networks for DNSP: 1) DNSP-SRCNN-AP -the base network is SRCNN and 2) DNSP-EDSR-AP -the base network is EDSR. Three trends emerge from the results: 1) DNSP-EDSR-AP outperforms the competition, 2) DNSP-SRCNN-AP does better than all the methods except EDSR, and 3) overall, deep SR methods, i.e. SRCNN, EDSR, DCSRN and DNSP perform better than other alternatives. To confirm this statistically, we performed a 2-way Analysis of Variance (ANOVA) on PSNR values for all the methods across the two datasets which is illustrated in Fig. 10 . It may be inferred from Fig. 10 that deep learning methods are statistically well separated from the traditional methods and further DNSP-EDSR-AP is well separated from all the competing methods indicating the effectiveness of using prior information. Figures 11 and 12 illustrate the results of the top 4 methods w.r.t. PSNR on a sample image from BW and ADNI databases respectively for a down-sampling factor of 2 while Figures 13 and 14 show results for a down-sampling factor of 4. DNSP-EDSR-AP particularly excels in recovering fine image detail (enlarged with zoom-in boxes), thanks to data-adaptive sharpness. Figure 15 compares the performance of the learning based methods for different percentage of training samples considered on both the datasets. Twenty five, 50 and 75 percent of the 400 training images are employed. Two inferences can be made: 1) DNSP-EDSR-AP consistently outperforms EDSR, SRCNN, DCSRN and SRSW, 2) The performance degradation of both DNSP-SRCNN-AP and DNSP-EDSR-AP is more graceful with a decrease in the number of training samples. For example, PSNR values for EDSR, SRCNN, and SRSW dropped by almost close to 1-1.5db whereas for DNSP-*, the drop is in between .5-1db, when the training drops to 25 percent. Another interesting observation is DNSP-SRCNN-AP does better than EDSR for the 25 percent training scenario. These results unequivocally demonstrate the value of priors (capturing domain specific signal structure) in enhancing performance when training imagery is limited. This is due to the fact that priors aid in capturing the structure of the images which in turn ensures that the deep network outputs images that are consistent with the structure of the original images. In the case of large training samples, the network has a sufficient amount of training samples to discover the inherent structure of the images. However, when the training data is limited, the network by itself does not have a sufficient amount of images to discover the inherent structure. In such cases priors guide the network to discover the appropriate structures of the underlying images, thereby enhancing the performance of the network. This fact is clearly brought out in Figure 9 where validation curves of different variants of our proposed method are illustrated. There we can observe that the prior guided networks start performing better than DNSP-NP (EDSR with no priors) right from epoch 1 which confirms that the network is being guided to discover appropriate structures."}, {"section_title": "C. Comparisons Against State-of-the-Art Methods", "text": ""}, {"section_title": "D. Performance in Varying Training Regimes", "text": "Further, to confirm this statistically, Fig. 16 shows the 2-way ANOVA analysis for the 25 percent training scenario for all the deep learning based methods. It can be observed that DNSP-EDSR-AP is well separated from the other methods. Note that LRTV and BC are excluded from this experiment since these methods are not learning based."}, {"section_title": "E. Enhancing Low Field MR Images", "text": "A key practical task is discovering the mapping from low to high field MRI. This is required in scenarios where expensive, high field MR captures are not available but low field MR images may be enhanced prior to diagnosis. This problem has received recent attention via learning based methods [8] , [24] . The dataset in [8] and [24] is however not [47] . Certain assumptions are made by the authors of [47]: 1.) the noise model is assumed thermal, and 2.) Further, a single global relaxation correction function is used to account for the signal change at different field strengths. We point to [47] for more details and the code that implements the degradation from high to low field MR. We use their code to simulate 1.5T images from the available 3T ADNI images.\nWe report results on the 25 percent training setup as described before. For this experiment, we compared against [24] since it is also a deep learning method developed specifically for low to high field MR enhancement. We call this method as LFMRI. Table III shows the results and the benefits of DNSP are readily apparent. Visual comparisons of enhanced images via both the methods are shown in Fig. 17 . A one way statistical ANOVA is further performed (using 400 test images) to confirm that the benefits of DNSP are indeed statistically pronounced -see Fig. 18 . "}, {"section_title": "F. Experiments on Real World Clinical Image Pairs", "text": "To further validate our framework in real world scenarios, we perform experiments on two new datasets obtained from Human Connectome Project (HCP) [48] . 6 Wide variety of datasets are available in the aforementioned link of which we selected two scenarios that are closely related to our work. [48] the datasets. It can be observed that the DNSP-EDSR-AP enhanced image is closer to ground truth image compared to EDSR."}, {"section_title": "G. Ablation Study Against a Total-Variation (TV) regularizer", "text": "Although a detailed ablation study is performed in Section IV-B, the study is centered around the variants of our own method. To demonstrate the benefits of the proposed priors comprehensively, we perform an experiment that incorporates both low-rank and total-variation (TV) regularizers into a deep learning framework. We particularly chose TV and low-rank as this combination has proven successful for brain images in [30] . TV regularizer is used for recovering fine structures while suppressing noise. Our proposed sharpness enhancement measure does a similar job to a TV regularizer but more effectively as data-adaptive filters for a given dataset are learned where as a TV regularizer is generic and does not exploit any available training. Table V reports the comparison of our method against the low-rank and TV priors Fig. 19 . Comparisons of EDSR and DNSP-EDSR-AP on 3T7T-DW dataset [48] . The numerical assessment is shown as PSNR-SSIM. The DNSP-EDSR generates the best results both numerically and visually compared to EDSR for 3T7T-DW dataset. Fig. 20 . Comparisons of EDSR and DNSP-EDSR-AP on 3T3T-T1 dataset [48] . The numerical assessment is shown as PSNR-SSIM. The DNSP-EDSR generates the best results both numerically and visually compared to EDSR for 3T3T-T1 dataset. incorporated with the EDSR network (called EDSR-TVLR) for the two real-world clinical datasets. Table V reveals that DNSP-EDSR-AP performs the best. Further, to confirm this statistically, Fig. 21 shows the 2-way ANOVA analysis for EDSR-TVLR and DNSP-EDSR-AP. It can be observed that DNSP-EDSR-AP is well separated from EDSR-TVLR for both the datasets."}, {"section_title": "V. DISCUSSIONS AND CONCLUSION", "text": "In this paper, we present a novel regularized deep network structure for MR image super-resolution, which excels in varying training regimes and experimental setups. This is accomplished by using two spatio-structural priors on the expected output HR image: 1) a low-rank prior, and 2) a sharpness prior. Our contributions include the development of new regularization terms that are inspired by the priors on the output of the network as well as tractable algorithmic methods to incorporate them in a deep learning set-up. We demonstrate the versatility of our method by experimental validation involving two widely used and highly competitive deep learning architectures for the SR problem. Because our priors are on the network output, the proposed DNSP method can be combined with many other deep SR networks as well.\nFuture work could develop and incorporate other meaningful priors such as those that are anatomically inspired [54] .\nThe interaction of prior induced regularization with specific network architectures can also be explored for speeding up network training and inference."}, {"section_title": "APPENDIX", "text": "First we derive the back-propagation equations for the loss function in Eq. (5) which is given by:\nwhere, Y = F(X s , ), \u03b1 and \u03b2 are positive regularization parameters. We learn by minimizing l( ) using a stochastic gradient descent method [43] . The weights ate each iteration are updated by the following rule\nwhere, t + 1 represents the iteration number, \u03b7 represents the learning rate for the stochastic gradient descent method and t represents the values of weights at previous iteration. As = {W l k , b l k }\u2200l, k, following gradients are to be computed:\n, where w l k denotes an arbitrary scalar entry in filter W l k . For simplicity, let output image Y be of dimension N \u00d7 N. The equation for computing the gradient of weight w l k in any given layer l is given by:\nwhere\nis the gradient of R \u03b4 (Y ) and D V is the gradient for V (Y ). The complete expression for D V is given by:\nwhere P = [p i, j ], and P is obtained by convolving Y with a 3 \u00d7 3 Laplacian operator L. Expression for p i, j is given by:\nGradient for R \u03b4 (Y ) is derived in [40] . Deriving an expression for D V is mentioned below.\nTo Let the Laplacian be represented by P, where p i, j is given by Eq. (20) . The variance of Laplacian V (Y ) is given by V (Y ) = var (P). Therefore,\nNow, the gradient of V Y w.r.t Y is obtained by following the chain rule:\nNote that y i, j can influence only p i, j , p i\u22121, j , p i+1, j , p i, j \u22121 , p i, j +1 . Hence the chain rule is restricted only to these values as the partial derivative of all the p i, j 's w.r.t y i, j is 0. It is straightforward to observe that \u2202 p i, j \u2202y i, j = 1, \u2202 p i, j \u22121 \u2202y i, j = \u2202 p i, j +1 \u2202y i, j = \u2202 p i+1, j \u2202y i, j = \u2202 p i\u22121, j \u2202y i, j = \u22121/4\nSubstituting these values in Eq. (22) gives:\nNow Eq. (19) directly follows by taking derivative of V (Y ) in Eq. (21) w.r.t p i, j to obtain d i, j ."}, {"section_title": "Back-Propagation Equations for Modified Loss Function:", "text": "The modified loss function is given by:\nFollowing the lines of the above derivation, the gradient of modified loss function w.r.t network parameter w z k in layer z is given by:\nNote that network parameter w z k does not depend on S( L ), and hence is not reflected in the back-propagation equations. The expression for D R \u03b4 remains same as described above. However, the expression for D V mod differs from the fixed Laplacian version. First, a set of N L 3 \u00d7 3 filters are used instead of a single 3 \u00d7 3 Laplacian filter wherein each filter is defined by set of coefficients W l L k,m , (k, m) = {\u22121, 0, 1}. Now, the expression for D V mod is given by:\nwhere P l = [p l i, j ], and P l is obtained by convolving Y with the l th 3 \u00d7 3 learnable sharpness filter W l L . Expression for p l i, j is given by:\nThe expression for D V mod directly follows from the derivation of D V except for the fact that constant values in D V are replaced by the learnable filter parameters W l L k,m and summed over all the N L filters.\nThe back-propagation equations for a given learnable sharpness filter parameter W l L k,m are given by:\nwhere \u2202 S( L ) \u2202 W l L k,m is given by: (29) where Sm a = [s a i\u2212k, j \u2212m ], s a i\u2212k, j \u2212m is the (i \u2212 k, j \u2212 m) coefficient of Sm, and N T is the number of training images. This expressions is derived by applying the chain rulethe derivative of Frobenious norm followed by derivative of convolution operation w.r.t to the filter parameter. Sh a is also defined similarly to Sm a . Following the same strategy, the expression for \u2202 V mod (Y ) \u2202 W l L k,m is given by:\nwhere Y = [y i\u2212k, j \u2212m ] and D V mod , is the same matrix as defined by Eq. (27) ."}]