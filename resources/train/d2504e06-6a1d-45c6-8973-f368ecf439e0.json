[{"section_title": "Abstract", "text": "Abstract-Alzheimer's disease (AD) is a progressive and irreversible neurodegenerative disorder that has recently seen serious increase in the number of affected subjects. In the last decade, neuroimaging has been shown to be a useful tool to understand AD and its prodromal stage, amnestic mild cognitive impairment (MCI). The majority of AD/MCI studies have focused on disease diagnosis, by formulating the problem as classification with a binary outcome of AD/MCI or healthy controls. There have recently emerged studies that associate image scans with continuous clinical scores that are expected to contain richer information than a binary outcome. However, very few studies aim at modeling multiple clinical scores simultaneously, even though it is commonly conceived that multivariate outcomes provide correlated and complementary information about the disease pathology. In this article, we propose a sparse multi-response tensor regression method to model multiple outcomes jointly as well as to model multiple voxels of an image jointly. The proposed method is particularly useful to both infer clinical scores and thus disease diagnosis, and to identify brain subregions that are highly relevant to the disease outcomes. We conducted experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, and showed that the proposed method enhances the performance and clearly outperforms the competing solutions."}, {"section_title": "", "text": "dementia in elderly subjects. The number of affected subjects increases significantly every year, and is projected to be 1 in 85 by the year 2050 [1] . Amnestic mild cognitive impairment (MCI) is often a prodromal stage to AD, and individuals with MCI may convert to AD at an annual rate of as high as 15% [2] . There has been a vast body of literatures studying AD and MCI using one or more neuroimaging technologies (modalities), including magnetic resonance imaging (MRI), functional magnetic resonance imaging (fMRI), positron emission tomography (PET), diffusion tensor imaging (DTI), among many others.\nThe majority of AD/MCI studies have been concentrating on differentiating AD and MCI subjects from the general population, because an accurate diagnosis of AD and MCI is particularly important for timely therapy and possible delay of the disease. This can be formulated as a classification problem, and a variety of statistical machine learning techniques have been employed for imaging-based diagnosis. See [3] [4] [5] for some excellent reviews. Moreover, in addition to classifying a binary or categorical disease outcome given brain image scans, there were studies establishing associations between image activity patterns and a continuous clinical outcome. A variety of cognitive and memory scores have been used as the response, including the Mini-Mental State Examination (MMSE) [6] [7] [8] [9] , Boston Naming Testing [9] , Dementia Rating Scale, Alzheimer's Disease Assessment Scale-Cognitive Subscale (ADAS-Cog), and Auditory Verbal Learning Test [8] .\nMore recently, there have emerged studies that associate image scans with multiple clinical outcomes [10] , [11] . Our motivating data example consists of 194 subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI), among which 93 are AD patients and 101 healthy controls. For each subject, the collected data include a MRI scan, after proper preprocessing and downsizing, and two clinical scores. One is the MMSE, which examines orientation to time and place, immediate and delayed recall of three words, attention and calculation, language and visuo-constructional functions [12] . The other is ADAS-Cog, which is a global measure encompassing the core symptoms of AD [13] . The ADAS-Cog is usually more sensitive, but requires more than 30 minutes for participants to complete all tasks. In contrast, the administration of MMSE takes only 10-15 minutes and thus is often used for fast screening for dementia. While both can be used to measure the severity of cognitive impairment, the scores of MMSE and ADAS-Cog carry, respectively, \"local\" and \"global\" information with respect to the cognitive capability, given the fact that the examination in MMSE is con- ducted on more specific tasks than in ADAS-Cog. Furthermore, recent studies have shown that the two scores are correlated, as they reflect on similar cognitive aspects such as orientation and memory [8] , [14] . In these regards, it is beneficiary to jointly consider these scores in AD/MCI studies. Our aim in this article is to jointly model multiple clinical outcomes given brain structural patterns, under the belief that the multivariate scores provide correlated and complementary information.\nWhereas there is an enormous body of statistics literature on modeling multivariate predictors, there have been much fewer works on modeling multivariate responses. Some popular multi-response solutions include partial least squares [15] [16] [17] , canonical correlations [18] , reduced-rank regressions [19] [20] [21] , sparse regressions with various penalties [22] [23] [24] , and sparse reduced-rank regressions [25] . All existing multi-response modeling methods universally treat the predictors as a vector and estimate a corresponding vector of coefficients. However, in neuroimaging analysis, the predictors take a more complex form of multi-dimensional array, a.k.a. tensor. Naively turning an array into a vector would result in extremely high dimensionality. For instance, a MRI image would imply parameters. Moreover, vectorizing an array would also destroy all the inherent spatial structural information of the image.\nIn this article, we propose a sparse multi-response tensor regression model to simultaneously infer multiple outcome variables and to identify brain subregions that are highly relevant to the clinical outcomes. A schematic overview of our proposed method is given in Fig. 1 . The new method enjoys a number of appealing features. First, it models the multiple responses jointly rather than separately, by employing a penalty function accounting for correlated multivariate responses while inducing group sparsity. Second, the new method models brain image predictor in the form of a tensor rather than a vector. This is achieved by extending and generalizing a recent proposal of tensor predictor regression [26] . Directly modeling a tensor image predictor takes into account spatial correlations among the voxels, and is intuitively superior than the one-voxelat-a-time modeling solution that ignores such correlations. This extension, however, is far from trivial, since [26] only considered a univariate response, and our proposal for multi-response requires a new form of penalty and a new optimization algorithm. Third, our method offers a competitive alternative to the common modeling strategy in neuroimaging literature that first groups individual voxels by predefined regions of interest (ROI) and then extracts a vector of useful features from ROIs. By contrast, our solution does not rely on any prior knowledge of ROIs but derives highly relevant features suggested directly by the data. Moreover, instead of conducting feature extraction and association modeling at two separate steps, our method simultaneously derives relevant features and builds their association with the outcomes. Last but not least, we develop a highly scalable computational algorithm that makes our method applicable to a range of massive imaging data."}, {"section_title": "II. MATERIALS A. Subjects", "text": "We analyzed a dataset from the ADNI database (http://adni. loni.usc.edu/). The data included 93 subjects with mild AD and 101 normal controls (NC), and each subject had both MMSE and ADAS-Cog scores recorded. The subjects were in the age between 55 and 90, with a study partner, who provided an independent evaluation of functioning. All of 194 subjects met the following general inclusion criteria: (a) NC subjects: an MMSE between 25 and 30 (inclusive), a clinical dementia rating (CDR) of 0, non-depressed, non-MCI, and non-demented; (b) mild AD subjects: an MMSE score between 18 and 27 (inclusive), a CDR of 0.5 or 1.0, and met the National Institute of Neurological and Communicative Disorders and Stroke and the Alzheimer's Disease and Related Disorders Association (NINCDS/ADRDA) criteria for probable AD. The AD and NC groups were matched in age (with the two-sample -test -value ) and gender (with the two-sample proportion test -value ). Table I presents the demographic characteristics of the subjects."}, {"section_title": "B. Preprocessing", "text": "All anatomical MRI data in this study were acquired using 1.5T scanners. The baseline MRI data were downloaded from ADNI in the neuroimaging informatics technology initiative (NIfTI) format, which had already been processed for spatial distortion correction caused by gradient nonlinearity and B1 field inhomogeneity. We further performed prevalent preprocessing procedures on all images, including Anterior Commissure-Posterior Commissure (AC-PC) correction, skull-stripping, and cerebellum removal. Specifically, for the AC-PC correction, we used MIPAV software to resample images to , and applied N3 algorithm [27] for non-uniform tissue intensity correction. Skull-stripping [28] was then performed, followed by cerebellum removal. We visually checked the skull-stripped images to ensure clean and dura removal. We next employed FAST of the FSL package (http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/) to segment the MR images into three tissue types: gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF). We used HAMMER [29] to spatially normalized all three tissues onto a standard space, based on a brain atlas aligned with the MNI coordinate space. Next, we generated the regional volumetric maps, called RAVENS maps, using a tissue preserving image warping method [30] . In this study, we considered only the spatially normalized GM densities (GMD), due to its relatively high relevance to AD compared to WM and CSF [31] . Finally, we downsized the GMD maps to voxels. Downsizing is for estimation and computational convenience, as it would considerably reduce the number of unknown parameters and save computation time and cost. It is a tradeoff and admittedly does lose some image information; however, our results and previous studies [32] suggest that the sacrifice in prediction is relatively limited."}, {"section_title": "III. METHOD", "text": ""}, {"section_title": "A. Model", "text": "Let denote a vector of responses. For our AD data, , and , where MMSE and ADAS-Cog. Let denote a -way tensor predictor. For our AD data, , , and denotes MRI scan. We propose the following multivariate tensor regression model,\nwhere , , denotes the tensor coefficient that captures association between the tensor predictor and the th response . The inner product between two tensors and is defined as where denotes a tensor operator that stacks the entries of into a column vector, and , denotes the th element of and , respectively. denotes a vector of errors, each of which follows a normal distribution with zero mean and constant variance. Without loss of generality, we omit the intercept term in (1) .\nThe tensor coefficients in (1) are the parameters of interest and require estimation given the observed data. If imposing no additional constraint, the total number of unknown parameters in , which equals , is prohibitive. For instance, for our AD data, there are parameters to estimate for each , , while the sample size is only . To alleviate the extremely high dimensionality, [26] introduced a low-rank structure on the coefficient tensor that substantially reduces the number of unknown parameters. Specifically, a -way tensor is said to follow a rank-CANDECOMP/PARAFAC (CP) decomposition [33] , if (2) where are all column vectors, , and denotes an outer product among vectors. For convenience, the CP decomposition is often represented by a shorthand, , where for . With this low-rank decomposition, the number of unknown parameters in decreases substantially from the order of to that of . For the MRI image in the AD example, the dimensionality reduces from 32,768 to the order of 96 for a rank-1 model, and 288 for a rank-3 model.\nIntroducing this CP decomposition to , model (1) becomes\nThis is our base model for multivariate responses and tensor predictors, upon which the subsequent regularization and estimation are built. To help concretize model (3), we consider one of its special cases when , , . That is, there are two response variables , a matrix-valued predictor , and is assumed to follow a rank-1 CP structure such that , . Then (3) reduces to (4) where denotes the Kronecker product. The first equality in (4) comes from the fact that , when the matrix admits a rank-1 CP decomposition (2). Furthermore, the Kronecker product equals the outer product when and are both column vectors. The second equality in (4) holds because . Examining model (4), we see that our proposed multivariate response tensor regression model in this special case essentially postulates that the relation between the th response variable and the matrix predictor is in the form of left multiplying a coefficient vector then right multiplying another coefficient vector with the matrix image . This relation is a natural extension of the classical linear model when is a vector and the response-predictor relation is governed by ."}, {"section_title": "B. Penalized Likelihood", "text": "Imposing the CP low-rank structure on the coefficient tensor substantially reduces the ultrahigh dimensionality of model (1) to a manageable level, leading to feasible estimation and prediction. However, the resulting number of unknown parameters can still be much larger than the available sample size. For instance, for our AD example, imposing a rank-3 CP structure yields parameters, and the sample size is merely . Moreover, model (3) itself treats the components of multivariate response separately, while it is commonly conceived that the multivariate outcomes, i.e., MMSE and ADAS-Cog in this work, are correlated and often provide complementary information. Regularization through penalized estimation is particularly useful to both handle the small--large-challenge and to incorporate potential correlations among the response variables. Therefore, we further introduce regularization into our multivariate response tensor regression model (3), and propose penalized likelihood estimation.\nGiven independent and identically distributed sample observations , we propose to minimize the following objective function over ,\nIn this function, the first component is the usual negative log likelihood after imposing the CP structure, i.e., where is the th response variable of subject . The second component in the objective (5) is a penalty function, which could have multiple choices. Here, we choose the group lasso penalty [34] , which, in our context, takes the form, (6) where is the th element of in the CP decomposition (2) . By imposing such a penalty, the individual coefficients that correspond to same subregion in an image but across different response variables are penalized as a group. As such, (6) encourages a subregion to drop out as a group if it is not associated with any of the multivariate response variables, which in effect takes into account potentially correlated responses. To further illustrate how the low-rank structure works and how the sparsity is imposed by this group penalty function, we consider a special case when , , , and . Fig. 2 shows that, for each response variable and its associated coefficient , element-wise sparsity in the column vectors translates into region-wise sparsity in the coefficient matrix . Meanwhile, the group penalty (6) encourages that the subset of elements in , , that correspond to the same region but across different response variables would enter or drop from the model simultaneously. A similar group penalty was also used in the classical multi-response model [23] . It encourages identification of the same subregions of the coefficient images across different responses. Meanwhile, it permits different magnitude of , i.e., strength of association, for different responses."}, {"section_title": "C. Estimation", "text": "Next, we investigate optimization of the objective function in (5). We first summarize the optimization procedure in Algorithm 1, then present details for individual steps. The optimization is achieved through the variational method [25] based on the following result, when . Consequently, minimizing (5) over is equivalent to minimizing the following objective function (7) over both and , where . Optimization of (7) can then be achieved in an alternating fashion, updating iteratively with one set of parameters renewed and the others fixed.\nSpecifically, with fixed, the update of is simply With fixed, the update of is achieved through a block relaxation algorithm [26] . That is, by imposing the CP structure on , the first part of (7) can be written as where denotes the modematricization that maps tensor into a matrix such that the th element of maps to the th element of , and denotes the Khatri-Rao product [35] . This reformulation allows one to focus the estimation on while keeping all the other parameters fixed. Meanwhile, the second part of (7) can be written as Therefore, the objective in (7) is essentially a quadratic function of individual when all the other parameters are fixed. There is a closed form solution for , such that equals where\nWe make a few remarks about the above optimization procedure. First, although the objective value decreases monotonically through iterations, the convergence to a global optimum is not guaranteed, since (5) involves a nonconvex optimization and there exist potentially multiple local minima. We adopt the common practice of using multiple starting values. In our setup, the stability of the algorithm with respect to initial values depends on several factors. A large sample size, a stronger signal strength, and a low-rank true image signal would all foster fast convergence and increase the chance to locate the global optimum from different initializations. In Section IV-B, we report the numerical convergence behavior of our algorithm with multiple starting values. Second, the computation of our method is fast, since both steps of iterations have closed form solutions. Actually the computational complexity for each iteration is for , and for , when . In addition, since only depends on , can be updated simultaneously within each iteration. Again in Section IV-B, we report the computation time in simulations. Third, we note that the variational method rarely produces estimates that are exactly zero in practice. Consequently, we set a thresholding value for to achieve the desired sparsity, which is a common practice in the applications of the variational method [25] . Last but not least, in addition to the variational method, one may also consider using the alternating direction method of multipliers (ADMM) for solving the optimization of (5). We have experimented with ADMM, and found it produced similar results as the variational method, but was slower. This is partly due to that, within each block update, our problem simplifies into minimizing a quadratic function plus a group lasso penalty. The variational method can further simplify the problem by optimization over separately, whereas ADMM cannot and has to construct relatively large matrices involving jointly. For that reason, we choose the variational method as our optimization solution."}, {"section_title": "IV. RESULTS", "text": "In this section, we first carry out Monte Carlo simulations to investigate the empirical performance of our proposed method. We then investigate its stability, convergence and computation time. Finally, we analyze the ADNI dataset to illustrate the efficacy of the new method."}, {"section_title": "A. Signal Recovery and Prediciton", "text": "We evaluate the empirical performance by two criteria: the prediction accuracy of the responses measured by root mean squared error (RMSE), and the estimation accuracy of the tensor coefficient shown by a plot. We compare our method with two alternative solutions. The first is to fit a tensor regression model for one response at a time. A comparison with this method would clearly show the gain of our method that jointly models the multivariate responses. The second is to vectorize the image predictor, ignoring all potential correlations among the image voxels, then fit a mluti-response linear regression model with a group lasso penalty [23] . This comparison would show the gain of our method that respects the image tensor structure. For abbreviation, we call our method and the two alternatives as \"Multi-Resp\", \"Uni-Resp\", and \"Vectorized\", respectively.\nThe data are simulated from model (3), with a 64 64 matrix predictor , whose entries independently follow a normal distribution, and coefficient matrices , , which take the value of 0 or 1 following specified patterns. We consider two scenarios: (i) We let all follow the same pattern of \"cross\", \"triangle\", and \"butterfly\", respectively. (ii) We let half of 's take the shape of \"cross\", and the other half \"triangle\". Among the three patterns, \"cross\" is of an exact rank 2, while \"triangle\" and \"butterfly\" are of infinitely high rank, whereas we use a fixed rank 3 to approximate all three patterns. We then generate response variables following (3), with the errors following a normal distribution with mean zero and covariance , where has diagonal elements equal to 1 and off-diagonals equal to . Consequently, the pairwise correlation among the responses is governed by , while controls the relative noise level. We examine a series of values of , and to investigate the empirical performance of different methods under varying response correlation strength and noise level. We marginally standardize the response variables, by subtracting mean and dividing by standard deviation. The models are fitted on a training set of size , tuned on an independent validation set, and evaluated on another independent testing set of the same size. Table II shows RMSE of prediction of \"future\" responses in the testing data under 50 Monte Carlo replications, whereas Fig. 3 gives a graphical summary of the estimated coefficient signal based on one data replication under the scenario (i). Since the underlying true patterns are the same for all responses here, we only report the estimator for the first one. We present in the  table the results when  , , and , respectively, but in the figure omit the results when , since they are visually similar to those of . We also omit in the figure the case for \"Uni-Resp\", because it fits each response variable separately, and thus the coefficient estimate is not affected by the number of responses . It is clearly seen that our proposed method outperforms the univariate solution. The difference is more dominant when the signal is relatively more noisy, as one would often encounter in real imaging data, and when the number of response variables is large, as they provide more complementary information. In addition, both the multivariate and univariate tensor regression solutions have produced a much better estimation than the vectorized solution, which fails to identify any meaningful patterns.\nSimilarly, Table III and Fig. 4 provide a summary of the results for the scenario (ii). Again, the proposed method clearly outperforms the two competitors. If one assumes the resulting criteria from multiple replications are normally distributed, then the two-sample -test would yield significant differences between \"Multi-Resp\" and \"Uni-Resp\" (with -values less than 0.001) for all combinations of , except for ( -value ) and ( -value ). Moreover, differences between \"Multi-Resp\" and \"Vectorized\" are significant for all situations ( -values ). It is also noteworthy that, in most multi-response literatures, the models are of a similar form as in the scenario (i). This does not necessarily imply that all the multiple response variables must admit exactly the same association with the predictors. The magnitude of those coefficients could vary. In our simulation, for simplicity, we only let the signal take the value of 0 or 1. The proposed method works best under the scenario (i), but outperforms the competing solutions under the scenario (ii) as well. "}, {"section_title": "B. Stability, Convergence and Computation Time", "text": "We next investigate the stability of the algorithm with respect to the regularization parameter and the rank of the CP decomposition. We adopt the setting of scenario (i), the \"triangle\" signal, , , and . Fig. 5 shows the results of applying different values of and . For all combinations, the method successfully identifies the signal region. However, insufficient or excessive penalty would both adversely affect the quality of the recovered signal. The rank of the CP decomposition essentially offers a bias-variance tradeoff. A larger rank implies a more flexible model and a smaller bias, but also more unknown parameters and thus a larger variation, whereas a smaller rank implies a more parsimonious model and a smaller estimation variability, but possibly a larger bias. In reality, the true signal tensor is hardly of an exactly low-rank structure. However, given the usually limited sample size in imaging studies, a low-rank estimate often provides a reasonable approximation to the true tensor regression parameter, even when the true signal is of a high rank [26] . In our analysis, we usually fix the rank at , which offers a good balance between model complexity and estimation accuracy. Fig. 6 shows the convergence behavior of the algorithm, as reflected by the objective value, under 100 randomly generated starting values, and the corresponding computation time. We see that, although there exist multiple local minima, the algorithm often converges to the same or similar point. The run time is recorded on a standard laptop computer with 2.9 GHz Inter i7 CPU. For instance, the median run time of fitting a rank-3 model in this example takes about 21 seconds."}, {"section_title": "C. ADNI Data Analysis", "text": "We analyze the ADNI dataset reviewed in Section II. The analysis consists of two parts: estimation of clinical scores and identification of brain subregions that are highly relevant to the clinical outcomes. We first aim to infer the clinical scores of MMSE and ADAS-Cog given the MRI scans. Such prediction is useful for both disease diagnosis as well as understanding of disease progression. The two clinical scores are normalized (subtracted the mean and divided by standard deviation) to avoid different response scales. A 10-fold cross-validation is performed. The rank of the coefficient tensor is set as , and the regularization parameter is optimized based solely on the training set with another nested 10-fold cross-validation. We then employ the resulting model to estimate the clinical scores on the testing data. The RMSE (the smaller the better), and the Pearson correlation coefficient (the larger the better), between the predicted and the observed clinical scores on the testing data are reported in Table IV . Moreover, we show the scatter plots of the predicted versus observed scores for MMSE and ADAS-Cog in Fig. 7 .\nWe also compare our method (\"Multi-Resp\") to two sets of alternative solutions. The first set consists of the univariate response solution (\"Univ-Resp\"), and the vectorized solution (\"Vectorized\"), as reported in Section IV-A. Both methods, as well our proposed method, directly model an image tensor and jointly incorporate all voxels of an image. The second set of alternatives include the multi-task method (\"M3T\") comprised of feature selection via a group lasso penalty and estimation via support vector regression [10] , support vector regression with feature selection via lasso (\"SVR lasso\"), and without any feature selection (\"SVR\"). It is important to note that, this family of methods do not directly handle a tensor image, but a vector of features extracted from an image. As such, we employ the Automated Anatomical Labeling (AAL) [36] to partition the image into 90 regions of interest (ROI) and then use the average intensity of each ROI as the extracted features. The results are again reported in Table IV . We see that, our solution clearly outperforms the one that models one response at a time, demonstrating the advantage of jointly modeling the multivariate responses that are correlated and complementary. We also see that, after taking the standard error into account, our new method performs essentially as well as the best solutions in the literature such as \"M3T\" and \"SVR lasso\". On the other hand, our method works without requiring a specific image atlas, and thus avoid its dependence and the choice among different atlases. In addition to the estimation of multiple clinical scores, the proposed method can simultaneously serve as a tool to select relevant brain regions. Our multivariate method is advantageous, since different clinical scores reflect the same underlying pathology while they also offer complementary information [8] , [10] , [14] . For this data, we use the optimal tuning parameter from the cross-validation to fit the full dataset. The voxels selected are those with non-zero regression coefficient estimates. To help visualize the selected regions, we next partition the brain into 90 ROIs based on the AAL. Then we plot the corresponding ROIs with at least 10% of its voxels selected by our method. Ordered by the percentage of selected voxels from highest, the identified regions that are relevant to the two clinical scores are: amygdala (left and right), hippocampus (left and right), parahippocampal gyrus (right and left), olfactory cortex (left), superior temporal gyrus (left), middle temporal gyrus (left), putamen (right and left), and insula (left). These regions have been shown by numerous studies to be highly relevant to AD. See Table V for a summary of the associated literatures. Moreover, to show the path of region selection, we repeat the same procedure with a gradually increasing sparsity tuning parameter . We summarize the results in Fig. 8 , where we map the estimate back to the original high-resolution MRI image, and Table V . It is worth noting that a smaller tuning parameter would result in a larger number of selected regions and the potential problem of overfitting. Conversely, a larger tuning parameter would result in a smaller number of selected regions and potentially underfitting."}, {"section_title": "V. DISCUSSION", "text": "We have proposed a sparse multivariate response tensor regression model in this article. Our proposed method models multiple response variables jointly, so as to exploit the correlated and complementary information possessed in multivariate responses. It also models multiple voxels in an image tensor jointly, so as to account for inherent spatial correlation in image covariates. The method is designed to simultaneously infer multiple responses and to identify brain subregions highly relevant to the outcomes. As such it is useful for both AD/MCI diagnosis, and for locating brain regions contributing to the disease. Our numerical analyses have demonstrated that the proposed method outperformed its competitors.\nThere are some alternative choices within our proposed model formulation. One is to consider a different penalty function than the group lasso penalty (6) , and the other is an alternative tensor regression model formulation than (3). First, an alternative to the group lasso penalty (6) for multi-response regression is the type penalty [22] . In our context, the penalty function takes the form, (8) This penalty, similar to the group lasso penalty, also induces row-wise sparsity to the regression parameters. However, it differs from the group lasso penalty in that, the penalty selects predictors based on their maximum contribution to any of the response variables, whereas the group lasso penalty selects predictors based on their joint contribution to all of the response variables. As a result, the penalty tends to select more variables than the group lasso penalty. The optimization with the penalty (8) is a linearly constrained quadratic optimization problem, which can be solved by an interior-point algorithm [45] . Second, an alternative to the multi-response tensor regression model (3) is the model (9) where is a -dimensional tensor that is assumed to admit a rank-CP decomposition, and denotes its modematricization. That is, where , for , and . To better understand this model, we again consider its special case when , , , i.e., two response variables with a matrix image predictor. In this case, is a tensor that admits a rank-1 decomposition, , and . Then model (9) becomes (10) A few remarks are in order for the comparison of model (10) with model (4), which is a special case of model (3) (10) requires fewer number of parameters and thus induces less estimation variability. For our problem, we note that, since the scores of MMSE and ADAS-Cog carry different levels of information with respect to the cognitive capability, it is more reasonable to assign different coefficients in predicting the two clinical scores. This is the reason we have primarily focused on model (3).\nFinally, an alternative tensor decomposition, the Tucker decomposition [33] , can be employed in our solution. The Tucker decomposition is more flexible than the CP decomposition, by allowing different number of factors along each mode of the tensor. However, it may introduce a larger number of unknown parameters than CP and require more parameter tunings. As such we have chosen the CP decomposition in this article.\nis coordinated by the Alzheimer's Disease Cooperative Study at the University of California, San Diego. ADNI data are disseminated by the Laboratory for Neuron Imaging at the University of California, Los Angeles."}]