[{"section_title": "Abstract", "text": "ABSTRACT In recent years, the radical advancement of technologies has given rise to an abundance of software applications, social media, and smart devices such as smartphone, sensors, and so on. More extensive use of these applications and tools in various industrial domains has led to data deluge, which has fostered enormous challenges and opportunities. However, it is not only the volume of the data but also the speed, variety, and uncertainty, which are promoting a massive challenge for traditional technologies such as data warehouse. These diverse and unprecedented characteristics have engendered the notion of ''Big Data.'' The data-intensive industries have been experiencing a wide variety of challenges in terms of processing, managing, and analysis of data. For instance, the healthcare sector is confronting difficulties in respect of integration or fusion of diverse medical data stemming from multiple heterogeneous sources. Data integration is critically important within the healthcare sector because it enriches data, enhances its value, and more importantly paves a solid foundation for highly efficient and effective healthcare analytics such as predicting diseases or an outbreak. Several data integration technologies and tools have been developed over the last two decades. This paper aims at studying data integration technologies, tools, and applications within the healthcare domain. Furthermore, this paper discusses future research directions in the integration of Big healthcare data."}, {"section_title": "I. INTRODUCTION", "text": "Healthcare is a highly data-intensive industry [1] . The everincreasing trend of healthcare data has already led to a massive growth of the volume. It was predicted that the data of the U.S.A healthcare sector alone would soon reach the Zettabyte scale and, not long after, the Yottabyte [2] . The increased usage of the term Big Data in healthcare literature is also an indicator of the emerging importance of large-scale data sets in healthcare and biomedicine [3] , and there is also an increasing awareness of the role that Big data can play in scientific and clinical research [4] .\nThe exponential growth in healthcare data has been forecasted to continue expanding in various forms, such as electronic health records (EHR), patient-reported outcomes, biometric data, medical imaging, biomarker data, wearable devices, and genomic information. These data are primarily\nThe associate editor coordinating the review of this manuscript and approving it for publication was Zehong Cao. stemming from multiple heterogeneous sources. Figure 1 shows some of the primary sources of healthcare data, which include medical service providers, pharmaceutical industries, public healthcare organizations, researchers, and medical insurance etc. The integration of such vast, real world, clinical data sets from Electronic Medical Record (EMR) with omics data, as well as targeted biochemical and hormonal analyzes, makes it possible to discover new diagnostic and therapeutic tools [5] as well as capture the full complexity of diseases [6] . In one elegant example [7] , the integration of continuous sensing of blood-glucose along with the evaluation of the gut microbiome, anthropometrics, drugs, dietary habits, and a variety of lab tests on 800 individuals, as illustrated in Figure 2 , was used to predict postprandial glycemic index, which has provided accurate information on dietary regimens to improve metabolic homeostasis.\nFeldman et al. [8] presents multiple unintegrated medical data pools controlled by six stakeholders: providers, payers, researchers, developers, consumers and marketers, VOLUME 7, 2019 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/ FIGURE 1. Sources of big data in healthcare.\nand government. These sources have been producing healthcare data for many years; consequently, the storage of healthcare data has become an ever-increasing container. Furthermore, lately, the social media mainly, the Twitter 1 has been explored as a data source that contains many different types of data of value to healthcare research on many various diseases [9] . These data are insightful, and therefore, Twitter is becoming a vital source of healthcare data. However, it is worth noting that Twitter data flow with high velocity (speed). 2 Data stemming from multiple heterogeneous sources increase data variety and uncertainty. Uncertainty is concerned with the quality of data. The quality of healthcare data is of critical importance to perform effective data analysis to extract meaningful intelligence that helps in decision making. The assessment of the quality of evidence to be derived is crucial; it will depend on the data sources to be integrated such as social network [11] or public repositories [12] , together with the standard quality indicators as selection bias, sample size, and measurement noise [13] . Variety is a well-known issue in Big Data. A wide range of unstructured data is available in the healthcare sector, including MRI image, surgical video, text, recorded conversation with patients. Also, there are structured data, such as EHR data. This essentially means that the technologies used in healthcare domain must be able to process data with diverse types.\nIn the light of the above discussion, we outline that healthcare data has four properties volume, huge velocity, significant variety, and substantial uncertainty (veracity) which constitute the notion of Big Healthcare Data. These characteristics foster a wide variety of challenges or barriers for the 1 Twitter: https://twitter.com/?lang=en 2 Every second, on average, around 6,000 tweets are tweeted on Twitter [10] users of healthcare data and also for the technologies that are used in the healthcare domain. The major challenges involved in Big Healthcare Data include: data integration, data processing and anlaysis. In fact, for healthcare data, integration is a huge obstacle, mainly due to the variety and velocity of data. According to Martin et al. integrating unstructured data is a huge challenge for the Big Data analyst [4] . Even with large scale structured EHR data, there are still many integration issues [14] .\nOver the last decade, an exhaustive number of Big Data tools and approaches have been proposed. Many solutions are available for dealing with significant data issues and challenges. These solutions have been reviewed in a large body of literature. For example, Merelli et al. [15] focus on technological aspects related to Big Data analysis in biomedical informatics including architectural approaches for big data, solutions for data annotation, data access measures, and security for health data. Priyanka and Kulennavar [16] discuss the definition of big data and characteristics of big data analytics in healthcare, and describe various sources and data types. Luo et al. [17] review the recent progress and breakthroughs of big data applications in four healthcare domains: public health informatics, clinical informatics, bioinformatics, and imaging informatics. Jee and Kim [18] explain how reforming the healthcare system based on big data analytics, could effectively reduce health concerns such as the selection of appropriate treatment paths, improvement of health systems, etc. However, big data analytics in healthcare require data integration to be successful. Unfortunately, the issues of healthcare data integration and utility have been largely overlooked in the current literature [19] .\nLenz et al. [20] review and discuss integration technologies in healthcare systems, where they identify technological integration (based on technological infrastructure) and semantic integration (based on the meaning of the data). Authors propose a document-based approach to support integration in healthcare networks. Zhang et al. [21] discusses several approaches proposed for data integration in bioinformatics, which is classified into five groups: data warehousing, federated databasing, service-oriented integration, semantic integration, and wiki-based integration. As Big data is often heterogeneous, noisy, unreliable, and dynamic; in this context, these traditional approaches do not apply to the nonrelational, schema-less dataset.\nWhat we instead aim to provide in this paper is a comprehensive survey of Big Data integration from different standpoints. We will study the developments in advanced solutions for Big healthcare data integration. We will present the design and development of varying integration strategies that are commonly adopted by the research communities. We will discuss current challenges concerning Big Healthcare data integration. Furthermore, we will discuss some possible future research directions in this area.\nThis remainder of the paper is organized as follows. In section II, we provide a background of concepts, tools, and technologies related to Big Healthcare Data integration. Illustration of various data sources to predict personalized nutrition [7] .\nIn Section III, we put forward our comprehensive survey. In section IV we discuss our findings followed by research directions presented in Section V. We conclude in Section VI."}, {"section_title": "II. BACKGROUND", "text": "In this section, we study different concepts related to Big Healthcare Data integration and provide extensive details about these concepts. This will help the readers to understand the underlying challenges and techniques of data integration."}, {"section_title": "A. BIG DATA", "text": "Over the past two decades, a deluge of data has been brought with the technological advancements from several fields (e.g., medical data and scientific sensors, user-generated data, financial data, and other) [22] . Big Data has been defined in a large volume of literature. It has been defined as datasets that could not be perceived, acquired, managed, and processed by traditional IT and software/hardware tools within an acceptable time [23] . However, the concept of Big data has been presented through the 3V model, which refers to high-volume, high velocity, and high-variety information assets [24] . Lately, this notion has been extended to a 5V model by including two new ''Vs.'': Value and Veracity which are incorporated into the Big data definition [25] .\nWithin the Healthcare sector, various definitions of Big data are found [26] ; this term was introduced in [27] as ''Big data in healthcare encompasses high volume, high diversity in biological, clinical, environmental, and lifestyle information collected from single individuals to large cohorts, in relation to their health and wellness status, at one or several time points.'' Several solutions have been proposed to tackle these challenges. These solutions rely on the most recent and advanced technologies: Apache Hadoop framework [28] , NoSQL [29] and Cloud computing [30] .\nThe advent of Big Data has given rise to a new concept called data lake. A data lake is a repository to store a vast amount of raw data in its native format. The term data lake is often associated with Apache Hadoop-oriented object storage. Hadoop provides these techniques through Apache Hadoop YARN and HDFS [31] . YARN presents the next generation of Hadoop compute platform and offers a pluggable architecture and resource management for data processing engines to interact with data in HDFS [32] . Data lake can be a powerful approach to resolve the problem of accessibility and integration of Big data [33] .\nThe data lake is different from the traditional Data Warehouse from various aspects. Even though relational data warehouses have led the complex integration and analytics, their slow-changing data models and rigid field-to-field mappings are too brittle to support Big data volume and variety. By contrast, data lake approach circumvents these problems, because data lake does not enforce a rigid metadata schema as do relational data warehouses [34] . Instead, data lake support a flexible ''schema-on-read'' access to all enterprise data, through multi-use and multi-workload data processing on the same sets of data, from batch to real-time [35] . The data flow in the data lake aims at decoupling the metadata from the raw data and storing them separately. In this way, end-users have the potential to query data from multiple perspectives (see Figure 3) . However, despite the robustness and availability of Big data tools, building and deploying Big data solutions is difficult. Therefore, domain-specific solutions are still needed. These solutions often depend on different data dimensions as well as the type of data and the target to be studied."}, {"section_title": "B. DATA INTEGRATION", "text": "The goal of data integration is the provisioning of unified access to data that requires information from multiple sources and providing users with a unified view of the data [36] . Ziegler and Dittrich [37] explain the reasons behind integrating multiple data sources, which are twofold:\n\u2022 Facilitate information access by providing an integrated view to a set of existing information systems.\n\u2022 Gain a more comprehensive basis by combining data from different complementing information systems. The traditional data integration process is assumed to be a three-step process (figure 4) where the last step is referred to, as data fusion. In this step, the duplicate representations of data are combined and fused into a single image while inconsistencies are resolved. The two other steps are schema mapping and duplicate detection [38] .\nThere is a wide variety of challenges regarding data integration. One of the key challenges is dealing with the problems emerging from the heterogeneity of data sources [39] . Dong and Naumann [40] introduce several challenges of data integration:\n\u2022 heterogeneity at the schema level, where different data sources often describe the same domain using different schemas, as well as heterogeneity at the instance level, where different sources can represent the same real-world entity in different ways. Several solutions proposed to address the heterogeneity challenge, at the schema level, as well as schema mapping and matching.\n\u2022 Data conflicts that can arise because of incomplete data, incorrect data, and out-of-date data. Data fusion addresses this second challenge by fusing records on the same real-world entity into a single record and resolving possible conflicts from different data sources. To sum up, data integration is the process of linking and connecting systems and giving users the illusion of interacting with one single information system. This process often encompasses a fusion step. However, the integration methods of such a traditional integration process, which focus on structured data sources, need to be significantly expanded to integrate a variety of data sources, both structured and unstructured. In particular, pairwise matching of schemas and entities is not scalable enough [41] ."}, {"section_title": "C. HEALTHCARE STANDARDS FOR DATA INTEGRATION", "text": "Interoperability is the ability of two or more components, applications, or systems to exchange and use information.\nIn health care, interoperability is the ability of the technologies to facilitate the integration of patient data from different systems. To achieve interoperability, healthcare organizations (management staff, vendors, service provider entities, etc.) have created Healthcare standards. Below is a list of key standards organizations relevant to the health sector [42] :\n\u2022 OpenEHR: The objective of the OpenEHR Foundation is to leverage ICTs, in particular, life-long interoperable EHRs to improve the quality of healthcare and research. The main work of the openEHR Foundation is performed by four 'programs' which respectively focus on specifications, clinical modeling, software, and education.\n\u2022 Health Level Seven (HL7): It provides a comprehensive framework and related standards for the exchange, integration, sharing, and retrieval of electronic health information.\n\u2022 International Health Terminology Standards Development Organisation (IHTSDO): determines global standards for health terms, an essential part of improving the health of humankind. It is committed to maintain and grow its leadership as the global experts in healthcare terminology, ensuring SNOMED CT, its world-leading product, is accepted as the universal common language for health terms.\n\u2022 World Health Organisation (WHO): The WHO is the directing and coordinating authority for health within the United Nations system. It is responsible for providing leadership on global health matters, shaping the health research agenda, setting norms and standards, articulating evidence-based policy options, providing technical support to countries, and monitoring and assessing health trends. However, data is one of the most critical aspects of the healthcare system. Therefore, interoperability of healthcare systems requires the integration of standard Data Models, Terminologies, and Messaging standards. [43] ."}, {"section_title": "1) HEALTHCARE DATA MODELS", "text": "The healthcare data models define the structure of the information to be stored in Electronic Health Records (EHRs). The most popular and recognized clinical data models are:\n\u2022 openEHR clinical model [44] "}, {"section_title": "2) HEALTHCARE TERMINOLOGY", "text": "Healthcare terminology or coding systems are structured list of terms, which provide specific codes for clinical concepts such as diseases, operations, allergies, drugs, and diagnoses. These vocabularies can be used to support the recording and reporting of patient care at different levels of detail. Examples of terminology standards are :\n\u2022 Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT) [47] : SNOMED CT is a medical terminology that includes terms of all medical domains and provides the general core terminology for the EHR. The concepts in SNOMED CT are divided into hierarchies as diverse as body structure, clinical findings, geographic location, and pharmaceutical/biological product.\n\u2022 The International Classification of Diseases (ICD) [48] :\nThe standard diagnostic tool for epidemiology, health management, and clinical purposes. It is used to monitor the incidence and prevalence of diseases and other health problems. There are two major revisions of ICD in use; ICD-10 and ICD-9, which are represented as entirely separate code systems.\n\u2022 Logical Observation Identifiers Names and Codes (LOINC) [49] : LOINC is a database and universal standard for identifying medical laboratory observations. Since its inception, the database has expanded to include not just medical and laboratory code names, but also: nursing diagnosis, nursing interventions, outcomes classification, and patient care data set."}, {"section_title": "3) HEALTHCARE MESSAGES", "text": "To provide integrated patient care, the different clinical systems of a hospital need to communicate with each other. Message standards provide a consistent data flow among systems and organizations, specifying the format, data elements, and structure.\n\u2022 HL7 messaging standard [50] : It is arguably the most widely implemented standard for interoperability in healthcare across the world and allows for the exchange of clinical data between disparate systems. HL7 is used for transmitting data related to patient charts, files, and other associated documents and audio recordings.\n\u2022 Digital Imaging and Communications in Medicine (DICOM) [51] : DICOM is the de facto standard for exchanging medical images. It defines the formats for medical images that can be exchanged with the data and quality necessary for clinical use. Unfortunately, these standards did not solve the problem of data integration. Brooks [52] pointed out some issues that organizations confront due to inconsistencies with data standards. For example, issues related to data exchange arise because the standards and terminologies are not designed to serve multiple purposes. Moreover, there are overlaps among standards; many standards have been named by one or more authoritative body, for example, HL7 and Accredited Standards Committee X12 (ASC X12) have some duplication in standards used for reporting of clinical data associated with the claims process."}, {"section_title": "III. INTEGRATION OF BIG HEALTHCARE DATA -THE SURVEY", "text": "With current trends in technology, Big data integration is turning into a complex process. Subsequently, there is no single methodology to suit all these data formats and requirements each provider brings. Effective data integration from heterogeneous massive amounts of data often requires a complex framework involving various methodologies. Putting together such a framework would be complicated. Therefore, we introduce a data integration framework (Fig. 5) a combination of Approaches, Technologies, Workflow, Tools, and Privacy, which ultimately helps to combine data from disparate sources into meaningful and valuable information.\nAn exhaustive number of methodologies have been proposed in large bodies of literature. Using the proposed framework as a blueprint, we studied these methodologies and reported in the following sections."}, {"section_title": "A. INTEGRATION METHODOLOGIES FOR BIG HEALTHCARE DATA", "text": "There are two main approaches for data integration: eager and lazy [53] .\n\u2022 Eager: In the eager or movement approach, the data from each source that may be of interest is extracted in advance, translated and iterated as appropriate, fused with relevant data from other sources, and stored in a centralized warehouse. When a query is posed, the query is evaluated directly at the repository, without accessing the original information sources.\n\u2022 Lazy: In the lazy or mediated approach, the data is extracted from the sources only when queries are posed. First, the query determines the appropriate set of information sources to answer the query and generates the appropriate sub-queries or commands for each information source. Second, the query obtains results from the information sources, performs appropriate translation, iterating, and merging of the information, and returns the answer to the user or application. There are effectively three Approaches of Big Data Integration. Table 1 introduces these Approaches as follows:\nData consolidation refers to the collection and merging of data from multiple sources systems into one integrated place. Data consolidation phases include: (1) analysis of data models and datasets of the source and target environments; (2) transformation of the source data set; and, (3) merging of the data sets [54] . The data warehouse is such an example. Bill Inmon described a data warehouse as being a subjectoriented, integrated, time-variant, and nonvolatile collection of data [55] that is considered a core component of business intelligence. Data warehouse was built specifically for relational databases. However, the complex characteristics of data, including Volume, Variety, Velocity, Veracity, promote the need for new technology to handle new demands. Today, multiple data warehousing techniques rely on Hadoop platform to meet the new requirements of Big Data [56] , [57] . Figure 6 shows a modern architecture of Big data warehouse where the raw data stream is stored in the HDFS system and then loaded into the data warehouse to complement the information that is already gathered.\nTwo different approaches could be found in the Big data consolidation architecture [58] , which combine BDW(Big data warehouse) and EDW(Enterprise data warehouse) to implement data integration solutions (see Figure 7) . In application architecture approach A, data ingestion mechanism is performed by the Hadoop platform, then specialized data integration tools are used to move data into RDBMS. Therefore, this approach is flexible in ingesting any data and also to address scale issues. In architecture B, the data that appear with Big data characteristics are stored and processed in the Hadoop platform, whereas RDBMS is used to store and process small and structured data. In the final stage, the information is available from both data stores.\nSeveral Big data warehouse based solutions have been proposed in the literature, such as specialized disease clinics [59] , combining clinical and genomics queries [60] , and a semantic warehouse to support the digital cancer patient [61] (see figure 8 ).\nApache Hive [63] often regarded as a distributed data warehouse infrastructure that enables easy data ETL from HDFS or other data storage like HBase. It provides HiveQL as a high-level query tool for accessing data. Cloud-based approach for interoperable EHRs [64] , biomedical data integration [65] and medical Big data processing system [66] are examples of Hive Data warehouse. Data variety is common in healthcare data. Therefore, the data lake is deemed as a potential solution for integrating diverse medical data. For instance, the classical data warehouse was often used to analyze the cost of care. The advanced cost analysis requires to integrate EHRs data with claims data. In the classical data warehouse, the process began with unloading the data in the warehouse then execute a new extract, transform, and load process. However, a data lake makes it more simple to enter a new data source or add advanced operation (queries, algorithms, etc.) [67] . Krause [68] summarized data lake as a matured Big data consolidation solution.\nTo sum up, conventional technology data warehouse can consolidate data with straightforward characteristics such as being small and structured. However, for Big Healthcare datasets specifically, data that comes with diversity pose a significant challenge on these technologies, and hence advanced technologies such as data lake for storing data in a scalable ecosystem and Apache Hive for warehousing data can be more efficient. However, Hive is a batch style warehousing solution; similarly, Apache Hadoop that is used in building scalable warehouse relies on batch style operational mode."}, {"section_title": "2) DATA VIRTUALIZATION", "text": "In the last decade, data sources have increased beyond the traditional structured world of databases and data warehouses to the stage of Big data where semi-structured and unstructured data are more common as well as some of it is stored outside of the local system. Moreover, the data warehouse is designed to host structured and mainly internal data from operational and transactional systems. Besides, building an enterprise data warehouse is a costly initiative that takes long to implement, and it is not always practical to move massive data from one source to another. Therefore, the data virtualization technique could enhance data integration and help to adopt new Big data source and modern formats [69] .\nData virtualization is a process that federates different data sources, such as website data sources, relational databases, file repositories, document files, and data service providers into a single data access layer. This new abstraction layer makes all integrated data sources seem as one extensive database is being accessed. However, to merge these different data sources, data virtualization uses several abstractions, transformation techniques, and data access mechanisms [70] . SAP HANA [71] is a Smart Data Access data-virtualization approach that enables unified access to heterogeneous data sources with a massive volume of data in real-time using in-memory processing. It provides Big data integration, allowing connection to data stored in Hadoop and NoSQL systems as SQL tables. Figure 9 illustrates the key components of SAP HANA architecture. Data federation is a type of data virtualization where on-demand integration is used for combining data residing within different data stores [72] . For instance, Hadoop offers an HDFS Federation [73] technique to federate different instances at the HDFS level through a separation of namespace and storage, enabling a generic block storage layer. In the healthcare domain, data federation technique was used in many research projects such as e-Health Service [74] , BioFed [75] , and Genomic Computing [76] ."}, {"section_title": "3) DATA PROPAGATION", "text": "Traditional batch-oriented ETL processing cannot satisfy the real-time requirements when data is integrated continuously and concurrently. Hence, the approach called data propagation was proposed. Data propagation is usually referred to as active data integration [77] , where a copy of the data from a data source or multiple data sources to discrete locations is done, often to make data more accessible to users. This process usually operates online and with event-driven architecture(push mode). In general, the process of change propagation could have multiple stages such as transformations and filtering of the changed data that is delivered to the dependent systems [78] . Constantinescu et al. [79] propose SparkMed as a data integration framework for mobile healthcare. An automated process (daemon) of the framework could be attached to applications, collects the multimedia data (such as the hospital information system, picture archiving, and reporting systems) and prepares them to be propagated in a convenient, reliable manner. SparkMed can integrate a wide range of different medical software and database systems into a cloud-like peer-to-peer multimedia data store."}, {"section_title": "B. INTEGRATION TECHNOLOGIES FOR BIG HEALTHCARE DATA", "text": "The complexity of the variety in Big data is well known, including complex heterogeneous data types (structured data, unstructured data, and semi-structured data, etc.), complex intrinsic semantic associations in data (clinical, genomic, etc.), and complex relationship networks among data (relationships between entities) [80] . For instance, integrating diverse data types such as clinical data, gene expression, DNA methylation, miRNA expression, and copy number alterations (CNA) has improved the prognostic prediction of glioblastoma multiforme [81] . The heterogeneity of different types of data mentioned above creates a challenge for the data integration process. The integration process requires a specific mechanism for aggregating data that develops specifically to deal with the nature of healthcare data to overcome these complexities. Data integration technologies such as semantic web, machine learning, information extraction, and linked data can enhance understanding the context of information. In this section, we discuss the existing technologies for data integration. "}, {"section_title": "1) SEMANTIC WEB", "text": "Semantic web standards are a perfect fit for data integration capabilities [82] , with multiple aspects to integrate data from globally separate, distributed, and heterogeneous data sources. Semantic web technology is a robust and extensible data model used for global naming, with the ability to reason based on Description Logic [83] . The semantic web comprises publishing information in languages specifically designed for data: Resource Description Framework (RDF), Web Ontology Language (OWL) and SPARQL (a query language for semantic web data sources), Figure 10 shows the different layers of the semantic web framework. According to this framework, information is represented in statements, called RDF triples. The three parts of each triple are called subject, predicate, and object. Besides that, OWL and Linked Open Data, which use RDF as the data model, have gained popularity in data integration.\n\u2022 Web Ontology Language (OWL): is a W3C standard [84] , that offers great machine interpretability for the web content. Moreover, OWL is used for the harmonization of integrated data from diverse sources. [92] , since it has the ability to merge multiple data sources and publish them onto the cloud [93] .\n\u2022 Linked Open Data (LOD): describes a method of publishing and linking structured data coming from different data sources that can be interlinked and published on the web. An increasing number of data providers have adopted Linked Data principles as a data structure. Linked data is resulting in the emergence of global data space on the web containing billions of RDF triples [94] . The linked data paradigm can integrate Big data by the mean of annotating unstructured data with open linked data from the cloud, which leads to linking those heterogeneous datasets to each other. Linked data is one of the widely used technologies for data integration in the healthcare sector. A variety of genomic and drugrelated datasets as Linked data were published by members of the W3C Healthcare and Life Sciences Interest Group (HCLS IG) [95] . LOD datasets have been crawled by the Semantic Web Search Engine (SWSE) and can be accessed via a faceted browsing interface. Certain datasets are interconnected through semantic vocabularies such as ''sameAs,'' ''seeAlso'' (see figure 11) , where others remain challenging to define a methodology for linking them with other Linked Data sources [96] . Described above techniques are capable of addressing numerous problems of data integration in the healthcare context. Many projects already used OWL and LOD to enhance data integration such as integrating heterogeneous wearable data in healthcare [97] , support of digital cancer patient [61] , obesity surveillance [98] and in the interoperability of electronic health records [64] . Big linked cancer data [99] presents a scalable Linked Data-driven solution for the continuous integration of biomedical data sources. The integration process relies on three types of datasets that are loaded into various SPARQL endpoints: Linked data version of TCGA and PubMed metadata in RDF and a set of mappings between these datasets. The latter aims at establishing a bridge between the structured data contained in TCGA and the constant flow of RDF data generated by analyzing PubMed. The bridging process is performed by matching the synonyms for every disease and gene found in Linked TCGA with PubMed article's abstract. The scalability of the framework is ensured through the novel TopFed federated query engine.\nSearching and exploring data about medicinal products and drugs from different data sources are essential requirements for physicians, to cover those information requirements, Kozak et al. [100] identify drug data sources such as MeSH, ATC, NDF-RT, NCI DrugBank, CZ-Drugs, and FDA, to integrate them. The data sources have a different format with structured and unstructured data. In [96] , the authors provide a solution which enables the analysis of existing Linked Data representations of each considered data source then it allows the creation and publishing of new Linked data for those with no Linked Data representation. Authors recommend linking the dataset to an accepted reference terminology, allowing anyone else to connect their datasets to this reference terminology and thus enabling integration with other datasets. Accordingly, technical challenges for broader adoption of the Semantic Web standards for Big Data include large-scale data reasoning and performance optimization of semanticbased systems. Another limitation of the Semantic Web technologies is that they are purely for graph data representation. Therefore, if the data is unstructured, it would not be enough to use these techniques alone for building a data integration solution."}, {"section_title": "2) MACHINE LEARNING", "text": "Automatically learning mappings between two datasets through machine learning, can remove much of the VOLUME 7, 2019 development effort involved in data integration. Machine learning has been proving itself in many diverse domains such as image processing, social network mining, finance, and computer vision [101] . Moreover, using machine learning to integrate various datasets, such as Medical Imaging, Electrophysiological monitoring, Clinical Information, has improved the accuracy of diagnosis and prediction of disease outcomes [102] - [104] . Generally, Machine learning methods can be divided into two main classes: supervised learning (predictive) and unsupervised learning (descriptive):\n\u2022 Supervised learning takes samples of training data with known labels as input to learn a general prediction rule mapping new data samples to existing labels. For example, the input data may include affected and unaffected diabetic patients. Thereby, a model is learned to help and accelerate the diagnosis of Diabetes disease [105] . Classification and regression are the main methods of supervised learning: in regression, the output variable is numeric (or continuous), while the one used for classification is categorical (or discrete). Some of the widely used classification techniques include Support Vector Machines (SVMs) that are widely applied to large medical datasets [106] . Its main drawback lies in colossal time and memory complexities, which depends on the training set size cardinality and heterogeneity [106] . Random Forest is another useful and easy to understand classifier. Due to its space limitation and overfitting problem, Random Forest may not be applicable for huge datasets [107] . Besides, regression algorithms are also widely used in health applications to model relationships between objects and targets [108] [109].\n\u2022 Unsupervised learning includes clustering which takes as input an unlabeled dataset. A model is learned by finding the structure of unknown input data and looking for the similarity between entities. Many algorithms can be used for clustering purpose, including partitioning, hierarchical, and density-based [110] . Only the partitioning algorithm is capable of handling large datasets, while hierarchical and density-based are slow for large datasets. These methods are often used in pattern discovery in gene expression data and molecular subtyping of cancer patients. Artificial Neural Networks ANN is a family of machine learning approaches whose models are hierarchical representation of supervised and unsupervised learning models. In a neural network, associations between result and input variables are described using multiple combinations of hidden layers of pre-specified functions. The purpose is to estimate weights using input data and results to minimize the average error between the results and their predictions. However, the disadvantages of the neural network when dealing with the Big data are the requirement for constant memory consumption and the computational time [111] . ANN has been used in various health care applications, such as diagnose cancer [112] , diagnose Parkinson's disease [113] , etc. A modified version of ANN called Deep Learning, which builds neural networks with a large number of hidden layers. Deep learning architecture (i.e., deep neural networks DNN, convolutional neural networks CNN, recurrent neural networks RNN) provides better capabilities for dealing with Big Data issues, such as volume and velocity [114] . Deep-learning systems can accept multiple data types as input, an aspect of particular relevance for heterogeneous healthcare data (figure 12)."}, {"section_title": "FIGURE 12.", "text": "Deep learning can be trained on a variety of data types (images, time-series, etc.) [115] .\nFrom the beginning, supervised, unsupervised, and hybrid machine learning approaches have been applied to the data integration field to support the integration process [116] . Examples include entity resolution that uses Decision trees, Logistic regression, and SVM [117] , as well as applying Deep learning model (e.g., Word2Vec, Par2Vec) to compare a long biomedical text [118] . At the same time, schema alignment adopted machine learning algorithms such as Naive Bayes and stacking to match types and attributes [119] .\nSince there is no single traditional machine learning technique which can perform well for healthcare data integration, one application often employs a combination of several methods.\nIn [120] , a variety of data integration techniques from a machine learning view have been reviewed. It has concluded the following:\n\u2022 Feature concatenation: With the modern high dimensionality of data and rich structural information, feature concatenation is often impracticable.\n\u2022 Bayesian models: In general, these models can use prior information and model measurements with various distributions.\n\u2022 Tree-based methods: These models can be applied in two strategies, 1) build a tree with all features, 2) collectively make a final decision based on trees learned from each view.\n\u2022 Kernel methods: In a first step, Metric learning aims at fusing the similarity matrices learned from personal views together, then a final Kernel learning model combines similarity between results.\n\u2022 Network-based fusion methods: they can infer direct and indirect connections in a heterogeneous network.\n\u2022 Multi-view matrix factorization models: the model begins by extracting new features from each data view first and then incorporate these new features together. Finally, a classification or clustering algorithm can be applied to the combined features. These models have the potential to learn interactions among features from different views.\n\u2022 Deep learning: Different deep learning models can be applied to individual data views, and then the result is integrated with multi-modal learning for capturing the complex mechanism of systems. Moreover, this study has emphasized the importance of methods such as multi-view matrix factorizations and multimodal deep learning for data integration in the Bioinformatics domain.\nZolfaghar et al. [121] used machine learning techniques to study the 30-day risk of readmission for congestive heart failure patients. In this study, the income factor represents the primary predictor variable for risk of readmission (RoR). The first step is to map income value which is available in National Inpatient Sample (NIS) dataset (with 8 million records and more than 100 features) to the MultiCare Health Systems (MHS) data. However, due to privacy restrictions, it is not possible to link patients in NIS to patients in MHS data. To achieve this, K-means is used to cluster the NIS dataset by relying on three variables; age, gender, and elective hospitalization. Then the average income is calculated for each cluster, and the computed value is used to map each record of data in MHS to the closest cluster based on the Euclidian distance function.\nA hybrid machine learning method was applied to classify schizophrenia and control individuals by integrating fMRI and single nucleotide polymorphism (SNP) data. Two SVMs were used, one on fMRI data and one on SNP data, and then the results were combined into a single module using majority voting [102] .\nNapolitano et al. [122] have combined drug and protein structures, disease states, and drug toxicity using a kernelbased (KB) method. Each data is represented by a kernel matrix in a drug-centered feature space. After combining these kernel matrices into a single kernel matrix, the authors applied SVM for classification. The result was used for repurposing and sensitivity prediction.\nIn [123] , three independent deep neural networks (DNN) were trained in clinical data, gene expression, and copy number for predicting the prognosis of breast cancer. The final multi-modal deep network was obtained by joining the predictive scores of each independent model. However, the characteristics of Big data create scalability challenges for traditional techniques [124] . For example, estimating model parameters through iterative procedures used by various machine learning methods, including ANN, SVM, and DT, could not be easily scaled across large data sets. Therefore, non-iterative training algorithms are increasingly used by Big data applications [125] . As well, the volume challenge can be reduced using dimensionality reduction. Methods such as principal component analysis (PCA), Multimodal deep learning, Isomap have proven their effectiveness in dimensionality reduction [126] ."}, {"section_title": "3) INFORMATION EXTRACTION", "text": "In Big Data, Variety problem refers to different types of data collected via different data sources, such as text, videos, images, data logs, audio, and so on. Therefore, this data will not be in a format ready for integration. Thus, the need for a process that gets information from the underlying data sources and explores the relations between entities. The term 'Information Extraction' refers to the process by which structured, useful information such as entities, relationships between entities, and attributes describing entities are automatically derived from unstructured or semi-structured raw data. Moreover, information extraction prepares and facilitates different types of sources to be integrated and queried [127] . Subsequently, information extraction must be incorporated into the data integration workflow to make use of the extracted knowledge [128] .\nInformation extraction is interdisciplinary, involving data mining, statistics, computing linguistics, and machine learning. Several methods have been introduced for the extraction of information. Sarawagi et al. [127] categorized extraction methods along two dimensions:\n\u2022 Hand-coded or Learning-based: A hand-coded system needs experts to determine the appropriate rules or regular expressions or program snippets to perform the extraction. Conversely, learning-based extraction relies on unstructured data labeled to train machine learning models.\n\u2022 Rule-based or Statistical: Rule-based methods are driven by hard predicates (rules), while statistical learning methods make decisions based on a weighted sum of rule firings. The EHRs contain text dictations from several physicians, structured data from sensors and measurements, and unstructured data from video and image (MRI, X-RAY). These resources require the development of strategies to transform unstructured data in a structured form suitable for integration. Several methods of information extraction have been applied in textual parts of EHRs. Information extraction was used to detect drug safety signals by transforming clinical notes into a feature matrix encoded using medical terminologies [129] . Furthermore, i/n conjunction with the warning of increased cardiovascular mortality resulting from Cilostazol medication, Leeper et al. [130] employed a novel textanalytics pipeline to quantify the adverse events associated with Cilostazol use in a clinical setting.\nInformation extraction approaches for image data are different from textual data. For instance, a method was designed and implemented in [131] to manipulate 240GB of brain FIGURE 13. Generic workflow for Big data integration: Huge data from heterogeneous sources are received as input and valuable and manageable data are output. During the workflow, various processing stages are applied to reduce the size of data on the one hand, on the other hand, extract structured data from semi-structured and unstructured data. Each stage could benefit from techniques such as entity extraction and enrichment with semantic web technologies, and dimensionality reduction and classification with machine learning technologies. Under certain circumstances, some Big Data integration problems could be readily solved in such way.\nimage data for 1200 patients stored by the Alzheimer's Disease Neuroimaging Initiative (ADNI), to predict to what degree a patient has Alzheimer's. Different steps are executed, which include spatial normalization, extraction of features, feature selection, and patient classification. The proposed filter features selection method is based on mutual information as relevance measure and redundancy between the features through minimal-redundancy-maximalrelevance criterion (mRMR).\nAccuracy is the biggest challenge in healthcare information extraction. Precisely, we mean that when the information is extracted, it should be extracted correctly, with acceptable accuracy for clinical staff that should be approximately 95%. This adds a performance overhead over current treatment capabilities."}, {"section_title": "C. INTEGRATION WORKFLOW FOR BIG HEALTHCARE DATA", "text": "Organizations need data in a consumable format to assist analysts in the decision-making process. The main challenges are the following:\n\u2022 how to convert the raw data from multiple sources affected by heterogeneity format into clear and coherent information?\n\u2022 which workflow is enough to take structured and unstructured ones and convert them into insight?\n\u2022 how the data will be stored in the cluster?\n\u2022 what tools to use to process and integrate the data?\n\u2022 how to provide access to the end consumer? To an extent, the technologies used for Big medical data integration are similar to that of traditional Big data integration projects. The main difference lies in how integration is performed [22] . Several frameworks are being developed for medical Big data integration and fusion, as mentioned in subsection III-A. However, the majority of existing frameworks cannot cope with those challenges, which require multiple processing steps during integration. Therefore, we provide a generic workflow, depicted in figure 13 , that summarizes the stages of Big data integration process.\nAt each step, there is work to be done [132] , and there are Big data tools and technologies to be used [133] . The workflow process begins with data ingestion from datasources, followed by data storing, data filtering and cleaning, data annotation and labeling, metadata extraction, information summarization, and aggregation, and ends in the Data Warehousing step where the information is ready to analyze or visualize."}, {"section_title": "1) DATA INGESTION", "text": "The first step involves loading data from different sources in various formats into a single or clustered store. Data ingestion is modeled as a pipeline consisting of several incremental steps with clearly defined interactions. Services for data ingestion must be compatible with standards such as HL7, CDA, DICOM, and IHE XDS. Through this pipeline, all patient identifying data may be anonymized to ensure data privacy. Moreover, much of this data is of no interest, and so, can be filtered and compressed by orders of magnitude. For instance, a Big Data platform [134] was implemented for the analysis of medical data in the Mayo Clinic. The platform can ingest and store 62 \u00b1 4 million HL7 messages per day. The data ingested by the platform can be of any medical data type, be it structured, semi-structured, or unstructured data."}, {"section_title": "2) DATA STORE", "text": "After the ingestion phase, the raw data is pooled into a centralized and scalable storage location(the data lake), which is an intermediate storage area and working environment for data that typically represents source data in its original format. Due to the different data formats, Data Lake will consist of various systems, such as relational database for storing and managing structured data(demographic information, vital signs, etc.), NoSQL for semi-structured data(medical device reports) and file system for unstructured data(clinical narratives, notes, letters, reports, images and omics data). For instance, CancerLinQ [135] (Cancer Learning Intelligence Network for Quality) is designed as an oncology rapidlearning health care system. CancerLinQ accepts all data in any format that a practice chooses to send, then stores and maintains the original data in a data lake."}, {"section_title": "3) DATA FILTERING AND CLEANING", "text": "EMR illustrates well the need for data cleaning as it may provide noisy data containing incomplete information [136] . Processing raw data without preparation routines may require additional computing resources that are not affordable in the context of Big data. Data filtering is achieved by removing unnecessary information for health care monitoring based on a defined criterion, while data cleaning is accomplished using several components such as noise reduction, missing data management, and normalization. In [137] , k Nearest Neighbour and K-means are used to remove the noise from diabetes dataset and thereby improving the quality of data."}, {"section_title": "4) DATA ANNOTATION AND LABELING", "text": "At this stage, the workflow has to explore the different data formats like clinical notes, images, and scientific publications and must be able to discover, extract and annotate them with actual labels such as the name of the entity, relations between them, etc. Ontologies are applied to clarify the meaning of the concepts by using standardized terms across various data sources. A typical step in medical data processing that aggregates unstructured clinical notes is the identification of these medical concepts from UMLS using MetaMap [138] . UMLS uses as well, the notion of a Concept Unique Identifier (CUI) to map terms with similar meaning in different terminologies [139] . The difficulty with current labeling techniques is that they do not understand model relationships between classes. Ayala et al. [140] proposed a two-phase machine learning approach that computes novel features that take into account the relationships."}, {"section_title": "5) METADATA EXTRACTION", "text": "At the heart of Big data integration process is Metadata. The lack of well-defined schemas characterizes big data. Besides, data integration in such an environment is subject to frequently changing requirements. Therefore, this step should extract structural information describing the schema of the data and clarifying the semantics of metadata. The result of this stage will help the end user to query over structured and semi-structured data, and to discover associated schema between various datasets [141] ."}, {"section_title": "6) INFORMATION SUMMARIZATION AND AGGREGATION", "text": "Frequently, health data have volumes of hundreds of millions or billions of records per day and are not in size ready for analysis. Thus, it is essential for business intelligence to summarize and aggregate the required information from the heterogeneous sources and express them in a structured form appropriate for analysis. Summarized and aggregated data and their associated metadata are then used to create abstractions or pattern representations. For instance, summarization of an extensive medical record, allows a set of events within a facet to be recursively aggregated and replaced with summary events, such as a series of atenolol and propanolol prescriptions can be aggregated into the beta-blockers category [142] ."}, {"section_title": "7) DATA WAREHOUSING", "text": "Data lake tend to be complicated to navigate for users unused to working with unprocessed data. Furthermore, researchers and clinicians tend to favor data warehouses. Conversely, data scientists could apply dimensionality modeling from the data lake to prepare the datasets and then feed them back into a traditional data warehouse for decision analysis. This final step facilitates the integration of different data sources and reduces data movement and latency. For instance, supporting critical precision medicine use cases, an automated workflow has been implemented for incorporating sequencing results from both structured and unstructured sources into a researchcentric clinical data warehouse [143] .\nThe successful completion of the workflow enables the users to use the data for analysis."}, {"section_title": "D. INTEGRATION TOOLS FOR BIG HEALTHCARE DATA", "text": "Scalability, Reliability, and Maintainability are a vital consideration when it comes to Big data integration tools. While the available tools are mostly open source and wrapped around Hadoop and related platforms, there are many trade-offs that developers and users of Big data analytics in healthcare must consider. While the development costs may be lower since these tools are open source and free of charge, the downsides are the lack of technical support and minimal security [2] .\nVarious frameworks and tools have been implemented to meet the management of the ever-growing size of complex heterogeneous data, from data ingestion to data visualization. So far, most Big data tools do not provide a complete process for data integration, but they can be a part of an integration architecture to store and process data."}, {"section_title": "1) INGESTION TOOLS", "text": "Since data are collected from a variety of sources and formats, ingestion tools need to take into account the volume and velocity of structured and unstructured data. Flume 3 is a reliable and distributed service for efficiently collecting large amounts of log data. Sqoop 4 is a tool that imports structured data from traditional RDBMS database and provides methods for transferring data to HDFS or Hive. Apache NiFi 5 is a reliable and scalable tool to load and collect data from different sources then dump it into other sources. NiFi is highly configurable and includes an easy to use user interface.\nFor instance, Tilve et al. [144] proposed a tool to integrate information from research processes from different fields. Notably, the information generated in the areas of proteomics, genomics, cell cultures, and histomorphology. The data is collected by acquiring a wide range of gross information from different local databases using the Sqoop tool and stored inside the data storage system HDFS."}, {"section_title": "2) STORAGE TOOLS", "text": "The complicated nature of health data means that today's healthcare sector cannot rely solely on traditional data storage methods. Data storage should continue to be innovated to accommodate data nature and growth. Methods must be scalable while maintaining high performance in data access.\n\u2022 File System: HDFS is a highly fault-tolerant distributed file system that stores data on the clusters, it can handle large amounts of data, regardless of format [145] . Amazon S3 6 (Simple Storage Service) is an online service that allows storing large amounts of data. S3 is free to join and is a pay-as-you-go service.\n\u2022 NoSQL: The traditional relational database has faced many challenges to store and process Big data effectively. Therefore, to solve these challenges, a variety of ''NoSQL'' databases appeared with many aspects such as reading and writing data quickly, supporting mass storage, ease of expansion, and low cost. Furthermore, NoSQL Databases are classified into three basic categories: Key-value (HBase, 7 Redis 8 ), Column-oriented (Cassandra, 9 Hypertable 10 ) and Document database (MongoDB, 11 CouchDB 12 ) [146] . As one of NoSQL data stores, Graph databases (Neo4J, 13 AllegroGraph, 14 Openlink Virtuoso 15 ) provide an enterprise-grade RDF triple store [147] .\n\u2022 Data Warehouse: Apache Hive 16 is a data warehousing infrastructure that provides a SQL-like interface: Hive QL [148] . It enables easy data ETL from HDFS or other data storage like HBase. Teradata 17 is one of the well-known RDMS, best suited for database warehousing application dealing with a considerable amount of data. Teradata has a patented PDE (Parallel database extension) software that enables parallel processing and allows faster processing with a large margin over traditional databases. For example, HMBDPS [66] is a distributed Hadoop-based Medical Big Data Processing System which aims at integrating and processing Big medical data(structured, semistructured and unstructured data that are produced by HISs) to study some features of user behaviors and provide personalized recommendations based on public behavior for each user. Figure 14 shows the data access layer of HMBDPS, where the physical unit of storage is built based on the Hadoop cluster and the logical unit of storage is implemented using Hive. This logical unit could be accessed by Hive QL (HQL) and user-defined functions (UDF) to store and manage data efficiently. "}, {"section_title": "3) PROCESSING TOOLS", "text": "Scalable data processing is essential for Big data integration that involves a complex sequence of processing steps with different medical data formats. Various types of processing tools are employed in Big Data integration:\n\u2022 Batch Processing: To process Big Data, MapReduce is a well-accepted method to perform parallel computing and distributed storage [145] . MapReduce is a programming model and an associated implementation for processing and generating large datasets, and is suitable for semi-structured or unstructured data. Around MapReduce, many projects, such as Pig, 18 use a highlevel language that spends less time writing mapper and reducer programs. Spark 19 project introduced a cluster computing engine for Big data applications that offers scalability, flexibility, and speed to deal with Big Data challenges. Spark has the power to process and hold data in memory across the cluster. Spark SQL 20 is a module for structured and semi-structured data processing, it is used to query data, both inside a Spark program and from an external repository [149] .\n\u2022 Stream Processing: Real-time or near-real-time data processing requires a different processing paradigm than the batch mode. Stream processing operates each entity data item as soon as it enters the system. In this respect, several distributed computing systems can manage and process Big Data in near real-time. Storm 21 is a low latency distributed stream processing framework that could handle very high stream data rates and deliver results with less latency than other solutions [150] . Spark Streaming 22 is a distributed batch processing framework (over a sliding window) with stream processing capabilities that speeds up batch processing workloads by offering full in-memory computation and processing optimization [151] . Kafka 23 is a real-time message publish-subscribe system that combines the benefits of traditional log aggregators and messaging systems. It is designed as a kernel for data stream architecture. A Kafka Streams library that provides a stream processing capability has been added to the Kafka client library. Kafka is built to be high-throughput, horizontally scalable, fault-tolerant, and allows geographic distribution of data streams and processing [152] . Flink 24 is a fully-fledged and efficient batch processor that lies on top of a streaming runtime. Flink follows a paradigm that encompasses data flow processing as a unifying model for real-time analysis, continuous streams, and batch processing, both in the programming model and in the execution engine [153] .\n\u2022 Machine Learning: Scalability, speed, coverage, usability and extensibility are the main factors to evaluate when choosing machine learning tools, with the note that the prioritization of these factors largely depends on the applications they are being used for [124] . For example, Mahout 25 enables the distributed implementation of machine learning algorithms for Big data, providing scalable feature selection, data sampling, and classification. MLlib 26 from Spark provides a scalable and distributed implementation of popular machine learning methods such as k-means clustering, regression models, SVM, Na\u00efve Bayes. Google's TensorFlow 27 is another tool successfully used for deepening approaches, including long short-term memory (LSTM) algorithms, convolutional neural networks (CNN), etc. TensorFlow allows distributed implementation of Deep learning model on many CPUs or GPUs for large scale analysis. As a case in point, Panahiazar et al. [154] discussed how to store multiple datasets from different resources including EHRs, Medical, and Genomics Images into the Hortonwork repository [155] and then used Pig to clean and prepare data. The authors performed a simple operation like AVERAGE to compare the performance of Pig with other tools like SQL. SQL took 18 minutes to run, but Pig ran in less than two minutes on two nodes. In this respect, Ding et al. [156] proposed a Shared Nearest-Neighbor Quantum Game-based Attribute Reduction (SNNQGAR) algorithm for performing consistent segmentations of cerebral cortical surfaces of the complex neonatal brain regions. SNNQGAR is parallelized using a new hierarchical coevolutionary Spark model combined with an improved MapReduce. This architecture provides improved attribute reduction solutions for big data processing."}, {"section_title": "E. PRIVACY AND SECURITY FOR BIG HEALTHCARE DATA", "text": "The integration of Big data raises many privacy concerns, particularly in the health care sector, due to the promulgation of the Health Insurance Portability and Accountability Act (HIPAA). Because the data integration process aggregates data into a centralized repository, it is extremely vulnerable to attack. Therefore, security and privacy policies should be considered as part of the design of the health data integration platform. Besides, legislation and regulation should often be regarded as re-evaluate emerging technologies and capabilities [157] .\nTraditional security and privacy mechanisms are insufficient to protect Big data. Nevertheless, new technologies also host unknown back doors. Therefore, integrity, confidentiality, and availability of data must be carefully considered."}, {"section_title": "1) SECURITY", "text": "Security is defined as protection against unauthorized access. Therefore, for providing secure access to clinical data, healthcare information systems must provide the following policies: authentication, access control, confidentiality, integrity, attribution/non-repudiation [158] . However, the diversity of data sources, data formats, streaming, and infrastructure can lead to unique security vulnerabilities. In this regard, Alshboul et al. [159] proposed a Big data security lifecycle model, designed to take into account the phases of the Big data lifecycle and correlate threats and attacks that face Big data environment within four phases:\n\u2022 Data collection phase: It is essential to collect data from reliable sources and to use specific security measures, such as the encryption of individual data fields (patient identifier).\n\u2022 Data storage phase: the collected data may contain sensitive information. Thus, some security measures can be used, such as the data anonymization approach, the permutation, and partitioning of data to ensure the safety of the collected data.\n\u2022 Data analytics phase: In this phase, machine learning methods such as clustering, classification, and association rule are used for link extraction and feature selection, which can extract sensitive data. Therefore, this phase needs to be protected while making sure only authorized staff can be engaged in this phase.\n\u2022 Knowledge creation phase: The created knowledge is treated as sensitive information, especially in the VOLUME 7, 2019 health sector. Therefore, organizations must ensure that this information (e.g., patient information) is not to be publicly released."}, {"section_title": "2) PRIVACY", "text": "Health care data sharing allows early detection of epidemics, but without evident privacy protection, it is difficult to extend these surveillance measures nationally or internationally [160] . Privacy is often defined as the ability to protect sensitive information about personally identifiable healthcare information. Various traditional methods guarantee some degree of privacy in Big Data, but their disadvantages have led to the emergence of newer methods [161] .\n\u2022 De-identification is a traditional method for privacypreserving, in which, data must first be sanitized with generalization and suppression before the publishing for data processing to protect patient privacy. K-anonymity, L-diversity, and T-closeness are three traditional methods of De-identification. Many scalable anonymization solutions within the MapReduce framework have been proposed to improve these traditional techniques of protecting Big data privacy.\n\u2022 Hybrid execution model is used for guaranteeing privacy in cloud computing. It utilizes public clouds only for an organization's non-sensitive data, whereas for an organization's sensitive, private data and computation, the model executes within their private cloud.\n\u2022 Privacy-preserving aggregation is built on homomorphic encryption as a widespread data collecting technique for event statistics. These encrypted texts can be aggregated, and the aggregated result can be retrieved with the corresponding private key. Thus, privacypreserving aggregation can protect the privacy of the patient during the Big data collection and storage phases. To address the scalability problem of big data privacy, Gheid and Challal [162] presented a general architecture of Big data analytics for multi-source Big data. The architecture introduced an efficient and privacy-preserving cosine similarity computing protocol in response to the efficiency and privacy requirements of data mining in the Big data era. Moreover, Zhang et al. [163] proposed a scalable two-phase top-down specialization approach for the anonymization of large data sets using the Map Reduce framework in the cloud."}, {"section_title": "F. INTEGRATION APPLICATIONS AND PLATFORMS FOR BIG HEALTHCARE DATA", "text": "The integration of Big data presents new opportunities to create novel applications in the field of healthcare, which provide many benefits to clinicians who can seamlessly search across healthcare systems to get the complete picture of a patient. In this section, we discuss these applications."}, {"section_title": "1) DIABETICS DATA INTEGRATION", "text": "MOSAIC system [164] has been designed to be potentially used in any context dealing with Type 2 Diabetes Mellitus (T2DM) patients. In MOSAIC system, the i2b2 [164] Data Warehouse (DW) allows integrating clinical information coming from hospital EHRs, administrative data from the local health care agencies, and environmental data collected from satellites. A common data model was defined and implemented using the i2b2 technology to query and integrate these heterogeneous huge data, [165] :\n\u2022 A query engine, implemented as back-end service, relies on a MongoDB and provides a logical layer between the user and the data.\n\u2022 A Temporal Abstraction module takes raw quantitative data stored in the i2b2 DW, analyzes the evolution of diabetes by taking advantage of different datasets, and then stores the integrated result in the DW."}, {"section_title": "2) DATA INTEGRATION IN DISEASES PREDICTION SOLUTION", "text": "IManageCancer project [61] aims to provide a cancerspecific self-management platform, with particular emphasis on avoidance, early detection, and management of adverse events of cancer. The high-level architecture of iManageCancer (shown in figure 8 ) is based on the Data Lake concept to store and manage heterogeneous, structured, and unstructured data sources. This Data Lake includes various databases such as PostgreSQL for storing patient information, Cassandra DBs for staging Big data available (e.g., activity monitoring data, sensor data, etc.). The data is queried, transformed into triples, and loaded into a semantic data warehouse where it is available for further analysis. This architecture can specify which of the available data have to be semantically linked and integrated by selecting the suitable mappings to a modular ontology. Furthermore, The Semantic Warehouse provides a 'semantically enriched' and 'search-optimized' index to fill the limited flexibility of query mechanisms for unstructured content of data lake. Based on this approach, the data warehouse has the flexibility to be created from scratch at any time.\nFang et al. [59] demonstrated how medical Big data integration and analysis could be used to construct early prediction and intervention models. A data center based on the Hadoop platform has been built to integrate healthcare data acquired from existing patient-related information systems, such as HIS, LIS, PACS, EMR, ECG, into a data warehouse. Data standardization and consistency are achieved using extract-transform-load (ETL) technology to integrate the vast amount of unstructured data. The main difference is that methods, such as MapReduce [166] , can be applied in each processing link to carry out parallel processing in those Big data."}, {"section_title": "3) DATA INTEGRATION IN PUBLIC HEALTHCARE", "text": "Obesity is a public health problem that has raised concern worldwide, and this problem requires the systematic collection, analysis, and interpretation of all factors affecting weight gain to drive health policy and promote a lifestyle, environmental and socioeconomic changes. Figure 15 shows a 'semantic ETL' service proposed in [98] which connects multiple information retrieved from different data sources: a) IHE-based documents with patient information from the IHE repository, b) CCD-based documents with patient information from patient PHR, c) stream data from sensing devices and d) messages from various web sources. The retrieved data is stored to NoSQL databases (a combination of MongoDB and HBase) in a schema-less format to provide flexibility. Finally, the data transformation module transforms data into RDF documents, and through ontology reasoning, high-level context data is derived and transformed into documents compliant with the integration schema. RDF documents are then exported from the ontologies and stored in the data repository module."}, {"section_title": "4) DATA INTEGRATION IN WEARABLE HEALTHCARE", "text": "Mezghani et al. [97] proposed a collaborative semantic web platform that copes with heterogeneous Big data analysis which comes from different wearable devices, based on the Knowledge as a Service (KaaS) approach. This architecture extended NIST Big data model with a Semantic Knowledge Layer that offers a common understanding of data. The platform produces more accurate and valuable information by fusing Big heterogeneous medical data. The proposed Wearable Healthcare Ontology (WH_Ontology) is designed to deal with the heterogeneity of wearable data to ensure semantic interoperability and to allow creating more accurate knowledge about the patient, such as detecting and predicting anomalies."}, {"section_title": "5) DATA INTEGRATION IN INTEROPERABLE EHR SYSTEMS", "text": "Most medical information systems store clinical information about patients in proprietary formats. Therefore, Bahga and Madisetti [64] presents a Cloud-based approach to integrating electronic health record systems named Cloud Health Information System Technology Architecture (CHISTAR). CHISTAR reference model extends and adapts the OpenEHR and HL7 v3.0 data types. Data integration is performed to throw a data integration engine and achieved in two steps. In the first step, a source connector connects to an external system where a meta-data lookup is performed to discover the semantics of the data elements in the source file. In the next step, semantic matching is done with the meta-data repository of the destination to find and retrieve a list of candidate mappings in an intermediate file. The data loaded by the integration engine is stored as a flat file in HDFS distributed storage; therefore, a MapReduce based bulk loader loads the data from flat files into HBase. This work has been extended by a cloud-based information integration and informatics (III) framework to facilitate the collection and analysis of heterogeneous and distributed healthcare systems within a scalable cloud infrastructure [167] ."}, {"section_title": "6) DATA INTEGRATION IN ADVANCED PRECISION MEDICINE", "text": "Many questions for clinical research (such as how cancer arises, how much complex diseases are dependent on personal genomic traits or environmental factors) could be answered by modern genomics. Moreover, a vast, intricate and incompatible raw data (Encyclopedia of DNA elements (ENCODE), Cancer Genome Atlas (TCGA), 1000 Genomes Project, etc.) have been produced by computational efforts in primary and secondary genomic data management coupled with the progress of RNA sequencing technology. Ceri et al. [76] proposed a data model that ensures the interoperability between different produced formats and allows merging datasets with different schemas. They also defined a new federated query language GMQL that has the ability of computing distance-related queries along the genome, seen as a sequence of positions. The defined query is executed based on simple interaction protocol, such as 1)requesting information about remote datasets, 2)transmitting a query in high-level format and obtain data about its compilation, 3) launching query execution and then controlling the transmission of results, so as to be in control of staging resources and communication load.\nA processing method is used in [65] for complex biological data processing operations to understand gene-disease associations. Both structured and unstructured data are loaded onto HDFS, and they are queried and integrated using both a commodity hardware-software cluster and a commercial Big Data System to find the records that match the given query. The data that were acquired consisted of 20 million literature abstracts obtained from PubMed in XML format, mRNA expression data and miRNA expression data from a single Glioblastoma patient downloaded from TCGA, along with a gene and disease lexicon, using EntrezGene and NCI Thesaurus, respectively. The result suggests that available technologies within the Big Data domain can reduce the time and effort needed to utilize and apply distributed queries over large datasets in practical clinical applications in the life sciences domain.\nG-DOC Plus [168] is used to integrate multiple datasets such as health data selected from private and public resources, Cancer Genome Atlas (TCGA) and recently added datasets from REpository for Molecular BRAin Neoplasia DaTa (REMBRANDT), caArray studies of lung and colon cancer, ImmPort and the 1000 genomes data sets. G-DOC Plus aims to support physician, scientists and researchers to understand the mechanisms of cancer and non-cancer diseases to drive new hypothesis for precision medicine. G-DOC Plus uses MongoDB to store variant data from sequencing studies. Images are stored in a DICOM Clinical Data Manager ''Dcm4chee'' system, and metadata are stored in a MySQL database. All engines are hosted on an EC2 server instance on the Amazon cloud."}, {"section_title": "7) DATA INTEGRATION IN MENTAL HEALTH CARE", "text": "Brain and mental health research from Big brain data have grown as an emerging area for both data analyst and neuroscience community. Electroencephalography (EEG) is by far the most commonly used technique to study brain function. It has proven its usefulness with advanced sensing technology and signal processing algorithms to support people with healthcare needs, such as identifying ketamine responses in treatment-resistant depression using a wearable forehead EEG [169] , exploring resting-state EEG complexity before migraine attacks [170] , indexing brain cortical dynamics and detecting driving fatigue and drowsiness [171] . Alternatively, functional magnetic resonance imaging (fMRI) is used extensively to identify regions linked to critical functions such as speaking, moving, sensing, or planning. Clinicians also use fMRI to further understand neurobehavioral disorders, such as Alzheimer's disease, epilepsy, brain tumors, stroke, traumatic brain injury, and multiple sclerosis [172] .\nSince EEG and fMRI are the two most commonly used noninvasive functional neuroimaging techniques, and because they exhibit highly complementary characteristics, their multimodal integration has been actively sought [173] . Hosseini et al. [174] introduce a new method for epileptogenic network definition and prediction of the seizure (ictal) onset, by integrating multimodal fMRI and EEG Big data. A deep-learning approach was developed to extract high order features for identification of interictal epileptic discharge (IED) and nonIED time intervals in electrographic data, leveraging the emerging mobile-edge computing platform. Figure 16 illustrates the pipeline of this integration. "}, {"section_title": "IV. DISCUSSION", "text": "In the previous section, We have extensively studied existing solutions found in the literature related to Big Healthcare Data integration. We investigated the ins and outs of these solutions from different perspectives including the mode of operations (real-time or batch style or both), what specific problems it solves, which of the Big Data characteristics it deals with (volume or velocity), does it tackle the quality problems, etc. We summarize our findings in Table 2 . In what follows, we will assess the weaknesses and challenges of Big Healthcare Data Integration solutions:\nA. WEAKNESSES We found some powerful solutions which can integrate data (section III-F). However, we found a few critical problems which are unsolved in existing technologies:\n\u2022 Lack of powerful solution: There is no ready to use solution (similar to commercial off the shelf) for medical data integration. The existing solutions were developed, aiming at building a solution for a specific area of the healthcare domain. For instance, the integration of cancer data. Such solutions can integrate some particular types of data such as image and text but cannot integrate surgical videos. Also, some solutions integrate a kind of data that relies on a specific data model such as RDF.\n\u2022"}, {"section_title": "Lack of solution for Integrating Medical Data Streams:", "text": "In our study, we have not found an efficient solution which can be used to integrate medical data streaming from sources such as social media and sensors. Such a solution is enormously useful for performing a critical analysis on the fly.\n\u2022 Lack of solution for integration with data quality consideration: Medical data integration faces a quality challenge. Integration of heterogeneous data may produce messy data or data that are not meaningful. This is why integration solutions must solve quality issues. Unfortunately, we did not find any solution which can address both issues effectively.\n\u2022"}, {"section_title": "Lack of solution for integration with real-time responses:", "text": "Many of the existing solutions are built on the HadoopMapReduce framework, which mostly solved the data volume challenge. However, due to the extravagant sorting algorithm that Hadoop relies heavily on for performing reduce function, the performance can be a bottleneck."}, {"section_title": "B. CHALLENGES", "text": "Our survey found several challenges regarding processing Big Data [132] , [175] , and specifically for healthcare Big data [1] , [13] , [176] . Integrating Big healthcare data has its challenges. Recent studies show that Big healthcare data could not be efficiently integrated using traditional techniques, technologies, and tools. Therefore, many issues have not been addressed and need to be answered. The most important challenges are briefly presented below:\n\u2022 Heterogeneous data: Data integration and fusion of the noisy, heterogeneous and longitudinal data generated by different technologies such as medical imaging, physiological signal, genomic data [177] , and social media [178] constitute the Big challenge for healthcare informatics. These data may be diverse in terms of Data types, file formats, data encoding, the data model (syntactic heterogeneity) as well as they may have differences in meanings and interpretations (semantic heterogeneity). \u2022 Unstructured data: Clinical context produces unstructured data (or at least in a semi-structured form) such as handwritten doctor notes, images, audio, and video streams. These unstructured resources contain a richness of information relevant to understanding human health [8] .\n\u2022 Problems with data standards: Medical data usually lack consistent data standards and are often fragmented or generated in legacy IT systems with an incompatible structure, which exacerbates the inconsistency of medical terminology. Integration problems in healthcare include gaps in data standards, overlapping standards, and multiple data standards development organizations [52] .\n\u2022 Health-monitoring data: Real-time integration of health-monitoring data such as vital signs monitoring devices, environmental exposure, and pharmacological profiles, into the existing medical data poses several technical challenges [179] .\n\u2022 Patient privacy and data security: There is a growing interest in the security of electronic medical information that is distributed in confidential silos owned by a multitude of stakeholders [8] . The patient has the right to determine when, how, and to what extent his health VOLUME 7, 2019 information is shared with others. That is, during data integration processes, essential factors related to patient privacy and consent and other legal issues related to these data need to be considered [180] . These statutory and regulatory aspects could create a potential barrier to the proper implementation of the data integration process.\n\u2022 Non-expert users: Data integration should be an automatic process. Due to the scale and heterogeneity of medical data, automatic integration is not completely accurate. End-users of integrated data are generally physicians, nurses, and health professionals with limited informatics training [181] . It would, therefore, be difficult to help non-expert users access heterogeneous data sources via data integration systems."}, {"section_title": "V. RESEARCH DIRECTIONS", "text": "We discovered some promising research directions related to the integration of Big Healthcare Data. We briefly explain these directions in the following:\nThe advent of Big Data has given rise to a new notion called Data Fusion, which is an extended concept of data integration. In data integration, data are gleaned from multiple heterogeneous sources, whereas fusion consists of data integration followed by data reduction or replacement operations. Fusion adds different levels of uncertainty to support a more narrow set of application workloads, which is critical specifically to perform analysis efficiently. Therefore, Big data fusion has become an important issue to healthcare industry practitioners and researchers and is gaining popularity as a concept for building efficient solutions. According to our study, Big Healthcare data fusion is an open and critical issue which can be dealt with by producing novel fusion techniques."}, {"section_title": "B. INFORMATION EXTRACTION", "text": "Data annotation technique aids data integration process to understand and fuse medical data such as images (MRI, radiology image, CT scan, molecular image), text (medical report, academic article), EHRs. These different types of data formats, make the annotation process of medical data very difficult compared to other domains. Moreover, the evolution of medical data has created a large heterogeneity of data sources and increased complexity to extract the needed information [182] . Therefore, hot opportunities have been arising in developing new information extraction and labeling methods."}, {"section_title": "C. DEVELOPING EFFICIENT INTEGRATION TECHNIQUES", "text": "Machine learning is a possibly feasible way to improve traditional data reduction techniques to process or even preprocess Big data. These emerging techniques may help to understand the trends of data, classify Big data, and detect similarities [175] . Recent developments in deep learning and artificial neural networks emerged as the preferred machine learning approach in machine perception such as computer vision, speech recognition, and natural language processing. Therefore, combining these models will open new chances to address many of the challenges of integrating structured and unstructured data, making better leverage of the informationrich yet unstructured data in EHRs [183] ."}, {"section_title": "D. REAL-TIME INTEGRATION OF DATA STREAMS", "text": "The increasing trend of using smart devices in the healthcare sector for carrying out several tasks such as change detection in real-time monitoring of EEG signals [184] and using smart equipment in surgical procedures [185] , requires integration of data streams on the fly. The wearable device technology is becoming popular and is gaining importance for long-term health monitoring [186] . The integration of patient-generated fitness data with Big medical data such as EMR can be compelling and robust. This integration can help clinicians in making more informed decisions about patient health [187] . Therefore, real-time data capturing from wearable medical devices regardless of the data format and the integration of all these new formats with medical data is a newly emerging research domain."}, {"section_title": "E. ADVANCED INTEGRATION TECHNOLOGIES", "text": "In recent years, in addition to acquiring healthcare data, it became possible to obtain data from various data sources (social networks, monitoring, IoT devices, etc.). This perspective raises new questions about the quality of the data. Therefore, the ADR-PRISM project [188] identified 21 criteria for evaluating social media to select the most informative data elements to support medical domain research. Thereby, the need for new integration technologies to benefit from the methodology of Big data integration to obtain a fully integrated picture of disease causality and help in effective early detection [189] . The following are some of the research works that can be done.\n\u2022 Precision Medicine: Combining genetic data with diseases, therapies, and outcomes can help to improve the selection of the best treatment. Also, integrating historical patient data about lifestyle and environmental exposure has the potential to determine the causes triggering the onset of a disease state.\n\u2022 Infectious diseases early detection: Combining data from web-based searches, social information, travel, trade, climate changes, etc., with syndromic surveillance and diagnostic data including the next generation sequencing, can improve the detection of early signs of disease outbreaks (e.g. influenza, bacterial-caused food poisoning) and coordinate quarantine and vaccination responses.\n\u2022 Chronic diseases detection: Combining data from social and physical behaviors, nutrition, genetic factors, environmental factors and the development of mental/physical diseases, can help to better understand the triggers of chronic diseases for effective early detection. "}, {"section_title": "F. SCALABLE DATA INTEGRATION PLATFORM", "text": "Since medical data hold the characteristics of Big data [176] , there is a need for a scalable solution that is based on the Hadoop framework. The platform should have the ability to store a huge amount of heterogeneous data and deliver a wide range of automated data integration processors like data filtering, cleaning, summarizing and link discovery, as well as it should provide a fusion solution that supports healthcare standards such as HL7, OpenEHR and others. It must also be able to merge data from internal and external sources. The main idea behind this platform is to summarize and integrate data graphically and interactively, enabling domain experts to find key information for supporting decision making interactively."}, {"section_title": "VI. CONCLUSION", "text": "The rapid growth of healthcare data has given rise to the notion of Big Healthcare Data. A wide variety of data are available in the healthcare sector, including text (e.g., prescriptions), images (e.g., MRI scanning reports), video (recorded operations in a surgical room), etc. Furthermore, in recent years, sensors and social media data are heavily used within the healthcare sector for various purposes. These data flow with high-speed. Uncertainty is a common issue of data when it flows from outside of the wellknown and reliable internal repositories. The complex nature of data, which include volume, variety, speed, and uncertainty raised a massive challenge for traditional technologies, e.g., spreadsheet-based tool, relational database, and single machine programming. The advent of Big Data promoted the growth of Big Data technologies for various operations, including collection, integration, processing, analysis, and visualization, over the years. In this paper, we studied existing technologies extensively. Our study focused on finding the strength and weaknesses of these technologies. In our survey, We found limitations in existing model technologies that are based on the Hadoop platform, NoSQL, parallel programming. We reported these limitations in this paper. We analyzed our findings concerning different parameters such as the ability of current integration technologies to deal with variety, speed, uncertainty. Furthermore, We discussed a few promising research directions. We planned to conduct an experimental study with some of the potential solutions for integrating Big Healthcare Data."}, {"section_title": "APPENDIX", "text": "All the acronyms used in the paper are enlisted in Table 3 YEHIA TAHER received the Ph.D. degree in computer science from the Universit\u00e9 Claude Bernard Lyon 1. He is currently an Associate Professor of computer science with the Universit\u00e9 de Versailles Saint-Quentin-en-Yvelines, France. His research interests include service-oriented computing, complex event processing, cloud computing, and business process management. In recent years, his research is largely dedicated to real-time big data analytics. He worked in different research institutes within Europe. He works in collaboration with academia and industries. He collaborates with academia in France and other European countries, including The Netherlands and Ireland. In addition, he has established collaboration with universities in middle-east. He has published around 50 conference papers and journals in top tier conferences and journals."}]