[{"section_title": "Abstract", "text": "Abstract Super-resolution, the process of obtaining one or more high-resolution images from one or more lowresolution observations, has been a very attractive research topic over the last two decades. It has found practical applications in many real-world problems in different fields, from satellite and aerial imaging to medical image processing, to facial image analysis, text image analysis, sign and number plates reading, and biometrics recognition, to name a few. This has resulted in many research papers, each developing a new super-resolution algorithm for a specific purpose. The current comprehensive survey provides an overview of most of these published works by grouping them in a broad taxonomy. For each of the groups in the taxonomy, the basic concepts of the algorithms are first explained and then the paths through which each of these groups have evolved are given in detail, by mentioning the contributions of different authors to the basic concepts of each group. Furthermore, common issues in super-resolution algorithms, such as imaging models and registration algorithms, optimization of the cost functions employed, dealing with color information, improvement factors, assessment of super-resolution algorithms, and the most commonly employed databases are discussed."}, {"section_title": "", "text": "resolution (LR) observations . It has been used for many different applications (Table 1) , such as, satellite and aerial imaging, medical image processing, ultrasound imaging [582] , line-fitting [18] , automated mosaicking, infrared imaging, facial image improvement, text images improvement, compressed images and video enhancement, sign and number plate reading, iris recognition [153, 586] , fingerprint image enhancement, digital holography [271] , and highdynamic range imaging [553] .\nSR is an algorithm that aims to provide details finer than the sampling grid of a given imaging device by increasing the number of pixels per unit area in an image [420] . Before getting into the details of SR algorithms, we need to know about the possible hardware-based approaches to the problem of increasing the number of pixels per unit area. Such approaches include (1) decreasing the pixel size and (2) increasing the sensor size [132, 417] . The former solution is a useful solution, but decreasing the pixel size beyond a specific threshold (which has already been reached by the current technologies) decreases the amount of light which reaches the associated cell of the pixel on the sensor. This results in an increase in the shot noise. Furthermore, pixels of smaller sizes (relative to the aperture's size) are more sensitive to diffraction effects compared to pixels of larger sizes. The latter solution increases the capacitance of the system, which slows down the charge transfer rate. Furthermore, the mentioned hardware-based approaches are usually expensive for large-scale imaging devices. Therefore, algorithmicbased approaches (i.e., SR algorithms) are usually preferred to the hardware-based solutions.\nSR should not be confused with similar techniques, such as interpolation, restoration, or image rendering. In interpolation (applied usually to a single image), the high-frequency details are not restored, unlike SR [160] . In image restoration, obtained by deblurring, sharpening, and similar techniques, 71, 73, 74, 82, 133, 156, 180, 181, 195, 199, 201, 209, 210, 217, 241, 248, 277, 281, 285, 288, 296] (Chinese text), [307, 313, 314, 317, 319, 366, 369, 376, 388 [237, 245, 255, 275] the sizes of the input and the output images are the same, but the quality of the output gets improved. In SR, besides improving the quality of the output, its size (the number of pixels per unit area) is also increased [207] . In image rendering, addressed by computer graphics, a model of an HR scene together with imaging parameters is given. These are then used to predict the HR observation of the camera, while in SR it is the other way around. Over the past two decades, many research papers, books [101, 294, 534] and PhD theses [4, 78, 84, 148, 182, 261, 263, 266, 312, 331, 390, 537] have been written on SR algorithms. Several survey papers [47, 48, 131, 132, 155, 198, 265, 292, 351, 417] have also been published on the topic. Some of these surveys provide good overviews of SR algorithms, but only for a limited number of methods. For example, [47, 48] provide the details of most frequency domain methods and some of the probability-based methods [131, 132, 417 ] take a closer look at the reconstruction-based SR methods and some of the learning-based methods [155, 198, 265] have provided a comparative analysis of reconstruction-based SR algorithms but only for a very few methods; and finally [292] provides details of some of the single-image based SR algorithms. None of these surveys provide a comprehensive overview of all the different solutions of the SR problem. Furthermore, none of them include the latest advances in the field, especially for the learning-based methods and regularized-based methods. In addition to providing a comprehensive overview of most of the published SR works (until 2012) , this survey covers most of the weaknesses of the previously published surveys. The present paper describes the basics of almost all the different types of super-resolution algorithms that have been published up to 2012. Then, for each of these basic methods, the evolving paths of the methods have been discussed by providing the modifications that have been applied to the basics by different researchers. Comparative discussions are also provided when available in the surveyed papers. The first parts (the basics of the methods) can be studied by beginners in the field so as to have a better understanding of the available methods, while the last parts (the evolving paths of the methods and the comparative results) can be used by experts in the field to find out about the current status of their desired methods.\nThe rest of this paper is organized as follows: the next section provides a taxonomy covering all the different types of SR algorithms. Section 3 reviews the imaging models that have been used in most SR algorithms. Section 4 explains the frequency domain SR methods. Section 5 describes the spatial domain SR algorithms. Some other issues related to Fig. 1 The proposed taxonomy for the surveyed SR algorithms and their dedicated sections in this paper SR algorithms, like handling color, the assessment of SR algorithms, improvement factors, common databases, and 3D SR, are discussed in Sect. 6. Finally, the paper comes to a conclusion in Sect. 7."}, {"section_title": "Taxonomy of SR algorithms", "text": "SR algorithms can be classified based on different factors. These factors include the domain employed, the number of the LR images involved, and the actual reconstruction method. Previous survey papers on SR algorithms have mostly considered these factors as well. However, the taxonomies they provide are not as comprehensive as the one provided here (Fig. 1) . In this taxonomy, SR algorithms are first classified based on their domain, i.e., the spatial domain or the frequency domain. The grouping of the surveyed papers based on the domain employed is shown in Table 2 . Though the very first SR algorithms actually emerged from signal processing techniques in the frequency domain, it can be seen from Table 2 that the majority of these algorithms have been developed in the spatial domain. In terms of the number of the LR images involved, SR algorithms can be classified into two classes: single image or multiple image. Table 3 shows the grouping of the surveyed papers based on this factor. The classification of the algorithms based on the number of the LR images involved has only been shown for the spatial domain algorithms in the taxonomy of Fig. 1 . This is because the majority of the frequency domain SR algorithms are based on multiple LR images, though there are some which can work with only one LR image. The time line of proposing different types of SR algorithms is shown in Fig. 2 . The single-image based SR algorithms (not all but) mostly employ some learning algorithms and try to hallucinate the missing information of the super-resolved images using the relationship between LR and HR images from a training database. This will be explained in more detail in Sect. 5.2. The multiple-image based SR algorithms usually assume that there is a targeted HR image and the LR observations have some relative geometric and/or photometric displacements from the targeted HR image. These algorithms usually exploit these differences between the LR observations to reconstruct the targeted HR image, and hence are referred to as reconstruction-based SR algorithms (see Sect. 5.1 for more details). Reconstruction-based SR algorithms treat the SR problem as an inverse problem and therefore, like any other inverse problem, need to construct a forward model. The imaging model is such a forward model. Before going into the details of the SR algorithms, the most common imaging models are described in the next section."}, {"section_title": "Imaging models", "text": "The imaging model of reconstruction-based SR algorithms describes the process by which the observed images have been obtained. In the simplest case, this process can be modeled linearly as [25] :\nwhere g is an observed LR image, f is the original HR scene, q is a decimation factor or sub-sampling parameter which is assumed to be equal for both x and y directions, x and y are the coordinates of the HR image, and m and n of the LR images. The LR image is assumed to be of size M 1 \u00d7 M 2 , and the HR image is of size N 1 \u00d7 N 2 where N 1 = q M 1 and N 2 = q M 2 . The imaging model in Eq. (1) states that an LR observed image has been obtained by averaging the HR intensities over a neighborhood of q 2 pixels [25, 109] . This model becomes more realistic when the other parameters involved in the imaging process are taken into account. As shown in Fig. 3 , these parameters, aside from decimation, are the blurring, warping and noise. The inclusion of these factors in the model of Eq. (1) results in [8] : where w is a warping function, h is a blurring function, d is a down-sampling operator, and \u03b7 is an additive noise. The down-sampling operator defines the way by which the HR scene is sub-sampled. For example, in Eq. (1) every window of size q 2 pixels in the HR scene is replaced by only one pixel at the LR observed image by averaging the q 2 pixel values of the HR window. The warping function stands for any transformations between the LR observed image and the HR scene. For example, in Eq. (1) the warping function is uniform. But, if the LR image of g(m, n) is displaced from the HR scene of f (x, y) by a translational vector as (a, b) and a rotational angle of \u03b8 , the warping function (in homogeneous coordinates) will be as:\nThe above function will of course change depending on the type of motion between the HR scene and the LR observations. The blurring function [which, e.g., in Eq. (1) is uniform] models any blurring effect that is imposed on the LR observed image, for example, by the optical system (lens and/or the sensor) [99, 589] or by atmospheric effects [125, 157, 260, 398, 402, 419, 420, 542] . The registration and blur estimation are discussed more in Sects. 3.1 and 3.2, respectively.\nIf the number of LR images is more than one, the imaging model of Eq. (2) becomes:\nwhere k changes from 1 to the number of the available LR images, K . In matrix form, this can be written as:\nin which A stands for the above-mentioned degradation factors. This imaging model has been used in many SR works (Table 4 ). Figure 4 shows graphically how three different LR images are generated from a HR scene using different parameters of the imaging model of Eq. (4.) Instead of the more typical sequence of applying warping and then blurring [as in Eq. (4)], some researchers have considered reversing the order by first applying the blurring and then the warping [171, 212, 234] . It is discussed in [171, 212] that the former coincides more with the general imaging physics (where the camera blur is dominant), but it may result in systematic errors if motion is being estimated from the LR images [171, 214] . However, some other researchers have mentioned that these two operations can commute and be assumed as block-circulate matrices, if the point spread function is space invariant, normalized, has nonnegative elements, and the motion between the LR images is translational [156] [157] [158] 186, 589, 611] . The imaging model of Eq. (4) has been modified by many other researchers (Table 4) , for example: -in [29, 45, 273] , in addition to the blurring effect of the optical system, motion blur has been taken into account. In this case, the blur's point spread function, h, of Eq.\n(4) is replaced by three point spread functions such as h sensor * h lens * h motion . - [123, 219, 253, [312] [313] [314] 347] assume that a global affine photometric correction resulting from multiplication and addition across all pixels by scalars \u03bb and \u03b3 respectively is also involved in the imaging process:\nThe above affine photometric model can only handle small photometric changes, therefore it has been extended to a non-linear model in [289] , which also discusses the fact that feature extraction for finding similarities would be easier between similarly exposed images than it would with images having large differences in exposure. Therefore, it can be more efficient to carry out a photometric registration to find similarly exposed images and then do the geometric registration.\n- [62] [63] [64] 225, 278, 388, 428] assume that a sequence of LR observations are blurred and down-sampled versions of a respective sequence of HR images, i.e., they do not consider warping effect between LR images and their corresponding HR ones, instead they involve warping between the super-resolved HR images. This provides the possibility of using temporal information between consecutive frames of a video sequence. - [104, 110, 137, 161] change the above imaging model to consider the quantization error which is introduced by the compression process. - [105, 127] change the imaging model of Eq. (4) in such a way that the SR is applied to the feature vectors of some LR face image observations instead of their pixel values. The result in this case will not be a higher resolution face image but a higher-dimensional feature vector, which helps in increasing the recognition rate of a system which uses these feature vectors. The same modification is followed in [165] , wherein Gabor wavelet filters are used as the features. This method is also followed in [361, 586] , but here the extracted features are directly used to reconstruct the super-resolved image. - [129, 163, 244, 246, 449, 597 ] change the imaging model of Eq. (4) to include the effect of different zooming in the LR images. - [155] [156] [157] [158] 160 ] change the imaging model of Eq. (4) to reflect the effect of color filtering for color images which comes into play when the color images are taken by cameras with only one charge-coupled device (CCD).\nHere some color filters are used to make every pixel sensitive to only one color. Then, the other two color elements of the pixels are obtained by demosaicing techniques. - [175, 247, 614] adapt the imaging model of Eq. (4) to hyper-spectral imaging in such a way that the information of different spectra of different LR images is involved in the model. - [184, 229] reflect the effects of a non-linear camera response function, exposure time, white balancing and external illumination changes which cause vignetting effects. In this case, the imaging model of Eq. (5) is changed to:\nwhere \u03ba is the non-linear camera response function, \u03b1 k is a gain factor modeling the exposure time, \u03b2 k is an offset factor modeling the white balancing, \u03b7 k is the sensor noise, and k is the quantization error. - [205] extends the imaging model of Eq. (4) to the case where multiple video cameras capture the same scene. It is discussed here that, just as spatial misalignment can be used to improve the resolution in SR algorithms, temporal misalignment between the videos captured by different cameras can also be exploited to produce a video with higher frame rates per second than any of the individual cameras. - [238] uses an imaging model in which it is assumed that the LR images are obtained from the HR scene by a process which is a function of (1) sub-sampling the HR scene, (2) the HR structural information representing the surface gradients, (3) the HR reflectance field such as albedo, and (4) Gaussian noise of the process. Using this structure preserving imaging model, there is no need for sub-pixel misalignment between the LR images, and consequently, no registration algorithm is needed. - [419, 420, 440] remove the explicit motion parameter of the imaging model of Eq. (4). Instead, the idea of a probabilistic motion is introduced (this will be explained in more detail in Sect. 5.1.3). - [582] changes the imaging model of Eq. (4) for the purpose of ultrasound imaging:\nwhere g is the acquired radio frequency signal and f is the tissue scattering function."}, {"section_title": "Geometric registration", "text": "For multiple-image SR to produce missing HR frequencies, some level of aliasing is required to be present in the LR acquired frames. In other words, multiple-image based SR is possible if at least one of the parameters involved in the imaging model employed changes from one LR image to another. These parameters include motion, blur (optical, atmospheric, and/or motion blur), zoom, multiple aperture [106, 447] , multiple images from different sensors [117, 205] , and different channels of a color image [117] . Therefore, in multiple-image SR prior to the actual reconstruction, a registration step is required to compensate for such changes, though, some of the methods (discussed in Sect. 6.1.2) do the reconstruction and the compensation of the changes simultaneously. The two most common types of compensation for the changes between LR images are geometric registration and blur estimation. The geometric registration is discussed in this section and the blur estimation in Sect. 3.2. Geometric registration compensates for the geometric misalignment (motion) between the LR images, with the ultimate goal of their registration to an HR framework. Such misalignments are usually the result of global and/or local motions [6, 8, 20, 35, 36, 55, 123, 243, 364] . Global motion is a result of motion of the object and/or camera, while local motion is due to the non-rigid nature of the object, e.g., the human face, or due to imaging condition, e.g., the effect of hot air [364] . Global motion can be modeled by: -a translational model (which is common in satellite imaging) [170, 175, 181, 183, 217, 231, 398] , -an affine model [20, 55, 195, 291, 293, 352, 426, 432, 484, 541] , or -a projective model [84] [85] [86] 216, 316, 491] , while local motion is modeled by a non-rigid motion model [364] . In a typical non-rigid motion model, a set of control points on a given image is usually combined using a weighting system to represent the positional information both in the reference image and in the new image to be registered with the reference image.\nThe first image registration algorithm used for SR was proposed in [6] , in which translational and rotational motions between the LR observations and the targeted HR image were assumed. Therefore, according to the imaging model given in Eq. (4), the coordinates of the LR and HR images will be related to each other according to:\nwhere (x t k , y t k ) is the translation of the kth frame, \u03b8 k is its rotation, and q x and q y are the sampling rates along the x and y direction, respectively. To find these parameters, [6, 8, 20, 195, 196, 243, 597] used the Taylor series expansions of the LR images. To do so, two LR images g 1 and g 2 taken from the same scene which are displaced from each other by a horizontal shift a, vertical shift b, and rotation \u03b8 are first described by:\nThen, sin \u03b8 and cos \u03b8 in Eq. (10) are expanded in their Taylor series expansions (up to two terms):\nThen, g 1 is expanded into its own Taylor series expansion (up to two terms):\nFrom this, the error of mapping one of these images on the other one can be obtained as:\nwhere the summation is over the overlapping area of the two images. The minimum of this error can be found by taking its derivatives with respect to a, b and \u03b8 and solving the equations obtained. It was shown in [6] that this method is valid only for small translational and rotational displacements between the images. This algorithm was later on used (or slightly changed) in many other works [13, 14, 24, 51, 91, 172, 196, 264, 284, 303, 304, 306, 307, 319, [481] [482] [483] 538, 575, 576] .\nIt is discussed in [8] that the above-mentioned method of [6] can be used for modeling other types of motion, such as perspective transformations, if the images can be divided into blocks such that each block undergoes some uniform motion [8, 370] . To speed up this registration algorithm, it was suggested to use a Gaussian resolution pyramid [8] . The idea is that even large motions in the original images will be converted to small motions in the higher levels (lower resolution images) of the pyramid. Therefore, these small motions are first found in the smaller images and then are interpolated in the lower level (higher resolution) images of the pyramid until the original image is met. This method, known as optical flow, works quite well when motion is to be computed between objects, which are non-rigid, non-planar, nonLambertian, and are subject to self-occlusion, like human faces, [55, 57, 58, 71, 82, 99, 100, 115, 116, 126, 128, 176, 190, 270, 288, 295, 296, 299, 307, 415, 469, 479, 493, 501, 530, 535] [ 556, 593] . It is discussed in [593] [365, 370, 398, 543, 545] , SSD [370] , sum of absolute transform differences, sum of squared transform differences [282] . A comparison of these search techniques can be found in [334] . Having applied one of these search techniques, the block with the smallest distance is considered to be the corresponding block of the current block. This process is repeated for every block until the motion vectors between every two corresponding blocks are found. This technique works fine but fails to estimate vectors properly over flat image intensity regions [55, 176] . To deal with this problem, it is suggested in [176] that motion vectors should only be calculated from textured regions and not from smooth regions. - [184, 216, 229, 583] , feature points are extracted by Harris corner detection and then are matched using normalized cross correlation. After removing the outliers by RANSAC, the homographies between the LR images are found by again applying RANSAC but this time to the inliers. - [219, 274, 384] , the sampling theory of signals with finite rate of innovation (FRI) is used to detect step edges and corners and then use them for registration in an SR algorithm. It is shown in [274] that this method works better than registration algorithms based on Harris corners. - [296] , normalized cross correlation has been used to obtain the disparity for registration in a stereo setup for 3D SR. - [322, 397, 456] , active appearance model has been used for registration of facial images in a video sequence. - [369] , a feature-based motion estimation is performed using SIFT features (and PROSAC algorithm for matching) to obtain an initial estimate for the motion vectors between an input image and a reference image. These estimated vectors are then used to extract individual regions in the input image which have similar motions. Then, a region-based motion estimation method using local similarity and local motion error between the reference image and the input image is used to refine the initial estimate of the motion vectors. This method is shown to be able to handle multiple motions in the input images [369] . - [371] , Fourier description-based registration has been used. - [372, 423, 593, 616] , SIFT and RANSAC have been used.\n- [401], a mesh-based warping is used.\n- [450], depth information is used for finding the registration parameters. - [614] , principal component analysis (PCA) of hyperspectral images is used for motion estimation and registration.\nEach motion model has its own pros and cons. The proper motion estimation method depends on the characteristics of the image, the motion's velocity, and the type of motion (local or global). The methods mentioned above are mostly global methods, i.e., they treat all the pixels the same. This might be problematic if there are several objects in the scene having different motions (multiple motions) or if different parts of an object have different motions, like different parts of a face image [270, 382] . To deal with the former cases, in [20] and later on in [42, 43, 86, 128, 147, 177, 264, 284] it was suggested to find the motion vectors locally for each object and use the temporal information if there is any. To do so, in [147, 177] Tukey M-estimator error functions of the grayscale differences of the inlier regions are used. These are the regions which are correctly aligned. Since these regions are dominated by aliasing, the standard deviation of the aliasing can be used for estimating the standard deviation of the grayscale differences. The standard deviation of the aliasing can be estimated using the results on the statistics of the natural images [147, 177] .\nTo give further examples of finding local motions, the following can be mentioned: -in [270] , a system is proposed for face images, in which the face images are first divided into sub-regions and then the motions between different regions are calculated independently. -in [382, 606] , a free from deformation (FFD) model is proposed for modeling the local deformation of facial components. -in [35] , the motion vectors of three channels of a color image are found independently and then combined to improve the accuracy of the estimation.\nGlobal motion estimation between the images of a sequence can be carried out in two ways: differential (progressive) and cumulative (anchored). In the differential method, the motion parameters are found between every two successive frames [290] . In the cumulative method, one of the frames of the sequence is chosen as the reference frame and the motions of the other frames are computed relative to this reference frame. If the reference frame is not chosen suitably, e.g., if it is noisy or if it is partially occluded, the motion estimation and therefore the entire SR algorithm will be erroneous. To deal with that, -Wang and Wang [172] use subsequences of the input sequence to compute an indirect motion vector for each frame instead of computing the motion vector between only two images (a new image and the reference image). These motion vectors are then fused to make the estimation more accurate. Furthermore, they have included a reliability measure to compensate for the inaccuracy in the estimation of the motion vectors. -Ye et al. [216] propose using the frame before the current frame as the new reference frame if the overlap between the reference frame and the current frame is less than a threshold (e.g., 60 %).\n-Nasrollahi and Moeslund [481] [482] [483] 538] and then [541] propose using some quality measures to pick the best frame of the sequence as the reference frame."}, {"section_title": "Blur estimation", "text": "This step is responsible for compensating for the blur differences between the LR images, with the ultimate goal of deblurring the super-resolved HR image. In most of the SR works, blur is explicitly involved in the imaging model. The blur effects in this group of algorithms are caused by the imaging device, atmospheric effects, and/or by motion. The blurring effect of the imaging device is usually modeled by a so-called point spread function, which is usually a squared Gaussian kernel with a suitable standard deviation, e.g., 3\u00d73 with standard deviation of 0.4 [248] , 5\u00d75 with standard deviation of 1 [155, 184, 229] , 15 \u00d7 15 with standard deviation of 1.7 [186] , and so on. If the point spread function of the lens is not available from its manufacturer, it is usually estimated by scanning a small dot on a black background [8, 13, 14] . If the imaging device is not available, but only a set of LR images is, it can be estimated by techniques known as Blind Deconvolution [275] in which the blur can be estimated by degradation of features like small points or sharp edges or by techniques like generalized cross validation (GCV) [68, 92] . In this group, the blur can be estimated globally (space-invariant blurring) [5, 6, 8, 9, 13, 14] or locally (space-variant blurring). Local blurring effects for SR were first proposed by Chiang et al. [30, 36, 37] by modeling the edges of the image as a step function v + \u03b4u where v is the unknown intensity value and \u03b4 is the unknown amplitude of the edge. The local blur of the edge is then modeled by a Gaussian blur kernel with an unknown standard deviation. The unknowns are found by imposing some constraints on the reconstruction model that they use [30, 36, 37] . Shekarfroroush and Chellappa [70] used a generalization of Papoulis's sampling theorem and shifting property between consecutive frames to estimate local blur for every frame. The blurring caused by motion depends on the direction of the motion, the velocity, and the exposure time [242, 273, 543] . It is shown in [543] that temporal blur induces temporal aliasing and can be exploited to improve the SR of moving objects in video sequences.\nInstead of estimating the point spread function, in a second group of SR algorithms known as direct methods (Sect. 5.1.3), a deblurring filter is used after the actual reconstruction of the HR image [30, 36, 37, 58, 74] . Using a high-pass filter for deblurring in the context of SR was first proposed by Keren et al. [6] . In Tekalp et al. [16] and then in [58, 87, 242, 464, 532] a Wiener filter and in [128] an elliptical weighted area filter has been used for this purpose."}, {"section_title": "Error and noise", "text": "In real-world applications, the discussed registration steps are error prone. This gets aggregated when inconsistent pixels are present in some of the LR input images. Such pixels may emerge when there are, e.g., -moving objects that are present in only some LR images, like a bouncing ball or a flying bird [125, 535, 539] . -Outliers in the input. Outliers are defined as data points with different distributional characteristics than the assumed model [124, 125, [155] [156] [157] .\nA system is said to be robust if it is not sensitive to these errors. To study the robustness of an algorithm against outlier, the concept of breakdown point is used. A breaksown point is, the smallest percentage of outlier contamination leading the results of the estimation to be outside of some acceptable range [157] . For example, a single outlier is enough to move the results of a mean estimator outside of any predicted range, i.e., the breakdown point of a mean estimator is zero. This value for a median estimator is 0.5, i.e., this estimator is robust to outliers when their contamination is less than 50 percent of all the data point [157, 216] . Besides errors in estimating the parameters of the system, an SR system may suffer from noise. Several sources of noise can be imagined in such a system, including telemetry noise (e.g., in satellite imaging) [22] , measurement noise (e.g., shot noise in a CCD, analog to digital conversion noise [226] ), and thermal noise [184, 229] . The performance of the HR estimator has a sensitive dependence on the model assumed for the noise. If this model does not fully describe the measured data, the results of the estimator will be erroneous. Several types of noise models are used with SR algorithms, for example: -linear noise (i.e., additive noise) addressed by:\n-averaging the LR pixels [6, 8, 13, 14, 91] -modeling the noise as a:\n\u2022 Gaussian (using an l 2 norm estimator).\n\u2022 Laplacian (using an l 1 norm estimator) [124, 125, [155] [156] [157] 226, 227, 463, 491, 542] which has been shown to be more accurate than a Gaussian distribution.\n-non-linear noise (i.e., multiplicative noise) addressed by:\n-eliminating extreme LR pixels [6, 8, 13, 14] .\n-Lorentzian modeling. In [308, 309] it has been discussed that employing l 1 and l 2 norms for modeling the noise is valid only if the noise involved in the imaging model of Eq. (4) is additive white Gaussian noise, but the actual model of the noise is not known. Therefore, it has been discussed to use Lorentzian norm for modeling the noise, which is more robust than l 1 and l 2 from a statistical point of view. This norm is defined by:\nwhere r is the reconstruction error and T is the Lorentzian constant.\nHaving discussed the imaging model and the parameters involved in the typical SR algorithms, the actual reconstructions of these algorithms are discussed in the following sections, according to the order given in Fig. 1 ."}, {"section_title": "Frequency domain", "text": "SR algorithms of this group first transform the input LR image(s) to the frequency domain and then estimate the HR image in this domain. Finally, they transform back the reconstructed HR image to the spatial domain. Depending on the transformation employed for transforming the images to the frequency domain, these algorithms are generally divided into two groups: Fourier transform-based and Wavelet transform-based methods, which are explained in the following subsections."}, {"section_title": "Fourier transform", "text": "Gerchberg [1] and then Santis and Gori [2] introduced the first SR algorithms. These were iterative methods in the frequency domain, based on the Fourier transform [178] , which could extend the spectrum of a given signal beyond its diffraction limit and therefore increase its resolution. Though these algorithms were later reintroduced in [26] in a non-iterative form, based on singular value decomposition (SVD), they did not become as popular as the method of Tsai and Huang [3] . Tsai and Huang's system [3] was the first multiple-image SR algorithm in the frequency domain. This algorithm was developed for working on LR images acquired by Landsat 4 satellite. This satellite produces a set of similar but globally translated images, g k , of the same area of the earth, which is a continuous scene, f ; therefore, g k (m, n) = f (x, y), where x = m + \u0394 m k and y = n + \u0394 n k . These shifts, or translations, between the LR images were taken into account by the shifting property of the Fourier transformation:\nwhere F g k and F f are the co*ntinuous Fourier transforms of the kth LR image and the HR scene, respectively. The LR images are discrete samples of the continuous scene; there-\nwhere T m and T n are the sampling periods along the dimensions of the LR image. Thus, the discrete Fourier transform of the LR images, G k , and their continuous Fourier transform, F g k , are related through [3, 126, 399, 512] :\nwhere M and N are the maximum values of the dimensions of the LR images, m and n, respectively. It is assumed that the HR scene is band limited; therefore, putting the shifting property Eq. (15) into Eq. (16) and writing the results in matrix form results in [3] :\nin which relates the discrete Fourier transform of the LR images G to the continuous Fourier transform of the HR scene, F f . SR here is therefore reduced to finding F f in Eq. (17) which is usually solved by a least squares (LS) algorithm. The seminal work of Tsai and Huang [3] assumed ideal noise-free LR images with no blurring effects. Later on, an additive noise [9, 16, 21, 68] and blurring effects [9, 68] were added to Tsai and Huang's method [3] and Eq. (17) was rearranged as [9, 16, 21] :\nin which \u03b7 is a noise term. From this model, Kim et al. [9] tried to minimize the following error, E, using an iterative algorithm:\nwhere\u1e1e f is an approximation of F f which minimizes Eq. (19) and \u2020 represents conjugate transpose [9] . Furthermore, [9] incorporated the a priori knowledge about the observed LR images into a recursive weighted least squares algorithm. In this case, Eq. (19) will be altered to:\nin which A is a diagonal matrix giving the a priori knowledge about the discrete Fourier transform of the available LR observations, G. In this case, those LR images which were known to have a higher signal-to-noise ratio are assigned greater weights. In [9, 16] it was assumed that the motion information was known beforehand. To reduce the errors of estimating the displacements between the LR images, [19, 21, 512 ] used a recursive total least squares method. In this case, Eq. (17) becomes:\nwhere P is a perturbation matrix obtained from the estimation errors [19, 21] ."}, {"section_title": "Wavelet transform", "text": "The wavelet transform as an alternative to the Fourier transform has been widely used in frequency domain-based SR algorithms. Usually it is used to decompose the input image into structurally correlated sub-images. This allows exploiting the self-similarities between local neighboring regions [357, 517] . For example, in [517] the input image is first decomposed into subbands. Then, the input image and the high-frequency subbands are both interpolated. Then, the results of a stationary wavelet transform of the highfrequency subbands are used to improve the interpolated subbands. Then, the super-resolved HR output is generated by combining all of these subbands using an inverse discrete wavelet transform (DWT). Similar methods based on the DWT have been developed for SR in [143, 150, 179, 257, 460] . In [162, 302, 320, 346, 400, 437, 448, 477, 550, 565, 566] , but the results obtained by DWT are used as a regularization term in maximum a posteriori (MAP) formulation of the problem (Sect. 5.1.6). In [391, 424] they have been used with compressive sensing (CS) methods (Sect. 5.2.1) and in [426] within a PCA-based face hallucination algorithm (Sect. 5.2).\nWavelet-based methods may have difficulties in efficient implementation of degraded convolution filters, while they can be done efficiently using the Fourier transform. Therefore, these two transforms have sometimes been combined together into the Fourier-wavelet regularized deconvolution [391] .\nIn addition to the above-mentioned methods in the frequency domain, some other SR algorithms of this domain have borrowed the methods that have been usually used in the spatial domain; among them are: [119, 211, 321, 371, 590] which have used a maximum likelihood (ML) method (Sect. 5.1.5), [144, 178, 201] which have used a regularized ML method, [197, 221, 267, 492, 512, 568] which have used a MAP method (Sect. 5.1.6), and [141, 175] which have implemented a projection onto convex set (POCS) method (Sect. 5.1.4). These will all be explained in the next section."}, {"section_title": "Spatial domain", "text": "Based on the number of available LR observations, SR algorithms can be generally divided into two groups: singleimage based and multiple-image based algorithms. The algorithms included in these groups are explained in the following subsections, according to the order given in Fig. 1 . "}, {"section_title": "Multiple image-based SR algorithms", "text": "Multiple image (or classical) SR algorithms are mostly reconstruction-based algorithms, i.e., they try to address the aliasing artifacts that is present in the observed LR images due to under-sampling process by simulating the image formation model. These algorithms are studied in the following subsections."}, {"section_title": "Iterative back projection", "text": "Iterative back projection (IBP) methods (Table 5) were among the first methods developed for spatial-based SR.\nHaving defined the imaging model like, e.g., the one given in Eq. (5), these methods then try to minimize ||A f \u2212 g|| 2 2 . To do so, usually an initial guess for the HR targeted image is generated and then it is refined. Such a guess can be obtained by registering the LR images over an HR grid and then averaging them [8, 13, 14, 20] . To refine this initial guess f (0) , the imaging model given in Eq. (4) is used to simulate the set of the available LR observations, g\nThen the error between the simulated LR images and the observed ones, which is computed by\nk || 2 2 (t is the number of iterations), is obtained and back-projected to the coordinates of the HR image to improve the initial guess [20] . This process is either repeated for a specific number of iterations or until no further improvement can be achieved. To do so, usually the following Richardson iteration is used in this group of algorithms:\nin which t is an iteration parameter, w\nk is the inverse of the warping kernel of Eq. (4),\u1e0b is an upsampling operator, * represents a convolution operation, and\u1e23 is a debluring kernel which has the following relationship with the blurring kernel of the imaging model of Eq. (4) [20] :\nwherein \u03b4 is the unity pulse function centered at (0, 0) [20] .\nIf the value of a pixel does not change for a specific number of iterations, its value will be considered as found and the pixel will not accompany the other pixels in the rest of the iterations. This increases the speed of the algorithm. As can be seen from Eq. (22), the back-projected error is the mean of the errors that each LR image causes. In [97, 124, 125] it has been suggested to replace this mean by the median to get a faster algorithm. In [249] this method has been extended to the case where the LR images are captured by a stereo setup. The main problem with the above-mentioned IBP methods is that the response of the iteration can either converge to one of the possible solutions or it may oscillate between them [8, 13, 14, 20, 75, 83] . However, this can be dealt with by incorporating a priori knowledge about the solution, as has been done in [81, 83, 124, 279, 280, 381, 407, 447, 493, 553] . In this case, these algorithms will try to minimize ||A f \u2212 g|| 2 + \u03bb||\u03c1( f )|| 2 , wherein \u03bb is a regularization coefficient and \u03c1 is a constraint on the solution. In [124, 125, 226, 227] it has been suggested to replace the l 2 norm by l 1 in both the residual term and the regularization term. Besides increasing the speed of the algorithm it has been shown that this increases the robustness of the algorithm against the outliers which can be generated by different sources of errors, such as errors in the motion estimation [124] . (24) in which B k models the relationship between the current and the previous HR image and \u03b6 represents the error of estimating B k . Besides these two equations, which were considered in the original works on using the Kalman filter for SR [62] [63] [64] 278, 388] , it is shown in [210] that modeling the relationship between the LR images can also be incorporated into the estimation of the HR images. To do so, a third equation is employed:"}, {"section_title": "Iterative adaptive filtering", "text": "where D k models the motion estimation between the successive LR images and \u03be is the error of this estimation. These algorithms have the capability of including a priori terms for regularization and convergence of the response. "}, {"section_title": "Direct methods", "text": "Given a set of LR observations, in the first SR algorithms of this group (Table 6 ) the following simple steps were involved: first, one of the LR images was chosen as a reference image and the others were registered against it (e.g., by optical flow [58, 115, 116, 190, 299, 469] ), then the reference image is scaled up by a specific scaling factor and the other LR images were warped into that using the registration information. Then, the HR image is generated by fusing all the images together and finally an optional deblurring kernel may be applied to the result. For fusing the scaled LR images, different filters can be used, such as mean and median filters [125, 156, 157, 190, 216, 299, 480] , adaptive normalized averaging [167] , Adaboost classifier [365] , and SVD-based filters [583] . These algorithms have been shown to be much faster than the IBP algorithms [30, 36, 37, 42, 43, 74] . In [125, 156, 157, 216] , it was shown that the median fusion of the LR images when they are registered is equivalent to the ML estimation of the residual of the imaging model of Eq. (4), and results in a robust SR algorithm if the motion between the LR images is translational and the blur is spatially locally invariant.\nThe order of the above-mentioned steps has sometimes been changed by some authors. For example, in [195, 319] , after finding the motion information from the LR images, they are mapped to an HR grid to make an initial estimate of the super-resolved image. Then, a quadratic Teager filter, which is an unsharping filter, is applied to the LR images and they are mapped to the HR grid using the previously found motion information to generate a second superresolved image. Finally, these two super-resolved images are fused using a median filter to generate the end result. It has been shown in [195, 319] that this method can increase the readability of text images.\nAs opposed to the above algorithms, in some other algorithms of this group, such as, e.g., in [290] , after finding the registration parameters, the LR pixels of the different LR images are not quantized to a finite HR grid, but they are weighted and then combined based on their positions in a local moving window. The weights are adaptively found in each position of the moving window. To combine the LR pixels after registration, in [306] , partition-based weighted sum filters are used. Using a moving window which meets all the locations of the HR image, the HR pixel in the center of the moving window is obtained based on the weighted sum of the LR pixels in the window. In each window location, the weights of the available pixels are obtained from a filter bank using the configuration of the missing pixels in the window and the intensity structure of the available pixels [306] .\nIn a recently developed set of algorithms, known as nonparametric SR algorithms [419] (Table 6 ), which can also be classified in the group of Direct methods, the two steps of motion estimation and fusion are combined. In this group of algorithms, which is shown to work very well with video sequences, the common requirement of SR algorithms for explicit motion estimation between LR input images has been replaced by the newly introduced concept of fuzzy motion. These methods can handle occlusion, local and complex motions like those for facial components when facial expressions change. Here, the LR images are first divided into patches. Then, every patch of a given image is compared to a set of patches, including the corresponding patch and some patches in its neighborhood, in the next LR image. Then based on the similarity of these patches and their distances from the current patch, a weight is associated to every patch indicating the importance of this patch for producing an output patch. Different methods have been developed for defining these weights. For example, in - [419, 420, 515, 561, 564, 618] , they are defined based on the non-local-means algorithm which has been used for video denoising:\nwhere\u1e90 is the reconstructed image which later on should be deblurred,w[k, l] which defined as:\nis the center of the current patch, N (k, l) is the neighborhood of the pixel (k, l), \u03a9 is the support of the entire image, D p is the down-sampling operator, R H k,l is a patch extraction operator at the location of (k, l) of the HR image, t is a time operator which changes to cover all the available LR images, T , R L i, j is a patch extraction operator at the location of (i, j) of the LR image, y is the LR input image, and w[k, l, i, j, t] is the weight associated to patch (i, j) for reconstruction of patch (k, l) and is defined by:\nwhere the first term considers the radiometric similarity of the two patches and the second one, f , considers the geometric similarities and may take any form, such as a Gaussian, a box, or a constant, and the parameter \u03c3 r controls the effect of the gray level difference between the two pixels [419, 420] . - [318, 427, 461, 465, 469] , they are defined based on steering kernel regression which takes into account the correlation between the pixel positions and their values. - [519] , they are found using Zernike moments.\n- [618], they are found based on the similarity of some clusters in the images. These measures are defined by Gaussian functions based on the structure and intensity distance between the clusters. - [510] , they are found using wavelet decomposition.\nIt worth mentioning that sometimes, as in [392] , the aboveexplained method of [419, 420] has been applied to only one single image. In such a case, usually a resolution pyramid of the single-input image is built and then the method of [419, 420] is applied to the images of this pyramid [392] ."}, {"section_title": "Projection onto convex sets", "text": "Another group of iterative methods is those based on the concept of POCS [7, 38, 42, 43, 49, 53, 72, 77, 93, 103, 122, 155, 175, 184, 204, 243, 352] . These algorithms define an implicit cost function for solving the SR problem [155] . Considering the imaging model given in Eq. (4), in the POCS method it is assumed that each LR image imposes an a priori knowledge on the final solution. It is assumed that this a priori knowledge is a closed convex set, S k , which is defined as [38] :\nwhere g k is the kth LR image, f is the solution, and \u03b4 l and -Channel by channel total variation, luminance total variation, inter-channel cross correlation sets, boundedness, and non-negativity constraints for color images [204] .\nIt has been discussed in [93] that since projections onto sets defined at edge locations cause a ringing effect, it is better to reduce the amount of deblurring at the edges using blur functions at the direction of the edge gradient more like an impulse. Furthermore, in [184] it is discussed that defining the constraints on individual pixel values rather than on the whole image leads to a simpler implementation."}, {"section_title": "Maximum likelihood", "text": "Let us assume that the noise term in the imaging model given in Eq. (5) is a Gaussian noise with zero mean and variance \u03c3 . Given an estimate of the super-resolved image,f , the total probability of an observed LR image g k is [22, 50, 87, 157, 253, 312, 359, 528] :\nand its log-likelihood function is:\nThe ML solution [22] (Table 7) seeks a super-resolved image, f ML , which maximizes the log-likelihood of Eq. (32) for all observations:\n(33) which can be solved by:\nwhich results in:\nThe ML solution of an SR problem, which is equivalent to the LS solution of the inverse problem of Eq. (4), is an illconditioned problem, meaning it is sensitive to small disturbances, such as noise or errors in the estimation of the imaging parameters. Furthermore, if the number of LR images is less than the square of the improvement factor, it is ill-posed as well. This means that there might not be a unique solution.\nTo deal with these problems, there is needed some additional information to constrain the solution. Such information can be a priori knowledge about the desired image. The a priori term can prefer a specific solution over other solutions when the solution is not unique. The involvement of that a priori knowledge can convert the ML problem to a MAP problem, which is discussed in the next section.\nIf the number of LR images over-determines the superresolved images, the results of ML and MAP are the same and since the computation of ML is easier, it is preferred over MAP. However, if the number of the LR images is insufficient for the determination of the super-resolved image, the involvement of a priori knowledge plays an important role and MAP outperforms ML [22] . Table 7 Reported probabilistic-based SR works"}, {"section_title": "Method", "text": "Reported in"}, {"section_title": "Maximum a posteriori", "text": "Given one or more LR images, g k , MAP methods [22] ( Table 7) find an estimate of the HR image,f , using Bayes's rules:\nBy deleting the known denominator of the above equation and taking logarithms, the estimated response of the superresolved imagef using the MAP method is: (g 1 , g 2 , . . . ,\nSince the LR images are independent of each other, the above equation becomes:\nwhich can be rewritten a\u015d\nUsing the same notations as used for ML, and assuming a zero mean white Gaussian noise, the likelihood distribution p(g k |f ) can be rewritten as:\nwhere C 1 is constant, and \u03c3 2 k is the error variance. Then, the a priori can be written in Gibbs form as:\nwhere C 2 is constant and \u0393 ( f ) is the a priori energy function. Putting Eqs. (40) and (41) into Eq. (39), the MAP solution can be written as:\nwhere \u03bb is the regularization parameter [109, 134, 509] . For finding the best possible value of \u03bb, different approaches can be used [588] . For example, in [83] the relationship between the residual term, ||A f \u2212 g|| 2 , and the regularization term, ||\u03c1( f )|| 2 , is studied for different values of \u03bb to generate an L-curve. Then, it is decided that the desired value for \u03bb is located at the corner of this curve. In [91] , GCV has been used to find the best possible value for \u03bb. In [508] , it is suggested to use U -curves for this purpose and also discussed that these curves not only perform better than L-curves, but also can provide an interval in which the best value of the regularization term can be found. In this method first a Ucurve function is defined based on the data fidelity and a priori terms of the SR algorithm, and then the left maximum curvature point of the function is chosen as the best value of the regularization term [508] . Based on the nature of the terms of Eq. (42), different optimization methods can be followed, which are discussed in Sect. 6.2. The first term of Eq. (42) is already given in Eq. (32); there are however many different possibilities for the second term, the regularization term, which are discussed in the next subsections. The regularization terms are mostly, but not always, used when the solution of an inverse problem is under-determined (they might be used when the system is determined or over-determined as well: in this case they are used mainly for removing any artifacts which might appear after reconstruction). Therefore, there might be many solutions for a given set of observations. The regularization terms, in the form of a priori knowledge, are used to identify one of these available solutions which best fits some predefined desired conditions, and also to help the convergence of the problem. Such conditions can be, e.g., the smoothness of the solution, discontinuity preserving, etc. These can be achieved by, e.g., penalizing the high gradients of the solution or by defining some specific relationships between the neighboring pixels in the solution. Regularization terms have been used along with different SR methods: from iterative methods, direct methods, POCS and ML, to MAP methods."}, {"section_title": "Markov random fields (MRF)", "text": "The first a priori term used in the literature for SR along with a MAP approach was introduced in [22] (Table 8) , by which the values of the obtained pixels in the super-resolved image were updated according to the values of the neighboring pixels (4 \u00d7 4 or 8 \u00d7 8). In other words, it imposes a similarity on the structures of the neighbors. This similarity can be best modeled by MRF. Therefore, MRFs have been widely used in the SR literature for modeling a priori knowledge. However, for reasons of feasibility, MRFs are usually expressed by the equivalent Gibbs distribution as in Eq. (41). In this case the energy function of Eq. (41) can be written as:\nwhere V r is a function of a set of local points r , which are called cliques, R is the set of all of these cliques, and \u03c1 r are potential functions defined over the pixels in each clique r . The first two sums are for meeting all the pixels and the last \nwhere Q is a symmetric, positive definite matrix. In the simplest case Q = I (a minimal energy regularization which limits the total energy of the image) results in the a priori term || f || 2 assuming a zero mean and an i.i.d. Gaussian distribution for the pixel values. This results in the following Gibbs distribution for the a priori term [84, 95, 123, 135, 199, 247, 248, 251, 258, 316, 379] :\nThe Tikhonov regularization term does not allow discontinuities in the solution and therefore may not recover the edges properly [354] . To preserve sharp edges, which carry important information, a distribution should be used which penalizes them less severely [313, 314] . To do so, Huber MRFs (HMRF) [34] are designed. The Gibbs distribution of a HMRF is defined as:\nwhere x here can be the first or the second derivative of the image and \u03b1 is a parameter separating the quadratic and linear regions [260] . A HMRF a priori is an example of a convex but non-quadratic prior, which results in non-linear cost functions (Table 8) .\nIf Q in Eq. (44) is non-diagonal, its off-diagonal elements model the spatial correlations between neighboring pixels, which results in a multi-variate Gaussian distribution over f . This a priori term is known as a Gaussian MRF (GMRF) [28] . In this case, \u0393 [of Eq. (43)] can be defined as a linear operator applied to f , like ||\u0393 f || 2 , which estimates the first and the second derivatives of f and imposes spatial smoothness (Table 8) . Therefore, it removes the pixels with highfrequency energies. This helps to remove noise, but at the same time smooths any sharp edges. Using the energy function of Eq. (43), it can be expressed as:\nin which \u03c1 r is a quadratic function of the directional smooth measurements of d m i, j which for every pixel located at (i, j) are defined by [509] :\nIt is discussed in [509] that the proper weighting of this directional smoothness measure can improve the performance of this regularization term. If the image gradients are modeled by generalized Gaussian distributions, a so-called generalized Gaussian MRF (GGMRF) [27] a priori term can be used, which has the following form:\nwhere 1 < p < 2. Similar to HMRF, GGMRF is also a convex non-quadratic a priori [27, 84, 123, 556] . In [109, 134] , this function has been defined as a quadratic cost function on pairwise cliques on a first-order neighborhood:\nTotal variation (TV) If \u03b1 in the Huber formulation of HMRF tends to zero, the Huber a priori term converts to a so-called TV norm that applies similar penalties for a smooth and a step edge and tends to preserve edges and avoid ringing effects (Table 8) . It is defined by:\nwhere \u2207 is the gradient operator. In [607] the TV terms are weighted with an adaptive spatial algorithm based on differences in the curvature. [125] (Table 8) , which is used to approximate TV, is defined by:"}, {"section_title": "Bilateral total variation (BTV)", "text": "where S k x and S l y shift f by k and l pixels in the x and y directions to present several scales of derivatives, 0 < \u03b1 < 1 imposes a spatial decay on the results [125] , and P is the scale at which the derivatives are computed (so it computes the derivatives in multiple scales of resolution [226] ).\nIt is discussed in [372] that this a priori term generates saturated data if it is applied to unmanned aerial vehicle data. Therefore, it has been suggested to combine it with the Hubert function, resulting in the following bilateral total variation Hubert:\nwhere A is the BTV regularization term as in Eq. (51) and \u03b1 is obtained by \u03b1 = median[|A \u2212 median|A||]. It is discussed in [372] that this term keeps the smoothness of the continuous regions and preserves edges in discontinuous regions. In [472] a locally adaptive version of BTV, LABTV, has been introduced to provide a balance between the suppression of noise and the preservation of image details [472] . To do so, instead of the l 1 norm of Eq. (51), an l p norm has been used, where p for every pixel is defined based on the difference between the pixel and its surroundings. In smooth regions where noise reduction is important, p is set to large values close to two and in non-smooth regions where edge preservation is important, p is set to small values close to one. The same idea of adaptive norms, but using different methods for obtaining the weights, has been employed in [485, 491, 525, 532, 535, 539, 542] ."}, {"section_title": "Biomodality priori (BP)", "text": "which is modeled as an exponentiated fourth-order polynomial. The maxima of this polynomial are located at centers of distributions of the foreground and background pixels. It can be expressed as [181] :\nwhere C is a normalization constant, \u03bc 0 and \u03bc 1 are the centers of the peaks of the polynomial which in [181] are estimated by expectation maximization (EM), and is the width of the foreground and background distributions. It is shown in [181] that this a priori works quite well for two class problems like text improvement in which the text is considered as the foreground and the rest of the image as the background.\nOther regularization terms Other regularization terms used in the multiple image SR literature are:\n-Natural image prior [139, 343, 465, 514, 613] .\n-Stationary simultaneous auto regression (SAR) [170] which applies uniform smoothness to all the locations in the image. -Non-stationary SAR [215] in which the variance of the SAR prediction can be different from one location in the image to another. -Soft edge smoothness a priori, which estimates the average length of all level lines in an intensity image [279, 407] . -Double-exponential Markov random field, which is simply the absolute value of each pixel value [282] . -Potts-Strauss MRF [303] .\n-Non-local graph-based regularization [358] . -Corner and edge preservation regularization term [363] . -Multichannel smoothness a priori which considers the smoothness between frames (temporal residual) and within frames (spatial residual) of a video sequence [441] . -Non-local self-similarity [518] . -Total subset variation, which is a convex generalization of the TV regularization term [467] . -Mumford-Shah regularization term [527] . -Morphological-based regularization [591] . -Wavelet based [346, 477, 550] .\nAnother form of regularization term is used with hallucination or learning-based SR algorithms, which are discussed in the next section."}, {"section_title": "Single-image based SR algorithms", "text": "During the sub-sampling or decimation of an image, the desired high-frequency information gets lost. The generic smoothness priors discussed in the previous section can regularize the solution but cannot help recover the lost frequencies, especially for high improvement factors [214] . In singleimage based SR algorithms, these generic priors are replaced by more meaningful and classwise priors like, e.g., the class of face images. This is because images from the same class have similar statistics. Furthermore, the accuracy of multipleimage based SR algorithms is highly dependent on the estimation accuracy of the motions between the LR observations, which gets more unstable in real-world applications where different objects in the same scene can have different and complex motions. In situations like these, single-image based SR algorithms (a.k.a class based) may work better [241] . There algorithms are either reconstruction based (similar to multiple-image based algorithms) or learning based. These are described in the following two subsections."}, {"section_title": "Learning-based single-image SR algorithms", "text": "These algorithms, a.k.a, as learning-based or Hallucination algorithms (Table 9 ) were first introduced in [4] in which a neural network was used to improve the resolution of fingerprint images. These algorithms contain a training step in which the relationship between some HR examples (from a specific class like face images, fingerprints, etc.) and their LR counterparts is learned. This learned knowledge is then incorporated into the a priori term of the reconstruction. The training database of learning-based SR algorithms needs to have a proper generalization capability [241] . To measure this, the two factors of sufficiency and predictability have been introduced in [241] . Using a larger database does not necessarily generate better results, on the contrary, a larger number of irrelevant examples not only increase the computational time of searching for the best matches, but also disturb this search [406] . To deal with this, in [406] it is suggested to use a content-based classification of image patches (like codebook) during the training. Different types of learning-based SR algorithms are discussed in the following subsections.\nFeature pyramids The notable work in this group was developed by Baker and Kanade [57, 71, 82, 99, 100] for face hallucination. In the training step of this algorithm, each HR face image is first down-sampled and blurred several times to produce a Gaussian resolution pyramid. Then, from these Gaussian pyramids, Laplacian pyramids and then Feature pyramids are generated. Features can be simply the derivatives of the Gaussian pyramids which contain the low-frequency information of the face images, Laplacian pyramids which contain the band-pass frequency of the face images, steerable pyramids which contain multiorientational information of the face images [208] , etc. Having trained the system, for an LR test image the most similar LR image among the available LR images in all the pyramids is found. In the original work of [57, 58, 71, 82] , the nearest neighbor technique was used for finding the most similar images/patches. But in [281, 285] , the LR images/patches were arranged in a tree structure, which allows fast search techniques to find the most similar patches. Having found the most similar image/patch, the relationships between the found LR image and its higher counterparts (which have been coded as child-parent structures) are used to predict/hallucinate the high-resolution details of the LR input image as an a priori term in a MAP algorithm similar to the one proposed in [25] which uses an HMRF a priori term. However, the employed a priori term similar to most of the MAP methods considers local constraints in the reconstructed image. Exactly this technique has been extended in [200] to hallucinate 3D HR face models from LR 3D inputs.\nBelief network The notable work of this group was developed by Freeman and Pasztor [65, 66] . These algorithms use a belief network such as a Markov network or a tree structure. In the case of the Markov network [23, 65, 66, 76, 102, 146, 203, 254, 259, 275, 411, 449] both the LR image and its HR counterparts are divided into patches. Then the corresponding patches in the two images are associated to each other by a so-called observation function. The observation function defines how well a candidate HR patch matches a known LR patch [259] . The neighbor patches in the superresolved HR image are assumed to be related to each other by a so-called transition function [259] . Having trained the model (parameters), it infers the missing HR details of LR input images using a belief propagation algorithm to obtain a MAP super-resolved image. For learning and inferring, the Markov assumptions are used to factorize the posterior probability. For inferring the HR patches, the following equation is used:\nwhere f i is the ith patch of the HR image f , g i is its associated LR patch, and j changes to include all the neighbor nodes of the ith patch and L i j is defined by:\nwhereL jk is the L jk from the previous iteration with initial values of 1 [23, 65, 66, 76, 102, 146] . In [185] , in addition to the association potential function of [65, 66, 76, 102, 146] , which models the relationship between the LR and HR patches, a so-called interaction potential function is also used to model the relationship between the HR patches. It is shown in [185] that these two potential functions can be used effectively to model the temporal information which can be available in an input LR video sequence.\nIn [259] a system has been developed for hallucination of face images in which the transition and the observation functions of [23, 65, 66, 76, 102, 146] are no longer static for the entire image, but are adapted to different regions of the face image. Similar to the above system, [111] has used a mixture of tree structures in the belief network for learning the relationship between the LR images and their corresponding HR ones in the training step.\nProjection The a priori term learned in [57, 71, 82, 99, 100] imposes local constraints on the reconstructed super-resolved image. For imposing global constraints, some SR algorithms have used projection-based methods for learning the a priori term of the employed MAP algorithm, e.g., in [85, 142, 348, 401, 426, 443, 444, 453, 454, 458, 459, 466, 468, 505] PCA in [323, 350, 503, 526] independent component analysis (ICA), and in [473] morphological component analysis (MCA) have been used. In PCA every face is represented by its PC basis, which is computed from training images at the desired resolution f = V y \u2212 \u03bc, where V is the set of PC basis vectors and \u03bc is the average of the training images [85] .\nTo consider both local and global constraints, Liu et al. [90, 301] combined non-parametric Markov networks (for considering the local variances) and a PCA-based a priori (for considering the global variances) for face hallucination. In [276, 466] , a Kernel-PCA based prior that is a non-linear extension of the common PCA was embedded in a MAP method to take into account more complex correlations of human face images. In [410] again Kernel PCA but this time with radius basis function (RBF) was used for face hallucination. In [127] PCA was again used for hallucination, but not directly for hallucination of face images, but for their features, i.e., the hallucination technique was applied to the feature vector of the LR face image to hallucinate the feature vector of the corresponding HR image, without actual reconstruction of the image. This hallucinated feature vector was then used for recognition. The same technique of [127] but with different types of features has been used in some other works, e.g., in\n- [164, 165, 484, 586] , Gabor filter responses are used as features to hallucinate a texture model of the targeted HR image. These systems can hallucinate a frontal face image from rotated LR input images. - [151, 206, 207, 345] , local visual primitives have been used as features and then have been hallucinated using a locally linear embedding (LLE) technique. - [374] two orthogonal matrices of SVD of facial images which contain the most important features for recognition have been used in the SR algorithm.\nIn PCA-based methods, usually the matrices representing each training image are first vectorized (by arranging, e.g., all the columns of each matrix in only one column) and then they are combined into a large matrix to obtain the covariance matrix of the training data for modeling the eigenspace. It is discussed in [361] that such vectorization of training images may not fully retain their spatial structure information. Instead, it is suggested to apply such a vectorization to the features extracted from training data and use them for SR. It is shown in [361] that bilateral projections can produce such features.\nIt has been discussed in [380, 456, 525] that PCA-based hallucination methods are very sensitive to occlusion since the PCA bases are holistic and they mostly render the hallucination results towards the mean face. Therefore, it has been proposed to use non-negative matrix factorization to divide the face images into relatively independent parts, such as eyes, eyebrows, noses, mouthes, checks, and chins. Then, a MAP sparse-representation based algorithm has been applied to these parts. A similar technique has been used in [554] ."}, {"section_title": "Neural networks", "text": "The same concept of belief-network based methods, but using different types of neural networks (NN), has been employed in many different SR algorithms. Examples of such networks are linear associative memories with single [61] and dual associative learning [192] , Hopfield NN [96, 327] , probabilistic NN [130, 304] , integrated recurrent NN [136] , multi-layer perceptron [196, 355, 386, 548] , feed forward NN, [232, 233] , and RBF [328, 608] .\nManifold Manifold-based methods [151, 206, 207, 310, 311, 328, 330, 344, 383, 387, 404, 405, 470, 496, 522, 554, 562, [569] [570] [571] assume that the HR and LR images form manifolds with similar local geometries in two distinct feature spaces [344] . Similar to PCA, these methods are also usually used for dimensionality reduction. These methods generally consist of the following steps:\n-Generate HR and LR manifolds in the training step using some HR images and their corresponding LR counterparts. -In the testing step, first divide the input LR test image into a set of LR patches. For each LR patch of this image, l x :\n-Find its k-nearest neighbor patches, l j , on the LR manifold. -Use these k-nearest neighbors to calculate the weights, w L x , for reconstructing the patch l x :\narg min\nwhere the summation of the weights is equal to one. -Use the involved weights and neighbors to find the corresponding objects in the HR manifold to reconstruct the HR patch of l x :\nwhere N L (x) includes the neighbors of l x , and the h j s are the HR patches associated to the LR neighbor patches of l x .\nThough most of the manifold-based methods use this manifold assumption that the reconstruction HR weights are similar to the LR weights, i.e.: w L x j = w H x j , it is shown in [404, 405, 470 ] that this might not be completely true and therefore it is suggested to align the manifolds before the actual reconstruction. To do so, in [404, 405, 470] , a so-called common manifold has been used, which helps the manifold alignment by learning two explicit mappings which map the paired manifolds into the embeddings of the common manifold.\nUnlike traditional dimensionality reduction techniques, such as PCA and LDA, manifold-based methods can handle non-Euclidean structures. Reported manifold-based methods for SR have mostly been applied to face hallucination and can be classified into three groups:\n-Locally linear embedding (LLE) [151, 206, 207, 345, 554, 562, 598 ]. -Locality preserving projections (LPP) [310, 311, 328, 330, 383, 462, 496] . -Orthogonal locality preserving projections (OLPP) [344] .\nThe main difference between LPP and LLE is that LPP can handle cases which have not been seen during training better than LLE [330] . OLPP can produce orthogonal basis functions and therefore can provide better locality preserving than LPP.\nAs opposed to the majority of the previously mentioned learning-based SR systems, in which for generating each HR image patch only one nearest LR patch and its corresponding HR patch in the training database are used, in manifoldbased methods [151, 206, 207] , multiple nearest neighbor LR patches are used simultaneously.\nManifold-based methods are usually applied in two steps. In the first step, they are combined with a MAP method [310, 311, 496, [569] [570] [571] or a Markov-based learning method [206] like those in [65, 66, 76, 102, 146, 203] to apply a global constraint over the super-resolved image. In the second step, they use a different technique like kernel ridge regression [344, 496] , graph embedding [569] , radial basis function and partial least squares (RBF-PLS) regression [570, 571] to apply local constraints to the super-resolved image by finding the transformation between low and HR residual patches.\nIn these algorithms, the sizes of the patches, their amount of overlap, the number of the patches involved (the nearest LR patches), and the features employed for representing the patches (e.g., first-and second-order gradients [151] , edges [387] ) are of great importance. If the number of patches are too large, the neighborhood embedding analysis gets difficult. If this number is too small, the global structure of the given data space cannot be captured [310, 311, 387] .\nTensor Tensors are a higher-order generalization of vectors (first order) and matrices (second order) [342] . Tensor analysis can be considered as a generalized extension of traditional linear methods such as PCA for which the mappings between multiple factor spaces are studied [341] . It provides a means of decomposing the entire available data into multimodal spaces and then studies the mappings between these spaces [88, 188, 189, 192, 193, 236, 262, 269, 341, 342, 555] .\nIn [188] the super-resolved reconstructed image is computed by an ML identity parameter vector in an HR tensor space, which is a space that has been obtained by applying tensor decomposition to the HR training images. In [189] it is shown that the method of [188] can improve the recognition rate of a face recognition system. In [193, 194] , a patchbased tensor analysis is applied to a given LR input image to hallucinate a face image. To do so, the K 1 nearest images of the input image are first found in the training database. Then, the input image is divided into overlapping patches and for each patch the K 2 nearest patches among the previously found K 1 training images are found. The patches are then weighted in a way that their fusion generates an HR image which in case of down-sampling possesses the minimum distance from the down-sampled versions of the HR images involved. This patch-based approach enforces local consistency between the patches in the hallucinated image. In [341] , a tensor-based hallucination has been generalized to different facial expressions and poses.\nCompressive sensing Though the sparsity of images had been implicitly utilized in many SR works, recent advances in sparse signal processing have introduced the possibility of recovering linear relationships between HR signals and their LR projections, and have motivated many researchers to explicitly use the sparsity of the images for SR algorithms. In sparse coding it is assumed that there is an overcomplete dictionary like D \u2208 R n\u00d7K which contains K prototype image patches. It is assumed that an HR image f \u2208 R n can be represented as a liner combination of these patches: f = D\u03b1 where \u03b1 is a very sparse coefficient vector, i.e., ||\u03b1|| 0 << K .\nThese algorithms [380, 381, 391, 487, 499, 504, 506, 518, 558, 572, 573, 577, 579, 580, 584, 596, 598, 602, 603, 605, 609] , [615] usually assume that there are two overcomplete dictionaries: one for the HR patches, D h , and one for their corresponding LR counterparts, D l . The latter has been produced from the former by a degradation process like that in Eq. (4) . Usually the mean of the pixel values of each patch is subtracted from its pixels to make the dictionaries more texture representative than intensity. Given an input LR image, g, the above-mentioned discussion for sparse coding is used to find a set of coefficients,\u03b1, which minimizes:\nextraction operator (usually a high-pass filter [380, 381, 506, 602] ), P extracts the overlapping area between the current target patch and the previously reconstructed image, and \u03b2 is a control parameter for finding a tradeoff between matching the LR input and finding an HR patch which is compatible with its neighbors.\nHaving found such an \u03b1 minimizing Eq. (57), the following equation is used to obtain an estimate for the reconstructed HR image:\nThe reconstructedf from Eq. (58) has been mostly used as an a priori term in combination with other SR methods. For example, in the following works, it has been combined with:\n-a MAP method [380, 391, 456, 518, 558, 584, 596, 598, 602, 603, 605] , -an IBP method [381, 506] , -a support vector regression [504] , and -a Wavelet-based method [424] .\nCS algorithms have been successfully used for SR along with wavelet-based methods. However, due to coherency between the LR images resulting from the down-sampling process and the wavelet basis, CS algorithms have problems with being directly employed in the wavelet domain. To reduce this coherency, [424] applies a low-pass filter to the LR inputs before using the CS technique in the wavelet domain.\nIt is discussed in [580] that preserving the local topological structure in the data space results in better reconstruction results. While trying to do so, the incoherency of the dictionary entries should be taken into account. To do this:\n- [424] applies a low-pass filter to the LR inputs before using the CS technique in the wavelet domain, and - [580] uses the non-local self-similarity concept of manifold-based learning methods."}, {"section_title": "Reconstruction-based single-image SR algorithms", "text": "These algorithms [138, 202, 279, 286, 287, 362, 368, 394, 407, 431, 445, 500, 520, 546] similar to their peer multiple imagebased SR algorithms try to address the aliasing artifacts that are present in the LR input image. These algorithms can be classified into the following three groups.\nPrimal sketches The idea of a class-based a priori, which is used in most of the hallucination-based algorithms, mostly for the class of face images, was extended to generic a prioris in [138, 286, 431, 500] , where primal sketches were used as the a priori. The key point in primal sketches is that the hallucination algorithm is applied only to the primitives (like edges, ridges, corners, T-junctions, and terminations) but not to the non-primitive parts of the image. This is because the a priori term related to the primitives can be learned but not those for the non-primitives. Having an LR input image they [138] first interpolate (using bicubic interpolation) it to the target resolution, then for every primitive point (basically every point on the contours inside the image) a 9 \u00d7 9 patch is considered. Then, based on the primal sketch prior, and using a Markov chain inference, the corresponding HR patch for every LR patch is found and replaced. This step actually hallucinates the high-frequency counterparts of the primitives. This hallucinated image is then used as the starting point for the IBP algorithm of [20] to produce an HR image.\nGradient profile Using the fact that the shape statistics of the gradient profiles in natural images is robust against changes in image resolution, a gradient profile prior has been introduced in [368, 546] . Similar to the previous approaches that learn the relationship between the LR and HR images, gradient profile-based methods learn the similarity between the shape statistics of the LR and HR images in a training step. This learned information will be used to apply a gradientbased constraint to the reconstruction process. The distribution of this gradient profile prior is defined by a general exponential family distribution [generalized Gaussian distribution (GGD)] [368, 546] :\nwhere \u0393 is the Gamma function and \u03b1 = \u0393 (\nis the improvement factor. An interesting property of \u03b1 is that it makes the second moments of the above GGD equal to \u03c3 2 . This means that the second order of this prior can be used for estimating \u03c3 . Finally, \u03bb controls the shape of the distribution [368, 546] .\nFields of experts Fields of experts is an a priori for learning the heavy non-Gaussian statistics of natural images [202, 394] . Here usually contrastive divergence is used to learn a set of filters, J , from a training database. Having learned these filters or bases, the a priori term is represented as:\nwhere\nis the number of cliques, N is the number of experts, and the \u03b1s are positive parameters which make the experts have proper distributions. The experts, \u03c6 i , are defined by:\nOther reported reconstruction-based a prioris in this group include:\n-Wavelet based [162, 302, 320, 437, 448] .\n-Contourlet transform based [237, 438, 601] . -Bilateral filters used for edge preserving in an IBP method in [280] for dealing with the chessboard and ringing effects of IBP. -Gaussian mixture model-based methods [587] ."}, {"section_title": "Discussion of other related issues in SR", "text": "In this section, some other issues that might arise while working with SR algorithms are discussed, including the methods for optimizing the cost functions in SR algorithms, color issues, improvement factors, the assessment of SR algorithms, the most commonly employed databases in these algorithms, 3D SR algorithms, and finally, speed performance of SR algorithms. But before going into the details of these issues, we first discuss some combined SR algorithms in the next subsection."}, {"section_title": "Other SR methods", "text": ""}, {"section_title": "Combined methods", "text": "The above-mentioned methods for solving the SR problem have often been combined with each other, resulting in new groups of algorithms. Examples of such combinations can be found in -[31,38], where ML and POCS are combined, which allows incorporating non-linear a priori knowledge into the process. - [104, 161] , MAP and POCS are combined and applied to compressed video. - [138] , MAP and IBP are combined.\n- [423, [481] [482] [483] 516, 538, 545, 548] , where a reconstructionbased SR is followed by a learning-based SR."}, {"section_title": "Simultaneous methods", "text": "The reconstruction of the super-resolved image and the estimation of the involved parameters sometimes have been done simultaneously. For example, in [10, 41, 44, 46, 113, 217, 224, 293, 295, 315, 329, 332, 430, 433, 469, 513, 530, 552] a MAP method has been used for simultaneous reconstruction of the super-resolved image and registration of the LR images. In these cases, the MAP formulation of Eq. (62) will be, as in [41, 44] :\nwherein W k can include all the decimation factors or only some of them, as in the blur kernel [10, 46] and the motion parameters [41, 44] . In other groups of simultaneous SR algorithms, the blurring parameter is estimated at the same time that the reconstruction of the super-resolved image is being carried out. Examples of this group are:\n- [70] which uses a generalization of Papoulis' sampling theorem and the shifting property between consecutive frames; - [68, 92] which use a GCV-based method; - [186, 214, 231, 258, 313, 366, 379, 419] where a MAPbased method has been used.\nFinally, in a third group of simultaneous SR algorithms, all the involved parameters are estimated simultaneously, as in [112, 140, 316] ."}, {"section_title": "Cost functions and optimization methods", "text": "The most common cost function, from both algebraic and statistical perspectives, is the LS cost function, minimizing the l 2 norm of the residual vector of the imaging model of Eq. (4) [155, 184, 223] . If the noise in the model is additive and Gaussian with zero mean, the LS cost function estimates the ML of f [155] .\nFarsiu et al. [124, 226, 227] proposed to use l 1 instead of the usual l 2 for both the regularization and error terms, as it is more robust against outliers than l 2 . However, choosing the proper optimization method depends on the nature of the cost function. If it is a convex function, gradient-based methods (like gradient descent, (scaled) conjugate gradient, preconditioned conjugate gradient) might be used for finding the solution (Table 10 ). The only difference between conjugate and gradient descent is in the gradient direction. 87, 91, 109, 118, 133, 172, 181, 184, 197, 199, 204, [216] [217] [218] 221, 223, 226, 229, [251] [252] [253] 272, 276, 279, 289, 322, 329, 352, 368, 372, 395, 407, 409, 465, 471, 494, 529, 546, 559, 589, 609] If it is non-linear, a fixed-point iteration algorithm like Gauss-Seidel iteration [27, 295, 301, 307, 313, 314, 316, 530] is appropriate. If it is non-convex, the time-consuming simulated annealing can be used [5, 6, 135, 241, 484] , or else graduated non-convexity [95, 293, 497] (with normalized convolution for obtaining an initial good approximation) [541] , EM [113, 181, 288, 455] , genetic algorithm [174] , Markov chain Monte Carlo using Gibbs sampler [209, 214, 234, 241, 254, 613] , energy minimization using graph-cuts [248, 279, 305, 536] , Bregman iteration [354, 591] , proximal iteration [358] , (regularized) orthogonal matching pursuit [391, 465] , and particle swarm optimization [449] might be used."}, {"section_title": "Color images", "text": "Dealing with color images has been discussed in several SR algorithms. The important factors here are the color space employed and the employment of information from different color channels. The most common color spaces in the reported SR works are shown in Table 11 .\nFor employing the information of different channels, different techniques have been used. For example, in - [13, 138, 151, 180, 246, 286, 322, 368, 387, 392, 538, 539, 546] , the images are first converted to YIQ color space (in [162, 558, 559, 609] to YCbCr). Since most of the energy in these representations is concentrated in the luminance component, Y, the SR algorithm is first applied to this component and the color components are simply interpolated afterwards. - [471] , the same process has been applied to all the three YCbCr channels of the images. - [442, 464, 527, 556] , the same process has been applied to all the three RGB channels of the images. - [117] , the information regarding every two different channels of an RGB image are combined to improve the third one, then at the end, the three improved channels are combined to produce a new improved color image. - [155] [156] [157] [158] 226, 227, 378] , inspired by the fact that most of the commercial cameras use only one CCD where every pixel is made sensitive to only one color and the other two color elements of the pixel are found by demosaicing techniques, additional regularization terms for luminance, chrominance, and nonhomogeneity of the edge orientation across the color channels in RGB color space are used. Similar terms have been used for the same purpose in [160] but the images are first converted to the YCbCr color space. [13, 151, 180, 246, 286, 322, 368, 387, 392, 538, 539, 546] YCbCr [160, 162, 558, 559, 609] RGB [117, [155] [156] [157] [158] 226, 227, 378] YUV [203, 216, 287, 393, 395, 398, 474, 494, 547] L*a*b [553] - [279, 407] , alpha matting along with soft edge smoothness prior has been used. - [287] , the SR algorithm has been applied to the luminance component of the images in YUV color space, then the absolute value of the difference of the luminance component of each pixel in the super-resolved image from its four closest pixels in the LR image is computed as d 1 , d 2 , d 3 , and d 4 . Then, these weights are converted to normalized weights by\nand \u03b1 is a partitive number which defines the penalty for a departure from luminance. Finally, the obtained w i s are used to linearly combine the color channels of the four neighboring pixels to obtain the color components of the super-resolved pixel."}, {"section_title": "Improvement factor and lower bounds", "text": "The bounds derived so far for the improvement factor of a reconstruction-based SR algorithm are of two types. The first type is deterministic, based on linear models of the problem and looking at, e.g., the condition numbers of the matrices and the available number of LR observations. These deterministic bounds tend to be very conservative and yield relatively low numbers for the possible improvement factors (around 1.6). It is shown in [89, 166, 255] that for these deterministic bounds, if the number of available LR images is K and the improvement factors along the x and y directions are s x and s y , respectively, under local translation condition between the LR images, the sufficient number of LR images for a reconstruction-based SR algorithm is 4(s x s y ) 2 . Having more LR images may only marginally improve the quality of the reconstructed image, e.g., in [89, 166, 255] it is discussed that such an improvement can be seen in only some disjoint intervals.\nAnother class of bounds on the improvement factors of reconstruction-based SR algorithms is stochastic in nature and gives statistical performance bounds which are much more complex to interpret. These are more accurate and have proven to be more reliable, but they are not used very often because there are many \"it depends\" scenarios, which are to some extent clarified in [168, 256, 317, 424] where using [ 9, 16, 21, 46, 119, 133, 141, 150, 151, 160, 162, 174, 179, 197, 207, 209, 210, [215] [216] [217] 223, 231, 237, 241, 247, 251, 252, 267, 273, 277, 281, 285, 308, 309, 317, 321, 323, 327, 332, 345, 346, 350, 354, 361, 365, [368] [369] [370] [371] 378, 399, 440, 450, 452, 455, 458, 459, 465, 487, 494, 497, 513, 515, 526, 541, 543, 546, 548, 556, 557, 561, 570, 571, 577, 578, 582, 593, 594, 597, 598, 600, 606, 607, 611, 614 ,617] 3 [ 84,85,87,113,124,126,138,151,160,174,226-228,241,245,251,252,279,281,283, 285,306,350,368,381,392,404,405,407,440,443,444,494,515,518,521,546,552, 558,559,564,577,580,587,594,596,598,600,602,606,609,611,613,618] 4 [ 30,36,37,61,84,85,90,91,94,105,112,113,127,128,136,140,144,151,157,160,164, 165,167,174,180,187-189,191-194,200,201,207,208,211,217,226-228,235,248,253,257,259,267,270,273,279,287,302,303,310,311,313,314,321, 323,337,344,346,348,350,354,368,370,374,378,380,387,392,393,395,404,405, 407,412-414,424,426,436,440,443,444,446,450,452,455,457-459,472,475,476,481-483,494-496,503,506,512,515,521,526,528,532,536,538, 541,543,546,548,549,551,556,557, It is discussed in [212] that one may check the practical limits of reconstruction-based SR algorithms which are regularized by Tikhonov-like terms by discrete picard condition (DPC) .\nIn [300, 349] , a closed-form lower bound for the expected risk of the root mean square error (RMSE) function between the ground truth images and the super-resolved images obtained by learning-based SR algorithms has been estimated. Here, it is discussed that having obtained the curve of the lower bound against the improvement factor, a threshold at which the lower bound of the expected risk of learningbased SR algorithms might exceed some acceptable values can be estimated. In [492, 589] , it is discussed that preconditioning the system can allow for rational improvement factors [598] .\nIn [592] , it is discussed that all the images in the available LR sequence do not necessarily provide useful information for the actual reconstructed image. To measure the usefulness of the images, a priority measure is assigned to each of them based on a confidence measure, to maximize the reconstruction accuracy, and at the same time to minimize the number of the sample sets.\nDespite the above-mentioned theoretical thresholds, the reported improvement factors listed in Table 12 are mostly higher than these bounds. Besides the improvement factors reported in the table the following ones have been reported by at least one paper: 9 [573] , 10 [374, 521] , 12 [600] , and 24 [330] ."}, {"section_title": "Assessment of SR algorithms", "text": "Both subjective and objective methods have been used for assessing the results of SR algorithms. In the subjective method, usually human observers assess the quality of the produced image. Here the results of the SR algorithms are presented in the form of mean opinion scores and variance opinion score [498] . With objective methods, the results of SR algorithms are usually compared against the ground truth using measures like MSE and (peak) signal to noise ratio (PSNR). MSE is defined as:\nThe smaller the MSE, the closer the result is to the ground truth. PSNR is defined as:\nwhere RMSE is the square root of the MSE of Eq. (63). Though these two measures have been very often used by SR researchers, they do not represent the human visual system very well [220] . Therefore, other measures, such as the correlation coefficient (CC) and structural similarity measure (SSIM), have also been involved in SR algorithms [220, 255] . CC is defined by: [290, 296, 306, 356, 376, 590] where\nand \u03bcf are the mean of f andf , respectively. The maximum value of CC is one which means a perfect match between the reconstructed image and the ground truth [220] . The other measure, SSIM, is defined by:\nwhere C 1 and C 2 are constants, \u03c3 f and \u03c3f are the standard deviations of the associated images, and \u03c3 ff is defined by:\nTo use SSIM, the image is usually first divided into subimages, and then Eq. (67) is applied to every sub-image and the mean of all the values is used as the actual measure between the super-resolved image and the ground truth. A mean value of SSIM close to unity means a perfect match between the two images. It is discussed in [595] that SSIM favors blur over texture misalignment, and therefore may favor algorithms which do not provide enough texture details. Table 13 shows an overview over the objective measurements used in SR algorithms. Besides those reported in the table, the following objective measurements have been used in at least one paper:\n-Correlation coefficient in [220, 496] , -Feature similarity in [558] , -Triangle orientation discrimination [264, 284, 337, 446] -Point reproduction ratio (PRR) and mean segment reproduction ratio (MSRR) [498] : PRR measures the closeness of two sets of interest points (extracted by Harris, or SIFT): one generated from the super-resolved image and the other from the ground truth. MSRR works with similar sets but the interest points are replaced by the results of some segmentation algorithms. -Feature similarity in [558] using histogram of oriented gradients, -Blind image quality index and sharpness index [583] , -Expressiveness and predictive power of image patches for single-image based SR [595] , -Spectral angle mapper [614] for hyper-spectral imaging, and -Fourier ring correlation [512] .\nBesides these assessment measures, some authors have shown that feeding some real-world applications by images that are produced by some SR algorithms improves the performance of those applications to some degree, for example, in - [181, 319, 548] , the readability of the employed optical character recognition system has been improved. - [189, 190, 213, 298, 299, 322, 323, 339, 340, 350, 434, 454] , [470, 481, 511, 538, 557, 586, 600, 606, 608, 610, 619] , the recognition rates of the employed face recognition systems are increased. - [239] , better contrast ratios in the super-resolved PET images are obtained. - [423] , the recognition rate of a number plate reader has been improved. Table 14 shows the list of the most common 2D facial databases that have been used in SR algorithms. Table 15 shows more details about these databases. One feature which is interesting for SR databases is providing ground truth data for different resolutions (i.e., different distances from the camera). Among the most common databases reviewed (shown in Table 15 ), only CAS-PEAL facial database provides such a data. Besides the list shown in Table 14 , the following facial databases have been used in at least one system: CohnKanade face database [208] , NUST [323, 350] , BioID [395, 401] , Terrascope [299] , Asian Face Database PF01 [328] , FG-NET Database [342] , Korean Face Database [356] , Max Planck Institute Face Database [356] , IMM Face Database [383] , PAL [397, 456] , USC-SIPI [490, 504, 578] , Georgia Tech [408, 524] , ViDTIMIT [401] , FRI CVL [481] , Face96 [481] , FEI face database [572] , MBGC face and iris database [586] , SOFTPIA Japan Face Database [606] .\nThe following list shows some of the reported databases that SR algorithms have used in other applications than facial imaging: -Aerial imaging: forward looking infrared [79] , IKONOS satellite [196, 604] , Moffett Field and BearFruitGray for hyper-spectral imaging [175] , Landsat7 [346] . -MDSP dataset (a multi-purpose dataset): [155, 157, 227, 228, 455, 599 ]. -Medical imaging: PROPELLER MRI database [352] , Alzheimer's disease neuroimaging initiative (ADNI) database. [489] , DIR-lab 4D-CT dataset [615] , and 3D MRI of the brain web simulated database [489, 565, 566 ]. -Natural images: Berkeley segmentation database [394] . -Stereo pair database [476] .\nMoreover, there are many SR works that are tested on common images, like Lena (for single-image SR) and resolution chart (for single-and multiple-image SR)."}, {"section_title": "3D SR", "text": "HR 3D sensors are usually expensive, and the available 3D depth sensors like time of flight (TOF) cameras usually produce depth images of LR; therefore, SR is of great importance for working with depth images. Much research on 3D SR has been reported [10, 28, 33, 80, 135, 250, 347, 355, 460, 561, 579] . Most of these methods have used 2D observations to extract the depth information. However, in [200, 250, 296, 324, 325, 355, 389, 422, 579] 3D-3D SR algorithms have been developed to generate HR 3D images from LR:\n-3D inputs using a hallucination algorithm [355, 356] .\n-Stereo pair images [296] .\n-Images captured by a TOF camera using a MAP SR algorithm [389, 422] .\nTo do so, e.g., in [355] two planer representations have been used: Gaussian curvature image (GCI) and surface displacement image (SDI). GCI and SDI model the intrinsic geometric information of the LR input meshes and the relationship between these LR meshes and their HR counterparts, respectively. Then, the hallucination process obtains SDI from GCI, which has been done by RBF networks. In [356] , the result of a hallucination algorithm has been applied to a morphable face model to extract a frontal HR face image from the LR inputs.\nAnother group of 3D SR algorithms, a.k.a joint image upsampling, works with a LR depth map and a HR image to super-resolve a HR depth image, like in [297, 533, 574] . This technique has been well applied to aerial imaging, in which for example a HR panchromatic image and a LR multi-spectral image are used to produce a HR multi-spectral image. This method in aerial applications is known as pansharpening, like in [54,114,159,604]."}, {"section_title": "Speed of SR algorithms", "text": "SR algorithms usually suffer from high computational requirements, which obviously reduces their speed performances. For example, POCS algorithms may oscillate between the responses and get too slow to converge. Some MAP methods, if they use non-convex prior terms for their regularization, may require computationally heavy optimization methods, like simulated annealing. Some researchers have tried to develop fast SR systems by imposing some conditions on the input images of the system. For example, in - [87] , a ML-based method was developed which was fast but only when the motion between its LR inputs is pure translational. Similar approach of considering translational motion has been followed in [398] . - [124, 365, 480] , a ML method was developed for the case that SR problem is over-determined. In this case the there is no need for a regularization term and the huge matrix calculations can be replaced by shift and add operations. This is mostly the case for the direct SR algorithm in which different types of filters are utilized, for example in [290, 534] adaptive Wiener filter has been used (Sect. 5.1.3). - [131, 221] , preconditioning of the system has been used so that one can use a faster optimization method, like CG.\nIn some SR algorithms it has been tried to improve the speed of a specific step of the system. For example:\n-in [252, 277, 308, 334] faster registration algorithms are used. -In [217] parallel processing has been used. -Some of the learning-based approaches perform fast in their testing phase, but they need a time-consuming training step and are mostly application dependent, like in [130, 292, 304] ."}, {"section_title": "Conclusion", "text": "This survey paper reviews most of the papers published on the topic of super-resolution (up to 2012) and proposes a broad taxonomy for these works. Besides giving the details of most of the methods, it mentions the pros and cons of the methods when they have been available in the reviewed papers. Furthermore, it highlights the most common ways for dealing with problems like color information, the number of LR images, and the assessment of the algorithms developed. Finally, it determines the most commonly employed databases for the different applications. This survey paper has come to its end, but we have still not answered a very important question: what are the stateof-the-art super-resolution algorithms? The fact is that the answer to this question is highly dependent on the application. A super-resolution algorithm that is good for aerial imaging is not necessarily good for medical purposes or facial image processing. In different applications, different algorithms are leading. That is why there still are many recent publications for almost all types of the surveyed algorithms. This is mainly due to the different constraints that are imposed on the problem in different applications. Therefore, it seems difficult to compare super-resolution algorithms for different applications against each other. Generating a bench-mark database which can include all the concerns of the different fields of application seems very challenging. But generally speaking, comparing the frequency domain methods against the spatial domain methods, the former are more interesting from the theoretical point of view but have many problems when applied to real-world scenarios, e.g., they mostly have problems in the proper modeling of the motion in real-world applications. Spatial domain methods have better evolved for coping with such problems. Among these methods, the single-image based methods are more application dependent while the multiple-image based methods have been applied to more general applications. The multipleimage based methods are generally composed of two different steps: motion estimation, then fusion. These had and still have limited success because of their lack of robustness to motion error (not motion modeling). Most recently, implicit non-parametric methods (see the last paragraph of Sect. 5.1.3) have been developed that remove this sensitivity, and while they are slow and cannot produce huge improvement factors, they fail very gracefully and produce quite stable results at modest improvement factors. "}]