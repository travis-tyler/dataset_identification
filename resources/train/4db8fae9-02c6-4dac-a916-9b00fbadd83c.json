[{"section_title": "Abstract", "text": "This part contains additional numerical studies and technical proofs of the asymptotic theories in the manuscript. In addition, we describe locally quadratic approximation (LQA) in the model identification procedure. The standard deviation is given in parentheses. From Table 1 , we conclude that\nS1 Complementary Numerical Studies"}, {"section_title": "S1 Complementary Numerical Studies", "text": ""}, {"section_title": "S1.1 Example 1(Continued)", "text": "First, we fix (n, m) = (30, 20) , and compute three-step M-estimators under the loss functions \u03c11, \u03c12, and \u03c13, denoted as Ls, Lad, and Hub, respectively. For comparison, we also compute the spline-based oracle estimator of varying-coefficient component functions given all additive component functions in advance and, analogously, the oracle estimator of additive component functions when all varying-coefficient component functions are known. We denote the oracle estimator by the suffix -O, e.g., Ls-O denotes the oracle estimator under the quadratic loss function. Based on 500 Monte Carlo replications, Table 1 compares the average MSE (AMSE) of the three-step M-estimators with that of the oracle estimators under different error distributions.\nThe standard deviation is given in parentheses. From Table 1 , we conclude that\n\u2022 under the normal error distribution, Huber estimators are comparable to least-squares estimators, and median estimators are slightly inferior. Moreover, the AMSE of the threestep M-estimators is similar to that of the oracle estimators, even for medium sample sizes.\nThis embodies the oracle property of the three-step M-estimator, as if more information is known in advance.\n\u2022 under non-normal error distributions, least-squares estimators exhibit worse performance than the others, especially under the t(1) error distribution. The Huber and median estimators have similar performance, and their AMSEs are similar to those of the oracle estimators.\n\u2022 The influence of the intra-subject covariance structure is not substantial under the different error distributions and loss functions.\nUnder normal error distribution N (0, 0.2), Figure 1 presents the iterative least square estimator (dashed line) and 95% CLT-based CI (dotted lines) and 500 wild bootstrap sampling (dash-dotted lines). We note that the similar performance with Figure 1 (under mixed normal error distribution) in the manuscript, which indicates the rationality of our estimation method and two types CI.\nFor normal error distribution and mixed normal distribution, we also investigate the average experience coverage probability (AECP) of three-step M-estimator at given 50 grid poins on the range of interested variable. We sample 200 times with dependent within-subject correlations (\u03b8 = 0.5) and make 500 Monte Carlo replications in each run. which we see that the pointwise CI is well-performed even in the presence of small proportion outliers. In addition, for the mixed normal error distribution, Figure 4 investigates AECPs of component functions under more general sampling plan, i.e., sparse observations for some subjects and dense observations for other subjects. Specifically speaking, we generate 30 subjects, the first r% subjects with sparse observations (m1 = 10) and the last (1 \u2212 r)% subjects with dense observation (m2 = 30). Here, we take r = 1/2, 1/3 and 2/3. The result shows that the AECPs of component functions remain acceptable even under the mixture sampling plan and small proportion outliers. Note that he bivariate function g(t, x) = \u03b11(t)\u03b21(x) can be estimated by\u011d(t, x) =\u03b11(t)\u03b21(x), Figure 5 compares the estimated surfaces of g under different loss function with the true surface.\nObviously, huber estimator is nearest to the true surface, while least squares estimator is the worst, and median estimator is in-between.\nFinally, to investigate the asymptotic properties of three-step M-estimators, we take n = 20, 40 and m = 20, 30. For different combinations of (n, m) and two kinds of intra-subject covariance structure, Table 2 compares the AMSE of three-step M-estimators with different loss functions under normal error distribution and mixed normal error distribution, and Table   3 is the analogue for the 0.2 \u00d7 t(1) and 0.5 \u00d7 t(2) error distributions. Note that for each given pair (n, m), the performance of the three-step M-estimator is similar to that presented in Table - 1. Moreover, as the total number of observations grows, the AMSEs of three-step M-estimators decrease with normal error distribution, no matter which loss function is used. For non-normal error distribution, the estimators based upon robust loss functions \u03c12 and \u03c13 decrease, however, the least square estimators haven't significant improvements."}, {"section_title": "S1.2 Numerical Study of Model Identification Procedure", "text": "In this subsection, we will investigate the finite-sample performance of the proposed model identification procedure. A VCAM with repeated measurements is given by yij = \u03b10(tij) + \u03b11(tij)\u03b21(xij1) + 5\u03b22(xij2) + 3\u03b13(tij)xij3 + wi(tij) + eij, where tij, xij1, wi, \u03b10, \u03b11, and the random noise eij are the same as in Example 1 in the manuscript, xij2 = t 2 ij + \u03b6ij with \u03b6ij independently drawn from N (0, 0.5), and xij3 are indepen- \n, and\nBased on 200 Monte Carlo replications, we compare the performance of the proposed model identification procedure for independent and dependent intra-subject covariance structures and three kinds of loss function used in Example 1. Table 4 lists the percentages of correct fitting (C-F), over-fitting (O-F), and under-fitting (U-F) in the identification of additive terms (AT), varying-coefficient terms (VCT), and true model (TM) for normal error and mixed normal error distributions. The counterparts for the heavy-tail error distributions of 0.2 \u00d7 t(1) and 0.5 \u00d7 t (2) are given in Table 5 . From the obtained results, we notice that under normal error distribution, all of the percentage of correctly identified additive terms, varying-coefficient terms, and true models increases as the total number of observations increases, regardless of which loss function is adopted. In the case of small proportion outliers, the power of model identification increases as the number of observations nm increases if we use robust loss function \u03c12 and \u03c13. However, the least-square-based identification procedure exhibits very poor performance, and no significant improvement is obtained by increasing the total number of observations. It is expected since mean regression method is sensitive to outliers, which greatly influence the power of model identification. Also, the influence of the intra-subject covariance structure on the power of model identification is insignificant."}, {"section_title": "S1.3 Comparison with existing method", "text": "Note that the two-step spline estimation method proposed by Zhang and Wang (2015) is applicable when the covariates are dependent on subjects but independent of observation time.\nUnder this case, we compare the average MSE(AMSEs) and its standard deviation between the two types estimators based on 500 Monte Carlo replications. We consider normal error Table 5 : Model identification in Example 2 under 0.2 \u00d7 t(1) and 0.5 \u00d7 t(2) error distributions. Table 6 shows that our estimators are superior to the two-step estimators of Zhang and Wang (2015) for sparse data and a small proportion outliers, while for dense data with normal error distribution, the difference between them is insignificant."}, {"section_title": "S1.4 Example 2 (Continued)", "text": "For the CD4 dataset considered in the manuscript, Figure 6 presents the estimated surfaces the study period between 1984 and 1991. The time variable tij is the time (in years) of the j-th measurement of the i-th individual after HIV infection; the response Yij is the i-th individual's CD4 percent measured at time tij. Fan and Zhang (2000a); Zhou (2002, 2004) have analyzed this dataset using a VCM, which is a special case of our VCAM. They adopted three covariates: X1 the smoking status of individual after his infection; X2 the centered age at HIV infection, and X3 the centered pre-infection CD4 percent. Now, we compare a VCAM and a VCM containing the three covariates. Note that X1 is attribute variable and covariates are all time-invariant, we can write the two model as below:\nVCM : yij = \u03b10(tij) + \u03b11(tij)xi,1 + \u03b12(tij)xi,2 + \u03b13(tij)xi,3 + wij + eij, (S1.1) and\nFor the VCM (S1.1), Figure 7 gives the fitted curves (solid lines) of varying-coefficient functions and 95% CI (dash-dotted lines). For the VCAM (S1.2), we select the optimal number of interior knots (\u02c6 C,\u02c6 A,KC,KA) = (2, 2, 3, 6) and optimal tuning parameter (\u03bb1,\u03bb2) = (0.01, 0.01). Employing the model identification procedure, we found \u03b12 and \u03b13 are non-constant In addition, the bivariate time-varying covariates effects g1(t, x) = \u03b12(t)\u03b22(x) and g2(t, x) = \u03b13(t)\u03b23(x) in VCAM (S1.2), and g1(t, x) = \u03b12(t)x and g2(t, x) = \u03b13(t)x in VCM (S1.1) are estimated in Figure 9 , which implies the VCAM (S1.2) provides more detailed information of change points due to the nonlinear covariate effects.\nCompared with the VCM (S1.1) of Zhou (2002, 2004) , the residual sum of squares (RSS) of VCAM (S1.2) decreases by 3% and the multiple determination coefficient (R 2 ) increases by 12%. Therefore, VCAM (S1.2) is more suitable for this real-life data. Figure 10 gives the estimated bivariate functions g k (t,"}, {"section_title": "S1.5 Example 3 (Continued)", "text": "the ADNI data."}, {"section_title": "S1.6 Cigarette Data", "text": "Example 3. We continue the analysis of cigarette data referred in Section 1 in the manuscript. \nFigure 9: The estimated surfaces of time-varying covariates effects g 1 (t, x) = \u03b1 2 (t)\u03b2 2 (x) and g 2 (t, x) = \u03b1 3 (t)\u03b2 3 (x) in VCAM (S1.2), whilst g 1 (t, x) = \u03b1 2 (t)x and g 2 (t, x) = \u03b1 3 (t)x in VCM (S1.1). We use the following VCAM yit = \u03b10(t) + 2 k=1 \u03b1 k (t)\u03b2(x itk ), i = 1, ..., 46; t = 1, ..., 30.\nUnder huber loss function given in Example 1, we obtain that the optimal knots in Step I estimation are (\u02c6 C,\u02c6 A) = (2, 2), and smoothing tuning parameters in model identification procedure are (\u03bb1,\u03bb2) = (6.31, 0.01), We then obtain the penalized estimators and conclude both \u03b11 and \u03b12 are time-invariant. In a word, a more parsimonious model is given by yit = \u03b10(t) + \u03b21(xit1) + \u03b22(xit2), i = 1, ..., 46; t = 1, ..., 30. (S1.\n3)\nThe estimated component functions are given in Figure 11 , from which we see that:\n\u2022 \u03b10 decreases before 1980, and ascends until 1990, and then decreases;\n\u2022 Cigarettes consumption decreases as the Cigarettes price increases;\n\u2022 Cigarettes consumption increases as the income grows until X2 is larger than 4.8. 1960 1965 1970 1975 1980 1985 1990 1995 "}, {"section_title": "S2 Proofs", "text": "We start with the properties of B-spline basis. Let {b1, ..., bL} be standardized version of B-spline basis defined on [a, b] . Then, it holds that:"}, {"section_title": "S2.1 A Proposition", "text": "To prove the main results, we start with the following proposition, which gives the convergence rates of the initial estimators of varying-coefficient component functions. Let h = A \u2227 C andh = A \u2228 C be the minimum and maximum of A and C, respectively, and\n\u22121 is the harmonic average of sequence {ni}.\nwe obtain the convergence rates\nin the L2 norm sense, and\nin the MSE sense. h / nNH ; andNH/n 1 r \u2192 \u221e and\n, that is, the asymptotic variance has a parametric rate.\nThus, we can split data as sparse or dense according to whetherNH/n\nwhere 0 < C \u2264 \u221e. The result is slightly different from Remark 5 in the manuscript since we now estimate bivariate nonparametric function and require larger sample size.\nAccording to Corollary 6.21 and Theorem 12.7 of Schumaker (1981) and Assumption (A3), there exists positive constants D0, ..., Dp, such that \u03b10(t) =\u03b10(t) + R0(t) =\u03b3"}, {"section_title": "|R0(t)| \u2264 D0", "text": "\u2212r C and sup\nwhere\nn \u03c0ij, where\n. Then, the minimizing problem (2.3) can be rewritten as\nDenote \u0393n(\u03b6) be the objective function above, \u03a6n(\u03b6) = E[\u0393n(\u03b6)|J ], and \u2206n(\u03b6) = \u0393n(\u03b6) \u2212\nThe following lemmas are useful to prove Proposition 1.\nLemma 1. Under Assumption A1 and A2, there exists positive constants L1 and L2, it holds that L1I \u2264 S 2 n /n \u2264 L2I, except on an event whose probability tends to zero, where I is l-order identity matrix, with l = JC + pJCJA, JC = q + C, and JA = q + A \u2212 1.\n\u2208 G, define the theoretical inner product and empirical inner product are\nThe induced theoretical norm and empirical norm are denoted as g and g n , respectively.\nFor any g = \u03b3 \u03c4 \u03c0(t, x) \u2208 G, according to Assumption (A1), (A2) and Lemma 1 of Stone (1985), we have\nwhere d1 is some positive constant.\nOn the other hand, there exists a positive constant d2 (> d1) such that\nBy Assumption (A2) and (S2.1), we obtain that\nAlong the line of Lemma A.2 of Huang, Wu and Zhou (2004), we can show g 2 n g 2 for any g \u2208 G. Therefore,\nSimilar to the procedure of Lemma 3.2 of He and Shi (1994) , we have the next lemma. , it holds that sup \u03b6 \u2264L |\u03ba \u22121 \u2206n(\u03ba 1/2 \u03b6)| = op(1), where\nLemma 3. Under the assumptions of Proposition 1,\nProof. We will show (S2.5) for convex loss function and non-convex loss function, respectively.\nAssume the convex loss function \u03c1(\u00b7) satisfies conditions A5, M1 and M2. Notice that\nwhich implies maxi,j(|Rij| + \u03ba 1/2 |\u03c0(tij) \u03c4 \u03b6|) = op(1). In combination with Lemma 2 and (S2.4), we can show (S2.5) along the lines of Theorem 1 of Tang and Cheng (2008) .\nFor the non-convex loss function \u03c1(\u00b7), we assume that conditions A5, N1 and N2 hold.\nNotice that\nwhere\n\u03c6 (\u03b5ij)udu, and\nSimilar to the proof of Theorem 1 of Tang and Cheng (2008), we can show\nby Assumption A4. After direct computations, we obtain that\nNote that\nand\nfrom Assumption A5 and (S2.3). According to Assumption N2, we derive that\nIn combination with (S2.6), (S2.7) and (S2.8), we have that\nwhich yields (S2.5).\nProof of Proposition 1.\nProof. From Lemma 3, we have \u03b6 = Op(\u03ba 1/2 ), which implies (1) from Lemma 1. Employing approximation theories of spline functions and (S2.1), we have\nThen, Cauthy-Schwartz inequality means\nwhere\nThe derivation of\n) is straightforward, and omitted the details. Finally, we show the convergence rate in the mean squared error sense. Similar to Lemma 1, we can show the largest eigenvalues of\nare bounded, which to yield\nAgain from (S2.9), we obtain that\nwhich completes the proof."}, {"section_title": "S2.2 Proof of Theorem 1", "text": "Under Assumption (A3), there exists positive constants c k,A (k = 1, ..., p) such that\nfrom Proposition 1 and Assumption (A3).\n. Then, the minimizing problem (2.5) can be rewritten as\nDenote \u0393n,A(\u03d1) be the objective function above,\nand empirical inner product\nDenote the induced theoretical norm and empirical norm as g A and g n,A , respectively.\nThen, for any g \u2208 GA t ,\nFurthermore, under Assumption (A2), we can show g 2 n,A g 2 A for any g \u2208 GA t , which\nAlong the lines of Lemma 3.2 of He and Shi (1994) , we can derive the following lemma. , it holds that sup \u03d1 \u2264L K \u22121 A \u2206n,A K 1/2 A \u03d1 = op(1).\nLemma 6. Under Assumptions A1-A5, M1-M2 or N1-N2, it follows that\nProof. It can be shown for the convex and non-convex loss function, respectively.\n\u2022 For the convex loss function \u03c1(\u00b7), assume that the conditions A5, M1 and M2 hold.\nFrom Proposition 1, we have that\nSimilar to the proof of Theorem 1 of Tang and Cheng (2008) , we can derive that\nfrom Assumption M1 and (S2.13). Furthermore, noticing that\nby Proposition 1, we obtain that\nOn the other hand,\nHence, from (S2.15), Lemma 5 and the convexity of \u03c1, we can show (S2.12).\n\u2022 For the non-convex loss function \u03c1(\u00b7), assume that the conditions A5, N1 and N2 hold.\nNote that (1)),\nFrom Proposition 1, we have that\nSimilar to the proof of Proposition 1, we can show that\nand I2 > c 2\n\u03d1 . Therefore, for any sufficient large L, (S2.12)\nholds."}, {"section_title": "Proof of Theorem 1", "text": "Proof. By Lemma 6, we have that \u03b8 = O(K 1/2 A ). Furthermore, Lemma 4 gives\nEmploying Cauchy-Schwartz inequality, we have\n. It is sufficient to deal with I1.\nUnder Assumption A2, we can show the largest eigenvalue of\nis bounded, which leads\nHence the rate of convergence in the sense of MSE.\nNext, we show L2 convergence rate of M-estimators of \u03b2 k . Again by Cauchy-Schwartz inequality,\nIt is sufficient to deal with\n. From (S2.1), we get\nA )."}, {"section_title": "S2.3 Proof of Theorem 2", "text": "Under Assumption (A3), there exists positive constants d k,C (k = 0, ..., p), such that\nby Theorem 1 and Assumption A3.\nThe minimizing problem (2.7) can be equivalently written as\nDenote \u0393n,C(\u03c2) be the objective function above, \u03a6n,C(\u03c2) = E \u0393n,C(\u03c2)|J , and \u2206n,C(\u03c2) =\nSimilar to Lemma 4-6, we have the following lemmas.\nLemma 7. Under Assumption A1 and A3, except on an event whose probability tends to zero, the eigenvalues of S 2 n,C /n has positive lower bound L1,C and upper bound L2,C. , it holds that sup \u03d1 \u2264L K \u22121 C \u2206n,C K 1/2 C \u03d1 = op(1), wher\u1ebd\nProof of Theorem 2.\nProof. In the same vein of Theorem 1, we can show that, for sufficiently large L,\nThen, by the identification condition 1 0 \u03b1 k (t)dt = 1 and triangular inequality, we have\nIt is not difficult to show the largest eigenvalue of\nIt is easy to derive that W1 = Op(KC/n + K"}, {"section_title": "\u22122r C", "text": "). Again by (S2.16), we can complete the proof."}, {"section_title": "S2.4 Proof of Theorem 3 and 4", "text": "Proof. LetWn,A = S \nfrom Lemma 4. Therefore,\nEmploying Proposition 1 and the conditions of Theorem 3, we get\nHence, for any vector h, it follows that (\u03bdi \u2212 \u03bdi) \u03c4 (\u011ci \u2212 Gi)\u03bdi.\nOn the one hand, it holds that \u03bdi = 1 n A k (x) \u00b7 (Wn,A/n) \u22121 \u00b7 \u03a8i = O( \u221a niKA/n).\nMoreover, we can write\nIt is easy to see that (1 + op(1)).\nIt is routine to show\n(1 + op (1)). Furthermore, the square of the first term in the last inequality can be bounded by\nTherefore,\nwhich completes the proof."}, {"section_title": "S3 Algorithm", "text": "We now formulate the algorithm for optimization problem (4.1) in the main text. Following the LQA procedure introduced by Fan and Li (2001) , we have ..,\nWe can then approximate Q(\u03b7) in (4.1), up to a constant, as Q(\u03b7; \u03bb1, \u03bb2) \u2248 (Y \u2212 Z\u03b7) \u03c4 W (Y \u2212 Z\u03b7) + 1 2 n\u03b7 \u03c4 (\u21261 + \u21262)\u03b7, which implies that the minimizer of (4.1) can be derived by iteratively computing the estimator"}]