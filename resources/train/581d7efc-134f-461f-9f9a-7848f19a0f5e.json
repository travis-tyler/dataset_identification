[{"section_title": "Abstract", "text": "Persistent homology has been applied to brain network analysis for finding the shape of brain networks across multiple thresholds. In the persistent homology, the shape of networks is often quantified by the sequence of k-dimensional holes and Betti numbers. The Betti numbers are more widely used than holes themselves in topological brain network analysis. However, the holes show the local connectivity of networks, and they can be very informative features in analysis. In this study, we propose a new method of measuring network differences based on the dissimilarity measure of harmonic holes (HHs). The HHs, which represent the substructure of brain networks, are extracted by the Hodge Laplacian of brain networks. We also find the most contributed HHs to the network difference based on the HH dissimilarity. We applied our proposed method to clustering the networks of 4 groups, normal control (NC), stable and progressive mild cognitive impairment (sMCI and pMCI), and Alzheimer's disease (AD).\nThe results showed that the clustering performance of the proposed method was better than that of network distances based on only the global change of topology."}, {"section_title": "", "text": "complex' is not a familiar term in brain network analysis, we refer to it as a 'network' that is generally used. It quantifies the shape of brain networks by using k-dimensional holes and their cardinality, the kth Betti number [5, 6] .\nA persistence diagram (PD) summarizes the change of Betti numbers during the filtration of networks by recording when and how holes appear and disappear during the filtration. The persistent homology also provides distances for distinguishing networks such as the bottleneck distance and kernel-based distances [6, 7] . Such distances mostly find network differences in their PDs. The Betti numbers and PDs are more often used than holes themselves in network applications.\nHoles represent the submodule of brain networks. 0-dimensional holes, i.e., connected components, modules or clusters have been widely studied for finding functional or structural submodules in a brain [8, 2, 9] . On the other hand, 1-dimensional holes have been rarely used for brain network analysis [10, 11, 12, 13, 14, 15] . Most studies in brain network analysis do not use 2-and higher order simplexes in networks since networks. Therefore, all cycles in a network are considered as 1-dimensional holes. There are few network measures based on cycles in brain network analysis such as cycle probability and the change of the number of cycles during graph filtration [10, 15] . These measures helped to compare the global property of networks but could not find the discriminative substructures of networks.\nIf higher order simplexes are introduced in a network, the number of 1-dimensional holes is significantly reduced due to the removal of filled-in triangles.\nThe previous brain network studies that studied higher order simplexes mostly found holes based on Zomorodian and Carlsson's (ZC) algorithm [13, 14, 16] .\nThe ZC algorithm is very fast in linear-time, however, it finds the sparse representation of a hole that identifies only one path around the hole and ignores the other paths. This introduces an ambiguity in hole identification in practice.\nA better approach would be to localize the holes by the eigen-decomposition of Hodge Laplacian of a network. Such holes are called as the harmonic holes (HHs). The HH shows all possible paths around the hole with their weights [17, 18, 19] . The HHs have been applied to brain network analysis for localizing persistent holes [12, 11] . The 1-dimensional holes in a network with higher order simplexes have at least one indirect path between every two nodes. Thus, the holes are related to the abnormality or inefficiency of the network. The previous studies found the persistent holes with long duration in a network as abnormal holes, and localized them by harmonic holes. Therefore, the duration of holes was used instead of HHs in network discrimination.\nIn this paper, we propose a new measure for estimating network dissimilarity based on persistent HHs (HH dissimilarity). The proposed HH dissimilarity is motivated from the bottleneck distance. The bottleneck distance first estimates the correspondence between holes between networks that are represented by points in PDs, and then chooses the maximum among all the distances between the estimated pairs of holes [20] . The HH dissimilarity also estimates the correspondence between HHs of two different networks that are represented by real-valued eigenvectors, and takes the averaged dissimilarities of the estimated pairs of HHs. The advantage of HH dissimilarity is not only to measure the network differences but also to quantify a HH's contributions to the network differences. We will call the amount of HH's of contribution the citation of HH.\nThis allows us to identify the discriminative subnetworks of networks.\nThe proposed method is applied to metabolic brain networks obtained from the FDG PET dataset in Alzheimer's disease neuroimaging initiative (ADNI).\nThe dataset consists of 4 groups: normal controls (NC), stable and progressive mild cognitive impairment (sMCI and pMCI), and Alzheimer's disease (AD).\nWe generated 2400 networks by bootstrap, and compared the clustering performance with the existing network distances such as L 2 -norm (L2) of the difference between distance matrices, Gromov-Hausdorff (GH) distance, KolmogorovSmirnov (KS) distance of connected components and cycles (KS 0 and KS 1 ), and bottleneck distance of holes [21, 8, 10, 20, 2] . The results showed that the HH dissimilarity had the superior clustering performance than the other distance measures, and comparing local connectivities could be more helpful to discriminating the progression of Alzheimer's disease. [24] . The diffusion distance is known to be more robust to noise and outliers."}, {"section_title": "Harmonic holes 2.2.1. Simplicial complex", "text": "The algebraic topology extends the concept of a graph further to a simplicial complex. Suppose that a non-empty node set V is given. If the set of all subsets of V is denoted by 2 V , an abstract simplicial complex K is a subset of 2 V such that (1) H P K, and (2) if \u03c3 P K and \u03c4 P \u03c3, \u03c4 P K [6, 25] .\nThe number of simplices in K i is denoted as |K i |.\na graph with nodes and edges is 1-skeleton K p1q . In this paper, we will only consider 2-skeleton K p2q of a simplicial complex that includes nodes, edges, and triangles. For convenience, we call it a (simplicial) network [26] ."}, {"section_title": "Incidence matrix", "text": "We denote a |K i |-dimensional integer space as Z |Ki| . Given a finite simplicial complex K, a chain complex C i is defined in Z |Ki| [6, 16] . The boundary operator B i and coboundary operator B J i for i \" 1, . . . , N pN \u0105 0q are functions such that\nGiven \u03c3 i \" rv 1 , ..., v i`1 s P C i , the boundary of \u03c3 i is algebraically defined as\nIf the sign of \u03c3 i\u00b41 in B i \u03c3 i is positive/negative, it is called positively/negatively oriented with respect to \u03c3 i . We denote the positive/negative orientation by \u03c3 i\u00b41 P`{\u00b4\u03c3 i . The boundary of the boundary is always zero, i.e., B i\u00b41 B i \" 0.\nIf the simplicial complex K has\n) , the boundary operator B i is represented by the ith incidence matrix M i P Z |Ki\u00b41|\u02c6|Ki| such that [17, 18, 19] \nThe coboundary operator B "}, {"section_title": "Combinatorial Hodge Laplacian", "text": "where [17, 18, 26, 19] The kernel and image of L i are denoted by kerL i and imgL i , respectively. The kerL i is called harmonic classes H i [26] .\nThe ith homology and cohomology groups of C \" tC i , B i u are defined respectively byH\nTheorem 2.1 (Combinatorial Hodge Theory [17, 26, 19] ). Suppose that a chain complex tC i pX; Rq, B i u is given for i \" 0, . . . , N , and C i pX; Rq is considered as an R-vector space. Harmonic classes H i obtained by the combinatorial Laplacian L i are congruent to the ith homology and cohomology groups,H i andH i of C, i.e.,\nProof. rankH i \" rankC i\u00b4r ankL i \" rankC i\u00b4p rankB i`r ankB i`1 q \" rankH i pC; Rq.\nThe harmonic classes H i \" kerL k is also called a harmonic space [26] . The homology groupH i in persistent homology can be replaced with a harmonic space H i , and the rank of H i is the same as the ith Betti number. We call a hole in H i a harmonic hole (HH), and a hole inH i estimated by Smith normal form a binary hole [16] .\nGiven a simplicial network with p nodes, q edges, and r filled-in triangles,\nwe estimate L 1 P Z q\u02c6q in (2), and\nof L 1 with zero eigenvalue, x P R q\u02c61 represents a HH. The entry of x can be positive or negative depending on the orientation of edges in the hole. The absolute value of the entry of x represents the weight of the corresponding edge in the hole. Since x and\u00b4x have zero eigenvalue, they represent the same hole, and x \" 1."}, {"section_title": "Computing persistent HHs", "text": "In this study, we have the distances between pairs of nodes in a brain net- In this study, we will estimate the youngest persistent HHs just like the ZC algorithm. First, we sort edges e 1 , . . . , e q in the ascending order of an edge distance, and perform the Rips filtration by the fast ZC algorithm. To avoid having the same edge distance, we select the ordered index 1, . . . , q as the filtration value, instead of the edge distance. The reason for performing the ZC algorithm first is that the computation of eigen-decomposition at every filtration value is too expensive. Then, we obtain a PD which is the set of the birth and death thresholds of persistent holes. If a persistent hole appears at i X and disappears at i Z , we perform the eigen-decomposition of Hodge Laplacian at i X , i Z , and i Y \" i Z\u00b41 to estimate the corresponding HH. The i Y is the threshold just before the death of the persistent hole.\nThe harmonic spaces at i X , i Y , and i Z are written by matrices\nrespectively. The HH appearing at i X and disappearing at i Z will be in H X and H Y , but not in H Z . We find which y P H Y does not depend on z i 's in H Z .\nIf y P H Y depends on H Z , the smallest singular value of the matrix rH Z , ys is close to 0. It implies that y still exists in H Z . Therefore, we choose y P H Y such that y \" arg max\nThe chosen y by (3) is the oldest persistent HH. Next, we choose the youngest persistent HH x P H X such that\nx \" arg min xPH X tthe smallest singular value of rx, ysu \" arg min\nThis procedure is repeated for all persistent holes. The incidence matrices are already estimated during the ZC algorithm. Since the incidence matrices and their combinatorial Hodge Laplacian are very sparse, the computation of persistent HHs is not so hard in our experiments. In our experiments, the total number of persistent holes during the filtration is not more than 50, and the number of persistent holes at each filtration value is not more than 20."}, {"section_title": "HH dissimilarity 2.3.1. Bottleneck distance", "text": "If K a and K b have m and n persistent holes. The PDs of K a and K b are denoted respectively by P D a \" tt \nwhere \u03b7 is a bijection from P D a to P D b and px, yq 8 \" max t|x|, |y|u is the L 8\u00b4n orm. If there is no corresponding hole in the other PD because of m \u2030 n, the points on the diagonal line x \" y that have the shortest distance from the point t are included. In this way, the bottleneck distance measures network distance by the difference of the birth and death thresholds of holes, not by the difference between holes themselves."}, {"section_title": "Dissimilarity between HHs", "text": "If the eigenvectors with zero eigenvalues of two different combinatorial Laplacians are denoted by x and y, their dissimilarity is defined by one minus the absolute value of their inner product, i.e.,\nThis is the smallest singular value of the matrix rx, ys in (4) that shows the dependency between x and y. If x and y are similar, their dissimilarity is close to 0; otherwise, it is close to 1."}, {"section_title": "HH dissimilarity", "text": "Suppose that two networks K a and K b have m and n persistent HHs, denoted by H a \" rx \nwhere \u03b6 is a bijection from H a to H b .\nThe correspondence \u03b6 between persistent HHs in two different networks is determined by minimizing the total distances between the pairs of HHs based on Munkres assignment algorithm, also known as Hungarian algorithm. Some of persistent HHs can not find their corresponding HHs in the other network because of m \u2030 n. In this study, we ignore them and average the dissimilarities of the obtained pairs of persistent HHs."}, {"section_title": "Citation of HH", "text": "The advantage of using HH dissimilarity is the ability to quantify how much a persistent HH contributes in differentiating networks. The degree of the contribution of HH is called the citation of HH. If a persistent HH x in H a corresponds to a persistent HH y \" \u03b6pxq in H b in (6), their dissimilarity is d h px, yq \" 1\u00b4|x J y|, and their similarity is defined by |x J y|. If the persistent HHs of l networks are denoted by H \" tH 1 ,\u00a8\u00a8\u00a8, H l u and they are compared with H a , the citation of x is defined by\nIf we find the most cited HHs by comparing networks within a group, we can determine which submodule makes two networks in a group close to each other.\nFurthermore, if we find the most cited HHs by comparing network between groups, we can determine which submodule makes differences."}, {"section_title": "Results", "text": ""}, {"section_title": "Brain network construction", "text": "We had 4 groups, NC, sMCI, pMCI, and AD which had 181, 91, 77 and 135 subjects, respectively. The subjects in a group could be heterogeneous. Thus, we obtained 600 bootstrap samples from each group by randomly selecting the subset of the number of subjects in each group with replacement [27] . The number of bootstrap samples was heuristically determined in comparison with previous study [27] . We constructed 600 bootstrapped networks from bootstrap samples in each group by diffusion distance in Sec. 2.1. The total number of generated brain networks was 2400."}, {"section_title": "Network clustering", "text": "We clustered 2400 bootstrapped brain networks into 4 groups by Ward's hierarchical clustering method. The Ward's hierarchical clustering method found Table 1 .\nthe group labels based on the distance between data points, which is a network in our application. The network distance was estimated by (a) L2, (b)\nGH distance, (c) KS 0 , (d) KS 1 , (e) bottleneck distance of holes, and (f) HH dissimilarity [21, 8, 10, 20, 2] . The obtained distance matrices of 2400 networks were shown in Fig. 1 . After clustering networks, we matched the estimated group label with the true group label of networks and calculated the clustering accuracy of 8 distance matrices. The clustering accuracy of 8 distance matrices was shown in Table 1 . We also clustered 1200 bootstrapped networks in sMCI and pMCI into 2 groups by the same way. The clustering accuracy was shown in Table 1 ."}, {"section_title": "The most cited HHs", "text": "We selected the 600 most cited HHs within NC, sMCI, pMCI, and AD, and divided them into 5 clusters based on the dissimilarity between HHs in (5). In were shown on the left of Fig. 3 (a-d) . In each panel, the upper row showed the HHs in a brain, and the lower row showed the HHs in a 2-dimensional plane. The location of nodes in the 2-dimensional plane was estimated by Kamada-Kawai algorithm implemented in a network analysis/visualization toolbox, Pajek [28] .\nIn Fig. 3 (a-d) , the width of an edge was proportional to the edge weight in the HH. The larger the weight of an edge, the darker the color of an edge. The color of nodes represented the location of nodes in a brain. If a node was located in frontal, parietal, temporal, occipital, subcortical, and limbic regions, the color of the node was red, blue, green, purple, yellow, and orange, respectively.\nWe also selected the 600 most cited HHs when we compared networks between sMCI and pMCI, and divided them into 5 clusters. In Fig. 2 (a) , the cluster 5 contained the outliers. Thus, we estimated the center HHs in cluster 1-4. The representative HHs in sMCI and the corresponding holes in pMCI were shown in Fig. 2 (b) ."}, {"section_title": "Discussion and conclusions", "text": "In this study, we proposed a new network dissimilarity, called HH dissimilarity. Unlike a binary hole estimated by the ZC algorithm, a HH show all possible paths of edges around a hole, and the contribution of paths to forming the hole is represented by the weight of edges on the paths. If an edge belongs to a unique path that forms a hole, its edge weight will be large. If an edge belongs to one of many alternative paths as in a module, its edge weight will be small. In this way, HHs can extract the substructures of a brain network including holes and modules. Moreover, since the HHs can be represented as real-valued orthonormal vectors we can define the dissimilarity between HHs as well as HH dissimilarity between brain networks easily using vector product.\nBrain networks of different groups may share common substructure as well as have different substructures that make individual and group differences. The proposed HH dissimilarity first finds candidates of common substructures between brain networks and estimates the over all dissimilarities between candidates. The clustering results showed that brain networks of different groups had similar substructures, however, the averaged similarities was much larger than that of brain networks within a group.\nThe goal of persistent homology may be to find persistent features that last for a long duration. However, in brain network analysis, it has been applied for finding the change of topology, especially the change of connected components, instead of the persistence of topology. This study suggested a more coherent framework to observe, capture, and quantify the change of holes in brain networks. Depending on imaging modality and study populations, brain networks may have different characteristics of shapes. Therefore, it is necessary to apply proper network measures to brain networks depending on modality and population. The results showed that when the Alzheimer's disease progresses, the hole structure was changed in metabolic brain networks, and HHs and HH dissimilarity could predict the disease progression. "}]