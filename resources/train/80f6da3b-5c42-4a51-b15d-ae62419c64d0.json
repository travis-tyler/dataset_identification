[{"section_title": "Abstract", "text": "Abstract-The wisdom of crowds (WOCs), as a theory in the social science, gets a new paradigm in computer science. The WOC theory explains that the aggregate decision made by a group is often better than those of its individual members if specific conditions are satisfied. This paper presents a novel framework for unsupervised and semisupervised cluster ensemble by exploiting the WOC theory. We employ four conditions in the WOC theory, i.e., diversity, independency, decentralization, and aggregation, to guide both constructing of individual clustering results and final combination for clustering ensemble. First, independency criterion, as a novel mapping system on the raw data set, removes the correlation between features on our proposed method. Then, decentralization as a novel mechanism generates high quality individual clustering results. Next, uniformity as a new diversity metric evaluates the generated clustering results. Further, weighted evidence accumulation clustering method is proposed for the final aggregation without using thresholding procedure. Experimental study on varied data sets demonstrates that the proposed approach achieves superior performance to state-of-the-art methods.\nIndex Terms-Cluster ensemble, pairwise constraints, semisupervised clustering, the wisdom of crowds (WOCs)."}, {"section_title": "I. INTRODUCTION", "text": "C LUSTERING, the art of discovering meaningful patterns in the unlabeled data sets, is one of the main tasks in machine learning. Semisupervised clustering is a branch of clustering methods that uses prior supervision information, such as labeled data, known data associations, or pairwise constraints, to aid the clustering process. This paper focuses on pairwise constraints, i.e., pairs of instances known as belonging to the same cluster (must-link constraints) or different clusters (cannot-link constraints). Pairwise constraints arise naturally in many real tasks and have been widely used in semisupervised clustering. There is a wide range of issues in the clustering methods. For instance, individual clustering algorithms provide different accuracies in a complex data set because they generate the clustering results by optimizing a local or global function instead of natural relations between data points [1] - [4] . As another example, pairwise constraints often result in highly unstable clustering performance, whereas they have the potential to improve clustering accuracy in practice [5] , [6] .\nAs a novel solution, cluster ensemble was proposed for achieving a robust and stable final result by combining the different individual clustering results [1] . Cluster ensemble selection (CES) is a new approach which combines a subgroup of individual clustering results. It uses a consensus metric(s) for evaluating and selecting the ensemble committee in order to improve the accuracy of final results [7] . Generally, CES contains four components; i.e., generation, evaluation, selection, and combination. First, individual clustering results are generated by using different kinds of clustering algorithms or repeating some algorithms, which can generate random results in each runtime such as the k-means. Next, a consensus metric(s) such as normalized mutual information (NMI) is employed to evaluate the generated results. After that, the evaluated results are selected by thresholding procedure. Lastly, the final clustering result is obtained by an aggregation mechanism [7] - [11] .\nThere are three challenges in the CES arena; i.e., strategy of generation, metric(s) of evaluation, thresholding procedure. As the first challenge, the strategy of generating the individual clustering results can dramatically affect the performance of CES [12] - [16] . There are generally two paradigms, i.e., some of these studies [7] , [9] , [13] , [17] separately run each component of the CES (generate all individual results, then evaluate them, etc.) whereas the rest of studies [12] , [18] employed feedback mechanism, which gradually runs each component of the CES (generating the first individual result, then evaluating it, etc.). On the one hand, feedback mechanism uses evaluated the results at each step for improving the quality of the generated results in the next steps. Therefore, it can usually provide better performance in comparison with the first paradigm [12] , [18] . On the other hand, it may not be compatible with many of classical structures/metrics in the ensemble learning. Evaluation is the next challenge. NMI is one of the most prevalent diversity metrics that is used in the CES because: 1) NMI is not sensitive to the cluster's indices [18] ; 2) it can be easily implemented [7] , [8] ; and 3) it has better time complexity in comparison with other classic methods [7] , [9] , [17] , [19] . The main disadvantage 2168-2267 c 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\nof NMI is that the symmetric problem. Indeed, it cannot provide an efficient evaluation while the numbers of instances in distinct clusters are highly different. For instance, consider a clustering analysis for partitioning emails to normal or spam groups, where the number of instances in the normal group is significantly greater than the number of data points in the spams group. Alizadeh et al. [9] , [17] , [18] proved that the NMI evaluates the similarity between these two clusters equal to 1, while the real similarity is near to zero. This issue can rapidly decrease the performance of the NMI-based CES methods in the big data analysis [9] , [12] , [17] , [18] . Recently, some of the studies proposed a modified version of the NMI such as APMM 1 [9] and maximum (MAX) [17] for solving this problem. Their proposed methods were utilized for evaluating diversity between a cluster and a partition. Since using mentioned methods for evaluating two partitions increases the time complexity, it is critical to propose a new metric, which directly can evaluate diversity between two partitions. The next challenge in the CES is thresholding. In practice, it is so hard to find optimum values of thresholds; and the performance of the CES significantly depends on the threshold values [12] . Most of the ensemble methods (especially in the CES) employs the (majority) voting systems [7] , [8] , [12] , [18] , such as boosting and error-correcting output codes in supervised learning [20] or evidence accumulation clustering (EAC) method in unsupervised learning [19] . Indeed, CES framework just provides a voting system for selecting the robust and stable individual results. Voting systems are first defined in the term of social science, where it is used for providing democratic societies, fair trials (in the courts), etc. [21] . There is a wide range of theories in social science, which can provide an environment for applying an effective voting system. They can be used to inspire new algorithms in machine learning. The wisdom of crowds (WOCs) is one of these theories, which explain a robust approach for generating accurate results in a voting system. It simply claims that decisions made by aggregating the information of groups are better than those made by any single group member if the four specific conditions of this theory are satisfied; i.e., diversity, independency, decentralization, and aggregation [18] , [21] . Indeed, we can find many modern concepts in different sciences, which used WOC as a fundamental resource, e.g., Delphi method in management [21] , crowdsourcing/funding in the market [21] , crowd computing [22] in computer networks, etc. In computer science, this theory was used for optimizing resources in wireless sensor networks [22] . Further, there is a wide range of studies in supervised learning [23] - [28] and unsupervised learning [12] , [18] , which use the WOC theory for proposing new approaches. These studies validated that the WOC theory usually leads to better performance and higher stability.\nFor solving the three mentioned problems in CES, this paper shows that the WOC theory well matches the target of cluster ensemble, and thus its four conditions can be employed to guide the designing of individual clusterings as well as the final ensemble. Based on this observation, we propose a robust framework, which is called wisdom of crowds 1 Alizadeh-Parvin-Moshki-Minaei.\ncluster ensemble (WoCE), for both unsupervised and semisupervised cluster ensemble. Our contribution in this paper can be summarized as follows.\n1) First, a new mapping between the WOC observations and the CES problems. Furthermore, a general framework is proposed based on WOC theory for generating diverse individual results and using the feedback mechanism to select individual clusterings with high independency and quality. This framework is the first WOC-based approach for semisupervised clustering. 2) After that, this paper introduces a novel technique in the term of mathematical independent random variables for mapping the data to new dimensions based on the natural correlation of raw data, which can satisfy the independency criterion in the WOC. This mapping can generate independent features, which increase the performance of individual clustering algorithms. 3) Then, to satisfy the decentralization criterion in WOC, this paper uses different numbers of clusters in the different kinds of clustering algorithms, which can effectively generate high quality individual clustering results. Moreover, this paper develops a new method for selecting features based on supervision information in the semisupervised approach. 4) Next, to satisfy the diversity criterion in the WOC, this paper proposes a new diversity metric called uniformity, which is based on the APMM criterion, for evaluating the diversity of two partition, directly [9] . 5) Lastly, to satisfy the aggregation mechanism in WOC, this paper proposed weighted EAC (WEAC) to obtain the final clustering with a weighted combination of all individual results. While the weight of each individual result in WEAC can be estimated with different metrics, the uniformity was used in this paper. The rest of this paper is organized as follows. In Section II, this paper first briefly reviews some related works. Then it introduces the proposed WoCE framework in Section III. Experimental results are presented in Section IV; and finally, this paper presents the conclusion and point out some future works in Section V."}, {"section_title": "II. RELATED WORKS", "text": ""}, {"section_title": "A. Wisdom of Crowds", "text": "Francis Galton was a British scientist, who introduced the correlation concept in statistics. In 1906, he went to annual West of England Fat Stock and Poultry Exhibition where the local farmers and townspeople gathered to estimate and gamble the quality of each other's cattle, sheep, pigs, etc. Each animal was shown to the crowd; and people wrote their estimations on the tickets. Final goal of this gambling was estimating the closest weight for each animal in comparison with the real weight of that animal. Galton considered that the average of tickets' value for each animal must be a value of significant distance in comparison with the exact answer because a few people (local farmers or experts) just knew the right answer. He borrowed all 787 tickets, which show the estimations of an ox's weight. While the weight of that ox was 1198 pounds, the average of estimated values in the tickets was 1197! In 1907, he published the \"Vox Populi\" paper in the Nature journal; and mentioned that the result seems more creditable to the trustworthiness of a democratic judgment that might have been expected. In fact, he understood that each ticket contains two data; i.e., information and error. Errors in the tickets omit each other, and the information summarized. This is the main reason that the average of those tickets was really quiet in comparison with the correct answer. This is the core idea of the WOCs theory in social science. Further, this theory is comparable with the jury theorem, which was proposed by Condorcet. Supported by a wide range of examples in business, management, economic, social science, mathematician, etc., Surowiecki [21] Beebe. An ant mill is an observed phenomenon in which a group of army ants separated from the main foraging party loses the pheromone track and begins to follow one another, forming a continuously rotating circle. Next is called needle in a haystack. In this type of problem, just a few members of a group know the right answer. The last is called random decisions. In this type of problem, the final result is completely generated independent of members' decisions. Although the WOCs cannot solve the mentioned problems, it is employed in the different fields of science as a novel theory for solving problems. For instance, it is one of the main references for the Delphi method in management, crowd sourcing and funding in business, the problem solving theorem and the central limit theorem in the mathematician, etc. [21] ."}, {"section_title": "B. Cluster Ensemble", "text": "Clustering groups data points into clusters so that members of the same cluster are more similar to each other than to members of other clusters. Semisupervised clustering uses supervision information to aid the clustering process. This paper focuses on pairwise constraints-based semisupervised methods. As constraint-based methods: Liu et al. [29] proposed semisupervised linear discriminant clustering (semi-LDC). Wang et al. [30] introduced a new technique by utilizing the constrained pairwise data points and their neighbors, which is denoted as constraint neighborhood projections that required fewer labeled data points (constraints) and can naturally deal with constraint conflicts. Chen et al. [31] recently proposed a clustering algorithm which is based on graph clustering and optimizing an appropriately weighted objective, where larger weights are given to observations with lower uncertainty.\nAs mentioned before, individual clustering algorithms provide predictions with different accuracy rates. In practice, individual algorithms may fail to provide accurate and stable results. For solving this problem, cluster ensemble proved that better final results can be generated by combining individual clustering results instead of only choosing the best one [1] . The idea that not all partitions are suitable for cooperating to generate the final clustering was proposed in CES. This method combines a selected group of best individual clustering results according to consensus metric(s) from the ensemble committee in order to improve the accuracy of final results [7] .\nThere are a wide range of studies in the unsupervised cluster ensemble (selection). Vega-Pons et al. [32] proposed weighted partition consensus via kernels (WPCKs) method, which analyzes the set of partitions in the cluster ensemble and extracts valuable information that can improve the quality of the combination process. In another study, Vega-Pons et al. [33] developed the weighted evidence accumulation algorithm by computing the weighted association matrix as the first step and after that, applying a hierarchical clustering algorithm for selecting the consensus partition with the highest lifetime criterion. They also introduced the generalized kernel partition consensus (GKPC) method that uses the Information Unification step after the generation in the methodology of the WPCK method [33] . Jia et al. [11] proposed SIM for diversity measurement, which works based on the NMI. Romano et al. [34] proposed standardized mutual information for evaluating clustering results. Yu et al. [14] proposed the hybrid clustering solution selection (HCSS) strategy that utilizes a weighting function to combine several feature selection techniques for the refinement of clustering solutions in the ensemble. Based on normalized crowd agreement index and multigranularity information collected among individual clusterings, clusters, and data instances, Huang et al. [35] proposed two novel consensus functions, termed WEAC and graph partitioning with multigranularity link analysis (GP-MGLA). Jing et al. [16] introduced a component generation approach for producing ensemble components based on Stratified feature sampling. Yu et al. [13] adopted affinity propagation in different subspaces of the data set for generating a set of individual clusterings. Alizadeh et al. [9] , [17] concluded the disadvantages of NMI as a symmetric criterion. They used the APMM and MAX metrics to measure diversity and stability, respectively, and suggested a new method for building a co-association matrix from a subset of the individual cluster results. While the proposed methods can solve the symmetric problem of the NMI method, they just can combine a subclusters of the generated partition in the reference set [9] , [17] . Yousefnezhad and Zhang [12] proposed weighted spectral cluster ensemble method by exploiting the concept of community detection and graph-based clustering.\nGao et al. [36] introduced a graph-based consensus maximization (BGCM) method for combining multiple supervised and unsupervised models. This method consolidated a classification solution by maximizing the consensus among both supervised predictions and unsupervised constraints. Since, this paper used a classification approach for unsupervised learning, it is sensitive to the quality of supervision information [36] . Huang et al. [35] extended extreme learning machines for both semisupervised and unsupervised tasks based on the manifold regularization. Anand et al. [5] proposed a semisupervised framework for kernel mean shift clustering (SKMS) that uses only pairwise constraints to guide the clustering procedure. They used the initial kernel matrix by minimizing a LogDet divergence-based objective function for first mapped to a high-dimensional kernel space where the constraints are imposed by a linear transformation of the mapped points [5] . Xiong et al. [6] proposed neighborhood-based framework (NBF) method. This method builds on the concept of neighborhood, where neighborhoods contain labeled examples\u0130 of different clusters according to the pairwise constraints. Furthermore, it expands the neighborhoods by selecting informative points and querying their relationship with the neighborhoods [6] .\nOne of the biggest challenges in the mentioned methods is that they did not use the achieved errors, i.e., false positive and false negative, for improving the quality of the final aggregation. As mentioned before, WOC theory uses information and errors for increasing the performance of the final result. Briefly, information aggregate with each other; and also, errors omit each other. There are several studies based on the WOC theory in supervised learning, e.g., in recollecting ordering information [25] , rank ordering problem [24] , estimating the underlying value (e.g., the class) in the image processing [26] , underwater mine classification with imperfect labels [27] , minimum spanning tree problems [28] , and classification ensemble [23] . As the first WOC-based unsupervised CES method, Alizadeh et al. [18] proposed the wisdom of crowds for selective cluster ensemble (WOCCE). They proposed a new strategy of generating, evaluating, selecting, and combining the individual clustering results based on WOC theory. The main advantages of the WOCCE are using feedback mechanism for managing errors in each iteration and utilizing the A3 metric (average of APMM) to avoid the NMI symmetric problem. There are also four disadvantages in the WOCCE method. First, WOCCE needs three distinct kinds of threshold values for generating final clustering result. Further, the performance of WOCCE is dramatically sensitive to the value of mentioned thresholds; and finding the optimum threshold values is so hard in the real application. Second, the concept of independency criterion in WOCCE was just limited to random and initial points in the same type individual clustering algorithms, whereas based on the independency definition in WOC, it can be defined in the term of mathematical independent random variables for all kinds of clustering algorithms. Third, the time complexity of A3 is really high because it is the average of the APMM for all existed clusters in a partition. Since APMM is technically designed for comparing the similarity between a partition and a cluster, there is a wide range of common parts that are sequentially repeated in the A3 metric. Lastly, the WOCCE is only developed for unsupervised learning, while this framework can be also used for semisupervised learning [12] , [18] . Indeed, this paper introduces a new framework for WOC-based CES to solve the mentioned problems in the WOCCE."}, {"section_title": "III. PROPOSED METHOD", "text": ""}, {"section_title": "A. Definition", "text": "Based on outlines of the WOC theory [18] , [21] , [23] , the conditions for a crowd to be wise are: diversity, independency, decentralization, and aggregation. Baker and Ellison [23] and Alizadeh et al. [18] redefined the WOC criteria for supervised learning and unsupervised learning, respectively. They used algorithms, data and results instead of people, information and opinions in the mentioned definitions, respectively. Same structure is utilized in this paper to redefine the criteria for proposing a framework in both unsupervised and semisupervised methods. So, our definition for WOC criteria listed as follows.\n1) Independency: The data, which is applied to clustering models, must have the lowest correlation between its features. 2) Decentralization: Algorithms are able to specialize the results based on the local knowledge. 3) Diversity: Each algorithm has private result, even if it is only an eccentric interpretation of the known facts. Aggregation Some method exists for combining private results into a collective decision (final result). As a whole, it can be stated that the WoCE can produce final results in four stages. First, the mapping function removes the correlation between the features of raw data set. This mapping function can satisfy independency criterion. Then, for satisfying the decentralization criterion, this paper applies local knowledge, i.e., the given number of clusters and supervision information. Further, it employs the various kinds of individual clustering algorithms. After these steps, diversity criterion evaluates the probability of accuracy in the generated clustering results. Finally, an effective aggregation method can increase the performance of the proposed method. In the rest of this section, the formulation of the proposed method will be discussed, and this paper will mention what WOC criterion is satisfied by using each part of the formulation. After that, we briefly summarized the whole algorithm procedure."}, {"section_title": "B. Independency", "text": "Based on definitions of the WOC theory, people must decide by using independent information. Hence, people can discover novel patterns, which are utilized to solve complex problems such as selecting the best person in the presidential election or finding an irregular engineering problem in the NASA's shuttle [18] , [21] . In machine learning arena, this concept can be defined in the term of mathematical independent random variables. In fact, independent features are generated by removing the correlation between the features of raw data. There are various methods for removing the correlation before applying individual clustering techniques, such as principle component analysis (PCA) or linear discriminant analysis (LDA). They can validate that removing the correlation dramatically improves the performance of clustering results [12] , [20] . Now, this paper defines independency criterion by utilizing the concept of correlation. In other words, this paper develops a new branch of mentioned methods in the CES for mapping data to different dimensions with less correlation between its features. In the rest of this section, we show that how our proposed method transforms features of raw data to stable dimensions with less correlation.\nGiven a set of data examplesX = {x 1 ,x 2 , . . . ,x n }, and the corresponding pairwise must-link constraint set M = {(x i , x j ); x i and x j belong to the same cluster} and pairwise cannotlink constraint set C = {(x i , x j ); x i and x j belong to different clusters}. The simple average ofX can be denoted as follows:\nwhere n is the number of instances in theX; andx i denotes the ith instance of the data points. Now, this paper denotes X as follows:\nwhereX is the data points, andX denotes simple average ofX, which calculated by (1). It is clear that X is zero-mean. In other words, the excepted value of X is zero as follows:\nFurther, this paper defines Q : X \u2208 R m\u00d7n \u2192 Y \u2208 R m\u00d7n , where m, n denote the number of features and data points, respectively. The main goal of this mapping is just minimizing the correlation between features. This problem can be reformulate as follows:\nIf the correlation (covariance) of X is considered R =\n, then the correlation of Y will be defined as follows:\nBased on above definition, the expected value of jth feature of X denotes as follows:\nwhere q denotes the jth index of the Q. In other words, our correlation problem is changed to a variance problem. Now, maximizing the q based on the variance of X will be omitted the correlation between features. Since the scale of data after mapping must be same, we assume following equation:\nSo, our problem will be reformulated as follows:\nwhere the symbol \u03b4q is an abbreviation for \"a small change in q.\" We consider (\u03b4q) \u03b4q \u2248 0, so the above definition denotes as follows:\nBased on (7) and (8), we can assume as follows:\nNow, this paper defines following equation by using (9) and (10):\nwhere \u03bb \u2208 R is a constant. Since (\u03b4q) = 0, the following equation must be satisfy for minimizing correlation between features:\nwhere R and \u03bb denote the eigenvectors and eigenvalues, respectively. For all features of X the above equation will be denoted as follows:\nwhich is called eigenstructure equation. In above equation, is a diagonal matrix. Based on (7), we can define the following equation:\nwhere I is identity matrix. Following equation denotes based on (13) and (14):\nwhere m denotes number of features in data X. Now, consider that R is a descending order based on values. For an optional feature selection in our unsupervised approach, we can define the following equation instead of (15):\nwhere d < m is the number of features, which must be selected for generating results. Algorithm 1 shows the mapping function, which can generate independent features by minimizing the correlation of data set. For reducing the time complexity, this paper uses an EM algorithm [37] "}, {"section_title": "C. Decentralization", "text": "In WOC theory, the decentralization criterion increases the crowd intelligence, the margin of error and the quality of the final result [18] , [21] . In the clustering problems, the same concept is the main reason for using the CES approach to improve the quality of the final result. So, there is a wide range of quality metrics in the previous CES methods [7] , [9] , [17] . Based on the WOC theory, this paper uses local knowledge for increasing the quality of individual clustering results. There are two different kinds of local knowledge in the CES; i.e., the number of clusters in unsupervised learning and supervision information in semisupervised learning. Moreover, employing different kinds of clustering algorithms significantly can affect to generate more specialize clustering results because they include different kinds of objective functions [18] . Briefly, this paper applies the different kinds of clustering algorithms on the mapped data for generating the individual clustering results in both unsupervised and semisupervised versions of the proposed method. Further, these algorithms use different numbers of clusters in the range of [2, k + 2], where k denotes the number of clusters in the final results. Since, this procedure generates all available kinds of patterns as the reference set, it can increase the robustness of the final results. In addition, this paper develops a new feature selection method based on supervision information for improving the performance of the final result. In the rest of this section, we show that how this paper uses supervision information for generating common/local knowledge in the semisupervised approach.\nAs mentioned before, our proposed method is based on pairwise constraint, i.e., must-links and cannot-links. This paper denotes the must-link constraint with M, and the cannot-link constraint with C. \nwhere n C and n M denote the cardinalities of C and M, respectively, and \u03b3 is a scaling coefficient. The intuition behind (17) is to let the average distance in the low-dimensional space between examples involved by the cannot-link C as large as possible, while distances between examples involved by the must-link M as small as possible. Since the distance between examples in the same cluster is typically smaller than that in different clusters, a scaling parameter \u03b3 is added to balance the contributions of the two terms in (17) and its value can be estimated as follows:\nWe can also reformulate the objective function in (17) in a more convenient way as follows:\nwhere S C and S M are, respectively, defined as\nThis paper calls S C and S M defined in (20) and (21), respectively, as cannot-link scatter matrix and must-link scatter matrix, which resemble the concepts of between-cluster scatter matrix and within-cluster scatter matrix, respectively, in LDA [20] . The difference lies in that the latter uses cluster labels to generate scatter matrices, while the former uses pairwise constraints to generate scatter matrices. Obviously, the problem expressed by (19) is a typical eigenproblem, and can be efficiently solved by computing the eigenvectors of S C \u2212 \u03b3 S M corresponding to the positive eigenvalues. In other words, just consider thatW and Z = {\u03b6 1 , \u03b6 2 , . . . , \u03b6 p , . . . , \u03b6 d } are eigenvectors and eigenvalues of S C \u2212 \u03b3 S M , respectively. TheW and \u03b6 is descending ordered based on \u03b6 values\nwhere p shows the position of positive eigenvalues (\u03b6 p > 0)}. Further, the transformed data set is calculated as follows: (18) , (20), (21) 4. Calculating the eigenvaluesW and eigenvectors \u03b6 of S C \u2212 \u03b3 S M . 5. Calculating the W by usingW p based on \u03b6 > 0. 6. Return Z = W Y Algorithm 2 illustrates the transformation algorithm for both unsupervised and semisupervised approaches. The transformed data is applied to different kinds of individual clustering algorithms for generating the reference set."}, {"section_title": "D. Diversity", "text": "Indeed, diversity is a common concept in both WOC theory and CES methods. For instance, NMI [19] and APMM [9] are two famous methods for calculating diversity in the cluster ensemble (selection). The diversity increases the stability of the final results. As mentioned before, NMI has the symmetric problem. This problem causes that evaluation of the diversity between two clusters always results equal, when those clusters are complements of each other. This fault is occurred when the number of positive clusters in the considered partition of reference set is greater than 1 [9] , [17] , [18] . Although some of the researches proposed alternative methods such as APMM [9] and MAX [17] for solving this problem, their proposed methods were utilized for evaluating diversity between a cluster and a partition. As a result, using mentioned methods for evaluating the diversity of two partitions increases the time complexity. In the rest of this section, we first explain that how NMI and APMM work. Then, we develop a new metric, which directly can evaluate diversity between two partitions. Indeed, NMI employed three different Shannon's entropy for evaluating the similarity between two partitions. Since, NMI is normalized, the 1\u2212NMI was always considered as the diversity between mentioned partitions. NMI used the entropy of common instances between two partitions as numerator, and also employed the sum of entropy of each partition as denominator [9] , [17] , [19] . As mentioned before, NMI has symmetric problem. As another alternative, APMM tried to solve the mentioned problem for evaluating the similarity between a cluster (C a i from P a ) and all clusters of another partition (P b ) [9] . Since, some common parts of APMM must be repeated for calculating diversity of two partitions, using the APMM for evaluating the diversity of two partitions increases the time complexity. Further, simple average was utilized for calculating the diversity between all clusters of a partition (P a ) versus all clusters of another partition (P b ) [9] , [18] . This averaging procedure causes to decrease the robustness of achieved evaluation because it finds the mean of similarity between all clusters of two partitions instead of calculating maximum similarity (minimum diversity) among of them. This paper proposes a new greedy method based on the main idea of the APMM. It can calculate diversity between two partitions without repeating common parts; and also it avoids using the averaging procedure.\nAs mentioned in the previous section, individual clustering results are generated by using the transformed data on the different kinds of clustering algorithms. This paper denotes the generated results as a reference set as follows:\nwhere T denotes the number of individual clustering results and p i is the ith partition of the generated results. Now, this paper finds the maximum similarity for each partition by considering the number of all instances in that partition versus the number of instances in each cluster of that partition as follows:\nwhere P is a partition from the reference set; c i denotes the ith cluster of partition P, and n and n i denote the cardinalities of P and c i , respectively. Furthermore, this paper finds the maximum similarity for each partition by considering the number of instances in each cluster of that partition versus the number of all instances in that partition as follows:\nwhere the notations of P, c i , n, and n i define same as the previous equation. Now, this paper determines the following equation as maximum similarity between a partition versus other partitions in the reference set:\nwhere E and P are the reference set and a partition from the reference set, respectively. Also, P i and c j denote the ith partition from the reference set E and jth cluster from the partition P i , respectively. Further, n j i and n are the cardinalities of c j and P, respectively. Now, this paper proposes the uniformity as the diversity of partition P versus all partitions of the reference set E as follows:\nwhere E is the reference set (ensemble committee), and P denotes a partition from the reference set. Uniformity is normalized between 0 \u2264 Uniformity \u2264 1. As a greedy metric, uniformity employs a strict strategy for evaluating the diversity between partition P and the other partitions of ensemble committee. In other words, uniformity represents a value near of zero for a partition with low diversity, and illustrates a ,j) ) are the same. This paper proposes WEAC for optimizing this method by using a weight for each individual clustering results instead of just counting their shared clusters. While the weight can have different definitions in the other applications, this paper uses average of uniformity of two partition as the weight in the WEAC (\nvalue near of one for a partition with high diversity. In addition, it avoids to repeat common parts, i.e., (25) and (26), for evaluating the diversity in each comparison."}, {"section_title": "E. Aggregation", "text": "Thresholding is used for selecting the evaluated individual results in the CES. Then co-association matrix is generated by using consensus function on the selected results. Lastly, the final result is generated by applying linkage methods on the co-association matrix. These methods generate the dendrogram and cut it based on the number of clusters in the result [18] , [19] . In recent years, many papers have used EAC as a high-performance consensus function for combining individual results [7] - [9] , [18] , [19] . EAC uses the number of clusters shared by objects over the number of partitions in which each selected pair of objects is simultaneously presented for generating each cell of the co-association matrix. Fig. 1 illustrates the effect of the EAC equation (c(i, j) = (\u03b1(i, j)/\u03b2(i, j) )) on the shape of dendrogram. Where \u03b1 (i,j) represents the number of clusters shared by objects with indices (i, j); and \u03b2 (i,j) is the number of partitions in which this pair of instances (i and j) is simultaneously presented. As a matter of fact; EAC considers that the weights of all algorithms results are the same. Instead of counting these indices, this paper uses following equation, which is called WEAC, for generating the co-association matrix:\nwhere \u03b1(i, j) and \u03b2(i, j) are same as the EAC equation. Also, \u03c1 i,j is the weight of combining the instances. Although, this weight can have different definitions in the other applications, this paper uses average of uniformity of two algorithms as follows for combining individual results:\nAlgorithm 3 WoCE Algorithm\nNumber of clusters k, Number of selected features d (default d=0), Output: P f as partition of data set into k clusters Method:\n1. Initial an empty set as Reference-Set. where Uniformity(P i , E) and Uniformity(P j , E) illustrate the uniformities of the algorithms, which generated the results for indices i and j. In other words, as a new mechanism, this paper generates the effective results when both algorithms have high uniformity values; and also the effects of individual results are near of zero when the both algorithms have small values in the uniformity metric. As a result, this paper just omits the effect of low quality individual results by using mentioned mechanism instead of selecting them by thresholding procedures. Further, the final co-association matrix, which is a symmetric matrix, will be generated by (28) as follows:\nwhere n is the number of data points; and c(i, j) denotes the final aggregation for ith and jth instances."}, {"section_title": "F. Summarization and Discussion", "text": "Algorithm 3 shows the pseudo code of the proposed method. In this algorithm, the distances are measured by an Euclidean metric. The clustering algorithm function builds the partitions of individual clustering results, which will be discussed in the next section; uniformity function evaluates individual clustering results (iResult) by using (27) . Then, evaluated results will be added to reference set. The WEAC function generates the co-association matrix, according to (28) . The averagelinkage function creates the final ensemble according to the average-linkage method [18] .\nThere are three points, which must be discussed before this paper starts to explain the empirical studies. First, why this paper chooses the WOC as a framework in the cluster ensemble? As mentioned before, the main reasons for using cluster ensemble are increasing performance, stability, robustness of the final results on the clustering problems. As already stated, the WOC theory is superior to that of a few experts. In other words, it is proven [18] , [21] , [23] that results made by aggregating the information of groups have better performance, stability, and robustness than those made by any single group member if the WOC criteria are satisfied. Therefore, the cluster ensemble and the WOC are the same solutions with the same goals in two different sciences, i.e., machine learning and social science, respectively. Next, what are the common concepts between our proposed criteria in the WOC and previous methods in clustering problems? In fact, diversity is existed in clustering with the same title, e.g., NMI and APMM are two famous methods for calculating diversity in the cluster ensemble (selection). The diversity increases the stability and robustness of the final results. Further, independency referred to the correlation concept in the learning methods. This correlation can be defined between features of raw data. There are some techniques, i.e., PCA, LDA, etc. for mapping data to new dimensions without any correlation between its features. This paper uses a new branch of these techniques for satisfying the independency criterion, which can increase the performance of the final results. In addition, decentralization guarantees that the quality of the final result is optimized. In other words, it uses different individual clustering algorithms, which use different objective functions, for generating all possible patterns as the reference set in the cluster ensemble problem. Moreover, an effective aggregation method can combine the final result without thresholding procedure. The last question is why all of the four conditions of the WOC must be satisfied in the ensemble learning? Based on previous question, the proposed method can be defined as a CES method that applied a feature mapping in advance. In practice, all of clustering analysis has these steps [12] , [18] , [36] . Therefore, the WOC framework does not add any new stage in the pipeline on clustering analysis. It just defined what is the robust and compulsory structure for an ensemble framework in real-world application."}, {"section_title": "IV. EXPERIMENTS", "text": "The empirical studies will be presented in this section. The unsupervised methods are used to find meaningful patterns in unlabeled data sets such as Web documents; and semisupervised employs supervision information for generating more robust and stable final results in real world application. Since, the real data set does not have class labels, there is no direct evaluation method for estimating the performance in unsupervised or semisupervised methods. Like many previous researches [5] , [7] , [9] , [17] , [18] , [36] , this paper compares the performance of its proposed method with other individual clustering methods and cluster ensemble (selection) methods by using standard data sets and their real classes. Moreover, the supervision information will be randomly generated based on real class labels. In this paper, all of algorithms are implemented in the MATLAB R2015a (8.5) by authors on a PC with certain specifications 2 in order to generate experimental results. All results are reported by averaging the results of ten independent runs of the algorithms. Table I demonstrates the individual clustering algorithms, which are used for generating the individual clustering results in our proposed method. Further, the number of individual clustering results for the ensemble methods is set as 20 in the reference set."}, {"section_title": "A. Data Sets", "text": "This paper uses three different groups of data sets for generating experimental results; i.e., image data sets, document data sets and other UCI data sets. Table II illustrates the properties of these data sets. This paper uses the USPS digits data set, which is a collection of 16 \u00d7 16 gray-scale images of natural handwritten digits and is available from [38] . Furthermore, this paper utilizes ImageNet [39] , MNIST, and CIFAR-10 [40] as three image-based data sets, which are mostly employed in deep learning studies [40] . As another alternative in the image-based data set, this paper uses Alzheimer's Diseases Neuroimaging Initiative (ADNI) data set for 202 subjects. This data set contains magnetic resonance imaging (MRI) and positron emission tomography (PET) images from human brain in two categories (which are shown by C1 and C2 in Tables II and III) for recognizing the Alzheimer diseases (ADs). In the first category, this data set partitions subjects to three groups of health control, mild cognitive impairment (MCI), and AD. In the second category, there are four groups because the MCI will be partitioned into high and low risk groups (HMCI/LMCI). This paper uses all possible forms of this data set by using only MRI features, only PET features and all of MRI and PET features (FUL) in each of two categories. More information about ADNI-202 is available in [41] . As a document based data set, the 20 Newsgroups is a collection of approximately 20 000 newsgroup documents, which is partitioned (nearly) evenly across 20 different newsgroups. Some of the newsgroups are very closely related to each other, while others are highly unrelated. It has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering. As two other document-based data sets, the Reuters-21578 [42] and Letters [15] are employed in this paper. The rest of standard data sets are from UCI [43] . This paper has chosen data sets which are as diverse as possible in their numbers of true clusters, features, and samples because this variety better validates the obtained results. The features of the data sets are normalized to a mean of 0 and variance of 1, i.e. , N (0, 1) ."}, {"section_title": "B. Performance Analysis for Unsupervised Methods", "text": "In this section, the performance (accuracy metric [20] ) of unsupervised version of proposed method (UWoCE) will be analyzed. As mentioned before, algorithms listed in Table I were employed for generating the individual clustering results in our proposed method. Further, the sets of supervision information (must-links and cannot links) are considered null in this section. Also, the final clustering performance was evaluated by relabeling between obtained clusters and the ground truth labels and then counting the percentage of correctly classified samples [12] , [18] . The results of the proposed method are compared with full ensemble (EAC) [19] as baseline, WPCK [32] , GKPC [33] , HCSS [14] , GP-MGLA [15] , and WOCCE [18] which are state-of-the-art cluster ensemble (selection) methods. The performance of full ensemble method (EAC) is reported for demonstrating the effect of selecting the best results in comparison combing all generated result with each others. In addition, the performance of the WPCK, GKPC, HCSS, and GP-MGLA are reported as four weighted clustering ensemble methods. For representing the effect of uniformity on the performance of the final results, it compares with three state-of-the-art metrics in diversity evaluation (A3 [18] , SACT [15] , and CA [33] ). This paper does not use optional feature selection in this section (d = 0).\nThe experimental results are given in Table III . In this table, the best result which is achieved for each data set is highlighted in bold. As depicted in this table, the results of the EAC illustrate the effect of evaluation and selection in CES methods. Since some of the four conditions of the WOC theory do not exist in EAC, this method is a good example of unwise crowd. According to this table, the proposed algorithm (WoCE) has generated better results in comparison with other individual and ensemble algorithms. Even though the proposed method was outperformed by a number of algorithms in four data sets (ADNI-MRI-C2, SA Heart, Sonar, and Yeast), the majority of the results demonstrate the superior accuracy of the proposed method in comparison with other algorithms. In addition, the difference between the performance of proposed method and the best result in those three data sets is lower that 2%. In addition, the WOCCE and the proposed method generate more stable results in comparison with other methods based on the standard variances. As mentioned before, this is the effect of WOC framework."}, {"section_title": "C. Performance Analysis for Semisupervised Methods", "text": "The empirical results of semisupervised methods will be analyzed in this section. Since most of the semisupervised cluster ensemble methods [6] , [29] , [36] use feature selection based on the supervision information, this paper compares the performance of semisupervised methods on high-dimensional and large-scale data set in Table II; i.e., 20 Newsgroups, Letters, and Reuters-21578 as document-based data sets, ADNI, CIFAR-10, ImageNet, MNIST, and USPS as imagebased data sets, and also Arcene, CNAE-9, Optdigit, and Sonar from UCI repository. This paper does not use the optional feature selection in this section (d = 0).\nIn this paper, 1%-5% of instances with class labels are randomly selected for generating the supervision information (a half for must-link and a half for cannot-link); e.g., 1% (2620) of instances are selected in the 20 Newsgroups data set for generating the pairwise constraints, where 655 mustlinks and 655 cannot-links constraints are generated by the selected instances. In addition, the supervision information, which applied to the methods, are same in each independent run for all methods. Notably, this paper does not employ all combinations of the randomly selected instances as the pairwise constraints (must-links and cannot-links). In other words, each randomly selected instance is used once for generating just a must-link or a cannot-link. There are two reasons for this strategy of generating pairwise constraints. First, this strategy provides better diversity among the generated pairwise constraints. Second, this strategy represents better simulation for the real application of prior supervision information. Indeed, there is no class label in the real-world applications, and generating the pairwise constraints from all combinations of the randomly selected instances is impossible or expensive [5] , [6] , [36] . For instance, just consider an interactive image search engine, where it shows two random images to users in each attempt and asks users to specify that these images are same (must-link) or different (cannot-link). Then, the search engine will improve the clustering results based on these limited feedbacks.\nThe final clustering performance (accuracy metric [20] ) was evaluated by relabeling between obtained clusters and the ground truth labels and then counting the percentage of correctly classified samples [18] . Fig. 2 illustrates the performance of the proposed method (WoCE) in comparison with the RP [44] , BGCM [36] , NBF [6] , and SKMS [5] . In this figure, the standard deviations of the results are lower than 1% (for ten independent runs). This paper reports the performance of RP as a classical method in the semisupervised cluster ensemble. Also, this paper reports the performance of BGCM as a novel graph-based approach in the semisupervised clustering. Notably, the BGCM has two versions; i.e., unsupervised and semisupervised. This paper uses the semisupervised version of BGCM in this section. What is more, this paper uses SKMS as a kernel-based method in semisupervised clustering. Last but not least, the empirical results of the proposed method are compared with NBF as another heuristic method in the semisupervised cluster ensemble. Even though WoCE was outperformed in one data set (Optdigit) by some algorithms, the majority of results demonstrate superior accuracy for the proposed method. In addition, the clustering performance of some algorithms in Fig. 2(k) and (l) become worse with increased number of pairwise constraints. As mentioned before, pairwise constraints often result in highly unstable clustering performance [5] , [6] . These figures are good examples for this issue, where some of the previous methods cannot handle the extra supervision information. In fact, the supervision information made unstable individual clustering results and significantly reduce the performance of the mentioned methods. In these cases, our proposed method has handled the supervision information by employing the WOC theory, i.e., better data representation (Algorithms 1 and 2), robust individual clustering evaluation (uniformity metric), and effective aggregating mechanism (WEAC)."}, {"section_title": "D. Optional Feature Selection in Unsupervised-Method", "text": "In this section, the performance of the proposed method will be analyzed by using the optional feature selection (d parameter). Since feature selection is automatically used by applying the supervision information on the mapped data set in the semisupervised version of the proposed method, the performance of the unsupervised version of the proposed method (UWoCE) will be analyzed in this section. This paper employs high-dimension data sets, i.e., Arcene, CIFAR-10, MNIST, and USPS, for analyzing the performance of the proposed method. Fig. 3(a) shows the relationship between the performances of the proposed method based on the percentage of selected features in different data sets. The vertical axis refers to the performance while the horizontal axis refers to the percentage of the selected features in each data set. As depicted in this figure, the optional feature selection can significantly increase the performance of final results on highdimensional data sets. So, this paper offers that the optional feature selection will be used for high-dimensional data sets to handle features' sparsity. Moreover, this experiment is the reason for using high-dimensional and large-scale data sets in the previous section. The important questions which must be discussed here are what is different between the mapping function and the optional feature selection? And where the optional feature selection can be employed? Indeed, the mapping function, which is illustrated in Algorithm 1, minimizes the correlation between features; and it maps data to a new domain, which the covariance between different features is near zero. Most of the time, this function maps data to the stable dimensions, which can dramatically improve the accuracy of the final results. It can be also formulated as follows: Q : X \u2208 R m\u00d7n \u2192 Y \u2208 R m\u00d7n , where Q can satisfy the independency criterion in the WOC theory. In a high-dimensional data set, some of the calculated eigenvalues ( = {\u03bb j }) approach near to zero. Since these eigenvalues trivially effect on the mapping Q, they can be omitted for reducing the dimensions of the data set and the time and space complexities of the clustering analysis. In other words, these eigenvalues may reduce the stability of the final results [12] . By considering the optional feature selection, the mapping Q can be formulated as follows: Q : X \u2208 R m\u00d7n \u2192 Y \u2208 R d\u00d7n , where d < m. Therefore, employing this optional feature selection for analyzing the high-dimension data set can improve the stability of the mapping Q as well as the performance of the final result [see Fig. 3(a) ]. In addition, this feature selection is better to use based on the fluctuation of the eigenvalues (remove near zero values) in each problem."}, {"section_title": "E. Time Complexity Analysis", "text": "In this section, the runtime of both unsupervised and semisupervised methods will be compared by using various data sets, i.e., three large-scale data sets (Letters, MNIST, and 20 Newsgroups) and two high-dimension data sets (Arcene and Reuters-21578). Fig. 3(b) illustrates the relationship between the runtime of the mentioned methods and the size of data sets. The vertical axis refers to the runtime while the horizontal axis refers to the algorithms. As mentioned before, all of the results in this experiment are generated by a PC with a certain specifications. As depicted in this figure, the runtime of the semisupervised methods (the first five bars) is more than the runtime of the unsupervised methods because they need an additional step to apply the supervision information (mostly in the form of an eigenproblem) [36] . By considering the performance of these methods in Table III and Fig. 2 , WoCE (the first bar) and UWoCE (the last bar) generated more efficient results in comparison with other clustering methods. Indeed, the proposed method selects the features based on the correlations between data points and supervision information (in semisupervised approach). So, the number of calculations for generating individual clustering results will be significantly decreased in comparison with other cluster ensemble methods, while the performance of the final results is rapidly increased.\nThere are some technical issues that must be discussed here. First, this paper uses an EM algorithm [37] for estimating the eigenvalues/vectors, which this algorithm can significantly reduce the time complexity of the mapping function in Algorithm 1. Second, the size of the transformed matrix (22) in the proposed method for applying the supervision information is limited to the size of pairwise constraints. This size is really small in comparison with the size of instances; e.g., in 20 Newsgroup data set, the size of this matrix for 1% of randomly sampled pairwise constraints is 655 \u00d7 655, while the instance similarity matrix is 26214 \u00d7 26214. Notably, most of the previous studies such as SKMS and BGCM directly used the instance similarity matrix for applying the supervision information. Lastly, this paper uses a modified version of the EAC for combining the individual clustering results. EAC applies a linkage method on a simple matrix, where the size of this matrix is the number of algorithms \u00d7 the number of instances (T \u00d7 n), where T << n in practice. By contrast, some of the previous studies such as BGCM utilized the graph methods for combining the individual results, where the size of the adjacency matrix of the graph in these methods is the square of the number of instances (n 2 ). Based on these technical issues, the proposed method can significantly increase the performance of the final results as well as an acceptable runtime."}, {"section_title": "V. CONCLUSION", "text": "In this paper, WOC theory in social science was mapped to the clustering ensemble arena. The main advantages of this mapping include the addition of two new aspects, i.e., independency and decentralization, for estimating the quality of individual clustering results, and a new framework to investigate them. To reach the four conditions of WOC, this paper incorporates a series of novel strategies for producing individual clustering results as well as obtaining the final ensemble result. Specifically, a mapping function is introduced to perform independency on individual clustering results. This function can minimize the correlation between features by using the concepts of expected value and covariance. The decentralization criterion is proposed for transforming the data from high-dimension to low-dimension based on pairwise constraints, to keep quality in the generated individual clustering results. Further, this paper evaluates the diversity of individual clustering results with a novel metric called uniformity. At last, WEAC is proposed for the final aggregation. To validate the effectiveness of the proposed approach, an extensive experimental study is performed by comparing with multiple state-of-the-art methods on various data sets. In the future, we will develop a new version of uniformity based on the concept of expected value instead of using the APMM."}]