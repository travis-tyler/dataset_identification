[{"section_title": "", "text": ".9 Items for which help text was accessed five or more times 3-13 Table 4.1 Reliability indices for enrollment since the NPSAS:96 base year interview 4-2 Table 4.2 Reliability indices for personal and work characteristics while enrolled 4-3 Table 4.3 Reliability indices for student loans and expenses 4-4 Table 4.4 Summary of indeterminate responses to questions by interview section 4-6 Table 4.5 Summary of analyses for order effects in items regarding frequency of specific school-associated activities and events 4-7 Table 4.6 Summary of upcoding for \"other, specify\" items 4-8 11 x The BPS series provides a unique perspective of what happens to persons as they enter and pursue education beyond high school because it includes both traditional and nontraditional students who began their postsecondary studies in the 1995-96 academic year (1994-95 for the field test sample). Nontraditional students are those who have delayed continuation of their education after high school due to military service, employment, family responsibilities, or other reasons. Other longitudinal studies, which tend to follow a single age cohort, do not contain enough nontraditional students to permit study of their progress and attainment relative to their more traditional classmates. This is important as nontraditional students represent a growing segment of the postsecondary student population."}, {"section_title": "List of Figures", "text": "The BPS study series also makes it possible to trace the paths of first-time beginning students (FTBs) throughout the entire system of postsecondary education over a number of years. Unlike the typical retention and attainment studies of entering freshmen at a single institution, BPS allows for the study of student persistence and attainment anywhere. Since, as the first BPS series showed, nearly half of all beginning students enroll at more than one institution during the five years after they begin postsecondary education, being able to monitor the progress of FTBs across postsecondary institutions is becoming increasingly important. BPS:96/98, as the first follow-up in the series, will serve to monitor academic progress through the first three academic years of postsecondary study. Most students at four-year institutions who never complete a baccalaureate degree have left school by the end of their freshman or sophomore year and BPS:96/98 will explore the reasons which factor into such patterns. It could be simply that they finished the course of study they had originally intended to complete, or transferred to a different institution to expand their education. There may have been other factors, such as cost of attendance or family responsibilities, which caused them to cut short or postpone their education. BPS:96/98 also monitors causes of delay in academic progress, if delay has occurred. Delays may be due to academic reasons, such as having had to take remedial courses or changing major, or personal reasons, such as having to work more than originally expected or increased family responsibilities. Likewise, two years is long enough to determine that a particular student is making progress as expected, as sufficient time has passed to catch up from minor early delays. For those students enrolled in a less-than-4-year program, a follow-up two years after the base year data collection allows assessment of whether the student has completed the original intended curriculum and the time needed to complete the program. It also allows for collection of limited information on initial rate of return after completion, or continuation into a more advanced level of study. "}, {"section_title": "Schedule and Products of BPS:96/98", "text": "The BPS:96/98 field test data collection was conducted from April to July 1997. The full-scale data collection is scheduled for February to September of 1998. Full-scale data, along with data from prior studies, will be used to examine a wide range of education policy questions. Electronically documented restricted access research files (with associated electronic codebooks) as well as a Data Analysis System (DAS) for public release will be constructed from the fullscale data and distributed to a variety of organizations and researchers. BPS:96/98 will produce the following reports: (1) a descriptive summary of significant findings; (2) a full-scale methodology report providing details of sample design and selection procedures, universe coverage, weighting methodologies, estimation procedures and design effects, the results of nonresponse analyses, and statistical quality evaluation; and (3) special tabulations of issues of postsecondary interest, which will become part of the final data library available for further analysis by researchers."}, {"section_title": "C.", "text": "\n"}, {"section_title": "Purpose of the Field Test", "text": "The main purpose of the field test was to use, test, and evaluate all operational and methodological procedures, instruments, and systems planned for use in the full-scale study. Many such methodological features, representing enhancements or refinements to previously used BPS and NPSAS approaches, had not been fully tested in the past. Using and testing methodologies in the field test that parallel the data collection procedures proposed for the fullscale study allow such procedures to be adjusted as necessary, prior to the start of full-scale data collection. This procedure of conducting a comprehensive field test has been used quite successfully throughout the BPS and NPSAS series to enhance and advance, after controlled evaluation, the methodologies used in these important studies. Based on the results of the BPS:96/98 field test reported herein, the BPS:96/98 full-scale study will be modified to maximize operational efficiency, improve responses, and collect a higher quality of information. It should be noted that the field test was conducted during a period of high mobility for sample members still enrolled in school, and the abbreviated locating/interviewing period somewhat limited field test success in these areas. Thus, the field test tracing, contact, and response rates reported in subsequent chapters are expected to improve in the full-scale study. BPS:96/98 Field Test Report 1-3 administrative data. These cases were screened for FTB eligibility during the BPS:96/98 interview, and some retrospective interview data were collected for those determined to be BPS-eligible. Consistent with previous NPSAS studies, the institutions eligible for the NPSAS:96 field test and, hence, eligible for the BPS:96/98 field test, were those institutions that satisfied all the following conditions for the 1994-95 academic year: offered an educational program designed for persons who have completed secondary education; offered more than just correspondence courses; offered at least one academically, occupationally, or vocationally-oriented program of study requiring at least three months or 300 contact hours of instruction; were open to the general public (i.e., not just to specific populations, such as prison inmates or the members of the organization offering the courses); and were located in the 50 States, the District of Columbia, or Puerto Rico. In addition, U.S. service academies were excluded because of their atypical funding and tuition base. Also ineligible were hospitals offering only internships or residency programs; institutions offering only noncredit continuing education units (CEUs); schools whose only purpose was to prepare students to take a particular examination (e.g., CPA or Bar exams); and branch campuses of U.S. institutions in foreign countries. The field test and full-scale institutional samples were constrained to be disjoint for NPSAS:96. To allow the broadest institutional population for the full-scale study, the full-scale sample was selected first and the field test sample was selected from the residual frame members. The field test institutions were chosen purposively to represent as complete a spectrum as possible of the residual institutions on the sampling frame and to represent each of the institutional strata planned for the full-scale study samples. Additionally, the sample was selected from several separate geographic areas (including Puerto Rico). A total of 78 institutions were selected for the field test; this figure was chosen to yield 65 institutions that were eligible and would provide lists for student sampling. Because the achieved institutional yield was greater than expected and budgeted for, 65 of the 66 eligible institutions providing lists were retained for field test implementation. The students eligible for the BPS:96/98 field test were the students eligible for the NPSAS:96 field test who were FTBs at the NPSAS sample institutions in the 1994-95 academic year. Consistent with previous NPSAS studies, the students eligible for the NPSAS:96 field test were those enrolled in eligible institutions who satisfied all of the following eligibility requirements: Chapter 2 Design and Method of the Field Test were enrolled in a term or course that began between May 1, 1994 andApril 30, 1995;2 were enrolled in (a) an academic program; (b) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree, or (c) an occupational or vocational program that required at least 3 months or 300 contact hours of instruction to receive a degree, certificate, or other formal award; were not concurrently enrolled in high school; and were not enrolled solely in a GED or other high school completion program. \"Pure\" FTBs are those NPSAS-eligible students who had never enrolled in a postsecondary institution after completing high school. \"Effective FTBs\" are those NPSAS-eligible students who had enrolled for at least one course after completing high school but had never completed a postsecondary course before the 1994-95 academic year. Both pure and effective FTBs were eligible for the BPS:96/98 field test, as were NPSAS:96 nonrespondents who were potential FTBs. The NPSAS:96 field test CATI interview identified 726 verified (pure or effective) FTBs. In addition, 59 NPSAS nonrespondents were sampled for BPS:96/98 as potential FTBs. Their distribution by type of institution is presented in table 2.1. In addition to FTBs whose BPS eligibility had been verified in the NPSAS:96 CATI interviews, we sampled from the pool of potential FTBs who were nonrespondents in the NPSAS:96 field test. During the BPS interview, we determined their eligibility for NPSAS and BPS, and identified the \"actual\" (pure or effective) FTBs. The actual FTBs were administered an abbreviated NPSAS:96 interview as part of their BPS:96/98 interview. 'This full year of enrollment is the operational survey population. The ideal target population consists of the terms in the 1994-95 financial aid award year, those beginning between July 1, 1994 and June 30, 1995. The survey year is slightly shifted from the ideal year to allow more timely data collection and dissemination of results."}, {"section_title": "BPS:96/98 Field Test Report", "text": ""}, {"section_title": "2-3", "text": "The goal was to select about 50 to 75 NPSAS field test nonrespondents within a small number of geographic areas in such a manner that the students selected were highly likely to be FTBs. Because we needed to test both field locating and CATI contacting with these students, a secondary goal was to select reasonable numbers of students that required pre-CATI intensive tracing as well as students not requiring such efforts, based on whether or not they had been located in the NPSAS:96 field test. Based on the Chi-squared Automatic Interaction Detector (CHAID) modeling of FTB status done at the conclusion of the NPSAS:96 field test and other analyses, we partitioned the NPSAS:96 field test nonrespondents who were sampled as potential FTBs into the following three categories: those highly likely, those moderately likely, and those not likely to be an FTB. The \"highly likely\" group consisted of those students sampled as potential FTBs who graduated from high school or received a GED in the current year or the previous 2 years (1993, 1994 or 1995 for the field test). The CHAID analysis indicated that about 95 percent of these students would be FTBs. The \"not likely\" group consisted of those students sampled as potential FTBs whose year in school was reported to be second or higher in Central Processing System (CPS) or Computer Assisted Data Entry (CADE), or with transfer credits reported in CADE. All other nonrespondents sampled as potential FTBs were assigned to the \"moderately likely\" category. The numbers of students classified as having high, moderate, or low likelihood of being an FTB were 158, 163, and 28, respectively, for a total of 349 NPSAS field test nonrespondents. We included in the BPS field test only those students classified as \"highly likely\" to be FTBs so that the resulting BPS interviews would provide an adequate test for all survey procedures and instruments, including collection of retrospective NPSAS:96 data. In order to assign students to geographic clusters and select areas from which NPSAS nonrespondents would be included for the BPS field test, we examined primarily the distributions of permanent addresses (city and/or state). We selected 59 NPSAS field test nonrespondents for the BPS field test. They were selected from the following four geographic areas: 13 from Massachusetts; 20 from Pennsylvania; 14 from Puerto Rico; and 12 from the Dallas, Texas metropolitan area."}, {"section_title": "B.", "text": "Data Collection Design 1."}, {"section_title": "Locating", "text": "The basic BPS:96/98 design involved tracing sample members to their current location and conducting a computer assisted telephone interview (CATI) or a computer assisted personal interview (CAPI) with them about their experiences since the NPSAS:96 interview two years earlier. The data collection activities, including locating, are shown in figure 2.1. While the flow shown is sequential for any given case, these activities are quite dynamic. At any given time during the locating/interviewing period, different sample members were at markedly different stages in the flow. "}, {"section_title": "Pre-CATI Locating", "text": "Base-year data (edited and with derived data elements, where applicable) and locating information (collected from institutional records, updated during base-year interviews, and, where applicable, by a National Change of Address [NCOA] and Telematch operation) were obtained during NPSAS:96. An experimental advance letter was sent to twothirds of the field test sample in December 1996. Extant data from the U.S. Department of Education (ED), including the Central Processing System (CPS) financial aid applicant database (for academic years 1994-95, 1995-96, and 1996-97), Pell grant, and National Student Loan Data System (NSLDS) data, were incorporated into the longitudinal database; address updates for the locator files were also obtained during this process. A standard lead letter was mailed in early April 1997, yielding additional postal service address updates, principally among those in the control group of the field test experiment. This information was then preloaded into the CATI instrument to help guide the interviews and assist in locating the sample members. For some NPSAS:96 nonrespondents and for cases with insufficient telephone number information prior to the start of CATI operations, pre-CATI intensive locating procedures were performed through Equifax, a locating service. Where Equifax tracing was successful, cases were prepared for CATI activities; when unsuccessful, the case was designated for field tracing/interviewing. Only a subset of the cases designated for field operations were actually selected and assigned to the field to contain costs. b."}, {"section_title": "CATI-Internal Locating", "text": "CATI locating activities began in April 1997, following forms clearance, and continued through July 1997. Updated locating information was entered into the CATI record prior to the start of CATI operations. When assigned a case, the telephone interviewer called the telephone number designated by the system as the best number (i.e., the number among all available locator numbers that appears to have the greatest potential for contacting the sample member) and attempted to interview the designated sample member. When the person answering the call said that the sample member could not be reached at that number, the interviewer asked the person how to contact the sample member. If this query did not provide the information needed, the interviewer initiated tracing procedures, using all information available to call other contact persons in an attempt to locate the student. When all tracing options available to the interviewer were exhausted without success, the case was assigned to CATI-external tracing. c."}, {"section_title": "CATI-External Locating", "text": "Cases that were not located during the CATI locating process were submitted to two subcontracted locating organizations outside of CATI, FastData and Equifax. FastData was used first in a batch process to update telephone numbers that were bad (i.e., disconnected, no longer in service, person no longer at number, etc.). This service provided a more economical alternative to in-house Directory Assistance calls and updates were generally Chapter 2 Design and Method of the Field Test provided within 24 hours. Equifax locating, a more labor intensive effort, was implemented only if CATI locating and Fast Data batch locating were unsuccessful. Unresolved cases were reviewed by a CATI tracing specialist. Cases with promising roster lines went back into CATI tracing. Cases that were not resolved were made eligible for field locating/interviewing. d."}, {"section_title": "Field Locating", "text": "The main purpose of the intensive field locating/interviewing effort was to increase the response rate. However, since the costs of conducting these operations were high, field efforts were implemented only when less costly efforts were exhausted. Students were identified as needing field locating/interviewing if they were not located using CATI-locating and centralized intensive tracing. Additionally, sample members who were located by telephone but initially refused to participate were identified. Due to cost constraints, only those cases eligible for field follow-up which were in one of four pre-determined geographic areas were assigned to field staff 2."}, {"section_title": "Instrumentation", "text": "The BPS:96/98 student interviews were conducted by telephone, using CATI technology, and in person, using CAPI technology. In preparation for the development of the CATI/CAPI instrument, a comprehensive set of data elements was developed from a thorough review of the data elements used in BPS:90, their relationship to the base-year data elements in NPSAS:96, the reliability of responses obtained in BPS:90, and their relevance to current research and policy issues. A preliminary set of BPS:96/98 data elements was refined with input from the study's Technical Review Panel as well as from NCES and other Department of Education staff The final set of data elements, presented in appendix D, was approved by OMB prior to the start of data collection. From the set of data elements, it was possible to structure the CATI/CAPI instrument by identifying section topics and determining the progression of items within sections. Individual items were designed with several goals in mind: (1) using NPSAS:96 items when feasible; (2) ensuring consistency with NPSAS:96 items when items were not identical; and (3) identifying and preparing wording for item verifications and probes as necessary. Instrument sections were reviewed on a flow basis by NCES and by selected contractor and subcontractor staff Despite different data collection methods, the CATI and CAPI interviews were programmed identically, using CASES 4.1 software, to facilitate the preloading of full-screen data entry and editing of \"matrix-type\" responses. The CATI/CAPI system presented interviewers with screens of questions to be asked of respondents, with the software guiding the interviewer and respondent through the interview. Inapplicable questions were skipped automatically based on prior response patterns and preloaded information. Wording for probes was suggested when a respondent provided a response that was out of range for a given item. As the CATI/CAPI instrument was being designed and programmed, instrument documentation was entered into an integrated data dictionary system (DDS) which enabled users to subsequently produce deliverable data files with CATI/CAPI variable documentation. In order to minimize the interview burden on respondents, the CATI/CAPI instrument used extant data whenever feasible. For NPSAS:96 respondents, base-year data were preloaded into the CATI/CAPI interview; this dictated the flow of many portions of the interview. Certain questions were asked only if the data were missing from the base year. Other questions used the NPSAS:96 preloads to provide context (e.g., the name of a second school attended during the NPSAS year may appear as a \"fill-in\" prompt to the respondent). In other questions, respondents were asked to update information since the last interview based on preloaded information (e.g., dates of enrollment). For NPSAS:96 nonrespondents, because telephone interview data were not available, a subset of NPSAS:96 items was collected in the first section of the BPS:96/98 interview and used to direct the branching of the interview. Once all CATI/CAPI sections had been programmed, test cases were developed and preloaded for testing the instrument and for training telephone and field interviewers. Project staff and staff from NCES systematically tested the CATI/CAPI instrument prior to the start of interviewer training. Finally, prior to data collection, preload files containing data from NPSAS:96 and the Department of Education databases were prepared and loaded into the CATI/CAPI system to both guide the interview and assist sample member locating efforts. A single, abbreviated instrument was developed for the purpose of interviewing special respondent groups: (1) students who were Spanish speakers with limited English proficiency; (2) students with known hearing-or speech-impairments; and (3) students who could not be reached by telephone or who indicated that they would complete a mailed copy of the instrument but would not participate in a telephone interview. The abbreviated instrument, presented in appendix C, focused on the respondent's postsecondary enrollment history and work experiences. NPSAS:96 nonrespondents also received a set of NPSAS/BPS eligibility questions."}, {"section_title": "3.", "text": "\n"}, {"section_title": "Training of Interviewers", "text": "The field test training program was designed to maximize the trainees' active participation and offered opportunities to work with BPS:96/98 procedures in addition to the instrument. Training manuals included a training guide, an interviewer's manual, and a questionby-question specifications manual. Combined training for telephone interviewers, field interviewers, and supervisors took place in April 1997 and consisted of lecture, demonstration, and hands-on practice exercises with the instrument and online coding modules. The BPS:96/98 training agenda is shown in appendix E. In addition, field interviewers were trained on field specific operations, including the field management system and field tracing procedures."}, {"section_title": "4.", "text": "\n"}, {"section_title": "Telephone Interviewing", "text": "CATI locating and interviewing began after final OMB approval of the data collection instrument was received and interviewer training was conducted. CATI procedures included attempts to locate, gain cooperation from, and interview study sample members by telephone. For NPSAS nonrespondents, NPSAS and BPS eligibility determination were also necessary. A reliability reinterview was conducted for a subsample of respondents. The initial CATI sample consisted of verified FTBs who had been located and interviewed successfully in the NPSAS:96 field test for whom locating information was available. Additionally, sampled NPSAS:96 nonrespondents for whom new or verified locating information was obtained were included in the CATI sample. The remaining sample members became part of the initial field tracing and interviewing sample. Locating information gleaned from the pre-CATI locating sources described above was preloaded for each case. Additionally, previously collected information from NPSAS:96 was preloaded to personalize interviews and to reduce respondent burden. An automated call-scheduler assigned cases in the CATI sample to interviewers based on time of day, day of week, appointment setting, and type of case considerations. Scheduler case assignment was designed to maximize the likelihood of contacting and interviewing sample members. Cases were assigned to various queues for this purpose. Some of the queues included new cases, Spanish language cases, initial refusals, and various appointment queues (firm appointments set by the sample member, appointments suggested by locator sources, and appointments for cases which were initial refusals). For each case, a calling roster determined the names and telephone numbers for the interviewers to call. The roster included school-provided and/or student-provided address information (student permanent, student local, parent, and other contact information) from the NPSAS:96 field test. Up to five roster-lines were preloaded with contact information. New roster-lines were added during CATI tracing operations and CATI-external tracing. Once located, some cases required special treatment. To deal with those who initially refused to participate (including locator sources who acted as \"gatekeepers,\" preventing access to the sample member), certain interviewers were trained in refusal conversion techniques. Spanishonly speaking sample members and their locator sources, primarily located in Puerto Rico, were initially assigned to bilingual CATI interviewers but then transferred to a bilingual field interviewer in Puerto Rico."}, {"section_title": "5.", "text": ""}, {"section_title": "Field Interviewing", "text": "Field locating and interviewing activities began after training was conducted and CAPI systems were operational. CAPI procedures included attempts to locate, gain cooperation from, and interview study sample members either by telephone or in person. The main purpose of the field interviewing, introduced in BPS for the first time during this field test, was to test procedures for increasing the response rate. The field interviewer documented every telephone call or field contact. Field interviewers were provided with a checklist which included example questions to help with tracing operations and demonstrated the correct order in which tracing activities should be performed. The checklist was completed for each case to help identify those sources which were most useful in locating the students. Primary tracing sources included: current or former neighbors, the NPSAS school, past or present employer, social agencies' records, and city and county offices. Secondary tracing sources included Directory Assistance (DA), Chamber of Commerce, public libraries, U.S. Postal Service, and Department of Motor Vehicles (DMV). Other field interviewer miscellaneous sources, useful in some cases, included small town police or sheriff's departments, fire departments or emergency rescue squads, local newspapers, public housing authorities, mobile home park managers, motel staff, probation officers, and permit issuing departments at the city level (new construction). A contact script guided interviewers in soliciting information from various sources. When field interviewers successfully located sample members, they introduced themselves and explained the purpose of the study, referring to the advance letter mailed previously. They then attempted to complete the interview using the same instrument used in the CATI interview. The field staff were supported by a computerized control system that tracked field assignments and captured pending and final result codes. Daily reports tracked the field effort. All students who were finalized as BPS field test CATI nonrespondents were eligible for assignment to the field for field locating and CAPI interviewing. CATI nonrespondents residing in the vicinity of a field interviewer were immediately assigned to the field. As clusters of nonrespondents in the same geographic area were identified, the field manager determined whether it warranted sending a field interviewer. Loss of the primary CATI bilingual interviewer resulted in the assignment of the remaining Puerto Rico cases to the Puerto Rican field interviewer."}, {"section_title": "The Integrated Management System (IMS)", "text": "All aspects of the study were under the control of an Integrated Management System (IMS). The modular structure of the IMS allowed for the streamlining of related tasks and served as a centralized, easily accessible repository for project data and documents. The Management Module of the IMS consisted of tools and strategies to assist the project staff and the NCES project officer in managing the study. Information was accessible via the World Wide Web which enabled secure, online, instant access to daily reports and other project information, as well as providing a means for delivering versions of the CATI/CAPI instrument for testing and review. The BPS home page also utilized the World Wide Web. The home page was a part of the field test experiment that was designed to allow a portion of the field test sample to learn about the study and to allow sample members to update their telephone and address information online. The home page was maintained as a completely separate module of the IMS and only those in the subsample were given the Uniform Resource Locator (URL) needed to access the site. The accessing of the home page was entirely student-initiated and voluntary. The Receipt Control System (RCS) monitored all student-related activities. The RCS allowed project staff to track participation closely, identify problems early, and implement solutions effectively. The Data Dictionary System (DDS) consisted of a set of linked relational files and associated utilities for developing and documenting the survey instruments. The master BPS:96/98 Field Test Report 2-12 dictionary file contained characteristics, long and short labels, and other descriptors for every variable in the instrument. The CATI/CAPI Module was developed within the DDS ensuring that all CATI/CAPI variables were linked to study data elements and that each variable was thoroughly documented. The instrument itself included data edit checks to detect apparently inconsistent responses or when unreasonably high or low values were keyed. The instrument also included several online coding modules (\"user exits\") to collect schools attended, terms of enrollment, industry, occupation, and field-of-study data. The instrument made extensive use of the NPSAS preload data. The RTI Field Case Management System allowed field staff to communicate with RTI staff via a store-and-forward electronic mail system, to transmit completed cases, to receive new cases, and to transmit production time and expense (PT&E) data back to RTI."}, {"section_title": "D.", "text": "\nUpcoding \"Other, Specify\" Items Typically, respondents choose \"other, specify\" options when existing response options are incomplete or when the meaning of an item is misunderstood. \"Other, specify\" also may be selected by the interviewer when it is unclear how a particular response may be categorized into existing response options. In the BPS:96/98 field test interview, particular items included, in addition to the fixed response options, the \"other, specify\" option. In most cases, use of the 'Some group size inequalities resulted from the random variation in the random start point assignment process. BPS:96/98 Field Test Report 4-7 \"other, specify\" option was restricted to items for which additional fixed response options would be needed for administration of the same items in the full-scale interview, that is, new, explicit response categories would be added to selected items based on the distribution of responses obtained in the field test. Subsequent to data collection, all \"other, specify\" responses were evaluated for possible manual recoding into existing categories, or into new categories created to accommodate responses of high frequency, through a process known as \"upcoding.\" Table 4.6 contains a summary of the upcoding results obtained for the field test interview. Relatively few respondents selected the \"other, specify\" response option for any one item. Of the 41 respondents taking licensing exams, none used the \"other, specify\" option, suggesting that the categories provided were adequate to cover the range of licensing exams required. 'Percentage based on the total number of cases for whom the item was applicable. 'Percentage based on the number of \"other, specify\" responses. Problems with the programming code prevented text strings from being stored for upcoding. Note: Analyses based on all cases indicating \"other\" as the response to a particular item, regardless of whether a response was subsequently specified. Some of the items with relatively higher percentages of \"other, specify\" responses were those identifying reasons for different patterns of enrollment in section E: reasons for leaving school, pursuing education beyond high school, attending more than one school, taking time off from school, and transferring from the last known school. A large percentage of \"other specify\" responses was obtained for \"primary reason for enrolling at current school,\" suggesting the item did not have appropriate response categories. For those responses that could not be upcoded into existing categories, the text strings were used to create additional categories to be used in the fullscale interview. "}, {"section_title": "Methodological Experiments and Evaluation Approaches", "text": "Evaluation of field test procedures has obvious implications for possible improvement of procedures for the subsequent full-scale study (as well as for enhancements for subsequent BPS follow-up studies). Each major component of the field test was evaluated. Methodology consisted of both formative and summative evaluations. Formative evaluations were of an ongoing nature, designed to assess tasks at intermediate stages so that the effects of employing alternate methodologies could be analyzed and modifications could be made and assessed prior to task completion. Summative evaluations will be used to optimize procedures in the full-scale study. A summary of BPS:96/98 field test evaluations that were planned and implemented is provided in table 2.2. Various measures were employed to assess the quality of data collection including quantitative monitoring, qualitative monitoring, quality circle meetings, and verification of field interviews. The primary objective was to pinpoint any problems with the interview and correct them early in data collection. Quantitative monitoring assessed the quality of the telephone interviewing, with respect to question delivery and coding of responses. It is explained in greater detail in Chapter 4. Qualitative monitoring evaluated whether interviewing procedures were implemented as intended and were effective. The utility of the interview items was also assessed. On occasion, monitoring revealed the need for retraining (e.g., explanation of the nature of the study or refusal avoidance) which was conducted immediately. Weekly quality circle meetings, during which interviewers, supervisors, and project technical staff met to discuss data collection issues, proved valuable in identifying problems with the instrument as well as in building rapport among interviewers and technical staff Summaries of the meetings were rapidly disseminated to all interviewers'and supervisors so that those who were unable to attend also benefited.  Analyze outcomes and costs/benefits of CATI-external tracing activities. Field locating/interviewing Assess the effectiveness of the field case management system and information flow between the central site and the field. Verify the quality of field interviewing data with verification reinterviews for a subsample of cases. Analyze the outcomes, costs, and benefits associated with field locating and field interviewing, both differentially and combined, in terms of locating success and CATI/CAPI response success."}, {"section_title": "CATI administration/ data quality", "text": "Analyze silent monitoring quality control data. Analyze CATI operational parameters (e.g., numbers of calls per case, total interviewer hours per completed interview). Debrief interviewers, refusal converters, bilingual interviewers, monitors, and supervisors. Analyze administration time statistics, overall, within section, and for individual questions and blocks of questions. Analyze rates of interview nonresponse, early and subsequent break-off, types of response inconsistencies detected during interview administration, and nonresponse patterns. Analyze response temporal stability of reinterviews for selected items (subsample). Validate student responses against federal aid applicant data. Assess feedback from mock interviews conducted with Technical Review Panel (TRP) members."}, {"section_title": "Online coding", "text": "Analyze success/accuracy of on-line coding of Integrated Postsecondary Education Data Systems (IPEDS) ID, industry/occupation, and major course of study."}, {"section_title": "File development", "text": "Observe and document any procedural difficulties encountered in preparation of raw CATI/CAPI files. The study design included a component for direct evaluation of data quality. A reliability reinterview was administered to a randomly selected subsample of field test respondents to assess the short-term stability of selected items. The results of the reinterview analysis are presented in Chapter 4. A different set of quality control procedures was used for the field data collection. For a sample of each field interviewer's completed cases, the field supervisor contacted the sample member using locating data collected in the interview, verified that the interview actually occurred, the date it took place, and the approximate duration. A small number of factual questions from the interview were asked again, and the responses compared to those originally recorded. The field test included a methodological experiment, evaluated for possible implementation in the BPS:96/98 full-scale study. The objective of the experiment was to keep locating information current and to encourage students to participate in the BPS CATI interview. All sample members, including the control group, were sent a letter informing them of the study immediately prior to data collection. In addition, approximately four months prior to the start of data collection, an advance mailing was conducted. Sample members in the partial treatment group received an advance letter, the standard BPS study brochure, and an address correction/verification sheet. Those in the full treatment group were sent the BPS e-mail address, a different advance letter and BPS brochure that advised them how to access the BPS:96/98 student home page, an address correction/ verification sheet, and a BPS magnet. Sample members in the treatment groups were asked to return the address correction/verification sheet if their locating information had changed. A student home page was developed and the full treatment group was provided the Uniform Resource Locator (URL) of the BPS home page to enable them to learn more about the study and contact us by sending e-mail directly from the home page. Results of the experiment are presented in Chapter 3. Attaining the participation rates required for BPS:96/98 demands high levels of cooperation at all stages of the survey process. The first sections of this chapter address the various aspects of obtaining the necessary participation outcomes, including locating of sample members, telephone interviewing response rates, refusal conversion, and field interviewing. The remaining sections focus on interview burden and effort and the field test experiment. A."}, {"section_title": "Overview of Locating and Interviewing Outcomes", "text": "The overall locating and interviewing results are shown in figure 3.1. Of the 785 students in the original sample, 599 were located and contacted and 37 were excluded (out of scope) because they were deceased, incarcerated, out of the country or incapacitated. Among the contacted subsample, 491 were interviewed, 484 of whom were verified First-Time Beginners (FTBs). This effort included a limited field locating and interviewing operation. To contain costs for the full-scale study, only four geographic areas were included in the field effort. The time frame for data collection was limited to 12 weeks. The raw contact rate for the field test was 80.1 percent (599/748). Exclusive of those either out of scope or for whom locating was limited to contain costs, the contact rate was 90.5 percent. For those contacted, the raw interview rate was 82.0 percent (491/599). Discounting those for whom full procedures were not applied due to cost constraints, the interview rate was 86.1 percent. The overall response rate, factoring in the reduced field test effort, was 78.0 percent (90.5 * 86.1). Higher response rates are expected in the full-scale study due to the longer data collection period and much broader geographic coverage for the field effort. Locating and interviewing rates were related to two examined factors: NPSAS:96 response status and type of NPSAS:96 school. Contact rates by NPSAS:96 response status, shown in table 3.1, were 16 percentage points higher for NPSAS respondents than for NPSAS nonrespondents. Interviewing, given contact, was similarly more difficult with the NPSAS nonrespondents. One of the observations from the field test was the difference in BPS:96/98 locating and interviewing rates between NPSAS partial respondents and NPSAS full respondents. Relative to NPSAS full respondents, we had expected the marked difference for nonrespondents and some difference for partial respondents, but the size of the difference was surprising. In many cases, the NPSAS partial respondents had refused to participate initially or had been difficult to contact by phone during the NSPAS:96 field test.    Contacting and interviewing rates by type of school are presented in table 3.2. As in past studies, students from private, for-profit institutions continue to be the most difficult to locate. Interviewing rates, given contact of sample number, varied by type of institution from 56 percent to 90 percent. The small sample size for some institution types contributed to the broad range of interviewing rates. "}, {"section_title": "Locating Prior to Data Collection", "text": "As indicated in Chapter 2, base-year data and locating information collected during the NPSAS:96 data collection were updated through a National Change of Address and Telematch operation. Additionally, data from the U.S. Department of Education, including loan application and financial aid information were incorporated into the locator database for BPS:96/98. At the start of BPS:96/98, 49 cases with insufficient telephone number information required pre-CATI intensive locating. These cases were sent to Equifax for this activity. Following the Equifax pre-CATI locating operations, 20 cases were identified for pre-CATI field operations. In addition to Equifax and pre-CATI field operations, locating information was obtained from an advance mailing which included a request that the sample member review, correct and return an address update sheet. During the week prior to data collection, we sent out the advance mailing to 733 sample members. We received address update reply sheets from 29 sample members. We received forwarding information for 33 cases. The mailing was returned undeliverable, with forwarding information for 3 cases. Another 51 mailings were returned, with no new information."}, {"section_title": "2.", "text": "\nInterviewer Hours A total of 1,465.5 telephone interviewer hours (exclusive of training, supervision, monitoring, administration, and quality circle meetings) were expended to obtain completed interviews from 375 sample members. This represents 3.91 hours per completed interview. Initially (for the first 150 completed interviews), the average number of hours per completed interview was approximately 2.0, but as the pool of potential respondents shrank the average time per interview increased steadily. Since the time to administer the interview was 22 minutes, on average, the large majority of interviewer time was spent in other activities. A small percentage of this time was required to bring up a case, review its history, and close the case (with appropriate reschedule, comment, and disposition entry) when completed. The bulk of the time, however, was devoted to locating and contacting the sample member. Consequently, any attempts at marked improvement in interviewer efficiency must address greater efficiency in CATI locating and interviewing (reflecting the considerable attention paid throughout this report to methods to improve efficiency).\nPersonal and Work Characteristics while Enrolled Table 4.2 contains measures of response consistency for current marital status, number of dependents, and work characteristics during the current enrollment period if the respondent is currently enrolled, or during the most recent enrollment period for those no longer enrolled. Temporal stability for current marital status and number of dependents was very high based on both percent agreement and the relational statistic. Lower percent agreement and the corresponding relational statistic for the item asking about access to the Internet, although still fairly high, may reflect changes in respondent access over time, particularly if the respondent was first interviewed on campus and reinterviewed while at home on summer vacation.  Among those who worked either while enrolled or during vacations, temporal stability of responses reporting the importance of work to financing the respondent's education was also very high. Questions asking about the amount of time worked by the respondent both while enrolled and during vacations showed only fair temporal stability both in terms of the percent agreement and the relational statistics. This is probably due to the nature of the response options offered respondentsthat is, respondents were asked to report whether they worked none, some, most or all of the time during enrollment and during vacations. Respondents might have had difficulty differentiating working \"some of the time\" from working \"most of the time.\" For the full-scale interview, there will be only three response options for this item: none, some, and all of the time. 3."}, {"section_title": "Intensive Locating During Data Collection", "text": "Cases for which the CATI preload locating information failed to result in contact required various intensive tracing steps. Table 3.3 presents the results of intensive tracing activities for the field test. Cases that were not located during the CATI operations were sent to FastData for telephone number updates and to Equifax when FastData did not yield a telephone number. A total of 120 cases were sent to FastData and 120 cases were sent to Equifax. Only cases that were unlocatable following Equifax tracing, or not sent to Equifax due to time constraints, were attempted using in-house tracing. These cases were not attempted in field tracing due to time constraints. 'Total is broken down into only two subgroups, those contacted (using all applicable steps) and those not contacted. bSixteen of these cases were part of the group which went directly to field interviewing (no CATI attempted). Not applicable indicates this is the last step taken."}, {"section_title": "Note:", "text": "Details do not sum to totals because some cases were attempted by more than one method. "}, {"section_title": "Refusal Conversion and Partial Responses", "text": "Efforts to gain cooperation from sample members during the field test included refusal conversion procedures. When a case initially refused to participate, the case was referred to a refusal conversion specialist. There were 182 initial refusals, 30 percent of the total 599 cases contacted. The refusal conversion specialists called the sample members to try to gain full cooperation with the interview. When full cooperation could not be obtained, an abbreviated interview (see appendix C) was attempted to obtain key information. Fifty percent (91 sample members) of the initial refusals were converted. Of the 484 verified FTBs who were interviewed, full interviews were completed for 421 sample members, partial interviews were completed for 26 sample members, and abbreviated interviews were completed with 37. An interview was considered a partial interview if at least section B (enrollment information) of the main interview was completed, but not the full interview. D."}, {"section_title": "Field Locating and Interviewing", "text": "Field locating and interviewing results are displayed in table 3.4 for the 102 cases assigned to the field. Two additional cases were assigned to field but were determined to be exclusions. Sixty-eight percent of the field cases were contacted, and 73 percent of those contacted in the field were interviewed. The \"field only\" cases consisted of those going directly to the field, i.e., not worked in CATI. Of the 30 field only cases, 18 were contacted and 14 were interviewed. Review of the CATI refusals in early June identified 23 cases in close proximity to a field interviewer's geographic area. Refusal conversion was expected to be more effective when done in person. Only three of the cases resulted in conversions. Many of the cases were located too far from the interviewer's home base to warrant the costs associated with travel. Additional phone calls were not successful in securing an interview. An additional 22 cases were identified for whom no Chapter 3 Student Locating and Interviewing Outcomes viable phone number was obtained in CATI or intensive tracing and who were located either in the same state or a neighboring state as a field interviewer. Because these were the last to be assigned, there was very little time to work the cases. Cases located in close proximity to one another were resolved at a higher rate than those residing farther apart. In addition, 27 (additional one identified as an exclusion) Spanish-speaking cases were assigned to the field (Puerto Rico), since most of these sample members were located in the same geographic area where a field interviewer was already conducting field locating and interviewing. The interviewer was able to administer the English CAPI in Spanish and interact with neighbors and family to obtain more current locating information. The interviewer completed a total of 24 interviews plus identified an additional exclusion. This clearly points to the critical importance of geographic clusters. Unlike the other field cases, the majority of these sample members lived in the same city or metro area, allowing for efficiencies of travel by the interviewer. Greater geographic coverage along with a longer data collection period are expected to yield greater success in the full-scale field effort."}, {"section_title": "E.", "text": ""}, {"section_title": "Interview Burden and Effort", "text": "The major variable expenses for CATI locating and interviewing involve interviewer time and toll charges, which are considered here. Telephone interviewer shifts were staffed to optimize likelihood of contact as well as toll charges. The time to administer the BPS:96/98 field test instrument, the hours per completed interview, the number of telephone calls, and analysis of help text use are presented in this section."}, {"section_title": "1.", "text": ""}, {"section_title": "Timing", "text": "Time to administer the BPS:96/98 field test interview, overall and by section, as well as by NPSAS response status, is shown in table 3.5. Timing results by NPSAS:96 institution level and control are provided in table 3.6. The principal utility of the timing analyses is to provide empirical data on the time to administer the field test instrument in order to pinpoint inefficiencies and make appropriate modifications to the full-scale instrument. The average administration time for the field test interview was 22 minutes. This is considerably less time than the 30 to 35 minutes estimated for the field test interview.' Given this figure, along with plans to eliminate a number of items asked in the field test, the average administration time for the full-scale interview is estimated to be 20 to 25 minutes."}, {"section_title": "3-7", "text": "Chapter 3 Student Locating and Interviewing Outcomes Section A was skipped for NPSAS respondents. b Some questions in section A were skipped for NPSAS partial respondents, based on preloaded information. Note: Includes all cases for whom the specified section was completed (in one or multiple sessions) and for whom complete timing data were available. On average, NPSAS:96 nonrespondents took 13 minutes longer to complete the interview than NPSAS:96 respondents. Half of that time was due to section A which was skipped by NPSAS respondents. The rest of the time was spread out fairly evenly over the seven remaining sections, with NPSAS nonrespondents taking roughly one-third longer per section than respondents. The time for section B, enrollment history, was a considerable improvement over that in past interviews. The path of the interview allowed those who were continuously enrolled to skip the enrollment user exit (n=105). Their average time in section B was 0.8 minutes. For the rest of the respondents who were required to provide full enrollment information, the enrollment grid was simplified to ask for continuous spells rather than a term by term accounting. For those who went through the enrollment user exit (n=342), the enrollment grid took, on average, 1.7 minutes and their average section time was 3.1 minutes. As shown in table 3.6, respondents at 4-year schools had the shortest times in section F while respondents at less-than-2-year schools took the longest. This was expected since the questions dealt with employment after graduating/leaving school and those enrolled in shorter programs were more likely to have completed school. These questions were skipped for those still enrolled. Respondents at less-than-2-year schools had faster times in section G. This may be due to their being asked the short series of questions about their job preparation rather than the longer sequence asking how often they did various education-related activities.     Table 3.7 shows that students who attended multiple postsecondary schools took longer to complete the interview. Much of the additional time was spent in section B, completing the enrollment grid and answering additional questions for multiple schools. Their time in section F, employment after leaving/graduating and employment before postsecondary school, was significantly shorter, suggesting that those who had attended multiple postsecondary schools were often still enrolled, thus skipping over the post-enrollment questions. "}, {"section_title": "Number of Calls", "text": "A total of 17,466 telephone calls were made during the field test, with an average of 22 calls per sample member. Those who were contacted (i.e., the interviewer spoke with the student in question), were phoned an average of 20 times. Those who were interviewed (partial and completed interviews) were called 16 times, on average. An average of 15 calls were made to those who completed the interview. Table 3.8 shows the number of calls, including breakdowns by institution level and control. Of the 17,466 calls made, roughly 35 percent reached an actual person, 26 percent reached an answering machine, and 39 percent were other non-contact calls (busy, ring no answer, fax line, pager, etc.)."}, {"section_title": "Help Text", "text": "Online help text was available for every screen in the CATI/CAPI instrument. Having additional information available at the touch of a key was beneficial to interviewers, 'particularly at the beginning of the field test, to immediately alleviate any confusion with questions while still on the telephone with the respondent. Counters were used to determine the number of times each help screen was accessed, making it possible to identify items that were confusing to the interviewer and respondent. This information will be used to reword problematic questions, eliminating confusion and improving reliability in the full-scale study. An analysis of the number of help text accesses revealed ten items for which the help text was accessed five or more times. These results are shown in table 3.9. Four of these items are income questions which, as expected, had high indeterminacy rates as well. Most of the income questions included conversion formulas in the help text in the event that the respondent knows, for example, his hourly wage but not his monthly earnings. One of the income items referred to \"salary\" but did not explicitly ask for an annual amount; adding the word \"annual\" to the question text is recommended to avoid confusion. Amount of monthly rent or mortgage payment may have been problematic for those living in dormitories; this item should be skipped for those living in school-owned housing. Future work plans and highest level of education ever expected involve speculation and may, therefore, have been difficult. The person who helped most during their first year of postsecondary school was ambiguous; respondents and interviewers were unclear whether the question referred to financial, emotional, or other type of support and accessed the help screen for clarification. It is recommended that intention be clearly stated in the question or this item be dropped from the full-scale instrument.  Table 3.9 Items for which help text was accessed five or lilt ore ti 11,1, es "}, {"section_title": "Field Test Experiment", "text": "As mentioned in Chapter 2, an experiment was performed as part of the field test to evaluate the effect of (1) an advance student mailing several months prior to data collection, and (2) access to a BPS student home page on the WWW (including a BPS e-mail address) on locating and participation rates. The objective was to keep locating information current and to encourage students to participate in the BPS CATI interview. The sample members were assigned to one of three experimental treatment groups (see appendix B for examples of mailing materials): Control group. Students received no special treatment."}, {"section_title": "2)", "text": "Letter only group. Students received an advance letter, the standard BPS study brochure, and an address correction/verification sheet."}, {"section_title": "3)", "text": "Letter and home page group. Students were given the BPS e-mail address, a different advance letter and BPS brochure that advised them how to access the BPS:96/98 student home page, an address correction/verification sheet, and a BPS magnet which specified the Uniform Resource Locator (URL) for the student home page. A randomized block experimental design was used to allow removal of the effects of differences between potential confounding variables (blocks) from analyses of differences between treatments (Group 1 versus Groups 2 and 3 together, and Group 2 versus Group 3)."}, {"section_title": "EST COPY VAILABLE", "text": "BPS:96/98 Field Test Report"}, {"section_title": "3-13", "text": "The blocks controlled for potential sources of differences in BPS response rates other than the treatment effects. After identifying blocks, treatments were randomly assigned to experimental units (sample members) within blocks. The hypotheses being tested were that the experimental treatments would increase both locating and response rates, relative to the control group, and that these rates would be greater in the home page group than the letter-only group. The results, however, proved disappointing. Sample members were evenly divided among the three groups. Of the 262 \"full treatment\" sample members who were sent the URL for the BPS:96/98 student home page, there were a total of 13 \"hits\" to the Web site.' No e-mail was received from any of the sample members who were sent this information. No data were submitted using the address update sheet on the BPS:96/98 student home page. A small number of students who were part of the treatment groups did return update sheets by mail. The first hypothesis, whether treatment had an effect on contacting, was first tested on Group 1 versus Groups 2 and 3 and then repeated for Group 2 versus Group 3. It was theorized that Groups 2 and 3 together would have higher contact rates than Group 1, and that Group 3 would have better contact rates than Group 2. In both cases, the analyses showed that treatment had no effect on contacting. The next test was whether treatment had an effect on interviewing. Again it was thought that Groups 2 and 3 combined would have higher interviewing rates than Group 1, and that Group 3 would fare better than Group 2. Again, in both cases, the analyses showed that treatment had no effect on interviewing. The advance mailing and student home page were found to be ineffective for this study and, given these results, it is recommended that both be eliminated from the full-scale study. Time and resources should be directed to more effective means of locating and interviewing sample members 2 Multiple hits in the same day from the same IP address were only counted once. "}, {"section_title": "Reliability Reinterviews", "text": "As in previous BPS data collections, the BPS:96/98 field test included a reinterview, administered to a randomly selected subsample of BPS respondents in order to assess the shortterm temporal stability of selected items. Across BPS data collections, the reinterview is designed to target new items, revised items, and items not previously evaluated either as part of a prior BPS reinterview or the NPSAS base year reinterview. Moreover, only data items that were expected to be stable for the time period between the initial interview and the reinterview (i.e., factual rather than attitudinal data) were selected. BPS respondents who agreed to participate in the reinterview process were contacted eight to 14 weeks after completing the initial interview. A total of 73 respondents agreed to participate in the reinterview; of those, 67 (92 percent) completed the reinterview. In the tables shown below, respondent sample sizes vary due to applicability of the item and indeterminate responses.' In the reinterview instrument, information from the initial interview was preloaded in order to ensure that reinterview questions were asked for the same school or the same job across the two interviews. Percent agreement and appropriate correlational analyses were used to estimate response stability between the two interview administrations. Lack of agreement (or low correlation) between responses from the same individuals reflects instability over short time periods due to measurement error. To the extent this occurs, items need to be deleted or revised prior to administration in the full-scale interview. In contrast, high indices of agreement suggest that interview responses were relatively free of measurement errors that cause response instability over short periods of time. Items on the reinterview included nominal, ordinal, and continuous variables. Percent agreement was computed for nominal and ordinal variables based on the number of responses that were exactly the same in both interviews; for continuous variables, percent agreement was based on the number of paired matches within one standard deviation unit of each other. One of three relational statistics was used, depending on the properties of the particular variable: (1) Cramer's V statistic for items with discrete, unordered response categories, (2) Kendall's Tau coefficient for items with discrete, ordered response categories, and (3) Pearson's product moment correlation coefficient for the continuous measures such as number of dependents. 'Analyses were restricted to cases with determinate responses in both the initial interview and the reinterview."}, {"section_title": "Enrollment Information", "text": "Reliability indices for enrollment information since the NPSAS:96 base year interview are presented in table 4.1. Reports of current enrollment at the last known school and of enrollment at any other postsecondary institution since the base year interview were highly stable across the two interview administrations, as measured both by the percent agreement and the correlational statistic. Similarly, temporal consistency was also fairly high, despite the small sample sizes, for transfer to another school, program completion, and enrollment in summer sessions. Reports of continuous enrollment, however, showed high percent agreeMent but somewhat lower temporal stability as measured by the correlational statistic (0.44), reflecting the sensitivity to small systematic changes of the responses examined. bAsked of respondents who had been enrolled as of the end of the NPSAS sample year. Asked of those not still enrolled at the last known school. dAsked of those not still enrolled at and who did not transfer from last known school. Asked of those still enrolled at the last known school. Note: Respondent sample sizes vary due to applicability of the item and indeterminate responses. Analyses were restricted to cases with determinate responses in both interviews. Percentage agreement is based on the number of responses that were exactly the same in both interviews. Source: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students:96/98."}, {"section_title": "Student Loans and Expenses", "text": "Items pertaining to respondents' borrowing plans, repayment history, and current monthly expenses were also evaluated during the field test reinterview, and the results are shown in table 4.3. Percent agreement for items asking about borrowing was high, with consistent relational statistics for borrowing for future education and current repayment of loans. The relatively lower relational statistic for the item asking about loans already repaid, together with observations of the BPS:96/98 Field Test Report 4-3 administration of the item during interviewing, identified potentially vague wording that will be revised for the full-scale interview. "}, {"section_title": "Indeterminate Responses", "text": "Allowances were made in the CATI /CAPI interview to accommodate responses of \"don't know\" and refusal to every item, by special keyed entry (i.e., F3 and F4) by the interviewers. Refusal (RE) responses to interview questions are most common for items considered sensitive by the respondent, while \"don't know\" (DK) responses may result from a number of potential circumstances. The most obvious reason a respondent will offer a DK response is that the answer is truly unknown or in some way inappropriate for the respondent. DK responses may also be evoked (1) when question wording is not understood by the respondent, without explanation by the interviewer; (2) when there is hesitancy on the part of the respondent to provide \"best guess\" responses, with insufficient prompting from the interviewer; and (3) as an implicit refusal to answer a question. RE and DK responses introduce indeterminacies in the data set and must be resolved by imputation or subsequently dealt with during analysis; to the extent possible, they need to be reduced. Summaries of DK and RE responses for the BPS:96/98 field test are provided, by interview section, in table 4.4. Item refusal rates were extremely low for the entire interview, with only two sections, D and H, evoking refusal responses on more than ten percent of the items. These sections have historically produced higher refusal rates than any other interview sections. Section D collects respondent financial information including financial aid, loan repayment, and monthly debts. Section H collects contact information for future follow-up BPS:96/98 Field Test Report 4-4 interviews. Some of the information collected, such as driver's license and Social Security numbers, tend to be considered personal and are typically refused at higher rates than other items. The maximum number of refusals (16) and percentage (64 percent) were both observed in section H. DK rates were also very low for the interview, evoking DK responses in less than 25 percent of interview items. The highest rate of DK responses was observed in section G, which collected information about the respondent's experiences during the first year of postsecondary education. Two items in section G evoked high numbers of DK responses. The first asked who helped the respondent most during his/her first year of study. The second asked about the respondent's plans for additional postsecondary study. The section F item asking about income for the first job held after postsecondary education evoked the highest number of DK responses (39 percent), most likely as an implicit refusal to respond, while address information for the second parent (when they do not reside together) evoked the highest rate of DK responses (55 percent). C."}, {"section_title": "Order Effects", "text": "The BPS:96/98 field test interview asked sample members for frequency ratings (i.e., never, sometimes, often) for each of ten subitems pertaining to activities and experiences during the respondent's first year of study at the NPSAS school. Responses to such \"ratings questions\" can be contaminated by changes in response propensities for individual subitems depending upon the order in which they are administered. These changes can result from general factors (such as the respondent becoming familiar with the response options or the tendency for responses to become less extreme over a sequence of rankings), or from factors more specific to the subitems themselves (such as changes in the interpretation of the meaning of a particular subitem within the context of previously administered subitems). To control for order effects in the administration of this item, sequential subitems were presented to respondents in the field test with a random start point within the sequence. Prior to data collection, starting points were allocated randomly in an attempt to equalize the distribution of start points of those who responded. Within the ten subitems, the joint distributions of responses by different random start points were examined using the x2 test of independence. A significant value of the x2 statistic indicates differential conditional distributions and suggests order effects. Because multiple tests were to be performed, a significance level of .005 was adopted. BPS:96/98 Field Test Report 4-5 51.  'Percent is based on number of cases for whom the question was applicable (i.e., reaching the point in the interview, not legitimately skipped, and not determined \"not applicable\"). It should be noted that, under this definition, the maximum percentage reported may not correspond to the same question as maximum count reported. Note: Statistics are provided for the number and percentage of items within each section for which any refusal (RE) or don't know (DK) response was given, and for maximum item-level RE and DK counts and rates for respondents. Respondent-based rates are calculated only for those sample members for whom each item was applicable and asked; as such, maximum counts and rates do not necessarily apply to the same items. Analytic results are shown in table 4.5. Up to 328 respondents (restricted to those respondents who provided a determinate response for the subitem) contributed to each analysis, with approximately 33 cases per random start point.2 The x2 values were distributed around the expected value (equivalent to the degrees of freedom, 18) of the applicable x2 distribution, suggesting no major order effects. So as to account for possible order effects, random start points will continue to be used for administration of the item in the full-scale interview. Separate X2 analyses were conducted for each activity listed, examining the joint distribution of random start point by response option. With 10 possible starting points and three response options, degrees of freedom were 18 for all X2 values. Expected cell frequencies less than five were observed in eight of the ten analyses, including one case for which the X2 value was statistically significant (received lower grades than expected). This significant result is assumed to be artifactual since the effect of low expected frequencies is to artificially inflate the x2 statistic. "}, {"section_title": "CATI Quality Circles", "text": "Regularly scheduled quality circle meetings, during which interviewers, supervisors, and project technical staff met to discuss operational issues, were planned components of the field test operations and evaluation. The purpose of the meetings was to provide an open forum for discussion of issues pertinent to locating and contacting respondents, and conducting CATI interviews. Telephone interviewers attended the quality circle meetings on a rotating basis so that all staff had the opportunity to attend at least one meeting every three weeks. Summaries of discussions and decisions were promptly distributed to all telephone interviewers and their supervisors so that those who did not attend the meeting could also benefit."}, {"section_title": "Some of the issues covered in quality circle meetings for the field test included:", "text": "Instrument item wording and logic: Ambiguous items, such as income while enrolled, were discussed, and concerns about the instrument path logic were addressed, resulting in modifications made to the instrument based on telephone interviewer input. An example clarification was that the years of 94-95 refer to the 1994-1995 school/academic year, rather than the period from 1/94 to 12/95. User exits: Early in data collection, interviewers encountered some difficulties maneuvering within the CATI user exit screens. The addition of online help text for user exits, including information about moving around the screen, along with increased user exit training, is recommended for the full-scale study. Help text: This feature provides additional explanation for each item. Interviewers used this to verify the intent of an item in the instrument. Interviewers could also access student information with the Fl function key. An online calculator was added to facilitate computation of respondents' salaries. Locating information: The need to gather correct locating information in section H was emphasized. Proper ways of gathering locating information were explained further. The locating process was refined (for maximal full-scale study benefit) based on problems identified during the field test data collection period. Due to a small sample size, the scheduler set frequent call backs, resulting in a very high average number of calls per completed interview."}, {"section_title": "F.", "text": ""}, {"section_title": "Quality Control Monitoring", "text": "Monitoring telephone data collection serves a number of goals, all aimed at maintaining a high level of data quality. These objectives are to obtain information about the interview process that can be used to improve the design for the full-scale study; to provide information about the overall data quality; to improve interviewer performance by reinforcing good interviewing behavior and discouraging poor behavior; and to detect and prevent deliberate breaches of procedure, such as data falsification. BPS:96/98 Field Test Report 4-9 CATI monitoring was conducted during the BPS:96/98 field test data collection using the RTI telephone monitoring system. The system provides for sampling of interviewers and interview items during CATI operations. Monitors listen to and simultaneously view the progress of the interview on screen, using remote monitoring telephone and computer equipment. They record their observations on laptop computers which contain computerized monitoring forms. Monitors listened to up to twenty questions during an ongoing interview and, for each question, evaluated two aspects of the interviewer-respondent interchange: whether the interviewer delivered the question correctly and keyed the appropriate response. Each of these measures was quantified and daily, weekly, and cumulative reports were produced. Monitoring took place during the first seven weeks of the field test and a total of 279 items were monitored during that time. While this sample is too small to draw conclusions, the general trend was a high number of errors in question delivery initially, tapering down as interviewers were retrained and gained experience. The number of CATI entry errors was quite low throughout the course of monitoring. G."}, {"section_title": "Recommendations for the Full-Scale Study", "text": "After completion of field test data collection and prior to the Technical Review Panel (TRP) meeting, TRP members were called by telephone interviewers to experience the CATI instrument first-hand. These interviews, along with discussion at the TRP meeting, resulted in a number of valuable recommendations for the full-scale study. Some suggested instrument revisions included item wording changes: removing references to \"allowance,\" changing \"importance of job in financing education\" to \"could you have attended school if you had not worked,\" and limiting pre-enrollment questions to the five years prior to enrolling. It was suggested that the series of financial aid questions be replaced by a set of yes/no questions by academic year, addressing grants/scholarships, student loans, work study, and employer tuition aid. Response option modifications and clarifications for a number of items, particularly new items for which there were incomplete response categories, are expected to improve data quality and efficiency in the full-scale data collection. Questions with many response categories should be given extra attention during interviewer training. Elimination of a number of items was suggested: date left school (available from enrollment grid), greatest obstacle to staying in school the first year, person who helped most during first year of postsecondary school, frequency of advance preparation for classes, and frequency of preparing more than one day in advance of final exams. A higher response rate than that achieved in the field test is imperative to the success of the full-scale study. Recommendations for improving the response rate include an advance mailing to parents to gain their cooperation, better locating of respondents through the use of an in-house specialized tracing operation, a shorter interview (20 to 25 minutes rather than the 30 to 35 minutes advertised for the field test), use of the abbreviated instrument when the interviewer believes it is the last opportunity to gain cooperation from the student, a greater proportion of BPS:96/98 Field Test Report 4-10 experienced interviewers including a specially trained group of refusal converters, and the identification of difficult cases (e.g., NPSAS nonrespondents) who will be handled only by experienced interviewers, refusal converters, or sent to field interviewers. The evaluation of the BPS:96/98 field test allows the opportunity for positive adjustments to the design and implementation of the study. The full-scale survey will reap the benefits of this evaluation. Given the longitudinal nature of the BPS series, later studies should also receive direct benefit from the BPS:96/98 field test. Your participation is important to the success of BPS and adds great value to education research and policy making. As you may remember, only a limited number of individuals were selected for the study. Therefore, you and each of the others selected represent hundreds of similar students who first entered a college or vocational school at the same time you did. The information provided through BPS serves as a vital resource for educators and policymakers as they address issues concerning the quality of education, the effect of that education on the lives of individuals, and the most productive way to support participation in postsecondary schools. An interviewer from RTI will call to conduct a telephone interview with you sometime in the near future. During the interview you will be asked questions about such things as your education, the school(s) you attended or are attending, your employment experiences both while in school and after, how you financed your education, and your goals and aspirations. NCES and its contractors adhere to strict confidentiality standards in protecting the privacy of individuals involved in our studies. Stringent measures are in place to safeguard the confidentiality of participants during the collection, analysis, and reporting of all survey data. NCES is authorized by federal law (P.L. 103-382) to conduct the Beginning Postsecondary Students Longitudinal Study. BPS collects data about the education and employment experiences of people who have continued their schooling after high school. NCES will authorize only a limited number of researchers to have access to information which could be used to identify individuals. They may use the data for statistical purposes only and are subject to fines and imprisonment for misuse. Data will be combined to produce statistical reports for congress and others. No individual data will be reported. Your participation is strictly voluntary. However, we do need your help in collecting these data, as you were selected to represent thousands of others like yourself. Your responses are necessary to make the results of this study accurate and timely. BPS:96/98 Field Test Report B-3"}, {"section_title": "INTRODUCTORY LETTER, PAGE 2", "text": "According to the Paperwork Reduction Act of 1995, no persons are required to respond to a collection of information unless it displays a valid OMB control number. The valid OMB control number of this information collection is 1880-0631, and it is strictly voluntary. The time required to complete this information collection is estimated to average 35 minutes/32 minutes per response, including the time to review instructions, search existing data resources, gather the data needed, and complete and review the information collection. If you have any comments concerning the accuracy of the time estimate or suggestions for improving the form, please write to: U.S. Department of Education, Washington, DC 20202-4651. If you have comments or concerns regarding the status of your individual submission of the form, write directly to: National Center for Education Statistics, 555 New Jersey Avenue, NW, Washington, DC 20208. Enclosed you will find a brochure with a brief description of BPS as well as greater detail about how you were selected and the confidentiality procedures which are in place. If you have any questions about the study or would like to set up an appointment to be interviewed, please call Marti Nash at RTI. The tollfree number is 1-800-647-9674. We thank you in advance for your participation in this important study. Your cooperation is greatly appreciated. Sincerely, Pascal D. Forgione Commissioner NOTE: Arrangements have been made to allow the participation of persons who are hearing or speech impaired. Call us (toll free) at 1-800-647-9659 (TDD). 50% of the beginning students completed a degree or certificate within five years 13% were still enrolled after five years and had not completed a degree 37% had not completed any degree and were no longer enrolled after five years 58% of beginning students received financial aid; students who received aid were more likely to finish their degree programs in five years than those who did not receive aid beginning students who did the following were more likely to complete their degree programs: started college immediately after high school attended full time enrolled continuously in all terms 29% of beginning students left the college where they started and transferred elsewhere; those who transferred were just as likely to complete a degree program as those who did not transfer 92% of the beginning students held a job at some time while they were enrolled. For BPS, students included in NPSAS who had just started their postsecondary education will be interviewed at two-or three-year intervals throughout their education and into the work force."}, {"section_title": "WHY IS BPS CONDUCTED?", "text": "Congress and other policymakers use BPS when they consider how new legislation will affect college students and others in postsecondary education. students who received aid were more likely to finish their degree programs in five years than those who did not receive aid a beginning students who did the following were more likely to complete their degree programs: started college immediately after high school attended full time enrolled continuously in all terms 29% of beginning students left the college where they started and transferred elsewhere; those who transferred were just as likely to complete a degree program as those who did not transfer 92% of the beginning students held a job at some time while they were enrolled. Working less than 20 hours a week while enrolled did not have an adverse effect on degree completion. Your participation is important and adds great value to education research and policy making. Thousands of students across the country have taken part in past BPS studies, and we sincerely hope that you will continue to do so as well. The information provided through BPS serves as a vital resource for educators and policymakers as they address issues concerning the quality of education, the effect of that education on the lives of individuals, and the most productive way to support participation in postsecondary education."}, {"section_title": "EXPERIMENT LETTER PARTIAL TREATMENT", "text": "In NPSAS, we obtained information on how to contact you so that you could continue to take part in this important study. We are now gathering current telephone and address data to prepare for the BPS. Please take the time to verify, correct, or update the information on the enclosed update sheet, especially if what we have is not correct, or if you plan to move before July 1997. Please return it to Research Triangle Institute in the enclosed postage-paid envelope within two weeks. An interviewer from RTI will call to conduct a telephone interview with you sometime during the period April through June, 1997. During the interview you will be asked questions about such things as your education, the school(s) you attended or are attending, your employment experiences both while in school and after, how you financed your education, and your goals and aspirations. Enclosed you will find a brochure with a brief description of BPS as well as greater detail about the confidentiality procedures which are in place. If you would like more information about BPS, please call Janice Kelly or me at the following toll-free number: 1-800-334-8571. We thank you for your participation and wish you a safe and happy holiday season. Your participation is important and adds great value to education research and policy making. Thousands of students across the country have taken part in past BPS studies, and we sincerely hope that you will continue to do so as well. The information provided through BPS serves as a vital resource for educators and policymakers as they address issues concerning the quality of education, the effect of that education on the lives of individuals, and the most productive way to support participation in postsecondary education. We are excited about the BPS, and we invite you to visit the BPS home page at http: / /public.rti.org/bps to learn more about this study. The BPS home page also provides you with access to other useful and interesting information, such as employment resources and undergraduate-and graduate-school information. You may correspond with us directly using the home page or reach us by electronic mail at bpsrti.org. In NPSAS, we obtained information on how to contact you so that you could continue to take part in this important study. We are now gathering current telephone and address data to prepare for the BPS. Please take the time to verify, correct, or update the information on the enclosed update sheet, especially if what we have is not correct, or if you plan to move before July 1997. Please return it to Research Triangle Institute in the enclosed postage-paid envelope within two weeks. An interviewer from RTI will call to conduct a telephone interview with you sometime during the period April through June, 1997. During the interview you will be asked questions about such things as your education, the school(s) you attended or are attending, your employment experiences both while in school and after, how you financed your education, and your goals and aspirations. Enclosed you will fmd a brochure with a brief description of BPS as well as greater detail about the confidentiality procedures which are in place. If you would like more information about BPS, please call Janice Kelly or me at the following toll-free number: 1-800-334-8571. Please accept the enclosed magnet as a reminder of the BPS study and its home page address. Please feel free to use features on our home page to inform us of future changes to your telephone number. We thank you for your participation and wish you a safe and happy holiday season. We have been trying to contact you concerning the Beginning Postsecondary Students (BPS) Longitudinal Study which we are conducting for the U.S. Department of Education's National Center for Education Statistics. Let me reassure you that this study is quite important, since results will be used to determine how student participation in higher education can be better supported and encouraged. Unfortunately, we have been unable to reach you by telephone. Since you may not have access to a telephone at this time and because your input is of great significance to the BPS study, we would like to suggest several alternatives that may enable you to take part in the study: 1) If you now have a home or work telephone number, please print your current telephone information on the enclosed interview information sheet, return it in the postage paid envelope, and an interviewer will call you. 2) If you would prefer that an interviewer call you at another location (e.g., the home of a parent, relative, friend), please indicate the telephone number for that person in the spaces on the enclosed interview information sheet. Be sure to include the date(s) and time(s) that you can be contacted at the number. 3) Is a neighbor's phone or any public phone convenient? If so, you may place a toll-free call to Marti Nash Monday through Friday anytime between the hours of 9:00 AM and 11:00 PM Eastern time. You may also call on Saturday between 9:00 AM and 9:00 PM or Sunday between 2:00 PM and 9:00 PM Eastern time. Select the most convenient time for you to complete the interview. The toll-free number is 1-800-647-9674. The interview will take approximately 30 minutes to complete. We thank you for your continued participation in this important study. Your responses are truly needed to make study results accurate and timely. Monday through Friday between 9:00am and 11:00pm EDT Saturday between 9:00am and 9:00pm EDT Sunday between 2:00pm and 9:00pm EDT To facilitate your participation in the study, we have enclosed the BPS Self-Administered Interview for you to complete. We have included instructions with the Self-Administered Interview. Your participation is strictly voluntary, and your participation and any answers you may provide will not affect any financial aid or other benefits you may receive. Let me reassure you that this study is quite important, since results will be used to determine how student participation in higher education can be better supported and encouraged. We thank you' ou Tor your participation in this important study. Your responses are truly needed to make study results accurate and timely. To facilitate your participation in the study, we have enclosed the BPS Self-Administered Interview for you to complete. We have included instructions with the Self-Administered Interview. Your participation is strictly voluntary, and your participation and any answers you may provide will not affect any financial aid or other benefits you may receive. Let me reassure you that this study is quite important, since results will be used to determine how student participation in higher education can be better supported and encouraged. We thank you for your continued participation in this important study. Your responses are truly needed to make study results accurate and timely.  As of July l, 1994, were you a U.S. citizen? 1 = yes, citizen or us national. 2 = no, but was a permanent resident or other eligible non-citizen with temporary resident's card. 3 = no, in the country on fl or f2 student visa or on a j 1 or j2 exchange visitor visa."}, {"section_title": ">A_LANG<", "text": "If value preloaded, goto A_MARR94. What language was spoken most often in your home as you were growing up?  As of July 1, 1994, were your parents... 1 = married to each other 2 = divorced 3 = separated 4 = never married to each other or 5 = was one or both of your parents deceased? 6 = never knew parents and no guardians 7 = never knew parents and had legal guardians  1 = did not complete high school 2 = completed high school or equivalent POSTSECONDARY VOCATIONAL 3 = less than I yr of occupational/trade/technical or business school 4 = one but less than 2 years of occupational/trade/ technical/business school 5 = 2 years or more = refused of occupational/trade/ business school POSTSECONDARY ACADEMIC 6 = less than 2 years of college 7 = 2 or more = refused years of college, including 2-yr degree 8 = bachelor's degree -4-or 5-year degree 9 = master's degree or equivalent 10 = md/dds/Ilb/other advanced professional degree 11 = doctorate degree -phd, edd, dba DK = DON'T KNOW RE = REFUSED  While enrolled during 1994-95, where did you live? NOTE: IF MORE THAN ONE RESIDENCE, GIVE THE PLACE R LIVED THE LONGEST 1 = on-campus in school-owned housing, 2 = off-campus in school-owned housing, 3 = in an apartment or house other than with your parents or guardians (including houses owned by fraternities and sororities), 4 = with your parents or guardians, 5 = with other relatives, or 6 = some place else? >A_NPPURP< If value preloaded, goto A_END.   COLLECT UP TO 3 RESPONSES. (ENTER 0 FOR NONE, OR NO MORE). 0 = no more 1 = done taking the desired classes/completed desired coursework 2 = financial reasons 3 = academic problems 4 = not satisfied with program/school/campus/faculty 5 = classes not available/class scheduling not convenient 6 = changed degree program/major/field of study 7 = change in family status (e.g.,marriage, baby, death in family) 8 = job/military considerations 9 = moved from the area 10 = personal problems/conflicts with demands at home 11 = other --please specify { According to the enrollment information you gave me earlier, you've been enrolled in school some terms and taken other terms off. Why did you decide to take a break from school? COLLECT UP TO 3 RESPONSES. (ENTER 0 FOR NONE, OR NO MORE). 1 = to decide on a different program of study 2 = financial reasons 3 = academic problems 4 = classes were full/not offered/scheduling not convenient 5 = change in family status (e.g.,marriage, baby, death in family) 6 = to participate in co-op/internship program 7 = job/military considerations 8 = personal problems/conflicts with demands at home 9 = to pursue other interests (e.g., travel, hobbies, etc.) 10 = wanted time off 11 = other -please specify 12 = other -please specify  COLLECT UP TO 4 RESPONSES. (ENTER 0 FOR NONE, OR NO MORE). 1 = advance in current job 2 = qualify for new job within current career field 3 = change careers 4 = required to get job/enter chosen career field? 5 = program is required by employer/military 6 = program is being paid for by employer/military 7 = finally had money to go 8 = personal satisfaction of earning a degree/certificate 9 = other -please specify 10 = other -please specify "}, {"section_title": ">F_LOCJOB<", "text": "How did you find the job? 1 = school's placement office (referral, posted job notice) 2 = responded to internet/www job notice --any source 3 = responded to newspaper/other advertisement 4 = direct, unsolicited contact with employer 5 = friend or relative 6 = referral by faculty/staff 7 = recruiting fair, professional meeting 8 = volunteer experience led to job 9 = co-op or internship led to job 10 = unemployment office (Employment commission posting/referral) 11 = employment agency/professional recruiter 12 = temporary job became permanent 13 = advancement within company 14 = other --please specify  "}, {"section_title": "Contact Information", "text": "Question 16. If both of your parents or guardians live at the same address, enter their contact information and check the box marked Both parents/guardians. If your parents or guardians do not live at the same address, write in the contact information for one of your parents, and check the appropriate box to indicate their relationship to you. Question 17. Enter the contact information of a person other than your parent(s) who will always know how to get in touch with you."}, {"section_title": "163", "text": "Associate's degree (AA, AAS, AS) -A degree requiring at least 2 but less than 4 years of full-time college work (or the equivalent). Bachelor's degree (BA, BS) -A degree awarded by a 4year college or university requiring at least 4 years of fulltime college work (or the equivalent). Certificate of high school completion -Awarded when a student attended a high school for the minimum number of days required but did not complete all the courses required for a diploma. Certificate or diploma program -An award certifying the completion of a postsecondary education program, usually requiring less than two years. Co-op placement -Paid work experience for credit. The student normally does not take classes at the same time. Internship -Paid work in which a student gains supervised practical experience in his/her field of study or other area of interest. General Education Development (GED) -A certificate awarded to those who did not finish high school who have earned the equivalent of a high school diploma by completing GED classes and passing required GED exams. Gross -Total income before taxes, social security and other withholdings are subtracted. "}, {"section_title": "National Postsecondary Student Aid Study (NPSAS) -A", "text": "comprehensive study of students enrolled in formal education programs beyond high school, including those offered by less-than-2-year institutions, community, junior, and other 2-year institutions, and 4-year colleges and universities in the United States and Puerto Rico. Issues addressed by NPSAS include trends in student financial aid and how students and families pay for postsecondary education. Postsecondary -Formal education program beyond high school, including those offered by less-than-2-year institutions, community, junior, and other 2-year institutions, and 4-year colleges and universities. Work study -A financial aid program providing students the opportunity to earn money to help pay education expenses.  In order to be able to contact you in the future, we need to collect address information for your parents or guardians, and for one other individual who will know your whereabouts over the next two to four years."}, {"section_title": "Work History", "text": "16. Please provide the name, address, and telephone number for your parent(s) or guardian(s).   Questions 9 through 14 collect information on your first job after leaving school. 9. Which of the following best describes your first job after graduating from, or otherwise leaving, school? (If you had more than one job at the time you left, please answer the following questions about the one job requiring the greatest number of hours per week.) 10 A new job with a new employer 20 A new job with the same employer you had while you were enrolled 3D The same job with the same employer you had while you were enrolled Abbreviated Interview"}, {"section_title": "Instrucciones", "text": "Favor de contestar cada pregunta anotando una X en la casilla al lado de la respuesta apropiada o Ilenando la informed& requerida. Instrucciones y definiciones para algunos articulos se encuentran mes abajo. Su participacion en este estudio es completamente voluntaria y su decision a participar no afectaria cualquier asistencia econ6mica u otros beneficios que usted este recibiendo. Usted puede negarse a contestar cualquier pregunta. Toda informaciOn que nos provea sera completamente confidencial. Cuando haya terminado este cuestionario por si mismo(a), favor de devolverlo antes de 2 semanas en el sobre que encontrara adjunto el cual contiene la direcci6n y, franqueo pagado. Gracias por haber participado en este estudio de tan gran importancia. Columna E. Anote su concentracion o enfoque principal de estudios. Columna F. Indique el tipo de grado/diploma o titulo. Columna G.. Indique si recibio un titulo de esta escuela (Si, No, No aplica). Columna H. Indique el mes y aria cuando recibio su titulo o es era recibirlo, si aplica a su situation."}, {"section_title": "Historial de Empleo", "text": "Preguntas a cerca de empleo o trabajo se refieren a un empleo con paga. Pregunta 3. Indique si el (los) empleo(s) que haya tenido mientras estudiaba y durante las vacaciones le ha sido importante para pagar sus gastos de estudios (incluyendo la matricula, libros, y derechos de matricula, gastos de vivienda y gastos en general mientras estaba matriculado(a)). Pregunta 5. Si el nitmero de horas que trabajaba por semana no es fijo, de un promedio de horas por semana. Pregunta 7. Anote el sueldo por hora (suma total/hora). Pregunta 13. Si no este seguro(a) de una suma anual, de un estimado de su sueldo por hora, semana o mes (sin restar impuestos). Multiplique la cantidad por 2080 si son horas, por 52 si es por semana , o por 12 si es por mes. Informacion_para mantenernos en comunicacion. Pregunta 16. Si ambos padres o tutores (guardianes) viven en Ia misma direccion, anote informed& para poder comunicarnos y marque Ia casilla Ambos padres/tutores. Si sus padres o tutores no tienen Ia misma direccion, escriba la informed& para comunicarnos can uno de sus padres, y marque la casilla apropiada para indicar el parentesco a usted. Pregunta 17. Anote Ia informed& para comunicarnos con alguna otra persona que no sea su padre/madre y quien siempre supiera cOmo ponerse en contacto can usted."}, {"section_title": "Glossary", "text": "Titulo/Grado Asociado (AA, AAS, AS) -Un titulo/diploma que requiere por lo menos 2 pero menos de 4 albs de estudios universitarios de tiempo completo (o lo equivalente). Bachillerato (Licienciatura) (BA, BS) -Un titulo conferido por una universidad (o colegio) de 4-ahos que requiere por lo menos 4 efts de estudios universitarios de tiempo completo (o lo equivalente). Certificado de terminar escuela secundaria (superior).-Conferido cuando un estudiante asiste a una escuela secundaria por unos minimos dias requeridos pero no terming todos los cursos para obtener un diploma. Programa de certificado o diploma.-Un certificado a diploma otorgado a personas que terminan algOn programa de estudios pos-secundarios que requieren menos de 2 anos. Colocacion \"Co-op\". -Empleo con paga en cambio de creditos. El estudiante normalmente no toma cursos (clases) a la misma vez. Internado (Practica) -Empleo can paga en donde el estudiante obtiene experiencia practice en su rama /especializaciOn u otra rama de interes. General Education Development (GED) -Un certificado conferido a personas que no terminaron la escuela secundaria pero quienes han tornado cursos de GED y han pasado un examen requerido de GED. Suma Total de Ingresos. -Ingresos totales antes de restarles impuestos de ingresos o de seguro social u otro tipos de deducciones. con requisitos de tomar menos de 300 horas (por reloj) para completer 20 Un programa otorando un certificado o diploma con requisitos de tomar por lo menos 300 horas (por reloj) pare completar Un programa otorgando un titulo/grado asociado 40 Un programa otorgando un titulo de bachiller (bachillerato o licienciatura) 50 No estaba matriculado(a) en un programa formal para lograr un diploma, titulo universitario, o certificado 50 Otro -(Favor de especificar) 1-4. La escuela NPSAS, ,fue esa Ia primera institucion de estudios pos-secundarios en la cual usted asistio despues de haber asistido a Ia escuela secundaria (superior)? Si 20 No 1-5. LEn que fecha asistio por primera vez Ia escuela NPSAS despues de la escuela secundaria (superior)? Mes Arlo Las proximas preguntas se relacionan con estar empleado mientras estaba en Ia escuela. Si no esta actualmente matriculado(a) en una escuela, conteste las preguntas dandonos la informaci6n acerca del empleo que desempefiaba durante su Ultimo periodo academic\u00b0( semestre) mientras estaba matriculado(a). Si esta actualmente matriculado(a), conteste las preguntas tomando en cuenta el periodo academico (semestre) actual o el mas reciente. Las preguntas 9 al 14 recolectan informacion sobre su primer empleo despues de dejar/salirse de la escuela."}, {"section_title": "9.", "text": "LCual de los siguientes describe mejor su primer empleo despues de graduarse de, o de otra manera dejar, la escuela? (Si tuvo mas de un empleo en ese entonces, favor de contestar las siguientes preguntas tomando en cuenta el trabajo que requeria Ia mayor cantidad de sus horas por semana.) Un nuevo empleo con un nuevo empleador (patrono) Un nuevo empleo con el mismo empleador (patrono) que tenia mientras estaba matriculado(a) 30 El mismo empleo con el mismo empleador (patrono) que tenia mientras estaba matriculado(a) "}]