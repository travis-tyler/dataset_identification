[{"section_title": "Abstract", "text": "The Computational Anatomy Gateway is a software as a service tool for medical imaging researchers to quantify changes in anatomical structures over time, and through the progression of disease. GPU acceleration on the Stampede cluster has enabled the development of new tools, combining advantages of grid based and particle based methods for describing fluid flows, and scaling up analysis from single scans to populations and timeseries. We describe algorithms for estimating average anatomies, and for quantifying atrophy rate over time. We report code performance on different sized datasets, revealing that the number vertices in a triangulated surface presents a bottleneck to our computation. We show results on an example dataset, quantifying atrophy in the entorhinal cortex, a medial temporal lobe brain region whose structure is sensitive changes in early Alzheimer's disease."}, {"section_title": "INTRODUCTION", "text": "The Center for Imaging Science has been developing tools for quantitatively studying anatomical changes in neurodegenerative diseases such as Alzheimer's. These tools are offered to the medical imaging community through a software as a service model, hosted through the Computational Anatomy Gateway, a science gateway powered by the Stampede and Gordon clusters through XSEDE (www.mricloud. org) [5] .\nComputational anatomy quantifies structural changes in biology by constructing smooth mappings to identify corresponding points in atlas and target neurimaging data (see [8] for a recent overview). Properties of these mappings, such as local expansion or contraction, are used to describe and make inferences about biological processes. An example is shown in Fig. 1 . Contours around a sagittal slice of the corpus callosum, a white matter tract connecting the hemispheres of the brain, are shown in cyan and red on the left. The cyan contour (our template), is mapped (blue contour) such that its shape closely matches the red contour (our target). The map itself can be seen through a deformed grid, and the local change in scale is used to quantify atrophy (tissue loss) of the target relative to the template.\nIn this work we show how these mapping techniques are used to estimate the average anatomical shape of a population, and to quantify atrophy by mapping onto a series of magnetic resonance imaging (MRI) scans of the same pa- Figure 1 : Left: example template and target contours describing a sagittal section of the corpus callosum. Middle: A diffeomorphism is used to deform the template to closely match the target (blue). Right: The diffeomorphism is used to measure areas of atrophy (red) and expansion (blue) to describe biological processes.\ntients over time. These mapping procedures rely heavily on techniques from computational fluid dynamics, including both particle and grid based methods.\nTraditionally, particle methods have been used when anatomy is described by labelled landmarks, or triangulated meshes (for example [12] ). This allows the advantage of an efficient representation of anatomical form, and increased statistical power through fewer multiple comparisons. Grid based methods have been used when anatomy is described by a dense neuroimage (for example [1] ). This has the advantage of removing unnecessary data processing, and better accounting for imaging noise. Parallel computing resources, in particular the NVIDIA Tesla K20 on Stampede, have allowed us to combine the advantages of these two methods [11] , designing mapping tools that are both powerful and robust [10] . Additionally, larger memory has enabled us to move from studying single scans to populations and timeseries, more accurately accounting for variability.\nHere we show how these tools are being used to study changes in the medial temporal lobe, a loci of the earliest changes in the onset of Alzheimer's disease (AD) [2] ."}, {"section_title": "METHODS", "text": ""}, {"section_title": "Medial temporal lobe MRI", "text": "T1 MRI neurimages from the publicly available Alzhimer's Disease Neuroimaging Initiative (ADNI) dataset were analyzed 1 . One hundred patients were selected, matching the inclusion criteria for an upcoming drug trial. Subjects included were older adults (age 73 \u00b1 7 years), 50% male, education of 16 \u00b1 3 years, with mild cognitive impairment. Each subject was scanned up to 5 times over 2 years. Medial temporal lobe structures were hand segmented, and the entorhinal cortex and immediately lateral cortex (the trans entorhinal region) were analyzed using our algorithms. For this study, a subset of 20 subjects were used.\nFor each subject, imaging data was rigidly aligned to the baseline scan by minimizing a sum of square error cost function between pairs of T1 images. Between subjects, entorhinal cortex segmentations were aligned based on minimizing the distance between automatically placed landmarks."}, {"section_title": "Equations of diffeomorphometry", "text": "In computational anatomy, the background space of an image \u2126 \u2282 R 3 is deformed by a diffeomorphism \u03d5 : \u2126 \u2192 \u2126, which is generated by a flow under smooth time varying velocity vector fields v \u03d5t(x) .\nThe vector fields are modelled as belonging to a Hilbert space of smooth functions V (see [14] for details), with norm given by v\noperator and L * its adjoint. This construction means smooth functions will have a small norm. Here L will be chosen implicitly such that its Green's func-\n2 . This metric defines geodesics, or shortest length paths, on the group of diffeomorphisms, characterized by Euler's equation. By defining the momentum \u00b5 = L * Lv, this geodesic equation can be written as [7] \nwhere Dv denotes the 3x3 matrix of partial derivatives. The velocity vector field can be reconstructed by\nWorking with geodesics reduces the complexity of our analysis: rather than needing to specify v as a function of time, we only need to specify its initial condition.\nHere we choose to model the momentum as a singular function, nonzero only on the boundary of the anatomical structures of interest. This representation is a parsimonious description of shape, whose dimension has been reduced from that of the background space (R 3 ) to that of a bounding surface (R 2 ). Under the assumption that neuroimages are a union of homogeneous tissue types, this representation does not lose any information [1] .\nWe describe these surfaces parametrically through a function f : U \u2282 R 2 \u2192 R 3 , and can specify the momentum and\nThe template estimation setup is illustrated here using entorhinal cortex boundary surfaces. An initial guess (green) is transformed into template (cyan) which represents the average of the population. This template is mapped (blue) to closely match each subject (red), and its shape is determined by minimizing the cost function in (7).\nvelocity through a function p defined on the surface\nSurfaces are transformed by the diffeomorphism according to ft = \u03d5t(f0). For notational convenience, we will write the solution to our system of geodesic equations by \u03d5t = exp(tKp0), where exp is the Riemannian exponential.\nHere we represent these anatomical boundaries by triangulated surfaces. Our template surface has 676 vertices and 1348 faces. This is represented by a 676 \u00d7 3 double precision array. The function p is represented using an array of the same size.\nIn this analysis we perform mappings between binary segmentation images that describe the entorhinal cortex. These are functions I from \u2126 to [0, 1]. They take the value 0 at locations outside a structure of interest, the value 1 inside it, and fractional values near the boundary depending on resolution and interpolation. An image is transformed by a diffeomorphism by acting on the right with the inverse:\nt . Here images are represented by a regular double precision 3D array. The temporal lobe structures of interest are contained within a 42 \u00d7 36 \u00d7 45 array at resolution 1mm \u00d7 1.2 mm \u00d7 1mm. Each of the x, y, z components of the diffeomorphism \u03d5, and velocity fields v are represented using an array of the same size."}, {"section_title": "Template estimation formulation", "text": "We are given a family of m neuroimages, each the baseline scan for a single subject, which we denote Ji, functions from the ambient space \u2126 \u2282 R 3 into [0, 1]. We begin with a initial guess for our template: the surface f is generated as an isocontour of the pointwise average of our population images, and the image I is generated by filling the interior of this surface.\nFollowing [6] , we formulate a cost function as follows E(p0, p1, . . . , pm)\nwhere we define Ii . = exp(Kpi)\u00b7exp(Kp0)\u00b7I. In this analysis each of the \u03c3 weighting parameters is set to 1.\nWe seek to determine the parameters p0, p1, . . . , pm that minimize the cost. This will give us a template image exp(Kp0)\u00b7 I and template surface exp(Kp0)\u00b7f that characterize the average of our population in a minimum square distance sense.\nThis setup is illustrated in Fig. 2 . The initial guess, I or f is shown at the bottom left in green. It is deformed using p0 to the estimated template, shown in the center in cyan. This template is deformed under pi (blue) to match each member of the population (red). This illustration shows entorhinal cortex surfaces for the first 8 subjects in our dataset."}, {"section_title": "Timeseries mapping formulation", "text": "We are given a family of n neuroimages for a single subject at times ti, i \u2208 {1, . . . , n}, which we denote Jt i each being a function from the ambient space \u2126 \u2282 R 3 into [0, 1], and a template image I : \u2126 \u2192 [0, 1], and template surface f : U \u2192 R 3 estimated previously. We seek to match our template onto each image by estimating two geodesic Figure 3 : The timeseries mapping setup is illustrated here using entorhinal cortex boundary surfaces. A template (cyan) is transformed to the baseline scan (blue, top-left), and further transformed along a trajectory to closely match each segmentation in the timeseries (red). The trajectory is determined by minimizing (8).\ntrajectories. First from template to time t1, then from t1 through the timeseries to tn.\nWe formulate a cost function as follows\nwhere It i is the template image deformed to time ti via exp(Kp1(ti \u2212 t1)) \u00b7 exp(Kp0) \u00b7 I. In this analysis, each of the \u03c3 weighting factor is set to 1.\nWe seek to determine the parameters p0, p1 that minimize the cost. The deformation exp(Kp0) tells us how a given subject differs from the population average, and the deformation trajectory exp(Kp1(t \u2212 t1)) exp(Kp0) tells us how the subject's brain is changing over time. This procedure is essentially linear regression in a minimum square distance sense.\nThis setup is illustrated in Fig. 3 . The template, I or f is shown at the bottom left in cyan. It is deformed using p0 to the baseline scan, and then further deformed using p1 to closely match each subsequent scan in the timeseries. Notice in this example the variable anterior-posterior (left-right) extent of the entorhinal cortex segmentation (e.g. the second timepoint appears much smaller). These inconsistencies in anatomical definitions are filtered out by mapping onto the whole timeseries simultaneously. Mapping onto each individual scan result in much more variability."}, {"section_title": "Adjoint algorithm", "text": "In each case, the minimization is undertaken using gradient descent, where the gradient is calculated using an adjoint algorithm. That is, the gradient with respect to each image matching term is calculated as 1 \u03c3\nand transported back to the appropriate template, following a linearized verson of the dynamics. Summing over all the error terms gives the cost function gradient. We omit the details here and refer the reader to [13] or [4] . The computation involves only variations of the interpolation and weighted sums described below."}, {"section_title": "Computation", "text": "Each mapping algorithm was implemented on the GPU using OpenCL.\nSurfaces are deformed by first computing the value of the velocity at each verte\u1e8b\nand then flowing forwards according to Euler's method\nThese calculations are parallelized on the GPU over each output variable. Dense velocity fields are generated by\nwith x ijk being the i, j, k-th gridpoint in our 3D array, and u l the l-th vertex in our triangulated surface. Note that this operation is \"like\" a convolution, but because ft(u l ) is not regularly spaced, fast Fourier transform techniques cannot be used. These sums are parellelized on the GPU with one thread per output ijk. Diffeomorphisms are generated from v using a technique from numerical weather prediction called semi-Lagrangian advection [9] , with\nand the evaluation is performed through trilinear interpolation. Images are also deformed through trilinear interpolation\nBoth these interpolation operations are parallelized with one thread per output ijk.\nIn general, each calculation needed can be expressed as an interpolation, or as a weighted sum over vertices.\nTests were performed on a Stampede GPU node with 2 8-core Intel Xeon CPUs (E5-2680) at 2.7 GHz, though the software examined did not parallelize over multiple CPU cores. GPU computations were performed on a Tesla K20m card in double precision arithmetic, with a maximum work group size of 1024 and 13 parallel compute cores. Global work group size was set to the nearest integer multiple of CL_KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE times CL_DEVICE_MAX_COMPUTE_UNITS (i.e. 13 on this card) larger than the desired number of output variables, and local work group size was chosen automatically by OpenCL through calling the enqueueNDRangeKernel function with the cl::NullRange argument."}, {"section_title": "Markers of shape change", "text": "At each vertex in our template triangulated surface, we measure the determinant of the 3 \u00d7 3 Jacobian of the diffeomorphism calculated for each subject at each timepoint. This measure tells us by what factor tissue is growing or shrinking in a small region around this vertex. The value 1 represents no change, values less than 1 represent atrophy or tissue loss, and values greater than 1 represent growth or tissue gain.\nWe fit the determinant of Jacobian to a linear model, and estimate a mean and atrophy rate through least squares. This gives atrophy rate as a biomarker for AD progression, at each vertex of our triangulated surface template."}, {"section_title": "RESULTS", "text": ""}, {"section_title": "Computational performance", "text": "At XSEDE2015 [11] we presented early results showing a dramatic increase in computation speed on the GPU versus CPU for the combined particle and grid methods. We have since developed these specific algorithms, and present results that illustrate how they perform on the Stampede cluster for differently sized datasets.\nFor the following results, total wall clock time was measured by comparing timestamps on empty files created before and after code execution, and computation time per iteration was determined by dividing this elapsed time by 100 iterations of gradient descent optimization."}, {"section_title": "Number of surface vertices", "text": "For this experiment vertex points were placed in a uniformly spaced 3D grid spanning the image volume, rather than on a surface contouring an anatomical structure. Shown in Fig. 4 are the timing results of our timeseries mapping procedure, with only a single timepoint, while varying the number of vertices in our template. Above approximately 1000, the time begins increasing quickly. This motivates our use of a template with slightly less than 1000 vertices."}, {"section_title": "Number of subjects in template estimation", "text": "For this experiment a template shape was estimated using differently sized subsets of baseline scans from our ADNI population. Shown in Fig. 5 are the timing results of our template estimation procedure, varying the number of subject images. It can be seen that computation time increases roughly linearly with number of patients in our population, indicating that there are no bottlenecks with regard to number of subjects at this scale."}, {"section_title": "Number of scans in timeseries mapping", "text": "For this experiment we used one ADNI subject with images at five timepoints, and performed mapping on subsets of the first 1 to 5 scans. Shown in Fig. 6 are the timing results of our timeseries mapping procedure, varying the number of subject images. It can be seen that computation time increases roughly linearly with number of scans, indicating that there are no bottlenecks with respect to number of timepoints at this scale."}, {"section_title": "Atrophy rate", "text": "The results of our template estimation and timeseries mapping procedure are shown in Fig. 7 . One sees atrophy throughout the cortex, but particularly near the most inferior point and lateral to it, an area known as the sulcal region of the entorhinal cortex [3] . This pattern is consistent with [2] , where these changes could only be identified in histology of post mortem brains."}, {"section_title": "CONCLUSIONS", "text": "GPU nodes on the Stampede cluster have allowed us to explore a class of computational anatomy algorithms that combine the advantages of particle based and grid based methods. At XSEDE2015 [11] we demonstrated that mapping a single template to a single target in this framework could be infeasible on a CPU for some typically sized images and surfaces. This year, through GPU technology, we are able to not only perform these basic mappings, but expand the size of datasets analyzed and complexity of algorithms used. Here we have characterized the performance of two algorithms, template estimation and timeseries mapping. We have identified the number of vertices in our triangulated surface template as an important computational bottleneck, and will investigate ways to improve performance.\nWe have applied these methods to studying a population of subjects in the early stages of Alzheimer's disease, and have identified regions of atrophy that are consistent with post-mortem techniques.\nSince code was developed in OpenCL, it can be compiled on different parallel architectures. Future work will include investigating the performance of our algorithms using the Intel Xeon Phi, which is available on 6400 nodes on Stampede, in addition to the NVIDIA Tesla K20 GPU which has only 128 available nodes."}]