[{"section_title": "Abstract", "text": "Inference for spatial generalized linear mixed models (SGLMMs) for high-dimensional non-Gaussian spatial data is computationally intensive. The computational challenge is due to the high-dimensional random effects and because Markov chain Monte Carlo (MCMC) algorithms for these models tend to be slow mixing. Moreover, spatial confounding inflates the variance of fixed effect (regression coefficient) estimates. Our approach addresses both the computational and confounding issues by replacing the high-dimensional spatial random effects with a reduced-dimensional representation based on random projections. Standard MCMC algorithms mix well and the reduceddimensional setting speeds up computations per iteration. We show, via simulated examples, that Bayesian inference for this reduced-dimensional approach works well both in terms of inference as well as prediction; our methods also compare favorably to existing reduced-rank approaches. We also apply our methods to two real world data examples, one on bird count data and the other classifying rock types."}, {"section_title": "Introduction", "text": "Gaussian and non-Gaussian spatial data arise in a number of disciplines, for example, species counts in ecology, tree presence-absence data, and disease incidence data. Models for such data are important for scientific applications, for instance when fitting spatial regression models or when interpolating observations across continuous spatial domains. Spatial generalized linear mixed models (SGLMMs) are popular and flexible models for spatial data, both for continuous domain or \"point-referenced\" data (Diggle et al., 1998) , where the spatial dependence is captured by random effects modeled using a Gaussian process, as well as for lattice or areal data (cf. Besag et al., 1991, Rue and Held, 2005) where dependence is captured via random effects modeled with Gaussian Markov random fields. SGLMMs have become very popular in a wide range of disciplines. In practice, however, SGLMMs pose some computational and inferential challenges: (i) computational issues due to highdimensional random effects that are typically strongly cross-correlated -these often result in slow mixing Markov chain Monte Carlo (MCMC) algorithms; (ii) computations involving large matrices; and (iii) spatial confounding between fixed and random effects, which can lead to variance-inflated estimation of regression coefficients (Reich et al., 2006 , Hughes and Haran, 2013 , Hanks et al., 2015 . In this manuscript we provide an approach for reducing the dimensions of the spatial random effects in SGLMM models. Our approach simultaneously addresses both computational issues as well as the confounding issue.\nThere is a large literature on fast computational methods for spatial models (cf. Cressie and Johannesson, 2008 , Banerjee et al., 2008 , Higdon, 1998 , Shaby and Ruppert, 2012 , Datta et al., 2015 , among many others). These methods have been very useful in practice, but they largely focus on linear (Gaussian) spatial models and do not consider the spatial confounding issue. The predictive process approach (Banerjee et al., 2008) has been an important contribution to the literature, and has also been studied in the SGLMM context. However, the predictive process approach requires that users provide reference knots, which can be challenging to specify; our method is more automated. We also find that in some cases we obtain similar performance to the predictive process at far lower computational cost. Crucially, our approach is also able to easily address the spatial confounding issue.\nINLA (Rue et al., 2009 ) provides a sophisticated numerical approximation approach for SGLMMs. As we later discuss, INLA may be used in combination with the projection-based reparameterization approach we develop in this manuscript. This is useful for addressing confounding while also reducing computational costs.\nRestricted spatial regression models for areal and point-referenced spatial data (Reich et al., 2006 , Hanks et al., 2015 address the confounding issue. However, these models are computationally intensive for large data sets. For areal data, Hughes and Haran (2013) alleviate confounding in a computationally efficient manner by proposing a reparameterization that utilizes the underlying graph to reduce the dimension of random effects. To our knowledge, no existing approach alleviates spatial confounding and is computationally efficient for point-referenced non-Gaussian data. In this manuscript we describe a novel method that utilizes the principal components of covariance matrices to achieve fast computation for fitting traditional SGLMMs as well as restricted spatial regression.\nOur method relies on the random projections algorithm (Banerjee et al., 2012 , Sarlos, 2006 , Halko et al., 2011 , which allows a fast approximation of the leading eigencomponents.\nWe show how we can build upon this projection-based approach to address the computational and inferential challenges of SGLMMs. The outline of the remainder of the paper is as follows.\nIn Section 2, we introduce spatial linear mixed models and explain how a generalized linear model formulation of these models is appropriate for non-Gaussian observations. We also examine the computational challenges and some current approaches. In Section 3, we explain spatial confounding, how it affects interpretation of regression parameters, and describe how to alleviate confounding via orthogonalization. In Section 4, we describe our projectionbased approach for both the continuous domain and lattice case. We study the inference and prediction performance of the proposed method via a simulation study in Section 5, and study our method in the context of applications in Section 6. We conclude with a discussion of our work in Section 7. T . This can be taken into account by including spatially dependent random effects W (s) to model the residual dependence,\nwhere \u03b2 are regression parameters. { (s) : s \u2208 D} is a small-scale (nugget) spatial effect/measurement error process, modeled as an uncorrelated Gaussian process with mean 0 and variance \u03c4 2 . For the continuous spatial domain, the random effects {W (s) : s \u2208 D} are typically modeled by a zero-mean stationary Gaussian process with a positive definite covariance function C(\u00b7). Hence, for a finite set of locations, W = [W (s 1 ), . . . , W (s n )] T follows a multivariate normal distribution MVN(0, \u03a3), with\nA commonly used class of covariance functions, assuming stationarity and isotropy, is the Mat\u00e9rn class (Stein, 1999) ,\nwhere h = ||s i \u2212 s j || denotes the Euclidean distance between pairs of locations, \u03c3 2 is a variance parameter, and \u03c1 is a positive definite correlation function parameterized by \u03c6, the spatial range parameter, and \u03bd, the smoothness parameter. \u0393(\u00b7) is the gamma function, and\nis the modified Bessel function of the second kind."}, {"section_title": "Spatial generalized linear mixed models", "text": "A popular way to model spatial non-Gaussian data is by using spatial generalized linear mixed models (SGLMMs) (cf. Diggle et al., 1998 , Haran, 2011 . Let {Z(s) : s \u2208 D} denote a non-Gaussian spatial field, and g {\u00b7} a known link function. Then, the conditional mean,\nConditional on\nT are mutually independent, following a classical generalized linear model (cf. Diggle et al., 1998) . We provide two commonly used examples of SGLMMs for spatial binary and count data to illustrate our projection-based approach, the Poisson with log link and binary with logit link respectively. The projectionbased approach presented in this paper generalizes to other link functions and observation models, as well as to cases where an additional nugget term is added to the model (2) (cf. Berrett and Calder, 2016) . Details for the nugget model are provided in the supplement S.5."}, {"section_title": "Model fitting and computational challenges", "text": "The hierarchical structure of spatial models makes it convenient to use a Bayesian inferential approach. Often in practice, we fix the value of \u03bd and assign prior, p(\u03b8, \u03b2), to parameters \u03b8 and \u03b2 where \u03b8 = (\u03c6, \u03c3 2 , \u03c4 2 ) T , then use Markov Chain Monte Carlo (MCMC) to sample from the posterior \u03c0(\u03b2, \u03b8, W | Z). Fitting SGLMMs generally requires the evaluation of an ndimension multivariate normal likelihood for every MCMC iteration, with matrix operations of order n 3 floating point operations (flops). There are often strong correlations between the fixed and random effects (Hodges and Reich, 2010) , and strong cross-correlations among the spatially dependent random effects. It is well known that this dependence typically results in poor mixing in standard MCMC algorithms (cf. Christensen et al., 2006 , Haran et al., 2003 , Rue and Held, 2005 . Furthermore, when the data locations are near each other, the covariance matrix may be near singular, resulting in numerical instabilities (Banerjee et al., 2012) . These issues motivate the development of our reduced-dimensional approach to inference for SGLMMs.\nConsiderable work has been done to address the above issues in the linear case, where model inference and prediction are based on the marginal distribution Y | \u03b2, \u03c6, \u03c3 2 , \u03c4 2 \u223c MVN(X\u03b2, \u03a3 + \u03c4 2 I). Several methods rely on low rank approximations or multi-resolution approaches to reduce computations involving the n \u00d7 n covariance matrix \u03a3 (cf. Banerjee et al., 2012 , Sang and Huang, 2012 , Nychka et al., 2015 , Cressie and Johannesson, 2008 .\nHowever, these methods do not readily extend to SGLMMs because the marginal distribution for Z | \u03b2, \u03c6, \u03c3 2 is not available. A notable exception is the predictive process approach (Banerjee et al., 2008) , where the extension to SGLMMs has been well studied. This approach replaces random effect W by W * , the realization of W (s) at m (<< n) refer- \n. For the linear case, this adjustment adds little extra computation. However, for the SGLMM case, the modified predictive process puts us back to working with a high-dimensional random effect W mod . Furthermore, determining the number and placement of reference knots is a non-trivial challenge (see Finley et al., 2009 , for some potential strategies).\nAnother challenge with SGLMMs arises from the strong-correlations among random effects, which often results in poor Markov chain mixing. Reparameterization techniques (Christensen et al., 2006) can help with mixing; however, for high-dimensional spatial data, the reparameterizing step is computationally expensive and may not result in fast mixing."}, {"section_title": "Confounding and Restricted Spatial Regression", "text": "Spatial confounding occurs when the spatially observed covariates are collinear with the spatial random effects. This is a common problem for both point-referenced and areal data (cf. Hanks et al., 2015 , Reich et al., 2006 ). Here we demonstrate the confounding problem in a continuous domain. Let \u03b7 = [\u03b7(s 1 ), . . . , \u03b7(s n )] T denote the transformed site-specific conditional means, where \u03b7(\nwhere the covariance \u03a3 is \u03c3 2 R(\u03c6), with R(\u03c6) a positive definite correlation matrix, R ij (\u03c6) = \u03c1(||s i \u2212 s j ||; \u03c6). X are spatially observed covariates that may explain the random field of interest. W is used as a smoothing device. When both X and W are spatially smooth, they are often collinear (cf. Hanks et al., 2015) . This confounding problem may lead to variance inflation of the fixed effects (Hodges and Reich, 2010) .\nLet P [X] and P"}, {"section_title": "\u22a5", "text": "[X] denote orthogonal projections onto the space spanned by X and its complement, respectively. Model (3) can be equivalently written as\nIn some cases, it may be reasonable to fit model (4) to address the confounding issue by restricting the random effects to be orthogonal to the fixed effects in X (Reich et al., 2006 , Hughes and Haran, 2013 , Hanks et al., 2015 . We refer to this as restricted spatial regression (RSR) in the remaining sections. After fitting the RSR via MCMC, we can obtain valid inference for \u03b2 using an a posteriori adjustment based on the MCMC samples (Hanks et al., 2015) . Let k indicate the k th MCMC sample, then\nFitting RSR is just as computationally expensive as regular SGLMMs because the dimension of random effects P\n[X] W remains large; their strong correlations lead to slow MCMC mixing."}, {"section_title": "Reducing Dimensions through Projection", "text": "Instead of working with the original size of the random effects W in the model, we consider a reduced dimensional approximation. We want to reduce the dimension of random effects from n to m so that: (i) for a fixed m, the approximation to the original process comes close to minimizing the variance of the truncation error (details below), (ii) the m random effects are nearly uncorrelated, and (iii) we reduce the number of random effects as far as possible in order to reduce the dimensionality of the posterior distribution. Let \u03b4 denote a vector of the reduced-dimensional reparameterized random effects. The main idea of our approach is to obtain \u03b4 by projecting W to its first-m principal direction we describe the motivation and properties of the reparameterization approach. We describe random projections for fast approximations of the eigencomponents and illustrate its approximation performance. We then explain how to fit the reparameterized SGLMMs with random projection. We conclude this section by showing how our approach is also applicable to areal data, and compare it to the method in Hughes and Haran (2013) .\nOur reparameterized random effects model achieves (i)-(iii) above. Consider the spatial\nand {\u03bb i : i = 1, . . . , \u221e} be orthonormal eigenfunctions and eigenvalues, respectively, of the (Adler, 1990) . By the Karhunen-Lo\u00e8ve (K-L) expansion, we can write\nwhere {\u03be i : i = 1, . . . , \u221e} are orthonormal Gaussian (Adler, 1990) . Assuming the eigenvalues are in descending order,\n, minimizes the mean square error, ||W \u2212 W ||, among all basis sets of order m (Banerjee et al., 2012, Cressie and Wikle, 2015) . A discrete analogue for the truncated expansion of the process realization W is similar to the above. W has expan-\ndenote an n \u00d7 n matrix of eigenvectors, and \u039b n = diag(\u03bb 1 , . . . , \u03bb n ) an n \u00d7 n diagonal matrix of eigenvalues. We define instance, we later demonstrate in a simulated example of data size n=1,000, rank m=50 is sufficient to achieve reasonable performance. We discuss heuristics for choosing an appropriate value for m in Section 4.6."}, {"section_title": "Random projection", "text": "Random projection is an approach that facilitates fast approximations of matrix operations (see Halko et al., 2011 , and references therein). Here we use it to approximate the principal components of covariance matrices. Before introducing the random matrix approach, we first describe a deterministic approach to approximate eigendecomposition. Various algorithms that approximate eigencomponents using a submatrix of the original matrix are compared in Homrighausen and McDonald (2016) ; In our implementation, we used the Nystr\u00f6m method Seeger, 2001, Drineas and Mahoney, 2005) . Let K denote an n \u00d7 n positive definite matrix to be decomposed; we can further denote its partition as\nwhere K 11 is k \u00d7 k dimensions. The central idea of Nystr\u00f6m's method is to compute exact eigendecomposition on the lower-dimensional submatrix K 11 , then use the resulting lowerdimensional eigencomponents to approximate eigencomponents of K. Let V c (A) and \u039b c (A) be matrices of the first c eigenvectors and eigenvalues, respectively, of a positive definite matrix A; therefore, both have c columns.\na k \u00d7 k diagonal matrix whose elements are the approximated eigenvalues and V k (K) is approximated by scaling V k (K 11 ) up to high dimensions via\nFrom the Nystr\u00f6m method, we also obtain an approximation to\n11 K 12 ||, which reflects the information lost from truncating K (Belabbas and Wolfe, 2009 ). However, the approximated eigenvectors above are not guaranteed to be orthogonal, hence we adopt a slight variant of the form (similar to Algorithm 5.5 in Halko et al., 2011) \nTherefore, U and D 2 are the first k eigencomponents of K, and they are used as our approximation to the eigencomponents of K, respectively. This Nystr\u00f6m's approximation is summarized as step 2 in Algorithm 1. [MH: should I cut out this paragraph]\nNystr\u00f6m's method obtains column space of K from subsampling its columns K\u03a6 =\nwhere \u03a6 is an n \u00d7 k matrix by permuting the rows of [I k\u00d7k , 0 k\u00d7(n\u2212k) ] T ; once \u03a6 is fixed, the approximation is deterministic. Alternatives to approximate the column space of K involving randomness have been proposed, such as weighted random subsampling of the rows and columns of K by Frieze et al. (2004) , subsampled randomized Hadamard transform by Tropp (2011) or a random projection matrix with iid elements (Halko et al., 2011 , Bingham and Mannila, 2001 , Banerjee et al., 2012 . Here we adopt the latter method with iid Gaussian random variables.\nRather than truncating K, we take \u03a6 to be K \u03b1 \u2126, where \u2126 is a n \u00d7 k random matrix with\n, and \u03b1 = 0, 1, or 2 takes a small non-negative integer value for improving approximation (see a comparison for \u03b1 in Section 4.2). Then K\u03a6 is randomly weighted linear combination of columns of K, by construction, it approximates the column space of K. The random matrix \u2126 is a low-dimensional embedding:\nJohnson-Lindenstrauss's transformation; it has a low distortion such that | ||\u2126 T v|| \u2212 ||v|| | is small for all v \u2208 V \u2282 R n with high probability (for details on the embedding, we refer the readers to Dasgupta and Gupta, 2003) . Taking small powers of K in the projection matrix \u03a6 = K \u03b1 \u2126, enhances our approximation performance but this involves a tradeoff in terms of computational speed; in our implementation, we see substantial improvement by taking \u03b1 = 1 or 2. We let k = m + l, where m denotes the target rank, and l is an oversampling factor typically set to 5 or 10 to reduce approximation error (Halko et al., 2011) . In our implementation, we noticed that taking small l is not enough to give a good approximation of the eigenvectors corresponding to the smaller eigenvalues; therefore, we take l = m. The random projection approach to approximate the column space of K is summarized as step 1 in Algorithm 1.\nIn the context of Gaussian process regression, Banerjee et al. (2012) used a similar random projection algorithm to approximate the covariance matrix \u03a3. Here we will directly approximate the eigencomponents of the correlation matrix R(\u03c6), because multiplying \u03c3 2 does not affect the approximation. In our MCMC implementation, we obtain U m and D m , the approximated leading m eigencomponents of R(\u03c6), using Algorithm 1 for every \u03c6 value.\nThen we can obtain reparameterized random effects\nAlgorithm 1 Random projection algorithm: Given a positive semi-definite matrix K. This algorithm approximates the leading m eigencomponents of K by utilizing Nystr\u00f6m's method."}, {"section_title": "Low dimensional projection from", "text": "2. Nystr\u00f6m's method to approximate eigendecomposition :\n. Take the first m columns of U , and the first m diagonal elements of D 2 as our approximation to the leading m eigencomponents of K"}, {"section_title": "Approximation comparison", "text": "To illustrate the performance of introducing randomness in approximating eigencomponents using Nyst\u00f6m's method as described in Section 4.1. we perform a numerical experiment to compare the approximation performance of the leading m eigenvectors and eigenvalues of K. Direct comparison of eigenvectors are difficult, because they are only uniquely defined up to a sign change. Let (U, V) denote the distance between two subspaces U and V. Here we follow Homrighausen and McDonald (2016) and define\nwhere \u03a0 U and \u03a0 V are the orthogonal projection associated with U and V, respectively. The smaller the distance between the subspaces generated by the approximated eigenvectors U m and the true eigenvectors V m , the better the approximation. To measure the approximation performance of the eigenvalues, for any vector containing the estimated leading eigenvalues in descending order, we compare it to the true leading eigenvalues \u03bb using ||\u03bb \u2212 \u03bb || l 2 . We simulate 1000 random locations in unit domain; based on these data points, we compute the correlation matrices K using the Mat\u00e9rn covariance function with \u03bd = 0.5 or 2.5 , and \u03c6 = 0.3. Figure 1 shows the approximation results when the projection matrix \u03a6 is\nT with rows permuted, or K \u03b1 \u2126 with \u03b1 = 0, 1, 2. We see that when random projections are used the approximation is improved. In addition, there are advantages in taking \u03a6 to be K \u03b1 \u2126, where in practice \u03b1 = 1 appears to be a good choice."}, {"section_title": "Random projection for spatial linear mixed models", "text": "Here we illustrate the random projection approach for a spatial linear mixed model (SLMM)\nwith an emphasis on dealing with confounding. Banerjee et al. (2012) proposes using random projection for efficient Gaussian process regression. In this subsection we extend their approach so it applies to both SLMMs and the linear restricted spatial regression model. This description also serves as an introduction to our more general approach to SGLMMs.\nFor the linear case, model fitting is based on the marginal distribution of Y | \u03b2, \u03c6, \u03c3 2 , \u03c4 2 .\nThe main computational challenge is therefore due to the expense in calculating inverses and\nSubspace Distance \u03b1 \u2126 improves the Nyst\u00f6m approximation to the eigenvectors (left column) and eigenvalues (right column). Letting \u03a6 to be K \u03b1 \u2126 with small power \u03b1 = 1, 2 also provides better approximation for both \u03bd = 0.5 (first row) and \u03bd = 2.5 (second row).\ndeterminants for large covariance matrices. Random projection may be used to approximate the correlation matrix using its principal components. To fit the full model with random projection (FRP), we apply Algorithm 1 to approximate\n. We rewrite the model as follows,\nAnalogously, our RSR model with random projection (RRP) is\nHereafter, R(\u03c6) will be referred to as R to suppress its dependency on the unknown parameter \u03c6. Fitting the FRP model (6) involves evaluating the inverse and determi-\nwith cost of m flops, since U m has orthonormal columns. The determinant calculation can also be simplified. By the determinant lemma (Harville, 1997) ,\nSimilarly, fitting the RRP model (7) involves calculating the inverse and determinant of\nfor which the dominant cost is tied to the\nFor the linear case we can simply approximate the correlation matrix of the random effect R with R without explicitly reparameterizing the random effects. However, for SGLMMs, we do not have closed-form marginal distribution. It is therefore necessary to obtain the reduce random effects \u03b4 and carry out inference based on \u03c0(\u03b8, \u03b2, \u03b4 | Z)."}, {"section_title": "Random projection for spatial GLMMs", "text": "Here we describe how to reparameterize and reduce the dimension of our models such that the resulting model is easier to fit and preserves the desirable properties of the original model.\nWe do this for both cases, first where confounding may not be an issue and the second where we want to address confounding.\nWe apply Algorithm 1 to R to obtain U m and D m . If confounding is not an issue, we replace random effect W with U m D 1/2 m \u03b4. The SGLMM (3) may be rewritten as\nWe refer to this as the full model with random projection (FRP). Essentially, the spatial dependence in W is transformed into a reduced-dimension spatially independent variable \u03b4 and synthetic spatial variable\nm . This combines the idea of spatial filtering (Getis and Griffith, 2002) and PCA, thereby reducing the dimension of the posterior distribution.\nBy also reducing the correlations among the parameters, our approach improves the mixing of MCMC algorithms for sampling from the posterior. Once priors p(\u03b2, \u03b8) are specified, we can sample from the full conditionals (see the online supplementary materials S.1) using\nMetropolis-Hastings random-walk updates.\nTo address the confounding problem, we follow the RSR approach to restrict the random effects to be orthogonal to the fixed effects (Hodges and Reich, 2010) . We can project our reduced random effects U m D 1/2 m \u03b4 to the orthogonal span of X. The restricted model with random projection (RRP) can be summarized as follows:\nFitting RRP is similar to FRP, except that in the data likelihood To better approximate the original random effects W , rather than P \u22a5\n[X] W , we therefore first approximate the correlation matrix and then perform the requisite orthogonal projection."}, {"section_title": "Random projection for areal data", "text": "Our approach reduces the dimension by decomposing its correlation matrix. Hence, it can be easily applied to Gaussian Markov random field models as well. Here we develop FRP and RRP for non-Gaussian areal data. Note that RRP model for areal data is similar to the approach proposed by Hughes and Haran (2013) . Both methods adjust for confounding and reduce the dimension of random effects. The only difference is that we decompose the covariance matrix, whereas their decomposition is performed on the Moran operator (details are provided later in this subsection).\nConsider spatial data located on a discrete domain, for instance mortality rates by county across the U.S. If we describe the data locations via nodes on an undirected graph with edges only between nodes that are considered neighbors, we can model the spatial dependence via a Gaussian Markov random field model. To model the dependence of\nwhere the index indicates block, we define the neighboring structure among blocks through an n \u00d7 n adjacency matrix A with diag(A) = 0 and A ij = 1 if the i th and j th locations are connected, or A ij = 0 if they are not connected (Besag et al., 1991) . The common model for W is an intrinsic conditionally auto-regressive (ICAR) or Gaussian Markov Random Field prior:\nwhere \u03c4 is a smoothing parameter that controls the smoothness of the spatial field, and Q = diag(A1) \u2212 A is the precision matrix (1 is a n-dimensional vector of 1s).\nFor the discrete domain, Hughes and Haran (2013) use the eigenvectors of the Moran\n, to reduce the dimension of the random effects. Their method alleviates confounding while preserving spatial dependence structure implied by the underlying graph. The eigenvectors can be interpreted as spatial patterns corresponding to different degrees of spatial dependency.\nWe can also fit both FRP and RRP to areal data and achieve similar dimension reduction and computational gains. First we obtain the covariance matrix by taking the generalized inverse of the precision matrix Q. Then apply random projection on the covariance matrix Q \u22121 , and proceed with either FRP or RRP as described in the preceding section. The eigenvectors corresponding to large eigenvalues represent large-scale spatial variation. The advantage of this PCA approach is that a relative small number of PC's is enough to capture most spatial variation. Computationally, our method is not as efficient as Hughes and Haran (2013) because of the extra cost of inverting the precision matrix. However, for the ICAR model, the precision matrix is fixed and defined beforehand, so the inversion and random projection only need to be performed once. The model estimates from RRP are comparable to those obtained by Hughes and Haran (2013) (see the online supplementary materials S.3)."}, {"section_title": "Rank selection", "text": "Here we provide a general guideline for selecting the rank for projection-based models. From a Bayesian perspective, the rank can be determined by model comparison criteria such as DIC (Spiegelhalter et al., 2002) . We can fit several models using different number of ranks and select the one with the smallest DIC. However, to reduce computational time, we recommend the following procedure to select the appropriate rank before fitting either FRP or RRP model.\nSince the projection-based models combine the idea of spatial filtering and PCA di- if there is not much change, we can stop and simply use the current rank. For each of the above smoothness values, we also conduct simulation studies using several ranks to study the effectiveness of the rank selection method and the performance of our projection-based approach. Our simple heuristic appears to work well in our simulation studies, the DIC values from model fits agree with BIC selection and the prediction performance increases by only a small margin above the selected rank. We find that it is not necessary to repeatedly fit many models with increasing rank. If researchers want a more rigorous comparison of models with different ranks and are willing to implement Bayes factor calculations, comparing Bayes factors would be a useful alternative to what we have proposed."}, {"section_title": "Computational gains", "text": "The advantages of our reparameterization schemes are shorter computational time per iteration and less MCMC iterations to achieve convergence. These result from: (1) reducing large matrix operations, (2) reducing the number of random effects, and (3) improving MCMC algorithm mixing. Although the main computational cost of our approach is of order O(n 2 m) from applying Algorithm 1, it is dominated by matrix multiplications that can be easily parallelized by multi-core processors. Leveraging parallel computing for matrix multiplication, the remaining dominant cost of fitting our model is of order O(nm 2 ) due to the singular value decomposition of n \u00d7 m matrices. To illustrate the computational gain of the projection-based models, we fit both the SGLMM and the projection-based models to simulated Poisson data. We fit the SGLMM using one-variable-at-a-time Metropolis-Hastings random-walk updates. To fit the projection-based models, we update random effects \u03b4 in a block using spherical normal proposal; simple updating scheme for \u03b4 is sufficient because it has a smaller dimension and are decorrelated. In our implementation, Algorithm 1 is To see the improvement in MCMC mixing, we compute the effective sample size (ESS) using the R coda package (Plummer et al., 2006) ; it provides the number of independent samples roughly comparable to the number of dependent samples produced by the MCMC algorithm, therefore a larger ESS implies better Markov chain mixing. Based on our results, the projection-based models have better mixing, for example, the univariate ESSs of the RRP are, on average, 12 times larger than the ones from the SGLMM and three times the ones from the predictive process (using the R package by Finley et al., 2013) for the same number of MCMC iterations. The mixing improvement of our projection-based models is implied from the a posteriori correlations ( Figure 2) ; our projection-based models (both FRP and RRP) produce weakly correlated random effects compared to the predictive process. The improvement in computational time is illustrated in Figure 3 . The time required increases dramatically for SGLMM as the data size increases, however we can still fit the random projection model in a reasonable amount of time. We also compute ESS per second to compare MCMC efficiency; our RRP model more than 120 times more efficient than the SGLMM. "}, {"section_title": "Simulation Study and Results", "text": "In this section, we apply our approaches to simulated linear, binary and Poisson data. For each case, we simulate 100 data sets where the locations are in the unit domain [0, 1] 2 . We fit both FRP and RRP models to simulated data with size of n = 1000 at random locations, then make predictions on a 20 \u00d7 20 grid. We adjust the regression parameters of RRP using equation (5) (denote this adjusted inference A-RRP). Throughout the simulation study, we let X be the xy-coordinate of the observations and \u03b2 = (1, 1) T . We simulate W from the Mat\u00e9rn covariance function with \u03b8 = (\u03bd, \u03c3 2 , \u03c6) T = (2.5, 1, 0.2) T , which has the form as below (Rasmussen and Williams, 2005 , Section 4.2):\nWe use a vague multivariate normal prior N (0, 100I) for regression coefficients \u03b2, inverse gamma prior IV G(2, 2) for \u03c3 2 and uniform prior U (0.01, 1.5) for \u03c6. We have experimented with different choice of prior; the inference performances are similar. To evaluate our approaches, we compare inference performance with a focus on the posterior mean estimates and 95% equal-tail credible intervals of \u03b2, and we compare prediction performance based on mean square error."}, {"section_title": "Linear case", "text": "The random projection models are first assessed under the linear case (details are presented in the online supplementary materials S.2). Let x 1 , x 2 denote the xy-coordinates. We simulate data from\nwhere the noise has variance \u03c4 2 = 0.1.\nWe fit both FRP and RRP models using rank m = 50 based on the marginal distribution of Y | \u03b2, \u03c6, \u03c3 2 , \u03c4 2 as described in (6) and (7), and we use IV G(2, 1) prior for \u03c4 2 . For the linear case, fitting the full SLMM and RSR model is fast for data of size n = 400, so we compare results across all four models. Our results show that inference and prediction provided by the random projection models are similar to the original models they approximate. As noted by Hanks et al. (2015) , when the data are simulated from the full SLMM, we see a low \u03b2 coverage for the RSR model; therefore, its approximated version RRP also has a low coverage.\nHowever, this problem is resolved after a simple adjustment (A-RRP) as recommended by Hanks et al. (2015) .\nWe also conduct a simulation study for larger data size n = 1000, and we fit both FRP and RRP with rank m = 50. Our results show that the distributions of \u03b2 estimates for both FRP and RRP are centered around the true value, and the distributions are comparable.\nCoverage of 95% credible intervals for FRP and A-RRP are comparable to the nominal rate. For prediction performance, the mean square error is similar for both models and the predicted observations at testing locations recover the spatial patterns well."}, {"section_title": "Binary case", "text": "The main goal of our approximation method is to fit spatial generalized linear mixed models for large data sets. Here we examine our model performance under the binary case generated with a logit link function logit(p) = log {p/(1 \u2212 p)}. We compare two simulation schemes:\nthe confounded case \u03b7 = x 1 + x 2 + W , and the orthogonal case \u03b7 = x 1 + x 2 + P \u22a5\n[X] W . For both cases we use the same parameter values as the linear case and simulate W from N (0, \u03c3 2 R(\u03c6)). We consider two simulation schemes because in practice we do not know whether there are spatial latent variables that may be collinear with our covariates. A careful approach, therefore, involves fitting both FRP (8) and RRP (9) models under both schemes to get a fair assessment of the FRP and RRP approaches. Because it is hard to fit full SGLMMs and the RSR models for moderate data size, we compare only FRP and RRP models for 100 simulated datasets of size n = 1000. Although we do not have a comparison with the original model fit, we can look at how well the true parameters are recovered and compare the prediction mean square error to judge the projection-based models.\nOur simulation results show that under the confounded case, \u03b2 estimates for both FRP and RRP have similar distributions (Figure 4 ). However the coverage of RRP, about 41%\n, is much lower than the 95% nominal rate. This is because the credible intervals obtained under the RRP are similar to the ones for RSR models, which are likely to be inappropriately narrow (Hanks et al., 2015) ; Figure 5 shows the estimated probability surface for the binary field at the training locations and the predicted probability surface at the testing locations under the confounded simulation scheme. We see that our projection-based approaches work well in recovering the true spatial pattern (results for the orthogonal simulation scheme are similar, hence not shown). Although the predictive surface seems somewhat smoother than the true surface, this could be because binary outcomes do not provide enough information for the latent variable."}, {"section_title": "Poisson case", "text": "We also examine our model performance under the Poisson case. The results are similar to the Binary case; hence, for brevity we summarize our results here and present the full results Figure 5: First row shows the estimated probability surface at all training locations. Second row shows the predicted probability surface on a 20x20 grid using random projection models. Left column is simulated data, middle column shows the FRP, and right column shows the RRP. Comparing the patterns from the true model (left column) to the ones from projection-based models using rank m = 50 (middle and right columns), we see the projection-based models are able to recover the true value quite well. FRP = full model with random projection, RRP = restricted model with random projection, A-RRP = adjusted inference for RRP.\nin the online supplementary materials. We simulate Poisson data with a natural logarithm link function using the same parameter values as the linear case; again, both simulation schemes are considered. Under the confounded simulation scheme, FRP and RRP have similar distribution for point estimates; RRP provides precise but inaccurate estimates, but after adjustment, A-RRP produces reasonable coverage. Under the orthogonal simulation scheme, RRP performs much better than FRP in terms of both point estimates and credible intervals. For both cases, the adjusted inference A-RRP is similar to the FRP, hence we can fit only the RRP model for its computational benefits and recover the results for fitting FRP. Figure 6 shows the estimated expectation of the Poisson process (log scale) at the training locations and the predicted expectation (log scale) at the testing locations under the confounded simulation scheme. We see that the projection-based models work well in recovering the true (results for the orthogonal simulation scheme are similar, hence not shown)."}, {"section_title": "Comparison to Predictive Processes", "text": "The predictive process approach (Banerjee et al., 2008) has been very influential among reduced rank approaches. In the context of spatial generalized linear mixed models for non-Gaussian data, we believe our approach offers some benefits over the predictive process approach: (i) We avoid having to choose the number and locations of knots. Instead, our approach requires specifying the rank for which we have a heuristic; no further user specifications are required. (ii) We provide an approach to easily alleviate spatial confounding.\n(iii) The reparameterization in our projection-based approach results in decorrelated parameters in the posterior, thereby allowing for a faster mixing MCMC algorithm. Simulation results show that our approach is comparable to the predictive process in terms of inference and better in terms of prediction. Our approach also allows us to fit and study both the Second row shows the predicted Poisson mean surface (in log scale) on a 20x20 grid using projection-based models. Left column is simulated data, middle column shows the FRP, and right column shows the RRP. Comparing the pattern from the true model (left column) to the ones from our models using rank m = 50 (middle and right columns), we see the projection models are able to recover the true value quite well. FRP = full model with random projection, RRP = restricted model with random projection, A-RRP = adjusted inference for RRP."}, {"section_title": "Applications", "text": ""}, {"section_title": "Binary data application", "text": "We apply our approach to classify rock types using a reference synthetic seismic data set.\nFluvsim is a computer program that produces realistic geological structures using a sequential scheme; it is used for modeling complex fluvial reservoirs (Deutsch and Wang, 1996) .\nThe high-resolution 100x120x10 3-dimensional grid data set is simulated from the program fluvsim conditioning on well observations. A similar reference data set obtained from fluvsim has been used to test the classification method in John et al. (2008) . Here we illustrate our projection-based approach on one layer of the rock profile . In the data set, there are five rock types: crevasse, levies, border, channel and mud stone. We combine crevasse and mud stone as one group and treat the rest as the other group for binary classification. Along with the rock type data, we also have acoustic impedance data that is associated with the rock properties; it is desirable to identify rock types from seismic-amplitude data using statistical methods (John et al., 2008) .\nWe fit both FRP and RRP models at 2000 randomly-selected locations and predict the rock profile on a 24x30 grid. Prior to fitting the projection-based models, the rank is selected by fitting non-spatial logistic regression models with synthetic spatial variables as described in Section 4.6. The BIC values from the resulting models suggest that rank m = 50 is sufficient. In order to help diagnose convergence, we ran multiple chains starting at dispersed initial values and compared the resulting marginal distributions while also ensuring that the MCMC standard errors for the expected value of each parameter of the distribution was below a threshold of 0.02 (cf. Flegal et al., 2008) .\nEstimated coefficients corresponding to x-coordinates, y-coordinates and impedance covariates differ between FRP and RRP models, which are (\u22120.717, 0.448, \u22120.233) T and (\u22121.587, 2.435, \u22120.396) T , respectively. Although interpretations for the estimates are slightly different the predictions, which are of primary interest, are identical between the two models (see Figure 7) . We also assess the predicted rock profile using higher ranks; however, the results are similar to using rank 50, hence they are not shown here. The time to fit either the FRP or RRP model is about 10 hours, whereas for the full model, it would have taken about three weeks to run the same number of MCMC iterations. In general, fitting SGLMM to binary observations is harder due to the poor Markov chain mixing; therefore comparison with the full model is prohibitively expensive. "}, {"section_title": "Count data application", "text": "Here we illustrate the usefulness of the projection-based models in the context of an environmental study. We consider the relative abundance of house finch (Carpodacus mexicanus), a bird species that is native to western North America (Elliott and Arbib Jr, 1953) . Figure   8 shows the number of bird counts obtained in 1999 by the North American Breeding Bird\nSurvey with the size of the circle is proportional to the number of counts. The bird surveys are obtained along more than 3,000 routes across the continental US. There are 50 stops per route, spaced roughly 0.5 miles apart. The observer make a three-minute point count at each stop. The bird count is then the total number of birds heard or seen for all 50 stops (Pardieck and Campbell, 2016) .\nThe data set being analyzed has 1257 highly irregular sampling locations. Here we fit the FRP model to approximate the SGLMM with only the intercept term for spatial interpolation. The time to fit FRP is about 7 hours, while the full model would take almost 2 days for the same number of MCMC iterations. Figure 8 shows the abundance map predicted by FRP on a high resolution of 40 x 100 grid. Not surprising, the abundance map is smooth. This reflects that the bird counts are very small in the center and most of the east coast of the US. Our map is also consistent with the observation that large counts are centered near New York area and the West Coast. Size of the circle is proportional to the bird counts. Predicted bird counts on a grid using the random projection method (right). The large number of observed counts on west coast and lack of observations in the central mid-west region is reflected on the prediction map."}, {"section_title": "Discussion", "text": "In this paper, we have proposed projection-based models for fast approximation to SGLMMs and RSR models. Our simulation study shows that our low rank models have good inference and prediction performance. The advantages of our approach include: (1) a reduction in the number of random effects, which lowers the dimensionality of the posterior distribution and decreases the computational cost of likelihood evaluations at each iteration of the MCMC algorithm,; (2) reparameterized and therefore approximately independent random effects, resulting in faster mixing MCMC algorithms; and (3) the ability to adjust for spatial confounding.\nOur simulation study shows both the restricted and unrestricted models provide similar results in prediction. RRP provides superior inference when the true model does not have confounding (and hence the spurious confounding effect needs to be removed); it is also computationally more efficient due to its faster mixing. Therefore, we recommend that in general users fit RRP models. If there is concern that the true model may actually exhibit confounding, we recommend adjusting the fixed effects a posteriori to recover the inference from FRP as recommended in Hanks et al. (2015) . As we demonstrate here, this is easy to do in practice.\nThe current methods rely on parallelization to handle large matrix computations; we have successfully carried these out for n of around 10,000. If we combine parallelization with a discretization of possible values of \u03c6 (to allow for pre-computing the eigendecomposition of the covariance matrix), this approach will likely scale to tens of thousands of data points.\nThe INLA approach (Rue et al., 2009 ) provides a fast approximate numerical method for carrying out inference for latent Gaussian random field models. An interesting avenue for future research is combining our reduced-dimensional reparameterization with INLA.\nThere have been a number of recent proposals for dimension reduction and computationally efficient approaches for spatial models. These include the fixed rank approximation by Cressie and Johannesson (2008) , predictive process by Banerjee et al. (2008) and random projection approach for the linear case Banerjee et al. (2012) . Our approach can be thought of as a fixed rank approach, but we use the approximated principal eigenfunctions as our basis. The advantage is that we have independent basis coefficients and our approximation minimizes the variance of the truncation error as described in Section 4. Our approach is also related to the predictive process in that we effectively subsample random effects (see discussion in Banerjee et al., 2012) . Developing extension of this methodology to spatial-temporal and multivariate spatial processes may provide fruitful avenues for future research.\nSupplementary materials to \"A Computationally Efficient Projection-Based Approach for Spatial Generalized Linear Mixed Models\" by Guan and Haran"}, {"section_title": "S.1 Full Conditionals for Projection-Based Approaches", "text": "The joint posterior distribution for the full model with random projection is \u03c0(\u03b4, \u03b2, \u03c3 2 , \u03c6 |\n. From this we derive the full conditionals, shown below, which can be easily sampled using one-variable-at-a-time\nMetropolis-Hasting algorithm.\nThe full conditionals for the restricted model with random projection is similar to the above except that U m is replaced by P"}, {"section_title": "S.2 Simulation Study Results", "text": "For the linear case, we simulate 100 data sets from the spatial linear mixed model (confounded simulation scheme) for data sizes of n = 400 and n = 1000. For the smaller data size, we fit both of our projection-based approaches, the spatial linear mixed model and restricted spatial regression model for overall comparisons. The distribution of \u03b2 estimates all center around the true value and are comparable among all four models ( Figure 9 ); inference and prediction provided by our projection-based approaches are similar to the original models they approximate (Table 1) . For the larger data size we fit both FRP and RRP with rank m = 50, which is selected based on our heuristic described in the main text. Figure 10 shows the estimated random effects at the training locations and the predicted observations at the testing locations. We see that our projection-based approaches work well in recovering the spatial patterns. Figure 9 : Distribution of posterior mean estimates of \u03b2 among four models and with adjustments. The distributions all center around the true value and are comparable. Random projection models FRP and RRP with rank=50 produce results that are similar to the models they approximate.\nFor the Poisson case, we simulate 100 data sets from the spatial linear mixed model (confounded scheme) and restricted spatial regression model (orthogonal schemes) for data sizes of n = 1000. Under the confounded simulation scheme, FRP and RRP have similar distributions for point estimates ( Figure 11) ; however, the credible interval(CI) of RRP is "}, {"section_title": "S.3 A Comparison with an Existing Method for Areal Data", "text": "Here we compare our approach with an existing method for lattice/areal data (Hughes and Haran, 2013) . We simulate a count data set with n = 900, \u03c4 = 1 from:\nThe ICAR model has improper prior, meaning its precision matrix is rank deficient; therefore, direct simulation from (10) is not feasible. Hence, the spatial random effects is simulated using the eigencomponents of the precision matirx Q. Let (\u03bb i , e i ) denote the eigenpairs of\nTo reduce the dimension of W using RRP, we will first invert Q using generalized inverse, then approximate Q \u22121 using Algorithm 1 from the main text. The full conditionals of RRP for this reparameterized model can be easily derived. We then fit both RRP and HH to the simulated data set for comparison. Figure 12 shows that the marginal posterior density plot are similar from the two models."}, {"section_title": "S.4 A Comparison with Predictive Process for Point-Referenced Data", "text": "To compare the performance of our projection-based approaches with the predictive process, we simulate 100 Poisson data sets from the traditional SGLMM. We fit both FRP and RRP with rank m = 50 to the datasets, and compare their results with the predictive process with reference points on a 7 \u00d7 7 grid. In this simulation study, our projection-based approaches provide comparable inference and smaller mean prediction square error (MPSE) (Figure 13 )."}, {"section_title": "S.5 SGLMMs with small-scale (nugget) spatial effect", "text": "For SGLMMs where inclusion of small scale, non-spatial heterogeneity is appropriate, the model becomes, g {E(Z(s) | \u03b2, W (s))} = X(s)\u03b2 + w(s) + (s),\nwhere (s) iid \u223c N (0, \u03c4 2 ). We provide implementations of our method for two cases: (1) when Gibbs sampling of the latent variables is available, and (2) when it is not. Examples for case\n(1) are the spatial binary model with probit link (considered by Berrett and Calder, 2016) and spatial probit model for correlated ordinal data ; examples for case (2) are already considered in this manuscript.\nWe begin by redefining some notation. Let W = (W 1 , . . . , W n ) T denote the latent variable, Z = (Z 1 , . . . , Z n ) T the observed spatial binary data and X the n \u00d7 p design matrix."}, {"section_title": "Case (1):", "text": "We first consider the case where Gibbs sampling is available for the latent variables, for example when using SGLMM with a probit link for binary data. The model is defined as while the coverage for RRP is much lower than the others, however after adjustment, the coverage for A-RRP is corrected and is comparable to FRP and PP (top right). Both FRP and RRP have better prediction performance than PP (bottom left). The length of the CIs for FRP and PP are comparable, while RRP produce much narrower CI; but after adjustment the CI gets much wider (bottom right). FRP = full model with random projection, RRP = restricted model with random projection, A-RRP = adjusted inference for RRP.\nis therefore multivariate normal with mean X\u03b2 and variance \u03c3 2 R \u03c6 + \u03c4 2 I. Our method can be used to facilitate model fitting in this case as follows: We approximate the eigen-components of R \u03c6 using random projections and obtain its first m eigenvectors U \u03c6 = [u 1 , . . . , u m ] and eigenvalues D \u03c6 = diag(\u03bb 1 , . . . , \u03bb m ). Let M \u03c6 = U \u03c6 D 1/2 \u03c6 be the projection matrix, then we reduce the dimension of the latent variables by approximating W with M \u03c6 \u03b4.\nFor a specific value of \u03c6, we can treat M \u03c6 as fixed spatial covariates and \u03b4 the corresponding coefficients. Write X \u03c6 = [X, M \u03c6 ] and \u03b2 \u03c6 = (\u03b2 T , \u03b4 T ) T as the reparameterized design matrix and coefficients, respectively, then Y i is approximated by X i \u03b2+M \u03c6,i \u03b4+ i and can be rewritten as X \u03c6,i \u03b2 \u03c6 + i . We use a normal conjugate prior for \u03b2, inverse gamma conjugate priors for \u03c3 2 and \u03c4 2 , and a uniform prior for \u03c6. Then, fitting the reduced-rank Bayesian probit model involves the following steps.\nAt the t th iteration of the algorithm,\nStep 1: Gibbs update for latent variables. Sample Y (t) from Y |Z, \u03b2 (t\u22121) , \u03c3 2 (t\u22121) , \u03c6 (t\u22121) , \u03c4 Step 2: Gibbs update for \u03b2 \u03c6 .\nSample from \u03b2 \u03c6 | Z, Y (t) , \u03c3 2 (t\u22121) , \u03c6 (t\u22121) , \u03c4 2 (t\u22121) \u223c MVN \u03b2 \u03c6 , ( Step 3: Gibbs update for \u03c4 2 .\nStep 4: Gibbs update for \u03c3 2 .\nStep 5: Metropolis-Hastings update for \u03c6.\nWe have not provided details for steps 3-5 since they remain the same as when fitting SGLMMs in general. Furthermore, techniques for dealing with non-identifiable parameters Calder, 2012, 2016) can also be used.\nCase (2): We now consider the case where Gibbs sampling from the latent variable is not available. We first explain why the reparameterization for Case (1) is not suitable here, and then provide an alternative strategy. In Case (1) above, W is reparameterized with a low-rank representation, however, the dimension of latent variable Y remains high;\nY is approximated by X\u03b2 + M \u03c6 \u03b4 + , and has a normal distribution with mean X\u03b2 and covariance \u03c3 2 M \u03c6 M T \u03c6 + \u03c4 2 I. Constructing efficient MCMC to sample Y from its full conditional distribution is not easy due to its high dimensions. Hence, we propose an alternative:\nreduce the dimension of Y by approximating W + with U \u03b8 D 1/2 \u03b8 \u03b4, where U \u03b8 and D \u03b8 are eigenvectors and eigenvalues of \u03c3 2 R \u03c6 + \u03c4 2 I, respectively. Hence, the eigencomponents here depend on all parameters \u03b8 = (\u03c3 2 , \u03c6, \u03c4 2 ) T of the covariance function. In fact U \u03b8 is identical to U \u03c6 from Case (1), and D \u03b8 is identical to \u03c3 2 D \u03c6 + \u03c4 2 I m\u00d7m . This alternative reparameterization provides some computational gains. The latent variable Y is now approximated by\nT whose full conditional distribution has m + p dimensions.\nReducing the dimension of the posterior distribution allows for easier construction of efficient MCMC."}]