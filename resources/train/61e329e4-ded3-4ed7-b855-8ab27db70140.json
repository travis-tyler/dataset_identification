[{"section_title": "Introduction", "text": "This paper supplements and extends information on response provided by the Interagency Group on Establishment Nonresponse (IGEN), focusing on work at the Bureau of Labor Statistics (BLS) and the Census Bureau. (Interagency Group on Establishment Nonresponse, 1998;Shimizu, 2000;Ramirez, et al., 2000). We first consider survey response measurement for BLS and Census Bureau surveys. We discuss each agency's approach to defining and measuring response rates, and provide examples of response rates and trends. In Section III, we examine methods that the BLS and Census Bureau have used to encourage response. We follow with a discussion of nonresponse reduction research that the two agencies have conducted before concluding."}, {"section_title": "Survey Response Measurement 2.1. BLS", "text": "In the 1980's, BLS developed a framework for computing similar response rates across all BLS surveys. Over the last several years, response rate definitions and formulas for each survey were revised to conform to the BLS-wide framework. Using the response rates computed using these definitions and formulas, BLS has begun analyzing response rates across similar surveys. This section presents BLS's framework and definitions, and describes the current status of the agency-wide analysis."}, {"section_title": "Overview of BLS Surveys", "text": "We review four major BLS establishment-based surveys that were studied qualitatively as part of the BLS response rate initiative. Table 1 provides a summary of these four BLS establishment-based surveys in terms of Office, purpose, scope, sample, and collection methods (Ferguson, et.al, 2003)."}, {"section_title": "Standardized Information on Data Collection", "text": "In March 1985, the BLS formed a Data Collection Task Force (DCTF) to develop a system for compilation of standardized information on data collection across programs. The task force (Bureau of Labor Statistics, 1985) recommended a framework of accountability codes that: \u2022 are mutually exclusive and exhaustive subsets of the next higher level, \u2022 are applicable to any BLS establishment or housing unit survey, \u2022 reflect the longitudinal nature of most BLS surveys, \u2022 allow for the differentiation of data collection and estimation operations, \u2022 are consistent with the standard definition of a response rate, \u2022 allow for the computation of field collection completion rates, \u2022 provide the capability of mapping all current BLS classification schemes into it. The task force developed the data collection and estimation phase classification schemes shown in Figures 1 and 2, respectively. This proposed framework supports the following definition of an unweighted response rate which may be useful in monitoring operations: The definition of a responding unit depends on the operation being monitored (see next section). Each survey can also use this strategy to compute weighted response rates for measuring value coverage from respondents by summing the appropriate weight across all units in the category. Depending on the survey, the weight may be the inverse of the probability of selection while it may be the current employment or volume of trade for other surveys. Over the nineties, BLS staff initiated efforts to ensure that all surveys were collecting response codes that could support this framework. Managers supported the taxonomy by ensuring that revisions to the processing systems and collection protocols would be consistent as surveys modernized and updated their methodologies and computer systems. Over the late nineties, response rates in general were declining. Most survey managers reported that maintaining good response rates was becoming more difficult. Program managers routinely only monitored response of active sample members, generating a survey specific stage of processing response rates. Even though individual programs could aggregate their response codes into a compatible taxonomy, the response codes differed from program to program because of different internal monitoring requirements. Thus, the BLS could not use the individual response codes monitored by each survey to identify systematic problems across surveys."}, {"section_title": "Current Practices", "text": "In early 2000, a team was formed to compile response rates based upon the DCTF methodology rather than based upon actual production samples in order to develop a BLS-wide strategy for improvement initiatives. This team generated its first report in October 2000, including response rates from 14 surveys, including household and business surveys. The team updates the report every three months to include as much data as possible from as many surveys as possible. Some surveys provide stage of processing rates as well as overall survey response rates while other surveys provide only an overall rate or only one or more stage of processing rates. Table 2 shows a recent summary of the unweighted response rates that appear in this quarterly report (Bureau of Labor Statistics, 2003) for the four surveys in this paper. All surveys except NCS have multiple closings. The CES, PPI, and IPP are monthly surveys that have short data collection periods to meet tight release dates. Due to unavailability of data in a reporting period, sampled establishments can report data in a later period for a scheduled revision. For surveys with multiple closings, we are presenting the response rates for first closing. Column 2 shows the initial data collection response rate. This is the response rate based on the initial contact with the establishment for the individual survey. For most surveys, this rate is computed based on sampled establishments. An establishment is considered cooperative if the company agreed to provide any of the requested data. Column 3 shows the update collection response rate. Where applicable, this is the response rate for the most recent update period for the survey. The update collection response rates show the ratio of establishments (or quotes) for which the survey collected any data during the update period, whether the data was usable for estimation purposes or not. This rate only applies to surveys that perform an initiation process to gain initial cooperation and then gather updated data on a regular basis for several years. Column 4 shows the update estimation response rate. This rate includes only establishments (or quotes) for which the company provided enough data to be included in the actual survey estimates. Both the Update Collection and Update Estimation response rates include in the denominator only those items that were obtained at the initiation contact. This rate only applies to surveys that perform an initiation process to gain initial cooperation and then gather updated data on a regular basis for several years. Column 5 shows the total survey response rate. The last column shows the overall survey response rate, when available. The numerator includes all data used in the estimation process while the denominator includes all in-scope sampled units. The PPI cannot estimate the rate at this time."}, {"section_title": "Trends in Response Rates at BLS", "text": "Response rates have been relatively stable over the last 5 years at BLS. Table 3 compares the current response rate to the average of response rates over 3, 12, and 36 months. Three trends are discernable: \u2022 The response rates for the CES have been improving. This was a result of an intensive effort by the Computer Assisted Telephone Interviewing (CATI) centers to elicit response from companies and maintain continued response from sample attritors over time. \u2022 The response rate for the NCS has improved because of the introduction of a new sample panel. Due to a major re-design of the program sample replacement schemes had been curtailed for a time. \u2022 The response rates for the IPP-Export and IPP-Import have fallen slightly over time."}, {"section_title": "Census Bureau", "text": "The Economic Programs Directorate of the Census Bureau conducts an economic census every five years and conducts current economic surveys monthly, quarterly, annually, and a few surveys less frequently than annually. In 1993 the Directorate adopted standard definitions for two response rates for economic programs (Waite, 1993). These definitions are widely used by the Economic Directorate's methodologists, survey practitioners, and system developers. In 1995 the Directorate decided to consolidate multiple processing systems for current surveys by developing a Standard Economic Processing System, referred to as StEPS (Ahmed and Tasky, 1999, 2001. Some response rate measures are calculated by this system. This section presents the Economic Programs Directorate's response rate definitions and some response rate trends."}, {"section_title": "Response Rate Computations --Current Practices", "text": "The Census Bureau's Economic Programs Directorate has adopted two standard definitions for response rates. The first is useful for monitoring progress. The second is useful for monitoring value or quantity coverage from actual respondents. The first response rate measures the proportion of attempted cases that provide a response, where an attempted case is a case for which data collection has been attempted. It is defined as follows: Response rate #1 = R / M, where: R = the number of units which provide a response, and M= the number of units for which one attempts to obtain a response. The second response rate measures the proportion of an estimated total (not necessarily the published total) that is contributed by respondents for an individual variable. It is defined as follows: where: w i = the design weight of the i th unit before adjustments for nonresponse, t i = the reported value for the i th unit of variable t for which the response rate is to be computed, and T = the estimated (weighted) total of the variable t over the entire population represented by the sampling frame. Response Rate Number 1 is frequently labeled \"return rate.\" It is generally calculated only at disaggregated levels-for example, disaggregated by mode of data collection, questionnaire version, or sizebased strata. Among Census Bureau surveys, there have been varied interpretations of whether returned forms that do not contain any respondent data or do not contain respondent data for specified key items should be included in the numerator. Response Rate Number 2 excludes imputed data from its numerator. Consequently, the quantity (1 -Response Rate # 2) is frequently calculated and is labeled \"imputation rate.\" The Economic Directorate includes imputation rates in the explanatory notes of press releases and in the \"Reliability of the Estimates\" section of publications. The Census Bureau's publication guidelines encourage the discussion of sources and magnitudes of errors in published estimates. Imputation rates are easily discussed in connection with the published estimates for different items. Response rates, on the other hand, are associated with the response process and not as easily discussed in connection with the published estimates. StEPS calculates response measures similar to Response Rate Number 1 in its management information module. This module is primarily used to monitor the progress of data collection operations and initial data editing. StEPS calculates imputation rates equal to (1-Response Rate Number 2) in its estimates and variances module. The definition of Response Rate Number 2 excludes from the denominator \"administrative records used in place of a planned attempt to collect data.\" The StEPS imputation rate, however, includes administrative data in the denominator when administrative data are included in the published estimate. For surveys that use weight adjustment to handle unit nonresponse, StEPS treats this as a type of imputation because the adjusted weights allow units that report to represent both reporting and non-reporting units. For surveys that use imputation to handle unit nonresponse or item nonresponse, StEPS calculates imputation rates based on the outcomes of processing performed in the StEPS general-imputation module. The general-imputation module imputes data using estimator type techniques (Giles and Patrick, 1986) and adjusts data items associated with additive relationships so that detail items sum to total items (Sigman and Wagner, 1997). With one exception, all reported item data changed by the imputation module are flagged as being imputed item data. This includes (1) items for which no data were reported, and the general-imputation module creates data; and (2) reported data that fail defined edits, and as a result the general-imputation module changes some of the data. The one exception is when reported or missing data are replaced by administrative data that are considered to be equivalent in quality to respondent-provided data. In this case, the changed data are treated neither as imputed data (used to calculate the imputation-rate numerator) nor as reported data (used to calculate the numerator of Response Rate Number 2) but are used to calculate the numerator of an associated administrative-data rate. The calculation of imputation rates requires that StEPS maintain tracking information indicating if a case is active, if the case has responded, and if the case has not responded how nonresponse is to be handled during processing. StEPS stores a status code for each case indicating if the case is active or inactive. StEPS also stores a \"coverage code\" for each case that specifies a reason why StEPS handles the case the way it does during data collection and subsequent data processing. The first and second columns of Figure 3 list the StEPS coverage codes for active cases, and the third column lists the coverage codes for inactive cases. To indicate response status, StEPS maintains a response code for each case indicating if the case has responded and if it has not, whether it is to be imputed or if it is contained in a subpopulation for which weight adjustment will be used to handle unit nonresponse. For nonrespondents, StEPS has the capability to track the classification of a case as a \"hard refusal,\" in which the respondent informs the Census Bureau that it will not participate, or as a \"soft refusal,\" in which the respondent does not report over a period of time but never actually informs the Census Bureau that it will not participate."}, {"section_title": "Trends in Response Rates at the Census Bureau", "text": "This paper reviews the response rate trends for two monthly Census Bureau establishment surveys -the Monthly Retail Sales Survey (12,000 sample units) and the Monthly Wholesale Survey (4,000 sample units). The sampling units for these surveys are aggregates of establishments. Both surveys are voluntary and use mail with telephone followup as the data collection method. We selected these surveys because they have maintained records of their response rates over a twelve-year period. Figures 4 and 5 display response rates (i.e., Response Rate # 2 = 1-Imputation Rate) for these two Census Bureau surveys. Figure 4 displays the response rates for retail sales in the Census Bureau's Monthly Retail Trade Survey. Figure 5 displays response rates for sales for the Census Bureau's Monthly Wholesale Survey. Both cover April 1991 through November 2003. These two surveys are redesigned approximately every five years. Between April 1991 and November 2003, new samples for these surveys were introduced at the beginning of 1987, 1992, 1997, and 2000. In the figures, the different samples are labeled \"BSR\" (for \"Business Sample Redesign\") followed by the year that the new sample is introduced. The new sample and old samples overlap for three months. Response rates tend to increase for a short time after a new sample is introduced, but then tend to decrease. The BSR 2K produced large increases in response rates. A possible reason for this is that in order to decrease respondent burden, some small and medium size firms that were in the BSR 97 sample were not selected for the BSR 2K sample. This procedure had not been used in earlier sample revisions. Kinyon, et al. (2000) provides additional details about the sample revision. Possible additional reasons for the increases in response rates with the introduction of the BSR 2K sample include that for the first time ever the mandatory annual survey was mailed prior to the voluntary monthly surveys and extra resources were devoted to training clerical staff to increase response and to monitor response progress."}, {"section_title": "BLS vs Census Bureau Differences in Response Rates", "text": "In addition to design and content differences in surveys conducted by the two agencies, differences in response rates may be due to differences in authority and data collection mode. Many Census Bureau surveys are mandatory and as a result obtain high response rates. Respondents may also think of the Census Bureau nonmandatory surveys as mandatory, resulting in higher response rates. On the other hand, except for the annual refiling survey for a few states and one national survey, the BLS surveys are voluntary. To compensate for the voluntary nature of the BLS surveys, BLS uses interviewers in the initiation process, if not for routine data collection. BLS turns to self-administered modes only after sample initiation and/or indoctrination, while the Census Bureau relies on self-administration alone for nearly all of its survey or census programs, using personal intervention (usually by telephone) only for nonresponse reminders or follow-up."}, {"section_title": "Methods to Encourage Response", "text": "Many methods used by the BLS and Census Bureau to reduce nonresponse on their establishment surveys run the gamut of traditional survey nonresponse reduction strategies, while some methods reflect characteristics more unique to establishment surveys. Both BLS and the Census Bureau conduct presurvey notification activities, providing advance notification to respondents of upcoming survey contacts. BLS Regional Offices have begun tailoring their contacts to characteristics of the establishment, especially when they deal with large establishments, and advance letters and other pre-survey information have been altered to fit the establishment. To the extent practical, both agencies tailor their questionnaires by industry, and offer multiple reporting modes simultaneously, including touch-tone data entry/voice recognition entry (TDE/VRE), fax and electronic options. Both conduct outreach and survey promotion through trade shows and contact with industry organizations. BLS Regional Offices also hold open houses to make potential respondents more aware of BLS survey programs. BLS and the Census Bureau focus the most intensive levels of outreach or nonresponse follow-up activities on selected cases, usually very large businesses, having the greatest potential impact on estimates. Both agencies undertake personalized contact with selected companies to encourage response. BLS and the Census Bureau have been working to reduce bureaucratic barriers between survey programs and to create an integrated approach to nonresponse reduction across surveys. The Census Bureau accomplishes this for very large multi-unit companies through its Customer Relationship Manager (CRM) program. CRMs act as Census Bureau liaisons with company contacts from more than 60 very large U.S. companies, serving as single points of contact for these companies' reporters. They provide quick, accurate answers about any of the various current survey programs in which a company participates and try to help coordinate reporting across programs. Within the Census Bureau, CRMs bring together company experts from each survey program and use a team approach to develop strategies that address complex company reporting issues. CRMs have also developed an internal repository of company and survey information to facilitate information sharing across survey programs. BLS Regional Offices coordinate contacts with large or multi-unit firms across surveys. These coordinated efforts include the design of promotional materials that highlight all BLS products. Refusal avoidance and reluctance training may be provided to groups of data collectors working on different BLS programs, allowing the staff to share insights from different surveys. At BLS, training of data collectors plays a critical role in gaining and maintaining cooperation. The Census Bureau, on the other hand, must take advantage of different techniques to encourage and maintain response on its many self-administered surveys. According to Monsour (1998), most economic surveys conducted by the Census Bureau 1) used one to four follow-up mailings, with or without a replacement questionnaire, and 2) switched from mail to telephone for selected chronic nonrespondents; however, use of personal visits for data collection was rare. Petroni, et al. 2004, provides a list of many additional nonresponse reduction strategies implemented by the Census Bureau."}, {"section_title": "Research on Nonresponse Reduction", "text": "Recent qualitative research at the Census Bureau provides some insights and suggests some hypotheses regarding businesses' motivations for responding to surveys. (Willimack et al. 2002). The findings led Willimack et al. to formulate a conceptual framework for business survey participation. Results of the research reported by Willimack et al. (2002) led to several initiatives to improve response to economic surveys at the Census Bureau (Sudman, et al., 2000). A number of activities were directed to reducing respondent burden directly associated with nonresponse or to improve services offered to respondents to maintain or improve response (Petroni, et al., 2004). The framework proposed by Willimack and her colleagues was adopted as the point of departure for a BLS study designed to learn more about the nature of establishment survey nonresponse (Fisher, et al., 2003;Fox, et al., 2002). It focused specifically on nonresponse trends, causes of nonresponse, patterns in nonresponse, and possible solutions to nonresponse. The ultimate outcome of this research is the identification and implementation of improved data collection procedures that will address establishment survey nonresponse problems (Petroni, et al., 2004). Studies of reasons for nonresponse in the 1998 Manufacturing Energy Consumption Survey (Ware-Martin, et al., 2000), and the 1994 Survey of Industrial Research and Development (U.S. Bureau of the Census, 1997) (Petroni, et al., 2004) found the main reasons for nonresponse to be lack of time to complete the form, data availability, difficulty identifying the appropriate respondent and noncontact."}, {"section_title": "Conclusions", "text": "The BLS has developed a corporate approach to measuring survey response that relies on standard definitions and formulas ensuring that rates can be compared across the various establishment surveys. It has also begun to implement this approach and is now computing some type of survey response rates for each survey on a regular basis. However, ensuring that all rates are in complete conformance with the corporate approach and are available at all levels of desired detail will take several more years to complete. The process of changing the disparate survey processing systems to collect all the needed data is complex and time consuming. However, BLS is optimistic that development of a BLS-wide system will enable them to compute and compare response rates across surveys in the not too distant future, so that trends in the rates across surveys can be examined. In order to develop a corporate strategy to improve response rates, the BLS has adopted a proposal for computing disaggregated response rates across all programs. This effort should enable BLS to compare similar respondents and nonrespondents to identify BLS-wide response problem areas. Where appropriate, the hope is to begin computing disaggregated response rates by collection area/region, size of sample unit, industry classification, and survey mode (i.e. mail, fax, telephone, internet, etc.). Most surveys are collecting the data necessary to compute these disaggregated rates. Plans have been proposed to collect some additional company demographics that would also help explain survey response above what is currently collected. BLS's priorities for reducing nonresponse include increasing BLS visibility with respondents, accelerating the introduction of additional data reporting options (including Internet reporting), evaluating existing contact and initiation strategies, producing more relevant and timely BLS publications, and bringing users and providers together. Other areas for future BLS research include ways to reduce burden, ways to enhance utility of BLS data for respondents, ways to increase BLS visibility, ways to improve contact and initiation strategies, and ways to improve the cost-effectiveness of data collection procedures (including methods of nonresponse followup). Currently the Census Bureau widely uses two standard definitions of response rates for their establishment surveys. The actual definition of components that define the rates can vary from survey to survey, depending on the intended use of the rate. Rates are typically obtained from the StEPS processing system, which has increased standardization in the way response rates are calculated. The Census Bureau is currently reviewing whether these definitions should continue to serve as the Census Bureau standard for establishment surveys. The philosophy of the Census Bureau's economic area is that issues related to response/nonresponse can and should be addressed through reducing respondent burden, providing better customer service, and adopting a \"company-centric\" point of view which recognizes all aspects of company reporting burden, resources, and organization during interactions with a company and during survey design to leverage Census Bureau and company resources and maximize each company's ability to respond. The agenda for continued research and development reflects these priorities.   S e 9 1 F e 9 2 J l9 2 D e 9 2 M y 9 3 O c 9 3 M a 9 4 A u 9 4 J a 9 5 J n 9 5 N v 9 5 A p 9 6 S e 9 6 F e 9 7 J l9 7 D e 9 7 M y 9 8 O c 9 8 M a 9 9 A u 9 9 J a 0 0 J u 0 0 N v 0 0 A p 0 1 S e 0 1 F e 0 2 J l0 Ap91 Oc91 Ap92 Oc92 Ap93 Oc93 Ap94 Oc94 Ap95 Oc95 Ap96 Oc96 Ap97 Oc97 Ap98 Oc98 Ap99 Oc99 Ap00 Oc00 Ap01 Oc01 Ap02 Oc02 Ap03 Oc03 Response Rate (%)= 100% -Imputation Rate (%) BSR 87 BSR 92 BSR 97 BSR 2K"}]