[{"section_title": "Abstract", "text": "Abstract-Sparse learning has been widely investigated for analysis of brain images to assist the diagnosis of Alzheimer's disease and its prodromal stage, i.e., mild cognitive impairment. However, most existing sparse learningbased studies only adopt cross-sectional analysis methods, where the sparse model is learned using data from a single time-point. Actually, multiple time-points of data are often available in brain imaging applications, which can be used in some longitudinal analysis methods to better uncover the disease progression patterns. Accordingly, in this paper, we propose a novel temporallyconstrained group sparse learning method aiming for longitudinal analysis with multiple time-points of data. Specifically, we learn a sparse linear regression model by using the imaging data from multiple time-points, where a group regularization term is first employed to group the weights for the same brain region across different time-points together. Furthermore, to reflect the smooth changes between data derived from adjacent time-points, we incorporate two smoothness regularization terms into the objective function, i.e., one fused smoothness term thatrequires that the differences between two successive weight vectors from adjacent time-points should be small, and another output smoothness term thatrequires the differences between outputs of two successive models from adjacent time-points should also be small. Manuscript received January 27, 2015; revised February 26, 2016 and April 5, 2016; accepted April 7, 2016 We develop an efficient optimization algorithm to solve the proposed objective function. Experimental results on ADNI database demonstrate that, compared with conventional sparse learning-based methods, our proposed method can achieve improved regression performance and also help in discovering disease-related biomarkers.\nIndex Terms-Alzheimer's Disease (AD), group sparsity, longitudinal data analysis, mild cognitive impairment (MCI), sparse learning, temporal smoothness."}, {"section_title": "I. INTRODUCTION", "text": "A LZHEIMER'S disease (AD) is the most common form of dementia, which leads to progressive loss of memory and cognition function [1] . As a prodromal stage of AD, mild cognitive impairment (MCI) tends to progress to probable AD at a rate of approximately 10-15% per year. Thus, early and accurate diagnosis of AD/MCI is of vital importance for early treatment and possible delay of disease. At present, many pattern classification and regression methods have been proposed for AD or MCI diagnosis and prognosis by using biomarkersfrom different modalities, e.g., structural brain atrophies measured by magnetic resonance imaging (MRI) [2] - [5] , metabolic brain alterations measured by fluorodeoxyglucose positron emission tomography (FDG-PET) [6] , [7] , and pathological amyloid depositions measured through cerebrospinal fluid (CSF) [3] , [8] - [10] , etc.\nRecently, sparse learning techniques have attracted increasing attention due to their excellent performances in a series of neuroimaging applications on different modalities. For example, in a recent study [11] , a voxel-based sparse classifier based on a L 1 -norm regularized linear regression model, also known as the least absolute shrinkage and selection operator (LASSO) [12] , was applied for classification of AD and MCI using MRI data, showing better performance than support vector machine (SVM), which is one of the state-of-the-art methods in brain imaging classification. In the literature, several other advanced sparse learning models (i.e., LASSO variants) have also been developed in neuroimaging applications. For example, researchers in [13] proposed to use elastic net [14] to identify both neuroimaging and proteomic biomarkers for AD and MCI based on MRI and proteomic data, and researchers in [15] proposed a generalized sparse regularization term with domain-specific knowledge for functional MRI-based brain decoding. Recently, group LASSO (gLASSO) [16] with a L 2,1 -norm regularization term was used for jointly learning multiple tasks including both classification tasks [e.g., AD/MCI versus Normal Controls (NC)]and regression tasks (e.g., estimation of clinical cognitive scores) with MRI data in [17] and multimodal data (i.e., MRI, FDG-PET, and CSF) in [18] , respectively. It is worth noting that both aforementioned methods assume that multiple regression/classification variables are inherently related and determined by the same underlying AD pathology (i.e., the diseased brain regions). With such assumption, both regression tasks and classification tasks can be solved jointly.\nMost existing sparse learning-based studies focus on using cross-sectional analysis methods, where only the data from a single time-point is used for model construction. However, multiple time-points of data are often available in some brain imaging applications, which can be used in longitudinal analysis to uncover the disease progression patterns. According to the number of time-points in the input and output of learning models, we can categorize the existing sparse models into four types: 1) single-time-point input and single-time-point output (SISO), 2) single-time-point input and multi-time-points output (SIMO), 3) multi-time-points input and single-time-point output (MISO), and 4)multi-time-points input and multi-timepoints output (MIMO). In Fig. 1 , we give an illustration for all these four learning problems, with more details given in next section.\nIn this paper, we address the above problems (i.e., SIMO, MISO, and MIMO) by using sparse learning-based methods, where longitudinal data in either output or input (or both) can be employed. For that purpose, we develop a novel temporallyconstrained group LASSO method, namely tgLASSO, where both the group regularizer and the temporal smoothness regularizer are incorporated into the objective function. Specifically, as in gLASSO, we first learn a sparse linear regression model by using data from each time-point, and further utilize a group regularizer to group the weights corresponding to the same brain region across different time-points together. In addition, to reflect the smooth changes between data from adjacent time-points, we also develop two smoothness regularizers: 1) a fused smoothness term (originated from fused LASSO [19] , [20] ), which requires the differences between two successive weight vectors from adjacent time-points to be small; 2) an output smoothness term, which requires that the differences between outputs of two successive models from adjacent time-points to be small. Furthermore,we develop an efficient optimization algorithm for solving the proposed problem. It is worth noting that, in order to capture temporal changing patterns of biomarkers in disease progression [21] , [22] , some researchers recently have explored to model disease progression via fused LASSO method [23] . However, different from their methods, our method incorporates a new smoothness regularizer (i.e., output smoothness term) into the objective function to capture the smoothness of outputs of two successive prediction models from adjacent time-points, which is one of our major contributions and was not investigated before [15] , [23] .\nTo validate the efficacy of our proposed method, we first perform a set of experiments (corresponding to the aforesaid-MIMO, MISO, and SIMO learning problems) on estimating clinical scores from MRI data on 445 subjects (including 91 AD, 202 MCI, and 152 NC) from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Here, each subject has MRI data and the corresponding clinical scores, including Mini Mental State Examination (MMSE) and Alzheimer's Disease Assessment Scale-Cognitive Subscale (ADAS-Cog), at fourdifferent time-points (i.e., baseline, six, 12, and 24 months). Then, we perform experiments on predicting MCI conversion from baseline MRI data using the biomarkers discovered in the first set of experiments. Our hypothesis is that, using longitudinal data, the proposed temporallyconstrained group sparse learning method would perform better in discovering AD-related biomarkers and, thus, would achieve better performances in subsequent regression and classification tasks than the conventional methods."}, {"section_title": "II. METHOD", "text": "The data used in the preparation of this paper were obtained from the ADNI database (www.adni-info.org). The ADNI was launched in 2003 by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, the Food and Drug Administration, private pharmaceutical companies, and nonprofit organizations, as a $ 60 million, five-year public-private partnership. The primary goal of ADNI has been to test whether the serial MRI, PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of MCI and early AD. Determination of sensitive and specific markers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as lessen the time and cost of clinical trials.\nThe Principal Investigator of this initiative is Michael W. Weiner, MD, VA Medical Center and University of CaliforniaSan Francisco, CA, USA. ADNI is the result of efforts of many coinvestigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the USA and Canada. The initial goal of ADNI was to recruit 800 adults, ages 55-90, to participate in the research, approximately 200 cognitively normal older individuals to be followed for three years, 400 people with MCI to be followed for three years and 200 people with early AD to be followed for two years. For up-to-date information, seewww.adniinfo.org."}, {"section_title": "A. Subjects", "text": "In the current study, we use all 445 ADNI subjects (including 91 AD, 202 MCI, and 152 NC) with all corresponding MRI data as well as two cognitive scores (MMSE and ADAS-Cog) at fourdifferent time-points (i.e., baseline, six, 12, and 24 months).\nIn particular, for the MCI cohort, it contains 104 MCI-C and 98 MCI non-converters (MCI-NC). In Table I , we list the demographic characteristics of all studied subjects."}, {"section_title": "B. MRI Data Acquisition", "text": "In our previous works, we have described in detail on acquiring MRI data from ADNI [18] , [24] . In short, structural MR scans were acquired from 1.5T scanners. Raw Digital Imaging and Communications in Medicine (DICOM) MRI scans were downloaded from the public ADNI site (adni.loni.usc.edu) [22] , reviewed for quality, and automatically corrected for spatial distortion caused by gradient nonlinearity and B1 field inhomogeneity."}, {"section_title": "C. Image Analysis", "text": "In our experiments, we follow our previous works [18] , [24] to perform image pre-processing for all MR images. Specifically, anterior commissure (AC) -posterior commissure (PC) correction is first performed on all images using MIPAV software (http://mipav.cit.nih.gov/index.php), followed by the N3 algorithm [25] which is used to correct the intensity inhomogeneity. Then, we perform skull-stripping on structural MR images, using a learning based method proposed in [26] that includes both brain surface extractor (BSE) [27] and brain extraction tool (BET) [28] . Next, the skull stripping results were further manually reviewed to ensure clean skull and dura removal. After the removal of cerebellum, the FSL package [29] is used to segment structural MR images into three different tissues: grey matter (GM), white matter (WM), and CSF. Afterwards, a fully automatic 4-dimensional atlas warping method called 4D HAM-MER [30] is used to register all different time-point images of each subject to a template with 93 manually-labeled Regions of Interest (ROI) [31] . After registration, we can label all images based on the 93 labeled ROIs in the template. For each of the 93 ROIs in the labeled MR image, we compute the total GM volume of that region and use it as feature. In this study, we only use GM for feature extraction, because GM is the most affected by AD and also widely-used in the literature [11] , [18] , [24] , [32] , [33] . Note that if there is no any GM in a specific region, the feature value for this region will be 0."}, {"section_title": "D. Temporally-constrained Group Sparse Learning (tgLASSO)", "text": ""}, {"section_title": "1) Four Different Learning Problems:", "text": "Since AD (and its prodromal form, MCI) is a progressive neurodegenerative disease, we can obtain a series of temporal changes reflected in MRI data and clinical scores (e.g., MMSE and ADAS-Cog for AD) from studied subjects. In this work, we focus on estimating clinical scores by using MRI data. According to the number of time-points in both MRI data (input) and clinical scores (output), there are four different learning problems as shown in Fig. 1 .\nSpecifically, as shown in Fig. 1(a) , in the first learning problem (i.e., SISO), we want to estimate the clinical scores at a certain time-point, e.g., time-point 1 (baseline), by using imaging data from single time-point (e.g., baseline). Because both input and output are derived from a particular single time-point, the SISO problem contains no longitudinal information, and thus can be easily solved by the existing sparse learning methods (e.g., LASSO [12] ). In the second learning problem, i.e., SIMO shown in Fig. 1(b) , the clinical scores at each time-point (ranging from 1 to T) can be estimated by using imaging data from single time-point (e.g., baseline). Similarly, in the third learning problem, i.e., MISO shown in Fig. 1(c) , we aim to estimate clinical scores at time-point T, by using imaging data from all time-points (from 1 to T ). Finally, in the fourth learning problem, i.e., MIMO shown in Fig. 1(d) , we want to estimate clinical scores at each time-point j (j = 1, . . . , T ), by using imaging data from its corresponding time-point j. It is worth noting that MIMO will degenerate to be SIMO if we set the input (imaging data) x j = x 1 (j = 1, . . . , T ). Similarly, MIMO will degenerate to be MISO if we set the output (clinical score) z j = z 1 (for j = 1, . . . , T ). In the following, we will develop a new temporally-constrained group sparse learning (tgLASSO) method for solving the MIMO (as well as MISO and SIMO) problems.\n2) Objective Function: Assume that we have N training subjects, and each subject has imaging data derived from T different time-points, represented as {x i1 , . . . ,\nas the training data matrix (input) and the corresponding clinical scores at the j-th time-point, respectively. We use the linear model to estimate the clinical score from the imaging data x at the jth time-point as h j (x) = xw j , where the feature weight vec-\ndenote the weight vector matrix for all T learning tasks, with each column vector corresponding to one specific task. The objective function of our temporallyconstrained group LASSO (tgLASSO) can be defined as follows:\nwhere R g (W) and R s (W) are the group regularizationterm and the smoothness regularizationterm, respectively. Specifically, the group regularizationterm is defined as below:\nHere, w d is the d-th row vector of W. It is worth noting that the use of L 2 -norm on row vectors encourages the weights corresponding to the d-th feature across multiple time-points to be grouped together, and the further use of L 1 -norm tends to jointly select features based on the strength of T time-points. The regularization parameter \u03bb 1 controls the group sparsity of the linear models.\nIn addition, the smoothness regularizationterm is defined as follows:\nwhere the first term in (3) is called the fused smoothnessterm which originates from fused LASSO [19] , [20] , and it constrains the differences between two successive weight vectors from adjacent time-points to be small. Due to the use of L 1 -norm in the fused smoothness term that encourages the sparsity on difference of weight vectors, there will be a lot of zero components in the weight difference vectors. In other words, a lot of components from adjacent weight vectors will be identical because of using the fused smoothness regularization. In our study, we will select those features with non-zero weights for subsequent regression or classification tasks. The second term in (3) is called the output smoothnessterm that requires the differences between outputs of two successive models from adjacent timepoints to be small as well. The regularization parameters \u03bb 2 and \u03bb 3 balance the relative contributions of the two terms and also control the smoothness of the linear models. It is easy to know that when both \u03bb 2 and \u03bb 3 are zero, our method will degenerate to gLASSO [16] . In the next section, we will develop an efficient optimization algorithm to solve the objective function defined in (1).\n3) Efficient Iterative Optimization Algorithm: It is worth noting that the above defined objective function is the first time to simultaneously include both the group and the (fused plus output) smoothness regularizations, which has not been studied before. In the Appendix, we have developed an efficient algorithm to solve the objective function. Here, the key idea is to separate the objective function into the smooth term and the non-smooth term and then use the iterative projected gradient descent approach [34] , which combines the gradient descent and proximal mapping to update the iterations for final solution. For more details, please refer to the Appendix."}, {"section_title": "E. Validation", "text": "In our experiments, each of the 445 subjects has the corresponding MRI data and clinical scores (including MMSE and ADAS-Cog) at 4 different time-points, i.e., baseline, 6-month (M06), 12-month (M12), and 24-month (M24). To evaluate the efficacy of our proposed tgLASSO method, we compare our method with existing popular sparse learning methods, including LASSO and gLASSO. In addition, we perform two sets of experiments on longitudinal data from ADNI database, i.e., estimating clinical scores and predicting MCI conversion.\nIn the first set of experiments, we estimate the clinical scores (i.e., MMSE and ADAS-Cog) from MRI data in three different problem settings, i.e., MIMO, MISO and SIMO, which involve the use of different types of longitudinal information as shown in Fig. 1 . To evaluate the regression performance of our proposed method, we use a 10-fold cross-validation strategy by computing the Pearson's correlation coefficient between the predicted and the actual clinical scores, and also computing the root mean square error (RMSE) between the predicted and the actual clinical scores. Specifically, the whole set of samples are first partitioned into 10 subsets (each subset with a roughly equal size). Then, the samples within one subset are selected as the testing data, and samples in the other 9 subsets are combined as the training data. This process is repeated for 10 times independently. In the experiment, we compute both the average value of the Pearson's correlation coefficients and the average value of the RMSEs in all 10-fold cross-validation as the final results.\nIn the second set of experiments, we predict the MCI conversion from baseline MRI data using the biomarkers discovered by tgLASSO under the MIMO problem setting. Specifically, we first perform feature selection by using our proposed tgLASSO method on longitudinal training data (with MRI data and corresponding clinical scores of MMSE and ADAS-Cog at 4 timepoints, i.e., baseline, M06, M12 and M24), in order to select the most discriminative brain regions. Then, a SVM classifier is constructed based on the baseline training data (with MRI data and corresponding class labels at baseline time-point) with the selected brain regions for the prediction of MCI. Similar to the first set of experiments, we also adopt a 10-fold cross-validation strategy to evaluate the classification performance by three statistical measures, including the classification accuracy (i.e., the proportion of MCI subjects correctly classified), the sensitivity (i.e., the proportion of MCI-C correctly classified), and the specificity (i.e., the proportion of MCI nonconverters correctly classified). Besides, we also calculate the area under receiver operating characteristic curve (AUC) as performance measure.\nIn our experiments, for each extracted feature value, we perform the following feature normalization, i.e., subtracting the mean and then dividing the standard deviation (of all training subjects). For all respective methods, another round of crossvalidation on the training data is used for determining the values for parameters (e.g., \u03bb 1 , \u03bb 2 , and \u03bb 3 ). Specifically, we, respectively, vary the values of \u03bb 1 , \u03bb 2 , and \u03bb 3 within the range of (0.25, 0.2, 0.15, 0.1, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01) and compute the prediction performance via the inner tenfold crossvalidation on the training subjects. The parameter values with the best performance (on the inner tenfold crossvalidation) will be used for prediction of the unknown subjects (i.e., testing subjects in each (outer) cross validation). The linear SVM is implemented using LIBSVM toolbox with the default parameter value (i.e., C = 1) [35] . It is worth noting that the crossvalidation on training subjects is only used to determine the optimal parameter values. "}, {"section_title": "III. RESULTS", "text": ""}, {"section_title": "A. Estimating Clinical Scores", "text": "In this group of experiments, we first estimate two regression variables (i.e., MMSE and ADAS-Cog) in three learning problems (i.e., MIMO, MISO, and SIMO) at four time-points, respectively. Before showing the estimation results, we first plot the average longitudinal changes of clinical scores from baseline to M24 in different kinds of subjects (i.e., AD, MCI-C, MCI-NC, and NC) in Fig. 2. Figs. 3 It can be seen from Fig. 2 that, as disease progresses, the cognitive performance of the AD and MCI-C subjects decline gradually as reflected by the decreased MMSE and increased ADAS-Cog scores, while the cognitive performance of the MCI-NC and NC subjects declines much slower than those of the AD and MCI-C subjects.\nAs can be seen from both Figs. 3 and 4, our proposed tgLASSO method consistently outperforms other methods in estimating clinical scores. Specifically, tgLASSO achieves the average (i.e., across four time-points) correlation coefficients of 0.613, 0.657, and 0.594 for estimating MMSE scores in the MIMO, MISO, and SIMO learning problems, respectively, while the best average correlation coefficients of the competing methods are 0.607, 0.647, and 0.589, respectively. Similarly, for estimating ADAS-Cog scores, tgLASSO achieves the average correlation coefficients of 0.639, 0.676, and 0.623 in the three learning problems, while the best average correlation coefficients of the competing methods are 0.635, 0.665, and 0.622, respectively. Also, tgLASSO achieves the average (i.e., across four time-points) RMSEs of 2.988, 2.845, and 3.022 for estimating MMSE scores in the MIMO, MISO, and SIMO learning problems, respectively, while the best average RMSEs of the competing methods are 3.011, 2.867, and 3.032, respectively. Similarly, for estimating ADAS-Cog scores, tgLASSO achieves the average RMSEs of 6.080, 5.853, and 6.181 in the three learning problems, while the best average RMSEs of the competing methods are 6.107, 5.925, and 6.179, respectively. Moreover, we perform, respectively, the paired t-test between correlation coefficients of the proposed tgLASSO method and the correlation coefficients of the competing methods (i.e., gLASSO and LASSO), and between RMSEs of the proposed tgLASSO method and RMSEs of the competing methods (i.e., gLASSO and LASSO). The results in both tests show that the proposed tgLASSO method is significantly better than gLASSO and LASSO methods in three learning problems (i.e., with all p-values less than 0.05). Also, we perform the paired t-test over squared residuals between the proposed tgLASSO method and each competing method, and show results in Table II. From  Table II , we can see that most of p-values in three learning problems are also less than 0.05. These results validate the efficacy of our proposed method in jointly estimating the clinical scores based on longitudinal analysis. Besides, both Figs. 3 and 4 also indicate that estimating later time-point scores often achieves better performance than estimating earlier time-point scores. This may be because the relationship between imaging features and clinical scores becomes much stronger with progress of disease or brain aging, e.g., atrophy in the brain is more obvious in advanced disease and, thus, the related features are more distinctive and correlated withthe clinical scores. In addition, from Figs. 3 and 4, we can further observe that the prediction results of tgLASSO with \u03bb 3 = 0 are worse than tgLASSO, while better than any other competing methods. These further show the advantage of using two smoothness items (i.e., fused smoothness and output smoothness). Besides, the prediction results of MISO learning model are usually superior to those of MIMO learning model in the first three time-points, which indicates that the clinical scores at last time-point (i.e., M24) may help induce more important features (i.e., brain atrophy regions)for prediction. It is worth noting that MIMO and MISO are the two different types of learning model, and should be used for different longitudinal analysis settings, respectively. Specifically, MIMO is a multi-time-points input and multi-time-points output learning model, which can be used to estimate clinical scores at multiple time-points by using imaging data from the same timepoint. On the other hand, MISO is a multi-time-points input and single-time-point output learning model, which can be used to predict clinical score at the last time-point by using imaging data from all previous time-points. Fig. 5 shows feature weight maps of three different methods in a certain cross-validation case when estimating MMSE scores for MIMO learning problem. In addition, both Figs. S1 and S2 in supplementary material also show the corresponding feature weight maps of different methods for the MISO and SIMO learning problems, respectively. Here, it is worth noting that both gLASSO and tgLASSO jointly learn weight vectors for the four time-points, while LASSO learns each weight vector independently for each time-point.\nAs can be seen from Fig. 5 and Figs. S1 and S2 in Supplementary Material, due to the use of group regularization,gLASSO and tgLASSO obtain more grouped weights across different time-points than LASSO. Furthermore, due to the use of smooth- ness regularization, tgLASSO achieves more smooth weights across different time-points than other two methods. These properties are helpful to discover those intrinsic biomarkers relevant to brain diseases. For example, as shown in Fig. 5 , both left and right hippocampal regions, the well-known AD-relevant biomarkers, are detected by tgLASSO, while only the left hippocampal region is detected by other two methods."}, {"section_title": "B. Predicting MCI Conversion", "text": "In this set of experiments, we predict the future conversion of MCI patients based on baseline data, using the biomarkers discovered in the first set of experiments corresponding to MIMO learning problem. Here, for both joint learning methods (i.e., gLASSO and tgLASSO), we first learn the corresponding gLASSO and tgLASSO models using longitudinal training MRI data (with longitudinal MMSE/ADAS-Cog scores) at four time-points to select the important brain regions (with respect to MMSE/ADAS-Cog scores), and then train SVM classifiers on the baseline training MRI data with above-selected brain regions, respectively. On the other hand, since LASSO cannot deal with longitudinal data, we learn a LASSO model using only the baseline training MRI data (with baseline MMSE/ADAS-Cog scores) to select the important brain regions, and then train a SVM classifier on the baseline training MRI data with aboveselected brain regions. Table III gives the results of different methods in predicting the MCI conversion.\nAs can be seen from Table III , our proposed tgLASSO method consistently outperforms the other two methods in all performance measures. Specifically, our proposed method achieves a classification accuracy of 75.7%, a sensitivity of 72.9%, and a specificity of 82.0% when learning the tgLASSO model with guidance from MMSE clinical score, while achieves a classification accuracy of 74.7%, a sensitivity of 73.9%, and a specificity of 76.1% when learning the tgLASSO model with guidance from ADAS-Cog clinical score. These results are consistently better than other methods on each performance measure. In addition, Table III also indicates that, by using longitudinal data, the gLASSO method can obtain better performance than the LASSO method, but it is still inferior to our proposed method (tgLASSO)."}, {"section_title": "C. Most Important Brain Regions", "text": "In this section, we investigate the top selected brain regions by our proposed tgLASSO method in the MIMO learning prob- lem. Since the selected brain regions are different in each tenfold crossvalidation, we chose the brain regions with top occurrence frequency in all crossvalidation as the most important brain regions, when learning models using the clinical scores of MMSE and ADAS-Cog, respectively. Table IV lists the 16 most important brain regions detected by the proposed tgLASSO method. Also, in Table IV , we give the average of each selected ROI's weights across all folds and time-points, as well as the corresponding standard deviation. The result shows that the most important regions obtained by our method include hippocampal, amygdala, temporal pole, uncus, and middle temporal regions, which are consistent with previous studies. In addition, from Table IV , we can see the obtained standard deviations are very small, indicating that the weight maps of each selected ROI across different time-points are very smooth. This furthermore shows the advantage of using our proposed smoothness regularizations. For visual inspection, in Fig. 6 , we also highlight those selected brain regions listed in Table IV ."}, {"section_title": "IV. DISCUSSION", "text": "In this paper we have proposed a novel temporallyconstrained group sparse learning method for longitudinal analysis with multiple time-points of data. Our proposed method has been validated on 445 subjects (including 91 AD, 202 MCI, and 152 NC) with cognitive scores at fourdifferent time-points (i.e., baseline, six, 12, and 24 months) through two sets of experiments, i.e., 1) estimating MMSE and ADAS-Cog scores at each time point in three learning problems (including SIMO, MISO, and MIMO), and 2) predicting future conversion of MCI subjects using baseline data. The experimental results show that our proposed method can not only significantly improve regression performance but also help in discovering disease-related biomarkers useful for MCI conversion prediction, compared with the conventional sparse learning methods."}, {"section_title": "A. Significance of Results", "text": "Recently, sparse learning methods have been widely used for diagnosis of AD/MCI. However, multiple time-points of data, which are often available and may potentially further improve performance, are not fully utilized in existing methods. Our study demonstrated that, by embedding the longitudinal information of data, our proposed method can achieve better performance in estimating the clinical scores as well as predicting the MCI conversion. It is worth noting that some recent works, e.g., methods in [36] and [23] , also adopted the sparse feature learning method for analyzing longitudinal data. Different from both aforementioned methods, we propose to use both the group and the (fused + output) smoothness regularizations in sparse learning to better reflect the longitudinal change patterns of the brain with the progression of disease. The experimental results also show the advantage of our proposed method compared with existing sparse learning methods. The brain regions selected by our proposed method are known to be related to the AD by many studies using group comparison methods, which include hippocampal [37] - [41] , amygdala [38] , temporal pole [42] , uncus [43] , and middle temporal regions [38] , [39] . For example, it has been reported that there exists a strong correlation between hippocampal volume and dementia severity [44] ."}, {"section_title": "B. Predicting Clinical Scores and MCI Conversion", "text": "A lot of works have studied the relationship between cognitive scores and imaging markers with neuroimaging data [45] - [47] . A variety of high-dimensional regression methods have been [50] , [51] , researchers used the sparse learning methods to predict scores of MMSE and ADAS-cog based on MRI data from ADNI dataset. Table V summarizes the results of these methods. As can be seen from Table V , our proposed method achieves comparable results in estimating clinical scores of MMSE and ADAS-Cog, compared with those recently published results in AD/MCI studies. In addition, MCI is a prodromal stage of AD, with high likelihood of conversion to AD. There was a strong association between the structural pattern of atrophy identified in AD and the pattern of atrophy found in MCI-C. It has been proposed in [52] that there is a long preclinical phase of AD with no symptoms of cognitive dysfunction but with an ongoing AD pathology, and recent study [53] has suggested that the structural changes detected by MRI may be evident even ten years before clinical diagnosis of AD. Therefore, a lot of recent studies in early diagnosis of AD have been focused on predicting the conversion of MCI to AD, i.e., identifying the MCI-C from MCI nonconverters (MCI-NC) [18] , [54] - [60] . For example, in a recent work [54] , an accuracy between 67.4% and 74.7% was reported on 21 MCI-C and 98 MCI-NC subjects using MRI data. More recently, in [60] , an accuracy of 0.68 was reported on 97 MCI-C and 93 MCI-NC subjects based on MRI data in the ADNI dataset. In contrast, our method achieves the accuracy between 74.7% and 75.7% on 104 MCI-C and 98 MCI-NC subjects from ADNI, which are comparable to the best results reported in those recent studies."}, {"section_title": "C. Effect of Parameters", "text": "In the objective function of our proposed tgLASSO method, there are two regularization terms, i.e., the group regularization term and smoothness regularization term, where the second one consists of two parts including the fused smoothness term and the output smoothness term. The regularization parameters \u03bb 1 , \u03bb 2 , and \u03bb 3 balance the relative contribution of these regularization terms. Here, the larger \u03bb 1 value means few features preserved for estimating the clinical scores due to the imposed \"group sparsity\" constraint via the L 2,1 -norm. The parameters \u03bb 2 and \u03bb 3 control the contributions of two smooth regularization items.\nTo investigate the effect of two smoothness regularization terms on the performance of our proposed method, we first fix the value of \u03bb 1 (i.e., setting \u03bb 1 to 0. Fig. 7 gives the regression performance of MMSE and ADAS-Cog scores using different values of \u03bb 2 and \u03bb 3 in our proposed method for the MIMO learning problem. It is worth noting that, for each plot, the bottom row and the right column denote the results when using only the output smoothness regularization (\u03bb 2 = 0) or only the fused smoothness regularization (\u03bb 3 = 0), respectively. As we can see from Fig. 7 , the larger values (i.e., better estimation performance) mainly focus on the inner intervals of the square, which indicates the effectiveness of combining two smoothness regularization terms for predicting clinical scores. This also implies that each term is indispensable for achieving good performance.\nFurthermore, we test the performance of our proposed method with different values of \u03bb 1 . Specifically, we vary the value of \u03bb 1 from the range of [0 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.15, 0.20, 0.25], and compute the prediction results of our proposed tgLASSO with the optimals \u03bb 2 and \u03bb 3 obtained by using the inner crossvalidation on training data. Fig.  8 graphically shows the obtained results for the MIMO learning problem. For comparison, we also give the prediction performance of gLASSO method, where only the group regularization term is included (i.e., setting both \u03bb 2 and \u03bb 3 to zero). It is worth noting that, for each plot, the leftmost points denote the results with no feature selection (i.e., using all features for estimating clinical scores). As can be seen from Fig. 8 , for estimating two kinds of clinical scores (i.e., MMSE and ADAS-Cog), our proposed tgLASSO method consistently achieves better performance than gLASSO method for all \u03bb 1 values. Specifically, at each time point, our method yields relatively high correlation coefficients for all \u03bb 1 values (except for zero), showing its robustness to regularization parameter \u03bb 1 , and also the advantage of including the smoothness regularization terms."}, {"section_title": "D. Limitations", "text": "The current study is limited by the following factors. First, our proposed method performs prediction based on longitudinal data and, thus, requires each subject having the corresponding data, i.e., MRI data and corresponding clinical scores, at each time point, which limits the size of subjects that can be used for study. For example, there are more than 400 MCI subjects in the ADNI dataset, while there are only 202 MCI subjects with MRI data and corresponding MMSE and ADAS-Cog scores at multiple time-points (including baseline, six, 12, and 24 months). Second, there also exist other modalities of data, e.g., PET and CSF. However, since the number of subjects with all modality data (including MRI, PET, and CSF) is too small for reasonable learning, the current study does not consider using multimodality data. In the future work, we will study how to utilize subjects with incomplete multimodality data (i.e., missing of certain modality data) for further performance improvement. Third,selecting important and stabilized features (i.e., brain regions) and determining the optimal regularization parameters are the two important problems for sparse-based methods. However, some brain regions reported in the literature, such as the precuneus and (posterior) cingulate, the entorhinal, perirhinal, and parahippocampal regions, and the lateral ventricles, were not found by our proposed method. In future work, we will explore some techniques, such as performing more crossvalidations on the training subjects to select most frequently occurring features as stabilized features, and also using Bayesian models (instead of the grid-searching approach) to determine the optimal parameter values, to address the aforesaidproblems. Finally, during image pre-processing, the brain region parcellation is also a very important step for the subsequent feature extraction and prediction. Also, previous studies have demonstrated that other methods, such as voxel-based methods, still obtained comparable results to region-based methods [55] . But this paper does not analyze the impact of different brain parcellation atlases on regression performance."}, {"section_title": "V. CONCLUSION", "text": "In this paper, we propose a new sparse learning method called tgLASSO for longitudinal data analysis with multiple timepoints of data, which is different from most existing sparse learning methods that focus on cross-sectional data analysis such as using only the data from single time-point. Our methodological contributions include two parts, i.e., 1) we propose to simultaneously use group and (fused + output) smoothness regularizations in sparse learning models; and 2) we develop an efficient iterative optimization algorithm for solving the new objective function. Experimental results on estimating clinical scores from imaging data at multiple time-points illustrate the advantages of our method over existing sparse methods in both regression performance and ability in discovering diseaserelated imaging biomarkers."}, {"section_title": "ACKNOWLEDGMENT", "text": "Data used in preparation of this study were obtained from the ADNI database (www.loni.ucla.edu/ADNI). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: www.loni.ucla.edu\\ADNI\\Collaboration\\ADNI_Authorship _ list.pdf."}]