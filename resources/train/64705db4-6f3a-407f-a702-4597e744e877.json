[{"section_title": "Abstract", "text": "Stein kernel (SK) has recently shown promising performance on classifying images represented by symmetric positive definite (SPD) matrices. It evaluates the similarity between two SPD matrices through their eigenvalues. In this paper, we argue that directly using the original eigenvalues may be problematic because: 1) eigenvalue estimation becomes biased when the number of samples is inadequate, which may lead to unreliable kernel evaluation, and 2) more importantly, eigenvalues reflect only the property of an individual SPD matrix. They are not necessarily optimal for computing SK when the goal is to discriminate different classes of SPD matrices. To address the two issues, we propose a discriminative SK (DSK), in which an extra parameter vector is defined to adjust the eigenvalues of input SPD matrices. The optimal parameter values are sought by optimizing a proxy of classification performance. To show the generality of the proposed method, three kernel learning criteria that are commonly used in the literature are employed as a proxy. A comprehensive experimental study is conducted on a variety of image classification tasks to compare the proposed DSK with the original SK and other methods for evaluating the similarity between SPD matrices. The results demonstrate that the DSK can attain greater discrimination and better align with classification tasks by altering the eigenvalues. This makes it produce higher classification performance than the original SK and other commonly used methods.\nIndex Terms-Discriminative learning, image classification, Stein kernel (SK), symmetric positive definite (SPD) matrix.\n2162-237X"}, {"section_title": "I. INTRODUCTION", "text": "I N THE past several years, symmetric positive definite (SPD) matrices have been widely applied to pattern analysis and computer vision. For example, covariance descriptor [1] is used in texture classification [2] , [3] , face recognition [4] , action recognition [5] , [6] , pedestrian detection [7] , [8] , visual tracking [9] , and 3-D shape matching/ retrieval [10] . Also, based on functional magnetic resonance imaging (fMRI), an SPD correlation matrix is used to model brain networks to discriminate patients with Alzheimer's disease (AD) from the healthy [11] . The tensors obtained in The authors are with the School of Computing and Information Technology, University of Wollongong, Wollongong, NSW 2522, Australia (e-mail: jz163@uowmail.edu.au; leiw@uow.edu.au; lupingz@uow.edu.au; wanqing@uow.edu.au).\nThis paper has supplementary downloadable material available at http://ieeexplore.ieee.org., provided by the author (File size: 1 MB).\nColor versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.\nDigital Object Identifier 10.1109/TNNLS. 2015.2435154 diffusion tensor imaging (DTI) [12] are SPD matrices too.\nSince the data in these tasks are either represented by or converted to SPD matrices, they are classified by the classification of the associated SPD matrices. A fundamental issue in the classification of SPD matrices is how to measure their similarity. Essentially, when a sufficiently good similarity measure is available, a simple k-nearest neighbor (k-NN) classifier will be able to achieve excellent classification performance. Considering that SPD matrices reside on a Riemannian manifold [13] , commonly used Euclidean-based measures may not be effective since they do not take the manifold structure into account. To circumvent this problem, affine-invariant Riemannian metric (AIRM) has been proposed in [14] for comparing covariance matrices and used in [15] - [17] for statistical analysis of DTI data. Although improving similarity measurement, AIRM involves matrix inverse and square rooting [18] , which could result in high computational cost when the dimensions of SPD matrices are high. In the past decade, effectively and efficiently measuring the similarity between SPD matrices on the Riemannian manifold has attracted much attention. One attempt is to map the manifold to a Euclidean space [7] , [19] , [20] , i.e., the tangent space at the mean point. However, these approaches suffer from two limitations. 1) Mapping the points from manifold to the tangent space or vice versa is also computationally expensive. 2) More importantly, the tangent space is merely a local approximation of the manifold at the mean point, leading to a suboptimal solution. To address these limitations, an alternative approach is to embed the manifold into a high-dimensional reproducing kernel Hilbert space using kernels. This can bring at least three advantages.\n1) The computational cost can be reduced by selecting an efficient kernel. 2) The manifold structure can be well incorporated in the embedding. 3) Many Euclidean algorithms, e.g., support vector machines (SVMs), can be readily used. Several kernel functions for SPD matrices have been defined via SPD-matrix-based distance functions [13] , including log-Euclidean distance [18] , [21] , Cholesky distance [22] , and power Euclidean distance [22] . Recently, another kernel function called Stein kernel (SK) has been proposed in [23] with promising applications reported in [3] , [24] , and [25] . Specifically, in [3] , SK is applied to evaluate the similarity of covariance descriptors for image classification. As shown in that work, SK is able to achieve better performance than other kernels in texture classification, face recognition, and person reidentification.\nSK evaluates the similarity of SPD matrices through their eigenvalues. Although this is theoretically rigorous and elegant, directly using the original eigenvalues may encounter the following two issues in practice.\nIssue I: Some SPD matrices, e.g., covariance descriptors [1] , have to be estimated from a set of samples. Nevertheless, covariance matrix estimation is sensitive to the number of samples, and eigenvalue estimation becomes biased when the number of samples is inadequate [26] . Such a biasness will affect SK evaluation. Issue II: Even if true eigenvalues could be obtained, when the goal is to discriminate different sets of SPD matrices, computing SK with these eigenvalues is not necessarily optimal. This is because eigenvalues reflect only the intrinsic property of an individual matrix and the eigenvalues of all the involved matrices have not been collectively manipulated toward greater discrimination. In this paper, we propose a discriminative SK (DSK) to address the two issues. Specifically, assuming that the eigenvalues of each SPD matrix have been sorted in a given order, a parameter is assigned to each eigenvalue to adjust its magnitude. Treating these parameters as extra parameters of SK, we automatically learn their optimal values using the training samples of a classification task. Our work brings forth two advantages. 1) Although it does not restore the unbiased eigenvalue estimates, 1 DSK mitigates the negative impact of the biasness to class discrimination. 2) By adaptively learning the adjustment parameters from the training data, it makes SK better align with specific classification tasks. Both advantages help to boost the classification performance of SK in practical applications. Three kernel learning criteria, including kernel alignment [27] , class separability [28] , and the radius margin bound [29] , are employed to optimize the adjustment parameters, respectively. The proposed DSK is experimentally compared with the original SK on a variety of classification tasks. As demonstrated, it not only leads to consistent improvement when the samples for SPD matrix estimation are relatively limited, but also outperforms the original SK when there are enough samples."}, {"section_title": "II. RELATED WORK", "text": "The set of SPD matrices with a size of d \u00d7 d can be defined as Sym + d = {A| A = A , \u2200x \u2208 R d , x = 0, x Ax > 0}. SPD matrices arise in various pattern analysis and computer vision tasks. Geometrically, SPD matrices form a convex half-cone in the vector space of matrices and the cone constitutes a Riemannian manifold. A Riemannian manifold is a real smooth manifold that is differentiable and equipped with a smoothly varying inner product for each tangent space. The family of the inner products is referred to as a Riemannian metric. The special manifold structure of SPD matrices is of great importance in analysis and optimization [23] .\nA commonly encountered example of SPD matrices is the covariance descriptor [1] in computer vision. Given a set of feature vectors {x i ;\nand \u03bc is defined as 1/n n i=1 x i . Using a covariance matrix as a region descriptor gives several advantages. For instance, the covariance matrix can fuse all kinds of features in a natural way. Also, the size of the covariance matrix is independent of the size of the region and the number of features extracted, inducing a certain scale and rotation invariance over regions.\nLet X and Y be two SPD matrices. How to measure the similarity between X and Y is a fundamental issue in SPD data processing and analysis. Recent years have seen extensive work on this issue. Respecting the Riemannian manifold, one widely used Riemannian metric is the AIRM [17] , which is defined as\nwhere log(\u00b7) represents the matrix logarithm and \u00b7 F is the Frobenius norm. The computational cost of AIRM could be high due to the use of matrix inverse and square rooting. Some other methods directly map SPD matrices into Euclidean spaces to utilize linear algorithms [6] , [30] . However, they fail to take full advantage of the geometry structure of Riemannian manifold.\nTo address these drawbacks, kernel-based methods have been generalized to handle SPD data residing on a manifold. A point X on a manifold M is mapped to a feature vector \u03c6(X) in some feature space F . The mapping is usually implicitly induced by a kernel function k : (M, M) \u2192 R, which defines the inner product in F , i.e., k(X i , X j ) = \u03c6(X i ), \u03c6(X j ) . Besides allowing linear algorithms to be used in F , kernel functions are often efficient to compute. A family of Riemannian metrics and the corresponding kernels are listed in Table I . A recently proposed SK [23] has demonstrated notable improvement on discriminative power in a variety of applications [3] , [24] , [25] , [31] . It is expressed as\nwhere \u03b8 is a tunable positive scalar. S(X, Y ) is called S-divergence, and it is defined as\nwhere det(\u00b7) denotes the determinant of a square matrix. The S-divergence has several desirable properties. 1) It is invariant to affine transformations, which is important for computer vision algorithms [31] . 2) The square root of S-divergence is proven to be a metric on Sym + d [23] . 3) SK is guaranteed to be a Mercer kernel when \u03b8 varies within the range of = {1/2, 2/2, 3/2, . . . ,\nReaders are referred to [23] for more details. In general, S-divergence enjoys higher computational efficiency than AIRM while well maintaining its measurement performance. When compared with other SPD metrics, such as Cholesky distance [22] , Euclidean distance [22] , log-Euclidean distance [18] , [21] , and power Euclidean distance [22] , S-divergence usually helps to achieve better classification performance [3] . All the metrics in Table I will be compared in the experimental study."}, {"section_title": "III. PROPOSED METHOD", "text": ""}, {"section_title": "A. Issues With Stein Kernel", "text": "Let \u03bb i (X) denote the i th eigenvalue of an SPD matrix X, where \u03bb i (X) is always positive due to the SPD property. Throughout this paper, we assume that the d eigenvalues have been sorted in descending order. Noting that the determinant of X equals d i=1 \u03bb i (X), the S-divergence in (3) can be rewritten as\nWe can easily observe the important role of eigenvalues in computing S(X, Y ). Inappropriate eigenvalues will affect the precision of S-divergence and in turn the SK. On Issue I: It has been well realized in the literature that the eigenvalues of sample-based covariance matrix are biased estimates of true eigenvalues [32] , especially when the number of samples is small. Usually, the smaller eigenvalues tend to be underestimated, while the larger eigenvalues tend to be overestimated. To better show this case, we conduct a toy experiment. A set of n data is sampled from a 40-D normal distribution N (0, ), where the covariance matrix is defined as diag (1, 2, . . . , 40) and the true eigenvalues of are just the diagonal entries. The n data are used to estimate and calculate the eigenvalues. When n is 100, the largest eigenvalue is obtained as 67, while the smallest one is 0.4, which are poor estimates. When n increases to 1000, the largest eigenvalue is still overestimated as 46. From our observation, tens of thousands of samples are required to achieve sufficiently good eigenvalue estimates. Note that the dimensions of 40 are common in practice. A covariance descriptor of 43-D features is used in [3] for face recognition, and it is also used in our experimental study.\nOn Issue II: As previously mentioned, even if true eigenvalues could be obtained, a more important issue exists when the goal is to classify different sets of SPD matrices. In particular, an SPD matrix can be expressed as\nwhere \u03bb i and u i denote the i th eigenvalue and the corresponding eigenvector. The magnitude of \u03bb i reflects only the property of this specific SPD matrix, for example, the data variance along the direction of u i . It does not characterize this matrix from the perspective of discriminating different sets of SPD matrices. We know that, by fixing the d eigenvectors, varying the eigenvalues changes the matrix X. Geometrically, an SPD matrix corresponds to a hyper-ellipsoid in a d-dimensional Euclidean space. This change is analogous to varying the lengths of the axes of the hyper-ellipsoid while maintaining their directions. A question then arises: to make the SK better prepared for class discrimination, can we adjust the eigenvalues to make the SPD matrices in the same class similar to each other, as much as possible, while maintaining the SPD matrices across classes to be sufficiently different? The similarity and difference are defined in the sense of SK. This idea can also be understood in the other way. An ideal similarity measure shall be more sensitive to interclass difference and less affected by intraclass variation. Without exception, this shall apply to SK too."}, {"section_title": "B. Proposed Discriminative Stein Kernel", "text": "Let \u03b1 = [\u03b1 1 , \u03b1 2 , . . . , \u03b1 d ] be a vector of adjustment parameters. Let X = U U denote the eigen-decomposition of an SPD matrix, where the columns of U correspond to the eigenvectors and = diag(\u03bb 1 , . . . , \u03bb d ). We use \u03b1 in two ways for eigenvalue adjustment and define the adjusted X, respectively, as\nIn the first way, \u03b1 is used as the power of eigenvalues. It can naturally maintain the SPD property because \u03bb \u03b1 i i is always positive. In the second way, \u03b1 is used as the coefficient of eigenvalues. It is mathematically simpler, but needs to impose the constraint \u03b1 i > 0 (i = 1, . . . , d) to maintain the SPD property. The two adjusted matrices are denoted b\u1ef9 X p andX c , where p and c are short for power and coefficient. Both ways will be investigated in this paper.\nGiven two SPD matrices X and Y, we define the \u03b1-adjusted S-divergence as\nFor the two ways of using \u03b1, the term S(X,\u1ef8 ) can be expressed as\nBased on the above definition, the DSK is proposed as\nNote that the DSK will remain a Mercer kernel as long as \u03b8 varies in the range of defined in Section II, because k \u03b1 (X, Y ) can always be viewed as k(X,\u1ef8 ), the original SK applied to two adjusted SPD matricesX and\u1ef8.\nTreating \u03b1 as the kernel parameter of k \u03b1 (X, Y ), we resort to kernel learning techniques to find its optimal value. Kernel learning methods have received much attention in the past decade. Many learning criteria such as kernel alignment [27] , kernel class separability [28] , and radius margin bound [29] have been proposed. In this paper, to investigate the generality of the proposed DSK, we employ all the three criteria, respectively, to solve the kernel parameters \u03b1.\nbe a set of n training SPD matrices, each of which represents a sample, e.g., an image to be classified. t i denotes the class label of the i th sample, where t i \u2208 {1, . . . , M} with M denoting the number of classes. K denotes the kernel matrix computed with DSK on , with K i j = k \u03b1 (X i , X j ). In the following section, three frameworks are developed to learn the optimal value of \u03b1.\n1) Kernel Alignment-Based Framework: Kernel alignment measures the similarity of two kernel functions and can be used to quantify the degree of agreement between a kernel and a given classification task [27] . Kernel alignment possesses several desirable properties, including conceptual simplicity, computational efficiency, concentration of empirical estimate, and theoretical guarantee for generalization performance [27] , [33] . Furthermore, kernel alignment is a general-purpose criterion that does not depend on a specific classifier and often leads to simple optimization. Also, it can uniformly handle binary and multiclass classification. Due to these merits, the kernel alignment criterion has been widely used in kernel-related learning tasks [33] , including kernel parameter tuning [34] , multiple kernel learning [35] , spectral kernel learning [36] , and feature selection [37] .\nWith the kernel alignment, the optimal \u03b1 can be obtained through the following optimization problem:\nwhere T is an n \u00d7 n matrix with T i j = 1 if X i and X j are from the same class and T i j = \u22121 otherwise. Note that this definition of T naturally handles multiclass classification. J (K , T ) is defined as the kernel alignment criterion\nwhere \u00b7, \u00b7 F denotes the Frobenius inner product between two matrices. J (K , T ) measures the degree of agreement between K and T, where T is regarded as the ideal kernel of a learning task. The \u03b1 0 is a priori estimate of \u03b1, and \u03b1 \u2212 \u03b1 0 2 2 is the regularizer that constrains \u03b1 to be around \u03b1 0 to avoid overfitting. We can simply set \u03b1 0 = [1, . . . , 1] , which corresponds to the original SK. \u03bb is the regularization parameter to be selected via cross validation. A denotes the domain of \u03b1: when \u03b1 is used as a power, A denotes a Euclidean space\nwhere \u03b1 z denotes the zth parameter of \u03b1 and the entry of\nwhere tr(\u00b7) denotes the trace of a matrix. Therefore, any gradient-based optimization technique can be applied to solve the optimization problem in (9) .\nOn Choosing \u03b8 : As observed in (8), there is a kernel parameter \u03b8 inherited from the original SK. Note that \u03b8 and \u03b1 play different roles in the proposed kernel and cannot be replaced with each other. The value of \u03b8 needs to be appropriately chosen because it impacts the kernel value and in turn the optimization of \u03b1. A commonly used way to tune \u03b8 is k-fold cross validation. In this paper, to better align with the kernel alignment criterion, we also tune \u03b8 by maximizing the kernel alignment and do this before adjusting \u03b1\nwhere 1 is a d-dimensional vector with all entries equal to 1 and K | \u03b1=1 denotes the kernel matrix computed by the original SK without \u03b1 adjustment. Through this optimization, we find a reasonably good \u03b8 , and then optimize \u03b1 on top of it. The maximization problem in (12) can be conveniently solved by choosing \u03b8 in the range of = {1/2, 2/2, 3/2, . . . , d \u2212 1/2} \u222a (d \u2212 1/2, +\u221e). \u03b8 is not optimized jointly with \u03b1 since the noncontinuous range of \u03b8 could complicate the gradient-based optimization. As will be shown in Section V, optimizing \u03b8 and \u03b1 sequentially can already lead to promising results.\nAfter obtaining \u03b8 * and \u03b1 * , the proposed DSK will be applied to both training and test data for classification, with certain classifiers such as k-NN or SVM. Note that for a given classification task, the optimization of \u03b8 and \u03b1 only needs to be conducted once with the training data. After that, they are used as fixed parameters to compute the SK for each pair of SPD matrices. The DSK with kernel alignment criterion is outlined in Algorithm 1.\n2) Class Separability-Based Framework: Class separability is another commonly used criterion for model and feature selection [28] , [38] , [39] . Recall that the training sample set is defined as\n. . , M}. Let i be the set of training samples from the i th class, with n i denoting the size of i . K , denotes a kernel matrix computed over two training subsets and , where\n. The class separability in the feature space F induced by a kernel k can be defined as\nwhere tr(\u00b7) is the trace of a matrix, and S B and S W are the between-class scatter matrix and the within-class scatter matrix, respectively. Let m and m i denote the total sample mean and the i th class mean. They can be expressed\ntr(S B ) and tr(S W ) can be expressed as\nand\nThe derivatives of tr(S B ) and tr(S W ) with respect to \u03b1 z can be shown as\nand\nThe class separability can reflect the goodness of a kernel function with respect to a given task. The DSK learning procedure outlined in Algorithm 1 can be fully taken advantage of optimizing the parameter \u03b1 when the class separability measure is used. The only modification is to replace the definition of J with (13) .\n3) Radius Margin Bound-Based Framework: Radius margin bound is an upper bound on the number of classification errors in a leave-one-out (LOO) procedure of a hard margin binary SVM [29] , [40] . This bound can be extended to L 2 -norm soft margin SVM with a slightly modified kernel. It has been widely used for parameter tuning [29] and model selection [38] . We first consider a binary classification task, and then extend the result to the multiclass case. Let \u222a be a training set of l samples, and without loss of generality, the samples are labeled by t \u2208 {\u22121, 1}. With a given kernel function k, the optimization problem of SVM with L 2 -norm soft margin can be expressed as\nwherek\nC is the regularization parameter; \u03b4 i j = 1 if i = j , and 0 otherwise; and w is the normal vector of the optimal separating hyperplane of SVM. Tuning the parameters ink can be achieved by minimizing an estimate of the LOO errors. It is shown in [41] that the following radius margin bound holds:\nwhere E(LOO) denotes the number of LOO errors performed on the l training samples in \u222a , R is the radius of the smallest sphere enclosing all the l training samples, and \u03b3 denotes the margin with respect to the optimal separating hyperplane and equals 1/ w . R 2 can be obtained by the following optimizing problem:\nNote that both R and w are the functions of the kernelk. We set the kernel function k as k \u03b1 defined in (8) . The model parameters ink, i.e., {\u03b8, \u03b1, C}, can be optimized by minimizing R 2 w 2 on the training set. As mentioned previously, we can first choose a reasonably good \u03b8 * by optimizing (21) with respect to \u03b8 and C while fixing \u03b1 as 1\nOnce \u03b8 * is obtained, {\u03b1, C}, denoted by \u03c5, can then be jointly optimized as follows:\nwhere \u03d2 = {\u03b1, C|\u03b1 \u2208 A; C > 0}. Let \u03c5 z be the zth parameter of \u03c5. The derivative of R 2 w 2 with respect to \u03c5 z can be shown as\nwhere\nand\nwhere \u03b7 * i and \u03b2 * i denote the optimal solutions to (18) and (20), respectively.\nFor multiclass classification tasks, we optimize \u03c5 by a pairwise combination of the radius margin bounds of binary SVM classifiers [38] . Specifically, an M-class classification task can be split into M(M \u2212 1)/2 pairwise classification tasks using the one-versus-one strategy. For any class pair (i, j ) with 1 \u2264 i < j \u2264 M, a binary SVM classifier, denoted by SVM i j , can be trained using the samples from classes i and j . The corresponding radius margin bound, denoted by R 2 i j w i j 2 , can be calculated by (18) and (20) . As shown in [38] , the LOO error of an M-class SVM classifier can be upper bounded by the combination of the radius margin bounds of M(M \u2212 1)/2 pairwise binary SVM classifiers. The combination is defined as\nAs previous, a reasonably good \u03b8 * can be first chosen by\nThe optimal kernel parameter \u03c5 * for an M-class SVM classifier can then be obtained by \u03c5 * = arg min \u03c5\u2208\u03d2|\u03b8=\u03b8 * J.\nThe derivative of J with respect to \u03c5 z is given by\nwhere \u2202 R 2 i j /\u2202\u03c5 z and \u2202 w i j 2 /\u2202\u03c5 z for SVM i j can be obtained by following (24) and (25) . J can be optimized using gradientbased methods.\nIn classification, the label of a test sample X can be assigned using the max-wins classification rule by\nwhere s i j is the decision score of the binary classifier SVM i j , and it is computed as\nWe also consider a variant of the radius margin bound by replacing R 2 i j with tr(S T ), where tr(S T ) = tr(S B ) + tr(S W ). It can be calculated using (14) and (15) on i \u222a j . As revealed in [28] , R 2 i j is closely related to tr(S T ) and both of them measure the scattering of samples in a feature space F . Replacing R 2 i j with tr(S T ) can often result in more stable optimization [42] , and solving the quadratic programming (QP) problem in (20) can also be avoided. In the experimental study, both methods, named radius margin bound and trace margin criterion, are implemented to investigate the performance of DSK. The overall procedure is outlined in Algorithm 2."}, {"section_title": "IV. COMPUTATIONAL ISSUE", "text": "As will be shown in the experiments, the optimization problems of the proposed DSK can be efficiently solved and often converge in a few iterations. Two stopping criteria are used.\n1) Optimization will be terminated when the difference of the objective values at two successive iterations is below a predefined threshold \u03c4. 2) The optimization will also be stopped when the number of iterations exceeds a predefined threshold T. We set \u03c4 = 10 \u22125 and T = 100 in our experiment. The kernel alignment and class separability criteria [defined in (10) and (13), respectively] can be quickly computed with a given kernel matrix. The major computational bottleneck is at the computation of the kernel matrix K , which is repeatedly evaluated for various \u03b1 values. For a training set of n SPD matrices, the time complexity of optimizing \u03b1 using the kernel alignment or the class separability criterion is O (mn 2 d 3 ) , where m is the total number of objective function evaluations and O(d 3 ) is the complexity of eigen-decomposition of a d \u00d7d SPD matrix. Note that the complexity O(n 2 ) is common for all kernel parameter learning algorithms. Once \u03b1 * is obtained, computing the proposed DSK on a pair of SPD matrices is O(d 3 ). It is comparable with the original SK, which has a complexity of O(d 2.373 ) via computing the determinant instead of conducting an eigen decomposition.\nIn the framework using the radius margin bound, \u03c5 [defined in (22) ] is optimized by a combination of M(M \u2212 1)/2 binary SVM classifiers. Once \u03c5 is updated, O((n i + n j ) 2 d 3 ) is required to calculate the kernel matrix for SVM i j , and a QP problem involving n i + n j samples needs to be solved to update R 2 i j or w i j 2 . Let O(QP(n)) denote the computational complexity to solve a QP problem of n samples. The overall complexity to optimize \u03b1 in the framework of radius margin bound will be O(m 1\u2264i< j \u2264M [(n i +n j ) 2 d 3 + 2QP(n i +n j )]). In addition, one QP optimization can be avoided when the trace margin criterion is used, leading to a reduced complexity of O(m 1\u2264i< j \u2264M [(n i +n j ) 2 d 3 + QP(n i +n j )]). Finally, the computational complexity of classifying a test sample by (30) is O( 1\u2264i< j \u2264M [(n i + n j )d 3 ])."}, {"section_title": "V. EXPERIMENTAL RESULTS", "text": "In this experiment, we compare the proposed DSK with the original SK on various image classification tasks. The other metrics listed in Table I (in Section II) will also be compared. Source code implementing the proposed method is publicly available. [43] for texture classification and the FERET data set [44] for face recognition, which have been used in [3] . The third one is the ETH-80 [45] data set widely used for visual object categorization [13] , [21] . The last one is a resting-state fMRI (rs-fMRI) data set from ADNI benchmark database (http://adni.loni.usc.edu). In Brodatz, FERET, and ETH-80 data sets, images are represented by covariance descriptors. In the rs-fMRI data set, correlation matrix is extracted from each image to represent the corresponding subject. The details of these data sets will be introduced in Sections V-A-V-D.\nWe employ both k-NN and SVM as the classifiers. For the kernel alignment and class separability frameworks, k-NN is used with the DSK as the similarity measure, since it does not involve any other (except k) algorithmic parameter. This allows the comparison to directly reflect the change from SK to DSK. For the radius margin bound framework, SVM classifier is used since it is inherently related to this bound.\nIn this experiment, the DSKs obtained by the kernel alignment and class separability are called DSK-KA and DSK-CS. Also, DSK-RM indicates the DSK obtained by the radius margin bound, while DSK-TM denotes the DSK obtained by trace margin criterion. Subscripts p or c are used to indicate whether \u03b1 acts as the power or the coefficient of eigenvalues. All the names are summarized in Table II. All parameters, including the k of k-NN, the regularization parameter of SVM, \u03bb in (9) , \u03b8 in all the kernels in Table I , and the power order \u03b6 in the power Euclidean metric, are chosen via multifold cross validation on the training set.\nIn the experiments, we perform binary classification on the Brodatz and rs-fMRI data sets and multiclass classification on the Brodatz, FERET, and ETH-80 data sets. For each experiment on the Brodatz, FERET, and ETH-80 data sets, the data are randomly split into two equal-sized subsets for training and test. The procedure is repeated 20 times for each task to obtain stable statistics. LOO strategy is used for the rs-fMRI data set because the size of this data set is small. Besides classification accuracy, the p-value obtained by paired student's t-test between DSK and SK will be used to evaluate the significance of improvement ( p-value \u2264 0.05 is used)."}, {"section_title": "A. Results on the Brodatz Texture Data Set (Binary and Multiclass Cases)", "text": "The Brodatz data set contains 112 images, each of which represents one class of texture. Following [3] , a set of subregions is cropped from each image as the samples of the corresponding texture class. The covariance descriptor [1] is used to describe a texture sample (subregion) as follows.\n1) Each original texture image is scaled to a uniform size of 256 \u00d7 256. I (x, y) denotes the intensity value at that pixel. 4) Each subregion is represented by a 5 \u00d7 5 covariance matrix estimated using all (1024 = 32 \u00d7 32) the features vectors obtained from that subregion. For the experiment of binary classification, we first run pairwise classification between the 112 classes using SK with the k-NN classifier. The obtained classification accuracies are sorted in ascending order. The top 15 pairs with the lowest accuracies, which represent the most difficult classification tasks, are selected. The rest of these pairs are not included because SK has been able to obtain almost 100% accuracy on them. As we observed in the experiment, DSK achieves equally excellent performance as SK on these pairs. The selected 15 pairs are shown in the supplementary material with image IDs. The texture images in each pair are visually similar to each other, and it is challenging to classify them. In short, we obtain 15 pairs of classes. Each class consists of 64 samples, and each sample is represented by a 5 \u00d7 5 covariance descriptor.\nThe average classification accuracies on the 15 binary classification tasks are compared in Table IV . The left half of Table IV shows the results when the k-NN is the classifier, while the right half is for the SVM classifier. As observed from the left half, SK achieves the best performance (76.67%) under the column of competing methods. At the same time, the proposed DSK-KA and DSK-CS consistently achieve better performance than SK, when the parameter \u03b1 is used as the power or coefficient. Especially, DSK-KA p achieves the best performance (80.85%), obtaining an improvement of above 4% points over SK and more than 6% points over the other methods. The relatively large standard deviation in Table IV is mainly due to the variation of accuracy rates of the 15 tasks. Actually, the improvement of DSK over SK is statistically significant because the p -value between DSK-KA p and SK is as small as 5.4 \u00d7 10 \u22126 . To better show the difference between DSK-KA p and SK, their performance on each of the 15 pairs is reported in Table III . As observed, DSK-KA p consistently outperforms SK on each task. The right half of Table IV compares the DSK obtained by the radius margin bound with the other kernel methods, using SVM as the classifier. As observed, the four variants of DSK in this case, DSK-RM p , DSK-RM c , DSK-TM p , and DSK-TM c , outperform the other kernel methods, including SK. Note that AIRM does not admit a valid kernel [13] and is not included in the comparison.\nWe also test DSK on multiclass classification involving all the 112 classes of Brodatz data. As observed from Table V, all the DSK methods outperform SK and other methods in comparison. Specifically, as shown in the left part of Table V , SK has the highest classification accuracy (76.80%) among all these existing methods when k-NN is used as the classifier. Meanwhile, compared with SK, DSK-CS p achieves a further improvement of 1.6% points with a p-value of 0.0018. When SVM is used in the right part of Table V, DSK-RM p boosts the performance of SK from 78.01% to 83.40%, obtaining an improvement of 5.39% points.\nIn addition, we investigate the performance of DSK with respect to the number of samples used to estimate the covariance descriptors. Recall that a 5 \u00d7 5 covariance descriptor is estimated from 1024 feature vectors \u03c6(x, y) to represent an image region. This number is sufficiently large compared with the dimensions of the covariance descriptor, which are only five. The improvement in the above results demonstrates the effectiveness of DSK over SK when there are sufficient samples to estimate the covariance descriptor. As discussed in Section III-A, covariance matrix estimation is significantly affected by the number of samples. This motivates us to investigate how the performances of DSK and SK will change, if the number of feature vectors used to estimate the covariance descriptor is reduced. We take DSK-KA p as an example. In Fig. 1(a) , we plot the classification accuracy of DSK-KA p and SK averaged over the 15 binary classification tasks, when different percentages of the 1024 feature vectors are used. As shown, DSK-KA p consistently achieves better performance than SK, although both of them degrade with the decreasing number of feature vectors. This result shows that the following.\n1) When the samples available for estimation are inadequate, SK will become less effective. 2) DSK can effectively improve the performance of SK in this case. Fig. 1(b) plots a similar result obtained on the FERET face data set with the same experimental setting, that is, pairwise classification is performed using SK with the k-NN classifier The classification accuracies of DSK-KA p and SK averaged over the 15 selected binary classification tasks are shown in Fig. 1(b) .\nIn this experiment, we observe that DSK can often be solved efficiently. The optimization in all the three frameworks requires only a few iterations to converge. An example of the evolution of objective function on the Brodatz data set is shown in the supplementary material. In that example, DSK needs at most 15 iterations to converge."}, {"section_title": "B. Results on the FERET Face Data Set (Multiclass Case)", "text": "We evaluate the proposed DSK for face recognition on FERET [44] face data set. We use the b subset, which consists of 198 subjects and each subject has 10 images with various poses and illumination conditions. Following [3] , to represent an image, a covariance descriptor is estimated for a 43-D feature vector extracted at each pixel: where I (x, y) is the intensity value and |G u,v (x, y)| is the image feature of 2-D Gabor wavelets [46] . Table VI compares the classification accuracy on all the 198 classes of FERET data. As observed, DSK-KA p obtains an improvement of 1.6% points over SK. The p-value between DSK-KA p and SK is 0.0026, which indicates the statistical significance of the improvement. When the radius margin bound framework is used, DSK also consistently performs better than SK. Especially, DSK-RM c achieves an improvement as high as 4.9% points over SK, with a p-value of 1.8 \u00d7 10 \u22125 .\nTo test how the DSK performs with the number of classes and the k value of k-NN, we evaluate DSK-KA p and DSK-KA c using 10, 20, 40, and all 198 classes, respectively, with k = [1 : 2 : 11]. The experiment is conducted as follows.\n1) For the classification tasks with 10, 20, and 40 classes, these classes are randomly selected from the 198 classes. Five facial images are randomly chosen from a class for training, and the remaining ones are used for testing. Both the selections of classes and samples are repeated 10 times, respectively. 2) For the classification tasks with all the 198 classes, five facial images are randomly chosen from each class for training, and the remaining ones are used for testing. The selection of samples is repeated 20 times. The averaged classification accuracies of 10, 20, 40, and 198 classes with various k values are plotted in Fig. 2(a)-(d) . As observed, both DSK-KA p and DSK-KA c consistently outperform SK and other methods. This confirms further that the proposed DSK can increase class discrimination by adjusting the eigenvalues of the SPD matrices, making SK better align with specific classification tasks. An example of the learned adjustment parameters in various classification tasks is shown in the supplementary material."}, {"section_title": "C. Results on ETH-80 Data Set (Multiclass Case)", "text": "ETH-80 contains eight categories with 10 objects per category and 41 images for each object. The features that are same as those used in the Brodatz texture data set are extracted from each image, and a 5 \u00d7 5 covariance descriptor is constructed as a representation of the image. The 10 objects in the same category are labeled as the same class. We perform an eight-class classification task using DSK. As mentioned previously, data are randomly split into 20 pairs of training/test subsets (50%:50%) to obtain stable statistics. Table VII reports the performances of various methods. As observed, DSK still demonstrates the best performance. Specifically, DSK-CS p achieves an improvement of 2.3% points over SK with k-NN as the classifier, while DSK-TM p achieves an improvement of 2.4% points over SK, when SVM is used as the classifier.\nThe above experiments demonstrate the advantage of DSK in various important image recognition tasks. Also, this advantage is consistently observed when DSK is learned with three different criteria. This verifies the generality of DSK."}, {"section_title": "D. Brain Imaging Classification", "text": "We now test DSK on brain imaging analysis using correlation matrix. Correlation matrix is an SPD matrix in which each off-diagonal element denotes the correlation coefficient between a pair of variables. It is commonly used in neuroimaging analysis to model functional brain networks for discriminating patients with AD from healthy controls [11] . In this task, a correlation matrix is extracted from each rs-fMRI image and used to represent the brain network of the corresponding subject. This is also a classification task involving SPD matrices.\nThe rs-fMRI data set from ADNI consists of 44 mild cognitive impairment (an early warning stage of AD) patients and 38 healthy controls. The rs-fMRI images of these subjects are preprocessed by a standard pipeline using SPM8 (http://www.fil.ion.ucl.ac.uk/spm) for rs-fMRI. All the images are spatially normalized into a common space and parcellated into 116 regions of interest (ROI) based on a predefined brain atlas. Forty-two ROIs that are known to be related to AD are selected [47] in our experiment, and the mean rs-fMRI signal within each ROI is extracted as the features. We then construct a 42 \u00d7 42 correlation matrix for each subject [11] . The rs-fMRI images and the correlation matrices are illustrated in the supplementary material.\nIn this experiment, we compare DSK with the other methods to classify the correlation matrices. The classification is conducted in the LOO manner due to the limited number of samples. Specifically, one sample is left out as the test set with the remaining samples as the training set. This process is repeated for each of the samples. As observed in Table VIII , DSK again achieves the best classification performance. DSK-CS c increases the classification accuracy of SK from 56.1% to 62.2% with k-NN as the classifier, obtaining an improvement of 6.1% points. Similarly, DSK-RM p obtains an improvement of 4.9% points over SK when SVM is used as the classifier. This experimental result indicates that DSK holds promise for handling various types of SPD matrices in broad applications."}, {"section_title": "VI. DISCUSSION", "text": ""}, {"section_title": "A. On Using \u03b1 as Power or Coefficient", "text": "The two ways of using \u03b1 can be theoretically related. This is because for any \u03bb \u03b1 p (\u03bb > 0), we can always find a coefficient \u03b1 c that satisfies \u03b1 c \u03bb = \u03bb \u03b1 p by setting \u03b1 c = (\u03bb \u03b1 p /\u03bb). In practice, these two ways could lead to different solutions, because the corresponding objective functions are different and the resulting optimizations are not convex. Comparatively, the power method is recommended due to the following. 1) Using \u03b1 as a power can automatically maintain the SPD property since \u03bb \u03b1 p (\u03bb > 0) is always positive, while using it as a coefficient requires an additional constraint of \u03b1 c > 0. 2) We empirically find that the power method often converges faster and achieves better performance than the coefficient method."}, {"section_title": "B. On the Computational Efficiency of DSK", "text": "Once the adjustment parameter \u03b1 is obtained, DSK can be computed for a set of SPD matrices. Fig. 3 compares the timing results of the methods in Table I for computing a similarity matrix of 100 SPD matrices. The dimensions of these SPD matrices are gradually increased from 5 to 100. The experiment is conducted on a desktop computer with 3.6-GHz Core i7-3820 CPU and 32-GB memory. As observed, DSK can be computed as efficiently as SK, PEK, and LEK, and all of them are significantly faster than AIRM. For example, DSK needs only 3.3 s to compute a similarity matrix of 100-D SPD matrices, while AIRM needs as many as 51 s. AIRM will become less efficient when the dimensions are high or the number of SPD matrices is large. The kernel methods, such as CHK, LEK, EUK, PEK, SK, and DSK, can usually handle the situation more efficiently.\nIn addition, the computational cost of learning \u03b1 could be reduced by taking advantage of the following facts.\n1) Kernel matrix computation can be run in a parallel manner by evaluating every entry separately. 2) The most time-consuming step, eigen-decomposition, could be speeded up using approximate techniques, such as the Nystr\u00f6m method [48] .\nThese improvements will be fully explored in the future work."}, {"section_title": "C. More Insight on When DSK Works", "text": "By adjusting the eigenvalues of SPD matrices, DSK can increase the similarity of the SPD matrices within the same class and decrease the similarity of those different classes. We want to gain more insight on in what case DSK can work effectively. Let D = {x 1 , x 2 , . . . , x n , x n+1 } denote a set of d-dimensional vectors randomly sampled from a normal distribution N d (\u03bc, ). It is known that the scatter matrix S follows the Wishart distribution [49] :\nx i , and n is called the degree of freedom. Increasing the degree of freedom results in a smaller overall variance of S [50] . Note that the features extracted from an image region or an entire image can be considered as a random sample set D, and the covariance descriptors used in the above experiments can be considered as the samples from certain Wishart distributions. In light of this connection, we use a set of synthetic SPD matrices sampled from various Wishart distributions to investigate the effectiveness of DSK. Specifically, in our experiment, two classes of SPD matrices are obtained by sampling Wishart distributions W d ( 1 , n) and W d ( 2 , n), where 1 is set as a 5 \u00d7 5 identity matrix I 5\u00d75 and 2 is set as (1 + \u03c4 )I 5\u00d75 . \u03c4 is a small positive scalar, and its magnitude controls the difference between 1 and 2 . By varying \u03c4 and the degree of freedom n, we can generate a set of binary classification tasks with different levels of classification difficulty to evaluate DSK. First, we set n as 200 and vary \u03c4 to generate two classes of SPD matrices, with 1000 samples in each class. Larger \u03c4 will make the classification task easier since it leads to more different distributions. For each classification task, we randomly halve the samples to create 20 pairs of training/test subsets. Fig. 4(a) shows the performances of DSK (DSK-RM p is used as an example) and SK with respect to \u03c4 , while Fig. 4(b) reports the corresponding p-values between DSK and SK. As observed, DSK can consistently achieve a statistically significant ( p-value <0.05) improvement over SK when 10 \u22121.8 \u2264 \u03c4 \u2264 10 \u22120.83 . When \u03c4 is out of this range, the classification task becomes too difficult or too easy. In this case, DSK cannot improve the performance of SK. Similar results are obtained in Fig. 4(c) and (d) , where we fix \u03c4 as 10 \u22121 and change the degree of freedom n. As observed, DSK outperforms SK when 8 \u2264 n \u2264 750 and has a similar performance as SK otherwise.\nThis experiment reveals that DSK can effectively improve the performance of SK for a wide range of classification tasks unless the task is too difficult or too easy. "}, {"section_title": "D. Comparison Between DSK and the Methods of Improving Eigenvalue Estimation", "text": "Reshaping the eigenvalues of a sample covariance matrix has been extensively studied to improve the eigenvalue estimation and recover the true covariance matrix, especially when the number of samples is small [51] - [53] . We highlight the differences of the proposed DSK from these methods as follows.\n1) Handling the biasness of the eigenvalue estimation is only one of our motivations to propose DSK. The other more important motivation is to increase the discrimination of different sets of SPD matrices through eigenvalue adjustment."}, {"section_title": "2) DSK does not aim at (and is not even interested in)", "text": "restoring the unbiased estimates of eigenvalues. Instead, DSK adaptively adjusts the eigenvalues of sample covariance matrices in a supervised manner to increase the discriminative power of SK. Although DSK and the methods of improving eigenvalue estimation have different goals, it is still desirable to make a comparison between them in terms of the classification performance. We perform the comparison using three kinds of SPD matrices: 1) the sample covariance matrix; 2) the covariance matrix improved by the methods in [51]- [53] ; and 3) the covariance matrix obtained by the proposed eigenvalue adjustment in DSK. SK is used in the classification with SVM as the classifier. As observed in Table IX , the three methods in [51] - [53] are comparable with the method using the sample covariance matrices on Brodatz, FERET, and ETH80 data sets, while they obtain a slightly better performance on fMRI data set. We believe that this is because the improved estimation of eigenvalues may boost the classification performance, when the ratio of sample size to the dimensions of covariance matrix (denoted by n/Dim in Table IX ) is small. For example, the ratio n/Dim is less than two for the fMRI data set. However, when the ratio becomes larger, the sample covariance matrices will gradually approach to the ground truth, and therefore the methods of improving eigenvalue estimation become less helpful. Since DSK aims to classify different sets of SPD matrices, the eigenvalues are adjusted toward better discriminability. This is why DSK achieves better performance than the methods in [51] - [53] on the four data sets. For example, the improvement of DSK is as high as 5.4% and 4.9% points over the method in [52] on Brodatz and FERET data sets."}, {"section_title": "E. On the Discovery of Better SPD Kernels", "text": "Finally, we discuss what aspects may benefit the discovery of better kernels for SPD matrices. From our point of view, the following two aspects play an important role. 1) Distance Measure: A good distance measure should effectively take the underlying structure of SPD matrices into account. In this paper, we utilize the recently developed SK to meet this requirement. As a specially designed distance measure, the (square rooted) S-divergence well respects the Riemannian manifold where SPD matrices reside. The other distance measures, such as Cholesky, log Euclidean, and power Euclidean, listed in Table I could be investigated within our framework in the future work. Also, all the existing distance measures for SPD matrices (except the simplest Euclidean distance) involve matrix decomposition or inverse operation. This results in significant computational cost, especially when a kernel matrix needs to be computed over a large sample set. In the course of discovering better SPD kernels, a computationally efficient distance measure will be highly desirable.\n2) Class Information: The class information should be effectively integrated into SPD kernels to improve its quality further. In this paper, we achieve this by utilizing the class information to adjust the eigenvalues to make SK better align with specific classification tasks. For SK, adjusting the eigenvalues (rather than including the matrix of eigenvectors) may only have been sufficient, because this kernel is invariant to affine transformations. 4 There could be different but effective adjustment ways for other types of kernels and this is worth exploring further. Besides, we focus on improving an SPD kernel in the supervised learning case in this paper. Nevertheless, the proposed approach shall be extendable to the unsupervised case, which usually has a wider range of applications. In that situation, how to incorporate cluster information to improve SPD kernels will also be an interesting topic to explore.\nIn addition, as shown in this paper, discovering better SPD kernels may need to optimize certain properties of SPD matrices. In this case, how to design and solve the resulting optimization problem becomes a critical issue. In particular, having a convex objective function and a computationally efficient optimization algorithm will be of great importance. This could be possibly achieved by appropriately convexifying and approximating the employed nonconvex objective functions. In this paper, we focus on validating the effectiveness of the proposed approach and demonstrating its advantages, and employ the commonly used gradient-based techniques to solve the involved optimization problems. In our future work, more advanced optimization techniques and algorithms will be developed to improve the proposed approach further."}, {"section_title": "VII. CONCLUSION", "text": "In this paper, we analyzed two potential issues of the recently proposed SK for classification tasks and proposed a novel method called DSK. It automatically adjusts the eigenvalues of the input SPD matrices to help SK to achieve greater discrimination. This problem is formulated as a kernel parameter learning process and solved in three frameworks. The proposed kernel is evaluated on both synthetic and real data sets for a variety of applications in pattern analysis and computer vision. The results show that it consistently achieves better performance than the original SK and other methods for SPD matrices. We also provided more insights on when and how DSK works, discussed the aspects that could contribute to discovering better SPD kernels, and pointed out the future work to enhance the proposed approach."}]