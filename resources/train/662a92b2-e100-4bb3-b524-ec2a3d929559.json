[{"section_title": "Abstract", "text": "Abstract. We developed a convolution neural network (CNN) on semiregular triangulated meshes whose vertices have 6 neighbours. The key blocks of the proposed CNN, including convolution and down-sampling, are directly defined in a vertex domain. By exploiting the ordering property of semi-regular meshes, the convolution is defined on a vertex domain with strong motivation from the spatial definition of classic convolution. Moreover, the down-sampling of a semi-regular mesh embedded in a 3D Euclidean space can achieve a down-sampling rate of 4, 16, 64, etc. We demonstrated the use of this vertex-based graph CNN for the classification of mild cognitive impairment (MCI) and Alzheimer's disease (AD) based on 6767 MRI scans of the Alzheimer's Disease Neuroimaging Initiative (ADNI). We compared the performance of the vertex-based graph CNN with that of the spectral graph CNN."}, {"section_title": "Introduction", "text": "Machine learning has been widely used as one crucial technique for medical image segmentation, registration, disease prediction and classification, in which image data are sampled on a Euclidean equi-spaced grid. However, the geometry of human organs is in general very complex, which characterizes the intrinsic properties of anatomy and physiological functions of the organs. For instance, myocardial contraction flows along the wall of the heart. Cortical thickness, representing the depth of the cortical ribbon, is related to the nature of the convoluted gyri and sulci of the cortex. Cortical thickness is thicker on cortical gyri but thinner on cortical sulci. Hence, it is preferred to represent brain image data in terms of its geometry that can be expressed as meshes embedded in the Euclidean space of the brain image. It has been demonstrated that geometric structure relating image data did introduce useful information in machine learning based methods for disease diagnosis (e.g., [26, 9, 1, 20] ).\nIn recent years, deep learning has been one of prevalent machine learning techniques to tackle a wide range of image-related applications. Among many architectures of deep neural networks, convolutional neural network (CNN) received a great attention for its successes in computer vision (e.g. [23, 21, 25, 24, 14, 11] ), medical imaging and diagonosis (e.g., [22, 18, 8, 15] ). The main blocks to build a CNN include convolution with localized filters, non-linear activation function, and pooling. In CNN, these three blocks are sequentially concatenated to model highly non-linear intrinsic patterns of training data and output the features for targeting applications. Most of these CNNs are developed for modeling image data defined on equi-spaced regular grids. The generalization of such CNNs to image data defined on the meshes embedded in a higher dimensional Euclidean space is non-trivial, especially for the localized convolution and pooling operations."}, {"section_title": "CNN on general graphs", "text": "One might view meshes as a special class of graphs. There have been several works on generalizing the CNN for modeling data on general graphs; (e.g., [2, 3, 12] ). Based on spectral graph theory, Henaff et al. [12] proposed a CNN for graph-structured data, in which convolution is defined as a diagonal multiplicative operation in graph Fourier transform derived from a normalized graph Laplacian. The localization of the convolution is imposed by regularizing those diagonal entries with a smoothness prior. To avoid the computation of a graph Laplacian and have a convolution with better localization, Defferrard et al. [3] introduced Chebyshev polynomial approximation such that the resulting convolution operator is a polynomial of the adjacency matrix of a graph. Kipf and Welling [13] further simplified the approximation using the linear polynomial of the adjacency matrix of a graph and applied the CNN for semi-supervised learning.\nNevertheless, the convolution built on the polynomials of the adjacency matrix is very different from classic convolution on equi-spaced grids. Consider such a generalized convolution derived from the polynomial degree k. Then, it is parameterized by totally (k + 1) parameters. The support of such a convolution, i.e., the number of the vertices it covers for each shift, is 3k 2 + 3k + 1 vertices for a regular triangular mesh. The idea of classic CNN for modeling images on equi-spaced grids is that it uses small filters to collect as much local information as possible, and then gradually increase the filter width and down-size the features to represent more global and high-level information. A convolution that covers a large number of vertices might lose important local features which are helpful for modeling.\nAs mentioned above, how to down-size the feature is also an essential operation for CNN to abstract more high-level information. Such a down-sizing operation happens in both pooling and convolution with stride > 1. The graph coarsening procedure used for the pooling in [3] is implemented by calling a weighted graph cut method [4] . From the coarsest to the finest level, fake vertices, i.e. disconnected vertices, are added to pair with the singletons such that each vertex has two children. The fake vertices artificially increase the dimensionality and thus the computation cost even though the number of singletons from multilevel clustering algorithms may not be large."}, {"section_title": "CNN on semi-regular triangular meshes", "text": "In this study, we proposed a vertex-based CNN approach for modeling image data defined on semi-regular triangular meshes that are well structured in terms of connectivities, e.g., the connectivity of most vertices is 6. A semi-regular triangular mesh has certain similarities to equi-spaced grids in the Euclidean space. When image data are defined on a semi-regular triangular mesh, a direct call of a generic CNN for a graph certainly is sub-optimal, as it discards specific connectivity properties of the mesh. Indeed, the connectivity property of a triangular mesh enables us to better mimic convolution and down-sizing operation such as to avoid the issues encountered in the CNN defined on a graph discussed in the previous section.\nThe key blocks of the CNN proposed in this paper, especially convolution and down-sizing, are directly defined in a vertex domain. By exploiting the ordering property of semi-regular meshes, the convolution is defined on a vertex domain with strong motivation from the spatial definition of classic convolution. Moreover, the down-sampling of a semi-regular mesh was efficient. The downsampling of a semi-regular mesh embedded in a 3D Euclidean space can achieve a down-sampling rate of 4, 16, 64, etc. We demonstrated the use of this vertexbased graph CNN for the classification of mild cognitive impairment (MCI) and Alzheimer's disease (AD) based on 3169 image datasets of the Alzheimer's Disease Neuroimaging Initiative (ADNI). We compared the performance of the vertex-based graph CNN with that of the spectral graph CNN [3] ."}, {"section_title": "Methods", "text": ""}, {"section_title": "Convolution in the vertex domain", "text": "Consider a signal f defined on an equi-spaced grid {k} k\u2208Z 2 , and a finite filter h supported on a finite set \u2126 \u2282 Z 2 . The convolution is then defined by\nIt can be seen that at the vertex m, the value of f \u2297 h is indeed weighted average of f over the neighbors of the vertex m 0 , whose weights are given by h and neighbors are determined by \u2126. In the following, we generalize such a concept to a semi-regular triangular mesh whose vertex in general has 6 neighbors. We begin with a semi-regular triangular mesh\nwhere {x i } denotes the set of vertex coordinates. {\u03a3 ijk } is the set of simplices and N represents the total number of vertices on the mesh. Each simplex \u03a3 ijk is a three tuple of points (i, j, k), i, j, k \u2208 {1, . . . , N } that specifies the vertices forming a triangular face, i.e., all three vertices x i , x j and x k are one-ring neighbours of each other. If T is a semi-regular mesh, then for each vertex, x i , the set of its neighboring vertices can be denoted as P i \u2282 {x i } :\nThe ordering of these vertices for this convolution is not straightforward. Fig. 1 illustrates how these vertices are ordered in this study. We first define a sphere, S i (blue in Fig. 1 ), that passes x i and approximates the mesh formed by P i . The tagent plane (orange in Fig. 1 ) of x i on the mesh, T , is defined as the tagent plane of x i on the sphere, S i . The xyz\u2212coordinate of the tagent plane of x i (red in Fig. 1 ) is the translation and rotation of the coordinate of S i (black in Fig. 1 ).\nWe then order these six vertices in a closewise sequence, where P [i, 1] is defined as the vertex whose projection is the closest to the x-axis of the tagent plane of the vertex, x i . Consider a 1-ring filter h \u2208 R 7 :\nThen, at the vertex x i , the value of a signal f defined on T convolved by h is defined by\nwhere f [i, j] denote the value of f at the vertex P [i, j] and\nIn a matrix form, we define a matrix D \u2208 R 7,N as\nThen, the convolution defined in Eq. (2) can be expressed in the form of a matrix multiplication:\nFor the vertex in T with valence < 6, whose corresponding column has nondefined entries. Analogous to classic convolution for finite signals, we can define the values of these entries using boundary extension. For example, assigning 0 to these entries which is the same as zero padding boundary extension in classic convolution.\nBy the same procedure, we can define 2-ring convolution and more. Consider a k-ring convolution, it is parameterized by totally 3k(k + 1) + 1 parameters, and its support also covers the same number of vertices. This is consistent with the behavior of classic convolution on equi-space grids. Such localization property enables CNN to extract very local features of the data on semi-regular triangular meshes, the same as what CNN is doing on equip-spaced grids. Such ring-type convolution also has been exploited in wavelet transform for surface processing [5] ."}, {"section_title": "Rectified Linear Unit and Pooling", "text": "For CNN, there are many types of non-linear activation function. The activation function is a map from R to R, which does not involve any geometrical property of the underlying structure. In our proposed CNN for image data on a semiregular mesh, we adopt the well-known rectified linear unit (ReLU):\nIn addition to convolution and ReLU, another important block is pooling, which can be viewed as a non-linear or linear down-sampling operation. The pooling enables us to reduce the size of representation and thus to reduce the number of parameters, which helps memory usage, computational efficiency and overfitting controlling. The pooling is done by either taking the maximum or taking the average of the neighbors of those vertices lying on the coarser grid/mesh. The key for defining a pooling operation on a mesh is about how to define a hierarchical triangular mesh:\nsuch that the vertices of T (j+1) contain all vertices of T (j) and new vertices, and T (L) is the original mesh T on which image data is defined.\nAs the design of the proposed CNN mainly aims at modeling brain image data, we first generate a hierarchical semi-regular triangular mesh such that the mesh in the finest scale is the mesh extracted from image data. There are many approaches for generating a hierarchical triangular mesh and we adopt the one used [17] \nSee Fig. 2 for an illustration. Starting with an initial mesh at the coarsest level, recursively applying the subdivision scheme above leads to a hierarchical semi-regular triangular mesh.\nThe vertex number of the mesh at each level is 4 times that of the mesh at the next coarse level. It can be seen that for any vertex x denote the set of this vertex and all its 1-ring neighbors. Then, the pooling operator with stride 2 is defined as\nwhere f [i] (j) denotes the value at the vertex x i in the j-th level mesh T (j) . Similarly, we define the pooling operator with stride 2 (and more) by running the same procedure on the vertices and all its 1-ring and 2-ring neighbors (and more) in the next finer level mesh. Based on main ingredients presented in the previous section, we propose a vertex-based CNN for analyzing image data defined on a semi-regular mesh, which is analogous to classic CNN for image data defined on equi-spaced grids. The CNN is composed of totally L+1 connected blocks"}, {"section_title": "CNN on a semi-regular mesh", "text": ". The first L blocks are the blocks for feature extraction. Each block contains three sequentially concatenated layers: (1) a convolution layer with multiple 1-ring convolutions; (2) a ReLU layer; (3) a pooling layer with stride 2 that uses mean pooling:\nThe last block M (L+1) is the classification layer using the features extracted from the previous blocks, In our implementation of the proposed CNN for classifying brain image data. It has 4 feature extraction blocks and 1 classification block. The numbers of the convolution filters in these blocks are [8, 16, 32, 64] respectively. The classification layer is implemented using a fully connected layer with 512 nodes and with a softmax output. Fig. 3 shows the architecture of the proposed CNN. The 1-ring convolution operation can be implemented in a matrix multiplication. The other layers can be implemented using the standard procedure. In our implementation, the CNN is trained using the Adam optimization algorithm. We implemented this CNN on a semi-regular triangulated mesh in Tensorflow. The code is available at the website 1 "}, {"section_title": "Results", "text": ""}, {"section_title": "MRI Data and Analysis", "text": "ADNI cohort. This study employed the data from the ADNI-1 and ADNI-2 cohort. The ADNI-2 cohort only included 1149 subjects (400 cognitive normal (CON), 301 early MCI (EMCI), 187 late MCI (LMCI) and 261 AD). The ADNI-1 cohort only included 1013/3598 subjects/scans (243/1067 CON, 415/1515 MCI, and 355/1016 AD). The number of visits of each subject varied from 1 to 7 (i.e., baseline, 3-, 6-, 12-, 24-, 36-, and 48-month). At each visit, subjects were diagnosed as one of the four clinical statuses based on the criteria described in the ADNI-2 protocol (http://adni.loni.usc.edu). The general diagnostic criteria for early and late MCI were the same except LMCI subjects had a lower cut-off point for logical memory II subscale from Wechsler Memory Scale. The demographic and clinical information of subjects from ADNI-2 and ADNI-2 are provided in Table 1 and Table 2 . Abbreviations. CON: Control normal; AD: Alzheimer's disease; MCI: Mild cognitive impairment; EMCI: Early MCI; LMCI: Late MCI. * The number of subjects for each group was based on the clinical status during the MRI acquisition visit. There were subjects who fall into 2 or more groups due to the conversion from one clinical status to another.\nMRI Data Analysis. All T1-weighted images were segmented using FreeSurfer [10] . The processed images were quality checked based on the criteria listed on the website 2 . We represented cortical thickness on the cortical surface generated by FreeSurfer. We employed a large deformation diffeomorphic metric mapping (LDDMM) algorithm [28, 6] to align individual cortical surfaces to the atlas and transferred the thickness of each individual subject to the atlas. As each subject may have multiple MRI scans, one at each visit, this study included all available T1-weighted images with good quality after processing. We used the clinical status at the MRI acquisition as the classification ground truth. For instance, a subject with multiple scans may have different clinical labels if he/she was from one clinical status to another over time. From 3365 scans from ADNI-2, we discarded 196 scans that missed demographic information or diagnosis labels of CON, EMCI, LMCI and AD, resulting in 3169 scans used in the following CNN analysis. From 3783 scans from ADNI-1, we discarded 185 scans that missed demographic information or diagnosis labels of CON, MCI, and AD, resulting in 3598 scans used below."}, {"section_title": "Comparison with the Graph CNN", "text": "In this experiment, we compared the computational cost and classification accuracy between the proposed vertex-based CNN and graph CNN [3] based on the ADNI-2 data. The graph CNN [3] incorporated 3 CNN layers with the number of filters of [8, 16, 32] respectively and a final fully connected layer with 128 nodes. The convolution in the graph CNN was approximated using Chebyshev polynomial with the order of 3. The network parameters were trained with a mini-batch size of 64, an initial learning rate of 1e \u22123 , a weight decay of 0.05, and a momentum of 0.9. During the training process, a l 2 \u2212norm regularization function of 5e \u22124 was applied on the weights of the final fully connected layer to prevent overfitting to the training data. This study employed the 10-fold cross-validation, where the scans from the same subject were assigned to the validation (or testing) to avoid the data leakage issue in the predictive model. We determined the parameters, such as the number of layers and the number of filters, and a learning rate, based on geometric mean (GMean= \u221a SEN \u00d7 SP E, where SEN and SP E respectively represent sensitivity (SEN) and specificity (SPE). We chose this measure because it not only maximized the accuracy on each of the two classes but also minimized the difference between the sensitivity and specificity, i.e., the balanced performance for both the positive and negative classes.\nWe performed the same procedure as mentioned above for six classifiers, including CON vs. AD, CON vs. LMCI, CON vs. EMCI, EMCI vs. LMCI, EMCI vs. AD, and LMCI vs. AD. The experiments were run on Telsa M40 GPU (24GB memory). The computational time for each epoch of our vertex-based CNN and graph CNN [3] was respectively 40 sec and 113 sec. Our proposed approach was 2.83 times faster than the graph CNN. Table 3 lists the classification accuracy, sensitivity, specificity, and GMean for each classifier. Our proposed vertex-based CNN was better performed than the graph CNN in most of the classifiers, including CON vs. AD, CON vs. LMCI, CON vs. EMCI, EMCI vs. AD, and LMCI vs. AD, except the EMCI vs. LMCI classifier. In addition, our vertex-based approach provided a relatively lower variability across all the four evaluation measures. These findings suggested that the proposed CNN is a fast computational model and has the potential to improve classification accuracy compared to the graph CNN [3] . "}, {"section_title": "Application to ADNI-1", "text": "In this study, we applied the CON vs LMCI, CON vs AD, and LMCI vs AD classifiers obtained from the ADNI-2 cohort to the ADNI-1 cohort. Table 4 lists the classification accuracy for MCI and AD, which is comparable to those listed in Table 3 . This suggests the robustness of the classifiers built based on the ADNI-2 cohort to the other dataset. This paper presented a vertex-based CNN on meshes, in particular, on semiregular triangulated meshes. We showed that the convolution operation on semiregular triangulated meshes has the property of translation, similar to that on the Euclidean space. The pooling operation on semi-regular triangulated meshes is analogous to that in the classic CNN in the Euclidean space. We employed this approach to the ADNI-2 data and compared its performance to that of the graph CNN [3] . Our results showed that our vertex-based CNN algorithm was faster than the graph CNN. This is partly because the mesh coarsening procedure used for the pooling in the graph CNN [3] requires adding fake vertices with the singletons such that each vertex has two children. This procedure increases the data dimension that is needed for CNN. In contrast, the pooling operation in our proposed vertex-based CNN is with stride 2, similar to the downsampling factor achieved in the Euclidean space. Moreover, compared to the graph CNN, our proposed vertex-based CNN improved the classification accuracies of the five classifiers, except the EMCI and LMCI classifier. One of the potential limitations of our proposed approach is that it requires meshes to be semi-regular. In general, the construction of semi-regular meshes for medical image data is not an issue. However, our approach does not apply to graph data, such as social networks and citation networks and so on. In the past decade, substantial studies reported the classification among CON, MCI, and AD based on the ADNI dataset (e.g., [16, 19, 7, 29, 27] . Some of them were based on multi-modal brain images and reported the classification accuracy better than that in Table 3 (e.g., [19, 7, 29, 27] ). But the sample size was relatively small hence it is unclear on the robustness of the classification results. Nevertheless, our approach can be easily extended to multi-channel vertex-based CNN for handling multiple-modal or multiple structural data, such as diffusion properties of the cortex, cortical surface area and hippocampal shape. Compared to the existing studies based on cortical thickness ( 85%) [16] , our approach reported the highest classification accuracy. To our best knowledge, our experiment employed the largest image data available in the ADNI-2 cohort, suggesting its potential robustness to other AD datasets."}]