[{"section_title": "Introduction", "text": "The School Survey on Crime and Safety (SSOCS) is managed by the National Center for Education Statistics (NCES) within the Institute of Education Sciences of the U.S. Department of Education. SSOCS collects extensive crime and safety data from principals and administrators of public schools in the United States. Data from this collection can be used to study the relationship between school characteristics and violent and serious violent crimes in U.S. public schools and to examine what school programs, practices, and policies are used by schools in their efforts to reduce or prevent crime. SSOCS has been conducted six times, during the 1999-2000, 2003-04, 2005-06, 2007-08, 2009-10, and 2015-16 school years. The latest administration of SSOCS, SSOCS:2016, was conducted by NCES, and the data collection was administered by the U.S. Census Bureau. Funding for the survey was supported by the National Institute of Justice (NIJ) of the U.S. Department of Justice through its Comprehensive School Safety Initiative, which was developed in response to a 2014 congressional appropriation to conduct research about school safety. Out of a sample composed of 3,553 primary, middle, high, and combined public schools, a total of 2,092 public schools submitted completed questionnaires, for a weighted response rate of 62.9 percent. Data were collected from February 22, 2016, through July 5, 2016 This manual is designed to assist users of the public-use SSOCS:2016 data file and offers information about the SSOCS:2016 collection, including its purpose, the sample design, data collection methods, and data processing procedures. The manual also contains a copy of the SSOCS:2016 questionnaire instrument (appendix A) as well as information specific to the SSOCS:2016 public-use data file, including a list of variables and the record layout of the fixedformat ASCII file (appendix B). The public-use data file may be obtained at https://nces.ed.gov/surveys/ssocs/data_products.asp. A restricted-use data file is also available. To protect the confidentiality of sampled schools, certain variables included in the restricted-use file are not available in the public-use file. The restricted-use data file, and a corresponding user's manual, may be obtained through a special licensing agreement with NCES. To learn more about obtaining a license, please visit http://nces.ed.gov/statprog/instruct.asp."}, {"section_title": "Background of the Study", "text": "A safe school environment is necessary for educating our nation's youth. Students who engage in criminal behavior at school or who are victims of crime at school may not meet their potential in the classroom or at home. While school crime has always been a major concern for educators, researchers, and policymakers, it gained national attention in the aftermath of several school shootings that took place in the 1997-98 school year. Although the federal government had collected crime and safety data for several decades, these events highlighted a need for a survey that would build upon prior school crime and safety surveys 1 while meeting an increased demand for quality and timely data pertaining to the condition of education in the United States. The SSOCS program was established by NCES in response to this need, specifically, to address safety in and around American public schools. SSOCS is the only recurring federal survey that collects detailed information on school crime and safety from the school's perspective. SSOCS has been designed to meet the congressional mandate for NCES to provide statistics on the frequency of school violence, the nature of the school environment, and the characteristics of school violence prevention programs. Such national data are critical, given the tendency to focus on anecdotal evidence of crimes without knowing the true frequency of problems in schools. Accurate information is necessary for policymakers to make informed decisions about school policy and to demonstrate to the public a proactive approach to school safety. SSOCS data help the policy and program offices at the U.S. Department of Education design grant programs intended to address school safety, violence prevention, and school climate. Additionally, the national estimates of school crime and safety that SSOCS provides assist NCES and NIJ in fulfilling the goal of the Comprehensive School Safety Initiative, which is to improve the safety of our nation's schools and students through rigorous research that produces practical knowledge. 2"}, {"section_title": "Questionnaire Development", "text": "Since its introduction during the 1999-2000 school year, the SSOCS questionnaire has evolved over each subsequent data collection. At various times in the history of SSOCS, the survey items have been examined both for the quality of their content and of the data collected and, when necessary, the questionnaire has been adjusted. However, to the extent possible, much of the questionnaire content remains unchanged between survey administrations in order to maintain consistent benchmarks over time. The SSOCS:2016 questionnaire is the result of extensive research and development on emerging issues of school crime and items preserved from previous SSOCS data collections. The original SSOCS questionnaire, used in the 2000 data collection, was developed in consultation with a technical review panel (TRP) 3 consisting of some of the nation's top experts on school crime and school programs relating to crime and safety. Revisions to the 2004 questionnaire were based on an analysis of responses to the 2000 questionnaire, a review of current literature in the field, feedback from a TRP and invested government agencies, and the results of extensive pretesting. The questionnaires used in 2006 and 2008 were essentially the same as the 2004 questionnaire. The questionnaire used in 2010 was similar to that used in 2008, but it incorporated minor revisions based on feedback from several SSOCS data users and school crime and safety experts. 4 Revisions to the SSOCS:2016 questionnaire were based on several sources of information, including an analysis of responses to the SSOCS:2010 questionnaire, a review of current literature in the field, feedback from a TRP and invested government agencies, the results of extensive cognitive testing, and NIJ's interest in collecting data on school security personnel and school mental health services. While the SSOCS:2016 questionnaire included topics covered in prior years, some individual items were modified and new content was added. A copy of the SSOCS:2016 questionnaire can be found in appendix A. Differences between the 2010 and 2016 questionnaires are detailed below."}, {"section_title": "Changes to definitions between SSOCS:2010 and SSOCS:2016", "text": "This section outlines the changes made to definitions between the 2010 and 2016 survey administrations. First, several definitions were added to the 2016 questionnaire to clarify terms used in new survey items. Second, definitions were added to clarify four terms contained in the 2010 questionnaire (as well as in the 2016 questionnaire), but that had not been formally defined: bullying, cyberbullying, gender identity, and sexual orientation. Finally, minor modifications were made to three definitions, and one definition was removed."}, {"section_title": "Definitions added to SSOCS:2016", "text": "\u2022 Active shooter -A formal definition was added to the survey using language from the Department of Homeland Security and the Department of Education's emergency recommendations. Active shooter is defined as \"an individual actively engaged in killing or attempting to kill people in a confined and populated area; in most cases, active shooters use firearm(s) and there is no pattern or method to their selection of victims.\" \u2022 Bullying -A formal definition was added to the survey using language from the Centers for Disease Control and Prevention (CDC). Bullying is defined as \"any unwanted aggressive behavior(s) by another youth or group of youths who are not siblings or current dating partners that involves an observed or perceived power imbalance and is repeated multiple times or is highly likely to be repeated.\" \u2022 Cyberbullying -The definition for cyberbullying was removed from the stem of item 33 and relocated to the definitions page since multiple survey items now include this term. Cyberbullying is defined as \"when willful and repeated harm is inflicted through the use of computers, cell phones, or other electronic devices.\" \u2022 Diagnostic assessment -A formal definition was added to the survey in accordance with the addition of a new section on school mental health services. Diagnostic assessment is defined as \"an evaluation conducted by a medical or mental health professional that identifies whether an individual has one or more medical and/or mental health diagnoses. This is in contrast to an educational assessment, which does not focus on clarifying a student's diagnosis.\" \u2022 Evacuation -A formal definition was added to the survey in accordance with the addition of a new survey item on the types of drills used for emergency procedures. Evacuation is defined as \"a procedure that requires all students and staff to leave the building. While evacuating to the school's field makes sense for a fire drill that only lasts a few minutes, it may not be an appropriate location for a longer period of time. The evacuation plan should encompass relocation procedures and include backup buildings to serve as emergency shelters, such as nearby community centers, religious institutions, businesses, or other schools. Evacuation also includes 'reverse evacuation,' a procedure for schools to return students to the building quickly if an incident occurs while students are outside.\" \u2022 Gender identity -A formal definition was added to the survey to clarify the term used in both new and existing survey items. Gender identity \"means one's inner sense of one's own gender, which may or may not match the sex assigned at birth. Different people choose to express their gender identity differently. For some, gender may be expressed through, for example, dress, grooming, mannerisms, speech patterns, and social interactions. Gender expression usually ranges between masculine and feminine, and some transgender people express their gender consistent with how they identify internally, rather than in accordance with the sex they were assigned at birth.\" \u2022 Lockdown -A formal definition was added to the survey in accordance with the addition of a new survey item on the types of drills used for emergency procedures. Lockdown is defined as \"a procedure that involves occupants of a school building being directed to remain confined to a room or area within a building with specific procedures to follow. A lockdown may be used when a crisis occurs outside of the school and an evacuation would be dangerous. A lockdown may also be called for when there is a crisis inside and movement within the school will put students in jeopardy. All exterior doors are locked and students and staff stay in their classrooms.\" \u2022 Mental health disorder -A formal definition was added to the survey in accordance with the addition of a new section on school mental health services. Mental health disorders are defined as \"collectively, all diagnosable mental disorders or health conditions that are characterized by alterations in thinking, mood, or behavior (or some combination thereof) associated with distress and/or impaired functioning.\" \u2022 Mental health professional -A formal definition was added to the survey in accordance with the addition of a new section on school mental health services. The definition aligns with the definition used in the School Health Policies and Practices Survey (SHPPS), which is administered by the CDC. Mental health professionals are defined as \"mental health services are provided by several different professions, each of which has its own training and areas of expertise. The types of professionals who may provide mental health services include psychiatrists, psychologists, psychiatric/mental health nurse practitioners, psychiatric/mental health nurses, clinical social workers, and professional counselors.\" \u2022 Restorative circle -A formal definition was added to the survey in accordance with the addition of a new item on student involvement in restorative circles. A restorative circle is defined as \"a formal mediation process led by a facilitator that brings affected parties of a problem together to explore what happened, reflect on their roles, find a solution, and ultimately restore harmony to individual relationships and the larger community.\" \u2022 Sexual orientation -A formal definition was added to the survey to clarify the term used in both new and existing survey items. Sexual orientation \"means one's emotional or physical attraction to the same and/or opposite sex.\" \u2022 Shelter-in-place -A formal definition was added to the survey in accordance with the addition of a new survey item on the types of drills used for emergency procedures. Shelter-in-place is defined as \"a procedure similar to a lockdown in that the occupants are to remain on the premises; however, shelter-in-place is designed to use a facility and its indoor atmosphere to temporarily separate people from a hazardous outdoor environment. Everyone would be brought indoors and building personnel would close all windows and doors and shut down the heating, ventilation, and air conditioning system (HVAC). This would create a neutral pressure in the building, meaning the contaminated air would not be drawn into the building.\" \u2022 Threat assessment team -A formal definition was added to the survey in accordance with the addition of two new survey items that ask about formal groups whose purpose is to identify students who might be a potential risk for violent behavior. A threat assessment team is defined as \"a formalized group of persons who meet on a regular basis with the common purpose of identifying, assessing, and managing students who may pose a threat of targeted violence in schools.\" \u2022 Treatment -A formal definition was added to the survey in accordance with the addition of a new section on school mental health services. Treatment is defined as \"a clinical service addressed at lessening or eliminating the symptoms of a disorder. In mental health, this may include psychotherapy, medication treatment, and/or counseling.\""}, {"section_title": "SSOCS:2016 definitions modified from SSOCS:2010", "text": "\u2022 Hate crime -The definition for hate crime was revised to align with the Federal Bureau of Investigation's definition and to specifically identify gender identity as a bias. For SSOCS:2016, a hate crime is defined as \"a committed criminal offense that is motivated, in whole or in part, by the offender's bias(es) against a race, religion, disability, sexual orientation, ethnicity, gender, or gender identity; also known as bias crime.\" \u2022 Rape -The definition of rape has been modified to specify that rape includes sodomy and to instruct respondents to report attempted rapes as rapes. Rape is defined as \"forced sexual intercourse (vaginal, anal, or oral penetration). This includes sodomy and penetration with a foreign object. Both male and female students can be victims of rape.\" \u2022 Sexual assault -An editorial change was made to revise \"sexual battery\" to \"sexual assault\" and the definition was updated to mirror the terminology used by the Office on Violence Against Women, within the U.S. Department of Justice, in its key elements. Sexual assault is defined as \"an incident that includes threatened rape, fondling, indecent liberties, or child molestation. Both male and female students can be victims of sexual assault. Classification of these incidents should take into consideration the age and developmentally appropriate behavior of the offender(s).\""}, {"section_title": "SSOCS:2010 definitions not included in SSOCS:2016", "text": "\u2022 Cult or extremist group -This definition was removed because the SSOCS:2010 item that included this term (item 20i) is not included in the SSOCS:2016 questionnaire."}, {"section_title": "Changes to items between SSOCS:2010 and SSOCS:2016", "text": "This section details the item additions, modifications, and deletions made between the 2010 and 2016 survey administrations. 5 In addition, throughout the questionnaire, the school year has been updated to reflect the 2015-16 school year."}, {"section_title": "Items added to SSOCS:2016", "text": "\u2022 Item 1f. Equip classrooms with locks so that doors can be locked from the inside (C0121) \u2022 Item 1p. Have \"panic button(s)\" or silent alarm(s) that directly connect to law enforcement in the event of an incident (C0139) \u2022 Item 2h. Post-crisis reunification of students with their families (C0157) \u2022 Item 3. During the 2015-16 school year, has your school drilled students on the use of the following emergency procedures? o Item 3a. Evacuation (C0163) o Item 3b. Lockdown (C0165) o Item 3c. Shelter-in-place (C0167) \u2022 Item 4i. Student involvement in restorative circles (e.g., \"peace circles,\" \"talking circles,\" \"conflict circles\") (C0179) \u2022 Item 4j. Social emotional learning (SEL) training for students (e.g., social skills, anger management, mindfulness) (C0183) \u2022 Item 5. During the 2015-16 school year, did your school have a threat assessment team or any other formal group of persons to identify students who might be a potential risk for violent or harmful behavior (toward themselves or others)? (C0600) \u2022 Item 6. During the 2015-16 school year, how often did your school's threat assessment team formally meet? (C0602) \u2022 Item 7. During the 2015-16 school year, did your school have any recognized student groups with the following purposes? o Item 7a. Acceptance of sexual orientation and gender identity of students (e.g., Gay-Straight Alliance) (C0604) o Item 7b. Acceptance of students with disabilities (e.g., Best Buddies) (C0606) o Item 7c. Acceptance of cultural diversity (e.g., Cultural Awareness Club) (C0608) \u2022 Item 13d. Wear a body camera (C0626) \u2022 Item 14a. Motor vehicle traffic control (C0628) \u2022 Item 14i. Recording or reporting discipline problems to school authorities (C0644) \u2022 Item 14j. Providing information to school authorities about the legal definitions of behavior for recording or reporting purposes (e.g., defining assault for school authorities) (C0646) \u2022 Item 15. During the 2015-16 school year, did your school have a sworn law enforcement officer (including School Resource Officers) present for all instructional hours every day that school was in session? (C0648) \u2022 Item 16. During the 2015-16 school year, did your school or school district have any formalized policies or written documents (e.g., Memorandum of Use, Memorandum of Agreement) that outlined the roles, responsibilities, and expectations of sworn law enforcement officers (including School Resource Officers) at school? (C0650) \u2022 Item 17. Did these formalized policies or written documents include language defining the role of sworn law enforcement officers ( \u2022 Item 20. During the 2015-16 school year, were the following mental health services available to students under the official responsibilities of a licensed mental health professional? o Item 20a_1. Diagnostic assessment for mental health disorders at school by a mental health professional employed by the school or district (C0662) o Item 20a_2. Diagnostic assessment for mental health disorders at school by a mental health professional other than a school or district employee, funded by the school or district (C0664) o Item 20a_3. Diagnostic assessment for mental health disorders outside of school by a mental health professional other than a school or district employee, funded by the school or district (C0666) o Item 20b_1. Treatment for mental health disorders at school by a mental health professional employed by the school or district (C0668) o Item 20b_2. Treatment for mental health disorders at school by a mental health professional other than a school or district employee, funded by the school or district (C0670) o Item 20b_3. Treatment for mental health disorders outside of school by a mental health professional other than a school or district employee, funded by the school or district (C0672) \u2022 Item 21. During the 2015-16 school year, to what extent did the following factors limit your school's efforts to provide mental health services to students? o Item 21a. Inadequate access to licensed mental health professionals (C0674) o Item 21b. Inadequate funding (C0676) o Item 21c. Potential legal issues for school or district (e.g., malpractice, insufficient supervision) (C0678) o Item 21d. Lack of parental support in addressing their children's mental health disorders (C0680) o Item 21e. Lack of community support for providing mental health services to students in your school (C0682) o Item 21f. Written or unwritten policies regarding the school's requirement to pay for the diagnostic assessment or treatment of students (C0684) o Item 21g. Reluctance to label students with mental health disorders to avoid stigmatizing the child (C0686) \u2022 Item 22c. Training in school-wide discipline policies and practices related to cyberbullying (C0265) \u2022 Item 22d. Training in school-wide discipline policies and practices related to bullying other than cyberbullying (C0267) \u2022 Item 22h. Training in intervention and referral strategies for students displaying signs of mental health disorders (e.g., depression, mood disorders, ADHD) (C0271) \u2022 Item 22i. Training in recognizing physical, social, and verbal bullying behaviors (C0273) \u2022 Item 27. Please record the number of arrests that occurred at your school during the 2015-16 school year. Please include all arrests that occurred at school, regardless of whether a student or non-student was arrested. (C0688) \u2022 Item 29. To the best of your knowledge, were any of these hate crimes motivated by the offender's bias against the following characteristics The phrase \"and wear badges\" was added to this item. \u2022 Item 1d. Require metal detector checks on students every day (C0116) o The phrase \"pass through\" was removed from this item. \u2022 Item 1x. Limit access to social networking websites (e.g., Facebook, Twitter, YouTube, Instagram) from school computers (C0151) o The examples were updated to replace outdated social networking sites. \u2022 Item 2. Does your school have a written plan that describes procedures to be performed in the following scenarios? (C0155, C0158, C0162, C0166, C0170, C0169, C0173, and C0157) o The stem of this item was revised to ask only about written plans to address crisis scenarios. Information on emergency drills is now captured in item 3. \u2022 Item 2a. Active shooter (C0155) o This item was changed from \"shootings\" to \"active shooter.\" \u2022 Item 4a. Prevention curriculum, instruction, or training for students (e.g., conflict resolution, anti-bullying, dating violence prevention) (C0174) o \"Conflict resolution,\" \"anti-bullying,\" and \"dating violence prevention\" were added as examples in a parenthetical notation. \u2022 Item 4b. Behavioral or behavior modification intervention for students (including the use of positive reinforcements) (C0176) o A parenthetical notation now specifies that behavioral or behavior modification intervention for students can include positive reinforcements. \u2022 Item 4g. Student involvement in peer mediation (C0175) o SSOCS:2010 item 3g was split into two separate items. This item now separately identifies what percentage of schools use peer mediation as a form of addressing student conflict. \u2022 Item 4h. Student court to address student conduct problems or minor offenses (C0177) o SSOCS:2010 item 3g was split into two separate items. This item now separately identifies what percentage of schools use student court as a form of addressing student conflict. \u2022 Item 11. During the 2015-16 school year, did you have any sworn law enforcement officers (including School Resource Officers) present at your school at least once a week? (C0610) o This item has been modified to no longer collect data on security guards and security personnel; the revised item asks only about the presence of sworn law enforcement officers. Information on security guards/personnel is now collected separately in item 19. \u2022 Item 12. Were sworn law enforcement officers (including School Resource Officers) used at least once a week in or around your school at the following times? (C0612, C0614, C0616, and C0618) o This item has been modified to no longer collect data on security guards and security personnel; the revised item asks only about the presence of sworn law enforcement officers. Information on security guards/personnel is now collected separately in item 19. \u2022 Item 13. Did any of the sworn law enforcement officers (including School Resource Officers) at your school routinely\u2026? (C0620, C0622, C0624, and C0626) o This item has been modified to no longer collect data on security guards and security personnel; the revised item asks only about the activities of sworn law enforcement officers. Information on security guards/personnel is now collected separately in item 19. \u2022 Item 14. Did these sworn law enforcement officers (including School Resource Officers) participate in the following activities at your school? (C0628, C0630, C0632, C0634, C0636, C0638, C0640, C0642, C0644, and C0646) o This item has been modified to no longer collect data on security guards and security personnel; the revised item asks only about the activities of sworn law enforcement officers. Information on security guards/personnel is now collected separately in item 19. \u2022 Item 18. How many of the following were present in your school at least once a week? (C0236, C0238, C0240, and C0242) o This item was modified to separate information on full-time and part-time sworn law enforcement officers (including School Resource Officers) from information on other security guards or security personnel; this item asks only about the number of sworn law enforcement officers present at school. Information on security guards/personnel is now collected separately in item 19. \u2022 Item 19. Aside from School Resource Officers or other sworn law enforcement officers, how many additional security guards or security personnel were present in your school at least once a week? (C0232 and C0234) o This item was modified to separate information on full-time and part-time security guards or security personnel from information on sworn law enforcement officers (including School Resource Officers); this item asks only about the number of security guards/personnel present at school. Information on sworn law enforcement officers is now collected separately in item 18. \u2022 Item 26b. Sexual assault other than rape (include threatened rape) (C0314 and C0316) o An editorial change was made to update \"sexual battery\" to \"sexual assault.\" \u2022 Item 28. During the 2015-16 school year, how many hate crimes occurred at your school? (C0690) o This item was modified to ask only about the number of hate crimes and to remove \"gang-related crimes\" and \"gang-related hate crimes.\" \u2022 Item 32d. Student harassment of other students based on sexual orientation (C0381) o SSOCS:2010 item 20d was split into two separate items. This item now separately identifies harassment based on sexual orientation from harassment based on gender identity. \u2022 Item 32e. Student harassment of other students based on gender identity (C0383) o SSOCS:2010 item 20d was split into two separate items. This item now separately identifies harassment based on gender identity from harassment based on sexual orientation. \u2022 Item 33. To the best of your knowledge, thinking about problems that can occur anywhere (both at your school and away from school), how often do the following occur? (C0389, C0391, and C0393) o The definition for cyberbullying was removed from the stem of this item and relocated to the definitions page, since multiple survey items now include this term."}, {"section_title": "SSOCS:2010 items not included in SSOCS:2016", "text": "\u2022 SSOCS:2010 Item 1k. Require drug testing for any other students (C0132) \u2022 SSOCS:2010 Item 2g. The U.S. national threat level is changed to Red (Severe Risk of Terrorist Attack) by the Department of Homeland Security (C0171) \u2022 SSOCS:2010 Item 20i. Cult or extremist group activities (C0388)"}, {"section_title": "Survey Topics", "text": ""}, {"section_title": "School Practices and Programs", "text": "The first section of the SSOCS:2016 instrument, \"School Practices and Programs,\" addresses current school practices and programs that may relate to crime and discipline. Respondents are asked about numerous practices through which schools attempt to prevent and reduce violence as well as whether procedures are in place in the event of a myriad of potential on-campus crises. The section also asks about various violence prevention programs, student groups to promote inclusion, and the presence of a threat assessment team to identify students who might be a potential risk for violent behavior. These items present a foundation from which policymakers and researchers can begin to understand environments in which crime occurs.\nDuring the 2015-16 school year, was it a practice of your school to do the following? 1. If your school changed its practices during the school year, please answer regarding your most recent practice."}, {"section_title": "Parent and Community Involvement at School", "text": "The second section, \"Parent and Community Involvement at School,\" collects information about schools' efforts to involve parents in maintaining school discipline and in responding to students' problem behaviors. In addition, this section addresses the level of parent or guardian participation in school-related activities and whether community groups and related organizations-including juvenile justice agencies, social service agencies, and religious organizations-are involved in schools' efforts to promote safe schools."}, {"section_title": "School Security Staff", "text": "The third section, \"School Security Staff,\" collects information focusing on the presence and roles of sworn law enforcement officers (including School Resource Officers) in schools. The questions in this section collect data that can be used to examine the relationship between the presence of these officers and reports of school crime. Respondents are asked whether sworn law enforcement officers were present at various times throughout the school day and after school hours, whether they were armed, and whether they participated in various activities, such as mentoring students or training teachers in school safety. This section also asks whether schools have formalized policies or written documents that govern the actions of these sworn law enforcement officers and, if so, what topics these documents cover. Respondents are asked to report the number of full-time and part-time sworn law enforcement officers as well as the number of full-time and part-time additional security personnel who are not sworn law enforcement officers.\nDuring the 2015-16 school year, did you have any sworn law enforcement officers (including School Resource Officers) present at your school* at least once a week?"}, {"section_title": "School Mental Health Services", "text": "The fourth section, \"School Mental Health Services,\" asks respondents about mental health services, funded by the school or district, that are available to students who attend their school. Specifically, respondents are asked about both diagnostic assessment and treatment services for mental health disorders, whether these services are available to students at school or away from school and if they are provided by mental health professionals employed by the school or school district. Respondents are also asked for their perceptions of the factors that might limit their school's efforts to provide mental health services to students.\n"}, {"section_title": "Staff Training", "text": "The fifth section, \"Staff Training,\" asks respondents about training provided by the school or school district for classroom teachers or aides. Topics addressed include classroom management; schoolwide discipline policies and practices related to violence, bullying, cyberbullying, and alcohol and/or drug use; safety procedures; recognizing signs of potentially violent students, bullying behaviors, and illegal substance abuse; and intervention strategies for students suspected of having mental health disorders. This section also asks respondents about training for positive behavioral intervention strategies and training in crisis prevention and intervention."}, {"section_title": "Limitations on Crime Prevention", "text": "The sixth section, \"Limitations on Crime Prevention,\" asks respondents whether their efforts to reduce or prevent crime have been constrained by any factors related to teachers, parents, students, or administrative policies. Such limitations include inadequate teacher training or lack of teacher support for school policies; the likelihood of complaints from parents; fear of student retaliation; and federal, state, or district policies on discipline and safety.\nTo what extent do the following factors limit your school's efforts to reduce or prevent crime?"}, {"section_title": "Frequency of Crime and Violence at School", "text": "The seventh section, \"Frequency of Crime and Violence at School,\" focuses on the incidence of homicides and shootings that occur at school. Fortunately, incidents of this type are rare; therefore, estimates based on these measures are not always reported in SSOCS publications.\nDuring the 2015-16 school year, have any of your school's students, faculty, or staff died as a result of a homicide committed at your school*? 24."}, {"section_title": "Number of Incidents", "text": "The eighth section, \"Number of Incidents,\" asks respondents to report counts of a variety of recorded incidents at their schools. It is important to note that this section refers to specific incidents, not the number of victims or offenders, and respondents are asked to include recorded incidents committed by both students and nonstudents. In addition to being asked to report the number of recorded incidents, respondents are asked to report the number of recorded incidents reported to the police. The incidents in this section include rape; sexual assault; robbery (with or without a weapon); physical attack and threats of physical attack (with or without a weapon); theft; possession of various weapons; distribution, possession, or use of alcohol or illegal drugs; inappropriate distribution, possession, or use of prescription drugs; and vandalism. Separate questions ask about the number of arrests and the number of unplanned disruptions, such as death or bomb threats. Respondents are also asked to report the number of hate crimes that occurred at school as well as their perception of the biases that motivated these crimes.\n"}, {"section_title": "Disciplinary Problems and Actions", "text": "The ninth section, \"Disciplinary Problems and Actions,\" asks about the degree to which schools face disciplinary problems, as well as, what actions they take in response to some offenses. School administrators are asked about the use of various disciplinary actions, such as removals from school, transfers, and out-of-school suspensions, and whether the actions were used during the 2015-16 school year. Since research has shown that a school's inability to control minor infractions may be indicative of a crime-prone school environment (Miller 2004), the data provided by this section will be helpful in assessing the impact of schools' control of lesser violations and will provide another measure of the disciplinary measures used in U.S. schools.\nTo the best of your knowledge, how often do the following types of problems occur at your school*?"}, {"section_title": "School Characteristics", "text": "The 10th section, \"School Characteristics: 2015-16 School Year,\" asks respondents about features of the school and characteristics of the student body. Features of the school for which data are collected include the schools' total enrollment; the number of daily classroom changes; the level of crime in the areas where students live and where the school is located; the number of student transfers after the start of the school year; average daily attendance; and the type of school (e.g., regular public, charter, magnet). To collect data on the characteristics of the student body, respondents are asked to report the percentage of students who are eligible for free or reduced-price lunch; are of limited English proficiency (LEP); are in special education; are male; are below the 15 th percentile on standardized tests; are likely to go to college after high school; and consider academic achievement to be very important."}, {"section_title": "2.", "text": ""}, {"section_title": "Sample Design and Weighting", "text": ""}, {"section_title": "Sampling Frame", "text": "The sampling frame for the 2016 School Survey on Crime and Safety (SSOCS:2016) was constructed from a modified version of the 2015-16 National Teacher and Principal Survey (NTPS) Universe File. The NTPS Universe File was created from the 2013-14 Common Core of Data (CCD) Public Elementary/Secondary School Universe File. The CCD is an annual NCES collection of fiscal and nonfiscal data on all public schools, public school districts, and state education agencies in the United States. CCD data are supplied by state education agency officials and include information that describes schools and school districts, including -contact information for the school (i.e., location address, phone number, website address) -school characteristics (i.e., grades offered, school type, locale) -student characteristics (i.e., counts of students by race/ethnicity, free or reduced-price lunch) Certain types of schools are excluded from the NTPS Universe File in order to create the SSOCS sampling frame, including -schools in the U.S. outlying areas 6 and Puerto Rico -Department of Defense schools -recently closed schools -Bureau of Indian Education schools -special education schools -vocational schools -alternative schools -virtual schools -ungraded schools -schools with a highest grade of kindergarten or lower Regular schools, charter schools, and schools that have partial or total magnet programs are included in the frame. The size of the SSOCS:2016 universe was approximately 84,000 schools."}, {"section_title": "Sample Design", "text": "The same general sample design previously used for SSOCS:2000, SSOCS:2004, SSOCS:2006, SSOCS:2008, and SSOCS:2010 was adopted for the selection of schools in SSOCS:2016. As in the prior collections, the objective of the SSOCS:2016 sample design was twofold: to obtain overall cross-sectional and subgroup estimates of important indicators of school crime and safety and to develop precise estimates of change in various characteristics relating to crime between SSOCS administrations. To attain these objectives, a stratified sample of 3,553 regular public schools was drawn for SSOCS:2016. For sample allocation and sample selection purposes, strata were defined by crossing school level, locale, and enrollment size (more information provided in section 2.4). These three explicit stratification variables have been shown to be related to school crime (Chen and Weikart 2008;Langbein and Bess 2002;Miller 2004). In addition, region; the percentage of White, non-Hispanic enrollment; state; and school district were used as implicit stratification variables by sorting schools on these variables within each stratum before selecting the sample."}, {"section_title": "Sample Size", "text": "The initial goal of SSOCS:2016 was to collect data from at least 2,550 schools. One possible method of allocating schools to the different sampling strata would have been to allocate them proportionally to the U.S. public school population. However, while the majority of U.S. public schools are primary schools, the majority of school violence is reported in middle and high schools. Therefore, a larger proportion of the desired completed interviews of 2,550 schools was allocated to middle and high schools. The desired number of completed interviews was allocated to the four school levels 7 as follows: 640 primary schools, 895 middle schools, 915 high schools, and 100 combined schools. The resulting sample allocation, described in section 2.4, by school level was: 849 primary schools, 1,230 middle schools, 1,347 high schools, and 127 combined schools. The total sample size was 3,553 schools. Schools in SSOCS:2000, SSOCS:2004, SSOCS:2006, SSOCS:2008, and SSOCS:2010 were allocated to instructional levels in a similar manner."}, {"section_title": "Stratification, Sample Selection, and Final Sample", "text": "\"Stratification\" refers to the process of subdividing, or grouping, the frame into mutually exclusive subsets called strata, from which samples are selected. Stratification has two main goals: (1) to ensure that selected subgroups of interest are adequately represented in the sample for analysis purposes; and (2) to improve sampling precision by permitting a more optimal allocation of the sample to the strata. For a fixed sample size, the optimum allocation (i.e., the allocation that produces the smallest sampling error) is a function of the number of schools in the stratum and the underlying within-stratum variance of the statistic of interest. As indicated earlier, the same variables and categories used in SSOCS:2000, SSOCS:2004, SSOCS:2006, SSOCS:2008, and SSOCS:2010 were used to stratify the SSOCS:2016 population of schools, namely, school level, locale, and enrollment size. SSOCS:2008, SSOCS:2010, and SSOCS:2016 differed from previous administrations of SSOCS in that the definition of locale was derived from the 12-level place-based code currently assigned in the CCD rather than the 8level metro-based code obtained from the CCD in previous administrations. Within each school level, the sample of schools was allocated among 16 strata formed by the cross-classification of enrollment size 8 and locale. 9 This allocation was proportional to the sum of the square roots of the total student enrollment of each school in that stratum. The sum of the square roots was used as the \"measure of size\" (MOS) in order to obtain a reasonable sample of lower enrollment schools while at the same time giving a higher probability of selection to higher enrollment schools. The MOS was calculated by first finding the square root of each school's enrollment and then aggregating over the schools in the stratum. The formula is given as where E hi is the enrollment of school i in stratum h and N h is the total number of schools in stratum h. The total measure of size for an instructional level-MOSTOT -was found by summing the MOSh values for the 16 strata at that instructional level. The ratio MOSh / MOSTOT determined the number of schools allocated to that stratum. For example, the MOS for the stratum of suburban primary schools with 500-999 students was 221,228, and the total across all 16 strata within the primary school level was 1,047,133. The ratio of this stratum to the overall school level is 221,228/1,047,133 = 0.21127. Therefore, roughly 21.1 percent of the desired 640 primary school interviews were allocated to this stratum (specifically, 640 x 0.21127 = 135.21), or 135 schools. The effective sample sizes (completed interviews) for each of the strata were then inflated to account for nonresponse by dividing the target stratum sample size by the expected stratum response rate; this inflated count was the sample size for the stratum. For example, the effective sample size for suburban primary schools with 500-999 students was calculated above as 135 schools. Based on prior experience, 10 the response rate for this stratum was expected to be 77.3 percent, so the number of schools to be sampled from this stratum was increased to 175 (135/0.773 = 174.64). Sample sizes were inflated by an additional 1.5 percent to account for out-of-scope schools, for a total of 178 schools in this stratum. Once the final sample sizes were determined for each of the 64 strata, 11 the schools within each stratum were sorted by the percentage of White, non-Hispanic enrollment, 12 region, 13 state, and school district (which has a similar effect as stratification), and a sample of 3,553 schools was selected using a systematic design, with a constant sampling rate in each stratum. Within each stratum, a systematic simple random sample was drawn. The sampling interval k was calculated as the ratio of the number of schools in the frame to the nonresponse-adjusted sample size. A random start r was selected between 0 and k for the stratum, and schools r, r + k, r + 2k, r + 3k, 10 Typically, the previous administration's response rates were used to inflate the initial sample size in each stratum, but the SSOCS:2010 response rates were unusually high. As a result, for SSOCS:2016, the averages of the response rates from SSOCS:2008 and SSOCS:2010 were used to inflate the sample sizes. In strata where the average response rate was higher than the SSOCS:2010 rate, the SSOCS:2010 rate was used to inflate the initial sample. In addition, to account for schools that might have been sampled for the 2015-16 National Teacher and Principal Survey (NTPS) and SSOCS:2016 in the same year, sample sizes for high schools and large schools (i.e., with more than 1,000 students) were inflated by 2.59 and 2.60 percent, respectively. These inflation factors were based on an analysis of response rates for schools that were sampled in both the 2007-08 Schools and Staffing Survey (SASS), the predecessor to the NTPS, and SSOCS:2008. The results from this analysis found that the response rates for high schools and large schools decreased by 2.59 and 2.60 percent, respectively, for schools that were sampled in both surveys. For other school levels and enrollment sizes, there was no decrease in response rate due to being sampled in both SASS and SSOCS in the same year, so an inflation factor is not used. 11 The 64 strata are formed by the cross-classification of enrollment size and locale in each of the four school-level categories. 12 \"Percent White enrollment\" refers to the variable PERCWHT, which represents the percentage of White, non-Hispanic students enrolled in the school. For the remainder of this report, this variable is referred to as \"percent White enrollment.\" 13 \"Region\" refers to the variable CENREGN, which represents Census regions. For the remainder of this report, this variable is referred to as \"region.\" The four categories are Northeast, Midwest, South, and West. etc., were selected (rounding up to the nearest whole number). Continuing the example of suburban primary schools with 500-999 students, there were 8,618 schools of this type in the frame. Because 178 schools were needed from this stratum, the sampling interval k was 48.42 (8,618/178 = 48.42). A random start was then chosen between 0 and 48.42 to select the first school, and 48.42 was successively added to the random start to select each of the remaining 177 schools in the sample (rounding up each time to get the number of the school in the sorted list). Table 1 shows the characteristics of the initial selected sample of 3,553 schools (which yielded 2,092 responding schools, 1,442 nonresponding schools, and 19 ineligible schools). Some categories of schools were more likely than others to respond; in particular, lower enrollment schools, schools in rural areas, and schools with a high percentage of White student enrollment were more likely to respond (see appendix tables M-2, M-5, and M-6 for statistical comparisons of response rates by school characteristics; respondents and nonrespondents; and odds ratios, by school characteristics, respectively). For a survey to be considered complete in SSOCS:2016, answers were required for at least 162 of the 296 total subitems eligible for recontact (i.e., all subitems in the questionnaire except those associated with the introductory items). Of the 296 total subitems, 92 were categorized as critical and respondents were required to provide answers for at least 75. Responses provided to the critical subitems counted toward the total 162 subitem responses needed for a survey to be considered complete. Items 26 and 35 (whose subitems were all categorized as critical) had additional completion criteria; respondents had to provide responses for at least 18 of the 30 subitems within item 26 and at least 6 of the 25 subitems within item 35. Questionnaires that did not meet established completion criteria were considered incomplete and are excluded from the SSOCS:2016 data file. 2 Nonrespondents include schools whose districts denied permission to NCES and those eligible schools that either did not respond or that responded but did not answer the minimum number of items required for the survey to be considered complete. 3 Ineligible schools include those that had closed, merged with another school at a new location, changed from a regular public school to an alternative school, or are not a school: \"not a school\" generally refers to a school record for an organization that does not provide any classroom instruction (e.g., an office overseeing a certain type of program or offering only tutoring services). 4 The unweighted response rate is calculated as the following ratio: completed cases / (total sample -known ineligibles). 5 The weighted response rate is calculated by applying the inverse of the probability of selection (including the sampling adjustment factor) to the calculation of the unweighted response rate. 6 Primary schools are defined as schools in which the lowest grade is not higher than grade 3 and the highest grade is not higher than grade 8. Middle schools are defined as schools in which the lowest grade is not lower than grade 4 and the highest grade is not higher than grade 9. High schools are defined as schools in which the lowest grade is not lower than grade 9 and the highest grade is not higher than grade 12. Combined schools include all other combinations of grades, including K-12 schools. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2015-16 School Survey on Crime and Safety (SSOCS:2016)."}, {"section_title": "2.5", "text": ""}, {"section_title": "Weighting", "text": "Sample weights allow inferences to be made about the population from which the sample units were drawn. Due to the complex nature of the SSOCS:2016 sample design, weights are necessary to obtain population-based estimates, to minimize bias arising from differences between responding and nonresponding schools, and to calibrate the data to known population characteristics in a way that reduces sampling error. The procedures used to create the SSOCS:2016 sampling weights are described below. Each school was assigned an initial (base) weight equal to the ratio of the number of schools available in the sampling frame in the school's stratum to the number of schools sampled from the school's stratum. 14 In other words, a school's base weight was equal to the inverse of the sampling rate within its stratum. Due to nonresponse, the responding schools did not necessarily constitute a random sample from the schools in the stratum. In order to reduce the potential bias due to nonresponse, weighting classes were determined by using the statistical algorithm CHAID (chi-square automatic interaction detection) to partition the sample such that schools within a weighting class were homogeneous with respect to their probability of responding. The CHAID analysis identified the following variables as being predictive of response -school locale -number of full-time-equivalent (FTE) teachers -school level -Census region -percent White, non-Hispanic enrollment -school enrollment size -student-to-teacher FTE staff ratio -percentage of students eligible for free or reduced-price lunch When the number of responding schools in a weighting class was below a minimum threshold, the class was combined with another to avoid the possibility of disproportionately large weights. Variables that are predictive of response are likely to be sources of nonresponse bias. These variables were therefore used to define the weighting adjustment cells.The base weights were adjusted so that the weighted distribution of the responding schools was similar to the initial distribution of the total sample based on the predictor variables listed above. This was implemented by multiplying the base weight by the inverse of the weighted response rate within the adjustment cell. The nonresponse-adjusted weights were then poststratified to calibrate the sample to the known population totals from the initial sampling frame. A pair of two-dimensional margins were set up for the poststratification: (1) school level and school enrollment size, and (2) school level and locale. An iterative process known as a raking ratio adjustment brought the sum of the weights into agreement with known control totals. Poststratification works well when the population not covered by the survey is similar to the covered population within each poststratum. Thus, for poststratification to be effective, the variables that define the poststrata must be correlated with the variables of interest, they must be well measured in the survey, and control totals must be available for the population as a whole. All three requirements were satisfied by the aforementioned poststratification margins. 15 The final analysis weight on the data file is named FINALWGT. Characteristics of FINALWGT are presented in table 2 below. The file also includes 50 replicate weights (REPFWT1 through REPFWT50) for use in variance estimate. For information on how to apply the weights in statistical analysis, refer to chapter 6. 3."}, {"section_title": "Data Collection Methods and Response Rates", "text": "Chapter 3 begins with an examination of the data collection activities that were conducted on behalf of SSOCS:2016. Other topics examined are interviewer training, data retrieval, efforts to increase response rates, and unit and item response rates and nonresponse bias analyses."}, {"section_title": "Data Collection Activities", "text": "SSOCS:2016 was conducted as a mail survey with telephone follow-up. A detailed list and schedule of the SSOCS:2016 data collection activities can be found in table 3 and are described below. Data collection activities began about 4 months prior to the initial mailout of the questionnaire, when the Census Bureau began working with the school districts of sampled schools that required district approval (also known as \"Special Districts\") to participate in the survey. 16 Approximately 1 week prior to the initial questionnaire mailout, an advance letter was sent to the principals of sampled schools, along with a brochure providing additional information about the survey. Letters were also mailed to chief state school officers (CSSOs) and district superintendents prior to the initial questionnaire mailout to inform them that schools within their states and districts, respectively, had been selected for SSOCS:2016 (see appendixes G and H for a copy of the CSSO and district superintendent cover letters, respectively). The letters were not designed to ask for permission for the schools' participation in the survey, but rather as a vehicle to enhance participation. Questionnaires were sent via FedEx 17 directly to the principals of the sampled schools along with a cover letter describing the importance of the survey, a promotional SSOCS pen, and a preaddressed, postage-paid return envelope. Schools located within Special Districts in which approval was granted also received inserts informing the principals that their districts had approved their participation in SSOCS. Please see appendixes E and F for a copy of the advance and cover letters, respectively, sent to principals and appendix A for a copy of the questionnaire. The reminder telephone operation, which was composed of two phases, began three weeks after the initial mailout. Phase 1 consisted of a follow-up call with the principal or school contact to determine the status of the questionnaire. In phase 2, which began approximately 2 weeks after the close of phase 1 reminder operations, a follow-up call to principals or school contacts was repeated for schools that had still not returned a questionnaire. The 2-week break between the two phases of the reminder operation was to allow time to send replacement questionnaires to schools that did not receive them or had misplaced them and to give principals time to complete and return the questionnaire. During the reminder operation, the interviewer could complete the SSOCS questionnaire over the phone at the respondent's request. Questionnaires were resent via FedEx to schools that had not received them or that had not been reached in either reminder operation. The nonresponse follow-up operation began a little over 2 weeks after the reminder operations ended. During this 4-week operation, interviewers collected data over the telephone and by fax submission. Follow-up activities, in which the U.S. Census Bureau contacted respondents in order to complete the remaining questionnaires, ended on June 15, 2016. There were 41 requests for replacement questionnaires during the nonresponse follow-up operation. Replacement questionnaires were sent via FedEx on a flow basis."}, {"section_title": "Activity", "text": ""}, {"section_title": "Description Date E-mail reminder", "text": "Sampled schools that had not returned a completed questionnaire were contacted by e-mail to encourage them to complete the questionnaire as soon as possible."}, {"section_title": "June 13, 2016", "text": "Data retrieval operation For cases in which critical subitems were left blank or responses were illogical, respondents were contacted to resolve issues related to the missing data. May 5-June 15, 2016 Keyed data The last day that keyed data were accepted July 5, 2016"}, {"section_title": "Interviewer Training", "text": "Interviewers working on SSOCS:2016 were employees of the U.S. Census Bureau's Jeffersonville Contact Center in Jeffersonville, Indiana. All interviewers were required to receive 10 hours of computer-assisted telephone interviewing (CATI) training-on topics such as what makes a good interviewer, how to interview, voice, and diction-before attending surveyspecific training sessions. Interviewer training on the content and data collection procedures of SSOCS:2016 was conducted from February through May of 2016. Details on the required survey-specific trainings, including the dates and number of participants, are provided below."}, {"section_title": "Training on Basic Interviewer Skills", "text": "A 1-hour self-study training was conducted for 50 interviewers prior to the start of incoming calls on February 16, 2016. Interviewers were given an Interviewer Self Study Guide to read at the beginning of the training session. The self-study guide covered all of the information necessary to be successful in making and answering phone calls to and from schools. The guide described the purpose, design, and sample size of the survey and provided an overview of all of the telephone operations. It described the challenges the interviewers might face when collecting data from schools and offered advice on how to work with the office staff. It also explained to interviewers how to encourage participants and how to document the outcome of each phone call. See appendix I for a copy of the Interviewer Self Study Guide."}, {"section_title": "Training on Questionnaire Follow-up", "text": "A 5-hour classroom training session for 50 interiewers was conducted on March 8-9, 2016, for the reminder phase 1 follow-up operations. The session included a review of the calling procedures, the frequently asked questions, and the forms relevant for the operation. A large portion of the training session was devoted to completing paired practices using the relevant forms. During the paired practices, interviewers alternated the role of interviewer and respondent in order to become proficient with the paper script and the SSOCS questionnaire. The paper script provided the interviewers with the wording to use to introduce themselves, ask for the appropriate staff member, and inquire about the status of the SSOCS questionnaire. A 2-hour self study training was conducted for 50 interviewers on April 16, 2016, for the reminder phase 2 follow-up operation, and a 4-hour classroom training session was conducted for 30 interviewers on May 2, 2016, for the non-response follow-up operation. Interviewers were given a Reminder and Non-Response Follow-Up Operation Interviewer Self Study Guide prior to phase 2 of the reminder operation and the nonresponse follow-up operation as well as brief training memos that highlighted key points of the specific operation about to be conducted. See appendix J for a copy of this guide."}, {"section_title": "Training on Refusal Conversion", "text": "All interviewers working on SSOCS:2016 were trained in both refusal aversion and conversion. The training distinguished between aversion and conversion and described keys to success, including strong communication skills, project knowledge, knowledge of the case history, and the ability to think on one's feet. Interviewers were instructed to respond to the issues the respondent raised, to remember that the respondent is always right, and to know when the interview is over. They were urged to be persuasive as well as calm and understanding, to probe for the reason the respondent was refusing, to be prepared to listen, and to use active listening techniques. They were also asked to vary their tone of voice, to use the resources available to them (e.g., frequently asked questions), and to leave good comments for the next interviewer working on the case. First-refusal cases were referred to experienced interviewers for a refusal conversion attempt."}, {"section_title": "Training on Data Retrieval", "text": "Training on data retrieval was conducted on May 2, 2016. This 5-hour training session, which was attended by 30 interviewers, was similar to the training for the other telephone operations in that it included a self-study guide and paired practice exercises. However, the data retrieval training included more time for paired practice than the other training sessions due to the complex nature of the task. The data retrieval form included a list of items for follow-up, and their respective page numbers, ordered by importance to the survey so that the most critical items were completed first in case the respondent could not complete the interview. Since one of the criteria for flagging an item was the ratio of an item's value to the school's enrollment, some items flagged for follow-up due to extreme values would no longer require follow-up if the new enrollment value caused the ratio to fall within an acceptable range. The following instruction was included for these cases: \"If the new enrollment exceeds 1000 then do not ask items from q26 and q35 that are range violations.\" Items that were range violations had the term \"range violation\" in parentheses next to the page and item number. See appendix K for a copy of the Failed Edit Follow-Up Operation Interviewer Self Study Guide."}, {"section_title": "Data Retrieval", "text": "The data were passed through an initial editing program that searched for inconsistencies in the data; blanked or flagged inconsistencies, where necessary; and imputed blank items based on responses to other items in the questionnaire. Next, a program was used to assess whether a questionnaire could be considered complete. To reduce unit nonresponse, if a returned survey did not meet the minimum completion criteria, the school was recontacted for data retrieval. A school was recontacted if any of the following criteria were met: \u2022 three or more rapes were reported in subitem 26a; \u2022 less than 55 percent of the total subitems eligible for recontact were filled in (at least 162 of the 296 total subitems needed to be complete); \u2022 less than 60 percent of question 26 subitems were filled in (at least 18 of the 30 subitems needed to be complete); \u2022 less than 24 percent of question 35 subitems for columns 1 through 5 were filled in (at least 6 of the 25 subitems needed to be complete); \u2022 less than 80 percent of the critical subitems were filled in (at least 75 of the 92 critical subitems needed to be complete); or \u2022 there were five or more soft-range violations. The critical items in SSOCS:2016 were questions 11, 12, 20, 24, 25, 26, 28, 32, 35, 36, 37, 38, 39, 43, 44, and 45. Soft-range violations occurred if an answer was unusually high or low, given the school's enrollment. In the 2015-16 SSOCS, 379 partially complete questionnaires were received by mail, of which 362 were successfully resolved and 17 did not meet the criteria to be considered a completed interview. An additional 12 cases that were finished over the telephone with survey respondents did not meet the criteria for a completed interview. Telephone interviews were not eligible for data retrieval because an interviewer had already attempted to complete the questionnaire with the respondent."}, {"section_title": "Efforts to Increase Response Rates", "text": "Several steps were taken to maximize survey response rates during data collection. All questionnaires were sent via FedEx (with the exception of cases where a physical address was not available, in which case USPS was used) to ensure their prompt receipt and to give the survey a greater sense of importance to the respondents. A preaddressed, postage-paid reply envelope was included in the mailing for respondents to use when returning their completed questionnaire. In addition, a toll-free number and an e-mail address were provided for respondents to contact with inquiries regarding the survey. Multiple follow-up contacts were made via telephone and e-mail throughout the data collection period to encourage and promote participation, as were targeted reminder mailings. Between scheduled mailouts, interviewers called nonrespondents to ensure that the questionnaire had been received and to follow up on its status. The questionnaire was resent via FedEx to schools indicating they had not received it and needed a new questionnaire and to schools that had not yet responded and were not reached during the reminder operations. After several rounds of telephone reminders to complete the questionnaire, interviewers contacted nonrespondents by telephone to attempt to complete the questionnaire over the phone or via fax submission. Several unique e-mail messages from the NCES project director were used as prompts and reminders (see appendix L for a copy of the reminder e-mails). The first e-mail message, sent to school principals on February 22, 2016, was used to alert them that the SSOCS questionnaire would be delivered within the next week. Several reminder e-mails containing statistics from the prior SSOCS collection were sent to school principals and other appropriate school staff members throughout the collection period. The advance mailing included a brochure that provided details about the issues addressed in the study, the importance of the data, and information regarding the SSOCS website. The initial questionnaire mailout to schools also contained informational materials about SSOCS and a promotional SSOCS pen. All correspondence to schools was personalized with the principal's name if it was available on the school's or district's website. Refusal conversion efforts were used to obtain responses from schools that had initially declined to complete the questionnaire. Refusals coded by interviewers as \"firm\" were reviewed by supervisors to determine whether another attempt should be made. A case was coded as a final refusal if interviewers received two refusals from any school contact (e.g., a secretary or assistant principal) during the reminder and nonresponse follow-up operations. If a school district refused, schools within that district were coded as final refusals as well."}, {"section_title": "Unit Response Rate", "text": "A unit response rate is, at its most basic level, the ratio of surveys completed by eligible respondents to the total count of eligible respondents respondents using the base weights (i.e., prior to nonresponse adjustments). Unit response rates are traditionally reported because they reflect the potential effects of nonsampling error and indicate whether portions of the population are underrepresented due to nonresponse. In order to calculate any of these measures, it is first necessary to know the disposition (outcome) of each sampled case. In some surveys, this calculation can be rather complicated because it is difficult to distinguish eligible and ineligible units. For school surveys, however, the U.S. Department of Education updates its list of known schools on a fairly regular basis, so estimating eligibility among sampled cases is relatively straightforward. SSOCS:2016 used three measures to evaluate response: the completion rate, the unweighted unit response rate, and the overall weighted 18 unit response rate. Table 4 shows the dispositions of the 3,553 cases selected for participation in SSOCS:2016. Table 4. Number of public schools, by interview status: SSOCS:2016   Interview status  Number of public schools   Total sample  3,553   Schools whose districts refused on their behalf  111  Completed survey returned 1  2,092   Partially completed survey returned  36  Ineligible schools 2  19 Other nonresponding schools 1,295 1 For a survey to be considered complete in SSOCS:2016, answers were required for at least 162 of the 296 total subitems eligible for recontact (i.e., all subitems in the questionnaire except those associated with the introductory items). Of the 296 total subitems, 92 were categorized as critical and respondents were required to provide answers for at least 75. Responses provided to the critical subitems counted toward the total 162 subitem responses needed for a survey to be considered complete. Items 26 and 35 (whose subitems were all categorized as critical) had additional completion criteria; respondents had to provide responses for at least 18 of the 30 subitems within item 26 and at least 6 of the 25 subitems within item 35. Questionnaires that did not meet established completion criteria were considered incomplete and are excluded from the SSOCS:2016 data file. 2 Ineligible schools include those that had closed, merged with another school at a new location, changed from a regular public school to an alternative school, or are not a school: \"not a school\" generally refers to a school record for an organization that does not provide any classroom instruction (e.g., an office overseeing a certain type of program or offering only tutoring services). SOURCE: U.S. Department of Education, National Center for Education Statistics, 2015-16 School Survey on Crime and Safety (SSOCS:2016). The completion rate is defined as the number of completed surveys (C) divided by the total sample size (T): While this figure represents the quality of the SSOCS:2016 data collection operations, it does not necessarily represent the quality of the data. To determine this, all schools selected for the study must be considered. A conservative measure, the unweighted response rate, divides the number of completed surveys (C) by the total initial sample size (T), subtracting known ineligible schools from the denominator (I). For SSOCS:2016, this calculation yields an unweighted unit response rate of C / (T -I) = 2,092 / (3,553 -19) = 59.2 percent. While unweighted unit response rates generally measure the proportion of the sample that produced usable information for analysis, weighted unit response rates can be used to estimate the proportion of the survey population covered by the units that responded. These two rates can differ if certain subpopulations are sampled with different selection probabilities, such as in SSOCS:2016. The weighted unit response rate is calculated by applying the base sampling weights and substituting the result in the equation above. For SSOCS:2016, the weighted response rate was calculated by dividing the weighted number of completed surveys (Cw) by the weighted total initial sample size (Tw), subtracting the weighted number of known ineligible schools from the denominator (Iw). Cw / (Tw -Iw) = 52,639.64 / (84,452 -829.9742) = 62.9 percent. Weighted and unweighted unit response rates by selected school characteristics are shown in table 1 in chapter 2. 19 The overall weighted unit response rate was 62.9 percent."}, {"section_title": "Analysis of Unit Nonresponse Bias", "text": "The existence of nonresponding schools has the potential to introduce bias into survey estimates, depending on the magnitude of the nonresponse and whether differences exist between responding and nonresponding schools in characteristics related to the estimates of interest. Because NCES Statistical Standard 4-4 requires analysis of nonresponse bias for any survey stage with a base-weighted unit response rate less than 85 percent, a nonresponse bias analysis was conducted to evaluate the extent of this bias in SSOCS:2016, since the base-weighted unit response rate was 62.9 percent (U.S. Department of Education 2012). The unit nonresponse bias analysis compared the sample and target population, respondents and nonrespondents, and relative response probability across frame variable categories to identify potential sources of bias. The frame variables used in the unit nonresponse bias analysis were school locale; number of FTE teachers; school level; region; percentage of White, non-Hispanic enrollment; enrollment size; student-to-teacher ratio; and percentage of students eligible for free or reduced-price lunch. These variables are available for all U.S. public schools from the CCD and thus were known for all schools sampled for SSOCS:2016, regardless of whether they responded. For such characteristics, bias can be measured directly. Based on these characteristics, the analysis found that there were significant differences between responding and nonresponding schools. For example, schools with an enrollment of 1,000 students or more, city schools, and schools in which less than 50 percent of students are White, non-Hispanic were significantly underrepresented among respondents, relative to their share of the target population. To provide a fuller picture of the risk of bias in key estimates, correlations between the frame characteristics and survey variables were analyzed, and key estimates were compared between the lowest propensity respondents (i.e. schools with characteristics resembling those of nonrespondents) and other respondents. The frame characteristics (which are known for both respondents and nonrespondents) were found to be correlated with a number of survey variables (which are known only for respondents). This implies that the observed bias in frame characteristics, if not adjusted for, would likely lead to bias in key SSOCS:2016 estimates. A CHAID analysis was conducted to inform the selection of weighting classes to be used to produce nonresponse-adjusted weights. Based on the CHAID analysis, the base weights were adjusted for potential nonresponse bias in school level; locale; enrollment size; percentage of White, non-Hispanic enrollment; region; percentage of students eligible for free or reduced-price lunch; pupil-teacher ratio; and number of FTE teaching staff. After the nonresponse-adjusted weights were applied, no significant bias remained in any of these characteristics. Because these characteristics are known to be correlated with survey variables, it suggests that the weighting adjustments incorporated into the SSOCS:2016 weights help to mitigate nonresponse bias in key estimates. However, some estimates may be subject to nonresponse bias that is not related to the observable characteristics used to create nonresponse-adjusted weights. This type of bias would not be removed by weighting adjustments. Therefore, data users are cautioned that, because survey variables are not observed for nonrespondents, the exact amount of nonresponse bias remaining in key estimates cannot be known with certainty and is likely to vary between estimates. See appendix M for detailed information on the SSOCS:2016 unit-level nonresponse bias analysis."}, {"section_title": "Item Response Rates", "text": "Just as principals sometimes chose not to respond to the SSOCS:2016 survey request, those that did respond did not always answer all of the survey items. Unweighted item response rates are calculated by dividing the number of sampled schools responding to an item by the number of schools to which the item was applicable. Weighted item response rates are calculated in the same way, but with each school weighted by the inverse of its probability of selection. Weighted 20 item-level response rates in SSOCS:2016 were generally high, ranging from 82 to 100 percent. The mean item response rate for SSOCS:2016 was about 98 percent. Of the 273 subitems in the SSOCS questionnaire (i.e., all of the subitems except those associated with the 23 introductory items), most (248) had response rates greater than 95 percent, 23 had response rates between 85 and 95 percent, and 2 had response rates below 85 percent. The two subitems with response rates below 85 percent are \u2022 C0326-Number of recorded incidents of physical attacks or fights with a weapon (weighted response rate of 84 percent) \u2022 C0330-Number of recorded incidents of physical attacks or fights without a weapon (weighted response rate of 82 percent) A detailed list of base-weighted item response rates for SSOCS:2016 questionnaire items is available in appendix N."}, {"section_title": "Analysis of Item Nonresponse Bias", "text": "NCES Statistical Standard 4-4 requires an analysis of item nonresponse bias for any item with a base-weighted item response rate less than 85 percent. Therefore, an item-level bias analysis was performed to determine the susceptibility of subitems C0326 and C0330 to bias. The magnitude of item nonresponse bias for a particular item is determined by several factors, including the level of item response, differences between item respondents and item nonrespondents in the characteristic being measured by the item, and the distribution of item responses across categories of auxiliary variables. Two methods were used to analyze the potential for item nonresponse bias in C0326 and C0330. First, extreme \"low\" and extreme \"high\" values were imposed on nonrespondents to determine the resulting change in the estimate. For both subitems, the extreme assumptions led to significant changes in the estimated mean, suggesting that the items are susceptible to bias if there are large differences between item nonrespondents and nonrespondents. Second, an analysis was conducted to determine the extent to which schools that did not answer each item differed from schools that did answer the item. Specifically, the distributions of two survey subitems and eight sampling frame variables were compared between respondents and nonrespondents to subitems C0326 and C0330. The two survey subitems-C0560 (perceived level of crime in students' neighborhood) and C0562 (perceived level of crime in the school's neighborhood)-both had weighted item response rates above 99 percent and are likely to be correlated with responses to critical items. The eight sampling frame variables used in the analysis were school locale; number of FTE teachers; school level; region; percentage of White, non-Hispanic enrollment; enrollment size; student-to-teacher ratio; and percentage of students eligible for free or reduced-price lunch. Results of the analyses indicated that the potential for bias was not enough to warrant the exclusion of C0326 and C0330 from the data file. More detailed information on the item nonresponse analyses, including the specific comparisons that were significant in the tests outlined above, is available in appendix O. Even though these items were demonstrated to have little potential for nonresponse bias, they were omitted from the public-use file to protect schools from disclosure risks. They are available in the restricted-use file. However, several of the composite variables included in the public-use file were constructed using these variables."}, {"section_title": "Nonsampling Error", "text": "\"Nonsampling error\" is the term used to describe variations in the estimates that may be caused by population coverage limitations and data collection, processing, and reporting procedures. The sources of nonsampling errors are typically problems such as unit and item nonresponse, the differences in respondents' interpretations of the meaning of survey questions, response differences related to the particular month or time of the year when the survey was conducted, the tendency for respondents to give socially desirable responses, and mistakes in data preparation. In general, it is difficult to identify and estimate either the amount of nonsampling error or the bias caused by this error. For SSOCS, efforts were made to prevent such errors from occurring and to compensate for them, where possible. For instance, during the survey design phase, cognitive testing of the new and revised questionnaire items was conducted with public school principals or the person most knowledgeable about school crime and policies to provide a safe environment in their school. Cognitive testing provided the opportunity to check for consistency of interpretation of questions and definitions as well as to eliminate ambiguous items. The questionnaire items were also extensively reviewed by NCES, a technical review panel consisting of some of the nation's top experts on school crime, and the National Institute of Justice, a partner federal agency who contributed funding for SSOCS:2016. In addition, extensive editing of the questionnaire responses was conducted to check the data for accuracy and consistency. Cases with missing or inconsistent items were recontacted by telephone to resolve problems. Data entered for all surveys, received by mail or telephone, were extensively reviewed to identify anomalies and verify that data were entered correctly."}, {"section_title": "4.", "text": ""}, {"section_title": "Data Preparation", "text": ""}, {"section_title": "Analysis of Disclosure Risk", "text": "Central to the mission of NCES is a commitment to protecting the identity of respondents to its various data collections. Thus, the SSOCS:2016 response data have been subjected to an extensive disclosure risk analysis and modified based on the results of that analysis to prevent positive identification of individual schools. Tests on the modified data were performed to ensure that the data remain accurate and useful. The penalty for unlawful disclosure of any individually identifiable information is a fine of not more than $250,000 (under 18 U.S.C. 3559 and 3571), or imprisonment for not more than 5 years, or both."}, {"section_title": "Editing Specifications", "text": "As questionnaires were returned to Census, they were sent to data keying staff, who used a data capture program to enter the responses. As the data were captured, they were reformatted into ASCII files and sent weekly to Census Bureau analysts in Suitland, Maryland, for data review. The data were then run through a series of editing programs. As described in section 3.3, computer programs were used to determine whether a returned questionnaire could be considered complete. Additional editing programs subsequently checked the data for consistency, valid data value ranges, and skip patterns. Detailed information on editing procedures is provided in appendix P."}, {"section_title": "Range Specifications", "text": "The frequencies for all survey items were reviewed to ensure that recorded values were acceptable. For the categorical variables, these values were predetermined by precoded response options available on the questionnaire. For numeric variables, the initial data were reviewed to determine whether the ranges met hard and soft boundary criteria for acceptable responses. Ranges from the SSOCS:2010 data were used as a basis of comparison. Out-of-range responses were flagged, and the value was verified if the school was contacted again during data retrieval. If the respondent was not contacted again during data retrieval, the out-of-range value was deleted and a new value was imputed. Range checks included both soft-and hard-range edits. A soft range is one that represents the reasonable expected range of values, but does not include all possible values. For critical items, 21 responses outside the soft range were confirmed with the respondent during data retrieval phone calls. If a respondent could not be reached, or if the item was not a critical item, the response was accepted as is. Hard ranges are those that have a finite set of parameters for an item. For example, a respondent may have given a date of February 1, 2016, as the date he or she completed the questionnaire. This value is out of range because the questionnaire was not mailed to the respondent until February 22, 2016. Similarly, on questions 38 and 39, responses greater than 100 percent were not accepted. For critical items with responses outside a hard range, respondents were called so that the question could be asked again; if a respondent insisted that a response was correct, or if the respondent could not be reached, the response was not accepted. If the item was not a critical item, a response outside a hard range was not accepted."}, {"section_title": "Consistency Checks (Logic Edits)", "text": "Cross-tabulations were reviewed to check that logical relationships were maintained across items. For example, column 1 in item 26 asks for the total number of various incidents of crimes, and column 2 asks for the number of crimes reported to police. Logically, column 1 should be equal to or greater than column 2. If an illogical relationship was found between two numeric items, a response was deleted during editing and later imputed. 22 Illogical relationships can also exist between two categorical items. For example, in item 34, column 1 asks whether the school allows the use of disciplinary actions and column 2 asks whether the school has used these disciplinary actions during the school year. Logically, if column 2 was answered \"yes,\" column 1 should be answered \"yes\" as well. In this case, the data were \"backward cleaned,\" and if the column 1 response was \"no,\" it was logically edited to a \"yes\" response. A detailed list of consistency checks and rectification procedures is provided in appendix P. All inconsistencies were flagged, reviewed, and rectified."}, {"section_title": "Review and Coding of Text Items", "text": "There are two \"other -please specify\" text subitems in the SSOCS:2016 questionnaire: respondent title (C0015) and item 43(5) (other type of school, C0565). For these subitems, a respondent is asked to record an original response if the supplied response options do not capture his or her experiences. The provided responses were reviewed to determine whether they could be coded into one of the response options supplied on the questionnaire (i.e., back-coded), and those responses that could not be were reviewed to determine which were used frequently. The SSOCS:2016 questionnaire contained two items regarding the respondent's title/position: C0014 asked whether the respondent was a principal, vice-principal/disciplinarian, or \"Other,\" and C0015 allowed a text response if \"Other\" was selected. In the restricted-use file, seven new response categories were added to C0015, which became C0015_R because of this addition. C0015_R is not included in the public-use file because of concerns about disclosure risk. The public-use file contains a new recoded variable, C0014_R, which combines the most common responses for variables C0014 and C0015_R. These new responses are shown in table 5. The SSOCS:2016 questionnaire contained two items regarding school type. Item C0564 asked whether the school was a regular public school, a charter school, a school with a magnet program for part of the school, exclusively a magnet school, or \"Other,\" and C0565 allowed a text response if \"Other\" was selected. Open-ended responses to C0565 were either back-coded as response options to item C0564 or, if it was determined that the responses could not readily be grouped into categories, left in the \"Other\" category. C0565 was omitted from the public-use file."}, {"section_title": "Imputation", "text": "Files containing missing data can be problematic because, depending on how the missing data are treated, the analysis of incomplete datasets may cause different users to arrive at different conclusions. Missing data may also create bias in the survey estimates, because certain groups of respondents may be more likely than others to leave some survey items unanswered. When completed SSOCS:2016 surveys contained some level of item nonresponse after the conclusion of the data retrieval phase, 23 imputation procedures were used to create values for all questionnaire items with missing information. Appendix N presents the base-weighted response rate for each survey item eligible for recontact, after data editing and cleaning, and the type of imputation used for each item. Appendix N includes response rates for survey items which are included in the public-use file as well as those that are included in the restricted-use file but have been removed from the public-use file. For each questionnaire item in the data file, there is an accompanying imputation flag variable to indicate the imputation method used. For details regarding imputation flags, refer to section 5.9 below. The base-weighted item response rates for SSOCS:2016 were generally high. After data cleaning and editing, the base-weighted item response rates of the 273 questionnaire items reviewed ranged from 82 to 100 percent. The mean weighted item response rate was about 98 percent, which is relatively high for a mailed self-administered questionnaire. In fact, the majority of items (99 percent) had weighted response rates of more than 85 percent."}, {"section_title": "Imputation Methods", "text": "The imputation methods used in SSOCS:2016 were tailored to the nature of each survey item. Three methods were used: aggregate proportions, hot deck, and clerical. Each method is described briefly below. A detailed discussion of SSOCS imputation methods can be found in appendix Q. Aggregate proportions. Many of the items in SSOCS:2016 were counts of incidents or disciplinary actions. These counts are likely to be related to other school characteristics such as enrollment. The imputation methods used for such items were designed to maintain these relationships. Namely, rather than imputing counts from a single donor or a mean count from a group of donors, proportions were imputed using two methods. For most items, the imputed proportions were derived from a single donor within an imputation class, as the donor's ratio of the item in question to another count (typically school enrollment). However, for a select number of items, ratios were calculated by using the sums of the items across multiple donors in the imputation class with the identical instructional level and enrollment size category as the recipient. 24 Regardless of how the donors were selected, the donor proportion was assigned to recipient schools in that imputation class, and the proportion was multiplied by a known value for the recipient school, such as the number of students. Unlike mean imputation, this method maintains variability. Since the proportion is based on multiple donors, the result is also more stable than if it had been based on a single donor. By using more stable, aggregate proportions, imputation of outlier values is also minimized. Hot deck. For categorical variables and several continuous variables, hot deck imputation was used. Hot deck imputation is a method for handling missing data in which each missing value is replaced with an observed response from a \"similar\" unit. A donor is chosen by observing responses from a similar unit, and a series of missing items is imputed directly from those items in the donor record. Clerical. In some instances, missing data were available from the CCD frame. For example, sampling frame data were used to impute values for schools missing student enrollment data (item 37). Frame data were also available for school type (item 43), the percentage of male student enrollment (item 38d), and the percentage of students eligible for free or reduced-price lunch (item 38a). In other instances, research was done on school administrative records to estimate logical values for missing data."}, {"section_title": "Imputation Order", "text": "The interrelationships between the items in the SSOCS survey necessitated that a specific imputation order be followed. Because item 37 (student enrollment) is used in imputation for other variables, it was the first item to be imputed. Because item 35 is closely linked to several survey items, including items 26, 34, 36, and 45, the components of this item were imputed next. After the imputation of the item 35 matrix was complete, items 26 and 34 were imputed. This imputation sequence was chosen because some item 34 values and some item 26 values are limited by the item 35 values. After these four items were imputed, items 36 and 45 were imputed. Similarly, this imputation sequence was chosen because the item 36 values are limited by the item 35 values, and the item 45 values are limited by the item 36 values. The remaining questionnaire items were then imputed."}, {"section_title": "Imputation Flags", "text": "The imputation flags indicate the imputation method used: aggregate proportions, hot deck, or clerical. The codes used for the imputation flags are described in section 5.9."}, {"section_title": "5.", "text": "Guide to the Public-Use Data File and Codebook"}, {"section_title": "Content and Organization of the Data File", "text": "The SSOCS:2016 data file contains data from all 2,092 completed questionnaires. The contents of the data file are presented in the following order: the unique school identifier (SCHID); questionnaire item variables, including categorized versions of the open-ended response variables; the composite (created) variables, including the nesting variable (STRATA); the sampling frame variables; the final sampling weight (FINALWGT); the jackknife replicate weights; and the imputation flags. Each of these sets of variables is described in sections 5.3 through 5.9 below. Use proc export to convert the SAS file into a comma-delimited file (.csv). In Stata, use the import delimited command to read in the .csv file. For example, if the SSOCS SAS file was saved in the C:\\ directory, use the following code in SAS: libname in \"c:\\\"; proc export data=in.pu_ssocs16_sas outfile=\"c:\\pu_ssocs16_stata.csv\" dbms=csv replace; run; In Stata, then use the following code to read in the .csv file, convert it to a Stata file, and save it in the C:\\ directory: cd c:\\ import delimited using pu_ssocs16_stata.csv, varnames(1) clear save pu_ssocs16_stata.dta, replace Alternatively, use proc export to convert the SAS file into an .xpt file. In Stata, then use the import sasxport command to read in the .xpt file. For example, if the SSOCS SAS file was saved in the C:\\ directory, use the following code in SAS: libname out XPORT \"c:\\pu_ssocs16_sas.xpt\"; data out.pu_ssocs16_stata; set \"c:\\pu_ssocs16_sas\"; run; In Stata, then use the following code to read in the .xpt file, convert it to a Stata file, and save it in the C:\\ directory: cd c:\\ import sasxport pu_ssocs16_stata, clear compress save pu_ssocs16_stata.dta, replace For additional information, see http://stats.idre.ucla.edu/other/mult-pkg/faq/how-do-i-convertamong-sas-stata-and-spss-files/."}, {"section_title": "Converting From SAS to SPSS", "text": "In SPSS, use the get sas data command to open the SAS data file in SPSS. For example, if the SSOCS SAS file was saved in the C:\\ directory, use the following code in SPSS: To save as an SPSS file in the C:\\ directory, use the following code in SPSS: For additional information, see https://stats.idre.ucla.edu/other/mult-pkg/faq/how-do-i-use-a-sasdata-file-in-spss/."}, {"section_title": "Reading into R", "text": "The foreign package contains functions that will allow users to import data files from SAS (.xpt format only), Stata, and SPSS. To download the foreign package from the CRAN website from within R, click on \"Packages\" and then \"Install package(s) from CRAN.\" Alternatively, the following syntax will allow users to download the package and view the package functions: Here are syntax examples of importing a Stata, SPSS, and .xpt SSOCS:16 file into R: A file that has previously been saved as a CSV file can be read into R using the read.csv() function in base R, an example of which follows: >pu_ssocs16_r <-read.csv (\"c:\\pu_ssocs16.csv\",stringsAsFactors=FALSE) Finally, the haven package allows SAS datasets to be imported directly into R through the read_sas() function, without first converting to a different format. An example of this function is as follows: >pu_ssocs16_r <-read.sas(\"c:\\pu_ssocs16_sas.sas7bdat\") The save() function allows users to save the data from the original format into the R data format: > save(pu_ssocs16_r, file = \"pu_ssocs16_r.RData\")"}, {"section_title": "Public-Use Data File", "text": "This manual is designed to assist users of the public-use SSOCS:2016 data file. The public-use data file can be found at http://nces.ed.gov/surveys/ssocs/data_products.asp. Data on school crime can be considered sensitive, and in order to encourage complete and honest responses, participating schools were promised confidentiality. To protect the confidentiality of sampled schools, the following several steps were taken to prepare a public-use data file: \u2022 The variables used for sampling were omitted or included only as categorical variables in order to lessen the amount of identifying information provided about each school. \u2022 Some data collected in the questionnaire were omitted or modified; for example, by being converted to categorical variables or by being replaced by composite variables that contained summary information. This is especially true for the continuous variables (such as the incident counts) because of their potential capacity to uniquely identify a school. \u2022 Some data were perturbed in ways that would not affect their overall distribution but so that the data no longer directly corresponded to the respondents' original data. \u2022 The data file was examined using disclosure analysis procedures in order to identify any threats to confidentiality. \u2022 Some variables were removed from the data file to reduce the risk of disclosure. This process resulted in the public-use data file. Though the public-use file was designed to meet the needs of most users, some users may desire the more specific data that were removed in the public-use file. Please see appendix C for a list of variables that can be found in the restricted-use file that are not included in the public-use file. These data can be obtained by requesting the restricted-use file from NCES; however, the perturbations that were made to the data were applied consistently to both the public-use and restricted-use files. To learn more about getting a license, please visit http://nces.ed.gov/pubsearch/licenses.asp."}, {"section_title": "Unique School Identifier", "text": "The sample file was sorted by control number, and the school case IDs were assigned sequentially. There were 3,553 ID numbers assigned, one for each sampled school. This identifier is called SCHID. SCHID is created specifically for the SSOCS data file and, while it is included for the 2,092 respondent cases that appear in the public-use file, it cannot be used to link schools to any other files."}, {"section_title": "Questionnaire Item Variables", "text": "The questionnaire, shown in appendix A, has 47 items and 273 subitems, not counting the introductory items. In the data file and accompanying codebook, these items are listed in the order in which they appear in the questionnaire; within items, subitems are listed in source code order. Response values for question item variables are indicated in the questionnaire. A value of \"-1\" indicates that the item was legitimately skipped. SSOCS variables are identified by source codes rather than by item numbers. The source code is \"C0\" followed by the 3-digit number next to the item in the questionnaire. For example, the first subitem of item 1 is variable C0110. Variables that have been recoded to preserve confidentiality are denoted with an \"_R\" following the variable source code. For example, a small number of schools reported having an arrest that occurred at school in item C0688. Therefore, the responses for this item were collapsed into four categories (None, 1-5, 6-10, or 11 or more) to prevent individual schools from being identified. The variable was renamed to C0688_R to reflect this revision. See section 5.5 below for more information regarding items that were recoded for the public-use file to preserve confidentiality in SSOCS:2016. There are two open-ended text questions in the questionnaire-respondent job title and other school type-and both were examined to identify common responses. When a write-in response appeared frequently, it was given a new code. The remaining responses were left in an \"other\" category.\" See section 4.3 for more information regarding the coding of text items in SSOCS:2016."}, {"section_title": "Variables Recoded for Public-Use File to Preserve Confidentiality", "text": "On the SSOCS:2016 questionnaire, schools were asked to report the number of arrests (C0688) and the number of hate crimes (C0690) that occurred at school. Due to the small number of schools reporting these incidents, including an incident count in the public-use file would present a disclosure risk. Therefore, these two variables were recoded for inclusion in the public-use file so that variables could be made available to users while simultaneously preserving the confidentiality of the respondents. The arrest variable was recoded from a continuous variable to a categorical variable. For the revised variable (C0688_R), the number of arrests reported by each school was sorted into one of the following categories: None, 1-5, 6-10, or 11 or more. The hate crime variable was recoded from a continuous variable to a binary variable, with \"Yes\" and \"No\" as the possible response options. For the revised variable (C0690_R), schools that reported at least one hate crime were coded as \"1\" while schools that reported no hate crimes were coded as \"2.\""}, {"section_title": "5.6", "text": ""}, {"section_title": "Composite Variables", "text": "Composite variables were created and are included in the SSOCS data file to simplify analysis for users and make it easier for analysts to replicate others' results. A list of the composite variables included in the public-use file is presented below with an explanation of how they were derived."}, {"section_title": "CRISIS16 -Number of types of crises covered in written plans", "text": "Purpose: To provide a summary measure of schools' advance planning for crisis situations. To provide a summary measure of the extent to which problems occur at school regularly. General explanation: Provides a school-level count of disciplinary problems listed in items 32a-i as happening \"daily\" or \"at least once a week.\" SAS code: PROBWK16=0; if C0374 in (1,2) then PROBWK16=PROBWK16 + 1; if C0376 in (1,2) then PROBWK16=PROBWK16 + 1; if C0378 in (1,2) then PROBWK16=PROBWK16 + 1; if C0380 in (1,2) then PROBWK16=PROBWK16 + 1; if C0381 in (1,2) then PROBWK16=PROBWK16 + 1; if C0382 in (1,2) then PROBWK16=PROBWK16 + 1; if C0383 in (1,2) then PROBWK16=PROBWK16 + 1; if C0384 in (1,2) then PROBWK16=PROBWK16 + 1; if C0386 in (1,2) then PROBWK16=PROBWK16 + 1; REMOVL16 -Total number of removals with no continuing school services for specified offenses Purpose: To provide a summary measure of the number of removals with no continuing school services for at least the remainder of the school year. General explanation: Sum of items 35a-e, column 2. SAS code: REMOVL16 = sum(C0460, C0470, C0480, C0490, C0500); SEC_FT16 -Total number of full-time security guards, SROs, and other sworn law enforcement officers Purpose: To provide a summary measure of the number of full-time security guards, School Resource Officers, and other sworn law enforcement officers present at school General explanation: Sum of items 18ai, 18bi, and 19i. If a school had no security staff (as answered in question 11), then the total was set to zero. Note. Schools that reported that they had no security staff (as answered in question 11) were coded as -1 for questions 18ai and 18bi to stay consistent with the legitimate skip coding. SAS code: SEC_FT16 = sum(C0232, C0236, C0240); if C0610=2 then SEC_FT16=0; SEC_PT16 -Total number of part-time security guards, SROs, and other sworn law enforcement officers Purpose: To provide a summary measure of the number of part-time security guards, School Resource Officers, and other sworn law enforcement officers present at school. General explanation: Sum of items 18aii, 18bii, and 19ii. If a school had no security staff (as answered in question 11), then the total was set to zero. Note. Schools that reported that they had no security staff (as answered in question 11) were coded as -1 for questions 18ai and 18bi to stay consistent with the legitimate skip coding. SAS code: SEC_PT16 = sum(C0234, C0238, C0242); if C0610=2 then SEC_PT16=0; STRATA -Collapsed sampling stratum (nesting variable) Purpose: To identify the sampling stratum for Taylor series variance estimation (described in section 6.2). General explanation: Sampling stratum defined by concatenating school level, enrollment size category, and four-level locale, and then collapsing small strata as needed. SAS code: STRATA = FR_LVEL || FR_SIZE || FR_URBAN; if STRATA in (\"143\",\"144\") then STRATA = \"144\"; if STRATA in (\"411\",\"412\") then STRATA = \"412\"; if STRATA in (\"413\",\"414\") then STRATA = \"414\"; if STRATA in (\"443\",\"444\") then STRATA = \"444\"; STUOFF16 -Total number of students involved in recorded offenses (regardless of disciplinary action) Purpose: To provide a summary measure of the number of students involved in specified recorded offenses. SAS code: VIOPOL16 = sum(C0312, C0316, C0320, C0324, C0328, C0332, C0336, C0340);"}, {"section_title": "Sampling Frame Variables", "text": "A number of variables from the 2013-14 Common Core of Data (CCD) sampling frame are included in the public-use data file, including variables used for stratification purposes. These variables were taken from the 2013-14 CCD school-level data file and provide key statistics about the schools sampled in SSOCS:2016. With the exception of the percentage of White enrollment (categorical), each sampling frame variable in the public-use file begins with the prefix \"FR_\" (to denote that it is a sampling frame variable) and has a variable label indicating that the variable was taken from the CCD school-level file. For example, \"FR_SIZE\" is described in the file as \"School size categories -taken from the 13-14 CCD (School).\" The frame variables listed in the SSOCS:2016 data file are described below in the order in which they appear in the codebook."}, {"section_title": "FR_LVEL", "text": "This is a SSOCS-created variable based on school grades offered as reported in the 2013-14 CCD school data file. This variable has four categories indicating the span of grades offered. 1 = primary, 2 = middle, 3 = high school, and 4 = combined. (Categorical) FR_LVEL can be created based on the variables FR_HIGD and FR_LOGD (listed in appendix C) as described above, as follows: SAS code: if (FR_HIGD <= 8 & FR_LOGD <= 3) then FR_LVEL = 1; else if (FR_HIGD <= 9 & FR_LOGD >= 4) then FR_LVEL = 2; else if (FR_HIGD <= 12 & FR_LOGD >= 9) then FR_LVEL = 3; else if (FR_HIGD = 9 & FR_LOGD = 9) then FR_LVEL = 2; else FR_LVEL =4; FR_SIZE This is a SSOCS-created variable of school size categories. This variable collapses the number of students into four categories: 1 = less than 300, 2 = 300-499, 3 = 500-999, and 4 = 1,000 or more students. (Categorical) FR_SIZE can be created based on the variable FR_NOST (listed in appendix C) as described above, as follows: PERCWHT This is a SSOCS-created variable representing percent White enrollment as reported in the 2013-14 CCD school data file. This variable has four categories: 1 = more than 95 percent, 2 = more than 80 to 95 percent, 3 = more than 50 to 80 percent, and 4 = 50 percent or less. (Categorical) PERCWHT can be created based on the variable FR_PERWT (listed in appendix C), as follows: SAS code: if FR_PERWT gt 95 then PERCWHT=1; else if 80 < FR_PERWT <= 95 then PERCWHT = 2; else if 50 < FR_PERWT <= 80 then PERCWHT =3; else PERCWHT =4;"}, {"section_title": "Weighting and Variance Estimation Variables", "text": "The final weight, \"FINALWGT,\" is needed to produce national estimates from the variables listed in the file. The final weight precedes the 50 jackknife replicate weights (REPFWT1 to REPFWT50). Also included in the data file are the variables \"STRATA\" and \"SCHID,\" which are the STRATA and primary sampling unit (PSU) variables needed for the nesting statement when producing Taylor series approximations in statistical analysis software. For a more detailed discussion of replicate weights and Taylor series approximations, see section 6.2."}, {"section_title": "Imputation Flag Variables", "text": "With the exception of the introductory items and open-ended text items, each questionnaire item in the data file has an imputation flag, which indicates whether any imputation was required. The naming convention appends the prefix \"I\" to the questionnaire variable. For example, row A of item 1 would have an imputation flag named IC0110. The flag values represent the type of imputation method used and are as follows: 0 = Value not imputed. 7 = Item was imputed by using data from the record for a similar case (donor). 8 = Item was imputed by using the mean or mode of data for groups of similar cases. 9 = Data value was adjusted during analysts' post-imputation review of data. A detailed discussion of SSOCS imputation methods can be found in appendix Q."}, {"section_title": "Applying the Weight and Computing Standard Errors", "text": ""}, {"section_title": "Applying the Weight", "text": "SSOCS data are intended to represent U.S. public schools nationwide rather than only the schools that responded to the SSOCS survey; therefore, most analyses should be done with the weighted SSOCS data. The final SSOCS analysis weight on the SSOCS data file is called FINALWGT. See section 6.2 for example code that incorporates the final weight."}, {"section_title": "Computing Standard Errors", "text": "Estimates derived from a probability sample are subject to sampling error because only a small fraction of the target population has been surveyed. In surveys with complex sampling designs, such as SSOCS, estimates of standard errors that assume simple random sampling typically underestimate the variability in the point estimates. Two commonly used methods for estimating sampling errors that account for complex sampling designs are (1) replication and (2) the Taylor series linearization procedure (TSP). Replication involves splitting the entire sample into a set of groups based on the actual sample design of the survey. The survey estimates can then be computed for each of the replicates by creating replicate weights that mimic the actual sample design and estimation procedures used in the full sample. The variation in the estimates computed from the replicate weights can then be used to estimate the sampling errors of the estimates for the full sample. A total of 50 replicates were defined for SSOCS:2016. The specific replication procedure used for SSOCS:2016 was the jackknife method, which involved dividing the sample into 50 subsamples (replicates) for the computation of the replicate weights. Replicate weights were created for each of the 50 replicates using the same estimation procedures that were used for the full sample. These replicate weights are included in the SSOCS:2016 data file as REPFWT1 through REPFWT50 and can be used to calculate sampling errors in a number of software packages specializing in complex sample designs. The formula for the jackknife standard error of an estimate is where is the estimate computed using the final analysis weight (FINALWGT) and is the estimate computed using the rth replicate weight (REPFWTr). Another valid approach to the estimation of sampling errors for complex sample design is to use TSP. Under TSP, sampling is assumed to be with replacement within each stratum to avoid estimating the variance at all stages of sampling, and the variance computation involves only the totals of PSUs within each stratum. Therefore, it is important to specify the PSU (i.e., the school) identified by the unique school variable and the stratum to which the PSU belongs for computing the variance. The SSOCS:2016 data file includes variables to obtain weighted estimates and to calculate standard errors using TSP. Table 6 gives a summary of weighting and sample variance estimation variables for data files from each administration of SSOCS. Data users should be aware that the use of different approximation methods or software packages in the calculation of standard errors may result in slightly different standard errors. Standard errors computed using the replication method and TSP are nearly always very similar, but not identical. The statistical programs that allow for the calculation of standard errors using both jackknife replication and TSP are SUDAAN, 25 Stata, 26 SAS (versions 9.2 and above), 27 and the survey package in R. 28 An additional program that offers the replication method is WesVar. 29 Additional programs that offer TSP are SAS (version 8 or above), SPSS, 30 and AM. 31 Sample code is provided below for calculating standard errors for means using the jackknife replication method in SAS-callable SUDAAN, SAS (version 9.2 and above), Stata, and the survey package in R. Sample code is also provided for calculating standard errors for means using TSP in SAS, Stata, SUDAAN, the SPSS Complex Samples module, and the survey package in R. Step One: CSPLAN ANALYSIS /PLAN FILE='C:\\SSOCS.CSAPLAN' /PLANVARS ANALYSISWEIGHT=FINALWGT /DESIGN STRATA=STRATA CLUSTER=SCHID /ESTIMATOR TYPE=WR. Step 7."}, {"section_title": "Data Considerations and Anomalies", "text": "This section discusses some of the anomalies and considerations that analysts should take into account when using the SSOCS:2016 data. In addition, it describes some of the data problems and the logical imputation edits that were implemented in the SSOCS:2016 data file. Note that many of the specific variables discussed below have been removed from the SSOCS:2016 public-use file. However, several of the composite variables included in the publicuse file were constructed using these variables, and they reflect the anomalies identified below."}, {"section_title": "Law Enforcement Officers: Items 11 (C0610) Through 18b (C0242)", "text": "In item 11, respondents are asked whether their schools have any sworn law enforcement officers. Respondents who answer \"no\" are then skipped to item 19. In some cases, however, respondents who answered \"no\" proceeded to answer positively to items 12, 13, 14, 15, or 18, which ask for descriptions of the security personnel. In these cases, the \"no\" response in item 11 was logically edited to a \"yes\" response."}, {"section_title": "Number of Incidents: Subitems 26a_1 (C0310) Through 26l_2 (C0364)", "text": "In item 26, respondents are asked to record the overall number of specific incidents that occurred at their school during the 2015-16 school year-for example, rape, robbery, physical attack, or theft-and then the number of those incidents that were reported to police. Logically, the number reported to police should not exceed the total number of incidents. If more incidents were reported to police than were recorded as having occurred, the overall number of incidents recorded was deleted and a revised count was later imputed. To protect respondents' confidentiality, the detailed responses were omitted from the public-use file and replaced by summary measures."}, {"section_title": "Use of Disciplinary Actions: Subitems 34a_1 (C0390) Through 34o_2 (C0456)", "text": "In item 34, respondents are asked to report whether various disciplinary actions are allowed in their school. If a respondent reports that a specific disciplinary action is allowed, he or she is then asked whether the action was used during the 2015-16 school year. Logically, a disciplinary action must be allowed in order for it to be used during the school year. Some respondents reported \"no\" to the question of whether the action was allowed, but \"yes\" to the question of use. In these circumstances, the \"no\" response to whether the action was allowed was logically edited to a \"yes\" response."}, {"section_title": "Number of Students Involved in Recorded Offenses of Use/Possession of a Firearm/Explosive Device: Subitem 35a_1 (C0458)", "text": "In item 35a_1, respondents are asked to report the total number of students involved in recorded offenses of use or possession of a firearm/explosive device. In the event that the value of C0458 is missing but there are valid values for each type of disciplinary action for this offense (C0460-C0466), the number of students (C0458) is edited to be equal to the sum of disciplinary actions taken for that offense. When applied to the SSOCS:2016 data file, this edit resulted in the largest values of C0458 in the data file. Specifically, about 21 percent of these edited values constitute the highest values of the distribution of this variable (about the highest 0.1 percent of the distribution). Because the values of disciplinary actions recorded were not the result of editing or imputation, the edited values of C0458 were left as is in the SSOCS:2016 data file. Data users may want to top-code responses to this item at 10 or eliminate them from analysis when using this variable. This item was omitted from the public-use file to protect respondents' confidentiality."}, {"section_title": "Disciplinary Actions Taken: Subitems 35a_1 (C0458) Through 35e_5 (C0506)", "text": "In item 35, respondents are asked to report the total number of students in their school who committed various offenses (column 1) and to provide counts of various disciplinary actions taken in response to those offenses (columns 2-5). In some cases, respondents provided a response of zero in the \"total students\" column, leaving the remaining columns blank (or a mixture of zeros and blanks). In these cases, missing data were recoded to values of zero during the data-editing process. To protect respondents' confidentiality, the detailed responses were omitted from the public-use file and replaced by summary measures."}, {"section_title": "Total Removals and Transfers: Subitems 36a (C0518) and 36b (C0520)", "text": "In item 36, respondents are asked to report the total number of removals and transfers from their school for disciplinary reasons. Logically, these counts should be equal to or greater than the total number of removals and transfers reported in item 35, column 2, \"Removals with no continuing school services for at least the remainder of the school year,\" and column 3, \"Transfers to specialized schools,\" for the specified offenses. In cases where the item 35 counts for the removal and transfer columns exceeded their respective subparts in item 36, the item 36 count was deleted and imputed."}, {"section_title": "Classroom Changes: Item 40 (C0538)", "text": "In item 40, schools are asked to report the average number of classroom changes most students make during a typical day. Some respondents may have interpreted this question to mean the number of classroom changes that occur throughout the school in a typical day, regardless of whether most students make all of those changes; therefore, some responses were quite high. These abnormally high responses were blanked, and a new value was imputed."}, {"section_title": "Average Daily Attendance: Item 44 (C0568)", "text": "In item 44, schools were asked to report the average daily attendance (percentage of students present). Some respondents may have interpreted this question to mean the percentage of students absent rather than present; therefore, some responses were quite low. These abnormally low responses were left in the data file; however, data users may want to code these responses in a different manner or eliminate them from analysis when using this variable."}, {"section_title": "Outliers in Count Variables", "text": "For some items that required schools to enter a count of incidents, students, or disciplinary actions, a small number of schools entered values that, while technically permissible under the SSOCS:2016 range and consistency rules, were unusually high. Specifically: \u2022 In item 28 (C0690), one school reported 50 hate crimes. The next highest entry for this item was 22. \u2022 In column 5 of item 35c (C0486), one school reported 300 \"Other disciplinary actions\" for distribution, possession, and use of illegal drugs. The next highest entry for this item was 100. \u2022 In some parts of item 35 (C0458-C0506), a handful of schools reported that the number of disciplinary actions for a particular offense (columns 2 through 5) substantially exceeded the number of students involved in that offense (column 1). While it is theoretically possible for a student to be disciplined more than once for the same type of offense, the size of the discrepancies for these cases raised concerns during data review. \u2022 In item 36a (C0518), one school reported that 94 students were removed without continuing services for disciplinary reasons. The next highest entry for this item was 44. \u2022 Two schools reported in item 45b (C0572) that the number of transfers out of the school exceeded 90 percent of the enrollment reported in item 37 (C0522). For all of these schools, the questionnaires were manually rechecked to verify that the unusual values were actually entered by the respondent and were not the result of a keying error. Because all of the outlier values noted above were confirmed to have been actually entered by the respondents, and did not violate prespecified range or consistency rules, they were left in the data file. However, when using count variables in analyses, data users may want to consider topcoding the counts or eliminating outlier cases from the analysis, as appropriate. \u25b2 \u25bc \u25bc \u25b2 \u00a7,#&\u00a4 110205"}, {"section_title": "DEFINITIONS", "text": "The following words are bolded and marked by an asterisk (*) wherever they appear in the questionnaire. Please use these definitions as you respond. Active shooter -an individual actively engaged in killing or attempting to kill people in a confined and populated area; in most cases, active shooters use firearm(s) and there is no pattern or method to their selection of victims. At school/at your school -activities happening in school buildings, on school grounds, on school buses, and at places that hold school-sponsored events or activities. Unless otherwise specified, this refers to normal school hours or to times when school activities/events were in session. Bullying -any unwanted aggressive behavior(s) by another youth or group of youths who are not siblings or current dating partners that involves an observed or perceived power imbalance and is repeated multiple times or is highly likely to be repeated. Cyberbullying -occurs when willful and repeated harm is inflicted through the use of computers, cell phones, or other electronic devices. Diagnostic assessment -an evaluation conducted by a medical or mental health professional that identifies whether an individual has one or more medical and/or mental health diagnoses. This is in contrast to an educational assessment, which does not focus on clarifying a student's diagnosis. Evacuation -a procedure that requires all students and staff to leave the building. While evacuating to the school's field makes sense for a fire drill that only lasts a few minutes, it may not be an appropriate location for a longer period of time. The evacuation plan should encompass relocation procedures and include backup buildings to serve as emergency shelters, such as nearby community centers, religious institutions, businesses, or other schools. Evacuation also includes \"reverse evacuation,\" a procedure for schools to return students to the building quickly if an incident occurs while students are outside. Firearm/explosive device -any weapon that is designed to (or may readily be converted to) expel a projectile by the action of an explosive. This includes guns, bombs, grenades, mines, rockets, missiles, pipe bombs, or similar devices designed to explode and capable of causing bodily harm or property damage. Gender identity -means one's inner sense of one's own gender, which may or may not match the sex assigned at birth. Different people choose to express their gender identity differently. For some, gender may be expressed through, for example, dress, grooming, mannerisms, speech patterns, and social interactions. Gender expression usually ranges between masculine and feminine, and some transgender people express their gender consistent with how they identify internally, rather than in accordance with the sex they were assigned at birth. Lockdown -a procedure that involves occupants of a school building being directed to remain confined to a room or area within a building with specific procedures to follow. A lockdown may be used when a crisis occurs outside of the school and an evacuation would be dangerous. A lockdown may also be called for when there is a crisis inside and movement within the school will put students in jeopardy. All exterior doors are locked and students and staff stay in their classrooms. Mental health disorders -collectively, all diagnosable mental disorders or health conditions that are characterized by alterations in thinking, mood, or behavior (or some combination thereof) associated with distress and/or impaired functioning. Mental health professionals -mental health services are provided by several different professions, each of which has its own training and areas of expertise. The types of professionals who may provide mental health services include psychiatrists, psychologists, psychiatric/mental health nurse practitioners, psychiatric/mental health nurses, clinical social workers, and professional counselors. Physical attack or fight -an actual and intentional touching or striking of another person against his or her will, or the intentional causing of bodily harm to an individual. Gang -an ongoing loosely organized association of three or more persons, whether formal or informal, that has a common name, signs, symbols, or colors, whose members engage, either individually or collectively, in violent or other forms of illegal behavior. Hate crime -A committed criminal offense that is motivated, in whole or in part, by the offender's bias(es) against a race, religion, disability, sexual orientation, ethnicity, gender, or gender identity; also known as bias crime. Rape -forced sexual intercourse (vaginal, anal, or oral penetration). This includes sodomy and penetration with a foreign object. Both male and female students can be victims of rape. [Counts of attempted rape should be added to counts of rapes in your reporting of item 26a.] \u25b2 \u25bc \u25bc \u25b2 \u00a7,#&\u00a4 110205"}, {"section_title": "DEFINITIONS -Continued", "text": "The following words are bolded and marked by an asterisk (*) wherever they appear in the questionnaire. Please use these definitions as you respond. Sexual assault -an incident that includes threatened rape, fondling, indecent liberties, or child molestation. Both male and female students can be victims of sexual assault. Classification of these incidents should take into consideration the age and developmentally appropriate behavior of the offender(s). Sexual harassment -conduct that is unwelcome, sexual in nature, and denies or limits a student's ability to participate in or benefit from a school's education program. The conduct can be carried out by school employees, other students, and non-employee third parties. Both male and female students can be victims of sexual harassment, and the harasser and the victim can be of the same sex. The conduct can be verbal, nonverbal, or physical. Shelter-in-place -a procedure similar to a lockdown in that the occupants are to remain on the premises; however, shelter-in-place is designed to use a facility and its indoor atmosphere to temporarily separate people from a hazardous outdoor environment. Everyone would be brought indoors and building personnel would close all windows and doors and shut down the heating, ventilation, and air conditioning system (HVAC). This would create a neutral pressure in the building, meaning the contaminated air would not be drawn into the building. Threat assessment team -a formalized group of persons who meet on a regular basis with the common purpose of identifying, assessing, and managing students who may pose a threat of targeted violence in schools. Treatment -a clinical service addressed at lessening or eliminating the symptoms of a disorder. In mental health, this may include psychotherapy, medication treatment, and/or counseling. Vandalism -the willful damage or destruction of school property, including bombing, arson, graffiti, and other acts that cause property damage. This includes damage caused by computer hacking. Violence -actual, attempted, or threatened fight or assault. Weapon -any instrument or object used with the intent to threaten, injure, or kill. This includes look-alikes if they are used to threaten others. Specialized school -a school that is specifically for students who were referred for disciplinary reasons, although the school may also have students who were referred for other reasons. The school may be at the same location as your school. Theft/larceny (taking things worth over $10 without personal confrontation) -the unlawful taking of another person's property without personal confrontation, threat, violence, or bodily harm. This includes pocket picking, stealing a purse or backpack (if left unattended or no force was used to take it from owner), theft from a building, theft from a motor vehicle or of motor vehicle parts or accessories, theft of a bicycle, theft from a vending machine, and all other types of thefts. Robbery (taking things by force) -the taking or attempting to take anything of value that is owned by another person or organization, under confrontational circumstances by force or threat of force or violence and/or by putting the victim in fear. A key difference between robbery and theft/larceny is that robbery involves a threat or assault. Restorative circle -a formal mediation process led by a facilitator that brings affected parties of a problem together to explore what happened, reflect on their roles, find a solution, and ultimately restore harmony to individual relationships and the larger community. Special education student -a child with a disability, defined as mental retardation, hearing impairments (including deafness), speech or language impairments, visual impairments (including blindness), serious emotional disturbance, orthopedic impairments, autism, traumatic brain injury, other health impairments, or specific learning disabilities, who needs special education and related services and receives these under the Individuals with Disabilities Education Act (IDEA). Sexual orientation -means one's emotional or physical attraction to the same and/or opposite sex."}, {"section_title": "A-4 WHERE SHOULD I RETURN MY COMPLETED QUESTIONNAIRE?", "text": "Please return your completed questionnaire in the enclosed postage-paid envelope or mail it to:"}, {"section_title": "Paperwork Burden Statement", "text": "According to the Paperwork Reduction Act of 1995, no persons are required to respond to a collection of information unless it displays a valid OMB control number. The valid OMB control number for this voluntary information collection is 1850-0761. The time required to complete this information collection is estimated to average 52 minutes per response, including the time to review instructions, search existing data resources, gather the data needed, and complete and review the information collection. If you have any comments concerning the accuracy of the time estimate, suggestions for improving this collection, or comments or concerns about the contents or the status of your individual submission of this questionnaire, please write directly to: School Survey on Crime and Safety (SSOCS), National "}, {"section_title": "SURVEY INSTRUCTIONS:", "text": "For most questions, please mark the box that best reflects your school's circumstances. Please mark your response with an \"X\". Some questions ask for counts or percents of items. Please place an \"X\" in the None box, rather than leaving the item blank, if the number of such items at your school is zero. It is not necessary to consult any records for items 9 and 39. Please provide estimates for these questions. Definitions are available for many terms on pages 2 and 3. Defined terms are bolded and marked with an asterisk (*) throughout the survey. Some questions refer to the 2015-16 school year. Please report for the school year to date. Please have this questionnaire filled out by the person most knowledgeable about school crime and policies to provide a safe environment. If you have any questions about this questionnaire, please contact the U.S. Census Bureau at: 1-888-595-1332 or at addp.education.surveys@census.gov. Please keep a copy of the completed questionnaire for your records. "}, {"section_title": "Require visitors to sign or check in and wear badges", "text": "Control access to school buildings during school hours (e.g., locked or monitored doors) b. Control access to school grounds during school hours (e.g., locked or monitored gates) Require metal detector checks on students every day Perform one or more random metal detector checks on students Close the campus for most or all students during lunch Use one or more random dog sniffs to check for drugs Perform one or more random sweeps for contraband (e.g., drugs or weapons*), but not including dog sniffs  Check \"Yes\" or \"No\" on each line. Provide an electronic notification system that automatically notifies parents in case of a school-wide emergency Provide a structured anonymous threat reporting system (e.g., online submission, telephone hotline, or written submission via drop box) 1 x. 141 143 a. Prohibit use of cell phones and text messaging devices during school hours y. 153 1 Equip classrooms with locks so that doors can be locked from the inside f. Have \"panic button(s)\" or silent alarm(s) that directly connect to law enforcement in the event of an incident p. *Please use the definition on pages 2 and 3. Please respond to each of these according to the definitions provided on pages 2 and 3. Student court to address student conduct problems or minor offenses h. 177 1 \u25b2 \u25bc \u25bc \u25b2 \u00a7,'\"\u00a4 110601 5. During the 2015-16 school year, did your school have a threat assessment team* or any other formal group of persons to identify students who might be a potential risk for violent or harmful behavior (toward themselves or others)? 7. During the 2015-16 school year, did your school have any recognized student groups with the following purposes? Check \"Yes\" or \"No\" on each line. 10. During the 2015-16 school year, were any of the following community and outside groups involved in your school's efforts to promote safe, disciplined, and drug-free schools? Check \"Yes\" or \"No\" on each line. "}, {"section_title": "11.", "text": "At any time during school hours a. "}, {"section_title": "13.", "text": "Do not include security guards or other security personnel who are not sworn law enforcement in your response to this item; information on additional security staff is gathered in item 19. Check \"Yes\" or \"No\" on each line. Wear a body camera d. 14. Did these sworn law enforcement officers (including School Resource Officers) participate in the following activities at your school*? Check \"Yes\" or \"No\" on each line. Teaching a law-related education course or training students (e.g., drug-related education, criminal law, or crime prevention courses) h."}, {"section_title": "1 2", "text": "Recording or reporting discipline problems to school authorities i.\nProviding information to school authorities about the legal definitions of behavior for recording or reporting purposes (e.g., defining assault for school authorities) j.\nDuring the 2015-16 school year, did your school have a sworn law enforcement officer (including School Resource Officers) present for all instructional hours every day that school was in session? 15. Include officers who are used as temporary coverage while regularly assigned officers are performing duties external to the school (such as attending court) or during these officers' personal leave time. Check \"No\" if your school does not have officer coverage while regularly assigned officers are performing duties external to the school (such as attending court) or during these officers' personal leave time. Do not include security guards or other security personnel who are not sworn law enforcement in your response to this item; information on additional security staff is gathered in item 19. During the 2015-16 school year, did your school or school district have any formalized policies or written documents (e.g., Memorandum of Use, Memorandum of Agreement) that outlined the roles, responsibilities, and expectations of sworn law enforcement officers (including School Resource Officers) at school? 16. 18. If an officer works full-time across various schools in the district, please count this officer as \"part-time\" for your school. Do not include security guards or other security personnel who are not sworn law enforcement in your response to this item; information on additional security staff is gathered in item 19. a. Aside from School Resource Officers or other sworn law enforcement officers, how many additional security guards or security personnel were present in your school at least once a week? 19. If a security guard or other security personnel works full-time across various schools in the district, please count this person as \"part-time\" for your school. If none, please place an \"X\" in the None box. "}, {"section_title": "20.", "text": "Diagnostic assessment* for mental health disorders* a. During the 2015-16 school year, were the following mental health services available to students under the official responsibilities of a licensed mental health professional*? Check \"Yes\" or \"No\" for each type of service available to students, regardless of whether the service was used this school year. 22. During the 2015-16 school year, did your school or school district provide any of the following for classroom teachers or aides? Check \"Yes\" or \"No\" on each line. "}, {"section_title": "23.", "text": "Lack of or inadequate teacher training in classroom management a. State or district policies on discipline and safety other than those for special education students* m."}, {"section_title": "304", "text": ""}, {"section_title": "2", "text": "During the 2015-16 school year, has there been at least one incident at your school* that involved a shooting (regardless of whether anyone was hurt)? Please include those incidents that occurred at school*, regardless of whether a student or non-student used the firearm*."}, {"section_title": "25", "text": ". "}, {"section_title": "26.", "text": "Rape* or attempted rape* a. Please record the number of incidents that occurred at school* during the 2015-16 school year for the offenses listed below. (NOTE: The number in column 1 should be greater than or equal to the number in column 2.) The number of incidents, not the number of victims or offenders. Recorded incidents, regardless of whether any disciplinary action was taken. Recorded incidents, regardless of whether students or non-students were involved. Incidents occurring before, during, or after normal school hours. If none, please place an \"X\" in the None box. Sexual assault* other than rape* (include threatened rape*) b. Robbery* (taking things by force) c. 28. During the 2015-16 school year, how many hate crimes* occurred at your school*? If none, please place an \"X\" in the None box. 30. How many times during the 2015-16 school year were activities disrupted by unplanned fire alarms (i.e., false alarms)? Number of unplanned fire alarms 370 31. Excluding planned and unplanned fire alarms, how many times during the 2015-16 school year were activities disrupted by other actions, such as death threats, bomb threats, or chemical, biological, or radiological threats? If none, please place an \"X\" in the None box. 27. Please record the number of arrests that occurred at your school during the 2015-16 school year."}, {"section_title": "Number of disruptions", "text": "Please include all arrests that occurred at school*, regardless of whether a student or non-student was arrested. 29. To the best of your knowledge, were any of these hate crimes* motivated by the offender's bias against the following characteristics?"}, {"section_title": "Number of arrests", "text": "Check \"Yes\" or \"No\" on each line. "}, {"section_title": "32.", "text": "Student racial/ethnic tensions a. Happens at least once a month 374  5Check one response on each line. Student harassment of other students based on sexual orientation* To the best of your knowledge, thinking about problems that can occur anywhere (both at your school and away from school), how often do the following occur?"}, {"section_title": "33.", "text": "Cyberbullying* among students who attend your school a. Happens at least once a month 389  5Check one response on each line. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? YES NO 390 1 2 Does your school allow for use of the following? If \"Yes,\" was the action used this school year? Removal with no continuing school services for at least the remainder of the school year Removal with school-provided tutoring/athome instruction for at least the remainder of the school year Transfer to a specialized school* for disciplinary reasons *Please use the definition on pages 2 and 3. 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 416 1 2 1 2 420 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 i. ii 35. During the 2015-16 school year, how many students were involved in committing the following offenses, and how many of the following disciplinary actions were taken in response? Please follow these guidelines when determining the number of offenses and disciplinary actions: If a student was disciplined more than once, please count each offense separately (e.g., a student who was suspended five times would be counted as five suspensions). 36. During the 2015-16 school year, how many of the following occurred? Students were removed from your school without continuing services for at least the remainder of the school year for disciplinary reasons. (NOTE: This number should be greater than or equal to the sum of entries in item 35, column 2.) a."}, {"section_title": "Total number", "text": "If none, please place an \"X\" in the None box."}, {"section_title": "518", "text": "Students were transferred to specialized schools* for disciplinary reasons. (NOTE: This number should be greater than or equal to the sum of entries in item 35, column 3.) b. *Please use the definition on pages 2 and 3. If none, please place an \"X\" in the None box. If more than one student was involved in an incident, please count each student separately when providing the number of disciplinary actions. If a student was disciplined in two different ways for a single infraction (e.g., the student was both suspended and referred to counseling), count only the most severe disciplinary action that was taken. If a student was disciplined in one way for multiple infractions, record the disciplinary action for only the most serious offense. \u25b2 \u25bc \u25bc \u25b2 \u00a7,1\"\u00a4 111601 40."}, {"section_title": "538", "text": "Count going to lunch and then returning to the same or a different classroom as two classroom changes. Do not count morning arrival or afternoon departure. If none, please place an \"X\" in the None box. If none, please place an \"X\" in the None box. If none, please place an \"X\" in the None box. Typical number of classroom changes \u25b2 \u25bc \u25bc \u25b2 \u00a7,2!\u00a4 111700 How would you describe the crime level in the area(s) in which your students live?"}, {"section_title": "41.", "text": "Check one response. During the 2015-16 school year, how many students transferred to or from your school after the start of the school year? Please report on the total mobility, not just transfers due to disciplinary actions. (NOTE: This number should be greater than or equal to the number of students who were transferred for disciplinary reasons, as reported in item 36b.) If a student transferred more than once in the school year, count each transfer separately.   This page is intentionally left blank              "}, {"section_title": "B-15", "text": "This page is intentionally left blank Coded title/position of respondent # of years respondent at the school C0232 # of full-time security guards C0234 # of part-time security guards C0236 # of full-time School Resource Officers C0238 # of part-time School Resource Officers C0240 # of full-time sworn law enforcement officers-not SROs C0242 # of part-time sworn law enforcement officers-not SROs C0310 # of rapes/attempted rapes -total C0312 # of rapes reported to police C0314 # of sexual assaults other than rape -total C0316 # of sexual assaults other than rape reported to police C0318 # of robberies with weapon -total C0320 # of robberies with weapon reported to police C0322 # of robberies without weapon -total C0324 # of robberies without weapon reported to police C0326 # of attacks with weapon -total # of attacks with weapon reported to police C0330 # of attacks without weapon -total C0332 # of attacks without weapon reported to police C0334 # of threats of attack with weapon -total C0336 # of threats of attack with weapon reported to police C0338 # of threats of attack without weapon -total C0340 # of threats of attack without weapon reported to police C0342 # of incidents theft/larceny -total C0344 # of incidents theft/larceny reported to police C0346 # of possession of firearms -total C0348 # of possession of firearms reported to police C0350 # of possession knife/sharp object -total C0352 # of possession knife/sharp object reported to police C0354 # of distribution, possession, or use of drugs -total C0355 # of distribution, possession, or use of prescription drugs -total C0356 # of distribution, possession, or use of drugs reported to police C0357 # of distribution, possession, or use of prescription drugs reported to police C0358 # of distribution, possession, or use of alcohol -total C0360 # of distribution, possession, or use of alcohol reported to police C0362 # of incidents of vandalism -total C0364 # of incidents of vandalism reported to police C0370 # of times school disrupted due to unplanned fire alarms C0372 # of times school disrupted (e.g., bomb, chemical, radiological, death threats) C0458 # students involved in use/possession firearm/explosive device -total C0460 # of removals for firearm use/possession C0462 # of transfers for firearm use/possession C0464 # of suspensions for firearm use/possession C0466 # of other actions for firearm use/possession C0468 # of students involved in use/possession weapon (other than firearm/explosive device) -total C0470 # of removals for weapon use C0472 # of transfers for weapon use C0474 # of suspensions for weapon use C0476 # of other actions for weapon use C0478 # students involved in distribution/possession/use illegal drugs -total C0480 # of removals for distribution/possession/use -illegal drugs C0482 # of transfers for distribution/possession/use -illegal drugs C0484 # of suspensions for distribution/possession/use -illegal drugs C0486 # of other actions for distribution/possession/use -illegal drugs C0488 # of students involved in distribution/possession/use alcohol -total C0490 # of removals for distribution/possession/use -alcohol C0492 # of transfers for distribution/possession/use -alcohol C0494 # of suspensions for distribution/possession/use -alcohol Teacher (staff) full-time equivalency (categorical) STPFTE16 Students per teaching staff full-time-equivalency STRCAT Student/teaching staff ratio (categorical) Imputation flags IC0232 Imputation flag for C0232 IC0234 Imputation flag for C0234 IC0236 Imputation flag for C0236  The 2015-16 School Survey on Crime and Safety public-use codebook was designed to accompany this user's manual to give the analyst a brief overview of the survey variables, composite variables, CCD variables, imputation flags, final weight, and replicate weights. For all categorical variables, unweighted and weighted frequencies and their associated percentages are provided. Descriptive statistics, including minimum value, maximum value, mean, standard deviation, and median, are provided for continuous variables.    "}, {"section_title": "D-2", "text": ""}, {"section_title": "SSOCS 2016 Codebook", "text": ""}, {"section_title": "----------------", "text": "\n34a_1. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Removal with no continuing school services for at least the remainder of the school year -allowed \n34a_2. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Removal with no continuing school services for at least the remainder of the school year -used \n34b_1. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Removal with school-provided tutoring/at-home instruction for at least the remainder of the school year -allowed 34c_1. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Transfer to a specialized school* for disciplinary reasons -allowed \n34c_2. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Transfer to a specialized school* for disciplinary reasons -used \n34d_1. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Transfer to another regular school for disciplinary reasons -allowed \n34d_2. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Transfer to another regular school for disciplinary reasons -used \n34e_i_1. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Outof-school suspension or removal for less than the remainder of the school year with no curriculum/services provided -allowed \n34e_i_2. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Outof-school suspension or removal for less than the remainder of the school year with no curriculum/services provided -used \n34e_ii_1. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Outof-school suspension or removal for less than the remainder of the school year with curriculum/services provided -allowed \n34e_ii_2. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Outof-school suspension or removal for less than the remainder of the school year with curriculum/services provided -used \n34f_i_1. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Inschool suspension for less than the remainder of the school year with no curriculum/services provided -allowed \nPage 59 D-61 34f_i_2. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Inschool suspension for less than the remainder of the school year with no curriculum/services provided -used \n34f_ii_1. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Inschool suspension for less than the remainder of the school year with curriculum/services provided -allowed \n34f_ii_2. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Inschool suspension for less than the remainder of the school year with curriculum/services provided -used \n34g_1. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Referral to a school counselor -allowed \n34g_2. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Referral to a school counselor -used \n34h_1. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Assignment to a program (during school hours) designed to reduce disciplinary problems -allowed \n34h_2. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Assignment to a program (during school hours) designed to reduce disciplinary problems -used \n34i_1. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Assignment to a program (outside of school hours) designed to reduce disciplinary problems -allowed \n34i_2. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Assignment to a program (outside of school hours) designed to reduce disciplinary problems -used \n34j_1. During the 2015-16 school year, did your school allow for the use of the following disciplinary actions? If \"yes,\" were the actions used this school year? Loss of school bus privileges due to misbehavior -allowed      \n"}, {"section_title": "INTRODUCTION", "text": ""}, {"section_title": "Purpose of the School Survey on Crime & Safety (SSOCS)", "text": "The SSOCS is the U.S. Department of Education, National Center for Education Statistics' (NCES) primary source of school-level data on crime and safety. It provides estimates of school crime, discipline, disorder, programs, and policies. The SSOCS questionnaire asks principals to report on a variety of topics related to crime and safety, including the following: \u2022 Characteristics of school policies and procedures; \u2022 School violence prevention programs and practices; \u2022 Use of law enforcement or security services; \u2022 Frequency of criminal incidents at schools; \u2022 Frequency of incidents reported to police or law enforcement; \u2022 Frequency of hate-related and gang-related incidents; \u2022 Disciplinary problems and disciplinary actions; \u2022 Mental health services available to students at school, and \u2022 Other school characteristics related to school crime.\nThe SSOCS is the U.S. Department of Education, National Center for Education Statistics' (NCES) primary source of school-level data on crime and safety. SSOCS was previously administered in 2000, 2004, 2006, 2008, and 2010. We anticipate a collection every two years. SSOCS provides estimates of school crime, discipline, disorder, and programs and policies that the school implements to promote a safe environment. The SSOCS questionnaire asks principals to report on a variety of topics related to crime and safety, including the following: \uf0b7 Characteristics of school policies and procedures; \uf0b7 School violence prevention programs and practices; \uf0b7 Use of law enforcement or security staff; \uf0b7 Frequency of criminal incidents at schools; \uf0b7 Frequency of incidents reported to police or law enforcement; \uf0b7 Frequency of hate-related incidents; \uf0b7 Disciplinary problems and disciplinary actions; \uf0b7 Mental health services available to students at school; and \uf0b7 Other school characteristics related to school crime."}, {"section_title": "Survey Design and Sample Size", "text": "The SSOCS is a nationally representative cross-sectional survey of about 3,550 public elementary and secondary schools. The SSOCS sample is large enough to provide national estimates of all public schools, while taking into account the level of instruction, type of location, and size of the student enrollment. The SSOCS is a self-administered survey. Paper questionnaires are mailed to school principals and a follow-up of non-respondents is conducted by telephone. The SSOCS is administered towards the end of the school year to allow principals to report the most complete information possible. For the 2016 SSOCS, an advance e-mail, as well as several e-mail reminders, will be sent to school principals. Therefore, you may receive incoming calls in response to e-mails as well as in response to mailings.\nThe SSOCS is a nationally representative cross-sectional survey of about 3,550 public elementary and secondary schools. The SSOCS sample is large enough to provide national estimates of all public schools, while taking into account the level of instruction, type of location, and size of the student enrollment. The SSOCS is a self-administered survey. Paper questionnaires are mailed to school principals and a follow-up of non-respondents is conducted by telephone. The SSOCS is administered towards the end of the school year to allow principals to report the most complete information possible. For the 2016 SSOCS, an advance e-mail, as well as several e-mail reminders, will be sent to school principals. Therefore, you may receive incoming calls in response to e-mails as well as in response to mailings."}, {"section_title": "SSOCS Telephone Operations", "text": "\n"}, {"section_title": "Incoming calls", "text": "Principals or other school staff may call in response to receiving the advance letter or the initial questionnaire package or may call in response to receiving the advance or the reminder e-mail.\nPrincipals or other school staff may call in response to receiving the advance letter, the initial questionnaire package, the advance email, or reminder e-mail. Page 4 8042016-202-T 3/1/2016"}, {"section_title": "Reminder", "text": "\u2022 Phase 1: The purpose is to remind schools to return their completed SSOCS questionnaire. However, you may complete the questionnaire over the phone at the respondent's request. \u2022 Phase 2: The purpose is still to remind schools to return their completed SSOCS questionnaire; however, you may complete the questionnaire over the phone at the respondent's request."}, {"section_title": "Non-response Follow-up", "text": "The purpose of Non-response Follow-up (NRFU) is to complete the questionnaire over the phone with the respondent."}, {"section_title": "Failed Edit Follow-up", "text": "The purpose of Failed Edit Follow-up (FEFU) is to call schools that have returned the SSOCS questionnaire to verify that their answers to critical questions are correct. This is done when the answer provided falls outside of the expected range or is inconsistent with other answers. During these callbacks, you may also be asking the respondent questions that he/she left blank."}, {"section_title": "II. CONCEPTS Challenges Collecting Data from Schools", "text": "\u2022 Principals have many responsibilities and are pressed for time to take surveys. \u2022 Schools are a heavily studied population -this survey may be one of several that the principal has on his or her desk. \u2022 Schools in many areas have faced budget cuts and have had to reduce personnel. Therefore, school staff members may have more responsibilities than they used to and less time to complete \"extra\" tasks. \u2022 Media coverage of No Child Left Behind (NCLB) legislation has been negative. Some principals may link the SSOCS to the NCLB provision regarding \"persistently dangerous schools.\" See Section III and Frequently Asked Question O. \u2022 Information may be perceived as sensitive, since some respondents may think it reflects negatively on the school. \u2022 Principals often have office staff to screen their calls.\n\uf0b7 Principals have many responsibilities and are pressed for time to take surveys. \uf0b7 Schools are a heavily studied population -this survey may be one of several that the principal has on his or her desk. \uf0b7 Schools in many areas have faced budget cuts and have had to reduce personnel. Therefore, school staff members may have more responsibilities than they used to and less time to complete \"extra\" tasks. \uf0b7 Schools may link the SSOCS to the No Child Left Behind (NCLB) and the Every Student Succeeds Act (ESSA) legislation. See Section III and Frequently Asked Question O for more information on NCLB and ESSA. \uf0b7 Information may be perceived as sensitive, since some respondents may think it reflects negatively on the school. \uf0b7 Principals often have office staff to screen their calls."}, {"section_title": "Special Permission Districts", "text": "Some school districts must approve the research project before data can be collected at their school(s). Some \"special permission\" districts were already identified and research applications were completed in the fall of 2015; however, some additional schools you talk to may notify you that they require \"special permission.\" If a school staff member calls to tell you that their Local Education Agency (LEA) or School District requires that permission be received to complete the SSOCS, ask who should be contacted at the school district to apply to conduct the SSOCS. Probe for as much information as possible, including contact name, contact phone number, and the type of approval process (e.g., written or verbal application).\nSome school districts must approve the research project before data can be collected at their school(s). Some \"special permission\" districts were already identified and research applications were completed in the fall of 2015; however, some additional schools you talk to may notify you that they require \"special permission.\" If a school staff member tells you that their Local Education Agency (LEA) or school district requires that permission be received to complete the SSOCS, ask who should be contacted at the school district to apply to conduct the SSOCS. Probe for as much information as possible, including contact name, contact phone number, and the type of approval process (e.g., written or verbal application). Note that if we are already aware that this school is part of a special district, that information will be provided on the cover page of the SSOCS-26 form."}, {"section_title": "Late Mail Returns (LMRs)", "text": "Once questionnaires begin to be received, your supervisor will receive a list of completed questionnaires daily. Although you will not be calling schools during this time, and therefore do not need to \"pull\" LMRs from the workload, respondents may want to verify that their completed questionnaire was received. Use the list of completed questionnaires to verify that their questionnaire was received. If it was not received, let the respondent know that sometimes it takes longer than expected to receive a questionnaire and that we should receive it soon. Some respondents may wish to be alerted when their questionnaire is received. Take their information so that you or another interviewer can contact them when the questionnaire is received. Be sure to thank the respondent for completing and returning the questionnaire.\nYour supervisor will receive a list of completed questionnaires daily. SSOCS-26 forms for schools that have returned a completed questionnaire will be pulled from the workload. You may still receive incoming calls from respondents who wish to verify that their completed questionnaire was received. Use the list of completed questionnaires to verify that their questionnaire was received. If it was not received, let the respondent know that sometimes it takes longer than expected to receive a questionnaire, and that we should receive it soon. Some respondents may wish to be alerted when their questionnaire is received. Take their information so that you or another interviewer can contact them when the questionnaire is received. Be sure to thank the respondent for completing and returning the questionnaire. "}, {"section_title": "III. NO CHILD LEFT BEHIND and EVERY STUDENT SUCCEEDS ACT", "text": "The No Child Left Behind Act of 2001 (NCLB), was replaced by the Every Student Succeeds Act (ESSA) on December 10, 2015. You may be asked how the SSOCS relates to either of these acts. Below you will find background information on both so you are better prepared to answer respondent questions about either act."}, {"section_title": "SSOCS and NCLB and ESSA", "text": "The Passage of the Safe and Drug-Free Schools and Communities Act of 1994 gave the Department the ability to support drug and violence prevention programs, and included an impact evaluation component and provision for NCES to collect data on the frequency, seriousness, and incidence of violence in elementary and secondary schools. Even with this provision, NCES only collected this data on an ad-hoc basis. It wasn't until the perceived increase in school shootings, including the Columbine shooting in 1999, where it was determined a recurring survey collecting crime and safety data was imperative to inform policy-makers on appropriate policies and programs to implement in schools. In addition to the aforementioned legislation, NCES is authorized to collect this data by the Education Sciences Reform Act of 2002. However, while these are the authoritative legislation that provide us the ability to collect these data, it is important to know this does not mean this collection is mandatory. There may be an assumption that all data collections sponsored by the U.S. Department of Education are mandatory. While some definitely are, especially those under the umbrella of NCLB and the newly reauthorized ESSA, SSOCS is not under this umbrella. SSOCS does continue to inform the Office of Safe and Healthy Students on the frequency, seriousness, and incidence of violence in schools but is no way a part of any mandatory collection. SSOCS was previously administered in 2000, 2004, 2006, 2008 and 2010. We anticipate a collection every two years. Information gathered in the SSOCS is confidential and is reported in aggregate to protect the identity of participating schools. Principals can be rest assured that this is also how the data are shared with program offices within the Department, outside of NCES. In addition, schools were not selected to participate in the SSOCS because they were deemed \"persistently dangerous\" as defined under NCLB. Furthermore, the results of the SSOCS 2016 will not flag participating schools as being \"persistently dangerous\" as determined by ESSA. Participating principals may object to filling out the survey because they feel as though they are repeating data reported under the former NCLB mandatory collections sponsored by other offices within the Department, such as the Civil Rights Data Collection. While we are sympathetic to their frustration, the fact remains that we are not privy to these data, and these data are collected on a later schedule than SSOCS, which does not meet the needs of researchers needing this data to inform program evaluations and policy-related decisions. It is important to stress to responding principals that not only is the SSOCS an important survey, it is the only national survey of its kind. Rather than relying on states to define certain crimes, the SSOCS uses common definitions across states to produce national estimates of school crime. Principals participating in 2000, 2004, 2006, 2008 and 2010 reported that they found the survey to be helpful because it allowed them to reflect upon incidents of crime and safety and direct policies and programs designed to prevent them. We realize that principals may see our survey as another nuisance in this era of standards and evaluation, but we believe good research can drive good policy. The SSOCS is therefore in the best interest of participating schools because it allows their particular experiences to be recorded, in aggregate, for researchers and policy-makers who will determine the next generation of education legislation."}, {"section_title": "IV. REFUSAL AVERSION AND CONVERSION Aversion vs. Conversion", "text": "Refusal Aversion is the process by which the general interviewing staff AVOIDS refusals with a respondent by practicing good interviewing skills and exhibiting a high degree of professionalism. Refusal Conversion involves contacting cases we have already had contact with, who have refused to participate in the study. When we contact these cases, we will be attempting to complete interviews with them and \"convert\" their refusal through persuasion, active listening techniques, or addressing concerns they may have about participation in the study. Studies have shown that the longer an interviewer can keep a respondent on the phone, the higher the chances of obtaining a complete interview.\nRefusal Aversion is the process by which the general interviewing staff AVOIDS refusals with a respondent by practicing good interviewing skills and exhibiting a high degree of professionalism. Refusal Conversion involves contacting cases we have already had contact with who have refused to participate in the study. When we contact these cases, we will be attempting to complete interviews with them and \"convert\" their refusal through persuasion, active listening techniques, or addressing concerns they may have about participation in the study. Studies have shown that the longer an interviewer can keep a respondent on the phone, the higher the chances of obtaining a complete interview."}, {"section_title": "Keys to Success", "text": "There are many reasons that a respondent may refuse to participate. They may not understand what we are doing or how important the survey is, we may have caught them at a 'bad time' when they are unable or unwilling to speak, or maybe they are simply exercising their right to refuse. If you can determine why they are reluctant to participate you will increase your chance at conversion. \uf0a7 The first key to success is strong communication skills. Pretend you're having a conversation with the respondent. Maintain a tone of confidence in your work and good will towards the person. Watch your delivery and avoid sounding mechanical at all costs. Listen carefully for the respondent's tone, mood, and disposition, and try to vary your tone accordingly. If the person sounds abrupt and cold, use a calm but business-like tone. If the person sounds timid and unsure, use a relaxed, friendly, warm tone. If he/she sounds rushed, speed up a little. If he/she sounds like he/she is used to taking his/her time, slow down. Refusal converters who can vary their tone to match the demeanor of the respondent obtain a higher number of completes and fewer refusals. Without the ability to think quickly and respond with a well thought out, professional response that is warm and courteous, you will not convince the respondent to participate. Have confidence! Be sincere! Listen! You cannot fake these skills. Believe in your ability to convert and you will be surprised at your success rate. \u2022 The second key to success is project knowledge. Possession of thorough and complete knowledge of the study's goals and objectives is vital. Without it, you will be ill prepared to alleviate your respondent's fears and answer their questions. \u2022 The third key to success is knowledge of the case history. Prior to calling a case, develop a strategy based upon the information contained in the Call Record and Comments Section (pertains to the Reminder and NRFU operations). If appropriate, acknowledge that we've called before with, \"Recently we called you about this study. . .\" If the comments indicate a specific reason for the refusal, be prepared to address this issue before you dial the case. If the respondent was concerned about solicitations, say \"We are not selling anything.\" in the introduction. If necessary, review the Frequently Asked Questions (FAQs) and Refusal Responses to determine which answers you are likely to need to convert the case. \u2022 The fourth key to success is the ability to 'think on your feet'. Averting a respondent's refusal during the initial call is more effective than attempting to convert the respondent later. During refusal conversion, respondents may feel pressured because we are calling back to try to gain their cooperation. Because of this, they may throw out comments and questions from all directions, and you have to gracefully field every one, while staying relaxed and confident. Stay focused and be prepared to think quickly and clearly of the most important thing to say to that respondent on the issues they have raised. Avoid the habit of saying the same thing to every respondent. \u2022 Respond only to issues the respondent has raised. This is a very easy rule to remember, but often difficult to follow. If you respond to issues that have not been raised, you are giving your respondent additional ammunition. For example, if the respondent states that they do not have time, it will not help to explain that answers will be kept in confidence. o Sometimes, however, it is helpful to be proactive in sharing information with a respondent. If you sense that they are getting bored, it may be helpful to assure them that the survey is almost done. If the respondent sounds hesitant, tell them a little more about why they are important. A few words of encouragement will go a long way. o Either way, immediately return to reading the survey questions after answering a respondent's question or giving a rebuttal. YOU are in control of the interview and it is more effective to assertively move forward than to passively wait for an indication that it is okay to continue. \u2022 As with customer service, the respondent is always right. Do not argue with a respondent or lose your composure. Know when to accept a refusal. Never hang up on respondents, even if they are being abusive, without first thanking them for their time. Always conduct yourself in a professional, courteous manner regardless of how the respondent is treating you. There are no exceptions to this rule. \u2022 Know when it's over. If the respondent understands the reason for the call and insists that they do not want to participate and you have given your best effort at a strong conversion attempt, let it go. Do not force the issue and anger the respondent. Always remember that participation is voluntary. Refusal converters do not make respondents feel coerced into providing information. Every respondent has a right to refuse to participate in the study or to refuse to answer any question in the study. \u2022 Persuasion is a must. Remind yourself on every call to focus on what each respondent may need to know about the study in order to feel good about participating. Conversion is most effective when you believe you can persuade the respondent to participate. Make conversion a conversation as often as possible-you and the respondent discussing a worthwhile goal you can only accomplish if you work together. o You need to convince the respondent that they want to do the survey. Don't tell them that it is an important study; you should already be conveying this in your tone and demeanor. Tell them why the study is important and what they will gain from participating (see FAQ H and T). Tell them what the problem is that the study is addressing and how they will be part of the solution. Tell them many people find the survey interesting and enjoy doing it. o Pay attention to which bits of information are most effective at converting respondents. With practice you will find what works best for you, but don't be afraid to try a new tactic. \u2022 Countering refusals requires calm and understanding. As you listen to your respondent, be aware of not only the words, but also the intensity of voice, pace of words, and tonal expression. Rather than jumping in with your rebuttal and appearing aggressive or rude, take it slowly and calmly. Let respondents say what they have to say without interrupting them, then retreat with a positive tone, recognizing the respondent's objections. Warmth and courtesy go a very long way! \u2022 Be ready to probe for the reason they refused. The most difficult refusal is the respondent who 'just doesn't want to.' Perhaps this person cannot think of a good reason to decline, or maybe the respondent understands that participation is voluntary and is exercising the right to refuse. It is nearly impossible to counter a 'no reason' with a reasonable reply. Don't be afraid to speak to your respondent conversationally. They are only human and the worst that can happen is they will say no. Talk to them and find out why they do not want to participate. \u2022 Be prepared to listen. Good listening skills are important for conducting good interviews. It is doubly important that these skills are used when gaining respondent cooperation. Actively listen to what the respondent is saying in words and in tone. Active listening means that you hear and remember what the respondent said in such detail you could write it down or repeat it back if necessary. Listen carefully to everything the respondent has to say, acknowledge that the respondent raised many issues, then start with the one that seems most important. To let the respondent know you are sincerely listening, an effective measure is to rephrase and repeat back what you heard: o \"I understand that you are busy; we could call you back at another time when it is more convenient for you, or we can start the interview now. I'll move through the questions as quickly as possible.\" o \"I understand that you are concerned about confidentiality, however, I can assure you that confidentiality is mandated by law.\" \u2022 Vary your tone to match the demeanor of the respondent. Listen carefully for the respondent's tone, mood, and disposition, and try to vary your tone accordingly. o If the person sounds abrupt and cold, use a calm but business like tone. o If the person sounds timid and unsure, use a relaxed, friendly, warm tone. o If the person sounds rushed, speed up a little. o If the person sounds like they are taking their time, slow down a little. You will have fewer refusals and more completes if you can work with the respondent. \u2022 Use the resources available to you. We have numerous ways in which a respondent can verify that the study is legitimate and an important survey, for example, the respondent may call headquarters at 1-800-221-1204 or visit the survey website at http://nces.ed.gov/surveys/ssocs/. A respondent who is reluctant to participate may change their mind when they can confirm legitimacy and call us. Sometimes, just offering all the sources of legitimacy is enough to convince them that we are! \u2022 Leave good comments. Writing detailed, accurate comments on each call informs and prepares other interviewers who may deal with that case next and it properly documents what happened when you called.\nThere are many reasons that a respondent may refuse to participate. They may not understand what we are doing or how important the survey is, we may have caught them at a 'bad time' when they are unable or unwilling to speak, or maybe they are simply exercising their right to refuse. If you can determine why they are reluctant to participate you will increase your chance at conversion. Refusal converters who can vary their tone to match the demeanor of the respondent obtain a higher number of completes and fewer refusals. Without the ability to think quickly and respond with a well-thought-out, professional response that is warm and courteous, you will not convince the respondent to participate. Have confidence! Be sincere! Listen! You cannot fake these skills. Believe in your ability to convert, and you will be surprised at your success rate. \uf0b7 The second key to success is project knowledge. Possession of thorough and complete knowledge of the study's goals and objectives is vital. Without it, you will be ill prepared to alleviate your respondents' fears and answer their questions. \uf0b7 The third key to success is knowledge of the case history. Prior to calling a case, develop a strategy based upon the information contained in the Call Record and Comments Section. If appropriate, acknowledge that we've called before with, \"Recently we called you about this study. . .\" If the comments indicate a specific reason for the refusal, be prepared to address this issue before you dial the case. If the respondent was concerned about solicitations, say \"We are not selling anything.\" in the introduction. If necessary, review the Frequently Asked Questions and Refusal Responses to determine which answers you are likely to need to convert the case. \uf0b7 The fourth key to success is the ability to 'think on your feet'. Averting a respondent's refusal during the initial call is more effective than attempting to convert the respondent later. During refusal conversion, respondents may feel pressured because we are calling back to try to gain their cooperation. Because of this, they may throw out comments and questions from all directions, and you have to gracefully field every one, while staying relaxed and confident. Stay focused and be prepared to think quickly and clearly of the most important thing to say to that respondent on the issues they have raised. Avoid the habit of saying the same thing to every respondent. Page 9 8042016-202-T 3/1/2016 \uf0b7 Respond only to issues the respondent has raised. This is a very easy rule to remember, but often difficult to follow. If you respond to issues that have not been raised, you are giving your respondent additional ammunition. For example, if the respondent states that they do not have time, it will not help to explain that answers will be kept in confidence. \uf0b7 As with customer service, the respondent is always right. Do not argue with a respondent or lose your composure. Know when to accept a refusal. Never hang up on respondents, even if they are being abusive, without first thanking them for their time. Always conduct yourself in a professional, courteous manner regardless of how the respondent is treating you. There are no exceptions to this rule. \uf0b7 Know when it's over. If the respondent understands the reason for the call and insists that they do not want to participate and you have given your best effort at a strong conversion attempt, let it go. Do not force the issue and anger the respondent. Always remember that participation is voluntary. Refusal converters do not make respondents feel coerced into providing information. Every respondent has a right to refuse to participate in the study or to refuse to answer any question in the study. \uf0b7 Persuasion is a must. Remind yourself on every call to focus on what each respondent may need to know about the study in order to feel good about participating. Conversion is most effective when you believe you can persuade the respondent to participate. Make conversion a conversation as often as possible-you and the respondent discussing a worthwhile goal you can only accomplish if you work together. o You need to convince the respondent that they want to do the survey. Don't tell them that it is an important study; you should already be conveying this in your tone and demeanor. Tell them why the study is important and what they will gain from participating (see FAQ H, T, and U). Tell them what the problem is that the study is addressing and how they will be part of the solution. Tell them many people find the survey interesting and enjoy doing it. o Pay attention to which bits of information are most effective at converting respondents. With practice you will find what works best for you, but don't be afraid to try a new tactic. \uf0b7 Countering refusals requires calm and understanding. As you listen to your respondent, be aware of not only the words, but also the intensity of voice, pace of words, and tonal expression. Rather than jumping in with your rebuttal and appearing aggressive or rude, take it slowly and calmly. Let respondents say what they have to say without interrupting them, then retreat with a positive tone, Page 10 8042016-202-T 3/1/2016 recognizing the respondent's objections. Warmth and courtesy go a very long way! \uf0b7 Be ready to probe for the reason they refused. The most difficult refusal is the respondent who 'just doesn't want to.' Perhaps this person cannot think of a good reason to decline, or maybe the respondent understands that participation is voluntary and is exercising the right to refuse. It is nearly impossible to counter a 'no reason' with a reasonable reply. Don't be afraid to speak to your respondent conversationally. They are only human and the worst that can happen is they will say no. Talk to them and find out why they do not want to participate. \uf0b7 Be prepared to listen. Good listening skills are important for conducting good interviews. It is doubly important that these skills are used when gaining respondent cooperation. Actively listen to what the respondent is saying in words and in tone. Active listening means that you hear and remember what the respondent said in such detail you could write it down or repeat it back if necessary. Listen carefully to everything the respondent has to say, acknowledge that the respondent raised many issues, then start with the one that seems most important. To let the respondent know you are sincerely listening, an effective measure is to rephrase and repeat back what you heard. For example, you might say the following: o \"I understand that you are busy; we could call you back at another time when it is more convenient for you, or we can start the interview now. I'll move through the questions as quickly as possible.\" o \"I understand that you are concerned about confidentiality, however, I can assure you that confidentiality is mandated by law.\" \uf0b7 Use the resources available to you. We have numerous ways in which a respondent can verify that the study is legitimate and an important survey; for example, the respondent may call headquarters at 1-800-221-1204 or visit the survey website at http://nces.ed.gov/surveys/ssocs/. A respondent who is reluctant to participate may change their mind when they can confirm legitimacy. Sometimes, just offering all the sources of legitimacy is enough to convince them that we are! \uf0b7 Leave good comments. Writing detailed, accurate comments on each call informs and prepares other interviewers who may deal with that case next, and it properly documents what happened when you called."}, {"section_title": "V. MATERIALS", "text": "\u2022 Copies of the correspondence sent to schools (SSOCS-10(L), SSOCS-11(L), SSOCS-12(L)/SSOCS-12(L)S, SSOCS-13(L), and SSOCS-13(I)) are also included in your training packet. \u2022 Pencils \u2022 Frequently Asked Questions and Refusal Responses (see Section VI; this will also be provided as a Job Aid). \u2022 Call Log: You will receive a call log for recording information about each incoming call. VI."}, {"section_title": "FREQUENTLY ASKED QUESTIONS & REFUSAL RESPONSES", "text": "A. Why did our school get selected? From all the public schools in the United States, we selected a random stratified sample of about 3,550 schools that represent the nation for the 2016 School Survey on Crime and Safety. Your school happened to be one of those selected. Your responses will represent schools with similar demographics that were not selected for the survey."}, {"section_title": "B. What kinds of questions does the School Survey on Crime & Safety ask?", "text": "\u2022 Frequency and types of crimes at schools, including homicide, rape, sexual battery, attacks with or without weapons, robbery, theft, and vandalism; \u2022 Frequency and types of disciplinary actions such as expulsions, transfers, and suspensions for selected offenses; \u2022 Perceptions of other disciplinary problems such as bullying, verbal abuse, and disorder in the classroom; \u2022 Description of school policies and programs concerning crime and safety; \u2022 Description of the pervasiveness of student and teacher involvement in efforts that are intended to prevent or reduce school violence; \u2022 Mental health services available to students at school; and \u2022 General school characteristics."}, {"section_title": "C. What is the purpose of this survey?", "text": "The SSOCS is the primary source of school-level data on crime and safety for the U.S. Department of Education. This study collects information on school crime and safety from school principals in elementary and secondary schools across the United States. As an ongoing survey, the SSOCS measures changes over time on key issues. Gathering this information will help schools compare their policies and programs to schools nationwide. It will also help researchers and policymakers identify trends in crime and safety issues across time and identify emerging problems or issues."}, {"section_title": "D. Why should I participate in this survey?", "text": "Although this is a voluntary survey, your cooperation is essential to make the results of this survey comprehensive, accurate, and timely. Policymakers and educational leaders rely on data from this survey to inform their decisions concerning school programs and policies to reduce crime. Since it is a sample survey, your responses represent the responses of many schools that serve similar student populations. Higher response rates give us confidence that the findings are accurate."}, {"section_title": "E. Who is conducting this survey?", "text": "The U.S. Census Bureau is conducting this survey for the National Center for Education Statistics ( As part of the Department of Education, the National Center for Education Statistics fulfills a Congressional mandate to: \u2022 Collect, collate, analyze and report complete statistics on the condition of American education; \u2022 Conduct and publish reports; and \u2022 Review and report on education activities internationally."}, {"section_title": "F. Will my responses be kept confidential?", "text": "I can assure you that no identifying information will be used by anyone besides those working on the School Survey on Crime and Safety project. The results from the survey will only be reported as combined totals across the thousands of schools who answer the survey, never as individual results. Your answers may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose except as required by law [Education Sciences Reform Act of 2002(ESRA 2002 20 U.S.C., \u00a7 9573]. Reports of the findings from the survey will not identify participating districts, schools, or staff. Individual responses will be combined with those from other participants to produce summary statistics and reports.\nI can assure you that no identifying information will be used by anyone besides those working on the School Survey on Crime and Safety project. The results from the survey will only be reported as combined totals across the thousands of schools who answer the survey, never as individual results. Your answers may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose except as required by law [Education Sciences Reform Act of 2002(ESRA 2002 20 U.S.C., \u00a7 9573]. Reports of the findings from the survey will not identify participating districts, schools, or staff. Individual responses will be combined with those from other participants to produce summary statistics and reports."}, {"section_title": "G. How will my information be reported?", "text": "The information you provide will be combined with the information provided by others in statistical reports. No individual data that links your name, address, or telephone number will be included in the statistical reports.\nThe information you provide will be combined with the information provided by others in statistical reports. No individual data that links your name, address, or telephone number will be included in the statistical reports. Page 22 8042016-202-T 3/1/2016 H. How will these data be used? These data are being collected for the U.S. Department of Education, National Center for Education Statistics (NCES). Results from the study will be used to increase knowledge of policies and programs schools use to address school crime and safety. Results will also show comparisons on crime and safety data across time from the 2000, 2004, 2006, 2008, and 2010 surveys. Summary data from the study will be placed into a public-use dataset for researchers and policy-makers. The dataset is rigorously tested prior to release to ensure no individual schools can be identified. Reports will be published based on the SSOCS data. You will be able to compare your school's problems and policies with those of schools that are similar to yours."}, {"section_title": "H. How will these data be used?", "text": "These data are being collected for the U.S. Department of Education, National Center for Education Statistics (NCES). Results from the study will be used to increase knowledge of policies and programs schools use to address school crime and safety. Results will also show comparisons on crime and safety data across time from the 2000, 2004, 2006, 2008 and 2010 surveys. Summary data from the study will be placed into a public-use dataset for researchers and policy makers. The dataset is rigorously tested prior to release to ensure no individual schools can be identified. Reports will be published based on the SSOCS data. You will be able to compare your school's problems and policies with those of schools that are similar to yours."}, {"section_title": "I. How often is the SSOCS administered?", "text": "The SSOCS was administered in the spring of the 1999-2000, 2003-04, 2005-06, 2007-08 and 2009-2010 school years. The SSOCS will now be administered every two years.\nThe SSOCS was administered in the spring of the 1999-2000, 2003-04, 2005-06, 2007-08 and 2009-10 school years. The SSOCS will now be administered every two years."}, {"section_title": "J. Where can I see the results of the SSOCS?", "text": "Downloadable reports from the 1999-2000, 2003-04, 2005-06, 2007-08 and 2009-10 collection of the SSOCS such as Crime and Safety in America's Public Schools: Selected Findings from the School Survey on Crime and Safety are available at http://nces.ed.gov/surveys/ssocs. Also included on the website is a table library with hundreds of tables that provide estimates on school crime and violence by selected school and student characteristics.\nDownloadable reports from the 1999-2000, 2003-04, 2005-06, 2007-08 and 2009-10 collections of the SSOCS such as Crime and Safety in America's Public Schools: Selected Findings from the School Survey on Crime and Safety are available at http://nces.ed.gov/surveys/ssocs. Also included on the website is a table library with hundreds of tables that provide estimates on school crime and violence by selected school and student characteristics."}, {"section_title": "K. How do I know this survey is legitimate?", "text": "I understand your concern. I am conducting this survey on behalf of the National Center for Education Statistics (NCES). Did you receive a letter from the NCES? An advance letter and the questionnaire were sent to explain the survey. We can send you another questionnaire package if you didn't receive it. You can verify the legitimacy of our survey or to find out more information on the survey's website at www.nces.ed.gov/surveys/ssocs.\nI understand your concern. I am collecting data in this survey on behalf of the National Center for Education Statistics (NCES). Did you receive a letter from the NCES? An advance letter and the questionnaire were sent to explain the survey. We can send you another questionnaire package if you didn't receive it. You can verify the legitimacy of our survey or find out more information on the survey's website at www.nces.ed.gov/surveys/ssocs. "}, {"section_title": "L. Has the survey been endorsed by any professional organizations?", "text": "Yes! The School Survey on Crime and Safety has been endorsed by: \u2022 The length of the survey will depend on the characteristics of your school, but for most people it will take about 52 minutes to complete. That time includes time spent filling out the survey itself, as well as referring to additional information sources for the information requested.\n"}, {"section_title": "N. What information was sent to us?", "text": "\u2022 On February 16th, your school was mailed an advance letter describing the study. \u2022 On February 22nd, the questionnaire was sent via FedEx, addressed to the Principal. \u2022 A packet of information about the study was mailed to your District Superintendent and the Chief State School Officer.\n\uf0b7 On February 16th, your school was mailed an advance letter describing the study. \uf0b7 On February 22nd, the questionnaire was sent via FedEx, addressed to the Principal. \uf0b7 A packet of information about the study was mailed to your District Superintendent and the Chief State School Officer. Each state decides how information will be organized for NCLB/ESSA and states may define terms differently. Therefore, it would be impossible to compare these data at a national level. SSOCS, on the other hand, uses standard definitions across states to create national estimates of school crime and the programs aimed at reducing school crime. Further, since SSOCS is a sample survey, and not a survey of all schools in the nation, it is imperative for us to obtain this information from as many schools as possible in our sample to ensure accuracy in how we report out estimates. The information your school reports will NOT be given to your school district or your state board of education."}, {"section_title": "O. What is the relationship between the SSOCS and the former No Child Left Behind Act of 2001 (NCLB) or the Every Student Succeeds Act (ESSA)?", "text": "The School Survey on Crime and Safety (SSOCS) is NOT related to former No Child Left Behind of 2001 (NCLB) or to the new Every Student Succeeds Act (ESSA). The data for both the SSOCS and NCLB/ESSA are reported to the U.S. Department of Education, so you may see similar types of questions, but they are not the same questions. Each state decides how information will be organized for ESSA and states may define terms differently. Therefore, it would be impossible to compare these data at a national level. SSOCS, on the other hand, uses standard definitions across states to create national estimates of school crime and the programs aimed at reducing school crime. The information your school reports will NOT be given to your school district or your state board of education."}, {"section_title": "P. Can I complete the questionnaire over the phone?", "text": "Prior to Reminder Phase 1: We will be conducting interviews over the phone beginning March 17th. We'll be happy to call you then to conduct the interview with you. Is there a day and time when it would be convenient for us to call you? During Reminder: Yes, we can complete the interview now. Interviewer: Record respondent name, and the appointment day and time; continue to answer respondent's questions. Refer the case to your supervisor after call.\nYes, we can complete the interview now."}, {"section_title": "Q. I don't want to buy anything.", "text": "I assure you we are not trying to sell anything. We are conducting a survey to help the National Center for Education Statistics gather information about school crime and safety across the United States. No information that identifies you or your school will ever be given to any company that is trying to sell products or services to you. In fact, no information about you will be given to anyone besides the National Center for Education Statistics.\nI assure you we are not trying to sell anything. We are conducting a survey to help the National Center for Education Statistics gather information about school crime and safety across the United States. No information that identifies you or your school will ever be given to any company that is trying to sell products or services to you. In fact, no information about you will be given to anyone besides the National Center for Education Statistics."}, {"section_title": "R. This is not a good time!", "text": "I apologize for the inconvenience. We can schedule a better time to call you back. When would be a more convenient day and time for us to reach you? Is there a direct line I can reach you at? S. I'm not interested / I'm too busy. / We do not want to participate we are too busy/we take part in so many other studies! We understand how overloaded schools are and that you probably get a lot of surveys in the mail. However, this is the fifth round of a national study to collect data on school crime and safety. The data from this study will help us in developing a national understanding of crime and safety issues, which rank among the most critical issues faced by U.S. schools. Because providing a safe, disciplined environment is a key responsibility of our school systems, researchers and policymakers need an accurate picture of crime and safety issues at public schools across the country. I understand that your time is limited. However, the data you provide represents other schools in the nation that serve similar student populations and your participation ensures we get an accurate picture for schools like yours across the country.\nI apologize for the inconvenience. We can schedule a better time to call you back. When would be a more convenient day and time for us to reach you? Is there a direct line I can reach you at? Page 25 8042016-202-T 3/1/2016 S. I'm not interested / I'm too busy. / We do not want to participate we are too busy/we take part in so many other studies! We understand how overloaded schools are and that you probably get a lot of surveys in the mail. However, this is the sixth round of a national study to collect data on school crime and safety. The data from this study will help us in developing a national understanding of crime and safety issues, which rank among the most critical issues faced by U.S. schools. Because providing a safe, disciplined environment is a key responsibility of our school systems, researchers and policy-makers need an accurate picture of crime and safety issues at public schools across the country. I understand that your time is limited. However, the data you provide represents other schools in the nation that serve similar student populations, and your participation ensures we get an accurate picture for schools like yours across the country."}, {"section_title": "T. I don't see the importance of this survey!", "text": "Measuring the extent of school crime is important for many reasons. The safety of students and teachers is a primary concern, but the nature and frequency of school crime have other important implications, as well. Safety and discipline are necessary for effective education. In order to learn, students need a secure environment where they can concentrate on their studies. Dealing with school crime requires school resources. Gathering this information should help researchers and policymakers devise strategies to address these problems in our schools. U. We're an elementary school, none of these crimes happen here. Why do we need to fill this out? Even if your school has little to no crime, your responses are important. They aid us in creating an accurate picture of the incidences of school crime in all levels of instruction across the nation. Without your responses, the crime level will appear greater than it actually is. We need to describe the policies and practices of ALL kinds of schools, rather than just large secondary schools where these types of crimes may occur. \nMeasuring the extent of school crime is important for many reasons. The safety of students and teachers is a primary concern, but the nature and frequency of school crime have other important implications, as well. Safety and discipline are necessary for effective education. In order to learn, students need a secure environment where they can concentrate on their studies. Dealing with school crime requires school resources. Gathering this information should help researchers and policy-makers devise strategies to address these problems in our schools. U. We're an elementary school; none of these crimes happen here. Why do we need to fill this out? Even if your school has little to no crime, your responses are important. They aid us in creating an accurate picture of the incidences of school crime in all levels of instruction across the nation. Without your responses, the crime level will appear greater than it actually is. We need to describe the policies and practices of ALL kinds of schools, rather than just large secondary schools where these types of crimes may occur."}, {"section_title": "I. INTRODUCTION", "text": ""}, {"section_title": "Reminder Calls", "text": "The purpose is to remind schools to return their completed SSOCS questionnaire. You may complete the questionnaire over the phone at the respondent's request. Do not offer this option. We would prefer that the respondent complete and return the questionnaire by mail."}, {"section_title": "Non-response Follow-up Calls", "text": "The purpose of Non-response Follow-up (NRFU) is to complete the questionnaire over the phone with the respondent."}, {"section_title": "Failed Edit Follow-up Calls", "text": "The purpose of Failed Edit Follow-up (FEFU) is to call schools that have returned the SSOCS questionnaire to verify that their answers to critical questions are correct. This is done when the answer provided falls outside of the expected range or is inconsistent with other answers. During these callbacks, you may also be asking the respondent questions that he/she left blank."}, {"section_title": "Data collection schedule", "text": "Data collection activity Date Mail advance letter to school principals 2/16 Send advance e-mail to principals 2/22 Fedex initial package containing a letter, questionnaire, brochure, and pen to schools 2/22 Follow-up e-mail to all principals 3/9 Reminder operation phase 1 3/14 -4/1 E-mail reminder 3/23 E-mail reminder and thank you email to responding schools 4/6 Second mailout to nonresponding schools not reached during phase 1 of the Reminder 4/18 Reminder operation phase 2 4/18 -4/22 E-mail reminder 4/27 Failed edit follow-up 5/3 -6/13 Non-response follow-up 5/9 -6/10 E-mail reminder 5/18 E-mail reminder 6/6 Page 5 8042016-202-T 3/1/2016"}, {"section_title": "III. NO CHILD LEFT BEHIND AND EVERY STUDENT SUCCEEDS ACT", "text": "The No Child Left Behind Act of 2001 (NCLB), was replaced by the Every Student Succeeds Act (ESSA) on December 10, 2015. You may be asked how the SSOCS relates to either or both of these acts. Below you will find background information on both acts so you are better prepared to answer respondent questions about either act."}, {"section_title": "SSOCS, NCLB and ESSA", "text": "The passage of the Safe and Drug-Free Schools and Communities Act of 1994 gave the Department of Education the ability to support drug and violence prevention programs. It also included an impact evaluation component and provision for NCES to collect data on the frequency, seriousness, and incidence of violence in elementary and secondary schools. Even with this provision, NCES only collected this data on an ad-hoc basis. It wasn't until the perceived increase in school shootings, including the Columbine shooting in 1999, that it was determined a recurring survey collecting crime and safety data was imperative to inform policy-makers on appropriate policies and programs to implement in schools. In addition to the aforementioned legislation, NCES is authorized to collect this data by the Education Sciences Reform Act of 2002. However, while these two acts are the authoritative legislation that provide NCES the ability to collect these data, it is important to know this does not mean the SSOCS collection is a mandatory data collection. There may be an assumption that all data collections sponsored by the U.S. Department of Education are mandatory. While some are, especially those under the umbrella of NCLB and the newly reauthorized ESSA, SSOCS is not under this umbrella. SSOCS does continue to inform the Office of Safe and Healthy Students on the frequency, seriousness, and incidence of violence in schools but is no way a part of any mandatory collection. Participating principals may object to filling out the survey because they feel as though they are repeating data reported under the former NCLB mandatory collections sponsored by other offices within the Department of Education, such as the Civil Rights Data Collection. While we are sympathetic to their frustration, the fact remains that we are not privy to these data, and these data are collected on a later schedule than SSOCS, which does not meet the needs of researchers using this data to inform program evaluations and policy-related decisions. It is important to stress to responding principals that not only is the SSOCS an important survey, it is the only national survey of its kind. Rather than relying on states to define certain crimes, the SSOCS uses common definitions across states to produce national estimates of school crime."}, {"section_title": "Confidentiality", "text": "Information gathered in the SSOCS is confidential and is reported in aggregate to protect the identity of participating schools. Principals can be rest assured that this is also how Page 7 8042016-202-T 3/1/2016 the data are shared with program offices within the U. S. Department of Education, outside of NCES. In addition, schools were not selected to participate in the SSOCS because they were deemed \"persistently dangerous\" as defined under NCLB. Furthermore, the results of the SSOCS 2016 will not flag participating schools as being \"persistently dangerous\" as determined by ESSA."}, {"section_title": "Benefits of Participating", "text": "Principals participating in the 2000, 2004, 2006, 2008, and 2010 administrations reported that they found the survey to be helpful because it allowed them to reflect upon incidents of crime and safety at their school and to direct policies and programs designed to prevent them. We realize that principals may see our survey as another nuisance in this era of standards and evaluation, but we believe good research can drive good policy. The SSOCS is therefore in the best interest of participating schools because it allows their particular experiences to be recorded, in aggregate, for researchers and policy-makers who will determine the next generation of education legislation."}, {"section_title": "V. COMPLETING THE SSOCS-26", "text": ""}, {"section_title": "Materials", "text": "Page 11 8042016-202-T 3/1/2016 \uf0b7 SSOCS-26 Form o You will receive a preprinted SSOCS-26 form for each school. To make the form easier to use, the school name, principal name (if available), and address information are printed within the text of the questions. Each form provides the script and GO TO instructions you will need for contacting the school, interviewing a knowledgeable respondent, and documenting the call outcome. o A copy of the SSOCS-26 is included in your training packet. \uf0b7 Copies of the correspondence sent to schools (SSOCS-10(L), SSOCS-11(L), SSOCS-12(L)/SSOCS-12(L)S, SSOCS-13(L), and SSOCS-13(I)) were provided as part of the training package for the Incoming Call Operation. Request copies of the correspondence if you do not have copies. The text that was used in the advance e-mail, as well as dates and planned text of the follow-up e-mails and letters that will be sent to nonrespondents, will be provided for your reference."}, {"section_title": "\uf0b7 Pencils", "text": "\uf0b7 Frequently Asked Questions and Refusal Responses (see Section IX, beginning on page 20; this will also be provided as a Job Aid) \uf0b7 Call Outcome Codes (see Section VIII, beginning on page 18; this will also be provided as a Job Aid) \uf0b7 Call Log from the Incoming Calls Operation -a call log may be attached to some of the SSOCS-26 forms OR notes may have been copied from the Call Log to the notes section of the SSOCS-26."}, {"section_title": "Form Overview", "text": "The SSOCS-26 form will be used for the Reminder and Non-response Follow-up (NRFU) operations. This will enable you to always have the most up-to-date information for the case. During the first phase of the Reminder operation, you will complete section A and either section B, C, or D. You may also complete section H or section I if the respondent refuses or tells you that they need their school district's/local education agency's (LEA) approval prior to completing the survey. \uf0b7 In section A, you will verify you have reached the correct school, verify the school's physical address, and introduce the survey. If we do NOT have the principal's name and e-mail address, you will ask for this information. Page 8042016-202-T 3/1/2016 You will also verify that the school received the questionnaire (questionnaires were FedExed on 2/22/16). If the principal tells you he/she has the questionnaire, probe for the status of the questionnaire. We requested the completed form by March 17th. The status of the questionnaire determines the path you will take in the form. \uf0b7 If the respondent isn't sure whether they received the questionnaire, but offers to check with other staff members, you will go to item A12, and tell the respondent you will call him/her the next day to follow-up. \uf0b7 If the respondent tells you that he/she (or the principal) completed and mailed the questionnaire, you will go to section B. In section B, you will ask when the questionnaire was mailed and thank the respondent for their participation. \uf0b7 If the respondent tells you that he/she (or the principal) is working on the questionnaire or received it, but hasn't started working on it yet, you will go to section C. In section C, you will encourage the respondent to participate and ask for an estimated mailing date. \uf0b7 If the respondent did not receive the questionnaire, or if it was received but later misplaced, you will go to section D. In section D, you will let the respondent know that we will send them a replacement questionnaire, find out to whose attention we should address the replacement questionnaire, and ask for an estimated mailing date (based on receiving the questionnaire the following week). \uf0b7 If the respondent refuses to participate in the survey, you will go to section H and attempt to convince the respondent to participate. \uf0b7 If at any point in the interview the respondent says that their district requires approval prior to their completing the questionnaire, you will go to section I and follow the appropriate path for their district's approval status. The district approval status is printed on the cover of the SSOCS-26 form. During phase 2 of the Reminder operation, you will use the outcome of phase 1 of the Reminder operation to determine which section of the SSOCS-26 to use (section E, F, or G). You may also use sections D, H, or I if the respondent needs a replacement questionnaire, refuses, or tells you that they need their district's/LEA's approval. \uf0b7 If you completed section B. Completed and Mailed during phase 1 of the Reminder operation, you will go to section E in phase 2. In section E, you will ask whether the respondent has a copy of the form he/she mailed. If he/she does, you will try to collect the survey information via phone or fax. If he/she does not, we will allow a little more time for the questionnaire to be received prior to attempting to complete it with the respondent. Page 13 8042016-202-T 3/1/2016 \uf0b7 If you completed section C. Working on Questionnaire during phase 1, you will go to section F during the Reminder operation. In section F, you will determine whether the questionnaire was mailed, and when it was or will be mailed. \uf0b7 If you completed section D. Needs New Questionnaire during phase 1, you will go to section G during the Reminder operation. In section G, you will verify that the questionnaire was received and determine when it was or will be mailed. During the NRFU operation, you will use section J."}, {"section_title": "Call Guidelines", "text": "\uf0b7 Acceptable calling times are Monday through Friday from 8:00 a.m. to 5:00 p.m. (respondent time), unless the respondent requests an appointment before 8:00 a.m. or after 5:00 p.m. Be sure to notify your supervisor of the request so he/she can assign it to another interviewer if necessary. Please note that it may be difficult to reach people at the schools after 3:00 p.m. \uf0b7 If you or a previous interviewer left an answering machine message, wait one day before contacting the school again. \uf0b7 Do not make more than two call attempts to a school per day. \uf0b7 Do not make more than 10 attempts to contact a school during phase 1 of the Reminder. Do not make more than 5 attempts to contact a school during phase 2 of the Reminder. Do not make more than 10 attempts to contact a school during NRFU. \uf0b7 If you get a Busy Signal, FAX Signal, Number Could Not Be Completed As Dialed, No Signal, Bad Connection, or Temporarily Not In Service, retry the number 15 minutes later. If it is still unavailable, then code the case as such (see section VIII for Outcome Codes and Descriptions). It is considered one attempt after the retry has been made."}, {"section_title": "Making the Call", "text": "\uf0b7 Read and become very familiar with the SSOCS-26 before calling any school. \uf0b7 Review the preprinted label and Call Record information on the cover page before contacting the school. If the principal's name or e-mail address are not printed on the cover page, or are printed but crossed-out, you will need to ask for this information during the call (items A9 and A10). \uf0b7 If you or a previous interviewer has reached the school before, look through the form before you begin so that you know the contact history. This is especially Page 14 8042016-202-T 3/1/2016 important for phase 2 of the Reminder operation as your path in the SSOCS-26 is determined by the outcome of the previous call. \uf0b7 Enter the date, call start time, and your Interviewer ID in the Call Record (begin with line \"1\"). If you are retrying the case because you originally got a Busy, FAX, etc., then erase the start time and record the retry start time in its place. \uf0b7 Make sure you mark (X) all applicable boxes and write legibly as the information will be used for future mailings and follow-up phone calls. Verify the spelling of any new address information, the respondent's name, and the principal's name and e-mail address (if appropriate). \uf0b7 If it is a Busy, FAX, etc. after the retry, enter the time you hung up the phone in the End time and enter the appropriate Outcome Code and abbreviated description in the Outcome Code and Outcome Notes columns. \uf0b7 If you reach a recording with a new phone number or area code (Outcome Code 91), record the new number in the space provided next to \"Corrected telephone number\" on the cover page of the SSOCS-26. For all future contact, use the corrected number. \uf0b7 If someone answers, continue with item A1. \"Hello, this is\u2026\" on page 3 and follow the appropriate GO TO instructions. If no GO TO instruction is present, you should continue with the next item. \uf0b7 If the school name is different than what is printed in item A1, record the new name on the line beneath item A2. If the new name is not similar to what is printed, you will need to confirm with the respondent that the name changed (i.e., respondent says the school name used to be what is printed on the form but changed to a new name) so that we know we have contacted the correct school. If the name change is not confirmed, continue with the call; refer the case to your supervisor when the call is complete. \uf0b7 If you need to make any corrections to the address mark (X) \"No\" for item A3 -\"Is your school located at:\" and write the correct address on the lines next to the preprinted address. \uf0b7 Please try to speak with the principal, or with the person whom the principal designated to complete the questionnaire (possibly a Vice Principal or Disciplinarian). In some cases, you may only be able to reach the secretary or the principal's secretary. If the secretary confirms that the principal mailed the questionnaire, or is working on the questionnaire and will mail it, you may accept this. However, in cases where the school is refusing to participate, you should try to speak with the principal so that you can attempt to convert the refusal. Page 15 8042016-202-T 3/1/2016 \uf0b7 After you record the principal or other respondent's e-mail address, read it back to the respondent to ensure that it is spelled correctly and includes the correct punctuation/special characters, for example, dot (.), \"@\" symbol, and possibly slash (/) or backslash (\\). It is CRUCIAL that you verify the e-mail address, as it will be used for additional follow-up efforts. Many school staff member's e-mail addresses follow a general format of name@district.k12.stateabbreviation.us; for example, if you are interviewing Bob Roe in the Citizen School District in MD, his e-mail address may be similar to: broe@citizen.k12.md.us. \uf0b7 If you are making an appointment with the respondent, be sure to enter the Outcome Code in the Outcome Code column and the date and time of the appointment in the Outcome Notes column. If you have an appointment with someone other than the person with whom you spoke with, enter the name of the person you have an appointment with in the Outcome Notes column. The person in the Contact Name column is not necessarily the person the appointment is with. In some cases, you may have spoken with the school secretary, and he/she may have made an appointment for you to call the principal. \uf0b7 Please note that items H1 and H2 in the SSOCS-26 are similar and only one should be read. If the respondent requests more information, or you think more information will be helpful in converting the respondent, refer to the Frequently Asked Questions and Refusal Responses job aid. \uf0b7 After you have completed your call, be sure to enter the end time and the call outcome in the Call Record section on page 2 the SSOCS-26. \uf0b7 Mark any applicable boxes in the table on the cover page (correct school not reached, school closed, different school name, district approval necessary, refusal, re-mail requested). Page 16 8042016-202-T 3/1/2016 \uf0b7 If you collected the principal's name and/or e-mail address, or another school staff member's e-mail address, write it in the row labeled e-mail address on the cover page."}, {"section_title": "VI. THE SSOCS-1 INTERVIEW COMPLETED VIA THE TELEPHONE", "text": "\uf0b7 It is critical that you ask each question exactly as it is written in the questionnaire. Asking the question using different wording could change the way the respondent interprets it and may cause bias in the data. \uf0b7 If you complete a questionnaire over the phone with the respondent, it is CRUCIAL that AT LEAST questions 11,12,20,24,25,26,28,32,35,36,37,38,39,43,44,45 are completed. Although these items are the critical items, do not resort to completing only these items and do not inform the respondent that these are the critical items. It is important that the entire questionnaire be completed. If any of these items are not completed during the initial interview, you should call the respondent back and attempt to complete the remainder of the critical items. \uf0b7 In addition to the critical items listed above, at least 69 other subitems (e.g., items 1a, 1b, 1c) must be answered for an interview to be considered complete. o If you sense that the respondent is short-on-time or is getting bored, it may be helpful to let him know his progress, for example, that he's completed half of the questionnaire or is almost done. If the respondent sounds hesitant, tell her a little more about why her school's responses are important. A few words of encouragement will go a long way. o Either way, immediately return to reading the survey questions. YOU are in control of the interview and it is more effective to assertively move forward than to passively wait for an indication that it is okay to continue. \uf0b7 Use pencil when making any entries on the SSOCS-1 questionnaire. \uf0b7 Never lead the respondent. \uf0b7 Copy the school control number, case ID, and school name from the SSOCS-26 form to the label area on the cover of the SSOCS questionnaire. Paperclip the SSOCS-26 form to the completed SSOCS-1 questionnaire. \uf0b7 On page 5, you will verify the school's grade range. The grade range is printed in the top right corner of the SSOCS-26 label. Sometimes, however, it is helpful to be proactive in sharing information with a respondent."}, {"section_title": "VII. SETTING OUTCOME CODES", "text": "Page 17 8042016-202-T 3/1/2016 Many of the Outcome Codes are standard; however, there are a few that require explanation."}, {"section_title": "-LMR Received:", "text": "A completed questionnaire was checked-in at NPC for the school. 02 -SSOCS-1 completed over the phone: Use this outcome if you completed the SSOCS questionnaire over the phone."}, {"section_title": "-Hard Refusal:", "text": "Use this outcome after attempting refusal aversion or conversion on a case where the PRINCIPAL ADAMANTLY REFUSED (even if it was the first refusal) with your supervisor's permission."}, {"section_title": "-School Closed:", "text": "Use this outcome if you marked the box for \"School Closed\" in item A1 on page 3. This only applies to schools that are permanently closed or temporarily closed (for an extended period due to unusual circumstances, e.g., natural disaster, no enrollment, etc.) This does not apply to schools that are closed for holidays, scheduled breaks, or inclement weather."}, {"section_title": "-Not a School:", "text": "If you reach an institution that does not seem to be a school, refer the case to your supervisor. Your supervisor will refer the case to HQ, and it will be researched. Only use this outcome after HQ has researched the case to ensure that the school does not exist."}, {"section_title": "-Out of Scope -School Wrongly Classified:", "text": "Use this outcome if the school reports that they are any of the following types of school: \uf0b7 \"Private\" \uf0b7 \"Home School\" \uf0b7 \"Department of Defense\" \uf0b7 \"Bureau of Indian Affairs\" \uf0b7 \"Special Education\" \uf0b7 \"Juvenile Justice\" \uf0b7 \"Alternative\" \uf0b7 \"Vocational/Career and Technical\" \uf0b7 \"Other\" 08 -Requires LEA Approval -Refer to Supervisor: Look at the cover page next to \"Special district approval.\" If \"School district approval was received,\" do not assign an outcome code of 08; assign an outcome code based on the status of the questionnaire. "}, {"section_title": "M. How long will the survey take to complete?", "text": "The length of the survey will depend on the characteristics of your school, but for most people it will take about 52 minutes to complete. That time includes time spent filling out the survey itself, as well as referring to additional information sources for the information requested."}, {"section_title": "Introduction to Failed Edit Follow-up", "text": "The purpose of Failed Edit Follow-up (FEFU) is to call respondents for schools that have returned the SSOCS questionnaire to verify that the answers given for critical items are correct. A school is included in the FEFU operation if the answer(s) provided to critical items are outside of the expected range, inconsistent with other answers, or if the question was left blank. Participation in this survey is voluntary. There are no penalties for not answering questions. However, it is very important to have the respondent's cooperation to ensure the quality of the data."}, {"section_title": "Completing the SSOCS-27", "text": "A. Materials \u2022 SSOCS-27 o You will receive a preprinted SSOCS-27 for each school. Each form provides the script and GO TO instructions you will need for contacting the school, interviewing a knowledgeable respondent, and documenting the call outcome. o A copy of the SSOCS-27 is included in your training packet. \u2022 SSOCS-1 o You will receive the SSOCS-1 questionnaire that the principal or other school staff member completed and returned. \u2022 GREEN pencils for use on the SSOCS-1 questionnaire. \u2022 Regular pencils for use on the SSOCS-27 form. \u2022 Call Outcome Codes (see page 9; this will also be provided as a Job Aid). \u2022 Post-it flags."}, {"section_title": "B. Form Overview", "text": "The SSOCS-27 FEFU form has 2 parts. The numbered items, 1-8, include the introduction and the appropriate GO TO instructions. The List of Items on page 4 contains the question number and the page number of each question that you will need to ask the respondent in the SSOCS-1 questionnaire. When reviewing the list of items, it is important to remember the following: \u2022 You may be calling for a variety of reasons: o Response(s) to item(s) did not fall within the expected range. o Illogical relationships between items (e.g., item 36a. should be greater than or equal to the sum of entries in item 35, column 2 -if the answers don't meet that criteria, the item will be listed for FEFU.) o Too many critical items or total items were left blank. \u2022 There is 1 item that is critical during follow-up. If item 37 (school's total enrollment) is on the List of Items, use all of your interviewing skills to try to convince the respondent to provide an answer. If the respondent is reluctant, try to convince him/her to answer at least item 37 (if applicable). After item 37 is answered, continue with the remainder of the items on the list if the respondent seems willing to cooperate. \u2022 Although the items are not listed in numerical order, they are listed in the order that they should be asked! Do not deviate from the order of the items on the list."}, {"section_title": "C. Call Guidelines", "text": "\u2022 Acceptable calling times are Monday through Friday, 8:00 a.m. to 5:00 p.m. (respondent time), unless the respondent requests an appointment before 8:00 a.m. or after 5:00 p.m. Be sure to notify your supervisor of the request so he/she can assign it to another interviewer if necessary. Please note that it may be difficult to reach people at the schools after 3:00 p.m. \u2022 If you or a previous interviewer left an answering machine message, wait two business days before contacting the school again. \u2022 Do not make more than two call attempts to a school per day. \u2022 Do not make more than 8 attempts to contact a school. \u2022 \u2022 If you make an appointment with the respondent, be sure to enter the outcome code in the 'Outcome Code' column and the date and time of the appointment in the 'Outcome Notes' column on the SSOCS-27. Enter the name of the person you have an appointment with in the 'Outcome Notes' column; this should be the person whose name is printed on the SSOCS-27. However, there may be cases where you will need to speak with someone else (e.g., the SSOCS-1 was filled out by more than 1 person, the person who completed it no longer works at the school, etc.) "}, {"section_title": "F. Making the Call", "text": "\u2022 If someone answers, begin with item 1. \"Hello, this is\u2026\" on page 2 of the SSOCS-27 and follow the appropriate GO TO instructions. If no GO TO instruction is present, you should continue with the next item. \u2022 Make sure you mark (X) all applicable boxes and write legibly. \u2022 If the respondent does not have a copy of his/her completed SSOCS-1 form, offer to fax a blank questionnaire so that the respondent can follow along in the interview. Make an appointment with the respondent or ask the respondent to call you back after he/she receives the fax. Provide your supervisor or a control clerk with the school case ID, the respondent's name, and the fax number. The supervisor or control clerk will complete a fax cover sheet and fax the questionnaire to the respondent."}, {"section_title": "G. Understanding the List of Items", "text": "\u2022 In some cases, you will not need to ask all of the items on the 'List of Items' (page 4 of the SSOCS-27). Instructions for which items to ask are included on the List of Items. Some items are on the list because they were higher than expected given the size of the school. If q 37 (enrollment) is on the 'List of Items,' it will include instructions for the other items that depend on the new response to question 37. Ex. \"If q 37 enrollment exceeds 900, then don't ask q 26 range violations.\" Look next to the items for q26 on the 'List of Items.' If the item says \"(range violation),\" you DO NOT need to ask the respondent that item. \u2022 Some of the question numbers on the 'List of Items' are complex. You will need to understand what each component stands for in order to have a successful interview. In all cases, the parts of the question number go from general to specific. Open your copy of the SSOCS-1 questionnaire to question 26 on page 16 to look at the following example. Ex. Q26c1_1 page 16 \u2022 \"q26\" is the question number. \u2022 Since question 26 is broken down into items 26a -26l, the \"c\" in the example indicates that you need to ask the \"Robbery\" item in question 26. \u2022 Since q26c is broken down into \"i. With a weapon\" and \"ii. Without a weapon,\" the first number after \"c\" tells which part of 26c you need to ask. The \"i\" and \"ii\" are lowercase Roman numerals. On the SSOCS-27 list of items, i = 1 and ii = 2. Since the example above is q26c1, you will ask about \"Robbery, with a weapon.\" \u2022 For items that have multiple columns, the number after the underscore indicates which column you need to ask about. In the example above, you need to ask about the first column \"Total number of recorded incidents.\" \u2022 For items that are not broken down into a \"i.\" and \"ii.,\" the number after the question number and letter indicates which column you should ask (e.g., q34n2 -You will ask if the school used \"Loss of student privileges\" as a disciplinary action during the 2015-16 school year). \u2022 For questions with multiple columns, it is acceptable to ask the respondent about the item in the first column before asking about the second item (e.g., q34n2 -You may ask if the school allows for use of the disciplinary action before you ask if they used it during the school year). \u2022 If item q26a1 page 16 is on the list of items, you will need to ask a follow-up question if the new answer provided is greater than 0. The follow-up question will be printed next to the item number on the list of items. Example: Q26a1 page 16 How many of the victims were male? Write the number of male victims next to the question in green pencil. If none of the victims were male, write \"0.\" If the respondent doesn't know, write \"DK.\" If the respondent refuses, write \"RF.\" Example: Q26a1 page 16 How many of the victims were male? 1 Q26a1 page 16 How many of the victims were male? 0 Q26a1 page 16 How many of the victims were male? DK Q26a1 page 16 How many of the victims were male? RF Background information: we are asking this follow-up question because the definition of rape was changed for the 2010 SSOCS to specify that both male and female students can be victims of rape. If the estimates of incidents of rape are higher than in previous administrations (through 2008) of the SSOCS, NCES is interested in whether it is because of an increase in the number of rapes or if it is because of the new explicit inclusion of males in the definition of rape."}, {"section_title": "Conducting the SSOCS-1 Interview", "text": "\u2022 It is critical that you ask each question exactly as it is written in the questionnaire. Asking the question using different wording could change the way the respondent interprets it and may cause bias in the data. \u2022 Many questions include examples or other information in parentheses. You should read the parenthetical text to the respondent when asking the question. For examples, replace \"e.g.\" with \"for example\" when reading the question. \u2022 Many questions include instructions indicated with apple bullet points. When the instructions contain information about what to include or exclude when answering the item, you should read them to the respondent or ask the respondent to review them if he/she has a copy of the questionnaire. In cases where the instructions are repeated for a series of items that are included on the List of Items, you may read it for the first question and let the respondent know that it applies to a group of questions (for example, questions 11-18). \u2022 You do not need to read instructions such as \"Check one response on each line,\" \"Check \"Yes\" or \"No\" on each line,\" or \"If none, please place an \"X\" in the None box.\" If the response options are not Yes/No or a quantity (for example, \"Limits in major way,\" \"Limits in minor way,\" and \"Does not limit\"), read the response options to the respondent. \u2022 Use green pencil when making any entries on the SSOCS-1 questionnaire. \u2022 Definitions are available on pages 2 and 3 for words or phrases that are followed by an asterisk (*). Inform the respondent that definition's are available for certain terms if they would like clarification during the interview. Offer the definition if the respondent asks for clarification or seems confused about any question. If item 20 is on the list of items, provide the definitions for mental health professional, mental health disorder, diagnostic assessment, and treatment prior to asking the items. Alternatively, if the respondent has a copy of the questionnaire, you may ask him or her to review these definitions prior to answering the question. \u2022 DO NOT prompt the respondent to give the same answer as provided on the form. For example: when asking item 37, you should not say, \"As of October 1, 2015, what was your school's total enrollment? You answered 652 students -is that correct?\" Ask the question as worded and record his/her response. Never lead the respondent as it could cause bias. \u2022 If the question was blank, record the answer in the space provided. \u2022 For questions where the respondent is instructed to mark a box: mark the box of the response given to you in green pencil, even if the respondent gave the same response in the original questionnaire. The green pencil will distinguish the new response from the original response and indicate that the question was asked during FEFU. Do not erase the respondent's original response! \u2022 For questions where the respondent was instructed to write in the response: o If the respondent DID NOT answer the question in the original questionnaire, write the response in the space provided. o If the respondent DID answer the question in the original questionnaire, write the new response as close to the original response as possible. In its statistical standards, the National Center for Education Statistics (NCES) requires that any survey stage of data collection with a base-weighted (weighted) unit response rate of less than 85 percent be evaluated for the potential magnitude of nonresponse bias before the data or any analysis using the data may be released (U.S. Department of Education 2012). This appendix summarizes the results of the unit-level nonresponse bias analysis performed on the 2015-16 School Survey on Crime and Safety (SSOCS:2016). Unless noted otherwise, estimates were produced for this appendix using the base weights."}, {"section_title": "Final Review Exercise", "text": "Nonresponse can greatly affect the strength and application of survey data by leading to an increase in variance as a result of a reduction in the actual size of the sample. It can also produce bias if the nonrespondents have characteristics of interest that are different from those of the respondents (Statistics Canada 2009). There are two types of nonresponse: unit nonresponse and item nonresponse. Unit nonresponse refers to sampled units, schools in this instance, that do not have completed interviews. The SSOCS:2016 sample consists of 3,553 schools, of which 19 were ineligible for the survey and 2,092 completed the survey. Item nonresponse refers to survey questions with missing responses for interviewed schools. The analysis of item nonresponse bias in SSOCS:2016 is located in appendix O. Two sources of information are used in the SSOCS nonresponse bias analysis: the sampling frame and the SSOCS survey. The sampling frame contains auxiliary information (called school characteristics in this document) about the sample, and therefore this information is known for both respondents and nonrespondents. The SSOCS survey contains responses to survey questions (called survey variables in this document), and therefore the information is only obtained from the respondents. In this appendix, we first compare the distributions of the SSOCS sample and the target population across eight school-level frame characteristics 33 to ensure that the sample is representative of the target population. Next, we compare respondent and nonrespondent distributions on these eight school-level characteristics. We used logistic regression to model a school's response propensity, allowing us to calculate the R indicator to suggest how representative the respondents are compared to the original sample. We compared key survey estimates between low-response-propensity schools and the balance of the respondent sample. Finally, we evaluate the effect of the nonresponse weighing adjustment. For this evaluation, we present the differences in response propensity across the nonresponse adjustment cells created using chi-square automatic interaction detection (CHAID), which identifies the school characteristics that are the best predictors of response. Then, we compare the distributions of the eight school characteristics using the full sample (using base weights) and respondents (using both base weights and the final weights adjusted for nonresponse)."}, {"section_title": "Comparison of the Sample and Population", "text": "Before examining nonresponse to the SSOCS survey, we first examine the appropriateness of the SSOCS sample design in representing the target population. This is done by comparing distributions across the selected school characteristic variables in the SSOCS sample to the corresponding distributions in the sampling frame. The sampling frame for SSOCS:2016 was derived from the 2013-14 Common Core of Data (CCD) Public Elementary/Secondary School Universe data File. The SSOCS sample was chosen by stratifying the subset of schools from the CCD population by enrollment size, school level, and type of locale. Within each stratum, schools were first sorted by the percentage of White enrollment and region and then a systematic random sample was drawn. 34 Table M-1 displays the distributions of the SSOCS:2016 sample (including the schools that were later determined to be ineligible) and compares it to the sampling frame across the selected eight school characteristic variables. A chi-square likelihood ratio test, which tests for independence between two distributions, was used to examine whether there were any differences between the distribution of the selected sample and the target population based on the school characteristic variable examined. Independence of the row and column variables implies that the distributions across row variable subgroups will be the same across the SSOCS sample and target population columns. For example, when examining school level, the SSOCS sample and target population distributions were compared to see if they were independent of school level. If they were, it could be argued that the distribution of the sample is the same as the target population across the categories of school level. The larger the chi-square statistic, the less likely it is that the two distributions are independent of the key statistic examined. The results show, with 95 percent confidence, that the SSOCS sample and the target population are independent across the eight frame variables examined (i.e., p values are greater than .05). This means that for all of the frame variables examined, the sample has the same distribution as the target population, and there is no potential selection bias in the sample selection design. 34 See chapter 2 for a more detailed explanation of the sampling process.  The t statistic tests the difference between the group-specific weighted response rate and the overall weighted response rate. Frequency distributions were compared between 76 key survey variables collected with the survey instrument and the eight frame characteristics given above to assess areas where there may be potential bias. The prior analysis showed that most of the frame characteristics are related to response status, and this analysis showed whether those differences may be meaningful in terms of causing bias in key survey estimates. If key survey variables are related to characteristics that we know are biased, then the estimates themselves are also likely to be biased prior to adjustment. Tables M-3 and M-4 provide marginal summaries of the analysis. Table M-3 summarizes the results from likelihood ratio tests of independence between each frame characteristic and the 76 key variables, while table M-4 summarizes the number of key survey variables by the number of frame characteristics with significant differences. Tests were conducted at the 5 percent significance level. If a significant difference was detected, there is evidence to suggest that distributions of the key variable vary across the levels of the frame characteristic. In several instances, the test was not conducted because at least one cell had zero observations.  Based on a chi-square distribution with df = 3, using the significance level \u03b1 = .05."}, {"section_title": "M-3", "text": "Over half of the key survey variables have significant relationships with at least four frame characteristics, providing reason to believe that that differences in response rates attributed to the frame characteristics are indicative of potential bias in key estimates. The following list summarizes   "}, {"section_title": "M-11", "text": ""}, {"section_title": "M-12", "text": "The logistic regression coefficients shown in table M-6 were used to assign each sampled school a response propensity score, which is interpreted as the school's predicted probability of responding to SSOCS:2016 based on its unique combination of frame characteristics. Using the estimated response propensities from the logistic regression model, we calculated the R indicator. The R indicator measures how representative the respondents are of the original sample or population with respect to the frame characteristics included in the model. 38 The standard deviation of the response propensities is obtained from the model, and the R indicator is estimated by the following equation: Where: = the standard deviation of the response propensities = the base weight for school = the estimated response propensity for school = the mean of the estimated response propensities, = the number of eligible schools in the sample. Values of the R indicator that are close to 1 indicate that respondents are more likely to be representative of the sample or population. The R indicator based on our logistic model is approximately 0.79. This can be interpreted as signifying a moderate representativeness. Lastly, we split the respondents into two independent samples based on estimated response propensity and calculated estimates of 76 key statistics using each sample. The group in the lowest response propensity quintile (20 percent) was the first sample and was used as a proxy for nonrespondents. Respondents with a low propensity to respond share similar frame characteristics as nonresponding schools. The second sample was composed of the balance of the respondents. We performed t tests to compare the estimates of the 76 key statistics calculated from both samples. Of the 76 key statistics, 43 significant differences were detected between the estimates calculated with the two samples. All of the significant differences are positive, meaning that the schools in the low-propensity group reported more crime and other disciplinary problems than the balance of the schools. This suggests that prior to nonresponse adjustments, SSOCS may be underestimating the prevalence of crime and other characteristics of interest. The results are provided in table M-7.   17.6 10.6 7.0 0.0228 * Lack of parental support in addressing their children's mental health disorders (C0680) 30.0 23.4 6.6 0.0667 Lack of community support for providing mental health services to students (C0682) 16.6 12.0 4.6 0.1206 Written or unwritten policies regarding the school's requirement to pay for the diagnostics assessment or treatment of students (C0684) 15.9 14.0 1.9 0.4998 Reluctance to label students with mental health disorders to avoid stigmatizing the child (C0686) 10.1 9.6 0.6 0.8028  To evaluate the effect of the nonresponse weighting adjustment, a comparison analysis was conducted of the eligible sample (3,534 cases with sample selection base weights) and the respondents only (2,092 completed questionnaires with the post-raking final weight, which is adjusted for nonresponse) to look for differences between these two groups. The weighting adjustment should minimize any differences originally found between the eligible sample and respondents only, with respect to the frame characteristics used to define the adjustment cells. This analysis evaluates the sample distributions. For all categories of the eight frame characteristic categories, the nonresponse bias is estimated as Where: = the estimated percentage based on all eligible sample cases (base weighted); and = the estimated percentage based on respondent cases (base weighted or final weighted). The relative bias for an estimated proportion using only the respondent data, , is calculated using the following formula: The mean and median estimated relative bias across all frame variables is calculated as a summary measure. Tables M-9 and M-10 contain summary statistics of the findings. Table M-9 provides the comparisons between respondents and the eligible sample on the frame characteristics. Baseweighted distributions were used to describe differences between the respondents and eligible sample before the noninterview adjustment, and final weights were used to describe differences after the adjustment. Table M-10 demonstrates that the adjustments were effective at removing the observed bias in the frame characteristics. According to the table, estimates of frame characteristics that were significantly biased before adjustments were no longer significantly biased after adjustments.   adjustment. The results show that before the nonresponse adjustment, approximately 44 percent of the 32 categories from the eight frame characteristics were significantly biased. After the adjustment, none of the categories were significantly biased. Therefore, the adjustments were effective in removing the observed bias in the eight frame characteristics. We cannot evaluate post-adjustment bias in the survey estimates because we do not have survey data for nonrespondents. Some survey estimates may be subject to nonresponse bias that is not related to the observable characteristics used to create nonresponse-adjusted weights. This type of bias would not be removed by weighting adjustments. Therefore, data users are cautioned that, because survey variables are not observed for nonrespondents, the exact amount of nonresponse bias remaining in key estimates cannot be known with certainty and is likely to vary between estimates. However, the strong relationships between frame variables and survey estimates observed in the prior analysis provide reason to expect that the adjustments removed some of the nonresponse bias in the survey estimates. "}, {"section_title": "Using Extreme Assumptions to Assess the Potential for Item Nonresponse Bias", "text": "To assess possible nonresponse bias, sets of imputed values were generated by imposing extreme assumptions on the item nonrespondents. This provides an estimate of bias that would result under a \"worst-case\" scenario in which all item nonrespondents have the highest or lowest value from the original distribution. Two new sets of imputed values, one based on a \"low\" assumption and one based on a \"high\" assumption, were created for each variable. A \"low\" imputed value variable was created by resetting imputed values to the minimum value of the original distribution, and a \"high\" imputed value variable was created by resetting imputed values to the maximum value of the original distribution. 40 Both the \"low\" imputed value variable distributions and the \"high\" imputed value variable means were compared to the original means, and the results are presented in table O-2. For items C0326 and C0330, the potential for bias exists for both low and high imputed values because the mean with low imputed values and the mean with high imputed values differ significantly from the original mean. In other words, if the missing responses tend to be low values for these items, then the SSOCS:2016 item estimate will be biased upward, whereas if the missing responses tend to be high values for these items, then the SSOCS:2016 item estimate will be biased downward. There appears to be a greater risk of noticeable downward bias than of noticeable upward bias because the difference between the high estimate and the original estimate is much larger than the difference between the low estimate and the original estimate. 40 The two analysis items are discrete count data and were treated as ordinal data when executing the analysis plan."}, {"section_title": "O-3", "text": "However, this is primarily a function of the highly skewed nature of these variables, for which a small number of schools reported a very large number of incidents. While this table shows a worst-case scenario, in practice it is highly unlikely that all item nonrespondents would have reported the highest value for these variables. Indeed, even if item nonrespondents on average are more likely to provide higher responses, it is likely that many would still be reporting 0 or close to 0. Thus, even if item nonrespondents do tend on average toward higher values of these variables, any downward bias is likely to be far smaller in magnitude than is implied by these results."}, {"section_title": "Item Nonresponse Bias", "text": "Comparison of item respondents and item nonrespondents across frame characteristics. Measuring the magnitude of nonresponse bias at an item level can be problematic, since we do not know how item nonrespondents' answers differ from item respondents' answers. We can, however, examine how the level of item response differs across frame characteristics. The SSOCS sampling frame has data available for eight school-level characteristic variables for the entire sample. Five categorical variables (enrollment size, school level, locale, percentage White enrollment, and region) were used directly in the sampling design, while the remaining three variables (number of full-time-equivalent (FTE) teaching staff, student-to-FTE teaching staff ratio, and percentage of students eligible for free or reduced-price lunch) were derived from continuous variables available in the sampling frame. For SSOCS:2016, the categorical definitions for the student-to-teacher ratio and the percentage eligible for free or reduced-price lunch variables were collapsed into the categories used in NCES table stubs. Since there were no corresponding NCES table stubs for the number of FTE teachers, the categorical definitions were kept consistent with those used for the SSOCS:2010 nonresponse bias analysis. Comparison of item respondents and item nonrespondents across survey variables with high item response rates. Two survey subitems-C0560 (perceived level of crime in students' neighborhood) and C0562 (perceived level of crime in the school's neighborhood)-both had weighted item response rates above 99 percent and are likely to be correlated with responses to critical items. Distributions of these variables were also compared between respondents and nonrespondents to C0326 and C0330. Item C0560 has four discrete response values, while C0562 has three values."}, {"section_title": "Number of attacks with a weapon (C0326)", "text": "The results of the likelihood-ratio chi-square tests for independence, based on each two-way comparison in table O-3, indicate that for item C0326 (total number of attacks with a weapon), statistically significant relationships exist between the propensity to respond and (1) enrollment size, (2) school level, (3) number of FTE teaching staff, and (4) perceived level of crime in the school's neighborhood. Primary schools, schools with 29 to less than 45 FTE teaching staff, and schools with a high perceived level of crime in the school's neighborhood were less likely to respond to item C0326, while high schools and schools with enrollment less than 300 were more likely to respond. There appears to be a relationship between these four variables and responses to item C0326, suggesting at least a moderate risk of item nonresponse bias. However, C0326 has highly skewed responses. About 97 percent of the responses are zero, and over 99 percent of the responses are 2 O-4 or less. If there is no discernible difference in the way schools are responding to item C0326 across the school-level characteristic variables, then the impact of such a relationship is probably not going to be as severe as it appears to be in the extreme value analysis for item C0326. This provides some reason to expect that the \"extreme\" scenario is unrealistic.  "}, {"section_title": "Number of attacks without a weapon (C0330)", "text": "The results of the likelihood-ratio chi-square test for independence in table O-4 indicate that for item C0330 (total number of attacks without a weapon), statistically significant relationships exist between the propensity to respond and (1) enrollment size, (2) school level, (3) number of FTE teaching staff, and (4) perceived level of crime in the school's neighborhood. Primary schools, schools with enrollment between 500 and 999, and schools with 29 to less than 45 FTE teaching staff were less likely to respond to item C0330, while high schools and schools with a low perceived level of crime in the school's neighborhood were more likely to respond. There appears to be a relationship between these four variables and responses to item C0330, suggesting at least a moderate risk of item nonresponse bias. However, C0330 has highly skewed responses. About 23 percent of the responses are zero, about 50 percent of the responses are 4 or less, and over 75 percent of the responses are 10 or less. If there is no discernible difference in the way schools are responding to item C0330 across the school-level characteristic variables, then the impact of such a relationship is probably not going to be as severe as it appears to be in the extreme value analysis. This provides some reason to expect that the \"extreme\" scenario is unrealistic.  "}, {"section_title": "Summary", "text": "The mean item response rate for SSOCS:2016 was about 98 percent. However, two survey items (C0326 and C0330)-out of the 273 items examined in this analysis-had a weighted item response rate lower than 85 percent. These items were examined for potential bias per NCES standards. Using extreme assumptions for imputation, both were found to be sensitive to the potential effects of nonresponse bias. The biggest risk appears to be in the scenario in which item nonrespondents report substantially higher values in these variables. The likelihood-ratio chisquare test statistics for independence suggested that the missing cases for variables C0326 and C0330 have at least a moderate risk of item nonresponse bias in four of the school characteristic variables considered. Analyses also showed that distributions for C0326 and C0330 were highly skewed. This leads to the conclusion that if there is no discernible difference in the way schools are responding to the two items across the school-level characteristic variables, then the impact of such relationships is probably not going to be as severe as it appears to be in each item's extreme value analysis. This provides some reason to expect that the \"extreme\" scenario is unrealistic. The combination of these analyses led to the determination that the potential for bias was not enough to warrant the exclusion of these items from the data file. The total response rate was not measured against any standard in this analysis. Given the unit response rate (62.9 percent) and the lowest item response rate (82.1 percent, corresponding with item C0330), the lowest total response rate for an item is 51.6 percent."}, {"section_title": "Survey item # Consistency edit", "text": "Rectification procedure"}, {"section_title": "26", "text": "The number of recorded incidents of specified offenses in item 16 column 1 must be greater than or equal to the number of specified incidents reported to police or other law enforcement in item 26 column 2. If the number of incidents reported to police or other law enforcement of a specific offense was larger than the number of specific offenses recorded, the number of specific offenses recorded (item 26 column 1) was deleted and imputed.\nIf any of the columns of item 35e is greater than zero, the total number of physical attacks or fights recorded (subitem 26d_i or subitem 26d_ii column 1) must also be greater than zero. If there are any non-zero responses in any of the columns of item 35e (columns 1 through 5), and the respondent also indicated that there were no recorded incidents of physical attacks or fights with or without a weapon (subitem 26d_i column 1=0 and subitem 26d_ii column 1=0), both subitem 26d_i column 1 and subitem 26d_ii column 1 were deleted and a value was imputed.\nIf any of the columns of item 35a is greater than zero, the total number of recorded incidents of possession of a firearm/explosive device (subitem 26g column 1) must also be greater than zero. If there are non-zero responses in any of the columns of item 35a (columns 1 through 5), and the respondent also indicated that there were no recorded incidents of possession of a firearm/explosive device (subitem 26g column 1=0), subitem 26g column 1 was deleted and imputed.\nIf the respondent indicated that there was at least one incident involving a shooting at the school (item 25=1) but there were not any possessions of a firearm or explosive device (subitem 26g), then one item was misreported. If the respondent indicated that there was at least once incident involving a shooting at the school (item 25=1) but said there were not any possessions of a firearm or explosive device (subitem 26g), then subitem 26g was deleted and imputed at a later stage. Item 30: In order to impute values for item 30, a ratio imputation technique similar to the one described for item 18 was used. If item 30 is unanswered, use the donor's ratio of the entry for that item to the number of enrolled students to impute a value. Item 31: In order to impute values for item 31, a ratio imputation technique similar to the one described for item 18 was used. If item 27 is unanswered, use the donor's ratio of the entry for that item to the number of enrolled students to impute a value. Item 32: The components of item 32 were imputed using a hot deck imputation approach identical to the technique described for item 1. If any parts of item 32 are unanswered, impute the donor's entry. Item 33: The components of item 33 were imputed using a hot deck imputation approach identical to the technique described for item 1. If any parts of item 33 are unanswered, impute the donor's entry. Item 34: In general, a simple imputation approach similar to that described for item 1 was used for the item 34 imputation. In each row of item 34, a value for the first column was imputed before a value was imputed for the second column. If an item in the first column is unanswered, impute the donor's entry. If \"No\" is imputed, blank the item in the second column. Certain item 34 data are directly related to data in items 35 and 36; therefore, item 34 rows a, c, and e were imputed using data from item 35. Column 2 of item 35 indicates the number of removals with no continuing services for at least the remainder of the school year for specific offenses. If a respondent indicated a nonzero value for the total removals with no continuing services in subitem 36a, columns 1 and 2 of item 34 row a were both edited to \"yes,\" indicating that the school both allows for and utilized removal with no continuing school services for at least the remainder of the school year. If the value at item 36a was greater than zero, and the respondent indicated that the school did not allow for the use of removals with no continuing services for at least the remainder of the school year in item 34a_1 (C0390=2) or that this action was not used during this school year in item 34a_2 (C0392=2), these \"no\" values were deleted and \"yes\" values were imputed. If no removals were reported in item 35 column 2, a hot deck approach similar to the technique described above for item 34 was used; however, in each item 34 row, the value of column 2 was imputed prior to the value of column 1. Similar imputation procedures were performed to ensure that item 35 column 3 and subitem 36b were consistent with item 34 row c and that item 35 column 4 was consistent with item 34 row e. Item 35: Imputation for item 35 was performed using an aggregate proportion technique. Donor classes were composed of schools with nonimputed item 35 values in the row of interest that shared the same school level, urbanicity, and enrollment size categories as the recipient. Values were imputed on a row-by-row basis so that the total number of students involved in the specific offense (column 1) was greater than or equal to the number of disciplinary actions that were handed out for the specific offense (sum of columns 2-5). Although a student could theoretically be disciplined for the same offense several times, it was unlikely that there would be multiple disciplinary actions assigned for a single offense. Q-5"}, {"section_title": "Q-6", "text": "Within each row, three scenarios were determined, each warranting its own imputation approach: Scenario 1: The first scenario occurred when the total number of students involved in a specific offense (column 1) was greater than zero and the items indicating the number of disciplinary actions taken for the specific offense (columns 2-5) were either blank or a mixture of blanks and nonzero values. An example of this scenario would be a respondent indicating that out of 30 students involved in the use/possession of a firearm/explosive device in subitem 35a_1 (C0458), four students were removed from the school in subitem 35a_2 (C0460), but failing to provide responses to subitems 35a_3 (C0462), 35a_4 (C0464), and 35a_5 (C0466). To impute values for subitems 35a_3, 35a_4, and 35a_5, the ratio of the sum of all disciplinary actions taken for the specific offense (e.g., use/possession of a firearm/explosive device) to the sum of students involved in a specific offense within the school's donor class was calculated. This ratio (Ra) is illustrated by equation 1 below using the subitem 35a example. This ratio was then multiplied by the recipient's item 35 column 1 value (30, in the example) to predict a total number of disciplinary actions for the specific offense. Continuing the example with subitem 35a, if within the recipient's donor class, the sum of the various disciplinary actions in subitems 35a_2-35a_5 (C0460-C0466) equals 200 and the sum of the total students involved in the offenses in item 35a_1 (C0458) equals 600, the ratio (Ra) would be 1/3. The ratio, Ra, was then multiplied by the recipient's item 35 column 1 value for the particular offense (30) to predict the total disciplinary actions for the particular offense (1/3 x 30 = 10, in our example = the predicted sum of disciplinary actions for use/possession of a firearm/explosive device)."}, {"section_title": "Equation 1:", "text": "where Q35ami is the subitem 35a value of donor school i in column m, is the subitem 35a_1 value of donor school i, and n is the number of schools in the recipient's donor class. The recipient's nonimputed disciplinary actions for the specific offense were then subtracted from the total disciplinary actions to determine the total number of disciplinary actions that must be distributed among the columns with missing values in each row (e.g., 10 total disciplinary actions -4 known disciplinary actions = 6 disciplinary actions to be distributed among subitems 35a_3, 35a_4, and 35a_5). The distribution of the remaining disciplinary actions was determined by calculating within the recipient's donor class the ratios (Rm) of the sum of the disciplinary actions to the sum of total offenses for each disciplinary action missing a value (e.g., subitems 35a_3, 35a_4, and 35a_5). If it was determined in the example that the disciplinary actions were distributed equally among donors across subitems 35a_3, 35a_4, and 35a_5, a value of 2 would be imputed for each of the three missing column values. Scenario 2: The second scenario occurred when the number of students involved in a particular offense (column 1) was unknown and the respondent indicated that at least one disciplinary action was taken for the offense (i.e., there was at least one nonzero value within columns 2-5). Item 45: The imputation for subitems 45a and 45b used the aggregate proportion or ratio imputation technique. However, the imputation for item 45 is unique because one component (subitem 45a) is independent of other data in the survey, and the other component (subitem 45b) must be greater than or equal to the subitem 36b value. Subitem 45a was imputed first, and donor classes for subitem 45a were formed on the basis of school level, urbanicity, and enrollment size categories. Values of zero were imputed for subitem 45a by calculating the percentage of schools with values of zero in the donor class and randomly choosing recipients to receive imputed zeroes, such that the percentage of recipients with imputed zeroes in subitem 45a mimics the percentage of donors with values of zero in subitem 45a. If item 45a is unanswered, use the donor's ratio of the entry for that item to the total number of enrolled students to impute a value. Because the subitem 45b values were directly related to the subitem 36b values, the item 45b values were imputed using aggregate proportions of donor class subitem 45b to donor class subitem 36b. Donor classes were formed by searching for schools with identical school level and enrollment size categories as the recipient. Donor classes were further refined by separation on the basis of subitem 36b values. Not surprisingly, schools reporting fewer transfers for all disciplinary reasons (subitem 36b) tended to be associated with larger ratios of subitem 45b to subitem 36b; therefore, donor separation based on subitem 36b values helped to ensure that unrealistically large subitem 45b values were not imputed. Subitem 33b values were imputed by finding the ratio of the aggregate subitem 45b values to the aggregate subitem 36b values for the entire donor class and multiplying this ratio by the recipient's subitem 36b value (after any necessary subitem 36b imputation)."}, {"section_title": "Q-9", "text": ""}, {"section_title": "Donor Type Description", "text": "No Donor Edit (Type 0) -No Donor Used Description: The missing item is imputed using responses from other items in the same questionnaire record. Similar edits may be run during the logic edit stage but due to manual review, it is possible that the prior logic edit would not have been triggered. This is called a \"No Donor Edit,\" and it is run prior to the donor imputation. Both the description and SAS code are included in the column \"NoDonor_Edit\" within the Imputation Table of the processing databases. There are no Donor Type 0's initialized in SSOCS."}, {"section_title": "Donor Type 1 -Simple Imputation", "text": "Description: The missing item is imputed directly from that item in the donor record. Example: Question 40 *If item 40 is unanswered, impute the donor's entry."}, {"section_title": "Donor Type 2 -Simple Imputation for Multiple Items", "text": "Description: A series of missing items is imputed directly from those items in the donor record. Example: Question 2 *If any part of item 2 is unanswered, impute the donor's entry. Donor Type 3 -Simple Imputation with Blanking Edit/Simple Imputation Description: Items requiring this type of imputation have two parts. The first part is a simple imputation, where the initial missing item (usually an item with a yes/no response, referred to as a \"screener\" item) is imputed directly from that item in the donor record. Then, depending on the imputed response, the subsequent item(s) will either need to be imputed using simple imputation (when \"yes\" is imputed to the screener item) or will need to be blanked (if \"no\" is imputed to the screener item). This type of imputation occurs for items where there is a skip pattern present. Note: For these items, there are ALWAYS two donors. The first donor is used when both parts (the \"screener\" portion and the subsequent items) of the imputed item are missing. The second donor is used when the respondent has answered the screener item with a \"yes\" response, but the subsequent item(s) are missing and need to be imputed. The method of imputation for this second donor is simple imputation."}, {"section_title": "Q-10", "text": "Example: Question 16 *D1: If item 16 is unanswered, impute the donor's entry. If the entry is \"No,\" mark all parts of item 17 as blank. If it is \"Yes,\" impute the donor's entry to any part of 17 that is unanswered. *D2: If \"Yes\" is marked in item 16 and any part of item 17 is unanswered, impute the donor's entry."}, {"section_title": "Donor Type 4 -Ratio Imputation", "text": "Description: The missing item is imputed using the donor's ratio of that item to some predetermined related item (\"ratio variable\") and applying it to that same related item in the record being imputed. Example: Q27 *If item 27 is unanswered, use the donor's ratio of the entry for that item to the total number of enrolled students to impute a value."}, {"section_title": "Donor Type 5 -Ratio Imputation for Multiple Items", "text": "Description: A series of missing items is imputed using the donor's ratio of each of those items to some predetermined related item (\"ratio variable\") and applying these ratios to that same related item in the record being imputed. Example: Question 26 *If any parts of item 26 are unanswered, use the donor's ratio of the entry for that item to the total number of enrolled students to impute a value. Donor Type 6 -Simple Imputation with Blanking Edit/Ratio Imputation Description: Items requiring this type of imputation have two parts. The first part is a simple imputation, where the initial missing item (usually an item with a yes/no response, referred to as a \"screener\" item) is imputed directly from that item in the donor record. Then, depending on the imputed response, the subsequent item(s) will either need to be imputed using ratio imputation (if \"yes\" is imputed to the screener item) or will need to be blanked (if \"no\" is imputed to the screener item). This type of imputation occurs for items where there is a skip pattern present. *If any part of item 18 is unanswered and item 11 is marked as \"yes\" or imputed as \"yes,\" then use the donor's ratio of the entry for that item to the total number of enrolled students to impute a value. Donor Type 7 -Ratio Imputation with Blanking Edit/Ratio Imputation Description: Items requiring this type of imputation have two parts. The first part is a ratio imputation, where the initial missing item (referred to as a \"screener\" item) is imputed using the donor's ratio of that item to some predetermined related item (\"ratio variable\") and applying it to that same related item in the record being imputed. Then, depending on the imputed response (whether a value of 0 or a value greater than 0 is imputed), the subsequent item(s) will either need to be imputed using ratio imputation (if a value greater than 0 is imputed to the screener item) or will need to be blanked (if a value of 0 is imputed to the screener item). This type of imputation occurs for items where there is a skip pattern present. There are no Donor Type 7's initialized in SSOCS."}, {"section_title": "Donor Type 8 -Complex Imputation Routine", "text": "The imputation cannot be dealt with using the combination of a database and macros. The item uses a method not covered in donor types 1-7, and there is no way to automate the imputation due to its level complexity (i.e., too many steps in the imputation process). Due to the uniqueness, each complex imputation routine is assigned a letter. For example, the first one (by way of processing order) is numbered donor type 8a, the second 8b, and so on. This would apply to question 35 in SSOCS 2016 (c0458 c0460 c0462 c0464 c0466 c0468 c0470 c0472 c0474 c0476 c0478 c0480 c0482 c0484 c0486 c0488 c0490 c0492 c0494 c0496 c0498 c0500 c0502 c0504 c0506)."}]