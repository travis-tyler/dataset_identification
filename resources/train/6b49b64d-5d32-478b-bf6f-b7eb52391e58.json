[{"section_title": "Abstract", "text": "By choosing different pulse sequences and their parameters, magnetic resonance imaging (MRI) can generate a large variety of tissue contrasts. This very flexibility, however, can yield inconsistencies with MRI acquisitions across datasets or scanning sessions that can in turn cause inconsistent automated image analysis. Although image synthesis of MR images has been shown to be helpful in addressing this problem, an inability to synthesize both T 2 -weighted brain images that include the skull and FLuid Attenuated Inversion Recovery (FLAIR) images has been reported. The method described herein, called REPLICA, addresses these limitations. REPLICA is a supervised random forest image synthesis approach that learns a nonlinear regression to predict intensities of alternate tissue contrasts given specific input tissue contrasts. Experimental results include direct image comparisons between synthetic and real images, results from image analysis tasks on both synthetic and real images, and comparison against other state-of-theart image synthesis methods. REPLICA is computationally fast, and is shown to be comparable to other methods on tasks they are able to perform. Additionally REPLICA has the capability to synthesize both T 2 -weighted images of the full head and FLAIR images, and perform intensity standardization between different imaging datasets."}, {"section_title": "Introduction", "text": "Magnetic resonance imaging (MRI) is the dominant imaging modality for studying neuroanatomy. In MRI, the soft tissues in the brain can be imaged with different tissue contrasts by using different pulse sequences. Pulse sequences like magnetization-prepared gradient echo (MPRAGE) and spoiled gradient recalled (SPGR) produce a T 1 -weighted ( T 1 w) tissue contrast which has good image contrast between gray matter (GM) and white matter (WM) tissues. T 1 w pulse sequences are used extensively as inputs for automated segmentation ( Prastawa et al., 2004; Roy et al., 2012 ) and cortical reconstruction algorithms ( Dale and Fischl, 1999; Han et al., 2004; Shiee et al., 2014 ) . Pulse sequences like dual spin echo (DSE) are used to produce P D -weighted ( P D w) and T 2 -weighted ( T 2 w) tissue contrasts, which are useful for visualizing tissue abnormalities like lesions. Fluid attenuated inversion recovery (FLAIR) is a T 2 w pulse sequence that uses inversion recovery as a mechanism to enhance the image contrast of white matter lesions (WMLs). Multiple sclerosis is an example of a disease whose * Corresponding author.\nE-mail address: amod@cs.jhu.edu , amodjog@jhu.edu (A. Jog) . URL: http://iacl.ece.jhu.edu/Amod (A. Jog) study benefits from the use of FLAIR imaging to detect and quantify WMLs ( Llado et al., 2012 ) .\nMRI thus provides us with a panoply of neuroimaging options via pulse sequences, each of which provides a unique view of the intrinsic MR parameters. The inherent diversity of MRI is a boon to diagnosticians but can prove to be a challenge for automated image analysis. In clinical scenarios, where the scanning time for a patient is always limited and generally expensive, the number of tissue contrasts that can be acquired is limited. Certain pulse sequences like MPRAGE and SPGR can be acquired at a high resolution (1 mm 3 or smaller voxel size) in a matter of 3-10 min. Sequences like DSE and FLAIR, which have long repetition times (TR) or inversion times (TI), are typically acquired at a lower resolution or, worse, not at all. If acquired at a lower resolution, these images are usually upsampled to match the highest resolution of the other modalities while performing multimodal image analysis. This makes it difficult to accurately understand and visualize the underlying neuroanatomy. MRI acquisitions can also be affected by artifacts that render them unusable ( Stuckey et al., 2007 ) . Such missing or inconsistent data affects the consistency of image processing applied to the whole dataset. In order to carry out consistent and automated image processing on such datasets, we propose to use image synthesis to generate images that fill the gaps in the data and/or enhance its quality.\nWe define image synthesis as an intensity transformation applied to a given set of input images to generate a new image with a specific tissue contrast. In general, we use image synthesis as a preprocessing step that takes place before applying more complex image processing algorithms such as segmentation and registration. Synthesis of an MR image that is perfectly identical to its true counterpart is not possible and therefore synthetic images are not intended to be used for diagnostic purposes. Our goal is to generate synthetic images that are close enough approximations to real images so that further automated image processing and analysis steps are either enabled or improved.\nThe practice of image synthesis has been gaining attention recently ( Rousseau, 2008; Roy et al., 2010a,b; Rousseau, 2010; Roy et al., 2011; Roohi et al., 2012; Jog et al., 2013a; Rousseau and Studholme, 2013; Konukoglu et al., 2013; Ye et al., 2013; Iglesias et al., 2013; Roy et al., 2013a,b; Jog et al., 2014a; Roy et al., 2014; Burgos et al., 2014; van Tulder and de Bruijne, 2015; Jog et al., 2015a; Cardoso et al., 2015; Van Nguyen et al., 2015; Bahrami et al., 2015; Zikic et al., 2014 ) and the number of applications where image synthesis methods are being used is also growing. Synthesis of modalities differs from the data imputation literature ( Hor and Moradi, 2015 ) in that the main goal is synthesis of the missing modality as opposed to classification in the absence of it. Image synthesis methods can be classified into two broad categories: (1) registration-based methods and (2) intensity transformation-based methods. Registration-based synthesis originated in the work of Miller et al. (1993) , where image synthesis was achieved by spatially aligning a single subject image and an image within a co-registered collection of images, which we call an atlas. Given a subject image b 1 of modality (or tissue contrast) M 1 and a pair of co-registered atlas images a 1 and a 2 of modalities M 1 and M 2 respectively; a 1 is deformably registered to b 1 and the same transformation is applied to a 2 to generate \u02c6 b 2 of modality M 2 . Burgos et al. (2014) extended this approach by introducing multi-atlas registration and intensity fusion to create the final synthesized image; they applied the method to synthesize computed tomography (CT) images from corresponding MR images. A variant of this approach was recently proposed by Cardoso et al. (2015) where a generative model learned using multiple atlases was used for outlier classification and image synthesis. Using registration for synthesis can lead to significant errors in the finer regions of the brain such as the cerebral cortex, where registration is not always accurate. Registration-based synthesis also fails in the presence of abnormal tissue anatomy since the atlases do not have pathology in the same locations as the subjects.\nOne of the first approaches using an intensity transformation for synthesis is called image analogies , as described by Hertzmann et al. (2001) . In this approach, the atlas images a 1 and a 2 are assumed to be related by an unknown filter h as a 2 = h * a 1 . Given a subject image b 1 , the goal is to synthesize \u02c6 b 2 , which should ideally be equal to h * b 1 . This is achieved by forming \u02c6 b 2 patch-by-patch. A patch in b 1 is considered for synthesis and its nearest neighbor in the set of patches extracted from a 1 is found. The corresponding a 2 patch is picked and placed to form the patch in \u02c6 b 2 . This synthesis approach is wrapped in a multiresolution framework, and attempts to mimic the action of the filter h . The image analogies method has been applied in MR synthesis ( Iglesias et al., 2013 ) , where synthesis of an alternate tissue contrast was shown to improve registration. Image synthesis has also been framed and solved as a sparse dictionary reconstruction problem called MIMECS ( Roy et al., 2011; . Patches extracted from the atlas image a 1 form a dictionary, which is used to describe the patches in the subject image b 1 as a sparse, linear combination of its atoms. The same corresponding patches from a 2 are combined with the same linear coefficients to synthesize patches in \u02c6 b 2 . The paper by Roy et al. (2013a ) demonstrated multiple applications of synthesis including intensity standardization and improved registration. A generative model of synthesis using patches was described by Roy et al. (2014) , which was used to synthesize CT images from ultra-short echo time (UTE) MR images.\nIntensity transformation approaches such as image analogies and MIMECS have long runtimes. Image analogies must find a nearest neighbor patch for each voxel while using its synthesized neighbor voxel during synthesis, and this process cannot be parallelized. MIMECS must solve a sparse reconstruction problem at each voxel, which can be parallelized but this is a relatively long step, and without this extra programming it is very time consuming. The range of application of these methods is also somewhat limited. For example, MIMECS was shown to be ill-suited for the purpose of synthesizing FLAIR images . With better training data we have improved MIMECS FLAIR synthesis but many deficiencies still remain.\nIn this paper, we describe an intensity transformationbased image synthesis approach that uses a nonlinear regression in feature space to predict the intensity in the target modality (tissue contrast). We use local patches and additional context features within a multi-resolution framework (cf. Hertzmann et al. (2001) ) to form our feature space. The nonlinear regression is learned using a regression ensemble based on random forests ( Breiman, 1996 ) . We call our approach R egression E nsembles with P atch L earning for I mage C ontrast A greement or REPLICA. Our approach is computationally much faster than existing approaches and delivers comparable and sometimes better synthesis results. It can also be used to synthesize FLAIR images, and is able to synthesize T 2 w images with skull; capabilities that have not been demonstrated by other intensity transformation synthesis approaches.\nAspects of REPLICA have appeared in previous conference publications ( Jog et al., 2013a; 2014b ) where we demonstrated its use in example-based super-resolution and intensity standardization scenarios. A previous version of REPLICA was used as part of another image synthesis strategy in Jog et al. (2015b ) . In this paper, we provide a complete description of REPLICA including its use of a multi-resolution framework and context features. We also include in-depth experiments showcasing the efficacy of synthesis both by direct comparison to real images and by comparison of further processing steps that use synthetic images as opposed to real images. This paper is organized as follows. We describe the REPLICA method in Section 2 . Parameter selection is described in supplementary material provided along with this manuscript. Results are shown in Section 3 , where we show synthesis of T 2 w and FLAIR images. We also justify our choice of features and the necessity of a multi-resolution framework by demonstrating results for the relatively challenging task of synthesizing a whole head image (i.e. without skullstripping). We demonstrate an intensity standardization application in which we reconcile two different imaging datasets to provide a more consistent segmentation. A brief conclusion including a discussion of future work is provided in Section 4 ."}, {"section_title": "Method", "text": "Let B = { b 1 , b 2 , . . . , b m } be a subject image set, imaged with pulse sequences 1 , . . . , m . This set contains m images from m different pulse sequences such as MPRAGE, FLAIR, DSE, etc. We sometimes refer to these different pulse sequences as \"modalities\" since they generate different tissue contrasts with images that often appear very different from one another. Let A = { a 1 , a 2 , . . . , a m , a r } be the atlas collection generated by the pulse sequences 1 , . . . , m (which are the same as those of the subject images) and r , where r is the target atlas modality that we want to synthesize from the subject set. The atlas collection can contain imaging data from more than one individual but for simplicity in this description we assume that there is only one individual in the atlas.\nOur goal is to synthesize a subject image \u02c6 b r that has the same tissue contrast as the atlas image a r . We frame this problem as a nonlinear regression where we want to predict the intensities of \u02c6 b r , voxel-by-voxel. This nonlinear regression is learned using random forest regression ( Breiman, 1996 ) . Training data is generated using the atlas image set by extracting feature vectors f ( x ) at voxel locations x in the atlas image domain . The random forest is trained so that these features predict the voxel intensity a r ( x ) of the atlas target modality. Once trained, these features are extracted from the subject image and the random forest is used to predict \u02c6 b r . These steps are described in detail below."}, {"section_title": "Features", "text": "Generating a synthetic image of modality r from modalities 1 , . . . , m involves calculating an intensity transformation that jointly considers features in these modalities to predict the intensities in a r generated by r . If we consider an individual voxel intensity as the only feature then in most realistic image synthesis scenarios the intensity transformation is not a one-to-one function between the input and target modality voxel intensities. For example, in T 1 w images, the cerebrospinal fluid (CSF) and bone are both imaged as dark regions. However in corresponding T 2 w images, the CSF is bright and the bone is dark (due to a very small T 2 and a negligible signal). Thus the same T 1 w dark intensities have two very different mappings in the T 2 w range. A regression trained on such ambiguous data and applied to unknown, test T 1 w intensities is bound to yield substantial errors.\nOne way to reduce such errors is to add additional features associated with each voxel x \u2208 . Previous approaches like image analogies ( Hertzmann et al., 2001 ) and MIMECS used small image patches (usually 3 \u00d7 3 \u00d7 3 voxels centered on x ) as features. Small patches add some spatial context to a single voxel, but they are still often unable to resolve the ambiguity of a one-to-many mapping from feature space to the target modality intensity. Registration-based approaches ( Burgos et al., 2014; Cardoso et al., 2015 ) attempt to solve this issue by carrying over the spatial context from the atlas images to the subject image via registration under the assumption that after registration, the intensity transformation is simple enough to be learned by using small patches. We address this problem in two ways, first by using a multi-resolution framework and second by adding remote features which we call context features , as described next."}, {"section_title": "Multi-resolution features", "text": "For each image a i in the atlas set, we construct a Gaussian pyramid (using \u03c3 = 1 voxel) on scales s \u2208 { 1 , . . . , S} ( Hertzmann et al., 2001 ) . Let the atlas images at scale s , be\nfirst level ( s = 1 ) corresponds to the original high resolution atlas images, and each successive level is created by Gaussian smoothing and downsampling the images by a factor of 2. These levels depicting the creation of training data are shown in stages (a)-(c) in Fig. 1 . At the coarsest resolution of the Gaussian pyramid ( S = 3 is sufficient in most scenarios), the intensity transformation is simple and can be learned using small\nset p = q = r = 3 ). This step is illustrated in Fig. 1 (c) ; the details of training the random forest regression are described later. For all the scales s for which 1 < s < S (which is just s = 2 in Fig. 1 ), the full feature vector consists of two distinct parts. The first part is a small patch at voxel location x from the image set A s , which can be described by\nond part is a small patch at x taken from a s +1 \u2191 r , the upsampled target image from one level lower resolution in the Gaussian pyramid, where upsampling is done via trilinear interpolation. We denote this upsampled target patch by q s ( x ). These steps correspond to stages (a) and (b) in Fig. 1 . The patch q s ( x ) helps to disambiguate regions of similar intensities by providing a low resolution estimate of the intensities in the target modality r . The feature vector at levels 1 < s < S is the concatenation of these two fea-"}, {"section_title": "High resolution context descriptor", "text": "Although multi-resolution features can help to disambiguate intensity mappings, their use alone can yield overly smooth synthetic images due to the information arising from the lower resolutions in the multi-resolution pyramid. To address this problem we introduce special context features that are used only at the finest level in the pyramid (stage (a) in Fig. 1 ).\nWe assume that the images have been registered to the MNI coordinate system ( Fonov et al., 2009; and are in the axial orientation, with the center of the brain approximately at the center of the image. This step is essential to ensure that the atlas and subject brains are roughly in the same coordinate frame and therefore the context features are comparable across the anatomies. Let the voxel x be located on slice z , with slice center at o z . Thus the unit vector u = o z \u2212 x / o z \u2212 x identifies the direction to the center of the slice from voxel x .\nWe then define eight directions by rotating u by angles\n4 } about the axis perpendicular to the axial slice (see Fig. 2 ). In each of these directions, at a radius r i \u2208 { r 1 , r 2 , r 3 , r 4 } we calculate the average intensities in cubic regions with increasing cube widths w i \u2208 { w 1 , w 2 , w 3 , w 4 }, respectively. In Fig. 2 , we have shown the voxel x at the center. The unit vector u pointing toward the slice origin o z is shown in red. We also show the cubic regions over which we average intensities as colored boxes at the eight orientations and four radii. Although they are shown as 2D rectangles in the illustration, these regions are actually 3D cubes of sizes w i \u00d7 w i \u00d7 w i , where the center voxel of the region lies at the designated rotations and radii within the axial slice of the voxel x . This yields a 32-dimensional descriptor of the context surrounding voxel x at the highest resolution. In our experiments, we have used the values w 1 = 3 , w 2 = 5 , w 3 = 7 , w 4 = 9 and r 1 = 4 , r 2 = 8 , r 3 = 16 , r 4 = 32 , which were determined empirically. Since the head region is roughly spherical, this feature can disambiguate patches from different locations of the brain slice based on their near and far neighborhoods. We denote this feature vector as v ( x ). The feature vector at the finest level in the pyra-\nwhich is used to train the final random forest regression stage to estimate the center voxel value a 1 r (x ) . Similar context descriptors have been independently formulated for segmentation tasks ( Bai et al., 2015; Zikic et al., 2012 ) ."}, {"section_title": "Training a random forest", "text": "We train a random forest regressor RF s at each level s to predict the voxel intensities in the target modality at that level from the feature vectors at that level (the orange, yellow, and green blocks in Fig. 1 ) . A random forest regressor consists of an ensemble of regression trees ( Breiman et al., 1984 ) with each regression tree partitioning the space of features into regions based on a split at each node in the tree.\n} be the set of all training sample pairs at a node q in the tree. Here, f i \u2208 R J denotes the feature vector (which can be any one of those in the multi-resolution pyramid as described earlier) and v i denotes a The unit vector u is directed from x to o z and is shown in red. It is rotated in increments of \u03c0/4 to identify the rest of the eight directions. At each radius \u2208 { r 1 , r 2 , r 3 , r 4 }, along these eight different directions, we evaluate the mean of image intensities within a 3D cubic region (depicted here as colored 2D squares). The cubic widths { w 1 , w 2 , w 3 , w 4 } are also shown for a set of regions. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) value in the modality to be predicted (which is the value of the image to be predicted at the same level in the multi-resolution pyramid). Let the mean of the target intensities of the samples at node q be v q . The nodal splits are determined during training by randomly selecting one third of the features (i.e., one third of the indices j = 1 , . . . , J of the feature vector) and then finding the feature j in this subset together with a corresponding threshold \u03c4 j that together minimize a least squares criterion as defined in the following paragraph.\nThe squared distance (SD) from the mean of the target intensities at node q is given by\nwhere t is the number of training samples at node q . This quantity is seen as a measure of compactness of the target intensities in a node. Once feature j and threshold \u03c4 j are selected to determine the split, as described below, then the training data are split\nThese training samples are then used in new left and right child nodes in the tree. But how is the split determined? In fact, the split is made to make the left and right training data subsets as compact as possible. Given the split, each of these child nodes has its own compactness given by\nwhere t L and t R are the number of samples in the left and the right child nodes, respectively, and the target intensities v i are taken from their corresponding training samples. The compactness of the resulting training data can be maximized, if j and \u03c4 j are chosen as\nThis is the least squares criterion that determines node splits during training to create each random forest regressor. The growth of each tree is controlled by three factors: t p , t c , and . If a node has fewer than t p training samples, it will not be split into child nodes (and it therefore becomes a leaf node). If the optimum split of a given node leads to one of the child nodes having fewer than t c training samples, then it will not be split (and it likewise becomes a leaf node). We set t p = 2 t c and t c = 5 in our experiments. If SD q \u2212 ( SD qL + SD qR ) < SD q then the node is not split (and it likewise becomes a leaf node). We used = 10 \u22126 in our experiments.\nA master list of training data is created from the atlas data by sampling the features in all tissues making sure that abnormal tissues (like WMLs) are well-represented. Each RF regressor consists of sixty trees where each tree is learned from bootstrapped training data, where bootstrapping is carried out by randomly choosing N = 1 \u00d7 10 5 training samples (with replacement) for each tree.\nEach trained tree contains the feature j and threshold \u03c4 j at each non-leaf node and the average value of the target intensity at each leaf node. To use an RF regressor, the same feature vector is \"fed\" to each root node and each tree is traversed according to the stored feature indices and thresholds until a leaf node is reached whereupon the tree provides an intensity. The average intensity of all sixty trees is then formed as the output of the regressor."}, {"section_title": "Predicting a new image", "text": "Given a subject image set, a Gaussian image pyramid is constructed and at each level s the subject image set is B s = (see stage (e) in Fig. 1 ). This process continues until s = 1 . At s = 1 , the highest available resolution, feature vectors f 1 (x ) = [ p 1 (x ) , q 1 (x ) , v (x ) ] which includes the high resolution context descriptor v ( x ) are calculated. The trained random forest RF 1 is applied to produce the final high resolution synthetic image \u02c6 b 1 r (stage (f) in Fig. 1 ) . We have provided a very general description of the REPLICA image synthesis pipeline. Depending on the complexity of the application we might use a subset of this pipeline. For example, when synthesizing images for which the input images are already skullstripped, the intensity mapping does not need the entire multiresolution treatment as the high resolution features are sufficient. REPLICA has a number of free parameters that can be tuned to improve the resulting synthesis. We performed extensive parameter selection experiments, the results of which are available in the Supplementary Materials. These parameters were set as follows, (a) number of trees ( = 60 ), (b) number of samples in a leaf node ( = 5 ), (c) size of local 3D patch ( = 3 \u00d7 3 \u00d7 3 ), (d) number of individuals in the atlas ( = 1 ), (e) use of alternate atlas images (does not affect synthesis), and (f) use of context and multi-resolution features, the results of which are shown in Section 3.2 . For the remaining parameter selection experiments, please refer to the provided Supplementary Materials document."}, {"section_title": "Results", "text": "In Section 3.1 we describe results of synthesizing skull-stripped T 2 w images using REPLICA. In Section 3.2 we describe results of synthesizing T 2 w images that have not been skull-stripped. In Section 3.3 we demonstrate the use of REPLICA to synthesize FLAIR images, and finally in Section 3.4 we perform intensity standardization between SPGR and MPRAGE datasets using REPLICA."}, {"section_title": "Synthesis of T 2 w images", "text": "In this experiment, we synthesized T 2 w images from skullstripped T 1 w MPRAGE images taken from the multimodal reproducibility resource (MMRR) data ( Landman et al., 2011 ) . The MMRR data consists of 21 subjects, each with two imaging sessions acquired within an hour of each other. T 2 w images can be used as registration targets while performing distortion correction on echo-planar images (EPI) images and also as input to lesion segmentation algorithms. Therefore, if T 2 w images are absent, we can use REPLICA to synthesize them. We compared REPLICA to the MIMECS method ) and a multi-atlas registration and intensity fusion method which we refer to as FUSION ( Burgos et al., 2014 ) . We used the NiftyReg affine and free-form deformation registration algorithm ( Ourselin et al., 2001; Modat et al., 2010 ) and implemented the intensity fusion as described in Burgos et al. (2014) . Burgos et al. use 40 atlases in their experiments with CT images. However, we have observed that using more than 20 atlases did not help in synthesizing abnormalities like lesions correctly. While in normal anatomy, the finer regions of the cortex get smoothed out due to fusion of more patches and are susceptible to mis-registration. Additionally, a large collection of appropriate paired atlas images is generally not available for most datasets. When atlas images are available, the registration + fusion for 20 + atlases can result in a run-time of 2 hours per synthetic image, which makes it unsuitable as a quick, preprocessing step. , and (e) REPLICA (our method), followed by the corresponding difference images with respect to the true T 2 w in (f)-(h), respectively. The lesion (green arrow) and the cortex (orange arrow) in the true image are correctly synthesized by MIMECS and REPLICA, but not by FUSION. The dark boundary just outside the cerebral tissue (yellow arrow) is incorrectly synthesized as bright by MIMECS, but not by FUSION and REPLICA. The maximum intensity in the difference images is 224.25 whereas the maximum image intensity is 255. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)\nWe used data from one randomly chosen subject as training for REPLICA (the same subject was used in the parameter selection experiments described in the Supplementary Materials) and MIMECS and one image each from five subjects as the atlases for FUSION. We set the parameters for FUSION to \u03b2 = 0 . 5 (weighting parameter) and \u03ba = 4 (use the top 4 best patch matches to fuse); refer to Burgos et al. (2014) for more details. For the remaining 16 \u00d7 2 = 32 MPRAGE images, we synthesized T 2 w images using all three methods. The input MPRAGE images were intensity standardized by scaling such that the white matter peak intensity in the histogram is 1. Synthesis of skull-stripped T 2 w images can be achieved with an intensity transformation that is captured well using just the high resolution features. Therefore we do not use the entire multi-resolution framework for this application but only use the high resolution context descriptors and a local\nThe atlas and subject images have the following specifications: a 1 : MPRAGE image (3 T, TR = 6 . 7 ms, TE = 3 . 1 ms, TI = 842 ms, 1.0 \u00d7 1.0 \u00d7 1.2 mm 3 voxel size) a 2 : T 2 w image from the second echo of a DSE (3 T, TR = 6653 ms, TE 1 = 30 ms, TE 2 = 80 ms, 1.5 \u00d7 1.5 \u00d7 1.5 mm 3 voxel size) b 1 : MPRAGE image (3 T, TR = 6 . 7 ms, TE = 3 . 1 ms, TI = 842 ms, 1.0 \u00d7 1.0 \u00d7 1.2 mm 3 voxel size)\nFor evaluation of synthesis quality, we used PSNR (peak signal to noise ratio), which is a mean squared error-based metric, UQI (universal quality index) ( Wang and Bovik, 2002 ) , and SSIM (structural similarity) ( Wang et al., 2004 ) , with respect to the ground truth images. UQI and SSIM are more sensitive to perceptual differences in image structure than PSNR since they take into account properties of the human visual system. For both UQI and SSIM, a value of 1 indicates that the images are equal to each other; otherwise their values lie between 0 and 1. UQI and SSIM also quantify the differences in the luminance and contrast between the two images. We can see from the results in Table 1 that REPLICA performs significantly better than the other methods for all metrics except PSNR. Fig. 3 shows the results for all three methods along with the true T 2 w image. FUSION ranks highest for PSNR, which can be explained by the fact that unlike the cortex, large, homogeneous regions of white matter have been reproduced quite well. However, it also produces anatomically incorrect images, especially in the presence of abnormal tissue anatomy (lesions for example) and the cortex (see Fig. 3 (c) ). MIMECS has a lower PSNR primarily due to boundary voxels, which are mostly skull voxels that were wrongly synthesized as CSF (see Fig. 3 (d) ). Overall, REPLICA produces an image that is visually closest to the true T 2 w image and has the highest UQI and SSIM values."}, {"section_title": "Synthesis of whole head images", "text": "Synthesis of whole head images, i.e. head images that are not skull-stripped, is challenging due to the presence of additional anatomical structures and the high variability of tissue intensities. Bone structures are typically dark on MRI and are adjacent to fat and skin, which are typically bright resulting in these regions having an extremely wide intensity range. The intensity transformation that needs to be learned is highly nonlinear and is generally Fig. 4. (a) Original input MPRAGE, (b) REPLICA T 2 w synthesis using 3 \u00d7 3 \u00d7 3 patch as feature vector, (c) with additional high resolution context descriptor feature, and (d) using the full, multi-resolution REPLICA framework. (e) shows the ground truth T 2 w image. In the next row we have the corresponding difference images with respect to the real T 2 w image in (f) -(h) respectively. It is clear that using multi-resolution REPLICA produces a higher quality synthesis for the challenging task of synthesis of full-head images. (i) -(p) Show the same images for a more superior slice. The maximum intensity in these images is 255. The difference images have a maximum intensity around 60.\none-to-many, especially if small patches are used as features. Small patches are useful if there are multiple input modalities capable of providing the necessary information to synthesize extra-cerebral tissues, as Roy et al. (2014) demonstrated by synthesizing CT images from input ultrashort echo time (UTE) images. The two UTE images together provide enough intensity information to differentiate the bone from soft tissues. But when these extra modalities are not available additional features are needed. Multi-resolution and context features, as described in Section 2.1 , provide additional information about the location of a voxel within the brain, enabling better synthesis of these tissues. This experiment demonstrates the impact of these additional features. We use the T 1 w MPRAGE images from the MMRR dataset ( Landman et al., 2011 ) and synthesize corresponding full-head T 2 w images. The input MPRAGE images were intensity standardized by scaling such that the white matter peak intensity in the histogram is 1. Data from one subject (the same data was used in the experiment described in Section 3.1 ) was used for training. The atlas and subject images have the same specifications as described in Section 3.1 . REPLICA parameters were set as follows: number of trees = 60 , and t c = 5 , t p = 2 t c , and = 10 \u22126 . We ran REPLICA with three different settings for input features: (a) only local 3 \u00d7 3 \u00d7 3 patches, (b) local patches + context features, and (c) local patches + context features + multi-resolution framework. Using each of these settings, we synthesized 20 \u00d7 2 = 40 synthetic images. Fig. 4 shows a progression of REPLICA results for a synthetic T 2 w image of a subject from the corresponding MPRAGE with both brain and non-brain tissues using a 3 \u00d7 3 \u00d7 3 local patch ( Fig. 4 (b) and (j) ), a combination of the local patch and high resolution context descriptor ( Fig. 4 (c) and (k) ), and the full REPLICA multi-resolution framework ( Fig. 4 (d) and (l) ). Fig. 4 (f) -(h), (n)-(p) also show the corresponding difference images with the ground truth T 2 w image in Fig. 4 (e) and (m). In this figure we show two slices, one from the inferior head region with a high variability of tissue intensities and structures, and another from a slightly superior region with more structured regions. This qualitative comparison reveals that the full multi-resolution framework works best.\nFor a quantitative comparison, we calculated PSNR, UQI, and SSIM from the 40 synthetic images to compare the effect of these feature settings. Boxplots for these metrics are shown in Fig. 5 . These plots show quantitatively that the multi-resolution version of REPLICA is statistically significantly better ( p < 0.01, one-tailed t -test) than the other two approaches in synthesizing T 2 w images from MPRAGE images when the whole head is present.\nFurther, we provide a qualitative comparison of REPLICA against MIMECS and FUSION for this task (see Fig. 6 ). At first glance the FUSION result ( Fig. 6 (c) ) looks appealing, but has errors in the cortical region which are similar to those found in the FUSION result in Section 3.1 . MIMECS uses a small 3 \u00d7 3 \u00d7 3-sized patch and is unable to disambiguate between skull and CSF, resulting in large errors, especially in the ventricles ( Fig. 6 (d) ). The REPLICA result ( Fig. 6 (e) ) looks visually closer to the truth. It appears smoother in some regions due to dependence on low-resolution information coming from the lower levels of the multi-resolution framework."}, {"section_title": "Synthesis of FLAIR images", "text": "In this experiment, we used REPLICA to synthesize a FLAIR image from T 1 w, T 2 w, and P D w images. The FLAIR pulse sequence is routinely used to image patients with multiple sclerosis (MS) and other diseases. It is particularly useful for visualization of white matter lesions (WML) which are observed in MS patients. White matter lesions appear hyperintense in FLAIR images and can be easily delineated using automated segmentation algorithms. The FLAIR sequence needs a long TI value and therefore, is generally acquired at a lower resolution for a faster scan time. FLAIR images also frequently suffer from artifacts, which result in hyperintensities that can be mistaken for lesions ( Stuckey et al., 2007 ) . Missing FLAIR images can also pose hurdles in lesion segmentation, as most leading lesion segmentation algorithms use FLAIR as an input modality ( Shiee et al., 2010; Geremia et al., 2011; Llado et al., 2012 ) . Synthesizing missing FLAIR images can help avoid these issues and enable segmentation and further image analysis.\nThe atlas set we used for this experiment is: All the modalities were registered and resampled to the MPRAGE image. All modalities were intensity standardized by scaling the intensities such that the white matter peak in each of the histograms was 1. This experiment did not need the multiresolution framework as the image analysis takes place on skullstripped images. We also tweaked the prediction of the decision trees in the random forest ensemble in this experiment. Instead of calculating the mean of the sample predictions in a single leaf, we calculated the mode. Calculating the mean resulted in oversmooth images, especially at the lesion-WM boundaries, which resulted in overestimation of lesion size by the segmentation algorithm. Using the mode results in crisper edges and better lesion segmentation. Input images along with the real and synthetic FLAIR images are shown in Fig. 7 .\nWe used our in-house MS patient image dataset with 125 images belonging to 84 subjects, with some subjects having images acquired longitudinally. We compared the synthesized images with existing true images using image similarity metrics (see Table 2 ). These values indicate that the synthetic FLAIR images are visually similar to the corresponding real FLAIR images. FLAIR synthesis results using FUSION ( Fig. 8 (b) and (f)), MIMECS ( Fig. 8 (c) and (g)) and REPLICA ( Fig. 8 (d) and (h)) for two different subjects can be compared visually with the corresponding real FLAIR images. FU-SION was run with the same parameters as for the T 2 w synthesis and it is unable to faithfully construct a synthetic image that is close enough to the ground truth. FUSION is based on registration and intensity fusion of multiple atlas images, and cannot synthesize lesion intensities at lesion locations if those intensities are not present in the atlas images at precisely those locations. We used only the T 1 w images for the multi-atlas registration step in FUSION. Presence of lesions also affects the quality of the registrations itself, thus leading to a worse than expected result. MIMECS works better than FUSION but has errors synthesizing large lesion areas and very small lesions. Overall, REPLICA produces the most visually acceptable synthetic FLAIR image. We have not quantitatively compared REPLICA with MIMECS and FUSION on a large dataset because of the obvious and large errors in both of the MIMECS and FUSION results. Next, we used the synthetic FLAIR images as inputs to a tissue segmentation algorithm. If synthesis has been done correctly, the segmentation algorithm should behave similarly when either real or synthetic images are used as inputs. To test this, we used the LesionTOADS algorithm ( Shiee et al., 2010 ) . We compared the overlap of segmentations obtained using synthetic FLAIR images to those obtained using real FLAIR images in terms of Dice coefficients. The Dice coefficients for WM, GM, CSF, and WML classes are shown in Table 3 . Fig. 9 shows the segmentations by LesionTOADS on real and synthetic FLAIR images. The overlap is high for WM, GM, and CSF, however it is relatively low for the WML class. The lesions are small and diffuse and even a small difference in the overlap can cause a low value for the Dice coefficient ( Geremia et al., 2011 ) .\nThus we looked at the lesion volumes as provided by Lesion-TOADS for real and synthetic FLAIR images. To understand how different the lesion volumes are for the synthetic images as compared to the real images, we created a Bland-Altman plot ( Bland and Altman, 1986 ) for these measurements (see Fig. 10 ). If y 1 and y 2 are two measurements by two different methods consisting of n samples each, then the Bland-Altman plot is a scatter plot of y 1 \u2212 y 2 versus (y 1 + y 2 ) / 2 . The measurements are considered to be interchangeable if 0 lies within \u00b11.96 \u03c3 where \u03c3 is the standard de- viation of y 1 \u2212 y 2 . Fig. 10 shows the Bland-Altman plot where y 1 are the lesion volumes for synthetic FLAIR images and y 2 are the lesion volumes for real FLAIR images, as produced, in both cases, by LesionTOADS. Even though the difference ( y 1 \u2212 y 2 ) is not zeromean ( p > 0.05 using a one sample t -test), both the measurements can be used interchangeably because 0 lies within \u00b1 1.96 \u03c3 in the plot ( Bland and Altman, 1986 ) ."}, {"section_title": "Intensity standardization", "text": "Tissue segmentation and cortical reconstruction in MRI are generally reliant on T 1 w images acquired with sequences like MPRAGE, SPGR, etc. ( Pham et al., 20 0 0; Dale and Fischl, 1999; Van Leemput et al., 1999 ) . However, most segmentation algorithms are not robust to variabilities in the T 1 w image contrasts ( Ny\u00fal and Udupa, 1999 ) . Intensity standardization of different contrasts has been proposed to alleviate this issue ( Ny\u00fal et al., 20 0 0; Ny\u00fal and Udupa, 1999; Jog et al., 2013b ) . Image synthesis can be used to standardize intensities by creating synthetic, standardized images from given images. These synthetic images can belong to a given, reference modality, on which the algorithm behavior is wellunderstood. We demonstrate such an intensity standardization application using the Baltimore Longitudinal Study of Aging (BLSA) dataset ( Thambisetty et al., 2010 ) . In this experiment, we used a subset of the dataset, consisting of 82 scans of 60 subjects, some of which are longitudinal acquisitions. Each scanning session was carried out on a Philips 1.5 T scanner and has an SPGR (see Fig. 11 (a) ) and an MPRAGE (see Fig. 11 (c) ) acquisition from the same session. We chose this dataset as it can mimic the scenario of a multi-site data acquisition where protocols differ across the sites. We use an in-house implementation of the probabilistic atlas-driven, EM-base segmentation algorithm by Van Leemput et al. (1999) , which we refer to as AtlasEM. AtlasEM provides us with a 4-class segmentation, the classes being sulcal CSF, GM, WM, and ventricles. We segmented the SPGR and the MPRAGE images using AtlasEM. In an ideal scenario, where the algorithm is impartial to the underlying T 1 w input, the segmentations should be identical. However, as we can observe in Figs. 11 (d) and 11 (f), the segmentations are quite different. We used REPLICA to generate a synthetic MPRAGE from input SPGR images, and ran AtlasEM segmentations on the synthetic MPRAGEs. The atlas set used for this experiment was: The synthesis did not need the multi-resolution framework as we work with skull-stripped images because the final segmentation takes place on skull-stripped Carass et al. (2011) images. We used the synthetic MPRAGE as an input for AtlasEM. Our goal is to show that the segmentations are now closer to those obtained by a real MPRAGE. Fig. 11 (b) shows the REPLICA-generated synthetic MPRAGE and Fig. 11 (e) shows the segmentation for the same. Visually, it is closer to the segmentation obtained from the real MPRAGE Fig. (11 (f) ), especially at the CSF-GM interface.\nWe also looked at tissue volumes provided by AtlasEM on all three sets of images, SPGR, MPRAGE, and synthetic MPRAGE. As stated earlier, if the AtlasEM algorithm were robust to the input modality, the tissue volumes for a particular tissue should be identical for SPGR and MPRAGE. In Fig. 12 , we show scatter plots for WM volumes obtained on real MPRAGEs ( x -axis) and those obtained for SPGRs (blue) and synthetic MPRAGEs (red). We also show the least square line fits to the scatter plots (blue for SPGR, red for synthetic MPRAGE). In the ideal scenario, where the algorithm is indifferent to input contrast, the least square line fits should be close to the x = y line. We can see that for WM, synthetic MPRAGE volumes fit is closer to the identity line than SPGR volumes fit."}, {"section_title": "Discussion and conclusions", "text": "We have described a new image synthesis algorithm called REPLICA. We have shown that REPLICA demonstrates significant improvement in image quality over other state-of-the-art synthesis algorithms ( Section 3.1 ). We have also described applications where image synthesis in general and REPLICA in particular can be beneficial as a preprocessing step for subsequent image processing steps. The T 2 w synthesis for full-head images described in Section 3.2 also highlights the capability of REPLICA to handle complex image synthesis scenarios, with limited intensity information at its disposal. To our knowledge, REPLICA is the first intensity transformation-based synthesis approach to achieve this.\nSynthesis of FLAIR images is a key application ( Section 3.3 ) that has not been demonstrated before; this process can be useful if FLAIR images were not acquired, if the acquired FLAIR images are corrupted, or if higher-resolution FLAIR images are required. REPLICA is computationally fast. Training an ensemble can take up to 20 min, but needs to be done only once. With (easy) parallelization over eight cores, synthesis of a 256 \u00d7 256 \u00d7 173 image takes less than a minute on a 3 GHz computer. Given the typical times of neuroimaging pipelines (usually many hours), this makes the use of REPLICA as a preprocessing step quite feasible.\nWe have also shown use of REPLICA in intensity standardization applications where we have two imaging datasets with slightly different acquisition properties that produce different image processing results.\nREPLICA has some limitations that should be addressed in the future. Since the predicted value of a random forest is the average of the results of all trees (each of which is an average of at least five values), the synthetic images often appear to have lower noise and be slightly smoother than their real counterparts. Lower noise may benefit algorithms but reduction in resolution is not typically beneficial. In a previous publication we have demonstrated superresolution as an application of REPLICA ( Jog et al., 2014b ) , so there may be a relatively straightforward way to enhance resolution in the exact amount needed to offset the inherent loss of resolution due to averaging. Also, we used the mode instead of the average in certain applications, but this is also an empirical strategy and not guaranteed to address the problem. Another limitation concerns the features that are used to train REPLICA. Although our features are sensible and effective for the applications we have explored, they are nevertheless empirically selected and they may not be optimal for these tasks or new scenarios that may be encountered in the future.\nIn our parameter selection experiments, despite improvements in image quality metrics with increasing patch size, in the experiments we used a 3 \u00d7 3 \u00d7 3 patch. Our rationale is as follows. First, we observed that improvements in our performance metrics from patch sizes 3 \u00d7 3 \u00d7 3 to 5 \u00d7 5 \u00d7 5 appear to be primarily due to noise reduction in large, homogeneous white matter regions. Such noise reduction yields synthetic images that are much smoother in appearance than real images. There is a concern that such images will not perform the same as real images in subsequent image processing. A second reason for using 3 \u00d7 3 \u00d7 3 patches is due to the increased computer memory burden (almost a factor of 3) of 5 \u00d7 5 \u00d7 5 patches. A third reason is that with the increase in feature dimensionality comes a requirement to increase the number of training samples and this puts a further memory burden on the software as well as a computation time increase. In particular, the training time goes from about 5 to 40 min when going from 3 \u00d7 3 \u00d7 3 to 5 \u00d7 5 \u00d7 5 patches. Taking into consideration the concerns over unnatural noise reduction and the computational costs, we chose to use a 3 \u00d7 3 \u00d7 3-sized patch in all of our experiments. This parameter of REPLICA could be changed on a case-by-case basis to gain advantage in subsequent image processing steps due to noise reduction.\nIn conclusion, the REPLICA image synthesis method was described and shown to be effective in medical image processing tasks. REPLICA was shown to be beneficial when images are missing or corrupted for subsequent processing steps. It is a simple, fast, and effective approach that can be readily employed as a preprocessing step in many neuroimage processing pipelines."}]