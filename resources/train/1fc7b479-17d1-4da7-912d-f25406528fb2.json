[{"section_title": "Abstract", "text": "Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the 1 and 2 norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer f eature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. We also develop a screening method-called DPC (decomposition of convex set)-for the nonnegative Lasso problem. Experiments on both synthetic and real data sets show that TLFre and DPC improve the efficiency of SGL and nonnegative Lasso by several orders of magnitude."}, {"section_title": "Introduction", "text": "Sparse-Group Lasso (SGL) [7, 23] is a powerful regression technique in identifying important groups and features simultaneously. To yield sparsity at both group and individual feature levels, SGL combines the Lasso [25] and group Lasso [35] penalties. In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34] , signal processing [24] , bioinformatics [18] etc. Many research efforts have been devoted to developing efficient solvers for SGL [7, 23, 13, 28] . However, when the feature dimension is extremely high, the complexity of the SGL regularizers imposes great computational challenges. Therefore, there is an increasingly urgent need for nontraditional techniques to address the challenges posed by the massive volume of the data sources.\nRecently, El Ghaoui et al. [6] proposed a promising feature reduction method, called SAFE screening, to screen out the so-called inactive features, which have zero coefficients in the solution, from the optimization. Thus, the size of the data matrix needed for the training phase can be significantly reduced, which may lead to substantial improvement in the efficiency of solving sparse models. Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33] , group Lasso [31, 29, 26] , etc. It is worthwhile to mention that the discarded features by exact feature screening methods such as SAFE [6] , DOME [33] and EDPP [31] are guaranteed to have zero coefficients in the solution. However, heuristic feature screening methods like Strong Rule [26] may mistakenly discard features which have nonzero coefficients in the solution. More recently, the idea of exact feature screening has been extended to exact sample screening, which screens out the nonsupport vectors in SVM [17, 30] and LAD [30] . As a promising data reduction tool, exact feature/sample screening would be of great practical importance because they can effectively reduce the data size without sacrificing the optimality [16] .\nHowever, all of the existing feature/sample screening methods are only applicable for the sparse models with one sparsity-inducing regularizer. In this paper, we propose an exact two-layer feature screening method, called TLFre, for the SGL problem. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to have zero coefficients in the solution. To the best of our knowledge, TLFre is the first screening method which is capable of dealing with multiple sparsity-inducing regularizers.\nWe note that most of the existing exact feature screening methods involve an estimation of the dual optimal solution. The difficulty in developing screening methods for sparse models with multiple sparsity-inducing regularizers like SGL is that the dual feasible set is the sum of simple convex sets. Thus, to determine the feasibility of a given point, we need to know if it is decomposable with respect to the summands, which is itself a nontrivial problem (see Section 2). One of our major contributions is that we derive an elegant decomposition method of any dual feasible solutions of SGL via the framework of Fenchel's duality (see Section 3). Based on the Fenchel's dual problem of SGL, we motivate TLFre by an in-depth exploration of its geometric properties and the optimality conditions in Section 4. We derive the set of the regularization parameter values corresponding to zero solutions. To develop TLFre, we need to estimate the upper bounds involving the dual optimal solution. To this end, we first give an accurate estimation of the dual optimal solution via the normal cones. Then, we formulate the estimation of the upper bounds via nonconvex optimization problems. We show that these nonconvex problems admit closed form solutions.\nThe rest of this paper is organized as follows. In Section 2, we briefly review some basics of the SGL problem. We then derive the Fenchel's dual of SGL with nice geometric properties under the elegant framework of Fenchel's Duality in Section 3. In Section 4, we develop the TLFre screening rule for SGL. To demonstrate the flexibility of the proposed framework, we extend TLFre to the nonnegative Lasso problem in Section 5. Experiments in Section 6 on both synthetic and real data sets demonstrate that the speedup gained by the proposed screening rules in solving SGL and nonnegative Lasso can be orders of magnitude.\nNotation: Let \u00b7 1 , \u00b7 and \u00b7 \u221e be the 1 , 2 and \u221e norms, respectively. Denote by B n 1 , B n , and B n \u221e the unit 1 , 2 , and \u221e norm balls in R n (we omit the superscript if it is clear from the context). For a set C, let int C be its interior. If C is closed and convex, we define the projection operator as P C (w) := argmin u\u2208C w \u2212 u . We denote by I C (\u00b7) the indicator function of C, which is 0 on C and \u221e elsewhere. Let \u0393 0 (R n ) be the class of proper closed convex functions on R n . For f \u2208 \u0393 0 (R n ), let \u2202f be its subdifferential. The domain of f is the set dom f := {w : f (w) < \u221e}. "}, {"section_title": "Basics and Motivation", "text": "In this section, we briefly review some basics of SGL. Let y \u2208 R N be the response vector and X \u2208 R N \u00d7p be the matrix of features. With the group information available, the SGL problem [7] is\nwhere n g is the number of features in the g th group, X g \u2208 R N \u00d7ng denotes the predictors in that group with the corresponding coefficient vector \u03b2 g , and \u03bb 1 , \u03bb 2 are positive regularization parameters. Without loss of generality, let \u03bb 1 = \u03b1\u03bb and \u03bb 2 = \u03bb with \u03b1 > 0. Then, problem (2) becomes:\nBy the Lagrangian multipliers method [4] (see the supplement), the dual problem of SGL is\nIt is well-known that the dual feasible set of Lasso is the intersection of closed half spaces (thus a polytope); for group Lasso, the dual feasible set is the intersection of ellipsoids. Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6 ]. When we incorporate multiple sparse-inducing regularizers to the sparse models, problem (4) indicates that the dual feasible set can be much more complicated. Although (4) provides a geometric description of the dual feasible set of SGL, it is not suitable for further analysis. Notice that, even the feasibility of a given point \u03b8 is not easy to determine, since it is nontrivial to tell if X T g \u03b8 can be decomposed into b 1 + b 2 with b 1 \u2208 \u03b1 \u221a n g B and b 2 \u2208 B \u221e . Therefore, to develop screening methods for SGL, it is desirable to gain deeper understanding of the sum of simple convex sets.\nIn the next section, we analyze the dual feasible set of SGL in depth via the Fenchel's Duality Theorem. We show that for each X T g \u03b8 \u2208 D \u03b1 g , Fenchel's duality naturally leads to an explicit decomposition X T g \u03b8 = b 1 + b 2 , with one belonging to \u03b1 \u221a n g B and the other one belonging to B \u221e .\nThis lays the foundation of the proposed screening method for SGL."}, {"section_title": "The Fenchel's Dual Problem of SGL", "text": "In Section 3.1, we derive the Fenchel's dual of SGL via Fenchel's Duality Theorem. We then motivate TLFre and sketch our approach in Section 3.2. In Section 3.3, we discuss the geometric properties of the Fenchel's dual of SGL and derive the set of (\u03bb, \u03b1) leading to zero solutions."}, {"section_title": "The Fenchel's Dual of SGL via Fenchel's Duality Theorem", "text": "To derive the Fenchel's dual problem of SGL, we need the Fenchel's Duality Theorem as stated in Theorem 1. The conjugate of f \u2208 \u0393 0 (R n ) is the function f * \u2208 \u0393 0 (R n ) defined by\n, and T (\u03b2) = y \u2212 X\u03b2 be an affine mapping from R p to R N . Let p * , d * \u2208 [\u2212\u221e, \u221e] be primal and dual values defined, respectively, by the Fenchel problems:\nOne has p * \u2265 d * . If, furthermore, f and \u2126 satisfy the condition 0 \u2208 int (dom f \u2212 y + Xdom \u2126), then the equality holds, i.e., p * = d * , and the supreme is attained in the dual problem if finite.\nWe omit the proof of Theorem 1 since it is a slight modification of Theorem 3.3.5 in [3] . Let f (w) = 1 2 w 2 , and \u03bb\u2126(\u03b2) be the second term in (3). Then, SGL can be written as\nTo derive the Fenchel's dual problem of SGL, Theorem 1 implies that we need to find f * and \u2126 * . It is well-known that f * (z) = 1 2 z 2 . Therefore, we only need to find \u2126 * , where the concept infimal convolution is needed:\nThe infimal convolution of h and g is defined by\nand it is exact at a point \u03be if there exists a \u03b7 * (\u03be) such that\nh g is exact if it is exact at every point of its domain, in which case it is denoted by h g.\nWith the infimal convolution, we derive the conjugate function of \u2126 in Lemma 3.\n. . . , G. Then, the following hold:\nwhere \u03be g \u2208 R ng is the sub-vector of \u03be corresponding to the g th group.\nTo prove Lemma 3, we first cite the following technical result.\nWe now give the proof of Lemma 3.\nProof. The first part can be derived directly by the definition as follows:\nTo show the second part, Theorem 4 indicates that we only need to show (\u2126 \u03b1 1 ) * (\u2126 2 ) * (\u03be) is exact (note that \u2126 \u03b1 1 and \u2126 2 are continuous everywhere). Let us now compute (\u2126 \u03b1 1 ) * (\u2126 2 ) * .\nTo solve the optimization problem in (9), i.e.,\nwe can consider the following problem\nWe can see that the optimal solution of problem (11) must also be an optimal solution of problem (10) . Let \u03b7 * g (\u03be g ) be the optimal solution of (11). We can see that \u03b7 * g (\u03be g ) is indeed the projection of \u03be g on B \u221e , which admits a closed form solution:\nThus, problem (10) can be solved as\nHence, the infimal convolution in Eq. (9) is exact and Theorem 4 leads to\nwhich completes the proof.\nNote that P B\u221e (\u03be g ) admits a closed form solution, i.e., [\n. Combining Theorem 1 and Lemma 3, the Fenchel's dual of SGL can be derived as follows.\nTheorem 5. For the SGL problem in (3), the following hold:\n(i) The Fenchel's dual of SGL is given by:\n(ii) Let \u03b2 * (\u03bb, \u03b1) and \u03b8 * (\u03bb, \u03b1) be the optimal solutions of problems (3) and (13), respectively. Then,\nTo show Theorem 5, we need the Fenchel-Young inequality as follows:\nAny point z \u2208 R n and w in the domain of a function h : R n \u2192 (\u2212\u221e, \u221e] satisfy the inequality\nEquality holds if and only if z \u2208 \u2202h(w).\nWe now give the proof of Theorem 5.\nProof. We first show the first part. Combining Theorem 1 and Lemma 3, the Fenchel's dual of SGL can be written as:\nwhich is equivalent to problem (13) . To show the second half, we have the following inequalities by Fenchel-Young inequality:\nWe sum the inequalities in (16) and (17) together and get\nwhich completes the proof.\nEq. (14) and Eq. (15) are the so-called KKT conditions [4] and can also be obtained by the Lagrangian multiplier method (see A.1 in the supplement).\nRemark 1. We note that the shrinkage operator can also be expressed by\nTherefore, problem (13) can be written more compactly as\nThe equivalence between the dual formulations For the SGL problem, its Lagrangian dual in (4) and Fenchel's dual in (13) are indeed equivalent to each other. We bridge them together by the following lemma."}, {"section_title": "Lemma 7.", "text": "[2] Let C 1 and C 2 be nonempty subsets of R n . Then\nIn view of Lemmas 3 and 7, and recall that\nCombining Eq. (21) and Theorem 1, we obtain the dual formulation of SGL in (4) . Therefore, the dual formulations of SGL in (4) and (13) are the same.\nRemark 2. An appealing advantage of the Fenchel's dual in (13) is that we have a natural decomposition of all points \u03be g \u2208 D \u03b1 g : \u03be g = P B\u221e (\u03be g ) + S 1 (\u03be g )) with P B\u221e (\u03be g ) \u2208 B \u221e and S 1 (\u03be g ) \u2208 C \u03b1 g . As a result, this leads to a convenient way to determine the feasibility of any dual variable \u03b8 by checking if S 1 (X T g \u03b8) \u2208 C \u03b1 g , g = 1, . . . , G."}, {"section_title": "Motivation of the Two-Layer Screening Rules", "text": "We motive the two-layer screening rules via the KKT condition in Eq. (15) . As implied by the name, there are two layers in our method. The first layer aims to identify the inactive groups, and the second layer is designed to detect the inactive features for the remaining groups. by Eq. (15), we have the following cases by noting \u2202 w 1 = SGN(w) and\nIn view of Eq. (22), we can see that\nThe first layer (group-level) of TLFre From (23) in Case 1, we have\nClearly, (R1) can be used to identify the inactive groups and thus a group-level screening rule.\nThe second layer (feature-level) of TLFre Let x g i be the i th column of X g . We have\nIn view of (24) and (25), we can see that\nDifferent from (R1), (R2) detects the inactive features and thus it is a feature-level screening rule. However, we cannot directly apply (R1) and (R2) to identify the inactive groups/features because both need to know \u03b8 * (\u03bb, \u03b1). Inspired by the SAFE rules [6] , we can first estimate a region \u0398 containing \u03b8 * (\u03bb, \u03b1). Let X T g \u0398 = {X T g \u03b8 : \u03b8 \u2208 \u0398}. Then, (R1) and (R2) can be relaxed as follows:\nInspired by (R1 * ) and (R2 * ), we develop TLFre via the following three steps:\nStep 1. Given \u03bb and \u03b1, we estimate a region \u0398 that contains \u03b8 * (\u03bb, \u03b1).\nStep 2. We solve for the supreme values in (R1 * ) and (R2 * ).\nStep 3. By plugging in the supreme values from Step 2, (R1 * ) and (R2 * ) result in the desired two-layer screening rules for SGL."}, {"section_title": "The Set of Parameter Values Leading to Zero Solutions", "text": "In this section, we explore the geometric properties of the Fenchel's dual of SGL in depth-based on which we can derive the set of parameter values such that the primal optimal solutions are 0. We consider the SGL problem in (3) and (2) in Section 3.3.1 and 3.3.2, respectively."}, {"section_title": "The Set of Parameter Values Leading to Zero Solutions of Problem (3)", "text": "Consider the SGL problem in (3). For notational convenience, let\nWe denote the feasible set of the Fenchel's dual of SGL by\nIn view of problem (13) [or (20)], we can see that \u03b8 * (\u03bb, \u03b1) is the projection of y/\u03bb on F \u03b1 , i.e.,\nThus, if y/\u03bb \u2208 F \u03b1 , we have \u03b8 * (\u03bb, \u03b1) = y/\u03bb. Moreover, by (R1), we can see that \u03b2 * (\u03bb, \u03b1) = 0 if y/\u03bb is an interior point of F \u03b1 . Indeed, we have the following stronger result.\nfollowing statements are equivalent:\nProof. The equivalence between (i) and (ii) can be see from the fact that \u03b8 * (\u03bb, \u03b1) = P F \u03b1 (y/\u03bb). Next, we show (ii)\u21d4(iii). Let us first show (ii)\u21d2(iii). We assume that \u03b8 * (\u03bb, \u03b1) = y/\u03bb. By the KKT condition in (14), we have X\u03b2 * (\u03bb, \u03b1) = 0. We claim that \u03b2 * (\u03bb, \u03b1) = 0. To see this, let \u03b2 = 0 with X\u03b2 = 0 be another optimal solution of SGL. We denote by h the objective function of SGL in (3) . Then, we have\nwhich contradicts with the assumption \u03b2 = 0 is also an optimal solution. This contradiction indicates that \u03b2 * (\u03bb, \u03b1) must be 0. The converse direction, i.e., (ii)\u21d0(iii), can be derived directly from the KKT condition in Eq. (14) . Finally, we show the equivalence (i)\u21d4(iv). Indeed, in view of the dual problem in (20), we can see that y/\u03bb \u2208 F \u03b1 if and only if\nWe note that S 1 (X T g y/\u03bb) is monotonically decreasing with respect to \u03bb. Thus, the inequality in (27) is equivalent to (iv), which completes the proof.\nWe note that \u03c1 g in the definition of \u03bb \u03b1 max admits a closed form solution. For notational convenience, let |w| be the vector by taking absolute value of w component-wisely and [w] (k) be the vector consisting of the first k components of w. Lemma 9. We sort 0 = |X T g y| \u2208 R ng in descending order and denote it by z.\nWe omit the proof of Lemma 9 because it is a direct consequence by noting that\nis piecewise quadratic."}, {"section_title": "The Set of Parameter Values Leading to Zero Solutions of Problem (2)", "text": "Theorem 8 implies that the optimal solution \u03b2 * (\u03bb, \u03b1) is 0 as long as y/\u03bb \u2208 F \u03b1 . This geometric property also leads to an explicit characterization of the set of (\u03bb 1 , \u03bb 2 ) such that the corresponding solution of problem (2) is 0. We denote by\u03b2 * (\u03bb 1 , \u03bb 2 ) the optimal solution of problem (2).\nBefore we prove Corollary 10, we first derive the Fenchel's dual of (2) . By letting f (w) = \u221a n g \u03b2 g + \u03bb 2 \u03b2 1 , the SGL problem in (2) can be written as:\nThen, by Fenchel's Duality Theorem, the Fenchel's dual problem of (2) \nLet\u03b2 * (\u03bb 1 , \u03bb 2 ) and\u03b8 * (\u03bb 1 , \u03bb 2 ) be the optimal solutions of problem (2) and (28). The optimality conditions can be written as\nWe denote by F(\u03bb 1 , \u03bb 2 ) the feasible set of problem (28) . It is easy to see that\nWe now present the proof of Corollary 10.\nProof. For notational convenience, let\nThe first half of the statement is (iii)\u21d4(iv). Indeed, by a similar argument as in the proof of Theorem 8, we can see that the above statements are all equivalent to each other. We now show the second half. We first show that\nBy the first half, we only need to show\nIndeed, the definition of \u03bb 1 implies that\nWe note that for any \u03bb 2 \u2265 0, we have\nTherefore, we can see that\nThe proof of (31) is complete. Similarly, to show that \u03bb 2 \u2265 \u03bb max 2 \u21d2\u03b2 * (\u03bb 1 , \u03bb 2 ), we only need to show\nBy the definition of \u03bb 2 , we can see that\nThus, we have y \u2208 F(\u03bb 1 , \u03bb 2 ), which completes the proof."}, {"section_title": "The Two-Layer Screening Rules for SGL", "text": "We follow the three steps in Section 3.2 to develop TLFre. In Section 4.1, we give an accurate estimation of \u03b8 * (\u03bb, \u03b1) via normal cones [20] . Then, we compute the supreme values in (R1 * ) and (R2 * ) by solving nonconvex problems in Section 4.2. We present the TLFre rules in Section 4.3."}, {"section_title": "Estimation of the Dual Optimal Solution", "text": "Because of the geometric property of the dual problem in (13), i.e., \u03b8 * (\u03bb, \u03b1) = P F \u03b1 (y/\u03bb), we have a very useful characterization of the dual optimal solution via the so-called normal cones [20] .\nProposition 11. [20, 2] For a closed convex set C \u2208 R n and a point w \u2208 C, the normal cone to C at w is defined by\nThen, the following hold:\n(iv) Let w / \u2208 C and w = P C (w). Then, P C (w + t(w \u2212 w)) = w for all t \u2265 0.\nBy Theorem 8, \u03b8 * (\u03bb, \u03b1) is known if\u03bb = \u03bb \u03b1 max . Thus, we can estimate \u03b8 * (\u03bb, \u03b1) in terms of \u03b8 * (\u03bb, \u03b1). Due to the same reason, we only consider the cases with \u03bb < \u03bb \u03b1 max for \u03b8 * (\u03bb, \u03b1) to be estimated.\nRemark 3. In many applications, the parameter values that perform the best are usually unknown. To determine appropriate parameter values, commonly used approaches such as cross validation and stability selection involve solving SGL many times over a grip of parameter values. Thus, given\n, we can fix the value of \u03b1 each time and solve SGL by varying the value of \u03bb. We repeat the process until we solve SGL for all of the parameter values.\nTheorem 12. For the SGL problem in (3), suppose that \u03b8 * (\u03bb, \u03b1) is known with\u03bb \u2264 \u03bb \u03b1 max . Let \u03c1 g , g = 1, . . . , G, be defined by Theorem 8. For any \u03bb \u2208 (0,\u03bb), we define\nwhere\nThen, the following hold:\nProof.\n(i) Suppose that\u03bb < \u03bb \u03b1 max . Theorem 8 implies that y/\u03bb / \u2208 F \u03b1 and thus\nBy the third part of Proposition 11, we can see that\nThus, the statement holds for all\u03bb < \u03bb \u03b1 max . Suppose that\u03bb = \u03bb \u03b1 max . By Theorem 8, we have\nIn view of the definition of X * , we have\nwhere n * is the number of feature contained in X * . Moreover, it is easy to see that\nTherefore, to prove the statement, we need to show that\nRecall Remark 1, we have the following identity [see Eq. (19)]\nThus, we have\nConsider the first term on the right hand side of Eq. (35), we have\nWe note that the second term on the right hand side of Eq. (36) can be written as \nCombining Eq. (35) and the inequality in (38), we can see that the inequality in (34) holds. Thus, the statement holds for\u03bb = \u03bb \u03b1 max . This completes the proof.\n(ii) We now show the second half. It is easy to see that the statement is equivalent to\nThus, we will show that the inequality in (39) holds.\nBecause of the first half, we have\nBy letting \u03b8 = \u03b8 * (\u03bb, \u03b1), the inequality in (40) leads to\nIn view of the first half and by letting \u03b8 = 0, the inequality in (40) leads to\nMoreover, the first half also leads to\nBy letting \u03b8 = \u03b8 * (\u03bb, \u03b1), the inequality in (43) results in\nWe can see that the inequality in (44) is equivalent to\nOn the other hand, the right hand side of (39) can be rewritten as\nIn view of (41), (45) and (46), we can see that (39) holds if v \u03b1 (\u03bb,\u03bb), n \u03b1 (\u03bb) \u2265 0. Indeed,\nConsider the first term on the right hand side of Eq. (47). By the first half of (42), we have\nSuppose that\u03bb < \u03bb \u03b1 max . By the second half of (42), we can see that\nConsider the second term on the right hand side of Eq. (47). It is easy to see that\nCombining (48), (49) and Eq. (50), we have v \u03b1 (\u03bb,\u03bb), n \u03b1 (\u03bb) \u2265 0, which completes the proof.\nFor notational convenience, we denote\nTheorem 12 shows that \u03b8 * (\u03bb, \u03b1) lies inside the ball of radius"}, {"section_title": "Solving for the Supreme Values via Nonconvex Optimization", "text": "We solve the optimization problems in (R1 * ) and (R2 * ). To simplify notations, let\nTheorem 12 indicates that \u03b8 * (\u03bb, \u03b1) \u2208 \u0398. Moreover, we can see that X T g \u0398 \u2286 \u039e g , g = 1, . . . , G. To develop the TLFre rule by (R1 * ) and (R2 * ), we need to solve the following optimization problems:"}, {"section_title": "The Solution of Problem (54)", "text": "We consider the following equivalent problem of (54):\nWe can see that the objective function of problem (56) is continuously differentiable and the feasible set is a ball. Thus, problem (56) is nonconvex because we need to maximize a convex function subject to a convex set. We first derive the necessary optimality conditions in Lemma 13 and then deduce the closed form solutions of problems (54) and (56) in Theorem 15.\nLemma 13. Let \u039e * g be the set of optimal solutions of (56) and \u03be * g \u2208 \u039e * g . Then, the following hold:\ng is a boundary point of \u039e g . Then, there exists \u00b5 * \u2265 0 such that\n(iii) Suppose that there exists \u03be 0 g \u2208 \u039e g and \u03be 0 g / \u2208 B \u221e . Then, we have (iiia) \u03be * g / \u2208 B \u221e and \u03be * g is a boundary point of \u039e g , i.e.,\n(iiib) The optimality condition in Eq. (57) holds with \u00b5 * > 0.\nTo show Lemma 13, we need the following proposition."}, {"section_title": "Proposition 14. [9]", "text": "Suppose that h \u2208 \u0393 0 and C is a nonempty closed convex set. If w * \u2208 C is a local maximum of h on C, then \u2202h(w * ) \u2286 N C (w * ).\nWe now present the proof of Lemma 13.\nProof. To simplify notations, let\nBy Eq. (1), we have\nIt is easy to see that h(\u00b7) is continuously differentiable. Indeed, we have\nThen, problem (56) can be written as\nwhere \u039e g = {\u03be g : \u03be g \u2212 c \u2264 r}. Then, Proposition 14 results in\n(i) Suppose that \u03be * g is an interior point of \u039e g . Then, we have N \u039eg (\u03be * g ) = 0. By Eq. (62), we can see that\nTherefore, we have\nBecause S 1 (\u03be g ) = \u03be g \u2212 P B\u221e (\u03be g ) (see Remark 1), Eq. (63) implies that\nThis completes the proof.\n(ii) Suppose that \u03be * g is a boundary point of \u039e g . We can see that\nThen, Eq. (57) follows by combining Eq. (64) and the optimality condition in (62).\n(iii) Suppose that there exists \u03be 0 g \u2208 \u039e g and \u03be 0 g / \u2208 B \u221e .\n(iiia) The definition of \u03be 0 g leads to\nMoreover, we can see that \u03be * g is a boundary point of \u039e g . Because if \u03be * g is an interior point of \u039e g , the first part implies that \u039e g \u2282 B \u221e . This contradicts with the existence of \u03be 0 g . Thus, \u03be * g must be a boundary point of \u039e g , i.e. \u03be * g \u2212 c = r. (iiib) Because \u03be * g is a boundary point of \u039e g , the second part implies that Eq. (57) holds. Moreover, from (iiia), we know that \u03be * g / \u2208 B \u221e . Therefore, both sides of Eq. (57) are nonzero and thus \u00b5 * > 0. This completes the proof.\nBased on the necessary optimality conditions in Lemma 13, we derive the closed form solutions of (54) and (56) \n(ii) Suppose that c is a boundary point of B \u221e , i.e., c \u221e = 1. Then,\n(iii) Suppose that c \u2208 int B \u221e , i.e., c \u221e < 1. Let i * \u2208 I * = {i :\nand c = 0, {r \u00b7 e i * , \u2212r \u00b7 e i * : i * \u2208 I * } , if \u039e g \u2282 B \u221e and c = 0, where e i is the i th standard basis vector.\nProof.\n(i) Suppose that c / \u2208 B \u221e . By the third part of Lemma 13, we have\nBy Eq. (69), we can see that \u00b5 * = 1 because otherwise we would have c = P B\u221e (\u03be * g ) \u2208 B \u221e . Moreover, we can only consider the cases with \u00b5 * > 1 because S 1 (\u03be * g ) = \u00b5 * r and we aim to maximize S 1 (\u03be * g ) . Therefore, if we can find a solution with \u00b5 * > 1, there is no need to consider the cases with \u00b5 * \u2208 (0, 1).\nSuppose that \u00b5 * > 1. Then, Eq. (69) leads to\nIn view of part (iv) of Proposition 11 and Eq. (70), we have\nTherefore, Eq. (71) can be rewritten as\nCombining Eq. (69) and Eq. (73), we have\nThe statement holds by plugging Eq. (74) and Eq. (72) into Eq. (71) and Eq. (73). Moreover, the above discussion implies that \u039e * g only contains one element as shown in Eq. (65).\n(ii) Suppose that c is a boundary point of B \u221e . Then, we can find a point \u03be 0 g \u2208 \u039e g and \u03be 0 g / \u2208 B \u221e . By the third part of Lemma 13, we also have Eq. (68) and Eq. (69) hold. We claim that \u00b5 * \u2208 (0, 1]. The argument is as follows.\nSuppose that \u00b5 * > 1. By the same argument as in the proof of the first part, we can see that Eq. (73) holds. Because S 1 (\u03be * g ) = 0 by Eq. (68), we have S 1 (c) = 0. This implies that c / \u2208 B \u221e . Thus, we have a contradiction, which implies that \u00b5 * \u2208 (0, 1].\nLet us consider the cases with \u00b5 * = 1. Because S 1 (\u03be * g ) = \u00b5 * r [see Eq. (69)] and we want to maximize S 1 (\u03be * g ) , there is no need to consider the cases with \u00b5 * \u2208 (0, 1) if we can find solutions of problem (54) with \u00b5 * = 1. Therefore, Eq. (69) leads to\nBy part (iii) of Proposition 11, we can see that\nCombining Eq. (75) and Eq. (68), the statement holds immediately, which confirms that \u00b5 * = 1.\n(iii) Suppose that c is an interior point of B \u221e .\n(a) We first consider the cases with \u039e g \u2282 B \u221e . Then, we can see that\nIn other words, an arbitrary point of \u039e g is an optimal solution of problem (54). Thus, we have\nOn the other hand, we can see that (b) Suppose that \u039e g \u2282 B \u221e , i.e., there exists \u03be 0 \u2208 \u039e g such that \u03be 0 / \u2208 B \u221e . By the third part of Lemma 13, we have Eq. (68) and Eq. (69) hold. Moreover, in view of the proof of the first and second part, we can see that \u00b5 * \u2208 (0, 1). Therefore, Eq. (69) leads to\nBy rearranging the terms of Eq. (76), we have\nBecause \u00b5 * \u2208 (0, 1), Eq. (76) implies that P B\u221e (\u03be * g ) lies on the line segment connecting \u03be * g and c. Thus, we have\nTherefore, to maximize S 1 (\u03be * g ) = \u03be * g \u2212P B\u221e (\u03be * g ) , we need to minimize P B\u221e (\u03be * g )\u2212 c . Because \u03be * g / \u2208 B \u221e , we can see that P B\u221e (\u03be * g ) is a boundary point of B \u221e . Therefore, we need to solve the following minimization problem:\nSuppose that c = 0. We can see that the set of optimal solutions of problem (79) is\n. For each \u03c6 * g \u2208 \u03a6 * g , we set it as P B\u221e (\u03be * g ). In view of Eq. (77) \nWe can see that\nFor each \u03c6 i * , we set it to P B\u221e (\u03be * g ). Then, we can see that the statement holds by Eq. (77) and Eq. (68). This completes the proof."}, {"section_title": "The Solution of Problem (55)", "text": "Problem (55) can be solved directly via the Cauchy-Schwarz inequality."}, {"section_title": "Theorem 16. For problem (55), we have t", "text": "\u03b1 (\u03bb,\u03bb) and t * g = t * g (\u03bb,\u03bb; \u03b1). Therefore, the set \u0398 in Eq. (52) can be written as\nThen, problem (55) becomes\nWe can see that\nThus, we have\nConsider v * = rx g i / x g i . It is easy to see that o + v * \u2208 \u0398 and\nTherefore, we have\nwhich completes the proof."}, {"section_title": "The Proposed Two-Layer Screening Rules", "text": "To develop the two-layer screening rules for SGL, we only need to plug the supreme values s * g (\u03bb 2 ,\u03bb 2 ; \u03bb 1 ) and t * g i (\u03bb 2 ,\u03bb 2 ; \u03bb 1 ) in (R1 * ) and (R2 * ). We present the TLFre rule as follows.\nTheorem 17. For the SGL problem in (3), suppose that we are given \u03b1 and a sequence of parameter\n) and s * g (\u03bb (j+1) , \u03bb (j) ; \u03b1) be given by Eq. (14), Theorems 12 and 15, respectively. Then, for g = 1, . . . , G, the following holds\nFor the\u011d th group that does not pass the rule in (\n(L 1 ) and (L 2 ) are the first layer and second layer screening rules of TLFre, respectively."}, {"section_title": "Extension to Nonnegative Lasso", "text": "The framework of TLFre is applicable to a large class of sparse models with multiple regularizers.\nAs an example, we extend TLFre to nonnegative Lasso:\nwhere \u03bb > 0 is the regularization parameter and R p + is the nonnegative orthant of R p . In Section 5.1, we transform the constraint \u03b2 \u2208 R p + to a regularizer and derive the Fenchel's dual of the nonnegative Lasso problem. We then motivate the screening method-called DPC since the key step is to decompose a convex set via Fenchel's Duality Theorem-via the KKT conditions in Section 5.2. In Section 5.3, we analyze the geometric properties of the dual problem and derive the set of parameter values leading to zero solutions. We then develop the screening method for nonnegative Lasso in Section 5.4."}, {"section_title": "The Fenchel's Dual of Nonnegative Lasso", "text": "Let I R \nIn other words, we incorporate the constraint \u03b2 \u2208 R p + to the objective function as an additional regularizer. As a result, the nonnegative lasso problem in (81) has two regularizers. Thus, similar to SGL, we can derive the Fenchel's dual of nonnegative Lasso via Theorem 1.\nWe now proceed by following a similar procedure as the one in Section 3.1. We note that the nonnegative Lasso problem in (81) can also be formulated as the one in (6) with f (\u00b7) = (\n, where R p 1 = (1, 1, . . . , 1) T .\nWe omit the proof of Lemma 18 since it is very similar to that of Lemma 3.\nRemark 4. Consider the second part of Lemma 18. Let C 1 = {\u03be : \u03be \u2264 1}, where \"\u2264\" is defined component-wisely. We can see that\nOn the other hand, Lemma 7 implies that\nThus, we have B \u221e + R (i) The Fenchel's dual of nonnegative Lasso is given by:\n(ii) Let \u03b2 * (\u03bb) and \u03b8 * (\u03bb) be the optimal solutions of problems (81) and (82), respectively.Then,\nWe omit the proof of Theorem 19 since it is very similar to that of Theorem 5."}, {"section_title": "Motivation of the Screening Method via KKT Conditions", "text": "The key to develop the DPC rule for nonnegative lasso is the KKT condition in (84). We can see that \u2202 w 1 = SGN(w) and\nTherefore, the KKT condition in (84) implies that\nBy Eq. (85), we have the following rule:\nBecause \u03b8 * (\u03bb) is unknown, we can apply (R3) to identify the inactive features-which have 0 coefficients in \u03b2 * (\u03bb). Similar to TLFre, we can first find a region \u0398 that contains \u03b8 * (\u03bb). Then, we can relax (R3) as follows:\nInspired by (R3 * ), we develop DPC via the following three steps:\nStep 1. Given \u03bb, we estimate a region \u0398 that contains \u03b8 * (\u03bb).\nStep 2. We solve the optimization problem \u03c9 i = sup \u03b8\u2208\u0398 x i , \u03b8 .\nStep 3. By plugging in \u03c9 i computed from Step 2, (R3 * ) leads to the desired screening method DPC for nonnegative Lasso."}, {"section_title": "Geometric Properties of the Fenchel's Dual of Nonnegative Lasso", "text": "In view of the Fenchel's dual of nonnegative Lasso in (82), we can see that the optimal solution is indeed the projection of y/\u03bb onto the feasible set F = {\u03b8 :\nTherefore, if y/\u03bb \u2208 F, Eq. (86) implies that \u03b8 * (\u03bb) = y/\u03bb. If further y/\u03bb is an interior point of F, R3 * implies that \u03b2 * (\u03bb) = 0. The next theorem gives the set of parameter values leading to 0 solutions of nonnegative Lasso.\nTheorem 20. For the nonnegative Lasso problem (81), Let \u03bb max = max i x i , y . Then, the following statements are equivalent:\nWe omit the proof of Theorem 20 since it is very similar to that of Theorem 8."}, {"section_title": "The Proposed Screening Rule for Nonnegative Lasso", "text": "We follow the three steps in Section 5.2 to develop the screening rule for nonnegative Lasso. We first estimate a region that contains \u03b8 * (\u03bb). Because \u03b8 * (\u03bb) admits a closed form solution with \u03bb \u2265 \u03bb max by Theorem 20, we focus on the cases with \u03bb < \u03bb max .\nTheorem 21. For the nonnegative Lasso problem, suppose that \u03b8 * (\u03bb) is known with\u03bb \u2264 \u03bb max . For any \u03bb \u2208 (0,\u03bb), we define\nThen, the following hold:\nProof. We only show that n(\u03bb max ) \u2208 N F (\u03b8 * (\u03bb max )) since the proof of the other statement is very similar to that of Theorem 12.\nBy Proposition 11 and Theorem 20, it suffices to show that\nBecause \u03b8 \u2208 F, we have x * , \u03b8 \u2264 1. The definition of x * implies that x * , y/\u03bb max = 1. Thus, the inequality in (87) holds, which completes the proof.\nTheorem 21 implies that \u03b8 * (\u03bb) is in a ball-denoted by B(\u03bb,\u03bb)-of radius \nBy plugging \u03c9 i into (R3 * ), we have the DPC screening rule for nonnegative Lasso as follows.\nTheorem 22. For the nonnegative Lasso problem, suppose that we are given a sequence of param-\n) is known and the following holds:"}, {"section_title": "Experiments", "text": "We evaluate TLFre for SGL and DPC for nonnegative Lasso in Sections 6.1 and 6.2, respectively, on both synthetic and real data sets. To the best of knowledge, the TLFre and DPC are the first screening methods for SGL and nonnegative Lasso, respectively. "}, {"section_title": "TLFre for SGL", "text": "We perform experiments to evaluate TLFre on synthetic and real data sets in Sections 6.1.1 and 6.1.2, respectively. To measure the performance of TLFre, we compute the rejection ratios of (L 1 ) and (L 2 ), respectively. Specifically, let m be the number of features that have 0 coefficients in the solution, G be the index set of groups that are discarded by (L 1 ) and p be the number of inactive features that are detected by (L 2 ). The rejection ratios of (L 1 ) and (L 2 ) are defined by r 1 = g\u2208G ng m and r 2 = |p| m , respectively. Moreover, we report the speedup gained by TLFre, i.e., the ratio of the running time of solver without screening to the running time of solver with TLFre. The solver used in this paper is from SLEP [12] .\nTo determine appropriate values of \u03b1 and \u03bb by cross validation or stability selection, we can run TLFre with as many parameter values as we need. Given a data set, for illustrative purposes only, we select seven values of \u03b1 from {tan(\u03c8) :\nThen, for each value of \u03b1, we run TLFre along a sequence of 100 values of \u03bb equally spaced on the logarithmic scale of \u03bb/\u03bb \u03b1 max from 1 to 0.01. Thus, 700 pairs of parameter values of (\u03bb, \u03b1) are sampled in total."}, {"section_title": "Simulation Studies", "text": "We perform experiments on two synthetic data sets that are commonly used in the literature [26, 36] . The true model is y = X\u03b2 * + 0.01 , \u223c N (0, 1). We generate two data sets with 250 \u00d7 10000 entries: Synthetic 1 and Synthetic 2. We randomly break the 10000 features into 1000 groups. For Synthetic 1, the entries of the data matrix X are i.i.d. standard Gaussian with pairwise correlation zero, i.e., corr(x i , x i ) = 0. For Synthetic 2, the entries of the data matrix X are drawn from i.i.d. standard Gaussian with pairwise correlation 0.5 |i\u2212j| , i.e., corr(x i , x j ) = 0.5 |i\u2212j| . To construct \u03b2 * , we first randomly select \u03b3 1 percent of groups. Then, for each selected group, we randomly select \u03b3 2 percent of features. The selected components of \u03b2 * are populated from a standard Gaussian and the remaining ones are set to 0. We set \u03b3 1 = \u03b3 2 = 10 for Synthetic 1 and \u03b3 1 = \u03b3 2 = 20 for Synthetic 2.\nThe figures in the upper left corner of Fig. 1 and Fig. 2 show the plots of \u03bb max 1 (\u03bb 2 ) (see Corollary 10) and the sampled parameter values of \u03bb and \u03b1 (recall that \u03bb 1 = \u03b1\u03bb and \u03bb 2 = \u03bb). For the other figures, the blue and red regions represent the rejection ratios of (L 1 ) and (L 2 ), respectively. We can see that TLFre is very effective in discarding inactive groups/features; that is, more than 90% of inactive features can be detected. Moreover, we can observe that the first layer screening (L 1 ) becomes more effective with a larger \u03b1. Intuitively, this is because the group Lasso penalty plays a more important role in enforcing the sparsity with a larger value of \u03b1 (recall that \u03bb 1 = \u03b1\u03bb). The top and middle parts of Table 1 indicate that the speedup gained by TLFre is very significant (up to 30 times) and TLFre is very efficient. Compared to the running time of the solver without screening, the running time of TLFre is negligible. The running time of TLFre includes that of computing X g 2 , g = 1, . . . , G, which can be efficiently computed by the power method [8] . Indeed, this can be shared for TLFre with different parameter values."}, {"section_title": "Experiments on Real Data Set", "text": "We perform experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) data set (http://adni.loni.usc.edu/). The data matrix consists of 747 samples with 426040 single nucleotide polymorphisms (SNPs), which are divided into 94765 groups. The response vectors are the grey matter volume (GMV) and white matter volume (WMV), respectively. The figures in the upper left corner of Fig. 3 and Fig. 4 show the plots of \u03bb max 1 (\u03bb 2 ) (see Corollary 10) and the sampled parameter values of \u03b1 and \u03bb. The other figures present the rejection ratios of (L 1 ) and (L 2 ) by blue and red regions, respectively. We can see that almost all of the inactive groups/features are discarded by TLFre. The rejection ratios of r 1 + r 2 are very close to 1 in all cases. Table 2 shows that TLFre leads to a very significant speedup (about 80 times). In other words, the solver without screening needs about eight and a half hours to solve the 100 SGL problems for each value of \u03b1. However, combined with TLFre, the solver needs only six to eight minutes. Moreover, we can observe that the computational cost of TLFre is negligible compared to that of the solver without screening. This demonstrates the efficiency of TLFre."}, {"section_title": "DPC for Nonnegative Lasso", "text": "In this experiment, we evaluate the performance of DPC on two synthetic data sets and six real data sets. We integrate DPC with the solver [12] to solve the nonnegative Lasso problem along a sequence of 100 parameter values of \u03bb equally spaced on the logarithmic scale of \u03bb/\u03bb max from 1.0 to 0.01. The two synthetic data sets are the same as the ones we used in Section 6.1.1. To construct \u03b2 * , we first randomly select 10 percent of features. The corresponding components of \u03b2 * are populated from a standard Gaussian and the remaining ones are set to 0. We list the six real data sets and the corresponding experimental settings as follows.\na) Breast Cancer data set [32, 21] : this data set contains 7129 gene expression values of 44 tumor samples (thus the data matrix X is of 44 \u00d7 7129). The response vector y \u2208 {1, \u22121} 44 contains the binary label of each sample.\nb) Leukemia data set [1] : this data set contains 11225 gene expression values of 52 samples (X \u2208 R 52\u00d711225 ). The response vector y contains the binary label of each sample. c) Prostate Cancer data set [19] : this data set contains 15154 measurements of 132 patients (X \u2208 R 132\u00d715154 ). By protein mass spectrometry, the features are indexed by time-of-flight values, which are related to the mass over charge ratios of the constituent proteins in the blood. The response vector y contains the binary label of each sample.\nd) PIE face image data set [22, 5] : this data set contains 11554 gray face images (each has 32 \u00d7 32 pixels) of 68 people, taken under different poses, illumination conditions and expressions. In each trial, we first randomly pick an image as the response y \u2208 R 1024 , and then use the remaining images to form the data matrix X \u2208 R 1024\u00d711553 . We run 100 trials and report the average performance of DPC.\ne) MNIST handwritten digit data set [11] : this data set contains grey images of scanned handwritten digits (each has 28 \u00d7 28 pixels). The training and test sets contain 60, 000 and 10, 000 images, respectively. We first randomly select 5000 images for each digit from the training set and get a data matrix X \u2208 R 784\u00d750000 . Then, in each trial, we randomly select an image from the testing set as the response y \u2208 R 784 . We run 100 trials and report the average performance of the screening rules.\nf) Street View House Number (SVHN) data set [15] : this data set contains color images of street view house numbers (each has 32 \u00d7 32 pixels), including 73257 images for training and 26032 for testing. In each trial, we first randomly select an image as the response y \u2208 R 3072 , and then use the remaining ones to form the data matrix X \u2208 R 3072\u00d799288 . We run 20 trials and report the average performance. We present the rejection ratios-the ratio of the number of inactive features identified by DPC to the actual number of inactive features-in Fig. 5 . We also report the running time of the solver with and without DPC, the time for running DPC, and the corresponding speedup in Table 3 . Fig. 5 shows that DPC is very effective in identifying the inactive features even for small parameter values: the rejection ratios are very close to 100% for the entire sequence of parameter values on the eight data sets. Table 3 shows that DPC leads to a very significant speedup on all the data sets. Take MNIST as an example. The solver without DPC takes 50 minutes to solve the 100 nonnegative Lasso problems. However, combined with DPC, the solver only needs 10 seconds. The speedup gained by DPC on the MNIST data set is thus more than 300 times. Similarly, on the SVHN data set, the running time for solving the 100 nonnegative Lasso problems by the solver without DPC is close to seven hours. However, combined with DPC, the solver takes less than two minutes to solve all the 100 nonnegative Lasso problems, leading to a speedup about 230 times. Moreover, we can also observe that the computational cost of DPC is very low-which is negligible compared to that of the solver without DPC."}, {"section_title": "Conclusion", "text": "In this paper, we propose a novel feature reduction method for SGL via decomposition of convex sets. We also derive the set of parameter values that lead to zero solutions of SGL. To the best of our knowledge, TLFre is the first method which is applicable to sparse models with multiple sparsity-inducing regularizers. More importantly, the proposed approach provides novel framework for developing screening methods for complex sparse models with multiple sparsity-inducing regularizers, e.g., 1 SVM that performs both sample and feature selection, fused Lasso and tree Lasso with more than two regularizers. To demonstrate the flexibility of the proposed framework, we develop the DPC screening rule for the nonnegative Lasso problem. Experiments on both synthetic and real data sets demonstrate the effectiveness and efficiency of TLFre and DPC. We plan to generalize the idea of TLFre to 1 SVM, fused Lasso and tree Lasso, which are expected to consist of multiple layers of screening."}]