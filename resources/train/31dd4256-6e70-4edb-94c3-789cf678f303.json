[{"section_title": "", "text": "members who completed interviews during the production (middle phase) were not offered an incentive. Base-year nonrespondents were offered $50 to complete the interview in the early response and nonresponse conversion phases. Of the 17,170 sample members included in the B&B:08/09 student interview data collection, 16,050 (93 percent) were successfully located, and 15,090 either partially or fully completed an interview. The response rate was 88 percent among the eligible sample and was 94 percent among those sample members who were successfully located. The majority of completed interviews (12,240) were obtained in web mode, wherein respondents accessed and completed the interview online. The B&B:08/09 interview took approximately 28 minutes to complete. On average, web respondents completed the interview in 26.6 minutes, telephone respondents completed the interview in 33.5 minutes, and field respondents completed the interview in 31.1 minutes. An evaluation of the quality of the data provided by the B&B:08/09 student interview showed that methodological features, such as help text and conversion text built into the instrument and training and supervision of interviewing staff, aided in the successful administration of the interview. Data collection quality control procedures for the student interview included frequent monitoring of telephone interviewers, a help desk that tracked and resolved difficulties encountered by sample members attempting to complete the web interview, and quality circle meetings and a debriefing for interviewers and tracers. Feedback from these procedures provided useful information for consideration when planning future administrations of B&B."}, {"section_title": "Transcripts", "text": "Postsecondary transcripts were collected as part of B&B:08/09. Transcripts were requested from the institution where B&B sample members completed their bachelor's degree requirements during the 2007-08 academic year (their NPSAS institution), and if this institution had any transcripts for any transfer schools previously attended, the transfer transcripts were requested, as well. To ease burden on participating institutions, the B&B:08/09 transcript collection was combined with the transcript collection for the 2004/09 Beginning Postsecondary Student (BPS:04/09) Longitudinal Study. Together, these transcript collections are referred to as the 2009 Postsecondary Education Transcript Study (PETS:09). Multiple transcript submission methods were available to institutions, including several secure electronic methods, fax, and FedEx. Information and instructions were available on a study website and institution contacting staff members were also available to assist institution staff with transcript submissions and questions about the study. Transcripts were requested from 1,100 postsecondary institutions attended by sample members that were reported by sample members in the B&B:08/09 interviews. Of these institutions, 1,020 (93 percent) provided transcripts for the cohort. Transcript data were collected via a keying and coding process that made use of a specially designed keying and coding system (KCS) and a staff of trained keyer/coders. The KCS was divided into sections based upon the categories of data found on transcripts, including case information, schools and terms, academics, tests, degrees and majors, and courses. A PETS coder was developed for the coding of courses by combining the 2010 NCES Classification of Instructional Programs and the 2003 College Course Map."}, {"section_title": "Background and Objectives of B&B", "text": "B&B is one of several NCES-sponsored studies developed to address the need for nationally representative data on key postsecondary education issues. These studies explore topics related to postsecondary access, choices, enrollment, persistence, progress, curriculum, attainment, continuation into graduate and professional school, and the benefits of postsecondary education to individuals and to society. B&B is a longitudinal spin-off of the National Postsecondary Student Aid Study (NPSAS), which is authorized by the following legislation: \u2022 the Higher Education Act of 1965, as amended by the Higher Education Opportunity Act of 2008, 20 U.S.C. \u00a7 1015(a) (2008); \u2022 the General Education Provisions Act, as amended, 20 U.S.C. \u00a7 \u00a7 9541 to 9548 (2007); \u2022 the Higher Education Act of 1965, as amended by the Higher Education Amendments of 1986, 20 U.S.C. \u00a7 1070 et seq. (2007); and \u2022 the National Education Statistics Act of 1994, as amended, 20 U.S.C. \u00a7 \u00a7 9541 to 9547 and 9573 (2007). Once students completing their baccalaureate degrees in the NPSAS academic year are identified, the B&B series follows them to monitor their progress. Figure 1 shows the data collection timeline for the base-year and subsequent B&B follow-up studies. Although the focus and principal content of the B&B student interviews in each of these three cohorts have remained relatively consistent, expert panels and other reviews of the interview have helped to shape and alter questions as needed for relevancy. For the B&B:08 cohort, the first follow-up interview (B&B:08/09) examined students' workforce participation; income and debt repayment; and entry into and persistence through graduate school programs; as well as several issues specifically related to teaching, including teacher preparation, entry into and persistence in the profession, and teacher career paths. B&B also gathers extensive information on bachelor's degree recipients' undergraduate experiences, demographic backgrounds, expectations regarding graduate study and work, and participation in community service. See appendix C for a complete list of the data elements in the B&B:08/09 student interview and appendix D for a facsimile of the instrument."}, {"section_title": "Schedule and Products of B&B:08/09", "text": "Table 1 summarizes the schedule for B&B:08/09. Electronically documented, restrictedaccess research files (with associated electronic codebooks [ECBs]) and NCES online application PowerStats for public release have been constructed from data collection and will be made available to a variety of organizations and researchers. In addition to this methodology report, B&B:08/09 has produced a First Look report that provides descriptive information for the B&B:08/09 cohort, special tabulations on issues of interest to the higher education community (as identified by NCES), and descriptive reports of significant findings for dissemination to a broad audience. Finalize student sample 2/9/2009 10/9/2009 Conduct web and telephone student interview data collection 7/7/2009 3/12/2010 Conduct field student interview data collection 12/9/2009 3/12/2010 Process student interview data, construct data files 7/8/2009 10/20/2010"}, {"section_title": "Chapter 2. Sampling", "text": "Identification of the B&B:08/09 sample required a multi-stage process that began with selection of the NPSAS:08 sample of institutions and was followed by selection of students within institutions. The final stage confirmed the B&B:08 cohort eligibility of sample members identified via NPSAS:08 as baccalaureate recipients during the 2007-08 academic year."}, {"section_title": "Respondent Universe", "text": "To be eligible for inclusion in the B&B:08 cohort, students must have been part of the student universe at an institution included in the NPSAS:08 institution universe. The definitions of the NPSAS:08 institution and student universes are presented below."}, {"section_title": "Institution Universe for NPSAS:08", "text": "The institutions eligible for NPSAS:08 were required meet all criteria for distributing federal Title IV aid during the 2007-08 year, including; \u2022 offering an educational program designed for persons who have completed a high school education; \u2022 offering at least one academic, occupational, or vocational program of study lasting at least 3 months or 300 clock hours; \u2022 offering courses that were open to persons other than the employees or members of the company or group (e.g., union) that administers the institution; and \u2022 being located in the 50 states, the District of Columbia, or Puerto Rico. Institutions providing only vocational, recreational, or remedial courses or only in-house courses for their own employees were excluded. U.S. service academies were excluded because of their unique funding/tuition base. These institution eligibility conditions are consistent with previous NPSAS studies, with two exceptions. First, the criterion of being eligible to distribute Title IV aid was implemented beginning with NPSAS:2000, 1 and second, the previous NPSAS studies excluded institutions that offered only correspondence courses. NPSAS:08 included such institutions if they were eligible to distribute Title IV student aid."}, {"section_title": "Student Universe for NPSAS:08", "text": "Students eligible for NPSAS:08 were those who were enrolled in an eligible NPSAS institution, and who satisfied both of the following eligibility requirements: \u2022 they were enrolled in any of the following: (a) an academic program, (b) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree, or (c) an occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; and \u2022 they were not concurrently or solely enrolled in high school, or in a GED or other high school completion program."}, {"section_title": "Base-Year Study (NPSAS:08)", "text": "The sampling design for the base-year study, NPSAS:08, was a two-stage design in which eligible institutions were selected in the first stage and eligible students, within eligible responding sample institutions, were selected in the second stage. The NPSAS:08 sampling process is described in the following subsection. For detailed information on the NPSAS:08 sample allocation and statistical design formulas, see appendix A."}, {"section_title": "Institution Sample for NPSAS:08", "text": "NPSAS:08 constructed its institution sampling frame from the IPEDS:2004-05 Institutional Characteristics, Fall Enrollment, and Completions files. The institutions on the sampling frame were partitioned into 46 institution strata based on institution level and control, highest level of offering, and proportion of bachelor's degrees awarded in education. 2 NPSAS:08 also included staterepresentative undergraduate student samples for four degree-granting institution sectors (public 4year; public 2-year; private nonprofit 4-year; and private for-profit 4-year) in six states: California, Georgia, Illinois,Minnesota,New York,and Texas. 3 Institutions were selected using Chromy's sequential probability minimum replacement (pmr) sampling algorithm (Chromy 1979), which is similar to systematic sampling. To avoid multiple selections of sample institutions, those with expected frequencies of selection greater than unity (1.00) were selected with certainty (certainty schools). Initially, a sample of about 1,630 institutions was selected in fall 2006 so that these institutions could be notified of their selection early and to allow a separate field test sample to be selected from the remaining institutions on the sampling frame. In summer 2007, the sample was refreshed using the IPEDS:2005-06 Institutional Characteristics, Fall Enrollment, and Completions files to include any newly eligible institutions within the sampling frame to ensure that the sample was representative of the current population. This process added about 10 institutions to the sample. In fall 2007, the decision was made to include state-representative undergraduate student samples for four degree-granting institution sectors (public 4-year; public 2-year; private nonprofit 4-year; and private for-profit 4-year) in the six states listed above. To accomplish this, a supplemental sample was drawn and added to the existing sample. The final NPSAS:08 sample included 1,960 institutions. The next step of the institution sampling process involved determining the eligibility of the sample institutions. Of the total institution sample (n = 1,960), about 1,940 (99 percent) were found to be eligible to participate in NPSAS:08. Of those, approximately 1,730 institutions (a weighted response rate of 90 percent among the eligible sample 4 ) provided student enrollment lists for use in the second stage of sampling (i.e., selecting the student sample). Table 2 shows the number of institutions that were sampled, the number of eligible institutions, and the count and unweighted and weighted percentages of institutions providing enrollment lists, by institution characteristics. 2 The proportion of bachelor's degrees awarded in education is used to ensure sufficient numbers of sample students receiving a bachelor's degree in education. Such students are an important analysis domain for B&B. 3 These six states were selected based on (1) the size of undergraduate enrollment in the four sectors; (2) prior inclusion in the NPSAS:04 twelve-state sample with high levels of cooperation and participation in that survey; and (3) unique or recently changed tuition and state grant policies that provided opportunities for comparative research and analysis. 4 The weight described here is a base weight. 3. in-state potential baccalaureate recipients who were STEM majors and SMART grant recipients; 4. out-of-state potential baccalaureate recipients who were STEM majors and SMART grant recipients; 5. in-state potential baccalaureate recipients who were STEM majors and not SMART grant recipients; 6. out-of-state potential baccalaureate recipients who were STEM majors and not SMART grant recipients; 7. in-state potential baccalaureate recipients in all other majors who were SMART grant recipients; 8. out-of state potential baccalaureate recipients in all other majors who were SMART grant recipients; 9. in-state potential baccalaureate recipients in all other majors who were not SMART grant recipients; 10. out-of state potential baccalaureate recipients in all other majors who were not SMART grant recipients; 11. in-state other undergraduate students who were SMART grant recipients; 12. out-of-state other undergraduate students who were SMART grant recipients; 13. in-state other undergraduate students who were Academic Competitiveness Grant (ACG) recipients; 14. out-of-state other undergraduate students who were ACG grant recipients; 15. in-state other undergraduate students who were not SMART or ACG grant recipients; 16. out-of-state other undergraduate students who were not SMART or ACG grant recipients; 17. masters students; 18. doctoral students; 19. other graduate students; and 20. first-professional students. For each student sampling stratum, the enrollment list was sampled at a rate designed to provide approximately equal student-level probabilities. To more accurately estimate the overall sample yield, student sampling rates were revised after sufficient lists had been received. The final sample included 137,800 students. Approximately 96 percent of the final sample (n = 132,800) was determined to be eligible for NPSAS. On the completion of data collection, 96 percent of the eligible sample (n = 127,700) was determined to have sufficient key data to meet the definition of a study respondent. A study respondent was defined as any sample member who was determined to be eligible for the study and, minimally, had valid data from any data source, including an institution record abstraction (computer-assisted data entry [CADE]), the NPSAS:08 student interview, and record matching against several administrative databases (e.g., the U.S. Department of Education's Central Processing System [CPS]) for the following: \u2022 student type (undergraduate or graduate/first professional); \u2022 date of birth or age; \u2022 gender; and \u2022 at least 8 of the following 15 variables:  Table 3 shows the number of students sampled, the number of eligible students, and the unweighted and weighted percentages of study respondents, by institution characteristics. See appendix A for more information on the NPSAS:08 institution and student sampling details. 1 A study respondent is defined as any eligible sample member for whom sufficient data were obtained from one or more sources, including student interview, institution records, and the U.S. Department of Education's Central Processing System (CPS). 2 Institution characteristics are based on data from the sampling frame formed from IPEDS:2004-05 and freshened from IPEDS:2005-06. 3 Sample member eligibility was determined during the student interview or from institution records in the absence of a student interview. 4 The weight described in this column is a base weight. NOTE: Detail may not sum to totals because of rounding. NPSAS = National Postsecondary Student Aid Study. IPEDS = Integrated Postsecondary Education Data System. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2007-08 National Postsecondary Student Aid Study (NPSAS:08). In previous NPSAS studies that derived a B&B cohort, lists of potential baccalaureate recipients were collected with the student list of all enrolled undergraduate and graduate/firstprofessional students. However, these baccalaureate lists often could not be provided until late in the spring or in the summer when baccalaureate recipients could be positively identified, which negatively affected the data collection schedule. To encourage an earlier receipt of enrollment lists, 4-year institutions were asked to include an indicator (B&B flag) of students who had received or would potentially receive a baccalaureate degree during the NPSAS year (between July 1, 2007, andJune 30, 2008). 5 Institutions were instructed to make this identification before spring graduation. Four-year institutions were also asked to include an indicator of class level for undergraduates (first year, second year, third year, fourth year, or fifth year). From NPSAS:2000, it was estimated that about 55 percent of the fourth-and fifth-year students would be baccalaureate recipients during the NPSAS year and that about 7 percent of third-year students would also be baccalaureate recipients. This class-level indicator was used when the B&B flag was not provided for any students. These two indicators were used instead of requesting a separate baccalaureate recipient list. Because most enrollment lists were received before June 30, and many were received before April, some sample students identified by the institution as baccalaureate candidates were determined during the NPSAS interview not to be baccalaureate recipients (false positives). Likewise, some sample students not identified by the institution as baccalaureate candidates were determined during the NPSAS interview to have received baccalaureate degrees (false negatives) during the specified timeframe."}, {"section_title": "First Follow-up Study (B&B:08/09)", "text": "The primary task of the B&B:08/09 sample definition process was to confirm or reject a potential respondent's baccalaureate status. Individuals eligible for the B&B:08 cohort were those who completed requirements for a bachelor's degree from a NPSAS:08-eligible institution between July 1, 2007, andJune 30, 2008, and were awarded their baccalaureate degree by the institution from which they were sampled no later than June 30, 2009. Eligibility for the B&B:08 cohort was based primarily on information obtained from the student's transcript. Transcripts were collected prior to the B&B:08/09 interview under the 2009 Postsecondary Education Transcript Study (PETS:09). Lacking a transcript, eligibility was based on responses provided during the NPSAS:08 student interview. Without either the transcript or the interview, eligibility was based on the student's institution record obtained through NPSAS:08 CADE or the enrollment list provided by the NPSAS:08 institution at the time of student sampling. Also, the transcript and NPSAS:08 interview data were reviewed to determine eligibility for students who confirmed in the NPSAS:08 interview that they received their bachelor's degree but whose transcript did not indicate degree receipt. If such students were deemed to be eligible or eligibility could not definitively be determined, then they were included in the sample. The National Student Clearinghouse (NSC) data on degree completion were used to identify eligible students but could not identify ineligible students with certainty. These data were used for stratification. Table 4 shows the distribution of the 25,050 NPSAS:08 sample members who were potentially eligible for membership in the B&B:08 cohort according to their NPSAS:08 interview, CADE, and/or enrollment list status. Of the 18,000 students who completed the NPSAS:08 interview and were confirmed to be eligible for the B&B:08 cohort, about 84 percent (15,050) had a transcript that confirmed eligibility at the time of sampling, 6 percent (1,060) were ineligible based on transcripts, and 11 percent (1,890) did not have a transcript. Table 5 shows the transcript status of the B&B:08 cohort with baccalaureate receipt confirmed in the NPSAS:08 interview. Additionally, transcripts were requested for the 7,050 NPSAS:08 interview nonrespondents who were either confirmed in CADE to be degree candidates or listed by the NPSAS:08 sample institution as bachelor's degree candidates. Approximately 5,150 of these NPSAS:08 nonrespondents were determined to be eligible or eligibility could not be determined for B&B:08/09 based on transcript data. In order to have full population coverage of the B&B:08/09 sample, a subsample of 500 of these 5,150 NPSAS:08 interview nonrespondents was selected. The sample was selected to maximize eligibility. The 5,150 NPSAS:08 interview nonrespondents were stratified based on study respondent, transcript, NSC, and CADE statuses. Within each stratum, the nonrespondents were first sorted by institution sector to ensure the representativeness of the sample and were also sorted by the NPSAS:08 sampling weight within sector. Then, the sample was drawn within each stratum with probabilities proportional to the NPSAS:08 sampling weight. The sampling rates used in each stratum were different in order to maximize response and eligibility rates while also representing the various types of sample members. The B&B:08/09 sample is not intended to be representative at the state level. Based on the B&B:08/09 field test results, the highest sampling rates were among students who were NPSAS:08 study respondents, were potentially eligible based on NSC or CADE, and were confirmed eligible by the transcript. The next highest sampling rates were among students who were NPSAS:08 study respondents, were potentially eligible based on the enrollment list but not based on NSC or CADE, and were confirmed eligible by the transcript. The third highest sampling rates were among students who were NPSAS:08 study respondents, were potentially eligible based on NSC, CADE, or the enrollment list, but had no transcript, and among students who were not NPSAS:08 study respondents, were potentially eligible based on NSC, CADE, or the enrollment list, and were confirmed eligible by the transcript. The lowest sampling rates were among students who were not NPSAS:08 study respondents, were potentially eligible based on NSC, CADE, or the enrollment list, but had no transcript. 6 Table 6 shows the distribution of the potential baccalaureate recipients without a NPSAS:08 interview and the subsample. These distributions are based on whether or not they were a NPSAS:08 study respondent, were confirmed eligible by the transcript or did not have a transcript, and were confirmed in NSC or CADE as being eligible. Table 7 shows the distribution of the full sample by institution control. Table 6. Eligible sample and subsample sizes of the NPSAS:08 potential bachelor's degree recipients without a NPSAS:08 interview"}, {"section_title": "B&B:08 Cohort", "text": "There were 25,050 NPSAS:08 sample members who were potentially eligible for membership in the B&B:08 cohort according to their NPSAS:08 interview, CADE, and/or enrollment list status. Between the NPSAS:08 data collection and the start of the B&B:08/09 data collection, sample members whose transcripts, NPSAS:08 student interview, or administrative data showed they were ineligible, as well as deceased sample members, were removed from the B&B:08 cohort. At the beginning of the B&B:08/09 data collection 18,500 individuals were included in the B&B:08/09 sample. Prior to the start of B&B:08/09 data collection, 1,320 individuals were found to be ineligible leaving 17,170 eligible individuals in the sample. At the end of the B&B:08/09 data collection 17,160 eligible sample members remained in the B&B:08 cohort (deceased cases were removed). Of the 17,160 eligible sample members 15,050 were considered B&B:08/09 student interview respondents, 16,070 were considered transcript respondents, and 14,010 were considered combined interview and transcript respondents. 7 A B&B:08/09 student interview respondent was defined as any sample member who was determined to be eligible for the study, was not deceased at the time of the B&B:08/09 data collection, and had a completed, partial, or abbreviated interview. A student transcript respondent was defined as any sample member who was determined to be eligible for the study, was not deceased at the time of the B&B:08/09 data collection, and had a transcript provided by the NPSAS:08 institution. A combined student interview and transcript respondent was both an interview and a transcript respondent."}, {"section_title": "Chapter 3. Student Interview Design, Data Collection, Outcomes, and Evaluation", "text": "The B&B:08/09 student interview was designed for web, telephone, and field administration and included an abbreviated Spanish interview. Sample members were primarily located using batch address and phone sources and were asked to complete the student interview between July 2009 and Mach 2010. Analyses and evaluation of data collection from a student interview field test as well as from this full-scale study provided information for consideration when planning future administrations of B&B."}, {"section_title": "Student Interview Design and Systems", "text": "The B&B:08/09 student interview consisted of seven sections, grouped by topic. B&B:08/09 abbreviated interviews were also offered in English and Spanish; these abbreviated interviews consisted of selected questions from all sections. This section provides the details of the student interview design and systems."}, {"section_title": "Student Interview Design", "text": "The content of the interview was based on previous B&B student interviews created for the B&B:93 and B&B:2000 cohorts, and on a B&B:08/09 student interview field test, building on data elements developed with input from the study's Technical Review Panel (TRP) and from NCES. For a list of TRP members, see appendix B; for a list of the final set of student interview data elements, see appendix C. The interview consisted of seven sections, grouped by topic. Respondents were guided through each section of the interview according to skip logic that took into account previously provided information from the NPSAS:08 and information recorded as the respondent progressed through the B&B:08/09 interview. Following are descriptions of the seven interview sections. 1. Eligibility determined eligibility for the survey based on date of completion of bachelor's degree requirements at the NPSAS institution. In order to continue with the survey, respondents had to indicate that they completed bachelor's degree requirements between July 1, 2007, andJune 30, 2008, and that they had been awarded the bachelor's degree from the NPSAS institution by June 30, 2009. 2. Undergraduate Education gathered enrollment information on all postsecondary institutions attended prior to receiving the bachelor's degree from the NPSAS institution. This section also captured NPSAS institution major or field of study, enrollment intensity, and academic experiences such as course withdrawals or academic honors. The section concluded with questions about undergraduate financial aid received by respondents and satisfaction with undergraduate education and choice of major. 3. Postbaccalaureate Education/Training collected information about the respondent's postbaccalaureate schools including any undergraduate or graduate degrees or certificates received after the NPSAS bachelor's degree, related majors or fields of study, enrollment intensity, reasons for attendance, and financial aid received. The section concluded with questions about nondegree coursework and future education plans. 4. Postbaccalaureate Employment captured the respondent's current employment status and job characteristics such as job title and duties, earnings, average hours worked per week, benefits, employer industry, and employer's ZIP code. This section also captured information about the relationship of the job to the bachelor's degree field, number of jobs held since graduation, and periods of unemployment. 5.  Teaching collected K-12 teaching preparation and interest and teacher and content area certifications. This section also collected teaching positions since graduation with the bachelor's degree, names of K-12 schools where the respondent taught, grade levels and subjects taught, experiences in the first teaching job such as participation in a teacher internship or mentor program, satisfaction with teaching and plans for staying in teaching, and awareness of the Teacher Education Assistance for College and. Higher Education (TEACH) Grant and teacher loan forgiveness programs. 6. Student Background obtained information about student demographic characteristics, including citizenship, military status, foreign language proficiency, marital status and household composition, annual income and monthly expenses, voting behavior, volunteerism, and disability status. 7. Locating captured contact information for the second follow-up study. The interview sections and principal topics in each section are summarized in figure 2. For the complete B&B:08/09 full-scale instrument facsimile, see appendix D. A single instrument was developed to be administered in one of three modes: web, telephone, or field. For telephone and field interviews, the interviewer accessed the web instrument through RTI's case management system. To minimize mode effects, specific methodological features were incorporated into the instrument to provide web respondents with the assistance normally provided by a trained interviewer: \u2022 help text on every form to define key terms and clarify question intent; \u2022 pop-up messages to correct responses that were out of range or in an incorrect format; \u2022 conversion text to encourage responses to critical items when these items were left unanswered; and \u2022 pop-up messages prompting sample members to provide a response when they left three consecutive questions blank. \u2022 The industry coder was based on the North American Industry Classification System (http://www.census.gov/epcd/www/naics.html). A text string was collected from the respondent, and then the respondent was asked to choose the category that best described his or her employer's industry. Industry choices were laid out in general categories across the screen. When the respondent selected a category, examples of businesses within that industry were displayed, allowing the respondent to determine the appropriateness of the industry chosen. \u2022 The elementary and secondary school coder (\"El/Sec coder\") was used to code any elementary or secondary schools where respondents had taught. The NCES data sources used for schools in the El/Sec coder were the Private School Universe Survey for private schools (http://nces.ed.gov/surveys/pss/) and the Common Core of Data for public schools (http://nces.ed.gov/ccd/). On the two forms prior to the El/Sec coder, the respondent indicated whether the school was public or private, and then provided the city and state of the school. For schools not identified within the El/Sec coder, the entered text string was retained, and respondents were asked to supply the school type (public, private, etc.); the names of the school's district, county, or both; and the lowest and highest grade levels that were taught at the school. Spanish interview. A Spanish abbreviated interview was developed for primarily Spanishspeaking sample members. The Spanish abbreviated interview comprised the same questions as the English abbreviated interview, which included selected questions from all sections. The Spanish interview was made available to respondents in web mode. This mode of administration required the translation into Spanish of not only question wording and response options, but also of all the specific methodological features incorporated into the instrument to provide web respondents with the assistance normally provided by a trained bilingual interviewer (i.e., help text, pop-up messages to correct responses that were out of range or in an incorrect format, conversion text, and general error messages). The two coders in the Spanish abbreviated instrument, the major coder and the occupation coder, were not translated; however, instructions were provided in Spanish to both respondents and bilingual interviewers explaining that they should choose a major or occupation code in English, if possible, or instead enter a text string in Spanish and not attempt the coding of the major or occupation."}, {"section_title": "Data Collection Systems", "text": "This section describes the data collection systems used for the B&B:08/09 data collection, including the Hatteras Survey Engine and Survey Editor (RTI's proprietary web-based interviewing software), the Instrument Development and Documentation System (IDADS), and the Integrated Management System (IMS). Hatteras Survey Engine and Survey Editor. The B&B:08/09 survey instrument was created with Hatteras, a web-based system in which project staff developed, reviewed, tested, modified, and communicated changes to specifications and code for the instrument. All information relating to the instrument was stored in an SQL Server database and was made accessible through web browser interfaces. Hatteras provided specification, programming, and testing interfaces for the B&B instrument as follows. \u2022 Specifications. Hatteras provided the tools and user interface for developing interview specifications. Specification content included wording at the form, question, item, and response option levels; help text content; item-level data documentation; and form-level question administration documentation. Specific capabilities of the Hatteras system allowed instrument designers to import any relevant specifications used in prior studies, create skip logic and item documentation, and search a library of survey items. Instrument designers were also able to take advantage of a comprehensive comment tracking system to communicate and test necessary instrument changes with programmers. A web interface provided access for project staff at MPR and at NCES to test and comment on the instrument throughout development. \u2022 Programming code. For simple instrument questions and items, Hatteras automatically translated specifications into web page scripts when the web page was accessed. For questions involving complex routing, multiple question wording or response option conditions, or nonstandard page layout or behavior, programmers entered custom programming code-HTML, JavaScript, and C#.NET script-into the Hatteras custom code interface. This code was stored in the SQL Server database, together with the instrument specifications for compilation by the survey execution engine. \u2022 Instrument testing and execution. The Hatteras system's survey execution engine allowed immediate testing of specification and code content on a test link. The execution engine also automatically handled such web instrument functions as backing up and moving forward, recording instrument timing data, displaying critical-item wording, validating user input, displaying conditional instructions based on interview mode (web, telephone, or field) and linking to context-specific help text. \u2022 Survey sites and data transfer-web/telephone. For web and telephone data collection, the Hatteras survey execution system was installed on NCES surveys web server farm and SQL Server database. Web respondents accessed the survey directly by web browser after logging in with a user ID and password. RTI's telephone interviewers accessed the same NCES web survey site by means of a web browser process launched from an RTI Case Management System (CATI-CMS). 8 All connections to the NCES web interview were secured with Secure Sockets Layer (SSL) encryption. Automated processes transferred data between RTI's local database and the NCES database via a secure, encrypted connection. \u2022 Survey sites and data transfer-field. For field interviews, the Hatteras survey execution system was installed on local web and database servers on laptop computers. Field interviewers accessed the laptop-based survey by logging in through three independent levels of security, including a whole-disk encryption outer level. Interview control and response data were transferred between RTI and field laptops via secure, encrypted automated connections."}, {"section_title": "IDADS.", "text": "The web-based IDADS documentation module contained the finalized version of all instrument items, their screen wording, and variable and value labels. Also included were the more technical descriptions of items such as variable types (alpha or numeric), to whom the item was administered, and frequency distributions for response categories based on completed interview data. The documentation module was used to generate the instrument facsimiles and the deliverable ECB input files."}, {"section_title": "IMS.", "text": "All aspects of the study were controlled using an IMS, a comprehensive set of desktop tools designed to give project staff and NCES access to a centralized, easily accessible repository for project data and documents. The B&B:08/09 IMS consisted of several components: the management module, the Receipt Control System (RCS) module, and the instrumentation module. \u2022 Management module. The management module of the IMS included tools and information to assist project staff and the NCES project officer in managing data collection. All management information pertinent to the study was located there, accessible via the Web, and protected by SSL encryption and a password-protected login. The IMS contained the current project schedule, monthly progress reports, daily data collection reports and status reports (generated by the RCS described below), project plans and specifications, project deliverables, instrument specifications, a link to the Hatteras system, staff contacts, the project bibliography, and a document archive. \u2022 RCS. The RCS is an integrated set of systems that was used to control and monitor all activities related to data collection, including tracing and locating. Through the RCS, project staff were able to perform tracing and data management operations, track case statuses, identify problems early, and implement solutions effectively. The RCS's locator data were used for a number of daily tasks related to sample maintenance. Specifically, mailout systems produced paper mailings and e-mailings to sample members, the query system enabled administrators to review the locator information and status for a particular case, and the mail return system enabled project staff to update the locator database as mailings or address update sheets were returned or forwarding information was received. The RCS also interacted with the computer-assisted telephone interviewing (CATI) system, sending locator data between the two systems as necessary. \u2022 Instrumentation module. The instrumentation module managed development of the multimode web data collection instrument within Hatteras. Developing the instrument with Hatteras ensured that all variables were linked to their item and screen wordings and were thoroughly documented."}, {"section_title": "Student Interview Data Collection", "text": "The B&B:08/09 interview data collection involved training data collection staff and locating, contacting, and interviewing sample members. Each of these procedures is detailed in this section."}, {"section_title": "Training of Interview Data Collection Staff", "text": "Members of the data collection staff included quality control supervisors (QCS), help desk agents (HDAs), telephone interviewers, field interviewers, and intensive-tracing staff. Prior to beginning work on B&B, all data collection staff completed a comprehensive training program. Topics covered in training programs included a review of confidentiality requirements, an overview of B&B:08/09, frequently asked questions, and administrative procedures for case management as well as hands-on practice. All training programs were designed to maximize active participation of the trainees. The training schedule and number of data collection staff trained are presented in table 8. The specific roles and duties of data collection staff are summarized in the following subsections, along with a description of the training program (see appendix E for training materials). and use of the B&B laptops. Field interviewers were also required to conduct successful certification mock interviews and multiple other certification exercises with their field supervisor over the phone before they were permitted to begin work. Field interviewer training materials included a field interviewer manual and additional handouts and forms used to document all field procedures and expectations of work. Tracing staff. Tracing staff (tracers) used intensive measures (described in section 3.2.3) to locate sample members designated as lacking good telephone contacting information. Tracers attended a comprehensive 16-hour training session that was led by RTI tracing managers within RTI's Call Center Services (CCS) and covered all tracing procedures. Tracers also received 2 hours of project-specific training. They received an overview of B&B, a review of the FAQs, background information on the B&B sample, and the tracing techniques best suited to locating B&B sample members. Additional trainings. Selected staff received additional training modules, such as refusalconversion training, and Spanish interview training (for certified bilingual staff). Additionally, quality circle meetings were routinely conducted as an extension of the training program for continual quality improvement. Data collection staff were given the opportunity to ask questions in meetings and as needs were identified, additional training topics were highlighted and addressed in subsequent meetings. After each meeting, quality circle notes were posted on the call center's project website and on the project IMS."}, {"section_title": "Study Website", "text": "B&B:08/09 sample members were provided a link to the B&B website prior to the start of data collection. The website provided general information about the B&B set of studies, including details about the study sponsor and contractors, how the data are used, answers to frequently asked questions, confidentiality assurances, and selected findings from earlier studies. The website also provided contact information for the study help desk and project staff at RTI, as well as links to the main NCES and RTI websites. Sample members were able to log in to the secure portion of the website to provide updated contact information and complete the student interview once it became available. Designed according to NCES web policies, the B&B:08/09 website used a three-tier security approach to protect all data collected. The first tier of security included secure log-ins, with a unique study ID and strong password provided to sample members prior to the start of data collection. The second tier of security protected any data entered on the website with SSL technology, allowing only encrypted data to be transmitted over the Internet. The third tier of security stored any collected data in a secured SQL Server database located on a server machine that was physically separate from the Web server. Figure 3 shows the home page for the B&B:08/09 website. "}, {"section_title": "Locating and Contacting Sample Members", "text": "Several locating methods were used to find and collect up-to-date contact information for the B&B:08/09 sample (figure 4). Batch searches of national databases and prenotification address update mailings were conducted prior to the start of data collection. After the start of data collection and for those sample members not yet found, follow-up locating methods were employed, including CATI locating, intensive tracing, and field tracing. Batch tracing. Before mailing activities began, batch database searches were conducted to update sample member contact information. These searches used the CPS and the U.S. Postal Service (USPS) National Change of Address databases. The information obtained from these sources was compared with the information previously available from the NPSAS:08 locator database to identify any new contact information. Then, just prior to the start of outbound telephone interviewing, all sample member addresses and telephone numbers were sent to Telematch, a computerized residential telephone number service with the not-yet-published numbers of new movers, to obtain any telephone number updates. Mailings. In May 2009, about 7 weeks before the start of B&B:08/09 data collection, a mailing was sent to the parents of base-year respondent sample members younger than 26 years old, to gain their assistance with providing up-to-date contact information for these sample members. This mailing included a study brochure, a letter with detailed information about B&B:08/09 signed by the associate commissioner of NCES, an address update sheet, and a business reply envelope. (Parents of base-year nonrespondent sample members younger than 26 years old received their version of the letter in October 2009.) The final step in the predata collection locating and contacting effort occurred in June 2009, approximately 4 weeks before the start of data collection, with a similar address update mailing going to sample members (using any updated contact information provided by parents, if applicable). The mailing contained a letter notifying sample members of the upcoming B&B:08/09 data collection, the study brochure, an address update sheet, and a business reply envelope. Sample members were asked to update their address information on the address update sheet and return it in the postage-paid envelope. They also had the option of entering the information using the online form available on the B&B study website. The address update sheet and online form included a space prompting sample members to indicate a preference for being notified by text message of the start of data collection. B&B data collection started on July 13, 2009, with the mailing of a data collection announcement to base-year respondents by USPS first-class mail in a 9 x 12 inch B&B envelope. Base-year nonrespondents received their mailing in November, also by USPS first-class mail in a 9 x 12 inch B&B envelope. The mailing to all sample members included a study brochure and a letter that announced the start of data collection. The letter, signed by both the B&B project director and the NCES project officer, included a $5 bill and informed sample members of the additional cash incentive for completing the interview by the early incentive deadline specified in the letter, provided the study website and sample member's user ID and password for accessing the web interview, and provided the study's toll-free help desk number and e-mail address. The same day, an e-mail containing information comparable to that in the data collection announcement letter was sent to sample members. As soon as a parent address was available, a letter was also mailed to parents of all base-year respondent sample members younger than 26 years old explaining the importance of the study and asking parents to encourage sample members to participate. The letter was sent to parents of all base-year nonrespondent sample members younger than 26 years old in October 2009. Additional mailings included a postcard reminder sent about 10 days after the data collection announcement and two additional e-mail reminders to encourage early interview response. Once outbound telephone interview efforts began and throughout data collection, periodic mailings and emails went to interview nonrespondents throughout the course of data collection (for student interview data collection notification materials, see appendix F). CATI locating and preintensive tracing. Telephone interviewers made prompting calls to base-year nonrespondents during the early response period of data collection. These calls, described in more detail in section 3.4.1, helped identify cases that required further tracing in addition to encouraging early response. Once outbound telephone interviewing began, telephone interviewers conducted limited tracing and locating activities as needed. The telephone number believed to be the best known number for contacting the sample member was attempted first. If the sample member could not be reached at that number after several attempts, any other numbers associated with the sample member, including parent and other contacts, were called. If the sample member could not be located, the case was designated for FastData and Accurint batch services which provided an automated search for matching phone numbers to sample members using combinations of address, name, and Social Security number (SSN). Cases for which neither FastData nor Accurint Batch generated new telephone numbers were sent for intensive interactive tracing by RTI's Tracing Operations (TOPS). Overall, for B&B:08/09 data collection, the batch matching successfully confirmed contact information or provided new contact information for 20,070 records. The most records, 8,300, were matched through Telematch. While the fewest records, 570, were matched through FastData, this data source combined with Accurint minimized the number of cases requiring more costly intensive tracing. Table 9 shows the match rates for each tracing source. Intensive tracing. The most difficult locating cases were traced at TOPS using a two-tiered strategy and a number of sources. TOPS-1, the first tier, identified sample members with SSNs available to trace through consumer databases (FastData's SSN search and Experian) that contain current address and telephone listings of consumers with credit histories. If a search generated a new telephone number for the sample member, tracers attempted to confirm the information by speaking with the sample member or with someone else who could verify the information. If the telephone number was confirmed, the case was sent back to CATI for telephone interviewing. This first level of effort minimized the time that cases were in tracing and unavailable for CATI efforts. Cases still not located and that were not in a field cluster underwent a more intensive level of tracing in TOPS-2. TOPS-2 included calls to other possible sources of information, including, for example, directory assistance, alumni offices, and contacts with neighbors or landlords. Whenever any of these sources provided information that indicated a sample member was not available for the study (e.g., deceased, incarcerated, or out of the country), no further contact efforts were made. Overall, about 7 percent of eligible sample members required intensive tracing (table 10). Thirty-three percent of the NPSAS:08 interview nonrespondents required intensive tracing, compared with 6 percent of NPSAS:08 interview respondents (z = 12.15, p < .001). Nine percent of sample members at private, for-profit, 2-year-or-more NPSAS institutions required intensive tracing, compared with 6 percent of sample members whose NPSAS institutions were private, nonprofit 4year doctorate-granting schools (z = 3.02, p < .001). Field tracing. Any cases not located after TOPS-1 intensive tracing and thought to be in one of the 17 selected geographical field clusters were designated for field tracing instead of TOPS-2. Information provided to field interviewers included all address and telephone information available for an assigned case, the results of TOPS-1 intensive tracing efforts, and the details of all call attempts made by telephone interviewers. In addition to these tracing resources, field interviewers had access to contacts within the community, such as post office mail carriers or local public records that could provide additional information. Many field interviewers also had the added advantage of calling from telephones with local area codes familiar to sample members, increasing the likelihood that sample members would respond to the telephone calls."}, {"section_title": "Interviewing", "text": "Data collection for the B&B:08/09 interview consisted of three phases (figure 5): 1. Early response phase. This phase began with the start of data collection in July 2009 for base-year respondents and lasted approximately 4 weeks. Data collection began in waves, based on the early incentive expiration date assigned to each case. The early response phase for base-year nonrespondents began in November 2009. Base-year respondents who completed the interview during their early response phase received an incentive of $30; base-year nonrespondents received a $50 incentive. 10 2. Production phase. During this phase, which only applied to base-year respondents, interviewers called to encourage sample members to complete the interview by telephone or on the Web. No incentive was offered during this phase. 3. Nonresponse conversion phase. Cases in this phase belonged to one of the following groups: interview refusal by the sample member or a sample member contact, hard-to-reach, 11 not locatable after intensive tracing, insist-pay, 12 base-year nonrespondents who did not complete during the early response phase, and field cases that did not fit into one of the preceding groups. Base-year respondents who completed the interview during the nonresponse conversion phase were offered a $30 incentive and base-year nonrespondents were offered a $50 incentive. Data collection ended in March 2010. Sample members could complete the interview on the Web or by telephone throughout the data collection period. The interview screens in the telephone and field interviews were identical to those in the web interviews, except that interviewer instructions on how to administer each question were visible at the top of each screen for telephone and field interviews. Following are details of the administration of the interview through the various modes. Web interviews. Sample members were informed of the web interview in the data collection announcement mailing. During the early response period (the first 4 weeks of data collection), only web interviews were completed unless sample members initiated a telephone interview by calling the help desk or sending an e-mail asking to be called. Reminder mailings and emails were sent throughout the production and nonresponse conversion phases of data collection to encourage sample members to complete the interview online. The website was accessible 24 hours a day, 7 days a week, throughout the data collection period, providing sample members with the option to complete the interview online at any time. Help desk operations. The help desk for B&B:08/09 opened on July 14, 2009, in anticipation of the first respondent calls after the data collection announcement mailing. Help desk staff were available to assist sample members who had questions or problems accessing and completing the web interview. A toll-free help line was established to accept incoming help desk calls. If technical difficulties prevented sample members from completing the web interview, help desk agents-also trained to conduct telephone interviews-encouraged sample members to complete a telephone interview. A help desk application was created to document incoming calls from sample members and other contacts. Specifically, the help desk application included the following: \u2022 information needed to verify the sample member's identity; \u2022 login information needed by the sample member to access the web interview; \u2022 a means to update sample member contact information, as needed; \u2022 functionality to unlock cases and send an e-mail containing the website and study login information to the sample member; \u2022 systematic documentation of each call; \u2022 a means for tracking calls that could not be resolved immediately; and \u2022 a record of the CATI-CMS events, which also included prior help desk events. The help desk application provided project staff with the resolution status of all help desk events and reports on the type and frequency of problems experienced by sample members. Telephone interviews. Telephone follow-up locating and interviewing began on August 13, 2009, after the 4-week early response period ended for the group of cases with the first early response incentive expiration date. Telephone interviewing procedures included attempts to locate, gain cooperation from, and interview sample members who had not yet completed the interview. Interviewers encouraged sample members to complete the interview by telephone; however, sample members could still complete the interview on the Web, if that was their preference. Sample members who did express a preference to complete a web interview were called back 5 days later for follow-up if the interview had not yet been completed. The CATI-CMS included an automated call scheduler that assigned cases to interviewers by case priority, time of day, day of week, existence of previously scheduled appointments, and type of case. Case assignment was designed to maximize the likelihood of contacting and interviewing sample members and cases were assigned to various queues accordingly. For example, the CMS included queues for new cases that had not been called, Spanish-language cases, initial refusals, 13 and various appointment queues. In addition, available telephone numbers for each case were automatically prioritized for the interviewers. As new roster lines 14 were added-as a result of CATI tracing, other tracing efforts, and information from other sources such as respondent e-mails or help desk call-ins-available telephone numbers were re-prioritized based on the new information. Some cases required special treatment. For cases with sample members or contacts who spoke only Spanish, bilingual interviewers were available to administer a Spanish interview (see section 3.1.1 for details regarding the Spanish interview). To gain cooperation from those sample members who initially refused to participate (including contacts who acted as gatekeepers to the sample member), interviewers were trained in refusal-conversion techniques. As the end of data collection approached, all telephone interviewers were trained to administer the abbreviated Englishlanguage interview to reluctant sample members. Field interviews. Field data collection activities began approximately 5 months after the start of outbound telephone interviewing, during the nonresponse conversion phase of data collection. Using the last known address for each case, RTI's Geographic Information System program conducted an analysis of the B&B:08/09 sample to identify the 17 geographic areas with the highest density of sample members residing within a 100-mile radius of the cluster center. On the basis of this analysis, 16 field interviewers were hired. An Integrated Field Management System provided reports that helped project staff manage the progress of the field interviewing effort. Once assigned to the field, cases were excluded from further outbound efforts from the call center, but could still be completed on the Web or by telephone if sample members called the help desk to complete the interview. See section 3.4.1 Student Interview Response Rates for results of field, telephone, and web interviews."}, {"section_title": "Other Procedures to Maximize Locating and Interview Response", "text": "Throughout data collection the B&B project team continued to work with TOPS and other available resources to evaluate additional tracing efforts that could benefit B&B data collection. In addition to the locating sources and methods already described, B&B:08/09 used several other procedures to maximize locating and interview response. Other locating methods. Other locating methods used to find sample members included: \u2022 Experian credit header search. In January 2010, an Experian credit header search was conducted to obtain phone numbers and addresses associated with sample members according to their credit histories. This search, conducted for sample members who had not yet been located or were located but not reached for several weeks, provided a relatively low-cost alternative to other intensive tracing methods. \u2022 Additional in-house tracing of field cases. Field cases that resulted in dead-end information were sent back through TOPS for additional leads. Cases for which additional leads were identified through this special tracing effort were returned to the field, and those for which no new information was found were closed. The results of this special tracing effort are described in more detail in section 3.4.1. Other contacting methods. Text messaging, social networking (Facebook and MySpace), and an informational video were additional methods used to contact sample members and encourage interview completion. \u2022 Text messages. Some sample members were contacted by Short Message Service technology, or text messaging. A text message reminder to complete the B&B:08/09 interview was sent during the early response data collection period to those sample members who had requested on their address update sheet that a text message be sent. The text message mentioned the B&B interview and included the help desk number, the early incentive expiration date, and the incentive amount available. The text messages were sent via e-mail addresses that were based on the sample member's phone number and their service provider. \u2022 Social networking. Two popular social networking sites, Facebook and MySpace, were used to generate new leads for and make contact with sample members who were difficult to locate. B&B:08 cohort information on record-such as postsecondary institutions attended, city/state networks and e-mail addresses-was used to search for sample members on Facebook or MySpace. Once the targeted individual was believed to be found, a message describing B&B and the incentive being offered, as well as reminding the individual of any past participation in the study, was sent through internal messaging on the social networking site. Although the message included the study website and help desk telephone number, no personally identifying information (such as login information) was included; this information could only be provided to a sample member who visited the study website or contacted the help desk and verified his or her identity. Efforts to contact sample members by Facebook were ceased because of restrictions on the number of messages sent to individuals. When few B&B messages were opened by sample members contacted through MySpace, the use of social networking sites was abandoned in favor of the other, more promising locating and contacting methods. \u2022 YouTube video. Near the end of B&B:08/09 data collection, RTI developed a brief video designed to encourage participation of sample members who had not yet completed the interview. This video was posted to YouTube, a website popular with the age group that makes up most of the B&B:08/09 sample. The video provided information about the study, including confidentiality procedures. The video also mentioned the incentive amount being offered. 15 Sample members who had not yet completed the interview were sent an e-mail on January 25, 2010, with a link to the video. Mention of the video, along with a shortened web page address that redirected visitors to the video's web page, was also included in a mailing sent on February 11, 2010. Before posting the video, project staff adjusted the account and video settings including turning off features to prevent sample members from identifying one another."}, {"section_title": "Data Collection Quality Control", "text": "A number of quality control procedures were implemented throughout the course of the B&B:08/09 student interview data collection. These procedures included frequent interview monitoring of telephone interviewers, a help desk that tracked and resolved difficulties encountered by sample members attempting to complete the web interview, quality circle feedback meetings, and help desk agent and interviewer debriefings at the conclusion of the study."}, {"section_title": "Interview Monitoring", "text": "Regular monitoring of telephone interviews during B&B:08/09 data collection was conducted to meet the following important data quality objectives: \u2022 Identification of problem items in the interview; \u2022 Reduction in the number of interviewer errors; \u2022 Improvement in interviewer performance through reinforcement of effective strategies; and \u2022 Assessment of the quality of the data collected. QCS and project staff monitored live and recorded interviews throughout data collection, using remote monitoring telephones and computer equipment. To guarantee an accurate reflection of data collection activities, QCS monitored day, evening, and weekend shift interviewers. In addition, each week QCS and interview project staff monitored one live interview session and one recorded interview session. The live session allowed for monitoring of calls and interviews in progress, including remotely viewing interviewers' computer screens as they progressed through the interview and listening to interviews in real time, while the session with recorded interviews allowed only listening to the interview but guaranteed an opportunity to hear complete interviews. QCS and interview project staff recorded observations on standardized monitoring forms that covered such topics as interviewer professionalism, question administration, and knowledge of the instrument. After each monitoring session, interviewers received feedback based on observations from the session. Issues and trends identified during monitoring were frequently incorporated into quality circle meetings to improve the quality of telephone interviews."}, {"section_title": "Help Desk", "text": "A help desk, described in Section 3.2.4, was available to sample members. To gain a better understanding of the problems encountered by sample members, HDAs used a web-based application to record each help desk incident that occurred during data collection. For each incident, an HDA confirmed contact information for the sample member, noted the source (e.g., incoming telephone call, voice mail, or email; request from the study website), recorded the type of problem, provided a description of the problem and resolution, and indicated the incident status (pending or resolved). If the problem was not resolved immediately, the HDA scheduled a follow-up appointment. Table 11 provides a summary of help desk incidents encountered during B&B:08/09 data collection. HDAs handled a total of 460 help desk incidents. The most common type of incident was from sample members requesting their study ID, password, or both (68 percent). Pop-up blocker issues were the second most common category (13 percent). "}, {"section_title": "Quality Circle Meetings", "text": "Quality circle meetings were essential as part of a feedback loop for ensuring that project staff, CCS, and telephone interviewers were communicating on a regular basis about the goals of the study and addressing challenges encountered along the way. These meetings provided a forum for discussing elements of the instrument design and interview cooperation tactics, motivating the group toward the goals of the study, and acquiring feedback on data collection issues. Weekly quality circle meetings for telephone staff were held at the call center, while quality circle meetings for the field staff were held via conference call. Issues discussed at the quality circle meetings were added to weekly quality circle notes, which all interviewers were required to access electronically. The quality circle notes included counts of interview completions to date, separate sections for general data collection issues and issues specific to the survey instrument, and project staff responses to questions from interviewers. Throughout the study, a variety of issues were addressed at the quality circle meetings that reinforced specific content from training and contributed to prompt problem solving. Some of the issues covered in quality circle meetings included the following: \u2022 clarification of questions and item responses and reinforcement of positive interviewing techniques; \u2022 methods of gaining cooperation from sample members and gatekeepers (e.g., parents and roommates); \u2022 problem sheets submitted by interviewers during interviews; \u2022 the importance of interviewers providing and reviewing detailed case comments; \u2022 data security protocols; and \u2022 study progress and general morale boosting. B&B:08/09 used an interactive, activity-based quality circle meeting format. In the new meeting structure, interviewers participated in training activities intended to make meetings more engaging and to improve the quality of data collected from telephone interviews. Debriefing of interviewers showed that they generally enjoyed the new quality circle meeting format and often attributed improvements in their interview times and knowledge to these training activities."}, {"section_title": "Debriefing", "text": "At the conclusion of the B&B:08/09 data collection, project staff held debriefing meetings with interviewers and HDAs. In the debriefings, interviewers were asked their opinions on the effectiveness of interviewer training; the success of the various techniques for tracing, locating, and gaining sample member cooperation; and any difficulties associated with administering the student interview. Interviewer feedback on their experience conducting the B&B student interview was typically positive, and interviewers provided several useful recommendations for future data collections. Interviewers suggested that future trainings could include spending more time on techniques for converting sample member refusals to participate in the student interview and for gaining cooperation from gatekeepers. Regarding student interview administration, interviewers indicated that the new quality circle meeting format, which included varied training activities, was beneficial because it allowed for increased communication between interviewers and project staff and because activities were interesting as well as informative. Interviewers suggested that for future studies, quality circle meetings continue to focus on the difficulties associated with administering interviewer coders, particularly the occupation and industry coders. Interviewers also noted that quality circle meetings were a useful time to work on improving interviewing skills such as keeping sample members engaged during interviews."}, {"section_title": "Student Interview Data Collection Outcomes", "text": "This section provides the results of the B&B:08/09 student interview data collection. Details of the overall student interview response rate of 88 percent are included, and a description of the success of various locating methods is also provided. A timing analysis shows that the student interview, on average, took about 28 minutes to complete."}, {"section_title": "Student Interview Response Rates", "text": "B&B:08/09 interviews were conducted from July 13, 2009 to March 12, 2010. Of the 17,170 eligible sample members in the B&B:08 cohort, 16,050 (93 percent) were successfully located and asked to complete the B&B:08/09 interview, while 15,090 (88 percent) did complete a full interview, an English or Spanish abbreviated interview, or a partial interview. 16 The overall locating and interviewing results for the B&B:08/09 interview data collection effort, including sample members who were determined ineligible and those located but later considered exclusions for reasons such as being incapacitated or deceased, are presented in figure 6. Response rates by base-year status and institution type. NPSAS:08 interview respondents were located and completed the B&B:08/09 interview at a higher rate than NPSAS:08 interview nonrespondents. 17 NPSAS:08 interview respondents had a locate rate of 94 percent, while 72 percent of NPSAS:08 nonrespondents were located. Of all NPSAS:08 respondents, 89 percent completed the B&B:08/09 interview, while 49 percent of NPSAS:08 nonrespondents completed the B&B:08/09 interview. Overall locate rates for the B&B:08/09 interview, based on the institution type of the sample member's base-year interview (NPSAS) school, 18 ranged from 90 percent (private, for-profit 2-year or more schools) to 94 percent (public 4-year doctorate-granting schools, private, nonprofit 4-year non-doctorate-granting schools and private, nonprofit 4-year doctorate-granting schools). Overall response rates for the B&B:08/09 interview, by NPSAS institution type, ranged from 85 percent (private, for-profit 2-year or more schools) to 89 percent (private, nonprofit 4-year non-doctorategranting schools). Locating and participation results by NPSAS:08 respondent status and NPSAS institution type are presented in table 12. Response rates by base-year status and interview type. About 96 percent of all B&B:08/09 interview respondents completed the full interview, about 3 percent completed an English or Spanish abbreviated interview, and less than 1 percent completed a partial interview. NPSAS:08 interview respondents completed a full interview at a higher rate (97 percent) than NPSAS:08 interview nonrespondents (63 percent). Thirty-five percent of NPSAS:08 interview nonrespondents completed an English abbreviated interview, compared with 2 percent of NPSAS:08 interview respondents. Table 13 provides detail on the number and percent of completed B&B:08/09 interviews by base-year response status and interview type. Interview outcomes by mode. B&B:08/09 interviews were completed in one of three modes: web, telephone, or field. Figure 7 shows that most (81 percent) interviews were completed on the Web, 16 percent of interviews were completed by telephone, and 3 percent were completed in field interviewing. Response by phase of data collection. Two-thirds (67 percent) of all completed B&B:08/09 interviews and over three-fourths (80 percent) of web interviews were completed during the early response phase. Approximately 9 percent of all interviews were completed during the production phase, and the remaining 24 percent were completed during the nonresponse conversion phase. Response, by phase of data collection, is shown in figure 8. The early response phase of data collection yielded a 59 percent response rate for the eligible sample of 17,170. In this phase, base-year respondents received $30 to complete the student interview while base-year nonrespondents received $50 to complete the interview. In the next phase of data collection, the production phase, no incentive was offered. Of the 7,040 eligible sample members remaining in the production phase, 19 percent completed the interview. In the final phase of data collection, the nonresponse conversion phase, 64 percent of the remaining 5,700 eligible sample members completed the interview. Sample members who completed the interview in this last phase of data collection received either $30 if they were base-year respondents, or $50 if they were not. Table 14 provides the number of completed interviews in each data collection phase and the percent of the eligible sample in each phase that completed interviews. Locate and response rates by source of address update. Address updates for the B&B:08 cohort were received from 6,780 eligible sample members in response to the parent initial contact mailing (for sample members younger than age 26), the student advance notification mailing, or through the B&B website. If an address update was received, the sample member was located almost 100 percent of the time and the sample member then completed the B&B:08/09 interview 98 percent of the time. Parents and students responded equally to the parent mailing and advance notification mailing (each mailing yielded a 26 percent update). Student address updates through the B&B website constituted about 18 percent of the updates. The parent reminder letter sent on November 30, 2009 elicited a website address update from about 1 percent of those sent the letter. Locating and interviewing outcomes of cases for whom an address update was provided are shown in table 15. Response rates by intensive tracing. Among the cases assigned to intensive tracing, TOPS-1 and TOPS-2, approximately 77 percent were located. Of those cases located through intensive tracing, about 45.8 percent completed the B&B:08/09 interview (table 16). About 19 percent of the cases assigned to intensive tracing required TOPS-2 tracing. Response rates by other locating methods. Although no quantifiable locating or interviewing outcomes were achieved through the use of social networking sites, nearly three-fourths (72 percent) of the cases sent through the Experian credit header searches or that were field cases receiving additional tracing were located. Eighteen percent of these located cases completed the interview. Results of these other locating methods are summarized in table 17. Of the 80 eligible sample members who received an early text message notifying them that data collection had begun, 96 percent completed the interview. These were sample members who had requested a text notification reminder of the start of data collection on their address updates. Prompting response rates. Base-year nonrespondents were selected to receive prompting calls to complete the web interview during the early response phase of data collection. Of the 460 eligible base-year nonrespondent cases, 380 cases (82 percent) were flagged for prompting. (The remaining 80 cases were not prompted because of either missing/invalid telephone numbers or because a B&B:08/09 interview was completed prior to the start of prompting.) Prompting calls began 8 days after the start of data collection, and 260 cases were successfully prompted. For those successfully prompted, there was not a significant difference between the response rate during the early response phase of data collection for sample members who were contacted (spoken to directly) in a prompting call (10 percent), compared with sample members who were not contacted (not spoken to directly; 7 percent). The response rates during the early response phase of data collection for base-year nonrespondents who were contacted when prompted versus those who were not contacted when prompted are compared in figure 9. Prompted base-year nonrespondents Early response rate 1 1 A sample member was considered not contacted when someone other than the sample member was reached or the call was directed to an answering machine or voicemail. NOTE: Counts include only eligible cases that were considered successfully prompted (spoke with someone or left an answering machine message). Excludes ineligible cases, cases completed before prompting began, cases without a phone number, and cases unsuccessfully prompted due to no answer or a dead-end number. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2008/09 Baccalaureate and Beyond Longitudinal Study (B&B:08/09). Response rates for field cases. Beginning about 5 months into data collection, cases identified as difficult to locate but believed to reside in a selected field cluster were assigned to field data collection. Of the 920 eligible cases assigned, 68 percent were located and 60 percent completed the interview in one of the three available modes (web, telephone, or field). Of the completed cases assigned to the field, 150 (26 percent) were actually completed by web, or by telephone through the call center, instead of through field efforts. Results by base-year response status and NPSAS institution type are also presented for these cases. Nearly 62 percent of base-year respondent cases assigned to the field completed the interview compared with 42 percent of base-year nonrespondent cases assigned to the field. Response rates among eligible cases assigned to the field also varied by sample members' NPSAS institution type. The largest number of cases assigned to the field for any one institution type, 370 cases, was for sample members whose NPSAS institution was a public 4-year doctorategranting institution; these cases yielded a response rate of 59 percent. Only 70 cases were assigned to the field for sample members from private, for-profit, 2-year-or-more schools; these cases yielded a response rate of 62 percent. Table 18 shows locate and response rates among the field cases. Chapter 3. Student Interview Design, Data Collection, Outcomes, and Evaluation "}, {"section_title": "Interview Timing Burden", "text": "Several analyses for the B&B:08/09 interview assessed the timing burden on respondents completing the interview. These analyses included computation of the overall average time it took respondents to complete the interview and the time it took respondents to complete the interview based on mode of administration (web, telephone or field), as well as analysis of whether particular respondent characteristics, such as employment or teacher status, were related to interview timing burden. To calculate the time it took to complete the interview, two time stamps were embedded on each form (web screen) of the interview. A start timer recorded the clock time on the respondent's or interviewer's computer when each form was first displayed. An end timer recorded the clock time on the respondent's or the interviewer's computer when the Next button on each form was clicked. From the two time stamp variables, an on-screen time and a transit time were calculated. The onscreen time was calculated by subtracting the start time from the end time for each form that the respondent saw. The transit time was calculated by subtracting the end time of the preceding form from the start time of the next form. Total on-screen time and total transit time were calculated for all respondents by summing all of the on-screen times for each screen received and summing all of the transit times for each respondent. Total instrument time was then calculated by summing a respondent's total on-screen and total transit times. The timing analysis included only cases that completed the full-scale interview in one session. Partially completed interviews and those interviews completed in multiple sessions (i.e., those cases that logged out from an incomplete interview and later resumed their interviews) were excluded from the analysis. The average overall interview time was calculated by summing the respondents' interview completion times and dividing the result by the total number of respondents. On average, the B&B:08/09 interview took 27.7 minutes to complete. Web interviews took 26.6 minutes to complete, field interviews took 31.1 minutes to complete, and telephone interviews took 33.5 minutes to complete. Average section completion times were 1.4 minutes for the Front End (introductory section), 1.1 minutes for Eligibility, 7.2 minutes for Undergraduate Education, 2.2 minutes for Postbaccalaureate Education/Training, 6.8 minutes for Postbaccalaureate Employment, 3.6 minutes for Kindergarten-12th Grade Teaching, and 3.8 minutes for Student Background. On average, all sections took the longest in telephone mode. Table 19 shows the average interview time overall, for each section, by mode of administration. Interview time by interview path. The time it took respondents to complete the B&B:08/09 interview varied by whether respondents had any postbaccalaureate employment, and by whether they had taught since receiving their bachelor's degree. The Postbaccalaureate Employment section focused on the job(s) held in the year after completing a bachelor's degree. This section collected information regarding current job duties, benefits and requirements, and periods of unemployment, if applicable. On average, the Postbaccalaureate Employment section took 5.8 minutes to complete. Table 20 shows that respondents who were employed spent longer in the employment section (6.8 minutes) compared to those respondents who were not employed (1.7 minutes). The K-12 Teaching section collected information about the respondent's experiences with or interest in teaching kindergarten through the 12th grade. Table 21 shows respondents who had never been a teacher and were not planning or preparing to become a teacher took an average of 0.4 minutes to complete the section. Respondents who were current or former teachers, or who were either considering or preparing for teaching, took 3.6 minutes. The latter group is divided even further between current teachers (6.5 minutes), former teachers (5.1 minutes), respondents preparing to become teachers (2 minutes), and respondents considering becoming teachers (1.5 minutes). Timing of abbreviated interview. The abbreviated version of the B&B:08/09 interview included the Front End, Eligibility section, and questions from the other sections of the interview. Table 22 shows that, on average, the B&B:08/09 abbreviated interview took 13.6 minutes. Overall, web abbreviated interviews took 11.8 minutes and were significantly shorter than telephone abbreviated interviews, at 16.9 minutes (t(313) = -7.38, p < .001 ). Telephone interviews were significantly longer than field interviews at 12.5 (t(212) = 5.08, p < .001). "}, {"section_title": "Telephone Interviewer Hours", "text": "During the course of B&B:08/09 data collection, 9,291 telephone interviewer hours were logged, for an average of 3.79 hours spent by telephone interview staff per completed or partial telephone interview. Because, on average, telephone interviews took 33.5 minutes to administer, most of the telephone interviewer hours were spent on case management activities. These activities included locating and contacting sample members, prompting sample members to complete interviews, reviewing call history, scheduling callbacks, entering detailed comments and suggestions to assist with reaching and interviewing sample members, and responding to incoming help desk calls. Near the end of data collection, telephone interviewers also spent about two weeks intensively reviewing nonrespondent cases to identify the most appropriate next step for each case."}, {"section_title": "Number of Calls to Sample Members", "text": "On average, nine calls were made per B&B:08/09 sample member during the interviewing period, except for in the early response phase when no outbound calls were made to sample members. The average number of calls per sample member varied according to B&B:08/09 response status, base-year response status, mode of administration, and phase of data collection. Cases that completed a B&B:08/09 interview received an average of 6 calls, while nonrespondents received an average of 32 calls during the interviewing period. Base-year respondents received 9 fewer calls, on average, than base-year nonrespondents (9 and 18, respectively) (t(478) = 10.98, p < .001). There were also call count differences depending on mode of interview administration. Overall, respondents who completed interviews over the telephone required more calls than respondents who completed interviews over the web, an average of 14 calls, compared with 3 calls (t(2,898) = 33.50, p < .001). However, when early response phrase interview completes were excluded, web respondents required more calls than telephone respondents, an average of 17 calls, compared with 14 calls (t (4,836) = 8.91, p < .001). The average number of telephone calls is shown in table 23. "}, {"section_title": "Evaluation of Student Interview Items", "text": "An evaluation of the B&B:08/09 student interview items included analyses of the data collected in the instrument coders and a review of help text access rates, success rates for conversion text, and item nonresponse."}, {"section_title": "Instrument Coders", "text": "Assisted coding systems were used to standardize the collection of data on, and code, any postsecondary schools attended, major or field of study, occupation, and any elementary or secondary schools where the respondent may have taught. Text strings were collected from the respondent, and then a keyword search of an underlying database was conducted, allowing the respondent to select the best option from a list of possible options returned. An assisted coding system was not used to code industries, but available industry classifications allowed respondents and interviewers to select an industry classification from among a list of standardized options (for a detailed description of each coder, see section 3.1.1). Recoding. Ten percent of the major, occupation, and industry codes chosen in the student interviews were randomly selected to be reviewed by expert coding staff for recoding. 19 Expert coders assessed the accuracy of codes chosen in the interview based on the text string provided by the respondent. Across modes of administration and across coders, expert coding staff generally agreed with the codes chosen for text strings in the interview. Overall, expert coders agreed with major, occupation, and industry codes chosen in the student interview 90 percent of the time, recoded codes chosen to a new value 7 percent of the time, and were unable to choose a code based on too vague a text string about 3 percent of the time. Only the industry coder showed significant differences in recode rates between modes of administration. Expert coders agreed with industry codes chosen by web respondents 79 percent of the time and with those chosen by interviewers 90 percent of the time (z = 4.24, p < .001). Expert coders recoded industry codes chosen by web respondents 17 percent of the time, and recoded those chosen by telephone and field interviewers 7 percent of the time (z = 4.65, p < .001). Industry text strings provided by web respondents and were too vague to code 4 percent of the time, as were industry text string provided by interviewers. Table 24 shows the rate of recoded values-same as original code, recoded to different value, or text string too vague to code-chosen by the expert coders for the major, occupation, and industry codes in the interview. Upcoding. Project staff chose an appropriate code for any text strings provided by respondents or interviewers for which a code was not selected in the IPEDS, major, occupation, industry, and elementary/secondary school coders. Text strings from web interviews generally required more upcoding than text strings from telephone and field interviews because interviewers received special training on coders. Results of the upcoding process are shown in table 25. "}, {"section_title": "Help Text", "text": "Respondents or interviewers were able to click on a help button provided on each B&B:08/09 interview screen for both general instrument and question-specific help. The general instrument help provided answers to FAQs about web browser settings and response types (i.e., how to respond using a check box, dropdown box, or radio button). The question-specific help provided definitions of key terms and phrases used in question wording and response options and provided any other explanations thought to help clarify and standardize the meaning of questions for respondents. The number of times that respondents or interviewers clicked the help button on each screen relative to the number of respondents who were administered the question determined the rate of help text access for that screen. The screen-level rate of help text access was analyzed overall and by mode of interview administration to identify screens that may have been problematic for users. For forms administered to at least 50 respondents, the overall mean rate of help text hits per screen was less than 1 percent. Help text was accessed 2 percent of the time during interviews by telephone and field, compared with 1 percent of the time by web respondents (z = 13.30, p < .001). The interview question asking respondents for their employer's primary industry (item name RDINDCD) had the highest overall rate of help text access, at 4 percent. The help text for this question was accessed 16 percent of the time by telephone and field interviewers, compared with 1 percent of the time by web respondents (z = 30.99, p < .001). It is worth noting here that interviewers were encouraged as part of their training to refer to the help text on this item when respondents showed hesitation in understanding the term \"industry.\" Table 26 shows the three interview questions administered to at least 50 respondents and for which help text was accessed at a rate of at least 2 percent. "}, {"section_title": "Conversion Text", "text": "Fifteen questions in the interview were considered critical; that is, responses to these questions were especially important to the study and high rates of missing data on these questions would impact the quality of these data. When respondents did not provide an answer to these questions and the Next button was clicked on the interview screen, then conversion language (or conversion text) appeared above the question to encourage a response. Interviewers were asked to read conversion text to respondents and then to reread the interview question. The conversion text attempted to relay the importance of that particular question to the study and emphasized the confidential nature of responses. Some critical questions also displayed a don't know response option for respondents once the conversion text was triggered. Dividing the total number of responses to the critical questions after the conversion text was displayed by the total number of cases where the conversion text was triggered provided a conversion rate for the questions that was attributed to the conversion text. Overall, conversion text was triggered in the student interview approximately 1170 times throughout data collection. Eightyfive percent, or 1000, of these cases were converted to a response after the conversion text was displayed. The web interviews accounted for 980 of the 1170 cases where conversion text was triggered and 880 of the 1000 converted cases. The remaining 190 cases where conversion text was triggered were in telephone and field interviews, of which 120 were converted. The rate of conversion as measured by the triggering of conversion text was 90 percent in web interviews, compared with 63 percent in telephone and field interviews (z = 9.75, p < .001). There was not a way to measure conversion to a response by telephone and field interviewers before conversion text was triggered. Conversion text was triggered more than 100 times for three interview questions. The occupation coder (RDOCC1) triggered conversion text in 340 cases and yielded a conversion rate of 96 percent. Web cases were converted at a rate of 96 percent, compared with telephone and field cases, which were converted at a rate of 67 percent (z = 2.69, p < .01). Monthly rent or mortgage payment amount (RFMTGAMT) triggered conversion text in 310 cases, with a conversion rate of 69 percent. Web cases were converted at a rate of 81 percent, compared with telephone and field cases, which were converted at a rate of 39 percent (z = 7.09, p < .001). Amount owed for undergraduate loans (RBUGOWE) triggered conversion text in 120 cases, and produced a conversion rate of 84 percent with no significant mode difference. Table 27 displays the rates of conversion for all 15 items in the interview with conversion text. Conversion rates were examined overall and by mode. "}, {"section_title": "Item Nonresponse", "text": "Rate of nonresponse was a data quality measure used to identify troublesome interview items and better understand the experiences of sample members in completing the interview. Total nonresponse rates were calculated for items with missing data (including don't know responses) that were administered to at least 100 respondents. Overall, the item-level nonresponse analysis yielded 36 items out of 1,358 interview items with more than 5 percent missing data. 20 Items on coders returned relatively high rates of nonresponse 21 . The item with the highest rate of nonresponse, (RESDST01), was the district name on the elementary/secondary school coder. Of the 210 respondents who received this item, approximately 67 percent did not provide a text string for their elementary/secondary school's district. Approximately 13 percent of respondents did not enter a text string for their original major in the major coder item (RBORGMAJ), 18 percent of respondents did not enter a text string for their job duties in the occupation coder item (RDJBDY), and 19 percent did not enter a text string for their employer's industry in the industry coder item 20 Partial and abbreviated interview completions and missing data for interview nonrespondents were excluded from this analysis. For interview items with multiple iterations, this analysis will evaluate the first iteration only. 21 Results of item nonresponse in this section apply only to the respondents to the B&B:08/09 interview and are not weighed by the B&B analysis weight. Chapter 6 provides additional details on the item nonresponse, the potential bias due to item nonresponse, and the impact of the item imputation which was used to fill in missing data and to reduce item nonresponse bias. (RDIND). In addition, there were several income questions among those items with nonresponse rates greater than 5 percent. Approximately 42 percent of respondents did not provide an estimate of their spouses income on spouse's income estimate for 2008 (RFINSRA), and 30 percent did not provide an amount for their private student loan debt on amount borrowed in private undergraduate loans (RBUGPRIV). Item-level nonresponse rates were also examined by mode of administration. There were significant differences between the web mode and interviewer (telephone and field) modes in the nonresponse rates of 16 interview items. Notably, the following income items showed higher rates of nonresponse among telephone and field respondents than among web respondents: income in 2008 (RFINCOM; z = 12.89, p < .001); spouse's income in 2008 (RFINCSP; z = 8.41, p < .001); and not married to spouse in 2008 (RFSPNOT; z = 9.05, p < .001). In contrast, the following items on coders showed higher rates of nonresponse among web respondents than among telephone and field respondents: NPSAS original major: string (RBORGMAJ; z = 4.65, p < .001); NPSAS primary major: string (RBNPMAJ; z = 6.33, p < .001); NPSAS second major: string (RBNPMJ2; z = 4.05, p < .001); postbaccalaureate degree 1 primary major: string (RCMAJ01; z = 10.48, p < .001); industry: coder (RDINDCD; z = 11.56, p < .001); [REJBTP01] school: lowest grade level offered (RESGLO01; z = 2.17, p < .05); and [REJBTP01] school: highest grade level offered (RESGHI01; z = 2.23, p < .05). Table 28 summarizes the item-level nonresponse for items administered to at least 100 respondents with a rate of at least 5 percent missing.  "}, {"section_title": "Student Interview Conclusions", "text": "B&B:08/09 interviews were conducted from July 13, 2009 to March 12, 2010. Of the 17,170 eligible sample members included in the B&B:08/09 student interview data collection, 16,050, or 93 percent, were successfully located. Successful locating methods included batch searches, such as Telematch and CPS, and address update information provided by both sample members and their parents. Overall, about 7 percent of sample members required intensive tracing. Locating methods attempted during the B&B:08/09 interview included text message reminders and the use of social networking sites. Of the 17,170 sample members included in the B&B:08/09 student interview data collection, 15,090, or 88 percent, completed a full, abbreviated (English or Spanish), or partial interview. About 96 percent of all B&B:08/09 interview respondents completed the full interview. Eighty-one percent of interviews were completed on the Web, 16 percent of interviews were completed by telephone, and 3 percent were completed in field interviewing. Eighty-nine percent of base-year respondents completed the B&B:08/09 interview, compared with 49 percent of base-year nonrespondents. Twothirds (67 percent) of all completed B&B:08/09 interviews and over three-fourths (80 percent) of web interviews were completed during the early response phase. Base-year respondents received $30 if they completed the interview in either the early response or nonresponse conversion phase. Baseyear nonrespondents received $50 to complete the interview. On average, the B&B:08/09 interview took 28 minutes to complete. On average, telephone interviews took the longest to complete and the Undergraduate Education section in the interview took the longest to complete. An evaluation of the quality of the data provided by the B&B:08/09 student interview showed that methodological features built into the instrument as well as training and supervision of interviewing staff aided in the successful administration of the interview. The design of assisted coding systems in the instrument and the training of interviewers on coders appeared successful. Overall, expert coders agreed with major, occupation, and industry codes chosen in the interview 90 percent of the time. The appearance of help text and conversion text in the instrument also appeared to improve question response. Help text was accessed significantly more often during interviews by telephone and field interviewers than by web respondents. It should be noted that interviewers had been encouraged to use help text, as needed, as this feature of the instrument was emphasized during telephone interviewer training. Eighty-five percent of the cases where conversion text was triggered in the interview were converted to a response after the conversion text was displayed. Overall, the item-level nonresponse analysis yielded 36 items out of 1,358 interview items with more than 5 percent missing data. Debriefing of tracers and field interviewers at the end of data collection indicated that frequent monitoring of telephone interviewers, a help desk that tracked and resolved difficulties encountered by sample members attempting to complete the web interview, and quality circle training and feedback meetings were useful as data collection quality control (QC) procedures. Most interviewers indicated that they felt they had all the tools necessary to successfully administer the B&B:08/09 student interview."}, {"section_title": "Chapter 4. Transcript Data Collection, Outcomes, and Evaluation", "text": "B&B:08/09 collected postsecondary transcripts for the B&B:08 cohort as an official record of sample members' academic experiences, including courses taken and performance in these courses. To ease burden on participating institutions, the B&B:08/09 transcript collection was combined with the transcript collection for BPS:04/09 under PETS:09. This chapter provides an overview of the B&B:08/09 portion of the transcript collection and will describe the processes and systems developed for collecting transcripts. It will also report on transcript keying and coding activities and the data and process evaluation procedures introduced to assure data quality."}, {"section_title": "Transcript Data Collection and Response Rates", "text": "A Transcript Control System (TCS) was designed to manage the transcript and other institution data requested from the institutions attended by the B&B:08 cohort. Institution contactors (ICs) served as liaisons to institutions that provided the requested materials through a variety of possible submission methods, including a study website. Transcripts were received for a total of 17,430 students. The details of transcript data collection and response rates are included in this section."}, {"section_title": "Transcript Control System", "text": "The integrated, web-based TCS supported each step of the B&B:08/09 transcript collection, including project management, communications, and tracking. The TCS comprised several transcript management systems: the Institution Contacting System was used to store and access data on students and track efforts to obtain their transcripts; the Data Receipt System managed data received on sample members, including transcripts and catalogs for the institutions attended; and the Keying and Coding System (KCS) facilitated the efficient and secure capture of data from student transcripts. See section 4.2 for a detailed discussion of the development and use of the KCS. Transcript control system data were stored in SQL databases accessible for use in reporting, documenting and delivering transcript data."}, {"section_title": "Training of Institution Contactor (IC) Staff", "text": "Institution contacting staff consisted of eight ICs and two QCS who were responsible for staff supervision. Prior to the start of transcript data collection, the ICs were trained over a 2-day period on transcript and catalog collection, gaining cooperation, and problem resolution. Training included information on B&B and a review of confidentiality regulations. Activities focused on guidelines for interactions with institution staff, gaining cooperation, collection of catalogs and transcripts, and collection and receipt systems. During the transcript collection period, staff were briefed on their progress, asked questions, and discussed issues at weekly quality circle meetings. The IC training agenda is included in appendix G."}, {"section_title": "Transcript Collection Procedures", "text": "Transcripts were requested from the 1,100 institutions where B&B sample members completed their bachelor's degree requirements during the 2007-08 academic year (their NPSAS institution). A complete transcript was requested from this institution for each student. Beginning in late October 2008, a transcript request packet was sent to the director of the institution research office at each institution. In the absence of an office of institution research, packets were sent to the registrar's office. The packet contained notification materials for transcript data collection (see appendix H), including the following: \u2022 a letter introducing PETS:09; \u2022 an introductory letter from NCES; \u2022 a letter of endorsement from the American Association of Collegiate Registrars and Admission Officers; \u2022 a list of other endorsing agencies; \u2022 information regarding how to log on to the study's secure website; \u2022 descriptions of and instructions for the various methods of providing transcripts; and \u2022 excerpts from the Family Educational Rights and Privacy Act that illustrated the transcript collection's compliance with the legislation. Follow-up calls by trained ICs were placed 2 days after the initial mailing to ensure receipt of the packet and to answer any questions about the study. Prompting calls were made and reminder emails sent, as needed, from November 2008 through July 2009. Transcript submission. Institutions were provided the following seven options for submitting transcripts: 1. File upload to the study website. Institutions were asked to submit electronic transcript files, preferably in an extensible markup language (XML) or electronic data interchange (EDI) format that conformed to the Postsecondary Electronic Standards Council standard. If the transcript data were not already in one of the two preferred formats, the institution was asked to convert the files before loading, or to prepare files using the file specifications provided on the study website. The transcript files were submitted directly to the secure study website. The latest technology systems were incorporated into the transcript website application to ensure strict adherence to NCES confidentiality guidelines. The web server included an SSL encryption certificate and was configured to force encrypted data transmission over the Internet. All of the data entry modules on the site were password protected, and the user was automatically logged out of the system after 20 minutes of inactivity. Just as with all the submission methods, once the transcript files were received, they were immediately moved to a secure project folder accessible only to a subset of project staff. 2. Submission of electronic transcripts by secure file transfer protocol (FTP) server. Transcript files could be submitted using an FTP server, which ensures an encrypted control session. As with the file upload, it was preferable for files to be submitted using an XML or EDI format, but files could be submitted in virtually any file layout. After being copied to the secure project folder, the files were immediately deleted from the FTP server. 3. Submission of transcripts via eSCRIP-SAFE\u2122. eSCRIP-SAFE\u2122 is a third-party vendor that receives and electronically converts transcripts to PDF files, then stores them on a secure server. Institutions registered with this service send data by secure internet connection to the eSCRIP-SAFE\u2122 server, where they can be downloaded only by a designated user. The electronic transcript files downloaded by project staff from eSCRIP-SAFE\u2122 were saved only to the secure project folder. 4. Submission of transcript files as encrypted attachments to e-mail. Electronic transcript files could be emailed as attachments to the project e-mail account. Guidelines on encryption and creating strong passwords for transcript attachments were provided to the institutions. Encrypted transcript files were moved to the secure project folder and deleted from the e-mail folder immediately."}, {"section_title": "Submission of transcript files through a dedicated server at the University of Texas at Austin.", "text": "A dedicated server at the University of Texas at Austin, developed to allow transcript exchange among registered institutions, was provided as an option to institutions submitting transcripts to the study. The server supported both XML and EDI formats. 6. Submission of transcripts via secure electronic fax. Transcripts were also accepted via secure electronic fax. To safeguard against information being misdirected or intercepted by individuals to whom access was not intended or authorized, RTI protocol only allowed for transcripts to be sent to an e-fax server housed in a secured data center at RTI. The transcript data were stored on the server as portable document files (PDFs). To ensure confidentiality, institutions were asked to send a test fax with nonsensitive data and to use a specific fax cover sheet from the project website that included a confidentiality statement. After being received and copied to the secure project folder, transcript files were deleted from the e-fax server. 7. Submission of transcripts via FedEx. Transcripts were also accepted via FedEx. To safeguard confidentiality, institution staff were instructed to redact any personally identifiable information from the transcript including student name, address, date of birth, and SSN (if present). Paper transcripts were kept in a locked file cabinet in RTI's secure data receipt facility, to which only a limited number of B&B:08/09 transcript staff had access. After the paper transcripts were scanned and stored electronically, they were shredded. In addition to transcripts, other information from each institution was needed for keying and coding. Institutions were asked to provide academic calendar and grading system information on the study website. If course catalogs could not be obtained separately through institution websites or through CollegeSource Online, a resource for over 50,000 postsecondary institution catalogs, they were requested from institutions. Transcripts and course catalogs received were inventoried, assigned unique identifiers, reviewed for any problems with legibility and completeness, and logged each day in the data receipt system. Project staff used daily monitoring reports to review problem transcripts and ICs assisted with resolving transcript problems directly with institutions."}, {"section_title": "Institution Website", "text": "The PETS:09 website (figure 10) was the portal used to collect institution data and transcripts. The website contained information about PETS, including research topics, the transcript collection, how transcript data would be used, answers to frequently asked questions, and confidentiality assurances. Contact information for the transcript data collection help desk and project staff at RTI, as well as links to the main NCES and RTI websites, were also included on the home page. From the secure portion of the website, institutions could view the list of their sampled students, view detailed instructions for providing transcript data, and upload data. Various systems were incorporated into the website application to ensure strict adherence to NCES confidentiality guidelines, including the following: \u2022 an SSL Certificate ensured secure data transmission over the Internet, \u2022 all data entry modules were password protected, \u2022 users were automatically logged out of the system after 20 minutes of inactivity, and \u2022 files uploaded to the secure website were immediately moved to a secure project folder accessible only to a subset of project staff. "}, {"section_title": "Transcript Collection Response Rates", "text": "Institution-level participation. Of 1,300 institutions in the transcript sample, 85 percent were determined to be eligible (i.e., they were confirmed as awarding bachelor's degrees during the NPSAS:08 academic year). Of these 1,100 eligible institutions, 1,020 (93 percent) provided transcripts for the sampled students. Across the institution types represented, participation in the transcript collection ranged from 79 percent at the public 2-year institutions to 95 percent at the public 4-year non-doctorate-granting institutions. (The private nonprofit 2-year or less institutions demonstrated 100 percent participation but with a very small number of institutions.) The most common reasons cited by institutions for not participating included lacking the available staff to fulfill the request for transcripts and the timing of the transcript request. Table 29 provides institution participation rates by institution type. Transcript submission method outcomes. Table 30 presents the distribution of transmission methods selected by the institutions. Providing transcripts via secure fax was the choice of the majority of the institutions: 61 percent of the institutions used the secure fax option. This was the most convenient option for most institutions that routinely generate and send out hard-copy transcripts. The next most common method was to upload transcripts via the institution website, accounting for about 20 percent of institutions. About 8 percent of institutions sent transcripts via FedEx and 6 percent sent transcripts as encrypted attachments via e-mail. The less common methods included secure FTP, used by 4 percent of institutions, and the dedicated server at the University of Texas at Austin and eSCRIP-SAFE\u2122 which were used by 1 percent or less of institutions. Student-level transcript collection. The transcript sample comprised 18,500 students. As seen in table 31, a transcript, or information indicating that a student was ineligible, was received for 17,430 (94 percent) students. \u2022 Schools and terms. Names of the transcript school and any transfer institutions reported on the transcript, terms attended at these schools, and attempted and accepted transfer credits. \u2022 Academics. Academic honors awarded (e.g., Dean's or President's List) and any probations, by term. \u2022 Tests. Institution exams (e.g., competency and placement exams) or externally administered exams (e.g., SAT and GRE), and related scores. \u2022 Degrees and majors. Degree programs attempted or earned, such as a bachelor's or associate's degree, degree receipt dates, and honors awarded at graduation such as cum laude. The specific majors or fields of study for each degree were coded in this section. \u2022 Courses. Key data on courses listed on transcripts, including the terms in which the courses were taken, course numbers and names, and grades and credit or clock hours earned. In this section, each course was also coded for standardization. To help ensure the quality of data keyed and coded, specific features were incorporated into the KCS. For example, the KCS provided links to institution course catalogs for easy reference; limited ranges and the types of characters input for fields such as dates and exam scores; and required that postsecondary institutions, majors, and courses be coded using specially designed coders. KCS coding systems. The school and major or field of study coders incorporated into the KCS were assisted coders identical to those used in the B&B:08/09 student interview instrument. The school coder used the set of institutions contained in IPEDS, developed by NCES (http://nces.ed.gov/IPEDS/). The major or field of study coder was based on the CIP taxonomy developed by NCES (http://nces.ed.gov/ipeds/cipcode). CIP codes not associated with postsecondary majors or fields of study were removed from this coder, including codes for basic skills and developmental education, citizenship activities, health-related knowledge and skills, interpersonal and social skills, leisure and recreational activities, personal awareness and selfimprovement, and high school/secondary diplomas and certificates. The KCS course coder was similar to the KCS school and major or field of study coders, with the addition of certain search features. When a text string with the course title was entered, a keyword search based on the course title was conducted on the underlying database allowing the keyer/coder staff person to select the best option from a list of possible course options returned. If the course title did not adequately capture the description of the course in the institution catalog, keyer/coders could search the course coder database using keywords found in the course description in the institution course catalog or they could do searches by broad categories and by database codes. The KCS also included a feature for entering problem sheets for particular schools or transcripts. Problem sheets were categorized and routed to supervising staff for resolution."}, {"section_title": "Development of the KCS course coder.", "text": "The underlying database for the course coder in the KCS included 2,119 course codes and code definitions. Course codes were developed by integrating selected courses from the College Course Map taxonomy (CCM) (Adelman 2004) into the 2010 CIP taxonomy from NCES. PETS codes were represented by six digits in keeping with the CIP taxonomy: the first two digits indicated the most general category; the first four digits narrowed the focus to a subcategory; and the complete 6-digit code provided the most specific definition of the subject. Figure 12 provides a visual representation of the structure of CIP codes. Course codes in the CCM taxonomy, which used the same 6-digit structure as the thencurrent CIP, were developed through extensive transcript analysis and with input from expert advisors, including postsecondary faculty familiar with the fields of study. To create a comprehensive course coder for PETS:09, content from the CCM was incorporated into the 2010 CIP in two ways: (1) course codes found in the CCM without equivalents in the CIP were added as new codes, and (2) CCM codes with equivalent CIP codes were reviewed and, when additional details or examples were found, they were added to the CIP definitions. The first method resulted in the addition of 352 unique CCM course codes to the KCS course coder. These additions were placed alongside related topics in the 2-digit category and 4-digit subcategory structure common to both the CCM and CIP taxonomies. To make these additions easily identifiable, the last two digits in their codes used a unique numbering scheme, starting with 98 and descending as needed. An example can be seen in code 01.0698 in figure 13. The second method for integrating the CCM and the 2010 CIP resulted in adding content to the definitions of 316 CIP codes. The additional text increased the likelihood of identifying appropriate course codes using the keyword searchable KCS course coder. In instances where a CIP code was elaborated, the CCM content was placed at the end of the CIP definition. Finally, in addition to content from the CCM, 47 general and other codes were added to the KCS course coder using KCS course coder fifth and sixth digit values of \"00\" for general and \"99\" for other, when these codes were not already present in the CIP. Figure 13 illustrates a representative set of codes in the KCS course coder. Transcript courses were originally coded using the available draft of the 2010 CIP. The database for the course coder in the KCS was updated when the final version of the 2010 CIP was released in July 2009. Compared to the draft version, the final 2010 CIP included 80 new codes, 21 codes with different code numbers, and two codes that were deleted from the draft. For the 80 new codes in the final 2010 CIP, courses on transcripts previously coded with similar codes from the draft 2010 CIP were reviewed by keyer/coder staff to determine if they fit better into the new CIP codes. There were 17,778 courses previously coded with draft 2010 CIP codes that were updated to the final 2010 CIP codes. There were just three courses previously coded with deleted CIP codes from the draft 2010 CIP which, when reviewed, were assigned to similar, related codes in the final version of the 2010 CIP."}, {"section_title": "Training of Transcript Keyer/Coder Staff", "text": "Over the course of three separate 5-day trainings, beginning in January 2009, 71 keyer/coder staff were trained to use the KCS. The keyer/coder staff were supervised by five QCS who were responsible for administrative and management issues, as well as quality review of keyed and coded transcripts and keying and coding, as needed. Each training session began with background on B&B:08/09, review of confidentiality regulations, fingerprinting, and signing of notarized affidavits. These activities were followed by an overview and discussion of the different types of transcript formats and key data elements to be located and entered into the KCS. Presentations on keying and coding fundamentals were followed by problem-solving exercises and practice sessions. The fifth day of training consisted primarily of supervised keying and coding practice using actual transcripts, followed by a practicum exam on which all trainees were required to obtain 90 percent or better proficiency for certification. The training agenda for transcript data collection is included in appendix G. In conjunction with the above trainings and because of the wide variation in transcript layouts and information provided by the institutions, quality circle meetings were held weekly for the first 8 months of keying/coding. During the meetings, QCS and a group of keyer/coders were briefed on production and performance measures and were invited to ask questions or raise concerns. Topics discussed ranged from the use of specific CIP codes, to keying of the more problematic data elements. As the project progressed, the frequency of the quality circle meetings was adjusted to biweekly. Often as a result of feedback during quality circle meetings, additional narrowly focused trainings were held as needed for specific topics, such as entering multiple transcripts for sample members, coding of electronic transcripts, and the use of problem sheets to record issues or questions with transcripts."}, {"section_title": "Transcript Keying and Coding Outcomes", "text": "Keying and coding was performed on 16,070 transcripts, with one transcript per student. These transcripts included a total of 741,450 courses, 28,090 terms, and 17,180 degrees. Of the 741,450 courses coded, 73 percent were coded with a specific 6-digit code. General codes were selected for 24 percent of the courses coded and were typically chosen for the many introductory level courses, while other codes were selected for 2 percent of the courses when no more specific code matches in the KCS course coder were found for a course on a transcript. Uncodeable courses accounted for 2 percent of all courses, often due to unclear course titles or inadequate information on course content. The results of course coding are shown in figure 14. "}, {"section_title": "Evaluation of Transcript Keying and Coding", "text": "Multiple evaluation steps were taken to ensure the quality of transcript data entered into the KCS. These activities included rekeying a sample of data elements, expert coding a sample of course and major/field of study coding, and upcoding of text strings for institutions or other transcript data elements that could not be coded initially. Rekeying. To evaluate the reliability of transcript data keyed into the KCS, approximately 10 percent (1,600) of the transcripts were randomly selected to be rekeyed. A subset of transcript data elements were rekeyed by quality control supervisors, which took approximately 10 to 15 minutes per transcript, depending on the number of the selected data elements found on the transcript (e.g., the number of terms attended). Figure 15 shows agreement rates for the rekeying activity, organized by keying and coding section. For both rekeying and recoding activities, the Cohen's kappa statistic was used to assess inter-rater reliability between the original coder and quality control supervisors, or expert coders. Cohen's kappa measures the proportion of agreement between raters, above what would be expected by chance. Landis and Koch (1977) proposed that kappa scores of 0.81-1.00 be considered \"near-perfect agreement,\" 0.61-0.80 \"substantial agreement,\" and 0.41-0.60 \"moderate agreement.\" 22 All of the rekeyed items have at least moderate agreement, with case information, terms, and degrees all within the range of near perfect agreement. Data collected in the test section of the KCS (exam name, date taken, and score), however, had a noticeably lower value for kappa (0.45) than other data elements. Further investigation into test data on transcripts revealed that of the 1,020 institutions that provided transcripts, only 28 schools included Advanced Placement (AP) tests with scores on their transcripts. Instead, tests, particularly exams for which course credit was awarded, were often included on transcripts in a format more similar to courses (e.g., \"AP biology, 3.0 credits\"). Due to the low interrater reliability 22 Although Landis and Koch's classification scheme is commonly used as a benchmark for kappa scores, there is debate regarding its utility and the appropriateness of kappa classification schemes, in general. See Gwet (2010), Sim and Wright (2005) and Fleiss (1981) for detailed discussions of criticisms and alternative classifications. score and frequency with which test data were found on transcripts, this category of data were determined to be unreliable and will not be included in B&B data file because its presence on postsecondary transcripts was determined to be unreliable. Expert coding. Expert coding was performed on 71,820 courses, both to evaluate the reliability of coded data and to create feedback opportunities to improve data quality. Expert coding used more experienced (expert) staff, all of whom held at least a bachelor's degree and who also performed coding in the field test study, to recode a subsample of coded courses and to provide feedback to keyer/coders on course code selection. Expert coding was performed from the beginning of the keying and coding process and continued until its conclusion so that keyer/coder staff could receive feedback on their performance and additional training needs could be addressed promptly. Initially, the expert coding process included two steps. In expert coding 1 (EC1), expert coder staff reviewed course information and selected a code, which was then compared to the keyer/coder's choice. In cases where the keyer/coder and expert coder selections did not match, expert coding 2 (EC2) was performed to assess the reliability of EC1. EC2, in addition to being performed on all cases where EC1 and keyer/coder choices did not match, was also performed on a 15 percent random sample of codes where the EC1 and keyer/coder agreed. EC2 was performed as a review of the quality of the EC1 staff and included review of course information and deciding upon the EC1 choice, the keyer/coder choice, or an entirely different code-to avoid potential bias, the EC2 could not identify which selection was made by the EC1 or keyer/coder. EC2 review of keyer/coder EC1 disagreements added reliability to the EC1 code selections upon which keyer/coder feedback was developed. EC2 was performed by the same project staff responsible for keyer/coder training and course code development. Based upon a sample of 3,350 disagreements between keyer/coder and EC1, EC2 agreed with EC1 in 60 percent of the cases, with the keyer/coder in 32 percent of the cases, and selected a different code (neither the EC1 nor the keyer/coder's choice) in 9 percent of the cases. The EC2 staff agreed with expert coder choices significantly more than the keyer/coder choices: \u03c7\u00b2(1, N = 3,350) = 308.22, p < 0.01). As with keying and coding, expert coding was performed in batches by school. Courses were not expert coded until all of a school's transcripts had been keyed and coded. For both EC1 and EC2, expert coders reviewed course number and name and had access to course catalogs to make coding decisions. For the purpose of reviewing keyer/coder work and providing feedback, expert coding was performed on both random and cluster samples of courses. EC1 was performed on a random 10 percent sample of all courses from each school. For schools with fewer than 10 total courses, all courses were expert coded. Cluster sampling was used to select courses coded with other codes (e.g., 26.0299, biochemistry, biophysics and molecular biology, other). Courses coded as needs review or uncodeable were also reviewed in expert coding. Interrater reliability for course coding was assessed using 5,000 courses randomly selected for calculating agreement statistics. The kappa statistic was used to assess interrater reliability between the original coder and expert coders. Expert coding results are shown in figure 16. Agreement rates are shown at three levels of specificity: 2-digit, 4-digit, and 6-digit. At the 2-digit level, the kappa statistic indicates near-perfect agreement between keyer/coder and expert coder. At both the 4-and 6-digit levels, the kappa statistic indicates substantial agreement. Review of \"other\" course codes. For the first six months of keying, courses coded using the \"other\" category in the PETS:09 coder were reviewed by expert coders with the goal of minimizing the use of the category. Keyer/coder training emphasized that \"other\" codes should be reserved for courses that fit within a 4-digit subject area but for which more specific 6-digit codes in that series were not appropriate. \"Other\" was not intended for coding problematic courses or those for which additional analysis would result in a more accurate code. Expert coders provided direct feedback to keyer/coders on cases for which there were more appropriate coding choices. In addition to the expert coding performed, \"other\" codes were also reviewed to determine if additional codes should be added to the PETS coder. The PETS course taxonomy included 231 courses with an \"other\" designation, such as 31.0599, \"Health and Physical Education/Fitness, Other,\" or 23.9999, \"English Language and Literature/Letters, Other.\" A review of the courses coded as \"other\" was undertaken to determine if there were common subjects within the codes that would merit introduction of new codes. The median number of uses of \"other\" for all course codes was 132. This median was used as the threshold for adding a new code: if 132 instances of the same subject could be identified within the courses coded using the \"other\" code, a new code would be added. However, a review of \"other\" codes did not identify any subjects that met this threshold, so no new codes were added. Upcoding and reliability recoding for major/field of study. Text strings for 158 entries for field of study that were not coded by keyer/coders were later reviewed by project staff to determine if an appropriate code could be identified. In addition to this upcoding activity, a random sample of 2,745 coded majors was also included in this process as a key-rekey step to evaluate the reliability of the field of study data. The results are shown in figure 17. For the randomly selected cases, the coder and recoder agreed in 90 percent of the cases. For the uncoded majors, project staff was able to identify a major code in 63 percent of the cases. In 22 percent of the uncoded cases, data on the transcripts were too vague to identify an appropriate code and, in 15 percent of cases, the original code selection was correct. Upcoding for institutions and variables with \"other, specify\" options. Uncoded text strings for institutions were reviewed by project staff to determine if an appropriate code could be identified. This task was performed on 2,020 cases by staff with greater familiarity with postsecondary institutions and with additional resources for researching school names and locations. The results of this institution upcoding are shown in figure 18. In 57 percent of the cases, the institution could not be identified in IPEDS, and thus remained uncodeable. Analysts were able to code 43 percent of the previously uncodeable institutions while for less than 1 percent of cases, the school name could not be identified at all. In addition to institutions, transcript data elements with \"other, specify\" options included: \u2022 noncourse credits awarded (e.g., course credit for Advanced Placement tests), \u2022 tests (e.g., SAT), \u2022 term honors (e.g., Dean's List), \u2022 term probations (e.g., academic probation), \u2022 degree programs (e.g., associate's), \u2022 grades (e.g., R), \u2022 bachelor's degree types (e.g., Bachelor of Education), and \u2022 degree honors (e.g., with distinction). All items coded as \"other, specify\" were reviewed by analysts to determine if the text strings could fit into existing choices or if there were common strings that merited addition of a new choice. For example, Bachelor of Education was not included in a drop-down menu for bachelor type, therefore it was entered as a text string under \"Other, specify.\" When the value appeared repeatedly as a text string, it was assigned as a category and upcoded accordingly. Table 32 shows the results of \"other, specify\" upcoding. The total number of cases is shown for each data element along with the number and percent that were upcoded. Keyer/Coder Staff Debriefing. Near the conclusion of keying and coding, a debriefing focus group was held with seven keyer/coders who had collectively keyed and coded more than 10,000 transcripts. Two participants had also performed duties as QCS. Focus group participants agreed that the keyer/coder training had been helpful and prepared them for the task. They also found quality circle notes and meetings to be useful. The keying and coding system facilitated entry of transcript data, although some data elements in the system were not commonly found on transcripts, such as \"transfer credits for grade point average\" and \"state basic skills tests.\" Regarding course coding, focus group participants found the search features of the KCS to be useful, but certain course subjects were difficult to code, such as engineering and computer courses, as well as some education courses, when it was difficult to discern whether the course was about learning the topic itself or learning about how to teach the topic. Finally, focus group participants indicated that identifying remedial courses was sometimes difficult, for instance when the course description sounded like it could be remedial but without stating so explicitly. In such cases, keyer/coders were sometimes able to confirm a course was remedial by noting a grade greater than an F with no credits awarded."}, {"section_title": "Timing of Transcript Keying and Coding", "text": "Transcript keying and coding was conducted from January 19, 2009, to June 11, 2010. On average each transcript took 88 minutes to key and code. The time to complete keying and coding varied by institution sector, ranging from a 75 minute average for transcripts from private, for-profit 2 years or more institutions, to 98 minutes for transcripts from private, nonprofit, 4-year nondoctorate granting institutions (table 33). "}, {"section_title": "Transcript Data Collection Conclusions", "text": "A transcript collection was conducted for B&B:08/09 as part of PETS:09. Institution contacting staff were trained to facilitate the data collection process, using a transcript control system to aid institution representatives in the submission of transcripts. A PETS:09 website was also developed to aid institutions in the submission of transcripts, providing instructions for several secure electronic transmission methods, fax, and FedEx. Data receipt staff completed initial processing and quality review of the transcript data and institution contacting staff communicated with staff at postsecondary institutions to resolve any missing data or problems. Transcript keying and coding was performed using a specially designed keying and coding system that was divided into sections for the entry of data for case information, schools and terms, academics, tests, degrees and majors, and courses. A postdata collection debriefing of keyer/coder staff indicated the system was effective for transcript data entry. The PETS coder, created by merging 2010 CIP and 2003 CCM, provided a detailed code taxonomy for the coding of courses by subject. The 5-day keyer/coder training and ongoing feedback offered through quality circle meetings prepared staff to reliably perform keying and coding tasks. All staff passed the proficiency test at the conclusion of training, and the results of the keying and course coding interrater reliability assessments indicate substantial agreement between keyer/coders and expert coders. Recoding of the random sample of major/field of study data further supported the reliability of the data. Upcoding was performed on all uncoded institutions and additional data elements where \"other, specify\" options were available, such as noncourse credit awarded, tests, honors, probations, and degree programs. Upcoding added greater detail to data collected. Courses coded with \"other\" codes were reviewed for common subjects, but none were found in great enough numbers to add new course codes."}, {"section_title": "Chapter 5. Postdata Collection Data File Processing Activities", "text": "The data files for B&B:08/09 contain student-level data collected from administrative databases, student interviews, and transcripts. These data are available to users in two ways. A set of restricted research files, fully documented, are available to restricted data licensees on a CD from NCES. Tables and regression analyses can be run by any user through the NCES online application PowerStats, which also contains variable documentation. PowerStats is available online via the DataLab site at http://nces.ed.gov/datalab/index.aspx. This chapter describes each file and details the editing and documentation processes applied to each."}, {"section_title": "Administrative Record Matching", "text": "In addition to the student interview, data collection for B&B:08/09 included record matching to the CPS, the National Student Loan Data System (NSLDS), and the NSC StudentTracker database. This section provides a discussion of the observed match rates for these three databases."}, {"section_title": "Central Processing System", "text": "The CPS contains data provided to the U.S. Department of Education by students and their families when they complete the Free Application for Federal Student Aid (FAFSA). Successful record matching to CPS can occur only for sample members who were federal student financial aid applicants for the years requested. Matching for B&B:08/09 was to CPS data for the 2008-09 and 2009-10 financial aid years, using a sample member's SSN concatenated with the first two letters of the last name as the CPS ID. The percentage of sample members who matched to CPS for the 2008-09 academic year was about 24 percent. For 2009-10, the rate was approximately 23 percent. As expected, 2009-10 match rates were lower than those for 2008-09 because fewer members of the B&B:08 cohort continued to be enrolled in postsecondary education and to apply for federal aid. Table 34 shows the CPS matching results. "}, {"section_title": "National Student Loan Data System", "text": "NSLDS matching was performed by the NSLDS contractor at the request of the U.S. Department of Education, using names, SSNs and dates of birth provided by RTI. Successful matching to NSLDS could occur only for sample members who were awarded federal loans, Pell Grants, TEACH Grants, SMART Grants, or ACGs. NSLDS files are historical, so information about a student's receipt of such loans and grants was available not only for the current academic year but also for any applicable prior years. Consequently, historical match rates reported for the B&B:08/09 sample members do not necessarily reflect only the 2009-10 academic year. The federal loan match rate was about 75 percent and the match rate for Pell Grants was about 52 percent. The number of sample members matching to the data system for ACGs or SMART Grants was about 19 percent, while the match rate for TEACH Grants was less than 1 percent. This is not surprising, given that less than 15 percent of our respondents reported in the interview that they were currently teaching or had taught since graduating. Table 35 summarizes the match rates observed for the B&B:08/09 sample members. "}, {"section_title": "National Student Clearinghouse", "text": "In addition to the CPS and NSLDS file matching, the B&B:08/09 sample was matched to the NSC StudentTracker database, which provides information on postsecondary enrollment, degree, and certificate records on behalf of participating postsecondary institutions. In order to perform the match, RTI supplied SSNs, names, and dates of birth for sample members to the NSC. Overall, a record match for a student's enrollment at any NSC-participating institution was obtained for about 94 percent of the B&B:08/09 sample. Match results in table 36 are based on enrollment and degree records from all participating institutions for the 2002-03 academic year through the 2008-09 academic year. "}, {"section_title": "B&B:08/09 Main Study Data Files", "text": "The primary analysis file, from which PowerStats was constructed, contains data for 15,050 respondents. The first data release was adjudicated and approved for public release July 20, 2011. The primary analysis file contains over 400 variables, developed from multiple sources. Throughout the data collection period, data were processed and examined for QC purposes. Editing of student data began shortly after the start of web data collection, when procedures and programs for this purpose were first developed. Anomalous values were investigated and resolved, where appropriate, through the use of data corrections and logical recodes. Interim files were delivered to the NCES for review throughout the data collection period. The restricted-use CD for B&B:08/09 contains the following files, each of which can be linked by the student's study ID: 23 \u2022 B&B:08/09 analysis file. Contains analytic variables derived from all B&B data sources and selected direct student interview variables available as of the initial release of B&B:08/09 PowerStats. \u2022 B&B:08/09 student data file. Contains student interview data collected from 15,050 respondents. Topics include eligibility, undergraduate and graduate education, employment, teaching and background. \u2022 B&B:08/09 undergraduate institution data file. Contains undergraduate institution and degree data obtained from the B&B:08/09 student interview for all respondents. It is a studentlevel file; however, a student can have more than one record in the file. There is a separate record for each degree obtained from each postsecondary institution that the student attended between the time they graduated from high school and the time they graduated with their bachelor's degree (the maximum number of reported institutions for any one respondent was seven). \u2022 B&B:08/09 graduate institution data file. Contains postbaccalaureate institution and degree data obtained from the B&B:08/09 student interview for all respondents. It is a studentlevel file; however, a student can have more than one record in the file. There is a separate record for each degree obtained from each postsecondary institution that the student attended since earning their bachelor's degree. \u2022 B&B:08/09 coding data file. Contains major/field of study, industry, and occupation strings collected in the B&B student interview and the associated codes. \u2022 CPS data files. Contains data received from the CPS for the eligible sample members who matched to the financial aid application files. 2007-08: file contains about 10,490 respondents matched 2008-09: file contains about 4,400 respondents matched 2009-10: file contains about 4,150 respondents matched \u2022 NSLDS loan data file. Contains raw loan-level data received from the NSLDS for the 13,800 respondents who were awarded loans through 2009-10. This is a history file with separate records for each transaction in the loan files; therefore, there can be multiple records per case spanning several academic years. \u2022 Pell data file. Contains raw grant-level data received from the NSLDS for the 9,550 respondents who were awarded Pell grants through 2009-10. This is a history file with separate records for each transaction in the Pell system; therefore, there can be multiple records per case. \u2022 ACG/SMART data file. Contains raw grant-level data received from the NSLDS for the 3,440 respondents who were awarded ACGs or SMART Grants through 2009-10. This is a history file with separate records for each transaction in the database; therefore, there can be multiple records per case. \u2022 Teacher data file. Contains raw grant-level data received from the NSLDS for the 30 respondents who were awarded TEACH Grants through 2009-10. This is a history file with separate records for each transaction in the database; therefore, there can be multiple records per case. \u2022 ACT data file. Contains data received from ACT for the 5,390 respondents who matched to the 2001-02 through 2006-07 ACT files. \u2022 CCD data file. The CCD file contains the most recent Common Core of Data records (from the 2008-09 academic year) for schools whose NCES ID's were reported by B&B:08/09 respondents as schools in which they worked. \u2022 PSS data file. The PSS file contains the most recent Private School Survey records (from the 2007-08 academic year) for schools whose NCES ID's were reported by B&B:08/09 respondents as schools in which they worked. \u2022 NPSAS:08 file. Contains the base-year data included in the NPSAS:08 ECB. \u2022 B&B:08/09 weights file. Contains all of the analysis weights created for B&B:08/09, including transcripts. There is a separate record for each study respondent. 24"}, {"section_title": "Transcript Data Files", "text": "The data files for the B&B:08/09 transcript component contain the data included on each transcript that was entered via the KCS, as well as approximately 315 composite variables derived from that data. Many of the student-level derived variables are available through PowerStats. 25 The following files, which contain records for the 16,070 transcript component respondents, were produced for the restricted CD: \u2022 Transcript analysis file. Contains student-level analytic variables derived from transcript data, and selected direct transcript variables. \u2022 Institution data file. Contains institution-level data obtained from the student transcripts with a record for each sampled NPSAS institution that sent transcripts and also for transfer institutions noted on those transcripts. This is a file of institutions only; it does not contain a student ID or transcript ID. Each record includes institution control, level, location, credit/clock hour uses, calendar system, grading system, and units required to 24 See Chapter 6 for a full description of the B&B:08/09 study weights. 25 A set of restricted research files fully documented through an ECB are available to restricted data licensees from the National Center for Education Statistics (NCES). Tables and regression analyses can be run by any user through NCES's online application PowerStats, which also contains variable documentation. PowerStats is available online via the DataLab site at http://nces.ed.gov/datalab/index.aspx. be designated full-time. This file also contains some institution-level derived variables such as institution selectivity and the percentage of faculty members who are full-time. \u2022 Student schools data file. Contains a record pertaining to a single pairing of student and school. There could be multiple records per student if a student's NPSAS institution transcript listed credits that were granted by another institution and transferred to the NPSAS institution. Each record contains student ID, school IPEDS ID, date student first attended institution, transfer credits attempted/accepted at institution, and transfer credits for grade point average. Records pertaining to the NPSAS institution also contain cumulative transcript totals and high school graduation date. This file also contains some student/school derived variables such as the proportion of terms enrolled full time and the ratio of credits earned to credit required for degree. \u2022 \u2022 Terms data file. Contains a record pertaining to a single pairing of student and term for all institutions. Each record contains the IPEDS ID of the institution, transcript ID, term name, start and end dates, and honors/probation indicators. This file also contains some term-level derived variables such as total earned credits, term grade point average, and enrollment status."}, {"section_title": "Data Editing", "text": "The B&B:08/09 data, including data from the transcript component, were edited using procedures developed and implemented for previous studies sponsored by NCES, including the base-year study, NPSAS:08. Following data collection, the information collected in the student instrument and in transcripts was subjected to various QC checks and examinations. For example, in the student interview these checks were conducted to confirm that the collected data reflected appropriate item routing (skip patterns). Another evaluation involved examination of all variables with missing data and substitution of specific values to indicate the reason for the missing data. For example, in the student interview data, an item may not have been applicable to particular students, a respondent may not have known the answer to the question, or a respondent may have skipped the item entirely (table 37). Skip-pattern relationships in the interview database were examined by methodically crosstabulating gate items and their associated nested items. In many instances, gate-nest relationships spanned multiple levels within the interview: Items nested within a gate question may themselves have been gate items for additional items. Consequently, validating the gate-nest relationships often required several iterations and many multi-way cross-tabulations to ensure the proper data were captured. Gate-nest relationships were also preserved and edited appropriately in the transcript data files; however, fewer of these relationships exist in those data. The data cleaning and editing process for the B&B:08/09 data files involved a multistage process that consisted of the following: 1. Blank or missing data were replaced with -9 for all variables in the interview and transcript databases. A one-way frequency distribution of every variable was reviewed to confirm that no missing or blank values remained. These same one-way frequencies revealed any out-of-range, or outlier, values, which were replaced with a -6 value (e.g., hourly wages of $0.10, rather than $10.00). Creating SAS formats from expected values and the associated value labels also revealed any categorical outliers. Descriptive statistics were produced for all continuous variables. All values that were less than zero were temporarily recoded to missing, and the minimum, median, maximum, and mean values were examined to assess reasonableness of responses; anomalous data patterns were investigated and corrected, as necessary. For transcripts, missing data were also replaced with a -9 (e.g., if high school graduation date did not appear on the transcript) and one-way frequencies were reviewed for any outlier values and also given a -6 value (e.g., credit hours of 100 per course, rather than 3). 2. Legitimate skips were identified through the use of interview source code and flowcharts. Gate-nest relationships were defined to replace -9s (data missing, reason unknown) with -3s (not applicable), as appropriate. Two-way cross-tabulations between each gate-nest combination were evaluated; high numbers of nonreplaced -9 codes were investigated to ensure skip-pattern integrity. Nested values were further checked to reveal instances in which the legitimate skip code overwrote valid data, which typically occurred if a respondent answered a gate question and the appropriate nested items but then reverted to change the value of the gate to one that opened on an alternate path of nested items. Because responses to the first nested items remained in the database, they required editing. For transcripts, gatenest relationships were limited; however, -3 values were set for inapplicable items. For example, if a transcript indicated that the student was still working on the degree, then a -3 value was given to the degree date variable. 3. Variables were formatted (e.g., dates were formatted as YYYYMM), and time units were standardized for items that collected amounts of time in multiple units. In addition, any new codes assigned by expert coders reviewing IPEDS, elementary and secondary school, industry, occupation, and major codes from the interview (including those strings that could not be coded during the interview) were merged back with the interview data files. Similarly, any new codes assigned by the expert coder reviewing the IPEDS, major, minor, concentration and other specify strings from the transcript data were merged back with the transcript data files. Also at this stage, logical recodes were performed when the value of missing items could be determined from answers to previous questions or preloaded values. For example, if a student was not currently repaying education loans, then the monthly payment amount was recoded to $0. For transcripts, missing IPEDS, major, and course codes were reviewed by expert coders. Concurrently with data cleaning, documentation was developed for both interview and transcript data to detail question text, response options, logical recoding, and the \"applies to\" text for each delivered variable (for documentation information, see the student interview facsimile in appendix D)."}, {"section_title": "Data Perturbation", "text": "To protect the confidentiality of NCES data that contain information about specific individuals and to minimize disclosure risks, B&B:08/09 data were subject to perturbation procedures. Perturbation procedures, which have been approved by the NCES Disclosure Review Board, preserve the central tendency estimates but may result in slight increases in nonsampling errors. All respondents were given a positive probability of being selected for swapping. Perturbation was carried out under specific targeted, but undisclosed, swap rates. In data swapping, the values of the variables being swapped are exchanged between carefully selected pairs of records: a target record and a donor record. Swapping variables were selected from all questionnaire items. Because perturbation of the B&B:08/09 data could have changed the relationships between data items, an extensive data quality check was carried out to assess and limit the impact of swapping on these relationships. For example, a set of correlations for a variety of variables was evaluated preand posttreatment to verify that the swapping did not greatly affect the associations."}, {"section_title": "Statistical Imputations", "text": "All variables from the student interview data and the derived variables in PowerStats with missing data were imputed. Imputed data are available in both PowerStats and the restricted derived data file. Derived variables obtained from student transcript data and the variables included in the remaining restricted files were not imputed. The variables were split into six batches to facilitate and expedite imputation, and a consistent imputation methodology was employed for each batch. Some of the B&B respondents had missing NPSAS data, 26 so these NPSAS variables were imputed first. These NPSAS variables were then used as part of the imputation process for the B&B variables. The general imputation methodology consisted of three steps. The first step, if applicable, was logical or deterministic imputation. That is, if the imputed value could be deduced from the logical relationships with other variables, then that information was used to deterministically impute the value for the recipient. The next step was the use of a tree-based methodology, or in a few cases a response propensity model, to create imputation classes. The final step used hot deck 27 imputation to stochastically impute missing values from donors within the identified imputation classes. Variables requiring imputation were imputed sequentially. However, some variables that were related substantively or had similar levels of missing response were grouped together into blocks, and the variables within a block were imputed simultaneously. The order in which variables, or blocks of variables, were imputed was primarily based on the level of missing data. The variables with lower levels of missing data were imputed before the variables with higher levels of missing data. When a variable was selected for imputation based on its level of missing data, three specific pieces of information were evaluated. First, logical consistency was checked to make sure that any known relationships were maintained throughout the imputation process. Second, the pattern of missing data were evaluated to determine whether other variables should be included to create a block of variables requiring imputation. Finally, the imputation class variables and sorting variables were identified. All stochastic imputations used a tree-based methodology to create imputation classes and the weighted sequential hot-deck (WSHD) methodology (Cox 1980;Iannacchione 1982) within imputation classes to replace missing values. The imputation classes were formed using nonparametric classification trees (Breiman et al. 1984). The nonparametric classification trees formed imputation classes from a prediction model based on the observations with valid values for the variable requiring imputation. The nonparametric classification tree recursively split the cases into homogenous groups, which were used to define the imputation classes. The observations with missing values for the variable to be imputed were assigned their imputation class based on the same variables used in the tree splits. The WSHD methodology replaced missing data with valid data from a donor record within an imputation class. The WSHD methodology also incorporated sorting within imputation class for additional control and uses the sample weight of each record in the donor selection process. The imputation classes in the application of the WSHD methodology were formed by identifying variables related to the variable requiring imputation. Data were sorted within each imputation class to increase the chance of obtaining a close match between donor and recipient. Within each imputation class, the hot-deck process searched for donors sequentially, starting with the recipient and progressing up and down the sorted file to find the set of eligible donors from which a random selection of a donor was made. The process was weighted since it incorporated the sample weight of each record in the search and selection routine, using the methodology described in Cox (1980). Imputation diagnostics consisted of four checks: number of times a donor was used, overall imputation checks, imputation checks by class variables, and multivariate consistency checks. The check for the number of times a donor was used was to ensure that donors were used a reasonable number of times. Using a donor too many times might indicate that an imputation class had too few donors, and the class needed to be enlarged. The overall imputation checks compared the distributions, weighted and unweighted, for each level of the imputed variable before and after imputation. Differences of 5 percent or more were flagged and examined to see if changes should be made to the imputation specification. The imputation checks by class variables compared the distributions, weighted and unweighted, for each level of the imputed variable in the defined imputation classes before and after the imputation. Differences of 5 percent or more were flagged for further review. Finally, multivariate consistency checks ensured that relationships between variables were maintained and that any special instructions for the imputation were implemented properly. If any of the four aforementioned diagnostic checks indicated a problem, i.e., a donor was used too many times, substantial deviation from the weighted sums, or any identified inconsistencies, the imputation process was revised and rerun. Some results of the imputation process are provided in Appendix J, which presents the percentage missing for each variable subject to imputation, as well as pre-and postimputation distributions for all of these variables. Appendix M shows that approximately 13 percent of the variables with a response rate less than 85 percent showed statistically significant estimated bias between the pre-and postimputation means and distributions (see section 6.4.2 for more details)."}, {"section_title": "Composite and Derived Variable Construction", "text": "Analysts created the main study analytic variables by examining the data available for each student from the various data sources, prioritizing the data sources on an item-by-item basis, and reconciling discrepancies within and between sources. In some cases, the derived or composite variables were created by simple assignment of a value from the available source with the highest priority. In other cases, interview items were recoded or otherwise summarized to create a derived variable. Similar procedures were used for transcript analytic variables using only data from transcripts and institutions providing transcripts. Details about the creation of each variable appear in the variable descriptions contained in the ECB and PowerStats. For a listing of the set of analysis variables derived for B&B:08/09, see appendix K."}, {"section_title": "Chapter 6. Weighting and Variance Estimation", "text": "This chapter provides information pertaining to the weighting procedures for B&B:08/09. The development of statistical analysis weights for the B&B:08/09 sample is discussed in section 6.1. Analysis procedures that can be used to produce design-unbiased estimates of sampling variances are discussed in section 6.2, including variances computed using Taylor series and bootstrap replication techniques. Section 6.2 also describes how the Taylor series strata and primary sampling unit (PSU) variables were constructed, and how the bootstrap replicate weights were constructed. Section 6.3 gives weighted and unweighted response rates. Section 6.4 discusses the accuracy of B&B:08/09 estimates for precision and the potential for nonresponse bias."}, {"section_title": "Analysis Weights", "text": "The weights for analyzing the B&B:08/09 data were derived from the NPSAS:08 weight, because the B&B:08/09 sample members are a subset of the NPSAS:08 sample. As described in chapter 2, a stratified sample of 500 NPSAS:08 student interview nonrespondents was selected with probabilities proportional to their NPSAS:08 sampling weight. The weight for these cases was adjusted for the subsampling. Three weights were developed for analyzing data from the B&B:08/09 data collection. One weight was developed for analyzing sample members who responded to the B&B:08/09 interview. A second weight was developed for analyzing cases with transcript data. A third weight was developed for analyzing cases with both interview and transcript data. The weights were adjusted for nonresponse and were also raked to IPEDS and NPSAS:08 control totals. This section describes the steps that were followed in order to develop each weight."}, {"section_title": "Analysis Weight for Cases With Student Interview Data", "text": "A B&B:08/09 respondent is someone who has a completed, partial, or abbreviated interview. The B&B:08/09 sample consisted of 18,500 students. At the conclusion of the B&B:08/09 data collection, 15,050 students were initially determined to be eligible respondents, 2,120 were nonrespondents, 1,320 were ineligible, and 10 were deceased. The 2007-08 National Postsecondary Student Aid Study (NPSAS:08) Full-scale Methodology Report (Cominole et al. 2010) (hereinafter referred to as the NPSAS:08 Full-scale Methodology Report) describes the development of the NPSAS study weight. The statistical analysis weight compensated for the unequal probability of selection of institutions and students in the NPSAS:08 sample. The weight also adjusted for multiplicity at the institution and student levels, unknown student eligibility, nonresponse, and poststratification. The institution weight was computed and then used as a component of the student weight. A weight was computed for NPSAS:08 respondents as the product of the following 10 weight components: 1. institution sampling weight (WT1); 2. institution multiplicity adjustment (WT2); 3. institution poststratification adjustment (WT3); 4. institution nonresponse adjustment (WT4); 5. student sampling weight (WT5); 6. student multiplicity adjustment (WT6); 7. student unknown eligibility adjustment (WT7); 8. student not located adjustment (WT8); 9. student other nonresponse adjustment (WT9); and 10. student poststratification adjustment (WT10). The B&B:08/09 sample contains both NPSAS study respondents and nonrespondents. Therefore, the B&B:08/09 base weight was formed as the product of the first seven of these adjustment factors. The subsample of 500 NPSAS:08 student interview nonrespondents was selected with probabilities proportional to the NPSAS:08 student weight. The B&B:08/09 base weight was multiplied by the inverse of this selection probability for the subsampled cases to obtain the weight for cases in the sample. An adjustment was made for interview nonresponse using a model-based constrained logistic weighting procedure. The weights were then calibrated to IPEDS and weight sums from NPSAS:08, which had been calibrated to IPEDS and external control totals as described in the NPSAS:08 Full-scale Methodology Report. 28 The procedure WTADJUST in SUDAAN (RTI, 2008) was used to implement the nonresponse and calibration adjustments. This weighting methodology is described by Folsom and Singh (2000). The adjustment for nonresponse was performed in multiple steps because the predictors of response propensity are potentially different for interview refusals and other nonrespondents. Using multiple steps of nonresponse adjustment can achieve greater reduction in nonresponse bias than a single-step adjustment. The first stage of adjustment for interview nonresponse was an adjustment for refusal. The refusal adjustment model included the 17,160 eligible cases who were not deceased; the response (nonrefusal) indicator was set to 1 for the 16,450 interview respondents and other nonrespondents and to 0 for the 720 cases who were interview refusals. Independent variables were chosen that were considered to be predictive of response status and were nonmissing for interview respondents, refusals, and other nonrespondents. Variables for the model include the frame and survey design variables that were used for the NPSAS:08 weight adjustments and other data known for both the respondents and nonrespondents. Candidate predictor variables include: \u2022 institution control; \u2022 region; \u2022 institution enrollment from IPEDS file (categorical); \u2022 Pell Grant receipt (yes/no); \u2022 Pell Grant amount (categorical); \u2022 Stafford Loan receipt (yes/no); \u2022 Stafford Loan amount (categorical); \u2022 Parent Loan for Undergraduate Students (PLUS) amount (categorical); \u2022 federal aid receipt (yes/no); \u2022 institution aid receipt (yes/no); \u2022 state aid receipt (yes/no); \u2022 any aid receipt (yes/no); \u2022 SSN indicator (yes/no); \u2022 NPSAS response status (three levels); \u2022 number of times answering machine was encountered (three levels); \u2022 in field cluster area (yes/no); \u2022 count of phone numbers we have for a student; \u2022 count of e-mail addresses we have for a student; and \u2022 count of mailing addresses we have for a student. Variables initially included in the nonresponse modeling included all of the candidate predictor variables as well as certain important interaction terms. To detect important interactions for the nonresponse model, a Chi-squared automatic interaction detection (CHAID) analysis was performed on the predictor variables. The CHAID analysis divided the data into segments that differed with respect to the response variable. The segmentation process first divided the sample into groups based on categories of the most significant predictor of response. It then split each of these groups into smaller subgroups based on other predictor variables. It also merged categories of a variable that were found to be nonsignificant. CHAID was run for up to three segments, resulting in the identification of two-way and three-way interactions. Variables that made up the CHAID interaction terms were NPSAS response status, number of times an answering machine was encountered, whether the student was in a field cluster area, counts of phone numbers and e-mail addresses we have for a student, Stafford Loan receipt, and PLUS amount. This initial model did not converge, but as many variables as possible were retained in the model. Table 38 presents the predictor variables used in the model to adjust the weight for refusals and the average weight adjustment factors resulting from these variables. The refusal weight adjustment factors have the following characteristics: \u2022 minimum: 1.00; \u2022 median: 1.02; and \u2022 maximum: 2.17.  The second stage of adjustment for interview nonresponse was an adjustment for other interview nonresponse, given that the student did not refuse. The other nonresponse adjustment model included the 16,450 interview respondents and other nonrespondents; the response (nonrefusal) indicator was set to 1 for the 15,050 interview respondents and to 0 for the 1,400 cases who were other nonrespondents. Candidate predictor variables were the same as those listed above for the first nonresponse adjustment model. As in the refusal adjustment, a CHAID analysis was performed on the predictor variables to detect important interactions. Variables that made up the CHAID interaction terms were NPSAS response status, number of times an answering machine was encountered, whether the student was in a field cluster area, and counts of phone numbers, e-mail addresses, and mailing addresses we have for a student. This initial model also did not converge, but as many variables as possible were retained in the model. Table 39 shows the predictor variables used in the model to adjust the weight for other nonrespondents and the average weight adjustment factors resulting from these variables. The other nonresponse weight adjustment factors have the following characteristics: \u2022 minimum: 1.00; \u2022 median: 1.03; and \u2022 maximum: 4.49.  To ensure population coverage and consistency with NPSAS:08, the B&B:08/09 interview weights were further adjusted to control totals. Variables used to define the control totals were similar to those used for the poststratification adjustments for NPSAS:08. The control totals for the B&B:08/09 weights were obtained using the weighted sums from NPSAS:08 (using the NPSAS:08 study weights) for these variables for the full B&B cohort (including ineligible and deceased students). The following variables were used in defining control totals from NPSAS:08 weight sums: \u2022 number of Stafford Loan recipients by institution control; 29 \u2022 total amount of Pell Grants awarded; 30 and \u2022 amount of PLUS grants awarded by institution control. Additionally, control totals were formed from IPEDS counts of bachelor's degree recipients for institution control, gender, and major. The following variables were used in defining control totals from IPEDs: \u2022 fall 2007 recipients of baccalaureate degree by gender; \u2022 fall 2007 recipients of baccalaureate degree by institution control; and \u2022 fall 2007 recipients of baccalaureate degree by major (12 categories). The control totals from NPSAS include cases who became ineligible or were deceased. Because of this, the ineligible and deceased cases were included in the calibration adjustment to the NPSAS totals but not the IPEDS totals. After the adjustment, the ineligible and deceased cases were dropped from the file; the sum of the final weights estimates the number of the NPSAS:08 population who were eligible for B&B and were still alive at the time of the B&B:08/09 interview. As part of the calibration process, students with extreme (outlier) weights had different bounds on their adjustment factors to accomplish weight trimming and smoothing in the same step as calibration. Extreme weights were identified as weights greater than the median weight + 3 times the interquartile range or less than the median weight -3 times the interquartile range. Weight values outside of these bounds were trimmed to the bounds. Table 40 shows the variables used for the calibration, the values of the control totals, and the average weight adjustment factors for each variable. The last column of table 3 shows the sum of the weights after removing the cases who were ineligible or deceased at the time of the B&B:08/09 data collection. Statistics for the weight adjustment factors are the following \u2022 minimum: 0.05; \u2022 median: 1.62; and \u2022 maximum: 9.32. The response adjusted, calibrated interview weight is the variable WTA000 on the data file. 29 NPSAS:08 weights were controlled to total Stafford Loan amounts disbursed in addition to the number of Stafford Loan recipients, but the B&B calibration model would not converge with both of these included. 30 The calibration model would not converge with amount of Pell Grants awarded by institution control, so total amount of Pell Grants awarded was used instead. Table 41 summarizes the weight distributions and the variance inflation due to unequal weighting by institution control. The median student interview weight ranges from 25 for students whose base-year institution was private for-profit to 82 for students whose base-year institution was public. The mean student interview weight ranges from 96 for students whose base-year institution was private nonprofit to 120 for students whose base-year institution was public. The unequal weighting effect overall is 2.41, and ranges from 2.33 for students whose base-year institution was public to 3.12 for students whose base-year institution was private for-profit. To assess the overall predictive ability of the nonresponse model, a Receiver Operating Characteristic (ROC) curve was used (Hanley and McNeil 1982). The ROC provides a measure of how well the model correctly classified individuals of known response type. For a more detailed example of the use of the ROC curve in nonresponse modeling, see Iannacchione (2003). The ROC curve was developed by calculating, for any specified probability, c, two proportions: \u2022 the proportion of respondents with a predicted probability of response greater than c; and \u2022 the proportion of nonrespondents with a predicted probability of response greater than c. The predicted probability of response for each student was the predicted response probability from the weight adjustment model. The plot of the first probability against the second, for c ranging from 0 to 1, resulted in the ROC curve shown in figure 1. The area under the curve measures the probability that a randomly chosen pair of observations-one respondent and one nonrespondent-will be correctly ranked. The probability of a correct pairwise ranking is the same quantity that is estimated by the nonparametric Wilcoxon statistic. The null hypothesis associated with the Wilcoxon statistic is that the variable is not a useful discriminator between the respondent and nonrespondent populations. This corresponds to the null hypothesis that the predicted response probability of a respondent is just as likely to be smaller than the predicted response probability of a nonrespondent as it is to be greater. Thus, if the null hypothesis is true, the ROC curve will be a diagonal line that reflects the equally likely chance of making a correct or incorrect decision, and the area under the curve will be 0.5. If the null hypothesis is not true, the ROC curve will rise above the diagonal and the area under the curve will be significantly greater than 0.5. Figure 19 shows that the area under the ROC curve is 0.82 such that 82 percent of the time (or more than 8 of 10 pairings), the predicted probabilities give the correct classification. The ROC area of 0.82 equals the value of the Wilcoxon test statistic; based on this result we reject the null hypothesis of no predictive ability (p < 0.05). This level of discrimination implies that the variables used in the model are highly informative, but not definite predictors of a sample student's overall response propensity. The predicted probabilities of response (c) were obtained as the product of the predicted response probabilities obtained at both of the nonresponse adjustment steps. Note that for the second step (other nonresponse adjustment), predicted probabilities were not directly available for students who had already been dropped from the model because, in the previous step, they refused. For these students, their predicted probability was set equal to the mean of the predicted probabilities of students still in the model. "}, {"section_title": "Analysis Weight for Cases With Transcript Data", "text": "A weight was also constructed for analyzing the cases with transcript data. Of the 17,170 students who were eligible for B&B:08/09, 10 were deceased, 16,070 had a transcript from the NPSAS school, and the remaining 1,090 were considered nonrespondents for this weight. As with the weight described in section 6.1.1, the base weight was formed as the product of the first seven of the NPSAS:08 weight adjustment factors. An adjustment was made for nonresponse using a model-based constrained logistic weighting procedure, then the weights were calibrated to the sums of the B&B:08/09 interview weights for eligible cases. The procedure WTADJUST in SUDAAN was used to implement the nonresponse and calibration adjustments. The first adjustment was for nonresponse, that is, not having transcript data. 31 The adjustment model included the 17,160 eligible cases who were not deceased, with the response indicator set to 1 for the 16,070 cases with transcript data and set to 0 for the 1,090 cases who were nondeceased transcript nonrespondents. Predictor variables were chosen if considered to be predictive of response status and were nonmissing for both transcript respondents and nonrespondents. Variables used in the nonresponse adjustment models for NPSAS were also included. Candidate predictor variables included a subset of the variables that were used for the interview weight (see section 6.1.1): \u2022 institution control; \u2022 region; \u2022 institution enrollment from IPEDS file (categorical); \u2022 Pell Grant receipt (yes/no); \u2022 Pell Grant amount (categorical); \u2022 Stafford Loan receipt (yes/no); \u2022 Stafford Loan amount (categorical); \u2022 PLUS amount (categorical); \u2022 federal aid receipt (yes/no); \u2022 institution aid receipt (yes/no); \u2022 state aid receipt (yes/no); and \u2022 any aid receipt (yes/no). Variables initially included in the nonresponse modeling included all of the candidate predictor variables as well as certain important interaction terms identified using CHAID. CHAID was run for up to three segments, resulting in the identification of two-way and three-way interactions. Variables that made up the CHAID interaction terms for the student transcript weight adjustment included all of the above variables except for federal aid receipt and Pell Grant amount. This initial model did not converge, but as many variables as possible were retained in the model. Table 42 shows the predictor variables used in the model to adjust the weight and the average weight adjustment factors resulting from these variables. The nonresponse weight adjustment factors have the following characteristics: \u2022 minimum: 1.00; \u2022 median: 1.06; and \u2022 maximum: 2.27. 31 Only one nonresponse adjustment was done for the transcript weight as opposed to two for the interview weight.  To ensure population coverage and consistency with the B&B:08/09 interview weight, the NPSAS:08 weight, and IPEDS, the B&B:08/09 transcript weight was adjusted to control totals determined by the B&B:08/09 interview weight sums. Cases which were deceased were not included in either the control totals or in the cases included in the adjustment. This adjustment was also implemented using the SUDAAN WTADJUST procedure. Variables used to define the control totals were the same as those used for the poststratification coverage adjustments for the B&B:08/09 interview weight, which are listed in section 6.1.1. The control totals for the B&B:08/09 transcript weights were established by the weighted sums from the B&B:08/09 interview weights. As part of the calibration process, students with extreme (outlier) weights had different bounds on their adjustment factors to accomplish weight trimming and smoothing in the same step as calibration. Extreme weights were identified as weights greater than the median weight + 3 times the interquartile range or less than the median weight -3 times the interquartile range. Weight values outside of these bounds were trimmed to the bounds. Table 43 gives the variables used for the calibration, the values of the control totals, and the average weight adjustment factors for each variable. Statistics for the weight adjustment factors are the following: \u2022 Minimum: 0.08; \u2022 Median: 1.52; and \u2022 Maximum: 9.45. The response adjusted, calibrated transcript weight is the variable WTB000 on the data file. Table 44 summarizes the weight distributions and the variance inflation due to unequal weighting by institution control. The median transcript weight ranges from 22 for students whose base-year institution was private for-profit to 77 for students whose base-year institution was public. The mean transcript weight ranges from 89 for students whose base-year institution was private forprofit to 112 for students whose base-year institution was public. The unequal weighting effect overall is 2.36, and ranges from 2.28 for students whose base-year institution was public to 3.17 for students whose base-year institution was private for-profit. To assess the overall predictive ability of the nonresponse model, an ROC curve was again used to provide a measure of how well the model correctly classified individuals of known response type. The plot of the first probability against the second (that is, the proportion of respondents with a predicted probability of response greater than c versus the proportion of nonrespondents with a predicted probability of response greater than c) for c ranging from 0 to 1, resulted in the ROC curve shown in figure 20. The area under the ROC curve is 0.67, such that 67 percent of the time (or almost 7 of 10 pairings), the predicted probabilities give the correct classification. The ROC area of 0.67 equals the value of the Wilcoxon test statistic; based on this result we reject the null hypothesis of no predictive ability (p < 0.05). This level of discrimination implies that the variables used in the model are highly informative but not definite predictors of a sample student's transcript response propensity. "}, {"section_title": "Analysis Weight for Cases With Both Student Interview and Transcript Data", "text": "A weight was also constructed for analyzing the cases with both student interview and transcript data. Of the 17,070 students who were eligible for both the interview and transcripts, 32 10 were deceased, 14,010 had both a student interview and a transcript from the NPSAS school, and the remaining 3,040 were considered nonrespondents for this weight. As with the weights described in sections 6.1.1 and 6.1.2, the base weight was formed as the product of the first seven of the NPSAS:08 weight adjustment factors. An adjustment was made for nonresponse using a model-based constrained logistic weighting procedure, then the weights were calibrated to the sums of the B&B:08/09 interview weights for eligible cases. The procedure WTADJUST in SUDAAN was used to implement the nonresponse and calibration adjustments. The first adjustment was for nonresponse, that is, not having interview or transcript data. 33 The adjustment model included the 17,060 eligible cases who were not deceased, with the response indicator set to 1 for the 14,010 cases with transcript data and set to 0 for the 3,040 cases who were nondeceased interview and transcript nonrespondents. Predictor variables were chosen if considered to be predictive of response status and were nonmissing for both interview and transcript respondents and nonrespondents. Variables used in the nonresponse adjustment models for NPSAS were also included. Candidate predictor variables included the same set of variables that was used for the interview weight (see section 6.1.1). Variables initially included in the nonresponse modeling included all of the candidate predictor variables as well as certain important interaction terms identified using CHAID. CHAID was run for up to three segments, resulting in the identification of two-way and three-way interactions. Variables that made up the CHAID interaction terms for the combined student interview and transcript weight adjustment were NPSAS response status, number of times an answering machine was encountered, whether the student was in a field cluster area, and counts of phone numbers and e-mail addresses we have for a student. This initial model did not converge, but as many variables as possible were retained in the model. Table 45 shows the predictor variables used in the model to adjust the weight and the average weight adjustment factors resulting from these variables. The nonresponse weight adjustment factors have the following characteristics: \u2022 minimum: 1.01; \u2022 median: 1.14; and \u2022 maximum: 5.18.  To ensure population coverage and consistency with the B&B:08/09 interview weight, the NPSAS:08 weight and IPEDS, the B&B:08/09 combined interview and transcript weight was adjusted to control totals determined by the B&B:08/09 interview weight sums. Cases which were deceased were not included in either the control totals or in the cases included in the adjustment. This adjustment was also implemented using the SUDAAN WTADJUST procedure. Variables used to define the control totals were the same as those used for the poststratification coverage adjustments for the B&B:08/09 interview weight, which are listed in section 6.1.1. The control totals for the B&B:08/09 transcript weights were established by the weighted sums from the B&B:08/09 interview weights. As part of the calibration process, students with extreme (outlier) weights had different bounds on their adjustment factors to accomplish weight trimming and smoothing in the same step as calibration. Extreme weights were identified as weights greater than the median weight + 3 times the interquartile range or less than the median weight -3 times the interquartile range. Weight values outside of these bounds were trimmed to the bounds. Table 46 gives the variables used for the calibration, the values of the control totals, and the average weight adjustment factors for each variable. Statistics for the weight adjustment factors are the following: \u2022 Minimum: 0.05; \u2022 Median: 1.55; and \u2022 Maximum: 7.67. The response adjusted, calibrated combined interview and transcript weight is the variable WTC000 on the data file. Table 47 summarizes the weight distributions and the variance inflation due to unequal weighting by institution control. The median combined interview and transcript weight ranges from 24 for students whose base-year institution was private for-profit to 86 for students whose base-year institution was public. The mean combined interview and transcript weight ranges from 105 for students whose base-year institution was private nonprofit or private for-profit to 128 for students whose base-year institution was public. The unequal weighting effect overall is 2.43, and ranges from 2.34 for students whose base-year institution was public to 3.14 for students whose base-year institution was private for-profit. To assess the overall predictive ability of the nonresponse model, an ROC curve was again used to provide a measure of how well the model correctly classified individuals of known response type. The plot of the first probability against the second (that is, the proportion of respondents with a predicted probability of response greater than c versus the proportion of nonrespondents with a predicted probability of response greater than c) for c ranging from 0 to 1, resulted in the ROC curve shown in figure 21. The area under the ROC curve is 0.65, such that 65 percent of the time (or more than 6 of 10 pairings), the predicted probabilities give the correct classification. The ROC area of 0.65 equals the value of the Wilcoxon test statistic; based on this result we reject the null hypothesis of no predictive ability (p < 0.05). This level of discrimination implies that the variables used in the model are highly informative but not definite predictors of a sample student's response propensity. "}, {"section_title": "Variance Estimation", "text": "For probability-based sample surveys, most estimates are nonlinear statistics. For example, a mean or proportion, which is expressed as is nonlinear because the denominator is a survey estimate of the (unknown) population total. In this situation, the variances of the estimates cannot be expressed in closed form. Two procedures for estimating variances of survey statistics are the Taylor series linearization procedure and the bootstrap replication procedure. Variables to use for both of these variance estimation procedures are available on the B&B:08/09 data files. The analysis strata and replicates created for the Taylor series procedure are discussed in section 6.2.1, and section 6.2.2 discusses the replicate weights created for the bootstrap procedure."}, {"section_title": "Taylor Series", "text": "The Taylor series variance estimation procedure is a well-known technique used to estimate the variances of nonlinear statistics. The procedure takes the first-order Taylor series approximation of the nonlinear statistic and then substitutes the linear representation into the appropriate variance formula based on the sample design. Woodruff (1971) presented the mathematical formulation of this procedure. For stratified multistage surveys, the Taylor series procedure requires variance estimation strata and variance estimation PSUs, also called replicates, defined from the sampling strata and PSUs used in the first stage of sampling. Because B&B:08/09 is a follow-up study of NPSAS:08, the variance estimation strata and PSUs for B&B:08/09 were derived from the variance estimation strata and PSUs that were developed for NPSAS:08. The steps in the construction of the NPSAS:08 stratum and PSU variables are described in chapter 6 of the NPSAS:08 Full-scale Methodology Report (Cominole et al. 2010). The variance estimation formulas require at least two PSUs in each stratum. The NPSAS:08 variance estimation strata and PSUs were examined for the B&B:08/09 sample, and strata with only one PSU were combined with other strata to obtain at least two PSUs. The following three rules were used: variance estimation strata were combined with other variance estimation strata within the original NPSAS:08 sampling strata, certainty schools were combined with other certainty schools, and noncertainty schools were combined with other noncertainty schools. In addition, the original sort order that was used for constructing the NPSAS:08 variance estimation strata and PSUs was used. If the stratum was the first in the sorted list, then it was combined with the next stratum in the list. The single PSU then became an additional PSU in the new variance estimation stratum. The resulting variance estimation strata and PSUs for B&B:08/09 are the variables ANALSTR and ANALPSU. Note that these strata and PSUs were formed such that they are applicable to use with any of the three analysis weights described in section 6.1. The procedure described above may overestimate the variance because it does not always account for the finite population correction (FPC) at the institution stage of sampling. The Taylor series procedure can account for the FPC if the secondary sampling units (SSUs) and PSU counts are considered in addition to the analysis strata and analysis PSUs. An alternate variance estimation method using replicate weights to account for the FPC is also provided for users of the B&B:08/09 data, as described below."}, {"section_title": "Bootstrap Replicate Weights", "text": "The variance estimation strategy that was chosen for B&B:08/09 is the same as that used for NPSAS:08 and satisfies the following requirements: \u2022 recognition of variance reduction due to stratification at all stages of sampling; \u2022 recognition of effects of unequal weighting; \u2022 recognition of possible increased variance due to sample clustering; \u2022 recognition of effects of weight adjustments for nonresponse and for calibration of selected total estimates to known external totals or weight sums; \u2022 satisfactory properties for estimating variances of nonlinear statistics and quantiles (such as the median) as well as for linear statistics; \u2022 ability to apply finite population corrections at the institution stage of sampling and reflect the reduction in variance due to the high sampling rates in some first-stage sampling strata; and b \u03b8 is the estimate based on the b-th replicate weight and B is the total number of sets of replicate weights. Once the replicate weights are provided, this estimate can be produced by most survey software packages (e.g., SUDAAN [RTI International 2008] computes this estimate by invoking the DESIGN=BRR option). The number of replicate weights was set at 200 for NPSAS:08 based on work that showed that this number of replicates has desirable properties for variance estimation in regression analyses. For the 200 replicate weights included on the weights file, both the nonresponse adjustment and calibration process were repeated so that the variance of survey estimates would include the variability due to the weight adjustments. For some of the replicates, not all of the control totals could be met because of model convergence problems, i.e. there was no solution to satisfy all model equations simultaneously. The analysis and replicate weights that are available on the weights file for B&B:08/09 are the following:"}, {"section_title": "Type of respondents", "text": "Analysis weight Replicate weights Interview respondents WTA000 WTA001-WTA200 Transcript respondents WTB000 WTB001-WTB200 Interview and transcript respondents WTC000 WTC001-WTC200"}, {"section_title": "Overall Weighted and Unweighted Response Rates", "text": "The overall B&B:08/09 response rate is an estimate of the proportion of the study population directly represented by the respondents. Because the B&B:08/09 study includes a subsample of NPSAS:08 nonrespondents, the overall B&B:08/09 response rate is the product of the NPSAS:08 institution-level response rate times the B&B:08/09 student-level response rate. Therefore, the overall B&B:08/09 response rates can only be estimated directly for defined institution characteristics. Table 48 gives the unweighted and weighted NPSAS:08 base-year institution and B&B:08/09 student response rate components by institution control. Only the weighted response rates can be interpreted as estimates of the proportion of the B&B:08/09 population that is directly represented by the respondents. The types of student respondents included in table 48 are the following: \u2022 B&B:08/09 interview respondents; \u2022 B&B:08/09 transcript respondents (i.e., cases with any transcript data); and \u2022 B&B:08/09 interview and transcript respondents (i.e., cases with both interview and transcript data). The institution-level response rates shown in table 42 are the percentage of institutions that provided sufficient data to select the NPSAS:08 student-level sample; these rates are presented and discussed in the NPSAS:08 Full-scale Methodology Report (Cominole et al. 2010, table 9, p.50). Table 48 shows that approximately 78 percent of the eligible sample responded to the B&B:08/09 interview. The rate varied from 70 percent to 79 percent, by type of institution. The overall weighted response rate, incorporating the NPSAS:08 base-year institution response rate, was 71 percent. The interview analysis weight described in section 6.1.1 (WTA000) was developed to compensate for the potentially biasing effects of interview nonresponse. Table 48 also provides weighted response rates for the transcript data collection component. Overall, a transcript was collected from 92 percent of the eligible students. This varied, by type of institution, from 90 percent to 96 percent. An analysis weight (the weight variable WTB000) was developed for analyzing students with transcript data. Overall, 73 percent of the sample were respondents to both the interview and the transcript data collection. This rate varied, by type of institution, from 68 percent to 74 percent. The weight variable WTC000 was developed for analyzing students with both interview and transcript data. Section 6.4.2 analyzes the potential bias due to unit nonresponse and the effect the weight adjustments had in reducing the bias."}, {"section_title": "Accuracy of Estimates", "text": "The accuracy of survey statistics is affected by both random and nonrandom errors. Random errors reduce the precision of survey estimates, while nonrandom errors result in bias (i.e., estimates that do not converge to the true population parameter as the sample size increases without limit). The sources of error in a survey are often dichotomized as sampling and nonsampling errors. Sampling error refers to the error that occurs because the survey is based on a sample of population members rather than the entire population. All other types of errors are nonsampling errors, including survey nonresponse (because of inability to contact sampling members, their refusal to participate in the study, etc.) and measurement errors, such as the errors that occur because the intent of survey questions was not clear to the respondent, because the respondent had insufficient knowledge to answer correctly, or because the data were not captured correctly (e.g., because of recording, editing, or data entry errors). The sampling errors are primarily random errors for well-designed surveys such as NPSAS:08 and B&B:08/09. However, nonrandom errors can occur if the sampling frame does not provide complete coverage of the target population. The B&B:08/09 survey instrument and data collection procedures were subjected to thorough development and testing to minimize nonsampling errors, because these errors are difficult to quantify and are likely to be nonrandom errors. In this section sampling errors and design effects for some B&B:08/09 estimates are presented for a variety of domains; these sampling errors and design effects are computed using the analysis weights that were constructed for analyzing the B&B:08/09 student and transcript data. Next, the results of analyses comparing B&B:08/09 nonrespondents and respondents using characteristics known for both nonrespondents and respondents are presented. An analysis of nonresponse bias is presented at both the student level and the item level."}, {"section_title": "Measures of Precision: Standard Errors and Design Effects", "text": "The survey design effect for a statistic is defined as the ratio of the design-based variance estimate divided by the variance estimate that would have been obtained from a simple random sample of the same size. The design effect is often used to measure the effects that sample design features have on the precision of survey estimates. For example, stratification tends to decrease the variance, but multistage sampling and unequal sampling rates usually increase the variance. Weight adjustments for nonresponse (performed to reduce nonresponse bias) and calibration often increase the variance because they can increase the weight variation. Because of these factors, estimates from most complex multistage sampling designs such as B&B:08/09 have design effects greater than 1.0. That is, the design-based variance is larger than the simple random sample variance. Specifically, the survey design effect for a given estimate, \u03b8 , is defined as The square root of the design effect can also be expressed as the ratio of the standard errors, or In appendix L, design effect estimates are presented for important survey domains to summarize the effects of stratification, multistage sampling, unequal probabilities of selection, and the weight adjustments. These design effects were estimated for interview and transcript data using SUDAAN and the bootstrap variance estimation procedure described in section 6.2.2. If an analysis of B&B:08/09 data must be performed without using one of the software packages for analysis of complex survey data, the design effect tables in appendix L can be used to make approximate adjustments to the standard errors of survey statistics computed using the standard software packages that assume simple random sampling designs. However, one cannot be confident about the actual design-based standard errors without performing the analysis with one of the software packages specifically designed for analysis of data from complex sample surveys. Large design effects imply large standard errors and relatively poor precision. Small design effects imply small standard errors and good precision. In general terms, a design effect under 2.0 is low, 2.0 to 3.0 is moderate, and above 3.0 is high. Moderate and high design effects often occur in complex surveys such as B&B:08/09, and the design effects in appendix L are consistent with those in past B&B studies. Unequal weighting causes large design effects and is often as a result of nonresponse and poststratification adjustments. However, in B&B:08/09 (as in NPSAS:08), the unequal weighting is also due to the sample design, different sampling rates between institution strata, different sampling rates between student strata, and subsampling of the nonrespondents that were included in B&B:08/09."}, {"section_title": "Measure of Bias", "text": "The bias in an estimated mean based on respondents, R y , is the difference between this mean and the target parameter, \u03c0, that is, the mean that would be estimated if a complete census of the target population was conducted and everyone responded. This bias can be expressed as follows, is the expected value of the mean based on respondents over repeated samples: The estimated mean based on nonrespondents, NR y , can be computed if data for the particular variable are available for most of the nonrespondents. The true target parameter, \u03c0, can be estimated for these variables as follows: where \u03b7 is the weighted unit (or item) nonresponse rate. For the variables that are from the frame, rather than from the sample, \u03c0 can be estimated without sampling error. The bias can then be estimated as follows: This formula shows that the estimate of the nonresponse bias is the difference between the mean for respondents and nonrespondents multiplied by the weighted nonresponse rate. Nonresponse bias analysis was conducted when the response rate at any level (institutions, students, items) was below 85 percent. 34 Institution nonresponse bias was performed as a part of NPSAS:08 and is described in the NPSAS:08 Full-scale Methodology Report (Cominole et al. 2010). A student nonresponse bias analysis was performed for the interview and the combined interview and transcript, and an item nonresponse bias analysis was also performed for both the interview and transcript data. The remainder of this section summarizes the unit and item nonresponse bias analyses that were conducted for B&B:08/09. Unit nonresponse bias analysis. Unit nonresponse bias analyses were conducted for the following sets of respondents: \u2022 B&B:08/09 interview respondents versus the full set of cases eligible for B&B:08/09 (interview respondents and interview nonrespondents), before and after the weight adjustment that resulted in the B&B:08/09 interview weight (WTA000); \u2022 B&B:08/09 interview respondents versus B&B:08/09 interview nonrespondents, before the weight adjustment that resulted in the B&B:08/09 interview weight (WTA000); \u2022 Interview and transcript respondents versus the full set of cases eligible for B&B:08/09 (interview and transcript respondents and interview and transcript nonrespondents), before and after the combined interview and transcript weight adjustment that resulted in the B&B:08/09 student transcript weight WTC000; and \u2022 Interview and transcript respondents versus interview and transcript nonrespondents, before the combined interview and transcript weight adjustment that resulted in the B&B:08/09 student transcript weight WTC000. The NCES Statistical Standards (NCES 2003) requires a bias analysis for any stage of a sample with a response rate less than 85 percent. From table 48, the weighted B&B:08/09 transcript response rate was greater than 85 percent overall and by control. Therefore, a unit-level nonresponse bias analysis was not necessary for transcripts. Tables in appendix M give the bias estimates as a result of the analyses listed above. The nonresponse bias was estimated for variables obtained from the sampling frame and from the NPSAS:08 data collection that are known for both respondents and nonrespondents. In all of the tables, the bias was estimated as follows. First, the percentage distribution was obtained for the respondents using the weight before and after weight adjustments. Next, the percentage distribution was obtained for the overall sample using the B&B:08/09 base weight (described above). Then, the bias was estimated as the difference in the percentages. Statistical tests of the bias were also computed using Taylor series estimates of the standard errors, and the tables in appendix M indicate when the bias is statistically different from zero. It is also informative to compare the distributions of the respondents and nonrespondents before weight adjustments, and the tables in appendix M include columns that give the weighted distributions of respondents and nonrespondents. From the above formulas, the bias prior to the weight adjustment can also be obtained as the nonresponse rate multiplied by the difference between respondents and nonrespondents. When the bias before the weight adjustment is statistically significant, the differences between the respondent and nonrespondent distributions are almost always statistically significant. Similarly, when the differences between the respondent and nonrespondent distributions are statistically significant, the bias is also statistically significant. When one is statistically significant but not the other, the p-values are very close to 0.05. The p-values are not identical because of the sampling error associated with the nonresponse rate. The results of the statistical tests are provided in appendix M for comparing the respondent and nonrespondent percentages. The variables that were used in the analyses for all sample members are the following: \u2022 institution control; \u2022 region; \u2022 institution enrollment from IPEDS file (categorical); \u2022 Pell Grant receipt (yes/no); \u2022 Stafford Loan receipt (yes/no); \u2022 federal aid receipt (yes/no); \u2022 institution aid receipt (yes/no); \u2022 state aid receipt (yes/no); and \u2022 any aid receipt (yes/no). The variables that were used in the analyses for sample members who were federally aided during NPSAS:08 are the following: \u2022 Pell Grant amount (categorical); \u2022 Stafford Loan amount (categorical); and \u2022 PLUS amount (categorical). The nonresponse bias was estimated for the above variables and tested to determine if the bias was significant at the 5 percent level. The tests are reported to be statistically significant if the p value is less than .05. Relative bias was also estimated and computed as the bias divided by the estimate of the full sample. Results are given in appendix M for all institutions combined and by institution control. Table 49 summarizes the results of the bias analysis for interview respondents before and after weight adjustments overall and by institution control. From table 48, the weighted B&B:08/09 interview response rate was less than 85 percent overall and for each of the three institution controls. This summary shows the estimated relative bias prior to the weight adjustment using the B&B:08/09 base weight or, equivalently, compared the B&B:08/09 interview respondents and interview nonrespondents. The summary also shows the estimated relative bias after the weight adjustments using the B&B:08/09 interview weight WTA000 or, equivalently, compared the B&B:08/09 interview respondents and the full sample. Tables in appendix M provide the detailed bias estimates for the interview bias analyses. As shown in table 49, some significant bias remains after the student interview weight adjustments. Significant bias was reduced after the nonresponse weighting adjustments for the variables known for respondents and nonrespondents. However, the calibration adjustment to IPEDS and NPSAS:08 totals caused some significant bias to reappear. The calibration was necessary to match the baccalaureate counts in B&B:08/09 to known IPEDS counts and NPSAS:08 weighted estimates of federal aid receipt and to get the B&B:08/09 weights and estimates more in line with the NPSAS:08 weights and estimates for the B&B:08/09 students. Adjust the weights to compensate for B&B:08/09 students who did not respond to the interview"}, {"section_title": "Interview poststratification adjustment", "text": "Adjust the student weights to match NPSAS:08 weight sums and known population totals from IPEDS to ensure population coverage. Includes trimming and smoothing of the weights to reduce unequal weighting."}, {"section_title": "Student transcript analysis weight", "text": "Transcript nonresponse adjustment Adjust the weights to compensate for B&B:08/09 students for whom a transcript was not collected"}, {"section_title": "Transcript poststratification adjustment", "text": "Adjust the student weights to match NPSAS:08 weight sums and known population totals from IPEDS to ensure population coverage. Includes trimming and smoothing of the weights to reduce unequal weighting. Combined student interview and transcript analysis weight\nAdjust the student weights to match NPSAS:08 weight sums and known population totals from IPEDS to ensure population coverage. Includes trimming and smoothing of the weights to reduce unequal weighting.  Table 50 summarizes the results of the bias analysis for students who were both interview and transcript respondents before and after weight adjustments overall and by institution control. From table 48, the weighted B&B:08/09 interview response rate was less than 85 percent overall and for each of the three institution controls. This summary shows the estimated relative bias prior to the weight adjustment using the B&B:08/09 base weight or, equivalently, compared the B&B:08/09 interview and transcript respondents and interview and transcript nonrespondents. The summary also shows the estimated relative bias after the weight adjustments using the B&B:08/09 combined interview and transcript weight WTC000 or, equivalently, compared the B&B:08/09 interview and transcript respondents and the full sample. Tables in appendix M provide the detailed bias estimates for the combined interview and transcript bias analyses. Table 50 shows some reduction of significant bias but significant bias still remains. Similar to the student interview bias analysis, the calibration causes significant bias to reappear after the nonresponse adjustments. Item nonresponse bias analysis. When item response rates were less than 85 percent, the NCES Statistical Standards required that a nonresponse bias analysis be conducted. This analysis was conducted on the data items collected in the B&B:08/09 interview based on interview respondents and for variables derived from student transcript data collection. As shown in the equation below, item response rates (RRI) are calculated as the ratio of the number of respondents for whom an in-scope response was obtained (I x for item x) to the number of respondents who are asked to answer that item. The number asked to answer an item is the number of unit-level respondents (I) minus the number of respondents with a valid skip for item x (V x ). When an abbreviated questionnaire is used to convert refusals, the eliminated questions are treated as item nonresponse (NCES 2003): Item response rates were computed using nonimputed data. Valid skips were later logically imputed to the follow-up items after the gate question was imputed (but these imputed skips count as missing for computing the response rate). Table J-1 in appendix J lists the items from the B&B:08/09 interview along with the number of cases who were eligible to answer each item, and the weighted item response rates and nonresponse rates. The B&B:08/09 interview weight (WTA000) was used to calculate the response rates. The nonresponse rate was also the same as the percentage of cases for which the item was imputed. As mentioned earlier, cases who did not respond to a gate item were treated as missing for the items within the gate. Of the 368 items listed in table J-1, 149 had an item response rate less than 85 percent. Table J-4 lists the derived variables from the transcript data along with the number of eligible cases and the weighted item response rates and nonresponse rates. The B&B:08/09 student transcript analysis weight (WTB000) was used to calculate the response rates. Of the 202 variables, all but nine had a response rate greater than 85 percent. A nonresponse bias analysis was conducted for items with a weighted response rate less than 85 percent for all B&B:08/09 interview respondents, and for derived transcript variables with weighted response rates less than 85 percent. The possibility of estimating the degree of bias depends on having some variables that reflect key characteristics of respondents and for which there is little or no missing data. The variables that were used (from the bulleted list above) are known for all B&B:08/09 interview respondents. These variables are important to the study and are related to many of the items being analyzed for low item response rates. For the items with a weighted response rate less than 85 percent, the nonresponse bias prior to imputation was estimated for each of these characteristics that are known for respondents. Table M-9 in appendix M illustrates the estimated bias (prior to item imputation) for one item (B1ADMSUP -Teacher satisfaction: Administrative support) for B&B:08/09 interview respondents. Similar computations were performed and tabulations were produced for each of the items. Table M-10 summarizes the results of the item nonresponse bias analysis for each of the items from the student interview, and gives the mean and median relative bias and the percentage of the variable categories with statistically significant bias. Across the items, the percentage of variables with statistically significant bias ranged from 3 percent to 98 percent. Table M-11 gives the same analysis for the derived transcript items that have a weighted item response rate less than 85 percent. Item imputation was used to fill in missing data for B&B:08/09 interview respondents and nonrespondents, as described in chapter 5. Item imputation was expected to reduce the bias due to item nonresponse, and was used instead of a separate weight adjustment for nonresponse for each item. All of the questionnaire items that are listed in table J-1 were imputed using the imputation process described in chapter 5. A by-product of imputation was the reduction or elimination of item-level nonresponse bias. While item-level bias before imputation was measurable, after imputation it was not. As a result, how well an imputation procedure worked in reducing bias could not be directly evaluated. Instead, the before-and after-imputation item estimates were compared to determine whether the imputation significantly changed the biased estimates, thus suggesting a reduction in bias. Weighted estimates were computed using the nonimputed data (including only those cases who responded to the item) and also using the imputed data (including cases who responded to the item and also cases with imputed data for the item). Table J-2 gives the means before and after imputation for the continuous variables, and table J-3 gives the distributions before and after imputation for the categorical variables. These tables also give the difference between the preimputation and postimputation estimates. The difference between the pre-and postimputation estimates was statistically significant for 13 percent of the variables and variable categories (see table M-10). This suggests that imputation was only slightly successful in reducing the bias due to item nonresponse. Imputation was not performed for the items obtained from student transcript data. A weight, adjusted for students without any transcript data, was computed. Most of the variables that were derived from the transcript data have high item response rates (table J-4)."}, {"section_title": "Transcript nonresponse adjustment", "text": "Adjust the weights to compensate for B&B:08/09 students who did not respond to the interview and for whom a transcript was not collected"}, {"section_title": "Woodruff, R.S. (1971). A Simple Method for Approximating the Variance of a Complicated", "text": "Estimate. Journal of the American Statistical Association, 66(334): 411-414."}, {"section_title": "NPSAS:08 Institution and Student Sampling Details", "text": "To develop the mathematical foundation for the 2007-08 National Postsecondary Student Aid Study (NPSAS:08) institutional and student sampling design, the following notation is used to represent the institutional and student sampling strata: r = 1, 2, ..., 46 indexes the institutional strata, and s = 1, 2, ..., 20 indexes the student strata. The strata accounted for selection of institutions in the six states where there were representative samples. The institution measure of size (described below) accounted for student counts and sampling rates. Further define the following notation: .., J(r) indexes the institutions that belong to institutional stratum r, M rs (j) = number of students enrolled during the NPSAS year who belong to student stratum s at the j-th institution in stratum r based on the latest IPEDS data, and m rs = number of students to be selected from student stratum s within the r-th institutional stratum, referred to henceforth as student stratum rs. The overall population sampling rate for student stratum rs, is then given by where The student sampling rates, f rs , were computed based on the final sample allocation and IPEDS data regarding the population sizes. The composite measure of size for the j-th institution in stratum r will then be defined as which is the number of students that would be selected from the j-th institution if all institutions on the frame were to be sampled. An independent sample of institutions was selected for each institutional stratum using Chromy's sequential probability minimum replacement (pmr) sampling algorithm to select institutions with probabilities proportional to their measures of size (Chromy 1979). However, rather than allow multiple selections of sample institutions, those institutions with expected frequencies of selection greater than unity (1.00) were selected with certainty, and the remainder of the institutional sample was selected from the remaining institutions in each stratum. This process made it unnecessary to select multiple second-stage samples of persons by precluding institutions Within each of the r institutional types, the type r sampling frame was implicitly stratified by sorting in a serpentine manner (see Williams and Chromy 1980) by the following variables: \u2022 level of for-profit 2-year-or-more institutions; \u2022 historically black colleges and universities (HBCU) indicator; \u2022 Hispanic-serving institution indicator; \u2022 Carnegie classification of postsecondary institutions; \u2022 institution region (from the IPEDS IC file) with Alaska and Hawaii moved to Region 9 with Puerto Rico; \u2022 state and system, for a subset of states; and \u2022 the institution measure of size. The objective of this additional, implicit stratification was to ensure proportionate representation of institutions across important characteristics. Procedures for obtaining and sampling from student lists included: \u2022 processing lists on a flow basis as they were received; \u2022 ensuring that each sample institution had at least ten sample students; \u2022 implementing quality assurance checks against the latest IPEDS data; and \u2022 compiling a master sample file on a flow basis as sample students were selected, including student sampling weights. Student samples were selected as stratified, systematic random samples. The student sampling rates were fixed for each sample institution rather than the student sample sizes: \u2022 to facilitate selecting the samples on a flow basis as the student lists were received from sample institutions; and \u2022 because sampling at a fixed rate based on the overall stratum sampling rate and the institution probabilities of selection results in approximately equal overall probabilities of selection within student strata. Recall that the overall population sampling rate for student stratum rs is given by where For the unconditional probability of selection to be a constant for all eligible students in stratum rs, the overall probability of selection should be the overall student sampling fraction, f rs ; i.e., it was ensured that or equivalently, Thus, the conditional sampling rate for stratum rs, given selection of the j-th institution, becomes However, in this case, the desired overall student sample size, m s , is achieved only in expectation over all possible samples. Achieving the desired sample sizes with equal probabilities within strata in the particular sample selected and simultaneously adjusting for institutional nonresponse and ineligibility requires that where R denotes the set of eligible, responding institutions. If the conditional student sampling rate for stratum rs in the j-th institution is it is required that Since it was necessary to set the student sampling rates before complete information on eligibility and response status was available, was calculated as follows: where S denotes the set of all sample institutions, E r = the institutional eligibility factor for institutional stratum r, R r = the institutional response factor for institutional stratum r, E rs = the student eligibility factor for student stratum rs. NPSAS is a multivariate survey with a p-dimensional parameter space, \u03b8 = {\u03b8 j }, f = 1, \u2026.., p, for which it is desired to estimate \u03b8 with \u03b8\u02c6 while minimizing cost (sample size) subject to a series of precision requirements. Consequently, optimal sampling rates can be obtained by solving the following nonlinear optimization problem: where, C 0 = fixed cost not affected by changes in the numbers of institutions or students selected; C r = variable cost per institution, depending on the number of participating institutions in the r-th institutional stratum; n r = number of participating institutions in the r-th stratum; C rs = variable cost per student, depending on the number of participating students in student stratum rs; and n rs = number of participating students in student stratum rs. In the above, variance constraints correspond to precision requirements that were specified for key survey estimates. Using data from NPSAS:04 and NPSAS:00 (for baccalaureate recipient constraints), all of the required variance components and their associated precision constraints were developed. Subsequently, the resulting nonlinear optimization problem to determine the most effective sample allocation was solved using Chromy's algorithm (Chromy 1987) to obtain feasible solutions to the above problem. The sample size for NPSAS:08 is larger than that for past NPSAS studies. The first reason for the increased sample size was to ensure sufficient yield for analytic purposes. The sample size was designed so that respondent yield would be sufficient for analyses even if actual response rates were lower than the targeted rates. Second, the NPSAS:08 sample was augmented to include representative samples of ACG and SMART Grant recipients. Third, as previously mentioned, NPSAS:08 includes state-representative undergraduate student samples for four types of institutions (public 2-year; public 4-year; private, not-for-profit 4-year; and private, for-profit, degree-granting) in six states. A larger overall sample size was necessary to achieve state-representative samples in addition to the nationally representative sample. The study was designed to ensure adequate sample sizes for the domains of interest.      1 = 4-year bachelor's degree 2 = 5-year bachelor's degree (also awarded by a 4year college or university, but generally requires 5 years of full-time, college-level work) 3 = Associate's degree 4 = Undergraduate certificate or diploma 5 = Undergraduate, not enrolled in a degree program 6 = Post-baccalaureate certificate 7 = Master's degree 8 = Post-master's certificate 9 = Professional degree (only includes the following degree programs: chiropractic, dentistry, law, medicine, optometry, osteopathic medicine, pharmacy, podiatry, ministry or divinity, or veterinary medicine) 10 = Doctoral degree 11 = Graduate, not enrolled in a degree program 12 = Multiple degrees in the 2007-08 school year Applies to: All respondents. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Appendix D. Facsimile of Full- In what month and year did you first attend any college, university, or trade school after completing your high school requirements? (Please select both a month and a year from the dropdowns.) [else] In what month and year did you first attend any college, university, or trade school after completing your high school requirements? RBFSTMY is provided in the YYYYMM format. Dates after December 2005 were replaced with a -6 to indicate the date was out of range. Applies to: Respondents whose first college enrollment after high school was at a school other than NPSAS. Instrument code: RBNFST ne 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RBNPBMY", "text": ""}, {"section_title": "Date first enrolled at NPSAS for bachelor's degree", "text": "In what month and year were you first enrolled at [NPSAS] for your bachelor's degree? RBNPBMY is provided in the YYYYMM format. Dates after June 2008 were replaced with a -6 to indicate the date was out of range. Applies to: All respondents. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview  .2599 = Veterinary biomed/clinical sci, other 51.3101 = Dietetics/dietitian 51.3102 = Clinical nutrition/nutrionist 51.3801 = Nursing/registered nurse 52.0101 = Business/commerce, general 52.0201 = Business admin and management, general 52.0205 = Operations management and supervision 52.0299 = Business/managerial operations, other 52.0301 = Accounting 52.0304 = Accounting and finance 52.0305 = Accounting and business/management 52.0401 = Admin asst/secretarial sciences, general 52.0407 = Business/office automation/data entry 52.0701 = Entrepreneurship/entrepreneurial studies 52.0799 = Entrepreneurial and small bus ops, other 52.0801 = Finance, general 52.0909 = Hotel, motel, and restaurant management 52.1001 = Human resources mgmt/pers admin, general 52.1101 = International business/trade/commerce 52.1201 = Management information systems, general 52.1401 = Marketing/marketing management, general 52.1902 = Fashion merchandising 52.2101 = Telecommunications management 52.9999 = Business, management, marketing, other 54.0101 = History, general Applies to: Respondents who had no preloaded original declared major and had a formal change in major at the NPSAS school (indicated either in a preload or in the instrument). Instrument code: [no preloaded original declared major from NPSAS:08 FS] and ([preloaded number of changes in major from NPSAS:08 FS] or RBNPCHNM in (1 2)) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Please type your major in the box provided and then click the \"Search for Major\" button. A list of categories that match your entry will be displayed. admin/mgmt 11.9999 = Computer/info sci/support services, other 12.0504 = Restaurant, culinary, and catering mgmt 12.0507 = Food srvc, waiter/waitress, dining mgmt 13.0101 = Education, general 13.0201 = Bilingual and multilingual education 13.1001 = Special education and teaching, general 13.1006 = Ed/teaching indiv w mental retardation 13.1007 = Ed/teaching indiv with mult disabilities 13.1008 = Ed/teaching indiv w physical impairments 13.1012 = Ed/teaching indiv w speech/lang impair 13.1015 = Ed/teaching early childhood special ed 13.1099 = Special education and teaching, other 13.1202 = Elementary education and teaching 13.1203 = Junior high/middle school ed/teaching 13.1205 = Secondary education and teaching 13.1210 = Early childhood education/teaching Appendix D. Facsimile of Full-    .3808 = Nursing science 52.0101 = Business/commerce, general 52.0201 = Business admin and management, general 52.0204 = Office management and supervision 52.0205 = Operations management and supervision 52.0212 = Retail management 52.0299 = Business/managerial operations, other 52.0301 = Accounting 52.0302 = Accounting tech/technician/bookkeeping 52.0304 = Accounting and finance 52.0305 = Accounting and business/management 52.0601 = Business/managerial economics 52.0701 = Entrepreneurship/entrepreneurial studies 52.0703 = Small business administration/mgmt 52.0801 = Finance, general 52.0901 = Hospitality administration/mgmt, general 52.1001 = Human resources mgmt/pers admin, general Recode note: If RBNFST = 0 then RBOTHSCH = 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RBNPMJCH", "text": ""}, {"section_title": "RBDBLMAJ", "text": ""}, {"section_title": "RBSCH01", "text": "[RBSCH01]: name [If RBNFST = 0 and iteration = 1 and COMPMODE = 0] What is the name of the first college, university, or trade school you enrolled in after completing your high school requirements? To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select \"Foreign Country\" from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. [else if RBNFST = 0 and iteration = 1 and COMPMODE = 1] What is the name of the first college, university, or trade school you enrolled in after completing your high school requirements, and in what city and state is it located? Please bear with me while I code this. [else if COMPMODE = 0] What is the name of that school? If you attended more than one other school between high school and before your graduation from [NPSAS] tell us about the most recent school first. You will have an opportunity to tell us about all schools later. To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select \"Foreign Country\" from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. [else] At what other school have you had enrollment between the time you graduated from high school and the time you graduated from [NPSAS], and in what city and state is it located? (If you attended more than one other school between high school and before your graduation from [NPSAS] tell me about the most recent school first. You will have an opportunity to tell me about all schools later.) Please bear with me while I code this. Applies to: Respondents who attended another college before earning their bachelor's degree from NPSAS. Source: B&B:08/09 full scale student interview"}, {"section_title": "RBIPED01 [RBSCH01]: IPEDS", "text": "[If RBNFST = 0 and iteration = 1 and COMPMODE = 0] What is the name of the first college, university, or trade school you enrolled in after completing your high school requirements? To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select \"Foreign Country\" from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. [else if RBNFST = 0 and iteration = 1 and COMPMODE = 1] What is the name of the first college, university, or trade school you enrolled in after completing your high school requirements, and in what city and state is it located? Please bear with me while I code this. [else if COMPMODE = 0] What is the name of that school? If you attended more than one other school between high school and before your graduation from [NPSAS] tell us about the most recent school first. You will have an opportunity to tell us about all schools later. To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select \"Foreign Country\" from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. [else] At what other school have you had enrollment between the time you graduated from high school and the time you graduated from [NPSAS], and in what city and state is it located? (If you attended more than one other school between high school and before your graduation from [NPSAS] tell me about the most recent school first. You will have an opportunity to tell me about all schools later.) Please bear with me while I code this. Applies to: Respondents who attended another college before earning their bachelor's degree from NPSAS. Source: B&B:08/09 full scale student interview"}, {"section_title": "RBCT01", "text": "[RBSCH01]: city [If RBNFST = 0 and iteration = 1 and COMPMODE = 0] What is the name of the first college, university, or trade school you enrolled in after completing your high school requirements? To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select \"Foreign Country\" from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. [else if RBNFST = 0 and iteration = 1 and COMPMODE = 1] What is the name of the first college, university, or trade school you enrolled in after completing your high school requirements, and in what city and state is it located? Please bear with me while I code this. [else if COMPMODE = 0] What is the name of that school? If you attended more than one other school between high school and before your graduation from [NPSAS] tell us about the most recent school first. You will have an opportunity to tell us about all schools later. To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select \"Foreign Country\" from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. [else] At what other school have you had enrollment between the time you graduated from high school and the time you graduated from [NPSAS], and in what city and state is it located? (If you attended more than one other school between high school and before your graduation from [NPSAS] tell me about the most recent school first. You will have an opportunity to tell me about all schools later.) Please bear with me while I code this. To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select \"Foreign Country\" from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. ["}, {"section_title": "else]", "text": "At what other school have you had enrollment between the time you graduated from high school and the time you graduated from [NPSAS], and in what city and state is it located? (If you attended more than one other school between high school and before your graduation from [NPSAS] tell me about the most recent school first. You will have an opportunity to tell me about all schools later.) Please bear with me while I code this. What is the name of the first college, university, or trade school you enrolled in after completing your high school requirements? To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select \"Foreign Country\" from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. [else if RBNFST = 0 and iteration = 1 and COMPMODE = 1] What is the name of the first college, university, or trade school you enrolled in after completing your high school requirements, and in what city and state is it located? Please bear with me while I code this. [else if COMPMODE = 0] What is the name of that school? If you attended more than one other school between high school and before your graduation from [NPSAS] tell us about the most recent school first. You will have an opportunity to tell us about all schools later. To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select \"Foreign Country\" from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. [else] At what other school have you had enrollment between the time you graduated from high school and the time you graduated from [NPSAS], and in what city and state is it located? (If you attended more than one other school between high school and before your graduation from [NPSAS] tell me about the most recent school first. You will have an opportunity to tell me about all schools later.) Please bear with me while I code this. 1 = 4-year 2 = 2-year 3 = Less-than-2-year Applies to: Respondents who attended another college before earning their bachelor's degree from NPSAS. Source: B&B:08/09 full scale student interview"}, {"section_title": "RBCTRL01", "text": "[RBSCH01]: control [If RBNFST = 0 and iteration = 1 and COMPMODE = 0] What is the name of the first college, university, or trade school you enrolled in after completing your high school requirements? To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select \"Foreign Country\" from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. [else if RBNFST = 0 and iteration = 1 and COMPMODE = 1] What is the name of the first college, university, or trade school you enrolled in after completing your high school requirements, and in what city and state is it located? Please bear with me while I code this. [else if COMPMODE = 0] What is the name of that school? If you attended more than one other school between high school and before your graduation from [NPSAS] tell us about the most recent school first. You will have an opportunity to tell us about all schools later. To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select \"Foreign Country\" from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed."}, {"section_title": "[else]", "text": "At what other school have you had enrollment between the time you graduated from high school and the time you graduated from [NPSAS], and in what city and state is it located? (If you attended more than one other school between high school and before your graduation from [NPSAS] tell me about the most recent school first. You will have an opportunity to tell me about all schools later.) Please bear with me while I code this. 1 = Public 2 = Private not-for-profit 3 = Private for-profit  1 1)Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview RBUGLAM Amount borrowed in undergraduate loans How much did you borrow in student loans for your entire undergraduate education? Please do not include any money borrowed from family or friends. (If you are unsure of the amount, provide your best estimate.) Values less than 100 and greater than 150,000 were replaced with a -6 to indicate the value was out of range. Applies to: Respondents who took out an undergraduate loan or did not know whether they took out an undergraduate loan. Instrument code: RBUGLN in (-1 1) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview RBUGPRIV Amount borrowed in private undergraduate loans [If RBUGLAM = -9] Of the amount you borrowed for your undergraduate education, how much money did you borrow through private student loans? [else] Of the $[RBUGLAM] you borrowed for your undergraduate education, how much money did you borrow through private student loans? Values less than 100 and greater than 100,000 were replaced with a -6 to indicate the value was out of range. Applies to: Respondents who took out a private loan or did not know whether they took out an undergraduate loan. Instrument code: RBUGLN = -1 or RBLNPRI = 1 Recode note: If RBLNPRI = 1 and RBLNFED = 0 and RBLNELSE = 0 then RBUGPRIV = RBUGLAM Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview RBUGOWE Amount owed for undergraduate loans [If RBUGLAM = -9] How much of the amount that you borrowed in total undergraduate loans do you still owe? (If you are unsure of the amount, provide your best estimate.) [else] How much of the $[RBUGLAM] in total undergraduate loans do you still owe? (If you are unsure of the amount, provide your best estimate.) Values greater than 150,000 were replaced with a -6 to indicate the value was out of range. Applies to: Respondents who took out an undergraduate loan or did not know whether they took out an undergraduate loan. Instrument code: RBUGLN in (-1 1) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RBRPYST", "text": ""}, {"section_title": "Currently repaying undergraduate loans", "text": "Are you currently repaying any educational loans for your undergraduate education? If you are married and your spouse is paying your loans for you, indicate \"yes.\" If someone other than a spouse is paying your loans for you, indicate \"no.\" 0 = No 1 = Yes Applies to: Respondents who took out an undergraduate loan and owed an amount other than 0 for the loan, or those who did not know whether they took out an undergraduate loan. Instrument code: RBUGLN in (-1 1) and RBUGOWE ne 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Appendix D. Facsimile of Full- 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select Foreign Country from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. [else] What is the name of that school, and in what city and state is it located? PLEASE BEAR WITH ME AS I CODE THIS -IT SHOULD JUST TAKE A SECOND. Applies to: Respondents who enrolled for a postbaccalaureate degree or certificate. Source: B&B:08/09 full scale student interview What is the name of that school? To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select Foreign Country from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. [else] What is the name of that school, and in what city and state is it located? PLEASE BEAR WITH ME AS I CODE THIS -IT SHOULD JUST TAKE A SECOND. RCIPED01 is the institution identification number from the U.S. Department of Education's Integrated Postsecondary Education Data System (IPEDS). May be preloaded from the IPEDS database or provided when the institution is coded using the IPEDS coder in the B&B:08/09 student interview. What is the name of that school? To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select Foreign Country from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list. Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed. [else] What is the name of that school, and in what city and state is it located? PLEASE BEAR WITH ME AS I CODE THIS -IT SHOULD JUST TAKE A SECOND. 1 = 4-year 2 = 2-year 3 = Less-than-2-year Applies to: Respondents who enrolled for a postbaccalaureate degree or certificate. Source: B&B:08/09 full scale student interview What is the name of that school? To code your school: 1. Enter all or part of the school name, and its city and state, if known, then click \"Search for School\" to display a list of matching schools. If your school is outside the U.S. and its territories, select Foreign Country from the state list and click \"Search for School.\" (We do not have schools in foreign countries in our database so we will then collect some additional information from you.) 2. Click on the name of your school in the resulting list.  13.0607 = Learning sciences 13.0701 = International and comparative education 13.1001 = Special education and teaching, general 13.1003 = Ed/teaching indiv with hearing impair 13.1004 = Ed/teaching gifted and talented 13.1005 = Ed/teaching indiv with emotional disturb 13.1006 = Ed/teaching indiv w mental retardation 13.1007 = Ed/teaching indiv with mult disabilities 13.1011 = Ed/teaching indiv w learning disab 13.1012 = Ed/teaching indiv w speech/lang impair 13.1013 = Ed/teaching indiv with autism 13.1015 = Ed/teaching early childhood special ed 13.  .0101 = Business/commerce, general 52.0201 = Business admin and management, general 52.0202 = Purchasing, procurement/contracts mgmt 52.0203 = Logistics materials/supply chain mgmt 52.0204 = Office management and supervision 52.0205 = Operations management and supervision 52.0206 = Non-profit/public/organizational mgmt 52.0211 = Project management 52.0213 = Organizational leadership 52.0299 = Business/managerial operations, other 52.0301 = Accounting 52.0304 = Accounting and finance 52.0305 = Accounting and business/management 52.0407 = Business/office automation/data entry 52.0501 = Business/corporate communications 52.0601 = Business/managerial economics 52.0701 = Entrepreneurship/entrepreneurial studies 52.0801 = Finance, general 52.0804 = Financial planning and services 52.0807 = Investments and securities 52.0899 = Finance/financial mgmt services, other 52.0901 = Hospitality administration/mgmt, general 52.0903 = Tourism and travel services management 52.1001 = Human resources mgmt/pers admin,   3 4then TCURENR = -9; else TCURENR = 0 Note: Some partial interviews (SUMSTFLG = 2) did not make it far enough in the interview to determine TCURENR, so their value of TCURENR is a -9 by default. 0 = No 1 = Yes Applies to: All respondents. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview TCURFT Current enrollment as a full-time student TCURFT is an internal variable indicating current enrollment as a full-time student. If TCURENR = 1 and ENRSTAT = 1 then TCURFT = 1 else if SUMSTFLG in (3 4) then TCURFT = -9 else TCURFT = 0 Note: Some partial interviews (SUMSTFLG = 2) did not make it far enough in the interview to determine TCURENR, so their value of TCURENR is a -9 by default. TCURENR is also -9 for completed abbreviated interviews (SUMSTFLG in (3 4)). 0 = No 1 = Yes Applies to: All respondents. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Appendix D. Facsimile of Full- Instrument code: RDJSTAT ne 1 and TCURFT ne 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDEMPTMP", "text": "Reason not working for pay: waiting to report to work or layoff Are you currently... Holding a job but waiting to report to work or on temporary leave or temporary layoff from work? 0 = No 1 = Yes Applies to: Respondents who were neither currently working for pay nor full-time students. Instrument code: RDJSTAT ne 1 and TCURFT ne 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDNUMJOB", "text": ""}, {"section_title": "Number of jobs for pay", "text": "How many jobs for pay do you have? Values greater than 5 were replaced with a -6 to indicate the value was out of range. 0 = 0 1 = 1 2 = 2 3 = 3 4 = 4 5 = 5 Applies to: All respondents. Recode note: If RDJSTAT = 0 then RDNUMJOB = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDWRKS", "text": "Primarily a student or employee while enrolled Since you are enrolled as a student and also working, would you say you are primarily... 1 = A student working to meet expenses 2 = An employee who decided to enroll in school Applies to: Respondents who were currently working for pay and currently enrolled in a degree or certificate program. Instrument code: RDJSTAT = 1 and TCURENR = 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDWRKHRS", "text": "Hours worked weekly while enrolled About how many hours per week do you work while enrolled? Values equal to 0 or greater than 60 were replaced with a -6 to indicate the value was out of range. Applies to: Respondents who were currently working for pay, currently enrolled in a degree or certificate program, and were primarily a student working to meet expenses. Instrument code: RDJSTAT = 1 and TCURENR = 1 and RDWRKS not in (-3 2) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDOCC2", "text": "Occupation coder: general code [If COMPMODE = 0 and RDNUMJOB > 1] Since you have more than one job, please refer to the job at which you work the most hours when answering the next few questions.(If you are a K-12 teacher, tell us about that job.)What is your job title and what do you do in your job? Please enter your job title and duties in the textboxes below and click on the \"Search for Occupation\" button. [else if COMPMODE = 0 and RDNUMJOB <= 1] What is your job title and what do you do in your job? Please enter your job title and duties in the textboxes below and click on the \"Search for Occupation\" button. [else if COMPMODE = 1 and RDNUMJOB > 1] Since you have more than one job, please refer to the job at which you work the most hours when answering the next few questions.  Since you have more than one job, please refer to the job at which you work the most hours when answering the next few questions.(If you are a K-12 teacher, tell us about that job.)What is your job title and what do you do in your job? Please enter your job title and duties in the textboxes below and click on the \"Search for Occupation\" button. [else if COMPMODE = 0 and RDNUMJOB <= 1] What is your job title and what do you do in your job? Please enter your job title and duties in the textboxes below and click on the \"Search for Occupation\" button. [else if COMPMODE = 1 and RDNUMJOB > 1] Since you have more than one job, please refer to the job at which you work the most hours when answering the next few questions. What is your job title and what do you do in your job? Please bear with me while I code this. [else] What is your job title and what do you do in your job? Please bear with me while I code this. Applies to: Respondents who were currently working for pay. Instrument code: RDJSTAT = 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDJBDY", "text": ""}, {"section_title": "Job duties", "text": "[If COMPMODE = 0 and RDNUMJOB > 1] Since you have more than one job, please refer to the job at which you work the most hours when answering the next few questions.(If you are a K-12 teacher, tell us about that job.)What is your job title and what do you do in your job? Please enter your job title and duties in the textboxes below and click on the \"Search for Occupation\" button. [else if COMPMODE = 0 and RDNUMJOB <= 1] What is your job title and what do you do in your job? Please enter your job title and duties in the textboxes below and click on the \"Search for Occupation\" button. [else if COMPMODE = 1 and RDNUMJOB > 1] Since you have more than one job, please refer to the job at which you work the most hours when answering the next few questions. What is your job title and what do you do in your job? Please bear with me while I code this. [else] What is your job title and what do you do in your job? Please bear with me while I code this. Applies to: Respondents who were currently working for pay. Instrument code: RDJSTAT = 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDEMPZIP", "text": ""}, {"section_title": "Employer zip code", "text": "What is the 5-digit zip code where you work? Values less than 00501 or greater than were replaced with a -6 to indicate the value was out of range. Applies to: Respondents who were currently working for pay within the United States. Instrument code: RDJSTAT = 1 and RDOUTUS ne 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview primary business or industry in the textbox and then, from the list below, select the category which best describes that business or industry. Examples of each industry will be displayed in the box as you make a selection."}, {"section_title": "RDOUTUS", "text": "[else] Please provide your [{if RDEMPTYP ne 6} employer's] primary business or industry. Please bear with me while I code this. (From the list below, please select the category which best describes the respondent's [{if RDEMPTYP ne 6} employer's] industry or business area. As you click on a selection, examples of the industry will be displayed in the box.) Industry text strings containing an employer name or address were set to a -9. Applies to: Respondents who were currently working for pay. Instrument code: RDJSTAT = 1 Recode note: 1) If RDEMPTYP = 5 then RDIND = MILITARY 2) If RDEMPTYP = 4 then RDIND = GOVERNMENT 3) If RDOCC6 in 252012 252021 252022 252023  252031 252032 252041 252042 252043  In what month and year did you begin your current selfemployment? [else] In what month and year did you start your current job? RDEMPMY is provided in the YYYYMM format. Dates after respondent's COMPDATE were replaced with a -6 to indicate the date was out of range. "}, {"section_title": "RDCURHRS", "text": ""}, {"section_title": "Hours worked weekly Job title: [RDJBTL]", "text": "On average, how many hours do you work per week in your job? Values equal to 0 or greater than 80 were replaced with a -6 to indicate the value was out of range. Applies to: Respondents who were currently working for pay and were not primarily a student working to meet expenses. Instrument code: RDJSTAT = 1 and RDWRKS ne 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview RDPREFT Prefer to work more hours Job title: [RDJBTL] [If TCURFT = 1] Would you prefer to work more hours than you do even though you are currently a full-time student? [else if TCURENR = 1] Would you prefer to work more hours than you do even though you are currently enrolled in school? [else] Would you prefer to work more hours than you do? 0 = No 1 = Yes Applies to: Respondents who were currently working for pay. Instrument code: RDJSTAT = 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDERNAMT", "text": ""}, {"section_title": "Amount earned from job Job title: [RDJBTL]", "text": "How much do you earn from your job? Values greater than 250,000 or less than 100 when RDEARNT = 1, greater than 20,833.33 when RDEARNT = 2, greater than 9,615.38 when RDEARNT = 3, greater than 4,807.69 when RDEARNT = 4, greater than 961.54 when RDEARNT = 5, or greater than 120.19 when RDEARNT = 6, were replaced with a -6 to indicate the value was out of range. Each calculation was based on the respondent reporting more than $250,000 per year. Values equal to 0 for all RDEARNT were also replaced with a -6 to indicate the value was out of range. In the full student interview, respondents who indicated current employment in certain teaching fields received teaching-specific income questions and did not receive this question. The abbreviated interview did not include teaching-specific income questions, so teachers who completed an abbreviated interview received RDERNAMT. Applies to: Respondents who participated in a completed full interview or a partial interview, were currently working for pay, and not a K-12 teacher, or who participated in an abbreviated interview and were currently working for pay. Instrument code: (SUMSTFLG in (1 2) and RDJSTAT = 1 and RDOCC6 not in (252012 252021 252022 252023 252031 252032 252041 252042 252043)) or (SUMSTFLG in (3 4) and RDJSTAT = 1) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview RDEARNT Time frame for earnings Job title: [RDJBTL] How much do you earn from your job? In the full student interview, respondents who indicated current employment in certain teaching fields received teaching-specific income questions and did not receive this question. The abbreviated interview did not include teaching-specific income questions, so teachers who completed an abbreviated interview received RDEARNT. 1 = Per year 2 = Per month 3 = Every two weeks 4 = Per week 5 = Per day 6 = Per hour Applies to: Respondents who participated in a completed full interview or a partial interview, were currently working for pay, and not a K-12 teacher, or who participated in an abbreviated interview and were currently working for pay. Instrument code: (SUMSTFLG in (1 2) and RDJSTAT = 1 and RDOCC6 not in (252012 252021 252022 252023  252031 252032 252041 252042 252043)) or (SUMSTFLG in (3 4) and RDJSTAT = 1) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview RDNSFPAY is based on item B20 from the NSF 2008 RCG paper-based questionnaire. B20 asks \"Did these factors influence your decision to work in an area outside the field of your highest degree? 1) Pay, promotion opportunities; 2) Working conditions (e.g., hours, equipment, working environment); 3) Job location; 4) Change in career or professional interests; 5) Familyrelated reasons (e.g., children, spouse's job moved); 6) Job in highest degree field not available; or 7) Some other reason, specify.\" For more information, see http://www.nsf.gov/statistics/srvyrecentgrads/. 0 = No 1 = Yes Applies to: Respondents who were currently working for pay and whose job was not related to their bachelor's degree. Instrument code: RDJSTAT = 1 and RDNSF19B = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDEMPMI", "text": ""}, {"section_title": "RDNSFCON", "text": ""}, {"section_title": "Reason working outside bachelor's field: working conditions", "text": "Which of the following factors influenced your decision to work in an area outside of your [NPSAS] bachelor's degree field... Working conditions (for example, hours, equipment, working environment)? RDNSFCON is based on item B20 from the NSF 2008 RCG paper-based questionnaire. B20 asks \"Did these factors influence your decision to work in an area outside the field of your highest degree? 1) Pay, promotion opportunities; 2) Working conditions (e.g., hours, equipment, working environment); 3) Job location; 4) Change in career or professional interests; 5) Familyrelated reasons (e.g., children, spouse's job moved); 6) Job in highest degree field not available; or 7) Some other reason, specify.\" For more information, see http://www.nsf.gov/statistics/srvyrecentgrads/. 0 = No 1 = Yes Applies to: Respondents who were currently working for pay and whose job was not related to their bachelor's degree. Instrument code: RDJSTAT = 1 and RDNSF19B = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDNSFLOC", "text": ""}, {"section_title": "Reason working outside bachelor's field: job location", "text": "Which of the following factors influenced your decision to work in an area outside of your [NPSAS] bachelor's degree field... Job location? RDNSFLOC is based on item B20 from the NSF 2008 RCG paper-based questionnaire. B20 asks \"Did these factors influence your decision to work in an area outside the field of your highest degree? 1) Pay, promotion opportunities; 2) Working conditions (e.g., hours, equipment, working environment); 3) Job location; 4) Change in career or professional interests; 5) Familyrelated reasons (e.g., children, spouse's job moved); 6) Job in highest degree field not available; or 7) Some other reason, specify.\" For more information see, http://www.nsf.gov/statistics/srvyrecentgrads/. 0 = No 1 = Yes Applies to: Respondents who were currently working for pay and whose job was not related to their bachelor's degree. Instrument code: RDJSTAT = 1 and RDNSF19B = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDNSFCHG", "text": ""}, {"section_title": "Reason working outside bachelor's field: career change", "text": "Which of the following factors influenced your decision to work in an area outside of your [NPSAS] bachelor's degree field... Change in career or professional interests? RDNSFCHG is based on item B20 from the NSF 2008 RCG paper-based questionnaire. B20 asks \"Did these factors influence your decision to work in an area outside the field of your highest degree? 1) Pay, promotion opportunities; 2) Working conditions (e.g., hours, equipment, working environment); 3) Job location; 4) Change in career or professional interests; 5) Familyrelated reasons (e.g., children, spouse's job moved); 6) Job in highest degree field not available; or 7) Some other reason, specify.\" For more information see, http://www.nsf.gov/statistics/srvyrecentgrads/. 0 = No 1 = Yes Applies to: Respondents who were currently working for pay and whose job was not related to their bachelor's degree. Instrument code: RDJSTAT = 1 and RDNSF19B = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDNSFFAM", "text": ""}, {"section_title": "Reason working outside bachelor's field: family-related", "text": "Which of the following factors influenced your decision to work in an area outside of your [NPSAS] bachelor's degree field... Family-related reasons (for example, children, spouse's job moved)? RDNSFFAM is based on item B20 from the NSF 2008 RCG paper-based questionnaire. B20 asks \"Did these factors influence your decision to work in an area outside the field of your highest degree? 1) Pay, promotion opportunities; 2) Working conditions (e.g., hours, equipment, working environment); 3) Job location; 4) Change in career or professional interests; 5) Familyrelated reasons (e.g., children, spouse's job moved); 6) Job in highest degree field not available; or 7) Some other reason, specify.\" For more information see, http://www.nsf.gov/statistics/srvyrecentgrads/. 0 = No 1 = Yes Applies to: Respondents who were currently working for pay and whose job was not related to their bachelor's degree. Instrument code: RDJSTAT = 1 and RDNSF19B = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Appendix D. Facsimile of Full- RDNSFFLD is based on item B20 from the NSF 2008 RCG paper-based questionnaire. B20 asks \"Did these factors influence your decision to work in an area outside the field of your highest degree? 1) Pay, promotion opportunities; 2) Working conditions (e.g., hours, equipment, working environment); 3) Job location; 4) Change in career or professional interests; 5) Familyrelated reasons (e.g., children, spouse's job moved); 6) Job in highest degree field not available; or 7) Some other reason, specify.\" For more information see, http://www.nsf.gov/statistics/srvyrecentgrads/ 0 = No 1 = Yes Applies to: Respondents who were currently working for pay and whose job was not related to their bachelor's degree. Instrument code: RDJSTAT = 1 and RDNSF19B = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDNSFOFR", "text": ""}, {"section_title": "Reason working outside bachelor's field: other", "text": "Which of the following factors influenced your decision to work in an area outside of your [NPSAS] bachelor's degree field... Other factor(s) not listed? RDNSFOFR is based on item B20 from the NSF 2008 RCG paper-based questionnaire. B20 asks \"Did these factors influence your decision to work in an area outside the field of your highest degree? 1) Pay, promotion opportunities; 2) Working conditions (e.g., hours, equipment, working environment); 3) Job location; 4) Change in career or professional interests; 5) Familyrelated reasons (e.g., children, spouse's job moved); 6) Job in highest degree field not available; or 7) Some other reason, specify.\" For more information see, http://www.nsf.gov/statistics/srvyrecentgrads/. 0 = No 1 = Yes Applies to: Respondents who were currently working for pay and whose job was not related to their bachelor's degree. Instrument code: RDJSTAT = 1 and RDNSF19B = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDNSF21B", "text": ""}, {"section_title": "Most important reason for working outside bachelor's field", "text": "Which of the following was your most important reason for working in an area outside of your bachelor's degree field... RDNSF21B is based on item B21 from the NSF 2008 RCG paper-based questionnaire. B21 asks \"Which two factors in question B20 were your most important reasons for working in an area outside the field of your highest degree? 1) Most important reason, and 2) Second most important reason.\" For more information, see http://www.nsf.gov/statistics/srvyrecentgrads/. 1 = Pay, promotion opportunities 2 = Working conditions 3 = Job location 4 = Change in career or professional interests 5 = Family-related reasons 6 = Job in [NPSAS] bachelor's degree field not available 7 = Other factor(s) not listed Applies to: Respondents who were currently working for pay, whose job was not related to their bachelor's degree, and who selected two or more reasons why they worked in an area outside their bachelor's degree. Instrument code: RDJSTAT = 1 and RDNSF19B = 0 and [selected two or more reasons why they worked in an area outside their bachelor's degree from the following variables: RDNSFPAY, RDNSFCON, RDNSFLOC, RDNSFCHG, RDNSFFAM, RDNSFFLD, and RDNSFOFR] Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Job while pursuing other interests 0 = No 1 = Yes Applies to: Respondents who were currently working for pay and did not consider their job to be the beginning of a career. Instrument code: RDJSTAT = 1 and RDCARIND = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview RDCURFUT Job description: deciding on future education/career Job title: [RDJBTL] Since this job is not the start of your career, how would you describe it? Working while deciding on future education/career 0 = No 1 = Yes Applies to: Respondents who were currently working for pay and did not consider their job to be the beginning of a career. Instrument code: RDJSTAT = 1 and RDCARIND = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDNF21B2", "text": ""}, {"section_title": "RDCUROTH", "text": ""}, {"section_title": "Job description: other Job title: [RDJBTL]", "text": "Since this job is not the start of your career, how would you describe it? Other 0 = No 1 = Yes Applies to: Respondents who were currently working for pay and did not consider their job to be the beginning of a career. Instrument code: RDJSTAT = 1 and RDCARIND = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview RDDIFNUM Time before current job offer: amount Job title: [RDJBTL] After you began applying for jobs, about how long did it take before you received an offer for your current job? Values greater than 1,460 when RDDIFTIM = 1, greater than 208 when RDDIFTIM = 2, greater than 48 when RFVLAMT = 3, or greater than 4 when RDDIFTIM = 1, were replaced with a -6 to indicate the value was out of range. Each calculation was based on the respondent reporting more than 4 years. Applies to: Respondents who were currently working for pay, whose job was related to their bachelor's degree, and who considered their job to be the beginning of a career. Instrument code: RDJSTAT = 1 and RDNSF19B ne 0 and RDCARIND ne 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview RDDIFTIM Time before current job offer: time frame Job title: [RDJBTL] After you began applying for jobs, about how long did it take before you received an offer for your current job? 1 = Day(s) 2 = Week(s) 3 = Month(s) 4 = Year(s) Applies to: Respondents who were currently working for pay, whose job was related to their bachelor's degree, and who considered their job to be the beginning of a career. Instrument code: RDJSTAT = 1 and RDNSF19B ne 0 and RDCARIND ne 0 and RDDIFNUM ne 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDJBPAY", "text": ""}, {"section_title": "Compensation satisfaction Job title: [RDJBTL]", "text": "Are you satisfied with each of the following in your current job... Compensation (pay and fringe benefits)? 0 = No 1 = Yes Applies to: Respondents who were currently working for pay. Instrument code: RDJSTAT = 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Values equal to 0 or greater than 9 were replaced with a -6 to indicate the value was out of range. 1 = 1 2 = 2 3 = 3 4 = 4 5 = 5 6 = 6 7 = 7 8 = 8 9 = 9 Applies to: Respondents who worked since graduation. Instrument code: RDWRK12M = 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Appendix D. Facsimile of Full- If RDOCC6 in (252012 252021 252022 252023 252031 252032 252041 252042 252043) then TTEACHR = 1; else if REEVRTCH = 1 then TTEACHR = 2; else if REPREPAR = 1 then TTEACHR = 3; else if RECONSID = 1 then TTEACHR = 4; else if RECONSID = -9 then TTEACHR = -9; else TTEACHR = 0 0 = Never taught 1 = Current teacher 2 = Had taught since NPSAS 3 = Preparing for teaching 4 = Currently considering teaching Applies to: All respondents. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview  1 2 3Recode note: If RECRTTYP = 6 then RECURCRT = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RDJBIMPO", "text": ""}, {"section_title": "RETCHAPP", "text": ""}, {"section_title": "RECRTTYP", "text": ""}, {"section_title": "Type of teacher certification", "text": "What type of teacher certificate do you hold? 1 = Regular/standard state certificate or advanced professional certificate 2 = Certificate issued after satisfying all requirements except a probationary period 3 = Certificate that requires additional courses, student teaching, or passing a test before obtaining regular certification 4 = Certificate issued to persons who must complete a certification program in order to continue teaching 5 = Other type of teacher certification 6 = None (no teacher certification) Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECRTMY", "text": ""}, {"section_title": "Date first certified to teach", "text": "In what month and year were you first certified to teach? RECRTMY is provided in the YYYYMM format. Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECGENA", "text": ""}, {"section_title": "Content area certification: elementary education", "text": "In what content area(s) are you currently certified to teach? Elementary education (general curriculum in elementary or middle grades) 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECGENB", "text": ""}, {"section_title": "Content area certification: secondary education", "text": "In what content area(s) are you currently certified to teach? Secondary education (general curriculum in middle or secondary grades) 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Appendix D. Facsimile of Full-scale Instrument-Section E. Teaching B&B:08/09 Full-scale Methodology Report D-81"}, {"section_title": "RECSPCED", "text": ""}, {"section_title": "Content area certification: special education", "text": "In what content area(s) are you currently certified to teach? Special education 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECART", "text": ""}, {"section_title": "Content area certification: arts and music", "text": "In what content area(s) are you currently certified to teach? Arts and music 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECENGL", "text": ""}, {"section_title": "Content area certification: English or language arts", "text": "In what content area(s) are you currently certified to teach? English or language arts 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECESL", "text": ""}, {"section_title": "Content area certification: English as a second language", "text": "In what content area(s) are you currently certified to teach? English as a second language (ESL) 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECFLNG", "text": ""}, {"section_title": "Content area certification: foreign languages", "text": "In what content area(s) are you currently certified to teach? Foreign languages 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECHELTH", "text": ""}, {"section_title": "Content area certification: health/physical education", "text": "In what content area(s) are you currently certified to teach? Health, physical education 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECMATH", "text": ""}, {"section_title": "Content area certification: math or computer science", "text": "In what content area(s) are you currently certified to teach? Mathematics or computer science 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECSCIEN", "text": ""}, {"section_title": "Content area certification: natural sciences", "text": "In what content area(s) are you currently certified to teach? Natural sciences 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECSOSCI", "text": ""}, {"section_title": "Content area certification: social sciences", "text": "In what content area(s) are you currently certified to teach? Social sciences 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECVOCTC", "text": ""}, {"section_title": "Content area certification: vocational/career/technical education", "text": "In what content area(s) are you currently certified to teach? Vocational, career, or technical education 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REMISC", "text": ""}, {"section_title": "Content area certification: miscellaneous", "text": "In what content area(s) are you currently certified to teach? Miscellaneous (driver education, humanities or liberal studies, library or information science, military science or ROTC, philosophy, religious studies, theology, or divinity) 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECOTHER", "text": ""}, {"section_title": "Content area certification: other", "text": "In what content area(s) are you currently certified to teach? Other 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were currently certified to teach, and indicated the type of certification they held. Instrument code: TTEACHR in (1 2 3) and RECURCRT = 1 and RECRTTYP ne -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REJBTP01", "text": ""}, {"section_title": "Type of K-12 teaching position 1", "text": "What type of K-12 teaching position did you have when you first started working as a teacher after graduating from [NPSAS]? (If you are still in this same position, the next few questions ask you to think about your job when you first started teaching.) 1 = Regular, full-or part-time, elementary or secondary school teacher 2 = Itinerant teacher 3 = Support teacher 4 = Teacher's aide 5 = Short-term substitute 6 = Long-term substitute 7 = Student teacher 8 = Other teaching position Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree. Instrument code: TTEACHR in (1 2) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REJBMY01", "text": ""}, {"section_title": "Starting date of [REJBTP01] position", "text": "In what month and year did you begin this position as a/an [REJBTP01]? REJBMY01 is provided in the YYYYMM format. Applies to: Respondents who were currently teaching or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position, and had already begun their teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) and REJBFL01 ne 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REJBFL01", "text": ""}, {"section_title": "Starting [REJBTP01] position in fall", "text": "In what month and year did you begin this position as [REJBTP01]? Check here if you have been hired for the fall but have not yet begun this position 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECNTY01", "text": "Itinerant teaching school 1: county When you first started working as an itinerant teacher, in what county, school district, and state were you working? County: Applies to: Respondents who were currently teachers or had taught as an itinerant teacher for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 = 2 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REDIST01", "text": "Itinerant teaching school 1: district When you first started working as an itinerant teacher, in what county, school district, and state were you working? School district: Applies to: Respondents who were currently teachers or had taught as an itinerant teacher for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 = 2 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Appendix D. Facsimile of Full-scale Instrument-Section  What is the name of the school? To code your school: 1. Enter all or part of the school name. The city and state that you have entered on the previous form will appear. Click \"Search for School\" to display a list of matching schools. After the school list appears, click the Select button next to your school. If it is not listed, try searching with no city or no school name. 2. If you still cannot find your school, click the \"None of the Above\" button at the bottom of the list of search results. HINT: Entering a school name with the city and state will help to limit the number of schools displayed and reduce the time it will take for the school list to load.  What is the name of the school? To code your school: 1. Enter all or part of the school name. The city and state that you have entered on the previous form will appear. Click \"Search for School\" to display a list of matching schools. After the school list appears, click the Select button next to your school. If it is not listed, try searching with no city or no school name. 2. If you still cannot find your school, click the \"None of the Above\" button at the bottom of the list of search results. HINT: Entering a school name with the city and state will help to limit the number of schools displayed and reduce the time it will take for the school list to load. County: Applies to: Respondents who were currently teachers or had taught as a regular teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position and were unable to code their school in elementary and secondary school coder. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 3 6 8) and RESCOD01 in ('999996' '999997' '999998' '999999') Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RESTYP01", "text": "[REJBTP01] school: type Is this school... 1 = A public school operated by a school/county district 2 = A private Catholic school 3 = A private school--other religious affiliation 4 = A private school--no religious affiliation 5 = A public school operated by state/federal agency (ex: BIA, DOD, prison school) 6 = Other (charter school, hospital school) Applies to: Respondents who were currently teachers or had taught as a regular teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position and were unable to code their school in elementary and secondary school coder. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 3 6 8) and RESCOD01 in ('999996' '999997' '999998' '999999') Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RESGLO01", "text": "[REJBTP01] school: lowest grade level offered What are the highest and lowest grade levels offered at this school? Lowest grade level at school: 0 = Kindergarten 1 = First grade 2 = Second grade 3 = Third grade 4 = Fourth grade 5 = Fifth grade 6 = Sixth grade 7 = Seventh grade 8 = Eighth grade 9 = Ninth grade 10 = Tenth grade 11 = Eleventh grade 12 = Twelfth grade 13 = Ungraded Applies to: Respondents who were currently teachers or had taught as a regular teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position and were unable to code their school in elementary and secondary school coder. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 3 6 8) and RESCOD01 in ('999996' '999997' '999998' '999999') Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RESGHI01", "text": "[REJBTP01] school: highest grade level offered What are the highest and lowest grade levels offered at this school? Highest grade level at school: 0 = Kindergarten 1 = First grade 2 = Second grade 3 = Third grade 4 = Fourth grade 5 = Fifth grade 6 = Sixth grade 7 = Seventh grade 8 = Eighth grade 9 = Ninth grade = Tenth grade 11 = Eleventh grade 12 = Twelfth grade 13 = Ungraded Applies to: Respondents who were currently teachers or had taught as a regular teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position and were unable to code their school in elementary and secondary school coder. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 3 6 8) and RESCOD01 in ('999996' '999997' '999998' '999999') Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REJBFP01", "text": ""}, {"section_title": "Full time or part time in [REJBTP01] position", "text": "When you first started working as a/an [REJBTP01], were you working full time or part time? 1 = Full time 2 = Part time Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REJBIC01", "text": ""}, {"section_title": "Base salary in [REJBTP01] position", "text": "When you first started working as a/an [REJBTP01], what was your base salary (prior to taxes and deductions) for the academic year? Values greater than 125,000 when REICAM01 = 1, greater than 10,416.67 when REICAM01 = 2, greater than 4,807.69 when REICAM01 = 3, greater than 2,403.85 when REICAM01 = 4, greater than 480.77 when REICAM01 = 5, greater than 60.10 when REICAM01 = 6, or greater than 125,000 when REICAM01 = -9,were replaced with a -6 to indicate the value was out of range. Each calculation was based on the respondent reporting more than $125,000 per year. Values equal to 0 for all REICAM01 were also replaced with a -6 to indicate the value was out of range. Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REICAM01", "text": ""}, {"section_title": "Time frame for base salary in [REJBTP01] position", "text": "When you first started working as a/an [REJBTP01], what was your base salary (prior to taxes and deductions) for the academic year? 1 = Per year 2 = Per month 3 = Every two weeks 4 = Per week 5 = Per day 6 = Per hour Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Appendix D. Facsimile of Full-scale Instrument-Section  (Only include income from the school system that was not included in your base salary. If you did not earn any other income, please enter 0 in the box.) 1 = Per year 2 = Per month 3 = Every two weeks 4 = Per week 5 = Per day 6 = Per hour Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position, and indicated earning other school-related income while in their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) and REJBOS01 ne 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REJBOT01", "text": "Additional income outside school system while in [REJBTP01] position When you first started working as a/an [REJBTP01], what additional income did you earn from employment outside your school system? (If you did not earn any other income, please enter 0 in the box.) Values greater than 40,000 when REJBOT01 = 1, greater than 3,333.33 when REJBOT01 = 2, greater than 1,538.46 when REJBOT01 = 3, greater than 769.23 when REJBOT01 = 4, greater than 153.85 when REJBOT01 = 5, greater than 19.23 when REJBOT01 = 6, or greater than 40,000 when REJBOT01 = -9, were replaced with a -6 to indicate the value was out of range. Each calculation was based on the respondent reporting more than $40,000 per year. Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REOTAM01", "text": ""}, {"section_title": "Time frame for additional income while in [REJBTP01] position", "text": "When you first started working as a/an [REJBTP01], what additional income did you earn from employment outside your school system? (If you did not earn any other income, please enter 0 in the box.) 1 = Per year 2 = Per month 3 = Every two weeks 4 = Per week 5 = Per day 6 = Per hour Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position, and indicated earning additional income outside of the school system while in their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) and REJBOT01 ne 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REGENA01", "text": ""}, {"section_title": "Subjects taught in [REJBTP01] position: elementary education", "text": "What subjects did you teach? Elementary education (general curriculum in elementary or middle grades) 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REGENB01", "text": ""}, {"section_title": "Subjects taught in [REJBTP01] position: secondary education", "text": "What subjects did you teach? Secondary education (general curriculum in middle or secondary grades) 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RESPED01", "text": ""}, {"section_title": "Subjects taught in [REJBTP01] position: special education", "text": "What subjects did you teach? Special education 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Appendix D. Facsimile of Full-scale Instrument-Section  Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REESL01", "text": ""}, {"section_title": "Subjects taught in [REJBTP01] position: English as a second language", "text": "What subjects did you teach? English as a second language (ESL) 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REFLN01", "text": ""}, {"section_title": "Subjects taught in [REJBTP01] position: foreign languages", "text": "What subjects did you teach? Foreign languages 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REHPE01", "text": ""}, {"section_title": "Subjects taught in [REJBTP01] position: health/physical education", "text": "What subjects did you teach? Health, physical education 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REMTH01", "text": ""}, {"section_title": "Subjects taught in [REJBTP01] position: math or computer science", "text": "What subjects did you teach? Mathematics or computer science 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RESCI01", "text": ""}, {"section_title": "Subjects taught in [REJBTP01] position: natural sciences", "text": "What subjects did you teach? Natural sciences 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RESOC01", "text": ""}, {"section_title": "Subjects taught in [REJBTP01] position: social sciences", "text": "What subjects did you teach? Social sciences 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REVOC01", "text": ""}, {"section_title": "Subjects taught in [REJBTP01] position: vocational/career/technical", "text": "What subjects did you teach? Vocational, career, or technical education 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REMISC01", "text": ""}, {"section_title": "Subjects taught in [REJBTP01] position: miscellaneous", "text": "What subjects did you teach? Miscellaneous (driver education, humanities or liberal studies, library or information science, military science or ROTC, philosophy, religious studies, theology, or divinity) 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REOTH01", "text": ""}, {"section_title": "Subjects taught in [REJBTP01] position: other", "text": "What subjects did you teach? Other 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REINT01", "text": ""}, {"section_title": "Participated in teacher internship program in first teaching job", "text": "In your first teaching job, did you participate in a teacher internship program? (By \"teacher internship program\" we mean a program in which you complete your teacher preparation coursework during your first year or two of teaching after receiving a bachelor's degree. Internship programs provide coursework and support from college or district faculty and result in a regular teaching certificate.) 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Appendix D. Facsimile of Full-scale Instrument-Section E. Teaching B&B:08/09 Full-scale Methodology Report D-93"}, {"section_title": "REIND01", "text": ""}, {"section_title": "Participated in formal teacher induction program in first teaching job", "text": "In your first teaching job, did you participate in a formal teacher induction program in which you were assigned a mentor teacher who provided guidance to you in your job? 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REDSCP01", "text": "Feel prepared in first teaching job: classroom management In your first teaching job, did you feel adequately prepared to... Handle a range of classroom management or discipline situations? 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REINVR01", "text": "Feel prepared in first teaching job: instructional methods In your first teaching job, did you feel adequately prepared to... Use a variety of instructional methods? 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RETCH01", "text": "Feel prepared in first teaching job: teach subject matter In your first teaching job, did you feel adequately prepared to... Teach your subject matter? 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REDISC01", "text": "Receive help in first teaching job: disciplining students In your first teaching job, did you receive help from your school or school district in... Disciplining students? 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REMTHD01", "text": "Receive help in first teaching job: selecting curriculum In your first teaching job, did you receive help from your school or school district in... Selecting and implementing appropriate instructional methods and curriculum? 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RECMNT01", "text": "Receive help in first teaching job: working with parents/community In your first teaching job, did you receive help from your school or school district in... Working with parents and the community? 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught as a regular teacher, itinerant teacher, support teacher, long-term substitute, or in another teaching position for their first teaching position. Instrument code: TTEACHR in (1 2) and REJBTP01 in (1 2 3 6 8) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview  1 2 3 4Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "REJBCR01", "text": ""}, {"section_title": "RELNFRGV", "text": ""}, {"section_title": "Aware of teacher loan forgiveness programs", "text": "Are you aware of loan forgiveness programs which allow you to cancel all or part of your student loans in return for service to the community through teaching? 0 = No 1 = Yes Applies to: Respondents who were currently teachers, had taught since completing their bachelor's degree, had prepared to teach, or were currently considering teaching. Instrument code: TTEACHR in (1 2 3 4) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RELNINCT", "text": "Teacher loan forgiveness programs influential Did knowing about a teacher loan forgiveness program influence you to prepare to become a teacher? 0 = No 1 = Yes Applies to: Respondents who were currently teachers or had taught since completing their bachelor's degree or had prepared to teach, and were aware of either the TEACH grant or a teaching loan forgiveness program. Instrument code: TTEACHR in 1 2 3      Recode note: If RFDEP2 = 0 then RFDEPS = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RFDEP2", "text": "Number of dependents [If RFMARR = 2] How many dependents do you or your spouse support financially? [else] How many dependents do you support financially? Values greater than 7 were replaced with a -6 to indicate the value was out of range. 0 = 0 1 = 1 2 = 2 3 = 3 4 = 4 5 = 5 6 = 6 Applies to: All respondents. Recode note: If RFDEPS = 0 then RFDEP2 = 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview  Do you own a home, pay rent, or both own a home and pay rent? (If someone other than your spouse makes housing payments on your behalf, please answer, \"Neither own a home nor pay rent.\") [else if RFMARR = 3] Do you own a home, pay rent, or both own a home and pay rent? (If someone other than your domestic partner makes housing payments on your behalf, please answer, \"Neither own a home nor pay rent.\") [else] Do you own a home, pay rent, or both own a home and pay rent? (If someone makes housing payments on your behalf, please answer, \"Neither own a home nor pay rent.\") 0 = Neither own home(s) nor pay rent 1 = Own home(s) (pay mortgage) 2 = Pay rent 3 = Both own home(s) and pay rent Applies to: All respondents. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Values between 0 and 50 or greater than 5,000 were replaced with a -6 to indicate the value was out of range. Applies to: Respondents who made rent or mortgage payments. Instrument code: RFHOUSE ne 0 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview Please indicate the range that best estimates your income from all sources for 2008 (including income from work, investments, alimony, etc.) prior to taxes and deductions for calendar year 2008. 1 = Less than $20,000 2 = $20,000-$29,999 3 = $30,000-$39,999 4 = $40,000-$49,999 5 = $50,000-$59,999 6 = $60,000-$69,999 7 = $70,000-$79,999 8 = $80,000-$89,999 9 = $90,000-$99,999 10 = $100,000-$149,999 11 = Above $150,000 Applies to: Respondents who did not provide an income amount. Instrument code: RFINCOM = -9 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RFDAGE", "text": ""}, {"section_title": "RFMTGAMT", "text": ""}, {"section_title": "RFCARAMT", "text": ""}, {"section_title": "RFSPEMP", "text": ""}, {"section_title": "Spouse employed in 2008", "text": "Did your spouse work for pay in calendar year 2008? 0 = No 1 = Yes Applies to: Married respondents. Instrument code: RFMARR = 2 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview"}, {"section_title": "RFINCSP Spouse's income in 2008", "text": "How much would you estimate your spouse earned from all sources (including income from work, investments, alimony, etc.), prior to taxes and deductions, in calendar year 2008? Values between 0 and 100, greater than 250,000, or values equal to 0 when RFSPEMP = 1, were replaced with a -6 to indicate the value was out of range. Applies to: Married respondents who were married to their spouse in 2008. Instrument code: RFMARR = 2 and RFSPNOT ne 1 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Source: B&B:08/09 full scale student interview  We will be contacting \u00absPfname\u00bb and other study participants in July to ask questions about \u00abpronoun3\u00bb education and work experiences after graduation. We are asking for your help in updating our records so that we will be able to get in touch with \u00abpronoun2\u00bb. Only a limited number of people are selected for this study so it is extremely important that we be able to contact \u00abpronoun2\u00bb. If \u00absPfname\u00bb completes the interview by the date provided in the announcement letter we will be sending in July, \u00abpronoun1\u00bb will receive a \u00abIncAmt\u00bb incentive as a token of our appreciation."}, {"section_title": "RFSPNOT", "text": "Before data collection can begin, we need your help to update our records for \u00absPfname\u00bb \u00absPlname\u00bb."}, {"section_title": "Please take a few minutes right now to update the enclosed Address Update Information sheet and return it in the enclosed postage-paid envelope.", "text": "NCES has contracted with RTI International to conduct the B&B study on its behalf. Please be assured that both NCES and RTI follow strict confidentiality procedures to protect the privacy of study participants and the confidentiality of the information collected. If you would like more information about the B&B study, please visit http://surveys.nces.ed.gov/bb/ or call the RTI study director, Dr. Jennifer Wine, toll-free at 1-866-662-8227. We sincerely appreciate your assistance and thank you in advance for helping us conduct this important study. Sincerely, El Centro Nacional de Estad\u00edsticas de la Educaci\u00f3n (NCES, por sus siglas en ingl\u00e9s) del Departamento de Educaci\u00f3n de los Estados Unidos, est\u00e1 realizando un importante estudio sobre estudiantes que se graduaron de la universidad durante el a\u00f1o escolar 2007-2008. El Estudio Longitudinal de Bachillerato y Estudios Posteriores (B&B, por sus siglas en ingl\u00e9s) ayudar\u00e1 a los educadores, as\u00ed como a las personas que realizan estudios y a los encargados de crear pol\u00edticas a entender mejor de qu\u00e9 manera el obtener un t\u00edtulo de bachiller afecta la vida de los estudiantes y las opciones que tienen para continuar su educaci\u00f3n y obtener empleo. De todos los graduados de la universidad durante el a\u00f1o escolar 2007-2008 en los Estados Unidos, \u00absPfname\u00bb fue seleccionado(a) para participar en el Estudio Longitudinal de Bachillerato y Estudios Posteriores. Nosotros nos estaremos comunicando con \u00absPfname\u00bb y otros participantes del estudio en julio para hacerles preguntas sobre sus experiencias con la educaci\u00f3n y el empleo despu\u00e9s de la graduaci\u00f3n. Le pedimos su ayuda para actualizar nuestros registros de manera que podamos comunicarnos con \u00abspan_pronoun2\u00bb. S\u00f3lo se seleccion\u00f3 a un n\u00famero limitado de personas para este estudio. Por lo tanto, es sumamente importante que nos podamos comunicar con \u00abspan_pronoun2\u00bb. Si \u00absPfname\u00bb completa la entrevista a m\u00e1s tardar en la fecha que se indica en la carta que enviaremos en julio anunciando la recopilaci\u00f3n de datos del estudio, \u00abspan_pronoun1\u00bb recibir\u00e1 un incentivo por \u00abIncAmt\u00bb d\u00f3lares, como muestra de nuestro agradecimiento. Antes de poder empezar la recopilaci\u00f3n de datos, necesitamos su ayuda para actualizar nuestros registros con respecto a \u00absPfname\u00bb. Por favor, tome unos minutos de su tiempo en este momento para actualizar el Formulario de actualizaci\u00f3n del domicilio que se adjunta a la presente y env\u00edelo de regreso en el sobre con porte postal prepagado que tambi\u00e9n adjuntamos. El Centro Nacional de Estad\u00edsticas de la Educaci\u00f3n ha contratado a RTI International para que realice el Estudio Longitudinal de Bachillerato y Estudios Posteriores en su nombre. Le aseguramos que tanto RTI como El Centro Nacional de Estad\u00edsticas de la Educaci\u00f3n siguen procedimientos estrictos de confidencialidad para proteger la privacidad de los participantes del estudio y la confidencialidad de la informaci\u00f3n que se recopile. Si usted desea m\u00e1s informaci\u00f3n sobre el estudio, por favor, visite el sitio de Internet http://surveys.nces.ed.gov/bb/ (disponible en ingl\u00e9s) o llame a la directora del estudio en RTI, Dra. Jennifer Wine, al n\u00famero de tel\u00e9fono gratuito 1-866-662-8227. Apreciamos sinceramente su ayuda y le agradecemos de antemano por su ayuda a realizar este importante estudio. We will be contacting \u00abfname\u00bb and other study participants in October to ask questions about \u00abpronoun3\u00bb education and work experiences after graduation. We are asking for your help in updating our records so that we will be able to get in touch with \u00abpronoun2\u00bb. Only a limited number of people are selected for this study so it is extremely important that we be able to contact \u00abpronoun2\u00bb. If \u00abfname\u00bb completes the interview by the date provided in the announcement letter we will be sending in October, \u00abpronoun1\u00bb will receive a $50 incentive as a token of our appreciation. Before data collection can begin, we need your help to update our records for \u00abfname\u00bb \u00ablname\u00bb. Please take a few minutes right now to update the enclosed Address Update Information sheet and return it in the enclosed postage-paid envelope. NCES has contracted with RTI International to conduct the B&B study on its behalf. Please be assured that both NCES and RTI follow strict confidentiality procedures to protect the privacy of study participants and the confidentiality of the information collected. If you would like more information about the B&B study, please visit http://surveys.nces.ed.gov/bb/ or call the RTI study director, Dr. Jennifer Wine, toll-free at 1-866-662-8227. We sincerely appreciate your assistance and thank you in advance for helping us conduct this important study. Sincerely, El Centro Nacional de Estad\u00edsticas de la Educaci\u00f3n (NCES, por sus siglas en ingl\u00e9s) del Departamento de Educaci\u00f3n de los Estados Unidos, est\u00e1 realizando un importante estudio sobre estudiantes que se graduaron de la universidad durante el a\u00f1o escolar 2007-2008. El Estudio Longitudinal de Bachillerato y Estudios Posteriores (B&B, por sus siglas en ingl\u00e9s) ayudar\u00e1 a los educadores, as\u00ed como a las personas que realizan estudios y a los encargados de crear pol\u00edticas a entender mejor de qu\u00e9 manera el obtener un t\u00edtulo de bachiller afecta la vida de los estudiantes y las opciones que tienen para continuar su educaci\u00f3n y obtener empleo. De todos los graduados de la universidad durante el a\u00f1o escolar 2007-2008 en los Estados Unidos, \u00absPfname\u00bb fue \u00abspan_fill2\u00bb para participar en el Estudio Longitudinal de Bachillerato y Estudios Posteriores. Nosotros nos estaremos comunicando con \u00absPfname\u00bb y otros participantes del estudio en octubre para hacerles preguntas sobre sus experiencias con la educaci\u00f3n y el empleo despu\u00e9s de la graduaci\u00f3n. Le pedimos su ayuda para actualizar nuestros registros de manera que podamos comunicarnos con \u00abspan_pronoun2\u00bb. S\u00f3lo se seleccion\u00f3 a un n\u00famero limitado de personas para este estudio. Por lo tanto, es sumamente importante que nos podamos comunicar con \u00abspan_pronoun2\u00bb. Si \u00absPfname\u00bb completa la entrevista a m\u00e1s tardar en la fecha que se indica en la carta que enviaremos en octubre anunciando la recopilaci\u00f3n de datos del estudio, \u00abspan_pronoun1\u00bb recibir\u00e1 un incentivo por $50 d\u00f3lares, como muestra de nuestro agradecimiento. Antes de poder empezar la recopilaci\u00f3n de datos, necesitamos su ayuda para actualizar nuestros registros con respecto a \u00absPfname\u00bb \u00absPlname\u00bb. Por favor, tome unos minutos de su tiempo en este momento para actualizar el Formulario de actualizaci\u00f3n del domicilio que se adjunta a la presente y env\u00edelo de regreso en el sobre con porte postal prepagado que tambi\u00e9n adjuntamos. El Centro Nacional de Estad\u00edsticas de la Educaci\u00f3n ha contratado a RTI International para que realice el Estudio Longitudinal de Bachillerato y Estudios Posteriores en su nombre. Le aseguramos que tanto RTI como El Centro Nacional de Estad\u00edsticas de la Educaci\u00f3n siguen procedimientos estrictos de confidencialidad para proteger la privacidad de los participantes del estudio y la confidencialidad de la informaci\u00f3n que se recopile. Si usted desea m\u00e1s informaci\u00f3n sobre el estudio, por favor, visite el sitio de Internet http://surveys.nces.ed.gov/bb/ (disponible en ingl\u00e9s) o llame a la directora del estudio en RTI, Dra. Jennifer Wine, al n\u00famero de tel\u00e9fono gratuito 1-866-662-8227. Apreciamos sinceramente su ayuda y le agradecemos de antemano por su ayuda a realizar este importante estudio. We are now asking for your help again. This summer we will be contacting you to ask some follow up questions as part of the Baccalaureate and Beyond Longitudinal Study (B&B). Data collected from B&B will help educators, researchers, and policymakers better understand how earning a bachelor's degree affects students' lives and choices about further education and work. When B&B data collection begins in July, you will receive a letter in a large white envelope that will provide specific information about how you can participate. The package will also include a $5 bill as a token of our appreciation for your participation. The letter will explain that if you complete the 25 minute interview on the Web by the date indicated, you will receive an additional $30 as a token of our appreciation. In the meantime, we need to update our contact information for you. Please help us by providing your mailing address, telephone numbers, and e-mail addresses on the enclosed address update sheet and returning it in the enclosed postage-paid envelope. To find out more about the B&B study or to update your contact information online, visit the study's website at https://surveys.nces.ed.gov/bb/ and enter the Study ID number provided in this letter. NCES has contracted with RTI International to conduct the B&B study on its behalf. The enclosed brochure provides a brief description of B&B, findings from the 2001 B&B study, and a summary of our strict confidentiality procedures. If you have additional questions or concerns about the study after reviewing this material, please call the RTI study director, Dr. Jennifer Wine, toll-free at 1-866-662-8227. We thank you in advance for your participation in this important study. Your cooperation is greatly appreciated. Sincerely, Ahora necesitamos su ayuda nuevamente. Este verano nos estaremos comunicando con usted para hacerle algunas preguntas de seguimiento como parte del Estudio Longitudinal de Bachillerato y Estudios Posteriores (B&B). Los datos obtenidos este estudio ayudar\u00e1n a los educadores, as\u00ed como a las personas que realizan estudios y a los encargados de crear pol\u00edticas a entender mejor de qu\u00e9 manera el obtener un t\u00edtulo de bachiller afecta la vida de los estudiantes y las opciones que tienen para continuar su educaci\u00f3n y obtener empleo. Cuando la recopilaci\u00f3n de datos del estudio B&B empiece en julio, usted recibir\u00e1 una carta en un sobre grande de color blanco que le proporcionar\u00e1 informaci\u00f3n espec\u00edfica acerca de c\u00f3mo puede participar. El sobre tambi\u00e9n incluir\u00e1 un billete de $5 d\u00f3lares como muestra de nuestro agradecimiento por su participaci\u00f3n. La carta le explicar\u00e1 que si completa la entrevista de 25 minutos en el Internet hasta la fecha indicada, usted recibir\u00e1 $30 d\u00f3lares adicionales como muestra de nuestro agradecimiento. Mientras tanto, necesitamos actualizar la informaci\u00f3n de c\u00f3mo comunicarnos con usted. Por favor, le pedimos su ayuda al proporcionarnos su direcci\u00f3n, n\u00fameros de tel\u00e9fono y direcciones de correo electr\u00f3nico en el formulario de actualizaci\u00f3n del domicilio que adjuntamos y que lo devuelva en el sobre adjunto con porte postal prepagado. Para obtener m\u00e1s informaci\u00f3n sobre el estudio B&B o para actualizar sus datos personales en el Internet, puede visitar el sitio web del estudio https://surveys.nces.ed.gov/bb/ y escribir el n\u00famero de identificaci\u00f3n del estudio que se proporciona en esta carta. El Centro Nacional de Estad\u00edsticas de la Educaci\u00f3n ha contratado a RTI International para realizar el Estudio Longitudinal de Bachillerato y Estudios Posteriores en su nombre. El folleto adjunto le proporciona una breve descripci\u00f3n del estudio B&B, as\u00ed como los hallazgos del estudio en el a\u00f1o 2001 y un resumen de nuestros procedimientos estrictos de confidencialidad. Si tiene alguna pregunta o preocupaci\u00f3n adicional acerca del estudio despu\u00e9s de haber revisado este material, por favor, llame a la directora del estudio, Dra. Jennifer Wine, al n\u00famero de tel\u00e9fono gratuito 1-866-662-8227. Le agradecemos de antemano por su participaci\u00f3n en este importante estudio. Apreciamos sinceramente su cooperaci\u00f3n. Data collected from B&B will help educators, researchers, and policymakers better understand how earning a bachelor's degree affects students' lives and choices about further education and work. Only a limited number of people are selected for this study, so your participation is extremely important. When B&B data collection begins in October, you will receive a letter in a large white envelope that will provide specific information about how to participate. That package will also include a $5 bill as a token of our appreciation for your participation. The letter will explain that if you complete the 25-minute interview on the Web by the date indicated, you will receive an additional $50 as a token of our appreciation. In the meantime, we need to update our contact information for you. Please help us by providing your mailing address, telephone number(s), and e-mail address(es) on the enclosed address update sheet and returning it in the enclosed postage-paid envelope. To update your information online, or to find out more about B&B, visit the study's website at https://surveys.nces.ed.gov/bb/. NCES has contracted with RTI International to conduct B&B on its behalf. The enclosed brochure provides a brief description of B&B, findings from the 2001 study, and a summary of our strict confidentiality procedures. If you have additional questions or concerns about the study after reviewing this material, please call the RTI study director, Dr. Jennifer Wine, toll-free at 1-866-662-8227. We thank you in advance for your participation in this important study. Your cooperation is greatly appreciated. Sincerely, \u00abstu_name\u00bb N\u00famero de identificaci\u00f3n del estudio: \u00abcaseid\u00bb \u00abaddr1\u00bb \u00abaddr2\u00bb \u00abcitystzip\u00bb \u00abspan_fill\u00bb \u00abfname\u00bb, Usted ha sido seleccionado(a) al azar para tomar parte en el Estudio Longitudinal de Bachillerato y Estudios Posteriores (B&B), el cual es patrocinado por el Centro Nacional de Estad\u00edsticas sobre la Educaci\u00f3n (NCES) en el Instituto de Ciencias de la Educaci\u00f3n del Departamento de Educaci\u00f3n de los Estados Unidos. El estudio B&B recopilar\u00e1 informaci\u00f3n sobre la educaci\u00f3n, empleo y otra informaci\u00f3n acerca de usted y estudiantes como usted que se graduaron de la universidad durante el a\u00f1o acad\u00e9mico 2007-2008. Los datos obtenidos del estudio B&B ayudar\u00e1n a los educadores, a las personas que realizan estudios y a los legisladores a entender mejor de qu\u00e9 manera el obtener un t\u00edtulo de bachillerato afecta las vidas de los estudiantes y las opciones que tienen de continuar con su educaci\u00f3n y obtener un empleo. Solamente un n\u00famero limitado de personas son seleccionadas para este estudio, de modo que su participaci\u00f3n es sumamente importante. Cuando la recopilaci\u00f3n de datos del estudio B&B empiece en octubre, usted recibir\u00e1 una carta en un sobre grande de color blanco que le proporcionar\u00e1 informaci\u00f3n espec\u00edfica acerca de c\u00f3mo participar. Este sobre tambi\u00e9n incluir\u00e1 un billete de $5 d\u00f3lares como muestra de nuestro agradecimiento por su participaci\u00f3n. La carta le explicar\u00e1 que si usted completa la entrevista de 25 minutos en el Internet hasta la fecha indicada, usted recibir\u00e1 $50 d\u00f3lares adicionales como muestra de nuestro agradecimiento. Mientras tanto, necesitamos actualizar la informaci\u00f3n de c\u00f3mo comunicarnos con usted. Por favor, le pedimos su ayuda al proporcionarnos su direcci\u00f3n, n\u00fameros de tel\u00e9fono y direcciones de correo electr\u00f3nico en el formulario de actualizaci\u00f3n del domicilio que adjuntamos y que lo devuelva en el sobre adjunto con porte postal prepagado. Para obtener m\u00e1s informaci\u00f3n sobre el estudio B&B puede visitar el sitio web del estudio en https://surveys.nces.ed.gov/bb/ . El Centro Nacional de Estad\u00edsticas sobre la Educaci\u00f3n ha contratado a RTI International para realizar el Estudio Longitudinal de Bachillerato y Estudios Posteriores en su nombre. El folleto adjunto le proporciona una breve descripci\u00f3n del estudio B&B, as\u00ed como los hallazgos del estudio en el a\u00f1o 2001 y un resumen de nuestros procedimientos estrictos de confidencialidad. Si tiene alguna pregunta o preocupaci\u00f3n adicional acerca del estudio despu\u00e9s de haber revisado este material, por favor, llame a la directora del estudio, Dra. Jennifer Wine, al n\u00famero de tel\u00e9fono gratuito 1-866-662-8227. Le agradecemos de antemano por su participaci\u00f3n en este importante estudio. Apreciamos sinceramente su cooperaci\u00f3n. Interviews for the Baccalaureate and Beyond Longitudinal Study (B&B) are now being conducted. The interview will take about 25 minutes to complete. As a token of our appreciation for your participation in the study, we have enclosed a $5 bill. If you complete the interview by \u00abdate\u00bb, we will mail you an additional $\u00abIncAmt\u00bb check. You may access the B&B interview by logging on to our secure website at https://surveys.nces.ed.gov/bb/ using the Study ID and password provided below. The password is case sensitive; you will need to enter it exactly as it appears here. Study ID = \u00abcaseid\u00bb Password = \u00abpassword\u00bb Enclosed you will find a brochure that provides a brief description of B&B, findings from the 2001 study, and our strict confidentiality procedures. Federal law requires that we protect your privacy. Your responses will be secured behind firewalls and will be encrypted during internet transmission. Your responses will be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. If you have questions, problems completing your interview online, or prefer to complete the interview over the telephone, simply call the B&B Help Desk toll-free at 1-877-262-4440. Your participation, while voluntary, is critical to the study's success. If you have any other questions or concerns about the study, please contact the B&B Project Director, Dr. Jennifer Wine, toll-free at 1-866-662-8227, jennifer@rti.org, or the NCES Project Officer, Mr. Ted Socha, at 1-202-502-7383, ted.socha@ed.gov. NCES will authorize only a limited number of researchers to have access to information that could be used to identify individuals. They may use the data for statistical purposes only and are subject to fines and imprisonment for misuse. According to the Paperwork Reduction Act of 1995, no persons are required to respond to a collection of information unless it displays a valid OMB control number. The valid OMB control number of this information collection is 1850-0729, and it is completely voluntary. The time required to complete this information collection is estimated to average 25 minutes per response, including the time to review instructions, search existing data resources, gather the data needed, and complete and review the information collection. If you have any comments concerning the accuracy of the time estimate or suggestions for improving the interview, please write to: U.S. Department of Education, 400 Maryland Avenue SW, Washington, DC 20006. If you have comments or concerns regarding the status of your individual interview, write directly to: Mr. Ted Socha, National Center for Education Statistics, 1990 K Street, NW, Washington, DC 20006. Estimado(a) \u00abfname\u00bb, Actualmente se est\u00e1n realizando las entrevistas para el Estudio Longitudinal de Bachillerato y Estudios Posteriores (B&B). La entrevista tomar\u00e1 alrededor de 25 minutos en completarse. Como muestra de nuestro agradecimiento por su participaci\u00f3n en el estudio, adjuntamos a la presente un billete de $5 d\u00f3lares. Si usted completa la entrevista a m\u00e1s tardar el \u00abdate\u00bb, nosotros le enviaremos por correo un cheque adicional por $\u00abIncAmt\u00bb d\u00f3lares. Usted puede tener acceso a la entrevista del estudio B&B al entrar en nuestro sitio de Internet seguro https://surveys.nces.ed.gov/bb/ al usar el n\u00famero de identificaci\u00f3n del estudio y la contrase\u00f1a que se muestra a continuaci\u00f3n. La contrase\u00f1a distingue letras may\u00fasculas y min\u00fasculas, de manera que necesitar\u00e1 escribirla exactamente como aparece aqu\u00ed. N\u00famero de identificaci\u00f3n del estudio = \u00abcaseid\u00bb Contrase\u00f1a = \u00abpassword\u00bb Adjuntamos a la presente un folleto que contiene una breve descripci\u00f3n del estudio B&B, as\u00ed como los hallazgos del estudio del a\u00f1o 2001 y nuestros procedimientos estrictos de confidencialidad. La ley federal requiere que nosotros protejamos su privacidad. Sus respuestas ser\u00e1n protegidas con un sistema de seguridad (\"firewall\") y ser\u00e1n codificadas al momento de ser transmitidas a trav\u00e9s del Internet. Sus respuestas se usar\u00e1n s\u00f3lo con prop\u00f3sitos estad\u00edsticos y no ser\u00e1n divulgadas o utilizadas en ninguna forma que lo/la identifique para ning\u00fan otro prop\u00f3sito, excepto cuando sea requerido por la ley. Si usted tiene preguntas o problemas para completar su entrevista en Internet, o si prefiere completar la entrevista por tel\u00e9fono, simplemente llame gratis a la Oficina de ayuda del estudio B&B, al 1-877-262-4440. Su participaci\u00f3n, aunque voluntaria, es esencial para el \u00e9xito del estudio. Si usted tiene alguna otra pregunta o preocupaci\u00f3n acerca del estudio, por favor, comun\u00edquese con la Directora del proyecto B&B, la Dra. Jennifer Wine, al n\u00famero de tel\u00e9fono gratuito 1-866-662-8227, o tambi\u00e9n puede enviarle un correo electr\u00f3nico a: jennifer@rti.org. Tambi\u00e9n puede comunicarse con el Funcionario del proyecto en el Centro Nacional de Estad\u00edsticas sobre la Educaci\u00f3n (NCES), el Sr. Ted Socha, al 1-202-502-7383, o a trav\u00e9s de su correo electr\u00f3nico: ted.socha@ed.gov. Le agradecemos de antemano por hacer que el Estudio Longitudinal de Bachillerato y Estudios Posteriores (B&B) sea todo un \u00e9xito. El Centro Nacional de Estad\u00edsticas sobre la Educaci\u00f3n (NCES) del Departamento de Educaci\u00f3n de los Estados Unidos est\u00e1 autorizado por ley federal (Ley P\u00fablica 107-279) para realizar el Estudio Longitudinal de Bachillerato y Estudios Posteriores (B&B). El NCES autorizar\u00e1 solamente a un n\u00famero limitado de estudiosos a tener acceso a informaci\u00f3n que pudiera ser utilizada para identificar a las personas. Los estudiosos pueden utilizar los datos solamente para prop\u00f3sitos estad\u00edsticos y est\u00e1n sujetos a multas y encarcelamiento en caso de mala utilizaci\u00f3n. De acuerdo a la Ley de Reducci\u00f3n de Trabajo Administrativo de 1995, ninguna persona tiene la obligaci\u00f3n de responder a un cuestionario que solicite informaci\u00f3n, a menos que lleve un n\u00famero de control de OMB (Oficina de Administraci\u00f3n y Presupuesto) v\u00e1lido. El n\u00famero v\u00e1lido de control otorgado por el OMB para esta recopilaci\u00f3n de datos es el 1850-0631 y esta recopilaci\u00f3n de datos es completamente voluntaria. Se calcula que el tiempo promedio para contestar cada cuestionario es de 25 minutos, incluyendo el tiempo para revisar las instrucciones, buscar la informaci\u00f3n, juntar los datos necesarios, completar y revisar la informaci\u00f3n recopilada. Si tiene alg\u00fan comentario acerca de la exactitud del tiempo estimado o sugerencias para mejorar la entrevista, favor de escribir a: U.S. Department of Education, 400 Maryland Avenue SW, Washington, DC 20006. Si tiene comentarios o dudas con respecto a su entrevista particular, favor de escribir directamente a: Mr. Ted Socha, National Center for Education Statistics, 1990 K Street, NW, Washington, DC 20006. The National Center for Education Statistics (NCES) of the U.S. Department of Education is authorized by federal law (Public Law 107-279) to conduct the Baccalaureate and Beyond Longitudinal Study (B&B). NCES will authorize only a limited number of researchers to have access to information that could be used to identify individuals. They may use the data for statistical purposes only and are subject to fines and imprisonment for misuse. According to the Paperwork Reduction Act of 1995, no persons are required to respond to a collection of information unless it displays a valid OMB control number. The valid OMB control number of this information collection is 1850-0729, and it is completely voluntary. The time required to complete this information collection is estimated to average 25 minutes per response, including the time to review instructions, search existing data resources, gather the data needed, and complete and review the information collection. If you have any comments concerning the accuracy of the time estimate or suggestions for improving the interview, please write to: U.S. Department of Education, 400 Maryland Avenue SW, Washington, DC 20006. If you have comments or concerns regarding the status of your individual interview, write directly to Mr. Ted Socha, National Center for Education Statistics, 1990 K Street NW, Washington, DC 20006. \u00abspan_fill\u00bb \u00abCpfname\u00bb, \u00abfname\u00bb \u00ablname\u00bb ha sido \u00abspan_fill1\u00bb para participar en el Estudio Longitudinal de Bachillerato y Estudios Posteriores (B&B), el cual est\u00e1 realizando RTI International para el Centro Nacional de Estad\u00edsticas sobre la Educaci\u00f3n (NCES) del Instituto de Ciencias de la Educaci\u00f3n del Departamento de Educaci\u00f3n de los Estados Unidos. Usted nos puede ayudar a que el estudio sea un \u00e9xito al animar a \u00abfname\u00bb a completar la entrevista del estudio B&B a m\u00e1s tardar el \u00abdate\u00bb. Si la entrevista se completa a m\u00e1s tardar el \u00abdate\u00bb, \u00abfname\u00bb recibir\u00e1 la cantidad de \u00abIncAmt\u00bb d\u00f3lares como muestra de nuestro agradecimiento. Los datos obtenidos durante el estudio B&B ayudar\u00e1n a los educadores, as\u00ed como a las personas que realizan estudios y a los encargados de crear pol\u00edticas a entender mejor de qu\u00e9 manera el obtener un t\u00edtulo de bachillerato afecta la vida de los estudiantes y la transici\u00f3n que pasan hacia el campo laboral y a los estudios de post grado. La entrevista trata de temas como las experiencias de \u00abfname\u00bb en el campo laboral; ganancias y gastos; participaci\u00f3n en actividades c\u00edvicas; as\u00ed como metas personales, profesionales y educativas. Adjuntamos a la presente un folleto que proporciona un descripci\u00f3n general de los hallazgos del estudio B&B en el a\u00f1o 2001 as\u00ed como nuestros procedimientos estrictos de confidencialidad. La participaci\u00f3n de \u00abfname\u00bb en este estudio, aunque voluntria, es esencial para el \u00e9xito del estudio. Si tiene alguna pregunta o preocupaci\u00f3n acerca del estudio, favor de comunicarse con la Directora del proyecto B&B, la Dra. Jennifer Wine, al n\u00famero de tel\u00e9fono gratuito 1-866-662-8227, o tambi\u00e9n puede enviarle un correo electr\u00f3nico a: jennifer@rti.org. Tambi\u00e9n puede comunicarse con el Funcionario del proyecto en el Centro Nacional de Estad\u00edsticas sobre la Educaci\u00f3n (NCES), el Sr. Ted Socha, al 1-202-502-7383, o a trav\u00e9s de su correo electr\u00f3nico: ted.socha@ed.gov. Apreciamos sinceramente su ayuda y le damos las gracias de antemano por ayudarnos en realizar este importante estudio. El Centro Nacional de Estad\u00edsticas sobre la Educaci\u00f3n (NCES) del Departamento de Educaci\u00f3n de los Estados Unidos est\u00e1 autorizado por ley federal (Ley P\u00fablica 107-279) para realizar el Estudio Longitudinal de Bachillerato y Estudios Posteriores (B&B). El NCES autorizar\u00e1 solamente a un n\u00famero limitado de estudiosos a tener acceso a informaci\u00f3n que pudiera ser utilizada para identificar a las personas. Los estudiosos pueden utilizar los datos solamente para prop\u00f3sitos estad\u00edsticos y est\u00e1n sujetos a multas y encarcelamiento en caso de mala utilizaci\u00f3n. De acuerdo a la Ley de Reducci\u00f3n de Trabajo Administrativo de 1995, ninguna persona tiene la obligaci\u00f3n de responder a un cuestionario que solicite informaci\u00f3n, a menos que lleve un n\u00famero de control de OMB (Oficina de Administraci\u00f3n y Presupuesto) v\u00e1lido. El n\u00famero v\u00e1lido de control otorgado por el OMB para esta recopilaci\u00f3n de datos es el 1850-0729 y esta recopilaci\u00f3n de datos es completamente voluntaria. Se calcula que el tiempo promedio para contestar cada cuestionario es de 25 minutos, incluyendo el tiempo para revisar las instrucciones, buscar la informaci\u00f3n, juntar los datos necesarios, completar y revisar la informaci\u00f3n recopilada. Si tiene alg\u00fan comentario acerca de la exactitud del tiempo estimado o sugerencias para mejorar la entrevista, favor de escribir a: U.S. Department of Education, 400 Maryland Avenue SW, Washington, DC 20006. Si tiene comentarios o dudas con respecto a su entrevista particular, favor de escribir directamente a: Mr. Ted Socha, National Center for Education Statistics, 1990 K Street, NW, Washington, DC 20006. \u00abfname\u00bb \u00abmname\u00bb \u00ablname\u00bb N\u00famero de identificaci\u00f3n del estudio: \u00abcaseid\u00bb \u00abaddr1\u00bb \u00abaddr2\u00bb \u00abcity\u00bb, \u00abstate\u00bb \u00abzip\u00bb Estimado(a) \u00abfname\u00bb, A medida que continuamos con la recopilaci\u00f3n de datos para el Estudio Longitudinal de Bachillerato y Estudios Posteriores (B&B, por sus siglas en ingl\u00e9s), queremos asegurarnos que todos los estudiantes seleccionados del a\u00f1o acad\u00e9mico 2007-08, incluyendo los estudiantes que principalmente hablan espa\u00f1ol, est\u00e9n bien representados en el estudio. Con el fin de logar este objetivo, hemos creado una versi\u00f3n de la entrevista en espa\u00f1ol, la cual toma m\u00e1s o menos 10 minutos en completarse. Para completar la entrevista en espa\u00f1ol en Internet, entre en nuestro sitio web seguro en https://surveys.nces.ed.gov/bb/ y use el n\u00famero de identificaci\u00f3n del estudio y la contrase\u00f1a que se proporcionan a continuaci\u00f3n. Para completar la entrevista por tel\u00e9fono con un(a) entrevistador(a) profesional, por favor, llame gratis a la Oficina de ayuda del estudio B&B al 1-877-262-4440. N\u00famero de identificaci\u00f3n del estudio = \u00abcaseid\u00bb Contrase\u00f1a = \u00abpassword\u00bbs La contrase\u00f1a distingue letras may\u00fasculas y min\u00fasculas; usted necesitar\u00e1 escribirla exactamente como se muestra aqu\u00ed. Cuando la pantalla de la entrevista se abra, presione el bot\u00f3n que dice \"Para espa\u00f1ol, presione aqu\u00ed\" y luego presione el bot\u00f3n que dice \"Comienzo\". Si completa la entrevista B&B, usted recibir\u00e1 un cheque por $\u00abIncAmt\u00bb d\u00f3lares como muestra de nuestro agradecimiento. El estudio B&B lo realiza RTI International para el Centro Nacional de Estad\u00edsticas sobre la Educaci\u00f3n (NCES) en el Instituto de Ciencias de la Educaci\u00f3n del Departamento de Educaci\u00f3n de los Estados Unidos. La participaci\u00f3n en este estudio es voluntaria y no afectar\u00e1 ninguna ayuda o ning\u00fan beneficio que usted reciba. Sus respuestas se usar\u00e1n s\u00f3lo con prop\u00f3sitos estad\u00edsticos y no ser\u00e1n divulgadas o utilizadas en ninguna forma que se le pueda identificar para cualquier otro prop\u00f3sito, excepto cuando lo requiera la ley. Si tiene alguna pregunta acerca del estudio, puede visitar el sito web del estudio B&B o nos puede enviar un mensaje por correo electr\u00f3nico a bbemail@rti.org. Si tiene alguna pregunta acerca de sus derechos como participante en el estudio, puede llamar a la Oficina de RTI para la Protecci\u00f3n de Participantes en Estudios al 1-866-214-2043 (n\u00famero gratuito). Gracias de antemano por hacer del estudio B&B todo un \u00e9xito. You may be wondering why we've been trying to contact you regarding \u00abfname\u00bb \u00ablname\u00bb's participation in the U.S. Department of Education's Baccalaureate and Beyond Longitudinal Study (B&B). B&B will help researchers and policymakers better understand how earning a bachelor's degree <<in Major>> can affect new graduates as they transition into the next phases of their lives. Last year, \u00abfname\u00bb participated in the 2007-2008 and agreed to participate in the B&B follow-up study in 2009. We are writing you because you were listed as someone who could help us contact <fname>. Due to the strict statistical procedures required, the study's success depends on the participation of its selected sample members. Participation is so important that <fname> will receive \u00abIncAmt\u00bb as a token of our appreciation for completing the 25-minute interview. There are two important ways you can help ensure the success of the study. 1. Providing updated contact information for \u00abfname\u00bb by calling us toll-free at 1-877-262-4440 or by visiting our website at https://surveys.nces.ed.gov/bb/ and clicking on \"Provide Address Update.\" You will need to provide the Study ID number <<Study ID>>. 2. Encouraging \u00abfname\u00bb to complete the B&B interview by calling one of our professionally trained telephone interviewers at 1-877-262-4440. Or, <Fname> can obtain the login information needed to complete the online interview by calling the B&B Help Desk toll-free at 1-877-262-4440 or emailing us at bbemail@rti.org. Included is a brochure that provides the answers to the most frequently asked questions. If you have additional questions about B&B, please contact the B&B Project Director, Dr. Jennifer Wine, toll-free at 1-866-662-8227, jennifer@rti.org, or the NCES Project Officer, Ted Socha, at 1-202-502-7383, ted.socha@ed.gov. Publications and additional information about past B&B studies can be found at http://nces.ed.gov/surveys/b&b/. Sciences. NPSAS answered a lot of questions about how students pay for college. We are now asking for your help again. We are contacting you to ask some follow-up questions as part of the Baccalaureate and Beyond Longitudinal Study (B&B). Data collected from B&B will help educators, researchers, and policymakers better understand how earning a bachelor's degree affects students' lives and choices about further education and work."}, {"section_title": "Generic Lead Letter", "text": "Enclosed you will find a brochure that provides a brief description of B&B, findings from the 2001 study, and our strict confidentiality procedures. Federal law requires that we protect your privacy. Your participation in the B&B is critical to the success of the study. We would like you to complete a 20minute interview with our field interviewer; the interview can be arranged at a time convenient for you. All of your responses will be kept confidential and will be protected to the fullest extent allowable under law. When you complete your interview, we will pay you ______ as a token of our appreciation. Thank you for helping to make B&B a success. If you would like to schedule an appointment to complete the interview, please call our field interviewer, _________________________, at _________________________ (call collect, if long distance), or you may call his/her supervisor toll-free at _______________________. Please do not hesitate to call me toll-free at 1-866-662-8227or to e-mail me at jennifer@rti.org if I can provide any additional information about the study. B&B is conducted by RTI international for the National Center for Education Statistics (NCES) in the U.S. Department of Education's Institute of Education Sciences. Thank you for your time and willingness to participate. El a\u00f1o pasado usted particip\u00f3 en el Estudio Nacional sobre Asistencia Econ\u00f3mica para Estudiantes en Escuelas Postsecundarias (NPSAS) para el Centro Nacional de Estad\u00edsticas sobre la Educaci\u00f3n (NCES), del Instituto de Ciencias de la Educaci\u00f3n del Departamento de Educaci\u00f3n de los Estados Unidos. El estudio NPSAS respondi\u00f3 muchas preguntas sobre c\u00f3mo los estudiantes pagan la universidad. Ahora le estamos pidiendo su ayuda una vez m\u00e1s. Nos estamos comunicando con usted para hacer unas preguntas de seguimiento como parte del Estudio Longitudinal de Bachillerato y Estudios Posteriores (B&B). Los datos recopilados por el estudio B&B ayudar\u00e1n a los educadores, a las personas encargadas de realizar estudios y a los legisladores, a entender mejor c\u00f3mo el obtener un t\u00edtulo de bachillerato afecta las vidas de los estudiantes y sus opciones sobre educaci\u00f3n posterior y empleo. Adjuntamos a la presente un folleto que proporciona una breve descripci\u00f3n del estudio B&B, as\u00ed como los hallazgos del estudio en el a\u00f1o 2001 y nuestros procedimientos estrictos de confidencialidad. La ley federal requiere que nosotros protejamos su privacidad. Su participaci\u00f3n en el estudio B&B es esencial para el \u00e9xito del estudio. Nos gustar\u00eda que completara una entrevista de 20 minutos de duraci\u00f3n con nuestro(a) entrevistador(a). La entrevista puede fijarse para un d\u00eda y hora que sean convenientes para usted. Todas sus respuestas se mantendr\u00e1n en forma confidencial y se proteger\u00e1n hasta donde lo permita la ley. Una vez que usted haya completado la entrevista, le pagaremos ______ d\u00f3lares como muestra de nuestro agradecimiento. Gracias por ayudar a lograr el \u00e9xito del estudio B&B. Si usted desea hacer una cita para completar la entrevista, por favor, llame a nuestro(a) entrevistador(a) _________________________, al _________________________ (puede llamar por cobrar si la llamada es de larga distancia) o puede llamar al supervisor al n\u00famero de tel\u00e9fono gratuito _______________________. Si necesita mayor informaci\u00f3n que yo pueda proporcionarle acerca del estudio, por favor, no dude en llamarme al n\u00famero de tel\u00e9fono gratuito 1-866-662-8227 o enviarme un mensaje de correo electr\u00f3nico a jennifer@rti.org. El estudio B&B lo realiza RTI International para el Centro Nacional de Estad\u00edsticas sobre la Educaci\u00f3n (NCES) del Instituto de Ciencias de la Educaci\u00f3n del Departamento de Educaci\u00f3n de los Estados Unidos. Gracias por su tiempo y por estar dispuesto(a) a participar en el estudio. Authorized by federal law (P.L. 103-382), the B&B:08/09 data will provide researchers, educators, and policymakers with critical information about the role that an undergraduate education plays in the short-and longterm outcomes of bachelor's degree recipients, while the BPS:04/09 data will help them better understand what percentage of beginning students complete their degree programs; the financial, family, and school related factors that prevent students from completing their programs; and what can be done to help students complete their schooling. I am writing to request transcripts for {sch_num} sample member{s} who attended your institution. To facilitate our coding of your institution's transcripts, it would be helpful to also receive a mapping of your institution's degree programs and courses to the Classification of Instructional Programs, if such a mapping exists. Included with this package are detailed instructions for preparing and transmitting transcript data to RTI. A list of students for whom transcripts are requested is posted, with their student ID numbers and dates of birth, at the secure study website listed in the box below. To gain access to the site, you will need to log in using the username and password printed at the bottom of this letter. Also available at the website are instructions for obtaining reimbursement for the requested transcripts, should your institution require it. We would appreciate receiving the requested transcript data on or before {DUE DATE}. Please do not hesitate to call me if you feel you need to have a later delivery date. from the website: http://www.securezip.com. Detailed instructions for downloading and using SecureZIP can be accessed from the link \"E-mail transcript data as an encrypted attachment.\" If you choose to use encryption software other than SecureZIP, please contact the Help Desk to ensure that the software complies with our security standards. Once the attachment is encrypted, send to pets@rti.org and include the file names and descriptions. If you need assistance with this process, please contact the Help Desk toll-free at 1-877-256-8029 or via e-mail at pets@rti.org. Sending Transcript Data by Secure File Transfer Protocol (sFTP): If you choose to submit transcript data using the sFTP site, please contact the Help Desk to obtain the sFTP site location and your username and password. The Help Desk may be reached toll-free at 1-877-256-8029 or via e-mail at pets@rti.org. Additional instructions on how to perform the data transfer via sFTP may be accessed from the link \"Send transcript data by secure File Transfer Protocol (sFTP).\" After you have successfully uploaded your files to the sFTP server, please send an e-mail to pets@rti.org with the names of the files and their structure. Sending Transcript Data via eSCRIP-SAFE\u2122: If you are a registered eSCRIP-SAFE\u2122 sender and wish to use eSCRIP-SAFE\u2122 to send your transcripts, please select RTI International, U.S. Department of Education Longitudinal Studies as the transcript recipient. Faxing Hard Copy Transcripts: If your institution is unable to provide the transcript data electronically via any of the methods previously mentioned, you may fax transcripts to our secure fax machine at 1-866-354-7066. First, fax the Student Transcript Fax Test Page included in your institution packet, with your name, telephone number, and fax number, to ensure that the transfer is working correctly. We will then confirm that we successfully received the test page. The confirmation is typically a return fax, but in some instances it is a phone call. If you do not receive confirmation within 15 minutes, please contact the fax help line at 1-866-662-8174. Once you receive confirmation, fax the completed Student Transcript Fax Transmittal Sheet along with the transcripts. If you do not have the fax test page or transmittal sheet, you may print a copy from the link \"Fax hard copy transcripts.\" Once you have sent the transcript data, click Submit on the Transcript Data Transmission page."}, {"section_title": "Provide CIP Code Mappings", "text": "Once transcript data are received, the individual courses and degree programs reported in the transcript data will be coded using a common classification system, the National Center for Education Statistics' (NCES) Classification of Instructional Programs (CIP). The mapping(s) can be transmitted to RTI by uploading to the secure study website or sending via e-mail to pets@rti.org. If you choose to upload the mappings, please select \"Upload CIP code mappings\" from the Main Menu."}, {"section_title": "Provide Requested Course Catalogs", "text": "If we need a course catalog from your institution, there will be a Catalog Transmittal Sheet in your packet. It includes a list of the catalogs requested and instructions for sending them to RTI. If you do not find a Catalog Transmittal Sheet in your packet, then no catalogs are needed from your institution at this time. B&B:08/09 Full- In order to properly code the transcript data we will receive for your students, we need to obtain a copy of your institution's course catalogs, bulletins, or other materials that describe the course offerings for each academic year listed below. <cat_yr_needed> <level> <cat_yr_needed> <level> <cat_yr_needed> <level> <cat_yr_needed> <level>"}, {"section_title": "Instructions:", "text": "If the catalog(s) exist electronically, please send an e-mail to pets@rti.org and either direct us to the catalog(s) online or send the catalog(s) as an attachment. If you need to send hard copy catalogs, please place check marks in the appropriate columns below to indicate the types of course catalogs you are sending for each year. Please do not send catalogs for which \"not needed\" appears. Please keep a copy of this completed sheet for your records and send the original to RTI with the catalogs using the FedEx materials provided."}, {"section_title": "Academic Year", "text": "Type of catalog provided (please place checks or comments as appropriate) Institutionwide/general Undergraduate Graduate Other school/program (please specify) 2003-2004 2004-2005 2005-2006 2006-2007 2007-2008 2008-2009 Other year Subpart A-General "}, {"section_title": "PART 99 -FAMILY EDUCATIONAL RIGHTS AND PRIVACY", "text": "The authority citation for part continues to read as follows: AUTHORITY: 20 U.S.C. 1232g, unless otherwise noted. Subpart A-General \u00a7 99.1 To which educational agencies or institutions do these regulations apply? (a) Except as otherwise noted in \u00a7 99.10, this part applies to an educational agency or institution to which funds have been made available under any program administered by the Secretary, if- (1) The educational institution provides educational services or instruction, or both, to students; or (2) The educational agency is authorized to direct and control public elementary secondary, or postsecondary educational institutions. (b) This part does not apply to an educational agency or institution solely because students attending that agency or institution receive non-monetary benefits under a program referenced in paragraph (a) of this section, if no funds under that program are made available to the agency or institution. on the contested information in the record or stating why he or she disagrees with the decision of the agency or institution, or both. (c) If an educational agency or institution places a statement in the education records of a student under paragraph (b)(2) of this section, the agency or institution shall: (1) Maintain the statement with the contested part of the record for as long as the record is maintained; and (2) Disclose the statement whenever it discloses the portion of the record to which the statement relates. (Authority: 20 U.S.C 1232g(a)(2)) \u00a799.22 What minimum requirements exist for the conduct of a hearing? The hearing require by \u00a7 99.21 must meet, at a minimum, the following requirements: (a) The educational agency or institution shall hold the hearing within a reasonable time after it has received the request for the hearing from the parent or eligible student. (b) The educational agency or institution shall give the parent or eligible student notice of the date, time, and place, reasonably in advance of the hearing. (c) the hearing may be conducted by any individual, including an official of the educational agency or institution, who does not have a direct interest in the outcome of the hearing. (d) The educational agency or institution shall give the parent or eligible student a full and fair opportunity to present evidence relevant ot the issues raised under \u00a7 99.21. The parent or eligible student may, at their own expense, be assisted or represented by one or more individuals of his or her own choice, including an attorney. (e) The educational agency or institution shall make its decision in writing within a reasonable period of time after the hearing. (f) The decision must be based solely on the evidence presented at the hearing, and must include a summary of the evidence and the reasons for the decision. (a) The parent or eligible student shall provide a signed and dated written consent before an educational agency or institution discloses personally identifiable information from the student's education records, except as provided in \u00a7 99.31. (b) The written consent must: (1) Specify the records that may be disclosed; (2) State the purpose of the disclosure; and (3) Identify the party or class of parties to whom the disclosure may be made. (c) When a disclosure is made under paragraph (a) of this section: (1) If a parent or eligible student so requests, the educational agency or institution shall provide him or her with a copy of the records disclosed; and (2) If the parent of a student who is not an B&B:08/09 Full-scale Methodology Report H-19 eligible student to requests, the agency or institution shall provide the student with a copy of the records disclosed. (1) The disclose is to other school officials, including teachers, within the agency or institution who the agency or institution has determined to have legitimate educational interests. (2) The disclosure is, subject to the requirements of \u00a7 99.34, to officials of another school, school system, or institution of postsecondary education where the student seeks or intends to enroll. (3) The disclosure is, subject to the requirements of \u00a7 99.35, to authorized representatives of- (4)(i) The disclosure is in connection with financial aid for which the student has applied or which the student has received, if the information is necessary for such purposes as to: (A) Determine eligibility for the aid; (B) Determine the amount of the aid; (C) Determine the conditions for the aid; or (D) Enforce the terms and conditions of the aid. (ii) As used in paragraph (a)(4)(i) of this section, \"financial aid\" means a payment of funds provided to an individual (or a payment in kind of tangible or intangible property to the individual) that is conditioned on the individual's attendance at an educational agency or institution. (Authority: 20 U.S.C 1232g(b)(1)(D)) (5)(i) The disclosure is to State and local official or authorities to whom this information is specifically-(A) Allowed to be reported or disclosed pursuant to a State statute adopted before November 19, 1974, if the allowed reporting or disclosure concerns the juvenile justice system and the system's ability to effectively serve the student whose records are released; or (B) Allowed to be reported or disclosed pursuant to a State statute adopted after November 19, 1974, subject  (B) Administer student aid programs; or (C) Improve instruction. (iii) The agency or institution may disclose in formation under paragraph (a)(6)(i) of this section if: (A) The study is conducted in a manner that does not permit personal identification of parents and students by individuals other than representatives of the organization; and (B) The information is destroyed when no longer needed for the purposes for which the study was conducted. (iii) If this Office determines that a third party outside the educational agency or institution to whom information is disclosed under this paragraph (a)(6) violates paragraph (a)(6)(ii)(B) of this section, the educational agency or institution may not allow that third party access to personally identifiable information from education records for at least five years. (iv) For the purposes of paragraph (a)(6) of this section, the term \"organization\" includes, but is not limited to, Federal, State, and local agencies, and independent organizations. (7) The disclosure is to accrediting organization to carry out their accrediting functions. (8) The disclosure is to parents, as defined in \u00a7 99.3, of a dependent student, as defined in section 152 of the Internal Revenue Code of 1986. (9)(i) The disclosure is to comply with a judicial order or lawfully issued subpoena. (ii) The educational agency or institution may disclose information under paragraph (a)(9)(i) of this section only if the agency or institution makes a reasonable effort to notify the parent or eligible student of the order or subpoena in advance of compliance, so that the parent or eligible student may seek protective action, unless the disclosure is in compliance with-(A) A Federal grand jury subpoena and the court has order that the existence or the contents of the subpoena or the information furnished in response of the subpoena not be disclosed; or (B) Any other subpoena issued for a law enforcement purpose and the court or other issuing agency has ordered that the existence or the contents of the subpoena or the information furnished in the response to the subpoena not be disclosed. (iii)(A) If an educational agency or institution initiates legal action against a parent or student, the educational agency or institution may disclose to the court, without a court order or subpoena, the education records of the student that are relevant for the educational agency or institution to proceed with the legal action as plaintiff. (B) If a parent or eligible student initiates legal action against an educational agency or institution, the educational agency or institution may disclose to the court, without a court order or subpoena, the student's education records that are relevant for the educational agency or institution to defend itself. 10The disclosure is in connection with a health or safety emergency, under the conditions described in \u00a7 99.36. (11) The disclosure is information the educational agency or institution has designated as \"directory information,\" under the condition described in \u00a7 99.37. (12) The disclosure is to the parent of a student who is not an eligible student or to the student. (13) The disclosure, subject to the requirements in \u00a7 99.39, is to a victim of an alleged perpetrator of a crime of violence or a nonforcible sex offense. the disclosure may only include the final results of the disciplinary proceeding conducted by the institution of postsecondary education with respect to that alleged crime or offense. The institution may disclose the final results of the disciplinary proceeding, regardless of whether the institution concluded a violation was committed. (14)(i) The disclosure, subject to the requirements in \u00a7 99.39, is in connection with a disciplinary proceeding at an institution of B&B:08/09 Full-scale Methodology Report H-21 postsecondary education. The institution must not disclose the final results of the disciplinary proceeding unless it determines that-(A)The student is an alleged perpetrator of a crime of violence or non-forcible sex offense: and (B) With respect to the allegation made against him or her, the student has committed a violation of the institution's rules or policies. (ii) The institution may not disclose the name of any other student, including a victim or witness, without the prior written consent of the other student. (iii) This section applies only to the disciplinary proceedings in which the final results were reached on or after October 7, 1998. (a)(1) An educational agency or institution shall maintain a record of each request for access to and each disclosure of personally identifiable information from the education records of each student. (2) The agency or institution shall maintain the record with the education records of the student as long as the records are maintained. (3) For each request or disclosure the record must include: (1) The names of the additional parties to which the receiving party may disclose the information on behalf of the educational agency or institution; and (2) The legitimate interests under \u00a7 99.31 which each of the additional parties has in requesting or obtaining the information. (c) The following parties may inspect the record relating to each student: (1) The parent or eligible student. (2) The school official or his or her assistants who are responsible for the custody of the records. (3) Those parties authorized in \u00a7 99.31(a)(1) and (3) for the purposes of auditing the recordkeeping procedures of the educational agency or institution. (d) Paragraph (a) of this section does not apply if the request was from, or the disclosure was to: (1) The parent or eligible student; (2) A school official under \u00a7 99.31 (a)(1); (3) A party with written consent from the parent or eligible student; (4) A party seeking directory information; or (5) A party seeking or receiving the records as directed by a Federal grand jury or other law enforcement subpoena and the issuing court or other issuing agency has ordered that the existence or the contents of the subpoena or the information furnished in response to the subpoena not be disclosed. (a)(1) An educational agency of institution may disclose personally identifiable information from an education record only on the condition that the party to whom the information is disclosed will not disclose the information to any other party without the prior consent of the parent or eligible student. (2) The officers, employees, and agents of a party that receives information under paragraph (a)(1) of this section may use the information, but only for the purposes for which the disclosure was made. (b) Paragraph (a) of this section does not prevent an educational agency or institution from disclosing personally identifiable information with the understanding that the party receiving the information may make further disclosure of the information on behalf of the educational agency or institution if: (1) The disclosures meet the requirements of \u00a7 99.31; and (2) The educational agency or institution has complied with the requirements of \u00a7 99.32(b). (1) Make a reasonable attempt to notify the parent or eligible student at the last known address of the parent or eligible student, unless: (i) The disclosure is initiated by the parent or eligible student; or (ii) The annual notification of the agency or institution under \u00a7 99.7 includes a notice that the agency or institution forwards education records to other agencies or institutions that have requested the records and in which the student seeks or intends to enroll: (2) Give the parent or eligible student, upon request, a copy of the record that was disclosed; and (3) Give the parent or eligible student, upon request, an opportunity for a hearing under Subpart C. (b) An educational agency or institution may disclose an education record of a student in attendance to another educational agency or institution if: (1) The student is enrolled in or receives services from the other agency or institution; and (2) The disclosure meets the requirements of paragraph (a) of this section. (1) Be protected in a manner that does not permit personal identification of individuals by anyone except the officials referred to in paragraph (a) of this section; and (2) Be destroyed when no longer needed for the purposes listed in paragraph (a) of this section. (c) Paragraph (b) of this section does not apply if: (1) The parent or eligible student has give written consent for the disclosure under \u00a7 99. 30; or (2) The collection of personally identifiable information is specifically authorized by Federal law.                                                   2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 37.19 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 37 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 33 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 48 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 35 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 38              1 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington; Outlying Areas = American Samoa, Federated States of Micronesia, Guam, Marshall Islands, Northern Mariana Islands, Puerto Rico, Palau, U.S. Virgin Islands.  1 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington; Outlying Areas = American Samoa, Federated States of Micronesia, Guam, Marshall Islands, Northern Mariana Islands, Puerto Rico, Palau, U.S. Virgin Islands.      "}]