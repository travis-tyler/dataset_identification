[{"section_title": "Abstract", "text": "Abstract. Recently, a high dimensional classification framework has been proposed to introduce spatial and anatomical priors in support vector machine (SVM) optimization scheme for brain image analysis. However, classical SVM has to convert 3D discrete brain images naturally represented by higher-order tensors to one-dimensional vectors in order to meet the input requirements. This traditional method destroys the natural structure and correlation in the original data, and generates high dimensional vectors. In this manuscript, the method is improved by a modified support tensor machine (STM) algorithm to make full use of spatial prior and the inherent information of tensor. The new approach reduces memory requirements and computational complexity significantly, and it is comparably demonstrated by experimental results on classification of Alzheimer patients and elderly controls."}, {"section_title": "Introduction", "text": "Alzheimer disease (AD) is the most prevalent neurodegenerative dementia in worldwide, and thus prevention and early accurate diagnosis of AD is increasingly crucial. In the last few years, support vector machine (SVM) methods for AD subject classification have become an incredibly active research topic. In the related work, SVM approach, as same as feature selection and extraction, has been taking the specificity of neuroimaging data into account. As an example, a frame work is proposed in [1] to include spatial and anatomical priors into SVM by using regularization operators, and it indicates a flexible way to model various types of proximity. However, for the purpose of satisfying the input requirements of classical SVM, 3D discrete brain images naturally represented by higher-order tensors have to convert to vectors in advance. This conversion breaks the inherent structure and correlation in the original data, and easily produces high dimensional vectors and leads to a poor performance in time complexity and space complexity. In order to improve above method including spatial prior of 3D discrete brain images in SVM, a new method is proposed in this manuscript to maintain the natural structure in the original tensor data and include spatial prior of all frontal slices of the tensors in STM [2] . To illustrate the improvement, we apply the proposed method to classification of 299 subjects, 137 AD patients and 162 cognitively normal (CN) controls, as the same study population in [3] . It can be observed that the proposed method reduces size of memory requested and the time complexity significantly and produces better classification performances."}, {"section_title": "Spatial prior in SVM", "text": "SVM is a supervised learning algorithm introduced by Vapnik and performs classification by mapping the data into higher dimensional space [4] . Let (x s , y s ), s\u22081,\u2026,S be a training set of instance-labeled pairs, x s \u2208\u03a9, y s \u2208{-1,1} and \u03a9 is the input space, then SVM optimization problem can be written as: If L is a graph Laplacian regularization and \uf062 is a parameter to control the size of the regularization, the optimization problem may be rewritten as:\nMeanwhile, the heat kernel is obtained as\nIn the case of 3D discrete brain images, using image connectivity (6, 18, or 26 ) is the simplest way to define spatial proximity. Let L s be the Laplacian matrix of the graph including the spatial proximity. It is not difficult to verify that L s is very close to a cyclic matrix, so it is diagonalizable to diagonal matrix S by a symmetric orthogonal matrix Q which can be constructed from discrete sine transform (DST) matrix [5] . Hence, the matrix Q and the matrix S can be calculated respectively as:\n, where L s , Q , S are T\u00d7T matrices, T= n 1 *n 2 *n 3 and the size of a 3D discrete brain image is n 1 \u00d7n 2 \u00d7n 3 .\nTherefore, the exponential matrix is computed as:\n. Finally, the computational complexity of the exponential matrix is O((n 1 *n 2 *n 3 ) 6 ).\nSpatial prior in STM Notation and Basic Definitions. In this section, we first introduce some traditional notation and basic definitions in the area of multilinear algebra [6] [7] . Definition 1 (tensor): A tensor, as known as multi-dimensional array, can be represented as\n, where N is the order of the tensor. The element of x is denoted by\nDefinition 2 (outer product): The outer product y x \uf06f of a tensor\nand another tensor\nfor all values of the indices.\nDefinition 3 (inner product): The inner product \uf03e \uf03c y x , of two same-sized tensors\n, y x is defined as the sum of the products of their entries, that is:\nDefinition 4 (n-mode (matrix) product): The n-mode (matrix) product of a tensor\nand is a tensor in\n. Elementwise, we have\nDefinition 5 (n-mode (vector) product): The n-mode (vector) product can be seen as a special case of the n-mode (matrix) product, and the n-mode (vector) product of a tensor\nwith a vector\nand is a tensor in\n. Elementwise, we have\nDefinition 6 (rank-one tensor):\nis a rank-one tensor if it can be represented as the outer product of N vectors, that is, x=a (1) \u2022\u2026\u2022 a (N) , where the symbol \"\u2022\" represents the vector outer product.\nDefinition 7 (CP decomposition): CP decomposes a tensor into a sum of component rank-one tensors.\nFor example, a third-order tensor x\u2208R I\u00d7J\u00d7K can be decomposed to and y m \u2208{-1,1}. Classic STM restricts tensor weight parameter to rank-one tensor as:\nand consists of N quadratic programming (QP) problems with inequality constraints, wherein the nth QP problem can be represented as: These N optimization models have no closed-form solution, so the alternating projection algorithm is used to solve them and updates one weight parameter using LIBSVM [8] at one time.\nFinally, the class label of a testing example x can be predicted as follow:\nSpatial Prior in STM. As described in the text, L s is a Laplacian matrix of the graph including the spatial proximity and the size of L s is (n 1 *n 2 *n 3 )\u00d7(n 1 *n 2 *n 3 ), nearly 10 6 \u00d710 6 in practice. Unfortunately, this size of matrix is so large that cannot be stored in the memory, and the same situation also appears in the process of calculating the matrix Q, the matrix S and the exponential matrix. Although these matrices are barely able to calculate by computation skills, the time complexity and the space complexity is rather poor.\nIn the following, this problem is reconsidered from the point of view of tensor which constitutes efficient representation of 3D discrete brain images.\nClassic STM updates one weight parameter at one time. In order to include spatial prior in STM, We modify the original algorithm and divide the iterative process into two steps. The first step calculates n-mode product of and updates one weight parameter w 3 using LIBSVM. The detailed procedures of modified STM algorithm are described in Table 1 .\nLet R be the result of n-mode (vector) product of a tensor x with w 3 , i.e., , and R is an n 1 \u00d7n 2 matrix as the same size as each frontal slice of the tensor. The Laplacian matrix of the graph including the spatial proximity of matrix R is named newL s . It keeps the same degree and adjacency relations as the Laplacian matrix of the graph including the spatial proximity of each frontal slice. Matrix R is a linear combination of all frontal slices according to the definition of n-mode (vector) product, hence, introducing spatial prior of matrix R in STM is equivalent to introducing spatial prior of all frontal slices in STM. In other words, different from including the spatial prior of 3D discrete brain images in SVM, the proposed method maintains the inherent structure and correlation in the original tensor data and introduces the spatial prior of all frontal slices of the tensors in STM. Similarly, the matrix Q, the matrix S and the exponential matrix (3), (4) and (5) respectively, with different dimensions. Obviously, the size of newL s is (n 1 *n 2 )\u00d7(n 1 *n 2 ). Therefore, the computational complexity of the exponential matrix is greatly reduces from O((n 1 *n 2 *n 3 ) 6 ) to O((n 1 *n 2 ) 6 ). In the first step of iterative process of modified STM algorithm, the optimal weight parameter w of training result of LIBSVM is an (n 1 *n 2 )\u00d71 matrix. The weight parameter w can be decomposed into w 1 and w 2 using the CP decomposition software package (www.sandia.gov/~tgkolda/TensorToolbox) according to the assumption of (6). Thus, LIVSM and CP decomposition makes it possible to update the two weight parameters, w 1 and w 2 , at the same time."}, {"section_title": "Experiment", "text": "After the proposed method is applied to classification of 299 subjects, 137 AD patients and 162 CN controls, as the same study population in [3] . Only T1-weighted, acquired at 1.5 T and pre-processed magnetic resonance (MR) images are selected and entire study on the Alzheimer Disease Neuroimaging Initiative (ADNI) public database (www.loni.ucla.edu/ADNI).\nThe Statistical Parametric Mapping (SPM) is used to segment the selected images into gray matter (GM), white matter (WM) and Cerebrospinal Fluid (CSF) [9] , and the Diffeomorphic Anatomical Registration through Exponentiated Lie Algebra (DARTEL) algorithm is used for spatial normalization [10] . The key process are applying SPM8 package to structural brain image segmentation, utilizing DARTEL algorithm to create GM/WM average-shaped template and normalize the segmented image to Montreal Neurological Institute (MNI) Space respectively. It is worth noticing that all GM probability maps in the MNI space should be modulated to ensure that the overall tissue amount maintains constant.\nFor better evaluation of the proposed approach, all experimental data and the whole process should be as close to the previous method as possible. Wherein 9 subjects, 6 AD patients and 3 CN controls, are failed in pre-processing as described in detail in [3] . Those subjects are excluded if they belong to the training set, and 50 percent misclassified if they belong to the testing set. As a consequence, the set of participants is randomly split, for 20 times, into two groups of the same size, 145 training and 145 testing, based on the Mersenne Twister algorithm.\nDiffusion parameter \uf062\uf020 should be set at 0.001, 0.003, 0.005, 0.007 and 0.009, because the optimal value is around 0.005 according to our experiment. Table 2 shows the average results of 20 times random experiment with different \uf062, wherein the number of iteration is set at 100 and the threshold parameter \u03b5 is set at 1e-4. Furthermore, contrast to classic SVM costing about 70 hours, the proposed method only spends nearly one percent of running time and classification accuracy (Acc.) still maintains a high level.\nWe propose two other ways of using STM for AD subject classification. The first method, named Direct-STM, applies classic STM for training 3D discrete brain images directly and yields the lowest accuracy of 89.53%. The second method, named ROI (Regions of Interest)-STM, applies classic STM for training ROI dataset extracting from 3D discrete brain images and produces the lowest running time because the ROI dataset is a large sparse tensor. Table 3 shows the classification performances of three proposed methods. It is observed that including spatial prior in STM can significantly improve classification accuracy.\nAs shown in Table 4 , a single spatial regularization kernel (Voxel-Regual-spacial) with a peak accuracy of 89%, a single anatomical regularization kernel (Voxel-Regual-atlas) yielding a peak accuracy of 90%, but combining the two regularization terms (Voxel-Regual-combinegraph) as the previous method only reaches a peak accuracy of 87% [1] . It can be observed that the proposed method based on including spatial prior in STM ranks first for all classification types. The sensitivity (Sens.) related to AD patients and specificity (Spec.) related to control subjects is also quantified. "}, {"section_title": "Conclusions", "text": "A novel method including spatial prior in STM algorithm is proposed for classification of Alzheimer patients and elderly controls. Theoretical analysis and experiment show that the proposed new method not only reduces the time complexity and space complexity significantly, but also produces better classification accuracy."}]