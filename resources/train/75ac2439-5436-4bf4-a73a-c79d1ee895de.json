[{"section_title": "Abstract", "text": "ABSTRACT Because of the vast availability of data, there has been an additional focus on the health industry and an increasing number of studies that aim to leverage the data to improve healthcare have been conducted. The health data are growing increasingly large, more complex, and its sources have increased tremendously to include computerized physician order entry, electronic medical records, clinical notes, medical images, cyber-physical systems, medical Internet of Things, genomic data, and clinical decision support systems. New types of data from sources like social network services and genomic data are used to build personalized healthcare systems, hence health data are obtained in various forms, from varied sources, contexts, technologies, and their nature can impede a proper analysis. Any analytical research must overcome these obstacles to mine data and produce meaningful insights to save lives. In this paper, we investigate the key challenges, data sources, techniques, technologies, as well as future directions in the field of big data analytics in healthcare. We provide a do-it-yourself review that delivers a holistic, simplified, and easily understandable view of various technologies that are used to develop an integrated health analytic application.\nINDEX TERMS Big data, cyber-physical systems, health analytics, machine learning, social networks analysis."}, {"section_title": "", "text": "EMR entries, such as doctors' notes, another solution that can analyze the SNOMED-CT database (which is a nomenclature database) is required. As these two databases are different, a fit-all analytic solution that can handle these datasets (which diverge in terms of data types, speeds, naming standards) cannot be determined.\nWithout the assistance of an integrated healthcare solution, physicians might use the traditional EMR or they might use it to a varying degree depending on how conversant the user is [8] , [10] . To consolidate data from EMR, additional data sources that are in use today need special analytics that should be integrated with EMR analytics. For example, to retrieve a meaningful insight from the patient's social network or that of patients with similar cases, we must integrate Social Networks Analysis (SNA) with EMR-based analytics and other cyber-physical systems.\nIn recent years, several studies [138] , [139] have attempted to integrate some of the sources of health data to develop integrated solutions. Specialized health solutions such as COHESY [11] , CARE (Collaborative Assessment Recommendation Engine) [140] , which relies only on medical record data, and AEGLE [12] , have attempted to integrate health analytics at a certain degree; however, they have not been able to cater to each aspect of healthcare data.\nThis survey serves as a guide to accommodate all or most of the critical data sources and produce an integrated analytic solution that can save lives and prevent unnecessary spending on health care. Fig.1 shows a blueprint of how FIGURE 1. Conceptual overview of health big data analytics technologies. Spark is used as an example of a big data platform.\nintegrated healthcare analytics would appear. Health data comes from both structured data sources such as EMR [9] and insurance providers' databases as well as unstructured forms such as doctors' notes, prescriptions, IoT devices (such as wearables), medical sensors, electronic monitors, mobile applications, social media, and research registries.\nWhile straightforward algorithms such as decision trees or k-means can solve forthright tasks such as deciding if a given patient should be hospitalized or not, certain tasks like those involving the application of MRI scans for differentiating a cancerous tumor from a benign one can require sophisticated algorithms such as Convolutional Neural Networks (CNNs). Hence, the choice of a technique for a given health instance requires a thorough analysis. In this review, we provide options to choose from, depending on the health predicament.\nTo scale up, the health analytic solution must rely on a big data platform or any other parallel computing hardware. The choice of the platform depends on the data. While some static data such as EMR records can be handled effectively by batch computing platforms such as Hadoop MapReduce, some other real-time data that are critical for the patient's survival such as real-time ECG readings or social network analysis will require stream computing platforms such as Apache Spark or Apache Flink. The platform must also accommodate support libraries such as machine learning libraries and graph libraries for relationship analysis."}, {"section_title": "B. ORGANIZATION OF THE PAPER", "text": "This paper is organized as follows: In section II we discuss the related healthcare analytics surveys. We analyze their key aspects and we cover their limitations as well as the added value of the current study. In section III we discuss key challenges that a healthcare analytics solution must address. In section IV we discuss key sources of healthcare data. In this section, we investigate sources that are usually not considered while building health informatics like social networks data and mIoT data sources. In section V we thoroughly cover the data mining techniques that are used in health analytics. We first provide a brief overview of some algorithms by recalling their general uses and then looking into how they can be tailored for health analytics. We provide an overall summary of disparate analytic techniques as well as selected use cases in health analytics.\nIn section VI we deeply analyze diverse Big data platforms that can be used as foundations upon which an inclusive analytic solution can be built. As in section V we choose some of the most popular platforms and discuss how they are generally used and their specific uses in health analytics then finally we provide an elaborate summary of trendy platforms."}, {"section_title": "II. RELATED WORK", "text": "The field of health analytics and big data has recently gained a big deal of attention. Table 1 summarizes the key related reviews. On each review, we describe its contribution and the topic covered.\nAt the end of the table, we provide a hasty comparison of our work with these others that are covered. Among all the papers in Table 1 none that instills a do-it-yourself motive by covering all the key problems from the data sources to insightful visualizations."}, {"section_title": "III. HEALTH BIG DATA ANALYTICS CHALLENGES", "text": "The goals of designing an integrated health analytics span a whole patient's ecosystem. The system should not only be able to help to the provision of a successful and timely care by recommending a practical diagnosis which worked well against similar cases but also is able to predict possible medical aggravations that might occur. This includes using the Complex Events Processing (CEP) for the determination of a disease progression which can help to stop it at earlier stages of development. Kuo et al. [16] consider the big data challenges per stages of big data pipeline, considering four distinct stages here each stage has its own challenges to overcome. The following challenges at each stage are considered:\nA. DATA AGGREGATION Health big data comes from various sources and sometimes these sources might be large repositories of data which have to be brought into a common platform for a unified analysis. The aggregation challenge is related to high volume and variety of data that needs to be brought together from divergent data warehouses and real-time data. The solution can be the use of high-speed file transfer technologies. From Fig.1 we can observe that the warehouses that host human genomes are completely different from another that hosts the SNOMED-CT nomenclature data. Some studies have been performed to deal with this aggregation hustle. One example is the use of EasyGenomics (BGI) a technology that is used by the Beijing Genomic Institute to transfer large genomic data. Another solution that is presented to deal with data aggregation challenge is data compression. Cox et al. [17] proposed a solution that is based on the Burrows-Wheeler Transform (BWT) to compress many DNA sequences. The BWT is a string compression algorithm that compresses the data by grouping similar characters in a series of strings."}, {"section_title": "B. DATA MAINTAINANCE/STORAGE", "text": "Data storage is a key challenge for health big data. The data is ever growing at an exponential rate hence cannot be managed with traditional database management systems. To solve this problem Non-SQL database systems like MongoDB, Cassandra and Hadoop Distributed File System (HDFS)are proposed but cloud computing [18] , [137] is argued to be a powerful solution as it can reduce the initial EMR costs."}, {"section_title": "C. DATA INTEGRATION AND INTEROPERABILITY", "text": "The health big data are hugely heterogeneous. They come from many sources with divergent forms and structure hence making interoperability a big challenge. Even the EMR which is the most structured of all data source can present a challenge, especially when more than one EMRs are involved. In Fig.2 , Kuo et al. [19] consider interoperability hustles into 3 types; Functional interoperability, metadata interoperability, and data instance interoperability.\nFunctional interoperability is sensed when two EMRs exchange data while having different naming standards or interpretation. Metadata interoperability can be observed in a relational database whereby a column name which is the metadata of column content has a different naming. As an example, ''GENDER'' and ''SEX'' can be used in two EMRs and have the same logical meaning hence it can result in metadata interoperability challenge. Data instance interoperability is when acronyms and other codes do not have the same meaning. For example, a gender might be ''M'' and ''F'' in one EMR and ''1'' and ''0'' in another EMR.\nHL7 (Health level7) is a set of standards that are used to help different EMR to communicate smoothly. It is composed of a set of rules that developers of Hospital Information Systems (HIS) must follow to achieve standardization. With HL7 also medical equipment can share information. However, for a long time, the HL7 adoption has been so lagging and some studies like in [21] and [22] have found this standard to be flawed. Various studies have considered reusing available standards to better deal with interoperability. Lopez and Blobel [20] proposed a semantic interoperability model which consists of trying to implement an existing HL7 standard as a UML profile then applying the profile to system models. Crichton et al. [23] and Mudaly et al. [24] have considered interoperability problems in the context of low-income countries. 65664 VOLUME 6, 2018 FIGURE 2. Health big data interoperability (source [19] )."}, {"section_title": "D. DATA ANALYSIS", "text": "Depending on the complexity of a health problem the traditional SQL based querying time increases exponentially as the number of records increases. The hardware and software needed to analyze data need to be so robust and expensive to get a meaning from the huge dataset. In terms of hardware Supercomputers and cloud computing are the most widely used. The aim of analyzing health data is to apply a predictive model to predict probable occurrences and complications. MapReduce programming model and its Hadoop implementation provide robust analytical tools to do the analysis."}, {"section_title": "IV. HEALTH BIG DATA SOURCES", "text": "Health big data sources are so innumerable and diverse. The sources depend on the level of technology that a health entity uses. Cyber-Physical systems, medical Internet of Things (mIoT), social networks, Electronic Medical Records, and genomic data are the big contributors to health data and are covered in this study."}, {"section_title": "A. ELECTRONIC MEDICAL RECORDS (EMR)", "text": "The primary source for any health analytic solution is the EMR also known as EHR. This is a hospital-based system that combines all the entries that are logged by health practitioners. To obtain an effective health analytic solution, it is paramount that the other sources of data be able to synchronize with the EMR. The EMR is a collection of entries that include doctor notes, diagnosis history, pharmacy data, and the insurance company's data. This aggregation of actors makes its design so complex that its adoption is sometimes hindered.\nThe challenges faced in designing a proper EMR start at the stage of data entry by the physicians. The traditional method for data entry which is straightforward for practitioners is using an easy to use Word document and a spreadsheet. however, this method exhibit difficulties in providing a meaningful insight through an appropriate analytic algorithm. The analytic solution should be able to process unstructured data while the EMR contains mainly structured data hence it is necessary to transform these unstructured entries.\nYang [38] proposed an XML-based scheme that consists of facilitating both the physicians as well as analytical solution designers. The solution consists of using an XML schema to process entries that were recorded in the usual word processor or spreadsheet fashion and transform it into XML data to be processed as structured data inside the EMR.\nAnother challenge with the EMR is the interoperability. A patient is treated at different hospitals, which may leave his health data scattered across various EMRs. Interoperability problems between hospitals still pose a big barrier for systems integration. This hindrance is a big obstacle to healthcare as with the absence of prior treatments and the tests, patients undergo different and repeated treatments for the same illness. There are two big questions to consider. The first is the access infrastructure, which is mostly standalone for each hospital and the second is the complexity of the integration of health systems in the context of a country or any other geographical entity.\nTo solve the access challenge, Wan and Sankaranarayanan [39] proposed a cloud-based EMR that can help all stakeholders to access the EMR by making use of three cloud computing components: Software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). The system allows mobile access for all stakeholders. Additionally, another big challenge to consider is confidentiality as not many hospitals permit their records to be accessed by others.\nTo solve the compatibility problems, various studies have been conducted and one of the most trusted technologies is Blockchain [169] . The Blockchain technology solves the challenge of personalization and ubiquitous access to records which are the two traits that are required in the healthcare industry. Blockchain ensures data integrity by allowing an immutable way to update records and prevents the tampering with them once they have been logged. Each entry is saved in a block and the content of each block is hashed to constitute the content of the next record.\nAs depicted in Fig.3 , to solve the mistrust between health providers, Azaria et al. [40] proposed MedRec a solution that decentralizes the EMR access using Ethereum smart contracts. The smart contract will not only help the patient but can also help other stakeholders. A use case would be to help insurance companies to verify if certain medical treatments like surgeries have occurred before paying for the services."}, {"section_title": "B. MEDICAL INTERNET OF THINGS(mIoT) AND CYBER-PHYSICAL SYSTEMS (CPA) DATA", "text": "With the pervasiveness of wireless sensors, the health industry has revolutionized a lot. With the advances in wireless sensor networks, mobile health and patient remote monitors from home, a huge amount of data is now generated in real time. Steps for a health provider adding patient records on an Ethereum blockchain (source [40] ) .\nFor a long time, EMR was believed to be the sole provider of patient-related information. However, with the introduction of real-time health systems (RTHS), the EMR is just a part of the overall health IT ecosystem. Dimitrov [25] has proposed a conceptual structure of an RTHS system that includes mIoT devices. Patient-based wearables, such as bio-shirts, and body implants (smart capsules) feed data into the EMR, which contains clinical data of the patient. A health analytic solution can then help a physician to treat an emergency or obtain an insight regarding an impending one. Various wearables that perform certain tasks like fall detection, position detection, glucose level monitoring, location tracking, have been developed. To integrate various sensors with different standards, customized middleware must be developed. Lu and Chen [27] proposed a middleware design for Tele-homecare applications. It consists of two layers, a device management layer as well as the data management layer. This Middleware which is an interface between vital signs sensors and tele-homecare applications receives vital signs data from the sensors and channels them up to the tele-homecare applications. Various other studies such as those in [26] , [28] , and [29] have focused on hardships to consider when designing a middleware for various remote sensing devices in healthcare. Medical IoT devices are used to monitor the physiological vital signs of inpatients as well as outpatients with temporary or chronic diseases. The main categories of vital signs these devices measure are the respiratory rate (RR), body temperature (BT), blood pressure (BP), oxygen saturation (SO2), blood glucose (BG), heart rate (HR), and so on. Table 2 summarizes the IoT sensors that are used in healthcare as well as their properties.\nThe advances in mIoT permit a remote diagnosis for patients who cannot arrive at hospitals. They also help to supervise the patients who are incapacitated and monitor them from the comfort of their home beds. This pervasiveness also can permit self-treatments especially in the earlier development of sicknesses. However, such easy availability would require appropriate solutions that can integrate IoT data with the EMR data once the patient is hospitalized and clinical tests are conducted."}, {"section_title": "C. SOCIAL NETWORK DATA", "text": "Traditionally, it is assumed that the physicians and their measuring equipment are the only sources of a patient's data. With the advent of behavioral informatics and their applications to personalized healthcare systems, it is now possible for the patients to be collaborators in their treatments. For example, consider a teen who visits his hospital for treatment and the physician performs various measurements such as ECG or MRI and feeds the readings into EMR to perform the diagnosis. But suddenly he checks the teen's Facebook page and finds out that the subject posted a suicide message last night. Treating the patient by omitting this critical information would be inappropriate. Using his own intuition without being able to incorporate the information with other readings would also not be elaborate enough. Hence, there is a need for the health analytic solution to accommodate the EMR data with information on social networks to perform a complete diagnosis.\nIn their study regarding human social influence, Zheng et al. [120] found out that social contagion alters behaviors such as the decision to go for treatment or adoption and compliance with medical prescriptions. They also concluded that social influence is apparently channeled through interactions with friends rather than in a professional context. This explains that a patient might not disclose fully his situation to a physician rather he can opt to convey his feelings to his close friends on social networks.\nSocial Networks Analysis(SNA) [121] [122] [123] uses networks and graph techniques to mine social network content. Social network data can be used as a social health support tool in a community for purposes such as raising local health awareness or crisis communication. Social media data can be integrated into clinical decision support systems and be a constituent of the diagnosis decision [124] . To obtain a unified analytics solution, it is necessary for the SNA to be integrated with other analytical tools. However, there can be possible resistance within the health institutions as data from social networks, such as self-reporting, are not sufficiently finegrained to be incorporated with other standards that are tested. Thus, a hierarchically integrated EMR with a patient portal that contains fine-grained or censored social media data that allows a patient to collaborate with peers is necessary."}, {"section_title": "D. GENOMIC DATA", "text": "The human genome is a chain of all genes that make up a human cell. The genes are made of DNA. The Human Genome Project (HGP) performed a full sequencing of the human genome to determine patterns that might be sources of some diseases [125] . Though this process is expensive, complex, and took more than 10 years to complete, a full sequencing of the human genome is a novelty and can save lives. A sequence of DNA proteins (ATCG) in the individual's genome can decide all the human traits like the color of eyes, skin color, vulnerability to a certain disease etc. Hence, if we apply analytical techniques such as machine learning to a human genome, we can obtain significant information and insights about the subject's health condition. We can study how certain drugs work and know specifically they work on an individual. Genomic data is a backbone to the personalized healthcare [127] whereby specific treatments for a patient can be considered.\nAs it is for other sources of health data, an analytical solution must fully integrate the genomic data with data obtained from other sources to provide an effective diagnosis. To do so, the system must integrate molecular pathology with clinical pathology. Even with these recent developments in genome sequencing, designing and implementing a genome-enabled electronic medical recording system is challenging [128] .\nThe big challenge is taxonomy and vocabularies necessary to transform genomic sequences into clinically meaningful descriptions that can support existing diagnoses that are deduced from EMR data. While clinical pathology uses a standard naming system, such as the SNOMED-CT, molecular pathology still falls short of a concrete standardization. Several recent studies have attempted to address this drawback. Hoffman et al. [127] proposed the clinical Bioinformatics Ontology (CBO), which is a semantic resource that can describe clinically meaningful genomics concepts.\nGreen et al. [129] concluded that owing to the lack of a standardized matching between phenotypes and genomic data, concrete integration and utilization of genomes for diagnosis will not be achieved until the year 2020. One of the successful studies was the eMERGE (Electronic Medical Records and Genomics) [130] . This study involved PhenomeWide Association Studies [131] , which is a process of using EMR and analyzing various phenotypes with reference to one genetic variant."}, {"section_title": "V. DATA MINING TECHNICS", "text": "Machine learning techniques are vital for predicting disease occurrences or their complications. Though all machine learning algorithms and practices can be applied to healthcare problems, each illness and its complications are best described by a single algorithm or a combination of some of them. Hence, it is necessary to inspect the algorithms closely and apply them where they are appropriate. The following algorithms are widely used in health informatics. we briefly described them as well as their specific use in diseases diagnosis."}, {"section_title": "A. K NEAREST NEIGHBOR ALGORITHM", "text": "K-nearest neighbor (KNN) [166] is among the simplest and most classical machine learning techniques used. Its use can be described as ''Tell me your close friends and I will tell you who you are''.\nIn building healthcare analytics, this algorithm is effective in classifying a disease by matching it with the already known cases and the resulted complications.\nIn healthcare, to determine if intestinal cells are cancerous, we need to classify intestinal cells in five possible classes of tumor cells: adenocarcinoma, sarcoma, carcinoid tumor, gastrointestinal tumor, and lymphoma. To classify cells under test, we can use Principal Components Analysis, a statistical feature that has more variance on data, and plots the known cell classes.\nIn the example shown in Fig.4 , we associate the undetermined cell T with one of the classes of our training data. After testing, and assuming the cells as per their principal FIGURE 4. K Nearest neighbor algorithm. VOLUME 6, 2018 components P1 and P2, we would like to find several k cells for which the sum of the distances to T is minimum (nearest neighbors of T). After calculating the sum, we conclude that the cells under inquiry are classified as Adenocarcinoma cell types. The value of k must be selected appropriately to avoid overfitting.\nShouman et al. [41] have proposed a method to use this algorithm to diagnose a heart condition. During their study, due to its complicity and its convergence, KNN has outperformed other classification techniques."}, {"section_title": "B. SUPPORT VECTOR MACHINES (SVM)", "text": "SVM [167] is an effective classifications algorithm that is used to classify data. As depicted in Fig. 5 , the SVM splits data using a hyperplane, which is a plane that separates the nearest points that are known as support vectors. Choosing the hyperplane is a constrained optimization problem as we must optimally choose the margin between classes that is wide enough."}, {"section_title": "FIGURE 5. A two-dimensional SVM model.", "text": "A hyperplane is a plane that is chosen to divide the classes, and sometimes, the data cannot be separated linearly. If such problems arise we can combine the SVM with additional kernel techniques like Radial Basis Functions. Though SVM is an old machine learning technique, recent studies have resulted in robust classification capabilities.\nBrown et al. [47] introduced a method to classify genes using gene expression data by applying SVM techniques. Furthermore, Guyon et al. [48] have used SVM based on Recursive Feature Elimination (RFE) and performed gene selection for cancer classification. The aim of the experiment was to determine if certain genes are active, silent, or hyperactive.\nKhedher et al. [49] have proposed a method for early diagnosis of Alzheimer's disease using SVMs on segmented MRI images. In this study, they used Principal Component Analysis to perform feature selection using an ADNI (Alzheimer's Disease Neuroimaging Initiative) dataset and SVM to determine if a certain patient is suffering from cognitive problems of old age or is developing the Alzheimer's disease."}, {"section_title": "C. NEURAL NETWORKS", "text": "Neural networks [168] are algorithms that perform exceptionally well in grasping insights from unstructured data. Health analytics require algorithms that can receive inputs from disparate sources and extract a meaning. Medical unstructured data comprise doctors' notes, radiology scans, MRI images, microscopy, CT scans, ultrasound images, and so on. Interpreting these data using neural networks is an excellent technique.\nNeural networks are a series of neuron-like layers of computation that apply a chain of computing algorithms to the input data computation cell to produce outputs. Fig.6 depicts a basic structure of a neural network. A Convolutional Neural Network(CNN) is comprised of many layers that are capable of transforming inputs by applying convolutional filters to produce a clear output. Neural networks are currently used for medical imaging, such as brain lesions analysis, fetal imaging, and cardiac analysis. To understand neural networks in the context of healthcare, consider its application in treating or predicting if a given breast tumor is malignant or benign. In this use case, inputs features can be various values of the most critical biopsies for the breast cancer which can be obtained by medical imaging. From the Breast Cancer Wisconsin Data Set [42] , the breast cancer biopsies that can be inputs of neural networks are clump thickness, cell size, cell Shape, Marg adhesion cell size, bare nuclei, bland chromatin, normal nuclei, and mitosis. The practitioner can input values of these biopsies and the neural network is able to conclude if the tumors are benign or malignant.\nCurrently, another key application of neural networks is the analysis of brain lesions. Kamnitsas et al. [43] were able to use 3D CNNs to analyze MRI brain scans. The process involved capturing each brain 3D voxel, obtained from a 3D scan and applying it to a network of convolutional layers and classification layers that produce a clear inference. Another key area of the neural network application is matching medical text reports with medical images, which is critical as we can interpret one using another. Kooi et al. [44] compared the traditional CAD-based mammography system, which relies on manual features, and a CNN-based mammogram.\nAnother interesting application of neural networks and brain activity scans is the prediction of survivability using MRI images. In Fig.7 , van der Burgh et al. [45] combined MRI images with clinical data obtained from Amyotrophic Lateral Sclerosis (a disease that causes the death of neurons that control voluntary muscles) patients. Using this data, they were able to predict survivability by applying deep neural networks. The system was comprised of four neural networks: a network made of Clinical data, Structural connectivity from MRI, brain morphology MRI, and a combination of these three."}, {"section_title": "FIGURE 7.", "text": "Classifying survivability using neural networks. Combining data from various source and predicting survivability as short, medium, and long (Source [45] ).\nBy combining these networks, the system was able to predict the survivability of a patient as short, medium or longterm. In this process, the number of input nodes depended to several characteristics of each neural network. For example, the input vectors for the clinical characteristic neural network were the age at onset and time for diagnosis. The prediction accuracy was improved by 84% compared to the results that were obtained without using Neural Networks [46] ."}, {"section_title": "D. K-MEANS CLUSTERING TECHNIQUES", "text": "The K-means clustering technique is one of the most popular unsupervised learning techniques [81] [82] [83] . It is a clustering algorithm that is used in classifying points on a Euclidian plane into categories. It is an iterative technique that takes several n points and classifies them into k possible clusters around k centroids.\nThere are so many concrete applications of the K-means in healthcare. Gash and Eisen [84] used fuzzy k-means clustering and identified overlapping clusters of yeast genes. Fuzzy k-means allows a given point to belong to every cluster; however, with varying degree of membership. Ng et al. [85] applied k-means algorithm to perform medical images segmentation. Their method combined k-means with an improved watershed algorithm with k-means taking 2D MRIs as inputs and producing clustered images. The clustered images were then segmented using the watershed techniques. Zheng et al. [86] applied hybrid k-means and SVMs for breast cancer diagnosis. During the experiment, k-means was used for identifying various hidden patterns of benign and malignant tumors."}, {"section_title": "E. ENSEMBLE LEARNING", "text": "Medical problems are usually complex and one learning algorithm might fail to yield an informative result. Hence, we can combine the learning techniques into a more effective and robust learning technique better than the constituent algorithms. An ensemble [87] , [88] , [91] is a collection of various learning algorithm working together in parallel or in sequence to produce better results. It is mainly used in classification problems like sentiment classification [89] . Ensemble learning has shown to provide very efficient and favorable outcome compared to individual learning systems as its usually perceived as a process of consulting multiple experts before deciding [90] . In this method which combines more than one classifier, the outcome depends on a set of rules that are used during combination.\nThe starting point of an ensemble process is feeding the training data to a base algorithm then voting is used to make an accurate decision from classification outcomes of each classifier in the ensemble. The most popular ensemble learning techniques are; Bagging and Boosting [92] . during the bagging process we take the training data and divide it into bags (or subsets).We then apply an individual classifier on each subset and finally, we apply voting to produce a final prediction. Boosting is rather dependent on refining wrong predictions. We divide the training set as we do for bagging, we apply a learning algorithm to a subset, we test the classifier and check the wrong predictions. We take these cases and we combine them with a new subset and we apply another classifier and so on until we finish all the subsets. Boosting produces extremely refined decisions.\nEnsemble learning has many applications in healthcare and is the best fit for most of the health problems. Gu et al. [93] used the FKNN ensemble learning method to classify membrane proteins using a hybrid approach of predicted secondary structural features (PSSF) as well as approximate entropy (ApEn) to predict the G-protein coupled receptors in low homology. Savio et al. [94] use the combination of Voxel-based Morphometry and used Support Vector Machine and neural networks to analyze structural MRI images for an earlier detection of Alzheimer's disease (AD) and myotonic dystrophy of type 1 (MD1)"}, {"section_title": "F. MARKOV DECISION PROCESS (MDP)", "text": "Markov Decision Process (MDP) [170] is a powerful stochastic control algorithm used in decision support. Even though the physician is presented with appropriate algorithms for a certain health case, there are some stochastic problems that are dependent on critical operations that need a decision support system. Some decisions such as a transplant or even the diagnosis itself will need mathematical models for accurate decisions to be made. Sonnenberg and Beck [171] proposed a methodology that can be used as a practical guide for using the MDP for clinical diagnosis. At each stage of the treatment, a patient's health state is analyzed as a Markov state and death or other complications are considered as terminal nodes in this MDP. The events that influence a patient's health are modeled as transitions between nodes. For example, bleeding (an event) can cause a patient's transition from a coma (state) to death (final state). By building a Markov state diagram, critical decisions will be made and decisions which lead to catastrophic results will be omitted. Hence, MDP can be incorporated into a health analytic solution for decisionmaking purposes. As a synthesis, Fig.8 provides a detailed view of use cases of various data mining techniques in healthcare analytics."}, {"section_title": "VI. HEALTH BIG DATA PLATFORMS AND TOOLS", "text": "Building scalable health analytics is complex because data varies in speed, size, urgency, availability etc. To scale and accommodating the data, the analytic system must be integrated into a parallel and distributed computing framework. The most challenge with healthcare data is the diversity in volume. Analytics must be performed proactively and reactively; hence, the architecture must be sufficiently extensible to support all the analytics. The big challenge faced while selecting an appropriate platform to use for the analytic solution is to accommodate the urgency of action and required insights. While some data such as ECG readings and other mIoT data might be urgent and require in-memory streaming and immediate analysis, some other data, such as HER records, might require batch processing. Though these platforms exhibit different architecture any parallel computing platforms share components to consider while designing scalable big data analytics. Fig. 9 depicts a conceptual architecture of a big data platform. The following big data platforms are popular in health informatics:\nA. HADOOP Hadoop [58] is a parallel computing platform that stores and processes very large computing clusters on its core architecture and is empowered by three main components.\nHDFS [59] , which is the underlying file system distributed on clusters, has storage devices arranged in various racks. This file system is distributed, and each data block is duplicated as copies across clusters. Another key component in Hadoop is YARN [60] , which is a resource management and job scheduling tool. Its role is mainly to manage the extensive storage resource and keep track of the computing workload across clusters.\nThe other component of Hadoop is the MapReduce [61] . Its role is to process the data stored on HDFS clusters and that role is accomplished in two functions of Map and Reduce. During a Map step, the master node will divide the job into smaller tasks and distribute the resources based on the task. After computations, the Reduce function aggregates all results to produce a solution to the original problem. Fig. 10 provides the detailed operational steps of the Hadoop framework."}, {"section_title": "1) HADOOP FOR HEALTH BIG DATA ANALYTICS", "text": "Hadoop has the potential to be used in building a healthcare analytics solution. However as discussed, it is a batch-only big data platform; hence, it cannot leverage fully the potential of real-time emergencies like ECG reading, whereby a patient may need an immediate attention as per the alarm. Various researches have introduced medical products that are built on Hadoop. Lijun et al. [66] have proposed Medoop, a Hadoop-based medical platform. This system leverages the attributes of scalability, high reliability and high throughput of Hadoop. Sweeney et al. [67] developed Hadoop Image Processing Interface (HIFI) which is used for Image-based MapReduce activities. Studies in [68] and [69] have also attempted to leverage the attributes of Hadoop and MapReduce for healthcare."}, {"section_title": "2) PROBLEMS WITH HADOOP MAPREDUCE", "text": "Though Hadoop is widely used it has some flaws [62] . The first flaw is that it is strictly a batch computing platform. Hadoop is mainly designed to perform computing loads in batches hence not appropriate for real-time streaming applications where immediate insights are required.\nAnother flaw with Hadoop is the Skew problem. This problem is observed during the Map and Reduce operation. After a Map step, the Reduce function must be notified regarding the availability of data before a Reduce operation. This elapsed time is called a shuffle. When there is an imbalance in computational loads between the two steps it can cause the execution time of one of them to delay and causing the skew problem [63] . However, various studies have tried to address this problem [64] , [65] ."}, {"section_title": "B. HIGH PERFORMANCE COMPUTING CLUSTER", "text": "High performance computing cluster (HPC) [160] is a highspeed computing paradigm made by a group of servers connected with a dedicated high-speed network. These individual servers are in some cases powered by arrays of GPUs (Graphical Processing Units). Each node in these clusters must solve a computing task. The cluster management is performed by a master node which also ensures proper parallelization using specific tools like OpenMP. Healthcare analytics need a high-speed robust computing paradigm hence HPD is highly regarded in building up health informatics. Samant et al. [157] have used HPC for the analysis of Deformable Image Registration(DIR). As DIR is a process that requires fast near real-time online analysis, the use of HPC computing offered significant performance acceleration for all the algorithms that were implemented."}, {"section_title": "C. SPARK", "text": "Spark was introduced as an improvement of Hadoop. It is a fast, general purpose, cluster computing platform for big data with a high-level API in Java, Scala, R, and Python. Spark can capture batch, streaming, and interactive jobs in a solution that combines some or all of them [70] . Perhaps the main capability of spark is the capability for the integration of a powerful machine learning library (MLlib) [56] .\nThe platform extends the MapReduce programming model with a data-sharing abstraction named ''Resilient Distributed Datasets,'' RDD [71] .\nThe importance of RDD provides one advantage. With its immutability, a single data object might be accessed in parallel and remain unaltered. Spark provides a faster parallel computing method than MapReduce. An example is the execution of the gradient descent for the minimization of the cost function in the logistic regression algorithm. To minimize the cost function, the gradient descent must be executed in a multitude of iterations, hence requiring more computing power.\nBecause Spark uses in-memory computing, it performs extremely faster. In the gradient descent execution, MapReduce takes around 110 s for each iteration as the data must be loaded from the disks. However, for the same operation, Sparks uses only one second for one iteration as it only needs to perform the first loading and the remaining iterations must be completed without memory loads. The big advantage that Spark demonstrates over MapReduce is that while MapReduce requires Impala [141] for querying operations and Mahout [146] for machine learning, for Spark you can develop an application and access all engines using a unified API. As an example, you would want to develop an application which needs to process user queries (SQL engine), predicts outcomes (Machine Learning) and maps user relationships (GraphX library) by using a single engine and in the main memory.\nSpark is a heavily used platform for healthcare big data analytics. It leverages its stream computing capabilities to perform faster analysis without the need to use other supportive frameworks. Wiewi\u00f3rka et al. [72] proposed SparkSeq [73] , a cloud-based genomic data analysis with nucleotide precision built on Spark. MacDonald [74] implemented the COPA (Cancer Outlier Profile Analysis) a system that analyze genes expression to detect repeated translocations for a given cancer type. Freeman et al. [75] built an analytical tool called Thunderbuilt that uses Apache Spark to analyze large-scale neural data obtained from a larval zebrafish brain. Apache Spark was used in this study to account for the high volume of data generated by neural recordings."}, {"section_title": "D. FLINK", "text": "Most of the big data platforms were designed for batch processing. However, data that needs real-time processing is increasing tremendously. The number of applications such as Twitter analytics, weblogs, and fraud detection has increased, and they need a real-time processing analysis. Even so, batch streaming is not dropped. Modern analytics application must encompass both batch and real-time data. Apache Flink [80] can process continuous data streams at the same time acknowledging that there is also a need to process historical batch data. From Fig.11 , we can see that even if Flink is built on a streaming processing engine, it has a Dataset API that processes batch datasets. Flink also offers a rich library support including machine learning, Graph API, and table API to process SQL like operations."}, {"section_title": "FLINK USE CASES IN HEALTHCARE", "text": "Apache Flink is an excellent platform of choice for eventdriven applications. Moreover, health analytics need to move away from a traditional reactive approach to a more proactive approach. Hence, a streaming platform is a choice for realtime health monitoring aspects of a health analytic application such as ECG monitoring, MRI readings, wearables monitoring, and other cyber-physical systems."}, {"section_title": "E. STORM", "text": "Apache Storm [79] is an alternative for Hadoop MapReduce when we need heavy real-time processing. Hence, when our analytic solution is expected to process huge data at a rapid rate, Apache Storm is the best performer for building such solutions. The key advantages of Storm are that it can work well for small and large-scale implementations and it is faulttolerant, scalable, and exhibits a higher reliability. Apache storm is superficially like Hadoop and while jobs are run in Hadoop, topologies are run in Storm. However, the key difference between jobs and topologies is that finally, a MapReduce job will finish while a Storm topology continues to handle incoming messages until the user terminates the process. The most basic data structure is a tuple made of a pair of elements and a given number of these tuples make a stream."}, {"section_title": "CRITERIA FOR CHOOSING A BIG DATA PLATFORM FOR A GIVEN HEALTHCARE APPLICATION", "text": "The choice of a big data platform for your health solution depends on many factors: Real-time needs, data size, speed, scalability, throughput etc.. . . Some healthcare applications like EMR records might not need real-time processing, hence for such application, a non-streaming platform like Hadoop MapReduce is enough. For others like ECG analysis will need a real-time intervention hence streaming will be paramount but scalability will not be a problem to consider for such applications.\nSome recommendation applications like diagnosis suggestion support will require a platform which can scale and accommodate the huge amount of data, in that case, a vertical scaling platform like CUDA [76] or High-Performance Computing (HPC) [77] will be of no use but horizontal scaling systems like Spark will be extremely useful. However, a health analytic solution encompasses many aspects of health analytic requirements hence the need to choose a platform that can be suitable for all these requirements. Another aspect to consider is fault-tolerance. This is the platform's ability to continue operating even after failure. To this aspect, all though not equally the horizontal scaling platforms like Hadoop and Spark wins as they distribute the work across many clusters whereas vertical scaling platforms have one point of failure. For a thorough comparison, Singh and Reddy [78] have compared the platforms with respect to their capabilities to perform a K-means machine learning algorithm. In this study though many criteria to consider for choosing an appropriate platform for a healthcare application are presented, few criteria are considered as paramount in processing healthcare big data. In table 3 we cover the most popular big data platforms and their use cases in healthcare analytics."}, {"section_title": "VII. CONCLUSION", "text": "The development of a scalable healthcare analytic application requires the amalgamation of various technologies whose choice requires a thorough scrutiny as a successful diagnosis and disease deterrence reckons on the incorporation of as many data sources as possible. In this work, we have highlighted most of the technologies to choose from to do so. The very first challenge is to choose from Big data platforms on which the application must rely upon. The platform should have all necessary libraries including the machine learning libraries. Spark provides integrated models for effective data ingestion, data processing as well as effective Machine learning libraries. Effective diagnosis requires medical images to be closely analyzed hence deep learning algorithms are better feet for identifying malignant spots on these images. Convolutional Neural Networks(CNN) which applies optimization algorithms like the Gradient Descent provides an effective analysis of life-threatening spots on images that are obtained from measurements like ECG, MRI, etc. As medical application needs enormous precision where a slight error can result in fatalities, utmost precision is required. The most challenging task in developing a robust solution is the aggregation of all data from divergent sources. As the semantics and context of data vary in a healthcare setting, the development of an inclusive middleware is the key to ingestion of all data. For data mining, no single algorithm provides a fit-all solution to health data. Hence an ensemble learning which includes the use of many machine learning algorithms can provide a better analysis. Social networks data's role into healthcare will continue to grow as more patient's health problems can be revealed on his social circles than to the physicians. This coupled with advances in Social Network Analysis as well as sentiment analysis tools will help practitioners to gain more insights than before. Genetics role in healthcare will be more evident. With the advances in human genomic researches, personalized healthcare can now be a reality. However, matching the genome sequences with medically related phenotypes is still an area which needs further researches."}]