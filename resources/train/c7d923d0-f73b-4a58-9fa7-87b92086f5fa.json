[{"section_title": "Introduction", "text": "The 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06), conducted for the U.S. Department of Education's National Center for Education Statistics (NCES), collected information about the education and employment experiences of students in the two years following their first enrollment in postsecondary education. The primary objective of BPS:04/06 is to follow a cohort of students from the start of their postsecondary education and collect further data from them in 2006 and 2009. This report describes the methodology and findings of the BPS:04/06 field test, which took place during the 2004-05 school year. The BPS:04/06 field test was used to plan, implement, and evaluate methodological procedures, instruments, and systems proposed for use in the full-scale study scheduled for the 2005-06 school year."}, {"section_title": "Sample Design", "text": "The respondent universe for the BPS:04/06 field test consisted of all students who began their postsecondary education for the first time during the 2002-03 academic year at any postsecondary institution in the United States or Puerto Rico. The students sampled were firsttime beginning postsecondary students (FTBs) who attended postsecondary institutions eligible for inclusion in the National Postsecondary Student Aid Study (NPSAS:04) 1 and who met the eligibility criteria for NPSAS:04. The BPS:04/06 field test sample was comprised of 2,610 2 students who had been identified as potential FTBs by their base-year sample institution, 310 of whom were confirmed as FTBs during the base-year interview. The remainder of the sample consisted of approximately 180 nonrespondents to the base-year interview, as well as a supplemental sample of potential FTBs who were not included in the NPSAS:04 field test. 3 \nThe BPS:04/06 field test sample was comprised largely of a supplemental sample of students selected from institution student lists submitted for the 2004 National Postsecondary Student Aid Study (NPSAS:04) field test data collection but not used. Over 80 percent of sample members were in the supplemental sample, and among the supplemental sample members and the base-year nonrespondents (7 percent of the sample), 17 percent were found to be study ineligible. For the full-scale, in contrast, a subsample of only 500 base-year nonrespondents will have unknown eligibility and will require administration of the first section of the interview on eligibility determination. However, in the full-scale administration, some base-year respondents will require a rescreening of their eligibility. First, among the 24,990 students who participated in the base-year interview and were identified as first-time beginners (FTBs), 1,370 students had evidence-from among institution records and/or federal financial aid sources 17 -contradicting their eligibility for BPS (false positives). Second, the institution and federal financial aid records for 1,420 students originally classified as \"other undergraduates\" in the NPSAS:04 base-year interview suggest that they may actually be eligible FTBs (false negatives). It is recommended that a brief set of items be developed to quickly verify eligibility for BPS before progressing to the main follow-up interview. These items would be administered to NPSAS interview nonrespondents and those with a questionable FTB status. The items should begin with a question verifying that the respondent's first enrollment in a postsecondary institution after high school occurred during the 2003-2004 academic year (excluding courses completed during high school and courses started but not completed) then confirm enrollment dates at the NPSAS sample institution and any additional institutions attended between high school and the end of the 2003-2004 academic year."}, {"section_title": "Instrumentation", "text": "Unlike instruments in previous BPS cycles, the BPS:04/06 student instrument was designed as a web-based instrument to be used both for self-administered \"interviews\" via the Web as well as by interviewers in both computer-assisted telephone interviews (CATI ) and computer-assisted personal interviews (CAPI). In addition, a study website was developed for access to the self-administered interview and to provide sample members with additional information about the study. The instrument was designed to accommodate the mixed-mode data collection approach and to ensure the collection of the highest quality data. Design considerations included the following: appropriate question wording for both self-administered and interviewer-administered surveys; providing extensive help text to assist self-administered respondents and interviewers; and pop-up boxes indicating out-of-range values. The instrument consisted of six sections grouped by topic. The first section was administered to base-year nonrespondents, and determined student eligibility for the NPSAS:04 study and the BPS study. The second section contained questions relating to postsecondary enrollment since the base year, collecting detail about all institutions attended and enrollment dates. The third section focused on the most recent enrollment characteristics, asking about major or field of study if declared, grade point average, education expenses, work while enrolled, student loan debt, and loan repayment, if applicable. The fourth section focused on the employment experiences of respondents no longer enrolled in postsecondary education. The fifth section of the interview gathered background and demographic information about students and their family members. The final section requested contacting information to make contact easier for the next follow-up interview.\nFeedback on the BPS:04/06 field test interview was obtained from a number of sources, including project staff observations while monitoring interviews, feedback from interviewers during quality circle meetings and as part of the debriefing, and from the National Center for Education Statistics (NCES) and members of the technical review panel who had the opportunity to review the survey as a self-administered and interviewer-administered interview. Modifications to the instrument include the clarifying items found to be particularly difficult, adding items to address particular topics of interest, and refining skip logic, particularly for baseyear respondents. As discussed in chapter 2, the field test interview compared coding systems used for coding text strings collected for major/field of study and occupation. For major/field of study, a set of drop-down boxes was compared to an assisted coding system on reliability and time required. For occupation, two versions of an O*NET coder were compared on the same dimensions. While the results (see chapter 4) showed no difference in the reliability of coders, the assisted coder for major/field of study took less time than the drop-down boxes and was the preferred method among interviewers. Consequently, the assisted coder is recommended for use in the BPS:04/06 full-scale instrument. For occupation, although the two types of coding systems were comparable in both reliability and timing, O*NET-B collects both job title and duties, yields more information about respondent jobs, and is recommended for use in the full-scale instrument."}, {"section_title": "Data Collection Design and Outcomes Interviewer Training", "text": "Field test training programs were developed for Help Desk operators (who also served as telephone interviewers) and field interviewers. Programs on successfully locating and interviewing sample members were developed for all telephone interviewers. Topics covered in telephone interviewer training included administrative procedures required for case management, quality control of interactions with sample members, parents, and other contacts; the purpose of BPS:04/06 and the uses of the data; and the organization and operation of the web-based student instrument to be used in data collection. Help Desk operators received essentially the same training as telephone interviewers because they were expected to complete the instrument over the telephone if requested by a caller; however, Help Desk operators also received specific training on \"frequently asked questions\" regarding the instrument and technical issues related to completing of the instrument via the Web. \u2022 Prompting calls will be made to base-year nonrespondents to increase interview participation during the early response period of data collection. \u2022 Incentives will be paid in the following manner: $30 for early response (all interviews completed within the first weeks of data collection), $20 during the production interviewing phase of data collection (when telephone interviewers make outbound calls to obtain interviews), $30 for all CAPI interviews, and $30 for nonresponse conversion, including refusals, hard to locate, and high call counts (more than 10 calls). \u2022 The full-scale instrument will use the coding systems chosen based on the comparison during the field test data collection based on efficiency and accuracy. To code major/field of study, an assisted coder will be used. To code occupations, an assisted coder based on O*NET that searches based on job title and duties will be used.  This introductory chapter describes the background, purposes, schedule, and products of the BPS:04/06 study, as well as the unique purposes of the field test. Chapter 2 describes field test design and procedures. Chapter 3 presents data collection results, including the results of two experiments implemented as part of the field test data collection. Chapter 4 presents evaluations of the quality of the data collected during the field test, and finally, chapter 5 summarizes the major recommendations for the full-scale study design based on field test findings. Materials used during the field test are provided as appendixes to the report and cited in the text where appropriate. All analyses conducted to evaluate the effectiveness of the BPS:04/06 procedures are discussed. Unless otherwise indicated, a criterion probability level of 0.05 was used for all tests of significance. Throughout this document, reported numbers of sample institutions and students have been rounded to further ensure the confidentiality of individual student data. As a result, row and column entries in tables may not sum to their respective totals, and reported percentages (based on unrounded numbers) may differ somewhat from those that would result from these rounded numbers."}, {"section_title": "Background and Objectives of BPS", "text": "Each academic year, several million students begin postsecondary education for the first time. The Beginning Postsecondary Students Longitudinal Study (BPS) series provides an opportunity to describe these students during their first year, and at multiple times after their first year. As one of several studies sponsored by NCES to respond to the need for a national, comprehensive database on postsecondary education, the BPS series addresses issues related to enrollment, persistence, progress, attainment, continuation into graduate/professional school, employment, and rates of return to society. The BPS series of studies is uniquely able to identify students as first-time beginners (FTBs) through its base study-the National Postsecondary Student Aid Study (NPSAS), a recurring survey of nationally representative, cross-sectional samples of postsecondary students designed to determine how students and their families pay for postsecondary education. Once identified, the BPS study series follows FTBs over a period of six years to monitor their progress in the issues of postsecondary education described above. Figure 1-1 presents the timelines for data collection for the base year and subsequent follow-up studies for each BPS in the series. The purpose of the BPS:04/06 follow-up is to monitor the academic progress and persistence in postsecondary education of 2003-04 FTB students during the 3 years following their initial entry into a postsecondary institution. The data collection will focus on continued education and experience, education financing, entry into the workforce, the relationship between experiences during postsecondary education and various societal and personal outcomes, and returns to the individual and to society on the investment in postsecondary education. The second follow-up of the BPS:04 cohort, scheduled for 2009, will monitor students' academic progress in the 6 years following their first entry into postsecondary education and will be able to assess completion rates in 4-year programs. Data collected will continue to focus on education and employment, and the survey will include many of the questions used in the first follow-up. The second follow-up will also be enhanced to focus on graduate and professional school access issues, and to further explore rate of return issues for those who will have completed their education."}, {"section_title": "Overview of the Field Test Study Design", "text": "The major purpose of the BPS:04/06 field test was to plan, implement, and evaluate operational and methodological procedures, instruments, and systems proposed for use in the full-scale study, particularly procedures that have not been previously tested. Some of the major topics tested and evaluated in the field test included the following: \u2022 Web-based self-administered interviewing, with help desk support, followed by computer-assisted telephone interview (CATI) and computer-assisted personal interview (CAPI) data collection. \u2022 Two experiments to determine: 1. whether prompting telephone calls made to sample members during the early response period of self-administered interviewing would increase response rates compared to sample members who did not receive the telephone prompt; and 2. whether offering a monetary incentive to sample members during the production interviewing phase of data collection would increase response rates compared to sample members not offered an incentive. \u2022 A comparison of two systems designed to code major field of study and occupational categories to determine which system to use in the full-scale data collection."}, {"section_title": "Schedule and Products of BPS:04/06", "text": "Table 1-1 summarizes the schedule for the field test, as well as the proposed schedule for the full-scale study in 2006. Electronically documented, restricted-access research files (with associated electronic codebooks) as well as NCES Data Analysis Systems (DASs) for public release will be constructed from the full-scale data and made available to a variety of organizations and researchers. BPS:04/06 will produce \u2022 a full-scale methodology report, providing details of sample design and selection procedures, data collection procedures, weighting methodologies, estimation procedures and design effects, and the results of nonresponse bias analyses; \u2022 special tabulations of issues of interest to the higher education community, as determined by NCES; and \u2022 a descriptive summary of significant findings for dissemination to a broad audience. This is the date on which the activity was or will be initiated. 2 This is the date on which the activity was or will be completed. "}, {"section_title": "NOTE: RIMS/OMB = Regulatory Information", "text": ""}, {"section_title": "Chapter 2 Design and Method of the BPS:04/06 Field Test", "text": "The purpose of the 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06) field test was to fully test all procedures, methods, and systems of the study in a realistic operational environment prior to implementing them in the full-scale study. This chapter describes the design of the field test data collection. An overview of the sampling design, sample member locating and contacting activities, interview design, and data collection procedures is presented, together with a description of the systems developed to support the BPS:04/06 field test data collection."}, {"section_title": "Sampling Design", "text": "The respondent universe for the BPS:04/06 field test consisted of all students who began their postsecondary education for the first time during the 2002-03 academic year at any Title IV-eligible postsecondary institution in the United States or Puerto Rico. The sample students were the first-time beginners (FTBs) from the 2004 National Postsecondary Student Aid Study (NPSAS:04) field test. Institutions eligible for the NPSAS:04 field test were required during the 2002-03 academic year to meet all the requirements for distributing Title IV aid, including: \u2022 offering an educational program designed for persons who have completed secondary education; \u2022 offering at least one academic, occupational, or vocational program of study lasting at least 3 months or 300 clock hours; \u2022 offering courses that are open to more than the employees or members of the company or group (e.g., union) that administers the institution; and \u2022 being located in the 50 states, the District of Columbia, or Puerto Rico. Institutions providing only vocational, recreational, or remedial courses or only in-house courses for their own employees were excluded, as were U.S. Service Academies because of their unique funding/tuition base. These institution eligibility criteria were completely consistent with previous NPSAS studies with two exceptions. First, the requirement to be eligible to distribute Title IV aid was implemented beginning with NPSAS:2000. 1 Second, the previous NPSAS studies excluded institutions that only offered correspondence courses. NPSAS:04 included such institutions if they were eligible to distribute Title IV student aid. The institutional sampling frame for the NPSAS:04 field test was constructed from the 2001 Integrated Postsecondary Education Data System (IPEDS) Institutional Characteristics (IC) and header files, and the 2001 Fall Enrollment file. A field test sample of 200 institutions was selected purposively from the complement of institutions selected for the full-scale study. This ensured that no institution would be burdened with participating in both the field test and fullscale studies while maintaining the representativeness of the full-scale sample. Certainty institutions for the full-scale study were excluded from the field test. The certainty institutions either were in strata where all institutions were selected or had expected frequencies of selection greater than unity (1.00). The field test sample of institutions was selected to approximate the distribution by institutional stratum for the full-scale study. The distribution of the field test institutional sample is presented in table 2-1. Overall, about 98 percent of the sampled institutions met the NPSAS eligibility requirements; of those, about 89 percent provided enrollment lists for student sampling. The students eligible for the BPS:04/06 field test were those eligible to participate in the NPSAS:04 field test who were FTBs at NPSAS sample institutions in the 2002-03 academic year. Consistent with previous studies, NPSAS-eligible students were those enrolled in eligible institutions who satisfied the following eligibility requirements: \u2022 were enrolled in either (a) an academic program; (b) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; or (c) an occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; \u2022 were not concurrently enrolled in high school; and \u2022 were not concurrently or solely enrolled in a General Equivalency Diploma (GED) or other high school completion program. NPSAS-eligible students who were FTB students at the NPSAS sample institutions were eligible for the BPS:04/06 field test. Those NPSAS-eligible students who enrolled in a postsecondary institution for the first time during the NPSAS year (i.e., July 1, 2002-June 30,2003) after completing high school were considered pure FTBs and were eligible for BPS:04/06. Those students who had enrolled for at least one course after completing high school but had never completed a postsecondary course before the 2002-03 academic year were considered effective FTBs and were also eligible for membership in the BPS:04 cohort. The student sample sizes for the NPSAS:04 field test were set to approximate the distribution planned for the NPSAS:04 full-scale study, with the exception that additional FTBs were selected in order to have more available for the BPS:04/06 field test. As shown in table 2-2, the NPSAS:04 field test was designed to sample 1,290 students, including 810 first time beginner students; 360 other undergraduate students; and 130 graduate and first-professional students. There were eight student sampling strata for the NPSAS:04 field test: \u2022 four sampling strata for undergraduate students: \u2212 FTB in-state tuition students, \u2212 FTB out-of-state tuition students, \u2212 other undergraduate in-state tuition students, and \u2212 other undergraduate out-of-state tuition students; \u2022 three sampling strata for graduate students: \u2212 other graduate students; and \u2022 a sampling stratum for first-professional students. The numbers of FTB students shown in table 2-2 included both true FTBs who began their postsecondary education for the first time during the NPSAS field test year and effective FTBs who had not completed a postsecondary class prior to the NPSAS field test year. Unfortunately, postsecondary institutions cannot readily identify their FTB students. Therefore, the NPSAS sampling rates for students identified as FTBs and other undergraduate students by the sample institutions were adjusted in order to achieve the expected counts after accounting for expected false positive and false negative rates. The false positive and false negative FTB rates experienced in NPSAS:96 (i.e., the most recent NPSAS to include a BPS base-year cohort) were used to set appropriate sampling rates for the NPSAS:04 field test. 2 The overall expected and actual student sample sizes are shown in table 2-2. Table 2-2. Expected and actual NPSAS:04 field test student samples, by student type and level of  institutional stratum: 2005   Student type and institutional stratum   Expected student  sample size   1   Actual student  sample size  Total  1,290  1,280   Potential FTB  810  790  Less-than-2-year  200  80  2-year  360  410  4-year  250  To create the student sampling frame for NPSAS:04, each participating institution was asked to provide a list of eligible students from which the student samples were selected. For the NPSAS:04 field test, students were selected from the first 80 institutions that provided lists. These 80 institutions provided a sufficient variation and number of sample students for the NPSAS:04 field test. However, because only 1,290 expected sample students were selected from the participating institutions, the sample size per institution was too small to adequately test procedures during the BPS:04/06 field test. Consequently, student lists from institutions not already used for the NPSAS:04 field test were used to supplement the field test sample for BPS:04/06, as described below. Table 2-3 provides the interview results from the NPSAS:04 field test for each of the institutional stratum. Of the 1,280 students sampled for the field test, 1,160 were determined to be NPSAS-eligible. There were 820 student interview respondents, and 310 of these were confirmed as FTBs in the student interview. The BPS:04/06 field test sample was drawn from the NPSAS:04 field test interview study respondents who confirmed their FTB status, and from most of the nonrespondents who were identified as potential FTBs by their institutions. However, to obtain the 1,000 interviews needed to adequately test the interview and procedures across institutional strata, the field test sample included a supplemental sample of potential FTBs not previously contacted for the NPSAS:04 field test. Each of these three groups is described below. Table 2-4 provides the details of the  field test sample distribution. \u2022 Confirmed FTBs who responded to NPSAS:04-All 310 of the students who responded to the NPSAS field test student interview and verified their FTB status. \u2022 Potential FTBs who were NPSAS:04 nonrespondents-Of the 340 students sampled for but who did not respond to the NPSAS:04 field test student interview, 210 were identified as FTBs by their sample institution and had a valid Social Security number. 3 To improve the likelihood that base-year nonrespondents would be eligible for inclusion in the BPS:04 cohort, the indicator for FTB status according to U.S. Department of Education's Central Processing System (CPS) 4 was considered whenever possible. Students who matched to CPS (2002/03) and were identified as FTBs (90 students) were included in the sample, as were base-year nonrespondents identified as potential FTBs by their institution who did not match to CPS (80 students), for a total of 180 students. Due to the difficulty in locating and interviewing nonrespondents to prior studies, students identified as FTBs by their institution who matched to CPS but were not identified by CPS as FTBs (40 students) were excluded from the sample. \u2022 Potential FTBs not yet contacted-A supplemental sample of 2,120 students selected for the NPSAS:04 field test but not included in the final base-year student sample was also included in the BPS:04/06 field test sample. To increase the likelihood of locating and interviewing an FTB from this group of students, the supplemental sample was restricted to those identified as FTBs by institution indicators with a valid Social Security number, and with locating information either from CPS or Telematch. The number of students in each group fielded for the BPS:04/06 field test data collection are presented in table 2-4. The field test sample for BPS:04/06 was designed to yield a total of 1,000 respondents. 1 Excludes 40 cases for whom CPS FTB indicator was \"no.\" NOTE: Numbers have been rounded to the nearest 10. Detail may not sum to totals because of rounding. Other includes public less-than 2-year, private not-for-profit 2-year, private not-for-profit less-than 2-year, and private for-profit 2-year and higher institutions. "}, {"section_title": "Data Collection Design", "text": "This section provides an overview of the procedures implemented for the BPS:04/06 field test data collection. The field test design offered sample members the option of completing a web-based, self-administered interview rather than either a telephone or in-person interview. It also tested the efficacy of telephone prompting and paying cash incentives on improving response rates, and compared results from different coding systems developed for coding major/field of study and occupation. Other design topics presented include website design, tracing and contacting sample members, and data collection systems."}, {"section_title": "Interview Design", "text": "The BPS:04 longitudinal series, beginning with the NPSAS:04 base-year interview, was the first of the BPS series to provide respondents the option of completing a self-administered interview. A single, web-based interview was developed to use in three modes: self-administered interview, computer-assisted telephone interview (CATI), and computer-assisted personal interview (CAPI). Sample members could access the interview directly from the study website by entering the Study ID and password provided to them in a mailing. Telephone interviewers could access the interview via RTI's integrated case management system, while field interviewers accessed the interview through an independent case management system installed on each field laptop. The content of the first follow-up interview remained primarily the same as that used in prior BPS first follow-up interviews (BPS:90/92 and BPS:96/98), building upon data elements developed with input from the study's Technical Review Panel (TRP) as well as from the National Center for Education Statistics (NCES). (See appendix A for a list of TRP members and appendix B for a list of the final set of data elements). The interview consisted of six sections, grouped by topic. Only base-year nonrespondents were asked questions in the first section, which determined eligibility both for NPSAS and for BPS. This section collected a subset of information already collected for respondents; specifically, postsecondary enrollment during the NPSAS year (July 1, 2002 to June 30, 2003), type of program, reasons for attending the sample institution, information on high school completion, and date of birth. The next sectioneducation history-was asked of all respondents and focused on their enrollment after the first year; that is, between July 1, 2003 and the time of the follow-up interview. All institutions attended and any degrees or certificates earned were collected, together with the dates of enrollment. The third section, education characteristics, focused on the respondent's experiences while enrolled. Questions pertained to the most recent degree sought, major or field of study if declared, grade point average, education expenses, work while enrolled, student loan debt, and loan repayment, if applicable. The fourth section of the interview, on post-enrollment employment, was asked only of respondents who were no longer enrolled in postsecondary education, whether or not they completed a degree/certificate. The fifth section collected and updated as needed student demographic characteristics, including race/ethnicity, citizenship, voting behavior, marital status and family composition, volunteerism, disability status, and goals. The final section collected contact information that will be used in locating sample members for the final follow-up data collection in 2008 (as part of BPS:04/09). Respondents were guided through each section of the interview according to skip logic that took into account both their current interview answers and any preloaded data available from the base year. Help text was available by clicking on the help text link on each interview page. Pop-up messages were used to clarify inconsistent and out-of-range values, and to convert item nonresponse. Like past BPS interviews, coding systems for standardizing the collection of data on schools attended, major or field of study, occupation, and industry were included in the BPS:04/06 field test interview. As part of the field test data collection, the effectiveness and time involved in using different coding systems for major and occupation were evaluated to identify the most efficient and reliable system for full-scale data collection. During data collection, text strings were collected for all majors and occupations before the strings were coded. For major, respondents were randomly assigned to use either a pair of drop-down boxes containing general areas and, as applicable, secondary areas of study, or an assisted coder which returned one or more specific areas of study that matched most closely to the text string provided by the respondent. If no areas matched, respondents were offered the dual drop-down boxes used by the other group. The same set of general and specific areas was used for the drop-down boxes and the assisted coder. Two different assisted coding systems for occupation, built from the O*NET database (for more information on O*NET, see http://online.onetcenter.org), were tested during the field test. Respondents were asked to first enter their job title and job activities. In the first version (O*NET-A) an assisted coder then returned a set of possible categories based on the job title provided. In the second version (O*NET-B) an assisted coder then returned a set of possible categories based on both the provided job title and activities. Like the major coder, the same set of codes was available from the two different O*NET coders; only the mechanism for identifying the codes differed. If none of the options based on the database search was selected, respondents were directed to a series of drop-down menus from which they selected a general category, a specific category, and finally a detailed category. Results of the major and occupation coding system comparisons are presented in chapter 4."}, {"section_title": "Pre-Data Collection Activities", "text": "Prior to the start of data collection, a study website was designed for use by BPS:04/06 field test sample members for updating address information and accessing the self-administered interview. The website also provided general information about the BPS set of studies, previous findings, contact information for the study Help Desk and project staff at RTI, and links to the NCES and RTI websites. The website was made available to sample members at the time of the first mailing to them, prior to data collection. Figure 2-1 shows the home page for the BPS:04/06 field test website. Designed according to NCES web polices, it used a two-tier approach to security to protect any data collected. At the first tier, sample members could log onto the secure areas of the website using a unique Study ID and password provided them prior to the start of data collection. At the second tier, data entered on the website were protected with Secure Sockets Layer (SSL) technology, which allowed only encrypted data to be transmitted over the Internet. "}, {"section_title": "Student Locating and Contacting", "text": ""}, {"section_title": "Pre-Data Collection Locating and Contacting", "text": "Tracing activities for all students selected for the BPS:04/06 field test were conducted prior to the start of data collection and before any mailouts to students and their families occurred. Batch searches using the U.S. Department of Education's CPS and the U.S. Postal Service's National Change of Address (NCOA) database were conducted using contact information available for each sample member and their parents. In November 2004, an initial mailing was sent to the parents of dependent sample members. The mailing included a study leaflet (see appendix C), an address update sheet, and a business reply envelope, together with a letter introducing the BPS:04/06 study and requesting parents' cooperation and assistance in locating the sample member. All updated addresses produced by the parent mailing were noted in the receipt control system (described below)."}, {"section_title": "Student Notification Mailings", "text": "In January 2005, a mailing to students was sent to the best known address. The accompanying letter announced the upcoming data collection and asked sample members to update their address information. The mailing included a study leaflet, address update sheet, and a business reply envelope. A link to the study website was provided so that sample members could update their address directly. Closer to the start of self-administered interviewing, all address information for sample members was sent to Gannett Co., Inc.'s Telematch service to obtain new telephone numbers and/or update existing numbers. Immediately prior to the April 1, 2005 start of data collection, a postcard announcing the availability of the web-based self-administered interview was sent to each sample member's current address. The postcard provided a unique Study ID and password and informed sample members that by completing the interview by April 24, 2005, they would receive $30. The postcard was folded and sealed with a mailing tab to ensure the privacy of the enclosed information. At the same time as the postcard mailing, a comparable mailing was sent via electronic mail (e-mail) to those sample members for whom a working e-mail address was available (provided during the base-year interview by the student or the institution, or in response to the student notification mailing via the address update sheet or the student website). Additional e-mail prompts were sent to nonrespondents throughout the course of data collection to encourage their participation."}, {"section_title": "Locating During Interviewing", "text": "Once telephone interviewing began, telephone interviewers would conduct limited tracing and locating activities as needed. These included calling all telephone numbers and contacts for a sample member or talking to persons answering the telephone to determine how to contact the sample member. When a sample member could not be located at a known address during CATI, interviewers conducted limited tracing using First Data Solutions FASTData batch locating service and directory assistance services. Cases that could not be located using any of the existing address information were identified for intensive tracing in RTI's Call Center Services (CCS). Cases that failed to be located a second time were either sent to the field for locating and interviewing, or returned to CCS for additional intensive tracing."}, {"section_title": "Intensive Tracing", "text": "The most difficult locating cases were sent to CCS for intensive tracing using a number of online tracing sources, beginning with the credit bureau services (Experian, TransUnion, and Equifax) for those cases with a Social Security number. Any new information obtained was processed immediately and the case returned to production interviewing. Remaining cases underwent a more intensive level of tracing, which included calls to directory assistance, alumni offices; contacts with neighbors and/or landlords, and other locating strategies. Each case was handled individually based on the extent of information already available, the age of the locating data, and the presence of a Social Security number."}, {"section_title": "Field Tracing", "text": "During the field test, a subset of the unlocatable cases was sent to field interviewers for tracing and interviewing. Field interviewers received all address information available for an assigned case, the results of any tracing conducted to date, and the results of efforts made by telephone interviewers to reach the sample member. Field interviewers used any and all tracing resources available to them, including many local resources not otherwise known or available outside the geographic area, contacts with the U.S. Postal Service, and searches of public records."}, {"section_title": "Student Interviewing", "text": "The BPS:04/06 field test data collection began with an early response period of about three weeks (April 1 through April 24, 2005), during which sample members could complete a self-administered interview via the Internet. A toll-free hotline to the study Help Desk was set up to assist those who had problems accessing the website or questions about the survey. If technical difficulties prevented a sample member from completing the interview, a Help Desk staff member, also trained to conduct telephone interviews (see appendix D for sample training materials), would encourage him or her to complete a telephone interview rather than attempt the web interview. An application designed for the Help Desk documented all calls from sample members and provided \u2022 information needed to verify a sample member's identity; \u2022 login information allowing a sample member to access the web interview; \u2022 systematic documentation of each call; and \u2022 a method for tracking calls that could not be immediately resolved. Reports on the types and frequency of problems experienced by sample members as well as a way to monitor the resolution status of all Help Desk inquiries were available to project staff. At the end of the early response period, the production interviewing phase of data collection (outbound CATI) began. Professionally-trained interviewers placed outgoing calls to sample members to complete a telephone interview. The interviewer-administered interview was identical to the self-administered interview, except that instructions to interviewers on how to administer each question were embedded at the top of each CATI screen. An automated callscheduler assigned cases to interviewers and allowed calls to be scheduled by case priority and time of day. If a self-administered interview was in progress or had recently been completed, the scheduler would prevent a CATI call to that case. If a sample member told an interviewer that he or she preferred to complete the self-administered interview, interviewers would set a call back appointment for 2 weeks from the date of the original contact for follow-up in the event that a self-administered interview had not yet been completed. CAPI or field interviewing with sample members who had not yet responded began June 6, 2005, following several weeks of CATI interviewing. Field interviews were conducted either in person or by telephone by local field interviewers assigned to any one of ten geographic clusters based on the last known address for the sample member: San Bernardino, Fresno, and Oakland, CA; Atlanta, GA; Topeka, KS; Brooklyn, NY; Greensboro, NC; Akron, OH; Philadelphia, PA; and Portsmouth, VA. Cases assigned to the field could not be accessed by CATI interviewers but could still be completed as a self-administered interview over the Internet. Like the CATI interview, the CAPI interview presented interviewer instructions at the top of each screen."}, {"section_title": "Prompting Experiment", "text": "Two experiments to improve response rates were included in the BPS:04/06 field test data collection. The first evaluated the effectiveness of prompting calls in increasing response rates during the early response period. The entire field test sample was notified that the interview link was available on the study website as of April 1, 2005, and that by completing the selfadministered interview within the specified time frame, they would receive $30. A Study ID and password for each sample member was provided as well. Sample members were contacted both by regular mail and by e-mail, if an e-mail address was available. Prior to data collection, the field test sample was randomly assigned to two groups: one would receive prompting calls about halfway through the early response period. These calls were distributed throughout the one week prompting period. Messages were left for sample members beginning with the third call, and a maximum of five call attempts were made overall.  The prompting calls served to provide another reminder about the study and the time frame in which the interview needed to be completed to qualify for the early response incentive, and provided the required login information if needed. Furthermore, the prompting calls allowed early tracing and locating of the sample member for individuals no longer at the address on file. While every effort was made to trace sample members in advance of the start of data collection, it was often several weeks into data collection before change of address notifications were received. For sample members in the prompting group who were unlocatable, more intensive tracing was conducted before the case was routed for telephone interviewing, saving time and project resources. Following the early response period, interview completion rates for the two groups (prompted versus not prompted) were compared. The results of the prompting experiment are presented in chapter 3."}, {"section_title": "Use of Incentives", "text": "The BPS:04/06 field test offered sample members an early response incentive of $30 for completing the web-based self-administered interview before production interviewing began on April 25, 2005. In addition, a nonresponse conversion incentive of $30 was offered if, during production interviewing, a sample member refused to be interviewed, was found to have a good mailing address but no telephone number, or was identified as hard to reach (i.e., those with 10 or more call attempts and with whom contact had been established but no appointment scheduled). In these cases, the respondent was removed from the experimental portion of data collection and offered a nonresponse incentive of $30. The nonrespondent incentive mailing consisted of a letter tailored to the specific type of nonrespondent (see appendix C) and an offer to receive a $30 check upon completion of the interview. All cases assigned to the field (CAPI) were also eligible to receive the $30 incentive. In addition to the early response and nonresponse incentives, a second field test experiment evaluated the impact of paying monetary incentives during production interviewing when interviewers placed outgoing calls to complete telephone interviews. The effectiveness of monetary incentives in improving response rates during early response and nonresponse conversion periods has been established in past data collection efforts for BPS and studies with similar populations (i.e., NPSAS, B&B, ELS). 5 "}, {"section_title": "Data Collection Systems", "text": ""}, {"section_title": "Instrument Development and Documentation System (IDADS)", "text": "The Instrument Development and Documentation System (IDADS) was a combination web and Visual Basic (VB) environment in which project staff developed, reviewed, modified, and communicated changes to specifications, code, and documentation for the BPS:04/06 field test instrument. All information relating to the instrument was stored in a Structured Query Language (SQL) Server database and was made accessible through web browser and Windows VB interfaces. There were three modules within IDADS: specification, programming, and documentation. Specification module. The IDADS specification module provided tools and graphical user interfaces for creating, searching, reviewing, commenting on, updating, importing, and exporting information associated with instrument development. A web interface provided access to the instrument specifications for project staff at MPR Associates, Inc. (MPR), and NCES. Programming module. Once specifications were finalized, the programming module within IDADS produced hypertext transfer markup language (HTML), Active Server Pages (ASP), and JavaScript template program code for each screen based on the contents of the SQL Server database. This output included question wording, response options, and code to write the responses to a database, as well as code to automatically handle such web instrument functions as backing up and moving forward, recording timer data, and linking to context-specific help text. Programming staff edited the automatically-generated code to customize screen appearance and to program response-based routing. Documentation module. The documentation module contained the finalized version of all instrument items, their screen wording, and variable and value labels. Also included were the more technical descriptions of items such as variable types (alpha or numeric), to whom the item was administered, and frequency distributions for response categories. The documentation module was used to generate the instrument facsimiles and the deliverable Electronic Codebook (ECB) input files."}, {"section_title": "Integrated Management System (IMS)", "text": "All aspects of the study were controlled using an Integrated Management System (IMS). The IMS was a comprehensive set of desktop tools designed to give project staff and NCES access to a centralized, easily accessible repository for project data and documents. The BPS:04/06 IMS consisted of several components: the management module, the Receipt Control System (RCS) module, and the instrumentation module. Management module. The management module of the IMS included tools and strategies to assist project staff and the NCES project officer in managing the field test data collection. All management information pertinent to the study was located there, accessible via the Web, and protected by SSL encryption and a password-protected login. The IMS contained the current project schedule, monthly progress reports, daily data collection reports and status reports (generated by the RCS described below), project plans and specifications, project deliverables, instrument specifications, staff contacts, the project bibliography, and a document archive. The IMS also had a download area from which staff at MPR and NCES could retrieve files as necessary."}, {"section_title": "Receipt Control System (RCS).", "text": "The RCS is an integrated set of systems that was used to monitor all activities related to data collection, including tracing and locating. Through the RCS, project staff were able to perform stage-specific activities, track case statuses, identify problems early, and implement solutions effectively. The RCS's locator data were used for a number of daily tasks related to sample maintenance. Specifically, the mailout system produced mailings to sample members, the query system enabled administrators to review the locator information and status for a particular case, and the mail return system enabled project staff to update the locator database as mailings or address update sheets were returned or forwarding information was received. The RCS also interacted with the CCS database, sending locator data between the two systems as necessary. A subcomponent of the RCS, the Field Case Management System (FCMS), controlled field interviewing activities. The FCMS allowed field staff to conduct tracing and CAPI, communicate with RTI staff via e-mail, transmit completed cases, and receive new cases. Instrumentation module. The instrumentation module managed development of the multimode web data collection instrument within IDADS. Developing the instrument with IDADS ensured that all variables were linked to their item/screen wording and thoroughly documented."}, {"section_title": "The Variable Tracking System (VTS)", "text": "The central mechanism for constructing input files for the NCES ECB was a software application called the Variable Tracking System (VTS). The VTS tracked and stored documentation for both interview and derived variables required for the ECB and NCES' Data Analysis System (DAS). This included weighted and unweighted variable distributions, variable labels and codes, value labels, and a text field describing the development of each variable and the programming code used to construct it. Input files for the ECB and DAS systems were automatically produced by the VTS according to NCES specifications."}, {"section_title": "Chapter 3 Data Collection Outcomes", "text": "The data collection effort for the 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06) field test involved several steps, including attempting to locate sample members, initiating intensive locating efforts for hard-to-locate sample members, evaluating the utility of incentives paid throughout the course of data collection, and completing either a self-administered, telephone, or in-person interview. This chapter reports the data collection outcomes of the field test. The response rates are reported first, including an overall summary of results, followed by a discussion of interviewing outcomes by prior response status, mode, and tracing. The second section discusses the interview burden on respondents, including times to complete various sections and transmit data. Results are presented for the entire interview, overall and by section. Timing results by mode of administration are also discussed. The third section discusses the results of the prompting experiment, and the fourth section offers conclusions."}, {"section_title": "Response Rates", "text": "This section will discuss contacting and interviewing outcomes, including response rates by mode and base-year response status; and locating and interviewing outcomes by tracing sources and methods."}, {"section_title": "Overall Summary of Interviewing Results", "text": "Overall locating and interviewing results for the BPS:04/06 field test are presented in figure 3-1. The sample for the field test study consisted of those sample members who participated in the base-year study-the 2004 National Postsecondary Student Aid Study (NPSAS:04) field test, as well as nonrespondents to the base-year interview, and a supplemental sample who were identified as potential first-time beginners (FTBs) by their sample institutions (described in chapter 2). Of the 2,610 sample members, 1,890 were located and 720 were not located. Of the cases that were located, 10 were excluded and 380 were found to be ineligible. The overall unweighted response rate 6 among eligible sample members was 47.7 percent. Among cases that were located, however, the response rate was 70.2 percent. Table 3-1 presents the distribution of response rates by type of interview completed and prior response status. Full interviews were completed by 99 percent of base-year respondents, and by 99 percent of the supplemental sample members. Approximately 95 percent of interviews completed by base-year nonrespondents were full interviews, while 5 percent of base-year nonrespondents completed a partial interview.   Table 3-2 presents the distribution of response rates overall and by base-year response status. Response rates are presented in two ways: among eligible sample members, and among eligible sample members who were located. Among all eligible sample members, approximately 48 percent completed the first follow-up interview. The response rate among eligible sample members who were base year respondents was 74 percent. Among all eligible base year nonrespondents, 27 percent completed the first follow-up interview, and approximately 45 percent of eligible supplemental sample members completed an interview."}, {"section_title": "Interviewing Outcomes by Prior Response Status", "text": "Approximately 83 percent of sample members who participated in the base-year interview were located, and nearly 90 percent of those located completed the interview. Locate and response rates were much lower among those who were not interviewed in the base year. For nonrespondents, 57 percent were located, and 47 percent of those located completed the interview. Among the supplemental sample, 66 percent were located, and 68 percent of those located completed an interview. Response rates among those located by the sector of the base-year institution are presented in table 3-3. Response rates ranged from 47 percent for private not-for-profit less than 4-year institutions, to 82 percent for private not-for-profit 4-year doctorate-granting institutions. Among all eligible sample members, including those not located, response rates ranged from 32 percent for private for-profit less-than-two-year institutions, to 70 percent for private not-forprofit 4-year doctorate-granting institutions. "}, {"section_title": "Interviewing Outcomes by Mode", "text": "The distribution of interview completions by mode of administration is presented in table 3-4. About 48 percent completed the self-administered interview, and nearly 52 percent completed an interview administered by an interviewer, either a telephone or a personal interview (41 percent and 11 percent, respectively). The majority of self-administered interviews (80 percent) were completed during the early response period (the first three weeks of data collection). Computer-assisted telephone interviewing (CATI) began on April 25, 2005, and continued until the end of July. By the end of data collection, 430 interviews had been completed by telephone, including 20 partial interviews. The last phase of field test data collection was computer-assisted personal interviewing (CAPI). Table 3-5 presents locate and interview rates among cases sent for field interviewing. In the field test, about 450 cases were sent for CAPI. Overall, about 200 of those cases were located, and 61 percent of these located were interviewed. Table 3-5 also presents CAPI response rates by base-year response status and stratum of the base-year institution. Among CAPI cases, response rates were 90 percent for base-year respondents, 68 percent for nonrespondents, and 56 percent for members of the supplemental sample. Once located, CAPI response rates ranged from 46 percent for private for profit 2-year or more institutions, to 100 percent for private not-for-profit less than 4-year institutions.  "}, {"section_title": "Locating and Interviewing Outcomes Tracing", "text": "For the BPS:04/06 field test, tracing began in the fall of 2001 by updating address and other contact information collected during the NPSAS:04 field test interview. Several tracing resources were used, including the Central Processing System (CPS), which contains federal financial aid application information, TransUnion's credit information, and databases from Telematch, Comserv, Inc.'s Death Information System (DIS), FASTData, and the National Change of Address (NCOA) file. All 2,610 potential field test cases were sent for batch tracing, and the sample was subsequently stratified and subsampled, based in part upon the information gathered during batch tracing. Table 3-6 shows the record match rate for each method of batch tracing employed. In addition, as part of each mailing to sample members and their parents, sample members were asked to complete an address update form either on the study website or on a hardcopy form. Table 3-7 shows the locate and interview rates for those who returned some form of address update sheet. Almost all sample members who provided updated address information were located (97 percent), and about 76 percent of those who updated their contact information completed an interview. Locate and interview rates by intensive tracing status are shown in table 3-8. Of cases that were sent to the first stage of intensive tracing, or CCS-1, 42 percent were located, and 28 percent of those completed an interview. Among cases sent to the second stage of intensive tracing, or CCS-2, 26 percent were located and 31 percent of them were interviewed. 960 50 4.9 1 Percent is based on the number of records sent for batch tracing. Since records were sent to multiple tracing sources, multiple record matches were possible. 2 The entire sample was sent to the NCOA, excluding 10 cases that did not have mailing addresses. 3 The FASTData search was conducted only for a subset of the original sample because it occurred late in data collection, after many cases had been completed or otherwise finalized.   "}, {"section_title": "Interview Burden", "text": "The following section presents the results of various analyses of the burden associated with the BPS:04/06 field test interview. The time required to complete the student interviews is examined. Next, timing measures associated with interviewing staff-hours and calls per caseare presented."}, {"section_title": "Time to Complete the Student Interview", "text": "Overall interview completion times. In order to monitor the time required to complete the student interview, two time stamp variables were associated with each interview question. The first, the start timer, was set to the clock time on the respondent's or interviewer's computer at the time that a particular web page was displayed on the screen. The second time stamp variable, the end timer, was set to the clock time on the respondent's or interviewer's computer at the moment the respondent or interviewer clicked the \"Continue\" button to submit the answers from that page. From the two time stamp variables, an on-screen time and transit time were calculated. The on-screen time was calculated by subtracting the start time from the end time for each web page that the respondent received. The transit time was calculated by subtracting the end time of the preceding page from the start time of the current page; it includes the time required for the previous page's data to be transmitted to the server, for the server to store the data and assemble and serve the current page, and for the current page to be transmitted to and loaded on the respondent's or interviewer's computer. A total on-screen time was then calculated for all respondents by summing the on-screen times for each web page that the respondent received. For each respondent, a total transit time was calculated by summing all the transit times. The total on-screen and total transit times were then summed to determine the total instrument time. On average, the BPS:04/06 field test interview took 25 minutes to complete. Table 3-9 presents the average interview completion time overall and by mode of administration. Interviewer-administered respondents, with an average time of 27 minutes, took longer to complete the field test interview than web respondents, who averaged 23 minutes (t = -8.77, p < 0001). Base-year enrollment, the first and one of the shortest sections of the field test interview, took about 4 minutes to complete. It was administered to base-year nonrespondents in order to determine eligibility for BPS:04/06. Section A focused on critical NPSAS and BPS eligibility criteria such as high school completion and the date of first postsecondary enrollment. The second section collected information about respondents' enrollment history, and took an average of about 3 minutes to complete. Respondents were asked to report enrollment information at all schools attended since the base year (or, including the base year for NPSAS nonrespondents). The time to complete section B varied depending on the number of schools attended. Section C proved the longest section in the BPS:04/06 field test interview, lasting nearly 7 minutes. This section asked respondents to provide information about their degree program, major or field of study, GPA, academic experiences, jobs while enrolled, earnings, and loan status. Section C applied only to respondents who had been enrolled in postsecondary education at some point since the NPSAS base year (the 2003-04 academic year). Respondents who were not currently enrolled received section D (employment). The average completion time for this section was about 4 minutes. Respondents who reported employment in at least one job were asked questions about their occupation, industry, earnings, degrees/certificates required for employment, benefits, and possible spells of unemployment. The employment section took an average of about 5 minutes to complete for employed respondents, and just under a minute for respondents who were not currently employed. The background section (section E), lasting about 6 minutes, was one of the longest sections. This section applied to all respondents and focused on basic demographics about students and their families. Topics of interest included income, household composition, and dependents. In addition, the background section investigated citizenship status, community service, and disability status. The locating section collected contact information for the purpose of the next BPS follow-up. Respondents were asked to provide information that could be used to contact them for the next interview. On average, the locating section took about 4 minutes. Timing by completion mode. Table 3-10 presents the average on-screen and transit times by completion mode for respondents who completed a web interview or a telephone interview. The amount of time spent both on-screen and in transit was significantly different, depending on the mode of administration. For instance, self-administered respondents experienced the shortest on-screen completion time with an average of about 19 minutes, compared to nearly 24 minutes for those who completed a telephone interview (t = -14.37, p < .0001). However, self-administered respondents experienced significantly longer transit times than did telephone respondents (4 minutes compared with 3 minutes, respectively; t = 12.59, p < .0001). As presented in table 3-11, most self-administered respondents completed the interview with some type of fast connection (73 percent). As expected, those who completed with a dial-up modem connection experienced longer transit times than all fast connections combined (8 minutes compared to 4 minutes, t = -7.76, p < .01). Computer-assisted telephone interview (CATI) only. Computer-assisted personal interview (CAPI) timing data are excluded since the CAPI interview was administered on a stand-alone laptop, not transmitted over the Internet. NOTE: Numbers have been rounded to the nearest 10. Detail may not sum to totals because of rounding. Outliers were excluded from this analysis. Outliers were identified separately for each section and for the total interview; therefore, individual section times do not sum to the total interview times. An outlier was defined as any case whose completion time exceeded two standard deviations above or below the average time for a given section.  Numbers have been rounded to the nearest 10. Detail may not sum to totals because of rounding. Outliers were excluded from this analysis. Outliers were identified separately for each section and for the total interview; therefore, individual section times do not sum to the total interview times. An outlier was defined as any case whose completion time exceeded two standard deviations above or below the average time for a given section. Timing of coding systems. The BPS:04/06 field test implemented an experiment to test the efficacy of two coding systems to categorize field of study and occupation. 7 Cases were randomly assigned to one of the two coding systems, and the results were evaluated to determine if there was a difference in the amount of time required to complete the coding process. For major coding, respondents were asked to enter a text string describing their major. 8 Respondents were either given the \"double drop-down\" list of majors, from which they could select a general and specific category, or the assisted coder that returned a set of categories based on a keyword search of the database. The two types of occupation coding systems, based on O*NET, searched the database according to text strings describing occupation title and duties. The first version searched the database only on job title, and the second version searched based on both job title and job activities. The timing results for the coding systems are presented in table 3-12. For field of study, the double drop-down system required more time than the assisted coder, taking 0.9 minutes, compared to 0.4 minutes, respectively (t = 17.83; p < .0001). Coding system completion times were also examined by administration mode. Mode differences were observed within the major coding system; however, the difference was attributable to transit time. In other words, the observed time difference was due to internet connection speed rather than the coding system itself. The version of occupation coding system did not change the amount of time required to categorize occupation. Both occupation coding systems took 0.8 minutes to complete. "}, {"section_title": "Interviewer Hours", "text": "Telephone interviewing for the field test required about 2,900 telephone interviewer hours, exclusive of training, supervision, monitoring, administration, and quality circle meetings. The average time spent per completed interview was 2.74 hours. Since the average time to administer the interview was 26.6 minutes for CATI and CAPI cases, the large majority of interviewer time was spent in other activities. While a small percentage of non-interview time was required to bring up a case, review its history, and close the case (with the appropriate reschedule, comment, and disposition entry when completed), the bulk of time was devoted to locating and contacting the sample member."}, {"section_title": "Number of Calls", "text": "As indicated above, a significant amount of interviewer time was spent attempting to locate and contact sample members. Table 3-13 shows the number of telephone calls made to sample members overall, by current response status, prior response status, and by mode of completion. An average of 16 calls was made per sample member, depending on response status and mode of completion. There was no statistical difference in calls per case by base-year response status. Those interviewed in BPS:04/06 were called 10 times, on average, compared to those not interviewed, who were called an average of 20 times (t = 12.86; p < .0001). Interviews completed via the Web required significantly fewer calls (7 calls) compared to CATI (15 calls, t = -8.58; p < .0001) and to CAPI (11 calls, t = \u22122.68; p < .01). Interview nonresponse is an increasing problem for CATI and CAPI studies, affecting the cost of data collection and the quality of the resulting data. Call screening devices, such as telephone answering machines, caller ID, call-blocking, and privacy managers, help sample members avoid unwanted telephone calls, but they can also affect the representativeness of data, lower study response rates, and increase project costs by requiring additional call attempts and interviewer time. Of the 2,610 field test cases, 1,570 (60 percent) had at least one answering machine event. Among completed interviews, an average of 18 calls was required to complete an interview in cases in which an answering machine was reached at least once, compared to just 1 call for cases in which no answering machine was reached during the course of contacting the respondent (t = -22.3; p < .0001). "}, {"section_title": "Results of Prompting Experiment", "text": "As discussed in chapter 2, the BPS:04/06 field test implemented an experiment to evaluate the effectiveness of prompting calls 9 in increasing web-based, self-administered interview response rates during the early response period. Following the early response period, interview completion rates for the two prompting groups (prompted versus not prompted) were compared. 10 As anticipated, participation rates were higher among sample members who were prompted with reminder calls: 21.5 percent participated compared with just 10.4 percent of non-prompted sample members (z = 5.52; p < .05). Table 3-14 shows the interview participation rates by prompting status. Table 3-15 presents interview participation rates by the type of prompting call. The most effective prompting calls were those in which sample members were contacted directly (26 percent participated). Leaving messages with contacts or on an answering machine resulted in participation rates of 21 percent and 17 percent, respectively. Interview participation rates by base-year response status and prompting outcome are shown in table 3-16. Prompting calls did not have a significant effect on interview participation among base-year respondents. Prompting calls did increase response rates among base-year nonrespondents-21 percent of those who received prompting calls participated compared to 9 percent of those not prompted (z = 5.57; p < .01). The most significant finding is that, among prompted cases, there was no difference in interview participation between base-year respondents and nonrespondents, which suggests that the prompting calls increase the likelihood that nonrespondents participate at the same rate observed for base-year respondents.    Table 3-17 presents the response rates obtained during each phase of data collection, by the type of incentive offered. The results of each phase of data collection are discussed below."}, {"section_title": "Results of Incentive Programs", "text": "Early response period. All sample members were eligible to receive a $30 incentive for completing the student interview within the first 3.5 weeks of data collection. A total of 520 sample members participated in the early response period, constituting 20 percent of the entire sample (n = 2,610). Of those who participated during the early response period, approximately 110 were found to be ineligible for the study, 11 and were not included in the final count of completed interviews. Thus, 410, or 16 percent of eligible sample members completed the webbased self-administered interview during the early response period. Furthermore, about 39 percent of all completed interviews (n = 1,060) were obtained during the early period. Production interviewing. The BPS:04/06 field test incentive experiment was designed to evaluate whether an incentive offered during the production interviewing phase affected the rate at which sample members participated. Results presented here are based on all sample members, excluding any cases determined to be ineligible (n = 380), those who completed during the early response period (n = 410), and all cases assigned to field interviewers (n = 450). 12 At the end of the early response period, interviewers began contacting the remaining sample members (n = 1,400) in an effort to have them complete a telephone interview. Prior to data collection, sample members were randomly assigned to a $0 or a $20 incentive group. A total of 22 percent of sample members eligible for the $20 response incentive completed the interview. By contrast, a 16 percent response rate was attained for sample members who were not eligible for an incentive (z = 2.66; p < .05). Excludes 110 cases determined to be ineligible at the end of the early response period. 3 Excludes the 380 ineligible cases, cases completed during the early response period, and cases assigned to field interviewers. 4 Excludes the 380 ineligible cases, cases completed either during the early response period or production interviewing, and cases assigned to field interviewers. NOTE Nonresponse conversion. After removing the 270 cases completed during the production interviewing phase, approximately 1,130 remaining sample members met the conditions required for the offer of a $30 nonresponse conversion incentive (refusal, high call count, or were difficult to locate). About 250 interviews (22 percent) were completed in response to this incentive offer."}, {"section_title": "CAPI.", "text": "Early in the production interviewing phase, 450 cases were assigned to field interviewers in an attempt to increase the likelihood of successful locating and interviewing outcomes for sample members that were particularly hard to locate (e.g. cases with no successful contact, and inconclusive results from tracing activities). Of these, 120 completed a personal interview (CAPI), for a response rate of 26.5 percent."}, {"section_title": "Conclusion", "text": "The purpose of the BPS:04/06 field test was to fully test all data collection procedures. The tracing, locating, and interviewing methods were successful for the field test and will be implemented again for the full-scale study. The results from the prompting experiment indicate that reminder calls made during the early response period of data collection have a positive impact on interview participation, particularly for base-year nonrespondents. The results from the production interviewing incentive experiment suggest that paying an incentive does increase the likelihood of a response. The full-scale study will first implement the prompting calls during the early response period to encourage web completion (and reduce costs associated with telephone interviewing) and then implement a similar type of incentive during production interviewing to increase interview completion via CATI. To reduce respondent burden and improve data quality, certain items will be eliminated or modified to decrease the respondents' overall time in the interview and to improve usability of the web instrument.\nThe purpose of the BPS:04/06 field test was to fully test all data collection procedures in preparation for the full-scale study. The primary focus of the field test was to evaluate the efficacy of the web-based student interview for administration as a self-administered and interviewer-administered, via computer-assisted telephone interview (CATI) and computerassisted personal interview (CAPI) instrument. As discussed in this working paper, the BPS:04/06 field test instrument was successful, and will require only minor modifications prior to full-scale administration. The tracing and locating procedures implemented for the field test, as well as the Help Desk support provided to web users, were successful for the field test and will be employed again as designed for the full-scale study. Materials developed for both Help Desk and interviewer training will include more realistic mock cases. The greatest challenge to the field test data collection was the inclusion of almost 2,200 supplemental sample members who did not participate in the base year, NPSAS:04 field test. Because it helped to improve their likelihood of participation in the interview, prompting of base-year nonrespondents will be used during the early response period for the full-scale study. With the recommended incentive plan, it is anticipated that full-scale data collection will achieve the desired response rate of 85 percent.  Students who first began their postsecondary education in the 2002-03 school year were selected to participate in the Beginning Postsecondary Students (BPS) Longitudinal Study sponsored by the U.S. Department of Education. This study collects information, over time, on students' postsecondary experiences, work while enrolled, persistence in school, degree completion, and employment following enrollment. The enclosed pamphlet describes BPS in more detail and presents selected findings from prior BPS studies. \u00absPfname\u00bb \u00absPlname\u00bb has been randomly selected to participate in this cycle of BPS. We need your help to update our records for \u00abpronoun2\u00bb. (IF SAMPLE TYPE = NPSAS04RESP then -When we last talked to \u00absPfname\u00bb in 2003, \u00abpronoun1\u00bb listed you as someone who would always know how to get in touch with \u00abpronoun2\u00bb.) Please take a few minutes to update the enclosed Address Update Information sheet and return it in the enclosed postage paid envelope. We will be re-contacting \u00absPfname\u00bb and other study participants beginning in early spring 2005 to ask questions about their recent education and employment experiences. Your help in updating our records will ensure the success of the study. Only a limited number of people were selected for the study. Therefore, each person selected represents many others, and it is extremely important that we be able to contact them. NCES has contracted with RTI International to conduct this cycle of the BPS data collection. Please be assured that both NCES and RTI follow strict confidentiality procedures to protect the privacy of study participants and the confidentiality of the information collected. If you have any questions about the study, please call the RTI study director, Dr. Jennifer Wine, toll-free at 1-877-225-8470. We sincerely appreciate your assistance and thank you in advance for helping us conduct this important study. In 2003, you participated in an interview for the U.S. Department of Education that focused on your early experiences as a postsecondary student and how you paid for your school expenses that year. We are now seeking your help with a follow-up interview with you and students like you who began their education in 2002-03. This new interview, conducted as part of the Beginning Postsecondary Students (BPS) Longitudinal Study, will focus on your experiences since the first interview, as you continued in, completed, or left postsecondary education. Results from previous BPS studies have been used by educators and policymakers to better understand the rate at which beginning students are completing degree programs, the factors preventing them from completing degree programs, and the effects of financial aid and jobs on academic performance. The interview will take about 25 minutes to complete on the web whenever it is convenient for you. When data collection begins in March, you will receive a postcard that will provide specific information on how to participate. If you complete the interview on the web by the date indicated on the postcard, you will receive a $30 check as a token of our appreciation. Your participation, while voluntary, is critical to the study's success. By law, we are required to protect your privacy. Your responses will be secured behind firewalls and will be encrypted during Internet transmission. Your responses may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. Enclosed you will find a pamphlet with a brief description of BPS, findings from prior BPS studies, and confidentiality procedures. If your contact information has changed, you may provide your new address and telephone number on the enclosed address update sheet and return it to us in the business reply envelope provided. To find out more about this BPS interview and to update your contact information online, visit the study's website at http://surveys.nces.ed.gov/bps. The BPS study is being conducted for the U. S. Department of Education's National Center for Education Statistics by RTI International. If you have any questions about the study, please call the RTI study director, Dr. Jennifer Wine, toll-free at 1-877-225-8470. We thank you in advance for your participation in this important study. Your cooperation is greatly appreciated. The National Center for Education Statistics (NCES) of the U.S. Department of Education is authorized by federal law (Public Law 107-279) to conduct the Beginning Postsecondary Students Longitudinal Study. NCES will authorize only a limited number of researchers to have access to information which could be used to identify individuals. They may use the data for statistical purposes only and are subject to fines and imprisonment for misuse. According to the Paperwork Reduction Act of 1995, no persons are required to respond to a collection of information unless it displays a valid OMB control number. The valid OMB control number of this information collection is 1850-0631, and it is completely voluntary. The time required to complete this information collection is estimated to average 25 minutes per response, including the time to review instructions, search existing data resources, gather the data needed, and complete and review the information collection. If you have any comments concerning the accuracy of the time estimate or suggestions for improving the interview, please write to: U.S. Department of Education, Washington, DC 20006. If you have comments or concerns regarding the status of your individual interview, write directly to: Tracy Hunt-White, National Center for Education Statistics, 1990 K Street, NW, Washington, DC 20006."}, {"section_title": "Chapter 4 Evaluation of Data Quality", "text": "This chapter includes summaries of evaluations conducted throughout the 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06) field test data collection, as well as a detailed analysis of the quality of data collected. Analyses of quality control procedures, coding processes, and item-level nonresponse are presented."}, {"section_title": "Identification of First-Time Beginners", "text": "All students who were interviewed in the 2004 National Postsecondary Student Aid Study (NPSAS:04) base year field test and confirmed as first-time beginners (FTBs) were included in the field test follow-up sample. No eligibility screening was performed on this group during the BPS first follow-up interview. However, all base-year nonrespondents to the student interview were asked the same set of items used to determine eligibility during the NPSAS:04 base-year interview. Approximately 17 percent of the base-year nonrespondents 13 who were potential FTBs were found to be ineligible for inclusion in the BPS:04 cohort."}, {"section_title": "Data Quality Evaluations", "text": ""}, {"section_title": "Reliability Reinterview Response Rates", "text": "A subsample of respondents to the BPS:04/06 field test was selected at random to participate in a reinterview designed to assess the temporal stability of items sampled from the field test instrument. After completing the initial interview (see appendix E), respondents selected for the reinterview were asked to participate again in no less than three weeks. Respondents were asked to participate in the reinterview within the mode of initial interview administration, either web-based self-administered interview or computer-assisted telephone interview (CATI), thus ensuring correspondence between the main interview and the reinterview (a facsimile of the reinterview is provided in appendix F). In total, 300 respondents comprised the reinterview sample. Overall, 62 percent of those selected (n = 190) ultimately completed the reinterview. Approximately equal proportions of respondents who completed a self-administered interview (61 percent) and CATI (63 percent) participated. The sample sizes per item vary because some items were not applicable to all respondents."}, {"section_title": "Reliability Reinterview Results", "text": "Tables 4-1 through 4-4 identify reliability estimates for each item included in the reinterview, by interview section. For each item, the number of cases, percent agreement between the initial interview and reinterview, and relational statistic are presented. For discrete variables, percent agreement was based on the extent to which responses to the initial interview were identical to responses to the reinterview. Responses to the only continuous variable included in the reinterview (amount earned during the school year) were considered to match the initial interview when the responses were within one standard deviation of each other. Relational statistics are shown to illustrate the strength of the association between two variables, with 1.00 indicating a perfect correlation (i.e., an exact match between the item on the initial interview and the same item on the reinterview for all respondents). For the purposes of reporting the relational statistic, Cramer's V was used for items with discrete, unordered response categories (e.g., yes/no). Kendall's tau-b (\u03c4 b ) estimated the relationship between items with ordered categories (e.g., not at all, occasionally, and frequently). Lastly, the Pearson productmoment correlation coefficient (r) was used for variables yielding interval or ratio responses (e.g., income). High percent agreement and strong correlational statistics indicate the items' stability over time, whereas a lack of agreement and lower correlations suggest respondents' answers are prone to vary in the short period between the interview and reinterview administrations."}, {"section_title": "Enrollment History", "text": "As shown in table 4-1, one item from the enrollment history section-taken a break from school for more than four months-was included in the reinterview. A total of 88 percent of participants provided the same response on both the interview and the reinterview. At .61, the relational statistic was lower compared to percent agreement. However, for this and several other items, the deflated relational statistic is associated with little variation across response categories (i.e., restriction of range). In the case of taken a break from school for more than four months, the majority of students reported continuous enrollment. Although in the reinterview only a small number of students changed their answer, because of the minimal variation in the response options, these slight changes to the distribution of the variable contributed to the low relational statistic shown in table 4-1. "}, {"section_title": "Enrollment Characteristics", "text": "Regarding the enrollment characteristics section shown in table 4-2, several of the major variables achieved 80 percent agreement or higher: major declared/undeclared (86 percent), primary major-general category (80 percent), and frequency of formal changes in major (85 percent). The values of the relational statistics varied for each, due to differences in variation across response options. The percent agreement for the primary major-specific category was lower (65 percent) compared to the other major variables, although it exhibited a strong relational statistic (.86). The relational statistic, influenced as it is by variation in responses, increases when the range of possible response options increases. Turning to financial variables, the number of jobs held during the academic year and the amount earned during the school year reflected a high percent agreement between the interview and reinterview: 85 and 100 percent, respectively. Five variables in the enrollment characteristics section assessed the extent to which parents helped the respondent financially. These items demonstrated considerable temporal stability, with 80 percent or more of participants reporting the same response over time. "}, {"section_title": "Employment", "text": "The employment section is summarized in table 4-3. The occupation code was consistently reported in the initial interview and reinterview by 54 percent of respondents. The corresponding relational statistic was .69. Examination of the job title text strings revealed variability over time in the job titles provided by respondents. As a result, the occupation codes derived from the text strings were not constant between the two interview administrations for some respondents. Across the two administration time points, the industry code matched 42 percent of the time and exhibited a strong relational statistic (.60) due to the considerable variation across the numerous industry response categories. Items pertaining to the degree required for respondent occupations demonstrated high percent agreement, particularly with respect to license (88 percent), certificate (88 percent), and bachelor's or associate's (89 percent) degree requirements. The item degree required: none matched over time for 75 percent of respondents. Although the relational statistics varied from .16 to .41 for these four items, the discrepancy with high percent agreement is related to lack of variation in these dichotomous variables. "}, {"section_title": "Background", "text": "One item in the background section-respondent born in the U.S.-demonstrated nearly perfect agreement between the interview and reinterview (table 4-4). More moderate associations were observed for the number in household (68 percent) and earnings in 2004 (55 percent). Whether respondents had volunteered in the last year evoked the same response in the interview and reinterview for 90 percent of the sample. When asked to rate their agreement with statements pertaining to their volunteer work, respondents provided consistent responses to this series of items 42 to 58 percent of the time. Corresponding relational statistics ranged from .29 to .40. Finally, respondent intentions to teach at the elementary or secondary education levels showed high reliability, with 77 percent agreement between the initial interview and reinterview. "}, {"section_title": "Recode Analysis", "text": "The reliability of the procedures used to code major/field of study and occupation were assessed by expert coders who evaluated a random sample of major and occupation codes representing 50 percent of respondents in the full sample. The interview was either selfadministered via a web-based survey or administered by a trained interviewer via CATI or computer-assisted personal interview (CAPI), thus allowing mode comparisons within the experimental coding methods. Two expert coders assessed each case that completed the coding process. Per respondent, one coder used the same coding method employed in the original interview, while the other used the alternative coding method. Percent agreement was calculated for each possible combination of variable type (major or occupation), coding method (double drop-down or assisted for major, and O*NET-A or O*NET-B for occupation), and survey mode (self-or intervieweradministered). The percent agreement for each comparison is presented in table 4-5."}, {"section_title": "Major", "text": "For major, respondents were randomly assigned to use either a pair of drop-down boxes containing general areas and, as applicable, secondary areas of study, or an assisted coder which returned one or more specific areas of study that matched most closely to the text string provided by the respondent. If no areas matched, respondents were offered the dual drop-down boxes used by the other group. The same set of general and specific areas was used for the drop-down boxes and the assisted coder. As shown in table 4-5, the major/field of study assisted coding system demonstrated 82 percent agreement when the expert coders used the same method employed in the interview and 80 percent agreement when the alternative coding method was used. Corresponding overall results for the double drop-down method were 76 percent and 73 percent, respectively. Comparisons within each coding method did not show differences across modes (selfadministered interview versus CATI/CAPI). Demonstrating the reliability of the coding methods and their corresponding administration modes, percent agreement did not change significantly when the expert coder used the opposite coding method employed in the interview. "}, {"section_title": "Occupation", "text": "As described in chapter 2, two coding systems were used for categorizing occupation, O*NET-A and O*NET-B. 14 A list of potential codes was returned after an automatic keyword search, with the option to select none of these. In O*NET-A, the keyword search was based solely on job titles. The keyword search used in O*NET-B was based on both job title and job activities. When selecting none of these, respondents were directed to a series of drop-down menus in which they identified a general category, a specific category, and finally, a detailed category. The occupation coding system O*NET-A achieved 69 percent agreement when the expert coding system and interview coding system were the same. When the expert coder used the opposite method, the rate of agreement was not significantly different at 62 percent. Similarly, O*NET-B garnered a 76 percent agreement when coding systems matched, compared to 73 percent when the expert coder used the alternative coding system. Both systems demonstrate agreement rates that are not statistically different from one another. Furthermore, the fact that percent agreement did not change significantly for either coding system when the expert coder used the alternative system suggests that both are reliable methods. Within each coding method, statistically significant mode differences were not apparent."}, {"section_title": "Help Text Analysis", "text": "The BPS:04/06 field test offered general and screen-specific help text on all instrument screens. The general help text provided answers to frequently asked questions about response types and browser settings for questionnaire completion. The screen-specific help provided definitions of terms and phrases used in question wording and response options, and explained the type of information requested. Each help text screen also provided a toll-free telephone number so that sample members could call the BPS:04/06 Help Desk for additional assistance. The number of times respondents clicked the help text button for each screen were tallied to determine the rate of help text access per screen relative to the number of respondents to whom the screen was administered. The screen-level rate of help text access was analyzed overall and by mode of interview administration to identify screens that may have been problematic for users. Overall, the mean percentage of help text hits per screen was less than one percent (0.6). Table 4-6 presents the rates of help text access for the interview screens in which help text was accessed at a rate of three percent or more. The item that asked about the type of school job had the highest rate of help text access, at nearly 6 percent. This item asked respondents to categorize their employer type into one of five options (work study, paid assistantship, cooperative employment, paid practicum, and none of the above). Furthermore, the majority of requests for help text on this screen were from interviewer-administered respondents. Approximately 10 percent of all interviewer-administered respondents used help text for this form, compared to 1 percent among self-administered respondents (z = -5.76; p < .01). "}, {"section_title": "Conversion Text Analysis", "text": "In order to minimize item-level nonresponse, the BPS:04/06 instrument implemented conversion text for 17 key items. These key items covered topics such as eligibility, employment, GPA, income, race, and citizenship status. If respondents did not provide an answer before continuing to the next screen, the original screen was reloaded with conversion text to encourage item completion. This additional text emphasized the confidential nature of the study as well as the importance of individual responses and explained how the information was to be used in research. Table 4-7 presents the results of the conversion text analysis. Conversion text was generally successful, yielding conversion rates between 18 to 100 percent. Items with a high rate of nonresponse conversion included those requesting information on enrollment dates and finances. The exception to this, an item requesting spouse's earnings, only converted respondents 18 percent of the time. Conversion text rates produced similar results for both self-administered and intervieweradministered interviews (table 4-7). However, two screens (amount borrowed for undergraduate loans and race) demonstrated statistically significant differences in conversion rates. More specifically, for these two screens, respondents were more likely to convert to a valid response when completing the interview in a self-administered mode. Conversion text resulted in a valid response for 69 percent of self-administered respondents to the item that asked for the amount borrowed in undergraduate loans, compared with 43 percent conversion among intervieweradministered respondents (z = 2.31; p < 0.5). Conversion text on the screen that collected respondents' race was also more successful among self-administered respondents, with 61 percent of cases subsequently providing a valid response, compared to 13 percent among interviewer-administered respondents (z = 3.77; p < 0.5). "}, {"section_title": "Item-level Nonresponse", "text": "Missing data for items in the field test student interview were associated with a number of factors: (1) a true refusal, (2) an unknown answer, (3) an inappropriate question for that respondent that he or she could not answer, (4) confusion related to the question wording or response options, or (5) hesitation to provide a best guess response. This section discusses items with high rates of missing data (including don't know responses) to identify items leading to higher rates of nonresponse for reconsideration in the full-scale. Total nonresponse rates were calculated for each of the items administered to at least 100 respondents. Of over 300 items, only four yielded a total nonresponse rate greater than 10 percent. Item-level nonresponse rates were examined by mode, and no statistically significant differences between self-administered and interviewer-administered nonresponse rates were observed for these four items. Results of the item-level nonresponse analysis are presented in table 4-8. The first three items with more than 10 percent nonresponse were all associated with enrollment dates. These items asked respondents to report the beginning and ending dates of their enrollment at each school attended since the base year (the 2002-03 academic year), as well as their enrollment intensity (full-time or part-time). Respondents who had been continuously enrolled were asked to provide the beginning and ending dates of their enrollment. Respondents who had not been continuously enrolled were asked to report the beginning and ending dates of spells of enrollment separated by at least four months. Nearly 70 percent of respondents who were administered this set of items did not provide a response. However, closer examination of the cases that did not give a response to the enrollment date items revealed that the nonresponse was most likely due to confusion about the gate item that asked whether the respondent had been continuously enrolled. The enrollment dates provided in the items collecting the first spell of enrollment indicated that these respondents had been continuously enrolled, so had nothing to report for the second spell. These items will be refined in the full-scale questionnaire. The other item with a high rate of nonresponse asked respondents to report their parents' income in 2004. Although the response categories offered broad ranges, 27 percent of respondents provided a don't know response. Nearly 3 percent of respondents left the screen blank, for a total nonresponse rate of 29 percent. "}, {"section_title": "Question Delivery and Data Entry Error Rates", "text": "Regular monitoring of CATI interviews improves interviewing and enhances data quality. Monitoring throughout the BPS:04/06 field test helped to meet these important quality objectives: \u2022 identification of problem items; \u2022 reduction in the number of interviewer errors; \u2022 improvement in interviewer performance by reinforcing desired strategies; and \u2022 assessment of the quality of the data collected. Specially-trained monitors simultaneously listened to and viewed CATI interviews using remote monitoring telephones and computer equipment. This system allowed monitors to observe live interviews without disturbing the interviewer or respondent. Monitors listened to up to 20 questions during an ongoing interview and evaluated both question delivery and data entry. To guarantee an accurate reflection of data collection activities, monitors conducted their evaluations throughout all of CATI data collection, including day, evening, and weekend shifts. Daily, weekly, and cumulative question delivery and data entry outcomes were measured and displayed on the Integrated Management System (IMS). During CATI data collection, 2,024 items were monitored. Of these items, call center staff observed only 22 total errors, yielding very low error rates overall. Fifteen of these errors occurred during question delivery, whereas 7 of them occurred during data entry. Typically, error rates by week fell below 2 percent. Figures  4-1 and 4-2 illustrate the question delivery and data entry errors respectively. The error rate peaks are attributable to the addition of new interviewer staff, who are more prone to errors due to their experience level. Because of call center schedule considerations, monitoring began during the second week of data collection. Likewise, monitoring efforts were reduced during the final weeks of data collection given the lighter caseloads.    "}, {"section_title": "Data Collection Evaluations", "text": ""}, {"section_title": "Help Desk", "text": "In order to gain a better understanding of the problems encountered by students attempting to complete the interview via the Web, a software program was developed to record each Help Desk incident that occurred during data collection. For each occurrence, Help Desk staff confirmed contact information for the sample member, recorded the type of problem, provided a description of the problem and resolution, identified the incident status (pending or resolved), and indicated the approximate time required to assist the caller. Help Desk staff were trained to answer incoming calls to the Help Desk hotline, as well as conduct telephone interviews as needed. Help Desk staff assisted sample members with questions about the web instrument, and provided technical assistance to sample members who experienced problems while completing the self-administered web interview. Help Desk agents also responded to voice mail messages left by respondents when the Call Center was closed. Table 4-9 provides a summary of Help Desk incidents. Help Desk staff assisted 110 sample members (4 percent of the sample) with 143 incidents. The most common type of incident (50 percent) recorded by the Help Desk was from sample members requesting their Study ID and/or password, with another 20 percent of the calls related to browser settings and computer problems. Fourteen percent of incidents were respondents who called because of a preference to complete the interview over the telephone, while another 13 percent of calls were for other miscellaneous issues. Program errors, questionnaire content, questions about the study, and reports of website unavailability accounted for the remaining 3 percent of Help Desk calls. "}, {"section_title": "CATI Quality Circle Meetings", "text": "Quality Circle (QC) meetings were vital components for ensuring that project staff, call center supervisory staff, and telephone interviewers were communicating on a regular basis about the goals of the study and addressing challenges encountered along the way. These meetings provided a forum for discussing elements of the CATI instrument, questionnaire design, and interview cooperation tactics, and were a way for project staff to motivate the group toward the goals of the study and acquire feedback on data collection issues. Meetings were held bi-weekly at the Call Center and an agenda was provided to those in attendance. For interviewing staff unable to attend the meeting, notes were distributed electronically to the Call Center supervisory staff and passed along accordingly. A summary of issues addressed in the meetings is outlined below: \u2022 clarification of questions and item responses, \u2022 BPS eligibility criteria, \u2022 submission of problem sheets, \u2022 the importance of providing detailed case comments, \u2022 methods of gaining cooperation from sample members and gatekeepers, and \u2022 general morale boosting and reinforcement of positive interviewing techniques. Throughout the duration of the study, a variety of issues were addressed at the QC meetings that reinforced specific content from training and contributed to prompt problem solving. Some of the issues covered in quality circle meetings included the following: Writing problem sheets. Reporting problems when they occur is an important part of telephone interviewing. Interviewers were trained to report problems electronically and to provide specific detail, including but not limited to, the problem that occurred, when it occurred, and the specific point in the interview in which it occurred. Problem sheets further delineated how the issue was addressed. Review of problem sheets in QC meetings was a critical means through which staff learned to recognize and manage the different problems they would encounter. Eligibility criteria. Due to the considerable complexity of the eligibility criteria, interviewers were reminded to allow eligibility determination to be made by the programmed instrument. Gaining cooperation. Discussions focused on the difficulty of gaining a sample member's trust during the initial phases of the call. Refusal avoidance strategies were revisited during QC meetings and adapted, as needed, for problems specific to the BPS:04/06 field test data collection. For example, obtaining new contact information from parents (for students no longer living at home) was a focal point for many discussions. Interviewers shared tips for overcoming parent concerns, and found ways to benefit and learn from each other's experiences. Questionnaire. Interviewers were given hard copies of the questionnaire and asked to read and review the questions to identify any items that seemed to be potentially confusing or misleading. During QC meetings, particular problems with question wording and other aspects of the interview were discussed. Interviewer debriefings. At the conclusion of the BPS:04/06 field test, project staff held debriefing meetings with the telephone and field interviewers to learn more about the field test experience. Interviewer debriefings focused on what worked well and what could be improved with respect to: \u2022 interviewer training sessions, \u2022 tracing strategies, \u2022 refusal conversion, and \u2022 interview questions and coding systems that were difficult for the respondents to answer or the interviewers to code. A summary of the telephone and field interviewer debriefing meetings was prepared and will be considered when planning the BPS:04/06 full-scale data collection."}, {"section_title": "File Preparation", "text": ""}, {"section_title": "Overview of the BPS:04/06 Field Test Files", "text": "The field test data files for BPS:04/06 contain a number of component data files from a variety of sources. Included are student-level data collected from student interviews and government financial aid databases. The following files were produced at the end of the BPS:04/06 field test: \u2022 Data System (NSLDS) for the nearly 1,600 sample members who received loans. This is a history file with separate records for each transaction in the loan files. Therefore, there can be multiple records per case spanning several academic years. \u2022 Pell data file. Contains raw grant-level data received from the NSLDS for the approximately 1,300 sample members who received Pell Grants during the 2004-05 academic year or prior years. This is a history file with a separate record for each transaction in the Pell system. Therefore, there can be multiple records per case."}, {"section_title": "Range and Consistency Checks", "text": "The web-based student instrument included edit checks to ensure that the data collected were within valid ranges. Examples of some of the general online edit checks include the following: \u2022 Range checks were applied to all numerical entries such that only valid numeric responses could be entered. \u2022 A consistency check was triggered when a respondent provided a valid answer and then checked a none of the above option. Respondents and interviewers were advised to uncheck other options before checking the none of the above option. Conversely, if a respondent selected none of the above first and then checked a valid answer, the valid response was kept. \u2022 If a respondent clicked an other box and did not type a response into the other specify textbox, an edit check was activated that reminded the respondent to enter text. \u2022 Consistency checks were also used for cross-item comparisons. For example, if a respondent indicated that he or she was 23 years of age but graduated from high school in 1988, the respondent was asked to verify this information."}, {"section_title": "Post Data-Collection Editing", "text": "The BPS:04/06 field test data were edited using procedures developed and implemented for previous studies sponsored by the National Center for Education Statistics (NCES), including the base year study, the 2004 National Postsecondary Student Aid Study (NPSAS:04). These procedures were tested again during the field test in preparation for the full-scale study. Following data collection, the information collected by the student instrument was subjected to various QC checks and examinations. These checks were to confirm that the collected data reflected appropriate skip patterns. Another evaluation examined all variables with missing data and substituted specific values to indicate the reason for the missing data. A variety of explanations are possible for missing data. For example, an item may not have been applicable to certain students, a respondent may not have known the answer to the question, or a respondent may have just skipped the item entirely. Table 4-10 lists the set of missing data codes used to assist analysts in understanding the nature of missing data associated with BPS data elements. Skip-pattern relationships in the database were examined by methodically running crosstabulations between gate items and their associated nested items. In many instances, gate-nest relationships had multiple levels within the instrument. That is, items nested within a gate question may themselves have been gate items for additional items. Therefore, validating the gate-nest relationships often required several iterations and many multiway cross-tabulations to ensure the proper data were captured. The data cleaning and editing process for the BPS:04/06 field test data files involved a multistage process that consisted of the following steps: 1. Blank or missing data were replaced with -9 for all variables in the instrument database. A one-way frequency distribution of every variable was reviewed to confirm that no missing or blank values remained. These same one-way frequencies revealed any out-of-range or outlier values, which were investigated and checked for reasonableness against other data values. Example: hourly wages of $0.10, rather than $10.00. Creating SAS formats from expected values and the associated value labels also revealed any categorical outliers. Descriptive statistics were produced for all continuous variables. All values less than zero were temporarily recoded to missing. Minimum, median, maximum, and mean values were examined to assess reasonableness of responses and anomalous data patterns were investigated and corrected as necessary. 2. Legitimate skips were identified using instrument source code. Gate-nest relationships were defined to replace -9's (missing for unknown reason) with -3's (not applicable) as appropriate. Two-way cross-tabulations between each gate-nest combination were evaluated, and high numbers of nonreplaced -9 codes were investigated to ensure skip-pattern integrity. Nested values were further quality checked to reveal instances in which the legitimate skip code overwrote valid data. This typically occurred when a respondent answered a gate question and the appropriate nested item(s), but then went back and changed the value of the gate, thus following an alternate path of nested item(s). Responses to the first nested item(s) remained in the database and, therefore, required editing. 3. Variable formatting (e.g., formatting dates as YYYYMM) and standardization of time units, for items that collected amount of time in multiple units, were performed during this step. In addition, any new codes assigned by expert coders reviewing Integrated Postsecondary Education Data System (IPEDS) codes from the student interview (including those institutions that were unable to be coded during the interview) were merged with the interview data files. Also at this step, logical recodes were performed when the value of missing items could be determined from answers to previous questions or preloaded values. For instance, if the student did not work while enrolled, then the amount earned should have been coded to $0 rather than -3 or -9. 4. One-way frequency distributions for all categorical variables and descriptive statistics for all continuous variables were examined. Out-of-range or outlier values were replaced with the value of -6 (bad data, out of range). 5. One-way frequencies on all categorical variables were regenerated and examined. Variables with high counts of -9 values were investigated. Because respondents could skip most items without providing an answer, -9's did remain a valid value, especially for sensitive items, such as those asking for income information. 6. Concurrent with the data cleaning process, detailed documentation was developed to describe question text, response options, recoding, and the applies to text for each delivered variable. The documentation information can be found in the student instrument facsimile in appendix E."}, {"section_title": "Conclusions", "text": "This chapter evaluated the quality of data collected by the BPS:04/06 field test instrument, and analyzed the quality control procedures, coding processes, and item-level nonresponse. The recode analysis yielded no differences in the error rate between coding variants in both the major and occupation coders. Therefore, the coders chosen for the full-scale survey will be based on the previously discussed timing analyses. The low percentage of help text hits, the successful administration of conversion text, and low item nonresponse rates suggest that the complete interview is a viable instrument. Respondents had some difficulty with the dates of enrollment and income questions, and these will be reconsidered during the full-scale instrumentation process. No major data quality issues were uncovered based on the quality assurance, CATI monitoring, and range and consistency checks."}, {"section_title": "Chapter 5 Recommendations for the Full-Scale Study", "text": "The purpose of the 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06) field test was to test procedures and inform planning for the full-scale study. Chapters 3 and 4 of this report documented key field test outcomes and evaluation results. Overall, essential aspects of the field test data collection, including the design and implementation of a single web-based instrument for self, telephone, and in-person interviewing were conducted successfully, while some results warranted procedural and/or substantive modifications to the full-scale study design. Recommended changes to the sampling design, tracing and data collection plans, and instrument are summarized below."}, {"section_title": "Tracing", "text": "Intensive tracing of sample members for the BPS:04/06 field test was performed by RTI's Call Center Services (CCS). Tracing activities were focused primarily on the supplemental sample, whose locating information consisted of the local and permanent addresses provided by the sample institution at the time of the base-year data collection in 2003. The locating data for the supplemental sample were significantly outdated, and no interim locating had been conducted since it was not included in the base-year interview. For the BPS:04/06 full-scale data collection, only a relatively small proportion of the sample will be base-year nonrespondents. Consequently, tracing activities can be more proportionately distributed across the entire sample, and address information for base-year respondents will have been updated in the time since the first interview. Prior to the first mailing to the full-scale sample, it will be helpful to compare results from different tracing sources to determine which offer the most complete information in the shortest amount of time, while staying within the budget allocated for tracing activities. Once data collection begins, those options found to be most efficient for the BPS:04/06 sample should be consulted first."}, {"section_title": "Training", "text": "Following the completion of field test data collection, both telephone and field interviewers participated in a debriefing during which they were asked for feedback on specific aspects of the data collection. Interviewers' suggestions for improving the training program involved the level of complexity of situations presented during mock sessions. For interview training, some interviewers indicated that the mock cases were simpler than are typically encountered during data collection. Interviewers wanted more experience with handling complex respondent situations, tracing (for field interviewers), and gaining cooperation from sample members and gatekeepers (for telephone interviewers). For Help Desk training, in contrast, the situations practiced in training were far more complex than encountered during data collection. The large majority of calls to the Help Desk were for Study IDs and passwords rather than for the more extensive computer hardware and software issues covered in training. Interviewer training always includes extensive review and practice of the coding systems, and this practice will be emphasized in the full-scale interviewer training sessions. Interviewers will also be reminded to confirm selections with respondents during training."}, {"section_title": "Data Collection", "text": "The field test data collection included an experiment evaluating the benefit of prompting calls in improving sample members' likelihood of responding during the early response period. As discussed in chapter 3, there was no significant difference in interview participation between prompted base-year respondents and prompted base-year nonrespondents. Prompting calls increased the likelihood of participation of nonrespondents to the level of respondents. Consequently, it is recommended that the full-scale data collection plan include prompting calls to base-year nonrespondents. During the field test data collection, all sample members received $30 for a completed interview during the early response period, prior to the start of production interviewing when telephone interviewers began making outgoing calls. An experiment conducted during production interviewing offered half of the sample $20 for a completed interview, while the other half was not offered an incentive. A comparison of response rates showed that sample members were more likely to complete an interview when offered $20 than they were when offered no incentive. Therefore, an offer of a $20 incentive during production interviewing is recommended for use during the BPS:04/06 full-scale data collection. In their debriefing, interviewers discussed how to successfully gain the cooperation of sample members and their gatekeepers. According to interviewers, both sample members and gatekeepers, particularly parents, were more willing to cooperate when the incentive offer was mentioned early in the contact. For the full-scale, the offer of an incentive should be included as part of the introductory scripts developed for tracing and interviewing sample members, once a sample member's location is confirmed. If, during production interviewing, a sample member refused to participate in the interview, was difficult to locate (e.g., had a known address but no telephone number), or had received more than 15 calls without a successful contact, the case was identified as \"difficult\" and offered a nonresponse conversion incentive of $30 for a completed interview. An additional 18 percent of completed interviews were obtained during the nonresponse conversion phase of data collection. Interviewers reported that sample members and gatekeepers with high call counts often complained about excessive calls during interviewing. Even when contacts with the sample member and/or other household members were not made, caller identification technology allowed the calls to be identified without someone actually answering the telephone. In order to improve the willingness of sample members and gatekeepers to cooperate with interviewers, the criterion of 15 calls to qualify a case as \"high call count\" should be decreased to 10 calls."}, {"section_title": "Initial Letter to Base Year Nonrespondent", "text": "\u00abDate\u00bb \u00abfname\u00bb \u00abmname\u00bb \u00ablname\u00bb \u00absuffix\u00bb Study ID: \u00abcaseid\u00bb \u00abaddr1\u00bb FT1/\u00abAddr_ID\u00bb \u00abaddr2\u00bb \u00abcity\u00bb, \u00abstate\u00bb \u00abzip\u00bb \u00abzip4\u00bb Dear \u00abp_fname\u00bb \u00abp_lname\u00bb: You have been randomly selected to take part in the Beginning Postsecondary Students (BPS) Longitudinal Study sponsored by the U.S. Department of Education. I am writing to ask you to participate in this important study by completing an interview about your experiences as a postsecondary student at <<INSTITUTION NAME>> in 2002-03, and your education and employment experiences since you first enrolled. Results from previous BPS studies have been used by educators and policymakers to better understand the rate at which beginning students are completing degree programs, the factors preventing them from completing degree programs, and the effects of financial aid and jobs on academic performance. The interview will take about 25 minutes to complete on the web whenever it is convenient for you. When data collection begins in March, you will receive a postcard that will provide specific information on how to participate. If you complete the interview on the web by the date indicated on the postcard, you will receive a $30 check as a token of our appreciation. Your participation, while voluntary, is critical to the study's success. By law, we are required to protect your privacy. Your responses will be secured behind firewalls and will be encrypted during Internet transmission. Your responses may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. Enclosed you will find a pamphlet with a brief description of BPS, findings from prior BPS studies, and confidentiality procedures. If your contact information has changed, you may provide your new address and telephone number on the enclosed address update sheet and return it to us in the business reply envelope provided. To find out more about this BPS interview and to update your contact information on line, visit the study's website at http://surveys.nces.ed.gov/bps. The BPS study is being conducted for the U. S. Department of Education's National Center for Education Statistics by RTI International. If you have any questions about the study, please call the RTI study director, Dr. Jennifer Wine, toll-free at 1-877-225-8470. We thank you in advance for your participation in this important study. Your cooperation is greatly appreciated. The National Center for Education Statistics (NCES) of the U.S. Department of Education is authorized by federal law (Public Law 107-279) to conduct the Beginning Postsecondary Students Longitudinal Study. NCES will authorize only a limited number of researchers to have access to information which could be used to identify individuals. They may use the data for statistical purposes only and are subject to fines and imprisonment for misuse. According to the Paperwork Reduction Act of 1995, no persons are required to respond to a collection of information unless it displays a valid OMB control number. The valid OMB control number of this information collection is 1850-0631, and it is completely voluntary. The time required to complete this information collection is estimated to average 25 minutes per response, including the time to review instructions, search existing data resources, gather the data needed, and complete and review the information collection. If you have any comments concerning the accuracy of the time estimate or suggestions for improving the interview, please write to: U. "}, {"section_title": "Thank you for participating in BPS!", "text": "Dear \u00abfname\u00bb \u00ablname\u00bb: Interviews for the Beginning Postsecondary Students (BPS) longitudinal study are now being conducted. If you complete your BPS interview by \u00abdate\u00bb, you will receive a $\u00abincent_amt\u00bb check as a token of our appreciation. You may access the web interview by logging on to our secure website at https://surveys.nces.ed.gov/bps/ using the Study ID and password provided below. Study ID = \u00abcaseid\u00bb Password = \u00abpassword\u00bb Your responses may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. If you have questions or problems completing your interview online, simply call the BPS Help Desk at 1-800-334-2321. If you have any questions or concerns about the study itself, please contact the BPS Project Director, Dr. Jennifer Wine, toll free at 1-877-225-8470 (e-mail: jennifer@rti.org), or the NCES Project Officer, Ms. Tracy Hunt-White, at 202-502-7438 (e-mail: tracy.hunt-white@ed.gov). Dear \u00abfname\u00bb \u00ablname\u00bb:"}, {"section_title": "Thank you in advance for making BPS a success.", "text": "\n"}, {"section_title": "Data Collection Prompt Flyer", "text": "Interviews for the Beginning Postsecondary Students (BPS) longitudinal study are now being conducted. If you complete your BPS interview soon, you will receive a <$> check as a token of our appreciation. You may access the web interview by logging on to our secure website at https://surveys.nces.ed.gov/bps/ using the Study ID and password provided below. Study ID = \u00abcaseid\u00bb Password = \u00abpassword\u00bb Your responses may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. If you have questions or problems completing your interview online, or would like to complete your interview over the telephone with a professional interviewer, simply call the BPS Help Desk at 1-800-334-2321. If you have any questions or concerns about the study itself, please contact the BPS Project Director, Dr. Jennifer Wine, toll free at 1-877-225-8470 (e-mail: jennifer@rti.org), or the NCES Project Officer, Ms. Tracy Hunt-White, at 202-502-7438 (e-mail: tracy.hunt-white@ed.gov). The Beginning Postsecondary Students Longitudinal Study (BPS) is now being conducted. Sponsored by the US Department of Education, the results of this study will be used to better understand the rate at which beginning students are completing degree programs, the factors preventing them from completing degree programs, and the effects of financial aid and jobs on academic performance."}, {"section_title": "Computer-Assisted Personal Interview (CAPI) Lead Letter", "text": "Your participation is critical to the success of the study. We need you to complete a brief interview with our Field Interviewer, which can be arranged at a time convenient to your schedule. All of your responses will be kept confidential and will be protected to the fullest extent allowable under law. When you complete your interview, we will pay you $30 to reimburse you for your time. Thank you for helping to make BPS a success. Please do not hesitate to contact me by telephone at 1-877-225-8470 or via e-mail at jennifer@rti.org if I can provide any additional information or assistance about the study or your interview. If you'd like to schedule an appointment to complete the interview, call our field interviewer directly (please call collect, if it's long distance) at the number below, or you may call her supervisor toll-free at 1-877-582-9769. Thank you for your time and willingness to participate. You may recall that at the end of your BPS interview, you agreed to participate in a quality control interview. The purpose of this second interview, which takes about five minutes, is to determine how well our questions collect reliable information. To complete it, log in to our secure website at https://surveys.nces.ed.gov/bps/ by August 31, 2005 using the Study ID and password provided below: Enclosed is a magnetic picture frame containing the Study ID number and password you will need in order to complete your interview on the web. Once you've completed it, you may keep the picture frame as a token of our appreciation. Please do not hesitate to contact me toll free at 1-877-225-8470 if I can provide any additional information or assistance. Again, thank you for your time. Your ongoing participation in this study is very important to ensuring its success. We will contact you again in a few days if we haven't received your response. "}, {"section_title": "Refusal Letter", "text": "\u00abDate\u00bb \u00abfname\u00bb \u00abmname\u00bb \u00ablname\u00bb \u00absuffix\u00bb Study ID: \u00abcaseid\u00bb \u00abaddr1\u00bb \u00abpanelinfo\u00bb \u00abaddr2\u00bb \u00abcity\u00bb, \u00abstate\u00bb \u00abzip\u00bb Dear \u00abfname\u00bb \u00ablname\u00bb: On behalf of the U.S. Department of Education, I am writing to ask for your participation in the Beginning Postsecondary Students Longitudinal Study (BPS). Because the results from this study will help develop policy regarding participation in higher education, your experiences and opinions will help determine how future tax dollars are spent. You will receive a $30 check as a token of our appreciation for the time you took to complete the survey. Please call us at 1-800-334-2321 to complete a telephone interview or, if you wish to complete the interview yourself over our secure website, log on to https://surveys.nces.ed.gov/bps/. You will need the Study ID and password provided below to access the web interview. Study ID = \u00abcaseid\u00bb Password = \u00abpassword\u00bb Be assured that all of your answers will be kept confidential and will be protected to the fullest extent allowable under law. Please do not hesitate to contact me directly at 1-877-225-8470 (toll-free) or by e-mail at jennifer@rti.org if I can provide any additional information about the study or your interview. Thank you in advance for your participation in this important study. Your cooperation is greatly appreciated. On behalf of the U.S. Department of Education and the staff of the Beginning Postsecondary Students Longitudinal Study, I would like to thank you for completing your BPS interview. Your participation in this study is very important to ensuring its success. Enclosed you will find a check to reimburse you for your time completing the interview. I would like to remind you about the quality control interviewing for which you were selected. We will be contacting you in a couple of weeks to conduct a very brief re-interview. This second interview will determine whether or not our questions are worded appropriately. Please do not hesitate to contact me directly at 1-877-225-8470 if I can provide any additional information or assistance. Again, thank you for your time and willingness to participate.  INTRODUCTION ...................................................................................................................1-1 Help Desk Table of Contents ( ..........................................................................................................5-       ? Please provide the state and city in which it was located. (If the school was located in another country, select \"foreign country\" From the list. You can also search for a school by state without listing a city.) From the list below, click on the name of the school you attended since the 2002-2003 school year. If the school is not listed:"}, {"section_title": "Field Interviewer Manual", "text": "Make sure the school is not located in a different city. You may change the city and/or state and click \"Continue\" to get a new list of schools. If you still cannot find your school, click \"Unable to Find School\". Applies to: All respondents.      "}, {"section_title": "KDIND", "text": ""}, {"section_title": "Industry verbatim string", "text": "In what type of industry are you currently working? (Earlier we asked about your occupational area of work. Occupation refers to job title and duties, while industry refers to the more general field of employment. For example, a restaurant manager has the occupation of manager in the industry of hospitality, while a manager at a department store has the occupation of manager in the retail trade industry.) Applies to: Respondents who are not currently enrolled and are employed. "}]