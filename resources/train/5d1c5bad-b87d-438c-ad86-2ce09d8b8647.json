[{"section_title": "Introduction", "text": "The Agricultural Resource Management Survey (ARMS) database is major source of information on the financial condition, production practices, and resource use of American farmers and farm household. ARMS database can help us forecast net farm incomes, expenses and productions. We can also study farm production and financial practices with ARMS dataset. It would be very useful for agricultural policy analysis if we have perfect knowledge on ARMS data. Predicting returns and cost of American farm businesses can be two important parts of agricultural economics analysis. As our research purpose, we want to investigate returns on each commodity produced by the farmers based on ARMS data. Unfortunately, ARMS dataset is collected with many questions on commodity returns never asked. Such missing data are that commodity-specific subsets of variables. It is necessary for us to find an efficient way to make imputations on these missing values and estimate the distributions of commodity returns. Here is a famous problem called the Netflix Prize Problem, which is nearly the same as ARMS missing-data dilemma. In Netflix recommendation system, each user would submit his/her ratings on a subset of Netflix films. Users only rate a few ordered items when facing to rate all the movies, which means many missing values occur in Netflix data matrix. Inspired the methodology to solve Netflix Prize Problem, we here present a novel approach, matrix completion, to recover ARMS data matrix. We use a fake data matrix to test the accuracy of matrix completion approach first. Because fake data matrix is completed and created by standard normal distribution, we can get accurate distance of recovered matrix from the true matrix. We use kernel density estimation to see whether the columns in recovered matrix are distributed as normal distribution or not. The results show that matrix completion approach works well on fake data matrix. Then, we apply this approach to ARMS dataset. We focus six main commodities: corn, wheat, soybean, hog, cattle and cotton in ARMS dataset. We construct a sub matrix from ARMS data matrix containing input variables for every commodity, net farm income of farmers and other necessary variables such as acreage and demographic information. After we calculate the net farm incomes for these six commodities with missing values, we apply matrix completion approach to recover this matrix. When we get the completed matrix, we get the summary statistics of net farm income for specific commodity and compared the imputed statistics to original statistics from ARMS data. Since it is difficult to find perfect kernel density for net return of each commodity, we use Kullback-Leibler divergence to find the closest distribution for each commodity."}, {"section_title": "Methodology", "text": "Mathematically, we can regard the ARMS data problem as following: There is a matrix W with 1 rows and 2 columns. The indices of rows represent ID of farmers and the indices of columns represent different commodities. We only observe w entries in this matrix, where m is much smaller than 1 \u00d7 2 , the total number of entries. We wish to estimate all the missing values in W based on m entries. A lot of articles focus on recovering the matrix from a small fraction of its entries, which is known as the matrix completion problem. Candes and Recht (2008) 2 prove that prove that we can recover incomplete square matrix with high probability if the number of sampled elements, , satisfies that \u2265 1.2 log , where C is a positive numerical constant, n denotes the number of rows and is the rank of this matrix. Keshavan et al. (2009) derive an algorithm to compute the Singular Value Decomposition of \u00d7 matrix W based on the observed element set | | = ( ). The stopping criteria is defined by the root mean square error \u2264 ( )( | | ) 1 2 . However, the optimal RMSE would decay with | |/ . There is still improvement for more general model for exact matrix completion. However, the optimal RMSE would decay with | |/ . There is still improvement for more general model for exact completion. Recht (2011) states that the sampling entries are always under Bernoulli model in previous work. He proposes a new incoherence condition on the sampled entries of a low-rank matrix from uniform model and then recover this matrix exactly under singular value decomposition. Candes and Tao (2009) propose that any unknown matrix can be recovered exactly by nuclear norm minimization when the assumption on the singular vectors of this matrix and the condition on the order of information theoretic limit hold. Nathan Srebro et al. (2005) investigate such a matrix completion problem by applying Maximum-Margin Matrix Factorization (MMMF) method. By minimizing trace norm of binary target matrix in a linear programming and factorize the target matrix into two semi-definite matrices, we can get optimal solution matrix, the specific recovered entries and predictions for the new observation. Cai et al. (2008) Candes and Tao (2009) propose that if we only observe m entries from 1 \u00d7 2 matrix W, where m is much smaller compared to 1 \u00d7 2 . Such matrix W can be regarded as a low-rank matrix. In general, one cannot hope to recover a matrix as W . We can only get zeros sample entries almost all the time. In our case, ARMS data matrix is denoted as W. Therefore, we need to set up the number of minimal sampling elements available in ARMS data if we want to recover it exactly. Set \u03a9 as a location set corresponding to the observed entries, i.e. if is the observed entry in ARMS dataset, then (i, j) \u2208 \u03a9. In \u03a9, all observed entries are sampled uniformly at random. Denote \u03a9 as the orthogonal matrix. There exists It is a common-sense approach that one can recover W perfectly if one can find the simplest dataset fitting observed data in W. Such an approach can be transformed into the following optimization problem (Candes and Tao, 2009): is the singular value (the root of the eigenvalue of * ) and and are the basis element spanned in and . Now, we present the minimal sampling theorem developed by Candes and Recht's (2008). Suppose observing entries of W with location sample uniformly at random with = max( 1 , 2 ). Then there exists a numerical constant satisfying (Candes and Tao, 2009 the decision matrix Q is unique and recover W exactly with probability at least 1 \u2212 \u22123 . In equation 3, is the rank of matrix W. In addition, if \u2264 1 5 \u2044 , then the minimal sampling can be expressed as We can calculate the rank of ARMS data matrix W even it contains a lot of missing values. As can be any numerical constant and is quite low in ARMS dataset, minimal sampling requirement can be easily meet."}, {"section_title": "Singular Value Thresholding (SVT) for matrix completion", "text": "Minimal sampling requirement is the prerequisite condition to recover matrix W. We also need to find a mathematical way to find solution matrix of W. Cai et al. (2009) propose a novel algorithm to find such a solution. We just present their work in this section. To find optimal solution matrix , we set up a sequence { } \u22651 of scalar step size. Starting with 0 = 0, the algorithm of Cai et al. (2009) is defined as: 5is nonlinear function which applies softthresholding algorithm at level . Following the work of Cai et al. (2009), we apply SVD of with rank r, we know the facts that = * and = diag({ } 1\u2264 \u2264 ), where and are 1 \u00d7 and \u00d7 2 matrix with orthogonal columns respectively. For each \u2265 0, the softthresholding operator is defined as: The operator can shrink the singular value of towards zero effectively. If many of the singular values of are below the threshold , the rank of ( ) may be considerably lower than the rank of . For each \u2265 0 and \u2208 \u211d \u00d7 , the singular value shrinkage operator obeys The shrinkage iteration needs to apply Singular Value Decomposition first and perform elementary matrix operations at each step. The explicit relationship between shrinkage iteration and equation 1defined by Cai et al. (2009) can be expressed as: Intuitively, by selecting a large value of parameter in equation 8, the sequence of the iteration process would converge the minimizer to (2). We can interpret equation 8as a Lagrange method, which can be set as ( )= 1 2 \u2016 \u2016 2 + \u2016 \u2016 * for some . Thus, the Lagrange form for this problem can be defined as (Cai et al., 2009): where \u2208 \u211d 1 \u00d7 2 . Strong duality holds between and : In this dual problem, each iteration step moves to the direction of gradient or of subgradient. We know that where \u0303 is the minimizer to the Lagrange function of . The original optimization problem can be rewritten as: where \u0391 is a linear transformation mapping 1 \u00d7 2 matrices into \u211d (\u0391 * is the adjoint of \u0391). Then the Lagrange formulation of such problems is where \u2208 \u211d \u00d7 and \u2208 \u211d . Starting with 0 = , the iteration shrinkage is given by (Cai et al., 2009): In 14, \u0391 can be regarded as sampling operator to extract sampling entries w from \u03a9 and \u0391 * \u0391 = \u03a9 at the same time. Our final goal is to minimize ( ) over a convex set \u2208 . Assuming that is given by = { : ( ) \u2264 0, \u2200i = 1, \u2026 , m}, where \u2032s are the convex functions. Based on the intuitive, as \u2192 \u221e, it becomes much easier to find the minimizer to equation (2). Denote ( ) \u2254 ( 1 ( ) \u2026 ( )). By applying Uzawa's method, we can make sure that every update for happen in nonnegative space. The minimization of this dual function is: A vector + denotes the maximum value in ( , 0). Considering is an affine mapping function, equation 15can be simplified into By Equation (16), we can have an intuitive understanding about linear inequality constraints. Cai et al. (2009) propose a theorem to prove that minimizing the objective function, ( )= 1 2 \u2016 \u2016 2 + \u2016 \u2016 * , is the same as minimizing the nuclear norm when tends to be very large: Let * be the solution to equation 12and \u221e be the minimum Frobenius-norm solution to equation 12, which can be defined as: The following lemmas proposed by Cai et al. (2009) will establish the framework of more general proof for convexity of matrix completion Lemma 1. Suppose that the sequence of step sizes obeys 0 \u2264 inf \u2264 sup < 2. Then the sequence { } derived from equation 5is converging to the solution in equation 8. Lemma 1 will lead to two properties: 1. The sequence {\u2016 \u03a9 ( \u2212 * )\u2016 } is nonincreasing and converging to the theoretic threshold. 2. As a sequence, \u2016 \u2212 * \u2016 2 \u2192 0 as \u2192 \u221e.If we assume the function ( ) is Lipschitz constant, then we know \u2016 ( ) \u2212 ( )\u2016 \u2264 ( )\u2016 \u2212 \u2016 . Lemma 2. If ( * , * ) is a primal-dual optimal pair for equation 12. For each > 0, * follows * = [ * + ( * )] + . Lemma 3. If the sequence of step sizes follows 0 \u2264 inf \u2264 sup < 2 \u2016 ( )\u2016 2 \u2044 , where ( ) is the Lipschitz constant derived by Lemma 1. Under strong duality assumption, the sequence { } in equation 16converges to the solution of equation 12. (Proofs for Lemma 1-Lemma 3 by Cai et al. (2009) are attached) Based on the Lemma 1-Lemma 3, we apply SVT iterations in a mathematical way and conclude that the solution matrix to recover ARMS data matrix is unique and converged. To make SVT more practically effective for implementation, an important aspect of implementing SVT iterations is setting up the stopping criteria (Cai et al., 2009)."}, {"section_title": "By letting", "text": "0 ( ) = , the solution * to equation (8) must also satisfy where is a matrix vanishing outside of \u03a9 . To make sure that is closed to * , it is sufficient to check how close ( , \u22121 ) is. The equation 18would be stopped if the error in the second equation 18is less than one specified tolerance. The stopping criteria can be where is a fixed small tolerance such as 10 \u22125 . If we can get a matrix which makes is less than the tolerance value, we will stop matrix completion process and take as the solution matrix to . In matrix completion problem, the suitable assumption \u2016 \u03a9 ( )\u2016 2 \u224d \u2016 \u2016 2 is applied to fixed matrix , where the symbol \u224d indicates that there must exist a constant . Afterwards, we could have \u2016 \u03a9 ( \u2212 )\u2016 2 \u224d \u2016 \u2212 \u2016 2 and . By controlling the difference between the sampled from the location set, we can control the relative reconstruction error."}, {"section_title": "Data", "text": "We use two datasets in this research. The first dataset is fake data matrix to validate matrix completion approach. The second dataset is ARMS data matrix with a large proportion of missing values. For the fake data as a validation matrix, it is a 2000 \u00d7 200 matrix denoted as X. We generate each column randomly by standard normal distribution. Then we assign positions for missing values and replace the true values with NAs. We get the incomplete matrix M and try to recover it. The ARMS data is a large dataset with a lot of missing values. The rows represent each farmer and the columns represent the statistics of each farmers. As there are so many kinds of statistics, we construct a subset of original ARMS data matrix consisting of all the variables we are interested in. We denote it as 0 . We are interested on farmers with large-scale operation. Thus, 0 consists of all farmers whose acreage is larger than 100. In matrix 0 , we get information on production, sales, net farm income, acreages, cost and expense of the following commodities: corn, wheat, soybean, hog, cattle, cotton, barley, oats, sorghum and hay. Matrix 0 also includes the age of every farmers. We concentrate on net farm income of major commodities. Major commodities are corn, wheat, soybean, hog, cattle and cotton. We extract the farmers who grow only one category of these six major commodities. The net farm incomes of these farmers are regarded as net return for each commodity. We create twelve additional columns and each column indicates the net return for one category of major commodities. In the first six additional columns, the net farm incomes of extracted farmers are the only available elements and other elements are missing values denoted as NAs. As we already have the net return of each commodity and acreages for each commodity, we can calculate the net return of each commodity in per acre based on the available elements and record these values with NAs in the latter six columns. The new matrix includes 0 and net return of each commodity in per acre with NAs. Matrix is our target matrix to recover."}, {"section_title": "Empirical Results", "text": ""}, {"section_title": "Validation matrix recovery effect", "text": "We increase the missing values' proportion in X from 20.5% to 70% with 0.5% increase each time. After matrix completion, we get the recovered matrix, . To check the accuracy of recovery effect, we set up the recovery effect as = where is row index and j is column index. We already know the positions of missing elements in . Thus, from , we can pick up the recovered values which are at the missing positions in and combine them into a subset 1 . We can calculate the interval of elements in 1 to check the accuracy. Figure 1 shows the intuitive range of values in with different missing proportions in the matrix. The densest part of the dots is just above zero. We set up the confidence interval as 1 \u2212 4 first. We denote the values in 1 below 1 \u2212 4 as T, referring to \"True\" and values above 1 \u2212 4 as F, referring to \"False.\" We count the proportion of T and F in 1 . Then repeat the above process by setting up confidence interval as 1 \u2212 3, 1 \u2212 2 and 1 \u2212 1. Table 1 shows the proportion of T and F for different confidence intervals with increasing missing proportions. For one missing proportion in a matrix, the proportion of T increases a little with the expansion of the confidence interval. We should notice that for different missing proportions, the matrix completion processes are independent with each other. For each missing proportion, the target matrix M is different. There is an overall trend that the recovery effect would be worse when the missing proportion is very large. The reason is that we can get little information based on the target matrix M when M is extremely sparse."}, {"section_title": "Validation matrix density estimation", "text": "There are 200 columns in and every column is created by same standard normal distribution. Columns are dual to each other. The density estimation process for each column should be the same. Thus, we just take one column as an example. Figure 2 shows the result of kernel density estimation. It seems that values in that column are distributed as Gaussian distribution. Then we do the Anderson-Darling Test. Anderson-Darling (A-D) test is a goodnessof-fit test which is used for deciding whether a sample is drawn from a specified distribution, most commonly whether the sample data is drawn from the normal distribution (M.J. De Smith, 2014). The null hypothesis of Anderson-Darling (A-D) test is that the sample is drawn from normal distribution. Our test result shows that the p-value is 0.1897. It fails to reject the null hypothesis and we can conclude that the values are distributed as normal distribution. Since we construct the element randomly based on standard normal distribution, the test result is reasonable. We can conclude that matrix completion approach is reliable and valid in our case based on previous test results."}, {"section_title": "ARMS data matrix recovered results", "text": "We impute the missing values among the net return of each commodity in per acre. Based on the acreages of each commodity, we can calculate total net return for each commodity. The statistics on net return for each commodity are in Table 2. From  Table 2, we can conclude that farmers will get negative net returns when they are raising hogs and they will get positive net returns when they focus on other five major commodities. In ARMS database, almost all the farmers are in small scale when they are raising hogs. Farmers always sign the contract with large company such as Tyson Corporation on raising hogs. When the farmers start to raise hogs by themselves, they find that the temperature conditions in their own farms are not very suitable to raise hogs with certain standard which is required by the large company. For example, the clause in contract may state that farmers are required to turn their hogs which are heavier than 100 pounds 5 times a year to the large company. Unfortunately, farmers with smaller scale can only turn in hogs heavier than 100 pounds 4 times a year limited by their poor farm conditions and raising skills. Thus, the cost of raising hogs for farmers is very high and farmers will make less money compared to the amount listed in the contract. During the survey, hog net returns of farmers with smaller scale and large corporation are recorded at the same time. This is the reason why the mean of hog net return is a negative while the maximum value of hog net return is large and positive. We can conclude that large corporations will make a lot of money by signing hog contract but farmers with smaller scale will lose money if they raise hogs. For other major commodities, we can conclude that farmers make money by growing wheat, soybean, corn and cotton. In addition, it is most profitable for farmers to raise cattle."}, {"section_title": "ARMS data matrix density estimation", "text": "Our final goal is to find the overall trend of net return for each commodity. It is important to estimate the kernel density of net return for each commodity. By plotting histograms of net return for each commodity, we find it is difficult to find a known distribution fitted the data perfectly. We need to detect the closest distribution for the net return of each commodity. We apply Kullback-Leibler divergence to find the best distribution which captures the main changes in trend of net return for each commodity. Kullback-Leibler divergence is a measurement on how far the distribution obtained from the observations is from the theoretical distribution. Kullback-Leibler divergence can regarded as a way of approximating the theoretical distribution to the observed data. Lower Kullback-Leibler divergence indicates that the observations are closer to the theoretical distribution. Based on values of Kullback-Leibler divergence in Table 3, we know that normal distribution is best theoretical distribution to depict the trends on net returns of corn, soybean, hog, cattle and cotton. The mean and variance derived from normal distribution will change when we try to estimate trend on net return of a different commodity. Log normal distribution is the best candidate distribution to graph the trend of net return for wheat."}, {"section_title": "Conclusion", "text": "Our study aims to recover the missing values in ARMS dataset and decide the distribution for each commodity. We apply a matrix completion approach to investigate such problem. We present the proof that there should be a unique solution to recover the target matrix when the condition about minimal sampling is achieved. We show that the Singular Value Threshold (SVT) algorithm will lead to a converged solution matrix at last. Theoretically, we can recover ARMS dataset with a large proportion of missing values based on previous work. Table 1 shows that recovery accuracy for most missing proportions is around 52%. It is quite desirable to us. The proportion of recover effect values is increasing with the expansion of confidence interval. The proportion of those values below the accuracy threshold is always larger the proportion for values above the threshold. After applying kernel density estimation, we conclude that the sample elements of one column from recovered matrix are distributed as normal distribution, which is corresponding with our column generation process. It is reasonable for us to apply the matrix completion approach to ARMS data matrix. After we recover all missing values in our target matrix and calculate the mean of net return for each commodity, we know farmers will make money when they are growing corn, soybean, wheat and cotton. It is the most profitable way for farmers to raise cattle based on the result in Table 2. Farmers will lose money when they are raising hogs due to their smaller scale compared to the large corporation. Government should not encourage farmers with small scale to raise hogs and subsidy farmers when they are raising hogs. Since large corporations on hogs make a huge amount of money, government should consider to adjust the tax policy on these companies. We apply Kullback-Leibler divergence to approximate the recovered net-return values of each commodity to some theoretical distributions. When we try to depict the trends of net returns for corn, soybean, hog, cattle and cotton, it is reasonable for us to apply normal distributions with different parameters based on values in Table 3. We can apply log-normal distribution to capture changes on trend of wheat net return.   Cai, J. F., et al., 2009) If ( * , * ) is primal-dual optimal pair for equation (12), by optimality condition, we can derive satisfying that for some \u2208 \u03b3 ( ) and some * \u2208 \u03b3 ( * ). Then, deduces that ( \u2212 * ) \u2212 \u03a9 ( \u22121 \u2212 * ) = 0 and equations obeys that Based on the observation, there should exist \u03a9 * = \u03a9 , \u2016 \u03a9 ( \u2212 * )\u2016 = \u2016 \u03a9 ( \u22121 \u2212 * ) + \u03a9 ( * \u2212 )\u2016 . If we assume = \u2016 \u03a9 ( \u2212 * )\u2016 , we can get 2 = \u22121 2 \u2212 2 \u2329 \u2212 * , \u03a9 ( \u22121 \u2212 * )\u232a + 2 \u2016 \u03a9 ( * \u2212 )\u2016 2 \u2264 \u22121 2 \u2212 2 \u2016 \u2212 * \u2016 2 + 2 \u2016 \u2212 * \u2016 2 For any matrix , \u2016 \u03a9 ( )\u2016 \u2264 \u2016 \u2016 . Under our assumptions about the size of , we have \u2212 2 \u2265 for all \u2265 1 and some > 0 and thus 2 \u2264 \u22121 2 \u2212 2 \u2016 \u2212 * \u2016 2 . Proof for lemma 2 (Cai, J. F., et al., 2009) Properties held by the projection 0 of one point onto a convex set C is where C = \u211d + = { \u2208 \u211d : \u2265 }. Because 0 \u2265 0, \u2329 \u2212 0 , \u2212 0 \u232a \u2264 0, \u2200 \u2265 0. * is dual optimal, then we derive ( * , * ) \u2265 ( * , ), \u2200 \u2265 0. By substituting the expression in Lagrangian form, \u2329 \u2212 * , ( * )\u232a \u2264 0, \u2200 \u2265 0, which is equivalent to \u2329 \u2212 * , * + ( * ) \u2212 * \u232a \u2264 0, \u2200 \u2265 0, \u2200 \u2265 0. Thus, it is reasonable to conclude that * must be the projection of * + ( * ) onto the nonnegative orthant \u211d + . Proof for Lemma 3 (Cai, J. F., et al., 2009) If ( * , * ) are primal-dual optimal pair for the equation 16. Based on the optimality condition, we know for all : \u2329 , \u2212 \u232a + \u2329 \u22121 , ( ) \u2212 ( )\u232a \u2265 0 \u2329 , \u2212 * \u232a + \u2329 * , ( ) \u2212 ( * )\u232a \u2265 0 for some \u2208 ( ) and some * \u2208 ( * ). As these two inequalities are nearly the same, we only need to prove one of them. For the first inequality, minimizes ( , \u22121 ) over all and therefore, \u2208 ( ) and \u2208 ( ), 1 \u2264 \u2264 , such that: Any projections onto a convex set \u211d + is a contracting projection. Therefore, \u2016 \u2212 * \u2016 2 = \u2016 \u22121 \u2212 * \u2016 2 + 2 \u2329 \u22121 \u2212 * , ( ) \u2212 ( * )\u232a We replace ( ) by for short. By assuming the size of , we have 2 \u2212 \u2265 for all \u2265 1 and some > 0. Finally, we get \u2016 \u2212 * \u2016 2 \u2264 \u2016 \u22121 \u2212 * \u2016 2 \u2212 \u2016 \u2212 * \u2016 , and the conclusion as stated in Lemma 3."}]