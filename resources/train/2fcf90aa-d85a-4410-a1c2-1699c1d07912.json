[{"section_title": "", "text": "Most doctoral students, at some point in their training, encounter the revelation that short summaries of research methods in journal articles tidy over much of the complexity and messiness in education research. These summaries spare readers from trivial details, but they can also misrepresent important elements of the research process. Research questions and hypotheses that are presented as a priori predictions may have been substantially altered after the start of an investigation. A report on a single finding may have been part of a series of other unmentioned or unpublished findings. For education researchers, summarizing and reporting how we conduct our investigations is among our most important professional responsibilities. Our mission is to provide practitioners, policymakers, and other researchers with data, theory, and explanations that illuminate educational systems and improve the work of teaching and learning. All of the stakeholders in educational systems need to be able to judge the quality and contextual relevance of research, and that judgment depends greatly on how researchers choose to share the methods and processes behind their work. Two converging forces are inspiring scholars from a variety of fields and disciplines to rethink how we publish our methods and research. On the one hand, digital technologies offer new ways for researchers to communicate and make their work more accessible. The norms of education research and publishing have been shaped by the constraints of the printed page, and the costs of sharing information have declined dramatically in our networked age. At the same time, the academic community is reckoning with serious problems in the norms, methods, and incentives of scholarly publishing. These problems include high failure rates of replication studies (Makel & Plucker, 2014;Open Science Collaboration, 2012, 2015, publication bias (Rosenthal, 1979), high rates of false-positives (Ioannidis, 2005;Simmons, Nelson, & Simonsohn, 2011), and cost barriers to accessing scientific research (Suber, 2004;Van Noorden, 2013). One of the foundational norms of science is that claims must be supported by a verifiable chain of evidence and reasoning. As we better understand problems in contemporary research, we have all the more reason to reaffirm our professional commitment to rigorous investigation. With the global spread of networked technologies, we have more tools than ever to confront these challenges by making our reasoning more transparent and accessible. Open Science is a movement that seeks to leverage new practices and digital technologies to increase transparency and 2 access in scholarly research. There is no single philosophy or unified solution advanced by Open Science advocates (Fecher & Friesike, 2014) but rather, a constellation of emerging ideas, norms, and practices being discussed in a wide variety of fields (see Table 1). In this article, we offer a framework for Open Education Science: a set of practices designed to increase the transparency of evidentiary reasoning and access to scientific research in a domain characterized by diverse disciplinary traditions and a commitment to impact in policy and practice. One challenge in defining Open Education Science is the great methodological diversity within the education fields, and our aim is to describe a framework that can be interpreted and implemented across qualitative, quantitative, and design research. An Open Genome Science might proceed with a defined set of common practices; an Open Education Science must be built on shared principles. For all the methodological variety in educational research, most studies proceed through four common phases that include (1) design, (2) data collection, (3) analysis, and (4) pub lication. In this article, at the invitation of the AERA Open editors, we synthesize approaches for increasing transparency and access in each of these four domains (see Figure 1). We can further clarify what Open Education Science is by explaining what it is not. There is no binary toggle between open and closed scientific practices but rather, a continuum-research practices can be made more or less transparent, and research products can be more or less accessible. Open Education Science is contextual, and sometimes less transparent practices that protect people's privacy or the integrity of a study have benefits that outweigh considerations of transparency. Open Education Science does not offer universal prescriptions to a diverse field. It does not restrict any particular research practice but rather, asks researchers to be transparent and honest about their practices. There is nothing wrong with analyzing data with no a priori hypotheses, but there is something fundamentally corrosive to publishing papers that present post hoc hypotheses as a priori. Open Education Science has an ideological kinship with the movements related to Open Educational Resources and Open (Online) Education (Peters & Britez, 2008)-these movements all seek to use digital tools to reconfigure existing publishing arrangements-but Open Education Science is concerned with the transparency of scientific research on education but not (directly) with the openness of educational practice. Above all, Open Education Science is a work in progress rather than a canonical set of practices. Open Science has critics as well as advocates; valid arguments for and against should be carefully examined and as much as possible, empirically tested. As researchers refine Open Science norms, some techniques will prove to not improve research quality, be unwieldy to implement, or not be cost-effective. However, if education researchers experiment with Open Science approaches, the quality of our research and dialogue will improve, and the public will have greater access to more robust education science."}, {"section_title": "Problems Addressed by Open Education Science", "text": "One way to understand the motivations of Open Education Science advocates is to consider the kinds of problems that they are trying to solve. Here we briefly consider four: the failure of replication, the file drawer problem, researcher positionality and degrees of freedom, and the cost of access."}, {"section_title": "The Failure of Replication", "text": "Among the most urgent reasons for greater transparency in research methods is the growing belief that a substantial portion of research findings may be reports of false positives or overestimates of effect sizes (Simmons et al., 2011). In Ioannidis's (2005) provocative article, \"Why Most Published Research Findings Are False,\" he described several problems in medical research that lead to false-positive rates: underpowered studies, high degrees of flexibility in research design, a bias toward \"positive\" results, and an overemphasis on single studies. Many of these concerns have been heightened by well-publicized failures to replicate previous findings. A large-scale effort to replicate 100 studies in the social sciences found that fewer than 50% of studies replicated (Open Science Collaboration, 2012, 2015; see also Etz & Vandekerckhove, 2016). In the education sciences specifically, one study found that only about 54% of independent replication attempts are successful (Makel & Plucker, 2014). In some instances, large-scale replications and meta-analyses have complicated research lines by failing to confirm original findings in scope or direction or casting doubt on the causal effect (if any) of derived interventions. Examples include ego-depletion (Hagger et al., 2016), implicit bias (Forscher et al., 2017), stereotype threat (Gibson, Losee, & Vitiello, 2014), self-affirmation (Hanselman, Rozek, Grigg, & Borman, 2017), and growth mindset (Li & Bates, 2017). Calls for a stronger focus on the replicability of education research are not new. Shaver and Norton (1980) argued that \"given the difficulties in sampling human subjects, replication would seem to be an especially appropriate strategy for educational and psychological researchers\" (p. 10). They reviewed several years of articles from the American Education Research Journal and found very few examples of replication studies, a pattern that continues across the field despite the proliferation of education research publications."}, {"section_title": "File Drawer Problem", "text": "Problematic norms of scholarly practice are shaped in part by problematic norms in scholarly publishing. Most scholarly journals, especially the most prominent ones, compete to publish the most \"important\" findings, which are typically those with large effect sizes or surprising findings. Publication bias, or the so-called file drawer problem (Rosenthal, 1979; but see also Nelson, Simmons, & Simonsohn, 2018), is the result of researchers and editors seeking to predominantly publish positive findings, leaving null and inconclusive findings in the \"file drawer.\" This is one factor contributing to a scholarly literature that consists disproportionately of positive findings of large effect sizes or striking qualitative findings (Petticrew et al., 2008) that are unrepresentative of the totality of research conducted (the garden of forking paths, described in the following, is another). For example, R. E. Clark (1983) noted that the literature on learning with multimedia was distorted due to journal editors' preference for studies with more extreme claims. Accurate meta-analyses and syntheses of findings depend on having access to all conducted studies, not just extreme ones."}, {"section_title": "Researcher Positionality and Degrees of Freedom", "text": "Qualitative researchers have long discussed the importance of stipulating researcher positioning and subjectivity in descriptions of methods and findings (Collier & Mahoney, 1996;Golafshani, 2003;Sandelowski, 1986). Readers need to understand what stances researchers take toward their investigation and whether those stances were set a priori to an investigation or changed during the course of a study to better understand how the researcher crafts a representation of the reality they studied. For instance, Milner (2007) and other advocates of critical race theory have encouraged researchers to be more reflective and transparent about when and how researchers choose to analyze and operationalize race in educational studies. In quantitative domains, statisticians have come to similar conclusions that understanding when researchers make analytic decisions has major consequences for interpreting findings. It is increasingly clear that post hoc analytic decision making and post hoc hypothesizing all can lead to the so-called garden of forking paths (Gelman & Loken, 2013). With enough degrees of freedom in analytic decision making, researchers can make decisions about exclusion cases, construction of variables, inclusion of covariates, types of outcomes, and other methodological choices until a significant, and thus publishable, effect is found (Gelman & Loken, 2013;Simmonset al., 2011). When researchers report the handful of models that meet a particular alpha threshold without reporting all the other models tested that failed to meet such a threshold, the literature becomes biased. For both qualitative and quantitative research, interpretation of results depends on understanding what stances researchers adopted before an investigation, what constraints researchers placed around their analytic plan, and what analytic decisions were responsive to new findings. Transparency in that analytic process is critical for determining how seriously practitioners or policymakers should consider a result."}, {"section_title": "Cost of Access", "text": "The effective use of research requires going beyond summaries of findings and into scrutiny of researchers' methodological choices. This makes access to published original research of even greater importance, precisely at a time where the costs of access to traditional journals are growing beyond the means of public institutions. Harvard University, one of the world's wealthiest, warned that the costs of journal subscriptions were growing at an unsustainable rate (Sample, 2012). One solution to this challenge is shifting from a toll access model of conventional scholarly publishing to an Open Access model, where digitally distributed research is made available free of charge to readers (Suber, 2004) and publication costs are borne by authors, foundations, governments, and universities. Greater access to education research 4 will provide greater transparency for a wider audience of researchers, policymakers, and educators. Addressing these four problems requires increasing transparency in and access to scientific processes and publication. In the following sections, we describe open approaches to research design, data, analyses, and publication."}, {"section_title": "Open Design and Preregistration", "text": "Research design is essential to any study as it dictates the scope and use of the study. This phase includes formulating the key research question(s), designing methods to address these questions, and making decisions about practical and technical aspects of the study. Typically, this entire phase is the private affair of the involved researchers. In many studies, the hypotheses are obscured or even unspecified until the authors are preparing an article for publication. Readers often cannot determine how hypotheses and other aspects of the research design have changed over the course of a study since usually only the final version of a study design is published. Moreover, there is compelling evidence that much of what does get published is misleading or incomplete in important ways. A meta-analysis found that 33% of authors admitted to questionable research practices, such as \"dropping data points based on a gut feeling,\" \"concealment of relevant findings,\" and/or \"withholding details of methodology\" (Fanelli, 2009). Given that these numbers are based on self-reports and thus suspect to social desirability bias, it is plausible that these numbers are underestimates. In Open Design, researchers make every reasonable effort to give readers access to a truthful account of the design of a study and how that design changed over the duration of the study. Since study designs can be complex, this often means publishing different elements of a study design in different places. For instance, many prominent science journals, such as Science and Proceedings of the National Academy of Science, and several educational journals, such as AERA Open, publish short methodological summaries in the full text of an article and allow more detailed supplementary materials of unlimited length online. In addition, analytic code might be published in a linked GitHub account, and data might be published in an online repository. These various approaches allow for more detail about methods to be published, with convenient summaries for general readers and more complete specifics for specialists and those interested in replication and reproduction. There are also a variety of approaches for increasing transparency by publishing a time-stamped record of methodological decisions before publication: a strategy known as preregistration. Preregistration is the practice of documenting and sharing the hypotheses, methodology, analytic plans, and other relevant aspects of a study before it is conducted (Gehlbach & Robinson, 2018;Nosek et al., 2015). Research methods are defined in part by when analytic decisions are made relative to data collection and analysis. In line with De Groot (2014), we define exploratory research as any kind of research in which important decisions about data collection, analysis, and interpreting are made in response to data that has already been analyzed. There are entire methodologies based on this iterative approach; for instance, in design-based research (for an introduction, see e.g., Sandoval & Bell, 2004), researchers often implement new designs in real educational settings, measure effects, and implement modifications to the design in real time. Much of traditional qualitative and quantitative research is exploratory as well. The phrase exploratory suggests that the research is conducted without an initial hypothesis, but that is rarely the case-what distinguishes exploratory research is that analytic decisions are made both before and after engaging with data. In an interview study, the coding of the transcripts might be adapted based on an initial coding scheme that was used on the first subset of transcripts that were coded. As this makes the analysis dependent on the data, it is exploratory. Historically, many education researchers have distinguished confirmatory from exploratory research by the use of methods that allow for robust causal inference, such as randomization, regression discontinuity design, or other quasi-experimental techniques. However, confirmatory research also requires-as the name implies-that researchers have a hypothesis to confirm along with a plan to confirm it. Tukey (1980) defined the essence of confirmatory research very clearly: \"1) RANDOMIZE! RANDOMIZE! RANDOMIZE! 2) Preplan THE main analysis.\" The first point has been very widely adopted, the second much less so. For instance, a 2003 report from the Institute of Education Sciences describing key elements of well-designed causal studies puts a great deal of emphasis on properly implemented randomization and makes no mention at all of preplanning. As important as randomization is, the rigor of a confirmatory study also depends on researchers ensuring that analytic decisions are not dependent on the data, and transparency in the timing of study design decisions is one powerful way of ensuring readers of this independence. Since the timing of methodological decisions is an important feature of many research methods, increasing transparency around these decisions can improve iterative, exploratory, and design-based research and is essential to making claims in confirmatory research."}, {"section_title": "Approaches to Preregistration", "text": "At its core, preregistration is about being transparent about which methodological choices were made prior to any data analysis and which decisions were informed by the data by creating an online, time-stamped record. A variety of technologies and systems are available for publishing preregistrations, some of which are mentioned in Table 2. The Open Science Framework (www.osf.io) has one widely accessible system used across disciplines, and many other affinity groups are creating similar systems. The Institute of Educational Sciences maintained a Registry of Efficacy and Effectiveness studies, which is currently being reestablished by the Society for Research in Educational Effectiveness (https://www.sree.org/pages/ registry.php). As with all Open Education Science practices, preregistration is not a single approach but comes in varying degrees (see Figure 2). Authors have to decide which aspects of a study they want to preregister and in how much detail. A form of \"preregistration light\" is stating only the hypotheses of a study before data collection takes place. More complete forms of preregistration include also stating the exact operationalization of these hypotheses, perhaps also with sampling procedures and explicit analysis plans-even including statistical code when the shape of the data is well understood. A second dimension of preregistration involves when and how publicly materials will be shared. The most private form of preregistration includes only making the (timestamped) form available after the study has been published, but plans can be made public long before any data have been collected. Some preregistration elements could be shared privately with collaborators, reviewers, or an ethic board early in a study, with more complete disclosure of preregistration materials after publication. As a general principle, it is better for preregistrations to be as early, complete, and public as possible, but there are all kinds of circumstances where this isn't possible: Studies in new contexts, with new instruments, or asking new types of questions will necessarily be less complete. The practice of preregistration is more developed among quantitative researchers, but qualitative researchers may find benefits from preregistering statements about their hypotheses, positionality, coding schemes, or other analytic approaches."}, {"section_title": "Arguments for and Against Preregistration", "text": "Preregistration can be useful in many different research traditions, but it is a functional imperative for valid hypothesis testing and preventing illusory results (Gehlbach & Robinson, 2018). At the heart of frequentist statistics, which still dominates the quantitative education sciences, is the concept of long-term error control. While false positives will individually be reported, the frequency of this type of error is controlled and will, in the long run, not exceed the alpha value-commonly set at 5%. Relative frequencies depend on a denominator: the total amount of tests that have been (or even could have been) performed. If the hypotheses and analyses are not predesignated and are thus exploratory, this denominator becomes unspecified and undefinable. Effectively, it makes null hypothesis tests lose their informative value and decisive nature. This problem was highlighted by De Groot back in 1956 (later translated to English, see De Groot, 2014): If the processing of empirically obtained material has in any way an \"exploratory character,\" i.e. if the attempts to let the material speak leads to ad hoc decision in terms of processing . . ., then this precludes the exact interpretability of possible outcomes of statistical tests. (De Groot, 2014, p. 191) Whenever choices are made based on the data instead of being predesignated, there are so many possible ways to analyze the data that at least one in this garden of forking paths will lead to a statistically significant result (Gelman & Loken, 2013). While this problem holds for studies of any size, it becomes more problematic with an increasing number of variables and/or samples (Van der Sluis, Van der Zee, & Ginn, 2017). Interpretable null hypothesis testing depends on preregistration of hypotheses and all other decisions that affect the kind and number of statistical tests that might be run and/or reported. "}, {"section_title": "6", "text": "Concerns with preregistration generally fall into two categories: the time-cost of preregistration and the concern that preregistration limits researcher creativity. Time is a valuable commodity, even more so in a \"publish or perish\" culture. While learning any new methodological approach requires an upfront investment of time, once a researcher has grown used to preregistration, it changes the order of operations rather than requiring more time. Even if preregistration does require some more time and effort, this is a small investment that comes with substantial rewards in the form of statistical validity of the analyses and increased transparency. Importantly, preregistering the hypotheses and methods of a study does not place a limit on a scientist's creativity or ability to explore the data. That is, researchers can do everything in a preregistered study that they could do in a nonpreregistered study. The difference is that in the former it will be made clear which decisions were based before the results were known and which decisions are contingent on the results. As such, it requires making a distinction in publication between exploratory and confirmatory work, but it does not hinder or limit either (De Groot, 2014)."}, {"section_title": "Incentivizing Open Design: Registered Reports and Supplementary Materials", "text": "The role that preregistration will start to play with the education sciences will depend to a large degree on the willingness of individual researchers to experiment with it and the extent to which the scientific community at large incentivizes preregistration. One compelling approach to incentivizing preregistration is for journals to adopt a new format of empirical research article called a Registered Report (Chambers, Dienes, McIntosh, Rotshtein, & Willmes, 2015;Nosek & Lakens, 2014). The Registered Report format has several advantages over the traditional publication and peer-review system, which we will explain with an example of a published Registered Report (Van der Zee, Admiraal, Paas, Saab, & Giesbers, 2017). In January 2016, the authors of this study submitted for peer review a manuscript containing only the Introduction and Method sections, containing a detailed analysis plan as well as the materials to be used in the study. Journal reviewers provided critical feedback, including suggestions to improve on various flawed aspects of the study design. As no data had yet been collected, these changes could be promptly included in the study design. After these changes were approved by the editors, the manuscript received \"in-principle acceptance\": The editors agreed to publish the study if it was completed as described regardless of the direction or magnitude of findings. After running the study and analyzing the data in accordance with the preapproved plan, the manuscript was submitted again for a brief round of peer review. As the study was performed according to protocol, it was published. As all editorial decisions were made before the results were known, Registered Reports are essentially free from publication bias. While preregistration requires planning and forethought from researchers, it is never too late to increase transparency in research design. Detailed supplementary materials can be submitted as online appendices to many journals or stored on a personal or institutional website and linked from a journal article. Journal articles publish only summaries of methods both because of the historical constraints of the printed page and to keep things concise for general readers, but in a networked world, any researcher or group can take simple steps to make their research designs and methods more transparent to practitioners, policymakers, and other researchers. One theme we will return to throughout this article is that Open Education Science is not a prescribed set of practices but an invitation for any researcher with any study to find at least one additional way to make the work more transparent for scientific scrutiny."}, {"section_title": "Open Data", "text": "Open Data often refers to proactively sharing the data, materials, analysis code, and other important elements of a study on a public repository such as the Open Science Framework (www.osf.io) or others mentioned in Table 3. Research data include all data that are collected or generated during scientific research, whether quantitative or qualitative, such as texts, visual stimuli, interview transcripts, log data, diaries, and any other materials that were used or produced in a study. In line with the statement that scholarly work should be verifiable, Open Data is the philosophy that authors should, as much as is practical, make all the relevant data publicly available so they can be inspected, verified, reused, and further built on. The U.S. National Research Council (1997) stated that \"Freedom of inquiry, the full and open availability of scientific data on an international basis, and the open publication of results are cornerstones of basic research\" (p. 2)."}, {"section_title": "Approaches to Sharing Data", "text": "Like the other forms of transparency, Open Data is not a dichotomous issue but a multidimensional one. Researchers have to decide what data they want to share, with whom, and when, as shown in the data sharing worksheet in Table 4 (adapted from Carlson, 2010). Fortunately, educational researchers in the United States and many other countries have a great deal of experience with Open Data. For instance, the National Center for Education Statistics makes a wide variety of data sets publicly available with a variegated set of approaches. The various data products from the National Assessment of Education Progress showcase this differentiated approach to balancing privacy and openness."}, {"section_title": "7", "text": "School-level data, which contain no personally identifiable information, are made easily accessible through public data sets. Student-level data are maintained with far stricter guidelines for accessibility and use, but statistical summaries, documentation of data collection methods, and codebooks of data are made easily and widely available. While some fields may be just beginning to share data, education has a rich history of examples to draw on. As the costs of data storage and transmission have dramatically decreased, it is now possible for individual researchers and teams to engage in some of the same kinds of practices that once required large institutional investments. The most common approach to open data, making data available on request, does not work. Researchers requested data from 140 authors with articles published in journals that required authors to share data on request, but only 25.7% of these data sets were actually shared (Wicherts, Borsboom, Kats, & Molenaar, 2006). What is even more worrisome is that reluctance to share data is associated with weaker evidence and a higher prevalence of apparent errors in the reporting of statistical results (Wicherts, Bakker, & Molenaar, 2011). To increase the transparency of research, data should be shared proactively on a publicly accessible repository. Long-term and discoverable storage is advisable for data that are unique (i.e., can be produced just once) and/or involved a considerable amount of resources to generate. These features are often true for qualitative and quantitative data alike. The value of shared data depends on quality of its documentation. Simply placing a data set online somewhere, without any explanation of its content, structure, and origin, is of limited value. A critical aspect of Open Data is ensuring that research data are findable (in a certified repository) as well as clearly documented by meta-data and process documents. Wilkinson and colleagues (2016) published an excellent summary of FAIR practices for data management, addressing issues of Findability, Accessibility, Interoperability, and Reusability. Elman and Kapiszewski (2014) wrote an informative guide to sharing qualitative data, which we recommend to qualitative researchers. In case research data cannot be shared at all, due to privacy issues or legal requirements, it is typically still possible to at least share meta-data: information about the scope, structure, and content of the data set. In addition, researchers can share \"process documents,\" which outline how, when, and where the data were collected and processed. In both cases (meta-data and process documentation), transparency can be increased even when the research data themselves are not shared. New data-sharing repositories like Dataverse allow institutions or individual researchers to create data projects and share different elements of that project under different requirements so that some elements are accessible publicly and others require data use agreements (King, 2007)."}, {"section_title": "Benefits of and Concerns With Sharing Data", "text": "Open Data can improve the scientific process both during and after publication. Without access to the data underlying a paper that is to be reviewed, peer reviewers are substantially hindered in their ability to assess the evidential value of the claims. Allowing reviewers to audit statistical calculations will have a positive effect on reducing the number of calculation errors, unsupported claims, and erroneous descriptive statistics that are later found in the published literature (Nuijten, Hartgerink, van Assen, Epskamp, & Wicherts, 2016;Van der Zee, Anaya, & Brown, 2017). Open Data also enables secondhand analyses and increases the value of gathering data, which require direct access to the data and cannot be performed using only the summary statistics typically presented in a paper. Data collection can be a lengthy and costly process, which makes it economically wasteful to not share this valuable commodity. Open Data is a research accelerator that can speed up the process of establishing new important findings (Pisani et al., 2016;Woelfle, Olliaro, & Todd, 2011). Well-established Open Data sets like the National Assessment of Education Progress, along with new data sets like the test scores, student surveys, and classroom videos from the Measures of Effective Teaching Project (https://www.icpsr.umich.edu/ icpsrweb/content/metldb/projects.html), provide the evidentiary foundation for scores of studies. As the education field gains expertise in generating, maintaining, and reusing these kinds of data sets-and as it becomes easier for smaller scale research endeavors to share data using repositories such as the Dataverse (King, 2007)-we will continue to see the benefits of investment in Open Data. Perhaps the strongest objection to Open Data sharing concerns issues of privacy protection. Safeguarding the identity and other valuable information of research participants is of utmost importance and takes priority over data sharing, but these are not mutually exclusive endeavors. Sharing data is not a binary decision, and there is a growing body of research around differential privacy that suggests a variegated approach to data sharing (Daries et al., 2014;Gaboardi et al., 2016;Wood et al., 2014). Even when a data set cannot be shared publicly in its entirety, it may be possible to share deidentified data or, as a minimum, information about the shape and structure of the data (i.e., meta-data). Daries et al. (2014) provided one case study of a de-identified data set from MOOC learners, which was too \"blurred\" for accurately estimating distributions or correlations about the population but could provide useful insights about the structure of the data set and opportunities for hypothesis generation. For textual data, such as transcripts from interviews and other forms of qualitative research, there are tools that allow researchers to quickly de-identify large bodies of texts, such as NETANOS (Kleinberg, Mozes, & van der Toolen, 2017), or other tools mentioned in Table 3. Even when a whole data set cannot be shared, subsets might be sharable to provide more insight into coding techniques or other analytic approaches. Privacy concerns should absolutely shape decisions about what researchers choose to share, and researchers should pay particular attention to implications for informed consent and data collection practices, but research into differential privacy shows that openness and privacy can be balanced in thoughtful ways. Another concern with data sharing is \"scooping\" and problems with how research production is incentivized. For example, in an editorial in the New England Journal of Medicine, Longo and Drazen (2016) stated that: \"There is concern among some front-line researchers that the system will be taken over by what some researchers have characterized as 'research parasites'\" (para. 3). Specifically, these authors were concerned that scholars might \"parasitically\" use data gathered by others; they suggested that data should instead be shared \"symbiotically,\" for example by demanding that the original researchers will be given co-author status on all papers that use data gathered by 9 them. This editorial, and especially the framing of scholars as \"parasites\" for reusing valuable data, sparked considerable discussion, which resulted in the ironically titled \"Research Parasite Award\" for rigorous secondary analysis (http://researchparasite.com/). Here we see not necessarily a clash of values, as none seem to have directly argued against benefits of sharing data, but instead a debate about how we should go about data sharing. Another fear expressed by some researchers is that proactively sharing data in a public repository will lead other researchers to use their data and potentially \"scoop\" potential research ideas and publications. These are real concerns in our current infrastructure of incentives, so along with technical improvements and policies to make data sharing easier, we need to address incentives in scholarly promotion to make data sharing more valued."}, {"section_title": "Incentivizing Open Data", "text": "The U.S. National Research Council (1997) has argued: \"The value of data lies in their use. Full and open access to scientific data should be adopted as the international norm for the exchange of scientific data derived from publicly funded research\" (p. 10). There are various ways to make better use of the data that we have already generated, such as data sets with persistent identifiers, so they can be properly cited by whoever has reused the data. This way, the data collectors continue to benefit from sharing their data as they will be repeatedly cited and have proof of how their data have been fundamental to others' research. There is evidence that Open Data increase citation rates (Piwowar, Day, & Fridsma, 2007), and other institutional actors could play a role in elevating the status of Open Data. An increasing number of journals have started to award special badges that will be shown on a paper that is accompanied by publicly available data in an Open Access repository (https://osf.io/ tvyxz/wiki/5.%20adoptions%20and%20endorsements/). Journal policies can have a strong positive effect on the prevalence of Open Data (Nuijten et al., 2017). Scholarly societies like AERA or prominent education research foundations like the Spencer Foundation and the Bill and Melinda Gates Foundation could create new awards for the contribution of valuable data sets in education research. Perhaps most importantly, promotion and tenure committees in universities should recognize the value of contributing data sets to the public good and ensure that young scholars can be recognized for those contributions."}, {"section_title": "Open Analyses", "text": "The combination of Open Design and Open Data sharing makes possible new frontiers in Open Analysis-the systematic reproduction of analytic methods conducted by other researchers. Replication is central to scientific progress as any individual study is generally insufficient to make robust or generalizable claims. It is only after ideas are tested and replicated in various conditions and contexts and results meta-analyzed across studies that more durable scientific principles and precepts can be established. While Open Design and Open Data are increasingly well-established practices, in this section on Open Analysis, we speculate on new approaches that could be taken to enable greater transparency in analytic methods. One form of replication is a reproduction study, where researchers attempt to faithfully reproduce the results of a study using the same data and analyses. Such studies are only possible through a combination of Open Data and Open Design so that replication researchers can use the same methodological techniques but also the same exclusion criteria, coding schemes, and other analytic steps that allow for faithful replication. In recent years, perhaps the most famous reproduction study was by Thomas Herndon, a graduate student at UMass Amherst who discovered that two Harvard economists, Carmen Reinhart and Kenneth Rogoff, had failed to include five columns in an averaging operation in an Excel spreadsheet (The Data Team, 2016). After averaging across the full data set, the claims in the study had a much weaker empirical basis. In quantitative research, where statistical code is central to conducting analyses, the sharing of that code is one way to make analytic methods more transparent. GitHub and similar repositories (see Table 5) allow researchers to store code, track revisions, and share with others. At a minimum, they allow researchers to publicly post analytic code in a transferable, machine-readable platform. Used more fully, GitHub repositories can allow researchers to share preregistered code-bases that present a proposed implementation of hypotheses, final code as used in publication, and all of the changes in between. As with data, making code \"available on request\" will not be as powerful as creating additional mechanisms that encourage researchers to proactively share their analytic code: as a requirement for journal or conference submissions, as an option within study preregistrations, or in other venues. Reinhart and Rogoff's politically consequential error might have been discovered much sooner if their analyses had been made available along with publication rather than after the idiosyncratic query of an individual researcher. Even when code is available, differences across software versions, operating systems, or other technological systems can still cause errors and differences. A powerful new tool for Open Analysis are Jupyter notebooks (Kluyver et al., 2016;Somers, 2018). Jupyter is an open-source Web application that allows publication of data, code, and annotation in a Web format. Jupyter notebooks can be constructed to present the generation of tables and figures in stepwise fashion, so a block of text description is followed by a working code snippet, which is followed by the generation 10 of a table or figure. A sequence of these segments can then be used to demonstrate the generation of a full set of figures and tables for a publication. Users can copy and fork these notebooks to reproduce analyses, test additional boundary conditions, and understand how each section of a paper is generated from the data. Jupityr notebooks point the way toward an alternative future where publications provide complete, transparent demonstrations of how analyses are conducted rather than the summaries of the findings of these analyses. While replication and reproductions are most common in quantitative research, they can be just as relevant and vital for many qualitative approaches (Anderson, 2010;Onwuegbuzie & Leech, 2010). At present, most research articles based on qualitative data indicate that the research process included multiple iterative steps, but the final research article presents only a summary of top-level themes with selected evidence. With unlimited storage space for supplementary materials in articles, qualitative researches could provide greater transparency in their analyses by making publicly available more of the underlying data, coding schemes, examples of coded data, analytic memos, examples of reconciled disagreements among coders, and other important pieces that describe the underlying analytic work leading to conclusions. At present, much of this material could be made publicly available by selectively releasing project files that can be exported from Dedoose, Nvivo, Atlas.ti, and other tools. Privacy concerns will prevent certain kinds of resources from being shared, but virtually every qualitative project has selections of data that can be de-identified to provide at least examples of the kinds of analytic steps taken to reach conclusions. Just as various new forms of open source software have made it increasingly possible for quantitative researchers to more widely share tools, data, and analyses, hopefully the next generation of qualitative data analysis software will also make qualitative research processes more transparent."}, {"section_title": "Open Publication", "text": "Open Access (OA) literature is digital, online, available to read free of charge, and free of most copyright and licensing restrictions (Suber, 2004). Most for-profit publishers obtain all the rights to a scholarly work and give back limited rights to the authors. With Open Access, the authors retain copyright for their article and allow anyone to download and reprint provided that the authors and source are cited, for example under a Creative Commons Attribution License (CC BY 4.0). Of the 1.35 million scientific papers published in 2006, about 8% were Open Access immediately or after an embargo period (Bj\u00f6rk, Roos, & Lauri, 2009), and a more recent analysis shows that of the articles published in 2015, a total of 45% were openly available (Piwowar et al., 2018). Open Access publishing is on the rise and has become mainstream, with benefits for both the scientific community and individual researchers. In the words of Merton (1973): \"The institutional conception of science as part of the public domain is linked with the imperative for communication of findings\" (p. 274). Opening access increases the ability of researchers, policymakers, and practitioners to leverage scientific findings for the public good. For individual researchers, scholarly works that are published in open formats are cited earlier and more frequently (Craig, Plume, McVeigh, Pringle, & Amin, 2007;Lawrence, 2001). Sharing a publicly accessible preprint can also be used to receive comments and feedback from fellow scientists, a form of informal peer review. We discuss two of the most important approaches to Open Access publishing: preprint repositories (sometimes called Green OA) and Open Access journals (sometimes called Gold OA)."}, {"section_title": "Preprints", "text": "Preprints are publicly shared manuscripts that have not (yet) been peer reviewed. A variety of peer-reviewed journals acknowledge the benefits of preprints. For example, the Journal of Learning Analytics states that Economists have embraced this approach for many years, through the NBER Working Paper series, and the openness of economics research magnifies its public impact (Fox, 2016). Across the physical and computer sciences, repositories such as ArXiv have dramatically changed publication practices and instituted a new form of public peer review across blogs and social media. In the social sciences, the Social Science Research Network and SocArXiv offer additional repositories for preprints and white papers. Preprints enable more iterative feedback from the scientific community and provide public venues for work that address timely issues or otherwise would not benefit from formal peer review. For example, the current paper has been online as a preprint since early 2018, which allowed us to garner feedback and improve the manuscript (Van der Zee & Reich, 2018). Whereas historically peer review has been considered a major advantage over these forms of nonreviewed publishing, the limited amount of available evidence suggests that the typically closed peer-review process has no to limited benefits (Bruce, Chauvin, Trinquart, Ravaud, & Boutron, 2016;Jefferson, Alderson, Wager, & Davidoff, 2002). Public scholarly scrutiny may prove to be an excellent complement, or perhaps even an alternative, to formal peer review. For an overview of relevant tools and websites, see Table 6."}, {"section_title": "Open Access Journals", "text": "Most research is still published by a publisher that charges an access fee. This so-called paywall is the main source of income for most publishers. As publishers essentially rely on free labor from scholars-they do not pay the people who write the manuscripts, conduct the reviews, and perform much of the editorial work-this raises the question of why society has to pay twice for research: first to have it done and then to gain access to it. The alternative infrastructure is Open Access, whereby readers get access to scholarly literature for free, and this literature is sometimes made available with only minimal restrictions around copying, remixing, and republishing. Most Open Access journals are online only, and so they avoid the costs of printing and publication. Many Open Access journals use article processing charges to cover the costs of publishing. These article processing fees vary between $8 and $3,900, with international publishers and journals with high impact factors charging the most (Solomon & Bj\u00f6rk, 2012). Additionally, journals published by societies, universities, and scholars charge less than journals from large publishers. A variety of strategies have been outlined on how subscription-based and expensive pay-to-publish journals can move to a model that is much cheaper (Bj\u00f6rk, 2017;Bj\u00f6rk & Solomon, 2014;Laakso, Solomon, & Bj\u00f6rk, 2016). This approach shifts the for-profit nature of scholarly publishing into one that is more aligned with the norms and values of the scientific method (e.g., Bj\u00f6rk & Hedlund, 2009). Not everyone is enthusiastic about Open Access journals. For example, Romesburg (2016) argues that Open Access journals are of lower quality, pollute science with false findings, reduce the popularity of society journals, and should be actively opposed. Some of these critiques are serious challenges to the progress of open science, while other critiques are sometimes based on incorrect assumptions, as discussed in Bolick, Emmett, Greenberg, Rosenblum, and Peterson (2017). A pertinent concern is the existence of so-called \"predatory journals\" or \"pseudo journals\" (J. Clark & Smith, 2015). These journals are not concerned with the quality of the papers they publish but seek financial gains by charging publication fees. Scholars who publish in these journals are either fooled by the appearance of legitimacy or looking for an easy way to boost their publication list-a tendency that has been attributed to the increasing pressure to publish or perish (Moher & Srivastava, 2015). Predatory journals have rapidly increased in number; from 2010 to 2014, the number of papers published in predatory journals rose from 53,000 to 420,000 (Shen & Bj\u00f6rk, 2015). Predatory publishing is an important issue that scholarly communities need to address, but the real force behind predatory publishing is not the expansion of Open Access business models but the publishor-perish culture of academia. Evidence-based educational policymaking and practice depend on access to evidence. So long as educational publishing is primarily routed through for-profit publishers, a substantial portion of the key stakeholders of education research will have limited access to the tools they need to realize evidence-based teaching and learning."}, {"section_title": "The Future of Open Education Science", "text": "In the decades ahead, we hope that Open Education Science will become synonymous with good research practice. All of the constituencies that education researchers seek to serve-our fellow scholars, policymakers, school leaders, teachers, and learners-benefit when our scientific practices are transparent and the fruits of our labors are distributed widely. Many of the limits to openness in education research are the results of norms, policies, and practices that emerged in an analog age with high costs of information storage, retrieval, and transfer. As those costs have dramatically declined, it behooves all of us-the first generation of education researchers in the networked age-to rethink our practices and imagine new ways of promoting transparency and access through Open Design, Open Data, Open Analysis, and Open Access publishing. Making the education sciences more open is not an abstract process at the system level but one that occurs in the daily life of individual researchers. The path toward Open Education Science is a flexible one and does not require immediate, dramatic change; rather, with each new study, with each student or apprentice, with each new publication, researchers and teams can take one step at a time toward more open practice. It will take experimentation, time, and dialogue for new practices to emerge, and there will be new technologies to try and ongoing assessment of how new practices are affecting the quality of research produced in our field. Researchers who adopt these new practices will be able to find support from new scholarly societies, like the Society for the Improvement of Psychological Science, and fellow researchers trying to find ways to improve on the methodological traditions of our different fields and disciplines. To be sure, major institutions such as the Institute of Education Science, American Educational Research Association, and education research foundations can take important steps to create new policies and incentives-new RFP requirements, recognitions and awards, and funding sources for research conforming to open practices. But even institutions can change at the behest of individual researchers: As authors, we are both currently editing special journal issues about Registered Reports. These opportunities came about simply because we reached out to individual editors from journals we respected, asked them to consider a new idea, and volunteered to help. If one volunteer from each of the many subdisciplines of education offers to help their community move toward Open Education Science, meaningful institutional changes will follow. Parts of the process of adopting open science will be difficult and contentious, as with all changes in norms and practices. But with a courageous spirit to reexamine past practices and imagine a more rigorous future, Open Education Science will lead to better research that better serves the common good."}, {"section_title": "ORCID iD", "text": "T. van der Zee https://orcid.org/0000-0002-8058-9163"}]