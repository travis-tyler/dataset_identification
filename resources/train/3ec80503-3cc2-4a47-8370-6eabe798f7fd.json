[{"section_title": "International", "text": ""}, {"section_title": "Outcomes of Learning in Mathematics Literacy and Problem Solving:", "text": "PISA 2003 Results From the U.S. Perspective"}, {"section_title": "U.S. Performance in Mathematics Literacy and Problem Solving", "text": "In 2003, U.S. performance in mathematics literacy and problem solving was lower than the average performance for most OECD countries (tables 2 and 3). The United States also performed below the OECD average on each mathematics literacy subscale representing a specific content area (space and shape, change and relationships, quantity, and uncertainty). This is somewhat different from the PISA 2000 results, when reading literacy was the major subject area, which showed the United States performing at the OECD average (Lemke et al. 2001). Along with scale scores, PISA 2003 also uses six proficiency levels (levels 1 through 6, with level 6 being the highest level of proficiency) to describe student performance in mathematics literacy (exhibit 5) and three proficiency levels (levels 1 through 3, with level 3 being the highest level of proficiency) to describe student performance in problem solving (exhibit 9). In mathematics literacy, the United States had greater percentages of students below level 1 and at levels 1 and 2 than the OECD average percentages (figure 5, table B-6). The United States also had a lower percentage of students at levels 4, 5, and 6 than the OECD average percentages. Results for each of the four mathematics content areas followed a similar pattern. In problem solving, the United States also had greater percentages of students below level 1 and at level 1 than the OECD average percentages, and a lower percentage of students at levels 2 and 3 than the OECD average percentages (figure 8, table B-15). This is also somewhat different from the PISA 2000 reading literacy results, which showed that while the percentages of U.S. students performing at level 1 and below were not measurably different from the OECD averages, the United States had a greater percentage of students performing at the highest level (level 5) compared to the OECD average . In mathematics literacy and problem solving in 2003, even the highest U.S. achievers (those in the top 10 percent in the United States) were outperformed on average by their OECD counterparts (figures 4 and 7, tables B-4 and B-13). There were no measurable changes in the U.S. scores from 2000 to 2003 on either the space and shape subscale or the change and relationships subscale, the only content areas for which trend data from 2000 to 2003 are available (table B-11). In both 2000 and 2003, about two-thirds of the other participating OECD countries outperformed the United States in these content areas."}, {"section_title": "U.S. Performance in Reading Literacy and Science Literacy", "text": "The U.S. average score in reading literacy was not measurably different from the OECD average in 2000 or 2003 (figure 9, table B-16), nor was there any measurable change in the U.S. reading literacy score from 2000 to 2003. The U.S. score was below the OECD average science literacy score in 2003 (figure 9, table B-17). There was no measurable change in the U.S. science literacy score from 2000 to 2003."}, {"section_title": "Differences in Performance by Selected Student Characteristics", "text": "\nThis section provides information about how students with various characteristics (males and females, students of different races and from different socioeconomic backgrounds) performed on PISA 2003. Because PISA 2003's emphasis was on mathematics literacy and problem solving, the focus in this section is on performance in these areas. 10 This report does not address possible changes in performance for these groups from 2000 to 2003. When considering these results, it is important to bear in mind that there need not be a cause-and-effect relationship between being a member of a group and achievement in PISA 2003. Student performance can be affected by a complex mix of educational and other factors that are not examined here."}, {"section_title": "Sex", "text": "Males outperformed females in mathematics literacy in the United States and in two-thirds of the other countries (figure 10, table B-18). Within the United States, greater percentages of male students performed at level 6 (the highest level) than female students in mathematics literacy, but larger percentages of females were not seen at lower levels (below level 1 and levels 1 through 5; table B-19). In other words, differences in the overall scores between males and females in the United States were due at least in part to the fact that a higher percentage of males were found among the highest performers, not to a higher percentage of females found among the lowest performers. In the majority of the PISA 2003 countries (32 out of 39 countries), including the United States, there were no measurable differences in problem-solving scores by sex (figure 10, table B-21). However, females outscored their male peers in problem solving in six of the seven remaining participating countries, as well as at the OECD average. Males outscored females in problem solving in Macao-China.\nFifteen-year-old females in the United States scored 480 on the combined mathematics literacy scale, which was lower than the average male score of 486 (figure 10, table B-18). Males also outperformed females in 25 other countries (20 OECD countries and 5 non-OECD countries), a pattern evident in the OECD average scores of 494 for females and 506 for males. Iceland was the only country in which females scored higher in mathematics literacy than males. Within the United States, greater percentages of male students performed at level 6 (the highest level) than female students in   "}, {"section_title": "Socioeconomic Background", "text": "In 2003, a few countries showed stronger relationships between socioeconomic background (as measured by parental occupational status) and student performance than the United States, while more showed weaker relationships. In 2003, the relationship between socioeconomic background and student performance in mathematics literacy was stronger in 5 countries (Belgium, Czech Republic, Germany, Hungary, and Poland) than in the United States, while 11 countries had weaker relationships (table B-25). Three of the same five countries (Belgium, Germany, and Hungary) had stronger relationships between socioeconomic background and problem-solving performance than the United States, while 12 had weaker relationships."}, {"section_title": "Race/Ethnicity", "text": "In the United States in PISA 2003, Blacks and Hispanics scored lower on average than Whites, Asians, and students of more than one race in mathematics literacy and problem solving (figure 11, table B-26). Hispanic students, in turn, outscored Black students. In both mathematics literacy and problem solving, the average scores for Blacks and Hispanics were below the OECD average scores, while scores for Whites were above the OECD average scores. For further results from PISA 2003, see the Organization for Economic Cooperation and Development (OECD) publication Learning for Tomorrow's World -First Results From PISA 2003, available at http://www.pisa.oecd.org (OECD 2004). A technical report for PISA 2003which describes in detail all the procedures used in the design, data collection, quality control, and analysis for the study, as well as the PISA 2003 data itself-will also be made available at that site. Tables   Table 1. Participation in the Program for International Student Assessment (PISA), by country: 2000 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1 Table 2. Average combined mathematics literacy scores and subscale scores of 15-year-old students, by country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14 Table 3. Average scores of 15-year-old students on the problem-solving scale, by country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .29 Table A-1. Coverage of target population, student and school samples, and participation rates in the Program for International Student Assessment (PISA), by country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .52 Table A-2. Response rates of 15-year-old students for selected background variables, by country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .63 x Tables   Table B-1. Percentage distribution of 15-year-old students, by grade and country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .69 Table B-2. Percentage distribution and average combined mathematics literacy scores of U.S. 15-year-old students, by type of mathematics class: 2003 . . . . . . . . . . . . .71 Table B-3. Average mathematics literacy scores and subscale scores of 15-year-old students, by country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .72 Table B-4. Combined mathematics literacy scores of 15-year-old students, by percentiles and country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .73 Table B-5. Standard deviations of 15-year-old students' combined mathematics literacy scores, by country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .75 Table B-6. Percentage distribution of 15-year-old students scoring at each proficiency level on the combined mathematics literacy scale, by country: 2003 . . . . . . .76 Table B-7. Percentage distribution of 15-year-old students scoring at each proficiency level on the mathematics literacy quantity subscale, by country: 2003 . . . . . .78 Table B-8. Percentage distribution of 15-year-old students scoring at each proficiency level on the mathematics literacy space and shape subscale,  Table B-16. Average reading literacy scores of 15-year-old students, by country: 2000 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .92 Table B-17. Average science literacy scores of 15-year-old students, by country: 2000 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .93 xi PISA 2003 Results From the U.S. Perspective Table B-18. Average combined mathematics literacy scores of 15-year-old students, by sex and country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .94 Table B-19. Percentage distribution of 15-year-old students scoring at each proficiency level on the combined mathematics literacy scale, by sex and country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .95 Table B-20. Average mathematics literacy subscale scores of 15-year-old students, by sex and country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .99 Table B-21. Average problem-solving scores of 15-year-old students, by sex and country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .103 Table B-22. Average combined reading literacy and science literacy scores of 15-year-old students, by sex and country: 2000 and 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . .104 Table B-23. Mean International Socioeconomic Index (ISEI) score of 15-year-old students, by quarters of the ISEI index and country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . .108 Table B-24. Average combined mathematics literacy scores of 15-year-old students, by quarters of the International Socioeconomic Index (ISEI) and country: 2003 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .109 \nRacial and ethnic groups vary between countries, so it is not possible to compare their performance across countries on international assessments. Thus, this section refers only to 2003 findings for the United States. Throughout this section, \"White\" refers to White, non-Hispanic students, \"Black\" to Black, non-Hispanic students, \"Asian\" to Asian, non-Hispanic students, and \"Hispanic\" to Hispanic students of any race. Results for two groups (American Indian or Alaska Native and Hawaiian or Other Pacific Islander) are not shown separately because small sample sizes did not allow for accurate estimates. In both mathematics literacy and problem solving, Blacks and Hispanics scored lower, on average, than Whites, Asians, and stu-dents of more than one race (figure 11, table B-26). Hispanic students, in turn, outscored Black students. This pattern of performance on PISA 2003 by race/ethnicity is similar to that found in PISA 2000 and on the National Assessment of Educational Progress (NAEP) (Braswell, Daane, and Grigg 2003;Lemke et al. 2001). In both mathematics literacy and problem solving, the average scores for Blacks and Hispanics were below the respective OECD average scores, while scores for Whites were above the OECD average scores. Students who were White, Asian, and of more than one race scored at level 3 in mathematics literacy, compared to level 2 for Hispanic students and level 1 for Black students (figure 11, exhibit 5). In problem solving, average scores for Whites and Asians placed them in level 2, while Black, Hispanic, and students of more than one race scored at level 1 (figure 11, exhibit 9). \nIn the United States, students' race/ethnicity was obtained through student responses to a two-part question. Students were asked first whether they were Hispanic or Latino, and then asked whether they were members of the following racial groups: American Indian or Alaska Native, Asian, Black or African American, Native Hawaiian or other Pacific Islander, or White. Multiple responses to the race classification question were allowed. Results are shown separately for Asians, Blacks, Hispanics, Whites, and students who selected more than one race. Students identifying themselves as Hispanic and also other races were included in the Hispanic group, rather than in a racial group."}, {"section_title": "List of", "text": ""}, {"section_title": "List of Reference", "text": ""}, {"section_title": "Introduction", "text": ""}, {"section_title": "PISA in Brief", "text": "The Program for International Student Assessment (PISA) is a system of international assessments that measures 15-yearolds' capabilities in reading literacy, mathematics literacy, and science literacy every 3 years. PISA was first implemented in 2000 (figure 1). PISA is sponsored by the Organization for Economic Cooperation and Development (OECD), an intergovernmental organization of 30 industrialized nations. In 2003, 41 countries participated in PISA, including 30 OECD countries and 11 non-OECD countries (table 1). Of those 41 countries, comparisons for 39 countries (29 OECD countries and 10 non-OECD countries) are provided in this report. Data for one country, Brazil, were not available at the time of report production, and data for one other, the United Kingdom, are not discussed due to low response rates.  Due to low response rates, PISA 2000 data for the Netherlands are not discussed in this report. For information on the results for the Netherlands, see OECD (2001). 2 Due to low response rates, PISA 2003 data for the United Kingdom are not discussed in this report. 3 Although Brazil participated in PISA 2003, its data were not available in time for production of this report. NOTE: A \"\u2022\" indicates that the country participated in PISA in the specific year. Because PISA is principally an OECD study, non-OECD countries are displayed separately from the OECD countries. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2000 and 2003. In order to provide a critical external perspective on the achievement of U.S. students through comparisons to other nations, the United States participates at the international level in PISA, the Progress in International Reading Literacy Study (PIRLS), and the Trends in International Mathematics and Science Study (TIMSS). 1 TIMSS and PIRLS seek to measure students' mastery of specific knowledge, skills, and concepts, and are designed to reflect curriculum frameworks in the United States and other participating countries. PISA provides a unique and complementary perspective to these studies by not focusing explicitly on curricular outcomes, but on the application of knowledge in reading, mathematics, and science to problems with a reallife context (OECD 1999). The framework for each assessment area is based upon content, processes, and situations or contexts. For example, for mathematics literacy, the content is made up of major mathematical ideas, such as space and shape and uncertainty. The processes describe what strategies students use to solve mathematics problems, such as making connections or performing simple calculations. The situations or contexts refer to the kinds of places in which students might encounter mathematical problems, such as personal or educational. Assessment items are then developed based on these descriptions. PISA uses the terminology of \"literacy\" in each subject area to denote its broad focus on application of knowledge and skills; that is, PISA seeks to ask if 15-year-olds are mathematically literate, or to what extent they can apply mathematical knowledge and skills to a range of different situations they may encounter in their lives. Literacy itself refers to a continuum of skills-it is not a condition that one has or does not have (i.e., literacy or illiteracy), but rather each person's skills place them in a particular place on the literacy continuum. Each PISA data-collection effort assesses one subject area in depth, even as all three are assessed in each cycle so that participating countries have an ongoing source of achievement data in every subject area. In addition to the reading literacy, mathematics literacy, and science literacy, PISA also measures general or cross-curricular competencies such as learning strategies. In this second cycle, PISA 2003, mathematics literacy was the subject area assessed in depth, along with the new cross-curricular area of problem solving. In 2006, PISA will focus on science literacy. Results from PISA 2000, which focused on reading literacy, are described in Lemke et al. (2001) and Organization for Economic Cooperation and Development (OECD) (2001). In addition, a series of thematic reports exploring topics related to reading literacy in greater depth are available through http://www.pisa.oecd.org (see also the PISA resources and publications section of this report for information about PISA publications). This report focuses on the performance of U.S. students in the two major areas assessed in 2003, mathematics literacy and problem solving. Achievement in the minor domains of reading literacy and science literacy in 2003 is also presented, and differences in achievement by selected student characteristics are covered in the final section."}, {"section_title": "The Unique Contribution of PISA", "text": "The United States has conducted surveys of student achievement at a variety of grade levels and in a variety of subject areas through the National Assessment of Educational Progress (NAEP) for many years. NAEP provides a regular benchmark for states and the nation and a means to monitor progress in achievement over time. International Outcomes of Learning in Mathematics Literacy and Problem Solving 1 The United States has also participated in international comparative assessments of civics knowledge and skills (CivEd 1999) and adult literacy (International Adult Literacy Survey [IALS 1994] and Adult Literacy and Lifeskills Survey [ALL 2003]). The target age of 15 allows countries to compare outcomes of learning as students near the end of compulsory schooling. PISA's goal is to answer the question \"what knowledge and skills do students have at age 15?\" taking into account schooling and other factors that may influence their performance. In this way, PISA's achievement scores represent a \"yield\" of learning at age 15, rather than a direct measure of attained curriculum knowledge at a particular grade level, since 15-year-olds in the United States and elsewhere come from several grade levels and are enrolled in a variety of classes (figures 2 and 3, tables B-1 and B-2)."}, {"section_title": "How PISA 2003 Was Conducted", "text": "PISA 2003 was sponsored by the OECD and carried out at the international level through a contract with the PISA Consortium, led by the Australian Council for Educational Research (ACER  Tomorrow's World -First Results From PISA 2003, available at http://www.pisa.oecd.org (OECD 2004). A technical report for PISA 2003-which describes in detail all the procedures used in the design, data collection, quality control, and analysis for the study, as well as the PISA 2003 data itself-is also available at that site. This report provides results for the United States in relation to the other countries participating in PISA 2003, distinguishing OECD countries and non-OECD countries. All differences described in this report have been tested for statistical significance at the .05 level. Additional information on statistical procedures used in this report is provided in the technical notes.  "}, {"section_title": "U.S. Performance in Mathematics Literacy", "text": "PISA's major focus in 2003 was mathematics literacy. Mathematics literacy is defined as: ...an individual's capacity to identify and understand the role that mathematics plays in the world, to make well-founded judgements and to use and engage with mathematics in ways that meet the needs of that individual's life as a constructive, concerned, and reflective citizen. (OECD 2003, p.24) PISA's emphasis is on the ability to apply a range of knowledge and skills to a variety of problems with real-life contexts. In the PISA 2003 mathematics literacy assessment, students completed exercises designed to assess their capabilities in using a range of mathematical competencies, grouped and described as \"competency clusters.\" These clusters-reproduction, connections, and reflection-describe sets of skills students may use to solve problems. The reproduction cluster involves the reproduction of the practiced material and performing routine operations. The connections cluster calls for integration and connection of material, and the modest extension of practiced material. The reflection cluster relates to students' abilities in advanced reasoning, argumentation, abstraction, generalization, and modeling applied to new contexts. The problems themselves were designed to come from the variety of situations (personal, educational/occupational, public, or scientific) that students encounter, and to have a real-life context. The mathematical content of the problems was drawn from four overarching ideas: space and shape, change and relationships, quantity, and uncertainty."}, {"section_title": "5", "text": "PISA 2003 Results From the U.S. Perspective These overarching ideas represent a way to organize mathematical content broadly and encompass many traditional curricular areas such as algebra or geometry (see also Steen 1990). \u2022 Space and shape includes recognizing shapes and patterns, describing, encoding, and decoding visual information, understanding dynamic changes to shapes, understanding similarities and differences and relative positions, and understanding the relationship between visual representations and real shapes and images. \u2022 Change and relationships covers the representation of change, including mathematical functions such as linear, exponential, or logistic, as well as data analysis needed to specify relationships or translate between representations. \u2022 Quantity focuses on quantitative reasoning (including number sense, estimating, mental arithmetic, understanding meaning of operations, having a feel for the magnitude of numbers, and computations) and understanding of numerical patterns, counts, and measures. \u2022 Uncertainty includes the two related topics of data and chance, or statistics and probability, including data analysis and graphic and numeric representations of data. A comparative analysis of the NAEP, PISA, and TIMSS mathematics assessments sponsored by NCES found that the 2003 PISA mathematics literacy assessment used far fewer multiple-choice items than NAEP or TIMSS. PISA also had a much stronger content focus on the \"data\" area (which often deals with using charts and graphs), which fits with PISA's emphasis on using materials with a real-world context (see technical notes for more information on the results of the assessment comparisons). 4 Sample mathematics literacy items for each of these areas and student responses are shown here. For more information about the mathematics literacy domain, refer to The PISA 2003 Assessment Framework: Mathematics, Reading, Science, andProblem Solving Knowledge andSkills (OECD 2003 Combined mathematics literacy scores are reported on a scale with a mean of 500 and standard deviation of 100. 5 Fifteen-year-old students in the United States had an average score of 483 on the combined mathematics literacy scale, lower than the OECD average score of 500 (tables 2 and B-3). U.S. students were less mathematically literate than their peers in 20 of the other 28 OECD countries and 3 of the 10 non-OECD countries. Eleven countries (5 OECD countries and 6 non-OECD countries) reported lower scores compared to the United States in mathematics literacy. U.S. students also had lower scores than the OECD average scores for each of the four content area subscales (space and shape, change and relationships, quantity, and uncertainty). Twenty-four countries (20 OECD and 4 non-OECD countries) outperformed the United States on the space and shape subscale, 21 countries (18 OECD and 3 non-OECD countries) outperformed the United States on the change and relationships subscale, 26 countries (23 OECD and 3 non-OECD countries) outscored the United States on the quantity subscale, and 19 countries (16 OECD and 3 non-OECD countries) outscored the United States on the uncertainty subscale.\nBecause the average was set for the combined mathematics literacy scale, average scores for the mathematics literacy subscales differ slightly from 500. PISA 2000 mathematics literacy scores were re-scaled using the greater detail in PISA 2003 data in order to provide a more complete measure of achievement than that available in 2000. See technical notes in appendix A for more information on scaling. PISA's intent for each subject area is to draw baseline information for describing changes and trends in achievement from the cycle in which that subject area is the major domain. The use of minor domains allows PISA to provide indicative information about changes in performance over time; however, changes in a subject area are best measured from the cycle in which it is the major domain. Thus, changes in reading literacy achievement are based upon PISA 2000 data, when reading literacy was the major domain, and changes in mathematics literacy scores, in turn, are based upon this 2003 cycle. Science literacy scores from 2000 and 2003 may be re-scaled based up on the much greater detail for science literacy which will be available in 2006.  Tunisia 363 Average is significantly higher than the U.S. average Average is not significantly different than the U.S. average Average is significantly lower than the U.S. average NOTE: Statistical comparisons between the U.S. average and the Organization for Economic Cooperation and Development (OECD) average take into account the contribution of the U.S. average toward the OECD average. The OECD average is the average of the national averages of the OECD member countries with data available. Because the Program for International Student Assessment (PISA) is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average. Due to low response rates, data for the United Kingdom are not discussed in this report. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2003."}, {"section_title": "13", "text": "PISA 2003 Results From the U.S. Perspective"}, {"section_title": "16", "text": "Along with scale scores, PISA 2003 also uses six proficiency levels (levels 1 through 6, with level 6 being the highest level of proficiency) to describe student performance in mathematics literacy (exhibit 5). An additional level (below level 1) encompasses students whose skills cannot be described using these proficiency levels. The proficiency levels describe what students at each level can do and allow comparisons of the percentages of students in each country who perform at different levels of mathematics literacy (see technical notes in appendix A for more information about how levels were set). The U.S. average score of 483 on the combined mathematics literacy scale was just above the bottom cut point for level 3; the OECD average score of 500 was near the midpoint of level 3 (table 2, exhibit 5). The cutoff score of 607 for U.S. high performers (those in the top 10 percent in the United States) placed it just into level 5; the OECD score for high performers was near the midpoint of level 5. The cutoff U.S. score of 356 for low performers (those in the bottom 10 percent) was below level 1, while the OECD cutoff score of 369 for the bottom 10 percent was a level 1 score (figure 4, exhibit 5). On average, the highest U.S. achievers (those in the top 10 percent of U.S. students) were outperformed by their OECD counterparts (figure 4, table B-4). To be in the top 10 percent in the United States, students had to score 607 or higher, while on average across the OECD countries, students would have had to score 628 or higher to be in the top 10 percent. Scores for the top 10 percent of students within countries ranged from 466 or better in Indonesia and Tunisia to 672 or better in Hong Kong-China. Low performers in the United States (those in the bottom 10 percent) had a cutoff score of 356 or lower, which was lower than the cutoff score of 369 or lower for the OECD average. There was approximately a 251 point score difference, or about two and a half standard deviations, between the cutoff scores for the top 10 percent and the bottom 10 percent of 15-year-old students for mathematics literacy in the United States, compared to about a 259 point difference using the OECD average scores. The standard deviation (which measures the spread of scores around the average) for the United States (95), in fact, was lower than the OECD average standard deviation of 100 (table B-5). Sixteen countries (10 OECD and 6 non-OECD countries) showed less variation in performance than the United States, while three countries (Belgium, Germany, and Uruguay) had larger standard deviations.   Task descriptions  Level 1 At Level 1 students can answer questions involving familiar contexts where all relevant information is present and the questions are clearly defined. They are able to identify information and to carry out routine procedures according to direct instructions in explicit situations. They can perform actions that are obvious and follow immediately from the given stimuli."}, {"section_title": "Level 2", "text": "At Level 2 students can interpret and recognize situations in contexts that require no more than direct inference. They can extract relevant information from a single source and make use of a single representational mode. Students at this level can employ basic algorithms, formula, procedures, or conventions. They are capable of direct reasoning and making literal interpretations of the results.\nAt Level 2 students use reasoning and analytic processes and solve problems requiring decision-making skills. Level 2 students apply various types of reasoning (inductive and deductive reasoning, reasoning about causes and effects, or combinatorial reasoning, that is, systematically comparing all possible variations in well-described situations) to analyze situations and to solve problems that require students to make a decision among well-defined alternatives. To analyze a system or make decisions, Level 2 students combine and synthesize information from a variety of sources. Students may need to combine various forms of representations (e.g., a formalized language, numerical information, and graphical information), handle unfamiliar representations (e.g., statements in a proto-programming language or flow diagrams related to a mechanical or structural arrangement of components), or draw inferences based on two or more sources of information."}, {"section_title": "Level 3", "text": "At Level 3 students can execute clearly described procedures, including those that require sequential decisions. They can select and apply simple problem solving strategies. Students at this level can interpret and use representations based on different information sources and reason directly from them. They can develop short communications reporting their interpretations, results, and reasoning.\nAt Level 3 students do not only analyze a system and make decisions, they also represent the underlying relationships in a problem and relate these to the solution. Level 3 students approach problems systematically, construct their own representations and verify that their solution satisfies all requirements of the problem. These students communicate their solutions to others using written statements and other representations. Due to low response rates, data for the Netherlands were not discussed for PISA 2000; data for PISA 2003 for the United Kingdom are also not discussed due to low response rates; data for Brazil were not available at the time of production for this report."}, {"section_title": "Level 4", "text": "At Level 4 students can work effectively with explicit models for complex concrete situations that may involve constraints or call for making assumptions. They can select and integrate different representations, including symbolic, linking them directly to aspects of real-world situations. Students at this level can utilize welldeveloped skills and reason flexibly, with some insight, in these contexts. They can construct and communicate explanations and arguments based on their interpretations, arguments, and actions."}, {"section_title": "Level 5", "text": "At Level 5 students can develop and work with models for complex situations, identifying constraints and specifying assumptions. They can select, compare, and evaluate appropriate problem solving strategies for dealing with complex problems related to these models. Students at this level can work strategically using broad, well-developed thinking and reasoning skills, appropriate linked representations, symbolic and formal characterizations, and insight pertaining to these situations. They can reflect on their actions and formulate and communicate their interpretations and reasoning."}, {"section_title": "Level 6", "text": "At Level 6 students can conceptualize, generalize, and utilize information based on their investigations and modeling of complex problem situations. They can link different information sources and representations and flexibly translate among them. Students at this level are capable of advanced mathematical thinking and reasoning. These students can apply this insight and understandings along with a mastery of symbolic and formal mathematical operations and relationships to develop new approaches and strategies for attacking novel situations. Students at this level can formulate and precisely communicate their actions and reflections regarding their findings, interpretations, arguments, and the appropriateness of these to the original situations. NOTE: In order to reach a particular level, a student must have been able to correctly answer a majority of items at that level. Students were classified into mathematics literacy levels according to their scores. Exact cut point scores are as follows: below level 1 (a score less than or equal to 357.77); level 1 (a score greater than 357.77 and less than or equal to 420.07); level 2 (a score greater than 420.07 and less than or equal to 482.38); level 3 (a score greater than 482.38 and less than or equal to 544.68); level 4 (a score greater than 544.68 and less than or equal to 606.99); level 5 (a score greater than 606.99 and less than or equal to 669.3); level 6 (a score greater than 669.3). States (four of these nine-Greece, Italy, Mexico, and Turkey-were OECD countries). These same nine countries, as well as the Russian Federation and Portugal, had more students at level 1 than the United States. The United States had a lower percentage of students at level 6 than the OECD average for each of the four content area subscales (space and shape, change and relationships, quantity, and uncertainty) and a smaller percentage than the OECD average at level 4 and level 5 on three of the four subscales (exceptions were for uncertainty at level 5 and change and relationships at level 4) (tables B-7 through B-10). The United States also had a higher percentage of students at level 1 than the OECD average on each of the four subscales and more at level 2 for all subscales except uncertainty. On the quantity and uncertainty subscales, the United States also had greater percentages of students than the OECD average percentages below level 1. The United States had greater percentages of students below level 1 and at levels 1 and 2 than the OECD average percentages (figure 5, table B-6). The United States also had a lower percentage of students at levels 4, 5, and 6, than the OECD average percentages. This is somewhat different from the 2000 results, when reading literacy was the major domain. PISA 2000 results showed that while the percentages of U.S. students performing at level 1 and below were not measurably different from the OECD averages, the United States had a greater percentage of students performing at the highest level (level 5) compared to the OECD average (Lemke et al. 2001). In For more information on scaling, see the technical notes in appendix A. Five countries had their scores improve on the space and shape subscale. Four of the five countries with improved scores on the space and shape subscale also showed improvements on the change and relationships scale (Belgium, Czech Republic, Luxembourg, and Poland; Italy improved its score on the space and shape scale but not on the change and relationships scale)."}, {"section_title": "U.S. Performance in Problem Solving", "text": "As noted, one of PISA's major goals is to assess skills that cut across traditional curricular areas. In 2003, PISA assessed students' abilities in problem solving. 7 Problem solving is defined as:  ence, or reading. (OECD 2003, p. 156). Students completed exercises that assessed their capabilities in using reasoning processes not only to draw conclusions but to make decisions, to troubleshoot (i.e., to understand the reasons for malfunctioning of a system or device), or to analyze the procedures and structures of a complex system (such as a simple kind of programming language). Problem-solving items required students to apply various reasoning processes, such as inductive and deductive reasoning, reasoning about cause and effects, or combinatorial reasoning (i.e., systematically comparing all the possible variations which can occur in a well-described situation). Students were also assessed in their skills in working toward a solution and communicating the solution to others through appropriate representations. Sample problemsolving items and student responses are shown here. On average, U.S. high achievers for problem solving (those scoring in the top 10 percent in the United States) were outperformed by their OECD counterparts (figure 7, table B-13). To be in the top 10 percent of students in the United States, students needed at least a score of 604, while they needed a score of 446 or better in Tunisia but 675 or better in Japan. Low performers in the United States (those in the bottom 10 percent) scored 347 or lower, which was lower than the cutoff score of 368 or lower for the OECD average. There was approximately a 256 point score difference, or two and a half standard deviations, between the cutoff scores for the top 10 percent (604) and the bottom 10 percent (347) of 15-year-old students for problem solving in the United States.  Along with scale scores, PISA 2003 also uses three proficiency levels (levels 1 through 3, with level 3 being the highest level of proficiency) to describe student performance in problem solving. An additional level (below level 1) encompasses students whose skills cannot be described using these proficiency levels (exhibit 9). The proficiency levels describe what students at each level can do and allow comparisons of the percentages of students in each country who performed at different levels in problem solving (see appendix A for more information about how levels were set). Of the 38 other participating countries, 22 countries (including 16 OECD countries) had less variation (as measured by standard deviation) in performance in problem solving than the United States, while 3 countries (Belgium, Japan, and Uruguay) showed greater variation in performance (table B-14). The U.S. variation in performance was not measurably different from the OECD average variation."}, {"section_title": "Exhibit 9. Description of proficiency levels for problem solving: 2003", "text": "Proficiency level Task descriptions  Level 1 At Level 1 students can solve problems where they have to deal with a single data source containing discrete, well-defined information. They understand the nature of a problem and consistently locate and retrieve information related to the major features of the problem. Level 1 students may be able to transform the information in the problem to present the problem differently (e.g., take information from a table to create a drawing or graph). Also, students may be able to apply information to check a limited number of well-defined conditions within the problem. However, Level 1 students are generally incapable of dealing with multi-faceted problems involving more than one data source or requiring the student to reason with the information provided."}, {"section_title": "9", "text": "Large standard errors for the United States in 2000 may account at least in part for the fact that U.S. reading literacy and science literacy scores were not measurably different from 2000 to 2003 and that the scores were not different from the OECD averages in 2000.   "}, {"section_title": "Socioeconomic Status", "text": "The measure of student socioeconomic status (SES) used in PISA 2003 is based on the occupational status of the student's father or mother (whichever was higher) as reported by the student. Parental occupation was coded based on the International Standard Classification of Occupations (ISCO) (International Labor Organization 1990). Occupational codes were in turn mapped onto an internationally comparable index of occupational status, the International Socioeconomic Index (ISEI), developed by Ganzeboom, De Graaf, and Treiman (1992). Using the index, students were assigned numbers ranging from about 16 to 90 based on their parents' occupations, so that they were arrayed on a continuum from low to high socioeconomic status, rather than placed into discrete categories. The overall linkage of ISEI to mathematics literacy and problem solving can be examined by the specific change in score on the combined mathematics literacy scale in response to a one standard deviation change in the ISEI index score for each country. A greater increase in the average achievement score in a country implies a stronger relationship between socioeconomic status and performance in that country. For example, in the United States, a one standard deviation change in the ISEI index was associated with an average difference of 30 points on the combined mathematics literacy and 31 points on the problem-solving scale (table B-25). In Macao-China, socioeconomic background differences in achievement were at a minimum-one standard deviation's difference on the ISEI index was associated with a 10 point difference on the combined mathematics literacy scale and a 12 point difference on the problem-solving scale. By contrast, among students in Hungary, a one standard deviation change in ISEI score was associated with about a 41 point difference in both mathematics literacy and problem-solving achievement scores. Twelve countries (including six OECD countries) had a weaker relationship between ISEI and problem-solving performance than the United States, while three countries (Belgium, Germany, and Hungary) had a stronger one. Belgium, Germany, and Hungary also had stronger relationships between ISEI and mathematical literacy than the United States, as did the Czech Republic and Poland. Eleven countries (including 6 OECD countries) had weaker relationships.\nThe measure of student socioeconomic status used in PISA 2003 is based on the occupational status of the student's father and/or mother (whichever is higher) as reported by the student. Parental occupation was coded to 4 digits based on the International Standard Classification of Occupations (ISCO). Occupational codes were in turn mapped onto an internationally comparable index of occupational status, the International Socioeconomic Index (ISEI), developed by Ganzeboom, De Graaf, and Treiman (1992). Using the index, students were assigned numbers ranging from about 16 to 90 based on their parents' occupations, so that they were arrayed on a continuum from low to high socioeconomic status, rather than placed into discrete categories. The range of ISEI scores given for the 1988 ISCO occupations listed in Ganzeboom and Treiman (1996) goes from 16, the lowest (agricultural laborer), to 90, the highest (judge). Typical occupations among Due to low response rates, data for the United Kingdom are not discussed in this report. NOTE: Cases where more than 15 percent of the student responses are missing are flagged in the supporting statistical data tables in appendix B. For more information about the variables, see the Description of Variables section in appendix B. The overall percentage refers to the sample estimate for the overall 15year-old student population. The International Socioeconomic Index (ISEI) is an internationally comparable index of occupational status, with a range of approximately 16 to 90, developed by Ganzeboom, De Graaf, and Treiman (1992) PISA schools and adding an additional measure of uncertainty of school and student identification through random swapping of data elements within the student and school files."}, {"section_title": "Sampling, Data Collection, and Response Rate Requirements", "text": "To provide valid estimates of student achievement and characteristics, the sample of PISA students had to be selected in a way that represented the full population of 15-year-old students in each country. The international desired population in each country consisted of 15-year-olds attending both publicly and privately controlled educational institutions in grades 7 and higher. A minimum of 4,500 students from a minimum of 150 schools was required. Within schools, a sample of 35 students was to be selected in an equal probability sample unless fewer than 35 students aged 15 were available (in which case all students were selected). International standards required that students be sampled based on an age definition of 15 years and 3 months to 16 years and 2 months at the beginning of the testing period. The testing period was required not to exceed 42 days between March 1, 2003, andAugust 31, 2003. Each country collected its own data, following international guidelines and specifications. A minimum response rate target of 85 percent was required for initially selected educational institutions. In instances in which the initial response rate of educational institutions was between 65 and 85 percent, an acceptable school response rate could still be achieved through the use of replacement schools. Replacement schools were to be selected at the time of sample selection. Three school response rate zones-acceptable, intermediate, and not acceptablewere defined (figure A-1). \"Acceptable\" meant that the country's data would be included in all international comparisons. \"Not Acceptable\" meant that the country's data would be a candidate for not being reported in international comparisons unless considerable evidence was presented that nonresponse bias was minor. \"Intermediate\" meant that a decision on whether or not to include the country's data in comparisons would be made while taking into account a variety of factors, such as student response rates, quality control, closeness of the response rates to the acceptable level, etc. For the purposes of calculating response rates, schools with less than 50 percent of students responding were considered nonresponding and their students were excluded from the student response rates. If the student response rates within such schools were at least 25 percent, these schools and students were included in the PISA 2003 database. Schools with student response rates below 25 percent were not used in any type of analysis nor are the data for these students or schools available in the PISA 2003 database. Note that schools with student response rates above 25 percent were included in the nonresponse bias analyses described in this report. six countries for schools that would otherwise have been excluded. Special booklets were used in Austria, Belgium, the Czech Republic, Hungary, the Netherlands, and the Slovak Republic. Within schools, exclusion decisions were made by staff members who were knowledgeable about students with Individualized Education Programs (IEPs) or students who were limited English proficient, using the following international guidelines on possible student exclusions: \u2022 Functionally disabled students. These were students who were permanently physically disabled in such a way that they could not perform in the testing situation. Functionally disabled students who could respond were to be included in the testing. Any sampled PISA 2003 also required a minimum participation rate of 80 percent of sampled students from original and replacement schools within each country. A student was considered to be a participant if he or she participated in the first testing session or a followup or makeup testing session. Exclusion guidelines allowed for 0.5 percent at the school level for approved reasons (for example, remote regions or very small schools), and 2 percent for special education schools. Overall estimated student exclusions to be under 5 percent. PISA's intent was to be as inclusive as possible. No accommodations were offered in the United States for PISA. A special one-hour booklet with lower difficulty items, which was scaled with the regular PISA booklets, was used in  student who was temporarily disabled such that s/he could not participate in the assessment was considered absent from the assessment. \u2022 Students with mental or emotional disabilities. These were students who were considered in the professional opinion of the school principal or by other qualified staff members to be intellectually disabled or who had been psychologically tested as such. This included students who were emotionally or mentally unable to follow even the general instructions of the test. Students were not to be excluded solely because of poor academic performance or normal disciplinary problems. \u2022 Students with limited proficiency in the test language. These were students who had received less than one year of instruction in the language of the test. Generally, these were students who were unable to read or speak the language of the test (English in the United States) and would be unable to overcome the language barrier in the test situation. Quality monitors from the PISA Consortium visited schools in every country to ensure testing procedures were carried out in a consistent manner across countries."}, {"section_title": "Sampling, Data Collection, and Response Rates in the United States", "text": "The 2003 PISA school sample was drawn for the United States in November 2002. The sample design for this school sample was developed to retain some of the properties of the 2000 PISA U.S. school sample, and to follow international requirements as given in the PISA sampling manual. Unlike the 2000 PISA sample, which had a three-stage design, the U.S. sample for 2003 was a twostage sampling process with the first stage a sample of schools, and the second stage a sample of students within schools. For PISA in 2000, the U.S. school sample had the selection of a sample of geographic Primary Sampling Units (PSUs) as the first stage of selection. The sample was not clustered at the geographic level for PISA 2003. This change was made in an effort to reduce the design effects observed in the 2000 data and to spread the respondent burden across school districts as much as possible. The sample design for PISA was a stratified systematic sample, with sampling probabilities proportional to measures of size. The PISA sample had no explicit stratification and no oversampling of subgroups. The frame was implicitly stratified (i.e., sorted for sampling) by five categorical stratification variables: grade span of the school (five levels), type of school (public or private), region of the country 11 (Northeast, Central, West, Southeast), type of location relative to populous areas (eight levels), minority status (above or below 15 percent). The last sort key within the implicit stratification was by estimated enrollment of 15-year-olds based on grade enrollments. At the same time that the PISA sample was selected, replacement schools were identified following the PISA guidelines by assigning the two schools neighboring the sampled school on the frame as replacements. There were several constraints on the assignment of substitutes. One sampled school was not allowed to substitute for another, and a given school could not be assigned to substitute for more than one sampled school. Furthermore, substitutes were required to be in the same implicit stratum as the sampled school. If the sampled school was the first or last school in the stratum, then the second school following or preceding the sampled school was identified as the substitute. One was designated a first replacement and the other a second replace-48 data collection, the school sample included only original schools from the sample that had refused to participate in the spring but indicated a willingness to participate in a fall assessment. Substitute schools were not included in the fall sample because their participation would have had little effect on raising the final response rate. In order to achieve a comparable sample of students in spring and fall, the age definition for students tested in the fall was adjusted such that all students tested were the same age. where Y denotes the set of responding original sample schools with age-eligible students, N denotes the set of eligible nonresponding original sample schools, W i denotes the base weight for school i, W i = 1/P i , where P i denotes the school selection probability for school i, and E i denotes the enrollment size of age-eligible students, as indicated on the sampling frame. In addition to the 249 participating original schools, 13 replacement schools also participated in the spring for a total of 262 participating schools. weighted school response rate before replacement ment. If an original school refused to participate, the first replacement was then contacted. If that school also refused to participate, the second school was then contacted. The U.S. PISA school sample consisted of 420 schools. This number was increased from the international minimum requirement of 150 to offset school nonresponse, reduce design effects, and include additional students in a metric-imperial experiment (described below). The schools were selected with probability proportionate to the school's estimated enrollment of 15-year-olds from the 2003 NAEP school frame with 2000-2001 school data. The data for public schools were from the Common Core of Data (CCD), and the data for private schools were from the Private School Survey (PSS). Any school containing at least one 7th-through 12thgrade class as of the school year 2000-01 was included on the school sampling frame. Participating schools provided lists of 15year-old students, and a sample of 35 students was selected within each school in an equal probability sample. The overall sample design for the United States was intended to approximate a self-weighting sample of students as much as possible, with each 15year-old student having an equal probability of being selected. In the United States, for a variety of reasons reported by school administrators (such as increased testing requirements at the national, state, and local levels, concerns about timing of the PISA assessment and loss of learning time), many schools in the original sample declined to participate. As it was clear that the United States would not meet the minimum response rate standards, in order to improve response rates and better accommodate school schedules, a second testing window was opened from September to November 2003 with the agreement of the PISA Consortium. For the fall A total of 7,598 students were sampled for the assessment. Of these students, 261 were deemed ineligible because of their enrolled grades, birthdays, or other reasons, and were removed from the sample. Of the eligible 7,337 sampled students, an additional 534 students were excluded using the criteria described above, for a weighted exclusion rate of 7 percent. Of the 6,803 remaining sampled students, a total of 5,456 students participated in the assessment in the United States, but 114 of these came from schools which had less than 50 percent student participation. Schools which had less than 50 percent student participation were classified as school nonrespondents, and these students (114 participating students and 187 nonparticipating students) were therefore excluded for the purposes of calculating student response rates. Thus, although data for 5,456 students are included in the database, student response rates were calculated by subtracting the 114 students from the 5,456 for a total of 5,342 participating students. The denominator for the student response rate is 6,502, which consists of 7,598 sampled students minus the following students: 261 ineligible, 534 excluded, 114 responding students from nonresponding schools, and 187 nonresponding students from nonresponding schools. An overall weighted student response of 83 percent was achieved (82 percent unweighted). Two separate bias analyses were conducted in the United States to address potential problems in the data due to school nonresponse and possible achievement differences between students in spring and fall testing windows. The analysis of school nonresponse was conducted in two parts, examining first the original sample of schools (spring and fall participants) and then the final sample of schools (including replacements), treating as nonrespondents those schools from whom a final response was not received (Ferraro, Czuprynski and Williams forthcoming). Schools with 25 to 49 percent student response rates were treated as respondents in the nonresponse bias analysis, since their data are included in the PISA database. Schools with student response rates less than 25 percent were treated as nonrespondents in the analysis and were not included in the PISA database. In order to compare PISA respondents and nonrespondents, it was necessary to match the sample of schools back to the sample frame to detect as many characteristics as possible that might provide information about the presence of nonresponse bias. Comparing frame characteristics for respondents and nonrespondents is not always a good measure of nonresponse bias if the characteristics are unrelated or weakly related to more substantive items in the survey; however, this was the only approach available given that no comparable school or student level achievement data were available. Frame characteristics were taken from the 2000-01 Common Core of Data (CCD) for public schools and from the 2000-01 Private School Survey (PSS) for private schools. For categorical variables, response rates by characteristics were calculated. The hypothesis of independence between the characteristics and response status was tested using a Rao-Scott modified Chi-square statistic. For continuous variables, summary means were calculated. The 95 percent confidence interval for the difference between the mean for respondents and the overall mean was tested to see whether or not it included zero. In addition to these tests, logistic regression models were set up to identify whether any While the implications of these analyses for the direction of any resulting bias achievement are not entirely clear, an attempt was made to minimize any bias by incorporating the variables in question into the adjustment for school nonresponse that was a component of the sampling weights. One other country, the United Kingdom, also fell below the acceptable range for school response rates, although response rate problems were largely limited to England (Scotland and Northern Ireland also participated). In that case, however, the PISA Consortium was unable to make adjustments for any potential bias, and data for the United Kingdom are therefore annotated and are not included in the main text or figures. Data for one additional participating PISA 2003 country, Brazil, were not available in time for production of this report. The other U.S. bias analysis aimed to address the question of whether there was a \"session\" effect between students tested in the spring and fall, in order to provide evidence for the acceptability of combining data from both sessions for the United States. Despite PISA's focus on an age sample, concern remained that students tested at the beginning of the school year might perform worse than their peers tested at the end of the previous school year. The approach taken was to investigate session effects in a multilevel model, since these were school-level effects-all students within a school were in either the spring or the fall sessions. Two similar twolevel models were estimated. In each, student achievement in PISA was modeled as a function of various school characteristics (in particular those on the sample frame known to be related to willingness to participate in the original testing window, including public/private status, number of age-eligible students, region, and location) and time of testing (spring/fall) and, in one model, the of the frame characteristics were significant in predicting response status. All analyses were performed using WesVar and replicate weights to properly account for the complex sample design. The JK2 method was used to create the weights. The school base weights used in these analyses did not include a nonresponse adjustment factor. The base weight for each original school was the reciprocal of its selection probability. The base weight for each replacement school was equal to the base weight of the original school it replaced. Characteristics available for public and private schools included: public/private affiliation, community type, region, number of ageeligible students enrolled, total number of students, and percentage of various racial/ethnic groups (percentage Asian or Pacific Islander; Black, non-Hispanic; Hispanic; American Indian or Alaska Native; White, non-Hispanic). Percentage of students eligible for free or reduced-price lunch was also available for public schools only (however, this variable was missing for 50 of the 359 public schools). For the original sample of schools, two of these variables showed a relationship to response status in tests of independence and in the multivariate logistic regression model: region (specifically, schools in the Central region were less likely to respond) and percentage of Asian or Pacific Islander students (responding schools had fewer of these students than the original sample schools). Using the same analytic procedure for the final sample (including replacement schools), tests of independence again showed that responding schools were more likely to be in the West. Responding schools were also more likely to have fewer Asian or Pacific Islander students and more Black, non-Hispanic students. However, the only variable found to be significant in the logistic regression model predicting response was the percentage of Asian or Pacific Islander students (again, responding schools were likely to have fewer Asian or Pacific Islander students). student characteristic grade level. In the simpler of the student-level models no predictors of achievement were included. In the second model, student grade level was included as a predictor of achievement to allow for the possibility that the school means predicted in the school-level model were affected by differences in the spring/fall distribution of students across grades. That is, the school-level model was predicting mean school achievement adjusted for grade-level differences. The two models proposed were estimated with HLM (Raudenbush and Bryk 2002). Neither model showed evidence of a statistically significant session effect. On this basis, and on the basis of the adjustments made to the sampling weights based on the nonresponse bias analysis, the PISA Consortium concluded that the data for the United States was adequate to generalize to the U.S. 15year-old population and should be included in the international report and database.  Approximately one-third of the mathematics literacy items were multiple choice and complex multiple choice, one-third were closed or short response types in which students wrote an answer which was simply either correct or incorrect, and about one-third were open constructed responses for which students wrote answers which were marked by trained scorers based upon an international scoring guide. In PISA 2003, every student answered mathematics items. Problem solving, science, and reading items were spread throughout other booklets. For more information on assessment design, see the OECD's PISA 2003 Technical Report (Adams forthcoming)."}, {"section_title": "Test Development", "text": "In order to examine similarities and differences between national and international assessments, NCES has sponsored a number of comparative studies of assessment frameworks and items. In October 2003 a study of the NAEP, TIMSS, and PISA 2003 mathematics assessments was undertaken. The aim of the study was to provide information that would be useful in interpreting and comparing the results from the three assessments, based on an in-depth look at the con-  Fifteen-year-olds in primary school in Greece were originally excluded from the assessment. Changes in the target population definition to 15-year-olds in grades 7 and above required Greece to adjust its data to reflect the fact that 15-year-olds in primary school would no longer be considered part of the target population. 2 Indonesia excluded 4 provinces and close to 5 percent of its eligible population for security reasons. There were 4,137,103 15-yearolds in the total population, but the 4 provinces were already excluded. Therefore, the 144,792 noted as being excluded in these provinces was added to this number to get 4,281,895 15-year-olds. The number of enrolled 15-year-olds was noted as 2,968,756 so 144,792 was also added to this. 3 Serbia and Montenegro excluded Kosovo; however, there were no estimates for the number of 15-year-olds, so this does not appear as an exclusion."}, {"section_title": "4", "text": "Tunisia noted late in the process that one French school needed to be excluded because of French (rather than Arabic) language. The school had 33 eligible students. adapt the instrument for cultural purposes, even in nations such as the United States that use English as the primary language of instruction. For example, words such as \"lift\" might be adapted to \"elevator\" for the United States. The PISA Consortium verified the national translation and adaptation of all instrumentation. Copies of printed materials were sent to the PISA Consortium for a final optical check prior to data collection. As noted, in the United States, an additional 4 test booklets were included in PISA 2003 that used adapted versions of 27 mathematics items. These items in their original format used metric units of measurement, such as meters, liters, etc. To investigate the possible effects of the use of metric units on U.S. student performance, the items were adapted to use \"imperial\" forms with familiar units such as feet, gallons, and degrees Fahrenheit. Differential item analysis showed that U.S. students were not disadvantaged by the use of metric units in PISA 2003. The few discrepancies that were observed are possibly due to (1) differences in the nature of the two systems (e.g., decimal vs. duodecimal, or no equivalent wording of the units), and (2) difficulties in the modification process (e.g., no comparable scoring guides for some incorrect approaches to an item). For more information, see Wilson and Xie (2004)."}, {"section_title": "Test Administration and Quality Assurance", "text": "PISA 2003 emphasized the use of standardized procedures in all countries. Each country collected its own data, based on comprehensive manuals and trainings provided by the PISA Consortium to explain the survey's implementation, including precise instructions for the work of school coordinators and scripts for test administrators for use in testing sessions. Test administration in the United States was carried out by professional staff trained according to the interna-tent of the respective frameworks and items. The results showed that PISA used far fewer multiple choice items and had a much stronger content focus on the \"data\" area (which often deals with using charts and graphs), which fits with PISA's emphasis on using materials with a real-world context. For more results from the study, see A Content Comparison of the NAEP, TIMSS, and PISA 2003 Mathematics Assessments (Nohara forthcoming). An earlier study compared NAEP 2000, PISA 2000, and TIMSS 1999 mathematics and science items. That study found that PISA items required multistep reasoning more often than TIMSS or NAEP and that PISA mathematics and science literacy items more often involved the interpretation of charts and graphs or other \"real life\" material (Nohara 2001). In addition to the cognitive assessment, students also received a 30-minute questionnaire designed to provide information about their backgrounds, attitudes, and experiences in school. Principals in schools were PISA was administered also received a 20-30 minute questionnaire about their schools. Results from the school survey are not discussed in this report but are available at http://www.pisa.oecd.org."}, {"section_title": "Translation and the Metric-Imperial Study", "text": "Source versions of all instruments (assessment booklets, questionnaires, and manuals) were prepared in English and French and translated into the primary language or languages of instruction in each nation. PISA recommended that countries prepare and consolidate independent translations from both source versions, and provided precise translation guidelines that included a description of the features each item was measuring and statistical analysis from the field trial. In cases where one source language was used, independent translations were required and discrepancies reconciled. In addition, it was sometimes necessary to tional guidelines. School staff were asked only to assist with listings of students, identifying space for testing in the school, and specifying any parental consent procedures needed for sampled students. Use of calculators was at the discretion of participating countries; in the United States, this choice was left to schools based on school, district, or state policy. Students were asked at the end of their test booklets if they had used a calculator and if so, what type. Approximately 12 percent of U.S. students did not respond. Of the responding students, 91 percent of U.S. students reported using a calculator. Students who reported using a calculator had a mean score of 498 on the combined mathematics literacy scale compared to 461 for those who reported not using a calculator. Members of the PISA Consortium visited all national centers to review data collection procedures, and members of the PISA Consortium also visited a randomly selected subsample of approximately 10 percent of the educational institutions to ensure that procedures were being carried out in accordance with international guidelines. For a detailed description of the quality assurance procedures, see the OECD's PISA 2003 Technical Report (Adams forthcoming)."}, {"section_title": "Scoring", "text": "At least one-third of the PISA assessment was devoted to items requiring constructed responses. The process of scoring these items was an important step in ensuring the quality and comparability of the PISA data. Detailed guidelines were developed for the scoring guides themselves, training materials to recruit scorers, and workshop materials used for the training of national scorers. Prior to the national training, the PISA Consortium organized training sessions to present the material and train the scoring coordinators from the participating countries, who trained the national scorers. For each test item, the scoring guide described the intent of the question and how to code the students' responses to each item. This description included the credit labels-full credit, partial credit, or no credit-attached to the possible categories of response. Also included was a system of double-digit coding for some mathematics and science items where the first digit represented the score, and the second digit represented different strategies or approaches that students used to solve the problem. The second digit generated national profiles of student strategies and misconceptions. In addition, the scoring guides included real examples of students' responses accompanied by a rationale for their classification for purposes of clarity and illustration. To examine the consistency of this marking process in more detail within each country and to estimate the magnitude of the variance components associated with the use of markers, the PISA Consortium conducted an interscorer reliability study on a subsample of assessment booklets. Homogeneity analysis was applied to the national sets of multiple scoring and compared with the results of the field trial. A full description of this process and the results can be found in the PISA 2003 Technical Report published by the OECD (Adams forthcoming)."}, {"section_title": "Data Entry and Cleaning", "text": "Responsibility for data entry was taken by the national project manager from each nation. The data collected for PISA 2003 were entered into data files with a common international format, as specified in the PISA 2003 Data Entry Manual. Data entry was facilitated by the use of a common software available to all participating nations (KeyQuest). The software facilitated the checking and correction of data by providing various data consistency checks. The data were then sent to the Australian Council for 56 weight for each replacement school was equal to the base weight of the original school it replaced."}, {"section_title": "Scaling and Plausible Values", "text": "PISA used Item Response Theory (IRT) methods to produce scale scores that summarized the achievement results. PISA 2003 utilized a mixed coefficients multinomial logit IRT model. This model is similar in principle to the more familiar two-parameter IRT model. With this method, the performance of a sample of students in a subject area or sub area can be summarized on a simple scale or a series of scales, even when different students are administered different items. Because of the reporting requirements for PISA and the large number of background variables associated with the assessment, PISA used these IRT procedures to produce accurate results for groups of students while limiting the testing burden on individual students. Furthermore, these procedures provided data that could be readily used in secondary analyses. IRT scaling provides estimates of item parameters (e.g., difficulty, discrimination) that define the relationship between the item and the underlying variable measured by the test. Parameters of the IRT model are estimated for each test question, with an overall scale being established as well as scales for each predefined content area specified in the assessment framework. For example, PISA 2003 had five scales describing mathematics (a combined score and subscale scores in four domains) and one each for reading, problem solving, and science. The reading literacy and science literacy reporting scales used for PISA 2000 and PISA 2003 are directly comparable. The value of 500, for example, has the same meaning as it did in PISA 2000-that is, the mean score in 2000 of the sampled students in the 27 OECD countries that participated in PISA 2000. Educational Research (ACER) for cleaning. ACER's role in this instance was to check that the international data structure was followed, check the identification system within and between files, correct single case problems manually, and apply standard cleaning procedures to questionnaire files. Results of the data cleaning process were documented and shared with the national project managers and included specific questions when required. The national project manager then provided ACER with revisions to coding or solutions for anomalies. ACER then compiled background univariate statistics and preliminary classical and Rasch Item Analysis. Detailed information on the entire data entry and cleaning process can be found in the forthcoming PISA 2003 technical report."}, {"section_title": "Weighting", "text": "Students included in the final PISA sample for a given country were not all equally representative of the full student population, even though random samplings of schools and students were used to select the sample. The use of sampling weights is necessary for the computation of statistically sound, nationally representative estimates. Survey weights help adjust for intentional over-or under-sampling of certain sectors of the population, school or student nonresponse, or errors in estimating size of a school at the time of sampling. Survey weighting for PISA 2003 was carried out by Westat, as part of the PISA Consortium. The internationally defined weighting specifications for PISA required that each assessed student's sampling weight be the product of the inverse of the school's probability of selection, an adjustment for schoollevel nonresponse, the inverse of the student's probability of selection, and an adjustment for student-level nonresponse. All PISA analyses were conducted using these adjusted sampling weights. The PISA 2000 and PISA 2003 assessments of mathematics, reading and science literacy are linked assessments. That is, the sets of items used to assess each of mathematics, reading and science literacy in PISA 2000 and the sets of items used to assess each of mathematics, reading and science literacy in PISA 2003 include a subset of items common to both sets. For mathematics there were 20 items that were used in both assessments, in reading there were 28 items used in both assessments and for science 25 items were used in both assessments. These common items are referred to as link items. To establish common reporting metrics for PISA 2000 and PISA 2003 the difficulty of link items (items used in 2000 and 2003) was compared. Items were calibrated using 2003 data only, and then 2000 items were re-calibrated using the 2003 parameters. Adjustments were then made to ability estimate to account for booklet effects seen in 2000. The comparison of the item difficulties on the two occasions was used to determine a score transformation that allows the reporting of the data from the two assessments on a common scale. The change in the difficulty of each of the individual link items is used in determining the transformation and as a consequence the sample of link items that has been chosen will influence the choice of transformation. This means that if an alternative set of link items had been chosen the resulting transformation would be slightly different. The consequence is an uncertainty in the transformation due to the sampling of the link items, just as there is an uncertainty in values such as country means due to the use of a sample of students. The section on statistical testing below describes how this uncertainty has been accounted for in making comparisons over time."}, {"section_title": "Plausible Values", "text": "During the scaling phase, plausible values were used to characterize scale scores for students participating in the assessment. To keep student burden to a minimum, PISA administered few assessment items to each student-too few to produce accurate content-related scale scores for each student. To account for this, PISA generated five possible scale scores for each student that represented selections from the distribution of scale scores of students with similar backgrounds who answered the assessment items the same way. The plausible values technology is one way to ensure that the estimates of the average performance of student populations and the estimates of variability in those estimates are more accurate than those determined through traditional procedures, which estimate a single score for each student. During the construction of plausible values, careful quality control steps ensured that the subpopulation estimates based on these plausible values were accurate. If an analysis is to be undertaken with one of these cognitive scales, then (ideally) the analysis should be undertaken five times, once with each of the five relevant plausible value variables. The results of these five analyses are averaged and then significance tests that adjust for variation between the five sets of results are computed. PISA uses the plausible value methodology to represent what the true performance of an individual might have been, had it been observed, using a small number of random draws from an empirically derived distribution of score values based on the student's observed responses to assessment items and on background variables. Each random draw from the distribution is considered a representative value from the distribution of potential scale scores for all students in the sample who have similar characteristics and identical patterns of item responses. The draws from the distribution are different from one another to quantify the degree of precision (the width of the spread) in the underlying distribution of possible scale scores that could have caused the observed performance. The PISA plausible values function like point estimates of scale scores for many purposes, but they are unlike true point estimates in several respects. They differ from one another for any particular student, and the amount of difference quantifies the spread in the underlying distribution of possible scale scores for that student. Because of the plausible values approach, secondary researchers can use the PISA data to carry out a wide range of analyses."}, {"section_title": "Levels", "text": "While the basic form of measurement in PISA describes student literacy in each country in terms of a range of scale scores, PISA also treats proficiency in mathematics literacy in terms of six described levels, and proficiency in problem solving in three described levels. In both cases, increasing levels represent tasks of increasing complexity. As a result, the findings are reported in terms of percentages of the population proficient at handling tasks of different levels of difficulty. Each of the five mathematics literacy scales-the combined score and the four subscale scores-is divided into six levels based on the type of knowledge and skills students need to demonstrate at a particular level. A seventh level (below level 1) is made up of students whose abilities could not be accurately described based on their responses. Exact cut point scores are as follows: below level 1 (a score less than or equal to 357.77); level 1 (a score greater than 357.77 and less than or equal to 420.07); level 2 (a score greater than 420.07 and less than or equal to 482.38); level 3 (a score greater than 482.38 and less than or equal to 544.68); level 4 (a score greater than 544.68 and less than or equal to 606.99); level 5 (a score greater than 606.99 and less than or equal to 669.30); level 6 (a score greater than 669.30. The tasks that represent each level of performance for the specific mathematics processes on the combined mathematics literacy scale are described in exhibit 5. Exhibit A-1 describes the kind of tasks that represent each level of performance on the mathematics subscales. The problem-solving scale is divided into three levels based on the type of knowledge and skills students must demonstrate at a particular level. A fourth level (below level 1) is made up of students whose abilities could not be accurately described based on their responses. In order to reach a particular "}, {"section_title": "Nonsampling Errors", "text": "Nonsampling error is a term used to describe variations in the estimates that may be caused by population coverage limitations, nonresponse bias, and measurement error, as well as data collection, processing, and reporting procedures. For example, the sampling frame was limited to regular public and private schools in the 50 states and the District of Columbia. The sources of nonsampling errors are typically problems like unit and item nonresponse, the differences in respondents' interpretations of the meaning of the questions, response differences related to the particular time the survey was conducted, and mistakes in data preparation. Some of these issues (particularly unit nonresponse) are discussed above in the section on U.S. sampling and data collection."}, {"section_title": "Missing Data", "text": "There are four kinds of missing data. \"Nonresponse\" data occurs when a respondent was expected to answer an item but no response is given. Responses that are \"missing or invalid\" occur in multiple-choice items where an invalid response is given. The code is not used for open-ended questions. An item is \"not applicable\" when it is not possible for the respondent to answer the question. Finally, items that are \"not reached\" are consecutive missing values starting from the end of each test session. All four kinds of missing data are coded differently in the PISA 2003 database. Missing background data are not included in the analyses for this report and are not imputed. In general, item response rates for variables discussed in this report were over the NCES standard of 85 percent to report without notation (table A-2). The one case in which more than 15 percent of the student responses were missing (for New Zealand for student report of parent occupation, with an item response rate of 84 percent) is flagged in the supporting statistical data tables in appendix B. proficiency level, a student must have been able to correctly answer a majority of items at that level. Students were classified into problem-solving levels according to their scores. Exact cut point scores are as follows: below level 1 (a score less than or equal to 404.06); level 1 (a score greater than 404.06 and less than or equal to 498.08); level 2 (a score greater than 498.08 and less than or equal to 592.10); level 3 (a score greater than 592.10). All students within a level are expected to answer at least half of the items from that level correctly. Students at the bottom of a level have a 62 percent chance of success on the easiest items from that level and a 42 percent chance of success on the hardest items from that level (overall response probability was 62). Students at the top of a level are able to provide the correct answers to about 70 percent of all items from that level, have a 62 percent chance of success on the hardest items from that level, and have a 78 percent chance of success on the easiest items from that level. Students just below the top of a level would score less than 50 percent on an assessment of the next higher level. Students at a particular level not only demonstrate the knowledge and skills associated with that level but also the proficiencies defined by lower levels. Thus, all students proficient at level 3 are also proficient at levels 1 and 2. Patterns of responses for students below level 1 suggest they are unable to answer at least half of the items in level 1 correctly."}, {"section_title": "Data Limitations", "text": "As with any study, there are limitations to PISA 2003 that researchers should take into consideration. Estimates produced using data from PISA 2003 are subject to two types of error, nonsampling and sampling errors. Nonsampling errors can be due to errors made in the collection and processing of data. Sampling errors can occur because the data were collected from a sample rather than a complete census of the population. In general, it is difficult to identify and estimate either the amount of nonsampling error or the bias caused by this error. In PISA 2003, efforts were made to prevent such errors from occurring and to compensate for them when possible. For example, the design phase entailed a field test that evaluated items as well as the implementation procedures for the survey. It should also be recognized that most background information was obtained from students' self-reports, which are subject to respondent bias. One potential source of respondent bias in this survey was social desirability bias, for example, if students reported that they were good at mathematics."}, {"section_title": "Sampling Errors", "text": "Sampling errors occur when the discrepancy between a population characteristic and the sample estimate arises because not all members of the reference population are sampled for the survey. The size of the sample relative to the population and the variability of the population characteristics both influence the magnitude of sampling error. The particular sample of 15-year-old students from the 2002-03 school year was just one of many possible samples that could have been selected. Therefore, estimates produced from the PISA 2003 sample may differ from estimates that would have been produced had another sample of 15-year-old students been drawn. This type of variability was called sampling error because it arises from using a sample of 15-year-old students in 2002, rather than all 15-year-old students in that year. The standard error is a measure of the variability due to sampling when estimating a statistic. The approach used for calculating sampling variances in PISA was the Balanced Repeated Replication (BRR), or Balanced Half-Samples (Fay's method). Standard errors can be used as a measure for the precision expected from a particular sample. Standard errors for all of the estimates are included in appendix B to this report. These standard errors can be used to produce confidence intervals. There is a 95 percent chance that the true average lies within the range of 1.96 times the standard errors above or below the estimated score. For example, it was estimated that 15.5 percent of U.S. students scored at level 1 on the combined mathematics literacy scale, and this statistic had a standard error of 0.81. Therefore, it can be stated with 95 percent confidence that the actual percentage of U.S. students at level 1 for the total population in 2003 was between 13.9 and 17.1 percent (1.96 x 0.81 = 1.59; confidence interval = 15.5 +/-1.59)."}, {"section_title": "Descriptions of Background Variables", "text": "Full PISA 2003 student and school questionnaires are available at http://nces.ed.gov/surveys/pisa or http://www.pisa.oecd.org."}, {"section_title": "Statistical Procedures", "text": ""}, {"section_title": "Tests of Significance", "text": "Comparisons made in the text of this report have been tested for statistical significance. For example, in the commonly made comparison of country averages against the average of the United States, tests of statistical significance were used to establish whether or not the observed differences from the U.S. average were statistically significant. The estimation of the standard errors that are required in order to undertake the tests of significance is complicated by the complex sample and assessment designs which both generate error variance. Together they mandate a set of statistically complex procedures in order to estimate the correct standard errors. As a consequence, the estimated standard errors contain a sampling variance component estimated by Balanced Repeated Replication (BRR)-the Fay method of BRR; and, where the assessments are concerned, there is an additional imputation variance component arising from the assessment design. Details on the BRR procedures used can be found in the WesVar 4.0 User's Guide (Westat 2000). In almost all instances, the tests for significance used were standard t tests. These fell into two categories according to the nature of the comparison being made: comparisons of independent and non-independent samples. In PISA, country samples are independent. To determine whether the average scores for two countries are different we test the null hypothesis: parents of 15-year-olds with between 16 and 35 points on the ISEI scale include smallscale farmer, metalworker, mechanic, taxi or truck driver, and waiter/waitress. Between 35 and 53 index points, the most common occupations are bookkeeping, sales, small business management, and nursing. As the required skills increase, so does the status of the occupation. Between 54 and 70 points, typical occupations are marketing management, teaching, civil engineering, and accountant. Finally, between 71 and 90 points, the top international quarter of the index, occupations include medicine, university teaching, and law (OECD 2001)."}, {"section_title": "Confidentiality and Disclosure Limitations", "text": "The PISA 2003 data are hierarchical and include school data and student data from the participating schools. Confidentiality analyses for the United States were designed to provide reasonable assurance that public use data files issued by the PISA Consortium would not allow identification of individual U.S. schools or students when compared against public data collections."}, {"section_title": "Disclosure limitation included the identification and masking of potential disclosure-risk", "text": "To test this hypothesis, the two observed values and their respective standard errors are needed to perform a t test. The standard error on the estimate for some statistic is: Thus, in simple comparisons of independent averages, such as the average score of country 1 with that of country 2, the following formula was used to compute the t-statistic: where est 1 and est 2 are the estimates being compared (e.g., averages of country 1 and country 2) and se 1 and se 2 are the corresponding standard errors of these averages. This test may also be used for comparisons within a particular country if the categorical variable used to define the groups being compared was used as an explicit stratification variable; however, there was no explicit stratification used in the United States sample. The second type of comparison used in this report occurred when comparing differences of non-subset, non-independent groups. When this occurs, the correlation and related covariance between the groups must be taken into account, such as when comparing a country mean with the OECD mean which includes that particular country, or when comparing the average scores of males versus females within the United States. How are scores like those for \u03bc (boys) and \u03bc (girls) correlated? Suppose that in the school sample, a coeducational school attended by low achievers is replaced by a coeducational school attended by high achievers. The country mean will increase slightly, as well as the males' and the females' means. If such a school replacement process is continued, \u03bc (boys) and \u03bc (girls) will likely increase in a similar pattern. Indeed, a coeducational school attended by high achieving males is usually also attended by high achieving females. Therefore, the covariance between \u03bc (boys) and \u03bc (girls) will be positive. What does the covariance between the two variables, i.e., \u03bc (boys), \u03bc (girls) , tell us? A positive covariance means that if \u03bc (boys) increases then \u03bc (girls) will also increase. A covariance equal or close to 0 means that \u03bc (boys) can increase or decrease with \u03bc (girls) remaining unchanged. Finally, a negative covariance means that if \u03bc (boys) increases, then \u03bc (girls) will decrease, and inversely. Next, to determine whether the females' performance differs from the males' performance, for example, as for all statistical analyses, a null hypothesis has to be tested. In this particular example, it will consist of computing the difference between the males' performance mean and the females' performance mean (or the inverse). The null hypothesis will be: The variance of the observed difference is needed to test this null hypothesis. The variance of a difference is equal to the sum of the variances of the two initial variables minus two times the covariance between the two initial variables. A sampling distribution has the same characteristics as any distribution, except that units consist of sample estimates and not observations. Therefore, the sampling variance of a difference is equal to the sum of the two initial sampling variances minus two times the covariance between the two sampling distributions on the estimates."}, {"section_title": "2", "text": "(\u03bc x -\u03bc y ) = 2 (\u03bc X ) + 2 The estimation of the covariance between, for instance, \u03bc (boys) and \u03bc (girls) requires the selection of several samples and then the In PISA, in each of the three subject matter areas, a common transformation was estimated from the link items, and this transformation was applied to all participating countries. It follows that any uncertainty that was introduced through the linking is common to all students and all countries. Thus, for example, suppose the unknown linking error (between PISA 2000 andPISA 2003) in reading literacy resulted in an over-estimation of student scores by two points on the PISA 2000 scale. It follows that every student's score will be over-estimated by two score points. This over-estimation will have effects on certain, but not all, summary statistics computed from the PISA 2003 data. For example, consider the following: \u2022 each country's mean will be over-estimated by an amount equal to the link error, (in our example this is two score points); \u2022 the mean performance of any subgroup will be over-estimated by an amount equal to the link error (in our example this is two score points); \u2022 the standard deviation of student scores will not be affected because the overestimation of each student by a common error does not change the standard deviation; \u2022 the difference between the mean scores of two countries in PISA 2003 will not be influenced because the over-estimation of each student by a common error will have distorted each country's mean by the same amount; \u2022 the difference between the mean scores of two groups (e.g., males and females) in PISA 2003 will not be influenced, because the over-estimation of each student by a common error will have distorted each group's mean by the same amount; analysis of the variation of \u03bc (boys) in conjunction with \u03bc (girls). Such a procedure is, of course, unrealistic. Therefore, as for any computation of a standard error in PISA, replication methods using the supplied replicate weights are used to estimate the standard error on a difference. Use of the replicate weights implicitly incorporates the covariance between the two estimates into the estimate of the standard error on the difference. To test such comparisons, the following formula was used to compute the t statistic: t = est grp1 -est grp2 /se(est grp1 -est grp2 ) Est grp1 and est grp2 are the non-independent groups estimates being compared; se (est grp1 -est grp2 ) is the standard error of the difference calculated using Balanced Repeated Replication (BRR) to account for any covariance between the estimates for the two non-independent groups. A third type of comparison (addition of a standard error term to the standard t test shown above for simple comparisons of independent averages) was also used when analyzing change in performance over time. The uncertainty that results from the link item sampling (described in the scaling section above) is referred to as linking error and this error must be taken into account when making certain comparisons between PISA 2000 and PISA 2003 results. Just as with the error that is introduced through the process of sampling students, the exact magnitude of this linking error cannot be determined. We can, however, estimate the likely range of magnitudes for this error and take this error into account when interpreting PISA results. As with sampling errors, the likely range of magnitude for the errors is represented as a standard error. The standard error of linking for reading is 3.74, the standard error of linking for science is 3.02, and the standard error for mathematics (space and shape scale) is 6.01 and mathematics (change and relationships scale) is 4.84. \u2022 the difference between the performance of a group of students (e.g., a country) between PISA 2000 and PISA 2003 will be influenced because each student's score in PISA 2003 will be influenced by the error; and \u2022 a change in the difference in performance between two groups from PISA 2000 to PISA 2003 will not be influenced. This is because neither of the components of this comparison, which are differences in scores in 2000 and 2003 respectively, is influenced by a common error that is added to all student scores in PISA 2003. In general terms, the linking error need only be considered when comparisons are being made between PISA 2000 and PISA 2003 results, and then usually only when group means are being compared. Because the linking error need only be used in a limited range of situations we have chosen not to report the linking error in the tables included in this report. The general formula is given by: The most obvious example of a situation where there is a need to use linking error is in the comparison of the mean performance for a country between PISA 2000 and PISA 2003. For example, let us consider a comparison between 2000 and 2003 of the performance of Italy in reading. The mean performance of Italy in 2000 was 487 with a standard error of 2.9, while in 2003 the mean was 476 with a standard error of 3.0. The standardized difference in the Italian mean is 1.97, which is computed as follows: and is statistically significant. + 1.97 = (487 -476) 2.9 2 + 3.0 2 3.7 2 In the U.S. report on PISA 2000, a Bonferroni adjustment was used in all multiple comparisons of countries. This was not the case in 2003, which may result in some differences in how 2000 results are reported in 2003. This may also result in some differences between the PISA 2003 U.S. and OECD reports (which uses a Bonferroni adjustment for multiple comparisons of country averages). The discontinuation of the use of the Bonferroni adjustment for multiple comparisons was made in order to avoid the possibility that comparisons of achievement between countries could be interpreted differently depending on the numbers of countries compared.         Due to low response rates, data for the United Kingdom are not discussed in this report. NOTE: In order to reach a particular proficiency level, a student must have been able to correctly answer a majority of items at that level. Students were classified into mathematics literacy levels according to their scores. Exact cut point scores are as follows: below level 1 (a score less than or equal to 357.77); level 1 (a score greater than 357.77 and less than or equal to 420.07); level 2 (a score greater than 420.07 and less than or equal to 482.38); level 3 (a score greater than 482.38 and less than or equal to 544.68); level 4 (a score greater than 544.68 and less than or equal to 606.99); level 5 (a score greater than 606.99 and less than or equal to 669.3); level 6 (a score greater than 669.3). The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average. s.e. means standard error. Detail may not sum to totals because of rounding. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2003.  Due to low response rates, data for the United Kingdom are not discussed in this report. NOTE: In order to reach a particular proficiency level, a student must have been able to correctly answer a majority of items at that level. Students were classified into mathematics literacy levels according to their scores. Exact cut point scores are as follows: below level 1 (a score less than or equal to 357.77); level 1 (a score greater than 357.77 and less than or equal to 420.07); level 2 (a score greater than 420.07 and less than or equal to 482.38); level 3 (a score greater than 482.38 and less than or equal to 544.68); level 4 (a score greater than 544.68 and less than or equal to 606.99); level 5 (a score greater than 606.99 and less than or equal to 669.3); level 6 (a score greater than 669.3). The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average. s.e. means standard error. Detail may not sum to totals because of rounding   Due to low response rates, data for the United Kingdom are not discussed in this report. NOTE: In order to reach a particular proficiency level, a student must have been able to correctly answer a majority of items at that level. Students were classified into mathematics literacy levels according to their scores. Exact cut point scores are as follows: below level 1 (a score less than or equal to 357.77); level 1 (a score greater than 357.77 and less than or equal to 420.07); level 2 (a score greater than 420.07 and less than or equal to 482.38); level 3 (a score greater than 482.38 and less than or equal to 544.68); level 4 (a score greater than 544.68 and less than or equal to 606.99); level 5 (a score greater than 606.99 and less than or equal to 669.3); level 6 (a score greater than 669.3). The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average. s.e. means standard error. Detail may not sum to totals because of rounding. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2003.  Due to low response rates, data for the United Kingdom are not discussed in this report. NOTE: In order to reach a particular proficiency level, a student must have been able to correctly answer a majority of items at that level. Students were classified into mathematics literacy levels according to their scores. Exact cut point scores are as follows: below level 1 (a score less than or equal to 357.77); level 1 (a score greater than 357.77 and less than or equal to 420.07); level 2 (a score greater than 420.07 and less than or equal to 482.38); level 3 (a score greater than 482.38 and less than or equal to 544.68); level 4 (a score greater than 544.68 and less than or equal to 606.99); level 5 (a score greater than 606.99 and less than or equal to 669.3); level 6 (a score greater than 669.3). The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average. s.e. means standard error. Detail may not sum to totals because of rounding. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2003.  Due to low response rates, data for the United Kingdom are not discussed in this report. NOTE: In order to reach a particular proficiency level, a student must have been able to correctly answer a majority of items at that level. Students were classified into mathematics literacy levels according to their scores. Exact cut point scores are as follows: below level 1 (a score less than or equal to 357.77); level 1 (a score greater than 357.77 and less than or equal to 420.07); level 2 (a score greater than 420.07 and less than or equal to 482.38); level 3 (a score greater than 482.38 and less than or equal to 544.68); level 4 (a score greater than 544.68 and less than or equal to 606.99); level 5 (a score greater than 606.99 and less than or equal to 669.3); level 6 (a score greater than 669.3). The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average. s.e. means standard error. Detail may not sum to totals because of rounding. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2003. Due to low response rates, data for the United Kingdom are not discussed in this report. NOTE: The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average. s.e. means standard error.      Due to low response rates, data for the United Kingdom are not discussed in this report. NOTE: In order to reach a particular proficiency level, a student must have been able to correctly answer a majority of items at that level. Students were classified into problem solving levels according to their scores. Exact cut point scores are as follows: below level 1 (a score less than or equal to 404.06); level 1 (a score greater than 404.06 and less than or equal to 498.08); level 2 (a score greater than 498.08 and less than or equal to 592.10); level 3 (a score greater than 592.10).The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average.            Due to low response rates, 2003 data for the United Kingdom are not discussed in this report. NOTE: The male-female score point difference is calculated by subtracting the average scores of females from the average scores of males. The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average. s.e. means standard error. Detail may not sum to totals because of rounding.  Due to low response rates, 2003 data for the United Kingdom are not discussed in this report. NOTE: The male-female score point difference is calculated by subtracting the average scores of females from the average scores of males. The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average. s.e. means standard error. Detail may not sum to totals because of rounding. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2003.    Due to low response rates, 2003 data for the United Kingdom are not discussed in this report. NOTE: The male-female score point difference is calculated by subtracting the average scores of females from the average scores of males. The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average. s.e. means standard error. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2000 and 2003. Due to low response rates, 2003 data for the United Kingdom are not discussed in this report. NOTE: The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average. The International Socioeconomic Index (ISEI) is an internationally comparable index of occupational status, with a range of approximately 16 to 90, developed by Ganzeboom, De Graaf, and Treiman (1992)  Due to low response rates, 2003 data for the United Kingdom are not discussed in this report. NOTE: The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average.The International Socioeconomic Index (ISEI) is an internationally comparable index of occupational status developed by Ganzeboom, De Graaf, and Treiman (1992  Due to low response rates, 2003 data for the United Kingdom are not discussed in this report. NOTE: The OECD average is the average of the national averages of the OECD member countries with data available. Because PISA is principally an OECD study, the results for non-OECD countries are displayed separately from those of the OECD countries and are not included in the OECD average.The International Socioeconomic Index (ISEI) is an internationally comparable index of occupational status, with a range of approximately 16 to 90, developed by Ganzeboom, De Graaf, and Treiman (1992). The overall linkage of ISEI to mathematics literacy and problem solving is examined using the specific change in score on the combined mathematics literacy scale or problem solving in response to a one standard deviation change in the ISEI index score for each country. A greater increase in achievement score in a country implies a stronger relationship between socioeconomic status and performance in that country. s.e. means standard error. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2003. "}]