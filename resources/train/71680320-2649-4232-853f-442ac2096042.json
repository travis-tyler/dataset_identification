[{"section_title": "Abstract", "text": "For the last decade, it has been shown that neuroimaging can be a potential tool for the diagnosis of Alzheimer's Disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI), and also fusion of different modalities can further provide the complementary information to enhance diagnostic accuracy. Here, we focus on the problems of both feature representation and fusion of multimodal information from Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). To our best knowledge, the previous methods in the literature mostly used hand-crafted features such as cortical thickness, gray matter densities from MRI, or voxel intensities from PET, and then combined these multimodal features by simply concatenating into a long vector or transforming into a higher-dimensional kernel space. In this paper, we propose a novel method for a high-level latent and shared feature representation from neuroimaging modalities via deep learning. Specifically, we use Deep Boltzmann Machine (DBM) 2 , a deep network with a restricted Boltzmann machine as a building block, to find a latent hierarchical feature representation from a 3D patch, and then devise a systematic method for a joint feature representation from the paired patches of MRI and PET with a multimodal DBM. To validate the effectiveness of the proposed method, we performed experiments on ADNI dataset and compared with the state-of-the-art methods. In three binary classification problems of AD vs. healthy Normal Control (NC), MCI vs. NC, and MCI converter vs. MCI non-converter, we obtained the maximal accuracies of 95.35%, 85.67%, and 74.58%, respectively, outperforming the competing methods. By visual inspection of the trained model, we observed that the proposed method could hierarchically discover the complex latent patterns inherent in both MRI and PET."}, {"section_title": "Introduction", "text": "Alzheimer's Disease (AD), characterized by progressive impairment of cognitive and memory functions, is the most prevalent cause of dementia in elderly subjects. According to a recent report by Alzheimer's Association, the number of subjects with AD is significantly increasing every year, and 10 to 20% of people aged 65 or older have Mild Cognitive Impairment (MCI), known as a prodromal stage of AD (Alzheimer's Association, 2012). However, due to the limited period for which the symptomatic treatments could be effective, it has been of great importance for early diagnosis and prognosis of AD/MCI in the clinic.\nTo this end, many researchers have devoted their efforts to find biomarkers and develop a computer-aided system, with which we can effectively predict or diagnose the diseases. Recent studies have shown that the neuroimaging such as Magnetic Resonance Imaging (MRI) (Cuingnet et al., 2011; Davatzikos et al., 2011; Li et al., 2012; Wee et al., 2011; Zhang et al., 2012; Zhou et al., 2011) , Positron Emission Tomography (PET) (Nordberg et al., 2010) , and functional MRI (fMRI) (Greicius et al., 2004; , can be nice tools for diagnosis or prognosis of AD/MCI. Furthermore, fusing the complementary information from multiple modalities helps enhance the diagnostic accuracy (Cui et al., 2011; Fan et al., 2007a; Hinrichs et al., 2011; Kohannim et al., 2010; Perrin et al., 2009; Walhovd et al., 2010; Wee et al., 2012; Westman et al., 2012; Yuan et al., 2012; Zhang and Shen, 2012; Zhang et al., 2011) .\nVarious types of features or patterns extracted from neuroimaging modalities have been considered for brain disease diagnosis with machine learning methods. Here, we divide the previous feature extraction approaches into three categories: voxel-based approach, Region Of Interest (ROI)-based approach, and patch-based approach. A voxelbased approach is the most simple and direct way that uses the voxel intensities as features in classification (Baron et al., 2001; Ishii et al., 2005) . Although it is simple and intuitive in terms of interpretation of the results, its main limitations are the high-dimensionality of feature vectors and also the ignorance of regional information. ROI-based approach considers the structurally or functionally predefined brain regions and extracts representative features from each region (Cuingnet et al., 2011; Davatzikos et al., 2011; Kohannim et al., 2010; Nordberg et al., 2010; Walhovd et al., 2010; Zhang and Shen, 2012) . Thanks to the relatively low feature dimensionality and the whole brain coverage, it is widely used in the literature. However, the features extracted from ROIs are very coarse in the sense that they cannot reflect small or subtle changes involved in the brain diseases. Note that the disease-related structural/functional changes occur in multiple brain regions. Furthermore, since the abnormal regions affected by neurodegenerative diseases can be part of ROIs or span over multiple ROIs, the simple voxel-or ROI-based approach may not effectively capture the diseased-related pathologies. To tackle these limitations, recently, Liu et al. proposed a patch-based method that first dissected brain areas into small 3D patches, extracted features from each selected patch individually, and then combined the features hierarchically in a classifier level (Liu et al., 2012 (Liu et al., , 2013 .\nAs for the fusion of multiple modalities including MRI, PET, biological and neurological data for discriminating AD/MCI patients from healthy Normal Control (NC), Kohannim et al. (2010) concatenated features from modalities into a vector and used a Support Vector Machine (SVM) classifier. Walhovd et al. (2010) applied multi-method stepwise logistic regression analyses, and Westman et al. (2012) exploited a hierarchical modeling of orthogonal partial least squares to latent structures. Hinrichs et al. (2011) , , and Zhang et al. (2011) , independently, utilized a kernel-based machine learning technique.\nIn this paper, we consider the problems of both feature representation and multimodal data fusion for computer-aided AD/MCI diagnosis. Specifically, for feature representation, we exploit a patch-based approach since it can be considered as an intermediate level between voxel-based approach and ROI-based approach, thus efficiently handling the concerns of the high feature dimension and also the sensitivity to small change. Furthermore, from a clinical perspective, neurologists or radiologists examine brain images by searching local distinctive regions and then combine the interpretations with neighboring ones and ultimately with the whole brain. In these regards, we believe that the patch-based approach can effectively handle the region-wide pathologies, which may not be limited to specific ROIs, and accords with the neurologists or radiologists' perspective in terms of examining images, i.e., investigating local patterns and then combining local information distributed in the whole brain for making a clinical decision. In this way, we can also extract richer information that helps enhance diagnostic accuracy.\nHowever, unlike Liu et al.'s method that directly used the gray matter density values in each patch as features, we propose to use a latent highlevel feature representation. Meanwhile, in the fusion of multimodal information, the previous methods often applied either simple concatenation of features extracted from multiple modalities or kernel methods to combine them in a high-dimensional kernel space. However, the feature extraction and feature combination were often performed independently. In this work, we propose a novel method of extracting a shared feature representation from multiple modalities, i.e., MRI and PET. As investigated in the previous studies (Catana et al., 2012; Pichler et al., 2010) , there exist the inherent relations between modalities of MRI and PET. Thus, finding the shared feature representation, which combines the complementary information from modalities, is helpful to enhance performance on AD/MCI diagnosis.\nFrom a feature representation perspective, it is noteworthy that unlike the previous approaches (Hinrichs et al., 2011; Kohannim et al., 2010; Liu et al., 2012 Liu et al., , 2013 Walhovd et al., 2010; Westman et al., 2012; Zhang and Shen, 2012; Zhang et al., 2011 ) that considered simple low-level features, which are often vulnerable to noises, we propose to consider high-level or abstract features for improving the robustness to noises. For obtaining the latent high-level feature representations inherent in a patch observation such as correlations among voxels that cover different brain regions, we exploit a deep learning strategy (Bengio, 2009; LeCun et al., 1998) , which has been successfully applied to medical imaging analysis (Ciresan et al., 2013; Hjelm et al., 2014; Liao et al., 2013; Shin et al., 2013; . Among various deep models, we use a Deep Boltzmann Machine (DBM) (Salakhutdinov and Hinton, 2009 ) that can hierarchically find feature representations in a probabilistic manner. Rather than using the noisy voxel intensities as features as Liu et al. (2013) did, the high-level representation obtained via DBM is more robust to noises and thus helps enhance diagnostic performances. Meanwhile, from a multimodal data fusion perspective, unlike the conventional multimodal feature combination methods that first extract modality-specific features and then fuse their complementary information during classifier learning, the proposed multimodal DBM fuses the complementary information from different modalities during a feature representation step. Note that once we extract features from each modality, we may already lose some good correlation information between modalities. Therefore, it is important to discover a shared representation by fully utilizing the original information in each modality during feature representation procedure. In our multimodal data fusion method, thanks to the methodological characteristic of the DBM (i.e., undirected graphical model), it allows the bidirectional information flow from one modality (e.g., MRI) to the other modality (e.g., PET) and vice versa. Therefore, we can distribute feature representations over different layers in the path between modalities and thus efficiently discover a shared representation while still utilizing the full information in the observations."}, {"section_title": "Materials and image processing", "text": ""}, {"section_title": "Subjects", "text": "In this work, we use the ADNI dataset publicly available on the web, 3 but consider only the baseline MRI and 18-Fluoro-DeoxyGlucose PET (FDG-PET) data acquired from 93 AD subjects, 204 MCI subjects including 76 MCI converters (MCI-C) and 128 MCI non-converters (MCI-NC), and 101 NC subjects. 4 The demographics of the subjects are detailed in Table 1 . With regard to the general eligibility criteria in ADNI, subjects were in the age of between 55 and 90 with a study partner, who could provide an independent evaluation of functioning. General inclusion/exclusion criteria 5 are as follows: 1) NC subjects: MMSE scores between 24 and 30 (inclusive), a Clinical Dementia Rating (CDR) of 0, non-depressed, non-MCI, and non-demented; 2) MCI subjects: MMSE scores between 24 and 30 (inclusive), a memory complaint, objective memory loss measured by education adjusted scores on Wechsler Memory Scale Logical Memory II, a CDR of 0.5, absence of significant levels of impairment in other cognitive domains, essentially preserved activities of daily living, and an absence of dementia; and 3) mild AD: MMSE scores between 20 and 26 (inclusive), CDR of 0.5 or 1.0, and meets the National Institute of Neurological and Communicative Disorders and Stroke and the Alzheimer's Disease and Related Disorders Association (NINCDS/ ADRDA) criteria for probable AD."}, {"section_title": "MRI/PET scanning and image processing", "text": "The structural MR images were acquired from 1.5 T scanners. We downloaded data in the Neuroimaging Informatics Technology Initiative (NIfTI) format, which had been pre-processed for spatial distortion correction caused by gradient nonlinearity and B1 field inhomogeneity.\n3 Available at 'http://www.loni.ucla.edu/ADNI'. 4 Although there exist in total more than 800 subjects in ADNI database, only 398 subjects have the baseline data including the modalities of both MRI and FDG-PET. 5 Refer to 'http://www.adniinfo.org' for the details.\nThe FDG-PET images were acquired 30-60 min post-injection, averaged, spatially aligned, interpolated to a standard voxel size, normalized in intensity, and smoothed to a common resolution of 8 mm full width at half maximum. The MR images were preprocessed by applying the typical procedures of Anterior Commissure (AC)-Posterior Commissure (PC) correction, skull-stripping, and cerebellum removal. Specifically, we used MIPAV software 6 for AC-PC correction, resampled images to 256 \u00d7 256 \u00d7 256, and applied N3 algorithm (Sled et al., 1998) to correct non-uniform tissue intensities. After skull stripping (Wang et al., 2014) and cerebellum removal, we manually checked the skull-stripped images to ensure the clean and dura removal. Then, FAST in FSL package 7 (Zhang et al., 2001) was used to segment the structural MR images into three tissue types of Gray Matter (GM), White Matter (WM), and CerebroSpinal Fluid (CSF). Finally, all the three tissues of MR image were spatially normalized onto a standard space, for which in this work we used a brain atlas already aligned with the MNI coordinate space (Kabani et al., 1998) , via HAMMER (Shen and Davatzikos, 2002) , although other advanced registration methods can also be applied for this process (Friston, 1995; Jia et al., 2010; Tang et al., 2009; Xue et al., 2006; Yang et al., 2008) . Then, the regional volumetric maps, called RAVENS maps, were generated by a tissue preserving image warping method (Davatzikos et al., 2001) . It is noteworthy that the values of RAVENS maps are proportional to the amount of original tissue volume for each region, giving a quantitative representation of the spatial distribution of tissue types. Due to its relatively high relatedness to AD/MCI compared to WM and CSF (Liu et al., 2012) , in this work, we considered only the spatially normalized GM volumes, called GM tissue densities, for classification. Regarding FDG-PET images, they were rigidly aligned to the respective MR images. The GM density maps and the PET images were further smoothed using a Gaussian kernel (with unit standard deviation) to improve the signal-to-noise ratio. We downsampled both the GM density maps and PET images to 64 \u00d7 64 \u00d7 64 voxels 8 according to Liu et al.'s (2013) work, which saved the computational time and memory cost, but without sacrificing the classification accuracy."}, {"section_title": "Method", "text": "In Fig. 1 , we illustrate a schematic diagram of our framework for AD/MCI diagnosis. Given a pair of MRI and PET images, we first select class-discriminative patches by means of a statistical significance test between classes. Using the tissue densities of a MRI patch and the voxel intensities of a PET patch as observations, we build a patch-level feature learning model, called a MultiModal DBM (MM-DBM), that finds a shared feature representation from the paired patches. Here, instead of using the original real-valued tissue densities of MRI and the voxel intensities of PET as inputs to MM-DBM, we first train a Gaussian Restricted Boltzmann Machine (RBM) and use it as a preprocessor to transform the real-valued observations into binary vectors, which become the input to MM-DBM. After finding latent and shared feature representations of the paired patches from the trained MM-DBM, we construct an image-level classifier by fusing multiple classifiers in a hierarchical manner, i.e., patch-level classifier learning, mega-patch construction, and a final ensemble classification."}, {"section_title": "Patch extraction", "text": "For the class-discriminative patch extraction, we exploit statistical significance for voxels in each patch, i.e., p-values, following Liu et al.'s (2013) work. It is noteworthy that in this step, we take advantage of a group-wise analysis via voxel-wise statistical test. That is, by first performing group comparison, e.g., AD and NC, we can find the statistically significant voxels, which can provide useful information for brain disease diagnosis. Based on these voxels, we can then define the classdiscriminative patches to further utilize local regional information. By considering only the selected discriminative patches rather than all patches in an image, we can obtain both performance improvement in classification and reduction in computational cost. Throughout this paper, a patch is defined as a three-dimensional cube with a size of w \u00d7 w \u00d7 w in a brain image, i.e., MRI or PET. Given a set of training images, we first perform two-sample t-test on each voxel, and then select voxels with the p-value smaller than the predefined threshold.\n9 For each of the selected voxels, by taking each of them as a center, we extract patches with a size of w \u00d7 w \u00d7 w, and then compute a mean p-value by averaging the p-values of all voxels within a patch. Finally, by scanning all the extracted patches, we select class-discriminative patches in a greedy manner with the following rules:\n\u2022 The candidate patch should be overlapped less than 50% with any of the selected patches.\n\u2022 Among the candidate patches that satisfy the rule above, we select patches whose mean p-values are smaller than the average p-value of all candidate patches.\nFor the multimodal case, i.e., MRI and PET in our work, we apply the steps of testing the statistical significance, extracting patches, and computing the mean p-values as explained above, for each modality independently. But for the last step of selecting class-discriminative patches, we consider multiple modalities together. That is, regarding the second rule, the mean p-value of a candidate patch should be smaller than that of all candidate patches of all the modalities. Once a patch location is determined from one modality, a patch of the same location in the other modality is paired for multimodal joint feature representation, which is described in the following section."}, {"section_title": "Patch-level deep feature learning", "text": "Recently, Liu et al. (2013) presented a hierarchical framework that gradually integrated features from a number of local patches extracted from a GM density map. Although they showed the efficacy of their method for AD/MCI diagnosis, it is well-known that the structural or functional images are susceptible to acquisition noise, intensity inhomogeneity, artifacts, etc. Furthermore, the raw voxel density or intensity values in a patch can be considered as low-level features that do not efficiently capture more informative high-level features. To this end, in this paper, we propose a deep learning based high-6 Available at 'http://mipav.cit.nih.gov/clickwrap.php'. 7 Available at 'http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/'. 8 The final voxel size is 4 \u00d7 4 \u00d7 4 mm 3 .\n9 In this work, we set the threshold to 0.05. level structural and functional feature representation from MRI and PET, respectively, for AD/MCI classification.\nIn the following, we first introduce an RBM, which has recently become a prominent tool for feature learning with applications in a wide variety of machine learning fields. Then, we describe a DBM, a network of stacking multiple RBMs, with which we discover a latent hierarchical feature representation from a patch. We finally explain a systemic method to find a joint feature representation from multimodal neuroimaging data, such as MRI and PET."}, {"section_title": "Restricted Boltzmann Machine", "text": "An RBM is a two-layer undirected graphical model with visible and hidden units or variables in each layer (Fig. 2) . Hereafter, we use units and variables interchangeably. It assumes a symmetric connectivity W between the visible layer and the hidden layer, but no connections within the layers, and each layer has a bias term, a and b, respectively. In Fig. 2 , the units of the visible layer v = [v i ], i = {1,\u22ef, D}, correspond to the observations while the units of the hidden layer h = [h j ], j = {1,\u22ef, F}, models the structures or dependencies over visible variables, where D and F denote, respectively, the numbers of visible and hidden units. In our work, the voxel intensities of a patch become the values of the visible units, and the hidden units represent the complex relations of the input units, i.e., voxels in a patch, that can be captured by the symmetric matrix W. It is worth noting that because of the symmetricity of the matrix W, we can also reconstruct the input observations, i.e., a patch, from the hidden representations. Therefore, an RBM is also considered as an auto-encoder (Hinton and Salakhutdinov, 2006) . This favorable characteristic is also used in RBM parameter learning .\nIn RBM, a joint probability of (v, h) is given by:\nwhere\nis an energy function, and Z(\u0398) is a partition function that can be obtained by summing over all possible pairs of v and h. For the sake of simplicity, by assuming binary visible and hidden units, the energy function E(v, h; \u0398) is defined by\nThe conditional distribution of the hidden variables given the visible variables and also the conditional distribution of the visible variables given the hidden variables are, respectively, computed as follows:\n\u00bd is a logistic sigmoid function. Due to the unobservable hidden variables, the objective function is defined as the marginal distribution of the visible variables as follows:\nIn our work, the observed patch values from MRI and PET are realvalued v \u2208 R D . For this case, it is common to use a Gaussian RBM ( Hinton and Salakhutdinov, 2006) , in which the energy function is given by\nwhere \u03c3 i denotes a standard deviation of the i-th visible variable and\nThis variation leads to the following conditional distribution of visible variables given the binary hidden variables\nDeep Boltzmann Machine A DBM is an undirected graphical model, structured by stacking multiple RBMs in a hierarchical manner. That is, a DBM contains a visible layer v and a series of hidden layers\nwhere F i denotes the number of units in the i-th hidden layer and L is the number of hidden layers. We should note that, hereafter, for simplicity, we omit bias terms and assume that the visible and hidden variables are binary 10 or probability, and the following description on DBM is based on Salakhutdinov and Hinton's (2012) work. Thanks to the hierarchical nature in the deep network, one of the most important characteristics of the DBM is to capture highly nonlinear and complicated patterns or statistics such as the relations among input values. Another important feature of the DBM is that the hierarchical latent feature representation can be learned directly from the data without human intervention. In other words, unlike the previous methods that mostly considered hand-crafted/predefined features (Fan et al., 2007b; Liu et al., 2013; Zhang et al., 2011) or outputs from the predefined functions (Dinov et al., 2005; Hackmack et al., 2012) , we assign the role of determining feature representations to a DBM and find them autonomously from the training samples. Utilizing its representational and self-taught learning properties, we can find a latent representation of the original GM tissue intensities and/or PET voxel intensities in a patch. When an input patch is presented to a DBM, the different layers of the network represent different levels of information. That is, the lower the layer in the network, the simpler patterns (e.g., linear relations of input variables); the higher the layer, the more complicated or abstract patterns inherent in the input values (e.g., non-linear relations among input variables).\nThe rationale of using DBM for feature representation is as follows: (1) it can learn internal latent representations that capture non-linear complicated patterns and/or statistical structures in a hierarchical manner (Bengio, 2009; Bengio et al., 2007; Mohamed et al., 2012) . However, unlike many other deep network models such as deep belief network (Hinton and Salakhutdinov, 2006) , and stacked autoencoder (Shin et al., 2013) , the approximate inference procedure after the initial bottom-up pass incorporates top-down feedback, which allows DBM to use higher-level knowledge to resolve uncertainty about intermediate-level features, thus creating better data-dependent representations and statistics for learning (Salakhutdinov and Hinton, 2012) . Thanks to this two-way dependencies, i.e., bottom-up and top-down, it was shown that DBMs outperform the other deep learning methods in computer vision (Montavon et al., 2012; Salakhutdinov and Hinton, 2009; Srivastava and Salakhutdinov, 2012) . To this end, we use a DBM to discover hierarchical feature representation from neuroimaging, e.g., MRI and PET, in our work. Fig. 3(a) shows an example of the three-layer DBM. The energy of the state (v, h 1 , h 2 ) in the DBM is given by\nwhere\nsymmetric connections of (v, h 1 ) and (h 1 , h 2 ), and\nThen the probability that the model assigns to a visible vector v is given by:\nwhere Z(\u0398) is a normalizing factor. Given the values of the units in the neighboring layer(s), the probability of the binary visible or binary hidden units being set to 1 is computed as follows:\nNote that in the computation of the probability of the hidden units h 1 , we incorporate both the lower visible layer v and the higher hidden layer h 2 , and this makes DBM differentiated from other deep learning models and also more robust to noisy observations (Salakhutdinov and Hinton, 2009; Srivastava and Salakhutdinov, 2012) .\nUnlike the conventional generative DBM, in this work, we consider a discriminative DBM, by injecting a discriminative RBM (Larochelle and Bengio, 2008) at the top hidden layer. That is, the top hidden layer is connected to both the lower hidden layer and the additional label layer (Fig. 3(b) ), which indicates the label of the input v. In this way, we can train DBM to discover hierarchical and discriminative feature representations by integrating the process of discovering features of inputs with their use in classification (Larochelle and Bengio, 2008) . Our model does not require an additional fine-tuning step for classification as done in Ngiam et al. (2011) and Salakhutdinov and Hinton (2009) . With the inclusion of the additional label layer, the energy of the state\nC denote, respectively, a connectivity between the top hidden layer and the label layer and a classlabel indicator vector, C is the number of classes, and \u0398 = {W 1 , W 2 , U}. The probability of an observation (v, o) is computed by\nThe conditional probability of the top hidden units being set to 1 is given by\n10 In our experiments, we trained a Gaussian RBM by fixing the standard deviations to 1 and transformed the observed real-values from MRI and PET to binary vectors using it as a preprocessor, following Nair and Hinton's (2008) work.\nFor the label layer, we use a logistic function\nIn this way, the hidden units capture class-predictive information about the input vector. Here, we should note that the label layer connected to the top hidden layer is considered during only the training phase of finding the class-discriminative parameters.\nFrom a feature learning perspective, in the low layer of our model, basic image features such as spots and edges are captured from the input data. The learned low-level features are further fed into the high-level of the network, which encodes more abstract and higher level semantic information inherent in the input data. But, here, we should note that the output layer linked to the top hidden layer imposes the learned features to be discriminative between classes.\nIn order to learn the parameters \u0398 = {W 1 , W 2 , U}, we maximize the log-likelihood of the observed data (v, o). The derivative of the loglikelihood of the observed data with respect to the model parameters takes the simple form of\nwhere b\u00b7N data denotes the data-dependent statistics obtained by sampling the model conditioned on the visible units v(\u2261 h 0 ) and the label units o clamped to the observation and the corresponding label, respectively, and b\u00b7N model denotes the data-independent statistics obtained by sampling from the model. When the model approximates the data distribution well, it can be reached for the equilibrium of data-dependent and data-independent statistics. In parameter learning, we use a gradient-based optimization strategy. In Eqs. (17) and (18), we need to compute the data-dependent and the data-independent statistics. First, because of the two-way dependency in DBM, it is not tractable for the data-dependent statistics. Fortunately, variational mean-field approximation works well for estimating the data-dependent statistics. For the details of computing the data-dependent statistics, please refer to Appendix A and Salakhutdinov and Hinton (2012) . Here, we should note that due to the large number of parameters involved in the DBM, it generally requires a huge number of training samples for generalization, which is not valid in practice, especially for the neuroimaging studies. However, Hinton et al. recently introduced a greedy layer-wise learning algorithm and successfully applied to learn a deep belief network (Hinton and Salakhutdinov, 2006) . Since the pioneering work, many research groups have used this approach to initialize the parameters in deep learning and called it 'pre-training' (Bengio, 2009; . We apply the same procedure to provide a good initial configuration of the parameters, which helps the learning procedure converge much faster than random initialization.\nThe key idea in a greedy layer-wise learning is to train one layer at a time by maximizing the variational lower bound. That is, we first train the 1st hidden layer with the training data as input, and then train the 2nd hidden layer with the outputs from the 1st hidden layer as input, and so on. That is, the representation of the l-th hidden layer is used as input for the (l + 1)-th hidden layer and this pairwise model becomes an RBM. Here, it should be mentioned that, unlike the other deep networks, because the DBM integrates both bottom-up and topdown information, the first and last RBMs in the network need modification by using weights twice as big as in one direction. Since the detailed explanation on this issue is out of domain of our work, please refer to Salakhutdinov and Hinton (2012) for details.\nIn a nutshell, the learning proceeds by two steps: (1) a greedy-layerwise pre-training for a good initial setup of the modal parameters, and (2) iterative alternation of variational mean-field approximation to estimate the posterior probabilities of hidden units and stochastic approximation to update model parameters (refer to Appendix A). After learning the parameters, we can then obtain a latent feature representation for an input sample, by inferring the probabilities of the hidden units in the trained DBM."}, {"section_title": "11", "text": ""}, {"section_title": "Multimodal deep feature fusion", "text": "There are increasing evidences that biomarkers from different modalities can provide complementary information in AD/MCI diagnosis (Hinrichs et al., 2011; Kohannim et al., 2010; Perrin et al., 2009; Zhang et al., 2011) . Unlike the previous methods that either simply concatenated features from multiple modalities into a long vector (Kohannim et al., 2010) or fused the modality-dependent features in a kernel space (Hinrichs et al., 2011; Zhang et al., 2011) , in this work, we propose a systematic method of extracting multimodal feature representations in a probabilistic manner. Different modalities will have different statistical properties, thus making it difficult to jointly model them using a shallow architecture. Therefore, simple concatenation of the features of multiple modalities can cause strong connections among the variables of individual modality, but few units across modalities (Ngiam et al., 2011) . In order to tackle this problem, Srivastava and Salakhutdinov (2012) proposed a MultiModal DBM (MM-DBM) to combine images and texts for information retrieval. Motivated by their work, in this paper, we devise a modified MM-DBM, in which the top hidden layer has multiple entries of the lower hidden layers and the label layer, to extract a shared feature representation by fusing neuroimaging information of MRI and PET. Fig. 4 presents a multimodal deep network in which one path represents the statistical properties of MRI and the other path represents those of PET, and the top shared hidden layer finally discovers the shared properties of the modalities in a supervised manner. We argue that this joint feature representation discriminates our method from the previous multimodal 11 Instead of the standard mean-field approximation, inspired by Montavon et al.'s (2012) work, in this work, we traverse the trained DBM in a feed-forward manner, i.e., f = sigm(W 2 \u00b7 sigm(W 1 v)), for feature representations. The same strategy is applied for the multimodal DBM described in the Multimodal deep feature fusion section.\n(a) (b) Fig. 3 . An architecture of (a) a conventional Deep Boltzmann Machine and (b) its discriminative version with label information at the top layer.\nmethods (Hinrichs et al., 2011; Zhang et al., 2011) , which first extracted features from each modality independently, and then combined them through kernel machines. The joint distribution over the multimodal inputs of MRI and PET can be estimated as follows:\nwhere the subscripts M, P, and S denote, respectively, units of the MRI path, the PET path, and the shared hidden layer. Regarding parameter learning for MM-DBM, the same strategy with the unimodal DBM learning can be applied. For details, please refer to Appendix B."}, {"section_title": "Image-level hierarchical classifier learning", "text": "In order to combine the distributed patch information over an image and build an image-level classifier, we use a hierarchical classifier learning scheme, proposed by Liu et al. (2013) . That is, we first build a classifier for each patch, independently, and then combine them in a hierarchical manner by feeding the outputs from the lower-level classifiers to the upper-level classifier. Specifically, we build a three-level classifier for decision: patch-level, mega-patch-level, and image-level. For the patch-level classification, a linear Support Vector Machine (SVM) is trained for each patch location independently with the (MM-)DBMlearned feature representations as input. The output from a patchlevel SVM, measured by the relative distance from the decision hyperplane, is then converted to a probability via a softmax function. Here, we should note that in patch-level classifier learning, we randomly partition the training data into a training set and a validation set.\n12 The patch-level classifier is trained on the training set, and then the classification accuracy is obtained with the validation set. In the following hierarchy, instead of considering all patch-level classifiers' output simultaneously, we agglomerate the information of the locally distributed patches by constructing spatially distributed 'mega-patches' under the consideration that the disease-related brain areas are distributed over some distant brain regions with arbitrary shape and size (Liu et al., 2013) . Similar to the patch extraction described in Section 3.1, we construct mega-patches and the respective classifiers in a greedy manner. Concretely, we first sort the patches in a descending order based on the classification accuracy obtained with the validation set in patch-level classifier learning. Starting with the patch with the highest classification accuracy as a new mega-patch, we greedily merge the neighboring patches into the mega-patch. The merging condition is that, if and only if, a mega-patch classifier, which is trained with the patches already included in the current mega-patch and also the candidate patch under consideration, produces a better classification accuracy. The process is repeated until all the patches are visited. The size of the constructed mega-patches and their component-patches are determined by a cross-validation. We also constrain that none of the final mega-patches overlap each other larger than the half of the respective mega-patches' size. Note that, in the step of the mega-patch classifier learning, some patches that are not informative in classification are discarded, and each mega-patch classifier covers different regions of the brain with a different size.\nFinally, we build an image-level classifier by fusing the mega-patch classifiers. We select an optimal subset of mega-patches in a forward greedy search strategy for a final fusion. However, since the megapatch selection is performed on the training data, the resulting imagelevel classifier may not be optimal for the testing data. To this end, we divide the training data into multiple subsets, and train an image-level classifier in each subset individually. 13 In this way, we can build multiple image-level classifiers, each of which selects possibly a different subset of mega-patches. By counting the selected frequency of mega-patches in each image-level classifier, we can finally compute the relative importance of the mega-patches. After normalizing the frequencies, we use them as weights of the respective mega-patches. The final decision in image-level classifiers is made by a weighted combination of the mega-patch classifiers' outputs."}, {"section_title": "Experimental results and discussions", "text": "In this section, we evaluate the effectiveness of the proposed method for (1) a latent feature representation with DBM and (2) a shared feature representation between MRI and PET with an MM-DBM, by considering three binary classification problems: AD vs. NC, MCI vs. NC, MCI converter (MCI-C) vs. MCI non-converter (MCI-NC). Due to the limited number of data, we applied a 10-fold cross validation technique. Specifically, we randomly partitioned the dataset into 10 subsets, each of which included 10% of the total data. We repeated experiments for each classification problem 10 times, by using 9 out of 10 subsets for training and the remaining one for testing at each time. It is worth noting that, for each classification problem, during a training phase, we performed patch selection, (MM-)DBM and SVM model learning only using the 9 training subsets. Based on the selected patches and also the trained (MM-)DBM and SVM models, we finally evaluated the performance on the left-out testing subset. We compare the proposed method with Liu et al.'s (2013) method, using the same training and testing set in each experiment for a fair comparison."}, {"section_title": "Experimental setup", "text": "As for the patch size w, we set it to 11 by following Liu et al.'s (2013) work. During mega-patch construction, the size of a mega-patch was allowed in the range of w \u00d7 [1.2, 1.4, 1.6, 1.8, 2] and the optimal size for each mega-patch was determined by cross-validation as explained in the Image-level hierarchical classifier learning section.\nIn building (MM)-DBM of GM patches and/or PET patches, we can use Gaussian visible units for the input patches by considering the voxels as continuous variables. However, learning (MM-)DBMs with Gaussian visible units is very slow and requires a huge number of parameter updates, compared with the binary visible units. To this end, we first trained RBM with 1, 331(= 11 3 ) Gaussian visible units and also 500 binary hidden units by using contrastive divergence learning for 1000 epochs.\n14 After training a Gaussian RBM for each modality, we used it as a preprocessor, following Nair and Hinton's (2008) work that effectively converts GM tissue densities or PET voxel intensities into 12 In our work, we set 80% of the entire training data as a training set and the rest for a validation set. 13 In this work, we set the number of subsets to 10. 14 The input data were first normalized and whitened by zero component analysis, and the standard deviation was fixed to 1 during the parameter updates. 500-dimensional binary vectors. We then used the binary vectors as 'preprocessed data' to train our (MM-)DBMs. We should note that the Gaussian RBMs were not updated during (MM-)DBM learning. We structured a three-layer DBM for MRI (MRI-DBM) and PET (PET-DBM), respectively, and a four-layer DBM for MRI + PET (MM-DBM). For all these models, we used binary visible and binary hidden units. Both the MRI-DBM and the PET-DBM were structured with 500(visible)-500(hidden)-500(hidden), and the MM-DBM was structured with 500(visible)-500(hidden)-500(hidden) for a MRI pathway, 500(visible)-500(hidden)-500(hidden) for a PET pathway, and finally 1000 hidden units for the shared hidden layer. In (MM-)DBM learning, we updated the parameters, i.e., weights and biases, with a learning rate of 10 -3 and a momentum of 0.5\nwith an increment gradually up to 0.9 for 500 epochs. We used the trained parameters of MRI-DBM and PET-DBM as the initial setup of the MRI and PET pathways in MM-DBM learning. We implemented the DBM method based on Salakhutdinov's codes. 15 We used a linear SVM for the hierarchical classifiers, i.e., patch-level classifier, mega-patch-level classifier, and image-level classifier. An LIBSVM toolbox 16 was used for SVM learning and classification. The free parameter that controls the soft margin was determined by a nested cross-validation."}, {"section_title": "Extracted patches and trained DBMs", "text": "In Fig. 5 , we presented the example images overlaid with p-values of the voxels, obtained from AD and NC groups, based on which we selected patch locations for AD and NC classification. It is worth noting that, for both modalities, the voxels in the subcortical and medial temporal areas showed low p-values, i.e., statistically different between classes, while for other areas, each modality presents slightly different p-value distributions, from which we could possibly obtain complementary information for classification. Samples of the selected 3D patches are also presented in Fig. 6 , in which one 3D volume is displayed in each row, for each modality. Taking these patches as input data to a Gaussian RBM and then transforming to binary vectors, we trained our feature representation models, i.e., MRI-DBM, PET-DBM, and MM-DBM. Regarding the trained MM-DBM, we visualized the trained weights in Fig. 7 by linearly projecting them to the input space for intuitive interpretation of the feature representations. 17 In the figure, the left images represent the trained weights of our Gaussian RBMs that were used to convert the realvalued patches into binary vectors as a preprocessor, and the right images represent the trained weights of the first-layer hidden units of the respective modality's pathway in our MM-DBM. From the figure, we can regard the hidden units in the Gaussian RBM as simple cells of a human visual cortex that maximally responds to specific spot-or edge-like stimulus patterns within the receptive field, i.e., a patch in our case. In particular, each hidden unit in a Gaussian RBM finds simple volumetric or functional patterns in the input 3D patch by assigning different weights to the corresponding voxels. For example, hidden units of the Gaussian RBM for MRI (left in Fig. 7(a) ) focus on different parts of a patch to detect a simple spot-or edge-like pattern in the input 3D GM patch. The hidden units in a Gaussian RBM for PET (left in Fig. 7(b) ) can be understood as descriptors that discover local functional relations among voxels within a patch. Note that the hidden units of a Gaussian RBM for either MRI or PET find, respectively, the structural or functional relations among voxels in a localized way. Meanwhile, the hidden units in our (MM-)DBM served as complex filters of a human visual cortex that combine the outputs from the simple cells and maximally responds to more complex patterns within the receptive field. For example, the weights of hidden units in the hidden layer of the MRI pathway in an MM-DBM (right in Fig. 7(a) ) discover more complicated structural patterns in the input 3D GM patch, such as combination of edges orienting in different directions. With respect to the PET, the weights of hidden units in the hidden layer of the PET pathway in an MM-DBM (right in Fig. 7(b) ) discover non-linear functional relations among voxels within a 3D patch. In this way, as it forwards to the higher layer, the MM-DBM finds complex latent features in the input patch, and ultimately in the top hidden layer, the hidden units discover the inter-modality relations in between the pair of MRI and PET patches, each of which comes from the same location in a brain."}, {"section_title": "Performance evaluation", "text": "Let TP, TN, FP, and FN denote, respectively, True Positive, True Negative, False Positive, and False Negative. In this work, we consider the following quantitative measurements and presented the performances of the competing methods in Table 2 .\n\u2022 ACCuracy ( Regarding sensitivity and specificity, the higher the sensitivity, the lower the chance of mis-diagnosing AD/MCI patients; also the higher the specificity, the lower the chance of mis-diagnosing NC to AD/MCI. Although the proposed method had a lower sensitivity than that of Liu et al.'s method for a couple of cases, e.g., 90.06% (Liu et al.'s method) vs. 88.04% (proposed) with PET in the AD diagnosis, 98.97% (Liu et al.'s method) vs. 95.37% (proposed) with MRI + PET in the MCI diagnosis, and 40.02% (Liu et al.'s method) vs. 25.45% (proposed) with PET in the MCI-C diagnosis, in general, the proposed method showed higher sensitivity and specificity in all three classification problems. Hence, from a clinical point of view, the proposed method is less likely to mis-diagnose subjects with AD/MCI and vice versa, compared to Liu et al.'s method.\nMeanwhile, because of the data imbalance between classes, i.e., AD (93 subjects), MCI (204 subjects; 76 MCI-C and 128 MCI-NC subjects), and NC (101 subjects), we obtained low sensitivity (MCI vs. NC) or specificity (MCI-C vs. MCI-NC). The balanced accuracy, which is calculated by taking the average of sensitivity and specificity, avoids inflated performance estimates on imbalanced datasets. Based on this metric, we clearly see that the proposed method is superior to the competing method. Note that in discrimination between MCI and NC, while the accuracy improvement by the proposed method with MRI + PET was 1.43% and 1.38% compared to the same method with MRI and PET, respectively, in terms of the balanced accuracy, the improvements went up to 3.93% (vs. MRI) and 2.95% (vs. PET). With a further concern on low sensitivity and specificity, especially in classifications of MCI vs. NC and MCI-C vs. MCI-NC, we also computed a Positive Predictive Value (PPV) and a Negative Predictive Value (NPV). Statistically, PPV and NPV measure, respectively, the proportion of subjects with AD, MCI, or MCI-C who are correctly diagnosed as patients, and the proportion of subjects without AD, MCI, or MCI-C who are (a) MRI (b) PET Fig. 7 . Visualization of the trained weights of our modality-specific Gaussian RBMs (left) used for data conversion from a real-valued vector to a binary vector, and those of our MM-DBM (right) used for latent feature representations. For the weights of our MM-DBM, they correspond to the first hidden layer in the respective modality's pathway in the model. In each subfigure, one row corresponds to one hidden unit in the respective Gaussian RBM or MM-DBM. (Hinrichs et al., 2011; Zhang et al., 2011) , we also obtained the best performances with the complementary information from multiple modalities, i.e., MRI + PET."}, {"section_title": "Comparison with State-of-the-Art Methods", "text": "In Table 3 , we also compared the classification accuracies of the proposed method with those of the state-of-the-art methods that considered multi-modality in classifications of AD vs. NC, MCI vs. NC, and MCI-C vs. MCI-NC. Note that, due to different datasets and different approaches of extracting features and building classifiers, it is not fair to directly compare the performances among methods. Nonetheless, it is remarkable that the proposed method showed the highest accuracies among the methods in all the binary classification problems. It is also worth noting that our method is the only one that considered the patch-based approach for feature extraction, while the other methods used an ROI-based approach."}, {"section_title": "Importance of brain areas in classification", "text": "For the investigation of the relative importance of different brain areas determined by the proposed method for AD/MCI diagnosis, we visualized the weights of the selected patches in Fig. 8 . Specifically, the weight of each patch was calculated by accumulating the selected frequency of mega-patches in final ensemble classifiers over crossvalidations. That is, the weight of a patch was determined with the sum of the weights of the mega-patches that included the patch and was used in the final decision. The high weighted patches were in accordance with the previous reports on AD/MCI studies. Those were distributed around a medial temporal lobe (that includes amygdala, hippocampal formation, entorhinal cortex) (Braak and Braak, 1991; Burton et al., 2009; Desikan et al., 2009; Devanand et al., 2007; Ewers et al., 2012; Lee et al., 2006; Mosconi, 2005; Visser et al., 2002; Walhovd et al., 2010) , superior/medial frontal gyrus (Johnson et al., 2005) , precentral/postcentral gyrus (Belleville et al., 2011) , precuneus (Bokde et al., 2006; Davatzikos et al., 2011; Singh et al., 2006) , thalamus, putamen (de Jong et al., 2008) , caudate nucleus (Dai et al., 2009) , etc."}, {"section_title": "Limitations", "text": "In our experiments, we validated the efficacy of the proposed method in three classification problems by achieving the best performances. However, there still exist some limitations of the proposed method.\nFirst, even though we could visualize the trained weights in our MM-DBMs in Fig. 7 , from a clinical perspective, it is difficult to understand or interpret the resulting feature representations. Particularly, with respect to the investigation of brain abnormalities affected by neurodegenerative disease, i.e., AD or MCI, our method cannot provide useful clinical information. In this regard, it could be a good research direction in which we further extend the proposed method to find or detect brain abnormalities in terms of brain regions or areas for easy understanding to clinicians.\nSecond, in our experiments, we manually determined the number of hidden units in each layer. Furthermore, we used a relatively small data samples (93 AD, 76 MCI-C, 128 MCI-NC, and 101 NC). Therefore, the network structures used to discover high-level feature representations in our experiments were not necessarily optimal. We believe that it needs more intensive studies such as learning the optimal network structure from big data for practical use of deep learning in clinical settings.\nThird, as the graphical model illustrated in Fig. 4 , the current method only considers bi-modalities of MRI and PET. However, it is generally beneficiary to combine as many modalities as possible to use their richer information. Therefore, it is necessary to build a more systematic model that can efficiently find and use complementary information from genetics, proteomics, imaging, cognition, disease status, and other phenotypic modalities.\nLastly, according to a recent broad spectrum of studies, there are increasing evidences that subjective cognitive complaint is one of the important genetic risk factors, which increases the risk of progression to MCI or AD (Loewenstein et al., 2012; Mark and Sitskoorn, 2013) . That is, among the cognitively normal elderly individuals who have subjective cognitive impairments, there exists a high possibility for some of them to be in the stage of 'pre-MCI'. However, in the ADNI dataset, there is no related information. Thus, in our experiments, the NC group could include both genuine controls and those with subjective cognitive complaints."}, {"section_title": "Conclusions", "text": "In this paper, we proposed a method for a shared latent feature representation from MRI and PET in deep learning. Specifically, we used DBM to find a latent feature representation from a volumetric patch and further devised method to systemically discover a joint feature representation from multi-modality. Unlike the previous methods that mostly considered the direct use of the GM tissue densities from MRI and/or voxel intensities from PET and then fused the complementary information in a kernel technique, the proposed method learned high-level features in a self-taught manner via deep learning, and thus could efficiently combine the complimentary information from MRI and PET during feature representation procedure. Experimental results on ADNI dataset showed that the proposed method is superior to the previous methods in terms of various quantitative metrics. where H(\u00b7) is the entropy functional, KL[\u00b7||\u00b7] denotes Kullback-Leibler divergence, and \u03a9 is a variational parameter set. For computational simplicity and learning speed, the na\u00efve meanfield approximation, which uses a fully factorized distribution, is generally used in the literature (Tanaka, 1998 \u00f0A:5\u00de\nRegarding the data-independent statistics, we apply a stochastic approximation procedure to obtain samples, also called particles, of e v, e h 1 , e h 2 , and e o by running repeatedly the alternate Gibbs sampler on a set of particles. Once both the data-dependent and data-independent statistics are computed, we then update parameters as follows: where \u03b1 t is a learning rate, and N and M denote, respectively, the numbers of training data and particles, and superscripts n and m denote, respectively, indices of an observation and a particle."}, {"section_title": "Appendix B. Learning multimodal DBM parameters", "text": "The same approach to the unimodal DBM described in the Deep Boltzmann Machine section can be applied, i.e., iterative alternation of the variational mean-field approximation for data-dependent statistics and the stochastic approximation procedure for data-independent statistics, and parameters update. Let H = {h M 1 , h M 2 , h P where \u03a9 = {\u03bc M 1 , \u03bc M 2 , \u03bc P 1 , \u03bc P 2 , \u03bc S 3 } is a mean-field parameter set with given a fixed model parameter \u0398, it is straightforward to estimate the mean-field parameters \u03a9.\nThe learning proceeds by iteratively alternating the variational mean-field inference to find the values of \u03a9 for the fixed current model parameters \u0398 and the stochastic approximation procedure to update model parameters \u0398 given the variational parameters \u03a9. Finally, the shared feature representations can be obtained by inferring the values of the hidden units in the top hidden layer from the trained MM-DBM."}]