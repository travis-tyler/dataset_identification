[{"section_title": "Abstract", "text": "Identifying associations between genetic variants and neuroimaging quantitative traits (QTs) is a popular research topic in brain imaging genetics. Sparse canonical correlation analysis (SCCA) has been widely used to reveal complex multi-SNP-multi-QT associations. Several SCCA methods explicitly incorporate prior knowledge into the model and intend to uncover the hidden structure informed by the prior knowledge. We propose a novel structured SCCA method using Graph constrained Elastic-Net (GraphNet) regularizer to not only discover important associations, but also induce smoothness between coefficients that are adjacent in the graph. In addition, the proposed method incorporates the covariance structure information usually ignored by most SCCA methods. Experiments on simulated and real imaging genetic data show that, the proposed method not only outperforms a widely used SCCA method but also yields an easy-to-interpret biological findings."}, {"section_title": "Introduction", "text": "Brain imaging genetics, which intends to discover the associations between genetic factors (e.g., the single nucleotide polymorphisms, SNPs) and quantitative traits (QTs, e.g., those extracted from neuroimaging data), is an emerging research topic. While single-SNP-single-QT association analyses have been widely performed [17] , several studies have used regression techniques [9] to examine the joint effect of multiple SNPs on one or a few QTs. Recently, bi-multivariate analyses [6, 12, 7, 18] , which aim to identify complex multi-SNPmulti-QT associations, have also received much attention.\nSparse canonical correlation analysis (SCCA) [14, 19] , a type of bi-multivariate analysis, has been successfully used for analyzing imaging genetics data [12, 6] , and other biology data [4, 5, 14, 19] . To simplify the problem, most existing SCCA methods assume that the covariance matrix of the data to be the identity matrix. Then the Lasso [14, 19] or group Lasso [6, 12] regularizer is often solved using the soft-thresholding method. Although this assumption usually leads to a reasonable result, it is worth pointing out that the relationship between those variables within either modality have been ignored. For neuroimaging genetic data, correlations usually exist among regions of interest (ROIs) in the brain and among linkage disequilibrium (LD) blocks in the genome. Therefore, simply treating the data covariance matrices as identity or diagonal ones will limit the performance of identifying meaningful structured imaging genetic associations.\nWitten et al. [19, 20] proposed an SCCA method which employs penalized matrix decomposition (PMD) to yield two sparse canonical loadings. Lin et al. [12] extended Witten's SCCA model to incorporate non-overlapping group knowledge by imposing l 2;1 -norm regularizer onto both canonical loadings. Chen et al. [3] proposed the ssCCA approach by imposing a smoothness penalty for one canonical loading of the taxa based on their relationship on the phylogenetic tree. Chen et al. [4, 5] treated the feature space as an undirected graph where each node corresponds to a variable and r ij is the edge weight between nodes i and j. They proposed network based SCCA which penalizes the l 1 norm of to encourage the weight values u i and u j to be similar if r ij > 0, or dissimilar if r ij < 0. A common limitation of these SCCA models is that they approximate X T X by identity or diagonal matrix. Du et al. [7] proposed an S2CCA algorithm that overcomes this limitation, and requires users to explicitly specify non-overlapping group structures. Yan et al. [21] proposed KG-SCCA which uses l 2 norm of to replace that in Chen's model [4, 5] . KG-SCCA also requires the structure information to be explicitly defined. Note that an inaccurate sign of r ij may introduce bias [10] .\nIn this paper, we impose the Graph-constrained Elastic Net (GraphNet) [8] into SCCA model and propose a new GraphNet constrained SCCA (GN-SCCA). Our contributions are twofold: (1) GN-SCCA estimates the covariance matrix directly instead of approximating it by the identity matrix I; (2) GN-SCCA employs a graph penalty using data-driven technique to induce smoothness by penalizing the pairwise differences between adjacent features. Thorough experiments on both simulation and real imaging genetic data show that our method outperforms a widely used SCCA implementation [19] 5 by identifying stronger imaging genetic associations and more accurate canonical loading patterns."}, {"section_title": "Preliminaries", "text": ""}, {"section_title": "Sparse CCA", "text": "We use the boldface lowercase letter to denote the vector, and the boldface uppercase letter to denote the matrix. The i-th row and j-th column of M = (m ij ) are represented as m i and m j . Let X = {x 1 ; \u2026; x n } \u2286 \u211d p be the SNP data and Y = {y 1 ; \u2026; y n } \u2286 \u211d p be the QT data, where n, p and q are the subject number, SNP number and QT number respectively.\nThe SCCA model presented in [19, 20] is as follows: (1) where the two terms and originate from the equalities and , where and approximate and to simplify computation. This simplification approximates the covariance matrices X T X and Y T Y by the identity matrix I (or sometimes a diagonal matrix), assuming that the features are independent. Most SCCA methods employ this simplification [3-5, 12, 19, 20] . Besides, \u2225u\u2225 1 \u2264 c 1 and \u2225v\u2225 1 \u2264 c 2 induce sparsity to control the sparsity of canonical loadings. In addition to the Lasso (l 1 -norm), the fused Lasso can also be used [5, 14, 19, 20] ."}, {"section_title": "Graph Laplacian", "text": "The Graph Laplacian, also called the Laplacian matrix, has been widely used in the spectral clustering techniques and spectral graph theory [2] , owing to its advantage in clustering those correlated features automatically. We denote a weighted undirected graph as G = (V; E; W), where V is the set of vertices corresponding to features of X or Y, E is the set of edges with e i,j indicating that two features v i and v j are connected, and w i,j is the weight of edge e i,j . Here we consider G as a complete graph and thus every two vertices are connected.\nFormally, the adjacency matrix of G is defined as: (2) Generally, w i,j is set to |r i,j | d , where r i,j is the sample correlation between the i-th and j-th variables. In this work, for simplicity, we set d = 2, i.e.\n. It can also be decided by domain experts in other applications.\nLet D be a diagonal degree matrix with the following diagonal entries: [8] . L has many merits such as the symmetry and the positive semi-definite structure. Most importantly, it can map a weighted graph onto a new space such that connected vertices stay as close as possible."}, {"section_title": "GraphNet based SCCA (GN-SCCA)", "text": "Inspired by the Graph Laplacian [11] and the GraphNet [8] technique, we define the penalty P(u) and P(v) as follows: where L 1 and L 2 are the Laplacian matrices of two complete undirect graphs defined by the sample correlation matrices of the SNP and QT training data, respectively. The terms u T L 1 u and v T L 2 v make each feature fair be penalized smoothly according to the correlation between the two features.\nApplying the two penalties above, the GN-SCCA model takes the form: (4) where the terms \u2225u\u2225 1 \u2264 c 3 and \u2225v\u2225 1 \u2264 c 4 are used to induce sparsity; and the P(u) and P (v) are Graph Laplcaian based GraphNet constraints [8] . Note that we use instead of , which is typically used in other models, and thus our model takes into consideration the full covariance information.\nUsing Lagrange multiplier and writing the penalties into the matrix form, the objective function of GN-SCCA is as follows: (5) where (\u03bb 1 , \u03bb 2 , \u03b2 1 , \u03b2 2 ) are tuning parameters, corresponding to (c 1 ,c 2 ,c 3 ,c 4 ). Take the derivative regarding u and v separately and let them be zero: (6) ( 7) where D 1 is a diagonal matrix with the k 1 -th element as , and D 2 is a diagonal matrix with the k 2 -th element as 6 .\nSince D 1 relies on u and D 2 relies on v, we introduce an iterative procedure to solve this objective. In each iteration, we first fix v and solve for u, and then fix u and solve for v. The procedure stops until it satisfies a predefined stopping criterion. Algorithm 1 shows the pseudocode of the GN-SCCA algorithm."}, {"section_title": "Convergence Analysis of GN-SCCA", "text": "We first introduce Lemma 1 described in [13] . 6 If or , we approximate it with or , where \u03b6 is a very small non-zero value. According to [13] , this regularization will not affect the result when \u03b6 \u2192 0."}, {"section_title": "Du et al. Page 4", "text": "Brain Inform Health (2015) . Author manuscript; available in PMC 2016 January 01."}, {"section_title": "Algorithm 1 GraphNet based Structure-aware SCCA (GN-SCCA)", "text": "Require:\nCanonical vectors u and v.\n1:\nonly from the training data;"}, {"section_title": "2:", "text": "while not converged do"}, {"section_title": "3:", "text": "while not converged regarding u do"}, {"section_title": "4:", "text": "Calculate the diagonal matrix D 1 , where the k 1 -th element is ;\n5:\n6: end while"}, {"section_title": "7:", "text": "while not converged regarding v do"}, {"section_title": "8:", "text": "Calculate the diagonal matrix D 2 , where the k 2 -th element is ;\n9:\n10: end while 11: end while 12: Scale u so that \u2225Xu\u2225 2 = 1;\n13: Scale v so that \u2225Yv\u2225 2 = 1.\nLemma 1-The following inequality holds for any two nonzero vectors \u0169, u with the same length,\nLemma 2-For any real number \u0169 and any nonzero real number u, we have (9) Proof: The proof is obvious, given Lemma 1, \u2225\u0169\u2225 1 = \u2225\u0169\u2225 2 and \u2225u\u2225 1 = \u2225u\u2225 2 . Brain Inform Health (2015) . Author manuscript; available in PMC 2016 January 01."}, {"section_title": "From", "text": "Step 5, we denote the updated value as \u0169. Then we have\nAccording to the definition of D 1 , we obtain (10) Then summing Eq. (9) and Eq. (10) on both sides, we obtain Let , , , we arrive at (11) Thus, the objective value decreases during Phase 1:\n(2) Phase 2: For Steps 7-10, v is the variable to estimate. Similarly, we have (12) Thus, the objective value decreases during Phase 2: .\nApplying the transitive property of inequalities, we obtain . Therefore, Algorithm 1 decreases the objective function in each iteration.\nWe set the stopping criterion of Algorithm 1 as max{|\u03b4| | \u03b4 \u2208 (u t+1 \u2212u t )} \u2264 \u03c4 and max{|\u03b4| | \u03b4 \u2208 (v t+1 \u2212v t )} \u2264 \u03c4, where \u03c4 is a desirable estimate error. In this paper, \u03c4 = 10 \u22125 is empirically chosen in the experiments."}, {"section_title": "Experimental Results", "text": ""}, {"section_title": "Results on Simulation Data", "text": "We used four simulated data sets to compare the performances of GN-SCCA and a widely used SCCA implementation [19] . We applied two different methods to generate these data Du et \nBrain Inform Health (2015) . Author manuscript; available in PMC 2016 January 01.\nwith distinct structures to assure diversity. The first two data sets (both with n = 1000 and p = q = 50, but with different built-in correlations) were generated as follows: 1) We created a random positive definite group structured covariance matrix M. 2) Data set Y with covariance structure M was calculated by Cholesky decomposition. 3) Data set X was created similarly. 4) Canonical loadings u and v were created so that the variables in one group share the same weights based on the group structures of X and Y respectively. 5) The portion of the specified group in Y were replaced based on the u, v, X and the assigned correlation. The last two data sets (with different n, p, q and built-in correlations) were created using the simulation procedure described in [5] : 1) Predefined structure information was used to create u and v. 2) Latent vector z was generated from N(0, I n\u00d7n ). 3) X was created with each According to Eqs. (6-7), six parameters need to be decided for GN-SCCA. Here we choose the value of tuning range based on two considerations: 1) Chen and Liu [4] showed that the results were insensitive to \u03b3 1 and \u03b3 2 in a similar study; 2) The major difference between traditional CCA and SCCA is the penalty terms. Thus their results will be the same if small parameters are used. With this observation, we tune \u03b3 1 and \u03b3 2 from small range of [1, 10, 100] , and tune the remaining ones from 10 \u22121 to 10 3 through nested 5-fold crossvalidation.\nThe true signals and estimated u and v are shown in Fig. 1 . The estimated canonical loadings u and v of GN-SCCA were consistent with the ground truth on all simulated data sets, while SCCA only found an incomplete portion of the true signals. Shown in Table 1 are the cross-validation performances of the two methods. The left part of the table shows that GN-SCCA outperformed SCCA consistently and significantly, and it has better test accuracy than SCCA on testing data. The right part of Table 1 presents the area under ROC (AUC), where GN-SCCA also significantly outperformed SCCA on all data sets. These results demonstrated that GN-SCCA identifies the correlations and signal locations more accurately and more stably than SCCA."}, {"section_title": "Results on Real Neuroimaging Genetics Data", "text": "We used the real neuroimaging and SNP data downloaded from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database to assess the performances of GN-SCCA and SCCA. One goal of ADNI is to test whether serial MRI, positron emission tomography, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early AD. Please see www.adni-info.org for more details.\nBoth the SNP and MRI data were downloaded from the LONI website (adni.loni.usc.edu).\nThere are 204 healthy control (HC), 363 MCI and 176 AD participants. The structural MRI scans were processed with voxel-based morphometry (VBM) in SPM8 [1, 15] . Briefly, scans were aligned to a T1-weighted template image, segmented into gray matter (GM), white matter (WM) and cerebrospinal fluid (CSF) maps, normalized to MNI space, and smoothed with an 8mm FWHM kernel. We subsampled the whole brain and yielded 465 voxels spanning all brain ROIs. These VBM measures were pre-adjusted for removing the effects of the baseline age, gender, education, and handedness by the regression weights derived from HC participants. We investigated SNPs from the top 5 AD risk genes [16] and APOE e4. In total we have 379 SNPs in this study. Our task was to examine correlations between the voxels (GM density measures) and genetic biomarker SNPs.\nShown in Table 2 are the 5-fold cross-validation results of GN-SCCA and SCCA. GN-SCCA significantly and consistently outperformed SCCA in terms of identifying stronger correlations from the training data. For the testing performance, SCCA did not do well possibly due to over-fitting, while GN-SCCA consistently outperformed SCCA. Fig. 2 shows the heat maps of the trained canonical loadings learned from cross-validation. We could observe that both weights, i.e. u and v, estimated by GN-SCCA were quite sparse and presented a clear pattern which could be easier to interpret. However, SCCA identified many signals which could be harder to explain. The strongest genetic signal, identified by GN-SCCA, was the APOE e4 SNP rs429358; and the strongest imaging signals came from the hippocampus. They were negatively correlated with each other. This reassures that our method identified a well-known correlation between APOE and hippocampal morphometry in an AD cohort. These results show the capability of GN-SCCA to identify biologically meaningful imaging genetic associations."}, {"section_title": "Conclusions", "text": "We proposed a GraphNet constrained SCCA (GN-SCCA) to mine imaging genetic associations, and incorporated the covariance information ignored by many existing SCCA methods. The GraphNet term induces smoothness by penalizing the pairwise differences between adjacent features in a complete graph or an user-given graph (correlation matrix used in this study). Our experimental study showed that GN-SCCA accurately discovered the true signals from the simulation data and obtained improved performance and biologically meaningful findings from real data. In this work, we only did comparative study between GN-SCCA and a widely-used SCCA method [19] . We have observed many recent developments in structured SCCA models. Some (e.g., [6, 18, 19, 14, 12] ) ignored the covariance structure information of the input data, which was usually helpful to imaging genetics applications. A few other models (e.g., [7, 21] ) overcome this limitation but impose different sparsity structures. Work is in progress to compare the proposed GN-SCCA with these structured SCCA models. Given the mathematically simple formulation of GN-SCCA, we feel it is a valuable addition which is complementary to the existing SCCA models. Table 2 5-fold nested cross-validation results on real data: The models learned from training data were used to estimate the correlation coefficients for both training and testing cases. p-values of paired t-tests between GN-SCCA and SCCA are shown. "}]