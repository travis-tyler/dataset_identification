[{"section_title": "", "text": "ation. Challenges arise when true disease status, commonly referred to as the gold standard, is not available due to cost constraints, ethics issue, or a lack of necessary biotechnology. In traditional diagnostic test setting, many methodologies have been proposed to address this issue (van Smeden et al., 2013; Collins and Huynh, 2014) . For example, Zhou et al. (2005) proposed a nonparametric approach to evaluating ordinal tests without a gold standard, which generalized the first model introduced by Hui and Walter (1980) . Qu et al. (1996) and Albert et al. (2001) introduced random effects to allow for dependence structure among the tests. Several authors explored Bayesian approach (Branscum et al., 2008 (Branscum et al., , 2015 Wu et al., 2016) . Wang et al. (2011) , Wang and Zhou (2012) , discussed the evaluation when the gold standard is ordinal to capture additional severity or subtype information. Different longitudinal models were proposed for repeated test measurements (Cook et al., 2000; Jones et al., 2012) .\nThere are three major issues when applying these methods to biomarker evaluation. First, these methods were developed for binary or ordinal tests, yet most biomarkers are continuous. Categorizing biomarkers can result in a loss of important information. In fact, Albert and Dodd (2004) showed that different models can become indistinguishable when only a small number of binary tests are used. Moreover, many biomarkers lack an established threshold for use in delineating clinically meaningful categories. Current methods for continuous tests without a gold standard usually require strong parametric assumptions, such as multivariate normality. Secondly, existing work rarely discusses the inclusion of covariates. Some exceptions can be found in Bandeen-Roche et al. (1997) , Branscum et al. (2008) and Pfeiffer et al. (2008) , where researchers included covariates to allow for a varying prevalence. However, covariates can affect accuracy of the assessments in at least one other way: they may influence the magnitude and accuracy of test results in different subgroups. These effects are prevalent among biomarker studies, as most biomarkers values can be highly sensitive to the protocol and assay applied, as well as other inter-individual differences such as age and gender. A pooled analysis ignoring covariates can therefore lead to biased accuracy estimates (Pepe, 2003; Janes and Pepe, 2009 ). Inclusion of covariates that affect test results also allows researchers to relax the conditional independence assumption. In fact, this approach is generally preferred in biomarker evaluations, as compared with a random effects approach. This is due to the potential for covariates to explain, more explicitly, the mechanism by which dependence occurs, and to show how biomarker levels vary with covariates, as well as how their diagnostic performance differs among subpopulations -all are crucial issues in biomarker evaluations. However, due to computa-tional difficulty and identifiability issues, only a limited body of work exists (Branscum et al., 2015; Huang and Bandeen-Roche, 2004) , and the methods are restricted to a single biomarker with covariate effect only in the disease group, or including only a small number of categorical covariates, or to impose modeling constraints, such as assuming constant covariate effect across disease groups. Thirdly, biomarker studies usually have slightly different foci than traditional diagnostic testing studies: In addition to assessing test accuracy, biomarker studies often aim to propose novel biomarkers to combine with, rather than to replace, current markers. Nevertheless, biomarker variation across subgroups introduces another key question, regarding how to synthesize the information to achieve more personalized diagnoses.\nOur research is motivated by such questions in AD biomarker studies. A definite diagnosis of AD needs a neuropathological brain autopsy, which requires proper medical resources and can only be conducted after a patient has died and provided consent. Moreover, AD-related pathophysiological changes are believed to begin 10 years or more before any detectable clinical symptom, and decades before sufficient cognitive impairment accrues to warrant a clinical diagnosis of AD -an event that usually occurs 5 to 30 years before a patient's death (Jack Jr et al., 2010) . Therefore, all other difficulties aside, merely having sufficient follow-up data to make a potential gold standard diagnosis is challenging. On the other hand, this long timegap provides an opportunity for early diagnosis and intervention. Guided by the amyloid hypothesis (Selkoe, 1991) , researchers seek to use biomarkers to detect preclinical AD patients before extensive neuron damage and synaptic loss occurs. Additionally, biomarker information can be useful for monitoring AD progression, as well as for evaluating new AD treatments and recruiting specific subsets of patients for inclusion in AD clinical trials. Existing studies often rely on clinical AD diagnoses as the gold standard in their evaluations, due to inadequate levels of follow-up data. Additionally, existing knowledge about risk factors and covariates that affect biomarker level is often ignored. In this paper, we propose a latent profile approach to simultaneously handling the above issues. The approach further provides a risk score based on multiple biomarkers, risk factors and subjects' characteristics, and therefore provides the opportunity for a more personalized diagnosis.\n2. Model. Let D denote the unobserved true disease status. For binary disease status, D = 1 indicates disease and D = 0 nondisease. For ordinal disease status, D = 0, . . . , L \u2212 1 denotes level of severity, with higher value of D indicating more severe status. Let T = (T 1 , . . . , T K ) de-note K continuous biomarkers. We assume that for all biomarkers, a higher value is more indicative of a more severe status. A latent profile model (or a finite mixture model with continuous component distributions) (Lazarsfeld and Henry, 1968) \nwhich is usually referred to as the latent structure part, and the conditional density\nwhich is usually referred to as the measurement part. The conditional independence assumption among the biomarkers within each severity group is commonly adopted to simplify the joint measurement model \u03b3 d ( t) as its univariate products K k=1 \u03b3 dk (t k ). When this assumption does not hold, a random effect approach can be used to account for the dependence (Qu et al., 1996; Albert et al., 2001) .\nTo extend this model to incorporate covariate information, let X = (X 1 , . . . , X q ) denote the covariates that may affect biomarker values within each severity group and Z = (Z 1 , . . . , Z p ) denote the covariates related to disease prevalence. The elements in X and Z can be overlapping or mutually exclusive. We assume that the biomarker values are conditionally independent, given the true disease status D and the covariates X. In other words, we assume that the possible dependence among biomarker values within each disease severity group can be explained by covariates X, such as some subjects' characteristics. Under this assumption, the latent profile model with covariates is given as follows:\nA natural way to model the varying prevalence\n, where i = 1, . . . , N is the subject ID and X denotes the covariate space. The model for the latent structure part is:\nwhere D = 0 is the baseline group with parameters \u03b1 0 = (\u03b1 00 , . . . , \u03b1 0p ) = (0, . . . , 0). A transformation regression to describe the measurement part T k | D, X is defined as:\nwhere G(\u00b7; \u03bd k ) is a distribution function with parameter \u03bd k , H k (\u00b7; \u03bb k ) is a prespecified monotonic function with parameter \u03bb k . When the error distribution is assumed normal, the transformation accounts for possible skewness of the biomarker values within each severity group. Then, the latent profile model with covariates is given as follows:\nThe advantage of this model is that it allows for a varying disease prevalence based on risk factors and simultaneous inclusion of differential covariates that affect biomarker values. Currently, such models are limited and often require constant covariate effects to ensure model identifiability (Huang and Bandeen-Roche, 2004) . In contrast, we allow covariate effect \u03b2 kd to depend on both biomarker index k and disease status d, that is, covariates can have differential effects among biomarkers and among disease groups.\nTo better understand this improvement, we consider an example when D is binary. The covariate-specific receiver operating characteristic (ROC) curve based on the measurement model is:\nIf covariate effects are assumed to be the same for every disease group with only intercept terms differing, i.e., \u03b2 kd = {\u03b2 kd0 , \u03b2 k1 , . . . , \u03b2 kq }, the ROC curve no longer depends on covariates. In other words, when D is binary, assuming a constant covariate effect across disease groups is equivalent to assuming that the covariate-specific ROC curves are the same as the marginal ROC curves, which is rarely the case in practice. A similar conclusion can be made about generalized disease status, D. Justification for model identifiability with this flexible structure is provided in section 4."}, {"section_title": "Estimation.", "text": "3.1. An EM Algorithm. The maximum likelihood estimates of parame-\n. . , L \u2212 1 and k = 1, . . . , K} in model (2.2) can be obtained by directly maximizing the observed data likelihood function, or by considering the true disease status as missing and applying the EM algorithm to the complete data likelihood function below: Details of the EM algorithm can be found in the appendix. In particular, the maximization in the M step can be broken down into two disjoint parts to simplify the computation, one for \u03b1 d and the other for \u03bb k , \u03b2 kd and \u03bd k . In addition, these two maximizations can be rewritten as a maximization of a weighted polytomous regression likelihood and a maximization of a weighted transformation regression likelihood. As a result, computational routine in standard statistical package can be adopted.\nAs mentioned before, the proposed model does not require constraints on the covariate effects across disease groups. However, if such constraints are desired based on scientific knowledge, we discussed an easy way to incorporate them in the design matrix without having to do a constrained maximization.\n3.2. Biomarker Accuracy and Combination. After obtaining parameter estimates of the model, accuracy measures of each biomarker can be easily derived. For example, sensitivity, specificity and the covariate-specific ROC curve can be calculated based on P (T k | D, X). Positive predictive value, negative predictive value and other measures can be obtained with additional quantity P (d). Furthermore, the model based risk P (D i | T i , X i , Z i , \u03b8), obtained by applying Bayes' Formula to P (T k | D, X) and P (d), provides a potential way for biomarker combination to improve accuracy. This risk score not only summarize diagnostic information from multiple biomarkers when a gold standard is not available, but also takes into consideration risk factors and subjects' characteristics, hence offering a more personalized diagnosis. However, if one wishes to evaluate the accuracy performance of the combination rule, one needs to have an independent test sample. Random starting values: if prior information on disease severity group is available, the prevalence is generated accordingly. Otherwise, randomly generate L \u2212 1 numbers a 1 , . . . , a L\u22121 from the standard uniform distribution U (0, 1). Set the prevalence as p = {a 1 , a 2 \u2212a 1 , . . . , a L\u22121 \u2212a L\u22122 , 1\u2212a L\u22121 } and generate disease label for each subject from a multinomial distribution with probabilities p. Then the initial values are obtained as previously described.\n3.3.2. Scaling the biomarker values. Scaling T k by the nth root of the Jacobian determinant det J of transformation H k can often make computation more stable, especially when T k 's have very different ranges or shapes. For example, when the Box-Cox transformation (Box and Cox, 1964 ) is used, the scaling is\nOther than numerical stability, this scaling also makes the Jacobian term in equation (7.3) disappear:\n3.3.3. Spurious local maximizers. If the error distribution in the measurement model is allowed to be heteroscedastic among disease groups, i.e.\n\u223c N (0, \u03c3 2 kd ), a relatively large local maximum can occur as a consequence of fitting a disease group having only a few, and very close observations (therefore the variance in the denominator of the likelihood function becomes very small). The resulting parameter estimates are usually called spurious local maximizers. This problem is due to the fact that the likelihood function for a heteroscedastic mixture model can be unbounded (Lehmann and Casella, 1998) .\nWhen the likelihood is unbounded, the maximum likelihood estimates do not exist as a global maximizer. This does not invalidate our method, because maximizing the likelihood function is not our goal; rather, it is to find estimates that are consistent. In this situation, there still exists a sequence of roots of the likelihood function corresponding to local maxima for our model, and they are consistent, efficient and asymptotically normal. Further discussion can be found in Redner and Walker (1984) , Cheng and Traylor (1995) . As a result, consistent estimates can still be obtained via an EM algorithm. We obtain the maximizers of l 2 by finding the roots of its derivative l 2 instead of direct maximization to avoid diverging to the unbounded area. After reaching convergence, one needs to examine the relative size or the variance of the fitted components to exclude possible spurious local maximizers. This can also be handled by imposing constraints in the maximization, such as requiring that for each given k,\nwhere R is a pre-specified positive constant that restricts the relative size of the component variances. We did not use the constraint approach because (1) in diagnostic settings spurious local maximizers can be easily identified with very minimal information about prevalence; (2) unconstrained maximization is easier and faster than constrained ones; and (3) it can be useful to examine all results because sometimes a fit with dissimilar components is not a spurious solution.\n4. Model Identifiability. Due to an inherent \"label switching\" problem (the distribution remains identical if the labels of the latent groups are switched) and that the likelihood functions of these models usually have multiple local maxima, research on latent variable model identifiability has mainly focused on local identifiability, which considers whether the likelihood can uniquely determine a set of parameter values in its neighborhood. Specifically, a model is a function p = f ( \u03b8 ) that maps points in the parameter space into the data space. The model is locally identifiable at \u03b8 0 \u2208 \u0398 if there exists some neighborhood U \u03b8 0 of \u03b8 , such that\nThis suggests that F is locally invertable in U \u03b8 0 . By the weak inversion theorem, this is equivalent to the Jacobian matrix J( \u03b8 0 ) having full column rank.\nFor latent class models, McHugh (1956) proposed sufficient conditions for local identifiability of models with dichotomous observed variables. Goodman (1974) extended these conditions to polytomous variables. Here we borrow these results to obtain identifiability conditions for models with covariates.\nThe link function \u03b7(\u00b7) in the structure model and transformation H k (\u00b7; \u03bb) are both monotonic functions and thus do not affect model identifiability. Therefore, without loss of generality, we assume that they are both the identity function. Model (2.1) can then be rewritten as follows,\n. . , J k } be the column vectors of the Jacobian matrix J of the corresponding latent class model without covariates. Ex-plicit expressions of these vectors are not necessary for the following derivation, but can be found in Wang (2013) ; Wang and Zhou (2014) . When covariates are included, by the chain rule, we have \u2202f /\u2202\u03b1 d = \u2202f /\u2202\u03c0 d \u00d7 \u2202\u03c0 d /\u2202\u03b1 d and \u2202f /\u2202\u03b2 dkj = \u2202f /\u2202\u03b3 dkj \u00d7 \u2202\u03b3 dkj /\u2202\u03b2 dkj . Therefore the column vectors of the Jacobian matrix of model (4.1) is\nTherefore, the Jacobian matrix J * of model (4.1), has similar pattern as the Jacobian matrix J of the corresponding latent class model without covariates, but has N times as many rows as J. By applying condition (20-21) in Goodman (1974) , we have the following theorem:\n. . , J k } all together are linearly independent for some X 0 \u2208 {X 1 , . . . , X N } and Z 0 \u2208 {Z 1 , . . . , Z N }; and (iv) Design matrix X and Z both have full rank.\nThe first condition simply requires the degrees of freedom in the data is greater than the number of free parameters in the model, a necessary condition for model identifiability. The second condition means that all biomarker values in the domain are potentially observable. Conditions (iii) and (iv) guarantee that column vectors A d and B kjd are linearly independent, which means the model Jacobian matrices J * has full column rank. Therefore, the model is locally invertable at \u03b8 0 ."}, {"section_title": "Simulation Studies.", "text": "5.1. Regular settings. This section assessed model performance under various correctly specified settings. In all simulations, we chose 3 biomarkers (K = 3) and 3 disease categories (L = 3). In the latent structure model, we assumed that a binary variable, Z \u223c Bernulli(0.5), affected the disease prevalence P (\nWe chose \u03b1 = (\u03b1 10 , \u03b1 11 , \u03b1 20 , \u03b1 21 )=(-0.5, 1, -1, 1.5). This parameter value resulted in prevalence p \u2248(0.51, 0.31, 0.19) among subjects with Z = 0 and p \u2248(0.23, 0.38, 0.38) among subjects with Z = 1, where p = (P (D = 0 | Z), P (D = 1 | Z), P (D = 2 | Z)). As a result, subjects with Z = 0 were most healthy with about 20% having severe conditions, whereas risk factor Z = 1 leads to more subjects having mild or severe conditions.\nWe used the Box-Cox and the Yeo-Johnson transformations for H k (\u00b7; \u03bb k ). We assumed that a normally distributed variable, X \u223c N (0, 1), affected the biomarker performance via regression model H k (T ik ) =\u03b2 kd0 +\u03b2 kd1 X i + ik , d = 0, 1, 2, with \u03bb = 1, 0.5 or 0. This can be reparameterized as\nError distributions were generated as ik \u223c N (0, 0.5 2 ) for all biomarkers. Two sets of coefficient values were used: \u03b2 k =(5, -1, 1, 2.5, 0.5, 1.5) representing biomarkers with good discriminating ability, and \u03b2 k =(5, -1, 0.5, 1, 0.5, 1.5) representing biomarkers with fair discriminating ability. For the first scenario, the covariate-specific AUC for discriminating D = 0 versus D = 1 is \u03a6(\n2+X) (\u03a6 denotes the standard normal cumulate distribution function) with an average value of the covariate-specific AUC in this population being \u03a6( Simulation results for biomarkers that have good or fair diagnostic performances with the Box-Cox transformation were shown in Table 1 and Table  2 . Results for the Yeo-Johnson transformation are similar (not shown). In each scenario, we considered sample sizes N = 500 and N = 800. All results were based on 1000 simulation replicates.\nThese results suggest that the proposed method converged to the true parameter values. In the current settings, parameters \u03bb k and \u03b1 d can be well estimated with a sample size of 500. Increasing the sample size to 800 did not change the results much. On the other hand, bias in the estimates for \u03b2 kd decreased when the sample size increased. This is because, in our model \u03b2 kd was assumed to differ across biomarkers and disease groups. The effective sample size for estimating \u03b2 kd was less than that for estimating \u03bb k and \u03b1 d . In addition, the biases for \u03b2 kd and \u03c3 k are smaller with smaller \u03bb k . Comparing these two tables, the strength of biomarkers' diagnostic ability seem to reduce the bias and standard error of the estimates. Note that bias Table 1 Mean and standard error (in parentheses) estimates based on 1000 simulations for biomarkers with good performance. and standard error appears to be smaller for \u03b2 k2 and \u03b2 k3 in Table 2 as compared to Table 1 because the magnitude of the true parameter values are different between these two tables (in order to have biomarkers with different discriminating ability).\nIn the simulations reported in Table 1 and Table 2 , although we used the same parameters to generate biomarker results for all three biomarkers, we allowed \u03b2 kd to be different in the estimations. Therefore, the results displayed here represent the model's performance when biomarkers had different diagnostic accuracy. The same parameters \u03b2 kd in the data generation were used for the sake of clarity in the presentation, and to help examine the possible effects of different transformations on the estimates. As a reference, Table 6 in Appendix B shows the results with one good biomarker and two fair biomarkers."}, {"section_title": "Model mis-specfications.", "text": "This section investigates the model performance under two types of mis-specified situations: deviations from the conditional independence assumption and inclusion of non-informative biomarkers.\nFor deviations from the conditional independence assumption, we used the same parameter specifications as Table 2 in the previous section, but adopted a multivariate error term to induce additional correlations among the biomarkers. We considered positive or negative correlation between two of the biomarkers, as well as among all three biomarkers, after adjusting for all covariates. Results on mean bias and mean square error (MSE) were Table 2 Mean and standard error (in parentheses) estimates based on 1000 simulations for biomarkers with fair performance. These results suggest that, residual correlations can lead to biased AUC estimates and the biases are not diminishing even with increased sample size. On the other hand, magnitude of the biases is minimal, for example, even for correlation 0.8, the average bias in AUC estimates are less than 8%, suggesting the proposed model has adequate robustness with respect to this type of violation.\nWe also investigated the effect of including non-informative biomarkers in the model. The results are shown in Table 3 . We found that including noninformative biomarkers can lead to biased results and higher MSE. This effect is greater if more non-informative biomarkers are included. Larger sample size can help reducing these biases. Although the proposed model exhibit adequate robustness with respect to including a small number of non-informative biomarkers, we would like to emphasize that the model was not designed for variable selection. For application purposes of such model, the researchers should have adequate understanding of the biomarkers and select only the ones that have established predictive ability of the disease based on biological information or previous knowledge."}, {"section_title": "5.3.", "text": "Setting that mimics the real data application. This section conducted simulations that intend to mimic the settings in our real data application in Table 3 Simulation results of mean bias (\u00d710) and MSE (\u00d710) in parenthesis of AU C 1vs.0 estimates based on the proposed method. section 6. The model has two disease levels but more covariates, including categorical and continuous. The simulation parameters are specified according to the model estimates based on real data application. Results are shown in the bottom rows of Table 3 . We found that both bias and MSE are small, for example, the average bias is less than 0.6% even with a sample of size 300.\n6. Example: Application to Alzheimer's Disease Data. AD is a progressive and fatal neurodegenerative disorder that affected 5.2 million people in the United States in 2013. Without advanced therapy, this number is predicted to rise to 13.8 million by 2050 (Hebert et al., 2013) . The reason for the current lack of effective AD therapy is the decades-long preclinical stage where too much neurodegeneration occurs before any clinical diagnosis can be made. Therefore, great effort has been put into preclinical AD detection. Studies suggested that the CSF biomarkers total tau (t-tau), phosphorylated tau (p-tau 181p ), and amyloid \u03b2 1-42 (A\u03b2 42 ) changes about 15 years before clinical AD onset and are particularly relevant to tracking the pathological onset and preclinical stages of AD (Bateman et al., 2012) . Appropriate assessment and effective combination of these biomarkers for early AD detection can provide a critical opportunity for therapeutic intervention, disease monitoring and to lessen the time and cost for clinical trials. We applied the proposed method to avoid using imperfect clinical diagnoses and to take into account AD risk factors and patients' characteristics that may affect biomarker levels.\nOur analyses used data from the Alzheimer's Disease Neuroimaging Initiative (ANDI) (http://adni.loni.ucla.edu/). The ADNI study is a multicenter longitudinal observation study launched in 2004 in the United States and Canada. It is conducted to examine whether biomarkers, and clinical and neuropsychological assessments can be combined to measure the progression of mild cognitive impairment (MCI) and early AD patients. ADNI enrolled subjects from three groups: cognitively normal (CN) subjects, subjects with MCI, and subjects with early AD based on clinical assessments. Detailed information about the study's design and inclusion and exclusion criteria can be found at http://www.adni-info.org. Briefly, all participants were recruited between the ages of 55 and 90 years and had at least 6 years of education. Participants who took specific psychoactive medications, or who had other neurological disorders, were excluded. CSF biomarkers, including A\u03b2 42 , t-tau, and p-tau 181p , along with subjects' demographic and clinical information were obtained at baseline. After the baseline visit, subsequent visits occurred at 6-or 12-month intervals but mostly without additional CSF measurements.\nWe applied the proposed model to compare and combine CSF biomarkers, A\u03b2 42 , t-tau, and p-tau 181p , for detecting preclinical AD related pathophysiological changes, which we will call the AD signature. Our analysis used a cross sectional dataset at baseline of all currently available ADNI subjects as of April 7th, 2015 with at least one CSF biomarker measurement. There were 808 subjects (223 CN, 457 MCI and 127 early AD). Two latent disease groups were used for this problem, defined by the presence or absence of an AD signature. CSF A\u03b2 42 , t-tau, and p-tau 181p are believed to reflect different processes in AD pathology (Storandt et al., 2012) , so we assumed the correlation among them within subjects with the same AD signature status can be explained by covariates. Covariates in the latent structure model included risk factors age, presence of ApoE 4 alleles and education. Covariates in the measurement model were age, gender and the interaction term between age and D. Thus, age could have different impacts on biomarkers among subjects with or without an AD signature. We used the Box-Cox transformation for H k (\u00b7; \u03bb k ). A total of 790 subjects with complete biomarker and covariate information were included in the analysis. Results were shown in Table  4 , where D1 stands for D = 1, denotes the group with an AD signature. Confidence intervals are obtained based on nonparametric bootstrap (Efron, 1981) with 1000 bootstrap replications. We note here that bootstrap confidence intervals are approximation of the actual confidence intervals (Efron, 1987) . Table 4 Estimates and 95% bootstrap CI (in parentheses) for CSF A\u03b242, t-tau and p-tau181p (Estimates in bold indicate a significant effect).\nAge ( These results are consistent with current knowledge that older age and ApoE 4 allele are risk factors for AD. Specifically, the results suggest that the log odds of having an AD signature is about 0.7 (95% bootstrap CI: 0.4, 0.9) higher for a 10-year increase in age, adjusting for ApoE 4 and education. Subjects with ApoE 4 alleles are estimated to have about exp(2.60)=13.5 fold higher odds of developing an AD signature (95% bootstrap CI: 9.3, 20.7), adjusting for age and education. Additionally, education seems to have a protective effect against AD related pathophysiological changes, but this effect was not significant based on these data. On the biomarker side, we found that subjects with AD pathology have significantly lower levels of A\u03b2 42 and higher levels of t-tau and p-tau 181p , consistent with the amyloid hypothesis. Normal aging process is revealed by lower level of A\u03b2 42 and ptau 181p , and higher levels of t-tau in older adults, suggesting that different diagnostic criteria should be considered for different age groups. Males apprear to have lower levels than women for all three biomarkers. In addition, the estimates for \u03bb were about 0.33 for A\u03b2 42 , -0.19 and -0.07 for t-tau and ptau 181p , which challenge the conventional assumption that A\u03b2 42 has normal distribution, and that tau measures are log normal. Based on the model, one can compute the estimated covariate-specific ROC curves. As an example, we plotted the estimated age-specific ROC curves for CSF biomarker A\u03b2 42 , t-tau and p-tau 181p in Figure 1 . The plots suggest that the ROC curves differ among age groups for both t-tau and ptau 181p , with higher AUCs for younger groups. This results also suggest that including covariates in the model and allowing for the covariate effect to vary between disease groups are important. On the other hand, the age-specific ROC curves for A\u03b2 42 do not vary much among these age groups. One explanation is that A\u03b2 42 changes much earlier than tau in AD pathophysiology; it is possible that, at this stage it is already relatively stabilized.\nCombinations of CSF biomarkers have not been well examined due to the unobserved AD signature. Current research mostly uses CSF A\u03b2 or tau protein separately or CSF tau/A\u03b2 ratio as measures to classify subjects with or without an AD signature. In addition to assess the diagnostic accuracy of these biomarkers, our model suggested a way to synthesize information from different biomarkers to reach a potentially better diagnosis. Specifically, we can use the model based risk P (D i | T i , X i , Z i , \u03b8) as a score for diagnosing whether AD related pathophysiological change has started. This risk score summarizes diagnostic information from multiple biomarkers, risk factors and subjects' characteristics, and therefore offers a more personalized diagnosis.\nDue to the unobserved gold standard, we evaluated the risk scores against clinical diagnoses at the last available follow-up using a subset of individuals who enrolled during the first phase of ADNI study (ADNI1: 2004 (ADNI1: -2010 and thus have relatively longer follow-up. The risk score results in an AUC of 0.78, higher than that based on any one of the biomarkers (A\u03b2: 0.75, t-tau: 0.73, p-tau 181p : 0.72). The improvement is more evident at the clinically relevant region of the ROC plot, where the false positive rate is low and true positive rate is high (plot not shown). For example, when choosing cut-points that give 0.8 in specificity, sensitivity for detecting an AD signature based on A\u03b2 42 , t-tau or p-tau 181p is 0.59, 0.55 and 0.57, respectively; while sensitivity based on the risk score is 0.69. It is worth noting that the reference standard used here, the clinical diagnosis, is imperfect. This can mask the good performance of the biomarkers especially when they are in fact superior to the clinical diagnosis (Pepe, 2003) . Additionally, since the clinical diagnosis is likely to miss preclinical AD subjects without manifested symptoms, it will erroneously decrease the specificity estimate of the biomarkers. Nevertheless, these results partially validate the advantage of biomarker combination based on the proposed model. Additionally, we found that the risk scores are well-separated in the population, as shown in Figure 2 . The histogram of the risk scores put most of the subjects either in the very low risk end or in the very high risk end of the distribution. Only 7.7% of the subjects had an estimated risk between 0.2 and 0.8, or 13.2% between 0.1 and 0.9. This suggests that the model based-risk had good discriminant ability to separate subjects with and without an AD signature. By contrast, the histograms for single biomarkers put most subjects in the middle range. We also compared the estimated proportions of subjects that have an AD signature (according to model estimates) with their clinical diagnoses. The results are shown in Table 5 . It is evident that the proportion of subjects with an AD signature increased from about 31% in the early MCI group to 86% in the AD group, representing a progression. Additionally, there were about 26% of subjects in the CN group that had an AD signature according to the model. This portion likely indicates that the subjects exhibit preclinical AD pathophysiological changes but still appear to have normal cognition. \u226516 years: 9-11; 8-15 years: 5-9 ; 0-7 years: 3-6) for EMCI, (\u226516 years:\u22648; 8-15 years: \u22644; 0-7 years: \u22642) for LMCI.\n7. Summary and Discussion. In this paper we proposed a latent profile model for assessing the accuracy of continuous biomarkers or diagnostic tests without a gold standard. We did not discuss the choice of number of classes, because in diagnostic testing studies, the number of disease classes is often apparent and governed by the scientific questions. Compared with currently available methods, this model has several advantages. First, the model can include covariates in both the latent structure model and the measurement model. The former can help to examine the impact of risk factors on constructs that are difficult to measure, such as the pathophysiological process in a preclinical AD brain. The latter can be helpful in understanding biomarkers' performances in subgroups of patients with different characteristics. This is especially valuable for obtaining a more personalized diagnostic procedure and for achieving higher diagnostic accuracy for each individual subject. Second, the model for covariate effect is much more flexible -there is no constraint on covariate type (i.e., categorical or continuous); additionally, covariates can have different impacts on different biomarkers and within different disease severity groups. Third, the transformation on the biomarker values attempts to account for possible skewness. Last, the method can deal with categorical disease status. An R package \"latentreg\" has been developed for the proposed methods to accommodate more general data structures and research questions.\nWe present the model in the situation where the latent disease status is binary or ordinal. However, this assumption is only used to identify a unique solution among several equivalent ones caused by the \"label switching\" problem in latent class or latent profile models. There is no additional constraint imposed for the estimation. Therefore, this assumption is not essential if there are other pieces of information that can help to determine the labeling of the latent disease groups -for example, the group labels can be determined if the sizes of the latent groups are known. As a result, when additional information is available to label the groups, the proposed methods can be applied to situations where the latent disease status is nominal. In this case, different intervals of the biomarker values do not translate to severity information but indicate nominal disease status, such as subtypes. As a result, it is possible for subjects with higher or lower biomarker levels to have easy to treat disease subtypes than whose with intermediate biomarker level, and vice versa.\nWhen applying latent variable models, one should realize that all these models merely offer a tool to \"cluster\" the population into groups in which the manifest variables (biomarkers) are relatively homogenous. These models cannot directly identify disease groups. In other words, these models do not guarantee the resulting clusters are grouped according disease status of interest. In our application, we chose biomarkers that are biologically relevant to AD pathophysiology, so that disease status was the fundamental factor that lead to the heterogeneity of the biomarker values. However, one should carefully evaluate this assumption in each application.\nOne possible extension to the proposed method is to use a semi-parametric transformation with an unspecified H instead of a Box-Cox transformation. In this case, the transformation H is a nuisance parameter with infinite dimensions, the maximum likelihood approach did not apply. One possible choice is to use estimating equations in each M step to estimate H. This type of algorithm (sometimes referred to as an EM-like algorithm) has been used in practice and appears to lead to reasonable results (Benaglia et al., 2009 ). However, one need to prove its convergence and consistency of the results since estimating equations uses a different criterion than maximizing the likelihood and no longer guarantee the non-decreasing property in a typical EM algorithm.\nThe proposed model assumes that the dependence among biomarkers can be explained by disease status and other covariates. Therefore, another extension to the model is to introduce random effects if one suspects that there are remaining correlations due to unobserved covariates. While more complicated model structures may be relatively easy and perhaps tempting to adopt (aside from the potential computational burden), one should be cautious when doing so as they are often hard to validate. Additionally, the identifiability issue for such models must be resolved before adopting them in practice. tion, HSR&D Research Career Scientist Award , and NIH/NIA grant U01AG016976. Data used in preparation of this article were obtained from the ADNI database (http://adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how to apply/ADNI Acknowledgement List.pdf."}, {"section_title": "APPENDIX A", "text": "An EM algorithm. E step computes the expectation of the complete data log likelihood given the observed data:\nwhere superscript (t) denotes iteration number, g\nk ), and P (t) (d) is the expected value of I d (D i ) at current parameter values,\n.\nM step maximizes the expected complete data log likelihood given in (7.1). Let\nk (H d ) is concave, thus its maximization can be carried out by borrowing a polytomous regression routine that exists in many statistical softwares. Our computation utilizes the R function \"multinom\" in the package \"nnet\".\nThe second part of the maximization function, l 2 (\u03bb\nkd , \u03bd\nk ), given in equation (7.3), is also a proper log likelihood function. In fact, it is the log likelihood function of a transformation regression model with N \u00d7 L \u00d7 K observations, weights P k (\u00b7). In fact, the original N \u00d7 K observations can be considered replicated L copies with 1 copy for each disease severity group. This log likelihood function takes the total N \u00d7 L \u00d7 K observations, but weights them according to the probability of a subject belonging to a disease severity group P k (t ik ) at the tth EM iteration. In addition, let W be a N \u00d7 L by N \u00d7 L diagonal matrix W (t) = diag{P (t) (0), . . . , P (t) (L \u2212 1)}, where P (t) (d) = (P Maximization can be performed with these stacked matrices as if D is known.\nThe above formulation with a design matrix also makes it easy to impose constraints on the covariate effect \u03b2 kd . For example, if some of the covariates are assumed to have the same effect across disease groups, one can simply remove the corresponding interaction terms between D and these covariates in X st . Similarly, if the design matrix and outcome vectors for each k are stacked, and the interaction terms between the covariate effects and the biomarkers are included, constraints on the covariate effects across different biomarkers can easily be imposed. Here, we do not further complicate the design matrix since the constraints on covariate effects across different biomarkers are used less commonly. Table 6 Mean and standard error (in parentheses) estimates based on 1000 simulations for biomarkers with different performances. "}, {"section_title": "APPENDIX B", "text": ""}]