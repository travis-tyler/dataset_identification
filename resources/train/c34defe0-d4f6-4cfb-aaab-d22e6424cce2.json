[{"section_title": "Abstract", "text": "Brain extraction or skull stripping is a fundamental procedure in most of neuroimaging processing systems.\nThe performance of this procedure has had a critical impact on the success of neuroimaging analysis. After several years of research and development, brain extraction still remains a challenging problem. Brain morphology and intensity characteristics are variable and complex, usually because of the variability in conditions of image data acquisition, or abnormalities in data such as tumor regions. These difficulties prevent brain extraction methods from producing acceptable results. In this paper, we propose an effective method for skull stripping in Magnetic Resonance Imaging (MRI) scans named ASM-CNN. Our system is a combination of Active Shape Model (ASM) and Convolutional Neural Network (CNN), taking full advantage of these two methods to achieve remarkable results. Instead of working with 3D structures, we process 2D image sequences in sagittal plane. First, we divide images into different groups such that, in each group, the shapes and structures of brain boundaries have similar appearances. This allows developing precise algorithms for each group in order to produce high performance segmentation results. Second, a modified version of ASM is used to detect the brain boundary in images by utilizing prior knowledge of each group.\nFinally, CNN and the post-processing methods such as Conditional Random Field, Gaussian Process and some special rules are applied to refine segmentation contour produced by ASM. We compared ASM-CNN with the latest version of five state-of-the-art, publicly available methods, namely BET, BSE, 3DSS, ROBEX and BEAST. The evaluation was carried out by using three public datasets IBSR, LPBA and OASIS. The experimental results show that the proposed method outperforms five states-of-the-art algorithms, surpassing all the other methods by a significant margin in all experiments."}, {"section_title": "Introduction", "text": "Whole brain segmentation is the problem of extracting brain regions from volumetric data, such as Magnetic Resonance Imaging (MRI) or Computed Tomography (CT) scans. The results of this process is segmentation map indicating brain regions after removing non-brain tissue such as eyes, fat, bone, marrow, and dura. Brain extraction is the first step in most neuroimaging analysis systems which usually consists of brain tissue classification and volumetric measurement [1] , template construction [2] , and cortical and sub-cortical surface analysis [3] . Early preprocessing steps such as bias field correction can also benefit from brain extraction [4] . Therefore, there is a need for high performance brain extraction methods that can produce accurate segmentation results.\nAutomatic skull stripping is a solution to replace manual brain delineation for the purpose of reducing processing time and preventing any kind of human bias in the results. This is especially true in large scale studies, where thousands of images with different characteristics and significant anatomical variations are examined. Most skull stripping methods are optimized and validated for MRI T1-weighted images, since high resolution T1-weighted structural images are prevalent in clinical studies [5] . Furthermore, T1-weighted images provide excellent contrast between different brain tissues, making it the leading imaging standard for volumetric measurements [6] . However, segmentation on T1-weighted data is generally a daunting task due to the complex nature of the images (ill-defined boundaries, low contrast) and the lack of standard in image intensity. Several methods have been proposed in the recent years. However, these methods show good performance only on certain datasets, they produce low quality results when the acquisition conditions or study populations change [5] . Existing brain extraction methods can be divided into four categories, namely edge-based, templatebased, label fusion with atlas-based and non-local patch based [6] . Edge-based methods focus on detecting edges between brain and non-brain regions by considering differences in appearance of these structures.\nThere are several techniques have been employed such as watershed [7] , level set [8] , histogram analysis [9] , morphological filtering [10] , region growing [11] , edge detection [12] , graph cuts [13] , and convolutional neural networks [14] . Although these methods have proven its effectiveness and achieved comparable results, their performance tend to be less accurate when working with pathology, different sites, scanners, and imaging acquisition protocols.\nTemplate-based methods register the subject to a template via affine transform [15] or deformable models [16] to create an initial estimate for the brain mask. After that, the boundary of brain mask is segmented again by a classifier, which helps increase the accuracy of final result. Templates can involve one or more distinctive atlases. Template-based methods are robust, stable in different conditions and highly accurate.\nLabel fusion with atlas-based techniques, such as Multi-Atlas Propagation and Segmentation (MAPS) [17] , Advanced Normalization Tools (ANTs) [18] , and Pincram [19] , implement registration of multiple atlases to a target subject by using deformable models. After being registered to the target space, the brain masks in all atlases are combined together by using Simultaneous Truth And Performance Level Estimation (STAPLE) [20] or joint label fusion. Because the main operations of these approaches are registration process, their performance depends on the accuracy of registration and quality of brain mask in each atlas. In addition, the variability representation in brain anatomy usually requires large number of atlases, hence these methods usually are time-consuming and computationally intensive.\nThe last category is non-local patch based methods. At first, these methods transform atlases to subject space by using affine registration in order to estimate the initial brain mask of the subject. Then, a neighborhood searching process for each small patch, which is around the initial estimates of brain boundary, is performed. Patches are derived from the registered atlases and located within the neighborhood of target patch. Then the patches are associated together and similarity weights are computed to generate the final brain mask. Several methods such as Brain Extraction using non-local Segmentation Technique (BEAST) [21] , Multi-cONtrast brain STRipping (MONSTR) [6] are inspired by this ideas and achieved remarkable performance in aspects of both accuracy and robustness. However, one difficulty of this approach is pinpointing optimal setting for parameters such as number of atlases or window size. Additionally, in researches related to disease study, the brain regions in MRI scans usually contain lesion tissues. Therefore, atlases from T1-weighted images may not be optimal for detecting brain boundaries since intensity values of structures such as hemorrhages, tumors, or lesions may be similar to that of non-brain tissues. To overcome this problem, non-local based methods use different MR acquisition protocols to obtain complementary information. As a result, it is a complex task and requires more processing time.\nIn this research, we present a novel approach for brain extraction in T1-weighted MRI data. Our system is a combination of Active Shape Model (ASM) [22] and Convolutional Neural Network (CNN) [23] , hence the name ASM-CNN. Unlike existing methods, our approach consider brain extraction problem as a segmentation task for 2D image sequences in sagittal plane instead of working with 3D structure. This approach has several benefits. First, it allows developing specific algorithms when dealing with images which have different brain boundary silhouettes. Along image sequences, the shape of brain boundaries and brain sizes vary significantly. Especially for sagittal slices located at the beginning and ending parts of 3D MRI volumes, the brain regions are small and the boundaries are very complex. Based on prior knowledge about brain structures, we developed specific rules representing the relationships between brain tissues. These rules was applied directly to control the segmentation process effectively. Second, images in sagittal plane are symmetry across two hemispheres. By utilizing this property, we are able to predict general shape of brain mask based on the positions of slices. This property also enables us to establish more extensive and accurate rules for segmentation.\nASM-CNN comprises three main stages: dividing images into groups, applying ASM to detect the brain boundaries, and finally using CNN also post-processing methods based on Conditional Random Field (CRF), Gaussian Process (GP) and special rules to refine segmentation contour. In the first stage, all images in sagittal plane are divided into three groups, namely group I, II, and III. The number of groups was decided according to our analysis on boundaries of brain masks. The divide of images is performed by a Support Vector Machine (SVM) classifier and a set of rates learned from data, for the purpose of determining position for each group. In group II and III, images have similar shapes and structures while group I consists of images which have small brain sizes and complex structures. In the next stage, ASM is applied to group II and group III to estimate initial contours for brain mask. After that, these contours are refined using CNN before fed into post-processing based on CRF to obtain the final segmentation. For group I, because the shapes of brain are complex and the sizes of brain regions are smaller than that in other groups, we take the segmentation result of the slice in group II which is right next to the beginning position of group I and use it as an initial brain mask. This brain mask is then fed into CNN for further analysis with specific rules and GP to produce high quality results. The flowchart of our system is given in Figures 1 . The main contributions of the proposed method are:\n1. Our approach is based on processing 2D images, this helps increase accuracy in segmenting small-sized brain regions while preserving stable performance in overall segmentation.\n2. CNN with high-level feature representations is utilized to refine voxels around brain boundaries. In addition, global spatial information of each voxels is combined with features from CNN by using deep neural networks. In this way, the proposed method is able to represent global and local features simultaneously.\n3. ASM is applied to different groups. The similarity of brain contours in each group allows ASM to detect the general properties of boundaries, guaranteeing the geometry attributes of object thus enhancing the accuracy of segmentation results.\n4. Finally, our framework does not depend on any specific MRI format so that it can be applied effectively in various acquisitions conditions. The remainder of this paper is organized as follows. In section 2, we introduce several notations, and present the design and algorithms of our system. In section 3, we report the experimental results, where the proposed method was applied to three public datasets, namely IBSR, LPBA, OASIS, and compared with five state-of-the-art brain extraction methods. Section 4 gives further analysis regarding important issues of the proposed method and discussions for future research. The conclusions of this research are presented in Section 5. "}, {"section_title": "Methodology", "text": "In neuroimaging, a three-plane coordinate system is used to describe the standard anatomical position of human body. These imaging planes are transverse plane, sagittal plane, and coronal plane. Transverse plane is an X-Z plane and parallel to the ground. Sagittal plane is an Y-Z plane and perpendicular to transverse plane, separating the left side and the right side. Coronal plane is a Y-X plane and perpendicular to sagittal plane, separating the front and the back. In our method, we choose to process 2D sagittal slices because human brains are nearly symmetrical with respect to the mid-sagittal plane. This property enable us predict the general shapes of brain regions more accurately. In the following subsections, we describe three main stages of the proposed method for brain extraction."}, {"section_title": "Dividing sagittal slices into groups", "text": "The first stage of the algorithm is dividing sagittal slices into groups. The rules for division is based on our analysis on shapes of brain regions. Slices whose brain regions have similar shapes will be put in the same group. Figure 3 indicates some images in group I. The areas of brains in this group are small compared to the sizes of images. Figures 4 and 5 illustrate images in group II and III, respectively. Brain sizes in these groups are relatively large, and in group III, brain regions extend out at the lower left corner of images. Due to the symmetry of brain, from the first to the last sagittal slice, the assigned groups of slices would be from group I to group II, then group III, to group II again and then group I. Before presenting our method in this stage, we introduce the notations used to describe the algorithms:\n\u2022 P = {p 1 , p 2 , ..., p m }: the set of people in each dataset, p i (1 \u2264 i \u2264 m) is the ith person.\n\u2022 N = {n 1 , n 2 , ..., n m }: the set that indicates the numbers of 2D images of a person P where n j (1 \u2264 j \u2264 m) is the number of 2D images of p j .\n\u2022 I pj = {I pj 1 , I pj 2 , ..., I pj nj }: the set of images of person p j where I pj k (1 \u2264 k \u2264 n j ) is the kth image of p j .\n\u2022 G = {G 1 , G 2 , G 3 }: image groups where G q (1 \u2264 q \u2264 3) is the set of images in group q for all people.\n\u2022\nrepresent the set of images in group I, group II, and group III respectively for person p h where a+b+c = n h .\n\u2022 M 12 , M 23 : the models trained by SVM for classify images between group I and group II, group II and group III.\n\u2022 R 1 , R 2 , R 3 , R 4 : the rates used for estimating the positions of slices in each group.\nIn this stage, we train an SVM classifier for the dividing task (Algorithm 1), in which the feature used for training is the histogram of oriented gradients (HOG) [24] extracted from a sub-rectangle. The sub-rectangle in Algorithm 1 is illustrated in Figure 6 , which indicates the differences between brain shapes in different groups. Figures 6a and 6b are two adjacent slices from group II and group I, respectively. As shown in this figure, the brain in 6a has a small tail at the lower left part, while the brain in 6b does not. It is apparent that the brain shapes are significantly different even though the slices are adjacent. Similarly, 6c and 6d are two adjacent slices from, respectively, group II and group III. The brain shape in group II is more convex, while that in group III is concave at the lower right part. We utilize these differences to maximize the classification performance of SVM. Algorithm 2 is used to divide sagittal slices of any MRI volumes into groups, using the SVM model produced by Algorithm 1. In the next stage, we use ASM to estimate initial contours for brain masks of slices in group II and group III. For group I, because the shapes of brain are complex and the sizes of brain regions are smaller than that in other groups, we need specific algorithms for this group. The algorithms for brain extraction in group I are detailed in subsection 2.4. \nRec \u2190 Extract rectangle containing the skull of I using Algorithm5 "}, {"section_title": "Active shape model with optimal feature", "text": "Recently, ASM has been applied successfully in several researches related to medical imaging [25, 26, 27] .\nIn our system, we utilize the similarity of brain shapes of images in the same group by applying ASM with specific model to each group, producing rough segmentations of brain regions which will be refined later.\nFor enhacing the modeling capacity of the gray-level variations around the border of the object, we use a modified version of ASM called Active Shape Model with optimal feature (ASM-OF) [22] . In particular, the optimal displacements for landmarks in original ASM is replaced by using a nonlinear kNN-classifier instead of the linear Mahalanobis distance [28] . We denote some annotations, which are used in section below for describing how ASM-OF is applied to segment each brain image in each group .\n\u2022 s: the number of training images.\n\u2022 n: the number of landmark points used to represent each image.\n\u2022 n s : the number of new positions evaluated during searching.\n\u2022 k: the number of points in profile located either inside and outside of the landmark point.\n\u2022 l max : the number of resolution levels for appearance model.\n\u2022 n grid : size of n grid \u00d7 n grid of points, which are sampled in training process.\n\u2022 n max : the number of iterations per resolution level.\n\u2022 k N N : number of neighbors used in k-NN classifier during searching and feature selection.\n\u2022 f v : part of variance to be explained by the shape model, determining number of modes.\nThis stage comprises three subroutines. First, the ASM-OF create a shape model from landmarks in the training set. Second, a k-nearest neighbor classifier (k-NN, [29] ) is constructed using a gray-level appearance model. Finally, the shape model and k-NN classifier are combined to produce preliminary estimations of brain masks. The detail of each subroutine is described below."}, {"section_title": "Shape Model", "text": "A brain image in s images is represented by vector x i by stacking n landmark ((x 1 , y 1 ), . . . (x n , y n )) as\nPrincipal component analysis (PCA) is used to calculate the mean shape x s and covariance matrix C, the eigenvectors \u03c6 m (m = 1, . . . , t) correspond to the first t largest eigenvalues of the covariance matrix, and \u03c6 m is the respective variances \u03bb m as\nwhere \u03c6 m and \u03bb m is calculated by singular value decomposition. When brain shape in the training set can be approximated by\nwhere \u03a6 s = (\u03c6 1 . . . \u03c6 t ) is the first t eigenvectors, and b s is a set of shape parameters which is calculated as\nIn experiment, b s is usually bounded by\nwhere q varies in the interval [2, 3] . The value t eigenvalues to retain is chosen so as to explain a certain proportion f v of the variance in the training shapes, usually ranging from 90% to 99.5%. The desired number of modes is given by the smallest t such that"}, {"section_title": "Gray-level appearance model with optimal features", "text": "The gray-level appearance model that describes the typical image structure around each landmark is obtained by applying k-NN instead from pixel profiles, sampled by using linear interpolation around each landmark, perpendicular to the contour. From each training image and for each landmark a square grid of n grid \u00d7 n grid points is defined where n grid is an odd integer and the landmark point at the center of the grid. In our experiments, n grid is set to 5 hence a feature vector has 60 elements which are calculated at 25\npoints. The output of feature vector is 1 if point is inside the objects or 0 if outside. The k-NN classifier with weight to each vote is exp(\u2212d 2 ), where d is the Euclidean distance to each neighbor in the feature space.\nFor selecting the best feature, MannWhitney algorithm is used [30] .\nGiven an input image, each position along the profile is calculated yielding 60 feature images and processing again to create the optimal feature. These features are then fed into k-NN classifier to determine the probability of being inside the object for this pixel. Then we determine the point g i in the set of point of the profile g such that the object function f (g) is minimized\nwhere g i is oriented from the outside to the inside of the object, runs from \u2212k, to +k."}, {"section_title": "Evolution of the models", "text": "After both shape model and k-NN classifiers are constructed from training images, ASM-OF can be applied to segment object by performing the procedures listed below.\n\u2022 Step 1. Initialize the shape model with x s using (2).\n\u2022 Step 2. For each landmark, put it at 2n s + 1 new locations, evaluate the equation (7) with the k-NN classifier to find and move it to a new position denoted is x N ew .\n\u2022 Step 3. Fit the shape model by calculating b sNew using (4) as (8), and limiting the values of b sNew using (5) .\nwhere\n\u2022 Step 4. Update the shape landmarks using (3) as\nStep 5. Iterate steps 2 and 4 up to predefined n max times\nBecause ASM-OF is only able to capture the general shapes of brain region so the derived boundaries are very smooth. Therefore, in the next stage, we apply convolutional neural networks to refine the preliminary estimations from ASM-OF for the purpose of localizing accurate brain boundaries."}, {"section_title": "Convolutional neural networks", "text": "Convolutional neural networks (CNNs) are developed from multi-layer perceptrons, which allow exploiting spatial information by using localized convolutional kernels. The main advantage of CNNs is the ability to learn complex, nonlinear and high-dimensional mappings from large data. This attribute makes CNNs achieve remarkable successes in 2D medical image analysis [31, 32] . The typical structure of CNN consists of several pairs of convolutional, sub-sampling layers and multi-layer perceptron. The convolutional layers take input as receptive fields in the previous layer and calculate features while spatial information is preserved.\nDenote h l j is the j-th feature map of the l-th layer and h l\u22121 n (n = 1, . . . , N ) is the m-th feature map of the (l \u2212 1)-th layer, the feature map h l j is computed by equation:\nwhere W l jn is the convolutional kernel associated with n-th feature map of the previous layer; b l is the bias at the l th layer; and \u03c3 is the non-linear activation function.\nThe main operation of sub-sampling is pooling over local neighborhood to reduce the resolution of feature maps. Max-pooling layers are special cases of sub-sampling in which non-max suppression and down-sample the resolution of the feature maps are performed. The multi-layer perceptron with fully connected layers is usually followed several convolutional layers and sub-sampling layers. The final layer of the multi-layer perceptron are values, which are the posterior probability for each class with softmax function.\nIn our approach, a CNN is used as a classifier to refine the brain contour produced by ASM-OF. Unlike the recent approach using CNNs, our method focuses on pixels located around boundaries of the preliminary brain masks produced by ASM-OF in the previous stage, instead of processing for all pixels in image. In addition, we also exploit the global spatial information [33] and combine it with feature maps learned from a CNN as a input for multi-layer perceptrons. The details of construction for our CNN are described below. and III, position of pixels around the boundary of M i (i \u2208 {2, 3}) with distance 5 pixels are detected by using algorithm 4 ( Figure 7 ). These position then are extracted information based on I j (j \u2208 {2, 3}). With images in I 1 , all pixels within the rectangle, which contains the skull, are used as features. The aim of employing a CNN is to classify pixels into brain and non-brain classes.\nWe extract two types of features for describing each pixel. The first type is local features, which are three adjacent image slices with size 11 \u00d7 11 centered around each pixel. The second type of features is global features, which are combining position (x, y) of each pixel and index z of image where pixel belongs in Figure 8 illustrate the feature extraction step and combine two features in network structure.\nIn the training process, the total of number brain and non-brain samples are approximately 4000000, 9000000 and 18000000 for IBSR, LPBA and OASIS dataset, respectively. The different in number samples between dataset is mainly caused by the number of subjects in each dataset. Figure 8 illustrates our network structure. It includes three convolutional layers followed by four fully connected layers, including the final two node layer for output. The size of convolutional layers is 3 \u00d7 3 and max pooling uses 2 \u00d7 2 kernel. We configures the depths of the first, second and third convolutional layers to 13, 26 and 39, respectively. The ReLU activation function is applied to the outputs of the convolutional layer."}, {"section_title": "CNN architecture", "text": "Three vectors from three slices of images obtained from convolutional layers are concatenated with the normalize coordinates to form a new vector. Then this vector is fed into the fully connected layers. The depths of four fully connected layers are 574, 300, 50, and 2, respectively. The final layer has the size of two, indicating the probability of the input pixel belonging to the brain or non-brain class."}, {"section_title": "Training and Optimization", "text": "The aim of CNN is to minimize the cost function:\nfor a labeled training set (x i , z i ), i \u2208 (1, n) and weights including convolutional kernel weights and bias weight (w 1 , . . . w L ) with a loss function l . The loss function l is defined as:\nwhere N is the number of training images in the batch, M is the number of classes, p m,n is the probability of the n the example being classified into the m-th class. If the n-th example is classified into the m-th class, B m,n equals 1, otherwise B m,n equals 0.\nIn equation (11), L 2 regularization is used to penalize the size of w with \u03b7 = 0.005 is the coefficient of regularization. The weights w t+1 at step t + 1 are updated by applied Adam algorithm [34] :\nwhere t is the iteration index, \u03b1 = 1e \u2212 04, \u03b2 1 = 0.9, \u03b2 2 = 0.999 and = 1e \u2212 08, m 0 = 0, v 0 = 0.\nThe weights in each layer at convolutional layer and are initialized from a normal distribution of N (0, 0.1) while weights in multi-layer perceptron layer are created from normal distribution of N (0, \u03b2) with\nwhere n hidden is the number of hidden layers of previous layer.\nThe CNN is trained by using mini-batch stochastic gradient descent with batch size of 128. For preventing over-fitting, we use dropout layer after each layer in fully connected layer with rate 0.5."}, {"section_title": "Special Rules for Group I", "text": "The brain regions in group I are small and have a lot of noise, there would be cases where CNNs produce two regions as segmentation results but it is difficult to automatically determine which area is the brain region. In this paper, we utilized Gaussian Process [35] to overcome this situation. Gaussian Process is used to to learn a line that changes center positions of the brain image from the beginning to the end. Based on this line we can predict the center of the cerebral images in region I. Then the closest area closest to the center of gravity is selected for processing. This line is learned based on the Gaussian Process, because it is able to learn complex functions by adjusting the kernel functions. \u2022 Because of the image acquisition condition, the image of a subject can be shifted, however, the rate of change of the center of the image along the slices remains unchanged. Therefore, it is possible to correctly predict the center of brain regions by translating the initial prediction by a value \u03b1.\n\u2022 The value \u03b1 is estimated based on a basis that, for each subject, we used a combination of ASM-OF, CNN, and CRF to find the positions of region II and III based on the proposed algorithm. Then we consider the center of all the images in region II and III. By compare the deviation with the result of the Gaussian Process and taking the mean value, we obtain the desired value of \u03b1. "}, {"section_title": "Post-processing", "text": "Because the main procedures are pixel-wise approaches, the segmentation maps are considerably noisy, decreasing overall accuracy of the brain extraction process significantly. To overcome this problem, we utilize a post-processing technique based on conditional random field (CRF) for validating the extraction results from ASM-OF and CNN, as well as finally refining the segmentation maps. Originally CRF was designed for text segmenting and labeling [37] , however there were several success in applying CRF to vision-based applications [38, 39, 40] , especially medical imaging [41, 42, 43] . In this research, we use fully connected CRF as the main framework for post-processing because of its capability in producing highly refined segmentation results.\nLet I be an image of size N and x be a segmentation map of I, the Gibbs energy of x is given by\nwhere x i , x j is the label assigned to pixel i, j, respectively. In our system, the properties of segmentation maps are x \u2208 L N , L \u2208 {0, 1} which are respective to \"brain\" (label 1) and \"non-brain\" (label 0) regions. The unary potential is given by \u03d5 u (x i ) = \u2212 log P (x i ), where P (x i ) is the probability of pixel i getting classified as \"brain\". This probability is acquired from the output of the main procedure. \nThe pairwise potential \u03d5 p (x i , x j ) is given by\nwhere \u00b5(x i , x j ) follows Potts model, which is\nThe weights are denoted as w m and k m (f i , f j ) is the Gaussian kernel depends on feature vectors f i and f j .\nSimilarly to [39] , we employ appearance kernel and smoothness kernel, which are respectively defined as\nwhere p i , p j are positions, I i , I j are intensity values of pixel i, j. The sigmas (\u03c3 \u03b1 , \u03c3 \u03b2 , \u03c3 \u03b3 ) are used to control the degrees of Gaussian kernels.\nThe most probable segmentation map x * is calculated by\nBecause finding the exact minimization is infeasible, the CRF distribution is approximated by a mean-field approximation. The detailed algorithm is presented in [39] . Besides performing mean-field approximation, we need to determine the values of parameters of the model. In our system, we hard-coded the values of w 2 and \u03c3 \u03b3 because their effects to segmentation accuracy were insignificant. The other parameters (w 1 , \u03c3 \u03b1 and \u03c3 \u03b2 ) were determined by random search with cross validation.\nFigures 11 and 12 illustrate all processing stages of the proposed brain extraction system."}, {"section_title": "Experiments", "text": ""}, {"section_title": "Methods for comparison and dataset", "text": "In this paper, we consider comparing our proposed system with five well-known methods, namely Brain Extraction Tool (BET) [44] , Brain Surface Extractor (BSE) [45] , 3DSkullStrip (3DSS) [46] , ROBEX [15] , and Brain Extraction based on nonlocal Segmentation Technique (BEAST) [21] . The implementations of all algorithms are optimized to work with T1-weighted data. BET is specifically designed for brain extraction tasks. This method utilizes deformable models based on locally adaptive forces to produce brain mask results. In the first stage, the center of the head is estimated. Then, the deformable model with a spherical mesh is applied to fit brain boundaries around this position. BET is very fast and simple but requires manual parameter settings to produce good results. In the experiment, we used BET 2.0, the latest version with several improvements. The next method for comparison is BSE. This method comprises several procedures: anisotropic diffusion filtering, edge detection, and morphological operators. Anisotropic diffusion filtering is used to reduce noise in images while preserving edge boundaries. Marr-Hildreth edge detection is employed to identify the border of the brain regions. Finally, morphological operators such as erosion and dilation is applied to ensure that the result is correct. Although BSE can provide high level of accuracy for whole-brain segmentations, fine parameter tuning is usually required for method to work effectively in specific studies. In this study, we used BSE in BrainSuite15c package, which was released in January 2016.\n3DSS is a modification of BET, included in the AFNI package [46] . It uses the spherical surface expansion paradigm, with several modifications to avoid the eyes and ventricles regions, reducing misclassification.\nFurthermore, not only points inside the surface, but also points located outside are combined together in order to support the evolution of the mesh. In our experiment, we used 3DSS in the AFNI package released on January 2016.\n[15] proposed a method called ROBEX. This approach is a combination of a discriminative and a generative model. The random forest classifier is used as a discriminative model to detect voxels located on the brain boundary. The generative model is based on a point distribution model to make sure the shape of the mask is reasonable. In the first stage, ROBEX registers the subject to a template via affine transform. The signal intensities are then normalized and applied bias correction before being fed into the discriminative model. ROBEX is able to work effectively across multiple datasets without requiring any parameter tunning.\nHowever, ROBEX uses an adult template as the standard for training the discriminative model, and the target subject is supposed to be aligned. This limits the flexibility of ROBEX in working with different conditions such as imaging modalities and young populations. For evaluating the performance of ROBEX, we used the ROBEX version 1.2 released in November 2013.\nThe last method used for comparison is BEAST. The ideas of this technique is based on non-local patch matching using multiple atlases. The sum of squared differences metric is used to determine suitable patches.\nFor optimization, input data and prior atlasyes library need to be normalized in terms of space and intensity.\nAn important characteristics of BEAST is that atlases library needs to be representative of the given data and therefore user is able to add custom priors to the library. When sets of library are selected appropriately, the method can achieve state-of-the-art performance for several data sets even when the data are related to disease such as Alzheimers Disease Neuroimaging Initiative [47] .\nThree publicly available datasets were used for evaluation. In each dataset, the samples were randomly divided into training and testing groups. The first dataset is Internet Brain Segmentation Repository (IBSR) our results comparable to [15] . Differently from IBSR and LPBA40, 20 out of 77 subjects were clinically diagnosed with very mild to moderate Alzheimers disease. Even though the brain masks are not created manually but by a customized method based on registration to an atlas, the results from this method were carefully checked by experts before releasing. This dataset is worthwhile by virtue of including scans from a very diverse subjects with a expansive age range in addition to diseased brains. On account of this, it is sufficient to use this dataset in order to prove the effectiveness and robustness of our approach.\nIn this research, 6 out of 20 subjects in IBSR dataset were used for training and the 14 remained subjects were for evaluating. Similarly, 12/28 is the number of training/testing subjects in LPBA40 dataset and 22/55 is the ratio for OASIS dataset."}, {"section_title": "Setup", "text": "The proposed method was implemented in MATLAB and Python. Applying ASM-OF as well as extracting features for CNN were done using MATLAB. The deep neural networks used for training and testing process was implemented in Python because of its various supported deep learning libraries e.g. TensorFlow.\nIn these experiments, we used the same deep neural network structure for all three datasets, as shown in Figure 8 . In the training process, the values of all weights of networks were randomly generated from a normal distribution with mean 0 and standard deviation 0.1. To overcome overfitting problem, we also added dropout layers with the rate 0. "}, {"section_title": "Evaluation Metrics", "text": "We evaluated the performance of our brain extraction method by comparing the segmentation results with the ground truth in each dataset. Among several popular metrics used for measuring the distance or similarity between two images, we used the Dice coefficient, the Jaccard index and the Average Haussdorff Distance (AHD). Furthermore, sensitivity and specificity scores were also calculated. The Dice coefficient is commonly used to measure repeatability in order to compare directly machinegenerated and ground truth segmentation. It is possibly the most-used measure for validating medical segmentations. The Dice coefficient is calculated by\nThe values of this coefficient is in the interval [0, 1] where the maximum value indicates that two segmentation map are identical.\nThe Jaccard index is another widely used similarity measure and closely related to the Dice coefficient.\nThe relationship of two metrics is represented by\nhence\nSensitivity and specificity are similar to each other. Sensitivity is essentially how good a prediction is at true positive. Analogously, specificity is a measure of how accurate a prediction is against false positives.\nHowever, it is not common to use these two measures for evaluation of medical image segmentation because of their sensibility to segments size. These metrics are defined as follows:\nDifferently from other metrics, the AHD is a spatial distance based metrics measuring dissimilarity between two segmentations, where one is the predicted result needs to be evaluated and the other is respective ground truth. These metrics are usually used when the overall accuracy, for instance the contour of the segmentation, of the result is of importance. Because the normal Haussdorff distance is sensitive to outliers, it is recommended to use the AHD which is known to be more stable. The AHD between two segmentation map X and Y is defined by\nwhere d(X, Y ) is the directed average distance given by"}, {"section_title": "Results", "text": ""}, {"section_title": "Qualitative evaluation", "text": "The segmentation results of all method including our proposed in a sagittal plane for IBSR, LPBA and OASIS are illustrated in Figure 13 -15 respectively. Each figure includes 6 typical testing scans from all three groups (2 scans for each group). Although ASM-CNN approaches the sagittal plane, it also works well and gives correct segmentation in two other planes, which are transverse and coronal plane. Figure 16 shows the comparison between our approach and other methods on each dataset for these two planes.\nFor the BET, it generally gives good results for almost testing samples. Missing the cerebellum in its segmentation is the crucial problem of this method. Even for scans in group III which are brains in a big size with not so complex structure, BET sometime provides false segmented result (see Figure 13 and Figure   15 ). This issue appears in IBSR and mostly in OASIS dataset which can be clearly seen in coronal plane in Figure 16 . For the LPBA samples it seems to be improved and gets better performance. However, comparing with other methods, it happens more frequent on BET. Moreover, Figure 16 shows that BET often leaves out the lateral ventricles of its segmented result along with BSE and 3DSS. Although fails in group II and III, this method can produce acceptable segmentations on images in group I (see Figure 15 ).\nBSE seems to have the same performance with BET method which also fails at cerebellum segmentation and does not include the ventricles. Different from other methods, sustainability is the main disadvantage of BSE. It can gives excellent results in LPBA dataset but very poor in IBSR and OASIS. This can be seen clearly in II scans in the same group III and same dataset in Figure 13 . BSE works well in the first scan although the dura matter and the cranium is oversegmented. However, for the second scan which has higher contrast in the same group III, this method completely fails to produce correctly result. One of the reasons for unstable segmentation is because we employed the default parameters. Hence carefully tuning the parameters is necessary when using BSE. Besides, Figure 13 also shows that this method sometime gives good segmentation for images in group I.\nThe extracted results obtained by 3DSS, which is a modified version of BET, are slightly better than BET and BSE. 3DSS produces great segmentation in LPBA and its performance decreases in IBSR and OASIS.\nUnlike BET, segmentation by 3DSS can avoid the eyes (see the second scan in Figure 13 ) and reduce leakage into the skull although there are some minor oversegmentation at temporal lobe of cerebral hemisphere (see the first scan in Figure 14 ). The same with BSE, this method leaves the ventricles out in its segmentation.\nHowever, the results are extremely sharp at the boundary which sometimes loss the interhemispheric tissue in the segmentation (see Figure 16) . Furthermore, the extracted brains in group I by 3DSS are also not good as those from BSE.\nBoth BEAST and ROBEX all have the higher performance than the aboved methods. They produce precise segmentations of cerebellum which BET and BSE often fails at. Besides, the ventricles, which are usually left out in 3DSS or BSE, are included in the segmentations. However, BEAST gives results sharp as those from 3DSS causing to decrease the accuracy. Moreover, the main problem of ROBEX is that it gives oversmoothed results, leading to inclusion of dura and gray matter loss [6] (see the second scan in Figure   16 ). Generally, both can provide accurate extracted brains but ROBEX seems to be slightly better than BEAST. Unfortunately, they both work bad for small size brains in group I, even worse than the aboved methods.\nFinally, ASM-CNN provides extremely accurate segmentations in all three datasets. The results obtained by our proposed are the same with those by ROBEX with smoothed boundaries. However we can still keep the gray and dura matter in our extracted brains in most of cases in which they are usually left out by ROBEX (see the second scans in Figure 16 ). Although there are some minor leakage into the skull in ASM-CNN, it happens least than ROBEX and BEAST with smaller oversegmentation. The critical impact of our method is that it can work precisely for the small size brains in group I where other methods usually fail at.\nWe observe in scans from this group in Figure 13 -15 that our extractions are identical to the groundtruth even for the tiny size brains (see two last scans in Figure 13 ). However the final result still has a few false negatives and false positive in some cases because of the complex structure in this group. In spite of some minor incorrect segmentation, ASM-CNN still has a better performance than others with higher accuracy."}, {"section_title": "Quantitative evaluation", "text": "We use five evaluation metrics which are Dice coefficient, Jaccard index, average Haussdorff distance, sensitivity and specificity to compare our method with others. For the sake of easy comparison, we evaluate all methods in 2D (sagittal plane) and 3D form for each dataset. Table 1 Because our approach is working with 2D scans, we formulated the result on 3D structure of other algorithms as 2D images sequences in sagittal plane for the reliable evaluating. Base on the result, all methods prone to getting higher scores on the LPBA dataset in which BSE has significant changes when working on this dataset such as the average of Dice coefficient of this method greatly increases about 9%\n(85% to 94.27%) for the LPBA scans. BEAST has the highest mean of Specificity with small SD for three datasets although there is no big difference between six methods at this evaluation metric. Surpassing all other algorithms, ASM-CNN gets the best performance with the highest average of Dice overlap (with 0.55 -1.63% higher than the second), Jaccard index (0.8 -2.45 % higher) and AHD on three datasets and the best Sensitivity on LPBA and OASIS while ROBEX produces the highest on IBSR. Besides, BSE also achieve the same AHD score on LPBA like ASM-CNN but its SD is higher than us (0.32 compared with 0.19). Furthermore, ASM-CNN not only has outstanding mean of evaluation metric scores but also gets the smallest SD values in Dice coefficient, Jaccard index and Sensitivity which can show the consistency of our method.\nAlthough process with 2D plane, we still create 3D results by combining sequences of scans to make ASM-CNN more comparable with other algorithms. There is a increase at the accuracy of five remain methods in this type of evaluation with a great improvement at standard deviation. Among the other algorithms, BEAST has major changes in its accuracy for several evaluated metrics on three datasets. For instance, it approximately gains more 3.47 -5.1% on Dice, Jaccard and Sensitivity metrics. Furthermore, BEAST still provides the highest Specificity which is almost 100% for all dataset. Meanwhile, ROBEX continually obtains the best scores in Sensitivity metric on IBSE and LPBA. Despite not our main approach, the 3D results produced by ASM-CNN are remarkable which can be competitive with others. It preserves the impressive performance which gets the best on 3 dataset at Dice coefficient, Jaccard index, AHD and on OASIS at Sensitivity.\nThe boxplots in Figure 17 and 20 shows that BSE has a big interquartile range (IQR) on IBSR which means the results given from it are more disperse and have larger variance than other algorithms. However this method has the best performance on LPBA ( Figure 18 , Figure 21 ) which gives extremely accurate results and better than others although it has a terrible outlier (about 72% at Dice overlap for 2D evaluation).\nIndeed, this issue was mentioned above that BSE can provide very accurate segmentations but also may give atrocious results which is based on tuning employed parameters. By contrast, ASM-CNN shows its durable and unaffected by datasets that outperforms than others with high accuracy and small IQR as well as SD on three datasets especially on OASIS ( Figure 19 , Figure 22 ). Unfortunately, its Specificity is worse than several methods. However it can be improved by employment a suitable post-processing method to mitigate several false positive in the segmentations. "}, {"section_title": "Discussion", "text": "In this research, we proposed a system for brain extraction using active shape model and convolutional neural network, hence the name ASM -CNN. The idea behind using ASM is to explore general structures of brain regions, ensuring the consistency of the shape of the object. However, in medical imaging we demand high precision boundaries for segmentation problems, while ASM only provides general shapes and quite smooth. Therefore, we implemented CNN to refine the boundaries produced by ASM. After this step, results from CNN may still be flawed, especially for small and complext brain regions. To overcome this challenge, we proposed some pre-processing techniques based on CRF and Gaussian Process as the final step. CRF with its ability to model the relationships between the pixels being predicted is used to produce a structured output effectively. Gaussian process is applied to learn a non -linear function indicating the changing of the cerebral images from the beginning to the end image due to its power in learning complex functions with a suitable kernel function.\nInstead of using whole 3D MRI volume, we decided to approach with 2D scans. This approach allows us to design special rules, adapted for identifying small and complex brain regions, which are often ignored by techniques that focus on processing 3D volume. In addition, we choose 2D scans in sagittal plane to process. Harnessing the symmetry of this plane, we can group brain images into pre-defined groups such that the images in each group have similar shapes. This property enables ASM to learn shape models and appearance model properly in place of learning all shapes, which change significantly when throughout the images.\nThe proposed method has been compared with five state-of-the-art brain extraction methods. By achieving the highest average of Dice coefficient and Jaccard index with smaller standard deviations, ASM-CNN surpassed all other methods in all experiments. The proposed method also got remarkable scores in other metrics, not only in 2D but also in 3D perspective as well. Despite the inferior result in Specificity, it is still sufficient when it made no significant difference between the top method and ours. ROBEX has shown that it was better than others with many great segmentations. However, the segmentation contours produced by ROBEX were not as sharp as the groundtruth, leading to the increase in false positive. It is also the main weakness of ROBEX when being tested with all three datasets. About BEAST and 3DSS, both methods performed great in general. Their segmentation results were regularly the same as the groundtruth in some cases and their average scores are approximate with each other. Nevertheless, it seemed that BEAST produced the more stable results than 3DSS which had fluctuation in its scores. For instance, there are some cases when 3DSS performed on OASIS dataset, it missed some region of cerebellum, as illustrated in Figure 15 . It is worth noting that BEAST was at top in Specificity with the highest average scores in all datasets. For BET and BSE, the quality of their segmentations is worse comparing with others but still acceptable. When dealing with the diversity of brain shapes and contrast in scans, BET and BSE produced some false segmented regions from cerebellum to brain stem which can be easily seen in Figure 13 . Their results were also unstable, especially when being tested with OASIS dataset. Moreover, it is shown that BSE has not been stable on IBSR with the highest standard deviation. On the other hand, BSE was surprisingly good on LBPA, albeit there was still one outlier which had an extremely low accuracy. It can be caused by parameters tuning because BSE is extremely sensitive to these values. For example, when default parameter values were used in LPBA, this tool has obtained superb results. But it was totally different when BSE was applied to other dataset like IBSR or OASIS.\nIt is worth noting that the properties of datasets affected the performance of brain extraction methods.\nDue to using two scanner to acquire MR images, IBSR comprises many heterogeneous scans with many obvious artifacts. Besides, this dataset also has the lowest resolution and most anisotropic voxels. LPBA has better resolution images with less noise but it is also fairly anisotropic. OASIS does not have lots of noises and it is isotropic, but it includes some diagnosed Alzheimers disease subjects. Therefore, all methods performed well on LBPA and got the results which did not have any clearly differences. But for other datasets such as IBSR and OASIS, ASM-CNN has shown its remarkable performance when comparing with others, especially in OASIS. It is because the contour refinement by CNN and post-processing by CRF and\nGaussian process have done their duty impressively which produced sharp brain boundaries as groundtruth.\nFurthermore, combining with specific rules for particular groups helped our method overcome the small brain region problems. Our result for this phenomenon is demonstrated in Figure 13 . It has to be noted that five other methods were run with the default parameters values. Hence, all the experimental evaluation can be reproduced.\nIn our system, ASM is used to keep the original structures. However, ASM can only work effectively when the shapes of the objects to be similar to the trained active appearance model. Therefore, the algorithm will likely produce poor results when processing unusual shapes. Unfortunately in medical imaging, the data are usually obscured by noise due to limitations of image acquisition systems, or the shapes and distributions of images are not inlcuded in training data. In such cases, ASM may produce severely imprecise boundaries even CNN cannot verify or refine. In the future work, we intend to study the techniques based on deep learning for the purpose of constructing better models for shape and apperance. In addition, with improvements in the future, such as GPU optimization, we believe that the proposed approach can be applied widely in clinical applications because of its fine and robust performance."}, {"section_title": "Conclusion", "text": "In this article, we proposed a novel method for brain extraction in magnetic resonance images, namely ASM-CNN. The method was named after its main components: Active Shape Model and Convolutional Neural Network. Unlike existing approaches, ASM-CNN processes 2D sequences of images instead of 3D brain structures. For each 2D scan, first we use a improved version of ASM is Active Shape Model with optimal feature to produce a rough estimate of the brain region, its boundary is then refined by a CNN, which is constructed and trained with several special rules. Finally, the brain region is post-processed by conditional random field and Gaussian process. The proposed approach has shown its consistency in performance, it can produce high accuracy segmentations in all cases, even when the brain regions are small and scattered. In the experiments, our method achieved remarkable Dice coefficients and Jaccard indexes for the whole three datasets (IBSR, LPBA and OASIS) in 2D scans as well as 3D structures, surpassed the performance of five other state-of-the-art methods (BET, BSE, 3DSS, BEAST and ROBEX). "}]