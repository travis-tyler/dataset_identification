[{"section_title": "E L E C T R O N I C P U B L I C A T I O N S A V A I L A B L E A T P U B L I C A T I O N S \u00c9 L E C T R O N I Q U E S D I S P O N I B L E \u00c0", "text": "w w w. s t a t c a n . g c . c a Survey Methodology, December 2012\nw w w. s t a t c a n . g c . c a Survey Methodology, December 2012\nw w w. s t a t c a n . g c . c a Survey Methodology, December 2012"}, {"section_title": "Introduction", "text": "This article h as been prep ared in recognition of Joe Waksberg's unique contributions and leadership in survey methodology. My first encounter with Joe's work was his article on res ponse errors in expe nditure surveys written with John N eter (Neter and Waksberg 1964). Am ong other things that article int roduced me to the cogn itive phenomenon called te lescoping. Later in life I had the opportunity to work with Jo e on the first conference and monograph on teleph one survey methodology where we were part of the edit orial group (Groves, Biemer, Lyberg, Massey, Nicholls and Waksberg 1988). We also collaborated on the p reparation of many of the Hansen Lectures that were pub lished in the Journal of Official Statistic s (JOS) during my term as its Chief Editor. Joe himself delivered the si xth lecture, which was publishe d in JOS (Waksberg 1998). Joe was a fantastic lea der and it is a great honor for me to have been invited to write this a rticle on survey quality, a topic that occupied his mind a lot. Many of my friends have conveyed their views or sent me materials in preparation of this article. Especially I want to th ank Paul Biemer, Dan Kasprzyk, Fritz Scheuren, Dennis Trewin, and Maria Bohata for helping me. Survey quality is a vague, albeit int uitive, concept with many meanings. In this article I discuss so me observations related to the deve lopment and treat ment of the concept over the last 70 years and for some developments it is possible to trace roots t hat can be found even farther back. Most of my discussion, however, concerns current issues in government statistical organizations. It is wit hin official statistics that m ost my survey quality examples take place. The article is organized as follows: In Section 2 I discuss the total survey error paradigm, including error typologies, treatment of th e errors, and survey design taking all error sources into account. In section 3 I discuss quality management philosophies that have had a larg e impact on survey organizations since t he early 1990's. This impact is manifested by methods and approaches like recognition of the user or the client, a d iscussion of costs and risks i n survey research, and the ne ed for organi zations to continuously improve. Section 4 provides examples of quality initiatives in survey organizations. Section 5 deals with the difficulties in measuring quality, either\nOver the last 30 years, su rveys have been increasingly used to explore sensitive topics (Tourangeau and Yan 2007). For example, data obtained from surveys have been used to investigate \"socially undesirable\" behaviors, such as the prevalence of illicit drug use (e.g., Botvin, Griffin, Diaz, Scheier, Williams and Epstein 2000;Fergusson, Boden and Horwood 2008), illegal abortion (e.g., Johnson-Hanks 2002; Varkey, Balakrishna, Prasad, Abraham and Joseph 2000), or alcohol consumption among adolescents (e.g., Strunin 2001; Zufferey, Michaud, Jeannin, Berchtold, Chossis, van Melle and Suris 2007). Such surveys have been commonly utilized in academic research and policy analysis (Davis, Thake, and Vilhena 2009), even though asking sensitive questions has generally been seen as p roblematic. The responses have been considered to be pr one to error and bias because respondents consistently underreport socially undesirable behaviors (Barnett 1998;Tourangeau and Yan 2007). Low response rates have been an additional concern. Those who are selected for a survey can simply refuse to take part in the survey or they can participate but refuse to answer the sensitive questions (Tourangeau and Yan 2007). Recent surveys at the household level have incorporated sensitive questions related to the extent of co ca growing areas (see e.g., Ibanez and Carlsson 2010). Coca is a native bush from the Amazon rainforest in South America from the leaves of which cocaine is extracted. Colombia's coca bush area represents 40%, Peru's 40%, and Bolivia's 20% of the total area under co ca cultivation worldwide, amounting to 154,100 hectares (UNODC 2011 ). In Peru and Bolivia, the leaves of this bush have been traditionally used for many purposes from around 3000 B.C. (Rivera, Au fderheide, Cartmell, Torres and Langsjoen 2005) until today. Those traditional uses mainly include coca chewing and coca tea drinking to overcome fatigue, hunger and thirst; and to relieve \"altitude sickness\" and stomach ache symptoms, respectively (Rospigliosi 2004). Since the 1970s, however, coca cultivation skyrocketed because of its use as the raw material for the production of cocaine (Caulkins, Reuter, Iguchi and Chiesa 2005). The cocaine content of the coca leaves is below 1%, and ranges from 0.13 to 0.86% (Holmstedt, Jaatmaa, Leander and Plowman 1977). Therefore narcotics traffickers need large quantities of coca leaves to obtain enough of the alkaloid for commercialization in the illegal market. In general, growing coca for the narcotics trafficking business is a profitable activity. In fac t, the income of a coc a growing farmer has been calculated to be 54% higher than the income of a non coca growing farmer (Davalos, Bejarano and Correa 2008). Consequently, coca-related research has become oriented towards evaluating the profitability of c oca versus other cash crops (see, e.g., Gibson and Godoy 1993;Torrico, Pohlan and Janssens 2005). Different attempts were made to replace coca by other crops, but it has been generally established that crop substitution as an anti-drug policy has been a failure (UNODC 2001). Decision makers and researchers have recognized that there are relevant socio-economic determinants that lead to coca growing other than economic profitability. These include social capital (Thoumi 2003), Statistics Canada, Catalogue No. 12-001-X saving account functions and financial reserve for large expenses (Bedoya 2003;Mansfield 2006). Comprehensive databases which include specific household-level information for coca growing areas are required to test those latter hypotheses. Coca growing is not illeg al per se in Peru (During the 1990s, the primary focus of the Peruvian Government was on \"pacifying\" the country by bringing terrorist groups under control. The Peruvian Government implemented what is currently known as the \"Fujimori Doctrine\". The idea underlying this Do ctrine was t hat the coca cultivation was not criminal in nature, but at tributable to poverty. Consequently, the Fujimori Doctrine decriminalized all coca farmers, which diminished the farmers' need for protection from terrorist associations, therefore making it easier for the Government to fight those violent groups (Obando 2006).), which partly reflects the social acceptance of traditional uses of coca in this country (UNODC 2001). Thus, the current legal framework seems to facilitate narcotics trafficking because coca used in illegal trade can be cultivated under the guise of traditional uses (INCB 2009;Durand 2005). Accordingly, Garcia and Antezana (2009) suggest that some farmers sell coca to those who purport to be traditional-use traders, but are actually narcotics traffickers who process coca leaves in different places, such as small towns at the border with Bolivia. Even though coca farm ing is not illegal, coca-growing regions which are perceived to be supplying narcotics traffickers (e.g., regions with large coca fields) can be targeted by the Government for the implementation of forced eradication programs (Obando 2006). After eradication, coca growers are likely to incur large economic losses, depending on the total extent of their individual coca cultivation areas. Thus, some of th e farmers might be reluctant to provide information on whether or not they have any coca under cultivation. It shou ld also be expected that some of the farmers who admit to cultivating coca, would not report the true extent of the area, given their fear that large coca fields could be more prone to eradication. Since there are both political and policy concerns in accurately and reliably measuring coca growing areas, it is necessary for su rvey methodologists to determine how to encourage response and truthful reporting of a nswers to sensitive questions related to coca growing. This article suggests and evaluates a number of strategies to increase both the reporting and the reliability of household-level responses in a remote coca growing region in Peru. Although the topic of this article is specifically related to coca growing, the lessons learned about survey design and implementation could be used as a re ference for dealing with other sensitive topics such as h ealth-related issues (e.g., anti-conception and sexual behavior) or undesirable behaviors (e.g., illegal drug use) in other regions in different countries. The structure of the article is as fo llows: Section 2 describes the community in Peru subject to study, the specific strategies to reduce non-response and misreporting as well as the lessons learned from data collection related to sensitive questions in the research area. Section 3 presents the coca growing-related survey results and their validation, while Section 4 is comprised of a summary of the main results followed by the conclusion.\nLongitudinal studies, in which data are collected from every sampled subject at multiple time points, are very common in research areas such as m edicine, population health, economics, social sciences, and sample surveys. The statistical analysis in a sample survey typically aims to estimate or make inference on the mean of a study variable at each time point. Nonresponse or missing data in the study variable is a serious impediment to performing a valid statistical analysis, because the response propensity (PSI) may directly or indirectly dep end on the value of the study variable. Nonresponse is monotone if, whenever a value is missing at a time point , t all future values at > s t are missing. We focus on nonm onotone nonresponse, which often occurs in longitudinal surveys. In the Survey of Industrial Research and Development (SIRD) conducted jointly by the U.S. Census Bureau and the U.S. National Science Foundation (NSF), fo r example, a bu siness may be a nonrespondent on research and development expenditures at year 1 t \uf02d but a respondent at year . t For ease we refer to SIRD in the present tense throughout, but we note that as of 2008, it has b een replaced by the Business R&D and Innovation Survey. Some existing methods for handling nonmonotone nonresponse can be briefly described as follows. The parametric approach assumes parametric models for both the PSI and the joint distribution of the study variable across time points (e.g., Lipsitz 1998, Troxel, Lip sitz and. The validity of the parametric approach, however, depends on whether parametric models are correctly specified. Vansteelandt, Rotnitzky and Robins (2007) proposed some methods under so me models of the PSI at time t conditional on observed past data. Xu, Shao, Palta and Wang (2008) derived an imputation procedure under the assumptions that (i) the PSI at t depends only on values of the study variable at time 1 t \uf02d and (ii) the study variables over different time points is a M arkov chain. Another approach, which will be referred to as censoring, is to create a dataset with \"mono tone nonresponse\" by discarding all observ ed values of the study variable from a sampled subject after its first missing value. Methods appropriate for monotone nonresponse (e.g., Diggle and Kenward 1994, Robins and Rotnitzky 1995, Paik 1997 can then be appl ied to the reduced dataset. This app roach may be inefficient when many observed data are discarded. Furthermore, in practical applications it is not desirable to throw away observed data. The purpose of this ar ticle is to propose an imputation method for longitudinal data with nonmonotone nonresponse under the past-value-dependent PSI assu mption described by Little (1995): at a time point , t the nonresponse propensity depends on values of the study variable at time points prior to . t This assumption on the PSI is weaker than Statistics Canada, Catalogue No. 12-001-X that in Xu et al. (2008) and is different from those in Vansteelandt et al. (2007). We consider imputation which does not require building a model for the PSI. Imputation is commonly used to compensate for missing values in survey problems (Kalton and Kasprzyk 1986). Once all m issing values are imputed, estimates of parameters are computed using the estimated means for complete data by treating imputed values as observations. The proposed imputation and estimation methodology, including a bootstrap method for variance estimation, is introduced in Section 2. To examine the finite sample performance of t he proposed method, we present some simulation results in Section 3. We also include an application of the proposed method to the SIRD. The last section contains some concluding remarks.\nConsider a finite population of size , N where N is known. For each unit , i i y is the study variable and x i is the q -dimensional vector of auxiliary variables. The parameter of interest is the finite population mean of the study variable, is assumed to be a random sample of size N from a sup erpopulation distribution ( , ). x F y Suppose a sample of size n is dr awn from the finite population according to a probability sampling design. Let \uf070 be the design weight, where i \uf070 is the firstorder inclusion probability of u nit i obtained from the probability sampling design. Under complete response, the finite population mean can be estimated by the Horvitz-Thompson (HT) estimator, where A is the set of indices appearing in the sample. In the presence of m issing data, the HT estimator HT \uf071 cannot be computed. Let r be the response indicator variable that takes the value one if y is observed and takes the value zero otherwise. Conceptually, as d iscussed by Fay (1992), Shao and Steel (1999), and Kim and Rao (2009), the response indicator can be extended to the entire population as where i r is a realization of the random variable . r In this case, the complete-case (CC) converges in probability to ( | = 1). E Y r Unless the response mechanism is missing completely at random in the sense that ( | = E Y r 1) = ( ), E Y the CC estimator is biased. To correct for the bias of the CC estimator, if the response probability is known, then the weighted CC If the response probability 1is unkno wn, one can postulate a parametric model for the res ponse probability ( , ; ) for som e 0 . \uf0ce \uf057 \uf066 We as sume that there exists a consistent n- \uf066 we can obtain the estimated response probability which is often called the propensity score (Rosenbaum and Rubin 1983). The propensity-scoreadjusted (PSA) estimator can be constructed as The PSA estimator (3) is widely used. Many surveys use the PSA estimator to reduce nonresponse bias (Fuller, Loughin and Baker 1994;Rizzo, Kalton and Brick 1996). Rosenbaum and Rubin (1983) and Rosenbaum (1987) proposed using the PSA approach to estimate the treatment effects in observational studies. Little (1988) reviewed the PSA methods for handling unit nonresponse in survey sampling. Duncan and Stasny (2001) used the PSA approach to control coverage bias in telephone surveys. Folsom (1991) and Iannacchione, Milne and Folsom (1991) used a logistic regression model for the response probability estimation. Lee (2006) applied the PSA method to a volunteer panel web survey. Durrant and Skinner (20 06) used the PSA approach to address measurement error. Despite the popu larity of PSA estim ators, asymptotic properties of PSA est imators have not received much attention in survey sampling literature. Kim and Kim (2007) used a Taylor expansion to obtain the asymptotic mean and variance of PSA estimators and discussed variance estimation. Da Silv a and Opsomer (2006) and Da Silva and Statistics Canada, Catalogue No. 12-001-X Opsomer (2009) considered nonparametric methods to obtain PSA estimators. In this paper, we discuss optimal PSA estimators in the class of PSA estimators of t he form (3) that use a nconsistent estimator \u02c6. \uf066 Such estimators are asymptotically unbiased for . \uf071 Finding minimum variance PSA estimators among this particular class of PSA estimators is a topic of major interest in this paper. Section 2 presen ts the main results. An optimal PSA estimator using an augmented propensity score model is proposed in Section 3. In Section 4, variance estimation of the proposed estimator is discussed. Results from two simulation studies can be found in Section 5 and concluding remarks are made in Section 6.\nExamples of studies that have modelled the predictors of different kinds of, and different reasons for the non-response that affect longitudinal studies are plentiful, stimulated by being able to draw on auxiliary variables obtained from sample members before (and after) the occasions at which they are non-respondents. See, for example, Lepkowski and Couper (2002) for an analysis that separates refusals from not being located or contacted; Hawkes and Plewis (2006) who separate wave non-respondents from attrition cases in the UK National Child Development Study; and Plewis (2007a) and Plewis, Ketende, Joshi and Hughes (2008) who consider non-response in the first two waves of the UK Millennium Cohort Study. The focus of this paper is on how we can assess the accuracy of these response propensity models . The paper is built around a framework that is widely used in epidemiology (Pepe 2003) and criminology (Copas 1999) to evaluate risk scores but has not, to our knowledge, been used in survey research before. Response propensity models can be used to construct weights intended to remove biases from estimates, to inform imputations, and to predict potential non-respondents at future waves thereby directing fieldwork resources to those respondents who might otherwise be lost. The accuracy of response propensity models has not, however, been given the amount of attention it warrants in terms of their ability to discriminate between respondents and nonrespondents, and to predict future non-response. Good estimates of accuracy can be used to compare the efficacy of different weighting methods, and to help to determine the allocation of scarce fieldwork resources in order to reduce non-response. The paper is organised as follows. The framework for assessing accuracy is set out in the next section. Section 3 introduces the UK Millennium Cohort Study and the methods are illustrated using data from this study in Section 4. Section 5 concludes.\nSmall area estimation and related statistical techniques have become a topic of growing importance in recent years. The need for reliable small area estimates is felt by many agencies, both public and private, for making useful policy decisions. An example where small area techniques are used in practice is in the monitoring of socio-economic and health conditions of different age-sex-race groups where the patterns are observed over small geographical areas. It is now widely recognized that direct survey estimates for small areas are usually unreliable due to their typically large standard errors and coefficients of variation. Hence, it becomes necessary to obtain improved estimates with higher precision. Model-based approaches, either explicit or implicit, are elicited to connect the small areas and improved precision is achieved by \"borrowing strength\" from similar areas. The estimation technique is also known as shrinkage estimation since the direct survey estimates are shrunk towards the overall mean. The survey based direct estimates and sample variances are the main ingredients for building aggregate level small area models. The typical modeling strategy assumes that the sampling variances are known while a suitable linear regression model is assumed for the means. For details of these developments, we refer to reader to Ghosh and Rao (1994), Pfeffermann (2002) and Rao (2003). The typical area level models are subject to two main criticisms. First, in practice, the sampling variances are estimated quantities, and hence, are subject to substantial errors. This is because they are often based on equivalent sample sizes from which the direct estimates are calculated. Second, the assumption of known and fixed sampling variances of typical small area models does not take into account the uncertainty in the variance estimation into the overall inference strategy. Previous attempts have been made to model only the sampling variances; see, for example, Maples, Bell and Huang (2009), Gershunskaya and Lahiri (2005), Huff, Eltinge and Gershunskaya (2002), Cho, Eltinge, Gershunskaya and Huff (2002), Valliant (1987) and Otto and Bell (1995). The articles Wang and Fuller (2003) and Rivest and Vandal (2003) extended the asymptotic mean square error (MSE) estimation of small area estimators when the sampling variances are estimated as opposed to the standard assumption of known variances. Additionally, You and Chapman (2006) considered the modelling of the sampling variances with inference using full Bayesian estimation techniques. The necessity of variance modelling has been felt by many practitioners. The latest developments in this area are nicely summarized in a recent article by William Bell of the United States Census Bureau 2008. He carefully examined the consequences of these issues in the context of MSE estimation of model based small area estimators. He also provided numerical evidence of MSE estimation for Fay-Herriot models (given in Equation 1) when sampling variances are assumed to be known. The developments in the small area literature so far can be \"loosely\" viewed as (i) smoothing the direct sampling error variances to obtain more stable variance estimates with low bias and (ii) (partial) accounting of the uncertainty in sampling variances by extending the Fay-Herriot model. As evident, lesser or no attention has been given to account for the sampling variances effectively while modeling the mean compared to the volume of research that has been done for modeling and inferring the means. There is a lack of systematic development in the small area literature that Statistics Canada, Catalogue No. 12-001-X includes \"shrinking\" both means and variances. In other words, we like to exploit the technique of \"borrowing strength\" from other small areas to \"improve\" variance estimates as we do to \"improve\" the small area mean estimates. We propose a hierarchical model which uses both the direct survey and sampling variance estimates to infer all model parameters that determine the stochastic system. Our methodological goal is to develop the dual \"shrinkage\" estimation for both the small area means and variances, exploiting the structure of the mean-variance joint modelling so that the final estimators are more precise. Numerical evidence shows the effectiveness of dual shrinkage on small area estimates of the mean in terms of the MSE criteria. Another major contribution of this article is to obtain confidence intervals of small area means. The small area literature is dominated by point estimates and their associated standard errors; it is well known that the standard practice of [point estimate q \uf0b1 \uf0b4 standard error], where q is the Z (standard normal) or t cut-off point, does not produce accurate coverage probabilities of the intervals; see Hall and Maiti (2006) and Chatterjee, Lahiri and Li (2008) for more details. Previous work is based on the bootstrap procedure and has limited use due to the repeated estimation of model parameters. We produce confidence intervals for the means from a decision theory perspective. The construction of confidence intervals is easy to implement in practice. The rest of the article is organized as follows. The proposed hierarchical model for the sample means and variances is developed in Section 2. The estimation of model parameters via the EM algorithm is developed in Section 3. Theoretical justification for the proposed confidence interval and coverage properties are presented in Section 4. Sections 5 and 6 present a simulation study and a real data example, respectively. Some discussion and concluding remarks are presented in Section 7. An alternative model formulation for small area as well as mathematical details are provided in the Appendix.\nWhen predictor variables in a regression model are correlated with each other, this condition is referred to as collinearity. Undesirable side effects of collinearity are unnecessarily high standard errors, spuriously low or high t-statistics, and p arameter estimates with illogical signs or ones that are overly sensitive to small changes in data values. In experimental design, it may be possible to create situations where the explanatory variables are orthogonal to each other, but this is not true with observational data. Belsley (1991) noted that: \"... in nonexperimental sciences, ..., collinearity is a natural law in the data set resulting from the uncontrollable operations of the data-generating mechanism and is simply a painful and unavoidable fact of life.\" In many surveys, variables that are substantially correlated are collected for analysis. Few analysts of survey data have escaped the problem of collinearity in regression estimation, and the presence of this problem encumbers precise statistical explanation of the relationships between predictors and responses. Although many regression diagnostics have been developed for non-survey data, there are considerably fewer for survey data. The few articles that are available concentrate on identifying influential points and influential groups with abnormal data values or survey weights. Elliot (2007) developed Bayesian methods for weight trimming of linear and generalized linear regression estimators in unequal probability-of-inclusion designs. Li (2007a, b) and Valliant (2009, 2011) extended a series of traditional diagnostic techniques to regression on complex survey data. Their papers cover residuals and leverages, several d iagnostics based on case-deletion (DFBETA, DFBETAS, DFFIT, DFFITS, and Cook's Distance), and the forward search approach. Although an extensive literature in applied statistics provides valuable suggestions and guidelines for data analysts to diagnose the presence of collinearity (e.g., Belsley, Kuh and Welsch 1980;Belsley 1991;Farrar and Glauber 1967;Fox 1986;Theil 1971), almost none of this research touches upon diagnostics for collinearity when fitting models with survey data. One prior, survey-related paper on collinearity problems is (Liao and Valliant 2012) which adapted variance inflation factors for li near models fitted with survey data. Suppose the underlying structural model in the superpopulation is = . \uf02b Y X e \uf062 The matrix X is an n p \uf0b4 matrix of predictors with n being the sample size; \uf062 is a 1 p \uf0b4 vector of parameters. The error terms in the model have a general variance structure where 2 \uf073 is an unknown constant and R is a unknown n n \uf0b4 covariance matrix. Define W to be the diagonal matrix of survey weights. We assu me throughout that the survey weights are constructed in such a way that they can be used for estimating finite population totals. The survey weighted least squares (SWLS) estimator is  decompositions for or dinary least squares. These are extended to be appropriate for survey estimation in section 3. The fourth section gives some numerical illustrations of the techniques. Section 5 is a c onclusion. In most derivations, we use model-based calculations since the forms of t he model-variances are useful for understanding the effects of collinearity. However, when presenting variance decompositions, we use estimators that have both model-and designbased justifications.\nMultiple imputation was first proposed for handling nonresponse in large complex surveys (Rubin 1987 ). Several other uses fo r multiple imputation have since been proposed, including statistical disclosure limitation and measurement error. An appeal of multiple imputation is that standard methods can be applied to each imputed dataset and then simple combining rules applied, which vary between applications. See Reiter and Raghunathan (2007) for a detailed overview of the different rules and applications. Existing multiple imputation combining rules were developed for use with random samples and superpopulation models (Deming and Stephan 1941). In fin ite population analyses of census data, where the sampling variance is zero, the combining rules for univariate estimands can still be applied as a sp ecial case; however, hypothesis tests for multivariate estimands break down. Motivated by the use of multiple imputation to generate partially synthetic data (Rubin 1993; Little 1993) for the U.S. Census Bureau's Longitudinal Business Database (Kinney, Reiter, Reznek, Miranda, Jarmin and Abowd 2011), an economic census, this paper derives a multivariate test for finite populations for u se with partially synthetic data and extends it to the application of missing data. Extensions to oth er multiple imputation applications are expected to be straightforward. The remainder of this paper is organized as follo ws. Section 2 describes the case of partially synthetic data and Section 3 pr esents the extension to missing data. Simulations in Section 4 evaluate the combining rules for both the missing data and partially synthetic data cases."}, {"section_title": "The total survey error paradigm 2.1 Some history of survey sampling", "text": "There are a number of papers describing the development of early survey sampling methodology. In that early development there is an implicit or explicit recognition of quality issues although they are hidden under labels such as errors and survey usefulness (Deming 1944). The historical overviews provided by, for instance, Kish (1995), Fienberg andTanur (1996), andO'Muircheartaigh (1997) all em phasize the fact that the period up to 1950 is characterized by a fullbloom development of sampling theory. During the 1920s the International Statistical Institute agreed to promote ideas on representative sampling suggested by Kiear (1897) and Bowley (1913). In 1934 Neyman published his land mark paper on the representative method. Later Fisher's (1935 ) randomization principle was used in agricultural sampling and Neyman (1938) developed cluster sampling, ratio estimation and two-phase sampling and introduced the concept of confidence interval. Neyman showed that the sampling error could actually be measured by calculating the variance of the estimator. Bill Cochran, Frank Yates, Ed Deming, Morris Hansen and many others further refined the concepts of sampling theory. Hansen led a research group at the U.S. Census Bureau where much of the applied work and new theory development was cond ucted in those days. On e remarkable result of the Census Bureau efforts was the twovolume textbook on sampling theory and methods (Hansen, Hurwitz and Madow 1953). As a matter of fact the advances in sampling theory were so prominent at the time that Stephan (1948) found it worthwhile to write an article about the history of modern sampling methods. It was early recognized that there could be survey errors other than those attributed to sampling. There are writings on the effects of question wording such as Muscio (1917). Research on questionnaire design was quite extensive in the 1940s. Problems with errors introduced by fieldworkers collecting agricultural data in India were addressed by Mahalanobis (1946), resulting in a method for estimating such errors. The method is called \"interpenetration\" and can be used to estimate, so called, correlated variances introduced by interviewers, editors, coders and those who supervise these groups. The most prominent error sources were certainly known around 1950. Deming had listed error sources (1944) that constitute the first published typology of survey errors and Hansen and Hurwitz (1946) had discussed subsampling among nonrespondents in an attempt to provide unbiased estimates in a situation with an initial nonresponse. But the methodological emphasis, up to then, had been on developing sampling theory, which is quite understandable. It was very important to be able to show that surveys could be conducted on a sam pling basis and in a variety of settings. By 1950 it had b een demonstrated quite successfully that this was indeed possible. So it was time to move on to other issues and refinements. In those early days the use of t he word qua lity was confined to mainly quality control, sometimes as quality control of survey operations. It was common that the quality control was verification and/or estimation of error sizes for various operations. Statistics were known to be plagued b y errors other th an those stemming from sampling but the process quality issue of how to systematically reduce these errors and biases was still to be developed (Deming 1944;Hansen and Steinberg 1956). The user 60 years ago was a somewhat obscure player, although not at all ignored by prominent survey methodology developers. For instance, Deming (1950) claimed that until the purpose is stated, there is no right or wrong way of going about a survey. Some other statisticians made similar statements. But the us er was really hiding behind terms, such as subject-matter problem, study purpose or t he key functions of a statistical system. Even now survey and quality are vague concepts. As pointed out by Morganstein and Marker (1997) varying definitions of quality undermine improvement work so we should, at least, try to distinguish between different definitions to see what purposes they might serve. One of the most cited definitions is attributed to Joseph Juran, namely quality being a direct function of \"fitness for use\". It turns out that Deming already in 1944 used the phrase \"fitness for purpose\", not to define quality, bu t rather to explain what made a survey product work. For a long time \"good\" quality was implicitly equivalent to a sm all mean squared error (MSE), i.e., data should be accurate and accuracy of an estimate can be measured by MSE, which is the sum of the variance and the squared bias. We have noticed that survey statistics should also be useful, later denoted \"relevant\". Many of today's quality dimensions were not really an issue at the time. The users, too, were accustomed to the fact that surveys took time to carry out; timeliness was surely on the agenda but not as explicitly as it is today. A census took years to process. The users were accustomed to a technology that could only deliver relatively simple forms of accessibility. Hence, it was natural for users and producers to concentrate on making sure that the statistical problem coincided reasonably well with the subject-matter problem and that MSE was ke pt on a Statistics Canada, Catalogue No. 12-001-X decent level, where MSE many times was and still is equivalent with just the variance, without a squared bias term added. Before proceeding any further, let us define \"survey\". A survey is a statistical stud y designed to measure population characteristics so that population parameters can be estimated. Two examples of pa rameters are the pr oportion unemployed at a given time in a population of individuals, and the total revenue of a business or industry sector during a given time period. A s urvey can be defined as a list of prerequisites (Dalenius 1985a). According to Dalenius a study can be classified as a survey if the following prerequisites are satisfied: 1. The study concerns a set of objects comprising a population; 2. The population under stud y has one or more measurable properties; 3. The goal of the study is to describe the population by one or more parameters defined in terms of measurable properties, which requires observing (a sample of) the population; 4. To get observational access to the population a frame is needed; 5. A sample of objects is s elected from the frame in accordance with a sampling design that specifies a probability mechanism and a sample size n (where n might equal N, the population size); 6. Observations are made on the sample in accordance with a measurement process (i.e., a measurement method and a prescription as to its use); 7. Based on the measurements, an estimation process is applied to compute estimates of the parameters when making inference from the sample to the population under study. This definition implicitly lists the s pecific error sources that are present in survey work. For each source there are a number of methods available that minimize the effects but also measure their sizes (Biemer and Lyberg 2003;Groves, Fowler, Couper, Lepkowski, Singer and Tourangeau 2009). Deviations from the definition reflect quality flaws. Moreover such deviations are common. In som e designs selection probabilities are unknown or the variance estimator chosen might not be the most suitable one, given the sample design applied. Whether such flaws are problematic or not depends on the purpose."}, {"section_title": "The components of the total survey error paradigm", "text": "The total survey error paradigm is a the oretical framework for optimizing surveys by minimizing the accumulated size of all error sources, given budgetary constraints. In practice this means that we want to minimize the mean squared error for sele cted survey estimates, namely those that are considered most important by the main stakeholders. The mean squared error is the most common metric for survey work consisting of a sum of variances and squared bias terms from each known error source. Groves and Lyberg (2010) provide a summary of the status of the paradigm in the past and in today's survey practice. The idea that surveys should be designed taking all error sources into account stems from the early giants in the field. Morris Hansen, Bill Hurwitz, Joe Waksberg, Leon Pritzker, Ed Deming and others at the U.S. Census Bu reau, Leslie Kish at the University of Michigan, P.C. Mahalanobis at the Indian Statistical Institute, and Tore Dalenius, Stockholm University were among those who took the lead in survey research, emphasizing errors and optimal design. They worried about the inherent limitations associated with sampling theory since non sampling errors could make the theory break down. They were very practical and thought a lot about balancing errors and the costs to deal with them. Some of them saw similarities between a factory assembly line (Deming and Geoffrey 1941) and the implementation of some of the survey processes and introduced control methods obtained from industrial applications. Dalenius (1967) realized that there was as yet no \"survey design formula\" that could provide an optimal solution to the design problem. The approach taken by Dalenius and also Hansen, Hurwitz and Pritzker (1967) was a strategy of minimizing all biases and going for a minimum-variance scheme so that the variance became an approximation of the MSE. This was su pposed to happen through intense verification schemes for ongoing productions and quite extensive evaluation studies for future productions. In 1969 Dalenius, inspired by Hansen, presented a paper on t otal survey design, where the word \"total\" reflected the thought about taking all error sources into account. Hansen, Hurwitz, Marks and Mauldin (1951), Hansen, Hurwitz and Bershad (1961), and Hansen, Hurwitz and Pritzker (1964) developed the U.S. Census Bureau Survey Model that reflected contributions from interviewers, coders, editors, and crewleaders and a llowed the estimation of th ose contributions to the total survey error. These estimation schemes were elaborated on by Bailar and Dalenius (1969) and consisted of var iations of replication and interpenetration. Bias estimation was assumed to be handled by comparing estimates obtained from the regular operations with those obtained from preferred procedures (that could not be used on a large scale due t o financial, administrative or practical reasons). Today this kind of approach is called the \"gold standard\". It was stated that good survey design called for reasonably effective control of the total error by careful Statistics Canada, Catalogue No. 12-001-X specifications of the survey procedures, including adequate controls. Hansen, Deming and others did worry about control costs bu t although statistical p rocess control and acceptance sampling had been implemented in a number of survey organizations, there was very little discussion about continuous process improvement. A lot of the quality work had to do with estimation of error rates, controlling error levels for individual operators and conducting large-scale evaluation studies that usually took a long time. Users were not directly involved in the d esign process but in the U.S. federal statistical system they had at least some influence on what should be collected and presented. Dalenius (1968) provides more than 200 r eferences on users and user conferences associated with the products of the U.S. Federal statistical system. While total survey design was first advocated by Hansen, Dalenius and others, users were seldom directly involved in the final determination of survey requirements. Quite often an official, administrator or statistician acted as a subjectmatter specialist. Several decades ago this was the way we thought about users. Their opinions counted but they were not really involved in design decisions. Lurking in the back of our heads was the thought that this might not be a perfect model and in the late 1970's Statistics Sweden published an internal booklet called \"What to do if a customer shows up on our doorstep\". The basic design approach suggested by Hansen, Dalenius and others contained a number of steps including:   Evaluating the alternatives by reference to associated preliminary assessments of MSE and costs."}, {"section_title": "\uf0b7", "text": "Choosing one of the alternatives or a modification of one of them or deciding not to conduct a survey at all.\nDeveloping the administrative design including feasibility testing, a p rocess signal system (currently called paradata), a design document, and a Plan B. Kish (1965) had slightly different views on design. He liked the neo-Bayesian applications in survey sampling and psychometrics advocated by colleagues at the University of Michigan (Ericson 1969;Edwards, Lindman and Savage 1963). For instance, Kish liked the idea that judgment estimates of measurement biases might be combined with sampling variances to construct more realistic estimates of the total survey error. Regarding the optimization problem Kish thought that the multipurpose situation was economically favorable for surveys but that it could be difficult to decide on what to base the design on. If one principal statistic can be identified then that alone can decide the design and if there are a small number of principal statistics a compromise design is possi ble but if statistics are too disparate a reasonable design might not exist. Kish also emphasized the need for design information obtained from pilot surveys and pretests to facilitate design decisions. Kish noted that survey design and measurement could vary greatly across environments while sampling did less so. That could be one reason that sampling can be easily placed among the traditional statistical theories and methods, while it is more difficult to place the survey process in one specific discipline (Frankel and King 1 996 in their interview with Kish). Kish, like the other giants, emphasized the importance of small biases but appreciated the fact that the reduction of one bias term might increase the total error. Kish was keen on getting a reasonable balance between d ifferent error sources and how err or structures varied under different design alternatives. Like Hansen and colleagues Kish thought that relevant information should be contemporaneously recorded during implementation (again we see the parallel to paradata). Hansen and colleagues were really concerned about excessiv e but inadequate controls. They realized that some controls might have to be relaxed due to limited improvements and that degree of im provement in terms of affecting the estimates should be checked out before any relaxation could take place. They also suggested that one might have to compromise relevance to get controllable measurements or abstain from the survey. Both Hansen and colleagues and Kish were vigorously in favor of ending the practice that sampling error is the only survey error measured. When we look at today's situation we can conclude that we still do not have a design formula for surveys. There is no planning manual to speak of and the literature on design is consequently very small, as is the literature on cost (Groves 1989 is an exception). And no design formula is in sight. Since the advent of the U.S. Census Bureau survey model a number of variants have appeared on the scene, some of them quite complicated (Groves and Lyberg 2010). A common characteristic is the fact that they tend to be incomplete, i.e., they do not take all error sources into account. Most statistical attention is on variance components and especially on measurement error variance. There are a number of other weaknesses associated with the total survey error concept. Most notably a us er perspective is missing and a vast majority of users are not in a position to question or even discuss accuracy. The complex error structures and interactions do not invite outside scrutiny and user contacts often tend to concern less technical issues such as timeliness, comparability and costs. Users are not really informed about real levels of accuracy and we know very Statistics Canada, Catalogue No. 12-001-X little about how users perceive information about errors and how to act on that. As pointed out by Biemer (2001), in his discussion of Platek and S\u00e4rndal (2001), there is a lack of routine measurements of MSE c omponents in statistical organizations. There are good reasons for this state of affairs. Complexity has already been mentioned and to that we can add factors such as costs, the fact that it is almost impossible to publish such information at the time data are released, and that there is no measure of total error that would take all error sources into account, either because a lack of proper methodology or that some errors defy expression. Groves and Lyberg (2010) list some other weaknesses associated with the total survey error paradigm. For instance, we need to know more about the interplay between variances and biases. It is possible that an increase in simple response variance goes hand in hand with a reduction in response bias, say, when we compare interview mode with self-administrative alternatives. Recently, West and Olson (2010) showed that interviewer variance can occur not only from individual interviewers' effect on the responses within their assignments but also because individual interviewers successfully obtain cooperation from different groups of sample members. Despite all its limitations, the strengths of the total survey error framework are qui te convincing. The framework provides a taxonomic decomposition of errors, it separates variance from bias and observation from nonobservation, and it defines the different steps in the s urvey process. It serves as a conceptual foundation of the field of survey methodology, where subfields are defined by their associated error structures. Finally, it identifies the gaps in the research literature since any typology will show that so me process steps are more \"popular\" than others. Just compare the respective sizes of the literatures on data collection and data processing. It seems, however, as if the total survey error framework needs some expansion along lines some of w hich were identified half a centu ry ago. We need some guidance on trade-offs between measuring error sizes and making processes more error-free. Spencer's (1985) question is: how much should we sp end on measuring quality versus quality enhancement? We also need some guidance on how to integrate additional notions into the framework, so that it becomes a total survey quality framework rather than a total survey error framework (Biemer 2010). For instance, if \"fitness for use\" predominates as a conceptual base, how can we launch research that incorporates error variation associated with different uses? This aspect will be discussed in the next section.\nThe critical product characteristics are identified together with the user, both broad and more single effort needs.\nA map of the process flow is developed by a team familiar with the process. The map should include the sequence of process steps, decision p oints and customers for each step.\nThe key process variables are identified among a larger set of process variables.\nThe measurement capability is evaluated. It is important that decisions are based on good data, not just data. Available data might be useless. This is an area where statistical organizations should have an advantage over other organizations. One s hould not reach conclusions about process stability without knowledge about measurement errors. Above all, data should allow quantification of improvement.\nThe stability of the process is determined. The variability pattern of the process data is analyzed using control charts and other statistical tools.\nThe system capability is de termined. If stabi lity is not achieved after special cause variation has been eliminated an improvement effort is called for. System changes must be made when the process variation is so large that it does not meet specifications, such as minimum error rat es or production deadlines. Typical methods to reduce variation are the development and implementation of a new training program or t he enforcement of a standard operating procedure. The latter can be a process standard, a current best methods standard or a simple checklist.\nThe final step of the improvement plan is to establish a system for continuous monitoring of the process. We cannot expect processes to remain stable over time. For many reasons they usually start drifting after some time. A monitoring system helps keeping track of new error structures, new customer requirements, and the potential of improved methods and technology and can suggest process improvements. The Morganstein and Marker book chapter had a distinct effect on quality wo rk and process thinking in many European statistical organizations. Interest in these issues increased and some organizations started their own quality management system where process improvement was central. At the 1998 Joint Statistical Meetings Mick Couper presented an invited paper on measuring quality in a CASIC environment. He meant that the new technology generated lots of by -product data that could be us ed to improve the data collection process. He named those paradata, not in his paper but in his session presentation. This naming caught on very quickly in the survey community and it made sense to define the trilogy data, metadata, and paradata. Thus we had one term for data about th e data (metadata) and another for data about th e process (paradata). Obviously paradata are process data but for a long time paradata were confined to data about the d ata collection process, while the term used in many European statistical organizations was \"process data\" and took all su rvey processes into account (Aitken, H\u00f6rngren, Jones, Lewis and Zilhao 2004). Recently a renewed broadening of the meaning of the concept has taken place. Kennickell, Mulrow and Scheuren (2009) remind us about what they call macro paradata, global process data su ch as response rates, coverage rates, edit failure rates, and coding error rates that always have been indicators of process quality in statistical organizations. Lyberg and Couper (2005), Kreuter, Couper and Lyberg (2010), and Smith (2011) also use th e more inclusive meaning of paradata where other processes than data collection are taken into account. There is a risk that paradata, like quality, becomes an overused concept. There are examples of discussions where all data, apart from the survey estimates, are considered paradata, which, of course, does not make sense. Paradata is a gr eat naming and they ar e necessary to judge process quality. However, a word of caution is in place. One should never collect paradata that are not related to process quality and it is important to kno w how to analyze them. Sometimes statistical process control methods can be used but at other times other analytical techniques are needed. For instance, to be able to control interviewer falsification we might need to look at several processes simultaneously, but th eory and methodology for such analysis might not be readily available. The expanded use of microdata that concern individual records, such as keystroke data and flagged imputed records, is an effect of using new technology. Modern data collection procedures generate enormous amounts of these kinds of paradata but so do systems for com puter-assisted manual coding and systems for pure automated coding as well as systems for scanning of data. It makes no sense to confine the concept to data collection. Quality management has taught us t o prevent process problems rather than fix them when they appear, that it is important to distinguish between different types of process variation since th ey require different actions, that any process intervention or improvement should be based on good data and proper analysis methods, and that even stable processes eventually start drifting, which calls for continuous monitoring.\nA standard is a do cument that should be adhered to almost without exception. Deviations are not recommended and require approval of se nior management. Corrective action should be taken when a standard is not fully met. An or ganization can become certified according to a standard. This is th e case for ISO standards, where a f ew are relevant to statistical organizations. \uf0b7 A policy should be applied without exceptions. For instance, an organization can have a policy regarding the use of incentives to boost response rates.\nSeveral organizations have developed guidelines for different aspects of the statistics production. Typically, guidelines can be skipped if there are \"good\" reasons to do so. \uf0b7 A recommended practice is promoted but adherence is not mandatory. Admittedly, the categories of this c lassification scheme are not mutually exclusive, especially if we al so take language and cultural aspects into account. For instance, in the Swedish language policies and guidelines are very close conceptually. If we con sult the unauthorized but consensus based Wikipedia it says that \"policies describe standards while guidelines outline best practices for following these guidelines\". This sentence contains three of the categories mentioned by Colledge and March. It is probably best to relate to these different kinds of documents in a similar fashion. They all attempt to improve quality by reducing various types of variation and we should not dwell too much on what they are called. Although standards have been an important part of survey methodology for a long time they have gained momentum since statistical organizations became interested in quality m anagement. Early standards such as  and U.S. Bureau of the Census (1974) concentrated on discussing the presentation of errors in data. At the U.S. Censu s Bureau all publications should inform users that data were subject to error, that analysis could be affected by those errors, and that estimated sampling errors are smaller than the total errors. For m ajor surveys the nonsampling errors should be treated in more detail unlike in the past. Many other statistical organizations imported this line of thinking. For instance, the quality frameworks mentioned earlier are expansions including also other quality dimensions than accuracy. The European Statistical System has successively developed and launched what was first called Model Quality Reports and currently just Standard for Quality Reports (Eurostat 2009a). The standard provides recommendations to European National Institutes (notice the conceptual complexity) for preparation of quality reports for a \"full\" range of st atistical processes and t heir outputs. The standard treats the basic quality dim ensions relevance, accuracy, timeliness, accessibility, coherence and comparability. Let us look at some examples. Regarding measurement error, which is part of the accuracy component, the standard says that the following information should be included in a quality report:  Regarding timeliness the standard says that the following information should be included: \uf0b7 For annual or more frequent releases: the average production time for each release of data. There are also sections on how to communicate information regarding trade-offs between quality dimensions, assessment of user needs and perceptions, performance and cost, respondent burden as well as confidentiality, transparency and security. Even though there is a section on user needs and perceptions, users have obviously not been involved in the preparation of the standard itself. We still know very little about how users perceive and use information about quality. The standard is backed by a much more detailed handbook for quality reports (Eurostat 2009b) and both documents are built around the 15 principles listed in the European Statistics Code of Practice, which is the basic quality framework for the European Statistical System. The C ode of Pr actice principles concern professional independence, mandate for dat a collection, adequacy of resources, quality commitment, statistical confidentiality, impartiality and objectivity, sound methodology, appropriate statistical procedures, nonexcessive burden on respondents, cost-effectiveness, relevance, accuracy and reliability, timeliness and punctuality, coherence and comparability, and, finally, accessibility and clarity. Each principle is accompanied by a set of indicators that the ind ividual organization can measure to establish whether it meets the Code or not. Some indicators are vague and very subjective in nature such as \"the scope, detail and cost of statistics are commensurate with needs\", while others are more specific, such as \"a standard daily time for the release of statistics is made public\". Peer reviews of compliance to a limited set of the principles have been conducted using an earlier version of the Code and, not surprisingly, many national statistical offices in Europe have problems living up to the Cod e (Eurostat 2011a). Therefore in order to assist the implementation of the Code a suppo rting framework has been developed, called the Quality Assurance Framework (QAF) that contains more specific guidance regarding methods and references (Eurostat 2011b). This seems to be a very useful document since its references are mainly summaries of the state-of-the-art in areas such as sampling, questionnaire design, editing and so on, which stimulates conformity to current best practices. The Code of Practice has many similarities with the UN Fundamental Principles of Official Statistics (de Vries 1999). The latter promotes also the principle of international cooperation and coordination, which is, to a large extent, an element that is missing in today's development of statistics production (Kotz 2005). Even neighbouring countries can have very different approaches and methodological competence levels and the differences are sometimes difficult to explain. Experience shows that global development collaboration is difficult to achieve. We meet, we talk, and we bring back ideas that might fit our own systems. It is harder to agree on common approaches. One global standard that relates to statistics production is the ISO 20252 on market, opinion and soci al research (International Standards Organization 2006). This is a process standard with around 500 requirements concerning the re search activities within an organization. It is a m inimum standard fo r what to do rather than how to do things. It is suitable for organizations that conduct surveys and the o rganization can apply for certification. In April 2010 more than 300 organizations world-wide had been certified, most of them marketing firms. One national statistical office (Uruguay) was certified in 2009 and Statistics Sweden is planning a certification in 2013 but those are the only national offices that have chosen this path. The standard concerns the organization's system for quality management, management of the executive elements of the research, data collection, data management and processing, and reporting on research projects (Blyth 2012).  (OMB 2002 ) whose purpose was to ensure and maximize the quality, objectivity, utility, and integrity of information disseminated by federal agencies. OMB (2006 a) has also issued standards and guidelines for surveys. They are built in a standard fashion. First comes a standard such as \"Response rates m ust be computed using standard formulas to measure the proportion of the eligible sample that is represented by the responding units in each study, as an indicator of potential nonresponse bias\". This standard is th en followed by a number of guidelines on how to make the necessary calculations while the final gui deline states that \"If the overall nonresponse rate exceeds 20%, an analysis of the nonresponse bias should be c onducted to see wh ether data are missing completely at r andom\". As in the case of the ESS standards, the OMB guidelines are complemented by a supporting document (OMB 2006b) that can facilitate adherence to the standards. Most agencies in the decentralized U.S. Federal Statistical System have documents in place that adapt the OMB guidelines. For instance, the U.S. Census Bureau has its own statistical quality standards that goes into more technical detail compared to the OMB documents. Each standard is described via requirements and sub-requirements and they often provide very specific examples of studies that can be conducted. Examples of other U.S. agen cies that have standards related to the quality of information disseminated include the National Center for Health Statistics, National Center for Education Statistics, and the Statistics Canada, Catalogue No. 12-001-X Energy information Administration. All these standards can be downloaded from the agencies' websites. Statistics Canada has issued quality guidelines since 1985. They are similar to the ESS guidelines since not just accuracy is em phasized. But they are much more detailed and contain lots of re ferences. A spec ial feature is th at for some processes the guidelines prescribe the use of statistical process control. No other agency seems to be doi ng that. The latest edition of the guidelines is provided in Statistics Canada (2009). Many other statistical organizations in the world have their own quality standards. They are sometimes described as guidelines or s tandards and s ometimes as bus iness support systems o r quality assurance frameworks. In any case, the contents and style vary across organizations but the variation should be m anageable. It should be possible to achieve higher degrees of standardization globally, since that has happened in other fields, such as air travel. Apted, Carruthers, Lee, Oehm and Yu (2011) discuss various ways to industrialize the statistical production process at the Australian Bureau of Statistics. The question is whether international standards would benefit survey quality in general. Some areas w here standards would be beneficial include computation of frequently used quality indicators such as error rates and design effects, as well as best practices for t ranslation of survey materials, handling non-native language respondents, and weighting for nonresponse. One must bear in mind that once a standard is issued it has to be con tinually updated and it is well-known that they can be difficult to enforce. If they are comprehensive, standards can overwhelm the practitioner and, as a result, unless mandated and audited, they are largely ignored."}, {"section_title": "Quality management philosophies in survey organizations", "text": "During the late 1980's and the early 199 0's some statistical organizations were under severe financial pressure and in some cases simultaneously criticized for not being sufficiently attentive to user needs. Governments in Sweden, Australia, New Zealand and Canada as well as the Clinton administration in the U.S. were all keen on improving efficiency and user influence within their respective statistical systems. It was natural for these organizations to look for inspiration in management theories and methods (Drucker 1985) and specifically on what was called quality management (Juran and Gryna 1988). In that newer literature it was po ssible to study the role of the customer, leadership issues, the notion of continuous quality improvement, and various tools that could help the statistical organization improve. Especially influential to survey practitioners was work by Deming (1986), since he emphasized the role of statistics in quality improvement. He vigorously promoted the idea that improvement work should be led b y statisticians, since they are trained in distinguishing between different kinds of process variation. He thought that there were too few statistical leaders advising top management in businesses and he wanted more proactive statisticians to become such leaders. He was especially keen on developing Shewhart's ideas about control charts as a means to distinguish between the different types of variation, namely common and special cause variation. Shewhart's improvement cycle Plan-Do-Check-Act was also part of Deming's thoughts on quality (Shewhart 1939). Management principles have, of course, existed since ancient times. Juran (1995) provides lots of examples of what was in place in, for instance, the Roman empire. Craftsmanship and a guild system were basic building blocks. There were methods for choosing raw materials and suppliers. Processes were inspected and improved. Workers were trained and motivated and customers got warran ties. All these features are found also in today's management systems. The more modern development includes quality frameworks or business excellence models such as Total Quality Management (TQM), International Organization for Standardization (ISO) stand ards, the Malcolm Baldrige quality award criteria, the European Foundation for Quality Management (EFQM), Six Sigma, Lean Six Sigma, and the Balanced Scorecard. These models are not totally different. They often share a common set of values and common criteria for excellence. Rather they represent a na tural development that can be seen in all kinds of work. Statistics Canada, Catalogue No. 12-001-X Thus, there has been a gradual adoption of quality management models and quality strategies in statistical organizations and a merging with concepts and ideas already used in statistical organizations. My personal timeline for this de velopment is t he following (readers are invited to come up with different sets of events and dates):"}, {"section_title": "1875", "text": "Taylor introduces what he called scien tific management; 1900-1930  The first quality framework in a st atistical organization containing more dimensions than relevance and accuracy; 1987-1989 Launching of the ISO 9000, Malcolm Baldrige Award, Six Sigma and EFQM models; 1990's Many statistical organizations start working with quality improvement and excellence models; 1997 The Monograph on Survey Measurement and Process Quality; 1998 Mick Couper introduces the concept \"paradata\" as a subset of process data;"}, {"section_title": "2001", "text": "The Eurostat leadership group on quality organizes the first conference on Quality Management in Official Statistics ; 2007 Business architecture ideas enter the survey world. From the mid 1990's and on quality management philosophies have had an enormous effect on many statistical organizations. The effect is not necessarily higher quality across the board (no one has ch ecked that). But the philosophies have led to an awareness in most organizations of the importance of good contacts with users and clients, and an aspiration in many of them to become \"the best\" or \"world class\". Quality is on the agenda."}, {"section_title": "The concept of quality", "text": "During the last decades it has become obvious that accuracy and relevance are necessary but not suffi cient when assessing survey quality. Other dimensions are also important to the users. The development of survey quality frameworks has taken place mainly within official statistics and has been triggered by the rapid technology development and other developments in society. These advanced technologies have created opportunities and u ser demands regarding potential quality dimensions such as accessibility, timeliness, and coherence that simply were not emphasized before. Decision-making in society has become more complex and global resulting in demands for harmonized and comparable statistics. Thus, there is a need for quality frameworks that can accommodate all these demands. Several frameworks of quality have been developed and they each consist of a n umber of quality dimensions. Accuracy and relevance are just two of these dimensions. For instance, the framework developed by OECD (2011) has eight dim ensions: relevance, accuracy, timeliness, credibility, accessibility, interpretability, coherence, and cost-efficiency (Table 1). Similar frameworks have been developed by Statistics Canada (Statistics Canad a 2002;Brackstone 1999), and Statistics Sweden (Felme, Lyberg and Olsson 1976;Ros\u00e9n and Elvers 1999). The Federal Statistical System of the U.S. has a strong tradition in emphasizing the accuracy component (U.S. Federal Committee on Statistical Methodo logy 2001) although it certainly appreciates other dimensions. Perhaps they are viewed as dimensions of a more nonstatistical nature that still need a share of the to tal survey budget. The International Monetary Fund (IMF) has developed a framework that differs from those of OECD, Australian Bureau of Statistics, Statistics Sweden, and Statistics Canada. IMF's framework consists of a set of prerequisites and five dimensions of quality: integrity, methodological soundness, Without sufficient accuracy, other dimensions are irrelevant but the opposite is also true. Very accu rate data can be useless if they are released too late to affect important user decisions or if they are presented in ways that are difficult for the user to access or interpret. Furthermore, quality dimensions are often in conflict. Thus, providing a quality product is a balance act where informed users should be key players. Typical conflicts exist between timeliness and accuracy, since it takes time to get accurate data through, for instance, extensive nonresponse follow-up. Another conflict is the one between comparability and accuracy since application of new and m ore accurate methodology might disturb comparisons over time (Holt and Jones 1998). Thus, many organizations have adopted a multi-faceted quality concept consisting not only o f accuracy but also other dimensions. We m ight talk about a quality vector whose components vary slightly between organizations both in number and in contents. There are a number of problems associated with the quality vector approach. First, the development has no t been preceded by user contacts. Producers of statistics have believed that users are interested in a specific set of dimensions even though it is obvious that a vas t majority of user s think that error structures are too complicated to grasp and assume that the producer should be resp onsible for delivering the best possible accuracy. In cases where the user or client has specific accuracy requirements a more in-depth dialog can take place between the two. In the rare studies that have investigated user perceptions of information on quality it turns out that users are mostly interested in dimensions that are easily understood, such as timeliness and indicators that are seemingly straight forward, such as response rates. The user wants the producing statistical organization to be credible, which translates into being capable of producing data with small or at least known errors and delivering them in a timely, reliable, and accessible fashion. The thought that it would be possible to produce a total quality measure based on weighted assessments of the differen t dimensions is not realistic, alth ough Mirotchie (1993) argues to the contrary. In that paper Mirotchie makes a case for a standard set of quality in dicators and provides a hypothetical illustration of indexing data quality indicators and computing an actual index (in this illustration the indicators are precision, nonresponse, reliability, timeliness and residuals). Even if a composite indicator in the form of an index were a possible development, the user would like to know which indicators contributed most to an index value. From a user's point of view the least favorable index value could still reflect a situation providing the highest quality. Rarely can a l ow accuracy be compensated by good ratings on other dimensions, not even in the case of election exit polls where timeliness is imperative. Accuracy is still necessary and there is wide agreement that all reputable organizations should meet accuracy standards (Scheuren 2001;Brackstone 2001). Phipps and Fricker (2011) provide an overview of q uality frameworks and literature on total survey error. Thus, we can agree that survey quality is a multi-faceted concept involving multiple features of a statistical product or service."}, {"section_title": "The quality movement's impact on statistical organizations", "text": "Just extending the quality fr amework from one or two dimensions to several is not sufficient to create a quality environment. In the late 1980 's and early 19 90's many statistical organizations became interested in quality issues beyond traditional aspects of data quality. Issues concerning customer satisfaction, communicating with customers, competition, process variability, cost of poor quality, waste, business excellence models, core values, best practices, quality assurance, and continuous quality improvement were suddenly p art of the ev eryday activities in many organizations. Successful organizations know that continuous improvement (Kaizen) is necessary to stay in business and they have developed measures that help them change. This is true also for producers of statistics. Changes that are suppo sed to improve the statistical product are tr iggered by user demands, competition from other producers and from producer values that emphasize continuous improvement as part of the general business environment. The measures that can help a statistical organization improve are basically identical to those of other businesses. They can be built on business excellence models such as the European Foundation for 114 Lyberg: Survey Quality Statistics Canada, Catalogue No. 12-001-X Quality Management (EFQM) (1999). The co re values of the EFQM model include results orientation, customer focus, leadership and constancy of purpose, management by process measures and facts, personnel development and involvement, continuous learning, innovation and improvement, development of partnerships, and public responsibility. This model has been adopted by the European Statistical System (ESS) a s a tool for national statistical institutes in Europe for achieving organizational quality. The thought is that good product quality, according to the dimensions mentioned (or some other product quality definition) cannot be achieved without good underlying processes used by the organization. It can also be argued that good product quality is achieved most efficiently and reliably by good process quality. If we view quality as a three-level concept it can be visualized as shown in Table 2."}, {"section_title": "Product quality", "text": "The deliverables ag reed upon are called the p roduct. It can be one or several estimates, datasets, analyses, registers, standard processes or other survey materials such as frames and questionnaires. Product quality is the traditional quality concept used when informing users or clients about the quality of the p roduct or service. It can be measured and controlled by means of degree of adherence to specifications and requirements for product characteristics adding up to quality dimensions of a framework. Measures of ac curacy and margins of error belong here. Also observations whether service levels agreements established with the client have been accomplished are rele vant. In line with quality management principles, it is also quite co mmon to conduct user satisfaction surveys to find out what users think about the products and services that are provided."}, {"section_title": "Process quality", "text": "All processes have to be designed so that they deliver what they are supposed to. This means that we have to have some kind of quality assurance perspective when processes are defined. For in stance, the process of interviewing implies that a number of elements must be in place for the process to deliver what is expected. Examples of elements are an effective selection of interviewers and a t raining program, a compensation system as well as supervision and feedback activities. Thus we aim at building quality into the process via the quality assurance. Quality control efforts are only used to check if t he process works as int ended. It cannot by itself be used to build quality into the process. In Section 4.4 this process view is discussed in more detail. Process quality is measured and controlled via selection, observation and analyses of key process variables, so called process data or paradata (Mo rganstein and Marker 1997;Couper 1998;Lyberg and Couper 2005). Theory and methods imported from statistical process control can help the producer distinguish between the two types of variation, common and special cause. As long as al l variation is contained within the upper and lower control limits associated with the control charts chosen, the process is said to be in statistical control and no process improvements are really possible by tryi ng to adjust individual outcomes. If there are observations falling outside of the control limits, usually set at 3 sigma, then we have indication s of special cause variation that should be t aken care of s o that the variation after adjustment is brought back to common cause variation. The fol lowing P-chart illustrates a possible situation: Thus, the action sequence is the following. First the roots of the special causes are taken care of so that these variations are eliminated. After that the process displays common cause variation only. If that variation is deem ed too large then the process has to change. The kinds of changes necessary are seldom obvious at the outset. Indeed perhaps several are necessary to decrease the process variation. Typically, a pr ocess improvement project is needed and the quality management literature has promoted a number of tools that are useful in such projects. Most of these tools are borrowed from statistics (control charts, experiments, regression analysis, Pareto diagrams, scatter plots, stratification) but there are also tools for identifying probable problem root causes (fishbone diagrams, process flow charts, brainstorming). The co mmon thinking is that improvement projects should be \"manned\" by people working with the process or by people very much familiar with the process in other ways. So metimes, we talk about forming an improvement team, where also the client or customer participates. In any improvement work suggested changes have to be tested. When Shewhart first developed his control charts he also suggested that improvement work should follow a se quence of operations, Plan-Do-Check-Act. What this sequence tells us is that any process changes suggested should be tested to see if they actually improve the process. If not, another change is made, and testing done again. Deming called this line o f thinking the Shewhart cycle but s ince Deming spent a lot of time promoting it, many eventually called it the Deming cycle. The changes sought after cou ld be d ecreased process variation, reduced costs, or increased customer satisfaction. The improvement project methodology is described in for instance Joiner (1994), Box and Friends (2006), Breyfogle (2003), and Deming (1986). Another way of checking the process quality is to use acceptance sampling. Acceptance sampling (Schilling and Neubauer 2009) can be applied in situations where process elements can be grouped in batches. The batches are controlled and based on the outcome of that control it is decided whether the batch should be approved or reworked. Acceptance sampling plans guarantee an average outgoing quality in terms of, say, error rate, but there is no dir ect quality improvement involved. It is a control instrument that is suitable for op erations such as coding , editing and scanning and then only when these processes are not really in statistical control. The method has been heavily criticized by Deming (1986) and others but can be th e only control means available in situations where staff turnover is high and there is no time to wait for stable processes. Global paradata (Scheuren 2001) are \"error\" rates of different kinds. Examples include nonresponse rates, coding error rates, scanning error rates, listing error rates, etc. In some operations the error rates are calculated using verification, which means that the operation is r epeated in some way. That is the case for the coding operation. In other operations the calculation can be based on a cl assification scheme, which is the case for nonresponse rate calculation. These global paradata tell us something about the process. They are process statistics, i.e., summeries of data. A large nonresponse rate indicates problems with the data collection process and a high coding error rate indicates problems with the coding process. From these summaries it is sometimes possible to distinguish common and special cause variation and decide what action to take. Some standardized processes can be controlled by means of simple checklists. Checklists are very effective when it is crucial that every process step is made and in the right order (Morganstein and Marker 1997). This is the case when airline pilots prepare for take-off. No matter how many times they have taken off, without a checklist the day will come when they forget an item. In st atistics production sampling is such a process, albeit with less severe consequences if items are missed. It might very well be the case that a statistical organization has a standardized process for sample selection and a checklist that can be used as a combination of work instruction and control instrument. There is a kind of checklist that can be used in more creative processes such as the overall survey design process. It is not possible to standardize the survey design process but it is possible to list a number of critical steps that always must be addressed. The list does not tell us how to address them. It just serves as a rem inder that an individual step should not be omitted or forgotten. Morganstein and Marker (1997) discuss this kind of checklist and call them (and the simpler checklists) Current Best Methods (CBM). They describe the CBM development process and how the CBMs can be used to decrease the process variation in statistical organizations. For instance, an organization might have seven different imputation methods and system s in its toolbox. It is c ostly to maintain these seven systems. It i s unlikely that they are equally efficient. If they are, it may not be economically feasible to keep them all. In this situation a CBM that describes fewer options to the organization seems like a go od idea. This could be accomplished by forming an improvement team consisting of the imputation experts and some clients. CBMs are supposed to be revised when new knowledge is obtained, which implies that there is an expiration date associated with every CBM. CBMs are of course \"best practices\" in some sense. Many organizations want best practices implemented and used. Morganstein and Marker offer a process for developing these best practices and keeping them current. It is beneficial for an organization if the variation in process design can be kept at a minimum. It then becomes easier to train people and change the process when it becomes unstable or when new methods are developed. On the other hand, if CBMs and other standards are not vigorously enforced within an organization, they will not be widely used and the investment will not pay off."}, {"section_title": "Organizational quality", "text": "Management is responsible for quality in its widest sense. It is the organization that provides leadership, competence development, tools for good customer relations, investments, and funding. The quality management field has given us business excellence models that can help us evaluate our statistical organizations in the same way other businesses are evaluated. The two main business excellence models are the Baldrige National Quality Program and the European Foundation for Quality Management (EFQM). These models consist of criteria to be checked when assessing an organization. The Ma lcolm Baldrige award uses seven main criteria: Leadership, strategic planning, customer and m arket focus, information and analysis, human resource focus, process management, and business results. Each criterion has a number of subcriteria. For instance, human resource focus consists of work systems, employee education, training and development, and employee well-being and satisfaction. The EFQM model has nine criteria: Leadership, strategy, people, partnerships & resources, processes, products & services, customer results, people results, society results, and key results. These models can be us ed for self-assessment or external assessment. The organization provides a description of what is in place regarding each criterion and the o rganization is scored based on that description. Typically self-assessments result in higher scores than external ones. It is very difficult to get a high score from external evaluators since the models are very demanding. For each criterion the organization is asked if it has a good approach in place somewhere in the organization. This is often the case. The next question is how wide-spread this good approach is within the organization. Many organizations lose momentum here, since there is very little truth in the mantra \"the good examples are automatically spread throughout an o rganization\". Instead good approaches usually have to be vigorously promoted before they are accepted within the organization. The third question asks whether the approach is periodically evaluated to check if i t achieves the results expected. This is where most organizations fail. Their usual strategy is to exhaust an approach until the problems are so great that the approach has to be replaced rather than adjusted. This strategy is, of course, disruptive and expensive and does not score highly in excellence assessments. The maximum number of points that can be obtained using these models is 1000 and very rarely does a winner get more than 450-600 points, which is an indication that there is a lot of room for improvement even in world class organizations. Some statistical organizations have used business excellence models for assessment. The Czech Statistical Office was announced Czech National Quality Award Winner for 2009 in the Public Sector category based on EFQM. The office got 464 points. Eurostat's leadership group on quality recommended the European national statistical offices to use the EFQM as a model for their quality work a nd Finland and Sweden are among those that have done so. Since the leadership group released its report in 2001 (see Lyberg, Bergdahl, Blanc, Booleman, Gr\u00fcnewald, Haworth, Japec, Jones, K\u00f6rner, Linden, Lundholm, Madaleno, Radermacher, Signore, Zilhao, Tzougas and van Brakel 2001) other frameworks and standards have been developed. The European Statistical System has launched its Code of Practice, which consists of a number of principles with associated indicators. Regarding some principles, however, the indicators are more like clarifications. The li st of principles resembles other lists that have been developed by the UN and other organizations. External assessments are probably more reliable than internal ones. There are a number of reasons for that. One is that it is difficult to criticize your peers since you have to interact with them in the future or if your own product or service will be assessed by t hose peers in the future. Experiences from Statistics Sweden and Statistics Canada show that self-assessments are limited in their capability of identifying serious weaknesses (see Section 5.3)."}, {"section_title": "Some specific consequences for statistical organizations", "text": "Most statistical organizations have adopted quality management ideas to varying degrees and with varying success. As pointed out by Colledge and March (1993) it is possible to list a number of obstacles associated with such implementation. For a government agency it can be difficult to motivate its staff th rough monetary incentives, since there are restrictions on how tax money can be spent. The variety of users and products makes the dialog between the service provider and the user complicated and as mentioned neither the users, or for that matter the providers are totally familiar with all the biases and other quality problems that are present in statistics production. The effect o f errors o n the uses can vary and are o ften unknown. To complicate matters further, unlike most other businesses, suppliers are not very enthusiastic. In other businesses suppliers get paid while statistical organizations must motivate theirs, the respondents, who are seldom even given a cash incentive. Statistics Canada, Catalogue No. 12-001-X On the other hand statistical organizations have a great advantage when it comes to applying quality management principles. A st atistical organization knows how to collect and analyse data that can guide improvement efforts. One of the cornerstones in quality management philosophies is that decisions should be based on data and businesses that do not have support from statisticians are often unaware of data quality problems, which can have consequences for their decision-making. By and large, though, a statistical organization is not different from any other bu siness and it is quite possible to apply quality management ideas to improve all aspects of work."}, {"section_title": "Examples of quality initiatives in statistical organizations", "text": "In this section we wi ll provide some examples of initiatives that statistical organizations have engaged in as a result of a general interest in quality in society."}, {"section_title": "The total survey error", "text": "Perhaps the most important thing to notice is that research and development in survey design, implementation, sampling and nonsampling errors, and the effect of errors on the data analysis continue to thrive. Data with small errors is the m ajor goal for reputable organizations, which is indicated by the steady flow of textbooks on data collection, sampling, nonresponse, questionnaire design, measurement errors, and comparative studies. New textbooks are in progress covering gaps such as business surveys, translation of survey materials, and paradata. There are journals such as the Journal of Official Statistics, Survey Methodology, and Survey Practice that are entirely devoted to topics related to statistics production in a wide sen se. Numerous other journals such as the Public Opinion Quarterly, the Journal of the American Statistical Association, and the Journal of the Royal Statistical Society devote much space to survey methods. The Wiley series on Survey Methodology and its associated conferences (on panel surveys, telephone survey methods (twice), measurement errors, process quality, business surveys, testing and evaluating questionnaires, com puter assisted survey information collection, nonresponse, and comparative surveys) have been very successful and that is th e case also for the continuing workshops on nonresponse and total su rvey error. Thus, there is no shortage of ideas regarding specific error sources and their treatment. Admittedly there are areas that are understudied such a s specification errors, data processing errors and the impact of errors on the data analysis but by and large there is a healthy interest in knowing more about su rvey errors. The challenge lies in communicating this kno wledge to people working in statistical organizations and in developing design principles that can be used to improve statistics production. There is a noticeable gap between what is known through research and what is known and applied in the statistical organizations. Thus, staff capacity building seems to be a continuing need, especially since the common idea that good e xamples spread like ripples within and between organizations is a myth. If that indeed were the case quality would by now be fantastic everywhere. Since it is not, many organizations have developed extensive training programs (Lyberg 2002)."}, {"section_title": "Risk and risk management", "text": "One element of quality management that has entered the survey world is risk and risk management. Eltinge (2011) even talks about Total Survey Risk as an alternative to the total survey error paradigm. The identification and management of risks is an important part of modern internal auditing (Moeller 2005) and is perhaps the only major element that is missing in quality management frameworks such as EFQM. An error source can be seen as more risky than another and should, therefore, be handled with m ore care and resources than another less risky. For instance, not having an effective system for statistical disclosure control is seen as a very risky situation. Unlawful data disclosure is very rare histo rically, but when it happens it could potentially destroy future data collection attempts. Certain design decisions can be seen as risky. For in stance, if we choose a data collection method that does not fit the survey topic we might get estimates that are so far from the truth that the results are useless. An example might be to study sensitive behaviors using face to face or telephone interviewing instead of a self-administered mode. There are also technical risks that need to be identified and assessed. For instance, the U.S. National Agricultural Statistical Service (Gleaton 2011) like many others has plans fo r disaster recovery. Groves (2011) and Dillman (1996) both discuss how the production culture and the research culture within a statistical organization might view risks in different ways. Change in statistical organizations is generally slow and there are so metimes good reasons for that. Change might result in failures such as unsuccessful implementation, large costs and decreased comparability. So i n some sense both producers and users have a tendency to be hesitant toward changes suggested by researchers and innov ators and that might be one reason why change takes a long time. It is very common to have parallel measurements for some time to handle risks associated with implementing a new method or system. According to Groves (2011) the production culture and the users have had the final say about any changes, at least up until now. At the same time innovation is badly needed in many production systems and there are examples of stove-pipe organizations that do not have much time left Statistics Canada, Catalogue No. 12-001-X (to remain unchanged) because the resources to maintain their systems are simply not there. So e ven though there is resistance against change, lack of resources and competition will make sure tha t statistical organizations become more process-oriented and efficient. Reducing the number of sy stems and applications and developing and using more standardization seem to be one road forward."}, {"section_title": "The client/customer/user", "text": "The advent of quality management ideas in statistical organizations has made the receivers of statistical products and services more visible. Commercial firms have always talked about the client or the cu stomer while government organizations have tended to call them users. In any case the recognition of someone who is supposed to u se the endproducts has not been obvious to some providers. Admittedly the user has been a s peaking partner since the beginning of t he survey industry. In the U.S., conferences for users were quite frequent already 50 years ago (Dalenius 1968;Hansen and Voight 1967). During six months 1965-66, for example, the U.S. Census Bureau organized 23 user conferences across the country and there were also advisory groups. The advisory nature of contacts with users has prevailed in many countries. The user conference format still exists but user input is now complemented by ot her means such as public discussions and internet forums. Rarely have users been directly involved in the planning and design of surveys. Even when it comes to discussions about the quality of data, producers have acted as stand-in users. The quality frameworks are a good example. The quality dimensions were defined with minimal consultation with users. The literature on how users perceive information about quality is extrem ely limited (Groves and Lyberg 2010). Also, we do not know if the in formation on quality that we provide is useful to them (Dalenius 1985b). In fact, an educated guess is tha t many times it is no t. In many surveys the users are many and sometimes unknown and their information and analytical needs cannot be foreseen ahead of time. It is often possible to single out one or a few main users to communicate with, but many of t he design and quality problems are so complicated that a vast majority of users expect the service provider to deliver a product with the smallest possible error. Hansen and Voight stated that accuracy should be sufficient to avoid interpretation problems. Today there seems to be consensus among many that what users are interested in are products and services that can be trusted, i.e., the service provider should be credible. It is impossible for most users to check levels of accuracy. Aspects that an average user can discuss are issues such as timeliness, accessibility and relevance. Detailed discussions about technical matters and design trade-off issues including accuracy and comparability are more difficult to have. During recent decades the user has indeed become more prominent. Some organizations develop service level agreements together with a main user or client, where requirements of the final product or service are listed and can be checked at the time of delivery. Many organizations conducting business surveys have created units that continuously communicate with the largest businesses, since their participation and provision of accurate information is absolutely essential for the estimation process (Willimack, Nichols and Sudman 2002). The large businesses are not users in the strict sense. Th ey are important suppliers often with an interest in the survey results. Another common communication tool is the customer satisfaction survey. The value of such surveys is li mited due to the acquiescence phenomenon and problems finding a knowledgeable respondent who is also willing to respond. Also, m any customer satisfaction surveys are based on self-selection resulting in zero inferential value. In those surveys the results can only be viewed as lists of issues and concerns that some customers convey. Such information can, of course, be v ery valuable but is not suitable for estimation purposes. Many survey organizations now conduct user surveys on a continuing basis (Ecochard, Hahn and Junker 2008)."}, {"section_title": "The process view", "text": "Quality management has reemphasized the importance of having a process view in statistics production. To view the production process as a series of actions or steps towards achieving a particular end that satisfies a user, leads to a good product quality. Process quality is an assessment of how far each step meets defined requirements or s pecifications. One way o f controlling the process quality is to collect process data that can vary with each repetition of the process. The interesting process variables to monitor are those that have a l arge effect on the process's end result. Thus to check a process for stability and variation we need mechanisms for identifying, collecting and analysing these key process variables. The quality management science has given us tools such as the Ishikawa fishbone diagram to identify candidates for key process variables. The statistical process control methodology has given us tools to distinguish between special and common cause variation and how to handle these two variation types. Usually we use control charts originally developed by Shewhart (Deming 1986;Mudryk, Burgess and Xiao 1996) to make those distinctions. Then, again, we use methods from quality management to adjust the process if necessary. Examples include flowcharts, Pareto diagrams, and other simple means for the production team to identify the root causes of problems (Juran 1988). Statistics Canada, Catalogue No. 12-001-X Process data have been used to check on processes used in statistics production since the 1940's, first within the U.S. Census Bureau and then at Statistics Canada and to some extent also in other age ncies. Typical processes that were checked included coding, keying and printing and the process data were mainly error rates. Some of t he process checks used at the U.S. Census Bureau were so complicated and expensive th at their value was questio ned (Lyberg 1981), especially since the associated feedback loops were inefficient and not always aiming for the root causes of the errors. It was common that operators were blamed for system problems and at the time there was no emphasis on continuous quality improvement. The thinking at the time was more directed toward verification and correction. Morganstein and Marker (1997) developed a gen eric plan for process continuous improvement that can be used in statistics production. They had worked in many statistical organizations since the 1980 's and observed that quality thinking was not really developed in most of t hem. Their generic plan was built on their first-hand exp eriences and the general quality management ideas laid out by e.g., Juran (1988), Deming (1986), Box (1990), and Scholtes, Joiner and Streibel (1996). In essence the plan consists of s even steps:"}, {"section_title": "Standardization and similar tools", "text": "One way of keeping process quality in control is to reduce variation by encouraging the use of s tandards and similar documents. Colledge and March (1997) discuss four classes of documents."}, {"section_title": "Statistical business process models", "text": "During recent years concepts lik e business process models and business architecture have become part of quality work in some statistical organizations. To make production processes more efficient and flexible they can be seen as part of a business architecture model (Reedman and Julien 2010). In statistics production a generic statistical process model is j ointly developed by UNECE, Eurostat, and OECD. Any system redesign should be driven by customer demands, risk assessments and new developments. The architectural principles behind this th inking are summarized in Doherty (2010), which discusses architecture renewal at Statistics Canada."}, {"section_title": "Some of the principles are:", "text": "\uf0b7 Decision-making should be corporately optimal, which entails centralization of informatics, methodology support and processing. These principles are very si milar to those we ide ntify when we apply quality management principles from the various frameworks and excell ence models described previously. The principles represent a move from decentralization to more corporate level thinking. Many statistical organizations realize that stove-pipe thinking is a thing of the past and that a move to more centralization is necessary."}, {"section_title": "Measuring quality", "text": "Thus, quality is a multi-faceted concept and measuring it is a complicated task. We have noted that survey quality can be viewed as a t hree-dimensional concept associated with the final product, the underlying p rocesses that lead to the product, and the organization that provides the means to carry out the processes and deliver the product or service in a successful way. There are basically two ways to measure quality. One is to directly estimate the total survey error or some components thereof. The ot her is to measure indicators of quality with the ho pe that they i ndeed reflect the concept itself."}, {"section_title": "Direct estimates of the total survey error", "text": "The existing decompositions of the mean squared error described in, for instance, Hansen et al. (1964), Fellegi (1964), Anderson, Kasper and Frankel (1979), Biemer and Lyberg (2003), Weisberg (2005), and Groves et al. (2009) are all incomplete in the sense that they do not reflect all error sources. It is seldom possible to compute the MSE directly in practical survey situations because this usually requires a parameter estimate that is essentially error free. However, it is possible to obtain a second best estimate of the true parameter value if there are resources available to collect data using some \"gold standard\" methodology that is not affordable or practical in a normal survey setting. This is "}, {"section_title": "123", "text": "Statistics Canada, Catalogue No. 12-001-X the standard evaluation methodology when the tru e parameter value can be uniqu ely defined. Gold standard methods are seldom error-free but they can to varying extents provide better estimates, and the difference between the regular estimate and the gold standard estimate can serve as an estimate of the bias, which is the methodology used in census post enumeration surveys (Un ited Nations 2010). Often an evaluation concerns a specific error component such as cen sus undercount, nonresponse bias, interviewer variance or simple response variance, since we want information not on total survey error per se but rather on the components' relative contribution to the total survey error so that root causes of problems can be identified and relevant processes improved. Large evaluation studies are very rare since they are so demanding and t heir value is sometimes questioned (United Nations 2010). Smaller regular evaluation studies, on the other hand, are n ecessary to get indications of process and methodological problems."}, {"section_title": "Indicators of quality", "text": "Continuing reporting of total survey error is a formidable task and no survey organization does that. In stead organizations provide indicators or statements regarding quality. For instance, according to Eurostat's (2009a) handbook for quality reports the following indicators should be measured: The common theme here is that these paradata summary items are indicato rs that can be calculated without conducting special studies. The set of indicators that can be calculated directly from the survey data is b y definition quite limited and their value questionable. For instance, to include overcoverage but not undercoverage just because only the former can be calculated directly from the available data does not make sense. It is undercoverage that poses the greatest coverage problem in surveys. Admittedly, the handbook prescribes the producer to assess the potential for bias (both sign and magnitude) but it is not clear how this should be ac complished. The producer is urged to i nclude evaluation and quality control results, if such information exists as well. Level of effort measures for processes such as questionnaire design and coder training would be welcomed. There is no standard reporting format for such qualitative and quantitative information. In any case, the key indicator list becomes severely limited when compared to the full list of main error sources and it is hard to see how they are perceived by the users and how they can be used by the producer to improve the process. The producer needs a more complete list of indicators to be able to measure or assess various levels of quality to make sure that the design implementation is in control or to be able to mount a quality improvement project. The initial survey design must be m odified or adapted during the implementation to control costs and maximize quality. Biemer (2010) discusses four strategies for reducing costs and errors in real time, i.e., continuous quality improvement (CQI), responsive design (Groves and Heeringa 2006), Six Sigma (Breyfogle 2003), and adaptive total desig n and implementation. When the continuous quality improvement strategy is used, key process variables are identified and so are process characteristics that are critical to quality (CTQ). For each CTQ, real-time, reliable metrics for the cost and quality are developed. The m etrics are continuously monitored during the process and intervention is done to ensure that costs and quality are within acceptable limits. The responsive design strategy was developed to reduce nonresponse bias in face to face interviewing. It includes three phases. In the experimental phase a few design options are tested (e.g., regarding incentive level). In the main data collection phase the option chosen in the experimental phase is implemented and the implementation continues until phase capacity is reached. In the nonresponse follow-up phase special methods are i mplemented to reduce nonresponse bias a nd control the data collection costs. Such methods include the Hansen-Hurwitz double sampling scheme, increased incentives, and using more experienced interviewers. Again the efforts continue until further reductions of the nonresponse bias are no longer cost-effective. Six Sigma is the most developed business excellence model since it relies so heavily on statistical methods. It contains a larg e set of techniques and tools that can be used to control and improve processes. Adaptive total design and i mplementation combines control features of CQI, responsive design and Six Sigma and does that so that it simultaneously monitors multiple error sources. Biemer and Lyberg (2012) give several examples of CTQs and metrics for various survey processes. For instance, regarding the measurement process attributes that are CTQs might include the abilities to identify and repair problematic survey questions, to detect and control response errors, and to minimize interviewer biases and variances. Corresponding metrics might include missing data item by question, refusal rate by size of business, results of replicate measurements, suspicious edits actually changed, and field work results by interviewer. The metrics can be analyzed using statistical process control or Statistics Canada, Catalogue No. 12-001-X analysis-of-variance methodologies. Different related metrics can be displayed together in a das hboard fashion. For instance if one CTQ is the ability to discover interviewer cheating we m ight want to have a dashboard showing the metrics average interview length by interviewer and the distribution of some sensitive sample characteristic, also by interviewer."}, {"section_title": "Self-assessments and audits", "text": "The quality management philosophy has intro duced the concepts of se lf-assessment and audit into statistics production. We are anxious to know what users, clients, owners and other stakeholders think about the products and services provided by the statistical organization. There are a number of tools available for this kind of ev aluation. We have already mentioned the customer satisfaction survey. Other tools include em ployee surveys, internal audits and external audits. Custo mer surveys can shed light on what users think about products and services provided. They can be used to determine user needs and to identify what product characteristics really matter to the users. Another line of questioning might concern the image of the organization and how it compares to the images of other organizations, be they competitors or not. The customer satisfaction survey is very common in society. Often it cannot be used to make inference to the target population of users due to its methodological and conceptual shortcomings. The abundance of satisfaction surveys in society, developed and implemented by people with no formal training in survey methods, contributes to lukewarm receptions in more serious settings resulting in nonresponse and measurement errors. For instance, the 2007 E urostat User Satisfaction Survey consisted of two separate surveys. One was launched on the Eurostat webpage and the target population consisted of 3,800 registered users. Only tho se registered users that entered the website during the data collection period were exposed to the survey request and this led to a response rate around 5%. The second survey used email that was sent to a num ber of main users identified by Eurostat. This more controlled environment generated a response rate of 28%. These surveys also have problems identifying the mo st suitable respondent. If the \"wrong\" respondent is cho sen within an organization this will most certainly lead to u ninformed and misleading results. The simplest type of self-assessment is the questionnaire or checklist that is filled out by the s urvey manager. An example is one from Statistics New Zealand. It is a checklist that consists of a number of indicators or assertions such as \"information needs are regularly assessed through user consultation\", \"good and accessible do cumentation\", \"indicators of accuracy regularly produced and monitored\", and \"presentation standards met\". The manager is asked to answer yes or no to each assertion and make a comment if deemed necessary. Statistics Sweden had a similar system in place where one of the questions was \"has overall quality of your product improved, declined or stayed the same compared to last year?\" When results were compiled for these three categories for the entire organization, a very small proportion of the managers reported declining quality, a somewhat larger proportion reported improved quality, while a vast proportion reported status quo. The managers simply did not h ave the proper means to assess overall quality. Furthermore, vague quantifiers like \"regularly\", \"good\", and \"meeting standards\" invite generous assessments. Also most managers do not want to look bad and status quo becomes a perfect escape route. This system of self-assessment was even tually abandoned by Statistics Sweden. It is possible to increase the value of these assessments by asking additional questions concerning details about how and when quality work was conducted. Some organizations use internal teams that audit important products. Julien and Royce (2007) describe a quality audit of nine products at Statistics Canada, where the purposes were to identify weaknesses and their root causes as well as identifying best practices. Review teams of assistant managers were formed so that each reviewer reviewed three different programs. The main weakness with an approach like this is the internal feature itself. Every reviewer knows that sooner or later it is his or her turn to be reviewed and there is a risk that this fact might hold them back. It is also internal in the sense that users are not explicitly present in the review process. In its general au dit program on data quality management, however, Statistics Canada puts great emphasis on its user liaison system (Julien and Born 2006), which is one of the five systems forming the a gency's quality assurance framework, the others being corporate planning, methods and standards, dissemination, and program reporting. A further variant of self-assessment is when it precedes an external audit. Statistics Netherlands (1997) describes how the Department of Statistical Methods is assessed by its staff. The assessment resulted in a listing of weak and strong areas that were later examined by an external team. Typically an external audit uses so me kind of benchmark like a set of rule s, a standard, or a code of pra ctice for assessment purposes. The audit then results in a number of recommendations for the organization or the indiv idual product or service. Recently a general system for evaluating the total survey error has been developed at St atistics Sweden. Sweden's Ministry of Finance wants quality evaluation results to be able to monitor quality improvements over time. Survey "}, {"section_title": "125", "text": "Statistics Canada, Catalogue No. 12-001-X quality must be assessed for many surveys, administrative registers, and other programs within the agency so there is need for some indicators that can serve as proxies for actual measures of quality. At the same time, the assessment process must be thorough, the reporting simple and the results credible. For each of the error sources specification, frame, nonresponse, measurement, data processing, sampling, model/estimation, and revision eight key p roducts were rated poor, fair, good, very go od, and excellent regarding each of five criteria. The criteria were knowledge of risks, communication with users, compliance with standards and best practices, available expertise, and achievement toward risk mitigation and/or improvement plans. The rating guidelines varied by criterion. For knowledge of risks they were: The evaluation process started with a s elf-assessment done by each of the eight key products. These reports and other relevant documents were studied by two external reviewers who then met with product owners and their staff to discuss the product processes. After that the reviewers presented detailed assessments and scored each product. The procedure identified important areas to improve within but also across p roducts. In this first ev aluation round measurement error turned out to be a problematic area for almost all the key products. As any other approach at measuring or indicating total survey error this one does not really reflect total mean squared error. It requires thorough documentation of processes and improvements made and it is highly d ependent on the skills and knowledge of the external reviewers. This study is r eported in Biemer, Trewin, Japec, Bergdahl and Pettersson (2012)."}, {"section_title": "Quality profiles", "text": "In continuing surveys there is an opportunity to develop quality profiles. Such documents contain all that is known about the quality of a continuing survey or other statistical product assembled over a number of years. Quality profiles exist for only a few m ajor surveys, all, except one, conducted in the U.S., including the Current Population Survey (Brooks and Bailar 1978), the Survey of Income and Program Participation (Jabine, King and Petroni 1990;Kalton, Winglee and Jabine 1998), the Schools and Staffing Survey (Kalton, Winglee, Krawchuk and Levine 2000), and the American Housing Survey (Chakrabarty and Torres 1996). The exception is the British Household Panel Survey (Lynn 2003). The main problem with a quality profile is that it is not timely, since it compiles results from often timeconsuming studies of quality. The goal of the quality profile is to identify areas where knowledge about erro rs is deficient so that improvements can be made. Kasprzyk and Kalton (2001) and Doyle and Clark (2001) review the use of quality profiles in the U.S."}, {"section_title": "Where do we go from here?", "text": "Quality management ideas have been influential in many survey organizations. Concepts such as leadership, quality culture, problem prevention, customer, competition, risk assessment, process thinking, improvement, business excellence, and business architecture are increasingly discussed by leaders of survey organizations, e.g., Trewin (2001), Pink (2010), Fellegi (1996), Brackstone (1999), de Vries (1999), Groves (2011), and Bohata (2011). It seems as if the survey community is moving in a dir ection where statistics production becomes more streamlined and cost-effective but the pace is sl ow. Some organizations have started using a quality management model for self-assessment and steering purposes. EFQM is th e recommended model for national statistical institutes within the European Statistical System and a couple of institutes, the Czech Republic and Finland, have even applied for their respective national EFQM awards. Some marketing firms are certified according to the ISO 9001 quality management standard and others are certified according to the ISO 2 0252 standard for market, opinion, and social research. This development ought to result in quality improvements but we cannot be really su re Statistics Canada, Catalogue No. 12-001-X until we sta rt collecting relevant data. One thing is su re, though. Some customers prefer service providers that are certified, have won awards or can show evidence that they are working according to some quality framework or model. Very few customers would think that this is a negative thing. The margins of error that we associate with estimates are usually too short, since they do not include all sources of variation. Point estimates can be off due to biases. Ideally it would be good if we were able to produce estimates of the total survey error instead of what we produce today. Such a development is, ho wever, not realistic. We are not in a position to produce such estimates, not even occasionally, for reasons that have to do with finances, timing and methodology. That leaves us with indicators of total survey error and i ts components. Such indicators are of lim ited value to the users. Users simply do not kno w what to do with information on nonresponse rates, response variance measured by reinterviews or edit failure rates. On the other hand, such indicators are very useful to the producers of surveys. For instance, reinterview studies can identify fabrication and survey questions with poor response consistency. A majority of users appreciate the service provider's credibility and part of the credibility is the ability to present accurate data. Another important part of credibility is the willingness of the providers to evaluate their own quality and to report the results of suc h evaluations. Even if these evaluations show p roblems, it is better for the provider to find the problems than if enti ties outside the provider's organization find them. Most users do not want to become involved in discussions about errors and trade-offs between errors and for good reasons. It is simply too technical and confusing. If we accept that a good process quality is a prerequisite for a good product quality, we should gradually improve the processes so that they approach ideal bias-free ones. In that way the variance of an estimate becomes a goo d approximation of the mean squared error. Despite endless discussions and a myriad of survey quality initiatives, practices have not changed much (Lynn 2004; Pink, Bo rowik and Lee 2010;Groves 2011;Boh ata 2011). Perhaps the lack of c ompetence within survey organizations is one root cause of the slow pace. Many theories and methodologies including statistics, IT, management, communication, and behavioral sciences are needed in survey research. The behavioral sciences are needed to identify the root causes of nonsampling errors. If errors are just quantified no improvement can happen. Current training programs emphasize sampling, nonresponse, coverage and estimation in the presence of these. Other processes and error sources such as measurement and data processing are not dealt with to the same extent. This leads to a situation where studies on measurement error and data processing error are rare compared to studies on, say, nonresponse. There is a con siderable confusion regarding concepts and methods in both the producer and the user camps. Another cause of slow pace might be the consensus philosophy that rules in some organizations when it comes to decision-making regarding changes. This philosophy is one of compromise. Input from many stakeholders is gathered and a de cision is us ually based on the smallest common denominator, which is never a goo d standard. Furthermore, arriving at this compromise usually takes a long time and lots of resources. This approach is very far from Plan-Do-Check-Act. Survey quality is not an absolute entity. Current quality reporting a la one-size-fits-all is not working since fitness for use is defined by each user. Quality dimensions such as timeliness, comparability and accessibility should be decided together with main users while best possible accuracy given various constraints is the responsibility of the service provider. Have the survey quality discussion and the adoption of quality management strategies resulted in better data? We do not kno w. Survey quality h as not been assessed in a before-after fashion. There is a t endency towards greater standardization and centralization, which should prove costefficient but when it comes to data quality some indicators point in the wrong direction. For instance, in many countries nonresponse rates are increasi ng and error properties of mixed-mode, translation of survey materials, and other design features are not fully known or are different across cultures. There is no design formula, which results in shaky trade-off decisions and problems deciding about intensities with which quality control should be applied. There is a persistent quest for best practices in survey organizations but implementation is difficult and scattered. There is definitely a great need for an upgrade in the competence level across the board. A struc tured international competence development program for service providers is necessary as is a syst ematic international collaboration on how to best design and implement surveys. We must serve our users better by providing data with small errors. We can do this by better combining our knowledge about statistics and cognitive phenomena with the principles of quality management. The great positive note is the overwhelming positive attitude toward quality improvement among statistical organizations around the world."}, {"section_title": "Data collection in a coca-growing community in rural Peru", "text": "This section describes the coca-growing community, and the primary data collection strategies applied in our study and the lessons learned."}, {"section_title": "Description of the research area", "text": "The research area was located in t he Upper Tambopata valley at the border with Bolivia, one of the most remote and difficult to access Amazon rainforest areas in Peru (UNODC Office in Peru 1999). This valley lies in the Vilcabamba-Amboro Biodiversity Corridor in c lose proximity to national protected areas (see Figure 1). The entire population of the upper Tambopata valley is composed of immigrants, especially descendants from the Aymara indigenous population. Aymara is a native ethnic group originally from the Andes and Altiplano regions of South America. During the 1950 s, most of the farm ers were seasonal immigrants who left their Altiplano subsistence plots for only three to six months every year, and made the 320 km journey to the upper Tambopata valley to cultivate coffee on their individually owned agricultural plots (Collins 1984). Over time, most farmers became permanent settlers in the upper Tambopata valley, and cultivate coffee as their main cash crop (ibid). Before 1989, coca cultivation in the upp er Tambopata valley was very minor. Small-scale coca production was limited to self-consumption or lo cal markets for traditional uses such as coca chewing by Andean farmers and miners. After 1989, coca cultivation was intensified, primarily in the neighboring upper Inambari valley. The change did not appear to be in response to increases in local demand or external demand by traditional users (UNODC Office in Peru 1999). C oca from those valleys is consi dered as low quality due to its bitterness, and it is in less demand for traditional chewing than coca from Cuzco region (Caballero, Dietz, Taboada and Anduaga 1998). Those increases were therefore related to narcotic traffic demand. In recent years, large increases in c oca cultivation in the upper Tambopata valley have been consistently reported by the Statistics Canada, Catalogue No. 12-001-X United Nations (UN), as ob served in Table 1. The percentage variation per year in the upper Tambopata valley is above the annual change of around 4% at national level.  (2009). Coca provided by the upper Tambopata valley and upper Inambari valley see ms to mainly supply cross border trade associations between Peruvian and Bol ivian narcotics traffickers. Bolivia remains the world's third largest producer of cocaine, and it is a significant transit zone for cocaine of Peruvian-origin (U.S. Depart ment of State 2009). Those valleys constitute a s trategic coca production area for narcotics traffickers due t o their proximity to an e xternal exit route (UNODC Office in P eru 1999). Coca leaves are not always transformed into cocaine in the agricultural plots. Narcotics traffickers seem to t ake advantage of the large quantities of coca leaves transported to urban areas, ostensibly for traditional user markets. This coca is then purchased and processed at hidden facilities in urban areas near the Bolivian border. In this way the risk of being caught by authorities is reduced. From Bolivia the cocaine is dispatched to Brazil and Europe (Garcia and Antezana 2009). Coca cultivation does not necessarily translate into better quality of life for the farmers in South America (Davalos, et al. 2008). According to the last pop ulation census, the living conditions in San Pedro de Putina Punco (SPPP), the district located in the heart of the Upper Tambopata valley, are difficult: 72% of the houses are rammed earth constructions, 88% have dirt floors, 16% have public electricity, 12% have public water, and only 9% have access to public sewage (INEI 2007). This situation is common in the major coca growing areas in Peru, where 70% of the inhabitants continue to live in poverty, and 42% in extreme poverty (Commission on Narcotic Drugs 2005)."}, {"section_title": "Data collection strategies and lessons learned", "text": "A feasibility study to test if farmers would answer cocarelated questions was conducted in December 2007. The pilot study for the designed questionnaire took place in May 2008, and the final survey was conducted between June and August 2008. The feasibility and pilot studies and the final survey were focused on the farmers located in San Pedro de Putina Punco (SPPP), a district in the upper Tam bopata valley which is located in the deepest rainforest. All the farmers in the research area produce coffee as cash crop and some supplement their income with coca cultivation. There are five coffee co-operatives in SPPP. F armers have to become a member of one of these co-operatives in order to be able to sell their coffee, because restrictions to coffee intermediaries are in place. The final survey was only c onducted among the m embers of four of t hese co-operatives because most of the members of the remaining co-operative are based in San Juan del Oro, a district outside the research area. The final survey consisted of a structured questionnaire which focused on agricultural production and social capital. The questionnaire was comprised of 15 sections: Asking farmers about their coca gro wing area is a sensitive question. Farmers who cultivate large areas of coca fear that the information provided could be accessed by authorities responsible for eradication programs. Thus, they might have concerns about the possible consequences of giving a tru thful answer should the information become known to a third party. In these cases, the farmers need to be assured anonymity. Farmers could also be tempted to provide socially desirable answers to the interviewers. Coca has become an important focal symbol in the indigenous population's struggle fo r self-determination (Office of Technology Assessment 1993). Coca \"yes\", cocaine \"no\" constitutes the slogan of indigenous people (Henman 1990); the formulation tries to clearly separate traditional uses (\"coca\") from narcotics trafficking (\"cocaine\"). Hence, traditional uses such as co ca chewing are ethnicity symbols (Allen 1981) and their persistence could be related to feelings of nationalism in Peru (Hen man 1990). In this sense, it could be expected that farmers would not find it very problematic to indicate that they grow coca, as long as they can associate it with traditional uses. On the o ther hand, due to the association of larger production areas with illegal activities, coca growers may underreport the total extent of their coca production areas in an attempt to give the impression that they are growing only for traditional use. Several strategies can help to reduce the potential biases associated with question sensitivity, item and unit nonresponse and deliberate misreporting. These strategies include: confidentiality assurances; careful selection of the data collection mode and setting of the sensitive question format; and tailoring interviewer characteristics and behaveior (see Coutts and Jann 2008;Tourangeau and Yan 2007). Further information on the implementation of these strategies in our case study is provided below."}, {"section_title": "Establishing trust, and anonymity assurances", "text": "Farmers in c oca growing areas tend to distrust external people. In this particular area, we found out that they trust the coffee co-operative directors. One of the directors of the coffee co-operatives signed a letter of presentation authorizing our research related to agricultural cultivation. The letter was shown to the farmers prior to conducting the survey. A pilot test conducted with and without the presentation letter demonstrated that the letter was important to reduce survey participation refusals. In the survey introduction, it was also indicated by the interviewer that the cooperative director authorized the survey because the director expected the results to benefit co-operative members. In addition, farmers were clearly told at the beginning of the survey that the data collected would remain confidential, and the academic purpose of the questionnaire was Statistics Canada, Catalogue No. 12-001-X high-lighted (see App endix 1a). This anony mity assurance was short and precise in order to minimize suspicion among farmers as s uggested by S inger, Hippler and S chwarz (1992). Coca growing was treated as a c ommon and ordinary behavior in the research region, and a long and elaborate confidentiality assurance might have aroused farmers' reservations instead of al leviating them. A b rief reminder of the assurance of confidentiality was included in the middle of the questionnaire, before the questions related to traditional coca uses and prior to the sensitive question on the coca area. The reminder stated: \"In thi s part of the survey, we will ask questions about co ca uses and cultivation. Please remember that the survey is anonymous and that there are no correct or incorrect answers\" (See Appendix 1b). This follows Willis (2005) who mentions that it is important to have warm-up questions and an announcement of the switching to the sensitive topic to reduce resistance to answer."}, {"section_title": "Data collection mode", "text": "Paper and pencil self-administration as data collection method was initially considered to try to reduce interviewer bias. However, during the feasibility study, it be came evident that many farmers, even those with above elementary school education (52% of the population; INEI 2007), were not able to read effortlessly. Farmers work in their fields almost all day long and do not have many opportunities to practice their reading skills. Similarly, audio computerassisted self-interviewing (ACASI) the method of choice for collecting data on sensitive topics in developed countries (Mensch, Hewett and Erulkar 2003), was out of the scope of this project due to the lack of equipment and power supply, and the computer illiteracy in the research area. The use of computers was likely to have increased the anxiety and suspicion about the su rvey as described in the African situation by Mensch, et al. (2003). Therefore, a face-to-face interview was the data collection mode selected and emphasis was placed on the selection of interviewers, their training and behavior."}, {"section_title": "Selection of i nterviewers, training, and interviewers' behavior", "text": "One problem with the selection of the interviewers was the lack of sufficiently educated professionals in the research area. Thus, a group of ten students from the nearest public university, located 16 hours away from the research area, was chosen as interviewers. All of the interviewers had Aymara or Quechua ethnic backgrounds; this was an attempt to partially match interviewer-respondent characteristics. It was though t that this could increase the lik elyhood of participation because the matching was likely to increase trust and sympathy between the interviewer and the respondent (Tourangeau and Yan 2007). The interviewers presented themselves as students from the local university, and no additional information was given about any university or organization outside of the country financing the study to avoid potential misun derstandings and reduce distrust among the res pondents. During the pilot study, some farmers had indicated concerns about externally financed coca eradication programs and therefore references to external institutions were minimized. As a result, only partial information was given to the respondents. This is unconventional, but under the specific circumstances of the study, there was no other alternative without facing potential security problems. For training, the interviewers first attended a two -day workshop in Puno city, followed by a th ree-day workshop in the research area. The same group of interviewers also conducted the pilot study to test the questions and questionnaire with the objective of identifying comprehension, recall, judgement and acceptability issues in the survey, and allowing rephrasing, eliminating or adding questions. The pilot study also allowed assessment of the performance of the interviewers, and in some cases identified areas requiring tailored training based on the feedback on performance. For example, at the beginning one of the interviewers was hesitant abo ut asking the coca-related question and that interviewer obtained a higher than average number of nonresponses to the sensitive qu estion. After tailored training, the interviewer was able to modify their interviewing approach."}, {"section_title": "Format of the sensitive question", "text": "The question format presupposed the sensitive behavior under study, as suggested by Tourangeau and Yan (2007). Therefore, farmers were not first asked if they had any coca areas, and then asked for the total extent of their coca areas. Instead, all farmers were directly requested to state the total extent of their coca areas (\"What is your coca growing area in meters or hectares?\"). However, it was found during the pilot study that the farm ers did not feel comfortable with this question format and they either skipped the question or simply withdrew from the survey. As a con sequence, the question format was changed and a forgiving wording was used instead. Farmers were asked: \"How many 'little bushes of coca' do you have in your agricultural plot?\" Thus, the farmer could answer \"Only a little, I have\u2026 coc a bushes\". Even though a difference was ha rdly perceptible, with the former question it was more difficult for the farmers to start their answers with \"Only a little\u2026\". So, using the latter question, it was easier for the farmers to a dd apologetic explanations to their answers making them feel more relaxed. This latter sensitive question format also had the advantage of employing a familiar wording for the Aymara who commonly use diminutives in their daily conversations. On the oth er hand, this question format might indirectly Statistics Canada, Catalogue No. 12-001-X imply that the interviewer expected that the respondent had a small number of coca bushes lik ely resulting in underreporting. Consequently, while nonresponses were avoided using this latter question format, underreporting was st ill expected to some extent."}, {"section_title": "Time period for conducting the survey and data collection setting", "text": "The farmers' agricultural plots are scattered in the mountainous Amazon rainforest in Peru. It was difficult to reach individual farmers on their agricultural plots for the survey. Therefore, to conduct the survey, we mainly took advantage of the Saint Peter's Day celebration and the General Assembly meetings of the co-operatives in June and August 2008 respectively, when the farmers congergated in the town square. Attendance to the General Assembly meetings is m andatory for all co-operative members so all of the targeted respondents would have been accessible at those events. The only way to reach or exit the town square is through an unpaved road. To take advantage of this, the survey was condu cted in a large tent that was erected on the unpaved road on those key days. This tent had ten divisions, one for each pair of i nterviewer and respondent. Absolute privacy was not enforced because during the pilot study, it was found that farmers did not feel comfortable being the \"only one\" who was being interviewed; they preferred to see others doing the same. However, farmers were not able to overhear other farmers' responses. Given that all farmers have to use the same unpaved road to reach the town square regardless of their specific geographic location, potential geographical biases, which in turn can be related to important variables such as farm size and incom e, were likely minimized in this research."}, {"section_title": "Sampling representativeness", "text": "A convenience sampling method was applied, but at the end of t he survey, we asked the farmers for their cooperative registration number and used the co-operative registration lists to infer the sample's representativeness. The co-operative registration number provided by the farmer was wr itten on separate piece of paper and was not attached to the respond ent's questionnaire. Respondents were informed about this procedure and were able to witness the procedure. The four co-operatives under study have 3,265 members in SPPP. Table 2 shows the number of respondents per cooperative. The number of collected questionnaires amounted to 508. In total, 12 respondents were excluded from the sample because their co-operative registration number was missing. In two cases, the f armers had r efused to provide this information and in ten cases, the interviewers had forgotten to ask the respondents about their registration number at the end of the interview.Therefore the absence of information was more associated with interviewer error than with the farmers' unwillingness to provide this information. In order to test for representativeness of the sample, the distribution of the co-operative registration numbers obtained from the survey sample was compared with the distribution of the co-operative registration numbers from a simulated simple random sample without replacement obtained from co-operative lists. The co-operative lists were ordered by the registration number of the co-operative members and co-operative registration numbers are associated with the members' date of registration. Thus, most of the older farmers have lower registration numbers and the younger farmers have higher ones. Unfortunately, the cooperatives did not have other m embership data available such as total land, coffee or coca hectares that might be used to select a str atified random sample. Two ty pes of tests were used for comparison of the samples: a two-sa mple Wilcoxon rank-sum (Mann-Whitney) test and a two-sample Kolmogorov-Smirnov test for equality of distribution functions. The first test assesses how probable it is that the two groups come from the same distribution, and assumes that differences observed are caused by chance fluctuation. The second test is similar to the first one, but in addition it is sensitive to differences in both the location and shape of the empirical cumulative distribution functions of the two groups. The results of both tests failed to reject the null hypothesis of eq uality of di stribution between the survey sample and the sim ulated simple random sample at a significance level of 0.05. Thus, the results suggest that the survey sample is equivalent to a simple random sample, and therefore representative of the population under study."}, {"section_title": "Survey results and validation issues 3.1 Survey results", "text": "The survey response rate was around 90%, which is well above the minimum recommended response rate of 60% (Punch 2003). From the 496 completed questionnaires, 19 Statistics Canada, Catalogue No. 12-001-X respondents (less than 4%) did not answer the coca-related question. When comparing the des criptive statistics of socio-economic, institutional, and coca-related variables, there were some significant differences between all the observations (without the non-respondents) and the 'sensitive question non-respondents' (see Appendix 2). The sensitive question non-respondents were all male, with a larger percentage of Aymara ethnic backg round, and more children. In addition, a larger percentage of them used coca as medicine. Interestingly, significantly more non-respondents are highly risk averse (73 .7%) compared to all the other respondents (28.6%). This could indicate a potential fear of the 'sensitive question non-respondents' of interviewer disclosure of information to third parties. The setup of the risk aversion test followed by Binswanger (1980) is presented in Appendix 1c. Basic comparative descriptive statistics of coca and non coca growers are presented in Table 3. The number of valid questionnaires was 477 , if we do not accoun t for the non respondents of the sensitive question. Of th em, 64% indicated that they are coca growers. There are no statistically significant differences with respect to general socio-economic characteristics (age, sex, ethnic group, and number of children) between coca and non-coca growers. The only difference was obs erved in education. Non-coca growers have more years of schooling than coca growers. Coca growers have less total and primary forest areas, and more fallow land than non coca growers, although these differences are no t statistically significant. Coca and non-coca growers have similar coffee and staple food ar eas. On the contrary, coca growers and non-coca growers show statistically significant differences in the social capital variables. More non-coca growers than coca growers find it important to obey national law. On the other hand, less non-coca growers than coca growers have experienced a negative change in trust towards their neighbors during the last five years, and have wo rked in community activities during the last year. There is a statistically significant relationship between coca growing and traditional uses. A higher percentage of coca growers than non-coca growers chew c oca and uses coca as medicine. More importantly, more coca growers find it easier to sell coca leaves than non-coca growers in the hypothetical case that they would cultivate coca for commercial purposes. Finally, it is important to mention that the average number of coca bushes is rela tively low, wh ich could be due to underreporting of commercial coca growing areas or to coca cultivation only for self-co nsumption, or both. It is not possible to distinguish between those two scenarios, which makes it e asier for com mercial coca growers to disguise themselves as coca growers who produce for traditional uses."}, {"section_title": "Validation issues", "text": "The validity of individual responses cannot b e verified directly because there is littl e prior empirical research on this topic, and there is an absence of other sources of confirming data. However, it is possible to provide a rough comparison between the survey data and the total area of coca production recounted by international organizations for the upper Tambopata valley using satellite data. The United Nations Office on Drugs and Crime (UNODC 2009) indicates that 940 hectares of coca were cultivated in the upper Tambopata valley in 2008. The conventional coca cultivation density for regions with traditional coca growers could be b etween 35,000 and 40,000 bushes per hectare (UNODC 2001) (During the 90s, the coca cultivation density was lower, between 20,000 and 25,000 bushes per hectare (UNODC 2009)). The coca cultivation density in the particular valley is relatively low because coca growers intercrop coca with coffee and staples, although the yields per bush have incr eased during the last years (UNODC 2009). Therefore, it is expected that the total number of coca bushes for this valley would be approximately from 32.9 to 37.6 million. Our sample of 477 respondents (excluding farmers who did not report their co -operative registration number and non respondents to the sensitive question) reported a total of 960,000 coca bushes. This sample corresponds to 14.6% of a total of 3,265 co-operative members in SPPP. Thus, extrapolating for the total number of co-operative members located in the SPPP district would result in a total of 6.6 million coca bus hes. In addition, we need to consider that the upper Tambopata valley also includes San Juan del Oro district which h as around the same population as SPPP district (INEI 2007). Under the very strong assumption that farmers in SPPP behave similarly to the farmers in San Juan del Oro -at least in terms of coca cultivation -this would double the num ber of coca bushes for the entire upper Tambopata valley to around 13.2 million. This last estimate is between 35 and 40% of the 32.9 to 37.6 million obtained from UNODC satellite data. This result is in the expected range of reporting on sensitive issues. For reporting on abortion, this range is b etween 35 to 59% (Fu, Darroch, Henshaw and Kolb 1998), and for the use of opiates or cocaine between 30 to 70% (Tourangeau and Yan 2007)."}, {"section_title": "Summary and conclusions", "text": "Coca, a raw material for the production of cocaine, is cultivated in Col ombia, Peru and Bolivia. In the latter two countries, traditional uses of coca by indigenous populations date back to around 3000 B.C. (Riv era, et al. 2005). Nevertheless, asking farmers about the extent of their coca cultivation areas is con sidered a sensitive question. Coca growers are afraid of eradication programs even if they do not sell coca to the narcotics traffic business because it is difficult to distinguish between coca growers whose production is commercially oriented and those who produce only for self-consumption. Thus, farmers tend not to participate in surveys, not to answer any sensitive questions, or to underreport their coca cultivation areas in an attempt to minimize their identification for possible eradication. Against this background, household-level data collection procedures need to consider and evaluate strategies to reduce nonresponses and misreporting. Most of the strategies used in our res earch area in Peru were based on bes t practices reported in the literature review. Some of the strategies that worked in our case were establishment of trust with the farmers using a presentation letter from a coffee co-operative director, confidentiality assurance at the beginning and in the middle of the questionnaire, matching of interviewer-respondent ethnic background characteristics, training of interviewers to reduce their hesitance to ask sensitive questions, changing the format of the sensitive question to a familiar and forgiving wording, and non enforcement of absolute privacy to prevent each farmer from feeling that they were the \"only one\" who was interviewed. The validity of farmers' individual responses on their coca area extensions cannot b e checked because the topic has produced little prior empirical research, and there is an absence of other sources of household-level confirming data. Thus, the extent of misreporting was evaluated using aggregate data. The results suggest that farmers only r eported between 35 to 40% of their actual coca areas. Still, those values are between the ranges o f what could be expected for answers to sensitive questions. In terms of survey nonresponse and sensitive question nonresponses, the results were more encouraging indicating values of 10% and of around 4%, respectively. When conducting the survey, we mainly took advantage of celebrations and co-operative General Assemblies for which farmers congregated in town, since farmers are otherwise highly disp ersed in the rainforest. The survey followed a conv enience sampling method but it was po ssible to test the representativeness of this sample because all of the farmers are registered in one of the co-operatives in the research area. The obtained sample was compared with a simulated simple random sample without replacement Statistics Canada, Catalogue No. 12-001-X where each farmer had the same probability to be selected by chance from the co-operative member lists. There were no statistical differences in the distribution functions, so the sample is equivalent to a simple random one. The main drawback of this approach is that after the interview, we needed to ask the respondents for their co-operative member number. Even though the respondents were told that the cooperative identification number was not attached to their questionnaires, some farmers might have had doubts about it, and this could have had effects on confidentiality assurance credibility in following interviews due to word spreading. On the other hand, comparing the characteristics of nonrespondents to sensitive questions with the rest of respondents indicates that n on-respondents were hig hly risk averse. Even though the number of non-respondents was small (less than 4% of the total sample), this could suggest that the main reason for item non-reporting is the fear of the consequences of the information leaking to third parties. The coca areas reported by the farmers were on average very small. This could be an a ttempt by commercial coca growers to appear to be cultivating only fo r self-consumption. Coca growing for traditional uses does not have a negative connotation per se given that it is a sy mbol of ethnicity and the indigenous population's struggle for selfdetermination (Office of Technology Assessment 1993). It is not possible to distinguish farmers who underreported the extent of their coca cultivation areas from those who grow coca for self-consumption. Unfortunately, commercial coca growers can take advantage of t his situation to continue growing coca under the guise of traditional uses."}, {"section_title": "Appendix 1", "text": ""}, {"section_title": "Relevant parts of the questionnaire A) Presentation:", "text": "Good morning/afternoon/night. My name is __________________. I am a student at______________. We are conducting a survey to identify the risks and vulnerabilities of coffee producers in your community. The coffee co-operative directives are aware of this survey and believe that the result could benefit the community. If you decide to an swer our questionnaire, you may skip any questions or withdraw from this study at any time. The data collected in this survey will remain CONFIDENTIAL and will be use d only for ACAD EMIC purposes. Your answers and opinions are extre mely important for the cooperative and us. Would you be prepared to respond to some questions? a) Yes (proceed) b) No (thank the respondent, withdraw the survey, and indicate the characteristics of the person in format 1) B) Coca Related Questions: In this part, we will ask about coca uses and cultivation. Please, remember that this survey is anonymous and that there are no correct or incorrect answers. Fergusson, D., Boden, J. and Horwood, L. (2008). The developmental antecedents of illicit drug us e: Evidence from a 25-Yea r longitudinal study. Drug and Alcohol Dependence, 96, 165-177. Fu, H., Darroch, J., Henshaw, S. and Kolb, E. (1998) Mensch, B., Hewett, P. and Erulkar, A. (2003). The reporting of sensitive behavior by adolescents: A me thodological experiment in Kenya. Demography, 40, 2, 247-268. Obando, E. (2006)  w w w. s t a t c a n . g c . c a Survey Methodology, December 2012"}, {"section_title": "Methodology", "text": "We consider the model-assisted approach for survey data sampled from a finite population . P We assume that the population P is divided into a fixed number of imputation classes, which are typically unions of some strata. Within each imputation class, the study variable from a population unit follows a superpopulation. Let t y be the study variable at time point , \uf064 be the indicator of whether t y is observed, and Since imputation is carried out independently within each imputation class, for simplicity of notation we assume in this section that there is only a single imputation class. Throughout this paper, we consider nonmonotone nonresponse and assume that there is no nonresponse at baseline = 1. t The PSI is past-value-dependent if where P is with respect to the superpopulation. When nonresponse is monotone, the p ast-value-dependent PSI becomes ignorable , since we either observe all past values o r know with certainty th at t y is missing if it is missing at 1, t \uf02d and an imputation method using linear regression proposed by Paik (1997) can b e used. When nonresponse is nonmonotone, however, the past-value-dependent PSI is nonignorable because the response indicator at time t is statistically dependent upon previous values of t he study variable, some of w hich may not be observed. In this case Paik's method does not apply."}, {"section_title": "Imputation for subjects whose first missing is at t", "text": "Let > 1 t be a fixed time point and 1 r \uf02b be the time point at which the first m issing value of y occurs. When 1 = , r t \uf02b i.e., a subject whose first missing value is at , t our proposed imputation procedure is the same as th at for the case o f monotone nonresponse (Paik 1997). However, we still need to provide a justification since we have a different PSI. It is sho wn in the Appendix that, under assumption (1), 1  1 1  1   1  1 1  1( ,..., , = = = 1, = 0) ( | ,..., , = = = 1, = 1) = 2,..., , where E is the e xpectation with respect to the superpopulation. Denote the quan tity on the first line o f (2) by is the same as the quantity on the second line of (2), which is the conditional expectation of an observed t y given observed 1   1, ..., t y y \uf02d and can be estimated by regressing t y on 1   1, ..., , t y y \uf02d using data from all subjects having observed t y and observed 1   1, ..., . t y y \uf02d Note that (2) is a counterpart of (5) in Xu et al. (2008) under the last-value-dependent assumption, which is stronger than the past-value-dependent assumption (1). Under a stronger assumption, we ar e able to utilize more data in regression fitting. Suppose that a sam ple S is selected from P according to a given probability sampling plan. For each  To illustrate, we consider the case of = 3 t or 4. The horizontal direction in Table 1 corresponds to time points and the vertical direction corresponds to different m issing patterns, where each pattern is represented by a vector of 0's and 1's with 0 indicating a missing value and 1 indicating an observed value. For = 3 t and = 2, r as the first of the two steps, we consider missing data at time 3 with first missing at time 3, i.e., pattern (1,1,0). According to imputation model (2), we fit a regression using data in pattern (1,1,1) indicated by \uf02b (used as predictors) and \uf0b4 (used as responses). Then, imputed values (indicated by ) \uf064 are obtained from the fitted regression using data indicated by \uf02a as pr edictors. For = 4 t and = 3, r imputation in pattern (1,1,1,0) can be similarly done using data in pattern (1,1,1,1) for regression fitting. Statistics Canada, Catalogue No. 12-001-X Table 1 Illustration of imputation process Step 1: = 2, = 3 r t Step 2: = 1, = 3 r t Time  Time  Pattern  1  2  3  1  2  3 ( Step 1: = 3, = 4 r t Step 2: = 2, = 4 r t Step 3: = 1, = 4 r t Time Time Time Pattern 1 2 3 4 1 2 3 4 1 2 3 4 \uf02b : observed data used in regression fitting as predictors. \uf0b4 : observed data used in regression fitting as responses. \uf0c4 : imputed data used in regression fitting as responses. \uf02a : observed data used as predictors in imputation. \uf064 : imputed values. and, hence, linear regression under the model-assisted approach can be used to estimate , 1 . is not linear, one of the methods described in Section 2.3 can be applied."}, {"section_title": "Imputation for subjects whose first missing is at 1 < \uf02b r t", "text": "Imputation for a su bject whose first missing value is at time 1 < r t \uf02b is more complicated and very different from that for the case of monotone nonresponse. This is because when 1 < r t \uf02b and nonresponse is monotone,  whereas (4) does not hold when nonresponse is nonmonotone (see the proof in the Appendix). Hence, we need to construct different models for subjects whose first missing value is at 1 < . r t \uf02b It is sho wn in the Appendix that, when   We now explain how to use (5) to impute missing values at a fixed time point . t r r y y \uf066 be the quantity on the first line of (5). If , t r \uf066 is known, then t y can be imputed by , 1 ( ,..., )."}, {"section_title": "t r r y y \uf066", "text": "Otherwise, it needs to be estimated based on (5). Unlike in model (2) or (4), the conditional expectation on the second line of (5) is conditional on a missing t y ( = 0),  Table 1. Following the first step for = 3 t discussed in Section 2.1, at the second step, we impute missing values with = 1 r in pattern (1,0,0). According to imputation model (5), we fit a regression using data in pattern (1,1,0) indicated by \uf02b (used as predictors) and \uf0c4 (previously imputed values used as Statistics Canada, Catalogue No. 12-001-X responses). Then, imputed values (indicated by ) \uf064 are obtained from the fitted regression using data indicated by \uf02a as predictors. For = 4, t following the first step discussed in Section 2.1, at the second step ( = 2) r we fit a regression using data in pattern (1,1,1,0) indicated by \uf02b (used as pr edictors) and \uf0c4 (previously imputed values used as responses). Then, imputed values (indicated by ) \uf064 at = 4 t in pattern (1,1,0,0) are obtained from the fitted regression using data indicated by \uf02a as predictors. At step 3 for = 4, t we fit a regression using data in patterns (1,1,0,0) and 1,1,1,0 \uf064 at = 4 t in patterns (1,0,0,0) and (1,0,1,0) are obtained from the fitted regression using data indicated by \uf02a as predictors. Although at tim e , t imputation has t o be c arried out sequentially as = 1, ..., 1, r t \uf02d imputation for different time points can be done in any order. This can be seen from the illustration given by Table 1, where the imputed values at = 3 t are not involved in the imputation process at = 4 t or vice versa, although some observed data will be repeatedly used i n regression fitting. When data come according to time, it is natural to impute nonrespondents in the order = 2, ..., .   This means that t y and t y \uf025 have the sam e conditional expectation, given 1 y are imputed values. Although all observed data at any time t are used for the estimation of ( ), t E y some but not all observed data at time < t are utilized in imputation to avoid biases under nonignorable nonresponse. This is different in the ignorable nonresponse case, where typically all past observed data can be used in regression imputation."}, {"section_title": "Regression for imputation", "text": "The conditional expectations in 5  is very large. This issue is commonly referred to as the curse of dimensionality. Thus, we consider the following alternatives under the additional assumption that the dependen ce of t \uf064 on 1  1,..., t y y \uf02d is t hrough a linear combination of 1   1,..., .  7, it is shown in the Appendix that   We refer to the method of simply applying linear regression as the linear reg ression imputation method, and the method of applying kernel regression to the index r z as the one-dimensional index kernel regression imputation method. An advantage of one-dimensional index kernel regression imputation over kernel regression imputation is that only a one-dimensional kernel regression is applied and, thus, it avoids the curse of dimensionality and has smaller variability. These methods can also be applied to the case o f is not linear. In theory, estimators such as the estimated means based on kernel regression or one-dimensional index kernel regression imputation are asy mptotically unbiased, bu t they may not be better than those based on linear regression imputation when the number of sampled subjects in each ( , ) t r category is not very large. The p erformances of the estimated means based on li near regression, kernel regression, and one-dimensional index kernel regression imputation are examined by simulation in Section 3."}, {"section_title": "Estimation", "text": "We consider the estimation of the finite popu lation total or the mean of t y at each fixed , t which is often the main purpose of a survey study. At any , and it y \uf025 be the im puted value using one of the methods in Section 2 when = 0."}, {"section_title": "it", "text": ""}, {"section_title": "\uf064", "text": "The finite population total and the mean of t y can be estimated by \u02c6= a n d = , respectively, where i w is th e survey weight con structed such that, in the case of no nonresponse, \u02c6t Y is an unbiased estimator of the finite population total at time t with respect to the probability sampling. The superpopulation mean of t y can also be estimated by . is an unbiased estimator of the finite population size N and, for some simple sampling designs, it is exactly equal to . N The survey weights should also be used in the regression fitting for imputation. Under the same conditions given in Cheng (1994), \u02c6t Y or t Y based on kernel regression or onedimensional index kernel regression imputation is consistent and asymptotically normal as the sample size increases to . \uf0a5 The required conditions and proofs can be found in Xu (2007). If we ap ply the lin ear regression imputation method as discussed in Section 2.3, then the resulting estimated mean at t may be asymptotically biased. This bias is small if the function , t r \uf079 can be well approximated by a linear function in the range of the data values. On the other hand, kernel or one-dimensional index kernel regression imputation may require a much larger sample size than that for linear regression imputation. Hence, the overall performance of the estimated mean based on linear regression imputation may still be better, as indicated by the simulation results in Section 3."}, {"section_title": "Variance estimation", "text": "For assessing statistical accuracy or inference such as constructing a confidence interval for the mean of t y at , t we need variance estimators of \u02c6t Y or t Y based on imputed data. Because of the complexity of th e imputation procedure, it is difficult to obtain explicit formulas for variance of \u02c6t The bootstrap method (Efron 1979) is then considered. A correct bootstrap can be obtained by repeating the process of imputation in each of the bootstrap samples (Shao and Sitter 1996 ). Let \uf071 be the estimator under consideration. A bootstrap procedure can be carried out as follows. \uf071 is the bootstrap variance estimator for \u02c6. \uf071 In application, each * b \uf071 can be calculated using the th b bootstrap data * ( , , ), w multiplied by the number of times unit i appears in the th b bootstrap sample. Note that the same *b i w can be used for all variables of interest, not just . y i\nWe now discuss variance estimation of PSA est imators under the assumed response model. Singh and Folsom (2000) and Kott (2006) discussed variance estimation for certain types of PSA es timators. Kim and Kim (2007) discussed variance estimation when the PSA estimator is computed with the maximum likelihood method. We consider varia nce estimation for the PSA estimator of the form 3  holds and the linearization in (23) can be exp ressed as x i i i y r are independent and identically distributed (IID), then are IID, we can apply the standard complete sample method to esti mate the variance of which is asymptotically equivalent to the variance of See Kim and Rao (2009). To derive the variance estimator, we assume that the variance estimator for som e ij \uf057 related to the joint inclusion probability, where for any g with a finite second moment and We also assume that To obtain the total variance, the reverse framework of Fay (1992), Shao and Steel (1999), and Kim and Rao (2009) is considered. In this framework, the finite population is first divided into two groups, a population of respondents and a population of nonrespondents. Given the population, the sample A is selected according to a probability sampling design. Thus, selection of the popu lation respondents from the whole finite population is treated as the first-phase sampling and the selection of the sample respondents from the population respondents is t reated as the secon d-phase sampling in the re verse framework. The total variance of HT \uf068 can be written as The conditional variance term where which follows by (25) and the existence of the fourth moment. See Kim, Navarro and Fuller (2006). The second term 2 where * h \uf067 is defined after (27). Therefore, is consistent for the variance of the PSA estimator defined in V should be taken into consideration, so that a consistent variance estimator can be constructed as in (29)."}, {"section_title": "Empirical results", "text": "We study \u02c6t Y or t Y in (9) based on the proposed imputation methods at each time point . t We first consider a simulation with a normal population for th e 's."}, {"section_title": "t y", "text": "An application to the SIRD data is presented next. To examine the performance of t he proposed methods for th e SIRD, a simulation with a population generated using the SIRD data is presented in the end. We have implemented the proposed imputation methods in R (R Development Core Team 2009). To fit the required nonparametric regressions, we use the R function loess with default settings, which fits a local polynomial surface in one or more regressor variables. The required linear regressions are easily fit in R usi ng the Statistics Canada, Catalogue No. 12-001-X function lm. Our implementations of the proposed methods include error checking; (such as ensuring that there are sufficient points for regression fitting at each stage) which is particularly important in bootstrap and simulation settings where the imputation methods are replicated many times, and each iteration cannot be examined manually. We defaulted to an overall mean imputation in cases where there were not enough data points to fit a regression."}, {"section_title": "Simulation results from a normal population", "text": "A simulation study was co nducted with normally distributed 1 ,..., , y y n = n 2,000, and = 4. T A single imputation class and simple random sampling with replacement was considered. In the simulation, 's y i were independently generated from t he multivariate normal distribution with mean vector (1.33, 1.94, 2.73, 3.67) and the covariance matrix having the A R(1) structure with correlation coefficient 0.7 and unit variance; all data at = 1 t were observed; missing data at = 2, 3, 4 t were generated according to and \uf046 is the standard normal distribution function. The unconditional probabilities of nonresponse patterns are given in Table 2. For comparison, we included a total of nine estimators of the mean of : t y they are sample means based on (1) the complete data (u sed as the gold standard); (2) respondents with adjusted weights assuming the probability of response is the same within each imputation class; (3) censoring and linear regression imputation, which first discards all observations of a subject afte r the first missing value to create a da taset with \"monotone nonresponse\" and then applies linear reg ression imputation as described in Paik (1997); (4) the proposed kernel regression imputation; (5) the proposed linear regression imputation; (6) the proposed one-dimensional index kernel regression imputation using the sliced inverse regression to obtain \u02c6; r \uf067 (7) the kernel regression imputation proposed in Xu et al. (2008) based on the last-value-dependent PSI; (8) the linear regression imputation based on a regression between respondents at time t and observed and imputed values at tim e points 1, ..., 1 t \uf02d (treating imputed as observed); (9) the linear regression imputation based on a reg ression between respondents at time t and observed data from units with the same missing pattern at time points 1, ..., 1. t \uf02d Table 2 Probabilities of nonresponse patt erns in the simulation study (Normal population) because method (7) requires the last-value-dependent assumption that is stronger than 1, method (8) treats previously imputed values as observed in regression, and method (9) requires the following condition that is not true under (1): \uf02d is a fixed missing pattern. Finally, as we discussed in Section 2.3, method (5) is also biased for 3 t \uf0b3 since linear regression is not an exactly correct model. However, methods 5, 8, and (9) may still perform well when the biases are not substantial, because the use of a simpler model and more data in regression for imputation may compensate for the loss in biased imputation. Furthermore, any assumption on the PSI m ay hold only approximately and it is desired to empirically study various methods in any particular application. For the case of = 1, r t \uf02d linear regression imputation is applied as discussed in Section 2.1. Hence, methods (3) -(6), (8) -(9) all give the same results when = 2. t Table 3 reports (based on 1,000 simulation runs) the relative bias and standard deviation (SD) of the mean estimator, the m ean of \uf0b6 boot SD , the bootstrap estimator of SD based on 200 bootstrap replications, and the coverage probability of the approximate 95% confidence interval (CI) obtained using point estimator \uf0b6 boot 1.96 SD . \uf0b1 \uf0b4 The following is a summary of the results in Table 3. 1. The sample mean based on ignoring missing data is clearly biased. Although in th e case of = 4 t its relative bias is only 3.5%, it still leads to a very low coverage probability of the confidence interval, because the SD of the estimated mean is also very small."}, {"section_title": "The bootstrap estimator of standard deviation per-", "text": "forms well in all cases, even when the mean estimator is biased. 3. t Y based on censoring and linear regression imputation has negligible bias so that the related Statistics Canada, Catalogue No. 12-001-X confidence interval has a coverage probability close to the nominal level 95%; but it has a large SD when = 3 t or = 4. t The inefficiency of t his method is obviously caused by discarding observed data from nearly 50% of sampled subjects who have intermittent nonresponse. Its performance becomes worse as t increases. 4. t Y based on the proposed kernel regression imputation has a relative bias between 0.0% and 0.5%, but the bias is large enough to result in a poor coverage performance of the related confidence interval at = 4. t"}, {"section_title": "t", "text": "Y based on the proposed linear regression imputation has negligible bias as well as a variance smaller than that of t Y based on kernel regression. The related confidence interval has a coverage probability close to the nominal level 95%. 6. t Y based on the p roposed one-dimensional index kernel regression imputation is generally good but slightly worse than that based on the linear regression imputation. 7. t Y based on methods (7) -(9) has non-negligible bias when = 3 t or = 4, t which resu lts in poo r performance of the related confidence interval. Although the kernel regression is asymptotically valid, in this simulation study the total number of subjects is 2,000 and, according to Table 2, the average numbers of data points used in k ernel regression under patterns ( , ) = t r (4,1) and (4,2) are 238 and 152, respectively, which may not be enough for kernel regression and lead to some small biases in imputation. On the other hand, linear regression is more stable and works well with a sample size such as 152. Although linear regression imputation has a bias in theory, the bias may be small when is linear."}, {"section_title": "Application to the SIRD", "text": "The SIRD is an annual survey of about 31,000 companies potentially involved in research and development. The NSF sponsors this survey as part of a mandate requiring that NSF collect, interpret, and analyze data on scientific and engineering resources in the United States. The survey is conducted jointly by the U.S. Census Bureau and NSF. The surveyed companies are as ked to provide information related to their total research and development (RD) expenditure for the c alendar year of the survey. The SIRD deterministically surveys some companies each year by placing them in a certainty stratum, since they account for a large percentage of the total RD dollar investment in the U.S. The remaining companies that appear in the survey are sampled each ye ar using a st ratified probability proportionate to size (PPS) sam pling design. Longitudinal m easurements are ava ilable on the core of companies that are sampled with certainty and on other companies that happen to be selected each year. For the purposes of illustrating our imputation methods, we rest rict attention to only those companies that were selected for the survey in each of the years 2002 through 2005 ( = 4), T and companies that provided a respon se in 2002. For documentation on the SIRD and detailed statistical tables, we refer to the document titled Research and Development in Industry: 2005, available from http://www.nsf.gov/statistics/nsf10319. Additional information on t he Business R&D and Innovation Survey is available online at http://bhs.dev.econ. census.gov/bhs/brdis/ and http://www.nsf.gov/statistics/ srvyindustry/about/brdis/. We divide the data into two imputation classes. One class consists of all companies contained in a certainty stratum for each of the four years; the other consists of the rest of companies. Within each imputation class, the data take the form ( , ), n where it y represents the total RD expenditure for company i at time = t 1 (2002), 2 (2003), 3 (2004), 4 (2005). The sample size here is = n 2,309 for the certainty strata class and = n 1,039 for the non-certainty strata class. Missingness is nonmonotone and the missing percentages for the years 2003, 2004, and 2005 were 10.4%, 14.0%, and 18.8%, for the certainty strata class, and 15. 2%, 20.7%, and 26.0% for the non-certainty strata class. Table 4 shows the estimated totals and standard errors obtained by using the methods (2) -(9) described in the simulation study in Section 3.1. As discussed in the end of Section 2.1, in each of the proposed imputation methods we use linear regression when 1 = . r t \uf02b The standard errors shown in Table 4 were computed using the bootstrap method. Table 4 also displays estimated totals obtained when missing data are filled in by the values that were put in place by the Census Bureau in order to produce the officially published data tables (officially published data tables are available from http://www.nsf.gov/statistics/ pubseri.cfm?seri_id=26). The method that was used by the Census Bureau to handle missing data wh en producing these published data tables (which we call the \"cu rrent method\") was ratio imputation for companies with prior year data using imputation cells formed by industry type; we refer to Bond (1994 ) for further details. Table 4 also presents the estimated RD totals obtained from respondents only with no weight adjustment which indicate that ignoring the missing data leads to biased estimates. Methods (3) - 9give comparable results, which is likely due to the strong linear dependence in the data so that theoretically biased methods exhibit negligible bias. The estimated totals based on the current method are comparable to those based on the proposed methods for the certainty strata case, but are different in the non-certainty strata case. The m ethod of censoring and linear regression has similar SD t o the proposed methods because the number of data points discarded under censoring is not too large. In the certainty strata imputation class only 10% of the sample has an intermittent nonresponse pattern and the percentage of complete cases is 72%. In the non-certainty class, only 9% of the sample has an intermittent nonresponse pattern and the percentage of complete cases is 66%."}, {"section_title": "Simulation results based on the SIRD population", "text": "An additional simulation study was conducted using a population constructed from the SIRD data. The simulation was run independently for th e certainty strata and noncertainty strata imputation classes. To construct the population, we begin with the SIRD data with missing values imputed using the current imputation method for the SIRD. Let i \uf064 be the observed response indicator vector for company i and y i \uf025 be the vector of either the observed or imputed values o f total RD expenditures for company i over time, = 1, ..., . i n For the simulation, we sample from a population based on {( , ), = 1, ..., } as follows. We first draw a sample of size n with replacement from 1 , ..., , y y n \uf025 \uf025 then we add independent normal random noise, with mean 0 and standard deviation 500, to each component Statistics Canada, Catalogue No. 12-001-X of each of t he sampled vectors. Any resulting negative values are set to zero. We denote these simulated RD totals by * * 1 , ..., , y y n where n is the same as that in Section 3.2. We denote the simulated response indicators by * * 1 , ..., ."}, {"section_title": "n", "text": ""}, {"section_title": "\uf064 \uf064", "text": "For all i and each = 2,3,4, were binary random variables with The coefficients ( ) ( ) are fixed throughout the simulation and they were obtained as the estim ated coefficients from an initial fit of a log istic regression of it \uf064 on Table 5 reports the simulation results for total estimators based on 1,000 runs and methods (1) -(9) described in Section 3.1, where the quantities appearing in the table are defined in Section 3.1. To compute the relative bias we obtain the true value of the total through a preliminary run of the simulation model. Several of the conclusions from the normal population simulation of Section 3.1 carry over to this setting. The following is a summary of some additional findings. 1. In contrast to the no rmal population simulation setting, the estimated total based on censoring and linear regression has SD that is comparable with the proposed imputation methods. This is because the number of data points discarded under censoring is small in this case. The probabilities of an intermittent response pattern are 17% and 19% for the certainty and non-certainty strata classes, respectively. In the normal population simulation these probabilities were nearly 50% as shown in Table 2. 2. All of t he proposed imputation methods give relatively similar performance. As noted prev iously, linear regression imputation is generally biased in theory. However, the bias is s mall because of the strong linear dependence in data. 3. Method (7) does not have a go od performance at 3 t \uf0b3 for t he non-certainty strata case, because the last-value-dependent PSI assumption does not hold. 4. Methods (8) and (9) perform well, again due to the strong linear d ependence in data. Although these methods use more observed data in reg ression imputation, they are comparable with the pr oposed linear regression method.  "}, {"section_title": "Concluding remarks", "text": "We consider a longitudinal study variable h aving nonmonotone nonresponse. Under the assumption that the PSI depends on past observed or unobserved values of the study variable, we propose several imputation methods that lead to unbiased or nearly unbiased estimators of the total or mean of the study variable at a given time point. Our methods do not require any parametric model on the joint distribution of the variables across time points or the PSI. They are based on regression models under different nonresponse patterns derived from the past-data-dependent PSI. Three regression methods are adopted, linear regression, kernel regression, and one-dimensional index kernel regression. Th e imputation method based on the kernel type regression is asymptotically valid, but it requires a large number of Statistics Canada, Catalogue No. 12-001-X observations in each nonresponse pattern. The imputation method based on linear regression is asymptotically biased when the linear relationship does not hold, but it is more stable and, therefore, it may still out-perform methods based on kernel regression. The method of censo ring, which discards all observed data from a subject after its first missing value, may work well when the number of data discarded is small; otherwise it may be very inefficient especially when T is large. For the SIRD data analysis in Sections 3.2 -3.3, censoring is comparable with the proposed linear regression imputation method. However, the results are based on four years of data only and censoring may lead to inefficient estimators when more years of data are considered. In applications, it may be a good idea to compare estimators based on censoring with those based on the proposed methods. Estimators based on the linear regression imputation methods (8) and (9) described in Section 3.1 are asymptotically biased in general. Although they perform well in the simulation study based on th e SIRD population, they have poor performance under the simulation setting in Section 3.1, while the proposed linear regression imputation performs well. The results in Section 2 can be extended to the situation where each sample unit has an observed covariate x t at time t without missing values. Assumption (1) may be modified to include covariates: x T Missing components of y i can be imputed using on e of the procedures in Sections 2.1 - After all missing values are imputed, we can also estimate the relationship between y and X using some popular approaches such as the generalized estimation equation approach. Some details can be found in Xu (2007). It is implicitly assumed throughout the paper that the yvalues are continuous variables with no restriction. When yvalues have a particular order or are i nteger valued, the proposed regression imputation methods are clearly not suitable. New methods for these situations have to be developed. supported by an NSF grant. This article is released to inform interested parties of ongoing research and to encourage discussion. The views expressed are those of the authors and not necessarily those of the U.S. Census Bureau."}, {"section_title": "Appendix", "text": "Proof of (2) -(3). Let ( ) L \uf078 denote the distribution of \uf078 and ( | ) L \uf078 \uf07a denote the conditional distribution of \uf078 given . . Then, both (2) and (3) follow from 1 1 where the first and third equalities follow from assumption (1). Proof of (5). Using the same notation as in the proof of (2) and letting = 1 we have and result 5follows. An example in which (4) does not hold. To show that (4) does not hold in general, we only need to give a co unterexample. Consider = 3. T Let  3  1  3  2  1  3  1  3  2   3  1  3   3  3  1  3  3   3  3  1  2  3  2  1  3  2  3   3  3  1  2  2  1  3  2  3   3  3  2  3  2  1  3  2   2  2  where the first equality holds because 1 y is always observed, the second equality holds because under 1  However, Proof of (8). Using the notation in the proof of (2) -(3) and writing the ( 2)  where the second equality follows from assumption (1)  w w w. s t a t c a n . g c . c a Survey Methodology, December 2012"}, {"section_title": "Main results", "text": "In this section, we discuss some asymptotic properties of PSA estimators. We a ssume that the response mechanism does not depend on . y Thus, we assume that  for some unknown vector 0 . \uf066 The first equality implies that the data are missing-at-random (MAR), as we always observe x in the sam ple. Note that the MAR condition is assumed in the population model. In the second equality, we further assume that the response mechanism is known up to an unknown parameter 0 . \uf066 The response mechanism is slightly different from that of Kim and Kim (2007) , where the response mechanism is assumed to be under the classical two-phase sampling setup and depends on the realized sample: 0 Pr( = 1 , , = 1) = Pr( = 1 , = 1) = ( ; ). Here, I is the sampling indicator function defined throughout the p opulation. That is, = 1 i I if i A \uf0ce and = 0 i I otherwise. Unless the sampling design is non-informative in the sense that the sample selection probabilities are correlated with the response indicator even after conditioning on auxiliary variables (Pfeffermann, Krieger and Rinott 1998), the two response mechanisms, (4) and (5), are different. In survey sampling, assumption (4) is more appropriate because an individual's decision on whether or not to respond to a survey is at his or her own discretion. Here, the response indicator variable i r is defined throughout the population, as discussed in Section 1. We consider a class of consistent nestimators of 0 \uf066 in (4). In particular, we consider a class of estimators which can be written as a solution to where ( ) = ( ; ) \uf066 a sm ooth function of x i and parameter . \uf066 Thus, the solution to (6) can be written as \u02c6, \uf066 h which depends on the choice of ( ). is zero under the respo nse mechanism in (4 ). If we drop the sam pling weights i w in (6), the estimated parameter \uf066 h is consistent for 0 \uf066 A in (5) and the resulting PSA estimator is consistent only when the sampling design is non-informative. The PSA estimators obtained from (6) using the sampling weights are consistent regardless of whether the sam pling design is non -informative or not. According to Chamberlain (1987), any consistent nestimator of 0 \uf066 in (4) can be written as a solution to (6). Thus, the choice of ( ) h \uf066 i in (6) determines the efficiency of the resulting PSA estimator. Let PSA, h \uf071 be th e PSA estimator in (3) using with \uf066 h being the solution to (6 ). To discuss the asymptotic properties of PSA, , h \uf071 assume a sequence of finite populations and samples, as in Isaki and Fuller (1982), such that for any population characteristics i u with bounded fourth moments. We also assume that the sampling weights are u niformly bounded. That is,  uniformly in . \uf066 Furthermore, the partial deriv-  where  The equality in (9) holds when \uf066 h satisfies where is the conditional expectation under the superpopulation model. Proof. Given ( ) = ( ; ) Since \uf066 h satisfies ( 6), we have PS\u00c2\u02c6= ( , ) \uf066 h \uf071 \uf071 \uf067 for any choice of . \uf067 We now want to find a particular choice of , \uf067 As \uf066 h converges in probability to 0 , \uf066 the asymptotic using the theory of Randles (1982)  where the last equality follows because i y is conditionally independent of x Since the last term in (14) is non-negative, the inequality in (9) is established. Furthermore, if \uf061 then (10) holds and implying that the last term in (14) is negligible. In 9, l V is the lower bound of the asymptotic variance of PSA estimators of the form (3) satisfying (6). Any PSA estimator that has the asymptotic variance l V in (9) is optimal in the sense that it achieves the lower bound of the asymptotic variance among the class of PSA estimators with \uf066 satisfying (2). The asymptotic variance of optimal PSA estimators of \uf071 is e qual to l V in (9). The PS A estimator using the maximum likelihood estimator of 0 \uf066 does not necessarily achieve the lower bound of the asymptotic variance. Condition (10) provides a way of constructing an optimal PSA estimator. First, we need an assumption for ( | ), E Y x which is often called the outcome regression model. If the outcome regression model is a linear regression model of the form 0 1 an optimal PSA estimator of \uf071 can be obtained by solving (1, )  Condition (15) is appealing because it says that the PSA estimator applied to = b x y a \uf0a2 \uf02b leads to the original HT estimator. Condition (15) is called the calibration condition in survey sampling. The calibration condition applied to x makes full use of the information contained in it if the study variable is well approximated by a l inear function of . x Condition (15) was al so used in Nevo (2003) and Kott (2006) under the linear regression model.  where \uf062 is a consistent nestimator of 0 \uf062 in the superpopulation model (16) and \uf066 is a consistent nestimator of 0 \uf066 computed by (6). The following theorem shows that the optimal estimator (17) achieves the lower bound in (9).  Proof. Define Note that opt \uf071 in (17) can be written ( , ) | = , = ] = 0 \uf062 \uf066 \uf062 \uf062 \uf066 \uf066 \uf0b6 and the condition of Randles (1982)  The (asymptotic) optimality of the est imator in (17) is justified under th e joint distribution of the respo nse model (4) and the superpopulation model (16). When both models are correct, opt \uf071 is optimal and the choice of \u02c6( , ) \uf062 \uf066 does not affect the effic iency of the opt \uf071 as long as \u02c6( , ) \uf062 \uf066 is consistent. n- Robins, Rotnitzky and Zhao (1994) also advocated using opt \uf071 in 17under simple random sampling. Remark 1 When the response model is correct and the superpopulation model (16) is not necessarily correct, the choice of \uf062 does affect the efficiency of the optimal estimator. Cao, Tsiatis and Davidian (2009) considered optimal estimation when only the response model is co rrect. Using Taylor linearization, the optimal estimator in (17) with \uf066 satisfying (6) is asymptotically equivalent to Thus, an opti mal estimator of \uf062 can be computed by finding \uf062 that minimizes The resulting estimator is design-optimal in the sense that it minimizes the asymptotic variance under t he response model."}, {"section_title": "Augmented propensity score model", "text": "In this section, we con sider optimal PSA esti mation. Note that the op timal estimator opt \uf071 in (17) is not necessarily written as a PSA estimator form in (3). I t is in the PSA estimator form if it sa tisfies Thus, we can construct an optimal PSA estimator by including in the model for the propensity score. Specifically, given where \uf066 is obtained fr om (6), we augment the response model by * 0 1 ( , ) , Statistics Canada, Catalogue No. 12-001-X where 0 1 = ( , ) \uf06c \uf0a2 \uf06c \uf06c is the Lagrange multiplier which is used to incorporate the additional constraint. If 0 1 ( , ) = \uf0a2 \uf06c \uf06c , 0 then *\u02c6( , ) = . \uf066 \uf06c i i p p The augmented response probability *( , ) \uf066 \uf06c i p always takes val ues between 0 and 1. The augmented response probability model (18) can be derived by minimizing the Kullback-Leibler distance Using 18, the optimal PSA estimator is computed by where \uf06c satisfies Under the response model (4), it can be shown that Furthermore, by the argument for Theorem 1, w e can establish that \uf028 \uf029 and the effe ct of estimating 0 \uf066 in = ( ; ) x \uf066 i i p p can be safely ignored. Note that, under the response model (4), \u02c6( , ) \uf066 \uf06c in (19) converges in probability to 0 ( , ), 0 \uf066 where 0 \uf066 is t he true parameter in (4). Thus, the propensity score from the augmented model converges to the true response probability. Because \uf06c converges to zero in probability, the choice of  in p robability. Thus, the optimal PSA estimator in (19) is asymptotically equivalent to the optimal estimator in (17). Incorporating \u02c6i m into the calibration equation to achieve optimality is close in spirit to the model-calibration method proposed by ."}, {"section_title": "Remark 2 The variance estimation of the optim al PSA", "text": "estimator with augmented propensity model (18) with ( , \uf066 \u02c6) \uf06c satisfying (20) can be derived by (29) using "}, {"section_title": "Simulation study 5.1 Study one", "text": "Two simulation studies were performed to investigate the properties of the proposed method. In the first simulation, we generated a finite population of size = 10,000 N from the following multivariate normal distribution: The variable of interest y was constructed as e We also generated response indicator variables i r independently from a Bernoulli distribution with probability 2 2 exp(2 ) = . 1 exp(2 ) From the finite population, we used simple random sampling to select two samples of size, = 100 n and = n 400, respectively. We used = B 5,000 Monte Carlo samples in the simulation. The average response rate was about 69.6%. To compute the propensity score, a response model of the form was postulated and an outcome regression model of the form was postulated to obtain the optimal PSA estimators. Thus, both models are correctly specified. From each sample, we computed four estimators of 1 =1 = : In the augmented PSA estimators, \uf066 was computed by the maximum likelihood method. Under model (30), the maximum likelihood estimator of 0 1 = ( , )\uf0a2 \uf066 \uf066 \uf066 was computed by solving (6) with 2 ( ) = (1, ). ( , ) \uf062 \uf062 for t he outcome regression model was computed using ordinary least squares, regressing y on 1 . x In addition to the point estimators, we also computed the variance estimators of the point es timators. The variance estimators of the PSA estimators were computed using the pseudo-values in (24) and the ( ) i h \uf066 corresponding to each estimator. For the augmented PSA esti mators, the pseudovalues were computed by the method in Remark 2. Table 1 presents the Monte Carlo biases, variances, and mean square errors of the four point es timators and the Monte Carlo percent relative biases and t -statistics of the variance estimators of the estimators. The percent relative bias of a varianc e estimator ( ) V \uf071 is calculated as 100 \uf0b4 \uf0d7 denote the Monte Carlo expectation and the Monte Carlo variance, respectively. The t -statistic in Table 1 is the test statistic for testing the ze ro bias of the variance estimator. See Kim (2004). Based on the simulation results in Table 1, we have the following conclusions. 1. All of the PSA estimators are asymptotically unbiased because the response model (30) is c orrectly specified. The PSA estimator using the calibration method is slightly more efficient than the PSA estimator using the maximum likelihood estimator, because the last term of (14) is smaller for the calibration method as the predictor for x \uf062 is better approximated by a linear function of The augmented PSA estimator is more efficient than the direct PSA estimator (3). The augmented PSA estimator is constructed by using the correctly specified regression model (31) and so it is asymptotically equivalent to the optimal PSA estimator in (17). 3. Variance estimators are all approximately unbiased. There are some modest biases in the variance estimators of the PSA estimators when the sample size is small ( = n 100)."}, {"section_title": "Study two", "text": "In the second simulation study, we further investigated the PSA estimators with a non -linear outcome regression model under an unequal probability sampling design. We generated two stratified finite popu lations of ( , ) x y with four strata ( = 1, 2, 3, 4), h where hi x were independently generated from a normal distribution (1,1) N and hi y were dichotomous variables that take values of 1 or 0 from a Bernoulli distribution with probability 1yhi p or 2 . yhi p Two different probabilities were used for two populations, respectively: In addition to hi x and , hi y the response indicator variables hi r were generated from a Bernoulli distribution with probability = 1 / {1 exp( 1.5 0.7 )}. The sizes of the four strata were 1 = N 1,000, 2 = N 2,000, 3 = N 3,000, and 4 = N 4,000, respectively. In each of the two sets of finite population, a stratified sample of size = n 400 was independently generated without replacement, where a simple random sample of size = h n 100 was selected from each stratum. We used = B 5,000 Monte Carlo samples in this simulation. The average response rate was about 67%. Table 1 Monte Carlo b ias, variance and mean square error(MSE) of th e four point estimators and percent relative biases (R.B.) and t -statistics(t -stat) of the variance estimators based on 5,000 Monte Carlo samples in (19) with \uf062 computed by the method of Cao et al. (2009) discussed in Remark 1. We considered the the augmented PSA estimator in (19) with = ( ), where h A is the set of indices appearing in the sample for stratum h and hi w is the sampling weight of unit i for stratum . h Table 2 presents the simulation results for each method. In each p opulation, the augmented PSA estimator shows some improvement comparing to the PSA estimator using the maximum likelihood estimator of \uf066 or the calibration estimator of \uf066 in terms of variance. Under (Pop1 ), since model (32) is true, there is essentially no difference between the augmented PSA estimators using different methods of estimating . \uf062 However, under (Pop2), where the assumed outcome regression model (32) is incorrect, the augmented PSA estimator with \uf062 computed by the method of Cao et al. (2009) results in slightly better efficiency, which is consistent with the theor y in Remark 1. Variance estimates are a pproximately unbiased in all cases in the simulation study."}, {"section_title": "Conclusion", "text": "We have considered the problem of estimating the finite population mean of y under nonresponse using the propensity score method. The propensity score is computed from a parametric model for the response probability, and so me asymptotic properties of PSA esti mators are discussed. In particular, the optimal PSA estimator is derived with an additional assumption for the distribution of . y The propensity score for the optimal PSA estimator can be implemented by the augmented propensity model presented in Section 3. The resulting estimator is still consistent even when the assumed outcome regression model fails to hold. We have restricted our attention to missing-at-random mechanisms in which the response probability depends only on the always-observed . x If the r esponse mechanism also depends on , y PSA estimation becomes more challenging. PSA estimation when missingness is not at random is beyond the scope of this article and will be a topic of future research. Table 2 Monte Carlo bias, variance and mean square error of the four point estimators and percent relative biases (R.B.) and t -statistics of the variance estimators, based on 5,000 Monte Carlo samples \nIn this paper, joint area level modeling of means and variances is developed for small area estimation. The resulting small area estimators are shown to be more efficient than the traditional estimators obtained using Fay-Herriot models which only shrink the means. Although our model is same as one considered in Hwang et al. (2009), our method of estimation is different in two ways: In the determination of the tuning parameter k and the use of , Z i for constructing the conditional distribution of the small area parameters . i \uf071 We demonstrated robustness properties of the model when the assumption that 2 i \uf073 arise from a inverse Gamma distribution is violated. The borrowing of i X information when estimating 2 i \uf073 as well as the robustness with respect to prior elicitation demonstrate the superiority of our proposed method. The parameter values chosen in the simulation study are different than in the real data analysis. The real data analysis given here is merely for illustration purposes. Our main aim was to develop the methodology of mean-variance modeling and contrast with some closely related methods to show its effectiveness. For this reason, we chose parameter settings in the simulation to be the same as in the well-known small area estimation article Wang and Fuller (2003). Obtaining improved sampling variance estimators is a byproduct of the proposed approach. We have provided an innovative estimation technique which is theoretically justified and user friendly. Computationally, the method is much simpler compared to other competitive methods such as Bayesian MCMC procedures or bootstrap resampling methods. We need sampling from posterior distribution only once during the model parameter estimation, and the sampled values can be used subsequently for all other purposes. The software is available from the authors upon request. where the expectation is with respect to the conditional distribution of , Similarly for a and , b we get the solutions by setting = 0 a S and = 0 b S where a S and b S are, respectively, the partial derivatives of ( 1)  with respect to a and b with expressions given in the main text. These equations are solved using the Newton-Raphson method which requires the matrix of second derivatives with respect to a and . b These are given by the following expressions: , log( ) , and ba ab S S At the th u step, the update of a and b are given by  where the superscript 1 \nDependence between predictors in a linear regression model fitted with survey data affects the properties of parameter estimators. The problems are the same as for nonsurvey data: standard errors of slope estimators can be inflated and slope estimates can have illogical signs. In the extreme case when one c olumn of the design matrix is exactly a linear combination of others, the estimating equations cannot be solved. The more interesting cases are ones where predictors are related but the dependence is not exact. The collinearity diagnostics that are avai lable in standard software routines are not entirely appropriate for survey data. Any diagnostic that involves variance estimation needs modification to account for sample features like stratification, clustering, and unequal weighting. This paper adapts condition numbers and variance decompositions, which can be used to identify cases of less than exact dependence, to be applicable for survey analysis. A condition number of a survey-weighted design matrix 1/2 W X is the ratio of the maximum to the minimum eigenvalue of the matrix. The larger the cond ition number the more nearly singular is , T X WX the matrix which must be inverted when fitting a linear model. Large cond ition numbers are a symptom of some of the numerical problems associated with collinearity. The terms in the decomposition also involve \"misspecification effects\" if the m odel errors are not independent as would be the case in a sample with clustering. The var iance of an estimator of a regression parameter can also be written as a sum of terms that involve the eigenvalues of 1/2 . W X The variance decompositions for different parameter estimators can be used to identify predictors that are correlated with each other. After identifying which predictors are collinear, an analyst can decide whether the collinearity has serious enough effects on a fitted model that action should be taken. The simplest step is to drop one or more predictors, refit the model, and observe how estimates change. The tools we provide here allow this to be done in a way appropriate for survey-weighted regression models. Bayesian penalized spline predictive estimators (Chen et al. 2010). The finite population quantiles are then estimated by inverting the predictive distribution function. The second method is a Bayesian two-moment penalized spline predictive estimator, which p redicts the values of nonsampled units based on a normal model, with mean and variance both modeled with penalized splines on the inclusion probabilities. We compare the performance of these two new methods with the sample-weighted estimator, the CD estimator, and the RKM's ratio and di fference estimators, using simulation studies on artificially generated data and farm survey data."}, {"section_title": "Models for predicting non-response", "text": "A typical response propensity model for a binary outcome (e.g., Hawkes and Plewis 2006) is: is the probability of not responding for subject i at wave ; t 0 it r \uf03d for a response and 1 for non-response; f is an appropriate function such as logit or probit. where n is the observed sample size at wave one. where i T is the number of waves for which it r is recorded for subject . i x are fixed characteristics of subject i measured at wave one, 0 0, ..., ; x \uf02d are time-varying characteristics of subject , i measured at waves , 1, ..., , 1, 2, ..., t k q Q k \uf02d \uf03d \uf03d often k will be 1. \uf0b7 , ri t k z \uf02d are time-varying characteristics of the data collection process, measured for subject i at waves , 1, ..., , 0, 1, ..., t k r R k \uf02d \uf03d \uf03d often k will be 1 but can be 0 for variables such as number of contacts before a response is obtained. Statistics Canada, Catalogue No. 12-001-X Model (1) can easily be extended to more than two response categories such as {response, wave non-response, at-trition}. Other approaches are also possible. For example, it is often more convenient to model the probability of not responding just at wave * t t \uf03d in terms of variables measured at earlier waves * , 1 t k k \uf02d \uf0b3 or, when there is no wave non-response so that non-response has a monotonic rather than an arbitrary pattern, to model time to attrition as a survival process. The estimated response probabilities , are derived from the estimated non-response probabilities in (1) and they can be used to generate inverse probability weights ( 1/ ). These are widely applied (see Section 4.2 for an example) to adjust for biases arising from non-response under the assumption that data are missing at random (MAR) as defined by ."}, {"section_title": "Assessing the accuracy of predictions", "text": "A widely used method of assessing the accuracy of models like (1) is to estimate their goodness-of-fit by using one of several possible pseudo 2 R statistics. Estimates of pseudo 2 R are not especially useful in this context, partly because they are difficult to compare across datasets but also because they assess the overall fit of the model and do not, therefore, distinguish between the accuracy of the model for the respondents and non-respondents separately. As Pepe (2003) emphasises, there are two related components of accuracy: discrimination (or classification) and prediction. Discrimination refers to the conditional probabilities of having a propensity score ( : s the linear predictor from (1)) above a chosen threshold (c) given that a person either is or is not a non-respondent. Prediction, on the other hand, refers to the conditional probabilities of being or becoming a non-respondent given a propensity score above or below the threshold. More formally, let D and D refer to the presence and absence of the poor outcome (i.e., non-response) and define ( ) s c \uf02b \uf03e and ( ) s c \uf02d \uf0a3 as positive and negative tests derived from the propensity score and its threshold. Then, for discrimination, we are interested in ( ), P D \uf02b\uf0f7 the true positive fraction (TPF) or sensitivity of the test, and ( ) P D \uf02d\uf0f7 its specificity, equal to one minus the false positive fraction (1 FPF). \uf02d For prediction, however, we are interested in ( ), P D\uf0f7 \uf02b the positive predictive value (PPV) and ( ), P D\uf0f7 \uf02d the negative predictive value (NPV). If the probability of a positive test ( ( ) ) P \uf02b \uf03d \uf074 is the same as the prevalence of the poor outcome ( ( ) ) P D \uf03d \uf072 then inferences about discrimination and prediction are essentially the same: sensitivity equals PPV and specificity equals NPV. Generally, however, {TPF, FPF, } \uf072 and {PPV, NPV, } \uf074 convey different pieces of information. TPF can be plotted against FPF for any risk score threshold . c This is the receiver operating characteristic (ROC) curve (Figure 1). Krzanowski and Hand (2009) give a detailed discussion of how to estimate ROC curves. The AUC -the area enclosed by the ROC curve and the x-axis in Figure 1 -is of particular interest and can vary from 1 (perfect discrimination) down to 0.5, the area below the diagonal (implying no discrimination). The AUC can be interpreted as the probability of assigning a pair of cases, one respondent and one non-respondent, to their correct categories, bearing in mind that guessing would correspond to a probability of 0.5. A linear transformation of AUC (= 2*AUC -1) -sometimes referred to as a Gini coefficient and equivalent to Somer's D rank correlation index (Harrell, Lee and Mark 1996) -is commonly used as a more natural measure than AUC because it varies from 0 to 1. Statistics Canada, Catalogue No. 12-001-X Copas (1999) proposes the logit rank plot as an alternative to the ROC as a means of assessing the predictiveness of a propensity score. If the propensity score is derived from a logistic regression then a logit rank plot is just a plot of the linear predictor from the model against the logistic transformation of the proportional rank of the propensity scores. More generally, it is a plot of logit( ) i p where i p is the estimated probability from any form of (1) i.e., ( , *, ), p D\uf0f7 x x z against the logits of the proportional ranks ( / ) r n where r is the rank position of case ( n on the propensity score. This relation is usually close to being linear and its slope -which can vary from zero to one -is a measure of the predictive strength of the propensity score. Copas argues that the slope is more sensitive to changes in the specification of the propensity model, and to changes in the prevalence of the outcome, than the Gini coefficient is. A good estimate of the slope can be obtained by calculating quantiles of the variables on the y and x axes and then fitting a simple regression model. The extent to which propensity scores discriminate between respondents and non-respondents is one indicator of the effectiveness of any statistical adjustments for missingness. A lack of discrimination suggests either that there are important predictors absent from the propensity score or that a substantial part of the process that drives the missingness is essentially random. The extent to which propensity scores predict whether a case will be a nonrespondent in subsequent waves -and what kind of nonrespondent they will be -is an indication of whether any intervention to reduce non-response will be successful."}, {"section_title": "The Millennium Cohort Study", "text": "The wave one sample of the UK Millennium Cohort Study (MCS) includes 18,552 families born over a 12month period during the years 2000 and 2001, and living in selected UK electoral wards at age nine months. The initial response rate was 72%. Areas with high proportions of Black and Asian families, disadvantaged areas and the three smaller UK countries are all over-represented in the sample which is disproportionately stratified and clustered as described in Plewis (2007b). The first four waves took place when the cohort members were (approximately) nine months, 3, 5 and 7 years old. At wave two, 19% of the target sample -which excludes child deaths and emigrants -were unproductive. The unproductive cases were equally divided between wave non-response and attrition, and between refusals and other non-productives (not located, not contacted etc.). Plewis (2007a) and Plewis et al. (2008) show that variables measured at wave one of the MCS that are associated with attrition at wave two are not necessarily associated with wave non-response then (and vice-versa). The same is true for correlates of refusal and other non-productives. Table 1 gives the accuracy estimates from the response propensity models. The estimate of the Gini coefficient for overall non-response (0.38) is relatively low: it corresponds to an AUC of 0.69 which is the probability of correctly assigning (based on their predicted probabilities) a pair of cases (one respondent, one non-respondent), indicating that discrimination between non-respondents and respondents from the propensity score is not especially good. Discrimination is slightly better for wave non-respondents than it is for attrition and notably better for other non-productive than it is for refusal. These estimates were obtained from pairwise comparisons of each non-response category with being a respondent. A similar picture emerges when we look at the slopes of the logit rank plots although these bring out more clearly the differences in predictiveness for the different types of, and reasons for non-response. Table 1 Accuracy estimates from response propensity models, MCS wave two"}, {"section_title": "Analyses of non-response 4.1 Accuracy of discrimination and prediction", "text": ""}, {"section_title": "Accuracy measure", "text": "Overall non-response (2) Non-response type (2) Non-response reason (2) Wave non-response Attrition Refusal Other non-productive AUC (1) 0.69 0.71 0.69 0.68 0.77 Gini (1) (1) AUC estimated under the binormal assumption (Krzanowski and Hand 2009); 95% confidence limits for (a) AUC not more than \u00b1 0.015, (b) Gini coefficient and logit rank plot slope not more than \u00b1 0.03. (2) Based on a logistic regression, allowing for the survey design using the svy commands in STATA with the sample size based on the sum of the productive and relevant non-response category. The correct specification of models for explaining nonresponse can be difficult to achieve. New candidates for inclusion in a model can appear after the model and the corresponding inverse probability weights have been estimated, others remain unknown. How much effect on measures of accuracy might the inclusion of new variables have? Here we examine the effects of adding three new variables to the MCS models: (i) whether or not respondents gave consent to having their survey records linked to health records at wave one; (ii) a neighbourhood conditions score derived from interviewer observations at wave two; and (iii) whether, at wave one, the main respondent reported voting at the last UK general election. The first two of these variables were not available for the analyses summarised in Table 1: refusing consent at wave t might be followed by overall refusal at wave 1, t \uf02b and non-response might be greater in poorer neighbourhoods. The voting variable is an indicator of social engagement that might be related to the probability of responding. As the neighbourhood conditions score could not be obtained for cases that were not located, we use this variable just in the model that compares refusals with productives. Table 2 presents the results using the same methods of estimation as for Table 1 with corresponding levels of precision. We see (from the notes) that each of the three variables is associated with at least one kind of nonresponse. The increase in accuracy of the AUC is more than would be expected by chance ( 0.001 p \uf03c apart from wave non-response: 0.06) p \uf03e but is small except for refusal where the inclusion of the three new variables does make a difference: the estimate of the Gini coefficient increases from 0.37 to 0.41 and the slope of the logit rank plot increases from 0.40 to 0.45 (although missing data for the neighbourhood conditions score does reduce the sample size).\nOverall non-response (1) Non-response type Non-response reason Wave non-response (2) Attrition (3) Refusal (4) Other non-productive  (1) Includes consent (odds ratio (OR) = 2.1, s.e. = 0.20) and vote (OR = 1.4, s.e. = 0.08). (2) Includes vote only (OR = 1.4, s.e. = 0.11), consent not important (t = 1.33; p > 0.18).   (1) Based on the product of the survey weights and the non-response weights using the model underpinning Table 1. (2) Non-response weights based on a model that includes consent and vote. (3) All weights standardised to have mean of one."}, {"section_title": "Using weights to adjust for non-response", "text": "Although non-response at wave two of MCS is systematically related to a number of variables measured at or after wave one, we have seen that the models' ability to discriminate between and predict categories of non-response is not high. We now consider what effect the weights generated from the response propensity models have on a longitudinal estimate of interest. We focus on transitions between not working and working across the two waves. As Groves (2006) argues, the keys to unlocking missingness problems of bias are to find those variables that predict whether a piece of data is missing, and which of those variables that predict missingness are also related to the variable of interest. We find that all the variables that predict overall non-response are also related to whether or not the main respondent works at wave two, conditional on whether she was working at wave one so we might expect the application of non-response weights to reduce bias. The results are presented in Table 3 and show that, compared with just using the survey weights, the introduction of the nonresponse weights based on the model underpinning Table 1 leads to small adjustments in the estimated transition probabilities. The consent and vote variables have no additional effect, however, and this is consistent with the marginal increases in accuracy reported in Table 2. Table 2 Accuracy estimates for enhanced response propensity models, MCS wave two"}, {"section_title": "Survey Methodology, December 2012 171", "text": "Statistics Canada, Catalogue No. 12-001-X"}, {"section_title": "Discussion", "text": "Survey methodologists working with longitudinal data have long been exercised by the problem of non-response. Nearly all longitudinal studies suffer from accumulating non-response over time and it is common even for wellconducted mature studies to obtain data for less than half the target sample. On the other hand, a lot can be learnt about the correlates of different types of non-response by drawing on auxiliary variables from earlier waves. The main purpose of this paper has been to introduce a different way of thinking about the utility of the approaches that rely on general linear models both to construct inverse probability weights and to inform imputations. Treating the linear predictors from the regression models as response propensity scores and then generating ROCs enables methods for summarising the information in these scores to be used to assess the accuracy of discrimination and prediction for different kinds of non-response. The application of this approach to the Millennium Cohort Study has shown that, despite using a wide range of explanatory variables, discrimination is rather low. One implication of this finding is that some non-response is generated by circumstantial factors, none of them important on their own, which can reasonably be regarded as chance. There is some support for this hypothesis in that the accuracy of the models for overall non-response, wave nonresponse and other non-productive (the latter two being related) were little changed by the introduction of the voting and consent variables. On the other hand, these variables (and the neighbourhood conditions score) did improve the discrimination between productives, and attrition cases and refusals (which are also related). Nevertheless, discrimination for these two categories remained lower than for the other types of non-response. A second possible implication is that the models do not discriminate well because data are not missing at random (NMAR) in  sense. In other words, it might be changes in circumstances after the previous wave that influences non-response at the current wave. The implications of our findings for prediction are that it might be difficult to predict which cases will become nonrespondents with a high degree of accuracy. If interventions to prevent non-response in longitudinal studies are to be effective then they need to be targeted at those cases least likely to respond because these cases are probably the most different from the respondents and therefore the major source of bias. This is where the ROC approach can be especially useful because, as Swets, Dawes and Monahan (2000) show, it is possible to determine the optimum threshold for the response propensity score based on the costs and benefits of intervening according to the true and false positive rates implied by the threshold. A more detailed assessment of these issues is beyond the scope of this paper but would include considering interventions to prevent different kinds of non-response, and the benefits of potential reductions in bias and variability arising from a sample that is both larger and closer in its characteristics to the target sample.\nSample-weighted estimators for finite population quantiles are wid ely used in survey practice. Although the sample-weighted estimators with Woodruff's confidence intervals are easy to compute and can provide valid largesample inferences, they may be inefficient and confidence coverage can be p oor in small-to-moderate-sized samples. Model-based estimators can improve the efficiency of the estimates when the model is correctly specified, but lead to biased estimates when the model is misspecified. To achieve the balance between robustness and efficiency, we considered spline-model-based estimators. For the qua ntile estimation of a continuous survey variable, we can either estimate the model-based distribution functions and invert the distribution functions to obtain quantiles, or model the survey outcome on the inclusion probabilities directly. In this paper, we proposed two Bayesian spline-model-based quantile estimators. The fi rst method is the Bayesian inverse-CDF estimator, obtained by inverting the splinemodel-based estimates of distribution functions. The second method is the B2PSP estimator, estimated by assum ing a normal distribution for the continuous survey outcome, with the mean function and the variance function both modeled using splines. The simulations suggest that the two Bayesian splinemodel-based estimators outperform the sample-weighted estimator, the design-based ratio and difference estimators, as well as the CD model-based estimator when its assumed model is incorrect. Both new methods yield smaller root mean squared errors whether there is no association, a linear association, or a nonl inear association between the survey outcome and the inclusion probability. In some scenarios, the improvement in effi ciency using the two Bayesian methods is sub stantial. When the normality assumption of the survey outcome given the inclusion probabilities is true, the B2PSP estimator has smaller RMSE and shorter credible interval than the inverse-CDF approach. Moreover, the two Bayesian model-based estimators are robust to the misspecification in both the mean and v ariance functions. In contrast, the CD model-based estimator is biased and inefficient when either the m ean function or the varia nce function is misspecified. Finally, the Bayesian model-based methods have the advantage of easier calculation of the 95% CI and inference based on the posterior distributions of parameters. This is appealing, because variance estimation for the alternative design-based estimators can be complicated. Woodruff's variance es timation method for sam pleweighted estimator performs well when a large fraction of the data is selected from the finite population, even in the moderate to extreme tail regions of the distribution function. However, when data from the population is sparse, th e Woodruff's method tends to und erestimate the confidence coverage, whereas both Bayesian methods have closer to nominal level confidence coverages. All the thr ee design-based estimators have comparable overall empirical bias to th e two Bayesian spline-modelbased estimators. However, there is a linear trend in the variation of bias for the sample-weighted estimator as the sample mean of inclusion probabilities increases. When Statistics Canada, Catalogue No. 12-001-X there is no association between the survey outcome and the inclusion probability, the ratio and difference estimators have relatively larger bias and RMSE than the sampleweighted estimator. However, in some simulation scenarios, the ratio and difference estimators achieve smaller RMSE than the sample-weighted estimator. The comparison between the conventional sample-weighted estimator and the smooth sample-weighted estimator suggests that fitting a smooth cubic curve to the sample-weighted CDF can improve the e fficiency, but the sm ooth sample-weighted estimator still has larger RMSE than the Bayesian inverse-CDF estimator. For normally distributed data, we recommend the use of the B2PSP estimator over the other estimators, because of smaller bias, smaller RMSE, and better confidence coverage with shorter int erval length. The B2PSP est imator and i ts 95% posterior probability interval are easy to obtain using the algorithm proposed by Crainiceanu et al. (2007), which also has the advantage of relatively short computation time. The B2PSP estimator is potentially biased when the conditional normal assumption does not hold. One possibility here is to transform the survey outcome to make the conditional normality assumption more reasonable. The B2PSP estimator can be applied to the transformed data, and the draws from t he posterior distributions of the nonsampled units are transformed back to the original scale before estimating the quantiles of interest. In our simulations with non-normal data, the inverse-CDF Bayesian approach was sti ll more efficient than the sample-weighted estimator. Improvement in the confidence coverage was restricted to situations where the sample size is small, with Woodruff's CI method performing well when the large sample assumption holds. Thus for non-normal data where there no clear transformation to improve normality, we do not recommend the inverse-CDF Bayesian approach when the sample size is large. Given the good properties of the B2PSP estimator in the normal setting, one extension for future work is to relax th e normality assumption in our proposed approaches. We use the probability of inclusion as the auxiliary variable here. When there is only one relevant auxiliary variable, it does not matter whether the inclusion probability or the auxi liary variable is modeled. However, if there is more than one relevant auxiliary variable, the inclusion probability is the key au xiliary variable that needs t o be modeled corrected, since misspecification of the model relating the survey outcome to the in clusion probability leads to bias. When other auxiliary variables are observed for all the units in the finite population, both of our Bayesian estimators can b e easily extended to include additional auxiliary covariates by adding linear terms for these variables in the corresponding penalized spline model. One reviewer suggested an alternative weighted Dirichlet approach, which is simple to calculate but it does not utilize the known auxiliary variables in the non-sampled units. Another possibility is to re-define the CD estimator by using the spline model we have used to define the B2PSP. Specifically, instead of assuming a regression model through the origin, a spline model is fitted to the first and second order moments of the conditional distribution of survey outcome given the inclusion probability. The spline-based CD estimator should perform similarly to the B2PSP estimator, and its variance can be estimated using resampling methods. In the official statistics context, the methods in this article illustrate the po tential benefits of a paradigm shift from design-based methods towards Bayesian modeling that is geared to yielding inferences with good frequentist properties. Design-based statistical colleagues raise two principal objections to this viewpoint. First, the idea of an overtly model-based -e ven worse, Bayesian -approach to probability surveys is not well received, although our emphasis here is on Bayesian methods with good randomization properties. We believe that classical design-based methods do not provide the comprehensive approach needed for the complex problems that increasingly arise in official statistics. Judicious choices of well-calibrated models are needed to tackle such problems. Attention to design features and objective priors can yield Bayesian inferences that avoid subjectivity, and modeling assumptions are explicit, and hence capable of criticism and refinement. See Lit tle (2004,2012) for more discussion of these points. The second objection is that Bayesian methods are too complex computationally for th e official statistics world, where large number of routine statistics need to be computed correctly and created in a timely fashion. It is true that current Bayesian computation may seem fo rbidding to statisticians familiar with simple weighted statistics and replicate variance methods. Sedransk (2008), in an article strongly supportive of Bayesian approaches, points to the practical computational challenges as an inhibiting feature. We agree that work remains to meet this objection, but we do not view it insuperable. Research on Bayesian computation methods has exploded in recent decades, as have our computational capabilities. Bayesian models have been fitted to very large and complex problems, in some cases much more complex than those typically faced in the official statistics world."}, {"section_title": "Proposed model", "text": "Suppose n small areas are in consideration. For the th i small area, let 2 ( , ) i i X S be the pair of direct survey estimate and sampling variance, for = 1, 2, ..., . be the vector of p covariates available at the estimation stage for the th i small area. We propose the following hierarchical model: are assumed to follow a chi-square distribution with 1i n \uf02d degrees of freedom (as a result of normality and SRS), we note that for complex survey designs, the degree of freedom needs to be determined carefully [e.g., Maples et al. 2009). More importantly, the role of the sample sizes in shrinkage estimation of 2 i \uf073 is as follows: For low values of , i n the estimate of 2 i \uf02d \uf073 is shrunk more towards the overall mean ( ) ab compared to higher i n values. Thus, for variances, sample sizes play the same role as precision in shrinkage estimation of the small area mean estimates. We note that You and Chapman (2006) also considered the second level of the sampling variance modelling. However, the hyperparameters related to prior of 2 i \uf073 are not data driven, they are rather chosen in such a way that the prior will be vague. Thus, their model can be viewed as the Bayesian version of the models considered in Rivest and Vandal (2003) and Wang and Fuller (2003). The second level modelling of \uf062 to accommodate covariate information in the variance modeling. Although our model is motivated by Hwang, Qiu and Zhao (2009), we like to mention that Hwang et al. (2009) considered shrinking means and variances in the context of microarray data where they prescribed an important solution by plugging in a shrinkage estimator of variance into the mean estimator. The shrinkage estimator of the variance in Hwang et al. (2009) is a function of 2 i S only, and not of both i X and 2 ; i S see Remarks 2 and 3 in Section 2. Thus, inference of the mean does not take into account the full uncertainty in the variance estimation. Further, their model does not include any covariate information. The simulation study described subsequently indicate that our method of estimation performed better than Hwang et al. (2009). In the above model formulation, inference for the small area mean parameter i \uf071 can be made based on the conditional distribution of i \uf071 given all of the data  The marginal likelihood M L involves integrals that cannot be evaluated in closed-form, and hence, one has to resort to numerical methods for its maximization. One such algorithm is the EM (Expectation-Maximization) iterative procedure which is used when such integrals are present.  where the expression of i \uf079 is given in Equation (4). Starting from an initial value of (0) , B B say, the EM algorithm iteratively performs a maximization with respect to . B At the th t step the objective function maximized is  The expectation in ( 1)  is taken with respect to the conditional distribution of each i \uf071 given the data,  However, drawing random numbers from the conditional distribution is also not straightforward since this is not a standard density. Samples are drawn using the accept-reject procedure (Robert and Casella 2004): For a sample from the target density , f sample x from the proposal density , g and accept the sample as a sample from f with probability One advantage of the acceptreject method is that the target density f only needs to be known upto a constant of proportionality which is the case for 2 ( 1 ) in (5); due to the non-standard form of the density, the normalizing constant cannot be found in a closed form. For the accept-reject algorithm, we used the normal density 2 ( ) exp{ 0.5( ) / \uf074 as the proposal density. The acceptance probability is calculated to be One can choose a better proposal distribution to increase acceptance probability or different algorithm (such as the adaptive rejection sampling or envelope accept-reject algorithms) but our chosen proposal worked satisfactorily in the studies we conducted. The maximizer of ( 1) at the th t step can be described explicitly. The solutions for \uf062 and 2 \uf074 are available in closed form as respectively. Also, ( ) "}, {"section_title": "Point estimate and confidence interval for i \uf071", "text": "Following the standard technique, the small area estimator of i \uf071 is taken to be the expectation of i \uf071 with respect to the conditional density with the maximum likelihood estimate B plugged in for . B The estimate \u02c6i \uf071 is calculated numerically using the Monte Carlo procedure (6) described in the previous section. Subsequently, all quantities involving the unknown B will be plugged in by B although we still keep using the notation B for simplicity. Further, we develop a confidence interval for i \uf071 based on a decision theory approach. Following Joshi (1969), Casella and Hwang (1991), Hwang et al. (2009), consider the loss function associated with the confidence interval C given by ( / ) ( where k is a tuning parameter independent of the model parameters, ( ) L C is the length of C and ( ) C I \uf071 is the indicator function taking values 1 or 0 depending on whether C \uf071 \uf0ce or not. Note that this loss function takes into account both the coverage probability as well as the length of the interval; the positive quantity ( / ) k \uf073 serves as the relative weight of the length compared to the coverage probability of the confidence interval. If = 0, k the length of the interval is not under consideration, which leads to the optimal C to be ( , ) \uf02d\uf0a5 \uf0a5 with coverage probability 1. On the other hand, if = , k \uf0a5 then the coverage probability is 0, leading to optimal C to be a point set. The Bayes confidence interval for i \uf071 is obtained by minimizing the risk function (the expected loss) The optimal choice of C is given by 1 2 2 ( ) = { : Since ( ) i C B is obtained by minimizing the posterior risk, one may like to interpret this as a Bayesian credible set. However, following Casella and Berger (1990, page 470), we will continue naming ( ) i C B as a confidence interval. From an empirical Bayes perspective also, this terminology is more appropriate. How the tuning parameter k determines the confidence level of ( ) i C B will be shown explicitly in Section 3.3. Assuming k is known for the moment, we follow the steps below to calculate ( ). ( 1)/2 1 2 2 2 1 / 2 and 5, respectively, which as mentioned before, are not available in closed form. Thus, similar to the case of , is computed numerically using the Monte Carlo method by approximating the expected value with the mean The accept reject procedure is used to draw random numbers from 2 2 Z B with a proposal density given by the inverse Gamma The next step is to determine the boundary values of ( ) i C B by finding two i \uf071 values that satisfy the equation This requires the normalizing constant in (5) to be evaluated numerically. This is obtained using the Gauss-Hermite integration with 20 nodes."}, {"section_title": "Choice of k", "text": "The choice of the tuning parameter k in (8) is taken to be \uf073 Also, B is replaced by B in (11). We demonstrate that the coverage probability of ( ) i C B with this choice of k is close to 1 . \uf02d \uf061 Theoretical justifications are provided in Section 4."}, {"section_title": "Other related methods for comparison", "text": "Our method will be denoted as Method I. Three other methods to be compared are briefly described below. Method II: Wang and Fuller (2003) considered the Fay-Herriot small area estimation model given by (1). Their primary contribution is the construction of the mean squared error estimation formulae for small area estimators with estimated sampling variances. In the process, they had constructed two formulae denoted by \uf0b7 1 MSE and \uf0b7 2 MSE . We use \uf0b7 1 MSE for our comparisons, which was derived following the bias correction approach of Prasad and Rao (1990). The basic difference with our approach is that they did not smooth the sampling variances, only taking the uncertainty into account while making inference on the small area parameters. The method of parameter estimation, which is moment based for all the model parameters, is also different from ours. Method III: Hwang et al. (2009) considered the log-normal and inverse Gamma models for 2 i \uf02d \uf073 in (2) for microarray data analysis. Their simulation study showed improved performance of confidence intervals for small area estimators under the log-normal model compared to the inverse gamma. We thus modified their log-normal model to add covariates and for unequal sample sizes i n as follows: independently for = 1, 2, ..., . i n Note that the model for the means in (12) is identical to (1).  Thus, the sample size 's i n determine the shape of the 2 \uf063 distribution via its degrees of freedom parameter. More importantly, as mentioned earlier, the different sample sizes account for different degrees of shrinkage for the corresponding true variance parameter. Similar to their estimation approach, the unknown model parameters v \uf06d and \uf074 are estimated using a moment based approach in an empirical Bayes framework giving \u02c6v \uf06d and 2 , v \uf074 respectively. Note that in Hwang et al. (2009), these estimates are obtained based on the hierarchical model for 2 i \uf073 of (13) only without regard to the modelling (1) of the mean. We refer to the Section 5 of their paper for details of the estimation of the hyper-parameters. We follow the same procedure using only 13  and with estimates plugged in for the unknown quantities. The conditional distribution This suggests the approximate Bayes estimator of the small area parameters given by In Section 3 of Hwang et al. (2009) pages 269-271, the interval H i C is matched with the 100 (1 ) to obtain the expression of k as 2 = exp{ / 2} exp{ / 2} / ( 2 ). Method IV: This method comprises of a special case of the Fay-Herriot model in (1) but with the estimation of model parameters adopted from Qiu and Hwang (2007). Qiu and Hwang (2007) considered the model as discussed, the confidence interval ( ) ( 1) 1 2 ( 1) 2( ) ( , , , ) = = . 1 2 1 1 2 Therefore, it is sufficient to consider the following two cases: . has coverage probability 1 . \uf02d \uf061 Thus, if 0 u and i \uf06d are replaced by 0 u and \u02c6, i \uf06d it is expected that the resulting confidence interval , i D \uf025 say, will have coverage probability of approximately 1 . \uf02d \uf061 From (19), we have establishing an approximate lower bound of 1 \uf02d \uf061 for the confidence level of ( )."}, {"section_title": "i C \uf025 B", "text": "In 21, B was assumed to be fixed and known. When B is unknown, we replace B by its marginal maximum likelihood estimate \u02c6. "}, {"section_title": "A simulation study", "text": ""}, {"section_title": "Simulation setup", "text": "We considered a simulation setting using a subset of parameter configurations from Wang and Fuller (2003). Each sample in the simulation study was generated from the following steps: First, generate observations using the model , independently for = 1, ..., , and it follows that i n Note that the simulation layout has ignored the second level modeling of sampling variances in (2). Thus, our result will indicate robustness with respect to the variance model misspecification. The above steps produced the data 2 ( , ), = 1,..., . i i X S i n To simplify the simulation, we do not choose any covariate information . i Z Similar to Wang and Fuller (2003), we set all 's i n equal to m to ease programming efforts. However, the true sampling variances are still chosen to be unequal: One-third of the 2 i \uf073 are set to 1, another one-third are set to 4, and the remaining one-third are set to 16. We take = 10 \uf062 and three different choices of 2 = \uf074 0.25, 1 and 4. These parameter values are chosen from Qiu and Hwang (2007). For each of 2 , \uf074 we generated 200 samples for the two combinations ( , ) = m n (9, 36) and (18, 180). In the simulation study, we compare the proposed method with the methods of Wang and Fuller (2003), Hwang et al. (2009) and Qiu and Hwang (2007) which are referred to as Methods I, II, III, and IV, respectively, based on bias, mean squared error (MSE), coverage probability (CP) of the confidence intervals and the length of the confidence intervals (ALCI). Table 1 contains the parameter estimates for , , a b \uf062 and 2 . \uf074 The numerical results indicate good performance of the maximum likelihood estimates for the model parameters; the estimated values of \uf062 and 2 \uf074 are close to the true values indicating good robustness properties with respect to distributional misspecification in the second level of (2). Statistically significant estimates for both a and b indicate that \"shrunk\" sampling variances are incorporated in the proposed method. Tables 2, 3 and 4 provide numerical results averaged over areas within each group having the same true sampling variances. The results in the Tables are based on 200 replications. Table 1 Simulation results for the model parameters, a (top left panel), b (top right panel), \uf062 (bottom left panel) and 2 \uf074 (bottom right panel). Here SD represents the standard deviation over 200 replicates. We took = 10 \uf062 and 2 = 0.  Table 3 Simulation results for p rediction when 2 = 1. \uf074 Here MSE, ALCI, CP rep resent the mean squared error, average confidence interval width and coverage probability, respectively  Bias Comparisons: In most cases, the bias of the four methods are comparable. There is no clear evidence of significant differences between them in terms of the bias. High sampling variance gives more weight to the population mean by construction that makes the estimator closer to the mean at the second level. On the other hand, Methods I -III use shrinkage estimators of the sampling variances which would be less than the maximum of all sampling variances. Thus, Methods I -III tend to have little more bias. However, due to shrinkage in sampling variances, one may expect a gain in the variance of the estimators which, in turn, makes the MSE smaller. Among Methods I -III, Method I performed better compared to Methods II and III, which were quite similar to each other. The maximum gain using Method I compared to Method II is 99%."}, {"section_title": "MSE Comparisons:", "text": "In terms of the MSE, Method I performed consistently better than the other three in all cases except when the ratio of 2 i \uf073 to 2 \uf074 is the lowest: 25. In this case, the variance between small areas (model variance) is much higher than the variance within the areas (sampling variance). When using our method to estimate , i \uf071 the information \"borrowed\" from other areas may misdirect the estimation: The estimated mean of the Gamma distribution for 2 i \uf02d \uf073 from the second level in (2) is \u00e2 b which equals 0.44 approximately for both the ( , ) m n combinations of (9, 36) and (18, 180) (the true value is = ab 0.4). Thus, is significantly smaller than 1 due to shrinkage towards the mean for the group which has the true value of 2 = 1. i \uf073 Also, since 2 i \uf073 is smaller than 2 , \uf074 the weight of i X should be much more compared to , \uf062 the overall mean. However, due to underestimation of 2 i \uf02d \uf073 in this case, the resulting estimator puts less weight on i X which leads to higher MSE. However, this underestimation will decrease for large sample sizes due to the consistency of Bayes estimators. This fact is actually observed when the sample size increases from = n 36 to = n 180 for the case 2 = 1 i \uf073 and 2 = 4. \uf074 Compared to Method II, Method I shows gains in most of the simulation cases; the maximum gain is 30% while the only loss is 9% for the combination 2 = i \uf073 1 and 2 = \uf074 4 for = n 36 and = m 9. Similarly, for Method III, the maximum gain of Method I is 77% and the only loss of 11% is for the same parameter and sample size specifications."}, {"section_title": "ACP Comparisons:", "text": "We obtained confidence intervals with confidence level 95%. Methods I and III do not indicate any under-coverage. This is expected from their optimal confidence interval construction. Method I meets the nominal coverage rate more frequently than any other methods. Method II has some under coverage and can go as low as 82%. ALCI Comparisons: Method I produced considerably shorter confidence intervals in general. Method IV produced comparable lengths as the other methods in all cases except when 2 i \uf073 was high, in which case, the lengths were considerably higher. The confidence interval proposed in Qiu and Hwang (2007) does not have good finite sample properties, particularly for small 2 . \uf074 To avoid low coverage, they proposed to truncate \uf061 -quantile of a chi-squared distribution with \uf06e degrees of freedom. When the ratio of sampling Statistics Canada, Catalogue No. 12-001-X variance to model variance, 2 2 / , i \uf073 \uf074 is high, 1 M tends to be higher than 0 . M This results in a nominal coverage but with larger interval lengths. For example, in case of 2 2 ( , ) = i \uf073 \uf074 (16, 0.25), the ALCI is 11.13 for Method IV whereas ALCI is only 2.78 and 4.56 for Methods I and II."}, {"section_title": "Robustness study", "text": "In order to study the robustness of the proposed method with respect to departures from the normality assumption in the errors, we conducted the following simulation study. Data was generated as before but with 's ij e drawn from a double-exponential (Laplace) and an uniform distribution. The estimators from Methods II and III had little effect. This is perhaps due to the fact that these methods used moment based estimation for model parameter estimation. Method IV resulted in larger relative bias, MSE and ALCI, and lower coverage probability. The MSE from Method I is always lower than that from Method II. For 2 = \uf074 0.25 and 1, ALCI is smaller for Method I compared to Method II for ( = 36, = 9) n m but the results are opposite when ( = n 180, = 18). m In terms of CP, Method II has some under coverage (lowest is 80%). However, Method I did not have any under-coverage. In order to save space we only provide the results for parameters , , a b \uf062 and 2 \uf074 under the Laplace errors (see Table 5)."}, {"section_title": "Real data analysis", "text": "We illustrate our methodology based on a widely studied example. The data set is from the U.S. Department of Agriculture and was first analyzed by Battese (1988). The data set is on corn and soybeans productions in 12 Iowa counties. The sample sizes for these areas are small, ranging from 1 to 5. We shall consider corn only to save space. For the proposed model, the sample sizes > 1 i n necessarily. Therefore, modified data from You and Chapman (2006) with 2 i n \uf0b3 are used. The mean reported crop hectares for corn ( ) i X are the direct survey estimates and are given in Table 6. Table 6 also gives the sample variances which are calculated based on the original data assuming simple random sampling. The sample standard deviation varies widely, ranging from 5.704 to 53.999 (the coefficient of variation varies from 0.036 to 0.423). Two covariates are considered in Table 6: 1 , i Z the mean of pixels of corn, and 2 , i Z the mean of pixels of soybean, from the LANDSAT satelite data. Table 5 Simulation results for the model parameters, a (top left panel)  \uf073 which is the mean of the Gamma distribution with parameters a and b is = ab 0.002295 with a square root of 0.048 (note that 1 / 0.048 = 20.85 consistent with the range of the sample standard deviations between 5.704 and 53.999). The small area estimates and their confidence intervals are summarized in Table 7 and Figure 1. Point estimates of all 4 methods are comparable: the summary measures comprising of the mean, median, and range of the small area parameter estimates for Methods I, II,III,and IV are (121.9,124.1,122.2,122.6),(125.2,120.4,115.0,114.5) and (23.1,53.0,58.4,56.6), respectively. The distribution of \u02c6i \uf071 (plotted based on considering all the 's) i are summarized in Figure  2 which shows that there is a significant difference in their variability. Method I has the lowest variability and is superior in this sense. Further, smoothing sampling variances has strong implication in measuring uncertainty and hence in the interval estimation. The proposed method has the shortest confidence interval on an average compared to all other methods. Methods II and III provide intervals with negative lower limits. This seems unrealistic because the direct average of area under corn is positive and large for all the 12 counties (the crude confidence intervals S do not contain zero for any of the areas either). Note that Method II does not have any theoretical support on its confidence intervals. Methods II and III produce wider confidence intervals when the sampling variance is high. For example, the sample size for both Franklin county and Pocahontas county is three, but sampling standard deviations are 5.704 and 43.406. Although the confidence interval under Method I is comparable, they are wide apart for Methods II and III. This is because although these methods consider the uncertainty in sampling variance estimates, the smoothing did not use the information from direct survey estimates, resulted the underlying sampling variance estimates remain highly variable (due to small sample size). In effect, the variance of the variance estimator (of the point estimates) is bigger compared to that in method I. This is further confirmed by the fact that the intuitive standard deviations of the \"smoothed\" small area estimates (one fourth of the interval) are smaller and less variable under method I compared to the others. Another noticeable aspect of our method is that the interval widths are similar for counties with same sample size. This could be an indication of obtaining equ-efficient estimators for equivalent sample sizes. Model selection: For choosing the best fitting model, we used the Bayesian Information Criteria (BIC) which takes into account both the likelihood as well as the complexity of the fitted models. We calculated BICs for the models used in Methods I and III (Hwang et al. 2009). These two models have the same numbers of parameters with a difference in only the way the parameters are estimated. The model BIC for Method I is 210.025 and that for Method III is 227.372. This indicates superiority of our model. We could not compute the BIC for Wang and Fuller (2003) since they did not use any explicit likelihood.  "}, {"section_title": "C. An alternative small area model formulation", "text": "It is possible to reduce the width of the confidence interval ( ) C \uf025 B based on an alternative hierarchical model for small area estimation which has some mathematical elegance.  independently for = 1, 2, ..., . i n Note that in the above formulation, it is assumed that the conditional variance of i \uf071 is proportional to 2 i \uf073 whereas the marginal variance is constant (by integrating out 2 i \uf073 using (C.4). In (1) and 2, the variance of i \uf071 is a constant, 2 , \uf074 independent of 2 , i \uf073 and there is no conditional structure for i \uf071 depending on 2 . i \uf073 The set of all unknown parameters in the current hierarchical model is = ( , , , ). a b \uf06c B \uf062 The inference procedure for this model is given subsequently. The model essentially assumes that the true small area effects are not identically distributed even after eliminating the known variations."}, {"section_title": "C.1 Inference methodology", "text": "By re-parameterizing the variance as in (C.2), some analytical simplifications are obtained in the derivation of the posteriors of i \uf071 and i \uf073 given where ( , ) IG a b stands for the inverse Gamma distribution with shape and scale parameters a and , b respectively. which can be seen to be a scaled t-distribution with 2 i n a \uf02b degrees of freedom and scale parameter * X S n a n a n a n a w w w. s t a t c a n . g c . c a"}, {"section_title": "Condition indexes and variance decompositions in ordinary least squares estimation", "text": "In this section we briefly review techniques for diagnosing collinearity in ordinary least squares (OLS) estimation based on condition indexes and variance decompositions. These methods will be extended in section 3 t o cover complex survey data."}, {"section_title": "Eigenvalues and eigenvectors of X X T", "text": "When there is an exa ct (perfect) collinear relation in the n p \uf0b4 data matrix , X we c an find a set of values, However, in practice, when there exists no exact collinearity but some near dependencies in the data matrix, it may be possible to find one or more non-zero vectors v such that = Xv a with \uf0b9 a 0 but close to . 0 Alternatively, we might say that a near dependency exists if the length of vector , a , a \uf050 \uf050 is sm all. To normalize the problem of finding the set of 's v that makes a \uf050 \uf050 small, we consider only v with unit length, that is, with = 1. v \uf050 \uf050 Belsley (1991) discusses the connection of the eigenvalues and eigenvectors of T X X with the normalized vector v and . a \uf050 \uf050 The minimum length a \uf050 \uf050 is simply the positive square root of the smallest eigenvalue of . T X X The v that produces the a with minimum length must be the eigenvector of T X X that corresponds to the sm allest eigenvalue. As discussed in the next section, the eigenvalues and eigenvectors of X are related to those of T X X and have some advantages when examining collinearity."}, {"section_title": "Singular-value decomposition, condition number and condition indexes", "text": "The singular-value decomposition (SVD) of matrix X is very closely allied to the eigensystem of , T X X but with its own advantages. The n p \uf0b4 matrix X can be decomposed as = , is th e diagonal matrix of singular values (or eigenvalues) of . X Here, the three components in the decomposition are matrices with very special, highly exploitable properties: U is n p \uf0b4 (the same size as ) X and is column orthogonal; V is p p \uf0b4 and both row and column orthogonal; D is , p p \uf0b4 nonnegative and diagonal. Belsley et al. (1980) felt that the SVD of X has several advantages over the eigen system of , T X X for the sake of both statistical usages and computational complexity. For prediction, X is the f ocus not t he cross-product matrix T X X since = Y \uf02e X\uf062 In addition, the lengths a \uf050 \uf050 of the linear combinations (1) of X that relate to collinearity are properly defined in terms of the square roots of the eigenvalues of , T X X which are the singular values of . X A secondary consideration, given current computing power, is that the singular value decomposition of X avoids the additional computational burden of forming , T X X an operation involving 2 np unneeded sums and products, which may lead to unnecessary truncation error. The condition n umber of X is d efined as ( ) = \uf06b X k k \uf068 \uf06d \uf06d The closer that min \uf06d is to zero, the nearer T X X is to being singular. Empirically, if a value of \uf06b or \uf068 exceeds a cutoff value of, say, 10 to 30, two or more columns of X have moderate or strong relations. The simultaneous occurrence of several large 's k \uf068 is always remarkable for the existence of more than one nea r dependency. One issue with the SVD is whether the 's X should be centered around their means. Marquardt (1980) maintained that the centering of observations removes nonessential ill conditioning. In contrast, Belsley (1984) argues that meancentering typically masks the role of the constant term in any underlying near-dependencies. A t ypical case is a regression with dummy variables. For example, if gender is one of the independent variables in a regression and most of the cases are male (or female), then the dummy for gender can be strongly collinear with the intercept. The discussions following Belsley (1984 ) illustrate the differences of opinion that occur among practitioners (Wood 1984;S nee and Marquardt 1984;Cook 1984). Moreover, in linear regression analysis, Wissmann, Toutenburg and Shalabh (2007) found that the degree of multicollinearity with dummy variables may be influenced by the choice of reference category. In this article, we do not center the 's X but will illustrate the effect of the choice of reference category in section 4. Another problem with the condition number is that it is affected by the scale of the x measurements (Steward 1987). By scaling down any column of , X the condition number can be made arbitrarily large. This situation is known as artificial ill-conditioning. Belsley (1991) suggests Statistics Canada, Catalogue No. 12-001-X scaling each column of the d esign matrix X using the Euclidean norm of each column before computing the condition number. This method is implemented in SAS and the package perturb of the statistical software R (Hendrickx 2010). Both use the root mean square of each column for scaling as its standard procedure. The condition number and condition indexes of the scaled matrix X are referred to as the scaled condition number and scaled condition indexes of the matrix . X Similarly, the variance decomposition proportions relevant to the scaled X (which will be discu ssed in next section) will be called the scaled variance decomposition proportions."}, {"section_title": "Variance decomposition method", "text": "To assess the extent to which near dependencies (i.e., having high condition indexes of X and ) T X X degrade the estimated variance of each regression coefficient, Belsley et al. (1980) reinterpreted and extended the work of Silvey (1969) by decomposing a c oefficient variance into a s um of term s each of which is associated with a singular value. In t he remainder of t his section, we review the results of ordinary least squares (OLS) under the model is th e n n \uf0b4 identity matrix. These results will be extended to survey weighted least squares in sect ion 3. Recall that the model variance-covariance matrix of the OLS estimator Using the SVD, = ,Var ( )"}, {"section_title": "T M", "text": "X UDV \uf062 can be written as: (2) and the th k diagonal element in V ar ( ) M \uf062 is the estimated variance for the th k coefficient, \u02c6. can be expressed as: where \uf0d7 is the Hadamard (elementwise) product. The variance-decomposition proportions are = / , T X UDV \uf025 However, in survey applications, it will virtually never be the case that the covariance matrix of Y is 2 1 \uf02d \uf073 W if W is the matrix of survey weights. Section 3 covers the more realistic case. In the variance decomposition 3 [ , ] X X with 1 2 = T X X 0 and let the si ngular-value decompositions of 1 X and 2 X be given, respectively, as  Thus 12 = . V 0 An analogous result clearly applies to any number of mutually orthogonal subgroups. Hence, if all the columns in X are orthogonal, all the = 0 kj v when k j \uf0b9 and = 0 kj \uf070 likewise. When kj v is nonzero, this is a signal that predictors k and j are not orthogonal. Since at least one kj v must be nonzero in (3), this implies that a high proportion of any variance can be associated with a large singular value even when there is no collinearity. The standard approach is to check a high condition index associated with a large proportion of the variance of two or more coefficients when diagnosing collinearity, since there must be two or more columns of X involved to make a near dependen cy. Belsley et al. (1980) suggested showing the matrix \uf050 and condition indexes of X in a var iance decomposition table as b elow. If two or more elements in the th j row of matrix \uf050 are relatively large and its associated condition index j \uf068 is large too, it signals that near dependencies are influencing regression estimates."}, {"section_title": "Condition", "text": ""}, {"section_title": "Proportions of variance", "text": "where is the misspecification effect (MEFF) that represents the inflation factor needed to correct standard results for the effect of intracluster correlation in clustered survey data and for the f act that  and (11) reduces to (3). However, the situation is more complicated when G is not the identity matrix, i.e., when the complex design affects the variance of an estimated regression coefficient.  where Q is the diagonal matrix with the row sums of Q on the main diagonal and 0 elsewhere. The interpretation of the proportions in (12) is not as clear-cut as for OLS because the effect of the MEFF matrix. Section 3.2 discusses the interpretation in more detail in the context of stratified cluster sampling. Analogous to the method for OLS regression, a variance decomposition table can be formed like the one at the end of section 2. When two or m ore independent variables are collinear (or \"nearly dependent\"), one singular value should make a large contribution to the variance of the parameter estimates associated with those variables. For exam ple, if the proportions 31 \uf070 and 32 \uf070 for the variances of SW1 \uf062 and Statistics Canada, Catalogue No. 12-001-X SW2 \uf062 are large, this would say that the third singular value makes a la rge contribution to both variances and that the first and second predictors in the regression are, to some extent, collinear. As shown in section 2.3, when the th k and th j columns in X are orthogonal, = 0 kj v and the th j singular value's decomposition proportion jk \uf070 on V ar( ) k \uf062 will be 0. Several special cases are worth noting. If 1 = \uf02d R W as assumed in WLS, then = . G I The variance decomposition in (11) has the same form as (2) in OLS. However, having 1 = \uf02d R W in survey data would be unusual since survey weights are not typically com puted based on the v ariance structure of a model. Note that V is still different from the one in OLS and is one component of the SVD of X \uf025 instead of . X Another special case here is w hen = R I and t he survey weights are equal, in which case the OLS results can be used. However, when the survey weights are unequal, even when = , R I the variance decomposition in (11) is different from (2) in OLS since . \uf0b9 G I In the next section, we will con sider some special models that take the population features such as clusters and strata into account when estimating this variance decomposition."}, {"section_title": "Variance decomposition for a model with stratified clustering", "text": "The model variance of S\u0174 \uf062 in 8  Units within each cluster are assumed to be correlated but the particular form of the covariances does not have to be specified for this analysis. The estimator S\u0174 \uf062 of t he regression parameter can be written as: where  Expression 16is a special case of (9) with 1 2 = ( , , ..., ),  Scott and Holt (1982, section 4), the MEFF matrix st G can be rewritten for a special case of h R in a wa y that will make the decomposition proportions in (12 ) more understandable. Consider the special case of (13) with  in (15) will be approximately the same as the OLS variance. If so, the SWLS variance decomposition proportions will be similar to the OLS p roportions. In regression problems, \uf072 often is small since it is the correlation of the errors, = , hit Y This is relat ed to the ph enomenon that design effects for regression coefficients are often smaller than for means-a fact first noted by Kish and Frankel (1974). In applications where \uf072 is larger, the variance decomposition proportions in (12 ) will still be useful in identifying collinearity although they will be affect ed by departures of the model errors from independence. Denote the cluster-level residuals as a vector, = hi X \uf062 The estimator of (15) that we consider was originally derived from design-based considerations. A linearization estimator, appropriate when clusters are selected with replacement, is: with the estimated misspecification effect as is consistent and approximately design-unbiased under a d esign where clusters are selected with replacement (Fuller 2002). The estimator in 17is also an approximately model-unbiased estimator of (15) (see Liao 2 010). Since the estimator S\u0174 var ( ) L \uf062 is also currently available in software packages, we will use it in the empirical work in section 4. Using (12) derived in section 2, the variance decomposition proportion matrix \uf050 for S\u0174 var ( ) L \uf062 can then be written as and L Q is the diagonal matrix with the row su ms of L Q on the m ain diagonal and 0 elsewhere."}, {"section_title": "Numerical illustrations", "text": "In this section, we will illustrate the collinearity measures described in section 3 and investigate their behaviors using the dietary intake data from 2007-2008 National Health andNutrition Examination Survey (NHANES)."}, {"section_title": "Description of the data", "text": "The dietary intake data are used to estimate the types and amounts of foods and beverages consumed during the 24hour period prior to the interview (midnight to midnight), and to estimate intakes of energy, nutrients, and other food components from those foods and beverages. NHANES uses a co mplex, multistage, probability sampling design; oversampling of certain population subgroups is done to increase the reliability and precision of health status indicator estimates for these groups. Among the respondents who received the in-person interview in the mobile examination center (MEC), around 94% provided complete dietary intakes. The survey weights were constructed by taking MEC sample weights and further adjusting for the additional nonresponse and the differential allocation by day of the week for the dietary intake data collection. These weights are more variable than the M EC weights. The data set used in our st udy is a sub set of [2007][2008] data composed of female respondents aged 26 to 40. Observations with missing values in the selected variables are excluded from the s ample which finally contains 672 complete respondents. The final weights in our sample range from 6,028 to 330,067, with a ratio of 55:1. The U.S. National Center for Health Statistics recommends that the design of the sample is app roximated by the st ratified selection with replacement of 32 PSUs from 16 strata, with 2 PSUs within each stratum."}, {"section_title": "Study one: Correlated covariates", "text": "In the first empirical study, a linear regression model of respondent's body mass index (BMI) was considered. The explanatory variables considered included two demographic variables, respondent's age a nd race (Bl ack/Non-black), four dummy variables for whether the respon dent is on a special diet of any kind, on a low-calorie diet, on a low-fat diet, and on a low-carbohydrate diet (when he/she is on diet, value equals 1, otherwise 0), and ten daily total nutrition intake variables, consisting of total calories (100kcal), protein (100gm), carbohydrate (100gm), sugar (100gm), dietary fiber (100gm), alcohol (100gm), total fat (1 00gm), total saturated fatty acids (100gm), total m onounsaturated fatty acids (100gm), and t otal polyunsaturated fatty acids (100gm). The correlation coefficients among these variables are displayed in Table 2. Note that the correlations among the daily total nutrition intake variables are often high. For Statistics Canada, Catalogue No. 12-001-X example, the correlations of the total fat intakes with total saturated fatty acids, total monounsaturated fatty acids and total polyunsaturated fatty acids are 0.85, 0.97 and 0.93. Three types of regr essions were fitted for the s elected sample to demonstrate different diagnostics. More details about these three regression types and their diagnostic statistics are displayed in Table 1 R W these are the variance decompositions that will be produced by standard software using WLS and specifying the weights to be t he survey weights; TYPE3: SWLS with estimated \u02c6; R the scaled condition indexes are es timated using (6); the scaled variance decomposition proportions are estimated using (12). Their diagnostic statistics, including the scaled condition indexes and v ariance decomposition proportions are reported in Tables 3, 4 and 5, respectively. To make the tables more readable, only the proportions that are larger than 0.3 are shown. Proportions that are less th an 0.3 are shown as dots. Note that some terms in decomposition (12) can be n egative. This leads to the possibility of some \"proportions\" being greater than 1. This occurs in five cases in Table 5. Belsley et al. (1980) suggest that a co ndition index of 10 signals that collinearity has a moderate effect on standard errors; an index of 100 would indicate a serious effect. In this study, we consider a scaled condition index greater than 10 to be relatively large, and ones greater than 30 as la rge and remarkable. Furthermore, the large scaled variance-decomposition proportions (greater than 0.3) associated with each large scaled condition index will be used to identify those variates that are involved in a near dependency. The intracluster correlation of the residuals is shown in the last row of Table 6 under the column labeled \"Original Model\". In the model used fo r Tables 3-5, = \uf072 0.0366 as estimated from a model with random effects for clusters. As noted in section 3.2, when \uf072 is small and the sample is self-weighting, the SWLS decomposition proportions can be interpreted in the same way as tho se of OLS. Although the NHANES sam ple does not have equal weights, \uf072 is small in this example and the decomposition proportions should still provide useful information.  a In all the regression models, the parameters are estimated by: The eigenvalues of this matrix will be used to compute the Condition Indexes for the corresponding regression model. c The terms 2 k j u and j \uf06d are from the singular value decomposition of the data matrix . X d The terms 2 k j u and j \uf06d are from the singular value decomposition of the weighted data matrix 1/2 = . X W X \uf025 e The terms 2 k j u and j \uf06d are from the singular value decomposition (SVD) of the weighted data matrix . X \uf025 The term \u02c6i k g is the unit element of misspecification effect matrix \u02c6. G Statistics Canada, Catalogue No. 12-001-X In Tables 3, 4 and 5, the weighted regression methods, WLS and SWLS, used the survey-weighted data matrix X \uf025 to obtain the condition indexes while the unweighted regression method, OLS, used the data m atrix . X The largest scaled condition index in WLS and SWLS is 566, which is slightly smaller than the one in OLS, 581. Both of these values are much larger than 30 and, thus, signal a severe near-dependency among the predictors in all three regression models. Such large condition numbers imply that the inverse of the design matrix, , T X WX may be numerically unstable, i.e., small changes in the x data could make large changes in the elements of the inverse. The values o f the decomposition proportions for OLS and WLS are very similar and lead to the same predictors being identified as potentially co llinear. Results for SWLS are somewhat different as sketched below. In O LS and WLS, six daily total nutrition intake variables-calorie, protein, carbohydrate, alcohol, dietary fiber and total fat-are involved in the dominant near-dependency that is associated with the largest scaled condition index. Four daily fat intake variables, total fat, total saturated fatty acids, total monounsaturated fatty acids and total polyunsaturated fatty acids, are involved in the secondary near-dependency that is associated with the second largest scaled condition index. A moderate near-dependency between intercept and age is also shown in all three tables. The associated scaled condition index is equal to 38 in OLS and 37 in WLS and SWLS. However, when SWLS is u sed, sugar, total saturated fatty acids and total polyunsaturated fatty acids also appear to be involved in the dominant near-dependency as sho wn in Table 5. While, only three daily fat intake variables, total saturated fatty acids, total monounsaturated fatty acids and total polyunsaturated fatty acids, are invo lved in the secondary near-dependency that i s associated with the second largest scaled condition index. Thus, w hen OLS or WLS is used, the impact of near-dependency among sugar, total saturated fatty acids, total polyunsaturated fatty acids and the six daily total nutrition intake variables i s not as strong as the ones in SWLS. If conventional OLS or WLS diagnostics are used for SWLS, this near-dependency might be overlooked. Rather than using the scaled condition indexes and variance decomposition method (in Tables 3, 4 and 5), an analyst might attempt to identify collinearities by examining the unweighted correlation coefficient matrix in Table 2. Although the correlation coefficient matrix shows that almost all the daily total nutrition intake variables are highly or moderately pairwise correlated, it cannot be used to reliably identify the near-depend encies among these variables when used in a regression. For e xample, the correlation coefficient between \"on any diet\" and \"on lowcalorie diet\" is relatively large (0.73). This near dependency is associated with a scaled cond ition index equal to 11 (larger than 10, but less than the cutoff of 30) in OLS and WLS (shown in Table 3 and 4) and is associated with a scaled condition index equal to 2 (less than 10) in SWLS (shown in Table 5). The impact of this near dependency appears to be not very harmful not matter which regression method is us ed. On the other hand, alcohol is weakly correlated with all the daily total nutrition intake variables but is highly involved in the dominant near-dependency shown in the last row of Tables 3-5. After the collinearity patterns are diagnosed, the common corrective action would be to drop the correlated variables, refit the model and reexamine standard errors, collinearity measures and other diagnostics. Omitting 's X one at a time may be a dvisable because of t he potentially complex interplay of explanatory variables. In this example, if the total fat intake is one o f the key variables that an analyst feels must be kept, sugar might be dropped first followed by protein, calorie, alcohol, carbohydrate, total fat, dietary fiber, total monounsaturated fatty acids, total polyunsaturated fatty acids and total saturated fatty acids. Other remedies for collinearity could be to transform the data or use some specialized techniques such as ridge regression and mixed Bayesian modeling, which require extra (prior) information beyond the scope of most research and evaluations. To demonstrate how the col linearity diagnostics can improve the regression results in this example, Table 6 presents the SWLS regression analysis output of the original models with all the explanatory variables an d a reduced model with fewer explanatory variables. In the reduced model, all of the dietary inta ke variables are eliminated except total fat intake. After the number of correlated offending variables is reduced, the standard error of total fat intake is only the one forty-sixth of its standard error in the original model. The total fat intake becomes significant in the reduced model. The reduction of cor related variables appears to have substantially improved the accuracy of estimating the impact of total fat intake on BMI. Note that the collinearity diagnostics do not provide a u nique path toward a final model. Different analysts may make different choices about whether particular predictors should be dropped or retained. Statistics Canada, Catalogue No. 12-001-X    The reference category is \"not being on diet\" for all the on-diet variables here."}, {"section_title": "Study two: Reference level for categorical variables", "text": "As noted earlier, u sing non-survey data, dummy variables can also play an important role as a possible source for collinearity. The choice of reference level for a categorical variable may affect the degree of collinearity in the data. To be more specific, choosing a category that has a low frequency as the reference and omitting that level in order to fit the model may give rise to collinearity with the intercept term. This phenomenon carries over to survey data analysis as we now illustrate. We employed the four on-diet dummy variables used in the previous study, which we denote this section as \"on any diet\" (DIET), \"on low-calorie diet\" (CALDIET), \"on lowfat diet\" (FATDIET) and \"one low-carbohydrate diet\" (CARBDIET). The model considered here is: Statistics Canada, Catalogue No. 12-001-X  where subscript hit stands for the th t unit in the selected PSU , hi black is the dummy variable of black (black = 1 and non-black = 0), and TOTAL.FAT is the variable of daily total fat i ntake. According to the survey-weighted frequency table, 15.04% of the respo ndents are \"on an y diet\", 11.43% of them are \"on low-calorie diet\", 1.33% of them are \"on low-fat diet\" and 0.47% of them are \"on lowcarbohydrate diet\". Being on a diet is, then, relatively rare in this example. If we choose the majority level, \"not being on the diet\", as the r eference category for all the f our on-diet dummy variables, we expect no severe collinearity between dummy variables and the intercept, because most of values in the dummy variables will be zero. However, when fitting model (20), assume that an analyst is interested to see the impact of \"not on any diet\" on respondent's BMI and reverses the reference level of variable DIET in model 20into \"being on the diet\". This change may cause a near dependency in the m odel because the column in X for variable DIET will nearly equal the column of ones for the intercept. The following empirical study will illustrate the impact of this change on the regression coefficient estimation and how we should diagnose the severity of the resulting collinearity. Table 7 and 8 present the regression analysis output of the model in (20) using the three regression types, OLS, WLS and SWLS, listed in Table 1. Table 7 is modeling the effects of on-diet factors on BMI by treating \"not being on the diet\" as the ref erence category for all the four on-diet variables. While Table 8 changes the reference level of variable DIET fro m \"not on an y diet\" into \"On any diet\" and models the effect of \"not on any diet\" on BMI. The choice of reference level effects the sign of the es timated coefficient for variable DIET but not its absolute value or standard error. The size of the estimated intercept and its SE are different in Tables 7 and 8, but the estimable functions, like predictions, will of course, be the same with either set of reference lev els. The SE of the in tercept is about three times larger when \"on any diet\" is the reference level for variable DIET (Table 8) than when it is not (Table 7). When choosing \"not being on any diet\" as the reference category for DIET in Table 9, the scaled condition indexes are relatively small and do not signify any remarkable neardependency regardless of the type of regression. Only the last row for the largest condition index is printed in Tables 9 and 10. Often, the reference category for a categorical predictor will be chosen to be analytically meaningful. In this example, using \"not being on any diet\" would be logical.  In Table 10, when \"on any diet\" is chosen as the reference category for variable DIET, the scaled condition indexes are increased and show a m oderate degree of collinearity (condition index larger than 10) between the ondiet dummy variables and the intercept. Using the table of scaled variance deco mposition proportions, in OLS and WLS, dummy variable for \"not on any diet\" and \"on lowcalorie diet\" are involved in the dominant near-dependency with the intercept; however, in SWLS, only the dummy variable for \"not on any diet\" is involved in the dominant near-dependency with the intercept and the other three ondiet variables are much less worrisome."}, {"section_title": "Estimators of the quantiles", "text": "Let s denote an unequal probability random sample of size , n drawn from the finite population of N identifiable units according to inclusion probabilities { , 1, ..., }, i i N \uf070 \uf03d which are assumed to be kno wn for all the units before a sample is dra wn. Let Y denote a continuous su rvey variable, with values 1 2 { , , ..., } n y y y observed in the random sample . s The finite-population \uf061quantile of Y is defined as: where ( ) 1 u \uf044 \uf03d when 0 u \uf0b3 and ( ) 0 u \uf044 \uf03d elsewhere. The ( ) \uf071 \uf061 is often estimated using the sample-weighted \uf061- Woodruff (1952) proposed a method of calculating confidence limits for the sample weighted \uf061quantile. First, a pseudo-population is obtained by weighting each sample item by its sampling weight; the standard deviation of the percentage of items less than the estimated \uf061quantile is estimated; and the estimated standard deviation is multiplied by the appropriate z percentile and is added to and subtracted from \uf061 to construct the confidence limits for the percentage of items less than the estimated \uf061quantile. Finally, the values of the survey variable corresponding to the confidence limits of the percentage of items less than the estimated \uf061quantile are read-off the weighted pseudopopulation arrayed in order of size. Variance estimation of the percentage of items in the p seudo-population less than the estimated \uf061quantile is discussed in Woodruff (1952). Sitter and Wu (2001) showed that the Woodruff intervals perform well even in moderate to extreme tail regions of the distribution function. An alternative variance estimate was derived by Francisco and Fuller (1991 ) using a sm oothed version of the large-sample test inversion."}, {"section_title": "Bayesian model-based approach, inverting the estimated CDF", "text": "The finite population quantile function is th e inverse of the finite population cumulative distribution function (CDF), defined as and ( ) 0 x \uf044 \uf03d elsewhere. We can estimate the finite population quantiles by first building a continuous and strictly m onotonic predictive estimate of ( ), F t by treating ( ) t y \uf044 \uf02d as a binary outcome variable and applying methods for estimating finite population proportions. In particular, Chen et al. (2010) proposed a Bay esian penalized spline predictive (BPSP) estimator for finite population proportions in unequal probability sampling. They regress the binary survey variable z on the inclusion probabilities in the sample, using the following probit penalized spline regression model (2) with m pre-selected fixed knots: Self-representing units are included by setting 1. i \uf070 \uf03d Assuming non-informative prior distributions for \uf062 and 2 , \uf074 they simulated draws of z for the non-sampled units from their posterior predictive distribution. A draw from the posterior distribution of the finite population proportion is then obtained b y averaging the observed sample units and the draws of the non-sample units. This is repeated m any times to simulate the posterior distribution of the finite population proportion. Simulation studies indicated that the BPSP estimator is more efficient than the sample-weighted and generalized regression estimators of the finite population proportion, with confidence coverage closer to nominal levels. We employ the BPSP approach n times to estimate ( ) F t at e ach of the sam pled values of , This estimator does not take into account the fact that we are estimating a whole distribution function, and is not n ecessarily a monotonic function. In addition, linear interpolation of the n estimated distribution functions may lead to a poor ly-estimated CDF. To overcome these two problems, we fit a smooth cubic regression curve to the n estimated distribution functions with monotonicity constraints (Wood 1994). We denote the resulting estimated distribution function as \u02c6( ). F t The Bayesian model-based estimator of ( ), \uf071 \uf061 obtained by inverting the estimated CDF, is then defined as follows: Survey Methodology, December 2012"}, {"section_title": "205", "text": "Statistics Canada, Catalogue No. 12-001-X We also fit two other monotonic smooth regression curves to the upper and lower limits of the 95% credible intervals (CI) of t hese estimated distribution functions, denoted as \u02c6( ) U F t and \u02c6( ). L F t To reduce computation time in our simulation studies, we only estimate the CDF at k n \uf03c pre-selected sample points. The basic idea behind this approach is shown graphically in Figure 1. Suppose a sample of size 100 is drawn from a finite population. We pick 20 observations from the sample and estimate their corresponding distribution functions and associated 95% CI using the BPSP estimator. In Figure 1(a) we plot the BPSP estimates of these 20 points with black dots and the upper and lower limits of 95% CI with \"-\" signs, and connect the upper and lower limits with solid lines. In Figure 1(b) we add three monotonic smooth predictive curves using black solid curve for the point estimate and black dash curves for the upper and lower limits of the 95% CI. In Figure 1(c) we draw a horizontal line across the graph with \uf061 as the y-axis value. We read , , A x x and B x respectively from the x-axis such that \u02c6( ) , x is the inverse-CDF Bayesian estimate of ( ). \uf071 \uf061 If the 95% CI of the distribution function ( ) F \uf0d7 is formed by splitting the tail areas of the posterior distribution equally, the interval formed by A x and B x is a 95% CI of ( ). \uf071 \uf061 The proof is as follows: If \uf061 is the lower limit of the 95% CI of ( ), A F x only 2.5 percent of the draws of ( ) A F x in the posterior distribution are smaller than . \uf061 That is, 1 1 Pr( ( ) ( ( ))) Pr( ( ) ) 0.025. Similarly with \uf061 as the upper limit of the 95% CI o f ( ), Therefore, there is 95% probability that ( ) \uf071 \uf061 is within A x and B x in the posterior distribution, given the sample. This inverse-CDF Bay esian model-based approach avoids strong modeling assumptions, and can be applied to normal or skewed distributions. Estimating the distribution function at all n sample units makes full use of the sample information, but is computationally intensive; estimating the distribution function at k n \uf03c values reduces computation time at the e xpense of some loss of ef ficiency. In the traditional approach, the population quantiles are estimated by inverting the unsmoothed empirical CDF. We recommend fitting a smooth cubic regression curve to the estimated distribution functions before inverting the estimated CDF. The resulting quantile estimates are more efficient, because the smooth curve exploits information from all the data. Simulations not shown here suggest that the estimated CDF distribution function curve estimated based on a wellchosen subset of the k sample units is similar to the curve estimated based on all sample units, but t he computation time is significantly reduced. We suggest choo sing the sub set of k data points at evenly spaced intervals in the middle of the distribution, and more frequent intervals in the extremes to improve the estimate of the CDF in the tails. For instance, in our simulation study with a sample size of 100, we estimated the distribution functions at 20 points: the 3 sm allest, the 3 largest, and 14 other equally spaced points in the middle of the ordered sample. We consider altern ative estimators of finite population quantiles of the form: where \u02c6j y is the predicted value of the th j non-sample unit based on a regression on the inclusion probabilities { }. i \uf070 A basic normal model for a c ontinuous outcome assumes a mean function that is linear in { }, i \uf070 that is: with known constants i c to model non-constant variance. This leads to a biased estimate of ( ) \uf071 \uf061 when the relationship is not linear. For estimating finite population totals, Little (2003, 2005) replaced the linear mean function in (5) with a pen alized spline, and assumed 2k i i c \uf03d \uf070 with some known value of . k Simulations suggested that their model-based estimator of the finite population total outperforms the sample-weighted estimator, even when the variance structure is misspecified. For estimation of quantiles rather than t he total, correct specification of the variance structure is important in order to avoid bias. Therefore, we extend the penalized spline model in Zheng and Little (2003) by modeling both the mean and the variance using penalized splines. The twomoment penalized spline model can be written as (Ruppert, Wand, and Carroll 2003,  In (6), the mean and the logarithm of the variance are modeled as penalized splines ( , ) m m and locations ( , ) k k\uf0a2 of the knots for the two splines. Ruppert et al. (2003) suggested an iterativ e approach to estimate the parameters in (6). T hey first assumed that 2 SPL was known and fitted a linear mixed model to estimate the parameters in 1 SPL . They calculated the square of the difference between Y and 1 SPL , which followed a Gamma distribution with the shape parameter as \u00bd and the scale parameter of 2 2SPL . They then fitted a generalized linear mixed model for the squared differences to estimate the parameters in 2 SPL . They iterated the above procedures until the param eter estimates converged. This iterative approach is simple to implement. However, our goal here is not to estimate the parameters but to obtain Bayesian predictions of Y for the non-sample units so that we can use (4) to estimate the quantiles. Crainiceanu, Ruppert, Carroll, Joshi, and Goodner (2007) developed Bayesian inferential methodology for (6). They noted that the implementation of MCMC using multivariate Metropolis-Hastings steps is u nstable with poor mixing properties. They suggested adding error terms to the second spline to make computations feasible, repl acing sampling from complex full conditionals by simple univariate Metropolis-Hastings steps. This idea can be expressed as  We used a prior distribution 6 (0,10 ) N for the fixed effects parameters \uf062 and , \uf061 and a proper inverse-gamma prior  (2007). The posterior distribution of t he finite population \uf061quantile is simulated by generating a l arge number D of draws and using the predictive estimator form is a draw from the posterior predictive distribution of the th j non-sampled unit of the con tinuous outcome. The average of these draws simulates the Bayesian two-moment penalized spline predictive (B2PSP) estimator of the finite population \uf061quantile, 1 ( ) B2PSP 1 ( ) ( ) The Bayesian 95% credible interval for the population \uf061quantile in the simulations is formed by splitting the tail area equally between the upper and lower endpoints. 3. Simulation study"}, {"section_title": "Simulation study with artificial data", "text": "We first simulated a super-population of size M \uf03d 20,000. The size variable X in the super-population takes 20,000 consecutive integer values from 710 to 20,709. A finite population of size N \uf03d 2,000 was then selected from the super-population using systematic probability proportional to size (pps) sampling with the probability proportional to the inverse of the size variable. Consequently, the size variable in the finite population has a right skewed distribution. The survey outcome Y was drawn from a normal distribution with mean ( ) f \uf070 and error variance equal to 0.04 (homoscedastic error) or \uf070 (heteroscedastic error). Three different mean structures ( ) f \uf070 were simulated: no association between Y and \uf070 (NULL) ( ) f \uf070 \uf03d 0.5, a linear association (LINUP) ( ) 6 , f \uf070 \uf03d \uf070 and a nonlinear association (EXP) ( ) exp( 4.64 f \uf070 \uf03d \uf02b -52 ). \uf070 For each of the six simulation conditions, one thousand replicate finite populations were generated, and a systematic pps sample (n \uf03d 100) was drawn from each population with x as the size variable; thus Scatter plots of Y versus \uf070 for these six populations are displayed in Figure 2. We compared the performance of the Bayesian inverse-CDF and the B2PSP estimators with five altern ative approaches: a) SW, the sam ple-weighted estimator defined by inverting \u02c6. x \uf071 \uf061 is the known population quantile of . X e) Diff, the R KM's difference es timator (1990) given by \u02c6( ) { ( ) ( )}, where R is the sample-weighted estimate of / ."}, {"section_title": "Y X", "text": "The seven estimators for the finite-population 10 th , 25 th , 50 th , 75 th , and 90 th percentiles were compared in terms of empirical bias and root mean squared error (RMSE). Because of the complexity in the variance estimation for the CD and RKM's estimators, we only compared the average width and the non -coverage rate of the 95% confidence/credible interval (CI) for the two Bayesian modelbased estimators and the sample-weighted estimator. For the 95% CI, we used Woodruff's method for the sam pleweighted estimator, the method illustrated in Figure 1(c) for the inverse-CDF Bayesian estimator, and the 95% posterior probability of the quantile with equal tails for the B2PSP estimator. We used cubic splines with 15 equally spaced knots. Tables 1 and 2 show the empirical bias and RMSE for the three normal distributions with homoscedastic errors and with heteroscedastic errors, respectively. Overall, the empirical bias in e stimating the five quantiles is s imilar using the two Bayesian estimators, the two sample-weighted estimators, and the RKM's two design-based estimators. In contrast, the CD estimator has large bias and RMS E in all scenarios except for LINUP with heteroscedastic error, where its underlying model is correctly specified. The two Bayesian model-based estimators yield smaller root mean squared errors than the other estimators, and this improvement in efficiency is substantial in some scenarios, especially using the B2PSP estimator. By applying a smooth cubic regression curve on th e estimated empirical sampleweighted CDF, the smooth-sample-weighted estimator gains some efficiency over the co nventional sampleweighted estimators, but the RMSE is still larger than the Bayesian Inverse-CDF estimator. Comparisons of the three design-based estimators suggest that none of th e three estimators uniformly dominates the other two. Specifically, the sample-weighted estimator has smaller RMSE than the RKM difference and ratio estimators for all five quantiles in the NULL and for the lower quantiles in the LINUP and EXP populations; on the other hand, the RKM esti mators have smaller RMSE at the upp er quantiles in the LINUP and EXP populations. Table 3 shows the average width and non-coverage rate of 95% CI for the two Bayesian model-based estimators and the sample-weighted estimator. Overall, the t wo Bayesian model-based estimators yield shorter average 95% CI widths than the sam ple-weighted estimator. The coverage rate of the 95% CI i s similar among the three estimators, except that when \uf061 is equal to 0.1, where the 95% CI of the B2PSP estimator has the shortest average width and very good coverage, while the sam ple-weighted estimator has serious under-coverage. This happens because the Woodruff method for estimating the variance of the sample-weighted estimator is based on a large sam ple assumption, but here the pps sam pling leads to only a small number of cas es being sampled in the lower tail. Although the sample-weighted estimator performs similarly with the tw o Bayesian spline-model-based estimators in terms of overall empirical bias, the conditional bias of estimates varies largely as the sample mean of the inclusion probability increases. Following Royall and Cumberland (1981), the estimates from the 1,000 samples were ordered according to the sample mean of the inclusion probabilities and were split into 20 groups of 50 each, and then the empirical bias was calculated for each group. Figure 3 displays the conditional bias of the two Bayesian estimators and the sample-weighted estimator for the 90 th percentile in the \"EXP + homoscedastic error\" case. Figure 3 shows that there is a linear trend for the bias in the sample-weighted estimator as the sample mean of the inclusion probabilities increases, while the grouped bias of the two Bayesian spline-model-based estimators is less affected by the sample mean of inclusion probabilities. Similar findings are also seen in other scenarios.   Table 2 Comparisons of empirical bias and root mean squared errors \u00d7 10 3 of ( ) \uf071 \uf061 for \uf061 \uf03d 0.1, 0.25, 0.5, 0.75, and 0.9: Scenarios with heteroscedastic errors "}, {"section_title": "Simulation study with the broadacre farm survey data", "text": "The B2PSP estimator assumes the outcome has a normal distribution, after conditioning on the inclusion probabilities. Since the inverse-CDF Bayesian model-based approach does not assume normality, we might expect it to out-perform the B2PSP when the normality assumption is violated. This motivates a comparison of the sampleweighted and the inverse-CDF Bayesian estimators for nonnormal data. The population considered here is defined by 398 broadacre farms (farms involved in the production of cereal crops, beef, sheep and wool) with 6,000 or less hectares that participated in the 1982 Australian Agricultural and Grazing Industries Survey carried out by the Australian Bureau of Agricultural and Resource Economics (ABARE 2003). The Y variable is the total farm cash receipts. One thousand systematic pps samples of size equal to 100 were drawn with the farm area, X, as the size variable, that is, larger farms are more likely to be selected into the sample. Figure  4 is the scatter plot of Y versus the size variable X for these farms, with filled circles representing a selected pps sample. This shows that the variation of Y increases as X increases. Moreover, Y is right-skewed given X. A simulation study using this broadacre farms data was conducted to compare the two Bayesian spline-model-based estimators with the sample-weighted estimator. Table 4 shows the simulation results. The inverse-CDF Bayesian approach yields smaller empirical bias and RMSE, and shorter average length of 95% CI than the sampleweighted estimator in general. The 95% CI of the inverse-CDF Bayesian approach also have closer to nominal level confidence coverage than the sample-weighted estimator when \uf061 is 0.1 and 0.25. However, in the upp er tail with \uf061 \uf03d 0.90, the non -coverage rate of the inverse-CDF Bayesian approach is higher than the nominal level 0.05, while the Woodruff CI o f the sample-weighted estimator does well. This is consistent with the findings of Sitter and Wu (2001) that the Woodruff intervals perform well even in the moderate to extreme tail regions of the distribution function. Since the conditional normality assumption is not reasonable here, the B2PSP estimator is biased and the 95% CI has poor confidence coverage. 1,000 2,000 3,000 4,000 5,000 6,000 Farm Area in Hectares 0 500,000 1,000,000 1,500,000"}, {"section_title": "212", "text": "Chen, Elliott and Little: Bayesian inference for finite population quantiles from unequal probability samples Statistics Canada, Catalogue No. 12-001-X Table 4 Empirical bias \u00d7 10 -2 , root mean squared errors \u00d7 10 -2 , average width of 95% CI \u00d7 10 -2 , and non-coverage rate of 95% CI \u00d7 10 3 of ( ) \uf071 \uf061 for \uf061 \uf03d 0.1, 0.25, 0.5, 0.75, and 0.9: The broadacre farm data 0.1 0.25 0.5 0.75 0.9"}, {"section_title": "Empirical bias", "text": "Inverse "}, {"section_title": "Partially synthetic data", "text": "Partially synthetic datasets are constructed by replacing selected values in the confidential data with m independent draws from their posterior predictive distribution. For a finite population of size , N let = 1, = 1, ..., j Z j N indicate that unit j has been selected to have any observed values replaced with im putations. Imputations should only be made from the posterior predictive distribution of those units with = 1."}, {"section_title": "j Z", "text": "For simplicity, in this paper, we assume = 1, = 1, ..., . Any proper imputation procedure from the broad literature on multiple imputation may be used to generate syn D from cen . D The finite population methods proposed here can be used regardless o f whether a finite p opulation was assumed in the generation of syn . D Under a finite population assumption, since the data are a fully observed census the imputation model parameters would be con sidered known and fixed. See Reiter and Kinney (2012) for an illustration of how valid inferences are obtained from partially synthetic random samples generated with both fixed and random imputation model parameters. Simulations (not shown) confirm the same is true in the finite population case. An analyst with access to syn D but not cen D can obtain valid inferences for a scalar or vector estimand Q using the following quantities:  Thus the proportionality assumption reduces the number of variance parameters to be estimated from ( 1) / 2 k k \uf02d to 1 and allows for the closed-form approximation of the integral in (2.4 ). As = 0, U \uf0a5 the derivation is simplified from Reiter (2005). To complete the integration , we need the distribution of syn ( | ). r D \uf0a5 Extending the scalar case in Reiter (2003), the sampling distribution of ( ) , i Q the estimate of Q obtained from ( ) "}, {"section_title": "Missing data", "text": "The extension to missing data is st raightforward. When = 0, U \uf0a5 the combining rules (Rubin 1987) for scalar estimands q simplify so that where com D is the set of m completed datasets. Similar to Section 2, the tests of Rubin (1987) and Li, Raghunathan and Rubin (1991) \nSimulations analogous to the syn thetic data simulations were conducted for the m issing data case. The missing values of Y are imputed from the posterior predictive distribution obs ( | ) f Y X assuming a normal linear model. Missingness is simulated to be co mpletely at random, with ( = 1) = 0.3, = 1, ..., , l P R l s where R is an indicator variable for missingness. Table 2 gives the nominal 5% rejection rat e for the proposed hypothesis test for multicomponent estimands, which are seen to be close to 0.05, and to the random sampling results. From these results it appears that the proposed combining rules for population data yield valid inferences. "}, {"section_title": "Simulation study", "text": "In this section, simple simulation examples illustrate the analytic validity of the proposed combining rules, first for the case of partially synthetic data, and then for the case missing data. Lastly, the robustness of the tests to the proportionality assumption is evaluated. = ( , ..., ) X X X is drawn from a multivariate normal distribution with mean zero and covariance matrix with 1 in each diagonal element and 0.5 in each off-diagonal element. Y is drawn from a standard normal distribution. For each of 5,000 iterations, a new finite population is generated and m imputations are drawn for {2, 5,10}. m \uf0ce The proposed hypothesis tests are conducted for 0 0 : = , H Q Q where Q is the v ector of regression coefficients, excluding the intercept, of the regression of Y on X and has dimension , k {2, 5, 20}, k \uf0ce and 0 Q is the true value of Q determined from the finite population ( , ). X Y Since 0 H is true by design, 0 H should be rejected 100 % \uf061 of the time, for significance level = \uf061 0.05. Random sampling scenarios are also simulated for comparison purposes. At each iteration, a random sample of size = s 50,000 from an infinite population is generated from the distributions described above, prior to generating the m missing data and synthetic imputations. The sam e hypothesis 0 0 : = H Q Q is tested where 0 Q is the vector of true population values. The combining rules for the hypothesis tests are those of Reiter (2005) in the sy nthetic data case and Li et al. (1991) and Rubin (1987) i n the missing data case."}, {"section_title": "Partially synthetic data imputations", "text": "Let Y be a confid ential response variable and X be unreplaced predictors. Then syn Y is generated by taking m independent draws from the posterior predictive distribution ( | ) f Y X assuming a normal linear model, using all available data. Table 1 gives the nominal 5% rejection rat e for the proposed hypothesis test for multicomponent estimands, which are seen to be close to the significance level 0.05, and close to the random sampling results. From these results it appears that the proposed combining rules for population data have good frequentist properties. Not shown are the rejection rates when the rules from random samples (Reiter 2005) were applied to finite populations, which were observed to be quite high, typically 1, in the simulations conducted. "}, {"section_title": "Robustness", "text": "The assumption that B r I \uf0a5 \uf0a5 \uf0b5 is striking at first glance, and is unlikely to be exactly true. In this section we evaluate the effect of strong correlations across components of . Q While moderately strong correlations were present in the previous simulations, here we increase the magnitude of the between-imputation variance, increasing the magnitude of the differences across the di agonal of B as w ell as the distance from zero of the off-diagonal elements of . B These simulations are set up as before, for the finite population case, with = 5 k and = 5. m The population in each iteration is generated in the same way as before, except that we let Increasing values of c yields increasingly higher correlations. The large variance for \uf068 induces larger and more variable values for elements of . B The results in Table 3 indicate that while the tests have good properties even with moderately high violations of the proportionality assumption, their performance declines with increasingly large co rrelations. Continuing our assumption that Q represents a vector of regression coefficients, presence of such large correlation may also be indicative of multicollinearity in the model at hand, so an alysts faced with high correlation across ( ) i Q might take steps to reduce multicollinearity before applying the propo sed tests. If variables are of substantially differing magnitude, standardization to rescale them will reduce differences across . Q "}]