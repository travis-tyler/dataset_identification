[{"section_title": "", "text": "Differences in children's preparation for formal schooling develop early and have been documented in language capabilities (Dickinson, 2011;Hart & Risley, 2003;Rowe, Raudenbush, & Goldin-Meadow, 2012), in the ability to regulate and adapt social, emotional, and behavioral responses (Duncan & Magnuson, 2011;Grissmer & Eiseman, 2008;Reardon & Portilla, 2015), and in a range of knowledge that undergirds school-based instruction (Morgan, Farkas, Hillemeier, & Maczuga, 2016;Sabol & Pianta, 2017). These early developing differences constitute what is increasingly being called the readiness gap, denoting the wide variation in knowledge, skills, and behaviors that exists among young children at kindergarten entry (Sabol & Pianta, 2017). These measurable gaps at kindergarten entry set the foundation for documented achievement gaps, such that students who are economically and socially marginalized have lower levels of educational attainment, on average, in U.S. schools (Darling-Hammond, 2010;Kena et al., 2015). Gaps in readiness and achievement are not inevitable, nor are these unchangeable. Indeed, cognitive differences at the outset of schooling are smaller in other economically developed countries, like Canada and Australia, compared to the U.S. (Bradbury, Corak, Waldfogel, & Washbrook, 2011). Similarly, the U.S. lags behind other economically developed countries in terms of equity on international assessments of science, math, and reading such that one's socioeconomic status (SES) is a stronger predictor of one's educational achievement in the U.S. than in other countries (Organization for Economic Co-operation and Development, 2018). Differences in readiness and achievement are manifestations of opportunity gaps faced by children and families such that some families have access to a range of supports necessary for the positive development and nurturance of children, whereas other families do not (Darling-Hammond, 2010;Nores & Barnett, 2014). Remedying this problem requires intervening early and then sustaining supports beyond the early years of schooling. Recent policy efforts aimed at reducing readiness and achievement gaps have focused on creating systems of evaluation in the early years (U.S. Department of Education, 2011). These systems of evaluation are a part of standards-based reform, which is defined by coherent systems of learning standards and assessments. The goal of standards-based reform is to set a high bar for learning in all U.S. schools by engaging children in data-driven instruction from the outset of schooling. The underlying premise posits that if teachers were equipped with the right kind of data about their students, then instruction could be tailored to and thus better support the individual learning needs of students (Connor et al., 2009;Fuchs, Fuchs, & Stecker, 2010). Early gaps could be identified and closed. Kindergarten screening tools such as state and federal policy-mandated Kindergarten Readiness Assessments (KRA) are one mechanism for identifying early gaps so that teachers and schools can respond effectively. Despite the widespread implementation of policy-mandated KRAs in kindergarten classrooms (Center on Standards & Assessment Implementation, 2017), little is known about teachers' opinions or uses of them. The purpose of this paper is to bridge the policy-practice divide by examining teachers' understandings and use of KRA data in one state. Building on a prior study in which we found that teachers saw limited value in their state's KRA for guiding instruction (Schachter, Strang, & Piasta, 2017), we revisit these issues after an additional year of implementation to examine how teacher perceptions of the KRA evolved over time."}, {"section_title": "The Purpose and Potential of KRAs", "text": "KRAs, alternately referred to as Kindergarten Entry Assessments, are a central feature of early childhood assessment systems incentivized and funded by the Early Learning Challenge Grant (U.S. Department of Education, 2016). To date, at least 40 states are either in the process of developing or implementing a KRA (Center on Standards and Assessment Implementation, 2017). Although KRAs vary from state to state in their form, content, and administration (Ackerman, 2018;Weisenfeld, 2017), these assessments are administered within the first months of kindergarten and are thought to provide an important foundational understanding of kindergarten students' knowledge, skills, and behaviors from the outset of formal schooling (Goldstein & Flake, 2016;Pianta, Cox, & Snow, 2007;Sabol & Pianta, 2017). The purposes of KRAs are multiple. Assessing students at the outset of schooling is thought to provide key information that can be used locally in classrooms as well as at the state systems level (Ohle & Harvey, 2017;Regenstein, Connors, Romero-Jurado, & Weiner, 2017). At the classroom or school level, KRAs can serve an important role in informing instruction and helping schools prepare to meet the needs of students on an individual and a group level. At the state systems level, KRAs are intended to: 1) target resources where these are most needed; 2) evaluate the progress of early education and intervention efforts in closing readiness gaps before kindergarten entry; and 3) describe and compare the level of knowledge, skills, and behaviors of each kindergarten class over time (Ohle & Harvey, 2017;Regenstein et al., 2017). Thus, KRAs offer the potential for a multipronged approach to prevent the development of the readiness gap prior to kindergarten entry and seek to redress existing inequities in the classroom. The value of KRAs depends on systems-level data to draw resources to students and families who are most underserved and under-resourced prior to kindergarten entry. Simultaneously, teachers and school leaders need ways to interpret and respond to classroom-level data in ways that can both advance the learning of those with the highest levels of pre-kindergarten preparation and accelerate the learning of those who bring less school preparation to the kindergarten classroom. Although there are many recommendations as to what KRAs should include or how these should be used (National Research Council, 2008;Regenstein et al., 2017), there is less evidence that KRAs are beneficial in improving instruction or closing the achievement gaps. An initial study by Shields, Cook, & Greller (2016) using the Early Childhood Longitudinal Study, kindergarten class of 2010-11 found that most schools were using kindergarten entry assessments, with many using the assessments for multiple purposes. Specifically, schools reported using the assessments to individualize instruction, identify students needing additional testing, and to make enrollment/placement decisions. However, they found no associations between reported uses of these assessments and either children's spring math or reading scores. Unfortunately, further evidence regarding the effectiveness of using KRAs either in improving child outcomes or in supporting other intended purposes of KRAs is limited. One avenue for exploring these gaps in knowledge is through focusing on teachers' perspectives."}, {"section_title": "The Role of Teachers in Achieving the Potential of KRAs", "text": "Teachers are critical in supporting KRAs to achieve their multiple purposes. In many ways teachers are responsible for fulfilling several key roles in implementing this policy. They are the ones who often administer (e.g., collect answers directly from children, conduct observations) the KRAs (Ackerman, 2018) and are expected to use the data locally, in their classrooms or schools, in ways that can support student learning. To date, little is known about teachers' perspectives on this process. Preliminary research conducted by the authors and others on both KRAs and kindergarten screening tools more generally (e.g., Costenbader, Rohrer, & DiFonzo, 2000;Daily, Burkhauser, & Halle, 2010;Dever & Barta, 2001;Ohle & Harvey, 2017) suggest that a number of obstacles may impede teachers' effective use of these data in enhancing classroom instructional practice. Our previous study focused on understanding teachers' perceptions of a KRA after the first year of implementation. We conducted a small-scale survey of teachers from one county in a Midwestern state about their experiences with the KRA (Schachter et al., 2017). Participants reported that the administration (i.e., directly assessing children or conducting and scoring observations) of the KRA was burdensome and took them away from more important instructional activities. This finding mirrors other research demonstrating the great time demands of assessments on teachers and practice (Tumblin, 2011;Zweig, Irwin, Kook, & Cox, 2015). In addition, teachers seemed to be unclear as to the purpose of the KRA, with many thinking that the KRA was intended to identify students who were \"ready\" for kindergarten or to evaluate statewide preschool programing. These findings were similar to others who have found lack of clarity regarding the purposes of readiness assessments to be problematic in their use (Daily et al., 2010;May & Kundert, 1992), in some cases with teachers and administrators not knowing that KRAs were meant to inform their instruction (Ohle & Harvey, 2017). Perhaps most importantly, in our previous study, teachers rarely used the KRA to inform instruction; only 12% of teachers reported that the KRA was beneficial in informing their teaching. Teachers pointed to the lengthy administration time of KRAs and lack of access to the data in a timely manner as obstacles to its use. Similarly, others have found limited utility of this type of data particularly in light of the timing of assessment (Dever & Barta, 2001;Ohle & Harvey, 2017). Additionally, we found that the KRA overlapped with other ongoing and required assessment tools, many of which teachers found more informative for tailoring instruction, thus decreasing use of KRA data. Others have also reported that teachers find broad readiness measures less useful in informing practice (Cosner, 2011) and that teacher satisfaction with KRA use has been linked to overall number of screenings used; teachers who used fewer screening tools were more satisfied with the process (Costenbader et al., 2000). Finally, we had preliminary evidence of district differences in KRA uptake such that teachers in high-need districts viewed KRAs as assessing what students would ultimately learn in kindergarten, whereas teachers in more affluent districts viewed the assessment as measuring what students should know prior to kindergarten. Such variations are not surprising given that students from lower SES backgrounds enter kindergarten roughly one year behind in language and academic skills compared to their high SES counterparts (Sabol & Pianta, 2017). In general, use of assessment data to inform instruction may be particularly challenging for kindergarten teachers. There is some evidence that, compared to teachers of older grades, teachers of younger grades have less confidence regarding data use (Zhang & Burry-Stock, 2003) and less knowledge regarding how to interpret and apply data to their classroom practices (Bertrand & Marsh, 2015;Spear-Swerling & Cheesman, 2012). Overall, training seems to be critical for informing how teachers use data (Young, 2006), with ongoing training benefiting teachers' data use practices (Roehrig, Duggar, Moats, Glover, & Mincey, 2008) and evidence that teachers need assistance integrating new assessments into existing assessment systems (Kamler, Moiduddin, & &Malone, 2014). Finally, from a practical standpoint, practice with an assessment also seems to be important, as test administration time has been found to decrease with increased familiarity (Jacobs, Gregory, Hoppey, & Yendol-Hoppey, 2009)."}, {"section_title": "Present Study", "text": "It is not unusual for new polices to require time for teachers and school leaders to adjust and figure out how to maximize the potential of a reform (Brown, Englehardt, Barry, & Ku, 2018;Payne, 2008). Thus, more experience may be needed for KRAs to achieve their full potential. It is also possible that changes can be made to assessments in order to better support teachers in their use. Indeed, between the first and second years of KRA administration, the Ohio Department of Education (ODE) made several changes to the KRA intended to improve the process, based on feedback from multiple sources. This included reducing the number of overall items from 63 to 50, increasing the number of items that could be administered on an iPad (from 12 to 17; ODE, 2015), promises to make the process for accessing data easier and faster for teachers (ODE, 2015), and changing guidelines such that the KRA could be used to meet early assessment requirements for later high-stakes state testing (ODE, 2016a). Given the adjustment process and efforts to improve the KRA, we sought to understand how teachers viewed the KRA after the second year of implementation (Y2). We asked the following research questions (RQs): (1) What were teachers' experiences with Y2 of implementation of the KRA? (2) How did teachers perceive the KRA as a tool for informing instructional decision-making in Y2? (3) How did these perceptions shift over time from the first year of implementation (Y1) to Y2? (4) Were there differences in teachers' perceptions across teacher demographics and district setting? To address these questions, we used an embedded mixed-method design (Creswell & Plano Clark, 2011) in which we collected multiple strands of data. Specifically, we embedded the concurrent gathering of qualitative data within quantitative data collection via an online survey. Including qualitative data collection allowed teachers to voice their own perspectives, thereby providing a more nuanced understanding of their perspectives regarding the KRA that could be interpreted alongside fixedchoice responses."}, {"section_title": "Method", "text": ""}, {"section_title": "Context", "text": "Data are from one Midwestern state in which kindergarten attendance is compulsory and based solely on child-age eligibility (ODE, 2019b). Importantly the state has a \"third grade reading guarantee\" to ensure that all students meet reading proficiency standards by the end of third grade. The policy requires identification and provision of extra support to students from kindergarten through third grade who are behind in reading. Third-grade reading proficiency is determined by a state-level English Language Arts assessment. Although the state mandates district screening every year, the KRA is the only measure used state-wide prior to third grade."}, {"section_title": "KRA", "text": "According to the state the KRA was, \"intended to be used by teachers to improve outcomes for all kindergarten children enrolled in public or community schools\" (ODE, 2018). The KRA assesses a variety of domains including: social foundations (social and emotional development, approaches to learning), mathematics, language and literacy, and physical well-being and motor development. It must be administered to all children matriculating into kindergarten and must be completed annually by November 1st. In the second year of administration, there were 50 items on the KRA. Ten items were to be administered directly to children one-on-one, 19 were to be administered in small groups of children, and 21 items were completed based on teacher observation. Seventeen KRA items could be administered directly via tablet, with scores for the remaining 33 items needing to be manually entered online. The ODE reported that assessing students may take anywhere between 20 to 60 minutes depending on the methods used by teachers. The reported internal consistency for items on the KRA in the second year of administration ranged from good to excellent (\u03b1s ranging .77-.91 across domain subtests) with an overall \u03b1 = .93 (ODE, 2015)."}, {"section_title": "Participants", "text": ""}, {"section_title": "Primary Sample", "text": "We invited 3,113 kindergarten teachers working in public elementary schools across one Midwestern state to participate in an online survey about their experiences with the KRA in the second year of implementation (2015-2016). In total, 841 kindergarten teachers (27% of invitees) responded to the survey, a rate typical for online surveys (Shih & Fan, 2009). However, 93 teachers did not complete the survey and were excluded from analyses. Teachers who completed the survey did not differ from those who did not finish the survey on any characteristic except for class size. On average, teachers who completed the survey had more students in their classes (M = 24.21) compared to teachers who did not complete the survey (M = 21.75; F[1, 961] = 6.20; p = .013). The final analytic sample of 748 teachers represented all of the major cities as well as most of the school districts within the state. Participants came from a range of school district types as identified by the state (12% urban, 32% suburban, 32% small town, 24% rural; 47% low poverty, 10% average poverty, 43% high poverty); this geographic and economic distribution was representative of the state as a whole (ODE, 2016b). On average, teachers had 10.36 years of experience specifically teaching kindergarten (SD = 7.65, range 1 to 43 years). All teachers in the public schools are required to complete a teacher education program and hold a bachelor's degree or complete post-bachelor's training in education (ODE, 2019a). The majority of teachers taught in full-day kindergarten programs (85.2%) with 9% teaching two half-day sessions, 2.2% teaching one half-day session, and the rest reporting another teaching format (3.6%); with an average of 23.18 (SD = 7.63) students per classroom. All had administered the KRA in Y2, and 91% had administered the KRA in Y1. Participants reported a range of training experiences regarding the KRA administration in Y2. The majority received 4 to 8 hrs training (46.1%); 36.8% received 8 to 16 hrs of training, 13.6% received under 4 hrs of training, and 3.5% received over 16 hrs of training. This training was in-person (87.8%), web-based (34.2%), via an online learning community (11.6%), and through the use of simulation activities (29.3%; note that participants could experience multiple training formats)."}, {"section_title": "Comparison Sample", "text": "In order to address our research question regarding how perceptions regarding the KRA shifted over time (RQ3), we used data from our previous study. Participants in that study were 143 kindergarten teachers who implemented the KRA in its inaugural year (2014-2015). Included in the present study are the 127 teachers who completed the entire survey. They were from one county in the state, representing a range of suburban and urban school districts with a similar characteristics to the Y2 sample, including years of teaching experience (M = 15.25, SD = 9.56; see Schachter et al., 2017 for more information). This sample did not include rural or small town districts; as such, we note this as a limitation in making comparisons across samples. However, these data represent at least half of the types of state school districts (ODE, 2016b) providing useful information for observing shifts from Y1 to Y2."}, {"section_title": "Data Collection", "text": "In the spring after the second administration of the KRA, teachers were sent an e-mail inviting them to participate in an anonymous online survey about their experiences with the KRA. The e-mail contained a direct link to the survey. Teachers who did not complete the survey were sent a reminder each week for four weeks until the survey administration window ended. Participants were entered via a separate survey into a raffle to win one of 10 iPad minis. The survey was similar to the one administered in the previous study (see Appendix A for a direct comparison). It contained 25 multiple/fixedchoice items and 6 open response questions. The fixed-choice items asked about basic background characteristics and district setting (5 items), the administration process (8 items), teachers' perceived benefits of the KRA (8 items), and how teachers used KRA data for different types of instruction (e.g., planning, working with individual students) across six different content areas (e.g., math, science; 24 items on which teachers checked \"yes\" for each way that they used the KRA data). Open response questions were intended to extend upon responses in the fixedchoice questions and asked about participants' experiences with the KRA, including perceived benefits of the KRA for teachers and students. For those reporting that they had administered the KRA in Y1, a new open response question asked about how participants' experiences with the KRA differed this year (Y2) as compared to the previous year (Y1). Every open response question was answered by approximately 90% of participants, suggesting that the responses were representative of the sample as a whole."}, {"section_title": "Data Analyses", "text": "Following the embedded mixed-method design (Creswell & Plano Clark, 2011), we used multiple analytic strategies to understand and interpret the data in concert with each other. First we analyzed the fixed-choice questions (quantitative data) and the open response items (qualitative data) separately. Then we integrated the findings in order to better understand the phenomenon (Greene, 2007). These steps are described next."}, {"section_title": "Fixed-choice Questions", "text": "Descriptive statistics were used to address RQ1 and RQ2 and describe Y2 teachers' experiences with the KRA. We were particularly interested in participants' responses regarding KRA administration (i.e., directly assessing children and collecting observational data), the benefits of the KRA for instruction, and how data from the KRA were used for instruction. As such, we created three new variables. We made two composite variables from the data by averaging participants' responses to a set of items regarding each topic: three administrative items (\u03b1 = .64) and eight benefits for instruction items (\u03b1 = .91; see Table 1). All of these questions were Likert items with a scale of 1 to 5, with 1 indicating strong disagreement with statements and 5 indicating strong agreement. Some items were reverse coded (e.g., \"Overall the KRA is not beneficial to my school\"). We also created a composite of teachers' instructional use of the KRA by summing participant responses regarding the number of ways they used the KRA to inform instruction (\u03b1 = .93). This composite ranged from 0 to 24 (i.e., the possibility of 24 different uses -four types of instructional practices across six domains). All composites are presented in Table 1. Additionally, we calculated the percentage of teachers reporting the use of each practice by domain, presented in Table 3. To address RQ3 and investigate patterns of difference between Y1 and Y2 in the responses to the fixed-choice questions, we conducted ANOVAs (see Table 1). Finally, we ran chi-square tests to determine differences by teacher and school district characteristics along with post-hoc analyses based on adjusted standardized residuals to determine differences between specific groups (RQ4; see Table 4)."}, {"section_title": "Open Response Items", "text": "We used both inductive and deductive coding (Maxwell, 2013) to examine the open response questions. Given the large corpus of data, over 3,500 individual open-responses, we used themes from Y1 as a starting point for analyzing the data. However, we were more focused on understanding Y2 participants' experiences, and as such, we allowed for new themes to emerge in the coding in order to understand Y2-specific perspectives. These codes were identified through extensive reading of responses by the first author and an advanced doctoral student. We independently a. Although all 875 teachers completed each item on the survey (across Y1 & Y2), the benefits and administration questions had an \"N/A\" response option. N/A responses were coded as missing. b. Item has been reverse coded. reviewed responses and then identified themes. We then met and agreed upon themes and their definitions. The qualitative themes cut across individual survey questions, with the same qualitative theme emerging in the open response to multiple survey questions. Importantly, we observed that most of the comments regarding the KRA were negative. As such we were intentional in creating codes identifying if participants said anything positive about the KRA in their additional comments or in regards to benefits for students or instruction. We did not give a code for negative comments as negative orientations were captured within the individual themes. Responses were then coded for all themes by a trained undergraduate research assistant; 15% were also coded by the first author to establish reliability, with 97% agreement between coders. These data were used to address all of the research questions."}, {"section_title": "Integrating the Data Types", "text": "After completing the separate quantitative and qualitative analyses, we then considered how the two sets of findings interconnected, following mixed-method procedures (Creswell & Plano Clark, 2011). The qualitative themes are both descriptive and explanatory, adding further dimension to the quantitative responses by providing explanatory detail. For instance, the quantitative data showed that the amount of time for administration proved problematic to teachers. Qualitative findings described how teachers viewed both the administration time and the timing of the assessment as problematic because they lost time for other valued beginning-of-the-year activities, adding explanatory detail to the quantitative finding. During this process we observed that there were four main themes or patterns across both data types. These themes comprised our major results and are described in the following section."}, {"section_title": "Results", "text": "Four major themes emerged across the Y2 qualitative and quantitative data: Improved Administration, Continuing Barriers to Use, Unclear Benefits for Teachers and Students, and Purpose of the KRA. Next, we discuss our research findings, integrating across both data types to describe and explain the themes. In each section we start by describing Y2 teachers' perspectives via both quantitative and qualitative data (RQ1 and RQ2).We also highlight shifts from Y1 as appropriate (RQ3). Finally, we conclude the Results section with findings regarding differences across teacher and district characteristics (RQ4)."}, {"section_title": "Improved Administration of the KRA", "text": "Teachers' reports of administration time in Y2 are listed in Table 2. Most teachers (90.8%) reported being able to administer the KRA to individual students in less than 2 hrs and to the whole class in less than 30 hrs (71.5%). Almost a third of the sample reported being able to administer the KRA to an individual student in less than an hour. Participants' open-response items help contextualize how this time commitment felt for teachers. In their additional comments about the KRA, almost 50% of participants noted that the KRA was too long. Some exemplary comments include: \"The test is too long \u2026 This is a waste of time and money,\" \"The process of administering this assessment was tedious, hugely time consuming and wasteful!\" and \"It takes too long and too much class is taken away.\" These difficulties were mirrored in the administration items (Table 1). Responses averaged between \"disagree\" and \"neutral\" on items about the ease of the administration process, with an average of 2.19 (SD = .90) on the overall administration composite. These findings are interesting when contextualized with the Y1 data. Y2 administration time was significantly less than reported in Y1 (overall class: \u03c7 2 [5] = 107.37, p < .001; individual students: \u03c7 2 [3] = 16.99, p = .001; see Table 2) with the majority of Y1 teachers (52.8%) reporting spending 30 or more hours administering the KRA. Additionally, Y2 teachers perceived the KRA to be easier to administer than those in . When asked directly about differences in Y2 implementation as compared to Y1, about a quarter of the participants commented on easier administration due to reduced items, easier technology, or shorter overall administration time. Importantly, all of the comments about how the KRA differed in Y2 centered on these types of administration-related components of the assessment. This is exemplified in comments such as, \" \u2026 the test was a little shorter,\" \"It was much easier to put the results in the computer,\" and \"It seemed a little easier.\" Despite some improvement in the process and acknowledged changes from Y1, there was a lingering perception that the KRA was shorter but still too time consuming. This is demonstrated more generally in the negative perceptions regarding administration described previously, as well as in explicit participant comments such as, \"There were fewer questions, but it still seemed long\" and \"There were a few less questions but it didn't make a big difference in the time needed to administer.\" The burden of administration was also noted as a barrier to teachers' use of the assessment, as discussed next."}, {"section_title": "Continuing Barriers to Use", "text": "Teachers in Y2 reported infrequent use of the KRA to inform instruction (see Tables 1 and 3). Of the 24 different potential KRA uses for instruction, the mean number of reported uses was 3.86 (SD = 5.22). Specifically, when asked about the various ways they could use the KRA to inform instruction (i.e., for planning, during teaching, working with individual students, or in connection with other assessments) across the domains, between 3.10% to 39.00% of teachers reported using the KRA for these practices. The greatest percentage of teachers reported using the KRA to inform instruction for language and literacy, especially when working with individual students ( Table 3). These findings were similar to those from the open-responses. When asked how the KRA improved instruction, in general, teachers overwhelmingly responded that the KRA did not improve instruction. A small percentage (10%) of Y2 of teachers commented that the KRA could be used to guide instruction and 9% commented that the KRA could be used to identify students potentially at-risk for learning difficulties. Importantly, these last two uses do align with stated purposes of the KRA; however, these were of low frequency across the sample. Participants consistently identified a variety of reasons as to why the KRA was not useful for them in informing instruction. These centered on the length of the KRA (as described in the previous section), the content of the KRA, and the overall utility of the KRA in supporting teaching. Participants reported that the KRA did not assess topics of interest to them. This is exemplified in comments like, \"I don't use the KRA to guide my instruction. It is too vague (covers a broad range of content, none of it in enough detail to be useful in class).\" Furthermore, several teachers reported that the content of the KRA quickly became irrelevant to their teaching; as one noted, \"KRA as it is currently set up cannot effectively improve instruction due to the late date at which it is given. By the time we were finished administering the KRA, we had already taught some of the content on the KRA and had moved beyond it.\" Thus, the timing of administration was problematic for having relevant data. Finally, it seemed that some participants did not see a purpose for the data in their teaching. As one teacher wrote, \"I do not understand what we are to do with the information. I do not use this information for any reading groups,\" and many teachers noted that they had better assessments that gave them information that they needed such as, \"We have a lot of other assessments besides the KRA that give us a more clear picture of where to start teaching.\" Results from chi-square analyses indicated that these findings were similar to those from Y1. Indeed, instructional use was not significantly different from reports in Y1, either by content area (all ps > .30) or by type of use/instruction (all ps > .20). Additionally, in Y1 teachers reported similar problems such as access to data and completeness of data which prohibited its use. "}, {"section_title": "Unclear Benefits for Teachers and Students", "text": "In general, Y2 teachers disagreed that there were benefits of the KRA, with teachers on average reporting that they disagreed with all statements about benefits for either their instruction or students (see Table  1). The open response questions mirrored these, at best, ambivalent attitudes about benefits of the KRA. When asked directly about the benefits of the KRA for teachers and students, 28% of teachers recognized the KRA as an opportunity to collect baseline data. This was evident in comments such as, \"The benefits of the KRA for teachers is baseline data on where students are,\" \"see where they stand compared to their peers,\" and \"identifies strengths and weakness.\" Additionally, there were some participants who identified the KRA as beneficial for planning instruction (8%), differentiating instruction (6%), and allowing for one-on-one time with students (7%). In contrast, over a third of teachers explicitly stated that there was no benefit of the KRA for teachers or students. Whereas some teachers were terse in their responses stating, \"nothing\" or \"none,\" other participants followed up on this idea of no benefit with further explanations that again spoke to the utility of the data noting that it was often incomplete or not as useful as data gathered via other assessments. This was evident in comments such as, \" \u2026 scores are not broken down into content areas. It would be nice to have a print out for teachers and parent that show strengths and weaknesses. Just a one number score doesn't help me a whole lot!\" or \"None. Again, I get more detailed information from my own assessments.\" Others returned to the burdensome nature of the KRA, with comments like \"None \u2026 takes too much time from teaching.\" It is important to note that almost a quarter of teachers reported that administering the KRA took away valuable time needed at the beginning of kindergarten to get to know students and acclimate them to formal schooling. This is exemplified in comments such as, It is a very lengthy process that takes away time from the beginning of the year where we should be going over rules and procedures and getting students acclimated to a schedule \u2026 whereas during the KRA assessment there is a hectic unpredictable schedule which is not beneficial to the students or the teacher. Others reported similar concerns such as in the response, \"At the beginning of the year in Kindergarten, it is critical to set rules, procedures, etc. into place. By giving the KRA so early in the year, this disrupts that process. Also, valuable information is not received from this test.\" Thus, the loss of the critical time at the beginning of kindergarten was perceived by teachers as a negative for both teachers and students. There was a significant difference between Y1 teachers and Y2 teachers on perceived benefits of the KRA composite (F[1, 873] = 8.08, p = .005), with scores slightly increasing in Y2. However, it is important to reiterate that Y2 teachers still disagreed that the KRA benefited teachers, students, or schools. Importantly, many of the critiques of the KRA noted by Y2 teachers regarding the KRA were also present in the Y1 critiques, including the loss of valuable time at the beginning of kindergarten."}, {"section_title": "Purpose of the KRA", "text": "Many Y2 teachers seemed to understand the intended purpose of the KRA. Specifically, 43% of teachers reported that the purpose of the KRA was to collect baseline data about students, and 7% noted that it can inform instruction. This is exemplified in quotes from teachers stating the purpose as \"See what level the students are at coming into kindergarten\" or \"Give information to the state on how prepared students are or not. Provide us with data about our new students.\" Furthermore, there was a decrease in the number of teachers reporting that the KRA was a gatekeeping mechanism (from 37% in Y1 to 2% in Y2). It should also be noted that almost a third of teachers indicated that the purpose of the KRA was to assess readiness for kindergarten with comments such as, \"To determine a student's readiness for kindergarten\" and \"To assess readiness of children based on their age and level of learning coming into Kindergarten.\" Thus, teachers seemed to be picking up on the language of the KRA but without making a clear connection as to what it meant to assess readiness skills to guide instruction. In Y2, teachers did have a better understanding of the purpose of the KRA. However, this understanding did not seem connected to perceptions of the KRA benefits or use, as many teachers reported that they did not use KRA data. Many participants returned to other problems with the KRA that served as barriers to its use and perceived benefits."}, {"section_title": "Differences by Teacher and District Characteristics", "text": "We did observe some differences in teachers' perceptions by teacher characteristics. On average, teachers who commented that the KRA was beneficial for students had been teaching for fewer years (M = 13.77) than teachers who did not comment on the benefits for students (M = 15.14; F[1, 733] = 3.83, p = .051). Additionally, teachers who made a positive comment regarding the benefits of the KRA for teaching had received, on average, more training (M = 1.72) than teachers who did not make a positive response about the KRA for teaching (M = 1.58; F[1, 729] = 3.89, p = .049). The number of years teachers taught kindergarten was not related to commenting on benefits for students or benefits for teaching (all ps > .120). At the district level, we observed some differences, particularly for the districts serving the highest SES populations. Teachers from suburban, very low poverty districts were less likely than teachers from other districts to comment that that KRA had benefits for teaching (\u03c7 2 [1] = 13.10, p < .001). Additionally, teachers from suburban districts with very low levels of poverty reported using the KRA in fewer ways (M = 2.42) than teachers from small town low poverty (M = 4.38), urban high poverty (M = 4.79), and urban very high poverty (M = 4.59) districts (F[7, 862] = 2.70, p = .009). Table 4 reports more specifically about differences in use for instruction by district characteristics. *Sum of whether teachers used the KRA to plan, teach, or work with individual students in six content areas: physical/motor, language and literacy, math, science, social studies, and social skills; 0 = did not use, 1 = did use with a possible range of 0 to 6. a. Tukey suburban very low poverty versus small town low poverty, p = .041; Tukey suburban very low poverty versus urban high poverty, p = .034; Tukey suburban very low poverty versus urban very high poverty, p = .040. b. Tukey suburban very low poverty versus rural high poverty, p = .044; Tukey suburban very low poverty versus urban very high poverty, p = .021. c. Tukey suburban very low poverty versus small town low poverty, p = .050; Tukey suburban very low poverty versus urban high poverty, p = .008. d. Tukey suburban very low poverty versus small town low poverty, p = .035; Tukey suburban very low poverty versus urban high poverty, p = .002; Tukey suburban very low poverty versus urban very high poverty, p = .009."}, {"section_title": "Discussion", "text": "The purpose of this study was to understand teachers' perceptions of the KRA in Y2 of implementation. We had anticipated that increased familiarity and training would improve teachers' understanding of the purpose of the KRA and better leverage their use of the assessment for instruction. Furthermore, the state made several changes to improve the KRA between Y1 and Y2 to facilitate ease of administration which could have improved teachers' views of the assessment. Although Y2 teachers did note some ease in the administration as well as clearer understandings of the purpose of the KRA, teachers still found very little utility in the assessment for instruction or students. Our findings are critical in demonstrating that at least one objective of the KRA was not being achieved -overall, teachers were not using the KRA to learn about students to inform instruction. Importantly, KRAs are created with multiple purposes in mind. In addition to providing statelevel data about closing the achievement gap, the data are also intended to be used locally to support student learning. Although many teachers recognized this as an intended purpose of the KRA, this did not translate into their instructional practice. These findings have important implications for understanding and bridging the research-to-practice gap, especially in light of almost ubiquitous KRA use across states (Center on Standards and Assessment Implementation, 2017)."}, {"section_title": "Importance of Focusing on Teachers' Perspectives", "text": "Teachers are often key stakeholders in enacting federal and state policies in the classroom, as is the case in the use of KRAs. Thus, soliciting their perspectives is critical to understanding how a policy is implemented in real-world contexts both in understanding impacts and informing revisions. It is important to note that our sample included a large number of teachers representing a variety of experiences and districts. Indeed, almost every district from the state was represented. Thus, our findings provide compelling evidence in considering teachers' views of KRA implementation. An important contribution of this study is identifying the added burden of the KRA to teachers with minimal benefit for instruction or students. This is evidenced both in the fixed-choice and open response questions. Although there was a positive change in teacher responses in Y2, it is important to note that their average scores still did not reflect positive orientations to the KRA -either disagreeing that there were benefits to the KRA or being close to neutral regarding the administration process (M = 2.13 and M = 2.85 on a 5 point scale, respectively). Critically, using the open response questions allowed us to understand some of the reasons behind this low favorability. Many teachers reported having better assessments for identifying the information they needed at the start of kindergarten. This indicates that teachers do use data to inform their teaching but that they do not find KRA data useful for these purposes. Thus, teachers did not view the KRA as adding to their overall assessment program or providing a better tool to support their teaching. Furthermore, teachers reported that the KRA administration (i.e., administering items directly to children and collecting and scoring observational data) took away instructional time deemed central to setting a strong foundation for learning in kindergarten, many students' first foray into formal schooling. In Y2, teachers seemed to have a better understanding as to the purpose of the KRA, yet that did not improve their use of the data to inform instruction. This is important as we had hypothesized that one reason teachers in Y1 did not use the measure was because they did not understand it. However, this is less of the case in Y2; with more experience and additional training, teachers were better able to understand the purpose of the KRA. However, our findings indicate that there is a difference between knowing the purpose of a measure and achieving this purpose."}, {"section_title": "Implications for Policy, Design, and Practice", "text": "This study illuminates the ongoing disconnect between what is happening on the ground in classrooms and what policy makers at both the state and federal level are seeking to accomplish. The desire to close the readiness gap and weaken the achievement gap is of high importance. Federal efforts incentivize the use of KRAs to support this work, and states are tasked with developing assessment systems to meet these goals. However, the reality is that teachers are left with the considerable task of implementing reforms like the KRA. It would be ideal if the KRA was designed such that it could provide state-level data while also meeting teachers' needs. Unfortunately, as it stands, this specific state's KRA was not positioned to do so. Instead there seemed to be multiple assessment systems used into which the KRA needed to integrate. Next, we discuss how our findings can support ongoing improvement of KRA systems."}, {"section_title": "Policy", "text": "As already described, our findings provide important insight into the uptake, or lack thereof, of particular KRAs as levers for improvement and have two major implications for policy. First, on a broader policy scale, it is important to consider variability in resources and uptake across districts. In particular, we observed that teachers in the very low poverty suburban districts were the ones with the least favorable orientations to the KRA and reported using the KRA the least. This could be because these districts are more well-resourced (ODE, 2016a) and had multiple assessment tools. It may also be that students in these districts entered kindergarten with more developed skills. Indeed, students attending these types of school districts typically start school with higher skill levels such that they are better positioned for academic success (e.g., Dickinson, 2011;Duncan & Magnuson, 2011;Hart & Risley, 2003;Rowe et al., 2012). Thus, state-level policy makers may need to consider differences in implementation that may emerge by district characteristics. Second, our findings also indicate variable perceptions of kindergarten readiness which indicates the need for more universal clarity regarding what constitutes readiness and how this aligns to state learning standards (Daily et al., 2010). This is evidenced in the mixed responses regarding how the KRA could be or could not be used as an early screener for the high-stakes third grade reading testing. However, it is unclear at the state-level if skills assessed on the KRA are ones that should be attained upon kindergarten matriculation. Additionally, many teachers noted that the content addressed was not relevant to their teaching more generally. This indicates that KRA generated data did not give information that they viewed as important at the start of kindergarten. Our findings mirror that of others demonstrating that views of what kindergarten should be have both shifted in the last decades and that these views are variable and contextually driven (Bassok, Latham, & Rorem, 2016;Brown et al., 2018). Thus, having clearer state-level (and possibly national-level) definitions of kindergarten readiness as well as guidelines for how readiness is connected to kindergarten learning guidelines and instruction may be needed."}, {"section_title": "Design", "text": "This study also provides critical information regarding the design of KRAs in order to ensure both feasibility of implementation and usability of the data. Teachers lamented both the amount of time it took to administer the KRA and the timing of the assessment during a vital part of the kindergarten year. Although this is not a new concern for teachers in administering assessments (Tumblin, 2011;Zweig et al., 2015), planning the administration window should take into account teachers' and students' needs at this critical phase of formal learning. Perhaps the KRA administration should happen prior to the start of school such that it does not diminish classroom instruction, and data could be accessed in advance of student matriculation into kindergarten. This, of course, would take substantial resources to support either teachers or other staff in preschool administration and may not be feasible given some of the observation-based items on the KRA. Further, there may be unintended consequences to assessing students prior to the beginning of kindergarten like holding students out for a year, which has been a concern with other readiness assessments (May & Kundert, 1992). More research is needed to address the double burden of a lengthy assessment administered at a key instructional time. In order for teachers to see benefits of the KRA, the state should continue efforts to ensure easy and fast access to useful KRA data. Indeed, part of teachers' concerns were that when the results became available, data were often out of date or no longer useful. Many teachers also noted that by the time the KRA administration was complete or they actually received the scores back, they had taught many of the skills measured on the KRA. Finally, there is a need to balance these changes with the technical adequacy/validity of the measure and measure length. Removing items from the measure in order to reduce administration time, such as the state did from Y1 to Y2, is only helpful if technical accuracy is retained. This should be considered in efforts to test the psychometric validity of the KRA. More consideration as to how KRAs integrate within existing data systems is merited. All of the teachers in this study reported using other assessments, and about a third explicitly reported that their existing assessments provided better data than the KRA. Thus, more intentionality concerning how KRAs fit into existing systems of assessment is warranted. Assessment on top of assessment cuts into instructional time and teacher satisfaction (Costenbader et al., 2000), especially when teachers view multiple assessments as varying in their utility. If it is more important to use the same assessment across the state (i.e., the KRA), other possible alternatives include replacing and or removing some assessment systems at the district level such that there is more systemic consistency. Another possibility is the creation a of tiered assessment system similar to Response to Intervention Models (e.g., Fletcher & Vaughn, 2009;Fuchs et al., 2010;Spear-Swerling & Cheesman, 2012). In this case, a KRA could be the first pass or initial screening and then more specific screening can be conducted if necessary, followed by diagnostic assessment as needed. Thus, the KRA would be the main assessment rather than an additional assessment, as it is currently being used. Part of designing assessment systems also involves balancing state initiatives with district initiatives. Importantly, we observed that there were differences in reported use of the KRA mostly by district with more teachers in higher-poverty districts using the KRA more than those in lowerpoverty districts. Additional research is needed to unpack these districtlevel differences in use of the KRA."}, {"section_title": "Training", "text": "Training and practice seem to be critical to supporting ongoing use of the KRA (Ackerman, 2018) and teachers' data use more generally (Datnow & Hubbard, 2015). Importantly, participants did report shorter administration times even if the KRA was still viewed as burdensome. It is reasonable to anticipate that this will continue to decrease further with more practice (Jacobs et al., 2009). One important finding is that teachers who had received more training on the KRA were more likely to note positive benefits of the KRA for informing instruction. This aligns with research on assessment use more generally indicating that teachers' use of data improves with additional training (Roehrig et al., 2008;Young, 2006). It may be that over time, with repeated training, teachers will come to view the KRA positively and use it in their practice. Thus, ongoing training is critical in supporting KRA implementation. As part of the training, teachers should also receive support as to how the KRA fits into existing assessment systems (after they have been well aligned). This is a critical need identified by other researchers as well (Kamler et al., 2014). Another positive findings is that teachers with less teaching experience tended to see more benefit of KRAs to students. It may be that teacher education programs are emphasizing assessment implementation and use more so than they have done in the past. As such, newer teachers may be able to find more benefit in KRA systems."}, {"section_title": "Conclusion", "text": "High-quality educational experiences informed by data-driven instruction can support all students but are especially crucial for those who face opportunity gaps in their access to a range of supports necessary for the positive development and nurturance (Darling-Hammond, 2010;Nores & Barnett, 2014). Those with the largest gaps in educational attainment are often students from low SES backgrounds who face a range of structural barriers to academic success (Sabol & Pianta, 2017). It seems that more work is needed to ensure that KRAs meet the mandate of supporting instructional practice at the local level in order to achieve these outcomes. Our findings indicate that with multiple changes to the KRA and an extra year of practice, teachers still did not feel that the KRA was beneficial for their instructional practice or their students. Bridging this policy-to-practice gap is critical in order to closing the readiness and achievement gaps."}]