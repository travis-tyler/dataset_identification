[{"section_title": "Abstract", "text": "Abstract-Nowadays, due to tremendous improvements in high performance computing, it has become easier to train Neural Networks. We intend to take advantage of this situation and apply this technology in solving real world problems. There was a need for automatic diagnosis certain diseases from medical images that could help a doctor/ radiologist for further action towards treating the illness. We chose Alzheimer's disease for this purpose. Alzheimer's disease is the leading cause of dementia and memory loss. Alzheimer's disease, it is caused by atrophy of the certain brain regions and by brain cell death. MRI scans reveal this information but atrophy regions are different for different people which makes the diagnosis a little trickier and often gets miss-diagnosed by doctors and radiologists. The Dataset used for this project is provided by OASIS, which contains over 400 subjects 100 of which having mild to severe dementia and is supplemented by MMSE and CDR standards of diagnosis in the same context. "}, {"section_title": "I. INTRODUCTION", "text": "Alzheimer's Disease (AD) is a neurological disorder that causes memory loss and dementia. It is mainly observed in elderly individuals over the age of 60 but can also be caused by concussions or traumatic brain injuries. It causes brain cells to die and spread the damage across the brain, in some severe cases rendering an individual unable to perform daily necessary tasks. It is also considered as neuro-degenerative type of dementia. There are many ways to diagnose and detect consisting of MRI scans, MMSE (mini-mental state exams) both expressed in terms of CDR standards. But identifying distinctions between Alzheimer's brain and normal brain in elderly individuals over the age of 75 is difficult because they share similar brain patterns and image intensities. AD causes shrinkage in hippocampus and cerebral cortex and it enlarges the ventricles in the brain. "}, {"section_title": "Detection of Alzheimer's Disease from MRI using Convolutional Neural Network with Tensorflow", "text": "(a) denotes no dementia, (b) denotes very mild dementia, (c) denotes mild dementia, and (d) denotes moderate dementia.\nII. DATA ACQUISITION The dataset which is being used in this project is obtained from OASIS (Open Access Series of Imaging Studies) which openly provides the dataset containing cross-sectional MRI scans for demented and non-demented individuals ranging from 18 years to 96 years. The dataset consists of 416 subjects which all are right handed and MRI data is standardized T1-weighted average from which 100 subjects have mild to moderate dementia and 198 subjects which are all over the age of 60.\nThe reliability data given in the format of comma separated values contains factors like CDR and MMSE which are important in diagnosing the AD. MMSE is a mini mental state examination and CDR is clinical dementia rating which is an efficient scale for severity of dementia. CDR also considers factors like impairments in memory orientation, judgement and problem solving. To avoid overfitting the dataset also includes 218 non-demented subjects from the age group of 18 to 59 years and includes demented elderly subjects from the age of 60 to 96 years with CDR scores shown in table 2.2.\nThe dataset also has 98 elderly subjects over the age of 60 with no dementia and an CDR of 0. We will use processed images provided within our dataset to train our model but some pre-labeling is required; labeling done on the basis of CDR = 0 indicating no dementia and CDR > 0 indicating dementia. preserving some data as test dataset is also beneficial to the training of our model. The additional data presented in the CSV file will be used to reproduce the demographics of the dataset and will be visualized in terms of graphs via appropriate machine learning algorithm for textual data."}, {"section_title": "III. TECHNOLOGIES", "text": "Technologies and tools used in the project include tensorflow, CuDNN, anaconda, other machine learning techniques for textual data analysis and image recognition. Networks such as convolutional neural network."}, {"section_title": "A. Neural Networks", "text": "A neural network is a graph or an interconnected network of nodes called neuron and the links are known as synapses. Each neuron represents on ML classifier holding biases 'B', it receives input 'X' with incoming synapse holding weight 'W' and then it is activated by some activation function and gives the output as the weight of the next neuron as f (\u2211i n Wi Xi + B), every initial neuron has initial blank in inputs as X0W0, which further generates hidden layers each with multiple neurons representing similar things."}, {"section_title": "B. Convolutional Neural Network", "text": "CNNs are mainly used for graphical data and recognition, processing of images. These are inspired by the human visual system created by modifications to classic neural networks. The input image is broken down into smaller chunks known as local receptive fields. CNN comes with 3 main layers each having a specific function to perform namely, convolutional layer, pooling layer and fully connected layer.\nConvolutional layer(CL) has set of learnable filters that can be applied on input images. It also consists of 3 dimensions and spatial portions. Each operation requires 4 hyper parameters. While convolutional layer handles image filtration, pooling layer deals with reducing the sample size and controlling overfitting. It is usually placed between convolutional layer and fully connected layer; which is classic neural network with every node interconnected with every other node. Usually RELU activation function is used in the convolutional neural network which stands for rectified linear units which makes training with image units much faster though SoftMax function can also be used in this regard for non-GPU architectures. RELUs also have distinguishable advantage that it does not require the input to be normalized which stops them from becoming saturated and as no modifications needed to the data the learning remains pure. Number of layers CL, PL (Pooling layer) and FC (Fully Connected Layer) may be increased as the need arises for more complex processing of large data."}, {"section_title": "C. Tools and Libraries", "text": "In developing our project, we intend to use tools and libraries, which are listed below; Anaconda Development Environment, provides all-around solution for Data-Science Needs as it comes pre-bundled with ML tools such as: I-python terminal, Jupyter Notebook, Scikit-learn which is a ML API Library which provides comprehensive modeling utilities, efficient ML APIs, widely supported by open communities. Graphics/ Plotting/ Visualization utilities, New bundles can easily be installed via wheel (.whl) package management and it is community supported. Tensorflow require users to create a Network graph, and the tensor variables flow in that graph, hence the name Tensorflow, Tensorflow also provides pretrained CNN known to everyone as Inception, which makes transfer learning possible for people who do not have GPUs available with them. It is relatively new technology which was made available in this decade. Top 20 Industry Giants use Tensorflow to optimize their software/ services."}, {"section_title": "Tensorflow-Tensorflow is the open", "text": "Creating visualizations in tensorflow is done through tensorboard. It is a utility used with training summaries to create visualizations like graphs, histograms and other similar techniques of visual representation. Training summaries are stored in a file while training the model, they contain information such as training step, accuracy, loss and other useful parameters.\nCuDNN-CuDNN library was developed by Nvidia for making HPC accessible. It makes use of CUDA cores in the GPU for parallel computing. So, it gives high performance throughput. It required at least 300 CUDA cores for small scale functioning, requires CUDA compute capability >= 5 and it also requires CUDA Toolkit.\nOnce you own a GPU, all the required drivers and software are freely available to download. It helps train ANNs faster through HPC (High-Performance Computing). Here, each node/ pixel gets processed simultaneously and individually. Neural Networks were fictional before the rise of Distributed Computing and/ or High-Performance Computing."}, {"section_title": "IV. RELATED WORK", "text": "Existing solutions used dataset from ADNI (Alzheimer's disease Neuroimaging Initiative). It used pre-labeled dataset which was classified on the basis of MMSE alone and claimed 99% accuracy but results may have been flawed as the dataset is not variable and could have led to over-fitting. It used feed forward and back propagation into its training and the model consist of each layer. Viz. convolution layer, pooling layer, fully connected layer. These solutions used caffe library along with high performance computing from Nvidia's CUDA and used RELU as an activation function.\nThe initial learning rate was kept at 0.01 and was decreased by the factor of 10 for each 10 iterations. These papers used prelabeled datasets and CDR scale was not taken into account while the data was labeled, CDR being a well-known and reliable scale, not using it would have made the labeling faulty and may have resulted in incorrect results.\nWhile others have used KSVD and SVM for classification of dataset. It claimed to have related and distinguished subjects on the basis of their dementia and schizophrenia. This paper does not use images or graphical dataset but only the supplementary CSV datasets, with using SVMs they have achieved 80% accuracy in their results."}, {"section_title": "V. PROPOSED SYSTEM", "text": "In the advanced stage of AD, severe shrinkage of the hippocampus and cerebral cortex, as well as significantly enlarged ventricles, can easily be recognized in MR images. But it's too late to treat the disease in final stages thus, earlier diagnosis is necessary. Robust machine learning algorithm such as CNN, which is able to classify Alzheimer's disease, will assist radiologists in diagnosing and will also aid in the accurate and timely diagnosis of Alzheimer's patients. Our intent attempts to provide an effective system for detecting atrophy in brain tissue of A.D. patients, help the radiologist get a better experience of analysis through pre-diagnosis, reduce the dependencies on manual evaluators, combine Image recognition efficiency of CNN with Neuro-science, increase the efficiency of a system using CNN & HPC Approach, train CNN model for recognizing atrophy and brain cell death.\nData preprocessing-Data wrangling is the foundation of our efforts through which we've obtained a usable set of training and testing data. We've created one-hot encoded arrays of the data which are randomized.\nThe images are saved as numpy arrays with one-hot encoding in order to train the model to have a binary decision as ouput.\nCNN Architecture-CNN is inspired by the human visual system and are similar to classic NN. Small portions of the image (local receptive fields) are treated as Inputs. It has multiple functional layers such as: Pooling layer which is placed between two consecutive Convolutional layers. whose job is to Reduce sample size and to control overfitting. Fully connected layer which is similar to ANN, here, each node is connected to every other previous/ next node. Convolutional layer, which deals with, set of learn-able filters consisting of 3 dimensions and spatial portion. "}, {"section_title": "It requires 4 hyper-parameters:", "text": "K, number of filters; Vector of weights to be applied to input image, it ensembles a feature in ML. F, their spatial extent; 3 Dimensions-width, height & depth. S, the size of stride; Amount of displacement for the filter, in pixels. P, the amount of zero padding; It control dis-proportionate input to fit the filter.\nApplication of filter or a convolution operation involves Shift, Scale, Rotate, Other transformations. To find patterns, CNN takes input images as W1 X H1 X D1. Then it outputs Next image as:\nIn our model architecture, we have 5 convolution layers, our model takes inspiration from the bell curve of the gaussian distribution. Our first convolution layer starts with 32 filters, followed by convolution layer with 64 filters, followed by convolution layer with 128 filters, followed by convolution layer with 64 filters, and followed by final convolution layer with 32 filters, each convolution layer is accompanied with a Max-Pooling of 5 layers. After the final convolution layer, we propagate the results through a fully connected layer of 1024 nodes with a dropout rate of 0.8. up till now, we have used RELU as our primary activation function. Now the activation function is set to SoftMax in order to facilitate the binary classification done by the final layer consisting of 2 nodes. Finally, we iterate and train our model with Adam optimizer and back-propagation. We've set the dropout rate to 0.8, meaning that in a randomized manner, every node will get activated only 80% of the time and is turned off 20% of the time. After performing dropout regularization on 1024 fully connected nodes, we needed to optimize the error with a gradient descent optimizer. We chose adam optimizer instead of usual stochastic gradient descent algorithm, because of its adaptive nature. It computes adaptive learning rates for parameters from the n-2 & n-1 moments of the gradient resulting in better optimization when compared to the stochastic methods. Tensorflow is highly scalable and can be ported to other platforms as internally it runs on low level programming languages such as C++. Tensorflow Model once trained, does't require HPC as the leaned features are preserved. Tensorflow does this by using protocol buffers, often referred as protobuffs with a (.pb) extension. Essentially, they are way to store information about network structure and weights, we obtain protobuffs by freezing a working tensorflow graph structure."}, {"section_title": "Algorithm-", "text": "Results-We trained our model for 100 epochs with each epoch consisting of 7 batches and found that from 400 th step our model was consistently giving good results. Figure 5 .2 shows the loss, at beginning the loss was as high as 90% but with the optimization performed at the end of each epoch consistently reduced the loss over the training period, resulting in the loss of less than 1% from 400 th step of the training. The loss was calculated by the categorical cross entropy method which is an efficient way of calculating loss in binary categories between predictions obtained and targets expected. This loss factor is taken into account by adam optimizer while updating weights and biased during the execution of back propagation. Figure 5 .3 shows the accuracy, at beginning the accuracy was as low as 10% but with the optimization performed at the end of each epoch consistently increased the accuracy over the training period, resulting in the accuracy of more than 99% from 400 th step of the training. The accuracy was calculated as 1 -loss and it is solely dependent on the loss factor; thus, lesser the loss greater the accuracy. After achieving satisfactory accuracy, we froze the model as a protobuff file with its parameters in order to preserve the learned features and graph structure. The model was then loaded for final execution purpose. We randomly picked 9 images from he testing set which our model had never seen before, 2 of which were of A.D. patients and rest of the images were of control group. Because of the dropout regularization and efficient data wrangling we accurately predicted all the test samples to comply with the targets and obtained the more generalized model that can classify images of A.D. subjects with minimal error."}, {"section_title": "VI. DISCUSSION AND COMPARISON", "text": "We have used methods described in the proposed system and to obtained maximum accuracy in our classification of AD as the dataset is the precise and well processed. The following table 6.1 shows the comparison between existing and proposed system and the intension behind the required improvement to the pre-existing systems.\nThough there are some difficulties faced by authors/ researchers such as: Resource heavy models, Use of outdated APIs, Expensive GPUs/ Cloud instances. Reliability of data is of utmost Importance, Because Bad data results into bad learning. Thus, correct and efficient data-wrangling was required and we have done so diligently. In ML, more we train our models, more accurate they become, we have minimized the error via Back-Propagating through 'adam optimizer' as well."}, {"section_title": "VII. CONCLUSION", "text": "We have shown that the proposed system has outperformed the existing systems and with the latest dataset and the latest technology such as Tensorflow, we can scale our model to production instantly. We observed that the CNN has successfully been tested and, in the past, has given maximum accuracy in image classification jobs viz. diabetic retinopathy, cucumber classification, etc. Here in this case as well, it has given similar success.\nFuture work of this project includes porting the system to MRI devices and making the system more usable with the highly flexible framework of Tensorflow. APIs and the MRI's will get processed in the system itself with built-in capabilities of our system to give the results out of the box which will reduce the manual labor for radiologists."}, {"section_title": "VIII. ACKNOWLEDGMENT", "text": "We express our gratitude towards Tensorflow team for making this project open source and the community supporting it. The Tensorflow is highly usable and has democratized data science and artificial intelligence. We express our gratitude towards OASIS for making such an accurate and reliable dataset openly available for use. We express our gratitude towards Nvidia for letting us individuals use CuDNN library which makes high performance computing possible to small scale Nvidia GPU owners. We express our gratitude towards continuum who made Anaconda bundle available which has helped data scientists get started with minimal efforts as required utilities comes pre-bundled. And we also express our gratitude towards known-unknown individuals and organizations that made this project possible."}]