[{"section_title": "Abstract", "text": "Abstract Automatically segmenting anatomical structures from 3D brain MRI images is an important task in neuroimaging. One major challenge is to design and learn effective image models accounting for the large variability in anatomy and data acquisition protocols. A deformable template is a type of generative model that attempts to explicitly match an input image with a template (atlas), and thus, they are robust against global intensity changes. On the other hand, discriminative models combine local image features to capture complex image patterns. In this paper, we propose a robust brain image segmentation algorithm that fuses together deformable templates and informative features. It takes advantage of the adaptation capability of the Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at:http://adni.loni.ucla.edu/wp-content/uploads/ how to apply/ADNI Acknowledgement List.pdf generative model and the classification power of the discriminative models. The proposed algorithm achieves both robustness and efficiency, and can be used to segment brain MRI images with large anatomical variations. We perform an extensive experimental study on four datasets of T1-weighted brain MRI data from different sources (1,082 MRI scans in total) and observe consistent improvement over the state-of-the-art systems."}, {"section_title": "Introduction", "text": "In neuroimaging studies, brain MRI segmentation is often a critical preprocessing step. Automated segmentation enables morphometric analysis of cortical and subcortical structures in large datasets (Fischl et al. 2002) , a scenario in which manual labeling is impractical. The automatically segmented regions can be used to extract informative characteristics of structures, such as volumes and shape. In the clinic, these features have the potential to be used to evaluate the condition of a subject. Moreover, the identified boundaries between cortical and subcortical structures can aid the planning of brain surgery (Wels et al. 2009 ). In neuroscience research, statistics derived from the segmentations of control and experimental groups can be used to identify structural differences between them. In the context of disease studies, such differences can lead to the identification of new pathological biomarkers. For instance, the atrophy and morphological change of hippocampus have been identified as important markers for Alzheimer's disease (Jack et al. 2008 ).\nSeveral practical segmentation methods are widely used in neuroimaging studies, e.g. Caret (2001) , FreeSurfer (2002) , FSL (2002) , ITK-SNAP (2006) , SPM (2003) , and the segmentation utilities in 3D Slicer (Pieper et al. 2006 ). However, robustness against variations in the input imaging data remains an open problem. The main difficulties are due to: (1) variations in image intensities due to differences in MRI acquisition (hardware, pulse sequence, imaging parameters); (2) anatomical variations within and across populations. Intensity normalization and image registration (Hou 2006; Klein and et al. 2009 ) can be used to standardize the images prior to segmentation, but only to some extent, since many of the variations are intrinsic.\nIn this paper, we aim to build a robust system that automatically segments T1-weighted brain MRI volumes into anatomical sub-cortical and cortical structures. We approach the 3D brain image segmentation problem from a statistical modeling perspective, combining a generative and a discriminative model with feature augmentation and adaptation. Generative and discriminative models were first explored and compared in the machine learning and computer vision literatures (Ng and Jordan 2002; Tu 2007) . It has been shown that, while generative models outperform discriminative models when the size of the training dataset is small, the latter often have a better asymptotic behavior (Liang and Jordan 2008) . Works that attempt to combine both types of model include (Jebara 2003; Raina et al. 2003; Lasserre et al. 2006; Holub et al. 2008) , which show that integrating the two types of models can be beneficial.\nSpecifically, the goal of brain MRI segmentation is to separate the voxels of an input scan into a number of classes. In some studies, these classes correspond to the three basic tissue types in the brain: gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) (Wells et al. 1996; Pham and Prince 1999; Leemput et al. 2001; Shattuck et al. 2001; Wu and Chung 2005; Bazin and Pham 2007; Li and Fan 2008) . Other works have attempted to produce labels at the level of brain structures (e.g., hippocampus, pallidum, putamen, etc.) (Fischl et al. 2002; Scherrer et al. 2007; Klauschen et al. 2009; Bazin and Pham 2009) , which is a more difficult problem but yields a richer description of the data.\nTo produce these labels, generative models typically rely on two components: a prior term that summarizes the statistical frequency and spatial distribution of labels and a likelihood term that models how these labels translate into intensities. Then, Bayesian inference can be used to estimate which labels (i.e., which segmentation) are the most likely given the observed image intensities. The prior term is usually in the form of a statistical atlas endowed with deformation model (Ashburner and Friston 2005; Leemput et al. 2001; Fischl et al. 2002) . Other priors include the use of principal component analysis (PCA) to model whole shape variations (Pohl et al. 2006; Yang et al. 2004) or Markov random fields (MRF) to impose local shape constraints (Fischl et al. 2002; Woolrich and Behrens 2006; Scherrer et al. 2009; Caldairou et al. 2011) . For the likelihood term, Gaussian distributions or mixtures thereof have been predominant in the literature, due to their ease of inference (Fischl et al. 2004; Yang et al. 2004; Pizer et al. 2003; Corso et al. 2008) . Because the Gaussian parameters (means and variances) are estimated during the optimization, these algorithms are robust against changes in MRI contrast. Moreover, they can also explicitly model image artifacts such as the MRI bias field, making them robust against them as well (see for instance Leemput et al. (2001) ).\nRecently proposed multi-atlas methods such as Gouttard (2007) , Wu and Chung (2008) , Klein et al. (2009) , Aljabar et al. (2007) , Heckemann et al. (2006) , Wolz et al. (2009) , Bazin and Pham (2009) , and Sabuncu et al. (2010) can also be seen as generative models. These methods are based on deforming a number of labeled templates to a test scan, and then fusing the deformed labels into a single, enhanced estimate of the segmentation. As explained in Sabuncu et al. (2010) , these algorithms can be interpreted as generative models in which the intensity and label of each voxel are taken from one of the deformed templates as indexed by a latent, discrete field.\nDiscriminative models attempt to directly estimate the label of each voxel given the local appearance of the image around it. To do so, a number of features are computed for each voxel and fed to a classifier that attempts to infer the corresponding label. Popular choices of features include image intensities, gradients, textures and other derived local measures (Tu et al. 2008) . Choices of classifier range from simple rule-based classifiers (Li et al. 1993 ) to more complicated frameworks such as support vector machines (SVM Lee et al. 2005; Lao et al. 2006; Akselrod-Ballin et al. 2006; Zhang et al. 2009 ), AdaBoost (Quddus et al. 2005; Morra et al. 2010 ) and the increasingly popular random forests (Breiman 2001; Yi et al. 2009; Geremia et al. 2010; Yaqub et al. 2011) . These techniques show promising results when segmenting tissues, structures, and even tumors (Bauer et al. 2011; Li and Fan 2012) if the variations of the test data with respect to the training data are relatively small. Unfortunately, changes in MRI contrast due to differences in imaging hardware or acquisition protocol considerably reduce the performance of these methods, limiting their applicability to MRI scans that have not been acquired the same way as the training dataset.\nComparing generative and discriminative models, we observe that deformable templates (Felzenszwalb 2005; Pizer et al. 2003) guided by generative models can efficiently model anatomical structures, thanks to their flexibility and adaptability. However, their simplified assumptions on underlying image statistics (e.g., Gaussian distributions) limit their ability to deal with complex intensity patterns. On the other hand, such patterns can be efficiently captured by discriminative models thanks to their capacity of fusing together a large number of features. However, as described above, they are sensitive to global intensity changes and have difficulties to include spatial information. In this paper, we propose combining the strengths of the two approaches by fusing deformable templates (generative) and informative features (discriminative). The presented approach is based on using the estimated segmentation and the parameters of the generative model to normalize the image intensities and extract robust, invariant local features. This creates an augmented feature space that can be used in a discriminative framework, effectively fusing the two types of model.\nThe rest of this paper is organized as follows. First, section \"Further Related Work\" surveys other attempts of combining generative and discriminative models in brain MRI segmentation. Section \"Model Description\" describes the proposed segmentation framework. Section \"Learning and Using Fusion Models\" describes how to train the model and use it to segment previously unseen test cases. A set of experiments and their corresponding results are described in section \"Experiments\". Finally, section \"Conclusion and Future Work\" concludes the paper."}, {"section_title": "Further Related Work", "text": "Here we discuss other works in the literature that are similar to the proposed approach, highlighting the differences between them: -Verma et al. (2008) use Bayesian and SVM models to classify the intra-and inter-patient tissue distributions. However, the two families of methods are separately used for the different sub-problems, rather than in an integrated fashion. -In Tu et al. (2008) , a discriminative classifier is used for appearance, and its classification results are regularized by generative models (PCA) capturing the shape prior. The two types of models are represented as separate modules and not jointly considered. In this paper, we utilize the same discriminative model, whereas the generative models are totally different. As shown in the experiments later on in this paper, the proposed method outperforms this algorithm, which is not robust across datasets due to the inability of the discriminative model to adapt to the new data. -Wels et al. (2009) cast the subcortical segmentation as a posterior maximizing problem with shape parameters. The whole problem is decomposed into four sub-problems in sequence and each is solved by a discriminative model. Their segmentation then locates, rotates, scales, and refines the boundaries of structures in order, which performs a rough generative process. Nevertheless, the four underlying classifiers are purely discriminative so their weaknesses remain. If some stage fails, wrong information is propagated without any adaptation or correction. On the contrary, our discriminative model can benefit from the adaptation capability of generative models so the robustness is achieved. -Wels et al. (2011) also propose a hybrid method for tissue segmentation. However, their approach is sequential: the discriminative model serves for initializing and constraining the subsequent fitting of the generative model, which is carried out with an expectation maximization algorithm. The approach is validated on 38 scans from the same dataset (Internet Brain Segmentation Repository) using only three classes (WM/GM/CSF). It is unclear how this method would generalize to a higher number of brain structures, since the application of discriminative models at the structure level is more challenging than at tissue class level (i.e., just three classes). -Fan et al. (2007) Compared with these approaches, the key aspect that differentiates our algorithm is the use of an adaptive, augmented feature space that allows us to effectively fuse generative and discriminative models (as described in section \"Model Fusion)\", rather than simply cascading them."}, {"section_title": "Model Description", "text": "In this section, a general formulation of the segmentation problem is provided. We first compare the generative and discriminative approach to the segmentation problem. The comparison reveals their complementary properties, and inspires our hybrid method. The idea is to first use a deformable template (the generative component, described in section \"Deformable Templates Guided by Generative Models\") that produces a first estimate of the segmentation for an input volume. This rough estimate is then used (as described in in section \"Model Fusion\") to: (1) calculate the regional statistics of the input volume for local (discriminative) feature normalization, and (2) derive local features such as region boundaries and label priors. These features, in combination with local appearance-based, discriminative features, give an augmented set that yields a rich description of the data by integrating information from the generative and discriminative sides. This augmented feature set will be used to train a classifier as described in section \"Learning and Using Fusion Models\"."}, {"section_title": "Problem Formulation", "text": "Our goal is to segment a given 3D volume/image V into K anatomical structures, where K is a fixed number. A training set of N volumes with their corresponding annotations (no less than K labels) is assumed to be available, and we denote this set as \nwhere R 0 refers to the background region, and each R i consists of all the voxels of the i th anatomical structure. These regions are disjoint and they cover the entire volume: K i=0 R i = , where defines the 3D lattice of the input V, and R i \u2229 R j = \u2205, \u2200i = j . i is a vector that includes the model parameters for the appearance and shape of region i.\nIf we define p(V(R i )|R i , i ) as the likelihood of the volume confined in region R i under model parameters i , the optimal solution in a Bayesian framework can be obtained as:\nwhere p(R i ) is the probability of the shape prior, whereas p( i ) puts a prior on the parameters and is usually assumed to be flat i.e., p( i ) \u221d 1. Moreover, we have assumed that the shapes of the different regions are independent, which allows us to write p(W ) = p(R i ). Whereas a more faithful model would consider dependencies between the shapes of the structures, this common assumption greatly simplifies both the training of the model and the inference in the Bayesian framework. We further assume that the intensity inhomogeneity of a region is smooth and small enough (Leemput et al. 1999) .\nIn general, independent identical distribution (i.i.d.) assumptions are made in the likelihood function, and the appearance of each structure is approximated by a Gaussian model (Fischi et al. 2004; Pohl et al. 2006) . Let G(\u00b7; i ) denote a Gaussian distribution parameterized by i and let v j be the intensity value of voxel j. The likelihood then can be represented as:\nwhere i contains the mean and standard deviation of region i: i = {v i , \u03c3 i }. As argued in Tu et al. (2008) , using only generative models with i.i.d. assumptions is often too simplistic to fully account for the realistic textures of MRI data. This insufficiency, especially between structural boundaries, will be addressed by the discriminative models in our method.\nFor a discriminative approach, there is no explicit data parameter estimated for each input volume V; the model is instead learned in the form of a classifier derived from a training dataset. Thus, the solution vector by a discriminative model becomes\nWe use |V| to represent the total number of voxels in V and l j to denote the label assigned to voxel j. R i is therefore the set of all l j = i. A discriminative classifier directly computes the class label at a voxel j which has the maximum class posterior, which is based on the local features computed on the sub-volume V(N j ) centered at j:\nwhere the label of voxel j maximizing the equation is denoted as l * j . If we compare the solution vectors W and W R in Eqs. 2 (generative) and 4 (discriminative), we make two observations. First, generative models explicitly estimate the data parameters and thus are adaptive to the input. Second, discriminative models can efficiently capture complex local image statistics by combining many low-and mid-level features. As discussed above, generative models make simplistic assumptions for the likelihood term modeling the local appearance, whereas discriminative models struggle capturing the information from larger regions. Therefore, in this study we will use deformable templates guided by generative models (as described in section \"Deformable Templates Guided by Generative Models\") and seek to fuse them with the discriminative model in Eq. 4 (as described in section \"Model Fusion\")."}, {"section_title": "Deformable Templates Guided by Generative Models", "text": "For the generative model of image intensities we adopt a Gaussian mixture model due to its modeling capability (Fischl et al. 2002; Yang et al. 2004; Pizer et al. 2003) and relatively low computational complexity. Let = { i , i = 0, . . . , K}, we introduce the weights of Gaussian components so the parameters for each region are given by: (m) i , and \u03b2 (m) i are respectively the mean, standard deviation and weight of Gaussian component m of region R i . In this paper, we assume two components m = 1, 2 for each model, which is empirically sufficient to describe the appearance of the anatomical regions. The parameters of these Gaussian components are obtained with a expectation maximization (EM) algorithm (Calinon et al. 2007) . According to this model, we have the following likelihood function of voxel j in region R i :\nGiven a volume V, we seek to minimize an energy function under the Bayesian formulation of Eq. 2 with flat p( i ):\nwhere the first term assumes the mixture model in Eq. 5 and the second term is a Markov Random Field prior for p(R i ) that encourages smooth region boundaries/surfaces; using other priors is also possible. N j is the set of neighboring voxels of j and l(j ) and l(j ) are respectively the region labels of j and j; \u03b4(\u00b7) is Kronecker's delta and \u03ba is a constant that balances the weight of the smoothness prior versus the likelihood of the intensities. We assume p( i ) \u221d 1 so this term can be omitted. Such a flat prior implies that, a priori, we do not prefer any values of the Gaussian parameters over others. In other words, we assume no prior knowledge on the intensities of the image. An estimate of W R and can be obtained by minimizing E(W R , , V):\nStarting from an initial solution (a deformed template containing a volume V a and its label annotation A a ), we minimize the energy E(W R , , V) in Eq. 6 using a region competition algorithm (Tu et al. 2008; Zhu and Yuille 1996) . Henceforth, we denote this algorithm as gmDT (generative model based on a deformable template).\nTo avoid the initial solution is biased, we use the set of training volumes with their corresponding labels,\ntr N , to generate the template denoted as (V a , A a ). We use D(V, V n ) to denote the dissimilarity between V and V tr n after applying a linear transformation (in our case, computed with AIR (Woods et al. 1993) ). The learned template volume V a minimizes the total dissimilarity with all other volumes in the training set:\nThe corresponding manual annotation of V a , A a , is used as the initial W R in Eq. 7. Then, the obtained segmentation estimated by gmDT,\u0174 R , is an approximation to the optimal solution. A byproduct of gmDT is\u02c6 , the parameter estimates."}, {"section_title": "Model Fusion", "text": "Next, we discuss the discriminative model in our method, which is used to incorporate the adapted information from gmDT. For a purely discriminative model, we denote the total number of candidate features by B and the k th feature for voxel j computed on volume\nof each voxel j can be written as:\nA discriminative classifier, e.g., boosting (Freund and Schapire 1997) , selects a number of informative features (typically a couple of hundred) from F d (\u00b7) and fuses them with appropriate weights. The training process is driven by the minimization of the classification error in the labeled training data and the generalization power of the classifier (VC dimension, Vapnik 1982) . Hence, the quality of a trained discriminative classifier is greatly determined by the effectiveness of its feature set.\nTo achieve enhanced robustness, the basic idea here is to integrate the adaptiveness and the fusion capability respectively from generative and discriminative models. This is done by augmenting the feature vector F d with (\u0174 R ,\u02c6 ) from the deformable template. This way, we achieve robustness against intensity variations while we equip F d with structure-adapted features."}, {"section_title": "Using\u0174 R for Intensity Normalization", "text": "Features computed directly from V(N j ) are often sensitive to geometrical and intensity variations, but\u02c6 from Eq. 7 can then be used to normalize V. We denote the normalized volume as V\u02c6 . The new, augmented feature vector F(j) is:\nComparing Eq. 10 with Eq. 9,\nrepresents the augmented features based on\u0174 R that will be discussed below.\nNormalization is achieved by intensity correction based on matching the intensity of the regions to those from the template volume (Hou 2006) . Specifically, we search for the linear transform that best matches the intensities in a least squares sense, a problem that can be solved with standard techniques (Tibshirani 1996) ."}, {"section_title": "Augmenting Atlas Features from\u0174 R", "text": "From\u0174 R (given by gmDT), we have an estimated region label for each voxel j. To differentiate this estimated label from l j in Eq. 4, we usel j to denote it. From\u0174 R , the displacement vector of a voxel j to the centroid of structure k, dR k (j ), can be calculated. This displacement vector is a spatial feature which is more adaptive than the absolute coordinates. Similarly, we can compute the signed distance function of each voxel j with respect to the estimated region boundary of each anatomical structure. The signed distance sR i (j ) of voxel j to the boundary of regionR i is:\nwhere d(j, j ) is the distance between j to any point j on the region boundary\u0108 i . The positive/negative sign indicates that j is inside/outsideR i . Now our augmented feature vector becomes:\nHenceforth, we denote these features derived from\u0174 R as \"atlas features\". They correspond to F\u0174 R (j ) in Eq. 10."}, {"section_title": "Learning and Using Fusion Models", "text": "Here we aggregate all the components in section \"Model Description\" to define the training and classification (testing) stages of our method. An atlas (template) will first be selected among the training data as the template. As described in section \"Deformable Templates Guided by Generative Models\"), the template will be guided by gmDT to adapt to the input volumes. Though we know the true labels of the training data, gmDT is applied (normalization and atlas features) in training the discriminative models so the resultant classifiers can model the estimates from gmDT in the test stage."}, {"section_title": "Atlas Selection and Feature onstitution", "text": "In the training stage, we have the set of training volumes with their corresponding labels S = V tr n , A tr n , n = 1..N , which have been normalized to the same scale and properly preprocessed. A template, (V a , A a ), can be learned based on Eq. 8.\nFor our model, we will need to learn the uncertainty from gmDT before training the discriminative models. Using A a as the initial labeling, we perform gmDT on the rest of training volumes in S. For each training volume V tr n , an estimated segmentation\u0174 n tr is obtained from gmDT. On the other hand, we define the feature set for the discriminative classifiers, F d (\u00b7), as local features such as gradients, curvatures, and Haar-like responses at various spatial scales (approximately 5,000 in total in this paper, but our method is not restricted to the specific F d (\u00b7)). Since the training volumes are in the same size, these features can be computed directly on a pre-defined sub-window (of size 11 \u00d7 11 \u00d7 11) centered at the target voxel. A detailed discussion of how these features are computed can be found in Tu et al. (2008) . We use the estimated\u0174 n tr to: (1) normalize the intensity of V tr n to V a as in section \"Model Fusion\" and then (V\u02c6 (N j )) on the normalized intensity volume; and (2) compute the atlas features F\u0174 R (j ) as in Eq. 12. Combining the two sets yields the augmented feature set (about 6,000 features), F(j), for each voxel j in the training volume V tr n . In short, gmDT is also applied to the training volumes. The role of the ground truth labels in the training stage is to train the discriminative, supervised classifiers; they do not participate before computing the augmented features."}, {"section_title": "Integration within a Discriminative Framework", "text": "Once all the features F(j) are computed, we train a classifier upon the training set\nwhere l(j ) is the true label for voxel j, and T is the total number of voxels in all the training volumes.\nA learning algorithm either directly combines all the features in F(j) like SVM (Vapnik 1998) , or selects a set of features out of F(j) such as boosting (Freund and Schapire 1997) and random forests (Breiman 2001) . Either way, features are combined into the classifier in order to minimize the training error. When training our model, no preference was given to features derived from the atlas over those computed from image intensities (e.g., Haar-like), and vice versa. Here we adopt the auto-context algorithm using a cascade of PBT (Probabilistic Boosting Tree) classifiers (Tu and Bai 2010 ) as the discriminative model, which performs feature selection and fusion by exploring a highdimensional feature space. Note that, in addition to the features in F(j), auto-context (Tu and Bai 2010) itself is an iterative method that incorporates contextual information into the classification by augmenting the feature space as follows. In a first iteration, the classifier is trained on the available set of features. In subsequent iterations, the label posteriors (as estimated by the current classifier) at a fixed set of shifted locations are added to the feature space, implicitly capturing the shape of the structures to segment. A summary of the algorithm is described in Fig. 1 ; the reader is referred to the original paper (Tu and Bai 2010) for further details.\nNevertheless, the key of our proposed method is the augmentation/normalization of features, which implicitly fuses the generative and discriminate aspects of the model. It is not tied to any specific choice of classifier so one could also use boosting, random forests, or any probabilistic classifiers as the discriminative classifier.\nOnce a classifier has been trained on the training set \u2135 = {(l(j ), F(j)), j = 1..T }, we can use it to estimate p(l|F(i)) for a given test voxel i. A test volume is required to have the same preprocessing steps and resized such that all features can be correctly computed. Each test voxel will be assigned to the label that maximizes the probability:\nWe summarize the training algorithm in Fig. 2 and illustrate it in Fig. 3 . In Fig. 3 , the identical template, (V a , A a ), will be used in both training and testing. The testing procedure consists of three stages analogous to those in the training procedure, except the last stage: performing classification using the trained classifier."}, {"section_title": "Experiments", "text": "To examine the effectiveness of the proposed algorithm, we perform a thorough empirical study using four MRI T1-weighted brain datasets and compare our method against the state-of-the-art systems. We focus on sub-cortical structures due to their popularity in the literature. Besides comparing our method with the two components in our method, gmDT and DM, we also include the methods developed by other researchers to show the integration can achieve better performance in most scenarios.\nWe will demonstrate the performance of the integrated method by four parts: (1) Increased importance of the adapted atlas features using the fusion mechanism; (2) Comparison using the same dataset for training and testing (intra-dataset). This scenario is very suit for DM and the proposed method is comparable to it; (3) Comparison using different datasets for training and testing (inter-dataset). This scenario is favored by gmDT and our method achieved better results; (4) Performance on longitudinal data, which shows the potential of our method to capture the morphological changes by the same subject. Though the fact that DM tends to fail in the last two parts is known, we still show its quantitative results for completeness."}, {"section_title": "Experimental Setup", "text": "In this section, we elaborate the datasets used in our four parts of experiments, the three main algorithms to compare, the pre-processing steps we applied to the heterogeneous datasets, and the measures we used for our comparisons. "}, {"section_title": "T1 MRI Datasets", "text": "All the datasets used in our experiments are the following (the suffix indicates the number of volumes in the dataset):\n( Note that not all the datasets have manual delineations of all subcortical structures. We summarize the main characteristics of these four datasets, as well as the role they play in our experiments, in Table 1 ."}, {"section_title": "Algorithms to Compare", "text": "We adopt region competition (Tu et al. 2008; Zhu and Yuille 1996) as the gmDT process which iteratively minimizes the energy E(W R , , V) in Eq. 6. We choose region competition due to its simplicity and effectiveness. The template (V a , A a ) is learned according to Eq. 8, using Mattes mutual Fig. 3 Training procedures of the proposed fusion method. A template of the volume/label pair is selected and guided by gmDT using other training volumes as the input. The augmented feature set F is then extracted to train the discriminative model giving the manual labels of the training volumes information as the dissimilarity function. The region competition process takes about 5 \u223c 15 minutes to reach a steady state (no more change of labels, or fluctuation of labels) of surface evolution. It runs typically for less than 15 iterations. Other approaches such as the level set methods (Chan and Vese 2001; Yang et al. 2004 ) could also have been used to perform energy minimization in a similar manner.\nOn the other hand, we use the auto-context algorithm (Tu and Bai 2010) with PBT as the baseline discriminative classifier. Henceforth, this discriminative model is denoted as DM. Its running time is 10 \u223c 20 minutes, depending on the number of structures for segmentation.\nComparisons between our method, gmDT and DM demonstrates the advantages due to the integration. We also list the measures from literatures for the same T1 MRI dataset and structures if available."}, {"section_title": "Pre-Processing", "text": "Before applying the three algorithms for comparison, these heterogeneous datasets several preprocessing steps. We eliminate the dominant spatial disparity between an input volume and an atlas by the following sequence of preprocessing steps: (1) re-orientation using AIR 2.5 (Woods et al. 1993 ) (if the two volumes were at different orientations); (2) skull stripping using BET in FSL 4.0.3 (Smith 2002) ; (3) a 12-parameter global affine registration using AIR 2.5; and (4) a diffeomorphic registration, SyN, from ANTS 1.9 (Avants et al. 2008) . We chose SyN as our non-linear method stage due to its speed and high accuracy (Klein and et al. 2009 ). We use the following settings: three resolution levels (30x50x5 iterations), step-length 0.15, probability mapping (PR) with 4 mm radius as cost function, and regularization with a Gaussian filter with standard deviation 3 mm. These parameters are obtained empirically and they provide sufficient spatial alignments for gmDT across the four test datasets. Under these settings, the SyN registration between an atlas and an image can be done under 30 minutes. Steps prior to SyN can be done in 5 minutes. Once the segmentation result is obtained, each preprocessing step is reverted to map the result back to the original space."}, {"section_title": "Measures for Comparison", "text": "The main evaluation measure used here is the Dice overlap, Dice = 2 * |L\u2229S| (|L|+|S|) , where L and S are the sets of voxels manually annotated and those automatically segmented. Precision and recall rates are also used in Table 6 , where P recision = |L \u2229 S|/|S| and Recall = |L \u2229 S|/|L|. Another popular measure in literature, Jaccard coefficient, can be directly calculated from Dice: J accard \u22121 = 2Dice \u22121 \u2212 1. In the inter-dataset tests, as our method and gmDT are adaptive, we further compare them in surface consistency. Hausdorff distances (Loncaric 1998 ) and the Mean distances (Yang et al. 2004 ) between two sets of surface voxels are measured:\nwhere A and B are sets of voxels. H (\u00b7) and M(\u00b7) are respectively the Hausdorff distance and the Mean distance. D is the underlying distance metric, which is usually the Euclidean distance or the Manhattan distance. C A is the surface of segment A. Note that both the directed Hausdorff distance and the Mean distances are not symmetric. We will use H(A,B), The first row shows the proportion of the corresponding type of features in the whole candidate feature pool for reference. The column \"Others\" includes the features of intensities, gradients, and curvatures H(B,A), and one direction (segmented to ground truth) of the Mean distances in our evaluation."}, {"section_title": "Importance of Atlas Features", "text": "To demonstrate the importance of using atlas features based on deformable templates, we compare the features used in two models trained to segment the 56 brain structures of LPBA, one with a fixed annotation A a (no adaptation to the input volume) and another with an adapted atlas based on gmDT. The feature pool is dominated by Haar-like responses due to their effectiveness to describe the appearance at different spatial scales. Derivative features perform similarly to Haar, but are limited to eighteen local derivatives in the x, y, and z directions. Features in the column \"Others\" include intensities, gradients, and curvatures. As mentioned in section \"Integration within a Discriminative Framework\", we perform feature selection when training the Probability Boosting Tree (PBT). For each node in PBT, Table 4 a limited number of features are chosen to form a decision criterion (a boosting classifier) such that the classification error is minimized for the training data arriving at this node. Giving the same complexity of the trained classifiers (the same tree depth and the same number of features used at each tree node), we observe in the table that more atlas features are selected when using an adapted atlas rather than a fixed one; the percentage of the selected atlas features increases from <3 % to 6 % ( Table 2) ."}, {"section_title": "Intra-Dataset Evaluation", "text": "In this section, we evaluate our algorithm using training and test data from the same dataset, which is common in brain image segmentation. IBSR18 and LPBA40 are included in this experiment.\n(1) IBSR18: A total of 14 subcortical structures including the left and right lateral-ventricles are evaluated. Due to the relatively few number of volumes in IBSR18, we perform a threefold cross-validation, i.e., six volumes in each fold are used as the test set, and the other twelve volumes are used to train our model. We use the box plots in Fig. 4 to compare the three methods and Table 3a to lists the Dice overlaps produced by DM, pure SyN, gmDT, and our method. The measures of SyN were included to clarify the improvements between it and our generative process.\nIn Table 3a , the row SyN lists six of the intermediate DICE overlaps before using gmDT. SyN performs a diffeomorphic transform mainly adapted to larger spatial variations of the whole brain so it can reduce location errors. Based on its rough results, gmDT utilized the relatively reliable statistics of each structure so the DICE values can be boosted at least 0.039(Thalami). Therefore, the use of SyN in our system is to reduce the risk of failure in gmDT; it is still a preprocessing step instead of a main factor in the whole our method.\nThe data in IBSR18 display larger variations in appearance and shape, From  Fig. 4 , we observe that the advantage of gmDT is inherited by our fusion method but the similar and better measures. A visual example of the extracted structures by DM, gmDT and our method is shown in Fig. 5 . Although not all of the 18 structures can be shown in a single 2D slice, better matched caudate nuclei, thalami, and putamens by our algorithm due to the fused information can still be observed. Table 3b lists several published reports on IBSR18, and our measures are at the top on the seven types of gray matter structures, with particularly significant improvements for the pallidum and amygdala. The results are close to Khan et al. (2009) which is based on a computationally expensive multi-atlas 40 subjects with fifty-six anatomical structures, both cortical and sub-cortical, were manually delineated for each T1-weighted MRI volumes. We randomly choose 25 volumes for training and use the remaining 15 for testing.\nTo compare our algorithm against gmDT and DM, we use Fig. 6 and the detailed measures in Table 4 for a comprehensive comparison. From Fig. 6 , we observe that fusing the two models in our method achieves higher average Dice coefficients than the two baseline methods. Our method gives better measures on L/R hippocampus (pvalue=0.017/0.003) than both the discriminative and the generative models, but the measures of caudate nuclei are worse than DM due to the considerably degraded performance of the generative model. Reasons for this could be (1) the intra-dataset test of LPBA40 containing smaller data variance is favored by DM, and (2) the boundaries between the caudate nuclei and the ventricles in LPBA40 are obscured so gmDT could be misled. The weaknesses of gmDT in this scenario are the inferior DICE scores and quartile positions. These are all improved by our fusion method due to the corrections in the discriminative framework, especially in the left/right caudates and the left putamen. The difference in the average Dice overlap between our method and gmDT is relatively small (\u223c 1 %), but still very statistically significant by the p-values shown in Table 4 .\nIn summary, the two intra-dataset tests show the proposed fusion method successfully combine the advantages of DM and gmDT, and its performance is at least comparable to DM. Fig. 7 Inter-dataset Hausdorff distance measures in mm, on 40 LPBA40 volumes for extracting the three types of subcortical structures (smaller is better). We denote the automated segmented result as W * and the ground truth as W."}, {"section_title": "Robustness Across Different Datasets (Inter-Dataset)", "text": "In the previous experiments, we trained and tested our method on volumes from the same dataset. To build a practical system dealing with clinical data, it is important to test its robustness on a large number of volumes from various sources.\nWe use the same model trained from IBSR18 as in section \"Intra-Dataset Evaluation\"(1). The test datasets include LPBA40 and LONI28. Both of them have manual annotations of subcortical structures (caudate nuclei, putamens, and hippocampi for both sets, lateral ventricles only for LONI28). These structures are all covered by the annotations of IBSR (see section \"Intra-Dataset Evaluation\"(1). We choose these subcortical structures because: (1) they are frequently used to evaluate automatic segmentation methods; and (2) they are very relevant in neuro-image studies of diseases such as Alzheimer's and Parkinson's. To keep the results comparable with other literatures on the same test datasets, we chose not to re-annotate the structures but keep the annotations as they were. In the following experiments, DM, gmDT, and our algorithm are trained from the same training data. The other methods (FreeSurfer and FSL) are off-the-shelf systems without any parameter adjustments. Although the difference among the three protocols introduce intrinsic Dice errors, they still share a high proportion of common structure regions and the relative performance between the three models trained by the same data can be observed.\nIn addition to the quantitative analyses on the two test datasets, we also test the model trained on LPBA40 -section \"Intra-Dataset Evaluation\"(2) -and show visual examples on ADNI980, LPBA40 and LONI28 at the end of this section.\n(1) LPBA40: Table 5 gives the Dice measures of three types of subcortical structures in LPBA40. All values are the averages of the corresponding measures of the left and right structures. Our method demonstrates the best performance among all five competing algorithms. Compared with the intra-dataset Fig. 9 Box plots of the Dice coefficients of the generative model based deformable template approach (gmDT) and our method on LPBA40. The top and bottom ends of a vertical line are the maximum and minimum values; The upper and lower edges of a box are the quartiles (25% and 75% data). The line inside a box indicates the average value, which is the same as Table 5 . The model was trained by IBSR18 The measures of FreeSurfer and Hybrid tested on 14 volumes of LONI28 were from Tu et al. (2008) , so we use the same measures (precision and recall rates) and F-values for comparison. Tu et al. (2008) can be considered as a combination of DM and a shape prior. gmDT is the generative model based deformable template approach. The abbreviations are: L/RH=Left/Right Hippocampus, L/RC=Left/Right Caudate Nucleus, L/RP=Left/Right Putamen, L/RV=Left/Right Lateral Ventricle, Av=Average. In Tu et al. (2008) , the Hybrid model is trained by another 14 volumes of LONI28. Our model is trained from IBSR18 and still showed the best average values among the four methods. The best F-value of each structure is marked in bold results in Table 4 , noticeable degradation of the Dice overlaps is found in hippocampus among all methods in Table 5 . This is because the subiculum region of hippocampus in LPBA40 was annotated differently from IBSR and many other datasets.\nThe improvement in surface distance measures given by our method is shown in Figs. 7 and 8. We denote the automated segmented result as W * and the ground truth as W . Our method achieves consistent improvement over gmDT. According to The improvement produced by our method can also be observed in the box plots in Fig. 12 . Our method not only increases the average Dice coefficient in all the tested structures (higher average values), but the worst cases have all improved. (3) 56 structures on ADNI980, LONI28, and IBSR18: In addition to the two subcortical tests on our IBSR18 subcortical model, we used the fifty-six structure model trained upon LPBA40 in section \"Intra-Dataset Evaluation\"(2) to perform cortical and subcortical segmentation on three datasets: ADNI980, LONI28, and IBSR18. Figure 13 shows a number of 2D MRI slices together with their segmentation results. We see from the figure that the intensity patterns and textures are quite different for these datasets. Even within the same dataset, ADNI980, the MRI slices show large variation since not all of them were acquired with the same scanner. However, despite such a high degree of variability in the data, the segmentation results are mostly satisfactory.\nBy the first two parts of tests containing disparate training and test datasets, we see the proposed fusion method can achieve the highest compatibility between different protocols, all by experts, than DM and gmDT given the same training set. The third test further shows its robustness to large data variation. Both properties are important when applying the proposed method as a common brain segmentation tool."}, {"section_title": "Performance in Longitudinal Studies", "text": "Section \"Robustness Across Different Datasets (Inter-Dataset)\" demonstrated the effectiveness of the proposed algorithm for segmenting structures from DM is the discriminative model (Tu and Bai 2010) and gmDT is the generative model based deformable template approach. We demonstrate the changes in left hippocampus, right hippocampus, and the average of both sides. Individual volume change is obtained by the baseline volume subtracting the volume of 12-month after. Values listed are the averages of the 490 subjects, and those in parentheses are the standard deviations several datasets with high variability in anatomy and image intensities. In this section, we show the results of our algorithm on data in a longitudinal study where the main source of variation is the temporal changes within the same subject. Since the results from Hua et al. 2009 indicate the significant atrophy of hippocampi in both MCI and AD slices shown were skull-stripped and scaled to similar size. These four sets contain totally more than 1,000 volumes, and all the results are obtained by the identical system without parameter tuning cases, and the scans from ADNI980 provide the longitudinal (12 months) data of 490 subjects from AD, MCI, and normal control groups, we use this dataset as our testbed. ADNI980 volumes also display large variations between subjects due to different scanning settings; nevertheless, volumes from the same subject still share the same acquisition configurations. Although our method may need more specific design for high precision hippocampus segmentation, this experiment still shows our advantage with respect to the baseline methods in the longitudinal study. The tested segmentation methods include DT, gmDT, and our algorithm. The training data contains volumes from 7 AD, 7 MCI, and 7 control subjects in ADNI with manual annotations of hippocampi by an expert (these subjects are not part of the 490). The measures listed in Table 7 include the average volume of baseline, the average volume after 12 months, the volume difference between the two times, and the percentage of volume loss.\nAll the measures obtained by a discriminative approach (Tu and Bai 2010) are listed in Table 7a and those by gmDT are listed in Table 7b . As DM is not adaptive, the coverage of the segmented region by DM is conservative if the test hippocampus has a drifted position, a shape distortion like that typically produced by AD, or a different statistical distribution of appearance. DM also fails to identify the hippocampal regions of 13 subjects, so we needed exclude their volumes from Table 7a . For the rest of the scans, the averages of volume change and percentage of loss show noticeable differences between groups. However, their standard deviations are relatively large, indicating that the measures from the direct discriminative model are not stable for the three groups. In Table 7b , the longitudinal differences between AD and normal are not fully demonstrated because gmDT (with a simple appearance model) could include more non-hippocampal regions than DM and our method. Instead, our approach gives apparent differences in all columns among the three groups; see Table 7c . The smaller standard deviations provide a better separation of the three groups compared to DM. In addition, the balanced performance among the left hippocampus, the right hippocampus and the average is evident.\nWe further compare the results by our method with the estimated hippocampal volume changing rates reported by Schuff et al. (2012) . Their estimated rates are modeled as nonlinear curves based on manual segmentations of ADNI data. As the average ages of our three groups are 76.82(standard deviation(SD) = 6.63 ) for Normal, 76.05(SD = 6.67) for MCI, and 76.82(SD = 6.44) for AD, our data are mostly located in the range [70] [71] [72] [73] [74] [75] [76] [77] [78] [79] [80] , where their estimated curves are still close to linear. Using the average hippocampal volume of 75 years old normal subjects as the standard level, their chart shows the estimated average volume losses as 400 mm 3 (sMCI, the subjects do not convert to AD), 650 (cMCI, the subjects would progress to AD), and 750 (AD). Our average volume of the whole MCI group is 573.40 mm 3 less than the Normal group and consistent with this estimation. However, the number of our AD group is 920.34 mm 3 which is 170 mm 3 larger than their estimation. This is a hint of what direction to work in if we want to specifically adjust our work in order to obtain precise hippocampal segmentations. Table 7 shows that our method achieves higher robustness when identifying inter-and intra-individual differences, and its potential to help indicate different pathological stages."}, {"section_title": "Conclusion and Future Work", "text": "In this paper, we have proposed a system for brain MRI image segmentation by fusing together deformable templates (generative) and informative features (discriminative). It takes the advantages of the generative model for being adaptive and the discriminative classifier for achieving classification power on high dimensional data. This approach uses a new way of combining generative and discriminative models and the complementary properties between them can be efficiently exerted; the information extracted from generative models is considered as an additional channel of features for training the discriminative models. The trained models are improved in two ways:\n(1) The original features can be normalized according to intrinsic structural statistics. Typical methods to accommodate large variation, such as performing histogram matching (Sled et al. 1998) or extracting features invariant to intensity change (Unay et al. 2008) , are only based on the statistics of whole volume data. (2) The feature set is augmented with features derived from the estimation by gmDT.\nA thorough experimental study on T1-weighted datasets demonstrates the robustness of our algorithm. Although discriminative models can perform well if the training and the test data share the same condition of variances, better performance can still be observed by our method as our discriminative models augment the informative feature set with generative features. This advantage leads to improvement over several state-of-the-art algorithms. The inter-dataset and the longitudinal tests show the deficiency of discriminative models in practice and the necessity of introducing the generative information. Our method demonstrates both adaptiveness and precision in these challenging tests and outperforms the two direct models in region overlaps (Dice) and surface fitness. This is different from Wang et al. (2011) that the first stage classifier is considered as a black box approach. These advantages also lead to the improvement over several state-of-the-art algorithms on standard datasets such as IBSR18, LPBA40, and ADNI.\nAn important aspect of the proposed method is its running time, which is approximately one hour. Whether the system is practical depends on the application. In neuroimaging studies, in which research labs often spend months collecting the data, slow running times are not a problem. For instance, our method is much faster than the widely used FreeSurfer, which requires on average 12 hours to segment a single brain scan. In clinical practice, one hour might no be sufficiently fast. The bottleneck of the algorithm is, as for many other brain MRI segmentation methods, the nonlinear registration. However, the registration can be dramatically sped-up through parallelization.\nAn aspect of the framework that was not evaluated was its performance on multispectral data. In this scenario, the different data channels represent images acquired with different MRI contrast (Prastawa et al. 2003; Menze et al. 2010; Yang et al. 2010; Geremia et al. 2011) or even different modalities (Fitzpatrick et al. 1999; Chen and Varshney 2003) . Segmentations on multispectral data have the potential to be more accurate thanks to the larger amount of information present in the different channels. In our framework, generalization to multispectral scenarios is immediate: the intensities of the additional channels are just extra dimensions of the feature vectors. Another possible line of research would be to analyze the performance of the framework using other generative or discriminative models. For instance, it would be interesting to assess whether introducing explicit shape, regional, or context information in the generative prior has a positive impact of the segmentation. Exploring all these directions remains as future work."}, {"section_title": "Information Sharing Statement", "text": "An implementation of the method is publicly available for download at the LONI 3 and NITRC 4 websites. We provide a Windows\u00ae, a Linux\u00ae, and a LONI pipeline version. The software can be used freely in research provided this paper is cited in any material using the results of their application. For other usage, contact the authors."}]