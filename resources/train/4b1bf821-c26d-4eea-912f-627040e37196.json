[{"section_title": "Abstract", "text": "We present a new semi-supervised algorithm for dimensionality reduction which exploits information of unlabeled data in order to improve the accuracy of image-based disease classification based on medical images. We perform dimensionality reduction by adopting the formalism of constrained matrix decomposition of [1] to semi-supervised learning. In addition, we add a new regularization term to the objective function to better captur the affinity between labeled and unlabeled data. We apply our method to a data set consisting of medical scans of subjects classified as Normal Control (CN) and Alzheimer (AD). The unlabeled data are scans of subjects diagnosed with Mild Cognitive Impairment (MCI), which are at high risk to develop AD in the future. We measure the accuracy of our algorithm in classifying scans as AD and NC. In addition, we use the classifier to predict which subjects with MCI will converge to AD and compare those results to the diagnosis given at later follow ups. The experiments highlight that unlabeled data greatly improves the accuracy of our classifier."}, {"section_title": "INTRODUCTION", "text": "The medical imaging community frequently relies on voxelwise image analysis to define areas of difference between groups [2] or to extract features for classification. However, this approach is not well suited for identify complex population differences, because it does not take into account the multivariate relationships in data [1, 3] . Moreover, regions showing significant group difference are not necessarily discriminative for classifying individuals. In order to overcome these limitations, high-dimensional pattern classification methods have been proposed in the recent literatures [4, 5] . A fundamental limitation in these methods with respect to medical imaging is their need for large training sets of labeled data. One way to address this issue is to train the methods using \u2020 Data used in the preparation of this article were obtained from the Alzheimer Disease Neuroimaging Initiative (ADNI) database (www.loni.ucla.edu/ADNI) unlabeled data, which may exist in large quantities. However, it is not clear how to exploit unlabeled data for dimensionality reduction. We will explore these topics in the subsequent sections.\nPresence of unlabeled data in the learning process, also referred to as semi-supervised learning, can have interesting applications in medical imaging. For example, subjects may deviate from the normal population and may be diagnosed with a certain disease in future follow-up scans; class labels of such subjects are not very well-defined. This is the case for subjects diagnosed as Mild Cognitive Impairment (MCI) who show some impairment in their cognitive scores and have high risk to develop Alzheimer's disease (AD) in near future. One may be interested to predict future follow-up labels (converging to AD or not) of the MCI subjects by considering them as unlabeled data. Considering MCI subjects as unlabeled data allows an algorithm to locate unlabeled subjects in the spectrum of normal vs. abnormal.\nOur proposed method is combination of two techniques: the first technique proposed in [1] reduces the dimensionality in a discriminative way while preserving the semantics of images; hence it is clinically interpretable and produce good classification accuracy. The second technique incorporates information provided by unlabeled data. It computes pair-wise similarity of subjects while considering intrinsic geometry of data distribution [6] . Pair-wise similarity between subjects encodes relationship between labeled and unlabeled data and it is shown to improve classification accuracy in presence of unlabeled data [7] . Finally, the method is cast as a constrained optimization problem similar to [1] but the optimization cost function and its constraints as well as our optimizer are significantly different. Section 2 briefly describes our general framework that we expand upon. Section 3 focuses on the added regularization term that makes use of the information latent in the unlabeled data. Section 4 presents a solution to the resulting optimization problem. The results of applying our method to a subset of ADNI data set are discussed in Section 5. Finally in Section 6, we conclude with a discussion of the method."}, {"section_title": "GENERAL FRAMEWORK", "text": "Dimensionality reduction is typically applied to achieve a generalizable classification rate when number of samples is less than dimensionality of features. We propose to use regularized matrix factorization formalism for dimensionality reduction. This framework allows to keep the semantics of images; hence it produces interpretable results. The idea is similar to [1] but formulation and its extensions (Section 3) and other details are significantly different.\nIn this section, we lay out the general framework. Regularized matrix factorization decomposes a matrix into two or more matrices such that the decomposition describes the matrix as accurate as possible. Such decomposition could be subjected to some constraints or priors. Let assume columns of The columns b k \u2208 B and c i \u2208 C are subjected to some constraints, which we denote with the feasibility sets B and C. We use variable y i \u2208 {\u22121, 0, 1} to denote labels of the subjects. Healthy subjects are denoted by 1 and abnormal ones by \u22121; 0 is used simply for unlabeled subject indicating that labels are not decided for them. In order to define the feasible sets (B), we need to elaborate the requirements that our algorithm should satisfy: 1) The basis vectors must be anatomically meaningful: this means that a constructed basis vector should correspond to contiguous anatomical regions preferably in areas which are biologically related to a pathology of interest. In other words, the basis vectors should not resemble spread disjoint voxels. Sparsity of the basis vectors, i.e. a relatively small number of voxels with non-zeros values, encourages it to be more spatially localized.\n2) The basis must be discriminative: we are interested in finding features, i.e. projections onto the basis vectors, that construct spatial patterns that best differentiate groups.\n3) The decomposition (BC) should be a good representative of data without compromising the previous properties.\nIn this paper, we assume that images are non-negative hence it is reasonable to impose non-negativity on B and C. Thus, our proposed method can be viewed as a variant of nonnegative matrix factorization (NMF). NMF [8] , [9] is an additive model that is known to decompose images into meaningful parts that are shared across subjects; this property is favorable for our application. Such part-based decomposition property encourages basis vectors to be similar to anatomically meaningful parts of images (i.e. Hippocampus, Caudate, etc. for brain images). We also assume that certain structures (e.g. Hippocampus) of an anatomy of interest (e.g. brain) are affected by the abnormality (e.g. shrinkage of Hippocampus Alzheimer's disease); this property can be viewed as a sparsity constraint on the basis vectors which also help the basis vectors to be more interpretable. We encode these properties via non-negativity on the coefficients and combination of non-negativity and 1 and \u221e norms on the basis vectors. The 1 -norm encourages the sparsity property and combination of \u221e and non-negativity promotes part-based decomposition:\nwhere ratio of \u03bb 3 /D encodes ratio of sparsity of the basis vectors.\nIn order to find optimal B and C, we define the following constrained optimization problem:\nT xi, w ) + w 2 subject to:\nThe cost function of the optimization problem consists of two terms: 1) Generative term (D(\u00b7; \u00b7)) that encourages the decomposition, BC, to be close to the data matrix (X); both labeled and unlabeled data contribute to this term. 2) Discriminative term ( (y i ; f (x i , B, w))) is a loss function that encourages a classifier f (\u00b7) to produce class labels that are consistent with available labels (y). The classifier parametrized by w, projects each image (x i ) on the basis vectors to produce new features (v i = B T x i ) and produces a labels. In this paper, we use a linear classifier, hence f (x i , B, w) = B T x i , w . Only labeled data contribute to the discriminative term. Various choice are possible for D(\u00b7; \u00b7) and (\u00b7; \u00b7). In this paper, we set D(X; BC) = \u03bb 1 X \u2212 BC 2 F where \u03bb 1 is a constant. For the loss function, we choose a hinge squared loss function: (y,\u1ef9) = (max{0, 1 \u2212 y\u1ef9}) 2 which is a common choice in Support Vector Machine literature. Summing over L for the loss function simply indicates that the labeled subjects participate in this term."}, {"section_title": "SEMI-SUPERVISED REGULARIZER", "text": "In this section, we extend the method for semi-supervised learning by extending the simple 2 regularization term of w in (Eqn. (2)) to a more general Regularization term (R(\u00b7; \u00b7)) involving both basis matrix (B) and w vector: We note that our model only regularizes B and w but not C. Our proposed regularization term is inspired by Laplacian-SVM (lapSVM) [7] , and particularly the linear version of [10] . In lapSVM, samples (labeled and unlabeled) are considered as nodes of a graph. Every two nodes are connected via an edge and there is a weight associated to the edge determining how similar the two nodes (samples) are. Properties of this graph is used to define a regularization for lapSVM [7] . Similar to lapSVM, our proposed regularization (Eqn.(3)) consists of two terms: the first term, which is a simple 2 -norm, controls the complexity of the classifier in the ambient space (here R r ) and the second term is an appropriate penalty term reflecting the intrinsic structure of distribution of sample. The second term encourages the classifier to produce class label for an image that is similar to the labels of its most similar neighbors on the graph:\nwhere Q is a r \u00d7 r matrix and z 2 Q = z T Qz; \u03b3 A and \u03b3 I balance relative weight between two terms and L is a N \u00d7 N precomputed graph Laplacian, given labeled and unlabeled data. We will shortly explain how to build a graph of labeled and unlabeled images and how to compute the graph Laplacian.\nInspired by [6] , we compute the Laplacian matrix on the complete, weighted, and undirected graph with each image in the dataset being represented by one node. First, we define the distance between two images I i : \u03a9 I \u2192 R and I j : \u03a9 J \u2192 R as the weighted sum of residual and harmonic energy after registration:\nwhere h : \u03a9 I \u2192 R 3 is the displacement field such that if x \u2208 \u03a9 I then x + h(x) \u2208 \u03a9 J and \u03ba 1 is the registration parameter [6] . The images are registered by the Diffemorphic Demons [11] with coarse resolution due to its fast speed. Then, the non-negative weights K ij between two images are assigned by exponential of a negative distance: K ij = e \u2212d(Ii,Ij )/\u03c3 . A weights matrix K is obtained by collecting the edge weights K ij for every image pair and a diagonal matrix T contains the row sums for each vertex\nThe Laplacian L encodes information relating to all pairwise relations between the images. In words, the second term in Eqn. 3 promotes smoothness of class labels on the graph. By multiplying the vector w on the left and right of Q(B), (Eqn.(3)) can be written as w"}, {"section_title": "OPTIMIZATION", "text": "Having all terms the cost function and the constraints, we can form the optimization problem as follows:\nsubject to: The optimization problem is not jointly convex with respect to all blocks of variables (B,C,w). However, fixing each pairs of blocks, the optimization is convex with respect the other block. Our proposed algorithm uses this property and applies a block-wise optimization scheme, i.e. fixing each pairs of blocks (e.g. fixing B,C) and optimizing with respect to the other (e.g. w) and iterate till some convergence criterion is satisfied. This scheme guarantees to decrease the cost function in each iteration and, specifically for this problem, is guarantees to converge to a local minimum. Block-wise optimization with respect to C and w are not challenging but it is challenging with respect to B because it is very high dimensional. Detail discussion of our proposed efficient method to circumvent this issue is beyond the scope of this paper; briefly, we use Spectral Projected Gradient (SPG) [13] with an efficient projection method on the feasible set (B)."}, {"section_title": "EXPERIMENTS", "text": "To evaluate the proposed method, we applied our method on a subset of Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. The dataset we used consists of structural MRI images of 54 Alzheimer's (AD), 238 Mild Cognitive Impairment (MCI), and 63 Normal Control (CN) subjects at the baseline time point. Among 238 MCI subjects, 68 patients converted to AD in a few follow-up months (cMCI) and 170 subjects have not converted yet or may not ever convert (ncMCI). The preprocessing pipeline (bias correction, skull stripping, tissue characterization, and spatial normalization) is designed according to what is suggested in [1] ; and it computes so-called RAVENS maps [14] that quantifies the density of the gray matter tissue. The observation matrix X is obtained by collecting the RAVENS map for each subject.\nThen, we conducted the semi-supervised basis learning method on X. For computational efficiency, the basis vectors B were learned only from 79 MCI subjects (unlabeled data), and 20 AD / 20 CN subjects (labeled data). The labeled subjects were divided into five folds for cross validation (4/5 for training and 1/5 for testing) and the 79 MCI subjects were shared as unlabeled data across folds. In order to investigate the affect of number of labeled data, we performed four basis learning by increasing number of revealed labels from 4 to 32; each fold has 4/5 \u00d7 (20 + 20) = 32 AD/CN subjects and we revealed labels of AD/CN subjects as: {(2, 2), (4, 4), (8, 8) , (16, 16)}. Figure 1 shows two examples of learned basis vectors in different sagittal cuts. Hippocampus, Cingulate and temporal lobe regions, which are associated with memory loss [15] , are precisely highlighted in the basis.\nAfter basis learning, features are extracted by projecting all images on the learned basis vectors. These features were fed into a supervised classifier (SC: Simple Logistic) and a semi-supervised classifier (SSC: linear Laplacian SVM [10] ) to produces labels. To have a reference point for comparison, we also learned the basis without unlabeled data (supervised basis learning). Figure 2a plots accuracy rates of AD/CN with respect to number labeled data in different settings. Accuracy rates were computed on the left out labeled data and the rest of the labeled data that was not introduced during basis learning or classifier learning; in other words, the 2/3 of the data not contributed in training was used for validation. For brevity, SF in Figure 2a indicates Supervised Features, i.e. using only labeled data to learn the basis vectors, and SSF(lap) denotes Semi-Supervised Features, i.e. using labeled and unlabeled data to learn the basis vectors while the Laplacian regularization in (Eqn. (3)) is used. Figure2 shows different scenarios for classification: supervised features fed into a supervised classifier ( SF + SC) and a semi-supervised classifier (SF + SSC) and compares it with semi-supervised features fed into a supervised classifier (SSF(lap) + SC) and a semisupervised classifier (SSF(lap) + SSC). The figure shows that semi-supervised basis learning in presence of unlabeled data helps improving accuracy of detecting AD/CN: compare the blue line with the green line and the pink line with the red line. It also shows that if the semi-supervised classifier (SSC) is used instead of a fully supervised classifier (SC), we achieve the maximum performance. It also shows that presence of unlabeled data helps improving classification rate with fewer labeled data as it saturates faster than the supervised cases. Figure 2b shows similar results for MCI subjects. However, real labeling for ncMCI subjects are vague because they may or may not convert to AD in the future unlike cMCI who have already converted in a few follow up scans. In order to evaluate performance of the algorithm on this dataset, we reported the Area Under Curve (AUC) instead of accuracy. This figure also shows that semi-supervised basis learning improves the AUC and we can see even higher boost by using semi-supervised classifier in comparison to a fully supervised classifier.\nFigure2 also includes experiments in which the Laplacian regularization is set to zero (i.e. \u03b3 I = 0). Notice that the algorithm is still a semi-supervised learning because both labeled and unlabeled contribute in the generative term (D(\u00b7; \u00b7)) but only the labeled data contributes the discriminative term ( (\u00b7; \u00b7)). This experiment is denoted by SSF comparing to SSF(lap) where the Laplacian term is in use. The purpose of this experiment is to investigate which term (the generative term or the Laplacian term) has more impact in the semisupervised learning. In Figure2a, there is no significant difference between SSF and SSF(lap) meaning that most of gain due to presence of unlabeled data is obtained by the generative term rather than Laplacian regularization (Eqn. (3)). However Figure2b shows that adding the Laplacian regularization improves AUC slightly.\nIt is worth mentioning that the proposed algorithm has couple of parameters to tune (\u03bb's and \u03b3's). Due to space limitation, sensitivity analysis with respect to these parameters is not presented in this paper but fully investigated in a longer version of this paper which is currently under review for journal publication. We refer interested readers to a draft version of the journal paper that soon will be accessible through the first author homepage."}, {"section_title": "CONCLUSION", "text": "We presented a framework to reduce the dimension of image features in presence of unlabeled data. Constrained matrix decomposition problem was adapted for generative and discriminative basis learning and extended to semi-supervised formulation. Semi-supervised dimensionality reduction method outperforms supervised dimensionality reduction for both classification tasks considered in this paper, both in terms of classifier accuracy and area under curves (AUC). It seems that the generative term has more impact in semi-supervised learning rather than the Laplacian regularization in (Eqn.(3)); nevertheless further investigation is required to find the right balance between these two terms to exploit unlabeled data further. Future work will involve evaluating semi-supervised dimensionality reduction method for other medical image datasets and other scenarios. "}, {"section_title": "ACKNOWLEDGMENT", "text": "The research was supported by an ARRA supplement to NIH NCRR (P41 RR13218)."}]