[{"section_title": "Introduction", "text": "Seasonal, or medium-range, weather forecasts on a timescale of one month to a year ahead are highly important in a range of applications. Decisions makers can e.g. greatly benefit from skillful forecasts of increased danger for natural disasters or extreme weather events, such as droughts, hurricanes or extreme snowfall and winds, for efficient mitigation efforts and emergency management. Unlike short-range weather forecasting, medium-range forecasts rely on the prediction of atmospheric modes with a low-frequency variability which can be predicted months ahead. This includes the El Ni\u00f1o Southern Oscillation, monsoon rains and the Northern Atlantic Oscillation (Hoskins, 2013). As ocean states change considerably slower than states in the atmosphere, these modes are typically associated with the sea surface temperature in certain regions. Therefore, reliable months-ahead forecasting of sea surface temperature is a crucial first step towards skillful seasonal forecasts of other weather phenomena. For example, the winter mean surface temperature in large parts of Europe is considered to be negatively correlated with the sea surface temperature in the Nordic seas in the preceding autumn (Kolstad and\u00c5rthun, 2018;Dobrynin et al., 2018). In order to be useful for decision-making, weather forecasts ought to be probabilistic in nature and well calibrated . Calibration implies that the probability of any event under the forecast distribution matches the actual frequency observed for the event. Current numerical weather prediction (NWP) models are typically deterministic and account for forecast uncertainty by generating an ensemble of forecasts where every ensemble member represents a possible simulation of the future, often generated by creating a small perturbation of the initial state of the prediction. However, as these models rely on simplifications of the underlying physical system, a (possibly too crude) discretization of space and imperfect initialization, they will be biased. Further, the ensemble spread may not accurately capture the forecast uncertainty. Hence statistical postprocessing is required, where the forecast model is recalibrated based on past performance and observations, see Vannitsem et al. (2018) for an overview. In the postprocessing of medium-range forecasts, obtaining enough training data is a particular challenge. The high-frequency variability patterns need to be filtered out, such that observations have to be averaged over several weeks or months. As the period of reliable sea surface temperature observations starts around 1980, the beginning of the satellite era, the number of available observations for each season is typically below 50. Therefore, medium-range postprocessing techniques must be robust to minimize the risk of overfitting. Additionally, ongoing climate change leads to significant trends in biases and model uncertainty over time (Van Schaeybroeck and Vannitsem, 2018). Many questions stated by forecast users share the feature that they depend on the forecast distribution at multiple locations, so that the forecast must take into account complex dependencies for a skillful prediction of the answers. Examples include predicting the probability of the maximal sea surface temperature in a specific area exceeding 26.5 \u2022 C, a necessary condition for the development of tropical cyclones (McTaggart-Cowan et al., 2015), or predicting the probability of observing sea ice along a shipping route. This requires multivariate postprocessing techniques. A common approach to multivariate postprocessing is to fit variograms of a parametric covariance family, such as exponential covariance functions (e.g. Feldmann et al., 2015). This approach generally assumes stationarity and often nonnegative correlations decaying with distance. Neither of these assumptions are natural when considering global sea surface temperature, as it is likely to depend on ocean currents and the presence or absence of land near, or in between, locations. This is highlighted by Figure 1 showing normalized observed sea surface temperature for May 2016. Nonstationary effects are visible, such as the strong horizontal correlations in the Pacific ocean westwards of Peru and Columbia due to the El Nino Southern Oscillation. These effects commonly carry forward to the forecast errors in that the model captures the pattern but not the exact magnitude. Given the physical complexity underlying the NWP model, forecast residuals of different locations may be negatively correlated. For example, the sea surface temperature forecast in our data set for July tends to underestimate the temperature in the Baltic sea, when overestimating the temperature in the Barents sea, and vice versa. Most parametric families are not able to model negative correlations, an exception being parametric hole effect models (e.g. Chil\u00e8s and Delfiner, 1999). However, these assume that locations at certain distances are always negatively correlated, which is not reasonable in our setup. We propose a probabilistic multivariate postprocessing approach to tackle these issues and apply it to forecasts of monthly mean sea surface temperature issued by the Norwegian climate prediction model (NorCPM). A moving average approach ensures that the postprocessing will be robust against lack of training data and trends in biases and uncertainties caused by climate change. To achieve spatially consistent forecasts, we explicitly model the spatial dependence structure of the forecast residuals. We utilize regularization of the covariance structure by tapering the covariance matrix, and use further dimension reduction based on principal component analysis to reduce the computational time and reduce the risk of overfitting. Validation on out-of-sample observational data demonstrates that this multivariate postprocessing approach yields spatially consistent and well calibrated forecasts. The paper is organized as follows: in Section 2, we first develop a univariate postprocessing technique based on moving averages, and extend this to the multivariate setting, incorporating covariance tapering, principal component analysis and a marginal variance correction. Section 3 outlines several validation and comparison tools for multivariate forecast distributions. In Section 4, we show how the proposed univariate and multivariate methods perform for the NorCPM forecasts and compare their performance to several reference methods. In Section 5, we consider a case study of a shipping route in the Northern Atlantic, forecasting the probability of ice along the route. The Section 6 gives the concluding remarks and discussion of results. The code for all our methods is available as R package at www.github.com/ClaudioHeinrich/pp.sst. 2 Modeling sea surface temperature"}, {"section_title": "Data", "text": "We consider monthly mean forecasts of sea surface temperature (SST) issued by the Norwegian climate prediction model (NorCPM), see Counillon et al. (2014Counillon et al. ( , 2016. The forecasts cover the entire globe on a longitude-latitude grid with resolution 1 \u2022 , for a total of 64 800 grid points, where approximately 43 000 are located in the oceans. NorCPM issues new forecasts every 3 months, at the beginning of January, April, July and October, such that the lead times of the forecasts vary between one and three months. Each forecast consists of nine exchangeable members. For postprocessing, we consider forecasts from 1985 to 2016. The validation period is set to 2001-2016, while the years 1985-2000 are used to train the model. Throughout the validation period, the model estimation is updated for each time point to include the most recent observations. Observations of monthly mean SST over the period are obtained from the Optimum Interpolation Sea Surface Temperature (OISST) dataset of the National Oceanic and Atmospheric Administration (Reynolds et al., 2007)."}, {"section_title": "Univariate postprocessing", "text": "A wide range of methods are available for univariate forecast postprocessing, e.g. ensemble Bayesian model averaging, nonhomogeneous regressions and quantile regression; see Wilks (2018) for a recent overview. In our data set, both bias and prediction uncertainty depend strongly on spatial location and calendar month. Here, we postprocess data from each calendar month separately, ignoring possible interactions between months. In the following, a fixed month is considered with the monthly index suppressed for simplicity. For a given year y \u2208 {1985, . . . , 2016} and location s \u2208 S, the SST forecast is assumed normally distributed, The mean and variance are estimated following a (weighted) moving average approach. Specifically, the mean is taken to be the bias-corrected NorCPM ensemble mean, \u00b5 y,s = f y,s \u2212 b y,s , where f denotes the mean of the raw ensemble. To account for trends in the bias over time, for instance caused by climatic changes not accounted for by NorCPM and improved reliability of observations, the bias and predictive variance are estimated by weighted moving averages: where t j,s denotes the observed temperature at year j and location s, and the sequences of weights w 1 , w 2 . . . are normalized. In Section 4, we compare the performance of a simple and an exponentially decaying weighting scheme. The weights for the bias are chosen by minimizing the mean squared error (MSE) of the bias-corrected forecast f \u2212 b. For the variance, they are chosen by minimizing the continuous rank probability score (CRPS). The CRPS for predictive distribution F and observation t is defined as where X, X \u223c F denote two independent random variables. It constitutes a proper scoring rule, cf. , and is often used to assess predictive performance. A main advantage of this approach is that the weighted averages automatically account for month-and location-specific bias and uncertainty. Therefore, parameters of a weighting scheme may be estimated using all locations and all past months which prevents overfitting and makes optimal use of the available training data. This comes at the cost that our univariate postprocessing technique is not inherently coherent, see Krzysztofowicz (1999); Zhao et al. (2017), in the sense that it automatically produces forecasts that are no worse than climatology. Coherent models estimate the correlation between forecast and observation and produce skillful forecasts even when this correlation is negative. In our context, if this is done for each location separately it introduces a large risk of overfitting due to the short duration of the training period, whereas estimating the same correlation at all locations makes the estimate too crude. The model (2) makes the assumption that the NWP forecast indeed contains some signal, which is sometimes not satisfied in seasonal forecasting, see Schepen et al. (2014). On the other hand, the model (2) only requires to fit the parameters of the weighting scheme. These can be estimated using all locations and months, since for any weighting scheme the bias and variance estimates are month-and location-specific. As a consequence, the model 2is very robust, which in our context is more valuable than coherence. Indeed, in Section 4.2 we demonstrate that it outperforms several coherent reference methods. Both estimators rely on the ensemble mean only and are independent both of the number of ensemble members and the ensemble spread which is commonly used as predictor of uncertainty in short range weather forecasting (e.g. Messner et al., 2017). In seasonal to decadal forecasts the ensemble spread is known to be a less reliable predictor of forecast uncertainty (Ho et al., 2013). This is supported by the findings in Section 3. Increasing the number of ensemble members usually also benefits the skill of the ensemble mean (Buizza and Palmer, 1998) and would decrease the variance of the estimators b and \u03c3. Therefore, increasing the ensemble size remains desirable in our context, even though it does not directly enter our equations. In the OISST dataset, the SST is truncated at \u22121.79 \u2022 C, the assumed freezing temperature of sea water. As this is relevant for relatively few grid points, we apply the same truncation to the predictive distributions after the parameter estimation rather than assuming a truncated normal model in (1). In numerical experiments, the truncation error was found to be substantially smaller than the forecast uncertainty. NorCPM, as most climate prediction models, will inherently account for global warming. However, it relies on simplification of the underlying physical processes and is unlikely to fully describe the effects of climate change. Moreover, numerical prediction models, once initialized, tend to drift towards a model attractor which on the seasonal to decadal scale introduces changes in model biases over time. While this may be accounted for with a linear trend term in the bias model (Boer, 2009), this was found to reduce the predictive ability here due to overfitting. The proposed approach is compared against the related and well known non-homogeneous Gaussian regression (NGR) approach (Gneiting et al., 2005). Here, the mean and the variance are modeled as linear functions of predictor variables, most commonly the ensemble mean and the ensemble spread, i.e. where S which is why we estimate the weights w b for the bias correction by MSE-minimization first, and thereafter estimate the variance for the assumed bias correction. In Section 3 we assess the skill of the mean model by (out-of-sample) MSE. For this reason we deviate from Gneiting et al. (2005) and estimate the regression parameters a, b by ordinary least squares. The main disadvantage of N GR m,s in our context is the vast number of parameters which makes it prone to overfitting. An alternative method is usually referred to as locally adaptive NGR, see for example Scheuerer and K\u00f6nig (2014) . This method avoids fitting a and b for each location while still allowing for location-specific bias of the model which is achieved by considering model and temperature anomalies instead of absolute temperatures. More precisely, defining the locally adaptive NGR fits parameters a, b by regressing t ano on f ano , and issues the mean of the predictive distribution as Unlike the classical NGR this method accounts for the the mean of the predicted and true temperature varying largely between locations, and is used to estimate one pair of regression coefficients a, b for all locations. We denote this reference method by N GR la m , since the parameters a and b are estimated for each month separately and the method is locally adaptive."}, {"section_title": "Multivariate postprocessing", "text": "In order to obtain physically consistent postprocessed forecast fields, the model (1) must be extended to include spatial correlation. The main challenge is that the set S of considered locations contains around 42 000 points for the entire globe, and is very large compared to the sample size of up to 31 training years. To allow for non-stationary effects and negative correlations, we propose a postprocessing procedure based on regularization of the sample covariance matrix. It relies on a combination of tapering the sample covariance matrix and principal component analysis (PCA). These are classical tools for high-dimensional covariance estimation, but have found little attention in the context of statistical postprocessing of spatial data. As reference methods we compare the proposed technique to a geostationary approach (Feldmann et al., 2015) and ensemble copula coupling (Schefzik et al., 2013)."}, {"section_title": "Postprocessing by regularization of the sample covariance matrix", "text": "The univariate model (1) is extended by estimating the covariance matrix of the forecast residuals. The residuals are assumed to follow a multivariate normal distribution where \u00b5 y denotes the vector of bias-corrected forecasts f y \u2212 b y , and t y the vector of observed temperatures. The covariance matrix \u03a3 y is multi-layered as it captures both the spatial climatological correlation between different locations on the globe and the forecast uncertainty including spatial interactions. Given an estimator of the covariance matrix, \u03a3 y , a spatial forecast is issued as generalizing the marginal model in Equation (1). In the following, the year and month are assumed fixed and the indices y and m are suppressed. The standard estimator of the covariance matrix is the sample covariance matrix (SCM): where the sum runs over all Y previously observed years. However, in the high-dimensional setting with limited training data, the sample covariance estimator requires regularization. We propose a two-step procedure for regularizing S, first applying a distance-dependent tapering, or weighting, of the covariance matrix and secondly utilizing principal component analysis (PCA) to regularize the eigenstructure and reduce dimensionality. For the first step, the tapering, we consider a positive, monotonically decreasing function The resulting tapered matrix is always positive semi-definite. Tapering covariance matrices by distances is frequently used in atmospheric sciences (Gaspari and Cohn, 1999). Gneiting (2002) argued that the weight function \u03c6 should be twice differentiable with \u03c6 (0) = 0 and a minimal second derivative |\u03c6 (0)|, and suggests the function Here, \u03c6 is supported on [0, 1], such that the tapering function \u03c6 L has a tuning parameter L determining its support. In numerical experiments, the performance of our postprocessing method performed best for L between 1000km and 4000km. For the remaining of the paper, L is set to 2500km. The tapering is beneficial in two ways: Firstly, the SCM does not consider distance between locations, and it will thus have a high risk of spurious correlations given the large number of locations pairs (\u223c 10 9 ). The spatial correlation is likely to decrease with distance, and the tapering down-weights high correlations between distant locations as these are less credible than those between close locations. Secondly, it removes the rank deficiency of the SCM S and changes it into a full rank matrix. Indeed, the rank of S is limited by the number of observed years, Y = 31, significantly lower than S \u2248 42 000. The tapered covariance matrix having full rank makes it benefit more from regularization by principal component analysis (PCA). To further reduce the risk of over-fitting and increase the speed of simulation, PCA is applied to the tapered covariance matrix estimate. PCA can be used to restrict the covariance estimator to a low-dimensional linear subspace with minimal information loss. In detail, we consider the eigenvalue decomposition of S \u03c6 with orthogonal eigenvectors U = [u 1 , ..., u S ] and eigenvalues \u039b = diag(\u03bb 1 , ..., \u03bb S ) in decreasing order. The ordered eigenvectors, usually referred to as principal components, are orthogonal linear combinations of the locations expressing the highest variance. The underlying assumption is that only the first d S principal components truly represent a signal, whereas the variability of the remaining components represents unstructured noise. Therefore, only the first d eigenvectors are considered for the covariance estimate: The truncation of the eigenvalue decomposition will decrease the marginal sample variance at location s, the diagonal element S ss , for a given month: Assuming the marginal postprocessing yields calibrated marginal distributions, we want the marginal variances of the multivariate method to equal those estimated by the univariate method. We will therefore compare two alternative approaches for correcting the variance deflation, a multiplicative and an additive correction. In the multiplicative correction, the PCA step is performed on the (tapered) correlation matrix and transformed back to the covariance matrix where the marginal variances \u03c3 2 y,s are estimated as in (2). Alternatively, we perform the PCA on the (tapered) covariance matrix and apply an additive correction to the marginal variances, where To ensure the positive definiteness of \u03a3 ac , the difference between the regularized and unregularized marginal variances has to be truncated at zero. The additive correction does not change the off-diagonal elements of \u03a3. It, however, only satisfies \u03a3 ac ss = \u03c3 2 y,s for locations s where \u03c3 2 y,s \u2265 \u03a3 ss . As the marginal estimator \u03c3 2 y,s is not equal to the standard sample variance (S) ss , this is not guaranteed for all locations. The main purpose of applying PCA in this way is to allow for more efficient sampling from the predictive distribution. For instance, when simulating an S-dimensional normally distributed X with zero mean and covariance matrix \u03a3 mc , it is sufficient to simulate a d-dimensional vector Y and set where U (d) and \u039b (d) contain the first d eigenvectors and eigenvalues. We found that a dimension reduction with order of magnitude S/d \u2248 100 leads to good results. As a consequence, simulating X with (previously computed) covariance matrix \u03a3 mc is approximately 100 times faster than simulating from a full rank normal distribution with known Choleskydecomposition of the covariance matrix, disregarding fixed computation costs. When the additive correction is applied, X is simulated by where Y is d-dimensional standard normal and Z is S-dimensional standard normal. This is slower than simulating from the multiplicative corrected covariance estimate, but nevertheless significantly faster than simulating from a general S-dimensional normal distribution."}, {"section_title": "Reference methods", "text": "We consider two reference methods commonly used in statistical postprocessing of spatial forecasts, see Schefzik and M\u00f6ller (2018). A geostationary approach fits a parametric correlation model, assuming spatial stationarity and isotropy. The parametric correlation function is usually assumed to be in the Whittle-Mat\u00e9rn or the exponential family. Feldmann et al. 2015suggest an exponential model with nugget, where the correlation of the forecast error at locations s i and s j can be written as The parameters \u03b8 and r are estimated by fitting the variogram of the parametric model to the empirical variogram, for details see Feldmann et al. (2015). Secondly, we compare to ensemble copula coupling (ECC), see Schefzik et al. (2013). The method constructs a postprocessed ensemble of the same size N as the original NWP ensemble. The new ensemble is univariately calibrated and follows the same rank-order structure as the raw NWP ensemble. This is achieved in two steps: First, a univariately calibrated ensemble N ) is generated by considering m equally spaced quantiles of the calibrated distribution (1), i.e. where F s denotes the cumulative distribution function of the univariate model (1) at location s. Thereafter, the ensemble indices are permuted at each location to obtain an ensemble with the same rank order structure, or empirical copula, as the raw ensemble forecast. To achieve this, denote by f s the value of the ith ensemble member at location s of the raw forecast and find for each location a permutation \u03c1 s such that is satisfied. Then, the ith member of the multivariate ECC forecast ensemble is ECC is computationally efficient and it does not require the specification of a full multivariate distribution. A limitation is that the newly generated ensemble has the same number of members as the NWP ensemble. While extensions to larger ensembles using the rank order structure of historical observations have been proposed (e.g. Schefzik, 2016), those do not apply in our setting of limited available historical observations. As a third reference method we compare to the Schaake shuffle (Clark et al., 2004). The Schaake shuffle constructs a postprocessed ensemble that has the same empirical copula as past observations. It works very similar to ECC, except that a post-processed ensemble with the same rank order structure as past observations is constructed rather than with the rank order structure of the raw ensemble forecast. The size of the ensemble created by the Schaake shuffle is therefore not limited by the size of the NWP ensemble but by the number of years in the training data set."}, {"section_title": "Validation methods", "text": "We validate predictive performance by assessing the sharpness of the predictive distributions subject to calibration . Calibration, or reliability, refers to the statistical consistency between the forecast and the observations in the validation period, while sharpness refers to the spread of the predictive distribution. Subject to being calibrated, a sharper forecast is less uncertain and thus more informative. Following Dawid (1984), probabilistic calibration of marginal forecasts is assessed by the probability integral transform (PIT), i.e. the predictive cumulative distribution function F evaluated at the observation t. If F is probabilistically calibrated, the PIT will be uniformly distributed, F (t) \u223c U ([0, 1]). To summarize the marginal calibration across grid point locations, we investigate the first two moments of the marginal PIT distribution over all time points in the validation period. A uniform distribution U ([0, 1]) has an expectation of 0.5 and a standard deviation of 1/(2 \u221a 3) 0.29. It follows that E( F (t)) < 0.5 indicates a positive bias and E( F (t)) > 0.5 indicates a negative bias. If SD( F (t)) < 0.29, the forecast is overdispersive, and reversely, for SD( F (t)) > 0.29, it is underdispersive. We assess multivariate calibration as proposed by Thorarinsdottir et al. (2016). Here, pre-rank functions are employed to map an ensemble of realizations from the multivariate forecast and the observation to real numbers which are subsequently ranked in a standard manner. If the forecast and the observations are statistically indistinguishable, the resulting histogram over the observation ranks is flat, whereas deviations from uniformity indicate miscalibration. Thorarinsdottir et al. (2016) propose two pre-rank functions which assess the multivariate calibration in slightly different manners. The average pre-rank function finds the average of the marginal univariate ranks while band depth ranking assesses the centrality of the observation within the forecast ensemble as proposed by L\u00f3pez-Pintado and Romo (2009). Forecast accuracy is typically assessed using proper scoring rules (Winkler and Murphy, 1968;. Scoring rules assign a numerical score to each forecastobservation pair, where a lower value indicates better predictive performance. To assess the marginal accuracy, we use the mean squared error (MSE), where \u00b5 denotes the mean of F . The continuous ranked probability score was defined in (3). For an ensemble x = {x 1 , . . . , x N }, the CRPS equals The MSE is fast to compute and compares different mean models for the predictive distribution. The CRPS provides a more complete picture in that it assesses both calibration and sharpness . For a multivariate assessment we utilize the multivariate variogram score (VS) proposed by Scheuerer and Hamill (2015). For a multivariate distribution function F and an observation vector t at S locations, the VS of order p is given by where t i is the observation at the ith location and X i the ith component of a random vector distributed according to F . The (nonnegative) weights \u03c9 ij are set to be constant, such that the correlation structure of all distances is assessed, and we select the order p = 0.5, as recommended by Scheuerer and Hamill (2015). To test significance of score differences, we apply a permutation test relying on resampling (M\u00f6ller et al., 2013;Good, 2013). Two predictive distributions F 1 and F 2 are compared under a scoring rule S(F, \u2022) using the statistic The permutation test is then based on resampling copies of s with the labels of When the dimension is much higher than that, and in particular larger than the number of available forecast-observation-couples, new issues arise. For example, the variogram score becomes computationally involved as the number of summands is S 2 . For multivariate rank histograms, on the other hand, slight misspecifications of the predictive marginal distributions tend to dominate the appearance of the histogram in very high dimension, making it less informative with regard to the multivariate predictive performance. 4 Results"}, {"section_title": "Training period", "text": "The first step of the analysis is to determine optimal weighting parameters in Equation (2). For simple moving averages and window length l, the corresponding weights are w k = 1{k \u2264 l}/l, while for exponential moving averages with scale parameter a the weights are w k \u223c exp(\u2212ak), for the kth preceeding year. For the bias-correction, the weighting parameters are chosen for each year in the validation period by minimizing MSE as follows: For a range of weighting parameters, we bias-correct a set of previous forecasts in an out-ofsample fashion and compute the MSE. We then select the weighting parameter for which the MSE is minimized. Figure 2 shows this selection process for 2017. Here, the best performance is obtained for relatively short training periods of l = 12 for SMA and a = 0.11 for EMA, with a substantial improvement in the predictive performance under EMA. For the variance estimation, the weighting parameters are estimated by minimizing the CRPS. Here, the improvement by using weighted averages is more marginal and unweighted averages lead to close to optimal results. Furthermore, the optimal training period is usually somewhat longer than for the bias-correction. For 2017, the values would be l = 28 for SMA and a = 0.05 for EMA. These values are typical for years for which sufficient past training data is available."}, {"section_title": "Marginal predictive performance", "text": "For a more formal skill assessment, we compare the aggregated MSE for the SMA and EMA methods against the three NGR reference method. The results are summarized in Table   1. The N GR m method performs substantially worse than all others, demonstrating that model biases strongly depend on location. Further, N GR m,s performs significantly better than N GR s , indicating that the bias also varies between seasons. The locally adaptive method N GR la m estimates the same number of parameters as N GR m , which it outperforms clearly, but performs worse than N GR m,s . Both SMA and EMA outperform all other methods, with EMA yielding the overall lowest value as demonstrated in Figure 2. The second row of the table shows p-values of a permutation test assessing significance of the score difference between the model and the best performing model EMA. The seemingly small score differences are all highly significant, as these values constitute average scores over roughly 8 million score evaluations (192 validation months and 42.000 grid points). The N GR m,s model relies on a total of approximately 10 6 parameters while the SMA and EMA approaches rely on one parameter each and are thus much more robust towards outliers. N GR m,s by both, N GR la m is the locally adaptive method described in Section 2.2, SM A is bias correction by simple moving averages, and EM A by exponential moving averages. The second row shows p-values obtained in a permutation test for the significance of the score difference to the best performing method (EMA). We continue our analysis using EMA for the bias-correction. Different models for estimating marginal variances are compared in Table 2 using the CRPS. In particular for the CRPS, miniscule differences can be significant when averaging over many score evaluations. Permutation tests reveal that even the difference between the mean CRPS for SMA and EMA is highly significant. In the supplementary material to this article we additionally assess mean and variance estimation for each month separately. This more detailed analysis supports the conclusion that the exponential moving average method performs best overall, in almost all instances significantly better than all other models. The calibration of EMA and the best NGR method, N GR m,s are assessed in Figure 3 "}, {"section_title": "Multivariate predictive performance", "text": "Here, we compare various multivariate postprocessing approaches where the marginal distributions are generated with EMA. For computational reasons, we restrict our analysis to an area covering the northern half of the Atlantic ocean, cf. Figure 5. The restricted area covers approximately 5600 grid points. Figure 5 shows the forecast residual, the difference between mean forecast and observations, for June 2016. The aim is for the multivariate correlation structure of the predictive distribution to produce similar spatial patterns. To assess this, we compare the methods in terms of variogram scores. To compute the variogram score, the moments of the predictive distribution, E F [|X i \u2212 X j | 1/2 ] are estimated using 500 simulations from the distribution. For the ECC, the 9 forecast ensemble members are used instead. Table 3 shows  France that is considered as shipping route in our case study is shown in black. Method:  validation period, with the significance of the score differences assessed by the permutation test. The lowest variogram score is achieved by the regularized covariance matrix with multiplicative correction, \u03a3 mc , and the score is significantly lower than those for both the geostationary and ECC approach, at a 5% level. The regularized covariance approach with additive correction, \u03a3 ac , achieves a similar variogram score as \u03a3 mc , with a non-significant score difference at the 5% level. An empirical assessment of simulated residulas (not shown) suggests that the regularization approach produces the most realistic spatial structure. The residuals generated by the geostationary approach look somewhat too coarse. This is likely caused by an overall poor fit of the parametric variogram to the empirical variogram, resulting in an overestimation of the nugget. The residuals generated by ECC, on the other hand, seem to vary too little on a large scale, which is mainly caused by the low number of only 9 ensemble members."}, {"section_title": "Case study", "text": "In a further assessment of the multivariate predictive distributions, we take a more applied angle and use the model to predict the minimum SST along a shipping route crossing  the Atlantic Ocean from Bordeaux, France to Norfolk, USA, see Figure 5. The route has a length of 6205 km and we consider all grid cells that are intersected by the geodesic from Bordeaux to Norfolk a part of the route, a total of 93 grid cells. The minimum SST along this route depends jointly on the SST at all locations along the route, requiring spatially coherent forecasts. We consider the same methods of multivariate postprocessing as in the previous section, i.e. the regularization approach, both with multiplicative and additive correction of the marginal variance, as well as the geostationary model and ECC as reference. For each method, we generate multiple simulations from the predictive distribution for each month of the validation period, and compute the minimum temperature along the route. The empirical distribution of simulated minima is then considered the probabilistic forecast of the minimum temperature along the route. For the regularization and geostationary approaches, we simulate 500 forecasts each, whereas for ECC the postprocessed ensemble containing 9 members is used. The accuracy of the forecasts is evaluated with the univariate scores MSE and CRPS, see Table 4. Permutation tests show that the score differences between the Schaake shuffle and the regularization approaches with multiplicative and additive correction of the marginal distribution are not significant at a level of 5%,  whereas both the geostationary model and ECC show lower skill. We further assess the multivariate calibration of the 93-dimensional forecasts using average and band depth ranking, see Figure 6. Note that the number of available observations in the test set is only 192, making it necessary to restrict the number of bins. The rank histograms of the regularization techniques and the geostationary approach look very similar. In fact, the correlation of the observation ranks is approximately 0.99 for any two of these methods, indicating that deviations from uniformity are mainly attributed to imperfect marginal calibration. All three methods exhibit too many high average ranks, corresponding to an overall underestimation of the temperature along the route, possibly caused by warming trends not accounted for in the numerical model. The same effect is responsible for too many low band depth ranks, signalizing non-centrality of the observation within the ensemble. The band depth rank histograms for ECC display a slight \u2229-shape indicating too strong spatial correlations in the empirical copula of the raw NWP ensemble. This is supported by plots of members from the raw ensemble forecast which tend to be visibly smoother than the observation (not shown). The band depth rank histogram of the Schaake shuffle looks fairly uniform. If anything, it is slightly \u2229-shaped which could indicate stronger correlations in the training period than in the validation period, and thus an increasing volatility in the minimum temperature possibly caused by warming. The average rank histogram for ECC and Schaake shuffle look more uniform than those for the other methods. This is presumably caused by the use of equally spaced quantiles which leads to a more evenly spread out predictive ensemble than any ensemble based on simulation. However, this comes at the cost of a very limited ensemble size."}, {"section_title": "Discussion", "text": "This paper proposes a fully probabilistic postprocessing approach for multivariate forecasts, the computational costs of which scale well to higher dimensions. The proposed method incorporates a moving average approach combined with regularization of the covariance matrix through tapering. The approach yields a predictive distribution allowing for non-stationary, non-isotropic and negative correlations in the forecasting error. Based on validation data, it performs well with little available training data and is therefore attractive for seasonal and long-range weather predictions. We have applied the method to seasonal forecasts of sea surface temperature issued by the Norwegian Climate Prediction Model. Performance comparisons indicate that our methodology has higher predictive skill than two reference models, specifically empirical copula coupling and a geostationary method. The Schaake shuffle, our third reference model performed equally good in our case study, but significantly worse when assessed by the variogram score. The geostationary method assumes the forecast error to have a positive and stationary correlation structure, which in weather forecasting is a highly restrictive assumption, as the underlying physics in numerical prediction models will depend on geographic features not taken into account. Both Schaake shuffle and ECC construct ensembles of fixed size, namely the number of years in the trainings period (16, Schaake shuffle) or the size of the NWP ensemble (9, ECC). The small size of these ensembles restricts the usefulness of these methods for some tasks such as quantile estimation. This limitation is not shared by our fully probabilistic postprocessing method which can be used to construct ensembles of any size. There are a variety of regularization methods for sample covariance matrices available in the literature, see Pourahmadi (2013) for an overview. We believe that our combination of tapering and PCA has several advantages making it appealing in the context of postprocessing seasonal weather forecasts. Firstly, the tapering regularizes by distance, in the sense that the sample covariance of distant gridpoints is down-weighted, but still allows for spatial correlations over long ranges. Given the lack of training data, regularization by distance is more reliable than purely data-driven regularization methods such as e.g. sparse PCA, thresholding or graphical lasso. There are some regularization methods that assume sparsity of the precision matrix while accounting for spatial structure of the data, such as for example nearest-neighbour Gaussian fields (Datta et al., 2016). We expect such methods to lead to similar results as tapering. However, both tapering and PCA have a long tradition in the field of meteorology and are therefore more familiar to practitioners. Note that in our model PCA is applied to the already tapered covariance matrix, and its main purpose is not regularization, but reducing the dimension of the predictive distribution. This leads to a massive decrease in the computational costs when sampling from the predictive distribution. For the covariance tapering we chose the range of our tapering function to be 2500 km. At the equator this corresponds to 22 grid points in each direction. In our experiments, choosing any range between 1000 and 4000 km led to good results and did not affect the conclusions derived from Sections 4 and 5. Given the very limited amount of data we refrained from attempting to fit the range parameter. For the PCA, we chose the number d of principal components such that 90% of the variance where retained. Let us stress that PCA is applied to the already regularized tapered covariance matrix and its main purpose is not regularization, but dimension reduction which allows for faster sampling from the predictive distribution. When the area with 5.600 gridpoints shown in Figure 5 is considered, 90% of the variance corresponds to approximately 170 principal components (the exact number varying from month to month). For comparison, if 99% of the variance should be retained, roughly 600 principal components are required. It is, in fact, not straightforward to derive an optimal number of principal components based on validation, as validating high-dimensional forecasts in itself is involved. An optimization of the variogram score using cross-validation, for example, comes at tremendous computational costs and the score differences are insignificant over a large range of principal components. In other words, the impact of truncating the eigenspace of the tapered covariance matrix by PCA is not picked up by the validation tools we applied. This allows us to choose a relatively small number of principal components in order to lower the computational costs for sampling from the predictive distribution. The rule of keeping 90% is therefore a compromise that recovers most of the covariance structure while leading to an increase of sampling speed of factor 20 compared to using the full tapered covariance matrix. We have investigated an additive and a multiplicative correction of the marginal variances of the multivariate model. The results indicate no significant difference between the skill of these corrections. As the additive correction does not recreate the marginal model (1) exactly, and is heavier computationally, the approach based on multiplicative correction should be preferred. As emphasized by Van Schaeybroeck and Vannitsem (2018), a main challenge for postprocessing of seasonal weather forecasts is the shortage of available training data. This is supported by the findings in Sections 4.2 and 4.3, which demonstrate the risk of overfitting when forecast distributions are estimated separately at each location. We utilized moving averages to estimate the biases and variances. Our approach estimates location-specific biases and variances, but we only need to determine a single, global, weighting parameter. Thus the approach will be more robust against outliers than non-homogeneous Gaussian regression grouped by month and location, the best performing reference model. Moreover, our moving average approach will account (to a certain extend) for the trends caused by global warming and the increase in reliability of temperature measurements over the last 30 years. Future research directions include to consider ensemble information beyond the ensemble mean, for instance the single ensemble members. The good performance of ensemble copula coupling, considering the high number of locations and low number of ensemble members, indicates that the ensemble members do contain valuable information. Ensemble copula coupling relies fully on the empirical copula of the ensemble to capture the multivariate forecast structure, while our approach only considers variability around the ensemble mean. Combining both sources of information may be a fruitful way to extend postprocessing techniques in the high-dimensional setting. Moreover, we have currently not considered interactions between different months or seasons throughout the year. Early exploratory analyses showed that including information from previous months as predictors did not improved the forecast distribution. It seems, however, reasonable that forecast errors of different months may in fact be correlated and developing detailed models for such interactions may improve the predictive skill. Finally, our model does not account for sea ice in an appropriate way and could be improved by being combined with an external sea ice model."}]