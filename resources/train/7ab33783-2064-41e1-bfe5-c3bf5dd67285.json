[{"section_title": "Figures and Tables", "text": ""}, {"section_title": "Figures Summary", "text": "The United States relies on a number of infrastructure systems-roads, the electric grid, ports, telecommunications networks, refineries, and the like-for carrying out basic social and economic functions. Disruptions of these systems could impose potentially significant economic, social, environmental, and national security consequences. The U.S. Department of Homeland Security Office of Infrastructure Protection is charged with identifying and prioritizing strategies and investments for improving the resilience of specific infrastructure systems in specific regions. A necessary first step in fulfilling this role is understanding how infrastructure is exposed to natural disasters. To support this work, the Department of Homeland Security, National Preparedness and Programs Directorate, Office of Infrastructure Protection, asked RAND to analyze national exposures to infrastructure from natural disasters. RAND's analysis includes 11 natural hazards and five infrastructure sectors. Tables S.1 and S.2 list the hazards, infrastructure sectors, and subsectors included in this analysis. This report serves as the technical documentation and reference document for the data, methods, and analytic approach used for this study. The report also documents how each infrastructure type and hazard is represented in data sets to act as a reference for any use of the Climate-adjusted hazards refer to those hazards for which we have projected future trends based on credible relationships between climate projections and hazard effects. For these hazards, this analysis enables both present-day and future views of infrastructure exposure. For hazards without such data, this analysis only enables a present-day view. When hazards are climate-adjusted, this simply means that credible projections are available for the effect of climate on these hazards and enable the user to look ahead to how infrastructure exposure might look in the future. So the \"adjustment\" refers to an ability to project into the future. Climate data for two hazards at a given point in time are never considered together unless those data are available for both hazards. x Characterizing National Exposures to Infrastructure from Natural Disasters: Data and Documentation data. For each analyzed hazard, this report includes a brief background that describes potential infrastructure impacts, and relevant metrics; a list of sources used in compiling hazard data; and an overview of existing methods and applications or modifications used to analyze regional exposure to varying levels of hazard severity. When analyzing infrastructure exposures with these data, it is important to understand this information to ensure that the analysis results reflect the scope, precision, and completeness of the data. Failure to appropriately use the data could result in analysis that misrepresents exposures. Appendix A provides an overview of all hazard and infrastructure data used to complete this analysis. Analytic findings about current and future exposures of infrastructure in the United States drawn from this data analysis are documented in a separate report. "}, {"section_title": "Introduction", "text": "The United States relies on a number of infrastructure systems-roads, the electric grid, ports, telecommunications networks, refineries, and the like-for carrying out basic social and economic functions. Disruption of these systems could impose potentially significant economic consequences in the form of delays and increased costs. To minimize such economic losses, it is important that critical infrastructure systems be resilient-that is, able to maintain and regain operational capacity in the aftermath of a major disruption, and to recover relatively quickly and at low cost following the disruption. In recent years, debates on federal infrastructure investment policies have focused largely on the need to renew aging facilities nearing the end of their design life as well as to add new capacity to meet increased demand. With the severity of the recent economic recession, there has also been strong interest in the role of infrastructure investment as a near-term economic stimulus, and as a way to create the foundations for longer-term economic growth. As shown by the significant damages and lasting disruptions caused by Hurricanes Katrina and Sandy, however, there can also be a significant federal interest in policies aimed at enhancing the resilience of the nation's critical infrastructure. The U.S. Department of Homeland Security (DHS) Office of Infrastructure Protection is charged with identifying and prioritizing strategies and investments for improving the resilience of specific infrastructure systems in specific regions. A necessary step in fulfilling this role is understanding how infrastructure is exposed to natural disasters. To support this work, the DHS, National Preparedness and Programs Directorate (NPPD), Office of Infrastructure Protection asked RAND to analyze national exposures to infrastructure from natural disasters. The objective of the completed analysis is to help NPPD highlight patterns of exposure that exist both today and in the future when looking across multiple hazards and infrastructures. For instance, this analysis could help answer such questions as: Which areas of the country are currently exposed to more-intense or multiple hazards? How is the picture likely to change in the future? How might climate change alter exposure patterns? The main focus of this analysis is not on definitively answering all of these questions but rather to provide NPPD and DHS a way to begin to screen for regions and infrastructures within the United States that might be more susceptible to natural hazards today and in the future. Infrastructure data used in this analysis are drawn from the Homeland Infrastructure Foundation-Level Data Homeland Security Infrastructure Protection (HSIP) Gold data set. 1 Because this data set contains proprietary commercial data sets, this analysis describes exposure to infrastructure aggregated at the county level so as not to reveal any of the underlying proprietary databases. Because of the uncertainty in natural disaster events, natural hazards are described with measures of intensity and likelihood: for example, sustained wind speeds with a 100-year return interval or flood depths with a 500-year return interval, or peak ground acceleration from seismic activity with a 2,500-year return interval. The hazard data used in this analysis include information of this type compiled from a variety of sources described in the rest of the report. The analytical steps involved in completing this work included: \u2022 Compiling present and future hazard data (from different sources, described in Chapters Two and Three) \u2022 Identifying infrastructure asset types to include in the analysis (using HSIP Gold as the only source of infrastructure asset data) \u2022 Deciding whether a given asset is truly at risk of \"exposure\" to a given hazard based on inherent attributes of each asset, 2 as well as whether exposure of the asset to a particular hazard is likely to have consequences. For instance, just existing in a region that is prone to wildfires does not actually expose an underground electrical distribution line to a wildfire. \u2022 Overlaying infrastructure and hazard data to provide views of infrastructure exposure to one or more hazards, now and in the future We note that the completed work is not a risk assessment, and focus solely on assessing whether different infrastructures are likely to be exposed to different hazard types. Conducting a complete risk assessment would require vulnerability and impact analyses for each considered infrastructure and hazard type. RAND's analysis considers 11 natural hazards and five infrastructure sectors. Table 1.1 lists the hazards included in each chapter, Table 1.2 lists the  return periods considered for each hazard, 3 and Table 1.3 lists the infrastructure sectors and subsectors included in RAND's analysis. We also note that of the several available data sets and methods for analyzing natural hazards, we necessarily had to choose one to carry out this analysis. Choosing one way to approach the analysis for a given hazard certainly introduces uncertainty in which elements of infrastructure are vulnerable to the different hazards. Given that the objective of this work is to provide a first look at infrastructure exposure to natural hazards from a screening perspective, we believe that the embedded uncertainties do not take away from the usefulness of the analysis. The report documents how each infrastructure type and hazard is represented in data sets to act as a reference for any use of the data. For each analyzed hazard, this report includes a brief background that describes potential infrastructure impacts and relevant metrics; a list of sources used in compiling hazard data; and an overview of existing methods and applications or modifications used to analyze regional exposure to varying levels of hazard severity. When Climate-adjusted hazards refer to those hazards for which we have projected future trends based on credible relationships between climate projections and hazard effects. For these hazards, this analysis enables both present-day and future views of infrastructure exposure. For hazards without such data, this analysis only enables a present-day view. When hazards are climate-adjusted, this simply means that credible projections are available for the effect of climate on these hazards and enable the user to look ahead to how infrastructure exposure might look in the future. So the \"adjustment\" refers to an ability to project into the future. Climate data for two hazards at a given point in time are never considered together unless those data are available for both hazards. analyzing infrastructure exposures with this data, it is important to understand this information to ensure that the results of analysis reflect the scope, precision, and completeness of the data. Failure to appropriately use the data could result in analysis that misrepresents exposures. The analysis addresses infrastructure exposure to hazards in the continental United States, excluding Alaska, Hawaii, and other territories. Hazards are adjusted to reflect climate change when credible relationships between climate projections and hazard effects exist. 4 These climate-adjusted hazards are described in Chapter Two. In the absence of such data and methods, hazards are treated as unchanging with climate change. Hazards not adjusted for climate change are described in Chapter Three. Adjustments to hazards reflect changes in frequency, severity, or impact area due to climate change. We include climate-adjusted hazards when possible in the interest of accounting for future climate change where possible. The lowest common denominator for all hazards is the present day snapshot, where no hazards are adjusted for future climate change. One limitation of this approach is that when looking ahead at cross-hazard interactions, some hazards would be climate-adjusted and others would not. This does not mean in all cases that we think these climate-unadjusted hazards are insensitive to climate change. This simply means we were unable to capture how climate change would affect these hazards. For several of these hazards, possible climate change interactions are beyond those captured in this analysis. Changes in the intensity or frequency of hurricanes could change the distribution of hurricane-force winds along coasts and modify risks of coastal flooding beyond the effects of sea level rise (SLR). Changes in precipitation patterns could modify exposures to riverine flooding, landslides, and ice storms. To the extent these effects amplify hazards, analysis using the data described in this report underestimates natural hazard exposures. To the extent these effects diminish hazards, such analysis overestimates natural hazard exposures. The report concludes by describing the process used to identify the specific infrastructure sectors to include in the analysis and by describing the approach used to assess interactions between infrastructure and hazards. Analytic findings about current and future exposures of infrastructure in the United States drawn from analysis of the data are documented in a separate report (Willis et al., 2016)."}, {"section_title": "References", "text": "Homeland Infrastructure Foundation-Level Data Subcommittee Online Community, \"Welcome to the HIFLD Subcommittee Home Page,\" web page, undated. As of April 7, 2016: https://gii.dhs.gov/hifld Willis, Henry H., Anu Narayanan, Jordan Fischbach, Edmundo Molina-Perez, Chuck Stelzner, Katie Loa, and Lauren Kendrick, Current and Future Exposure of Infrastructure in the United States to Natural Hazards, Santa Monica, Calif.: RAND Corporation, RR-1453-DHS, 2016.\nAs in Chapter Two, for each hazard, we provide a brief overview of the significance of the hazard and describe the specific data sets and analysis methods used to assess the likelihood of regional exposure to varying levels of hazard severity.\n"}, {"section_title": "CHAPTER TWO", "text": ""}, {"section_title": "Climate-Adjusted Hazards", "text": "This chapter describes the data and methods used to analyze the following four climateadjusted hazards: For each hazard, we provide a brief overview of the significance of the hazard-both at present and in the future with climate change-and describe the data sets and analysis methods used to assess the likelihood of regional exposure to varying levels of hazard severity. Where future hazard projections are presented, data reflect hazards in three time periods: 2040, 2065, and 2100. The first two of these periods represent common time horizons for infrastructure planning. The third represents a longer time horizon for which climate scenarios diverge significantly. Two different types of climate projections are used to project future changes in these hazards as a response to climate change. For the drought, wildfire, and extreme temperature hazards, the Coupled Model Intercomparison Project Phase 5 (CMIP5) climate projections are used for analysis. For the tidal flooding, storm surge, and permanent inundation, all grouped broadly under the coastal flooding hazard, National Oceanic and Atmospheric Administration (NOAA) year 2100 SLR projections were used as inputs. We combined these projections into a set of \"aggregated climate change scenarios\" that could be used across all four climate-adjusted hazards. These aggregated scenarios are listed in Table 2.1 and described in two dimensions: (1) emissions level and (2) percentile of the general circulation models (GCMs) considered. CMIP5 projections are classified in two dimensions: (1) representative concentration pathways (RCP) and (2) percentile of the GCMs considered. We first estimated the projected SLR values associated with each CMIP5 RCP and mapped these SLR values to the 2100 NOAA SLR scenarios: low SLR (low), low-to-intermediate SLR (int-low), intermediate-to-high SLR (int-high), and high SLR (high). Comparing the derived CMIP5 SLR values with the NOAA SLR values and using the model percentile information for each CMIP5 projection enabled us to create five aggregated climate scenarios: (1) Low Emissions-Median, (2) Low Emissions-High, (3) High Emissions-Median, (4) High Emissions-High, and (5) Worst Case, which are mapped across both types of climate projections. For instance, the aggregate Low Emissions-Median climate scenario corresponds in the CMIP5 projections to emissions scenario RCP 4.5 and the conditions described by the 50th percentile of the 22 GCMs considered. Table 2.2 lists these 22 GCM models, with the names denoting the author modeling group and the ensemble used for the projections. These models include the RCP 4.5 and 8.5 emissions scenarios and cover the 2006-2100 time period. 1  "}, {"section_title": "Coastal Flooding", "text": "Coastal communities in the United States are exposed to a range of potential hazards, but the most concerning and damaging of these events are coastal floods caused by high tides and extreme storms. The latter-including both tropical depressions and extra-tropical storms that threaten the Eastern Seaboard and Gulf Coast of the United States-can produce storm surges and waves that can damage or destroy coastal property, threaten the health and wellbeing of residents, and degrade or destroy natural habitats (Burkett and Davidson, 2012). Natural and anthropogenic factors influence the frequency and severity of flooding from tidal flooding and coastal storms (Woodruff, Irish, and Camargo, 2013). For instance, poorly understood annual, interannual, and decadal natural climate variations influence the frequency and intensity of coastal storms, and there is a limited observed historical record from which to improve scientific understanding and make predictions. A warming climate can further exacerbate the risks posed by coastal flooding, and also adds additional uncertainty to flood risk projections. Climate change is expected to increase the risk of flooding to coastal communities and assets through several pathways. Most significant among these is SLR, which can impact tidal range, storm surge, and wave heights (Karl et al., 2008). In turn, SLR directly increases the frequency and severity of coastal flooding by increasing the depth of flooding when high tides and coastal storms occur. Global mean sea levels have risen approximately eight inches since 1900 (Church and White, 2011), which have exacerbated the damage caused by storms like Hurricane Sandy (Leifert, 2015). In some cases, SLR in the future could lead to land area becoming permanently inundated unless steps are taken to protect communities from flooding. Climate factors influencing global and local sea level changes include thermal expansion of ocean water, melting ice sheets, wind, atmospheric pressure, oceanic circulation variances, and water density differences. But projecting future SLR is challenging because of the uncertain rate of atmospheric warming and uncertainty in regard to the underlying physical   (Pachauri and Reisinger, 2007). But future SLR could be driven by the rate and magnitude of ice sheet melting from Greenland and West and East Antarctica. Since the full range of ice sheet melting scenarios were not considered, some scientists argue that the IPCC AR4 failed to estimate the full range of SLR possibilities (Horton et al., 2008). The Intergovernmental Panel on Climate Change, Fifth Annual Report (IPCC AR5), includes SLR projections based on ice sheet and ocean models, and as a result raises the 2081-2100 global sea level projection compared with the IPCC AR4 (Miller et al., 2013). For the Third U.S. National Climate Assessment (Walsh et al., 2014), NOAA developed four global mean SLR scenarios by 2100, which we adopt in this study (Parris et al., 2012). These scenarios are shown in Figure 2.1, and summarized as follows: \u2022 Lowest: derived from a linear extrapolation of the historical SLR rates from tide records. \u2022 Intermediate-Low: derived from upper end of the IPCC AR4 global SLR projection using the B1 emissions scenario. \u2022 Intermediate-High: derived from the average of high-end semi-empirical, global SLR projections, which include recent ice sheet loss.  SLR can be further exacerbated by localized factors, including land sinking or subsidence. When land subsides, the effect sums with eustatic SLR to create a relative sea level rise (RSLR) rate, which includes both processes. Subsidence rates vary across different U.S. coastal regions, with notably higher rates of subsidence in coastal Louisiana and the mid-Atlantic. Figure 2.2 shows relative sea level change in 2013 for tide gauge stations across U.S. coastal areas. Note that in some areas of the Pacific Coast and Canada, RSLR is decreasingthis is due to land uplift in these areas that is sufficient to offset global eustatic SLR. In addition to SLR, there is a significant amount of literature describing the potential links between climate change and future increases in the frequency or severity of tropical depressions. These effects could modify exposures to coastal flooding and extreme winds (see the hurricane winds discussion later in this report). Though there is growing consensus that changes in climate will have effects on the intensity and frequency of storms, the literature does not describe how the distribution of exposure will be modified. Thus, these effects were not included in the climate scenarios described here. Depending on where exposures are occurring, the resulting analysis either under-or overestimates exposures. "}, {"section_title": "Mean Sea Level Trends (2013)", "text": "Sea level trends mm/yr (feet/century) 9 to 12 (3 to 4) 6 to 9 (2 to 3 3 to 6 (1 to 2) 0 to 3 (0 to 1) -6 to 0 (-2 to 0) -12 to -6 (-4 to -2) -18 to -12 (-6 to -4) SOURCE: NOAA/National Ocean Service (2013a). NOTE: Figure shows the location of each gauge station, along with an estimate of the sea level trend observed as of 2013. The primary metric for coastal flood hazard is expected flood depth (in feet), estimated at different annual exceedance probabilities (AEPs). The flood exposure analysis determines the flood depth for each facility location (infrastructure) or county centroid (population), and uses a simple binary assessment to determine if the depth exceeds a critical threshold. Critical thresholds were set at 1 foot and 6 feet of flood depth, representing low and high damage intensities, respectively. This exercise was repeated for all three coastal flood hazard types described below."}, {"section_title": "Data Sources", "text": "The coastal flood exposure analysis draws on existing federal data sources developed to support flood risk and climate resilience planning. The methods to incorporate these data are described in the next subsection. Key coastal flooding data sources for this analysis include: \u2022 Digital elevation model: We adapted a digital elevation model (DEM) from the U.S. Geological Survey (USGS) National Elevation Dataset (NED) (USGS, 2014) at a 1 arcsecond resolution. These ground elevations were used to calculate flood depths for each facility or county centroid. \u2022 Coastal land subsidence rates: The source for coastal land subsidence rates is observed tidal gauge data, provided via NOAA's Regional Mean Sea Level Trends data set, which covers the period from 1920 to 2006 (NOAA/National Ocean Service, 2013b). These data report sea level trends at NOAA coastal monitoring stations across the country, and they were used to identify locally varying land subsidence rates. These trends are adopted as-is and projected forward; no scenario adjustment is planned for these data due to lack of sources that provide a systematic nationwide assessment of future scenarios or changes. \u2022 Permanent inundation SLR scenarios: SLR scenarios, ranging from 1 to 6 feet above current conditions, were also adapted from the NOAA Digital Coast (DC) analysis (2015). \u2022 NOAA global mean SLR scenarios: We adopted four consensus global SLR scenarios developed by NOAA to support the Third National Climate Assessment (Parris et al., 2012;Burkett and Davidson, 2012). These scenarios reflect different assumptions about the future SLR acceleration rate (see above). \u2022 Tidal flood projections with SLR: As discussed below, Kriebel and Geiman (2013) developed a method to project tidal flood stages (FSs) at a range of exceedance probabilities with SLR incorporated. These data were obtained from the U.S. Army Corps of Engineers (USACE) and are applied for the higher-frequency flood analysis. have not yet been digitized and (2) a slow analytic update and approval process for new maps. As a result, not all coastal regions can be included in the analysis focused on the 100-year floodplain. 2 SLR and subsidence were added subsequently to FEMA's 100-year elevations according to the methods described in the following section.\nWe use CMIP5 projections in our analysis of extreme temperature. One potential disadvantage of the CMIP5 projections is that they do not include measures of humidity. While humidity is a key component of apparent temperatures and hence a driver of mortality, in the context of this analysis infrastructure humidity is of less importance since most infrastructures are directly impacted by temperature only. Humidity can have important effects (for example, demand for electric power will increase with humidity and evaporative cooling of power plants will be less effective with higher levels of humidity), but high temperatures will primarily drive damage to infrastructure.\nThe primary source of data for the future climate-influenced changes to dryness is CMIP5 projections (Lawrence Livermore National Laboratories, 2013). These data contain various models of future climate that simulate future daily temperature and precipitation across various IPCC climate scenarios. Each climate projection is a result of combining a GCM with a forcing emissions scenario. CMIP5 projections used the latest version of the GCMs and an updated set of emissions scenarios. In this analysis, 22 different GCMs covering the period from 2006 to 2100 and two emissions scenarios (RCP 4.5 and RCP 8.5) are considered. The RCPs reflect advancements in integrated assessment modeling to characterize future developments in global greenhouse gas emissions. The data considered in the study for these projections cover the time period from 2006 to 2100.\nWe use CMIP5 climate data to adjust the Wildland Fire Potential (now called Wildfire Hazard Potential) (USDA, 2012) based on projected changes in the KBDI index calculated for drought as described in the previous section.\nData for seismic hazards are taken from the USGS Seismic Hazards layer contained in the HSIP Gold data set. Associated with the data are 500-and 2,500-year return periods.\nWe use hurricane wind speed data included in the HAZUS Hurricane Model (MH 2.1) that contain return periods of 20, 50, 100, 200, 500 and 1,000 years.\nIce storm data were taken from the digitized data available from the American Lifelines Alliance's Extreme Ice Thickness from Freezing Rain hazard assessment (American Lifelines Alliance, 2004). We consider a 50-year return period for ice storms.\nRiverine flooding data are derived from the FEMA SFHAs (100-year return period), provided in the National Flood Hazard Layer (FEMA, 2015b).\nTsunami exposure is derived from the Risk Management Solutions (RMS) Tsunami Report and USGS NED. The associated return period is \u2264 500 years.\nTornado wind-speed return periods were taken from the extreme wind-speed analysis conducted by Pacific Northwest National Laboratory (Ramsdell and Rishel, 2007) covering the years 1950 to 2003. While the data set covers a range of return periods, our analysis uses a 100,000-year return period.\nLandslide data were drawn from the USGS Landslide Incidence and Susceptibility assessment (USGS, 2013)."}, {"section_title": "Analysis Methods", "text": "We divide the hazard analysis described in this section into three parts, roughly corresponding to differing likelihood ranges for coastal flooding. The first portion considers the possibility of permanent inundation due to future SLR. The second focuses on tidal or \"nuisance\" flooding, which occurs on a more-frequent basis (e.g., less-than-one-year, one-year, or two-year event) but typically produces only minor damage during each event (Spanger-Siegfried, Fitzpatrick, and Dahl, 2014). The final subsection considers flooding from extreme storm events and tropical depressions, which have lower annual likelihoods (e.g., 100-year event) but can cause extensive damage and disruption. The frequency and damage from all three coastal flood types will be influenced by future SLR.\nWe use extreme value analysis software called extRemes for R to estimate extreme value models (The Weather and Climate Impact Assessment Science Program, 2014). We use the block maximum approach to estimate the model. This approach looks at each year (the block) and selects the highest temperature (the maximum) to estimate the extreme value models. 5 We estimate the default generalized extreme value (GEV) distribution, which is theoretically justified to fit the block maxima of data (see Gilleland and Katz, 2014, for an in-depth explanation). Of the two methods generally used to estimate extreme value models, namely maximum likelihood and likelihood-moments estimation, we use the latter, less computationally intensive option because we are calculating tens of thousands of these models. We project the potential vulnerabilities from climate-induced increases in extreme heat by applying a range of temperature thresholds to climate projections and identifying when and where temperatures exceed those thresholds. For example, we might be interested in flagging infrastructure where the 50-year exceedance temperature-that is, the temperature that is likely to be exceeded on average only once in 50 years-was 130 degrees or higher if a 2 percent AEP of reaching temperatures higher than 130 degrees was deemed an unacceptable risk. Our analysis examines where extreme temperatures are expected to exceed two thresholds-120 degrees and 130 degrees. The data used could be adapted to analyze other temperature thresholds. Table 2.6 outlines the procedure we use to estimate climate-induced changes to extreme heat. Figures 2.3 and 2.4 conclude this section of the chapter on extreme temperature by showing example calculations of the 100-year extreme temperatures forecast for 2100, and the change in these extreme temperatures from the present day. Areas with a high-forecast temperature in Figure 2.3 will have the most temperature stress on infrastructure, but many of those areas are already hot. Areas with the biggest changes in Figure 2.4 are likely to have the most difficulty adapting to higher temperatures.  \nThis study's projections of soil dryness use CMIP5 data. CMIP5 projections consider soil moisture and net surface water changes (defined as the precipitation minus evapotranspiration [P-ET]) (Wuebbles et al., 2013). Critical to modeling drought risk, the CMIP5 simulations of the North American monsoon are an improvement over previous versions that failed to adequately simulate all circulation patterns (Wuebbles et al., 2013). Despite large inter-model variations, CMIP5 comparisons are consistent with offline hydrology models (Wuebbles et al., 2013). We use the KBDI to measure projected changes in dryness. Calculation of projections for this index requires measures of daily maximum temperature and daily precipitationmeasures that can be calculated with data available from CMIP5 for all of CONUS. KBDI estimates for the water content of soil/duff are based on daily temperature and precipitation observations over time. Originally developed to estimate the likelihood of forest fires in the southeastern United States, KBDI is more generally applicable to measuring dryness. KBDI values range from 0 (completely saturated soil) to 800 (completely dry soil), scaled to assume that soil can hold from 0 to 8 inches of water . A number of other measures are used in the literature to measure lack of precipitation and soil dryness. These measures can be broken into those that only require data about temperature and precipitation (of which KBDI is one) and those that require other data, such as evapotranspiration, soil moisture, and runoff. Tables 2.7 and 2.8 provide a list and brief descriptions of some of these other methods. \nCurrent, steady-state (i.e., not accounting for present year-to-year changes in drought conditions) wildfire risk for the United States is estimated by the Wildland Fire Potential from the U.S. Department of Agriculture, Forest Service, Fire Modeling Institute. The methodology for creating the Wildland Fire Potential map is described in Dillon, Menakis, and Fay (2013). To create the map, the researchers estimated the likelihood of wildfires and wildfire intensity using the Large Fire Simulator (FSim) (Finney et al., 2011). FSim simulates wildfires using a 10,000-to 50,000-year simulation driven by simulated weather. Many areas of the country have a relatively high annual probability of burning (up to 10 percent), but many of the fires would be small and relatively easy to control, thus not a major threat to infrastructure. The Wildland Fire Potential map calculates the risk of combining the likelihood of any wildland fire starting with the likelihood that long flame lengths would result in a crown fire that burns the entire forest (rather than only material on the ground or at low heights). To account for the consequences, the researchers weigh crown fires and surface fires based on fire intensity. Finally, the researchers adjust these consequences to account for the ease of controlling fires, which varies across different fuel types. The Wildland Fire Potential and FSim projections do not account for short-term variations in weather or fuel moisture, thus they are long-term projections based on historical weather observations measured over the past 10 years to 30 years by the National Fire Danger Rating System (NFDRS) Remote Automated Weather Stations. Therefore, the projections are long term-when moisture content and weather conditions are more favorable for fires, true fire risk will exceed the projections; and when moisture content and weather conditions are less favorable for fires, true fire risk will be below the projections. Thompson et al. (2011) offer a similar, but more complex methodology for calculating wildfire risk. As in the Wildland Fire Potential projections, they use the FSim projections of likelihood and intensity. Rather than integrating these projections into a measurement of risk, such as the Wildland Fire Potential, they apply a set of weights that measure the damage that different flame lengths cause to various types of infrastructure to estimate the average annual loss from wildfires. 7 Liu, Stanturf, and Goodrick (2010) forecast climate-induced changes in future Wildland Fire Potential indexes using the KBDI index. They use four IPCC scenarios (A1, A2, B1, and B2) and the results for four GCMs to forecast worldwide changes in KBDI indexes. Because 7 Subject-matter experts determined the weights of eight different types of infrastructures or environmental amenities. Low-intensity fires were assumed to benefit some infrastructures and environmental amenities (e.g., ski areas), while having little effect on most infrastructure and environmental amenities (e.g., energy infrastructure), and having a near total loss for residential housing. High-intensity fires were assumed to be very destructive to most infrastructures and environmental amenities.  the GCM models are coarse, the projections of the indexes are coarse, too. The authors summarize the results for four regions of the United States (southwest, northwest, southeast, and northeast). Average projected increases in KBDI range from 60 (northeast, B1 scenario) to 240 (northwest, A1 scenario). Liu, Goodrick, and Stanturf (2013) builds on Liu Stanturf, and Goodrick (2010) by focusing on the United States and looking at a more-detailed set of regions. This follow-on study also looks at two weather factors-wind speed and relative humidity-that are not accounted for by the KBDI index. The study calculates KBDI separately for four seasons. The Wildland Fire Potential (WFP) is an index that is calculated by integrating measures of severity (the occurrence of various intensities of flame and crown fires) with likelihood. Therefore, it accounts for both likelihood and consequence, so index cutoffs and thresholds are for overall wildfire risk. Table 2.10 shows the cutoffs that were used to classify WFP values. WFP values are approximately distributed log-normally, and the cutoffs for the levels are approximately one log-unit above each other (i.e., a factor of 2.7 more than the previous threshold). Dillon et al. (2013) detail the procedure for setting the classification cutoffs. To identify infrastructure at risk of wildland fires, we choose a threshold in the risk index (e.g., 401, which represents the cutoff for \"moderate\" risk) and identify all infrastructure that touches a cell in the Wildland Fire Potential map above that risk index. 8 This cutoff can vary by type of infrastructure; for example, a type of infrastructure that is more vulnerable to fire could have a lower cutoff. The climate projections in this analysis estimate changes in wildfire risk by adjusting the Wildland Fire Potential (or the FSim likelihood) by a factor based on projections of the KBDI (as computed in the climate projections of meteorological and agricultural drought). KBDI can be interpreted as a measure of wildfire potential based upon temperature and precipitation over time that ranges from 0 (representing fully saturated soil) to 800 (representing total moisture deficiency). A fundamental difficulty in applying these changes to fire potential to the present-day WFP index is that there is no direct link between KBDI and either the likelihood of a fire 8 Each cell, or pixel, in the Wildland Fire Potential map is approximately 270 meters by 270 meters. occurring or the intensity of fires increasing. The research from Liu, Stanturf, and Goodrick (2010) and Liu, Goodrick, and Stanturf (2013) make clear that conditions are expected to become more favorable for wildfires with climate change. Furthermore, wildfire seasons are expected to lengthen, which would further increase the likelihood of wildfires. Given the limitations of these data, we use the simple assumption that the projected change in KBDI scales to increase the Wildland Fire Potential index . 9 Ideally, we would want to calculate the likelihood and intensity of wildfires as a function of weather inputs, such as temperature and precipitation and the changes in each. However, literature explaining these relationships does not exist. The KBDI index indicates the state of meteorological drought, which is one factor that is associated with increased wildfire risk. However, until research is available to connect weather factors directly to wildfire indexes, we are left to rely on observations that correlate the KBDI index to wildfire indexes. Figure 2.8 shows the change in the 75th percentile KBDI from present day to about 2100 that is forecast by the CanESM2 r2i1p1 model for the RCP 8.5 scenario. (This is the same example used in the agricultural drought section.) The model forecasts increased drought in most of the country. These changes are most extreme in the northeast and northwest-as well as scattered other areas-where they exceed a one-level increase in fire risk (i.e., an increase in KBDI of 200, as used in Liu, Stanturf, and Goodrick, 2010). This particular model forecasts moderate decreases in KBDI for much of the interior western United States. The procedure to model climate-induced changes in wildfire is outlined in Table 2.11.   3. \u039b\u039a\u0392D\u0399 is calculated as = the difference between time periods and the baseline (e.g., 2100-2015) for each individual climate model (e.g., CanESM2_r1i1p1), for each emissions scenario (e.g., RCP 4.5), and for each frequency metric (e.g., Q75 ) compare future Wildland Fire Potential with risk thresholds and locations of infrastructure. NOTE: Frequency of drought adjustment reflects the percentage of time in a year the KBDI would be expected to be at or lower than the stated percentile. e \u0394\u039a\u0392DI Figure 2.9 shows the Wildland Fire Potential adjusted for 2100 based on the changes in the KBDI projected by the CanESM2 r2i1p1 model. Comparing Figure 2.7 reveals many places where fire potential increases significantly. The biggest increases are in the Northwest, the Southeast, and around Texas, but many other areas also increase significantly. Very few areas show a noticeable decrease in fire potential.   As described previously, climate change could affect the distribution of exposure to several of these hazards. For example, patterns of occurrence for hurricane winds, ice storms, riverine flooding, tornadoes, and landslides could all be influenced by changes in weather and climate. However, these effects are not included in this analysis because literature on these hazards does not conclusively describe how these distributions would change.\nThe USGS National Seismic Hazard Mapping Project creates earthquake hazard maps for the United States intended to guide building codes (USGS, undated). The USGS and its partners create the maps using knowledge of historical earthquakes and geology. The current maps were made in 2008, but periodic updates occur as building codes are revised, and updates incorporate new research results (USGS, 2008). The USGS maps include two levels of likelihood (-2-percent or 10-percent chance of being exceeded in a 50-year period) and three types of consequences (peak ground acceleration [PGA], peak ground velocity, and spectral acceleration). The likelihood percentages correspond roughly to a 0.04-percent AEP (or a return period of about 2,500 years) and a 0.2-percent AEP (or a return period of around 500 years). 1 In this analysis, we use PGA because it is a standard determinant of infrastructure damage. USGS maps bin PGA into seven ranges: 0-4 percent, 4-8 percent, 8-16 percent, 16-32 percent, 32-48 percent, 48-64 percent, and 64 percent plus gravitational acceleration (g). Our analysis sets thresholds on PGA and return periods to classify earthquakes by intensity. Earthquakes are classified according to their severity corresponding to the Modified Mercalli Intensity Scale in the following way: \u2022 low severity-observed damage: PGA above 0.1 g \u2022 high severity-severe shaking: PGA above 0.5 g. These maps are available for the entire United States, but they are relatively coarse. Detailed studies-for example, of specific metro areas-can account for more detailed spatial differences, such as soil types. However, such maps are not available for the entire country. Figures 3.1 and 3.2 show sample maps for the Los Angeles area and CONUS, respectively, indicating the PGA associated with a 2,500-return period (or 0.0-percent AEP). Climate change is not expected to increase earthquake risk significantly, so we do not include it in our climate-adjusted hazards. The SREX does cite instances where climate change can be expected to increase the frequency of small earthquakes, for example, as glaciers recede (Seneviratne et al., 2012, p. 188), but these earthquakes are small and tend to be in isolated regions.  \nThe data used in this study consider different wind-speed return periods: 10, 20, 50, 100, 200, 500 and 1,000 years. We use two maximum wind-speed thresholds to classify the severity of different hurricane scenarios. Hurricane scenarios in which maximum wind speeds exceed 96 miles per hour are classified as low-severity hurricanes; wind speeds above 130 miles per hour represent high-severity hurricanes. These cut points are in agreement with the HAZUS scale, as shown in Figure 3.3 for a sample return period of 100 years. \nThe American Lifelines Alliance study estimated four return periods for ice storms (50, 100, 200, and 400 years). A parallel, ongoing effort estimates the occurrence of extreme ice storms for use in building codes, contained in ASCE 7-10 (American Society of Civil Engineers, 2013). ASCE 7-10 uses the 50-year return maps. We obtained a digitized version of the 50-year return maps directly from the USACE, Cold Regions Research and Engineering Laboratory (Jones, 2014). 2 Other returns could be used, but they would require digitizing maps from the 2004 American Lifelines Alliance study. We used cutoffs for estimating potential ice damage using the ice thickness and wind speed projections defined by the Sperry-Piltz Ice Accumulation (SPIA) Index (see Figure 3.4). The index ranges from 0 to 5, with higher levels indicating larger ice loads, higher wind speeds, and greater potential for damage to electrical power systems. Figure 3.5 shows the SPIA index that is calculated for a 50-year return period for CONUS. Note that a large portion of the country is in the most severe category (5), meaning that a catastrophic ice storm is expected at least once every 50 years. Additionally, the hatching in the figure indicates mountainous areas with topography that could lead to more severe ice than is predicted by the lower-fidelity analysis used to create the projections. This means that small areas can have a more severe SPIA index for the 50-year return period than designated by the index value for the surrounding area.  Minimal risk of damage to exposed utility systems; no alerts or advisories needed for crews; few outages. Catastrophic damage to entire exposed utility systems, including both distribution and transmission networks. Outages could last several weeks in some areas. Shelters needed. Prolonged and widespread utility interruptions with extensive damage to main distribution feeder lines and some high voltage transmission lines/structures. Outage lasting 5 to 10 days. Numerous utility interruptions with some damage to main feeder lines and equipment expected. Tree limb damage is excessive. Outages lasting 1 to 5 days. Scattered utility interruptions expected, typically lasting 12 to 24 hours. Roads and travel conditions may be extremely hazardous due to ice accumulation. Some isolated or localized utility interruptions are possible, typically lasting only a few hours. Roads and bridges may become slick and hazardous. SOURCE: RAND calculations applying the SPIA index to the 50-year return ice thickness and wind from American Lifelines Alliance, 2004.  RAND RR1453/1- 3.6 Ice thickness (inches) 0.75 1.00 1.25 1.50\nThe primary data source for riverine flooding is FEMA's National Flood Hazard Layer, which provides FEMA's estimate of the 1 percent AEP flood elevation. As with areas exposed to coastal flooding, described in Section 2.1.2, the 1-percent AEP elevation is used to identify riverine flooding SFHAs for the NFIP's FIRMs. Residents of the SFHA are required to purchase flood insurance via NFIP under most circumstances-in order to qualify for a federally insured mortgage, for example. The National Flood Hazard Layer includes flood maps that have been formally adopted and digitized. The actual flood elevation and flood depth data do not include all areas exposed to flood risk in the United States, however, with gaps from (a) paper maps that have not yet been digitized and (b) a slow analytic update and approval process for new maps. As a result, not all regions can be included in the analysis focused on the 100-year floodplain. Data available to support nationwide assessments of riverine flooding are limited. In many riverine flood areas, FEMA provides the boundaries of the SFHA, but it does not provide estimates of BFEs or flood depths within these areas. As a result, this analysis uses a very simple binary assessment to assess riverine flood exposure. Using a simple geospatial comparison, any facilities within a noncoastal SFHA polygon are counted as exposed to riverine flooding, whereas those outside the polygons boundaries are not considered. This assessment was made using a snapshot of the SFHAs obtained in 2014, and assumes no future change.\nData supporting the assessment of tsunami risks are generally sparse and of poor quality. Better data would help strengthen the associated analysis. Recent efforts are attempting to engage in more comprehensive modeling of tsunami hazards across the United States. NOAA is developing DEMs of the ocean floor and shoreline to model tsunamis on U. S. coasts (NOAA National Geophysical Data Center, undated). These DEMs are highly detailed and can be fed into a NOAA tsunami simulator called the Method of Splitting Tsunami (MOST), which performs scenario-based simulations of tsunamis, including inundation of small areas (NOAA Center for Tsunami Research, undated). These NOAA efforts are mainly focused on estimating, at a detailed level, the consequences of a tsunami arising from specific scenarios (e.g., an earthquake of a given magnitude occurring in a particular location). There is a gap in research that attempts to assess overall tsunami hazards, i.e., both the likelihood and the inundation of tsunamis. There are also local and regional efforts to identify areas of high tsunami hazards. For example, the California Department of Conservation publishes tsunami hazard maps for populated coastlines (California Geological Survey, 2007). These maps were created by using the MOST model for a \"suite of tsunami source events\" that represents \"a number of extreme, yet realistic, tsunami sources\" (California Emergency Management Agency, 2009). The maps show the maximum inundation across these scenarios, but they do not contain information about the likelihood of such scenarios occurring. RMS estimated the return period for a 5-meter tsunami along shorelines throughout the world (RMS, 2006). The RMS estimates have four bins, ranging from high hazard, where return periods are less than 500 years, to negligible hazard, where geology is not conducive to creating a tsunami. Although any coastline has some chance of being hit by a large tsunami (e.g., from a meteor strike), for the purposes of this analysis we only consider areas with a return period of less than 500 years. Therefore, we focus our tsunami hazard estimates on high-hazard areas (in red in Figure 3.7). To identify areas that are likely to be inundated during a tsunami, we conducted a simple analysis of coastal topography on high hazard coastlines (in Northern California, Oregon, and Washington) that flags any location with an elevation of 5 meters or less as being in a high tsunami-hazard zone. The RMS hazard assessment does not differentiate between high, low, and medium hazards by tsunami height. Instead, it differentiates them only by the likelihood of a tsunami of at least 5 meters. The 5-meter height is associated with a significant loss of life-about 5 percent of the population. Tsunami heights from earthquakes can greatly exceed 5 meters and result in higher mortality rates. For example, the highest run-up in the 2004 Indian Ocean tsunami was 40 meters. However, these heights are very unlikely as they depend on specific locations of earthquakes, bathymetry, and topography. The elevation data set is from the USGS NED (USGS, 2006), which is a compilation of raster elevation data for CONUS. This analysis does not account for the effects of the ocean floor or topography, which can attenuate tsunamis. NOAA tsunami models could better account for these details, but such models would have to be run across a range of different tsunami scenarios across a range of areas, thus such modeling is not feasible in this project. High hazard with return period <500 years Moderate hazard with return period 500-2,000 years Low hazard with return period 2,000-plus years\nThe most-detailed study we identified on tornado likelihoods was conducted by Pacific Northwest National Laboratory (Ramsdell and Rishel, 2007) on behalf of the Nuclear Regulatory Commission. Unlike most infrastructures, nuclear facilities must consider extremely low likelihoods, such as the probability of being hit by a tornado, into their design basis. To estimate the likelihood of a tornado strike and the associated winds, the researchers used a database of tornado strikes since 1950 kept by the National Centers for Environmental Information (formerly the National Climactic Data Center). These data provide the location, length, width, and maximum intensity. They estimated probability/wind-speed distributions for tornado strikes to either a point of land or any point within a 200-foot-wide area (to represent a large building). These estimates of distributions were made for grids laid over the United States where the cells were either 1-degree, 2-degree, or 4-degrees. Higher levels of resolution result in a more-detailed map but reduce the number of tornado observations, which makes the estimates less reliable (Figure 3.10). Based on these findings, the Nuclear Regulatory Commission set a design basis for nuclear facilities to withstand wind speeds of 230 miles per hour in central United States, 200 miles per hour in the western Great Plains and on the East Coast, and 160 miles per hour in the western United States. The report presents the estimate distributions in maps that show each cell and report the tornadic wind speed estimated for the map's return period. Each map has a return period of either 10 -5 (once in 100,000 years), 10 -6 (once in 1 million years), or 10 -7 (once in 10 million years). Additional data are presented for each cell that give the strike probability of any intensity tornado, and data that allow adjustments in likelihood to be made that account for 200-foot structures. Furthermore, the report provides a method for adjusting for other sizes of structure as well. Unfortunately, the result of these adjustments adjust only the likelihoods (e.g., a 400-foot structure in Florida would have an increase in likelihood from 10 -5 to 1.059 x 10 -4 [Ramsdell and Rishel, 2007, p. A-2]), which means that cells in the adjusted maps would vary in their likelihood, which makes comparing risk difficult. In our analysis, we consider a return period (10 -5 -that is, once in 100,000 years-which is the highest likelihood available) and compare thresholds with the reported wind speeds to identify areas of high tornado hazards. Figure 3.11 shows the digitized map. White areas of the map correspond to areas where tornadoes have not been observed. Note that the observation of tornadoes is more likely in populated areas; hence areas in the sparsely populated West that have observed tornadoes tend to be in large cities like Phoenix and Salt Lake City. Therefore, the calculations are likely to underestimate tornado severity in lesser-populated areas.  104  106  108  110  112  114  116  118  120  122  124  102  80  82  84  86  88  90  92  94  96  98  100  78  68  70  72  74  76  66   104  106  108  110  112  114  116  118  120  122  124   45   43   41   39   37   35   33   31   29   27   47   146  155  144  135  134  130  116  88  0  0  0  0  0  0  78  0  0  0  75  0  135  136  103  133  138  97   153  156  154  146  140  122  122  116  72  0  0  82  0  0  0  0  0  0  85  81  97  93  130  101  145  140  146  153  82   168  162  156  159  154  135  122  97  76  0  0  0  0  0  0  0  0  0  126  127  125  134  154  153  152  163  163 \nResearch on landslide susceptibility is limited. The most commonly cited source of countrywide landslide hazards is a map of Landslide Incidence and Susceptibility from the USGS (Radbruch-Hall et al., 1982), which has been digitized (USGS, 2013) and is included in HSIP Gold. 4 The USGS Landslide Incidence and Susceptibility maps were created by looking at formations identified on the geologic map of the United States (King and Beikman, 1974) and estimating landslide incidence and susceptibility across the formation or groups of formations. Many of these formations are large in areas-especially east of the Rockies-so large areas have the same rating, even if local incidence or susceptibility varies. For example, the USGS incidence and susceptibility ratings do not take into account topography-a point on a flat plain would get the same rating as a point on a steep hillside if they are in the same formation. We focus our analysis on landslide susceptibility. While the data contain incidence information, they do not provide an estimate of likelihood of a landslide for a particular time period, only the incidence of a landslide at some point in history. Furthermore, the map is coarse, which diminishes the impact of local topography that can affect the likelihood of a landslide. In addition to not providing a direct measure of likelihood, the available data also do not provide a direct measure of consequences either, because they do not indicate how extreme the landslides are. We assume that the consequence of a landslide would be severe if it occurred. were incorporated in the finalized list of data layers to include and exclude for each of the five sectors. We further pruned the data to identify a subset of data elements (e.g., individual electric power plants are data elements of the electric power generation plants data layer in the energy sector) to include for each subsector. The rationale for pruning at this level is consistent with the rationale for focusing on subsets of sectors and data layers-to ensure that model results are useful and include critical elements without unduly straining computational resources. With input from sector experts, we identified a set of criteria including status (in operation vs. obsolete), capacity (any available measure of size; e.g., generation capacity for electric power generation plants), throughput (any available measure of traffic or flow), and connectivity (any available measure of node criticality; e.g., number of lines connected to a substation) with accompanying thresholds for inclusion to arrive at a final list of data elements for each chosen infrastructure subsector. For some subsectors, we chose to include all elements when either the number of total elements was small or the data layer had already undergone filtering (e.g., interstate highways). Table 4.1 describes for each infrastructure layer the sample of infrastructure elements that are considered in this analysis. The table shows that all elements are taken into account for the majority of the infrastructure layers. For some infrastructure types, a sample of the most relevant is selected based on capacity, status, functionality or relevance. Table 4.2 lists the number of infrastructure assets by type included in this analysis. a Estimated number of infrastructure points. Point counts were estimated when the original geospatial information systems infrastructure data set type was in line or area form. In this case, centroid or distance rules were applied for point creation. In characterizing the exposure of infrastructures to hazards, we only count an infrastructure asset in a location as being exposed to a given hazard if we assessed that there could be a conceivable vulnerability of the infrastructure to the hazard (i.e., a \"1\" in Table 5.2) and if the hazard exists in the particular location. When assessing vulnerability, we generally considered interactions that result in direct physical damage, not service disruption or physical damage caused by subsequent reactions or cascading events. These judgments are subjective and other judgments might be appropriate for other analyses. An important feature of this analysis is that it is possible to assess each infrastructure type's susceptibility to being affected by multiple hazards of different severities and different likelihoods. All hazards considered are classified into categories that make it possible to compare their aggregated exposure on a given infrastructure asset; Table 5.3 presents the criteria for classifying hazards into these categories. We consider two levels of hazard severity, high and low, with high severity representing the most potentially damaging hazards, and low severity representing all potentially damaging hazards, including those also considered to be of high severity. Additionally, we consider three likelihood periods: (a) return periods less than or equal to 100 years; (b) return periods greater than 100 years but less than or equal to 1,000 years; and (c) return periods greater than 1,000 years. Like severity, likelihood also is cumulative-when a hazard event has a return period of less than or equal to 100 years and is expected to occur in likelihood period (a), it is also included in the less likely likelihood periods, periods (b) and (c). Allowing for variation across the two hazard severities and the three likelihood periods results in six severity-likelihood categories, represented as columns in Table 5.3. The rows represent individual hazards; each cell contains the criteria for classifying each hazard across the severity-likelihood categories (blank cells indicate an instance where the hazard does not qualify). For example, for the earthquake hazard, high severity includes all earthquakes with PGA at or above 0.5 g, thus a 500-year return period earthquake of 0.5 g PGA falls inside of both the \"High Severity; 100 yr<p\u22641,000 yr\" and \"High Severity; p>1,000 yr\" categories. Additionally, low severity also includes earthquakes of 0.5 g PGA, so this example earthquake also falls within the \"Low Severity; 100 yr<p\u22641,000 yr\" and \"Low Severity; p>1,000 yr\" categories. Blank cells indicate instances where the hazard does not qualify.  only 100-yr return period Tornado EF0 and EF3; all assumed to be p\u22651000 yr EF3; all assumed to be p\u22641,000 yr Table 5.3-Continued"}, {"section_title": "SLR and Permanent Inundation", "text": "The first hazard approach for coastal flooding was to consider areas that would be permanently inundated by SLR in future climate scenarios. To estimate flood depths in this case, we adapted the NOAA DC analysis to directly consider the effects of a fixed level of SLR ranging from 1 foot to 6 feet above the current vertical datum (NAVD88). As DC data include flood depth DEMs for each of these SLR scenarios, the flood depth DEMs were left as-is to be run against relevant infrastructure. Specifically, flood elevations were compared with a 10-meter resolution DEM for each of the six cases, creating a depth layer for each case. These depth scenarios were later matched to the nearest NOAA SLR scenario at different points in time (see below) to develop a common set of future scenarios, though the DC analysis uses fixed thresholds at each foot and does not consider alternate pathways for SLR acceleration over time. Table 2.3 summarizes the data sources, scenarios, and procedure used to estimate flood depths from permanent inundation. The years 2040 and 2065 represent scenario projections 25 years and 50 years into the future, which were intervals of interest for DHS. The year 2100 is a commonly used end date for global and national climate projections, and is the furthest date for which downscaled future climate projections are typically available."}, {"section_title": "Tidal Inundation and Higher-Frequency Coastal Flood Events", "text": "Historical coastal tide levels and flood data are collected from gauge stations on the Pacific, Atlantic, and Gulf Coasts (Figure 2.2). The short lengths of the historical record and distance between gauge stations, however, limit the data available from these stations. In addition, quantitative comparison between simulated extreme events and station data is often difficult because of variations in scale and trends (Osborn and Hulme, 1997). To address these limitations, the Kriebel method describes an approach to determine major FS when no National Weather Service (NWS) FS data are available at these gauges. Using a generic FS allows for a reference comparison between different coastal areas within a given geographic region, with results that are consistent with moderate and major NWS FS data. The Kriebel method uses observed monthly extreme water levels from NOAA tide gauges: A major storm is aligned with extreme high tide when it is within three standard deviations of the mean, and moderate storm is aligned with extreme high tide when it is within two standard deviations of the mean (Kriebel and Geiman, 2013). Essentially, the Kriebel method provides a simplified way to estimate tidal flood elevations at different exceedance probabilities in a consistent way for all NOAA tide gauge stations. SLR scenarios can then be added to these estimates in order to project future tidal flooding. Data were provided for exceedances ranging from the two-year to the 100-year event, but we generally consider the estimates from tidal flooding of this kind-which typically do not consider or include extreme storm surge events-to be more reliable in the two-year to the 20-year interval range. For this analysis, we directly adopted data produced by Kriebel and Geiman (2013) using this method described above and provided to the research team by the USACE Institute for Water Resources. The data set included tidal flood elevations ranging from the two-year to 100year event at each NOAA tide gauge station. We estimated RSLR by combining the NOAA global mean SLR scenarios ( Figure 2.1) with local subsidence rates at each of these station locations, and then estimating the final RSLR for each year and scenario. These values were then summed with the Kriebel tidal flood elevations to produce scenario-and year-specific values for each gauge station and exceedance probability. Finally, we interpolated these values for points in-between the gauge stations, creating a data set filling the continental United States (CONUS) coastal counties. After interpolating, we used DC DEMs with USGS NED DEMs to fill CONUS coastal counties. The DC DEMs were coarsened to match the 1-arc-second resolution of the NED, and the two data sets were then combined, retaining the maximum ground elevation in areas of overlap. We used this combined DEM to estimate a tidal flood depth for each location of interest in the analysis, repeated for each exceedance probability and RSLR scenario. Table 2.4 summarizes the data sources, scenarios, and procedure used to estimate flood depths from tidal flooding. "}, {"section_title": "Low-Frequency Storm Surge Events", "text": "Lastly, we developed coastal flood scenarios for low-frequency storm surge events (Table 2."}, {"section_title": "5).", "text": "This analysis builds on the most recent 100-year base flood elevations (BFEs) provided by FEMA as part of the National Flood Hazard Layer. As discussed above, these were only provided for certain coastal regions, so this analysis does not provide a complete picture of lowfrequency flooding in CONUS. To estimate future flood depths under different SLR scenarios, we first made a simplifying assumption that storm surge and wave flooding sum linearly with RSLR. This likely underestimates future flood depths in some areas, because it does not account for nonlinear effects from RSLR on storm surge propagation (e.g., RSLR leading to loss of coastal marsh that previously absorbed storm surge and wave energy). Nevertheless, such an assumption is necessary to conduct an assessment at this scale and level of resolution. For this analysis, FEMA Flood Hazard Zones (FHZs) were divided into coastal and noncoastal (riverine) areas. Investigation of the literature revealed that a previous study used FHZ classifications for this purpose, taking FHZs of type V as a distinguishing feature of coastal areas . 3 Therefore, we retained all FHZs of type V as coastal, retaining additional adjacent FHZs of type A as determined by a local inspection of inland FHZ extent. Once complete, all noncoastal FHZs were removed from the analysis. FEMA BFE lines represent absolute flood depth for a 100-year flood. As delivered, the BFEs contained depths in a number of different formats, each measured with respect to a different model of sea level. After converting the disparate depths into a single sea level model, the BFEs were used to dice the coastal FHZs into smaller areas. Each new FHZ area was assigned the maximum flood depth of the BFE lines it touched. To complete the analysis, a modified \"bathtub\" approach was used. For each RSLR scenario, the scenario's relative flood depths were added to the BFE absolute flood depths. The summed flood depths at the outer edges of the FHZs were then extrapolated outward, and surrounding ground topography, using the same DEM as in the Kriebel method, was sub- tracted in the areas outside of the FHZs, resulting in relative flood depths. All areas outside of and directly adjacent to the FHZs, which contained positive relative flood depths, were then retained, adding to the FHZs to create an extended FHZ area. Finally, a script added any outlying areas with positive relative flooding that were within one raster cell of the extended FHZ area, building the extended FHZ area out until all relevant additional areas had been added, yielding the final flooded area. This analysis makes use of the best-available national data from federal agencies for coastal flood risk, both in current conditions and projected forward into a climate-altered future. But notable gaps remain. First, higher-resolution, localized SLR scenario projections might be needed for some areas of the country, because local conditions could lead to sea levels well above the global mean projections. For example, the New York Panel on Climate Change (NPCC) recently noted that the average rate SLR over the last century in New York City has been twice the rate observed globally, leading to a 1-foot increase in mean sea level for this area. NPCC also developed new SLR projections out to 2100 for New York and the mid-Atlantic region that are higher than the consensus IPCC AR5 scenarios (Horton et al., 2015). Similar high-resolution projections might be necessary to better assess exposure in other key coastal areas, including the Gulf Coast. Second, as noted earlier, the FEMA flood maps applied in this analysis are incomplete, with some coastal areas lacking digitized BFEs and others working with maps updated in previous decades. FEMA's FIRMs are also limited in that their design is mainly for regulatory assessments versus risk analysis, and only include information at two exceedance probability intervals (100-year and 500-year). A systematic reassessment of coastal storm flood risk nationwide using updated methods would substantially improve upon the information available at present, but at present the only possible improvement would be a site-by-site, state-by-state review to identify the best available and most recent coastal flood risk estimates produced by local or state governments or through the academic and consulting community. Finally, this assessment uses a simple \"bathtub\" assumption that applies RSLR rates linearly to FEMA BFE estimates. In practice, SLR and subsidence can actually produce storm surge and wave effects that increase non-linearly due to changes in surface friction and energy dissipation as coastal marshes and other natural barriers are submerged. As a result, the estimates here likely underestimate the potential storm surge flood risk with RSLR included for some coastal areas considered."}, {"section_title": "Extreme Temperature", "text": "Temperature extremes often are based either on the probability of occurrence or on a threshold value. In general, extreme temperature is defined as the number, percentage, or fraction of days with a maximum (or minimum) temperature, below (or above) the 1st, 5th, or 10th percentile or the 90th, 95th, or 99th percentile, for a given timespan (days, month, season, annual) with respect to a reference time frame. For the purposes of this analysis, we are concerned only with extreme heat. Specifically, we estimate from climate data the highest temperature a location is expected to experience every T years (T is the return period). We chose this metric to conduct our analysis for two reasons. First, this maximum temperature seems most likely to impact infrastructure, while measures of temperature over time and measures of nighttime low temperatures are geared toward assessing human health impacts. 4 Second, this measure is typically established using monthly temperature projections, which are much less computationally intensive to analyze than daily projections. NOAA classifies U.S. temperature rankings as near normal (within the middle third of all periods on record), above normal (within top third), below normal (within bottom third), much-above (within the top tenth), or much-below (within the bottom tenth). Percentile rankings allow accounting for regional differences. The impacts of increases in extreme temperature for infrastructure systems are primarily understood anecdotally. R\u00dcbbelke and V\u00d6gele (2011), for example, noted that a summer 2009 heat wave in France coupled with a drought resulted in cooling water shortages that put onethird of France's nuclear power plants out of service. The authors modeled how decreased water supply and warmer cooling water could limit nuclear power plant capacity to cool during peak summer periods. Smoyer-Tomic, Kuhn, and Hudson (2003) conducted a literature review to outline a list of the impacts of heat waves. Of relevance to this study are the potential impacts of heat waves on infrastructure assets, including \u2022 increased damage to roads, railroad tracks, and bridges \u2022 decreased efficiency in power transmission \u2022 decreased power from hydropower and dams. Smoyer-Tomic, Kuhn, and Hudson (2003) noted that most of the impacts of heat are relative to typical temperatures because people, plants, and animals adapt to changes in climate. On the other hand, impacts to infrastructure are absolute-they depend on the design of the infrastructure. Adaptation is probably most feasible with short-lived equipment needing replacement often (e.g., higher-capacity air conditioners that result in higher energy costsand potentially higher risk if power grids cannot keep up with increasing temperatures). Such adaptations are more difficult with relatively long-lived infrastructure, but modifications and retrofits might be possible at some cost. Therefore, the adaptability of infrastructure over time is a major uncertainty. This study, with its focus on identifying regions and infrastructures to target for investments that can fund these infrastructure adaptations, will assume that infrastructure does not adapt from its current state. Of all hazards, the IPCC has found the most support for climate change increasing the likelihood and severity of extreme temperature. The IPCC's Special Report on Managing Risks of Extreme Events (SREX) states that since 1950 it is very likely that temperatures have increased on a global scale and with medium confidence finds that the length and number of warm spells have increased globally (Seneviratne et al., 2012, p. 111). The SREX finds that it is \"virtually certain\" that \"the frequency and magnitude of warm daily temperature extremes\" will occur in the future and very likely that the frequency, length, and intensity of heat waves will increase (Seneviratne et al., 2012, p. 112). For example, the once-in-20-year high temperature is likely to become a once-in-two-year high temperature across most areas (Seneviratne et al., 2012, p. 112). But the various and complex mechanisms underlying increases in extreme temperatures make projections uncertain. Mechanisms leading to extreme temperature include atmospheric blocking, land-atmosphere feedbacks, soil moisture memory, snow feedbacks, and aerosols . Drought conditions (and drier soil, which is a contributing factor to drought) can exacerbate temperature extremes (Andersen et al., 2005). Significantly, soil moisture and surface heating factors lead to changing mean temperature not always scaling with changes in temperature extremes (Haarsma, 2009; Murray and Ebi, 2012)."}, {"section_title": "Meteorological Drought", "text": "Drought is usually defined relative to a region's historical climate using one of several metrics, some of which are outlined in the analysis section of this report. Meteorological drought refers to conditions when dry weather patterns dominate an area. This type of drought is in contrast to hydrological drought (i.e., when low water supply becomes evident), agricultural drought (i.e., when crops are affected), or socioeconomic drought (i.e., when the supply and demand of commodities are affected). Because human activities across geographies have evolved under pressure from climate conditions, infrastructure has been adapted to meet local climate conditions. The specific conditions for drought (e.g., rainfall and temperature) will vary across geography because of differences in climate. For example, a wet year in a typically arid desert could be a year of extreme drought for a region along the Gulf Coast. Drought can have an especially large impact on agriculture. For example, Backus, Lowry, and Warren (2013) estimated the potential economic impacts from climate-change-induced drought through 2050. In these models, decreases in agricultural productivity from water deficiencies resulted in some of the largest economic losses from climate-induced drought in the U.S. economy due to higher transportation costs to agriculture-demanding industries (see Warren et al., 2010). But beyond the agricultural sector, drought could also have notable impacts for electric power generation and transmission systems. In 2007, drought in the southeastern U.S. caused nuclear and coal-fired plants in the Tennessee Valley Authority system to curtail operations. In 2006, nuclear plants in Illinois and Minnesota were affected by drought affecting water levels in the Mississippi River (Argonne National Laboratory, 2012). Because drought is so closely linked to climate, changes in drought brought about by climate change can be more worrying than typical climatic variability because infrastructure has evolved under the pressures of the typical variability. For example, the Drought Severity Index in Aqueduct Global Maps 2.0 (Gassert et al., 2013) considers a drought to be any time soil moisture dips below the 20th percentile of observed soil moisture observations from 1901 to 2008. By this definition of drought, all regions have historically been in a drought 20 percent of the time in that period. These droughts vary by severity (the length of time soil moisture is below the 20th percentile before it increases, and the relative dryness of the drought relative to average conditions), but they have occurred everywhere with the same frequency. As climate changes, what used to be the 20th percentile for soil moisture can change. Given this historical definition of a drought, some regions can be in drought more frequently than others as climate changes. Furthermore, the relative severity of those droughts might change."}, {"section_title": "Standard Precipitation Index", "text": "Based on standardized precipitation data to a normal distribution N(0,1) and calculated on the number of selected time periods (1-48 months): \u2022 -3 (extreme droughts) \u2022 -1.5 to -2 (severe droughts) \u2022 -1 to -1.5 (moderate droughts) \u2022 -0.5 to -1 (mild droughts) \u2022 0 to +2 (mild or severely wet) \u2022 +3 (extremely wet) (Edwards, 1997;Lloyd-Hughes and Saunders, 2002) Consecutive dry days (CDD) Based on the maximum consecutive number of days of no rain, below a given threshold (e.g., <1 mm per day) within a considered period (e.g., 1 year) (Tebaldi et al., 2006) Crop Moisture Index (CMI) Based on mean temperature and total precipitation, from the existing and previous week, to measure short-term conditions: \u2022 -3 (severely dry) \u2022 +3 (excessively wet) (Palmer, 1968) No-rain episodes Duration of no-rain periods (e.g., >20, 30, or 60 days) (Groisman and Knight, 2008) We use frequency statistics instead of exceedance metrics because exceedance metrics are not compatible with the way that KBDI is scaled. According to the U.S. Forest Service's Wildland Fire Assessment System (2014), KBDI has four categories: typical early spring (KBDI of 0 to 200), typical late spring (KBDI of 200 to 400), typical late summer (KBDI of 400 to 600), and severe drought (KBDI of 600 to 800). In other words, most of the KBDI scale typically will be reached during a year somewhere in the United States because of seasonal variation. KBDI is not a distribution with a long tail like many of the other hazards-it has a fixed maximum that is often reached. Therefore, we calculate frequency statistics, which will be impacted by changes in the length of dry spells (e.g., longer dry spells will increase the frequency of high values of KBDI) and the severity of dry spells (e.g., more severe dry spells will increase KBDI). The 75th percentile will be driven by dryness during the dry season. The 95th percentile will be driven by the most extreme dryness events. 6 The procedure to model climate-induced changes in dryness is outlined in Table 2.9. Figure 2.5 shows the calculations for the 75th percentile KDBI value for one model for the RCP 8.5 scenario for 2090 to 2099, i.e., the scenario and time period expected to be most impacted by climate change. The 75th percentile is roughly the average minimum KBDI during the dry season. Figure 2.6 clarifies the 400 and 600 cutoff points; on average, areas in yellow are in a severe drought for at least three months of the year, while areas in red are in an extreme drought for at least three months of the year. 6 We also calculated the average and 50th percentile, but we do not plan to use them as thresholds.  (Williamson, 2010). \u2022 Use the \"index_KBDI\" function, which requires temperature, precipitation, and the mean annual precipitation (calculated from baseline data) b 2. Calculate frequency statistics 3. Compare KBDI statistics with thresholds and locations of infrastructure a A KBDI of zero assumes that the soil is saturated at the beginning of the period. Ideally, KBDI would be calculated across all years-including historical years-for each model to assure that KBDI always reflects historical conditions. However, we assume a starting KBDI of zero to compensate for high computational requirements of calculating KBDI across all years. We believe this assumption has minimal impact on calculations of the 75th and 95th percentile because the soil in most U.S. locations is relatively saturated on January 1 and generally becomes more saturated over the winter because there is little evaporation. b When implementing this modeling, we discovered that the code was extremely slow, so it could not be used to do a large number of calculations as would be needed in our modeling. Therefore, we rewrote a table lookup function to make it faster and we changed how some of the calculations were using memory, but we did not make any material changes in the calculations. 2100 KBDI 75th percentile 400 to 600 > 600 NOTE: Areas in orange are in a typical late-summer drought, and areas in red are in a severe drought, for at least three months of the year. There is stronger agreement between models on long-term drought and large spatial resolution, while there is more variability of shorter-term drought on a regional scale (Blenkinsop and Fowler, 2007). Two important uncertainties concerning future drought trends include ocean circulation and land-atmosphere feedback interactions (e.g., drought impacts on vegetation physiology) . Lack of observations and full representation of soil moisture-evapotranspiration factor into climate models and to existing uncertainties (Seneviratne et al., 2010)."}, {"section_title": "Wildfires", "text": "Wildfire impacts range from property damage, to physical and psychological harm. Economic costs include property damage, reduction in tourism, and timber losses (Morton et al., 2003). Forest fires impact mortality and increase the incidence of respiratory and cardiac illnesses (Gamble et al., 2008). Indirect health impacts can further occur from increased risk of landslides or soil erosion (McMichael et al., 2003). At the same time, there are often resources, such as insurance and private donations, made available to recover from this hazard (Banks et al., 2012). Fire risk can be measured by length, frequency, and severity, as well as by a decrease in fire extinguishment and faster fire spread . Fire risk can increase because of changes in climate extremes, including drought, low humidity, and high temperatures. For example, droughts can turn vegetation into wildfire fuel. Combined with human sources of ignition (i.e., deforestation), these variables simultaneously can change fire risk levels from low risk to medium risk and medium risk to high risk . However, no one process directly describes an increase in fire occurrence. The SREX cites studies that show drought and increased temperatures in North America are linked to increased risk of wildfires, and that wildfire activity has increased substantially since 1950 (Handmer et al., 2012, pp. 252, 259). For example, some research shows that higher temperatures lead to earlier snowmelt, which increases the length of the fire season. The SREX finds that people and infrastructure have been moving into more vulnerable areas. For example, the movement of urban areas into the bush exacerbated the impact of the 2009 Australian fires (Handmer et al., 2012, p. 239)."}, {"section_title": "Earthquakes", "text": "Earthquakes can have serious economic, social, and environmental impacts. Several factors, including characteristics of the earthquake itself (e.g., where it falls on the Richter scale) and characteristics of the affected region (e.g., population density and infrastructure clustering), govern the impact of an earthquake. From an infrastructure standpoint, in the short term, earthquakes could destroy infrastructure above and below the ground. Buildings and transit systems could be destroyed. Underground gas pipelines could explode, leading to fires and related secondary effects. In the longer term, the economic and social costs of rebuilding infrastructure following an earthquake could be enormous (American Society of Civil Engineers, 2007). An understanding of the expected likelihood and severity of earthquakes affecting the region informs investments targeted at boosting the resilience of a community or area to seismic activity."}, {"section_title": "Hurricane Winds", "text": "Hurricane winds can topple power lines, destroy buildings, bring down trees, and create dangerous flying debris. The higher the wind speeds associated with a hurricane, the greater the potential for property damage. Tropical cyclones are commonly associated with extreme winds; the extent of a tropical cyclone's wind field can play a significant role in determining the impacts from storm surge. Like other extreme phenomena, wind extremes can by defined by percentiles, maximums over a stated time period, or a threshold value (e.g., 99th percentile 10-meter hourly wind). Wind gusts are measured by the highest winds during a short time period. However, extreme wind is often not characterized from its own observations, but rather defined by the hazard phenomena that generated it (including tropical and extratropical cyclones and thunderstorms). In turn, wind speed can influence additional climate stressors including SLR, wildfires and droughts. Extreme wind changes need to be distinguished from natural wind variability flows. For example, there is substantial tropical cyclone frequency variability based on different regions and ocean basins (Karl et al., 2008;. In this analysis we focus on the effects of high wind speeds that typically accompany hurricanes. The IPCC's SREX reviewed literature on historical tropical cyclones and found that there is low confidence in trends in historical frequency (Seneviratne et al., 2012, p. 163). The SREX also concluded that there is low confidence in how the geography of tropic cyclones (i.e., the tracks and areas of impact) will change in the future. The SREX states that it is likely that the frequency of tropical cyclones will not increase globally, even though it is likely that rainfall rates and maximum wind speeds will increase in tropical cyclones. The SREX states with high confidence that the damages from tropical cyclones will increase from increases in exposure (Handmer et al., 2012, p. 271), but that the main driver for increasing losses in many regions will likely be socioeconomic factors rather than climate change-related factors (Handmer et al., 2012, p. 273). Still, these effects could modify exposures to extreme winds. These effects were not considered in the climate scenarios described here. However, this literature does not describe how the distribution of exposure will be modified. Depending on where exposures are occurring, the resulting analysis either underestimates or overestimates exposures."}, {"section_title": "Ice Storms", "text": "Following an extreme ice storm in 1998 that led to widespread, long-term power outages in Quebec, Canada, and adjacent U.S. states, there was an increased recognition of the potential damage from winter ice storms to infrastructure. In response, FEMA formed the American Lifelines Alliance, which in 2004 released a study that estimated the likelihood and magnitude of winter storms (American Lifeline Alliance, 2004). Deposits of heavy amounts of ice from ice storms can damage infrastructure. Wind further exacerbates the force of the ice, thus the estimates look at both ice thickness and the winds that would accompany or follow the ice storm. To produce these estimates, the researchers used their expert judgment along with newspaper and weather reports since 1950. Ice storms likely will be impacted by climate change, but not much research exists on what these changes will be. Historically in the United States, there has been a trend for more extreme snowstorms-even in warm years-but no trends in ice storms have been observed over the past century (Kunkel et al., 2013). Our inclusion of ice storms, but not snowstorms, reflects the fact that ice storms can damage infrastructure, whereas snowstorms are more likely to merely disrupt infrastructure."}, {"section_title": "Riverine Flooding", "text": "Riverine flooding is an extremely relevant natural hazard, as many population centers have developed around rivers and communities have been established inside floodplains that are currently dry but could be reclaimed by water in future. As a result, riverine flooding can be a costly natural hazard (Olsen, 2006). For instance, the 1993 summer riverine flooding in the upper Mississippi river was a devastating flood event that affected nine states with estimated economic losses up to $20 billion (Kunkel, Changnon, and Angel, 1993). Riverine flooding occurs when flows going into natural water bodies, such as rivers, streams, and lakes exceed the capacity of these banks, causing overflows to adjacent areas. Riverine flooding can be induced by excessive runoff from intense rainfall, channel erosion, and infrastructure failure (i.e., dams and levees). There FEMA classifies areas at risk of riverine flooding using the SFHA classification system. SFHAs are areas subject to a 100-year return period flooding. Riverine flooding falls mainly into two categories: A or AE zones. FEMA defines A zones as \"areas subject to inundation by the 1-percent-annual-chance flood event generally determined using approximate methodologies\" and AE zones as \"areas subject to inundation by the 1-percent-annual-chance flood event determined by detailed methods\" (FEMA, 2015a). The difference between these zones is that the chance and impact of flooding are estimated using different methods. For AE areas, precise flood levels are determined, while only approximate levels of flooding are provided for A zones. Regardless, these classifications are determined using numerical models for riverine flood analyses. These methods include the Hydrologic Engineering Center Modeling System (HEC-HMS) and the U.S. Geological Survey National Flood Frequency program . Climate change is expected to influence the frequency and intensity of riverine floods because of changes in precipitation patterns and other climate-influenced factors (Walsh et al., 2014). We are aware of at least one preliminary study looking at the potential effects of climate change on future riverine flooding across the United States (AECOM, Michael Baker Jr. Inc. and LLP Deloitte Consulting, 2013), and recently routed CMIP5 river flows could be adapted for a future national assessment of climate-influenced riverine flood exposure. At present, however, there are no data sets that systematically describe potential changes in riverine flood depths across the United States as a result of climate change, and it was determined to be beyond the scope of this analysis to develop and apply such an analysis. As a result, we consider riverine flood exposure in a simplistic manner only, and do not consider potential future changes for this hazard."}, {"section_title": "Tsunamis", "text": "Tsunamis can have a range of impacts from barely being noticed to causing widespread and devastating damage. As with earthquakes, the extent of a tsunami's impact depends on several factors including its distance from the point of origin, its magnitude, and ocean depth in the affected coastal areas. In extreme cases, tsunamis can destroy buildings, bridges, transit systems, power lines, and most structures on their path. Again, as with earthquakes, the period of recovery and rebuilding following a tsunami can be extremely taxing both economically and socially. Another mechanism through which tsunamis can damage infrastructure is run-up, i.e., water from the force of tsunami waves pushed inland and to higher elevations. These effects are extremely dependent upon the directionality and force of waves, geography of coasts, and elevations of land. These effects are not represented in the tsunami hazard data, because detailed modeling of tsunami run-up is not available for CONUS. Thus, the exposures to tsunami hazards are likely an underestimate of exposure. The cause of most tsunamis is not related to climate, thus they are not studied in depth by the climate literature. Climate change is likely to have some impact on tsunamis because of increases in sea level, but these are relatively small compared with the size of a tsunami. Furthermore, much of the coast in the Pacific Northwest is rising, thus mitigating some of the impact of SLR (U.S. Army Corps of Engineers, 2014). Overall, the quality of tsunami hazard data for the United States is relatively poor. Even for regions that have studied these hazards in depth, such as Oregon (Priest, 1995), the hazard maps tend to be scenario-based instead of likelihood-based, so it is difficult to understand tsunami risk."}, {"section_title": "Negligible hazard with return period of tens to hundreds of thousands of years", "text": "To estimate the potential area of inundation for a tsunami, the following procedure was used: \u2022 Elevation data were selected for all areas in CONUS that are: -within 1 kilometer of the Pacific Coast -north of 40-degrees, 15-minutes north latitude (the southern limit of the RMS \"high hazard\" coastline) \u2022 Any areas with an elevation at or below 5 meters were deemed at risk of a 500-year tsunami. -Other areas are considered outside the 5-meter tsunami zone \u2022 To estimate the height of the tsunami, we assume that the 500-year tsunami is 5 meters tall, so the height of inundation is the difference between 5 meters and the elevation. RMS analysis considers tsunamis that inundate land that is 0.5 to 1 kilometer from the coast. Tsunami water can reach much farther inland (for example, 4 kilometers on part of Sumatra in the 2004 tsunami); however, most of the damage occurred near the coast. The choice of 40-degrees, 15-minutes north latitude coincides with RMS's estimate of the highhazard area with tsunami return periods of 500 years or less. These coastlines lie along subduction zones formed by the North American Plate, the Gorda Plate, and the Juan de Fuca Plate. The tsunami estimates from RMS that we use might be conservative in some places. The high hazard zone is anything with a return period of 500 years or less. It is likely that some areas have a return period for a 5-meter tsunami of less than 500 years. In such a region, a 500year tsunami would correspond to a tsunami above 5 meters in height. Additionally, during an earthquake that triggers a large tsunami, land can subside, which could push the tsunami run-up higher relative to the current elevation. For example, models used to construct Oregon's tsunami hazard zone project (Priest, 1995) include up to 4 feet of subsidence (see Figure 3.8). 3 Finally, if the tsunami occurred at high tide, it would run up higher (and conversely, if at low tide, it would run up lower). Unfortunately, comprehensive data that are detailed enough to make these estimates are not available. Our method assumes that the tsunami run-up elevation is constant (within the cutoff of 1 kilometer to the coast). In reality, tsunami run-up dissipates as tsunamis travel across land, thus the run-up from a 5-meter tsunami 500 meters inland is likely to be less than 5 meters. Figure 3.9 shows tsunami hazards predicted for Pacific City, Oregon. Note that the behavior shown on the map is similar for much of the Pacific Coast. Generally, the only area that lies below 5 meters of elevation is sandy beach (and considered on many maps to be part of the ocean). The steep beaches of the Pacific Coast mitigate the damage that would be caused by a 5-meter tsunami.  "}, {"section_title": "Tornadoes", "text": "Tornadoes occur relatively frequently, but they usually affect small areas of land. Data about historical tornado observations can be incomplete because they require that somebody observe a tornado, which might not occur in a relatively unpopulated area or at night. Since 1950, there has been a steady increase in the number of tornadoes reported in the United States, but this rise has been driven by increases in reporting of the weakest, F0 tornadoes (Ramsdell and Rishel, 2007). Likely because of a building's relatively small chance of being hit by a tornado, building codes do not require construction in anticipation of tornado winds. In contrast, hurricane winds, which are likely to hit a relatively large area of land, are considered in building codes. Climate change can impact tornado frequency and severity, but there is little understanding of what changes would occur. There are two major limitations for studying convective storms. First, historical data are problematic, which leads to low confidence in measured historical trends (Kunkel et al., 2013;. As mentioned in the following section, this can lead to underestimates of the risk of tornadoes in the data we are using, but these underestimates are likely to be concentrated in less-populated regions without much infrastructure where tornadoes go undetected. The second limitation is that climate models do not have the resolution to model tornadoes. Overall, some changes in climate are likely to help tornado formation (atmospheric instability) and other changes likely to hurt (reduced vertical shear) (Seneviratne et al., 2012, p. 151). Others find that the trends tend to be favorable but statistically insignificant (Kunkel et al., 2013). Therefore, we do not model changes in tornado exposure caused by climate change."}, {"section_title": "Landslides", "text": "Small landslides are relatively frequent but do not tend to have notable impacts, whereas large landslides, while rare, can have a range of impacts. In addition to affecting water supplies, fisheries, forests, and sewage disposal systems, larger landslides can cause damage to dams and transportation routes. Because landslides are highly localized events, regional or national losses associated with landslides are hard to determine. Additionally, landslides tend to coincide with other disasters, such as hurricanes and earthquakes, making it difficult to isolate the specific impacts of landslides. Avalanches and landslides are related. We included landslides because we judge it unlikely that avalanches present a high risk for most infrastructure assets as they tend to occur in remote areas without a high density of infrastructure. The SREX concludes with high confidence that the frequency of landslides will increase in the future because of climate change from heat, glacial retreat, permafrost degradation, as well as increased heavy rainfall (Seneviratne et al., 2012, p. 114). However, the SREX concludes there is low confidence about future locations and timing because \"these depend on local geological conditions and other non-climatic factors\" (Seneviratne et al., 2012, p. 114). Similarly, the data we are using to assess landslide risk are coarse and reflect general trends in susceptibility, but site-specific topography is the greatest determinant of landslide risk. Because our data do not support detailed assessment of landslide risks, we do not think they justify any climate adjustment, but should be interpreted with the realization that landslide risk is likely to increase with climate change if changes in precipitation effect areas where the elevation contours of land create a susceptibility for landslides."}]