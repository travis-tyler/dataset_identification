[{"section_title": "Abstract", "text": "ABSTRACT A fusion of medical imaging data obtained from different modalities plays an important role in the current clinical practice. In this paper, we propose a novel multimodal fusion algorithm for brain imaging data based on the statistical properties of nonsubsampled shearlet transform (NSST) coefficients and a novel energy maximization fusion rule. The marginal distributions of the high-frequency NSST coefficients exhibit heavier tails than the Gaussian distribution. As a consequence, after studying its characteristics, we use a heavy-tailed probability density function, student's t location-scale distribution, to describe the highly nonGaussian statistics of empirical NSST coefficients by learning the parameters using maximum likelihood estimation. Then, we employ this model to develop a maximum a posteriori estimator to obtain the noisefree coefficients. Then, for the first time, a novel fusion rule for obtaining the fused NSST coefficients based on maximizing the energy in the high-frequency subbands is proposed. Experiments are carried out on fusing two or more multimodal neuroimages taken from the BrainWeb, Alzheimer's Disease Neuroimaging Initiative (ADNI), and Whole Brain Atlas databases. It is seen from the subjective and objective results that the proposed multimodal neuroimaging fusion method significantly outperforms the state-of-the-art methods including under noisy scenarios, and hence, it is more robust. It is also observed that the signal intensities in the fused images are better enhanced when a more number of source images are being fused. The proposed technique should benefit the medical professionals in diagnosing neurological disorders, such as Alzheimer, epilepsy, and multiple sclerosis.\nINDEX TERMS Maximum a posteriori estimation, multimodal fusion, neuroimaging data, nonsubsampled shearlet transform, student's t location-scale distribution."}, {"section_title": "I. INTRODUCTION", "text": "Brain is the most complex organ in the human body. Over the past 25 years, the burden of neurological disorders have increased significantly due to population growth and ageing [1] . In modern medicine, recent advances in noninvasive neuroimaging technology have had enormous impact on the diagnosis and treatment of brain diseases. In general, the neuroimaging technology falls into one of the two main categories, namely, structural and functional imaging. The structural neuroimaging deals only with the valuable anatomical information of the brain. As a result, the brain The associate editor coordinating the review of this manuscript and approving it for publication was Victor Hugo Albuquerque.\nimaging data obtained from one of the above two techniques is not sufficient, in general, to carry out a proper diagnosis, and hence, data from multiple sources are acquired. As an example, structural magnetic resonance imaging (MRI) can detect changes in different regional structures such as lateral ventricle size with quite detailed resolution as shown in the Fig.1(a) . On the other hand, the functional neuroimaging technique deals only with the functional changes such as metabolic activity of neurons, local changes in blood flow, regional composition, and absorption of brain [2] . Moreover, it usually produces images of low resolution and fail to convey spatial information as shown in the Fig.1(b) . In such situations, it is important to be able to see the relation between the anatomical details visible at one location in one modality VOLUME 7, 2019 This and the functional details that are more visible at the corresponding location in the other modality for better diagnosis and surgical planning. It is not easy to see such a relation by observing multiple modalities separately and to interpret and correlate the information provided by each modality. The desire to visualize such complementary information in a single image has led to investigations in multimodal image fusion. Such a fused image is more suitable for human and machine perception or for further image-processing tasks such as segmentation, feature extraction and object recognition. However, the existing fusion algorithms face a number of challenges such as loss of significant attributes, staircase effects, blocking artifacts, noisy patches, and contrast reduction. Specifically, our aim is to fuse 2D multimodal brain imaging data that results in a single fused image, which overcomes most of the existing challenges. It is useful to have a precise comparison of brain functional details with its underlying structure, avoiding possible mistakes resulting from alternate viewing of two or more separate images. However, the manual way of integrating the information obtained from multiple techniques is an extremely time-consuming and expensive process, which also demands years of experience to reduce human error. Hence, automated multimodal neuroimaging fusion is recognized as a promising solution to understand the different aspects of brain diseases at an early stage [3] . In recent years, several techniques have been developed to handle the multimodal fusion problem for neuroimaging data [3] - [20] based on spatial, or transform domain approaches [21] . Despite these efforts, it still remains a challenging task and requires more research to overcome some of the drawbacks such as staircase effects, loss of significant attributes, blocking artifacts, and contrast reduction of the existing automated fusion algorithms. In [6] , the authors have developed a fusion method which is based on edgepreserving guided image filter. The idea is to first find two sharpened images by considering one as an input image and the other as a guidance image and vice versa. This is found by subtracting the input images from the blurred images which is obtained using guided image filtering. Then, the sharpened images are combined based on the weights obtained through image statistics. Although it provides better gradient information, it is clear that the intensity information could get lost. In addition, choosing a guidance image in case of fusing three or more images is a challenging task. Zhao and Lu [8] have proposed an adaptive fractional order total variation (TV) technique to estimate the fused image in the spatial domain. It can suppress the noise including the staircase effects present in the original TV models [22] , [23] ; but it still suffers from contrast reduction, just as other spatial domain techniques do [3] . In view of improved data representation, energy compaction, and reduced complexity, transform domain approaches are known to be more effective in handling multimodal image fusion problem [21] . The transform domain methods are generally categorized into sparse representation-based and multiscale decompositionbased methods. Liu et al. [11] have proposed a convolutional sparse representation-based fusion framework in which the source images are modeled as the sum over a set of convolutions between sparse coefficient maps and dictionary filters. In [9] , the source image patches have been classified based on geometrical features and coded sparsely via online multiclass dictionary learning algorithm. The sparse representation-based methods learn dictionary elements from the source images. However, in case of noisy input, a highly redundant dictionary may cause unwanted visual artifacts in the reconstructed output [24] . Recently, multiscale transforms such as local Laplacian pyramid [5] , wavelet [7] , [25] , curvelet [20] , [21] , contourlet [3] , [19] , shearlet [17] , or cascade of such transforms [15] , [26] have also been employed to develop fusion algorithms for neuroimaging and other medical images. In local Laplacian pyramid-based method [5] , the source images are first decomposed using local Laplacian filtering [27] into approximate and residual images at different scales. Then, the approximate and residual images are fused using a local energy maximum scheme and an information of interest-based scheme, respectively. However, it is known that the widely-used Laplacian pyramid and wavelet transform is not effective in representing anisotropic features such as edges of multidimensional data. To overcome this limitation, Cand\u00e8s et al. [28] proposed the simpler, faster, and less redundant curvelet representation to capture the geometric information in images. In [20] , a curvelet based fusion is implemented based on the application of the additive wavelet transform on the source images. The maximum fusion rule is applied on the ridgelet transform of high frequency bands. However, the implementation of curvelet in discrete domain is very challenging as it requires a rotation operation, which does not preserve the digital lattice [29] . To deal with this issue, two other multiresolution tools, contourlet [30] and shearlet [31] were introduced. Although these two transforms have good directional sensitivity, it is found that shearlet transform is more advantageous as there are no constraints on choosing the number of directions and the size of the supports for the shearing. It is also computationally more efficient unlike the usage of the directional filter banks in contourlet transform [31] . However, the downsamplers present in the Laplacian pyramid makes the shearlet transform, which is constructed by combining the Laplacian pyramid and shearing filters, shift invariant resulting in pseudo-Gibbs phenomena around the singularities such as edges [32] . Nonsubampled shearlet transform (NSST) [16] , [31] , [33] resolves this artifact by using nonsubampled filters in shearlet transform and it is proved as an excellent multiscale decomposition tool for medical image fusion [4] , [34] . Taking this into account, in this paper, we employ NSST in decomposing the input data into different scales and directions. Furthermore, model-based fusion approaches [17] , [25] have been developed using generalized Gaussian distribution (GGD) to characterize the non-Gaussian properties of subband coefficients. However, the traditional GGD model often exhibits exaggerated tails [35] . Hence, we proposed a statistical modeling technique [36] and found that student's t location-scale distribution has the potential to approximate the heavy tailed nature of NSST coefficients more closely when compared to GGD.\nThis paper introduces a novel Bayesian inference-based method for fusing neuroimaging data from an arbitrary number of modalities in the NSST domain using the student's t location-scale distribution as a prior for developing a posterior probability density function (PDF). In view of the fact that the high frequency subbands contain highly discriminative features such as edges, corners, blobs and ridges, a new rule for multimodal fusion based on maximizing the energy in all such subbands is proposed.\nThe paper is structured as follows. Section 2 reviews the basic structure of NSST. Section 3 describes the simplest forward model for neuroimaging fusion problem. Section 4 discusses the proposed neuroimaging fusion algorithm. Section 5 presents the experimental results of fusing two or more images using both synthetic data from BrainWeb [37] , and real data from Alzheimer's disease neuroimaging initiative (ADNI) [38] , and Whole Brain Atlas (WBA) [39] databases, followed by conclusion in Section 6."}, {"section_title": "II. NONSUBSAMPLED SHEARLET TRANSFORM", "text": "The nonsubsampled shearlet transform, a multiscale, multidirectional, and shift invariant framework proposed in [31] , is found to be a highly efficient multiresolution tool to provide an optimal sparse approximation for multidimensional signals with anisotropic features such as edges and other spatial discontinuities present in images. In this section, we briefly discuss the implementation of NSST and the readers can find the complete details in [31] . In fact, shearlet is an expansion of composite wavelets introduced in [40] . Consider the 2-D affine system generated by \u03c5 \u2208 L 2 (R 2 ), a collection of functions obtained by applying dilation and translation of the form:\nwhere \nThe generating function \u03c5 is a well-localized band-limited function and adequately meets the admissibility condition [41] associated with the shearlet approach so that x \u2208 L 2 (R 2 ) can be reconstructed by\nFor any \u03c9 = (\u03c9 1 , \u03c9 2 ) \u2208 R 2 , \u03c9 1 = 0, let \u03d2 is chosen to be of the form:\nwhere \u03d2 1 and \u03d2 2 are smooth functions in which the supports are the proper subset of [(\u22122, \u2212 \nwhere \u03d2 a,s,t (\u03c9 1 , \u03c9 2 ) has frequency support in which \u03c9 1 is in the set [(\u2212\nShearlet is a function of three parameters, namely, the scale a, the shear s, and, the translation t.\nThe discrete shearlet transform is obtained by discretizing the CST SH \u03c5 x(a, s, t) = x, \u03c5 a,s,t on suitable discrete set (scaling, shear, and translation parameters) [42] . NSST is a special type of discrete shearlet transform and its decomposition is shown in Fig. 2 . This figure depicts a two-scale (a = 2) decomposition, where the shear parameter s is chosen to be 8 and 4 at scales 1 and 2, respectively. The basic structure of NSST includes the nonsubsampled Laplacian pyramid (NSLP) transform, which establishes the multiscale property, in combination with shearing filters (SF) that offer a higher amount of directional information. Here, the use of nonsubsampled filters, without upsamplers or downsamplers, ensure the invariance property to the shift of the input functions. Given an image I of size N \u00d7 N , the procedure for finding the NSST at fixed scale l is summarized below [31] . As an example, the shearing filters (wedge shaped) computed using the Meyer wavelet function are shown in "}, {"section_title": "III. THE FORWARD LIKELIHOOD MODEL", "text": "Let f i (x, y) represent the unknown heterogeneous features such as structural and functional information of the brain, which are acquired by M different sensors and g i (x, y) the observed measurements. We consider neuroimaging fusion as the inverse of a linear forward model [22] that relates f i (x, y) and the observed brain images g i (x, y) by the following:\nwhere n i (x, y) is the noise associated with the i th sensor at location (x, y). In general, signal fluctuations originate in the physical processes of imaging rather than in the tissue textures. In MRI, the acquired real and imaginary data in the frequency domain are known to be corrupted by white noise having Gaussian probability distribution. The frequency domain data is transformed into Cartesian domain via inverse Fourier transform (IFT). After IFT, the real and imaginary parts are still corrupted by Gaussian noise, since the transform is linear and orthogonal. However, in reconstructed PET images, unlike MRI, the exact nature of the noise propagation is not well known. Hence, in this model, we assume that the target brain image is corrupted by additive white Gaussian noise with zero mean and a known standard deviation \u03c3 n on each acquisition. Also, it is necessary that the input images are spatially aligned for developing image fusion algorithms [43] . Then, we apply non-subsampled shearlet transform (NSST) to all the observed images g i (x, y) to be fused. In general, if an image is decomposed into k scales and D(k) denotes the number of directions chosen at scale k, the NSST transform provides a low frequency subband at the finest scale and D(k) high frequency subbands at each scale k, in which the size of the subbands are similar to the size of the input image. We can write the model in (6) by an equivalent frequency domain representation:\nwhere\ni (u, v) for the high frequency subbands where\nwhere the terms in the upper case letters are the NSSTs of the corresponding terms in (7) and M , K and D(k) denote the number of source images, number of scales and number of directions at each scale, respectively. The goal of the proposed neuroimaging fusion algorithm is to estimate the fused coefficients that combine the most significant information of F k,d i (u, v) from the NSST coefficients of the given brain images, g i (x, y). Since we assume the probabilistic model associated with noisy NSST coefficients,\nis Gaussian, the conditional probability is given by: We use a robust median estimator [44] to estimate the noise variance \u03c3 2 N i (u,v) ) which is given by:\nwhere MAD is the median absolute deviation and G i (u, v) denotes the NSST coefficients of the observed neuroimaging data."}, {"section_title": "IV. PROPOSED NEUROIMAGING FUSION ALGORITHM", "text": "In this section, we compute the fused NSST coefficients statistically using Bayes' rule with some knowledge of the prior distribution. "}, {"section_title": "A. STATISTICAL MODELING OF NSST COEFFICIENTS OF NEUROIMAGING DATA", "text": "At first, we propose to use the student's t location-scale PDF as a prior for modeling the heavy tailed nature of the NSST coefficients corresponding to the unknown target brain images under the assumption of independent identically distributed subbands [7] . Let x be a student's t location-scale distributed random variable; its PDF is described by\nwhere \u03c3 t > 0 is the scale parameter, \u00b5 t is the location parameter and \u03bd > 1 is the shape parameter, which determine the nature of the distribution. The shape parameter, \u03bd is the most important parameter in controlling the shape of the distribution. When \u03bd > 0, the three-parameter student's t locationscale distribution behaves like a traditional one parameter student's t distribution. When \u03bd \u2192 \u221e, it approaches the Gaussian distribution. The smaller values of the shape parameter yields a sharp peak around zero. To validate the behavior of the proposed prior PDF qualitatively, we examine the cumulative distribution function of the empirical real NSST coefficients with the student's t location-scale and the generalized Gaussian (GG) PDF's of the neuroimaging data. We examine the statistical properties of NSST coefficients of different multimodal neuroimaging data. Fig. 6 emphasizes the modeling performance of the real NSST coefficients of MR neuroimaging data of an AD patient. In addition, Fig.7 shows the qualitative fitting results in terms of the P-P plot of empirical, student's t location-scale and GG for four NSST detail subbands of multimodal neuroimaging data such as MR, FDG-PET, and SPECT images. The above two figures show that the student's t location-scale distribution provides a better fit in comparison with that provided by the GG distribution."}, {"section_title": "1) PARAMETER ESTIMATION", "text": "In order to use student's t location scale distribution as a prior for estimating the fused coefficients, it is necessary to estimate the parameters from the NSST coefficients of the neuroimaging data. We employ the maximum likelihood estimation method [45] for learning the location, scale and shape parameters of the distribution. The maximum likelihood estimates are obtained through the expectation maximization (EM) algorithm [46] , where the location and scale parameters can be estimated as\nwhere\nUsing the values of \u00b5 and \u03c3 , the following equation\ncan be solved numerically to estimate the shape parameter. In summary, the EM algorithm for the maximum likelihood estimator is as follows: Given the estimates at iteration, k, the k + 1 th iteration of EM consists of the following two steps: expectation (E) step and maximization (M) step. The E step computes the expected value of the log likelihood, given the observed data and current estimate of the model parameters. The M step maximizes the resulting function with respect to the model parameters, yielding new estimates to be used in the next iteration. These steps are repeated until convergence criterion is satisfied. Furthermore, for quantitative analysis, we use the Jensen-Shannon divergence (JSD) [47] test to calculate the goodness of fit between the empirical and fitted distributions. The JSD metric, a symmetric form of Kullback-Leibler divergence (KLD), is a measure of similarity between two probability distributions and is defined by\nwhere p(x) is the PDF of the fitted random variable and q(x) is the PDF of the empirical data. The JSD values corresponding to the student's t location-scale and GG distributions in Fig. 6 are 0.0030 and 0.0078, respectively. Table 1 shows the comparison between GG and student's t location-scale distributions by averaging the values of JSD metric computed over various subbands for different neuroimaging data. It can be seen that the student's t location-scale fits the empirical cumulative distribution function better than the popularlyused GG distribution does."}, {"section_title": "B. FUSION DECISION RULE", "text": "In Section IV(A), we have shown the modeling ability of student's t location-scale distribution for the NSST coefficients of neuroimaging data. Therefore, after estimating the parameters, this well approximated distribution is used as a prior PDF to estimate the noise-free coefficients using MAP estimator in which we find F k,d\ni (u, v) that maximizes the posterior PDF p( (u,v) ). Since the approximation coefficients, F L i (u, v) deal with low frequency information, i.e., the coarse representation of the image, we follow averaging operation to combine the coefficients of the low frequency subbands of the various images [48] . The detailed coefficients contain high frequency information including important details such as edges and corners present in the images in different directions.\nAs the coefficients of the high frequency subbands are complex numbers, one of the most commonly used procedures for finding the decision map is the absolute maximum selection rule [49] . According to this rule, when two images are fused, the fused coefficient\nare the coefficients located at (u, v) in the subband at the k th scale and in the d th direction of the images 1 and 2, respectively. Thus, in the absolute maximum selection rule, the energy of the fused image at the position (u, v) is that of image 1 or image 2 at that position depending on the one that is larger. Thus, using this rule, the energy level of the entire fused image is, in general, larger than that of either of the two images used for fusion. The improvement in the quality of the fused image over that of the individual images can be attributed to the increased level of its energy. In order to further improve the quality of the fused image, one should be investigating an approach that will increase the energy level at its individual pixel position even further. For this purpose, we now propose the following fusion rule.\niI (u, v) be the transform domain coefficient located at (u, v) in the high frequency subband at the k th scale and in the d th direction of the image\nfI (u, v) be the corresponding coefficient at (u, v) in the subband at the k th scale and in the d th direction of the fused image. The fusion rule is defined as\nand\nwhere the decision maps, W iR (u, v) and W iI (u, v) are defined by (19) , as given at the bottom of this page. It is to be mentioned that if there is more than one value of i that satisfies i = \u00b5 (or i = \u03b3 ), then we choose any one of the i values as \u00b5 (or \u03b3 ) and let the corresponding weight to '1', and the rest of the weights to '0'. Thus, in the proposed fusion rule, the real and imaginary parts of the fused coefficient It can easily be seen that, in general,\nfor all values of i. Thus, the total energy in the high frequency subbands using the proposed fusion rule will always be greater than that obtained using absolute maximum selection rule. We will refer to the proposed fusion rule as the energy maximizing fusion rule."}, {"section_title": "C. IMPLEMENTATION STEPS", "text": "In this subsection, we summarize the main steps involved in the proposed medical image fusion algorithm. 1) Apply the NSST transform on the individual source image. 2) Estimate the parameters \u00b5 t , \u03c3 t , \u03bd from the NSST coefficients of the each input image using the method described in Section IV(A). 3) Apply the fusion rule on the NSST coefficients as explained in Section IV(B). 4) Apply the inverse NSST to the fused coefficients obtained in Step 3."}, {"section_title": "V. EXPERIMENTAL RESULTS AND DISCUSSION", "text": "In this section, we study the performance of the proposed technique on different pairs of 2-D multimodal synthetic and real brain images. Also, we compare the performance of the proposed method with that of five other methods, namely, curvelet transform-based (CTB) method [20] , contourlet transform-based (CB) method [19] , guided image filter-based (GIFB) method [6] , local Laplacian filtering domain-based (LLDB) method [5] , and parameter-adaptive pulse coupled neural network in nonsubsampled shearlet transform-based (PA-PCNN-NSSTB) method [4] . For a fair comparison, we choose a number of pairs of images from various databases (BrainWeb [37] , Alzheimer's disease neuroimaging initiative (ADNI) [38] , and Whole Brain Atlas (WBA) [39] ), and obtain the fusion results for each of the methods mentioned above as well as for the proposed method. For the methods of [5] , [6] , we have used the codes published by the authors to obtain the fusion results, whereas for the other methods [4] , [19] , [20] , we have written codes in Matlab. For the purpose of obtaining the experimental results, we select two scales for the contourlet and non-subsampled shearlet transforms to decompose the source images, where for the first scale six directions and for the second scale ten directions are chosen. Thus, in total, we have 16 high frequency subbands and one low frequency subband. For quantitative analysis, five performance metrics, peak signal to noise ratio (PSNR), structural similarity (SSIM), mutual information (MI), entropy and Q f 1 f 2 F are used, and these are briefly defined next. The SSIM measure is a function of three independent components, namely, luminance, contrast, and structure.\nwhere \u03b1, \u03b2, and \u03b3 are the quantities used to calibrate the relative importance of these particular components. We choose \u03b1 = \u03b2 = \u03b3 = 1; the readers are referred to [50] concerning the details on the choice of the parameters and the steps involved in the calculation of the SSIM metric. In the context of image fusion, the MI metric measures the amount of information the fused image conveys about each of the source images. Considering two input images f 1 (x, y) and f 2 (x, y) and a fused image F(x, y), we can calculate how much information F(x, y) holds from f 1 (x, y) and f 2 (x, y) based on Kullback-Leibler measure in (11)\nwhere p Ff k (i, j) is the joint histogram of images F(x, y) and f k (x, y), p F and p f k are the normalized histograms of F(x, y) and f k (x, y), respectively, and L is the number of intensity values. Thus, the image fusion MI measure is defined by simply taking the average of the composite image F(x, y) and each of the input images f 1 (x, y), and f 2 (x, y), i.e.,\nHigher the value of MI better the quality of the fused image. Another metric that is used to measure the quality of fusion is normalized weighted performance metric Q f 1 f 2 F . It calculates the amount of edge information transfers from each of the input images into the fused image. The normalized weighted performance measure Q f 1 f 2 F of a given fused image F that operates on input images f 1 and f 2 is defined as follows:\nwhere w f 1 (m, n) and w f 2 (m, n) are the edge strength values of the input images f 1 and f 2 , respectively, at each pixel location (x,y) and are used as weight factors in (24) . Q f 1 F (x, y) and Q f 2 F (x, y) are the edge information preservation values of f 1 and f 2 , respectively, in the fused image F at each pixel location (x, y). Each of these quantities is the product of the edge strength preservation values and the orientation preservation values calculated by using the Sobel operator [51] , [52] . The value of the normalized weighted performance measure Q f 1 f 2 F ranges between 0 (complete loss of edge information) and 1 (no loss of edge information) [52] . As explained in [17] , for color images, we first calculate the performance measures SSIM, MI, and Q f 1 f 2 F of an individual channel separately using (21), (23) and (25) , respectively, followed by averaging out over the number of channels. As an example, for color images "}, {"section_title": "(a) and (b) correspond to MR-T1 and MR-PD images of a normal brain, respectively. (c), (d), (e), (f), (g)", "text": ", and (h) correspond to the fused images using curvelet-based [20] , contourlet-based [19] , guided image filter-based [6] , local Laplacian filtering domain-based [5] , parameter-adaptive pulse coupled neural network and nonsubsampled shearlet-based [4] , and proposed methods, respectively. (d) , (e), (f), (g), and (h) correspond to the fused images using curvelet-based [20] , contourlet-based [19] , guided image filter-based [6] , local Laplacian filtering domain-based [5] , parameter-adaptive pulse coupled neural network and nonsubsampled shearlet-based [4] , and proposed methods, respectively. [20] , contourlet-based [19] , guided image filter-based [6] , local Laplacian filtering domain-based [5] , parameter-adaptive pulse coupled neural network and nonsubsampled shearlet-based [4] , and proposed methods, respectively. such as RGB, we calculate MI by considering the red, green and blue color components of each individual pixel so that (23) becomes"}, {"section_title": "FIGURE 11. Multimodal fusion results of 2-D simulated neuroimages with 40% intensity non-uniformity level and 5% noise taken from the brainWeb database. (a) MR-T1 image. (b) MR-PD image. (c), (d), (e), (f), (g), and (h) correspond to the fused images using curvelet-based", "text": "The other common performance measures, PSNR and entropy, are also calculated in the same manner for the color images. We first investigate the energy values obtained in each high frequency subband when the absolute maximum selection rule and the proposed energy maximizing fusion rule are applied. For this purpose, we consider Figs. 8 (a) and (b) taken from the WBA database. It is found that the energy level of each subband is increased using the proposed fusion rule, with a total increase of 7.15% over that using the absolute maximum selection rule. We now compare the performance of our method that involves student's t location scale modeling using the proposed fusion rule with that using the conventional absolute maximum selection rule. The objective quality of the fused images, using the two rules, as measured by the various metrics, is summarized in Table 2 . It is seen from this table that there is an improvement in the values of every one of the metrics indicating the performance using the proposed fusion rule is superior to that using the absolute maximum selection rule. This improved performance is further reinforced by the fused images shown in Figs. 8 (c) and (d) using the absolute maximum selection and proposed fusion rules, respectively. This improvement is clearly due to the use of the proposed fusion rule, which retains important high frequency details such as edges, corners, blobs and ridges better than the conventional absolute maximum selection rule does. Hence, in all our subsequent experiments, fusion is carried out using the novel energy maximizing fusion rule."}, {"section_title": "A. EXPERIMENTAL RESULTS USING SIMULATED DATA", "text": "To assess the performance of our proposed method, we carry out extensive experiments on pairs of simulated brain MR images taken from the BrainWeb database. For this purpose, we consider slices along the axial plane from the longitudinal relaxation (T1)-weighted, transverse relaxation (T2)-weighted, and proton density (PD)-weighted MR brain volume with various slice thicknesses, intensity nonuniformity levels, and noise. Fig. 9 illustrates the results of fusion of T1-weighted MR (MR-T1) and PD-weighted MR (MR-PD) images of a normal brain. The objective quality of the fused images obtained for the various methods as measured by the different metrics is summarized in Table 3 . It is seen from this table that the proposed method yields the highest values in all the measures considered. This is further reinforced by observing the visual quality of the fused images Fig. 9 and Fig. 10. shown in Figs.9 (c) -(h) using the curvelet-based, contourletbased, guided image filter-based, local Laplacian filtering domain-based, parameter-adaptive pulse coupled neural network in nonsubsampled shearlet transform-based, and proposed methods, respectively. Fig. 10 illustrates the results of fusion of T2-weighted MR (MR-T2) and PD-weighted MR (MR-PD) images of a multiple sclerosis brain. The objective quality of the fused images obtained for the various methods as measured by the metrics is also summarized in Table 3 . It is also seen from this table that the proposed method yields the highest values in all the measures considered. This is further reinforced by observing the visual quality of the fused images shown in Figs.10 (c)-(h) using the curvelet-based, contourletbased, guided image filter-based, local Laplacian filtering domain-based, parameter-adaptive pulse coupled neural network in nonsubsampled shearlet transform-based, and proposed methods, respectively. From these two figures, it is clear that the proposed method is more efficient in transferring the structural details from the MR images of T1/T2 and PD weighting techniques compared to that obtained using other traditional methods.\nIn addition, we now consider the same two MR images shown in Figs. 9 (a) and (b), but with 40% intensity nonuniformity level and containing 1% or 5% or 9% noise. The noise in the simulated scans has Rayleigh statistics in the background and Rician statistics in the signal regions [37] . The fusion results as measured by the various metrics is summarized in Table 4 . It is seen from this table that the proposed method has the highest values in all of the five performance metrics considered. This is further reinforced by the subjective fusion results shown in Fig. 11 , when the noise is 5%. It is observed from the fused images that even in the noisy case, unlike the other traditional methods, the proposed method could retain most of the structural information of the source images in the fused image, and provide a better image quality. Thus, the proposed method is more robust than the other traditional methods."}, {"section_title": "B. EXPERIMENTAL RESULTS USING REAL DATA", "text": "In this subsection, we carry out extensive experiments on th fusion of real neuroimaging data taken from Alzheimer's Disease Neuroimaging Initiative (ADNI) [38] and Whole Brain Atlas (WBA) [39] databases. "}, {"section_title": "1) FUSION OF TWO MR IMAGES TAKEN FROM ADNI DATABASE", "text": "We now consider the fusion of a pair of real MR images taken from the Alzheimer's disease neuroimaging initiative (ADNI) database. The images in Fig. 12 (a) and (b) are the slices along the axial plane from the MR-T1 and MR-PD volumes, respectively. Table 5 lists the objective fusion results for various methods. It is observed from this table that the proposed method outperforms all the other methods in terms of all the measures considered. In particular, it has a significantly higher entropy as well as a normalized weighted performance metric, Q f 1 f 2 F . This is further reinforced by observing the visual quality of the fused images shown in Figs. (c)-(h) using the curvelet-based, contourlet-based, guided image filter-based, local Laplacian filtering domainbased, parameter-adaptive pulse coupled neural network in nonsubsampled shearlet transform-based, and proposed methods, respectively. It is seen from these fused images that the proposed technique preserves the structural details from both the T2 and PD weighting techniques better than the other traditional methods do."}, {"section_title": "2) FUSION OF TWO MR IMAGES TAKEN FROM WBA DATABASE", "text": "We now consider the fusion of all possible pairs of MR-T1, MR-T2, and MR-PD images of a lyme encephalopathy patient and a mild Alzheimer's disease patient taken from the Whole Brain Atlas (WBA) database. Table 6 lists the objective performance measures for the three pairs of 2-D MR images of the lyme encephalopathy patient, while (d) , (e), (f), and (g) correspond to the fused images using curvelet-based [20] , contourlet-based [19] , guided image filter-based [6] , local Laplacian filtering domain-based [5] , parameter-adaptive pulse coupled neural network and nonsubsampled shearlet-based [4] , and proposed methods, respectively. Fig. 13. patient. It is seen from these tables that the proposed technique outperforms all the other methods in terms of all the measures considered. This is further reinforced by the subjective fusion results shown in Fig. 13 for the lyme encephalopathy patient and the corresponding results shown in Fig.14 for the mild Alzheimer's disease patient. It is seen from these fused images that the proposed technique preserves the structural details of the source images for all the three pairs better than the other traditional methods do."}, {"section_title": "TABLE 6. Objective fusion results of 2-D neuroimages in", "text": ""}, {"section_title": "3) FUSION OF AN MR IMAGE WITH SPECT OR PET IMAGE TAKEN FROM WBA DATABASE", "text": "We now consider the fusion of four pairs of multimodal brain images taken from the Whole Brain Atlas (WBA) database. The various pairs of images are shown in Fig. 15 . The pair of images in Figs. 15(a) and (e) correspond to T2-weighted magnetic resonance imaging (MR-T2) and positron emission tomography using F-18-fluorodeoxy-glucose (FDG-PET) from a 70 year-old man with mild Alzheimer's disease, respectively. The pair of images in Figs. 15(b) and (f) correspond to MR-T2 and single photon emission computed tomography with Thallium-201 (SPECT-T1) from a 51 year old woman with Anaplastic Astrocytoma, a type of rare malignant tumor, respectively. The pair of images in Figs. 15(c) and (g) are that of the T1-weighted MR (MR-T1), after gadolinium-diethylenetriamine pentaacetic acid (Gd-DTPA)and FDG-PET from a 53 year old man with Astrocytoma, a type of cancer of the brain, respectively. (c) , respectively. Here, columns 1 to 6 correspond to the fused images of curvelet-based [20] , contourlet-based [19] , guided image filter-based [6] , local Laplacian filtering domain-based [5] , parameter-adaptive pulse coupled neural network and nonsubsampled shearlet-based [4] , and proposed methods, respectively.\nFinally, the pair of images in Figs. 15 (d) and (h) are MR-T2 and SPECT with perfusion agent Tc99m-HM-PAO (SPECT-Tc) from a 76 year old man with Subdural Hygroma, accumulation of cerebrospinal fluid in the subdural membrane. The objective performance measures obtained for each of the four pairs of 2-D neuroimages using various fusion methods are summarized in Table 8 . It is seen from this table that the proposed method consistently outperforms all the other methods in terms of all the objective measures considered and in particular, it has significantly higher values of the normalized weighted performance metric, Q f 1 f 2 F , signifying that the proposed fusion algorithm conveys the gradient information without any significant loss.\nThe fused images for each of the four pairs using the curveletbased method are shown in Figs.15 (i)-(l) , the corresponding fused images using the contourlet-based method are shown in Figs.15 (m)-(p) , those using the guided image filter-based are shown in Figs.15 (q)-(t) , those using the local Laplacian filtering domain-based are shown in Figs.15(u)-(x) , and those using the parameter-adaptive pulse coupled neural network in nonsubsampled shearlet transform-based are shown in Figs.15 (y)-(ab) . The fused images using the proposed method for each of the four pairs are shown in Figs.15 (ac)-(af) . In order to appreciate the effectiveness of the proposed method over that of the other methods in preserving both the structural and functional information of (c) , respectively. Here, columns 1 to 6 correspond to the fused images of curvelet-based [20] , contourlet-based [19] , guided image filter-based [6] , local Laplacian filtering domain-based [5] , parameter-adaptive pulse coupled neural network and nonsubsampled shearlet-based [4] , and proposed methods, respectively. the two modalities of each pair of images, a small segment (red box as shown in Figs.15 (a)-(h) ) of each pair of the original images and the corresponding segments of the fused images obtained by various methods are all zoomed in and the resulting zoomed segments are shown in Fig. 16 . It is clearly seen from Fig.16 that the proposed technique preserves the structural features such as the edge information in MR and the functional features such as the color information in both PET and SPECT images better than the other existing techniques do. This is in view of the fact the NSST used in our approach provides optimal representation of neuroimaging data with edges and texture, and further, the student's t location-scale distribution, which acts a prior distribution, closely fits the empirical NSST coefficients of source images; more importantly, it is due to the proposed fusion rule, which increases the overall high frequency subband energy. Also, it is noticed that the guided image filter scheme produces blocking artifacts reflecting the lack of sufficient directional information. Although the local Laplacian filtering scheme provides good contrast and no blocking artifacts, the functional information is not well preserved as observed from the color details in Fig.16 (v) and some noise is introduced as seen from Fig.16 (u) . Also, it is noticed from the fused image, shown in Fig. 16(ad) , that substantial structural information is lost in lieu of functional superimposition. However, only a subset of the large region of a mixed signal on MR-T2 corresponds [20] , contourlet-based [19] , guided image filter-based [6] , local Laplacian filtering domain-based [5] , parameter-adaptive pulse coupled neural network and nonsubsampled shearlet-based [4] , and proposed methods, respectively. Fig.14 (a) -(h) ] of all the pairs of neuroimages in Fig.12 . The four images in each row correspond to the enlarged area of the images in Fig.14. to the active tumor which is difficult to localize. The thallium uptake of tumor region in SPECT has been shown to increase the sensitivity and specificity of tumor detection over MRI. Hence, fusing these two images provides clinicians with a single image that combines higher sensitivity of the color SPECT image with the improved localization of the suspected details in the MRI image. We next consider the fusion of an MR image and SPECT image of a lyme encephalopathy patient taken from the WBA database."}, {"section_title": "4) FUSION OF AN MR IMAGE WITH THE SPECT IMAGE OF A LYME ENCEPHALOPATHY PATIENT", "text": "We now consider fusing each of the MR images of a lyme encephalopathy patient shown in Figs. 18 (a), (b) , and (c) with the functional SPECT image shown in Fig. 18 (d) . Figs.18(a), (b) , and (c) are, respectively, the same as Figs. 13 (a), (b) , and (c), and are repeated here for convenience. Table 9 lists the corresponding objective fusion results. The corresponding subjective fusion results are shown in Fig. 18 . It is clear from these results that the proposed method performs better than the traditional methods do in terms of both the objective and subjective fusion results.\nWe next consider the fusion of an MR image and FDG-PET image of a mild Alzheimer's disease patient taken from the WBA database. [20] , contourlet-based [19] , guided image filter-based [6] , local Laplacian filtering domain-based [5] , parameter-adaptive pulse coupled neural network and nonsubsampled shearlet-based [4] , and proposed methods, respectively. NOTE: Figs. 18(a), (b) , and (c) are, respectively, the same as Figs. 13 (a), (b) , and (c), and are repeated here for convenience. [20] , contourlet-based [19] , guided image filter-based [6] , local Laplacian filtering domain-based [5] , parameter-adaptive pulse coupled neural network and nonsubsampled shearlet-based [4] , and proposed methods, respectively. NOTE: Figs. 19 (a), (b) , and (c) are, respectively, the same as Figs. 14 (a), (b) , and (c), and are repeated here for convenience. "}, {"section_title": "5) FUSION OF AN MR IMAGE WITH THE FDG-PET IMAGE OF A MILD ALZHEIMER'S DISEASE PATIENT", "text": "We now consider fusing each of the MR images of a mild Alzheimer's disease patient shown in Figs. 19 (a), (b) , and (c) with the functional FDG-PET data shown in Fig. 19 (d) . Figs. 19(a), (b) , and (c) are, respectively, the same as Figs. 14 (a), (b), and (c), and are repeated here for convenience. Table 10 lists the corresponding objective fusion results. The corresponding subjective fusion results are shown in Fig. 19 . It is clear from these results that the proposed method performs better than the traditional methods do in terms of both the objective and subjective fusion results."}, {"section_title": "6) EXPERIMENTS ON FUSING MORE THAN TWO IMAGES TAKEN FROM WBA DATABASE", "text": "We now consider the fusion of three 2-D brain images from the Whole Brain Atlas database (WBA) using the proposed method. Since data is not available for considering the fusion of more than two functional images, we have considered only the fusion of three MR images that reveal structural information.\nTo compare the performance of the proposed method on fusing three images with that of fusing two images, we choose the source images, MR-T1, MR-T2, and MR-PD shown in Fig.13 (a), (b) , and (c), respectively, of the lyme encephalopathy patient. The objective quality of the fused image as measured by the various metrics is summarized in Table 11 . By comparing these results with that shown in Table 6 , it is observed that the values of the various metrics when all the three images are fused are higher than those when any two of the three images are fused. This is further confirmed by the subjective fusion results. Fig. 20 shows the fusion results of the three multimodal MR images. It is clearly seen from Fig. 20 (d) that the signal intensities are well enhanced when all the different weighting techniques, namely, T1, T2, and PD of MR images are considered for fusion unlike fusing only any two different weighting techniques of MR images as in Fig. 13 . Similar experiments are carried out for the source images, MR-T1, MR-T2, and MR-PD shown in Fig.14 (a), (b) , and (c), respectively, of the mild Alzheimer's disease patient. The corresponding objective fusion results are given in Table 11 and the subjective fusion results are shown in Fig.21 .\nWe next consider the fusion of two MR images and one PET/SPECT image. To compare the performance of the proposed method on fusing three images with that of fusing two images, we choose the source images, MR-T1, MR-T2, MR-PD, and SPECT shown in Fig.18 (a), (b) , (c), and (d), respectively, of the lyme encephalopathy patient. The combinations are MR-T1/MR-T2/SPECT, MR-T1/ MR-PD/SPECT, MR-T2/MR-PD/SPECT. The objective quality of the fused images obtained using as measured by the various metrics is summarized in Table 12 . By comparing these results with that shown in Table 9 , it is observed that the values of four of the five metrics considered are higher than those when any one of the MR images and the SPECT image are fused at a time. The subjective fusion results are shown in Fig. 22 . It is clear from the fused images that the features are well enhanced when any two of the MR images and the SPECT image are fused unlike fusing any one of the MR images and SPECT image. Similar experiments are carried out for the source images, MR-T1, MR-T2, MR-PD, and PET images shown in Fig. 19 (a), (b) , (c), and (d) respectively, of the mild Alzheimer's disease patient. The corresponding objective fusion results are given in Table 12 . By comparing features are well enhanced when any two of the MR images and the PET image are fused unlike fusing any one of the MR images and PET image.\nFinally, we consider the fusion of four input images, MR-T1, MR-T2, MR-PD, and SPECT, shown in Figs. 22 (a), (b) , (c), and (d), respectively, of the lyme encephalopathy patient. The objective quality of the fused image obtained using as measured by the various metrics is summarized in Table 13 . By comparing these results with that shown in Tables 9 and 12 , it is observed that the proposed method yields the highest values in all the measures considered except the mutual information and entropy and measures, when any one or two of the MR images and the SPECT image are fused. We believe that these scores could be further increased by considering more number of functional images. We also notice that the proposed technique boosts the normalized weighted performance metric, Q f 1 f 2 F as the number of images to be fused increases. Fig. 24 (a) is the subjective fusion result. It is clear from the fused image that when we integrate the structural features from different MR images along with the functional data, the features are well enhanced unlike having only two or three images. Similar experiments are carried out for the input images, MR-T1, MR-T2, MR-PD, and PET, shown in Figs. 23 (a), (b) , (c), and (d), respectively, of the mild Alzheimer's disease patient. The corresponding objective fusion results are given in Table 13 and the subjective fusion result is shown in Fig. 24 (b) .\nAs mentioned earlier, in our experiments we have chosen two scales, one with six and the other with ten directions. However, the performance could be further improved by increasing the number of scales and directions at the expense of increased complexity. In future, a study could be undertaken to evaluate the trade-off between the fusion performance and the computational complexity resulting from different numbers of scales and directions."}, {"section_title": "VI. CONCLUSION", "text": "In this paper, for the first time using the concept of energy maximization in high frequency subbands, a novel method for multimodal neuroimaging fusion has been proposed. The proposed method has employed the NSST transforms of the images whose coefficients are modeled using student's t location-scale distribution and fused using the novel energy maximizing fusion rule. It has been shown that this distribution results in a better approximation of the empirical NSST coefficients of the neuroimaging data than the traditional generalized Gaussian distribution does. A Bayesian MAP estimator has been devised by using this distribution as a prior in order to obtain the noise-free coefficients of the images. The new fusion rule is based on the idea of maximizing the total energy of the high frequency subbands in the fused image. This has been achieved by separately fusing the real and imaginary parts of the corresponding high frequency coefficients of the images being fused.\nExperiments have been carried out to assess the performance of the proposed method. Based on our results using both synthetic and real brain data, it has been shown that the proposed method significantly outperforms the other traditional methods in terms of all the performance measures considered, namely, peak signal to noise ratio (PSNR), structural similarity (SSIM), mutual information (MI), entropy and Q f 1 f 2 F . In addition, from the subjective results, it is clear that the proposed method preserves the structural and functional details better than the other existing techniques do. It is also evident that the traditional challenges such as blocking artifacts, noisy patches, and loss of significant attributes have been overcome and hence, the proposed method is more robust, thus producing fused images of higher quality. The proposed method is completely general in that it can be applied to any number of neuroimages, subject to their availability. It has been found that the signal intensities in the fused images are better enhanced when more number of source images are being fused.\nThe improved performance of the proposed method is due to the fact that the NSST transform captures the image details in different scales and directions, and the student's t locationscale PDF, which acts as a prior for Bayesian inference-based fusion, approximates the NSST coefficients very well, and more importantly in view of the maximization of energy in the high frequency subbands using the proposed fusion rule. We believe that the framework developed here for multimodal neuroimaging fusion will help in the study of brain diseases, such as Alzheimer's, epilepsy, and multiple sclerosis."}]