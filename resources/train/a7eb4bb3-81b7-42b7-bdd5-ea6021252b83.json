[{"section_title": "Abstract", "text": "A plethora of deep learning models have been developed for the task of Alzheimer's disease classification from brain MRI scans. Many of these models report high performance, achieving three-class classification accuracy of up to 95%. However, it is common for these studies to draw performance comparisons between models that are trained on different subsets of a dataset or use varying imaging preprocessing techniques, making it difficult to objectively assess model performance. Furthermore, many of these works do not provide details such as hyperparameters, the specific MRI scans used, or their source code, making it difficult to replicate their experiments. To address these concerns, we present a comprehensive study of some of the deep learning methods and architectures on the full set of images available from ADNI. We find that (1) classification using 3D models gives an improvement of 1% in our setup, at the cost of significantly longer training time and more computation power, (2) with our dataset, pre-training yields minimal (< 0.5%) improvement in model performance, (3) most popular convolutional neural network models yield similar performance when compared to each other. Lastly, we briefly compare the effects of two image preprocessing programs: FreeSurfer and Clinica, and find that the spatially normalized and segmented outputs from Clinica increased the accuracy of model prediction from 63% to 89% when compared to FreeSurfer images."}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD) is a degenerative brain disease and the most common cause of dementia. It is a burdensome and costly disease that affects 5.5 million people in the United States. The total value of care provided to AD patients is estimated to be around $250 billion a year (Association et al., 2017) .\nThe symptoms of AD include memory loss, challenges in planning, difficulty completing simple tasks, and decline in other cognitive skills that often lead to a dependency on external care, which takes a toll on both the family of the individual with AD, as well as society as a whole.\nThe disease is characterized by the degeneration of specific nerve cells, presence of neuritic plaques outside the neurons, and the accumulation of neurofibillary tangles inside the neurons (McKhann et al., 1984) , where the latter two proteins lead to cell death by interfering with various functions of the cell (Association et al., 2017) . As a result, brains of people with advanced AD show inflammation, dramatic shrinkage from cell loss, and widespread debris from dead or dying neurons (Association et al., 2017) .\nWhile no cure exists for the disease yet, there is consensus on the need and benefit for early diagnosis of AD. For example, an overwhelming percentage (80%) of older adults wish to know as early as possible if they have AD (Dale et al., 2008) , and early diagnosis may help caregivers of AD patients feel more competent in caring for the patients, as well as increasing the possibility that the caregiver can involve the patient in making medical, financial, legal, and care decisions (de Vugt and Verhey, 2013) . Furthermore, Weimer and Sager (2009) used Monte Carlo cost-benefit analysis to suggest potential social and financial benefits of early detection.\nDiagnosis of AD is typically done with a comprehensive evaluation of the patient, which may include tests of memory, problem solving, and attention, as well as brain scans using computed tomography (CT), magnetic resonance imaging (MRI), or positron emission tomography (PET) scans.\nRecently, machine learning techniques, specifically deep learning, show great potential in aiding the diagnosis of AD using MRI scans. A variety of neural network architectures have been proposed, some operate on 2D images (Hon and Khan, 2017) (Sarraf et al., 2017) , while others operate on 3D MRI scans (Wang et al., 2018 ) (Hosseini-Asl et al., 2016) (Payan and Montana, 2015) . Because of the relatively small number of available MRI scans, many proposed architectures (Payan and Montana, 2015) (Hosseini-Asl et al., 2016) also incorporate an auto-encoder architecture with tied weights to pre-train the model on a reconstruction task before the classification task to prevent over-fitting on the data.\nHowever, while there is a sizable number of studies on this topic, for people who wish to utilize a discriminitive MRI feature extractor for other downstream AD-related tasks, it is still difficult to glean insights that would be helpful for this task. For example, it is not clear how much performance is gained by using a 3D convolutional neural network (CNN) to process the MRI, which has a high computation cost, compared to using a simple 2D model and processing only one slice, or multiple slices of the MRI. It is also not clear how much pre-training the model helps with raising the performance of the model, as Gupta et al. (2013) , Payan and Montana (2015) , Hosseini-Asl et al. (2016) have done. Finally, it is also not clear how much the various model architectures contribute to the high accuracy that various authors have claimed in their paper.\nIn this study, we explore the aforementioned variables one must consider when selecting a discriminative CNN setup for AD classification, 1) the difference in performance between using 2D and 3D models; 2) pre-training the model on reconstruction task before training it on the classification task; and 3) the performance gain from using model X versus model Y."}, {"section_title": "Related Work", "text": "The idea of using computers to assist in the diagnosis of AD has been around before the current wave of enthusiasm for neural networks. Plant et al. (2010) used a feature selection algorithm to achieve 92% accuracy in a binary classification of AD (Alzheimer's disease) versus HC (Healthy Control). Jha et al. (2017) used a dual-tree complex wavelet transform to extract features (DTCWT) from the MRI scans, performed principle component analysis (PCA) to reduce the dimension of the extracted features, and a feed-forward neural network to classify between AD and HC individuals.\nEver since Krizhevsky et al. (2012) won the ImageNet challenge with a seven-layer convolutional neural network, surpassing the previous winner by a large margin of 10%, the computer vision community has mainly shifted from using hand-crafted features with support vector machines (SVM) or feed-forward neural networks, to large, end-to-end trainable neural networks with many layers. Gupta et al. (2013) trained a sparse auto-encoder on both natural images and MRI images to learn a set of bases or filters. The filters were then convolved with the target MRI to produce a set of features, which were then down-sampled using max-pooling before fed into a feed-forward network for classification. Similarly, Payan and Montana (2015) trained a sparse auto-encoder on randomly selected patches from the MRI scans to learn the features. They also used max-pooling to reduce the size of the features before introducing them into a feed-forward network.\nMore recently, Hon and Khan (2017) used the image entropy of each slice to decide which of the slices of the MRI scans to use. They then fed the slice through a pre-trained 2D VGG-16 network (Simonyan and Zisserman, 2014) , as well as Inception V4 network (Szegedy et al., 2015) , to obtain the final result. Wang et al. (2018 ) used DenseNet (Huang et al., 2017 and ensemble methods to classify the entire 3D MRI scan, leading to a new state-of-the-art three-class (Alzheimer versus Mild Cognitive Impairment versus Cognitively Normal) classification accuracy of 97.19%. Their study showed that around 2.5% increase in accuracy was attributed to applying ensemble method, which falls in-line with other studies that employed the method."}, {"section_title": "Methods", "text": "In this section, we detail the setup we used for this study, including the acquisition and preprocessing of the data, and the model architectures we use in our comparison. Figure 1 provides an overview of the various components our study will cover."}, {"section_title": "Data Acquisition", "text": "For this study, we obtained MRI scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI) 1 . Specifically, we are using MRI scans with the description \"MPR; GradWarp; B1 Correction; N3\", which we will henceforth refer to as \"preprocessed\" scans, as well as the FreeSurfer post-processed MRI scans with the description \"FreeSurfer Cross-Sectional Processing brainmask\" in the ADNI database. A total of 3415 \"preprocessed\" scans and 3177 FreeSurfer post-processed scans were obtained from the website. "}, {"section_title": "Data Preprocessing", "text": "Due to the prominence of extraneous parts of the head that are captured in MRI scans, which can potentially intorduce noise in the network, most studies use FSL 2 and/or Statistical Parameteric Mapping (SPM) 3 for the preprocessing of the MRI scans. FSL provides brain extraction and tissue segmentation functionality, and SPM realigns, spatially normalizes, and smoothes the scans.\nFor our experiments, we used the Clinica software platform 4 developed by the ARAMIS Lab 5 , which supports FSL, SPM, FreeSurfer, as well as a few other technologies. We are using the t1-volume pipeline, which is a wrapper of the \"segmentation\", \"run dartel\", and \"normalize to mni space\" routines implemented in SPM.\nThe inputs to the Clinica pipeline were the \"preprocessed\" scans from ADNI, and the outputs of the Clinica pipeline include spatially normalized gray matter, white matter, and cerebrospinal fluid segmentation maps, as well as a brain extraction output similar to the FreeSurfer post-processed scans from ADNI's dataset, with a difference being that the Clinica outputs are spatially normalized.\nFurthermore, we extract 2D slices from fixed indices for each view of the spatially normalized brain extraction output. The indices were chosen based on visual prominence of the hippocampus of the brain. Specifically, they were the 52nd slice of axial view, the 58th slice of sagittal view, and the 92nd slice of coronal view. We also experimented with using (c) The 52nd axial, 92nd coronal, and 58th sagittal view of the brain extraction outputs from Clinica, which we used for 2D classification. neighboring slices for classification. Figure 2 shows the 2D slices of the brain extraction output by Clinica."}, {"section_title": "Demographics", "text": "After processing the \"preprocessed\" scans with Clinica, we obtained 2023 sets of scans. Table 1 shows the demographics of the FreeSurfer post-processed scans, and Table 2 shows the demographics of the outputs from the Clinica pipeline. Note that the output from Clinica had fewer scans because some were lost in the process due to unknown reasons. Table 2 : Demographics for the output scans of the Clinica pipeline. Note that some of the scans were lost in the process due to unknown reasons."}, {"section_title": "Training / Validation / Testing Split", "text": "To evaluate the performance of our models, we split the data into a training set, validation set, and testing set, with a ratio of 6:2:2. We save the model with the lowest validation loss, and use that to obtain the final accuracy on the test set. For each run, we split the data into three classes: (AD, MCI, CN). We then perform shuffling on each of the splits. The first N scans of each class are used to create a balanced dataset, where N is the size of the smallest class. We then split each of the classes into training, validation, and testing set. See Section .1 in Appendix A for details on the procedure, and Table 3 for the final distribution for both the Clinica and FreeSurfer images.\nMany previous works such as Wang et al. (2018) , Hosseini-Asl et al. (2016), and Sarraf et al. (2017) , performed ten-fold cross-validation as a way of measuring the performance of their model. We decided to deviate from this setup because we are not performing hyperparameter search, and therefore require an extra set of data for final evaluation so we can use the validation set to guide early-stopping. To compensate for the reduced number of possible combinations of data that we can train, validate, and test in one run, we utilize the aforementioned shuffling scheme, and run each experiment four to six times to obtain a good randomized sample over the training, validation, and testing set. All of the 2D experiments were run six times, but we were only able to run the 3D experiments four times each due to the significantly longer time and greater computation power required."}, {"section_title": "2D Convolutional Neural Network Models", "text": "For classifying 2D slices of MRIs, we use an 18-layer residual network (He et al., 2016a), a 16-layer VGG network (Simonyan and Zisserman, 2014), a 121-layer DenseNet (Huang et al., 2017) , and the Inception V3 architecutre (Szegedy et al., 2016) . We believe that our selection of networks is representative of the most popular network architectures currently being used by the computer vision community, and the designs of many of the more complex networks we have come across, such as Wang et al. (2018)'s 3D DenseNet, borrow architectural ideas from these networks.\nThe models we selected are pre-built models from the PyTorch model zoo, and unless specified otherwise, the weights of the models are initialized from weights pre-trained on the ImageNet dataset. The last layer of each of the models was replaced with a linear layer that reduced the dimension of the extracted features of the convolutional layers down to three, each corresponding to one of three classes (AD/MCI/CN)."}, {"section_title": "3D Convolutional Neural Network Models", "text": "Because there are no pre-built 3D models available, we built a 16-layer (Table 4) and 22-layer (Table 5 ) CNN model following the architectural designs of the residual network (He et al., 2016a) . Specifically, we used the \"bottleneck\" configuration to reduce the number of filters in the inner layer of each residual block (He et al., 2016a) , and the \"full pre-activation\" layout (He et al., 2016b) for the residual blocks. The layout of each residual block can be found in Table 6 ."}, {"section_title": "Convolutional Autoencoder", "text": "Similar to Hosseini-Asl et al. (2016) and Payan and Montana (2015), we added a decoder to our 22-layer CNN by reversing the weights of the CNN to form an autoencoder. Specifically, a simplified formula for the operation is described in Equation 1, where X is the input matrix, W is the weight matrix, f is an activation function, Y is a matrix representing the feature extracted from X,X is a reconstruction of X by the network, and \u00b7 denotes dot product. \nn/2 n 1 1 Table 6 : The layers for the residual block used in our 3D residual network. n is the number of input channels."}, {"section_title": "Optimization Process", "text": ""}, {"section_title": "Reconstruction Task", "text": "To train the convolutional autoencoder on the reconstruction task, we minimize the mean squared error (MSE), its formulation is shown in Equation 2, where X is the input matrix (the MRI scan),X is obtained from the formulation shown in Equation 1, n is the number of MRI scans, and m is the number of features (or voxels) in the scan.\nFollowing Hosseini-Asl et al. (2016) and Payan and Montana (2015), we apply a sparsity constraint \u03b2 to the hidden representation Y , giving us the end-to-end formulation of the objective for the reconstruction task, shown in Equation 3. We exclude the formulation in Equation 3 for the weight decay (L2 Regularization) for brevity."}, {"section_title": "Classification Task", "text": "For the classification task, we minimize the cross-entropy loss between the predicted probability distribution of the correct label and the ground truth distribution. The objective has the following formulation, shown in Equation 4, wherep is a vector of probability distribution predicted by the model, l is index of the correct class (e.g. 0 for AD, 1 for MCI, 2 for CN), andp l andp j are the lth and jth element ofp, respectively."}, {"section_title": "Training", "text": "We ran all of our experiments for 100 epochs. The weights that led to the lowest validation loss were saved and later used to make predictions on the test set. We chose to save the model with the lowest validation loss instead of the lowest validation accuracy because the validation loss captures the divergence between the predicted distribution and the true distribution, whereas accuracy may or may not capture this. For hardware, hyperparameters, and other information on training, see Section .2 in Appendix A."}, {"section_title": "Results", "text": ""}, {"section_title": "Evaluation Approach", "text": "The goal of this study is to answer three queries one may have when deciding among the numerous proposed architectures as feature extractors for other AD-related downstream tasks. Specifically, we are looking to understand the performance gain, if any, from 1) using a 2D model on a slice of the scan versus 3D model on the entire MRI scan, 2) pretraining the model on the reconstruction task before training it on the classification task, and 3) different choices of models and architectures.\nWe attempt to provide insight into these three questions by keeping all other variables constant, such as preprocessing pipelines, variations in data distribution, hyperparameters for training and testing.\nAll of the experiments were performed at least four times, and no more than six times, due to resource constraints. To calculate the reported validation accuracy, we take the highest validation accuracy that we encounter in every run, and take the mean as the final validation accuracy. For the test accuracy. we simply take the mean of all of the test accuracy across the runs."}, {"section_title": "2D versus 3D Models", "text": "For this comparison, we evaluated the performance of 2D ResNet-18 pre-trained on ImageNet, and our own 3D ResNet-22 pre-trained on extra MRI scans left over from class balancing (see Section 3.2.2). The 2D ResNet-18 is trained on the coronal slice of the gray matter map from Clinica, and the 3D ResNet-18 is trained on the entire gray matter map.\nFrom Table 7 , we can conclude that there is a performance gain of close to 1% from utilizing 3D models instead of similar 2D models, which may be desirable if accuracy is crucial in the target application. However, there is a large cost of going from 2D to 3D, namely the time required to train the model, and the large amount of GPU memory required to fit the image and model. See Section .2 in Appendix A for more training information."}, {"section_title": "Model name", "text": "Validation Accuracy Test Accuracy 2D gray matter (coronal) 89.5 \u00b1 1.8% 87.4 \u00b1 2.0% 3D gray matter -w/ pre-train 90.3 \u00b1 1.2% 88.5 \u00b1 1.7% Table 7 : A comparison between 2D ResNet-18 pre-trained on ImageNet and 3D ResNet-22 pre-trained on MRI scans. There is a slight performance advantage with the 3D model.\nValidation Accuracy Test Accuracy 89.2 \u00b1 2.5% 87.5 \u00b1 1.1% DenseNet121 (coronal) 90.0 \u00b1 1.3% 88.8 \u00b1 1.9% InceptionNet V3 (coronal) 89.1 \u00b1 1.7% 88.6 \u00b1 1.4% VGG19 bn (coronal) 89.1 \u00b1 1.6% 88.1 \u00b1 2.7% Table 9 : Three-class classification accuracy of the 2D models."}, {"section_title": "Pre-training versus No Pre-Training", "text": "For this comparison, we evaluate the effects of pre-training on the model's ability to learn and the final accuracy. Table 8 contains the final accuracy, where there is little evidence to suggest that the model is unable to learn without pre-training, or that pre-training leads to better performance."}, {"section_title": "Model", "text": "Validation Accuracy Test Accuracy gray matter -no pre-train 90.1 \u00b1 1.0% 88.9 \u00b1 2.0% gray matter -w/ pre-train 90.3 \u00b1 1.2% 88.5 \u00b1 1.7% gray matter + CSF + white matter -no pre-train 91.0 \u00b1 0.7% 89.2 \u00b1 1.9% gray matter + CSF + white matter -w/ pre-train 90.4 \u00b1 0.8% 88.9 \u00b1 2.0% Table 8 : Comparison between models with pre-training versus models without pre-training. The results show negligible difference in performance, with a difference in mean accuracy of less than < 0.5%. ."}, {"section_title": "Model Comparisons", "text": "For this comparison, we look at how much impact the model architecture influences the performance of the model, by comparing four popular 2D networks. Table 9 shows a comparison of the results. Again, we observe little difference between the various architectures, suggesting that the commonly used architectures should be able to perform well on the task."}, {"section_title": "FreeSurfer versus Clinica", "text": "We performed an additional set of experiments to gain some insight into the difference in performance between using FreeSurfer data from the ADNI website, which were skullstripped but not spatially normalized, and white matter, gray matter, and cerebrospinal fluid maps from the Clinica pipeline. We used the same 3D 16-layer ResNet that we built for both experiments, and found a significant difference in accuracy between the two sets, highlighting the importance and advantage of using the Clinica pipeline. See Table 10 "}, {"section_title": "Discussion", "text": "By comparing the various setups, we have come to the conclusion that: (1) there is a slight improvement of about 1% in performance by using a 3D model over a 2D model, (2) the advantage of pre-training the model on MRI scans before training it end-to-end for classification was not noticeable, (3) all of the current popular 2D CNN architectures had similar performance.\nFor (1), we leave the reader with the caveat that training a 3D model requires significantly more time and computation power, and the extra 1% gain in accuracy may not be worth the cost.\nFor (2), while we have concluded that pre-training does not aid our model in achieving higher accuracy, we are not rebutting the previous studies over their claim that pre-training helped. We have to consider that we have more data available to us, likely enough to a point where pre-training may no longer be necessary.\nFor (3), our results suggest that the features of the model, such as densely connected layers or Inception modules, do not contribute as much to the performance in the task of AD classification as previous studies (Wang et al., 2018) (Hon and Khan, 2017) ."}, {"section_title": "Data Splitting Methodology", "text": "Suppose we have 20 scans, where 7 are labeled as AD, 5 are labeled as MCI, and 8 are labeled as CN. We first shuffle each of the three splits, then take the first 5 scans from each of the class sets, where 5 = min (7, 5, 8) , resulting in a final dataset size of 15 from the three classes. We then split each of the three sets into training, validation, and testing sets, resulting in nine total sets. Lastly, we combine the training, validation, and testing sets across all three classes to create a final training set of 9 scans, validation set of 3 scans, and testing set of 3 scans.\n.2. Additional Training Information ."}, {"section_title": "Hardware and Training Time", "text": "For training the 3D models, we used pairs of NVIDIA Tesla M40 GPUs with 24GB of memory. We were able to fit two images on each of the GPUs, bringing our maximum possible batch size to 4. The training time for the experiments ranged from 12-14 hours for the Clinica preprocessed images, to 24-26 hours for the FreeSurfer images. The significantly longer training time was mostly due to the larger file size, which led to slow network IO and longer computation time.\nFor training the 2D models, we used NVIDIA Titan X GPUs with 12GB of memory. We used a batch size of 16, which was not dictated by memory limit, but rather by intuition, as smaller batch size intuitively leads to better generalization with adaptive learning algorithms such as Adam. The training time for most 2D models completed within 2 hours.\n."}, {"section_title": "Hyperaparameters", "text": "For all of the 3D models, we used Adam optimizer for both the reconstruction task and the classification task. For the reconstruction task, we used a learning rate of 0.001 and weight decay (also known as L2 regularization) of 0.0001, as well as 0.0003 for \u03b2, the sparsity constraint parameter. For the classification task, we use a learning rate of 0.001 and weight decay (L2 regularization) of 0.0001.\nFor the 2D models, we used Adam optimizer for the classification task, with learning rate set to 0.001 and weight decay set to 0.01 for ResNet18, and learning rate set to 0.0001 and weight decay set to 0.01 for Inception V3, VGG-16, and DenseNet.\nThe aforementioned hyperparameters are chosen from past experience, as they generally work well for Adam. To keep the experiments consistent, no additional hyperparameter search was performed, with the exception of training Inception V3, VGG-16, and DenseNet, where we lowered the learning rate from 0.001 to 0.0001 because the models were having difficulty training with the higher learning rate. Table 13 : The training hyperparameters used for all of the models. In the task column, R denotes reconstruction, and C denotes classification."}]