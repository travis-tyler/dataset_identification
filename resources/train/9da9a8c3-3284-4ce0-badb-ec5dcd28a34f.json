[{"section_title": "Abstract", "text": "Over the past decade, machine learning gained considerable attention from the scientific community and has progressed rapidly as a result. Given its ability to detect subtle and complicated patterns, deep learning (DL) has been utilized widely in neuroimaging studies for medical data analysis and automated diagnostics with varying degrees of success. In this paper, we question the remarkable accuracies of the best performing models by assessing generalization performance of the stateof-the-art convolutional neural network (CNN) models on the classification of two most common neurodegenerative diseases, namely Alzheimer's Disease (AD) and Parkinson's Disease (PD) using MRI. We demonstrate the impact of the data division strategy on the model performances by comparing the results derived from two different split approaches. We first evaluated the performance of the CNN models by dividing the dataset at the subject level in which all of the MRI slices of a patient are put into either training or test set. We then observed that pooling together all slices prior to applying cross-validation, as erroneously done in a number of previous studies, leads to inflated accuracies by as much as 26% for the classification of the diseases."}, {"section_title": "I. INTRODUCTION", "text": "Deep learning (DL) models have attracted a great deal of research interest in medical imaging due to their advantages and successes in various fields such as image and speech recognition, automation, security, computer-aided diagnosis (CAD), just to name a few. In particular, medical image analysis using DL opened a new door into CAD. In recent years, convolutional neural networks (CNNs) have been used to detect and classify a range of diseases from cancer to neurological disorders [2] - [5] .\nThe CNN models used in these studies are mostly utilized on well-known big datasets such as ImageNet [6] and MNIST [7] . A sample CNN architecture used in medical image classification can be seen in Figure 1 . Model training and testing are generally done by splitting the dataset into three subsets: training, validation, and test. Training and validation are used to learn parameters and decide whether training is complete, whereas test data are used to evaluate model performance on new previously unseen data. However, CNN models may not perform well when presented with the new data as well as previously believed [8] . A recent study in computer vision has indicated that the true generalization performance of even classic CIFAR-10 photograph classification CNNs to new data are questionable and lower than previous results [9] . In domains such as disease detection, that kind of mismatch can cause serious problems as the researchers could design models which perform well on the specific test set but are incapable of generalizing, and fail when new data are presented [10] .\nIt has been long known that having an appropriate data division is crucial to achieve a generalization performance [11] , [12] . There are various statistical sampling techniques such as simple random sampling [13] , deterministic methods [14] , DUPLEX [15] , and stratified sampling [16] which may be used in different types of data to decrease the variance of the model performance.\nIn most image classification applications, the data are randomly divided into training, validation and test sets. To measure the model's ability to adapt properly to new, previously unseen data, the ideal test set should be the reflection of the data that could be encountered elsewhere.\nHowever, in medical image classification, the accuracy on a test set which is randomly sampled from the data may not reflect the model's performance on new, previously unseen data and may create a major bias which can be explained as data leakage [17] , [18] . Generally, data leakage is a phenomenon caused by the presence of the same data both in the training and testing processes. A more subtle version of this problem is when the test data are disjoint from the training data but come from a distribution that is more similar to that of the training set than one would expect from new data [19] , [20] . In 3D medical imaging such as MRI or CT, dividing the overall data randomly causing slices or patches from the same patient to be in both training and test sets and leads to a biased assessment.\nIn this work, we assessed the generalization performance of the networks on the classification of the two most common neurological disorders: Parkinson's Disease (PD) and Alzheimer's Disease (AD). The contributions of this paper are as follows:\n\u2022 We proposed a framework for PD and AD classification using CNNs and MR images; \u2022 We utilized two state of the art convolutional neural network models together with a smart data selection algorithm and demonstrated the use of the proposed framework on two public datasets: PPMI and OASIS; \u2022 We demonstrated the impact of the data division strategy on the model performances by comparing the results based on two different split approach, one of which affected by data leakage. This paper is organized as follows: In Section II, there is an overview on the related work. Section III describes the steps of the proposed methodology in detail. Classification results are presented in Section IV and discussed in Section V. Finally, Section VI concludes the paper with some remarks and indicates possible future directions."}, {"section_title": "II. RELATED WORK", "text": "PD is a neurological disorder caused by the progressive death of dopamine producing cells in the brain [29] , [30] . It is the second most common neurodegenerative disorder after Alzheimer's Disease (AD) [31] . An estimated 7 to 10 million people worldwide have been affected by PD and related disorders in 2018 [32] .\nIn recent years, several neuroimaging studies have utilized machine learning (ML) algorithms for detection and diagnosis of PD [33] - [35] . Various modalities like Magnetic Resonance Imaging (MRI), Single Photon Emission Computed Tomography (SPECT), Positron Emission Tomography (PET) and functional Magnetic Resonance Imaging (fMRI) are used within these research to diagnose PD [36] , [37] . In 2018, Esmaeilzadeh et al. [22] used 3D CNN for simultaneous classification and regression of PD diagnosis based on MRI and personal information (i.e., age and gender). They achieved 100% accuracy on both test and validation sets. In that study, they reached to the conclusion that Superior Parietal part on the right hemisphere of the brain is very critical in the diagnosis of PD. Lei et al. [38] performed a multi-class classification of three different clinical statuses: PD, SWEDD, and healthy conditions (HC) via SVM. They concluded that the classification performance with multi-modality features (GCD) combined with cerebrospinal fluid (CSF) biomarkers and clinical scores (DSSM) is always better than those without additional features. Recently, Sivaranjini et al. [21] utilized AlexNet to diagnose PD. The image dataset with 80% of the input data are used for training, and the remaining 20% is used for testing. Through TL, they achieved an accuracy of 88.9% on the classification of MRI slices. However, they did not test their model with subjects that were not included in the training data.\nAD, on the other hand, is the most common neurodegenerative disorder [39] . It is predicted that by 2050, half (51%) of all people 65 and older will be facing with AD [40] .\nSarraf et al. [26] used a CNN model for AD diagnosis in adults (above 75 years old) using fMRI and MRI. The data was divided into three parts: training (60%), validation (20%), and test (20%). They achieved 99.9% accuracy for functional MRI data and 98.84% for MRI data, respectively. However, data division was not done at the subject-level leading data from the same subject to be in both the training and test sets.\nIn [28] , Payan and Montana designed a classification system that combines sparse autoencoders and convolutional neural networks. They divided ADNI dataset into training set (1,731 samples), validation set (306 samples) and test set (228 samples) and achieved 95.39% classification accuracy with both 2D CNNs and 3D CNNs. Again, they did not perform subject level division. Lastly, Hon et al. [24] utilized two stateof-the-art architectures, namely VGG16 and Inception V4 to classify AD. They used 5-fold cross-validation to obtain the results, with an 80% -20% split between training and test. By using a pre-trained model for transfer learning (TL), they reported 92.3% accuracy with VGG16 model and 96.25% with Inception model.\nWhen we check the literature, we see that the phenomenon "}, {"section_title": "III. METHODS", "text": "In this section, we briefly describe the datasets we have used, the pre-processing steps and finally, the model architectures together with training protocols."}, {"section_title": "A. Data Splitting", "text": "Throughout the work, we realized that a common misconception occurs in many different papers which use machine learning algorithms in 3D medical imaging. Performance of the models was often determined by dividing the pooled slices into training and test sets [21] , [24] - [26] , [42] (see Table I ).\nThus, training and test sets included the different brain slices of the same subjects. Unfortunately, in that case, the high accuracies may stem from high intra-subject correlation. To test our hypothesis, we employed two different data splitting approaches. First, we divided the data by subject, in which all of the MRI slices of a subject are placed either in the training or in the test set. Then, in the second part, we pooled all slices together and then split the overall set randomly, meaning that the different slices of the same patient could appear both in the training and test sets."}, {"section_title": "B. Datasets", "text": "In this study two datasets were used, namely Parkinsons Progression Markers Initiative (PPMI) database [43] for PD and Open Access Series of Imaging Studies (OASIS) [44] for AD.\n1) PPMI: The axial T2 weighted MRI slices used to classify PD in this work are from the PMMI database (Table II) . The reason behind using T2 weighted MRI for PD is that T2 weighted sequences are better at detecting changes in tissue properties [45] . As a result, the data has the potential to monitor the structural changes of the brain caused by PD, such as the reduced volume of caudate and putamen [46] .\nThe PPMI database is publicly available and helps researchers to conduct research on identifying biomarkers of PD progression. It consists of a set of three-dimensional brain slices of 452 PD patients (292 males and 160 females) and 204 HC (134 males and 70 females). The average age of the patients is 61, where the minimum age is 30, and the maximum age is 89.\nThe PPMI subset used in this study consists of 408 subjects with 204 HC and 204 PD subjects. It has 6569 MRI slices derived from HC and 4467 slices from PD subjects. We randomly picked 7030 slices in total for our slice-based PD subset. Of these, 3515 slices were PD, and the remaining 3515 were HC. For the random division case, we used 80% of these slices in the training process while the rest were assigned to the test set. For the subject based case, we divide the data by patient meaning that the MRI slices of 164 patients from each class are placed in the training set and the slices of 40 AD patients and 40 HC are assigned to the test set.\n2) OASIS: For classification of AD, we used crosssectional, structural MRI data from the OASIS database (Table III). For the random split tests, we have employed the exact data set which were used in Hon et al.'s work [24] in order to replicate their approach while avoiding bias. 1 The subset they have used in their work consists of cross-sectional T1-weighted MRI scans. In their experiments, they randomly picked 200 subjects, 100 of whom were chosen from the AD group, while the other 100 from the HC group. The sample MRI slices from OASIS data can be seen in Figure 2 .\nFor the subject based case, we created a similar subset from the OASIS database by picking 200 subjects, half of whom were AD patients, while the other half was HC. MRI slices of 80 subjects from each class are used to train the model, while the other subjects took part in testing process. MRI scans from OASIS database are in hdr/img file format. To pre-process the scannings, we first converted them into NIfTI format, then into 2D (jpg) format.\nThe decision criteria of AD is a variable called Clinical Dementia Rating (CRD) with 0 suggests HC and any value greater than 0 implies AD. OASIS-1 dataset includes two different data: Raw and processed. Processed images are the brain-masked version of atlas registered image that are used in both types of experiments."}, {"section_title": "C. Image Pre-processing", "text": "The input of the 2D CNNs that we utilized in our approach is the set of 2D slices extracted from the MRI volume. Typically, each MRI volume contains many slices that correspond to a different cross section of the brain. To increase the performance of classication, we decided to pick the most informative slices to train the network. It is known that a signicant grey matter intensity loss with changes in the striatum region is observed in PD when compared with HC [46] . By calculating the image entropy for each slice, we aimed to select the slices which can illustrate such degenerated structure [24] . Two sets of MRI slices that belong to a PD patient are shown in the Figure 3 . The slice on the left of the figure is not very informative in terms of the amount of gray matter it reveals when compared to the slice on the right.\nEntropy is a measure of histogram dispersion which illustrates the variation in a slice. In the case of an image which has been perfectly histogram equalized, all 256 such states are equally occupied, and the entropy of the image is maximum. On the other hand, if all of the pixels of an image have the same value, the entropy is zero. Therefore, if the entropy of the image is reduced, its information is reduced as well. Thus, to obtain the most informative slices for network training, an entropy threshold has been determined (4.5, based on our empirical analysis). For a slice, the entropy can be calculated as follows:\nwhere M is the number of gray levels (256 for 8-bit images) and p i is the probability of a pixel having gray level intensity.\nAfter eliminating the slices which fail to carry the necessary information, normalization is performed on the remaining MRI slices to obtain an unvaried contrast and intensity range. For this reason, each MRI slice in the data set is normalized to the range (0, 1). To be compatible with the pre-trained models of VGG16 and Resnet50, the slices were resized to be 224\u00d7224.\nWe followed the same pre-processing structure for the AD slices as well."}, {"section_title": "D. CNN Models", "text": "We utilized two different architectures (VGG and ResNets) which are widely used in disease detection frameworks. 1) VGG16: VGG16 is a 16-layer network built by Oxfords Visual Geometry Group (VGG) and presented in their paper entitled \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" [47] . It won the ImageNet competition in ILSVRC-2014 with the accuracy of 92.7%. It replaces large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) in the Alexnet with multiple 33 kernel-sized filters.\nThe input to the first layer is a fixed-size 224 \u00d7 224 RGB image. The image is then passed through a stack of convolutional layers as well as max pooling layers. Finally, convolutional layers are followed by three Fully-Connected (FC) layers and the soft-max layer for 1000-way ILSVRC classification. The architecture of VGG16 is shown in the Figure 4 .\n2) Resnet50: Residual neural network (ResNet) ranked first in the ILSVRC 2015 classication competition with top-5 error rate of 3.57%. He et al. [48] ease the training process of deep neural networks while making their model deeper than those used previously. They reformulate the layers as learning residual functions with reference to the layer inputs, rather than learning unreferenced functions. Residual neural networks solve the problem known as vanishing gradient. When the network is too deep, the gradients of the loss function approaches zero, making the network hard to train. As a result, the weights are not updated, and thus learning cannot be achieved. With ResNets, the gradients can flow directly through the skip connections backward from latter layers to initial filters. The building block of a sample residual neural network structure is shown below in the Figure 5 ."}, {"section_title": "E. Training Protocols and Transfer Learning", "text": "Acquiring large sets of labeled data in medical imaging is a hard task as it is mostly sealed due to privacy and institutional policies, or expensive to label. To avoid the common problem of overfitting which generally stems from small data set and deep networks, transfer learning (TL) is employed to train a model efficiently on a smaller data set.\nThe idea behind TL is that many deep neural networks trained on images exhibit a common behavior: the first layers extract generic features and perform general operations such as edge detection or color blob detection [49] . Such low level features might be applicable to many datasets and tasks. Thus, when a network is pre-trained on an extremely large dataset, such as ImageNet, comprising 1.4 million images with 1000 classes, knowledge extracted from there can be applied to the given task of interest. Even for cross-domain application, such as networks trained on natural images used with medical images, TL has been proved to be robust [50] . For transfer learning, we follow the fine-tuning approach, where the last three layers of the pre-trained model are modified. The weights of the other layers of the model were frozen during fine-tuning to prevent overfitting. For VGG16, 50 epochs were used with a batch size of 40. The stochastic gradient descent and Adagrad optimization algorithms were used to minimize cross-entropy type of error. For Resnet50, 100 epochs with batch size of 32 were used. The optimization model was stochastic gradient descent. The loss function was categorical cross-entropy.\nData selection method and pre-processing part mentioned in Section III are implemented in MATLAB [51] . Then, deep learning methods are executed using Keras [52] with a TensorFlow [53] backend. Architectures as well as the pretrained weights were available to download in open source repositories of the models."}, {"section_title": "IV. EXPERIMENTAL RESULTS", "text": "The main aim was to differentiate AD and PD patients from HC by analyzing MRI data derived from two different databases via the CNN models and to show the importance of data division method on the generalization performance of the models. Table IV illustrates the accuracy results of the two models across two separate datasets using subject-level data splitting and random splitting after pooling all slices.\nAs it can be seen from the Table IV , both VGG16 and Resnet models can classify PD from HC with more than 82% accuracy when data are randomly split (biased split). However, on subject based split (unbiased split), we observed a large drop in accuracy (17% to 25%) for classification of the disease. Again, for AD classification, the same pattern can be detected. When data are divided at subject level, classification accuracy of VGG16 model is 64.3% whereas Resnet50 model achieves 67.1%. Alarmingly, pooling then splitting at slice level can inflate the classification accuracy by 26.1 percent points compared to the subject level split."}, {"section_title": "V. DISCUSSION", "text": "Comparison of classification performances across studies is an arduous task as each study has various pre-processing stages, validation approach or hyperparameter selection. In studies which create subsets from publicly available datasets, the selection of the subset is often a random process, which makes it impossible to replicate the work accurately [24] . Moreover, some of the studies do not provide sufficient implementation details, especially about the validation procedures adopted, with the risk that the reported performances are affected by significant bias. Dividing the data at the slice-level in medical image classification is a significant problem which is currently widespread in the field. Our results show that this may artificially inflate the accuracy of classifiers by as much as 26 percentage points.\nTo evaluate prospective clinical feasibility of automated diagnosis, unbiased and accurate assessment of the model performances is crucial. We argue that despite the impressive accuracies of the previous works, there still exist some serious issues that must be resolved and much room for improvement in medical image classification and automated diagnosis."}, {"section_title": "VI. CONCLUSION", "text": "In this paper, we utilized a transfer learning-based method to detect two most common neurological diseases from structural MRI images. We employed two state-of-the-art architectures, namely VGG16 and Resnet, to classify PD subject from HC and AD subjects from HC. We test our models on MRI slices from the PMMI and OASIS brain imaging datasets, where MRI slices of more than 300 patients are used to train the models. We compared the results of two data split approaches across separate data sets, and showed that there is a large overestimation in accuracy when slices from all subjects are pooled together prior to validation.\nThe large discrepancy of accuracies between two types of data division suggests that the test accuracy from the random division approach is not a valid measure of performance on new subjects. Subject level tests are required to show the accurate performance of the classification model.\nWhile we are confident that most researchers are well aware of the issue and would never split data from the same subject into test and training data, we have found that this is still a serious problem in the literature. With the recent advances in machine learning and AI, more and more people are becoming interested in applying these techniques to biomedical imaging and there is a real and growing risk that many of them will not be familiar with the possible issues and the good practices.\nIn the future, we will investigate other state of the art models as well as the effect of deep fine tuning on performance. Optimizing the hyperparameters of the models and expending the datasets via collaborations may be crucial to achieve better results. With these efforts, we aim to solve the problem behind the low accuracy of subject level tests. We hope to achieve better patient group classication and ease the diagnosis of neuro-degenerative disorders in the near future."}]