[{"section_title": "Abstract", "text": "Mining discriminative features for graph data has attracted much attention in recent years due to its important role in constructing graph classifiers, generating graph indices, etc. Most measurement of interestingness of discriminative subgraph features are defined on certain graphs, where the structure of graph objects are certain, and the binary edges within each graph represent the \"presence\" of linkages among the nodes. In many real-world applications, however, the linkage structure of the graphs is inherently uncertain. Therefore, existing measurements of interestingness based upon certain graphs are unable to capture the structural uncertainty in these applications effectively. In this paper, we study the problem of discriminative subgraph feature selection from uncertain graphs. This problem is challenging and different from conventional subgraph mining problems because both the structure of the graph objects and the discrimination score of each subgraph feature are uncertain. To address these challenges, we propose a novel discriminative subgraph feature selection method, Dug, which can find discriminative subgraph features in uncertain graphs based upon different statistical measures including expectation, median, mode and \u03d5-probability. We first compute the probability distribution of the discrimination scores for each subgraph feature based on dynamic programming. Then a branch-and-bound algorithm is proposed to search for discriminative subgraphs efficiently. Extensive experiments on various neuroimaging applications (i.e., Alzheimers Disease, ADHD and HIV) have been performed to analyze the gain in performance by taking into account structural uncertainties in identifying discriminative subgraph features for graph classification."}, {"section_title": "Introduction", "text": "Graphs arise naturally in many scientific applications which involve complex structures in the data, e.g., chemical compounds, program flows, etc. Different from traditional data with flat features, these data are usually not directly represented as feature vectors, but as graphs with nodes and edges. Mining discriminative features for graph data has attracted much attention in recent years due to its important role in constructing graph classifiers, generating graph indices, etc. [22, 11, 4, 14, 20] . Much of the past research in discriminative subgraph feature mining has focused on certain graphs, where the structure of the graph objects are certain, and the binary edges represent the \"presence\" of linkages between the nodes. Conventional subgraph mining methods [22] utilize the structures of the certain graphs to find discriminative subgraph features. However, in many real-world applications, there is inherent uncertainty about the graph linkage structure. Such uncertainty information will be lost if we directly transform uncertain graphs into certain graphs.\nFor example, in neuroimaging, the functional connectivities among different brain regions are highly uncertain [6, 8, 7, 25] . In such applications, each human brain can be represented as an uncertain graph as shown in Figure 1 , which is also called the \"brain network\" [2] . In such brain networks, the nodes represent brain regions, and edges represent the probabilistic connections, e.g., resting-state functional connectivity in fMRI (functional Magnetic Resonance Imaging). Since these functional connectivities are derived based upon processing steps, such as temporal correlations in spontaneous blood oxygen level-dependent (BOLD) signal oscillations, each edge of the brain network is associated with a probability to quantify the likelihood that the functional connection exists in the brain. Resting-state functional connectivity has shown alterations related to many neurological diseases, such as ADHD (Attention Deficit Hyperactivity Disorder), Alzheimer's disease and virus infections that may affect the brain functioning, such as HIV [21] . Researchers are interested in analyzing the complex structure and uncertain connectivities of the human brain to find biomarkers for neurological diseases. Such biomarkers are clinically im- perative for detecting injury to the brain in the earliest stages before it is irreversible. Valid biomarkers can be used to aid diagnosis, monitor disease progression and evaluate effects of intervention.\nMotivated by these real-world neuroimaging applications, in this paper, we study the problem of mining discriminative subgraph features in uncertain graph datasets. Discriminative subgraph features are fundamental for uncertain graphs, just as they are for certain graphs. They serve as primitive features for the classification tasks on uncertain graph objects. Despite the value and significance, the discriminative subgraph mining for uncertain graph classification has not been studied in this context. If we consider discriminative subgraph mining and uncertain graph structures as a whole, the major research challenges are as follows: Structural Uncertainty: In discriminative subgraph mining, we need to estimate the discrimination score of a subgraph feature in order to select a set of subgraphs that are most discriminative for a classification task. In conventional subgraph mining, the discrimination scores of subgraph features are defined on certain graphs, where the structure of each graph object is certain, and thus the containment relationships between subgraph features and graph objects are also certain. However, when uncertainty is presented in the structures of graphs, a subgraph feature only exists within a graph object with a probability. Thus the discrimination scores of a subgraph feature are no longer deterministic values, but random variables with probability distributions.\nThus, the evaluation of discrimination scores for subgraph features in uncertain graphs is different from conventional subgraph mining problems. For example, in Figure 2 , we show an uncertain graph dataset containing 4 uncertain graphs G 1 , \u00b7 \u00b7 \u00b7 , G 4 with their class labels, + or \u2212. Subgraph g 1 is a frequent pattern among the uncertain graphs, but it may not relate to the class labels of the graphs. Subgraph g 2 is a discriminative subgraph features when we ignore the edge uncertain- ties. However, if such uncertainties are considered, we will find that g 2 can rarely be observed within the uncertain graph dataset, and thus will not be useful in graph classification. Accordingly, g 3 is the best subgraph feature for uncertain graph classification. Efficiency & Robustness: There are two additional problems that need to be considered when evaluating features for uncertain graphs: 1) In an uncertain graph dataset, there are an exponentially large number of possible instantiations of a graph dataset [26] . How can we efficiently compute the discrimination score of a subgraph feature without enumerating all possible implied datasets? 2) When evaluating the subgraph features, we should choose a statistical measure for the probablity disctribution of discrimination scores which is robust to extreme values. For example, given a subgraph feature with (score, probability) pairs as (0.01, 99.99%) and (+\u221e, 0.01%), the expected score of the subgraph is +\u221e, although this value is only associated with a very tiny probability. In order to address the above problems, we propose a general framework for mining discriminative subgraph features in uncertain graph datasets, which is called Dug (Discriminative feature selection for Uncertain Graph classification). The Dug framework can effectively find a set of discriminative subgraph features by considering the relationship between uncertain graph structures and labels based upon various statistical measures. We propose an efficient method to calculate the probability distribution of the scoring function based on dynamic programming. Then a branch-and-bound algorithm is proposed to search for the discriminative subgraphs efficiently by pruning the subgraph search space. Empirical studies on resting-state fMRI images of different brain diseases (i.e., Alzheimer's Disease, ADHD and HIV) demonstrate that the proposed method can obtain better accuracy on uncertain graph classification tasks than alternative approaches.\nFor the rest of the paper, we first introduce preliminaries in Section 2. Then we introduce our Dug subgraph mining framework in Section 3. Discrimination score functions based upon different statistic measures are discussed in Section 3.1. An efficient algorithm for computing the score distribution based upon dynamic programming is proposed in Section 3.2. Experimental results are discussed in Section 4. In Section 6, we conclude the paper."}, {"section_title": "Problem Formulation", "text": "In this section, we formally define the model of uncertain graphs and the problem of discriminative subgraph mining in uncertain graph datasets. Suppose we are given an uncertain graph dataset\n\u22a4 denotes the vector of class labels, where y i \u2208 {+1, \u22121} is the class label of G i . We also denote the subset of D that contains only positive/negative graphs as"}, {"section_title": "Definition 1. (Certain Graph)", "text": "A certain graph is an undirected and deterministic graph represented as G = (V, E). V = {v 1 , \u00b7 \u00b7 \u00b7 , v nv } is the set of vertices. E \u2286 V \u00d7 V is the set of deterministic edges."}, {"section_title": "Definition 2. (Uncertain Graph)", "text": "An uncertain graph is an undirected and nondeterministic graph represented as G = (V, E, p). V = {v 1 , \u00b7 \u00b7 \u00b7 , v nv } is the set of vertices. E \u2286 V \u00d7 V is the set of nondeterministic edges. p : E \u2192 (0, 1] is a function that assigns a probability of existence to each edge in E. p(e) denotes the existence probability of edge e \u2208 E.\nConsider an uncertain graph G(V, E, p) \u2208 D, where each edge e \u2208 E is associated with a probability p(e) of being present. As in previous works [27, 26] , we assume that the uncertainty variables of different edges in an uncertain graph are independent from each other, though most of our results are still applicable to graphs with edge correlations. We further assume that all uncertain graphs in a dataset D share a same set of nodes V and each node in V has a unique node label, which is reasonable in many applications like neuroimaging, since each human brain consists of the same number of regions. The main difference between different uncertain graphs is on their linkage structures, i.e., the edge sets E( G) and the edge probabilities p(e).\nPossible instantiations of an uncertain graph G are usually referred to as worlds of G, where each world corresponds to an implied certain graph G. Here G is implied from uncertain graph G (denoted as G \u21d2 G), iff all edges in E(G) are sampled from E( G) according to their probabilities of existence in p(e) and E(G) \u2286 E( G). There are 2 |E( G)| possible worlds for uncertain graph G, denoted as W( G) = {G| G \u21d2 G}. Thus, each uncertain graph G corresponds to a probability distribution over W( G). We denote the probability of each certain graph G \u2208 W( G) being implied by the uncertain graph G as Pr( G \u21d2 G), and we have\nSimilarly, possible instantiations of an uncertain graph dataset D = { G 1 , \u00b7 \u00b7 \u00b7 , G n } are referred to as worlds of D, where each world corresponds to an implied certain graph dataset \nThe concept of subgraph is defined based upon certain graphs. Different from conventional subgraph mining problems where each subgraph feature can have multiple embeddings within one graph object, in our data model, each subgraph feature g can only have one unique embedding within a certain graph G.\nWe use g \u2286 G to denote that graph g is a subgraph of G. We also say that G contains subgraph g.\nFor an uncertain graph G, the probability of G containing a subgraph feature g is defined as follows: which corresponds to the probability that a certain graph G implied by G contains subgraph g.\nWe focus on mining a set of discriminative subgraph features to define the feature space of graph classification. It is assumed that a graph object G i is represented as a feature vector\n\u22a4 associated with a set of subgraph features\nHere,\nis the probability that G i contains the subgraph feature g k . Now suppose the full set of subgraph features in the graph dataset D is S = {g 1 , \u00b7 \u00b7 \u00b7 , g m }, which we use to predict the class labels of the graph objects. The full feature set S is very large. Only a subset of the subgraph features (T \u2286 S) is relevant to the graph classification task, which is the target feature set we want to find within uncertain graphs.\nThe key issues of discriminative subgraph mining for uncertain graphs can be described as follows: (P1) How can one properly evaluate the discrimination scores of a subgraph feature considering the uncertainty of the graph structures? (P2) How can one efficiently compute the probability distribution of a subgraph's discrimination score by avoiding the exhaustive enumeration of all possible worlds of the uncertain graph dataset? Moreover, since the subgraph enumeration is NP-hard, it is also infeasible to fully enumerate all the subgraph features for an uncertain graph dataset.\nIn the following sections, we will introduce the proposed framework for mining discriminative subgraphs from uncertain graphs.\n3 The Proposed Framework 3.1 Discrimination Score Distribution In this subsection, we address the problem (P1) discussed in the previous section. In conventional discriminative subgraph mining, the discrimination scores of subgraph features are usually defined for certain graph datasets, e.g., information gain and G-test score [22] . The score of a subgraph feature is a fixed value indicating the discriminative power of the subgraph feature for the graph classification task. However, such concepts don't make sense to uncertain graph datasets, since an uncertain graph only contains a subgraph feature in a probabilistic sense. Now we extend the concept of discriminative subgraph features in uncertain graph datasets. Suppose we have an objective function F (g, D) which measures the discrimination score of a subgraph g in a certain graph dataset D. The corresponding objective function on an uncertain graph dataset D can be written as\ncorresponds to a random variable over all possible outcomes of F (g, D) (i.e., Range(F )) with probability distribution:\nwhere s i \u2208 Range(F ). The probability distribution of the discrimination score values can be defined as follows:\nwhere I(\u03c0) \u2208 {0, 1} is an indicator function, and I(\u03c0) = 1 iff \u03c0 holds. In other words, \u2200s \u2208 Range(F ), Pr[F (g, D) = s] is the summation over the probabilities of all worlds of D in which the discrimination score F (g, D) is exactly s. Based on the discrimination score function on uncertain graphs, we define four statistical measures that evaluate the properties of the distribution of F (g, D) from different perspectives.\nDefinition 4. (Mean-Score) Given an uncertain graph dataset D, a subgraph feature g and a discrimination score function F (\u00b7, \u00b7), we define the expected discrimination score Exp(F (g, D)) as the mean score among all possible worlds of D:\nThe mean discrimination score is the expectation of the random variable F (g, D). The expectation is usually used in conventional frequent pattern mining on uncertain datasets [27, 26] . However, it's worth noting that the expectation of discrimination scores may not be robust to extreme values. In discriminative subgraph mining, the value of a score function (e.g., frequency ratio [10] , G-test score [22] ) can be +\u221e. Such cases can easily dominate the computation of expectation, even if the probabilities are extremely small. For example, suppose we have a subgraph feature with the (score, probability) pairs as (0.01, 99.99%) and (+\u221e, 0.01%). The expected score will be +\u221e. In order to address this problem, we either need to bound the maximum value of the objective function like min(F (g, D), Definition 5. (Median-Score) Given an uncertain graph dataset D, a subgraph feature g and a discrimination score function F (\u00b7, \u00b7) on certain graphs, we define the median discrimination score Median (F (g, D) ) as the median score among all possible worlds of D:\nThe median score is relatively more robust to extreme values than expectation, although in some cases the median score can still be infinite. The same results can also hold for any quantile or k-th order statistic. Another commonly used statistic is the mode score, i.e., the score value that has the largest probability. The mode score of a distribution means that the score is most likely to be observed within all possible worlds of D.\nDefinition 6. (Mode-Score) Given an uncertain graph dataset D, a subgraph feature g and a discrimination score function F (\u00b7, \u00b7), we define the mode discrimination score Mode (F (g, D) ) as the score that is most likely among all possible worlds of D:\nNext we consider the probability of a subgraph feature being observed as a discriminative pattern within all possible worlds of \ncalled \u03d5-probability. The higher the value, the more likely that the subgraph feature is a discriminative pattern with a score larger or equals to a threshold \u03d5.\nDefinition 7. (\u03d5-Probability) Given an uncertain graph dataset D, a subgraph feature g and a discrimination score function F (\u00b7, \u00b7), we define the \u03d5-probability for discrimination score function F (g, D) as the sum of probabilities for all possible worlds of D, where the score is greater than or equals to \u03d5:\nThe \u03d5-probability is robust to extreme values of the objective function. For the previous example, we have a subgraph feature with score distribution: (0.01, 99.99%), (+\u221e, 0.01%). The \u03d5-probability is 0.01%, when \u03d5 = 1.\nWe have already introduced four statistical measures of the distribution of a discrimination score function. Now the central problem for calculating all these measures is how to calculate Pr[F (g, D) = s] efficiently, which we will discuss in the following section."}, {"section_title": "Efficient Computation", "text": "In this subsection, we address the problem (P2) discussed in Section 2. Given a certain graph dataset D, we denote the subsets of all positive graphs and all negative graphs as D + and D \u2212 , respectively. Suppose the supports of subgraph feature g in D + and D \u2212 are n g + and n\nMost of the existing discrimination score functions can be written as a function of n g + , n g \u2212 , n + and n \u2212 :\nThe definition in Eq. 3.1 covers many discrimination score functions including confidence [5] , frequency ratio [10] , information gain, G-test score [22] and HSIC [13] , as shown in Table 2 . For example, frequency ratio can be written as r(g) = | log\n. Because n + and n \u2212 are fixed numbers for different subgraph features, we simply use f (n \nThen the values of F (g, D) in all possible worlds with non-zero probabilities can be covered by the n + \u00d7 n \u2212 cases.\nMoreover, because different uncertain graphs are independent from each other, we have\nwhere Pr[n Figure 4 , we showed the dynamic programing algorithm to compute the target values using Eq. 3.3. Figure 3 \nWe will show later that the dynamic programming process is highly efficient in all the applications studied in Section 4. For dataset with even larger number of graphs, the divid-and-conquer method in [19] could also be used here to further optimize the computational cost."}, {"section_title": "Upper-Bounds for Subgraph Pruning", "text": "In order to avoid the exhaustive enumeration of subgraph features, we derive some subgraph pruning methods. One natural pruning bound for subgraph search is the expected frequency of a subgraph feature,\n, since it's can be easily proved with anti-monotonic property. For the expectation and \u03d5-probability, we can also derive additional bounds for subgraph pruning. LetF (g, D) =f (n g + , n g \u2212 ) be the estimated upper-bound function for g and its supergraphs in certain graph dataset D. We can derive If the score is larger than the worst feature in T , replace it and update \u03b8 = ming\u2208T M F (g, D) 2. Test pruning criteria for the sub-tree rooted from node g as follows:\nif Exp-Freq(gc) \u2264 min sup, prune the sub-tree of gc if Bound-M F (gc, D) \u2264 \u03b8, prune the sub-tree of gc 3. Recursion: Depth-first search the sub-tree rooted from node gc Output:\nT : the discriminative subgraph features for uncertain graph classification. Pr\nFigure 4: The dynamic programming algorithm for probability computation.\nthe corresponding upper-bounds as follows:\nFor the median and mode measures, it is difficult to derive a meaningful bound, thus we simply use the expected frequency to perform the subgraph pruning.\nWe now utilize the above bounds to prune the DFS-code tree in gSpan [23] by the branch-and-bound pruning. The top-t best features are maintained in a candidate list. During the subgraph mining, we calculate the upper-bound of each subgraph feature in the search tree. If a subgraph feature with its children pattern cannot update the candidate feature list, we can prune the subtree of gSpan rooted from this node. It is guaranteed by the upper-bounds that we will not miss any better subgraph features. Thus, the subgraph mining process can be speeded up without loss of performance. The algorithm of Dug is summarized in Figure 6 ."}, {"section_title": "Experiments", "text": "In order to evaluate the performance of the proposed approach for uncertain graph classification, we tested our algorithm on real-world fMRI brain images as summarized in Table 3 ."}, {"section_title": "Data Collection", "text": "In order to evaluate the performance of the proposed approach for uncertain graph classification, we tested our algorithm on real-world fMRI brain images.\n\u2022 Alzheimer's Disease (ADNI): The first dataset is collected from the Alzheimer's Disease Neuroimaging Initiative 1 . The dataset consists of records of patients with Alzheimer's Disease (AD) and Mild Cognitive Impairment (MCI). We downloaded all records of restingstate fMRI images and treated the normal brains as negative graphs, and AD+MCI as the positive graphs. We applyed Automated Anatomical Labeling (AAL 2 ) to extract a sequence of responds from each of of the 116 anatomical volumes of interest (AVOI), where each AVOI represents a different brain region. The correlations of brain activities among different brain regions are computed. Positive correlations are used as uncertain links among brain regions. For details, we used SPM8 toolbox 3 , and functional images were realigned to the first volume, slice timing corrected, and normalized to the MNI template and spatially smoothed with an 8-mm Gaussian kernel. Resting-State fMRI Data Analysis Toolkit (REST 4 ) was then used to remove the linear trend of time series and temporally band-pass filtering (0.01-0.08 Hz). Before the correlation analysis, several sources of spurious variance were then removed from the data through linear regression: (i) six parameters obtained by rigid body correction of head motion, (ii) the whole-brain signal averaged over a fixed region in atlas space, (iii) signal from a ventricular region of interest, and (iv) signal from a region centered in the white matter. Each brain is represented as an uncertain graph with 90 nodes corresponding to 90 cerebral regions, excluding 26 cerebellar regions.\n\u2022 Attention Deficit Hyperactivity Disorder (ADHD): The second dataset is collected from ADHD-200 global competition dataset 5 . The dataset contains records of resting-state fMRI images for 776 subjects, which are labeled as real patients (positive) and normal controls (negative). Similar to the ADNI dataset, the brain images are preprocessed using Athena Pipeline 6 . The original dataset is unbalanced, we randomly sampled 100 ADHD patients and 100 normal controls from the dataset for performance evaluation.\n\u2022 Human Immunodeficiency Virus Infection (HIV): The third dataset is collected from the Chicago Early HIV Infection Study in Northwestern University [21] . The dataset contains fMRI brain images of patients with early HIV infection (positive) as well as normal controls (negative). The same preprocessing steps as in ADNI dataset were used to extract a functional connectivity network from each image."}, {"section_title": "Comparative Methods", "text": "We compared our method using different statistical measures and discrimination score functions summarized as follows:\n\u2022 Frequent Subgraphs + Expectation (Exp+Freq): The first baseline method is finding frequent subgraph fea- tures within uncertain graphs. This baseline is similar to the method introduced in [27] . In our data model, this baseline method computes the exact expected frequency of each subgraph features, instead of approximated values. The top ranked frequent patterns are extracted as used as features for graph classification.\n\u2022 Dug with HSIC based discrimination scores: we compare with four different versions of our Dug method based upon HSIC criterion, which maximize the dependence between subgraph features and graph labels [13] . \"Exp-HSIC\" computes the expected HSIC value for each subgraph feature, and find the top-k subgraphs with the largest values. \"Med-HSIC\" computes the median HSIC value for each subgraph feature, while \"Mod-HSIC\" computes the mode HSIC value. \"\u03d5 Pr-HSIC\" computes the \u03d5-probability of HSIC value for each subgraph feature.\n\u2022 Dug with Frequency Ratio based discrimination scores: we also compare our method based upon Frequency Ratio, i.e., \"Exp-Ratio\", \"Med-Ratio\", \"ModRatio\" and \"\u03d5 Pr-Ratio\".\n\u2022 Dug with G-test based discrimination scores: we then compare our method based upon G-test criterion, i.e., \"Exp-Gtest\", \"Med-Gtest\", \"Mod-Gtest\" and \"\u03d5 PrGtest\".\n\u2022 Dug with Confidence based discrimination scores: the 5th group of methods are based upon G-test criterion, i.e., \"Exp-Conf\", \"Med-Conf\", \"Mod-Conf\" and \"\u03d5 PrConf\".\n\u2022 Simple Thresholding: Another group methods we have compared are the feature selection methods for certain graphs. In order to get the certain graphs from the uncertain graphs in the dataset, we perform simple tresholding over the weights of the links to get the binary links. These baseline methods include: \"Freq\", \"HISC\", \"Ratio\", \"Gtest\" and \"Conf\", which correspond to the discrimination scores used in previous 5 groups separately. LibSVM [3] with the linear kernel is used as the base classifier for all compared methods. The min sup in the gSpan for ADHD, ADNI and HIV datasets are 20%, 40% and 40% respectively. Since the range of different discrimination functions can be extremely different. We set the default \u03d5 for HSIC criterion, G-test score, frequency ratio and confidence as 0.03, 200, 1 and 0.5, respectively. Table 5 : Results on the ADHD (Attention Deficit Hyperactivity Disorder) dataset with different number of features (t = 100, \u00b7 \u00b7 \u00b7 , 500). The results are reported as \"average performance + (rank)\".\nMethods t = 100 t = 200 t = 300 t = 400 t = 500 t = 100 t = 200 t = 300 t = 400 t = 500 Rank 4.3 Performance on Uncertain Graph Classification In our experiments, we first randomly sample 80% of the uncertain graphs as the training set, and the remaining graphs as the test set. This random sampling experiment was repeated 20 times. The average performances with the rank of each method are reported. The reason for using classification performances to evaluate the quality of subgraph features is that classification methods can usually achieve higher accuracy with features of better discriminative powers. We measure the classification performance by error rate and F1 score. Table 5 and Table 4 show the evaluation results in terms of classification error rates and F1 scores with different number of selected subgraph features (t = 100, \u00b7 \u00b7 \u00b7 , 500). The results of each method are shown with average performance values and their ranks among all the other methods. Values with * stand for the best performance for the corresponding evaluation criterion. It is worth noting that the neuroimaging tasks are generally very hard to predict very accurately. According to a global competition on ADHD dataset 7 , the average performance of all winning teams is about 8% over the prediction accuracy of chance (i.e., randomly assigning diagnoses). Thus in neuroimaging tasks, it is very hard for classification algorithms to achieve even moderate error rates. And in ADHD dataset, the best performance that Dug can achieve is with error rate 37%, which is 13% improvement over the prediction error rate of chance. We find that our discriminative subgraph mining method with different settings outperforms the baseline method (Exp-Freq) for frequent subgraph mining, which selects subgraph features based upon expected frequencies in the uncertain graph dataset. This is because that frequent subgraph features in uncertain graph dataset may not be relevant to the classification task.\nMoreover, we can see that almost all the Dug methods outperform the simple thresholding methods which directly convert the uncertain graphs into certain graphs and then use different discimination functions to select subgraph features. This is because that simply converting uncertain graphs into certain graph can loss the uncertainty information about the linkage structures of the graphs, thus the classification performances on certain graphs are not as good as the performance of uncertain graphs.\nA third observation is that the performance of each method on different dataset can be quite different. However, the best methods that consistently outperforms other methods in all datasets are Med-Conf and \u03d5-PrRatio. They both have their advantages in different perspectives. Med-Conf method has one less parameter than that of \u03d5-Pr-Ratio. \u03d5-Pr-Ratio method has an additional subgraph pruning bound compared to MedConf method, which can be important for datasets with even larger graphs."}, {"section_title": "Influence of Parameter", "text": "In the \u03d5-Pr based methods, there is an additional threshold parameter than the other methods. In Figure 7 (a) and Figure 7(b) , we tested the \u03d5-Pr-HSIC with \u03d5 values among {0.01, 0.02, \u00b7 \u00b7 \u00b7 0.06} separately. We can see that the method is not sensitive to the parameter \u03d5. Generally, the performance of \u03d5-Pr-HSIC with default setting (\u03d5 = 0.03) is pretty good. If we try to optimize the selection of \u03d5 value, the accuracy can be even better.\nWe also compare Dug models with and without pruning in the subgraph search space as summarized in Figure 7 (c). The CPU time with different min sup for Exp-HSIC in ADNI dataset is reported. Dug can improve the efficiencies by pruning the subgraph search space. In other datasets Dug shows similar trends. Figure 8 shows the running time for mod-HISC with different number of graphs in the dataset. In addition to the dynamic programming method we used in Dug, we also find that the brute-force searching method that enumerates all possible worlds of the uncertain graph dataset cannot work on small datasets with even 40 graphs. The running time of Dug scales almost linearly with the number of graphs in the dataset. Althought the dynamic programming process of Dug is O(n 2 ), which is quadratic to the number of graphs in the dataset. However, in the ADHD dataset, the main computational cost of Dug algorithm is for the subgraph enumeration step, which is linear to the number of the graphs in the dataset. In cases of even larger datasets, the dynamic programming process could eventually dominant the computational cost. In these cases, the divide-and-conquer method in [19] could be used to further optimize the computational cost."}, {"section_title": "Related Work", "text": "Our work is related to subgraph mining techniques for both certain graphs and uncertain graphs. We briefly Mining subgraph features in graph data has been studied intensively in recent years [15] . Most of the previous research has been focused on certain graph datasets, where the edges of the graph objects are binary/certain. The aim of these subgraph mining method is to extract important subgraph features based on the structure of the graphs. Depending on whether the class labels are considered in the feature mining steps, existing methods can roughly be categorized into two types: frequent subgraph mining and discriminative subgraph mining. In frequent subgraph mining, Yan and Han proposed a depth-first search algorithm, gSpan [23] , which maps each graph to a unique minimum DFS code and use right-most extension technique for subgraph extension. There are also many other frequent subgraph mining methods that have been proposed, e.g., AGM [9] , FSG [15] , MoFa [1] , and Gaston [16] , etc. Discriminative subgraph mining have also been studied intensively in the literature, such as LEAP [22] and LTS [10] , where the task is to find discriminative subgraph for graph classifications.\nRecently, there has been a growing interest in exploiting data uncertainty, especially structural uncertainty in graph data. There are some recent works on mining frequent subgraph features for uncertain graphs [27, 26, 28, 17] . The problem of mining frequent subgraph in uncertain graphs are more difficult to those of certain graphs. The authors [27] proposed a method to estimate approximately the expected support of a subgraph feature in uncertain graph datasets. In [26] , the authors studies the \u03d5-probabilities for frequent subgraph features within uncertain graph datasets. The difference between these works and our paper are as follows: 1) In this paper, we study how to find discriminative subgraph features for uncertain graph data. The class labels of the graph objects are considered during the subgraph mining.\n2) The graph model in our paper is different from previous uncertain graph data, since we assume different graph object shares the same set of nodes as inspired by the neuroimaging applications. Thus, our method compute the exact discrimination scores of each subgraph features, instead of approximate scores. There are also many other works on uncertain graphs, which focus on different problems, e.g., reliable subgraph mining [12] , k-nearest neighbor discovery [18] , subgraph retrieval [24] etc.\nOur work is also motivated by the recent advances in analyzing neuroimaging data using data mining and machine learning approaches [6, 8, 7, 25] . Huang et. al. [6] developed a sparse inverse covariance estimation method for analyzing brain connectivities in PET images of patients with Alzheimer's disease."}, {"section_title": "Conclusion", "text": "In this paper, we studied the problem of discriminative subgraph feature selection for uncertain graph classification. We proposed a general framework, called Dug, for finding discriminative subgraph feature in uncertain graphs based upon various statistical measures. The probability distributions of the scoring function are efficiently computed based on dynamic programming."}]