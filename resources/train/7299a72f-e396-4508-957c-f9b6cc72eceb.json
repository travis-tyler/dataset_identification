[{"section_title": "Background", "text": "The ECLS-K:2011 is the third and latest study in the Early Childhood Longitudinal Study (ECLS) program, which comprises three longitudinal studies of young children: the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K); the Early Childhood Longitudinal Study, Birth Cohort (ECLS-B); and the ECLS-K:2011. The ECLS program is broad in its scope and coverage of child development, early learning, and school progress. It draws together information from multiple sources, including children, parents, teachers, school administrators, and early care and education providers, to provide data for researchers and policymakers to use to answer questions regarding children's early educational experiences and address important policy questions. The ECLS-K:2011 provides current information about today's elementary school children. Also, coming more than a decade after the inception of the ECLS-K, the ECLS-K:2011 allows for cross-cohort comparisons of two nationally representative kindergarten classes experiencing different policy, educational, and demographic environments. The three studies in the ECLS program provide national data on children's developmental status at birth and at various points thereafter; children's transitions to nonparental care, early education programs, and school; and children's home and school experiences, growth, and learning. The ECLS 1-3 program also provides data that enable researchers to analyze how a wide range of child, family, school, classroom, nonparental care and education provider, and community characteristics relate to children's development and to their experiences and success in school. Together, these three studies provide the range and breadth of data needed to more fully describe and understand children's education experiences, early learning, development, and health in the late 1990s, 2000s, and 2010s. More information about all three of these studies can be found on the ECLS website (https://nces.ed.gov/ecls)."}, {"section_title": "Periods of Data Collection", "text": "The ECLS-K:2011 is following a cohort of children from their kindergarten year (the 2010-11 school year, referred to as the base year) through the 2015-16 school year, when most of the children are expected to be in fifth grade (exhibit 1-1). The sample includes both children who were in kindergarten for the first time and those who were repeating kindergarten during 2010-11. Although the study refers to later rounds of data collection by the grade the majority of children are expected to be in (that is, the modal grade for children who were in kindergarten in the 2010-11 school year), children are being included in subsequent data collections regardless of their grade level. 2 During the 2010-11 school year, when both a fall and a spring data collection were conducted, approximately 18,170 kindergartners from about 1,310 schools 3 and their parents, teachers, school administrators, and before-and after-school care providers participated in the study. Fall and spring data collections were also conducted during the first-grade year. While the fall kindergarten collection included the full ECLS-K:2011 sample, the fall firstgrade collection was conducted with children in one-third of the sample of primary sampling units (PSUs) selected for the study. These children are referred to as the fall subsample. The data collection schedule for second grade was similar to the schedule for first grade, with a fall second-grade collection that included the same subsample of children from the fall of first grade and a spring collection that included the entire sample of children who participated in at least one of the two base-year data collection rounds. In third grade, a spring data collection was conducted with the entire sample of children who participated in the base year. For fourth and fifth grade, spring data collections with the entire sample of children who participated in the base year are also planned. 4 Exhibit 1-1. Data collection schedule: School years 2010-11 through 2015-16 School 1 Grade indicates the modal grade for children who were in kindergarten in the 2010-11 school year. After the kindergarten rounds of data collection, children are included in data collection regardless of their grade level. 2 All but two rounds of data collection include the entire sample of children. The fall first-grade data collection included approximately one-third of the total ECLS-K:2011 sample of children. The fall second-grade data collection included the same subsample selected for the fall of first grade. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011)."}, {"section_title": "Overview of the Third-Grade Round of Data Collection", "text": "As described in chapter 1 of the base-year User's Manual, the ECLS-K:2011 collects information from children, parents, classroom teachers, special education teachers, and school administrators. In the base year, information was also collected from children's before-and after-school care providers. Data collection instruments for all of these different respondent types were included in the third-grade round of data collection, with the exception of the care provider questionnaires. The care provider component was included in the base year to obtain more information about young children's activities outside of school, which is particularly important for understanding differences in the educational environments of children attending full-day kindergarten and of those attending part-day kindergarten. The assessments and instruments used in third grade were largely the same as those used in earlier rounds to allow for longitudinal analysis. However, the earlier assessments and instruments were revised, as necessary, to make them appropriate for the third-grade data collections. For example, questions in the school administrator questionnaire asking about the school's second-graders were revised to ask 4 Beginning with the fall first-grade data collection, children who moved away from their original base-year schools were subsampled for follow-up. More information about the sample for third grade, including the subsampling of movers, is provided in chapter 4."}, {"section_title": "1-5", "text": "about the school's third-graders. One new instrument was introduced for third grade: the child questionnaire. Children completed an audio computer-assisted self-administered questionnaire about themselves. More detailed information about the third-grade study instruments, including how they differ from the instruments used in the kindergarten, first-, and second-grade rounds, is provided in chapter 2. In addition, during the third-grade round, hearing evaluations were again conducted with the same subsample of children who participated in the fall 2012 round of hearing evaluations. The procedures and data for the third grade hearing evaluations are described in a separate manual, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011 User's Manual for the Third-Grade Hearing Evaluations Component Data File (NCES 2018-090)  )."}, {"section_title": "ECLS-K:2011 Kindergarten-Fourth Grade (K-4) Public-Use Data File", "text": "The ECLS-K:2011 kindergarten-fourth grade (K-4) public-use data file includes the baseyear, first-grade, second-grade, third-grade, and fourth-grade data encompassing both the fall and spring rounds of data collection in kindergarten, first grade, and second grade and the spring rounds of data collection in third and fourth grade. The data file includes information for all students who participated during the kindergarten year even if they did not participate during later rounds. Third-grade data for students who did not participate in the third-grade round are set to \"system missing.\" The K-4 public-use file (PUF) is intended to replace the previously released PUFs; the K-4 PUF includes all of the cases included in prior PUFs and has some important corrections and updates to previously released data, including the child assessment scores. In preparing data files for release, NCES takes steps to minimize the likelihood that individual schools, teachers, parents, or students participating in the study can be identified. Every effort is made to protect the identity of individual respondents. The process of preparing the files for release includes a formal disclosure risk analysis. Small percentages of values are swapped across cases with similar characteristics to make it very difficult to identify a respondent with certainty. The modifications used to reduce the likelihood that any respondent could be identified in the data do not affect the overall data quality. Analysts should be aware that the ECLS-K:2011 data file is provided as a child-level data file containing one record for each child who participated in the base year. The record for each child contains information from each of the study respondents: the child, as well as his or her parent, teacher(s), school administrator and, if applicable, before-or after-school care provider."}, {"section_title": "1-6", "text": "The ECLS-K:2011 K-4 data are provided in an electronic codebook (ECB) that permits analysts to view the variable frequencies, tag selected variables, and prepare data extract files for analysis with SAS, SPSS, or Stata. The public-use version of the data will be available online."}, {"section_title": "Contents of Manual", "text": "The remainder of this manual contains more detailed information on the third-grade data collection instruments (chapter 2) and the direct and indirect child assessments (chapter 3). It also describes the ECLS-K:2011 sample design and weighting procedures (chapter 4), response rates and bias analysis (chapter 5), and data preparation procedures (chapter 6). In addition, this manual describes the structure of the data file and the composite variables that have been developed for the file (chapter 7). The last chapter of this manual contains a short introduction to the Electronic Codebook (ECB) and how to use it (chapter 8). Additional information about the ECLS-K:2011 study design, methods, and measures can be found in the earlier round user's manuals noted above, as well as in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Kindergarten Psychometric Report (Najarian et al. forthcoming), the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, First-Grade and Second-Grade Psychometric Report (Najarian et al. forthcoming), and the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Third-through Fifth -Grade Psychometric Report (Najarian et al. forthcoming). Also, as noted earlier, additional information about the ECLS program can be found online at https://nces.ed.gov/ecls."}, {"section_title": "2-1", "text": ""}, {"section_title": "DATA COLLECTION INSTRUMENTS AND METHODS", "text": "This chapter describes the data collection instruments used in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) third-grade round of data collection, including the child assessments, child questionnaire, parent interview, school administrator questionnaires, and teacher questionnaires. 1 Differences between earlier rounds of data collection and the third-grade round in the study instruments and data collection procedures are discussed. For more information on the earlier rounds of data collection instruments and methods, consult the user's manuals for those rounds."}, {"section_title": "Data Collection Instruments", "text": "The design of the ECLS-K:2011 and its survey instruments is guided by a conceptual framework of children's development and learning that emphasizes the interaction among the various environments in which children live and learn and the resources within those environments to which children have access. A comprehensive picture of children's environments and experiences is created by combining information from children themselves, their parents, their school administrators, their teachers, and their kindergarten before-and after-school care providers. Exhibit 2-1 presents a listing of the ECLS-K:2011 data collection instruments and the rounds of data collection in which they were used. The instruments for the kindergarten, first-grade, second-grade, and third-grade collections are included on the ECLS-K:2011 kindergarten-third grade (K-3) restricteduse DVD and are available online at https://nces.ed.gov/ecls, with the exception of copyrighted materials or items adapted from copyrighted materials that cannot be publicly distributed without copyright holder and NCES permission. Study instruments and items for which copyright permissions are needed are discussed further in section 2.1.7. The information collected in the ECLS-K:2011 instruments can be used to answer a wide variety of research questions about how home, school, and neighborhood factors relate to children's cognitive, social, emotional, and physical development. Sections 2.1.1 through 2.1.6 describe the major topics covered in each instrument. 1 For ease of presentation, this chapter refers to all students as \"third-grade students\"; however, the reader should keep in mind that some children had been retained in a grade and a very small number of students had been advanced to a higher grade. These children are included in the group being referred to as third-graders."}, {"section_title": "2-2", "text": "Exhibit 2-1. Instruments used in the ECLS-K:2011 kindergarten, first-, second-, and third-grade rounds of data collection: School years 2010-11, 2011-12, 2012-13, and   2-4 through the assessment in English; therefore the language screener was not administered beyond the spring of first grade. Cognitive domains. The third-grade cognitive assessment focused on four domains: reading (language use and literacy), mathematics, science, and executive function (working memory and cognitive flexibility). For the reading, mathematics, and science assessments, assessors asked the children questions related to images or text that were presented on a small easel, such as pictures, words, or short sentences for reading; numbers and number problems for mathematics; and predictions based on observations and cause-and-effect relationships for science. For the reading assessment, children were also asked questions about short reading selections they were asked to read in a passages booklet developed for the assessment. These questions were also presented on the easel. Children were not required to explain their reasoning. The executive function component included a computer-administered card sort task, for which children entered responses in the assessor's laptop computer, and a backward digit span task, for which children provided verbal responses to the assessor. A brief description of each of the cognitive assessment components follows. Reading (language and literacy). The reading assessment included questions measuring basic skills (e.g., word recognition), vocabulary knowledge, and reading comprehension. Reading comprehension questions asked the child to identify information specifically stated in text (e.g., definitions, facts, supporting details); to make complex inferences within texts; and to consider the text objectively and judge its appropriateness and quality. The reading assessment began with a set of 20 routing items, with the children's score on these items determining which second-stage form (low, middle, or high difficulty) the child received. Mathematics. The mathematics assessment was designed to measure skills in conceptual knowledge, procedural knowledge, and problem solving. The assessment consisted of questions on number sense, properties, and operations; measurement; geometry and spatial sense; data analysis, statistics, and probability; and patterns, algebra, and functions. A set of 17 routing items was administered to all children, and the score on these items determined which second-stage test (low, middle, or high difficulty) a child received. Most of the text that the children could see on the easel pages, for example, question text for word problems or graph labels, was read to them by the assessor to reduce the likelihood that the children's reading ability would affect their mathematics assessment performance. 3 Paper and pencil were offered to 3 Numbers were read to the child only when the question text referenced the number."}, {"section_title": "2-5", "text": "the children for use during the mathematics assessment, and children were periodically reminded of the availability of paper and pencil as part of the assessment protocol. Science. The science assessment domain included questions about physical sciences, life sciences, Earth and space sciences, and scientific inquiry. The science assessment included 14 routing items that all children received, followed by one of three second-stage forms (low, middle, or high difficulty). As with reading and mathematics, the second-stage form children received depended on their responses to the routing items. The questions, response options, and any text the children could see on the easel pages (for example, graph labels) were read to the children to reduce the likelihood that their reading ability would affect their science assessment score.  (Zelazo 2006). Different versions of the DCCS were used in different rounds of data collection because there was no single task that was age appropriate across all rounds of data collection when the study began. During the kindergarten and first-grade rounds, the hard-copy or physical version of the DCCS, as described in Zelazo 2006, was administered using cards that children were asked to sort into piles. Because the physical version of the DCCS would have been too easy for the majority of the study children during the second-grade rounds, beginning in the fall second-grade round children were administered a new, ageappropriate, computerized version of the DCCS in which the \"cards\" are presented on a computer screen and children sort them into \"piles\" on the computer screen using keys on the keyboard to indicate where to place each card. The computerized task was developed as part of the National Institutes of Health (NIH) Toolbox for the Assessment of Neurological and Behavioral Function and is appropriate for ages 3-85 (Zelazo et al. 2013). The NIH Toolbox DCCS has two different administrations based on the age of the child: one for children 7 years and younger and one for children 8 years and older. The task had been under development during the kindergarten and first-grade rounds of data collection but became available in time to be incorporated into the second-grade data collections. The ECLS-K:2011 used the version for children 8 years and older beginning in the fall second-grade round. Although the physical and the computer versions assess the same construct, the scoring and the way in which the construct is assessed differ across the two tasks (for information on scoring, see section 3.2.1). Like the physical version of the DCCS administered in the kindergarten and first-grade data collections, the computerized version asks children to sort cards either by shape or color. However, rather 2-6 than administer the cards in sections with a consistent sorting rule (with cards first sorted only by color, then only by shape, and finally by color or shape depending on whether a card had a black border), in the computerized DCCS the sorting rules are intermixed across the 30 trials of the task. In the computerized DCCS, one rule is more common than the other to build a response tendency (i.e., a response that is \"preferred\" because it happens more frequently, resulting in a predisposition to respond in that manner). Also, whereas performance on the physical version is measured by sorting accuracy, performance on the computerized version is measured as a function of both accuracy and reaction time. Reaction time is calculated based on reaction time only for trials using the sorting rule that is presented less often and only when there is a correct response. The reaction time of the less frequent trials or nondominant trials is of most interest because when a child is predisposed to respond in a particular way, it is harder and takes more time to inhibit that response tendency and switch the response to maintain accuracy. As children get older, it is important to incorporate reaction time into the DCCS score because older children and adults tend to slow down in order to respond accurately. Younger children do not tend to show a speed/accuracy tradeoff, and therefore accuracy is a better metric of performance for young children (Davidson et al. 2006). Performance on the computerized version of the DCCS is derived from a formula that takes into consideration both accuracy and reaction time (Zelazo et al. 2013;. After the card sort, children were administered the Numbers Reversed task, which is a measure of working memory. In this task, children were asked to repeat strings of orally presented numbers in reverse order. The sequence of numbers became increasingly longer, up to a maximum of eight numbers. The task was ended when children responded incorrectly to three consecutive number sequences, so that they would not be asked to continue at a level that was too difficult, or when all number sequences had been completed. Height and weight measurement. In addition to the cognitive domains described above, children's height and weight were measured during each data collection. A Shorr board (a tall wooden stand with a ruled edge used for measuring height) and a digital scale were used to obtain the measurements. 4 Assessors recorded the children's height (in inches to the nearest one-quarter inch) and weight (in pounds to one decimal place) on a height and weight recording form and then entered the measurements into a laptop computer. Each measurement was taken and recorded twice to ensure reliable measurement. Hearing evaluations. In the spring third-grade data collection, a subsample of the children also had their hearing evaluated by specially trained health technicians. Study protocol called for the health technicians to conduct the 15-minute hearing evaluations immediately after each selected child's assessment and height and weight measurement. 5 For the hearing evaluation, the health technician first asked the child a few questions about his or her hearing and recent experiences that could affect the results of the evaluation, including whether the child had an earache or recent cold or had recently heard any loud noises. Next, the child's ears were visually examined to see if there was any blockage that could affect the evaluation. The child's responses to the questions and the results of the visual examination were entered into a laptop computer. Then the health technician used a tympanometer to measure inner-ear functioning. Finally, the child listened to short tones of various pitches and decibel levels that were presented through headphones connected to an audiometer in order to determine hearing thresholds (the softest sounds the child could hear) for each ear. The data collected from the tympanometer and audiometer were automatically transferred from the hearing equipment and saved to the health technician's laptop."}, {"section_title": "Child Questionnaire", "text": "Beginning in the spring of third grade, a child questionnaire (CQ) was administered to children prior to the cognitive assessment components. The questionnaire had 37 questions and took approximately 11 minutes to complete. Unlike the hard-copy child questionnaires that were administered during the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) by assessors who read the questions/items to the children, the ECLS-K:2011 child questionnaire was administered on a computer using audio computer-assisted self-interview (audio-CASI) technology and headphones. Children listened as the software system read the instructions and questionnaire items. One questionnaire item at a time was displayed on the laptop's screen, and a recorded human voice read each question and response options to the child. The child responded by selecting the desired response on the laptop's touch screen. The audio-CASI questionnaire standardized administration and accommodated the variation in children's reading ability levels. It also allowed the child to hear and respond to the questions/items in private and limited distractions for the child because the headphones worn during the administration minimized extraneous noise. content overlaps somewhat with the third-grade child questionnaire administered in the ECLS-K. During the third-grade and fifth-grade data collections of the ECLS-K, children were asked about their interest and competence in reading, mathematics, and all subjects and about their perceptions about their peer relationships. The child questionnaire for third-grade in the ECLS-K:2011 asked many of the same questions about reading, mathematics, and peers and adapted questions about \"all subjects\" to ask about interest and competence specifically in science. New to the ECLS-K:2011 child questionnaire were additional items relating to peers. Children were asked questions about prosocial behavior toward peers and about social anxiety, specifically fear of negative evaluation by peers. Using questions parallel to teacherand parent-reported items, children were asked about peer victimization. Children were also asked new items on how happy they were with different aspects of their lives. Adapted from the Self Description Questionnaire I (SDQI) \u00a9 Herbert Marsh. SELF Research Centre (Bankstown Campus) University of Western Sydney, Australia. Used with permission. 2 Peer victimization items were adapted from a 21-item scale by Espelage, D. L. and Holt, M. (2001). Bullying and victimization during early adolescence: Peer influences and psychosocial correlates. Journal of Emotional Abuse, 2: 123-142. 3 Adapted from the Social Anxiety Scale for Children-Revised \u00a91993 Annette M. La Greca, University of Miami. Used with permission. La Greca, A. M. and Stone, W. L. (1993). Social anxiety scale for children-revised: Factor structure and concurrent validity. Journal of Clinical Child Psychology, 22(1): 17-27. 4 Adapted from the Children's Social Behavior Scale-Self Report (CSBS-S). Crick, N.R. and Grotpeter, J.K. (1995). Relational aggression, gender, and social psychological adjustment. Child Development, Adapted from the NIH Toolbox for Assessment of Neurological and Behavioral Function (version 1. 2-9\nIn the spring of third grade, a child questionnaire was administered to children prior to the cognitive assessment components. As discussed in section 2.1.1, the ECLS-K:2011 child questionnaire (CQ) was administered on a computer using audio computer-assisted self-interview (audio-CASI) technology and headphones. Children were asked questions about their interest and competence in reading, mathematics, and science, as well as items relating to peers, such as their perceptions about their peer relationships, prosocial behavior toward peers, and social anxiety, specifically fear of negative evaluation by peers. Using questions parallel to teacher-and parent-reported items, children were asked about peer victimization. Children were also asked items on how happy they were with different aspects of their lives. Exhibit 3-3 shows the content areas included in the third-grade child questionnaire and the corresponding item-level variables along with their sources. Variable names for the item-level data begin with \"C7\" for spring third grade. Items in the child questionnaire were adapted from existing scales and were used with the permission of the author. Data for the individual items are included in the K-3 data file, "}, {"section_title": "Parent Interview", "text": "A parent interview was conducted during the spring of third grade. Unlike the kindergarten, first-grade, and second-grade data collections that had both fall and spring interviews, an interview was not conducted in the fall of third grade. The average length of the spring third-grade parent interview was approximately 36 minutes. The spring third-grade parent interview was longer than the spring second-grade parent interview and shorter than both the spring kindergarten and spring first-grade parent interviews, but captured much of the same information. The spring third-grade parent interview included many of the same questions that were included in the kindergarten, first-grade, and second-grade rounds of the study, for example, questions about parent involvement in the child's school; homework; time children spent playing video games; children's participation in out-of-school activities; whether there had been a change in the relationship of one of the parent figures to the child (e.g., adoption); and child health and well-being. In addition, information about children's country of origin was collected if it had not been collected in kindergarten, first grade, or second grade. The spring third-grade parent interview also included some questions that were added in the spring of second grade, including nonresident parents' country of origin and peer victimization. New to the third-grade data collection were questions about whether the parent monitored that the child's homework had been completed, the child's specific ethnic origin (that is, to what specific Hispanic/Latino, Asian, or Pacific Islander group the child belongs, how many hours of sleep the child got on a school night, and whether parents had been (since the child was born) or currently were on active duty in the military; questions to assess the child's working memory; and questions about whether the child had been on field trips focused on science activities. Exhibit 2-3 shows the content areas included in the parent interview in the fall and spring of kindergarten, first grade, and second grade, and in the spring of third grade, by data collection round. While many of the same topics were addressed in multiple rounds, there were some differences in the specific questions asked for each topic. For example, there was only one question about employment in the spring of third grade, but multiple questions about employment in earlier grades. Also, questions about whether parents were on active duty in the military were asked in the employment section of the spring third-grade parent interview, but were not asked in earlier grades."}, {"section_title": "2-10", "text": "Exhibit 2-3. Parent interview topics, by round of data collection in the ECLS-K:2011: School years 2010-11, 2011-12, 2012-13, and  Child disabilities and services 3 X X X X X X Child health and wellbeing Child social skills, problem behaviors, and approaches to learning 4 X X X X Country of origin of parent and child 5 X X X X Family structure X X X X X Food sufficiency and food consumption X X X Household roster X X X X X Home environment, activities, resources, and cognitive stimulation 6 Parent income and assets X X X X Parent involvement with the child's education X X X X X Parent marital history 5 X X Parent marital status X X X X X See notes at end of exhibit."}, {"section_title": "2-11", "text": "Exhibit 2-3. Parent interview topics, by round of data collection in the ECLS-K:2011: School years 2010-11, 2011-12, 2012-13, and  In the fall of kindergarten, questions were asked about current child care and child care in the year before kindergarten. In the spring of kindergarten, questions about child care in the year before kindergarten were asked if information had not been collected in the fall. In the fall of first and second grades, questions were about child care during the previous summer. In the spring of first, second, and third grades, questions asked about current child care. 2 Questions about child demographic characteristics were asked in the fall and spring of kindergarten and then asked in later rounds of the study if the information was missing from a previous round. Questions about the child's specific ethnic origin were asked in the spring third-grade parent interview. 3 Questions in the fall first-and second-grade interviews were about services for special needs or participation in a special education program over the previous summer. Questions about disabilities and services in other rounds of the study were not limited to the past summer. 4 In the spring of third grade, the questions in this section were about working memory. In previous rounds of the study, the questions were about social skills, behavior, and approaches to learning. 5 Asked if information had not been collected in a previous round. 6 Questions in the fall first-and second-grade interviews were about home activities, outings with family members, camps, and summer school during the previous summer. Questions in other rounds of the study were not limited to the summer. 7 In the spring of third grade, employment was asked about in a single question about whether a parent figure worked part-time, full-time, was a stay-at-home parent or guardian, or was not working. In previous rounds of the study, multiple questions about employment and occupation were asked. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, and spring 2014. The parent interview was conducted by telephone for most cases. The respondent to the parent interview was usually a parent or guardian in the household who identified himself or herself as the person who knew the most about the child's care, education, and health. During the spring third-grade data collection round, interviewers attempted to complete the parent interview with the same respondent who 2-12 completed the parent interview in the previous rounds. Another parent or guardian in the household who knew about the child's care, education, and health was selected if the previous respondent was not available. The parent interview was fully translated into Spanish before data collection began and was administered by bilingual interviewers if parent respondents preferred to speak in Spanish. The parent interview was not translated into other languages because it was cost prohibitive to do so. However, interviews were completed with parents who spoke other languages by using an interpreter who translated the English version during the interview."}, {"section_title": "General Classroom Teacher Questionnaires", "text": "Unlike the kindergarten, first-grade, and second-grade years, there were no fall teacher questionnaires administered during the third-grade year. Similar to the spring kindergarten, first-grade, and second-grade collections, both teacherlevel and child-level questionnaires were included in the third-grade year. The general classroom teachers of children in the study completed two self-administered hard-copy questionnaires about themselves and their classrooms as well as a child-level questionnaire for each child in their classrooms who was participating in the ECLS-K:2011. The purpose of the teacher-level questionnaires was to collect information about the classroom environment and experiences that may relate to children's academic and social development. They included questions about the classroom and student characteristics, class materials, instructional practices and curricula, evaluation practices, parent involvement, teacher's background, teaching experience, staff development and support activities, and attitudes about teaching and the school climate. In prior years these topics were contained in a single teacher-level questionnaire, but for third grade the instructional practices and curricula items were moved into a separate teacher-level subject-area questionnaire. These items were grouped together by subject area in separate sections pertaining to reading and language arts, mathematics, science, and social studies and captured information about the specific skills and concepts taught by the subject-area teacher(s). This separate subject-area questionnaire was shared by the general classroom teacher with other teachers if the study child was taught by another teacher for one or more of these specific subject areas. The items in this separate teacher-level subject-area questionnaire were parallel to the subject-specific skills and concepts items in the second-grade 2-13 teacher-level questionnaire but were revised to reflect third-grade standards in the Common Core State Standards. 6 Taken together, the two teacher-level questionnaires used in the spring of the third-grade year were very similar in content to those that were used in the spring kindergarten, spring first-grade, and spring second-grade collections. There were no new constructs added for third grade and some items were omitted in order to shorten the questionnaire. These omitted items included items on the following topics: \uf06e distribution of students in the classroom by age, race/ethnicity, and gender; \uf06e some items about teachers and students who speak languages other than English (non-English languages used by teachers in the classroom, by purpose; use of written materials in languages other than English; and non-English languages spoken by children other than those who are English language learners; the type of instruction English language learners received); \uf06e time spent working independently, in small groups, or large groups; \uf06e availability of certain resources and materials other than technology-related equipment; \uf06e use of achievement groups (although this topic was captured for the study children in the child-level questionnaire); \uf06e some teacher background items (highest level of education completed by teacher's parents; number of years teacher has taught in current school unless it is his or her first year in the school; status on National Board for Professional Teaching Standards certification; name and location of university attended for highest degree obtained; whether teacher has taken college courses in early childhood education, special education, child development, and English as a Second Language; area of certification; and status regarding Highly Qualified Teacher requirements); \uf06e some items related to methods of assessment (types of tests and other activities used for student evaluation and how student progress is communicated to parents); and \uf06e some items related to specific professional development activities (types of training activities and peer observation). The purpose of the child-level questionnaires was to collect information specifically about each study child's experiences and performance in the classroom. Information was collected in the childlevel questionnaires about the child's academic and cognitive abilities, behavior, social skills, executive 6 See www.corestandards.org for further information. An effort led by state governors and state commissioners of education to develop the Common Core State Standards for kindergarten through grade 12 was begun in 2009, through the National Governors Association Center for Best Practices and the Council of Chief State School Officers."}, {"section_title": "2-14", "text": "function, specific programs and services received, and achievement group placement in mathematics and reading, if applicable. The spring third-grade child-level questionnaire was very similar to the one used in the secondgrade data collection. A few items were omitted for third grade, including the falling behind with work and why, additional instruction outside of school, assessment accommodations for disability (although this is included in the special education teacher child-level questionnaire), teacher's prediction of the child's educational attainment, and some items on parent involvement (specific activities in which parents were involved and communication with parents). New items were added to the child-level questionnaire in third grade to capture aspects of the students' peer relationships and of working memory. Exhibits 2-4 and 2-5 show the topics addressed in the kindergarten, first-grade, second-grade, and third-grade teacher-level questionnaires and child-level questionnaires, respectively, by data collection round. Although the same topics are included across rounds, the actual items can vary by data collection round."}, {"section_title": "2-15", "text": "Exhibit 2-4. General classroom teacher teacher-level questionnaire topics, by round of data collection in the ECLS-K:2011: School year 2010-11, spring 2012, spring 2013, and spring 2014 Activities and resources related to Response to Intervention programs X X X X Teacher evaluation and grading practices Meeting with other teachers X Respect from and cooperation with other teachers Teacher's views on teaching, school climate, and environment X X X X X X Teacher's experience, education, and background X X 2 X X X X Exhibit 2-5. General classroom teacher child-level questionnaire topics, by round of data collection in the ECLS-K:2011: School years 2010-11, 2011-12, 2012-13, and  2-17"}, {"section_title": "Special Education Teacher Questionnaires", "text": "As was done in each year from kindergarten through second grade, a set of special education teacher questionnaires was completed in the spring of the third-grade year for each participating child with an Individualized Education Program (IEP) or equivalent program on record with the school. The respondent to the questionnaire could have been a staff member identified as the child's special education teacher, a related service provider if the child was not taught by a special education teacher, or the child's general classroom teacher if that teacher provided all of the child's education and services required by an IEP. Two self-administered hard-copy instruments were used, a teacher-level questionnaire and a childlevel questionnaire. The special education teacher-level questionnaire used in third grade contained the same topics and most of the same items as the questionnaire used in the second-grade round of data collection. It collected information on the special education teacher's background, education, teaching experience, teaching position, and caseload. The special education teacher child-level questionnaire addressed the following topics: current services received through an IEP, child's disabilities (primary and all those for which the child received services), IEP goals and meeting those goals, classroom placement, expectations regarding general education goals, the special education teacher's communication with other teachers and the child's parents, grade placement, and participation in assessments. Exhibit 2-6 shows the topics addressed in the kindergarten, first-grade, second-grade, and third-grade special education teacher-level and child-level questionnaires by data collection round."}, {"section_title": "2-18", "text": "Exhibit 2-6. Special education teacher questionnaire topics, by round of data collection in the ECLS "}, {"section_title": "School Administrator Questionnaires", "text": "As in first and second grades, there were two versions of the school administrator questionnaire (SAQ) used in third grade: (1) a version for schools that were new to the study or for which a completed school administrator questionnaire was not received in a prior data collection, and (2) a shorter version for schools for which a school administrator questionnaire had been completed in a prior year. To"}, {"section_title": "2-19", "text": "reduce respondent burden, the shorter version did not include questions for which the responses were not expected to change significantly from year to year, for example, grades offered by the school, type of school (public, private, magnet, charter), adequacy of facilities, and neighborhood problems. The school administrator questionnaires were hard-copy paper questionnaires completed by the school principal/administrator and/or his or her designee during the spring data collection round of the third-grade year. The school administrator questionnaires addressed the following topics: school characteristics; facilities and resources; school-family-community connections; school policies and practices; implementation of Response to Intervention programs and practices; school programs for particular populations (language minority children and children with special needs); federal programs; staffing and teacher characteristics; and school administrator characteristics and background. The school administrator questionnaires for the third grade-both that for new schools and that for returning schools-were very similar to those used in the second-grade year. Compared with the second-grade questionnaires, in third grade the following topics were omitted from both versions of the SAQ: school capacity, hearing and vision screenings, and school policies regarding uniforms and grade retention. A few additional topics were omitted from the version for returning schools, including student race/ethnicity distribution, students attending the school under school choice or for special needs, schoolbased programs for families, and recent changes in school enrollment. Items that referred specifically to second grade were reworded to refer to third grade. Exhibit 2-7 shows the topics addressed in the kindergarten, first-grade, second-grade, and third-grade school administrator questionnaires by data collection round."}, {"section_title": "2-20", "text": "Exhibit 2-7. School administrator questionnaire topics, by round of data collection in the ECLS-K:2011:  School programs for particular populations (language minority children and children with special needs) Health technicians, who accompanied the teams in the schools to conduct the hearing evaluations, were trained in a 5-day, in-person training. For more information on the hearing evaluations component, see the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011 User's Manual for the Third-Grade Hearing Evaluations Component Data File (NCES 2018-090) . Advance school contact in the fall. Advance school contact for third grade remained the same as in the fall of second grade. Data collection. Data collection procedures used in third grade were the same as those used during the second-grade year. As described above, however, revisions were made to the instruments that had been used in the second-grade rounds. A child questionnaire was added in third grade and was administered via audio computer-assisted self-interviewing (audio-CASI). Also, the hearing evaluation component was conducted in the subset of schools in which the evaluation was first conducted in the fall of second grade. Tracing activities. Tracing activities for the third-grade round remained the same as those used in the second-grade rounds. Quality control. Quality control and validation procedures for the third-grade round remained the same as those used in in the second-grade rounds. This page intentionally left blank. 3-1"}, {"section_title": "ECLS-K:2011 DIRECT AND INDIRECT ASSESSMENT DATA", "text": "This chapter provides information primarily about the direct and indirect assessment data from the third-grade collection of the ECLS-K:2011. The chapter begins with a description of the direct cognitive assessments, providing information about the scores available in the data file. The chapter then presents information on the executive function assessments. Beginning in third grade, study children completed a child questionnaire, which is described in a section on children's reports about themselves, their interests, and their feelings. Finally, the chapter closes with information on teacher-and parent-reported assessments of children's cognitive and socioemotional knowledge and skills. This chapter includes information about assessment data from the kindergarten through second-grade rounds of data collection in three instances: when those data have been changed since their release on previous files, when new data from those rounds have been added to the kindergarten through fourth-grade (K-4) data file, and when necessary to illustrate how third-grade data related to a particular measure or construct differ from data related to the same measure or construct released for the earlier rounds. Information about assessments that were used in prior rounds but not in third grade, for example the Spanish Early Reading Skills (SERS) assessment, and about scores that were produced only for earlier rounds, such as raw number-right scores, can be found in the Early Childhood Longitudinal Study,  )."}, {"section_title": "Direct Cognitive Assessment: Reading, Mathematics, and Science", "text": "The kindergarten, first-grade, second-grade, and third-grade direct cognitive assessments measured children's knowledge and skills in reading, mathematics, and science. This section presents information about the assessment scores available in the data file. More detailed information about the 3-2 development of the scores, including a more complete discussion of item response theory (IRT) procedures, can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Kindergarten Psychometric Report (Najarian et al. forthcoming), in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, First-Grade and Second-Grade Psychometric Report (Najarian et al. 2016-122), and in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Third-through Fifth-Grade Psychometric Report (Najarian et al. forthcoming). A description of the administration of the direct assessments is provided in chapter 2. It must be emphasized that the assessment scores described below are not directly comparable with those developed for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). Although the IRT procedures used in the analysis of data were similar in the ECLS-K and in the ECLS-K:2011, each study incorporated different items and the resulting scales are different."}, {"section_title": "IRT-Based Scores Developed for the ECLS-K:2011", "text": "Broad-based scores using the full set of items administered in the kindergarten, first-grade, second-grade, and third-grade assessments in reading, mathematics, and science were calculated using IRT procedures. IRT is a method for modeling assessment data that makes it possible to calculate an overall score for each domain measured for each child that can be compared to scores of other children regardless of which specific items a child is administered. This method was used to calculate scores for the ECLS-K:2011 because, as discussed in chapter 2, the study employed a two-stage assessment (in reading and mathematics in kindergarten and in reading, mathematics, and science in first, second, and third grades) in which children were administered a set of items appropriate for their demonstrated ability level rather than all the items in the assessment. Although this procedure resulted in children being administered different sets of items, there was a subset of items that all children received (the items in the routing tests, plus a set of items common across the different second-stage forms). These common items were used to calculate scores for all children on the same scale. IRT also was used to calculate scores for all children on the same scale for the science assessment fielded in the spring of kindergarten even though that assessment was not two-stage. In that assessment, the assortment of items a child received was not dependent upon routing to a second stage, but instead on omissions by the child or the discontinuation of the administration of the assessment. In those 3-3 cases, IRT was used to estimate the probability that a child would have provided a correct response when no response was available. IRT uses the pattern of right and wrong responses to the items actually administered in an assessment and the difficulty, discriminating ability, 1 and \"guess-ability\" of each item to estimate each child's ability on the same continuous scale. IRT has several advantages over raw number-right scoring. By using the overall pattern of right and wrong responses and the characteristics of each item to estimate ability, IRT can adjust for the possibility of a low-ability child guessing several difficult items correctly. If answers on several easy items are wrong, the probability of a correct answer on a difficult item would be quite low. Omitted items are also less likely to cause distortion of scores, as long as enough items have been answered to establish a consistent pattern of right and wrong answers. Unlike raw number-right scoring, which treats omitted items as if they had been answered incorrectly, IRT procedures use the pattern of responses to estimate the probability of a child providing a correct response for each assessment question. Finally, IRT scoring makes possible longitudinal measurement of gain in achievement, even when the assessments that are administered to a child are not identical at each point, for example, when a child was administered different levels of the second-stage form in the fall and spring data collections within one year or different sets of items across grades."}, {"section_title": "Theta and the Standard Error of Measurement (SEM) of Theta", "text": "A theta score is provided in the ECLS-K:2011 data file for each child who participated in the direct cognitive assessment for each cognitive domain assessed and for each data collection in which the assessment was administered. The theta score 2 is an estimate of a child's ability in a particular domain (e.g., reading, mathematics, or science) based on his or her performance on the items he or she was actually administered. The theta scores are reported on a metric ranging from -8 to 8, with lower scores indicating lower ability and higher scores indicating higher ability. Theta scores tend to be normally distributed because they represent a child's latent ability and are not dependent on the difficulty of the items included within a specific test. 1 The discriminating ability describes how well changes in ability level predict changes in the probability of answering the item correctly at a particular ability level. 2 Theta is iteratively estimated and re-estimated and the theta score is derived from the means of the posterior distribution of the theta estimate."}, {"section_title": "3-4", "text": "The standard error of theta provides a measure of uncertainty of the theta score estimate for each child. Adding and subtracting twice the standard error from the theta score estimates provides an approximate 95 percent confidence interval or range of values that is likely to include the true theta score. Unlike classical item theory, in which the precision of the scores is consistent across all examinees, IRT allows the standard error to vary. Larger standard errors of measurement can be the result of estimations of thetas in the extremes of the distribution (very low or very high ability) or for children who responded to a limited number of items (i.e., children who responded to all items administered generally have lower standard errors of measurement than those children responding to fewer items because more information about their actual performance is available, thereby making estimates of their ability more precise). Tables 3-1 and 3-2 list the names of the variables pertaining to the reading, mathematics, and science IRT theta scores and standard errors of measurement available in the data file, along with the variable descriptions, value ranges, weighted means, and standard deviations. 3 As can be seen in the tables, theta scores are available for all data collection rounds for reading and mathematics. For science, theta scores are available for all rounds except the fall of kindergarten; the science assessment was not included in that first round of data collection. The variable names and descriptions end with K3, indicating these are scores released on the kindergarten-third grade (K-3) longitudinal restricted-use data file. The method used to compute the theta scores allows for the calculation of theta for a given round that will not change based on later administrations of the assessments (which is not true for the scale scores, as described in the next section). Therefore, for any given child, the kindergarten, first-grade, second-grade, and third-grade theta scores provided in subsequent data files will be the same as theta scores released in earlier data files, with one exception: the reading thetas provided in the base-year data file. After the kindergarten-year data collection, the methodology used to calibrate and compute reading scores changed; therefore, the reading thetas reported in the base-year file are not the same as the kindergarten reading thetas provided in the files with later-round data. Any analysis involving kindergarten reading theta scores and reading theta scores from later rounds, for example an analysis looking at growth in reading knowledge and skills between the spring of kindergarten and the spring of first grade, should use the kindergarten reading theta scores from a data file released after the base year. The reading theta scores released in the kindergarten-year data file are appropriate for analyses involving only the kindergartenround data; analyses conducted with only data released in the base-year file are not incorrect, since those 3 The name and description for each variable in the tables begin with an \"X,\" indicating that it is a derived/calculated variable, and a data collection round number (1 for the fall kindergarten round, 2 for the spring kindergarten round, 3 for the fall first-grade round, 4 for the spring first-grade round, 5 for the fall second-grade round, 6 for the spring second-grade round, and 7 for the spring third-grade round). These variable naming conventions are used for all the variables mentioned in this chapter. More information about variable naming conventions can be found in chapter 7."}, {"section_title": "3-5", "text": "analyses do not compare kindergarten scores to scores in later rounds that were computed differently. However, now that the recomputed kindergarten theta scores are available in the kindergarten through firstgrade, kindergarten through second-grade, and kindergarten through third-grade data files, it is recommended that researchers conduct any new analyses with the recomputed kindergarten reading theta scores. For more information on the methods used to calculate theta scores, see the ECLS-K:2011 First-Grade and Second-Grade Psychometric Report (Najarian et al. forthcoming) and the ECLS-K:2011 Thirdthrough Fifth-Grade Psychometric Report (Najarian et al. forthcoming).  "}, {"section_title": "Scale Scores", "text": "The IRT-based overall scale score for each content domain is an estimate of the number of items a child would have answered correctly in each data collection round if he or she had been administered all of the questions for that domain that were included in the kindergarten, first-grade, second-grade, and third-grade assessments (that is, all of the 141 unique questions in the router and the three second-stage reading forms administered in kindergarten, first grade, second grade, and third grade; all of the 135 unique questions in the router and the three second-stage mathematics forms administered in kindergarten, first grade, second grade, and third grade; and all of the 87 unique items administered in the router and three 3-7 second-stage science forms in first grade, second grade, third grade, and in the single-stage kindergarten science form). To calculate the IRT-based overall scale score for each domain, a child's theta is used to predict a probability for each assessment item that the child would have gotten that item correct. Then, the probabilities for all the items fielded as part of the domain in every round are summed to create the overall scale score. Because the computed scale scores are sums of probabilities, the scores are not integers. Gain scores in each domain may be obtained by subtracting the IRT scale scores at an earlier round from the IRT scale scores at a later round. For example, subtracting the fall kindergarten mathematics score from the spring kindergarten mathematics score would result in a score indicating gain across the kindergarten year. Similarly, a gain score from kindergarten entry to the end of third grade would be obtained by subtracting the fall kindergarten mathematics score from the spring third-grade mathematics score. Users should note that the scale scores are only comparable across rounds within a single data file. In other words, the scale scores for a given domain in the K-3 data file are all comparable to one other, but they are not comparable to the scale scores for that domain reported in the previously released files. Although the thetas remain the same for a given domain across rounds, the scale scores are recomputed for each file because the scale scores represent the estimated number correct for all items across all assessments administered; the total number of items in the pool expands each year as more difficult items are added to the assessments. Scores for different subject areas are not comparable to each other because they are based on different numbers of questions and content that is not necessarily equivalent in difficulty. For example, if a child's IRT scale score in reading is higher than in mathematics, it would not be appropriate to interpret that to mean the child performs better in reading than in mathematics. Table 3-3 provides the names of the variables pertaining to the IRT scale scores available in the data file, along with the variable descriptions, value ranges, weighted means, and standard deviations. Table 3-3. Direct cognitive assessment: IRT scale scores, fall and spring kindergarten, fall and spring  first-grade, fall and spring second-grade, and spring third-grade assessments: School years  2010-11, 2011-12, 2012-13, and spring "}, {"section_title": "3-8", "text": ""}, {"section_title": "Variables Indicating Exclusion from the Direct Assessment Due to Disability", "text": "The variables X1EXDIS, X2EXDIS, X3EXDIS, X4EXDIS, X5EXDIS, X6EXDIS, and X7EXDIS can be used to identify children who were excluded from the assessment because they needed an accommodation the study did not provide or because they had an Individualized Education Program (IEP) that indicated they could not take part in standardized assessments. These variables are coded 1, Excluded from assessment due to disability, for children who were excluded from the assessment for these reasons. All other children are coded 0 for variables X1EXDIS, X2EXDIS, X4EXDIS, X6EXDIS, and X7EXDIS. For the variables pertaining to the fall first-grade and fall second-grade data collections (X3EXDIS and X5EXDIS), children who were part of the subsample in those rounds and not excluded 3-9 from the assessments are coded 0 and children who were not part of the subsample (and, therefore, not eligible for the assessments in these rounds) are coded as system missing. 4 "}, {"section_title": "Choosing the Appropriate Score for Analysis", "text": "When choosing scores to use in analysis, researchers should consider the nature of their research questions, the type of statistical analysis to be conducted, the population of interest, and the audience. The sections below discuss the general suitability of the different types of scores for different analyses."}, {"section_title": "\uf06e", "text": "The IRT-based theta scores are overall measures of ability. They are appropriate for both cross-sectional and longitudinal analyses. They are useful in examining differences in overall achievement among subgroups of children in a given data collection round or across rounds, as well as in analysis of correlations between achievement and child, family, and school characteristics. The fall kindergarten, spring kindergarten, fall firstgrade, spring first-grade, fall second-grade, spring second-grade, and spring third-grade theta scores included in the K-3 data file are on the same metric. Therefore, an analyst looking at growth across the kindergarten year could subtract the fall kindergarten score from the spring kindergarten score to compute a gain score. Or when looking at growth from kindergarten entry to the end of third grade, an analyst could subtract the fall kindergarten score from the spring third-grade score to compute a gain score. The theta scores may be more desirable than the scale scores for use in a multivariate analysis because their distribution generally tends to be more normal than the distribution of the scale scores. It is recommended that analysts review the distributions for normality. In assessments where the number of items or number of observations is low, the normality of the distribution may be affected. In the ECLS-K:2011, the kindergarten science and kindergarten and first-grade SERS distributions deviated from normal, due to the limited number of items and observations, respectively. Additionally, in the extreme tails of the theta distributions in each domain, a combination of some extremely low-performing and some extremely high-performing children who took the assessment and the instrument itself may result in clustered estimates. By design, in order to limit the length of the assessment and the number of too easy or too difficult items any one child would be administered, the assessment does not have many items administered at the difficulty ranges in the tails. Including more items appropriate for children at the ability extremes would have required a reduction in the number of items at the range of ability of nearly all the sampled children (> 99 percent). Thus, some clustering of thetas may be observed in the extreme tails of the theta distributions.\nThe IRT-based scale scores also are overall measures of achievement. They are appropriate for both cross-sectional and longitudinal analyses. They are useful in examining differences in overall achievement among subgroups of children in a given data collection round or in different rounds, as well as in analysis looking at correlations between achievement and child, family, and school characteristics. The fall kindergarten, spring kindergarten, fall first-grade, spring first-grade, fall second-grade, spring second-grade, and spring third-grade scale scores included in the K-3 data file are on the same metric. Therefore, an analyst looking at growth across the kindergarten year could subtract the fall kindergarten score from the spring kindergarten score to compute a gain score. Or when looking at growth from kindergarten entry to the end of third grade, an analyst could subtract the fall kindergarten score from the spring thirdgrade score to compute a gain score. Results expressed in terms of scale score points, scale score gains, or an average scale score may be more easily interpretable by a wider audience than results based on the theta scores.\nThe first set of questions asks where parent 1 and parent 2 were born (P7PARCT1 and P7PARCT2) and when, if applicable, they moved to the United States (P7PAREM1 and P7PAREM2).\nThe second set of questions asks about how often a parent 1 and parent 2 use a language other than English when speaking to the child (P7RES_1, P7RES_2) and how often the child uses a language other than English in speaking to a parent (P7CHL_1, P7CHL_2). There is also a set of \"pointer\" variables that hold the household roster number of the person who was the subject of the language questions (P7PLQHH1 and P7PLQHH2). However, since the language questions were asked only of parent(s) or parent figure(s) in the household, the value of language pointer variables is the same as the value for the composite parent identifier variables, other than when a pointer is not applicable. If a pointer is applicable, the household roster number indicated in the pointer variable for the language data for parent 1, P7PLQHH1, is always equal to the household roster number indicated in the composite parent identifier variable of parent 1, X7IDP1. Similarly, the household roster number indicated in the pointer variable for the language data for parent 2, P7PLQHH2, is always equal to the household roster number indicated in the composite parent identifier variable of parent 2, X7IDP2 (where applicable). The PLQ parent pointers are set based on P7ANYLNG and the parent identifiers X7IDP1 and X7IDP2. If P7ANYLNG = 2, -7, or -8 (no, refused, don't know), section PLQ is not applicable and the pointers are set to -1 (not applicable). Thus, if P7ANYLNG = 2, -7, or -8 (no, refused, don't know), P7PLQHH1 could be -1, even though there is a person for X7IDP1. If P7ANYLNG = -9 (not ascertained), the PLQ parent pointers are set to -9 (not ascertained). If P7ANYLNG = 1, then P7PLQHH1 will have a value that matches X7IDP1. If P7ANYLNG = 1, P7PLQHH2 will have a value that matches X7IDP2. If there is no parent 2, P7PLQHH2 = -1 (not applicable).\nThe third set of questions about parent 1 and parent 2 were about parent education. For parent education, there is also a second set of \"pointer\" variables that hold the household roster number of the person who was the subject of the education questions (P7PEQHH1 and P7PEQHH2). For the education questions, the pointer variables are applicable to up to two parents in the household. If there are two parents in the household, P7PEQHH1 and P7PEQHH2 are the roster numbers of the first and second parent, respectively. If there is only one parent in the household, P7PEQHH1 is the roster number of the first parent and P7PEQHH2 = -1 (not applicable). Since the parent education questions were asked only of parent(s) or parent figure(s) in the household, the value of parent education pointer variables is the same as the value for the composite parent identifier variables.\nAs in previous rounds of the study, there are races and ethnicities for persons on the data file who did not qualify to have race and ethnicity asked in the current round, but did qualify to have race and ethnicity collected in an earlier round of the study. Persons who qualify to have race and ethnicity on the file include the focal child, those with a relationship of mother/female guardian or father/male guardian in any round (P*REL_*= 1 or 2 or P*UNR= 3 or 4), those who were a respondent in any round (P*PER *= 1), and persons who were spouse/partners of respondent parents in any round. Some cases have additional race and ethnicity data that were collected in A-3 previous rounds when a household member was incorrectly coded as having a relationship that would make collecting his or her race/ethnicity applicable (e.g., a respondent or a parent figure), but was later reported in interviewer comments to be a boyfriend to the parent, or a sibling, other relative, or other nonrelative to the child. The race/ethnicity data for these persons who did not qualify to have race and ethnicity collected were retained on the data file for person 3 in the household for cases with CHILDID=10000654, 10003762, 10006539, 10011925; for person 4 in the household for cases with CHILDID=10006096, 10006539, 10011925, 10012510, 10012832, 10016707, 10017583; for person 5 in the household for cases with CHILDID = 10003905, 10005500, 10011049, 10012527, 10013433, 10014926, 10015826, 10017300, 10018006; for person 6 in the household for cases with CHILDID= 10006539, 10011611; and for person 10 in the household for cases with CHILDID=10010479.\nIn one case (CHILDID=10018155) there was a household member who was identified as a father or male guardian, but has a -7 (refused) answer for the type of father (e.g., biological, adoptive, etc.). Because the type of father was unknown, the CAPI program did not ask questions about the second parent in the household and variables associated with the second-parent questions are coded -9 (not ascertained).\nThere are cases that have a disability diagnosis for the focal child and have follow-up questions about that diagnosis recorded in variables other than those used for the child's specific diagnosis. In the parent interview, respondents were asked to provide the diagnosis of the child's disability, if applicable, in question CHQ125 (P7LRNDIS-P7OTHDIA). If a diagnosis did not fit one of the categories in the parent interview specifications, the diagnosis was entered as \"other.\" Follow-up questions about age at diagnosis and medication taken for a particular diagnosis (CHQ130-CHQ173) were asked about the diagnosis entered as \"other.\" During data editing and review of \"other\" responses conducted after the parent interview was completed, it was determined that some answers in the \"other\" category fit within existing codes that were available in the interview and were assigned codes for those existing categories. For example, in a situation in which the parent report was initially coded as an \"other\" diagnosis in CHQ125 but was later determined to be depression, the diagnosis was recategorized from \"other\" to depression (P7DEPRESS=1), but the information collected in followup questions about age at diagnosis and medication taken for a particular diagnosis (CHQ130-CHQ173) remain in the variables pertaining to the \"other\" category. If the category for depression was already chosen in CHQ125, the follow-up questions about age at diagnosis and medication taken for a particular diagnosis (CHQ130-CHQ173) would be both in the variables pertaining to depression and in the variables pertaining to the \"other\" category. Five cases (CHILDID= 10000833, 10000896, 10004043, 10009107, 10009905) are coded -9 (not ascertained) for DWQ010-DWQ077c and PPQ100-PPQ120, even though these cases did not experience a breakoff during the parent interview (i.e., X7BRKFNL=-1). These questions were not asked during the interview because of skip patterns in the interview specifications based on the relationship type of the respondent recorded at the time of the interview. After data collection, editing was conducted and changes were made to the household relationships such that these questions would have been asked had the newly edited relationships been recorded during data collection. As a result, these questions were coded -9 (not ascertained).\nThe nonresident parent section of the parent interview (NRQ) is designed to ask about biological and adoptive parents who are not in the household. If there is one adoptive parent in the household, questions are asked about contact the child may have with another adoptive parent who is not in the household. Questions in this section are asked about a nonresident adoptive parent who is the opposite sex of the adoptive parent in the household. Questions are not asked about a nonresident adoptive parent who is the same sex as the other adoptive parent in the household.\nRange checks include logical soft checks for continuous variables.\nConsistency checks include logical soft comparisons between related variables within a form to check for inconsistencies. When data were identified during quality control (QC) processes as possibly in error, the original questionnaire returned by the respondent was reviewed to determine whether the response was incorrectly captured during the questionnaire scanning process. For those cases listed in this section as having anomalous data, data reviewers confirmed that the data matched the response provided on the A-5 questionnaire and a reasonable correction(s) could not be determined. Therefore, the data were left as reported. Teacher Questionnaires: Spring 2014 Anomalies \uf06e The responses to T7TCMTH (Are you this student's primary [math] teacher?) for cases 10016226 and 10017615 are different and contradict the response to A7TCHMTH (Do you teach [math] to any or all students in your class?) despite the fact that both are linked to T7_ID 2016249. The data for these cases match the forms and are inconsistent responses provided by the respondent.\nIn two cases (CHILDID=10001835, 10015435), the child's height and weight measurements (C7HGT1, C7HGT2, X7HEIGHT, C7WGT1, C7WGT2, X7WEIGHT) are inconsistent with prior rounds. It appears the values may have been switched when they were entered into the assessment computer application, but this cannot be confirmed. Analysts should use their judgment in using the values from these cases. Analysts who choose not to use the reported values will also want to set X7BMI (body mass index) to missing for these cases, as BMI is based on the height and weight measures.\nIn two cases (CHILDID=10005679, 10015019), the parents' ages in X7PAR2AGE were updated to correct errors for the parents' ages in round 4.\nFor a small number of cases, values for X2KRCETH, X4RCETH, X2FLCH2_I, X2RLCH2_I, X4FMEAL_I, X4RMEAL_I, S4NMFRM_I, and S4NMRDM_I were corrected due to errors in imputation present in these variables on previously released data files.\nIn chapter 7, it is noted in the description of X7FRMEAL_I (Students Eligible for Free or Reduced-Price School Meals) that there are some schools for which is appears data were reported by administrators as number of students eligible, rather than as a percent. All such cases are coded in the highest category of X7FRMEAL_I on the public-use file. Data users are encouraged to use X7FRMEALFLG to identify these cases, evaluate A-6 whether their inclusion has an impact on analyses, and make statistical adjustments, if needed, that best serve the analysis goals.\nVariables with too few cases and/or a sparse distribution are suppressed in the K-4 PUF. The values for these variables are set to -2 and labeled \"suppressed\" in the Electronic Codebook (ECB). The value -2 means that the data for this variable are suppressed to protect the respondent's confidentiality.\nVariables that provide a particularly identifying characteristic, such as a specific disability, or information that could be matched against external data sources to obtain a specific identifying characteristic, such as exact date of marriage or divorce, are also suppressed. The values for these variables are set to -2."}, {"section_title": "3-10", "text": "For a broader audience of readers unfamiliar with IRT modeling techniques, the metric of the theta scores (from -8 to 8) may be less readily interpretable than the metric of the scale scores. Researchers should consider their analysis and the audience for their research when selecting between the theta and the scale score."}, {"section_title": "Analytic Considerations for Measuring Gains in the ECLS-K:2011", "text": "An important issue to be considered when analyzing achievement scores and gains is assessment timing: children's age at assessment, the date of assessment, and the time interval between assessments. Most sampled children were born throughout the second half of 2004 and first half of 2005, but their birth dates were not related to testing dates. As a result, children were tested at different developmental and chronological ages. Assessment dates ranged from August to December for the fall data collections, and from March to June for the spring data collections. Children assessed later in a data collection period in a particular grade level, for example in December during a fall collection, may be expected to have an advantage over children assessed earlier in the data collection period, for example in the first days or weeks of school, because they had more exposure to educational content before being assessed. Substantial differences in the intervals between assessments may also affect analysis of gain scores. Children assessed in September for the fall data collection and June for the spring data collection have more time to learn knowledge skills than do children assessed first in November and then again in March. These differences in interval may or may not have a significant impact on analysis results. In designing an analysis plan, it is important to consider whether and how differences in age, assessment date, and interval may affect the results; to look at relationships between these factors and other variables of interest; and to adjust for differences, if necessary."}, {"section_title": "3-11", "text": "When using the IRT scale scores as longitudinal measures of overall growth, analysts should keep in mind that gains made at different points on the scale have qualitatively different interpretations. Children who made gains toward the lower end of the scale, for example, in skills such as identifying letters and associating letters with sounds, are learning different skills than children who made gains at the higher end of the scale, for example, those who have gone from reading sentences to reading passages, although their gains in number of scale score points may be the same. Comparison of gains in scale score points is most meaningful for groups that started with similar initial status. One way to account for children's initial status is to include a prior round assessment score as a control variable in an analytic model. For example, the fall kindergarten scale score could be included in a model using the spring kindergarten scale score as the outcome."}, {"section_title": "Reliability of the ECLS-K:2011 Scores", "text": "Reliability statistics assess consistency of measurement, or the extent to which test items in a set are related to each other and to the score scale as a whole. For tests of equal length, reliability estimates can be expected to be higher for sets of items that are closely related to the underlying construct than for tests with more diversity of content. Conversely, for tests with similar levels of diversity in content, reliabilities tend to be higher for longer tests compared to shorter tests. Reliabilities range from 0 to 1. Table 3-4 presents the reliability statistics computed for the IRT-based scores for each subject area for the fall and spring of kindergarten, the fall and spring of first grade, the fall and spring of second grade, and the spring of third grade. The reliability of the overall ability estimate, theta, is based on the variance of repeated estimates of theta for each individual child compared with total sample variance. The reliabilities calculated for theta also apply to the scores derived from the theta estimate, namely, the IRT scale scores. The reliabilities are relatively high, ranging from .75 to .95. Science, the domain with the most diverse content and the smallest number of items, had lower reliability coefficients than reading and mathematics. 5 The reading reliability has decreased longitudinally, a result of the reduction in the number of items administered. 6 Table 3-4. Reliability of IRT-based scores (theta and scale scores), by round of data collection and  domain, for fall and spring kindergarten, fall and spring first grade, fall and spring second  grade, and spring third grade: School years 2010-11, 2011-12, 2012-13, and spring "}, {"section_title": "Validity of the ECLS-K:2011 Scores", "text": "Evidence for the validity of the direct cognitive assessments was derived from several sources. A review of national and state performance standards, comparison with state and commercial assessments, and the judgments of curriculum experts all informed the development of the test specifications. The content category specifications for the ECLS-K:2011 reading assessments in kindergarten through second grade are based on the 2009 Reading Frameworks for NAEP (National Assessment Governing Board 2008), with the addition of basic reading skills and vocabulary categories suited for the earlier grades. Although the NAEP framework was selected for its rigorous design and its use in many years of national administrations by NCES, because the NAEP assessments are administered starting in fourth grade, it was necessary to consult other sources to extend the NAEP content percentage specifications down to earlier grades. Experts in reading assessment development consulted the ECLS-K kindergarten, firstgrade, and third-grade reading assessment frameworks; current curriculum standards from Texas, California, New Jersey, Florida, and Virginia; and the Common Core State Standards. 7 The ECLS-K:2011 reading specifications for third grade are built upon those developed for the earlier grades and supplemented by the fourth-and eighth-grade NAEP Reading Frameworks for 2011 (National Assessment Governing Board 2010), as well as the third-grade standards from the same five states noted. "}, {"section_title": "3-13", "text": "The ECLS-K:2011 mathematics test specifications for kindergarten through second grade are based on the frameworks developed for the ECLS-K assessments, which were based on the NAEP mathematics frameworks and extended down to earlier grades. The content of the mathematics framework is consistent with recommendations presented in the Mathematics Framework for the 2005 NAEP (National Assessment Governing Board 2004a), the National Council of Teachers of Mathematics Principles and Standards for School Mathematics (2000), and with state standards of California, New Jersey, Tennessee, Texas, and Virginia. These are also consistent with general findings from the National Mathematics Advisory Panel (2008). For third grade, the content covered in the ECLS-K:2011 mathematics assessment was determined by comparing the state or national standards from Texas, Virginia, NAEP, and the National Council of Teachers of Mathematics (NCTM). Common Core State Standards were not used in the comparison since these standards are similar to the national standards set by NCTM and NAEP. As in reading, the framework in the later grades builds on the framework developed for the earlier grades, using the same sources. The science knowledge and skills assessed in the ECLS-K:2011 were chosen based on the areas identified as being important to assess in the 1996-2005 NAEP science framework (National Assessment Governing Board 2004b). However, because the NAEP science frameworks begin in fourth grade, the science standards of six states (Arizona, California, Florida, New Mexico, Texas, and Virginia) were analyzed to find common topics that are taught at the lower grade levels. In these states and for each grade level, three or four standards were drawn from each of four common content categories (scientific inquiry, life science, physical science, and Earth and space science) and these four areas were selected as the content categories for the ECLS-K:2011 science assessment framework. Pools of potential assessment items were developed for each content domain based on the framework or standards pertinent to the domain. An expert panel of school educators, including curriculum specialists in the subject areas, then examined the pool of items for content and framework strand design, accuracy, nonambiguity of response options, and appropriate formatting. The items were included in a field test and better performing items were selected for the final assessment battery."}, {"section_title": "Direct Cognitive Assessment: Executive Function", "text": "Executive functions are interdependent processes that work together to regulate and orchestrate cognition, emotion, and behavior and that help a child to learn in the classroom. Two measures"}, {"section_title": "3-14", "text": "of executive function were included in the kindergarten, first-grade, second-grade and third-grade direct child assessment battery: the Dimensional Change Card Sort (DCCS) (Zelazo 2006;Zelazo et al. 2013), assessing children's cognitive flexibility, and the Numbers Reversed subtest of the Woodcock-Johnson III (WJ III) Tests of Cognitive Abilities (Woodcock, McGrew, and Mather 2001), assessing working memory. The same versions of the DCCS and the Numbers Reversed tasks were administered in fall and spring of the kindergarten year and fall and spring of first grade. In second grade, the DCCS was changed to computerized administration to remain age-appropriate through fifth grade. The Numbers Reversed task remained the same as used in the earlier data collection rounds."}, {"section_title": "Dimensional Change Card Sort", "text": "The Dimensional Change Card Sort (DCCS) (Zelazo 2006;Zelazo et al. 2013) is used to collect information on children's cognitive flexibility. In the kindergarten and first-grade data collections, the DCCS was administered as a physical, to a computerized version of the task was made so that the DCCS would remain age-appropriate through the end of data collection for ECLS-K:2011. For more information on the physical, table-top card sort task administered in kindergarten and first grade and differences between the physical version and computerized version, see chapter 3 of the User's Manual for the Kindergarten-Second Grade Data File and Electronic Codebook, Public Version (NCES 2017-285)  . This section describes the computerized version of the DCCS that was administered in the spring of third grade, which is the same version administered in the second-grade rounds. The computerized task was developed as part of the National Institutes of Health Toolbox for the Assessment of Neurological and Behavioral Function (see www.nihtoolbox.org) and is appropriate for ages 3-85 (Zelazo et al. 2013). The task had been under development during the planning phases for the earliest rounds of the ECLS-K:2011 and became available in time to be incorporated into the second-grade data collections. The NIH Toolbox Dimensional Change Card Sort Test (NIH Toolbox DCCS) is a task is that is used across the 3 through 85 age range, but it has two different start points based on the age of the child in order to limit administration time. The NIH Toolbox DCCS consists of 40 trials, including 5 pre-3-15 switch trials (where children are asked to sort by one dimension, e.g., color), 5 post-switch trials (where children are asked to sort by a different dimension, e.g., shape), and 30 mixed-block trials (in which the sorting dimension, either color or shape, varies by trial). Testing conducted in the development of the NIH Toolbox DCCS indicated that 8-year-olds typically scored at ceiling on the pre-switch and post-switch trials. Consequently, children under age 8 begin with the pre-switch trials, and children age 8 and above begin with the mixed-block trials and are given credit in the scoring for completing the pre-switch and postswitch trials accurately. For the ECLS-K:2011 administrations of the computerized DCCS, all ECLS-K:2011 children were administered the version of the NIH Toolbox DCCS for ages 8 years and older, regardless of their age at the time of assessment. In third grade, nearly all children who participated in the DCCS (99.95 percent) were at least 8 years old when the DCCS was administered. The decision to administer the same version of the DCCS from second grade forward, regardless of whether the child was age 8, was made so that all study children would receive the same version of the DCCS task in second grade and in later rounds of data collection. Use of the same measure allows for a longitudinal analysis of performance on the DCCS from second grade into later rounds of data collection. As noted earlier, the construct assessed in the physical version of the DCCS that was administered in kindergarten and first grades and the computerized version of the DCCS is the samecognitive flexibility. However, the way the construct is assessed and the scoring differ across the versions. One key difference between the two versions is that the computerized version captures data on the amount of time in milliseconds that it takes the child to complete any given item; it is not possible to accurately measure reaction time at the necessary level of precision in the physical version. Therefore, the computerized version supports the use of both accuracy of sorting and reaction time to assess overall performance while the physical card sort assesses performance by accuracy alone. In each of the 30 mixed-block trials administered via computer to children in the ECLS-K:2011 beginning in the second-grade rounds, the children were presented with a stimulus picture of a ball or truck that was either yellow or blue. A prerecorded female voice announced the sorting rule to be used for that trial (\"color\" or \"shape\") as the appropriate word \"color\" or \"shape\" was briefly displayed in the center of screen. Next, the stimulus picture was displayed in the center of screen, where the word had just appeared. Children then selected one of two pictures at the bottom of the screen (a blue ball on the left or a yellow truck on the right) that was either the same shape or the same color as the stimulus picture, depending on whether the shape or color sorting rule was in effect for the trial. Children indicated their choice of"}, {"section_title": "3-16", "text": "picture by pressing the arrow key on the laptop keyboard that was associated with the picture; the left arrow key was used to select the picture on the left side of the screen and the right arrow key was used to select the picture on the right side of the screen. Children were instructed to use just one pointer finger to press the arrow keys. They were asked to return their pointer finger to the button in between the left and right arrow keys (marked with a fuzzy sticker, and so identified as the \"fuzzy button\") in between trials to standardize the start location for every child's finger, with the goal of maximizing accuracy in the measurement of response time. Both reaction time to sort the card and accuracy of its placement according to the sorting rule in effect for the trial were recorded by the computer program. The sorting rules (i.e., to either sort by shape or color) were intermixed across the trials, and one rule was more common than the other. The shape rule was used for 23 trials while the color rule was used in 7 trials. For example, the child may be asked to sort by shape for 4 trials in a row, then to sort by color on trial 5, and then to sort by shape on trials 6 and 7. One sorting rule was presented more frequently in order to build a response tendency (i.e., a response that is \"preferred\" because it happens more frequently, resulting in a predisposition to respond in that manner). A predisposition to sort by the dominant rule (i.e., shape) can result in either more errors or a slower reaction or response time on nondominant trials because it is necessary to inhibit the dominant response (i.e., sorting by shape) in order to shift to the less frequent sorting rule (i.e., color). The \"cost\" associated with the shift from a more frequent rule (the \"dominant\" rule) to a less frequent rule (the \"nondominant\" rule) tends to differ by the age of the participant (Davidson et al. 2006). The \"cost\" to younger children is that they tend to make more errors on the nondominant rule trials; that is, they do not demonstrate the cognitive flexibility to make the switch between rules even when prompted. Younger children do not tend to slow themselves down in favor of higher accuracy and, therefore, accuracy is a better metric of performance for young children (Zelazo et al. 2013). In contrast, older children and adults tend to demonstrate a speed/accuracy tradeoff; they slow down the pace at which they respond in order to maintain accuracy. Thus, the \"cost\" to older children and adults is seen in reaction time on the nondominant rule trials. The formula used to produce scores from the data collected by the computerized DCCS factors in reaction time on the infrequent or nondominant trials when a child demonstrates sufficiently accurate performance across all the test trials, defined as being accurate on more than 80 percent of the trials (Zelazo et al. 2013). Thus, the computerized DCCS provides a measure of performance through this developmental shift to learning to trade speed for accuracy. More information on scoring is provided below."}, {"section_title": "3-17", "text": "The 30 test trials were administered only to children who successfully completed the practice portion of the DCCS. The practice consisted of a minimum of 8 trials and a maximum of 24 trials, depending upon how quickly the child demonstrated that he or she understood the task. For the first set of practice trials, the assessor instructed the child how to sort by shape using text automatically presented on the DCCS screen that was read by the assessor along with additional standardized instructions presented by the assessor. Following the instructions, the computer administered four practice trials asking the child to sort by shape. If the child sorted at least three of the four items correctly by shape, he or she progressed to the color practice. If the child sorted more than one item in the set of four incorrectly, he or she was presented with a second set of four practice items. If the child failed to sort three of four items correctly by shape in the second set of practice items, he or she was presented a third set; failure of this third set ended the DCCS program before any actual scored trials were presented. Once a child passed the shape practice trials, the assessor instructed on how to sort by color, and the computer presented 4 to 12 practice trials asking to sort by color. Like the shape practice trials, up to three sets of 4 items could be presented before the DCCS advanced to the scored trials. If the child was not able to pass the color practice, the DCCS program ended after the third set of color practice items, again before any actual scored trials were presented. In contrast with the scored trials, the practice trials maintained one sorting rule for all items presented in succession until practice for the rule was complete. An additional difference between the practice and scored trials was that the stimulus pictures in the practice trials were white or brown rabbits and boats. Item-level data for the 30 test trials are included in the data file. They are provided in three blocks of 30 items for each participant that indicate: (1) correct versus incorrect responses, (2) the type of trial, reported as dominant (most frequently presented but not included in reaction time scores; shape is the dominant sorting rule) or non-dominant (less frequently presented and used to calculate reaction time scores; color is the non-dominant sorting rule), and (3) reaction times reported in milliseconds. Variable names for the item-level data begin with \"C7\" for spring third grade. As in second grade, the overall computed score reported for the third-grade DCCS is derived using a formula provided by the task developer and follows the scoring algorithm used for this task in the NIH Toolbox (see the NIH Toolbox Scoring and Interpretation Guide, , for additional information on scoring). Scores range from 0 to 10, with weight given to accuracy (0 to 5 units)"}, {"section_title": "3-18", "text": "and reaction time (0 to 5 units) in the computation of the scores. Accuracy is considered first. If the child's accuracy rate is less than or equal to 80 percent, the child's overall computed score is based entirely on accuracy. If the child's accuracy rate is more than 80 percent, the child's overall computed score is based on a combination of accuracy and reaction time. The accuracy score factored into the computation of the overall score can range from 0 to 5. There are a total of 40 accuracy points that are scaled down to a maximum score of 5: for each correct response, the child earns a score of .125 (5 points divided by 40 trials). Because all children used the start point of the DCCS for children 8 years and older, each child was administered the 30 mixed-block trials, and each child who successfully passed the practice items was automatically given 10 accuracy points for the 5 pre-switch and the 5 post-switch trials of the DCCS that were not administered. Therefore, the accuracy component of the overall computed DCCS score is calculated as follows: DCCS accuracy score = 0.125 * number of correct responses 8 If the child's accuracy rate is higher than 80 percent, a reaction time score is added to the child's accuracy score. 9 Like the accuracy score, the reaction time score ranges from 0 to 5 points. The reaction time component of the overall computed score for the computerized DCCS is computed using the child's median reaction time to correct nondominant trials (i.e., the trials with the less frequently used sorting rule, color), following the same scoring algorithm outlined in the scoring manual for the NIH Toolbox . First, for those children with greater than 80 percent accuracy on the 40 trials, the median reaction time is calculated based on reaction times for correct nondominant trials with reaction times greater than or equal to 100 milliseconds (msec) and within plus or minus three standard deviations from the child's mean reaction time on the correct nondominant trials. The minimum median reaction time allowed is 500 msec; the maximum median reaction time is 3,000 msec. If the child's median reaction time falls outside this range, the child's median reaction is set to the minimum 8 The number of correct responses = 10 + the number of correct trials out of the 30 mixed block trials. Once the child has passed the practice trials and advanced into the scored portion of the assessment, 10 accuracy points are automatically awarded due to the chosen start point for the task. For this reason, it is not possible for ECLS-K:2011 children to get an accuracy score of 0. Therefore, the minimum possible value for the DCCS accuracy score is 1.25 and the maximum possible DCCS accuracy score is 5. 9 The criterion of greater than 80 percent accuracy is calculated based on all 40 trials (30 administered trials plus the 10 trials not administered). That is, 80 percent of 40 trials is 32 items. However, this can also be thought of in terms of how many items out of the 30 administered trials are required. If the criterion is 80 percent of the 40 trials, this translates to 23 of the 30 administered trials. For example, if a child responds accurately on 23 of the 30 mixed block trials, the child's accuracy rate equals 82.5 percent (10 points automatically awarded for the pre-switch and postswitch trials plus the 23 correct mixed block trials divided by 40; 33/40 = .825). In this example, the child's accuracy score would be [(10 + 23) * .125] = 4.125. Because the accuracy rate is greater than 80 percent, the child's reaction time score would be added to this accuracy score to obtain the overall computed score for the DCCS. Alternatively, if the child responded accurately on 22 of the 30 mixed-block trials, the child's accuracy rate would equal 80 percent and, therefore, the child's accuracy is not greater than 80 percent and the child's overall score would be based solely on accuracy (overall computed score = [(10 + 22) * .125] = 4)."}, {"section_title": "3-19", "text": "or maximum allowable range: reaction times between 100 msec and 500 msec were set to 500 msec and reaction times between 3,000 msec and 10,000 msec (the maximum trial duration) are set to 3,000 msec. A log (base 10) transformation is applied to the median reaction times to create a more normal distribution. The log values are then algebraically rescaled to a 0 to 5 range and then reversed such that faster (better) reaction times have higher values and slower reaction times have lower values. The formula for rescaling the median reaction times is the following: where RT is the median reaction time on nondominant trials within set outer limits. 10 To summarize, the overall computed score on the computerized DCCS is equal to the child's accuracy score if the child's accuracy rate is less than or equal to 80 percent. If the child's accuracy rate is greater than 80 percent, the child's overall computed score is equal to the child's accuracy score plus the child's reaction time score, which is derived from the child's reaction time on correct nondominant trials as described above. Additional details on the calculation of the computed score are available in the NIH Toolbox Scoring and Interpretation Guide  "}, {"section_title": "and the NIH Toolbox Technical", "text": "Manual (Slotkin, Kallen et al. 2012). The fall and spring second-grade and spring third-grade computed scores (X5DCCSSCR, X6DCCSSCR, and X7DCCSSCR) range from 0 to 10, with weight given to accuracy (0 to 5 units) and reaction time (0 to 5 units) in the computation of the score. The overall computed score for the computerized DCCS can be used to examine change across rounds that use the computerized DCCS (i.e., performance in the fall of second grade can be directly compared to performance in the spring of second grade and the spring of third grade). It is important for researchers using the DCCS data to be aware of the characteristics of the overall DCCS scores and determine how best to use these scores in their analyses. As noted above, the NIH-developed scoring model computes scores differently depending on sorting accuracy. The use of this scoring model with the data collected from children in the ECLS-K:2011 resulted in a non-normal distribution, with approximately 4 percent of children in the third-grade data collection who have a 10 The median reaction time (RT) used to calculate the reaction time score falls within the range of 500 msec through 3,000 msec. Calculation of the median score requires a minimum of at least one correct nondominant trial reaction time that is greater than 100 msec. When the child reached the accuracy threshold for including the reaction time component in the scoring but did not have any within-range reaction times on correct nondominant trials, the child's overall computed score on the DCCS was set equal to the child's accuracy score, and reaction time was not factored into the child's score."}, {"section_title": "3-20", "text": "computed overall score failing to achieve greater than 80 percent accuracy and, therefore, having their score calculated based solely on accuracy.  . The following scores based on the third-grade computerized administration are presented on the data file: overall score for spring third grade (range: 0-10); accuracy score for spring third grade (range: 0-5); and reaction time score for spring third grade (range: 0-5). Researchers should note that the reaction time score was only computed for cases for which the accuracy score was greater than 80 percent. If the accuracy score was not greater than 80 percent, then the reaction time score was set to -9 (not ascertained). New DCCS scores to summarize the accuracy of the 30 test trials by trial type are being released for the first time on the kindergarten-third grade file and include scores for all rounds in which the computerized DCCS was administered (second-and third-grade rounds). The number of correct, dominant trials (X*CSDAC) can range from 0 to 23, and the number of correct, nondominant trials (X*CSNDAC) can range from 0 to 7. These scores represent accuracy by trial type and are different from the total accuracy score [X*CSACC, DCCS Accuracy Component (0-5) Score] that is derived to compute the overall DCCS computed score, which ranges from 0-5 and is based on the accuracy of both dominant and nondominant trials. Cases affected by the programming error were examined to determine whether they met the criteria for moving into the test trials based on the items for which they did provide a response (that is, whether they demonstrated sufficient understanding of the task despite receiving erroneous feedback)."}, {"section_title": "3-21", "text": "These cases, children who had at least one instance of nonresponse in the practice, are flagged as a 6 or 7 in the DCCS flag variable depending on whether they met the criteria. Cases that have X7DCCSFLG=6 passed the practice trials with the responses they provided during the administration of the DCCS. For example, a child may have had 3 correct responses and 1 nonresponse within the block of four practice trials and, thus, the criterion of responding correctly to at least 3 of 4 correct in order to proceed was still reached. As another example, the child could have had two nonresponse trials and two incorrect trials and failed the first practice set. In this case, the child would have been administered another practice block of 3-24 four trials and could have passed on that set of practice trials. Cases that have the value of 6 on the DCCS flag are cases that successfully met the criteria for passing both the shape and color practice and advanced to the test trial, despite receiving at least one instance of erroneous feedback. There are 189 cases that have X7DCCSFLG=6, and data for these cases are provided on the data file. Additional information on this error is provided in the appendix. Cases that have X7DCCSFLG=7 did not demonstrate sufficient understanding of the task with the responses they provided and were not given sufficient practice per the administration protocols to have their scores included in the data file. These cases were not given the opportunity to meet the criterion for passing the practice because nonresponse was incorrectly recorded as a correct response. For example, children who had 2 correct trials, 1 incorrect trial and 1 nonresponse trial (incorrectly scored as \"correct\") were incorrectly given credit for passing the practice, even though they only had 2 correct trials and did not meet the criterion of at least 3 of 4 correct to pass. In this example, if the program had performed correctly, the child would have been given additional training and additional opportunities to pass the practice. Because of the programming error, this did not happen and the child progressed to the test trials without truly meeting the criterion for successfully passing the practice. Because it was not possible to determine whether the children could have passed the practice if given the correct opportunities, the data were suppressed. There are 92 cases that have X7DCCSFLG= 7. These cases have DCCS data set to -4 (suppressed due to insufficient practice)."}, {"section_title": "Numbers Reversed", "text": "The Numbers Reversed measure assesses the child's working memory. It is a backward digit span task that requires the child to repeat an orally presented sequence of numbers in the reverse order in which the numbers are presented. For example, if presented with the sequence \"3\u20265,\" the child would be expected to say \"5\u20263.\" Children are given 5 two-number sequences. If the child gets three consecutive two-number sequences incorrect, then the Numbers Reversed task ends. If the child does not get three consecutive two-number sequences incorrect, the child is then given 5 three-number sequences. The sequence becomes increasingly longer, up to a maximum of eight numbers, until the child gets three consecutive number sequences incorrect (or completes all number sequences)."}, {"section_title": "3-25", "text": "Item-level data for the Numbers Reversed subtask for the fall and spring of kindergarten, first grade, second grade, and third grade are provided in the ECLS-K:2011 K-3 data file. The maximum number of items any child could have been administered in all data collection rounds was 30 items (5 two-digit number items; 5 three-digit number items; 4 four-digit number items; 4 five-digit number items; 4 six-digit number items; 4 seven-digit number items; and 4 eight-digit number items). Each item is scored \"correct\" (i.e., the child correctly repeated the number sequence in reversed order), \"incorrect\" (i.e., the child did not correctly repeat the number sequence in reversed order), or \"not administered\" (i.e., the child was not administered the item because he or she did not answer enough items correctly to advance to this item). The \"not administered\" code is different than a system missing code in that only those children who were administered the Numbers Reversed subtask could have a \"not administered\" code. If a child was not administered the Numbers Reversed subtask at all, his or her case would have a missing code for the Numbers Reversed scores. Variable names for the item-level data from the fall kindergarten assessments begin with \"C1,\" and variable names for the item-level data from the spring kindergarten assessments begin with \"C2.\" Similarly, variable names for item-level data from the fall and spring first-grade assessments begin with \"C3\" and \"C4,\" while those for fall and spring second grade and spring third grade begin with \"C5\", \"C6,\" and \"C7\", respectively. Variable descriptions for these items indicate the length of the digit sequence (e.g., C1 Numbers Reversed Two-digit sequence #1). In addition to the item-level data, five scores developed using guidelines from the publisher's scoring materials are included in the data file for Numbers Reversed: the W-ability 11 score, the age standard score, the grade standard score, the age percentile score, and the grade percentile score. Before analyzing the Numbers Reversed data, it is important that researchers understand the characteristics of these scores and how these characteristics may affect the analysis and interpretation of the Numbers Reversed data in the context of the ECLS-K:2011. Depending on the research question and analysis being conducted, one of the scores may be more preferable than another. For example, the W score may be best for a longitudinal analysis, whereas the age or grade percentile rank and/or age or grade standardized score may be better suited for an analysis focusing on one point in time. The descriptions below provide more information about which score may be better suited for a given analysis. 12 The W score, a type of standardized score, is a special transformation of the Rasch ability scale and provides a common scale of equal intervals that represents both a child's ability and the task difficulty. The W scale is particularly useful for the measurement of growth and can be considered a growth scale."}, {"section_title": "3-26", "text": "Typically, the W scale has a mean of 500 and standard deviation of 100. Furthermore, the publisher of the WJ III has set the mean to the average of performance for a child of 10 years, 0 months. This means that it would be expected that most children younger than 10 years, 0 months would obtain W scores lower than the mean of 500, and most older children would be expected to have scores above the mean of 500. Also, as a child develops with age, it would be expected that the child's W score would increase to reflect growth. For example, when a child's W-ability score increases from 420 to 440, this indicates growth, and this would be the same amount of growth in the measured ability as any other student who gained 20 W points elsewhere on the measurement scale. Although the W score is reflective of the average performance of 10-year-olds, and the ECLS-K:2011 children are younger, it is included in the data file to enable the measurement of changes in children's working memory longitudinally across all rounds of the study. Also, it facilitates comparisons of the ECLS-K:2011 data with data from other studies that include the Numbers Reversed task. Users should keep in mind that most ECLS-K:2011 sample children were 5 or 6 years old during the kindergarten data collections, 6 or 7 years old during the first-grade data collections, 7 or 8 years old during the secondgrade data collections, and 8 or 9 years old during the third-grade data collections while the W scores compare their performance to that of 10-year-olds. As a result, W scores from the ECLS-K:2011 sample appear to show that the ECLS-K:2011 children demonstrated below average performance on this task. As expected, the discrepancy is declining as the participating children grow older and closer to age 10. A score of 403 (393 for the Spanish administration) is potentially a meaningful baseline value for the ability level of children who are unable to answer any items correctly. Over time, as children develop more ability that is measurable by the WJ III Numbers Reversed task, the study will be able to compare their baseline Numbers Reversed W score (fall kindergarten and/or spring kindergarten Numbers Reversed W score) with their scores across future administrations of the task. However, researchers should understand that a raw score of 0 (which translates to a W score of 403 for the English administration and 393 for the Spanish administration) is an imprecise measure of children's ability in the area of working memory, because it is unknown how close a child was to getting at least one answer correct."}, {"section_title": "3-28", "text": "In the fall of kindergarten, approximately 40 percent of students did not demonstrate sufficient skills as measured by this assessment to score above the lowest scalable score (403 for English assessment and 393 for Spanish assessment). In the spring of kindergarten, approximately 20 percent of students did not score above the lowest scalable score (403 for English, 393 for Spanish). In the fall of first grade, less than 13 percent scored at the lowest scalable score, and only 6 percent scored at the lowest scalable score in the spring of first grade. In the fall of second grade, less than 4 percent scored the lowest scalable score, and slightly more than 2 percent received the lowest score in the spring. In the spring of third grade, approximately 1 percent scored at the lowest scalable score. These percentages show a general improvement over time. A factor that may contribute to the large number of children scoring 403 (and 393 for Spanish) in kindergarten is that some ECLS-K:2011 assessors did not properly administer the practice items, which may have resulted in some children never fully understanding what they were being asked to do during the Numbers Reversed task. During field observations of the assessors, it was noted that when children did not correctly answer the first practice item, there were inconsistencies in the administration of additional practice items. It is not possible to determine the extent to which improper administration of the practice items affected the results. However, readers should keep in mind that this may have affected performance for some (but not all) children. In conducting analyses, researchers need to decide how to handle the 403 (393 for Spanish) scores; the decision for how to do so is left up to the analyst based on his or her analytic goals. For the first-grade and later data collections, assessor training for the Numbers Reversed task was changed to improve the consistency and clarity of administration of the practice items. periods, the W scores would be better to assess change and/or stability across time."}, {"section_title": "3-30", "text": "The weighted means for the ECLS-K:2011 population are lower than the established means from the WJ III norming sample: 14 the average W scores for the ECLS-K:2011 population are less than 500, the average age and grade standard scores are less than 100, and the average age and grade percentile scores are less than 50. The lower mean for the W scores in the ECLS-K:2011 may be attributed to the derivation of the score being a comparison to the average 10-year-old (generally 10-year-olds are in fourth grade) or to differences between the ECLS-K:2011 population and the WJ III norming sample. The lower means for the standard percentile scores in the ECLS-K:2011 may also be attributable to differences between the ECLS-K:2011 population and the WJ III norming sample. The variable names, descriptions, value ranges, weighted means, and standard deviations for the Numbers Reversed scores from the fall of kindergarten to the spring of third grade are shown in 14 Normative data for the WJ III were gathered from 8,818 subjects in more than 100 geographically diverse U.S. communities (McGrew and Woodcock, 2001). The kindergarten through 12th grade sample was composed of 4,783 subjects. The norming sample was selected to be representative of the U.S. population from age 24 months to age 90 years and older. Subjects were randomly selected within a stratified sampling design that controlled for the following 10 specific community and subject variables: census region (Northeast, Midwest, South, West); community size (city and urban, larger community, smaller community, rural area); sex; race (White, Black, American Indian, Asian and Pacific Islander); Hispanic or non-Hispanic; type of school (elementary, secondary, public, private, home); type of college/university (2-year, 4-year, public, private); education of adults; occupational status of adults; occupation of adults in the labor force. There is one other flag, X*NRGEST, related to Numbers Reversed that is provided for each round of data collection. The Numbers Reversed grade-normed scores (X*NRSSGR, X*NRPEGR) are normed according to how far into the school year the assessment was conducted. Decimals are used to indicate the number of months into the school year the child had been in the grade at the time of the assessment (e.g., 0.1 = 1 month; 0.2 = 2 months, etc.; 0.9 = 9 months, including time in the summer prior to the start of the next grade level). When school year start and end dates were not available, it was necessary to estimate the decimal representing the proportion of the school year completed when the assessment occurred. X*NRGEST indicates whether the number of months completed in the grade was estimated for that round of data collection."}, {"section_title": "Teacher-and Parent-Reported Measures of Child Behavior", "text": "Teachers reported their perceptions of the child's behavior and their relationship with the child, and parents reported their perceptions of the child's behavior. This section provides information on teacher-reported social skills, approaches to learning behaviors, attentional focusing, inhibitory control, student-teacher relationship, working memory, and peer relationships, and parent-reports of their child's 3-34 working memory. Parents were not asked about their child's social skills and approaches to learning in the third-grade data collection."}, {"section_title": "Teacher-Reported Social Skills", "text": "In the fall and spring data collections in kindergarten, first grade, second grade, and the spring data collection of third grade, teachers reported how often their ECLS-K:2011 students exhibited certain social skills and behaviors using a four-option frequency scale ranging from \"never\" to \"very often.\" Teachers also had the option of indicating that they had not had an opportunity to observe the described behavior for the child being asked about. in chapter 2 have additional information on the teacher questionnaires. Four social skill scales were developed based on teachers' responses to these questionnaire items. The score on each scale is the mean rating on the items included in the scale. The four teacher scales are as follows: Self-Control (4 items), Interpersonal Skills (5 items), Externalizing Problem Behaviors (6 items), 16 and Internalizing Problem Behaviors (4 items). A score was computed when the respondent provided a rating on at least a minimum number of the items that composed the scale. The minimum numbers of items that were required to compute a score were as follows: Self-Control (3 out of 4 items), Interpersonal Skills (4 out of 5 items), Externalizing Problem Behaviors (4 out of 6 items), and Internalizing Problem Behaviors (3 out of 4 items). Higher scores indicate that the child exhibited the behavior represented by the scale more often (e.g., higher Self-Control scores indicate that the child exhibited behaviors indicative of self-control more often; higher Interpersonal Skills scores indicate that the child 15 The Social Skills Rating System is a copyrighted instrument (1990 NCS Pearson) and has been adapted with permission. These are items developed by Gresham and Elliott (1990). 16 For children who were in first grade during the first-grade data collections (rounds 3 and 4) and for all children in subsequent rounds of data collection (rounds 5, 6, and 7), the externalizing problem behaviors composite is based on 6 items. This is different from how the composite was created for the kindergarten rounds (rounds 1 and 2). One additional item was included at the end of the \"Social Skills\" section of the questionnaire in first, second, and third grades. The item asked about the child's tendency to talk at times when the child was not supposed to be talking. The item was added because it had been included in the first-grade round of the ECLS-K and was factored into the calculation of that study's first-grade composite score."}, {"section_title": "3-35", "text": "interacted with others in a positive way more often). Variable names for the teacher scale scores, descriptions, value ranges, weighted means, and standard deviations for these scales are shown in table 3-7. 17 Data for the individual items contributing to each scale are not included in the K-3 data file due to copyright restrictions.    3-38"}, {"section_title": "3-37", "text": ""}, {"section_title": "Teacher-Reported Approaches to Learning Items and Scale", "text": "The child-level teacher questionnaire fielded in every round of data collection from the fall of kindergarten to the spring of third grade included seven items, referred to as \"Approaches to Learning\" items, that asked the teachers to report how often their ECLS-K:2011 students exhibited a selected set of learning behaviors (keeps belongings organized; shows eagerness to learn new things; works independently; easily adapts to changes in routine; persists in completing tasks; pays attention well; and follows classroom rules). 18 These items were presented in the same item set as the social skills items adapted from the Social Skills Rating System (described above in section 3.4.1), and teachers used the same frequency scale to report how often each child demonstrated the behaviors described. The Approaches to Learning scale score is the mean rating on the seven items included in the scale. A score was computed when the respondent provided a rating on at least 4 of the 7 items that composed the scale. Higher scale scores indicate that the child exhibited positive learning behaviors more often. The item-level data for the teacher-reported Approaches to Learning items are included in the data file along with the other child-level teacher questionnaire data. Variable names for the item-level data from the fall first-grade child-level teacher questionnaire begin with \"T3.\" Those for the item-level data from the spring first-grade child-level teacher questionnaire for children in first grade begin with \"T4,\" while those for children held back in kindergarten begin with \"T4K.\" Variable names for the fall of second grade begin with \"T5,\" and those for the spring of second grade begin with \"T6.\" Variable names for the spring of third grade begin with \"T7.\" The variable names, descriptions, value ranges, weighted means, and standard deviations for the teacherreported Approaches to Learning scale scores are shown in table 3-9. The Approaches to Learning scale has a reliability estimate of .91 for each round of data collection, as measured by Cronbach's alpha. Additionally, the item-level data for the teacher-reported Approaches to Learning items are included in the data file along with the other child-level teacher questionnaire data. 18 The Approaches to Learning teacher items were developed specifically for the ECLS-K; they are not taken from an existing source. These are the same items that were fielded as part of what was called the Teacher Social Rating Scale in the ECLS-K. The first six items (i.e., keeps belongings organized; shows eagerness to learn new things; works independently; easily adapts to changes in routine; persists in completing tasks; pays attention well) were included in the Teacher Social Rating Scale used in the kindergarten rounds of the ECLS-K. The seventh item (i.e., follows classroom rules) was added in the first-grade round of the ECLS-K.  Teachers were presented with statements about how the children might have reacted to a number of situations in the past 6 months and were asked to indicate how \"true\" or \"untrue\" those statements were about that child on a 7-point scale ranging from \"extremely untrue\" to \"extremely true,\" with a middle option of \"neither true nor untrue.\" If a statement or situation did not apply to that child, the teacher could indicate \"not applicable.\" The CBQ is appropriate for assessment of children ages 3 through 7 years, so it could not be used past the first-grade rounds of data collection. To remain age appropriate, the CBQ was replaced with the Temperament in Middle Childhood Questionnaire (TMCQ) (Simonds and Rothbart 2004) 20 in the spring of second grade. The TMCQ was designed as an upward age-extension of the CBQ and is appropriate for children ages 7 through 10 years. While many of the items from the TMCQ are different from the items on the CBQ, the items are believed to assess the same or similar constructs in an age-appropriate way. Teachers received the same instructions for the CBQ and TMCQ items, although the TMCQ items were rated on a 5point scale instead of the 7-point scale used for the CBQ items. For the TMCQ items, teachers used a 5point scale ranging from \"almost always untrue\" to \"almost always true,\" with a middle option of \"sometimes true, sometimes untrue.\" Like the CBQ, there was a \"not applicable\" option that the teacher could select if the statement or situation did not apply to the child. Item-level data for the items that make up the Attentional Focusing and Inhibitory Control scales are provided on the kindergarten-third grade data file. Variable names for the item-level data from the fall and spring kindergarten child-level teacher questionnaire begin with \"T1\" and \"T2,\" respectively. Variable names for the item-level data from the spring first-grade child-level teacher questionnaire for children in first grade begin with \"T4,\" while variable names for children held back in kindergarten begin with \"T4K.\" Variable names for the spring second grade begin with \"T6,\" and those for spring third grade begin with \"T7.\" The data file includes two scale scores for each round of data collection in which each measure was included: (1) Attentional Focus and (2) Inhibitory Control. In kindergarten and first grade these scores are derived from the CBQ, and in second and third grade these scores are derived primarily from the TMCQ, as explained further below. The scale scores were developed using guidelines from the developers of both the CBQ and TMCQ. In kindergarten and first grade, the ECLS-K:2011 fielded all 6 items from the Attentional Focusing subscale and all 6 items from the Inhibitory Control subscale of the CBQ Short Form. As such, the kindergarten and first-grade Attentional Focus and Inhibitory Control scores are each based on all 6 items in the relevant Short Form subscale. Because the CBQ was initially designed as a parent-report measure, the item wording for 3 of the items from the CBQ Inhibitory Control subscale was modified slightly for use in the ECLS-K:2011 to make them more appropriate for a school setting."}, {"section_title": "3-41", "text": "In second and third grade, the ECLS-K:2011 fielded 6 of the 7 items from the original TMCQ Attentional Focusing subscale. For the inhibitory control dimension, the ECLS-K:2011 fielded 6 of the 8 items from the TMCQ Inhibitory Control subscale and one item from the CBQ Inhibitory Control subscale. Therefore, the second-and third-grade Attentional Focusing scale scores reflect the 6 items fielded by the ECLS-K:2011, not the full set of items in the original TMCQ scale. The second-and third-grade Inhibitory Control scale scores reflect the 7 items fielded by the ECLS-K:2011 (6 from the TMCQ and one from the CBQ), again not the full set of items in the original TMCQ scale. Because the TMCQ was designed as a parent-report measure, the item wording on one item from the TMCQ Attentional Focusing subscale was modified slightly to make it more appropriate for a school setting and, similarly, one item on the TMCQ Inhibitory Control subscale was modified. For the kindergarten, first-grade, second-grade, and third-grade Attentional Focusing and Inhibitory Control scales, the score on each scale is the mean rating on the items included in the scale. A score was computed when the respondent provided a rating on at least 4 of the 6 or 7 items that made up the scale. Higher scale scores on the Attentional Focus scale indicate that the child exhibited more behaviors that demonstrate the ability to focus attention on cues in the environment that are relevant to the task. Higher scale scores on the Inhibitory Control scale indicate that the child exhibited more behaviors that demonstrate the ability to hold back or suppress a behavior as necessary for a particular situation. The variable names, descriptions, value ranges, weighted means, and standard deviations for these scales are shown in tables 3-10 and 3-11. has an internal consistency reliability coefficient of .96, and the Inhibitory Control scale ( X6INTMCQ 22 )has an internal consistency reliability coefficient of .87. For the spring of third grade, the Attentional Focus scale (X7ATTMCQ) has an internal consistency reliability coefficient of .96, and the Inhibitory Control scale (X7INTMCQ) has an internal consistency reliability coefficient of .85. The study received copyright permission to include item-level data from both the CBQ and the TMCQ in the ECLS-K:2011 data files. Therefore, these data have been included in the kindergarten through third-grade data file with the other child-level teacher questionnaire data. Variable names for the item-level data from the fall of kindergarten, the spring of kindergarten, the spring of first grade, the spring of second grade, and the spring of third grade begin with \"T1,\" \"T2,\" \"T4,\" \"T6,\" and \"T7,\" respectively. Variable names that begin with \"T4K\" are for item-level data from the spring of first grade for students retained in kindergarten.   3-44"}, {"section_title": "Teacher-Reported Student-Teacher Relationship Scale", "text": "The Student-Teacher Relationship Scale (STRS) (Pianta 2001) is a 15-item, teacher-reported measure of closeness and conflict between the teacher and child. As part of the spring kindergarten, spring first-grade, spring second-grade, and spring third-grade child-level teacher questionnaire, the teacher was presented with 15 descriptive statements about his or her relationship with the ECLS-K:2011 child and asked to indicate the degree to which each statement applied to their relationship using a 5-point scale ranging from \"definitely does not apply\" to \"definitely applies.\" Two scales were developed based on guidelines from the developer of the scale: Closeness and Conflict. The Closeness scale score is the average rating on 7 items included in the STRS, while the Conflict scale score is the average rating on the other 8 items included in the STRS. A score was computed when the respondent provided a rating on at least 5 of the 7 or 8 items that composed the scales. The Closeness scale is a measure of the affection, warmth, and open communication that the teacher experiences with the student. The Conflict scale is a measure of the teacher's perception of the negative and conflictual aspects of the teacher's relationship with the student. Higher scores on the Closeness scale indicate that the teacher perceived he or she had a closer relationship with the child. Higher scores on the Conflict scale indicate that the teacher perceived his or her relationship with the child to be characterized by more conflict. The variable names, descriptions, value ranges, weighted means, and standard deviations for the STRS scales are shown in table 3-13. Table 3-14 presents the internal consistency reliability coefficients (Cronbach's alpha) for the teacher-reported STRS Closeness and Conflict scores for kindergarten, first grade, second grade, and third grade. In the springs of kindergarten, first grade, second grade, and third grade, the Closeness scale (X2CLSNSS, X4CLSNSS, X4KCLSNSS, X6CLSNSS, and X7CLSNSS) has a reliability estimate that ranges from .86 to .89, and the Conflict scale (X2CNFLCT, X4CNFLCT, X4KCNFLCT, X6CNFLCT, and X7CNFLCT) has a reliability estimate that ranges from .88 to .90. The study received copyright permission to include item-level data from the STRS on the ECLS-K:2011 restricted-use data files. Therefore, these data have been included in the kindergarten through third-grade data file with the other child-level teacher questionnaire data. 23 Variable names for the itemlevel data from the spring of kindergarten, the spring of first grade, the spring of second grade, and the 23 Item wording is redacted in the questionnaires made available to the public. It is provided in the codebook view of the Electronic Codebook (ECB)."}, {"section_title": "3-45", "text": "spring of third grade begin with \"T2,\" \"T4,\" \"T6,\" and \"T7,\" respectively. Variable names that begin with \"T4K\" are for item-level data from the spring of first grade for students retained in kindergarten.  3-46"}, {"section_title": "Parent-and Teacher-Report Working Memory", "text": "In the spring of third grade, parents and teachers reported on child behaviors related to working memory. Parents were asked 4 items as part of the parent interview, and teachers were asked the same 4 items as part of the child-level teacher questionnaire. 24 The items are 4 of the 10 items that make up the Parent and Teacher Forms of the Working Memory Scale of the Behavior Rating Inventory of Executive Function (BRIEF). 25 Items from the BRIEF Working Memory Scale measure \"the capacity to hold information in mind for the purpose of completing a task\" (Gioia et al. 2000, p. 19). Parents and teachers were presented with statements that describe child behaviors related to working memory, and they were asked to rate how often (never, sometimes, or often) the child has had problems with these behaviors over the past 6 months. Item-level data are provided on the kindergarten-third grade data file. Variables for the itemlevel data from the spring third grade parent interview begin with \"P7,\" and variables from the spring third grade child-level teacher questionnaire begin with \"T7.\" The data file also contains a scale score for parent-reported working memory X7PWKMEMand a scale score for teacher-reported working memory (X7TWKMEM). For both the parent scale score and the teacher scale score, a score was computed when the respondent provided a rating on at least 3 of the 4 items that made up the scale. Scores on rated items were summed and divided by the number of items rated to derive the scale score. Higher scale scores indicate that the child exhibited more behaviors indicating problems with working memory. That is, higher scores indicate worse working memory. Lower scale scores indicate fewer difficulties related to working memory, and, therefore, indicate better working memory. The variable names, descriptions, value ranges, weighted means, and standard deviations for these scales are shown in table 3-15. 24 The items used for teachers and parents were the same and matched the items from the Behavior Rating Inventory of Executive Function (BRIEF). The instructions were adapted from the instructions on the cover of the BRIEF questionnaire to be appropriate for the mode of data collection used in this study. The instructions were adapted to be as similar as possible to the intent of the BRIEF instructions. The instructions varied slightly for parents and teachers because the parent items were administered within the parent interview and teachers completed items in a hard-copy questionnaire. 25 ECLS-K:2011 used 4 of 10 items from the Teacher Form of the BRIEF and 4 of 10 items from the Parent Form of the BRIEF. The items used were adapted and reproduced by special permission of the Publisher, Psychological Assessment Resources, Inc., 16204 North Florida Avenue, Lutz, Florida 33549, from the Behavior Rating Inventory of Executive Function by Gerard A. Gioia, Peter K. Isquith, Steven C. Guy, and Lauren Kenworthy, Copyright 1996, 1998 by PAR, Inc. Further reproduction is prohibited without permission from PAR, Inc.   3-48"}, {"section_title": "3-47", "text": ""}, {"section_title": "Teacher-and Parent-Reports of Children's Peer Relationships", "text": "Teachers reported their perceptions of the child's peer relationships in the child-level teacher questionnaire, and parents reported their perceptions of the child's peer relationships in the parent interview. Exhibit 3-4 shows the constructs on peer relationships included in the second-and third-grade child-level teacher questionnaires and the corresponding item-level variables along with their sources. In second and third grade, teachers provided information on peer victimization, both with the child as the victim and with the child as the aggressor. In the spring of third grade, teachers were asked additional questions on children's peer relationships. Teachers were asked about whether the child was excluded or ignored by peers and about whether the child exhibited prosocial behaviors with peers. These items were adapted from existing scales and were used with the permission of the authors. Data for the individual items are included in the K-3 data file. Composite variables for each construct are not provided; it is left to analysts to decide how best to use these data in their analyses. "}, {"section_title": "3-49", "text": "In addition to teacher provided information on peer victimization in second-and third-grade, parents provided information on peer victimization in second-and third-grade. Exhibit 3-5 shows the constructs on peer relationships included in the second-and third-grade parent interviews and the corresponding item-level variables along with their sources. The teacher-and parent-provided information complements information collected from children on peer victimization, which is described above in section 3.3. Children were asked only about their experiences as a victim, not as the aggressor. "}, {"section_title": "SAMPLE DESIGN AND SAMPLING WEIGHTS", "text": "The Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) is providing national data on children's characteristics as they progress from kindergarten through the 2015-16 school year, when most of the children will be in fifth grade. In the 2010-11 school year, the ECLS-K:2011 collected data from a nationally representative sample of 18,174 children enrolled in 968 schools. 1 This chapter summarizes the process used to select the sample for the study in the base year (i.e., kindergarten), describes how the sample design changed for the first-through third-grade years, and provides information necessary to properly analyze the data that were collected."}, {"section_title": "Sample Design", "text": "The optimal sample design for collecting data to produce national child-level estimates is to sample children with probabilities that are approximately the same for each child. In most studies, this is achieved using a multi-stage sampling design that involves sampling primary sampling units (PSUs) and schools with probabilities proportional to the targeted number of children attending the school and selecting a fixed number of children per school. Such a sampling procedure was used for the ECLS-K:2011. Additionally, a clustered design was used to minimize data collection costs, which are strongly related to the dispersion of the children in the sample. Restricting data collection to a limited number of geographic areas and to as few schools as possible helps to minimize costs while still achieving an acceptable level of precision in the estimates produced with the data. The sample for the ECLS-K:2011 was selected using a three-stage process. In the first stage of sampling, the country was divided into primary sampling units (PSUs), or geographic areas that are counties or groups of contiguous counties, and 90 PSUs were sampled for inclusion in the study. In the second stage, samples of public and private schools with kindergarten programs or that educated children of kindergarten age (i.e., 5-year-old children) in ungraded settings were selected within the sampled PSUs. Both PSUs and schools were selected with probability proportional to measures of size (defined as the population of 5-year-old children) that took into account a desired oversampling of Asians, Native Hawaiians, and Other Pacific Islanders (APIs). 2 In the third stage of sampling, children enrolled in kindergarten and 5-year-old children in ungraded schools or classrooms were selected within each sampled 1 This is the number of schools with at least one child or parent respondent at the end of the spring data collection; this number includes originally sampled schools and substitute schools. Children who transferred from the school in which they were originally sampled during the kindergarten year were retained in the study and followed into their new school; this number does not include schools to which study children transferred during the kindergarten year. 2 Asian, Native Hawaiian, and Other Pacific Islander children were oversampled as one group, not as three groups that were distinct from one another.  , hereinafter referred to as the base-year User's Manual."}, {"section_title": "ECLS-K:2011 School Sample", "text": "A total of 1,221 clusters of schools 3 were originally selected for the ECLS-K:2011, of which 1,003 were clusters of public schools and 218 were clusters of private schools. This resulted in 1,036 sampled public schools and 283 sampled private schools, for a total of 1,319 sampled schools. The sample frames used to select schools were the 2006-07 Common Core of Data (CCD) and the 2007-08 Private School Survey (PSS), which were the most recent CCD and PSS data available at the time of sampling. Because the 2006-07 CCD and the 2007-08 PSS school frames were several years old, additional schools were sampled from supplemental frames that included newly opened schools and existing schools that added a kindergarten program after the 2006-07 CCD and the 2007-08 PSS data were collected. These additional schools were added to the original school sample. In total, 33 new schools were added, of which 16 were public, 4 were Catholic, and 13 were non-Catholic private schools. The total number of sampled schools after updating was 1,352 (1,052 public schools and 300 private schools). For a detailed discussion of the supplemental school sample, see section 4.1.2.7 of the base-year User's Manual. Early in the process of recruiting schools that had been sampled for the study, it was determined that the rate at which public schools were agreeing to participate was lower than expected and it would be difficult to meet the target number of participating schools by the end of the recruitment period.   Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these characteristics cannot be replicated with variables on the released data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011."}, {"section_title": "The Sample of Children", "text": "The goal of the sample design was to obtain an approximately self-weighting sample of children, with the exception of Asians, Native Hawaiians, and Other Pacific Islanders (API) who needed to be oversampled to meet sample size goals. Table 4-2 shows the distribution of the eligible children sampled for the ECLS-K:2011, by selected characteristics. Table 4-3 shows the distribution of the children who were respondents in the base year, by selected characteristics. To be considered a base-year respondent, a student had to have child assessment data (defined as having at least one set of scoreable mathematics/reading/science data OR a height or weight measurement, or having been excluded from the assessment due to lack of accommodation for a disability) or parent interview data from the fall or spring data collection, or both, in the base year. Later rounds of data collection were conducted only with baseyear respondents. Sampled students who did not participate in the base year were not recontacted for later rounds of data collection, and no new students were added to the study sample after the base year. As mentioned in the base-year User's Manual, operational problems prevented the study from conducting data collection activities in some areas of the country where Asian, Native Hawaiian/Other Pacific Islander, and American Indian/Alaska Native students sampled for the study resided. For this reason, base-year response rates for these groups of students were lower than response rates for students of other racial/ethnic backgrounds. As a result, a relatively small number of ECLS-K:2011 sample children in the Native Hawaiian/Other Pacific Islander group reside in Hawaii. Additionally, nonresponse on the child assessment, parent interview, or both, leads to some of these sampled cases not being included in weighted analyses depending on the weight used. Also, none of the ECLS-K:2011 sample children in the American Indian/Alaska Native group resided in Alaska at the time of sampling. Users are encouraged to consider these sample characteristics when making statements about children in these two racial groups. As a reminder, however, the study was not designed to be representative at the state level or for subgroups within any specific racial or ethnic group. 1 Data for this school characteristic are taken from the original school sampling frame. Therefore, the table estimates for this characteristic cannot be replicated with variables on the released data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity information was obtained from schools at the time of sampling. 6 This category includes children who are more than one race (non-Hispanic) and children whose race/ethnicity is unknown. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011. Data for this school characteristic are taken from the original school sampling frame. Therefore, the table estimates for this characteristic cannot be replicated with variables on the released data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity information is from the third-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R is revised after every data collection. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011."}, {"section_title": "4-6", "text": "4-7"}, {"section_title": "4.2", "text": "Sample Design for the First-Through Third-Grade Years"}, {"section_title": "Fall First Grade and Fall Second Grade", "text": "This section describes the sample design for the fall data collections that occurred in first and second grades. Beginning with third grade, data collections only occur in the spring of the school year. A subsample of students was selected for the fall first-grade and second-grade data collections from the full study sample described above via a three-step procedure. This subsample was designed to be representative of the full sample. In the first step, 30 PSUs were sampled from the 90 PSUs selected for the base year. Within the 30 subsampled PSUs, the 10 self-representing PSUs are large in population size and were included in the fall first-grade sample with certainty. The remaining 20 PSUs were selected from the 80 non-self-representing PSUs in 40 strata. To select the 20 non-self-representing PSUs, 20 strata were sampled with equal probability, and then one PSU was sampled within each stratum also with equal probabilities. This is equivalent to selection with probability proportional to size since the original PSU sample was selected with probability proportional to size. In the second step, all schools within the 30 subsampled PSUs that were eligible for the baseyear collection were included in the fall subsample for both first and second grades. However, data collection was not conducted in the subsampled schools in which no children participated in the base year because the study did not try and recruit base-year nonrespondents for later round of data collections. Table   4-4 shows the characteristics of all fall subsampled schools in the 30 PSUs selected in the first stage of sampling. 4 Table 4-5 shows the characteristics for the subsampled schools with base-year respondents; these are the schools in which data collection was conducted. Transfer schools (those schools that children moved into after the fall of kindergarten) are not included in this table. Of the 346 original sampled schools at the start of the fall data collections, 306 schools still cooperated in fall second grade. 5 In the third step of sampling, students attending the subsampled schools who were respondents in the base year and who had not moved outside of the United States or died before the day assessments began in their school for the fall first-grade data collection were included as part of the fall sample for the first-grade data collection. This sample formed the base sample for the fall second-grade data collection as well, though subsampled children who had died or moved outside of the United States before the day assessments began in their school for the fall second-grade data collection were excluded. Table 4-6 shows the characteristics of base-year respondents in the fall subsample who were selected in the third sampling step. Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these characteristics cannot be replicated with variables on the released data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and fall 2012. Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these characteristics cannot be replicated with variables on the released data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and fall 2012. Data for this school characteristic are taken from the original school sampling frame. Therefore, the table estimates for this characteristic cannot be replicated with variables on the released data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from the third-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R is revised after every data collection. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and fall 2012."}, {"section_title": "4-8", "text": ""}, {"section_title": "4-10", "text": ""}, {"section_title": "4-11", "text": "Tables 4-7 and 4-8 show the characteristics of base-year respondents in the fall samples, by whether the students were still in the original sampled schools or had transferred to other schools by the end of first grade and second grade, respectively. Table 4-7 shows that 81 percent of students were still attending their original sampled schools in the fall of first grade. Table 4-8 shows that 70 percent of students were still attending their original sampled schools in the fall of second grade. In the fall of first grade, the lowest percentages of students who were still attending their original sample schools are for students in non-Catholic private schools, students in the West, students in the suburbs, and Black students. The same is true for the fall of second grade with the percentage of students in non-Catholic private schools even lower than in first grade. 6 6 Significance tests were not conducted for the comparisons in this chapter because the differences discussed were based on the same sample of base-year respondents. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from the third-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R is revised after every data collection. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from the third-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R is revised after every data collection. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012."}, {"section_title": "4-12", "text": ""}, {"section_title": "4-13", "text": ""}, {"section_title": "4-14", "text": ""}, {"section_title": "Spring First Grade, Spring Second Grade, and Spring Third Grade", "text": "All base-year respondents were statistically eligible for the spring first-grade, spring secondgrade, and spring third-grade data collections, with the exception of those who moved outside the United States or died before the assessments began in their school.  Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these characteristics cannot be replicated with variables on the released data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012, spring 2013, and spring 2014."}, {"section_title": "4-15", "text": "The characteristics of base-year respondents who were eligible for the spring first-, second-, and third-grade data collections are those presented above in table 4-3; since there was no subsampling for the spring rounds of data collection, all base-year respondents were initially eligible for data collection if they had not moved outside the United States or died prior to data collection. By the end of the third-grade data collections, about 160 base-year respondents had moved out the country and 5 had died. Tables 4-10 to 4-12 show the characteristics of base-year respondents in the spring samples, by whether the students were still in their original sampled schools or had transferred to other schools. In the spring of first grade, 78 percent of base-year respondents were still attending their original sampled schools. This percent is 68 for the spring of second grade, and 59 for the spring of third grade. As is seen with the fall subsample, the lowest percentages of students who were still attending their original sample schools in the spring of first grade are for students in non-Catholic private schools, students in the West, students in the suburbs, and Black students. For the spring of second grade and for third grade, the pattern is the same except that students in different types of private schools moved at about the same rate, while students in public schools moved at a higher rate than students in Catholic schools and in non-Catholic private schools, and students in the Northeast moved at a higher rate than students in other census regions. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from third-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R is revised after every data collection. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from third-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R is revised after every data collection. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2013. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from the third-grade race/ethnicity composite X_RACETH_R. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. In order to control data collection costs, there are some students who are part of the statistical samples for the first-, second-, and third-grade data collections but who were excluded from actual data collection. These students, while statistically eligible for the study, were operationally ineligible."}, {"section_title": "4-16", "text": ""}, {"section_title": "4-17", "text": ""}, {"section_title": "4-18", "text": "Specifically, not all students who moved away from their original base-year schools after the spring baseyear data collection (known as \"movers\") were followed into their new schools. While some movers were followed with certainty, some subsampling of other movers occurred, as described below. Although information was not collected from all students in every round, the study sampling procedures, combined with the use of sampling weights that include mover subsampling adjustments (described below in section 4.3.2.2) in data analysis, result in the collected data being representative of the students in the kindergarten class of 2010-11 who remain living in the United States. Homeschooled children (i.e., those who were enrolled in a school at the time of sampling in the base year but left school to become homeschooled) were followed with certainty; they were assessed in their home if there was parental consent to do so. Destination schools. When four or more students moved from an original sampled school into the same transfer school, all those movers were followed into the new school, which is referred to as a destination school. This type of movement occurred for children who attended sampled schools that ended at a particular grade, which are referred to as terminal schools. For example, study students who attended an original sample school that ended with second grade would move as a group to a new school for third grade. In some cases, an original sampled school did not terminate in a particular grade, but for some reason four or more students from that school moved together into the same transfer school for the subsequent data collections. For example, this would happen if an original sample school closed after the spring first-grade data collection. More than one destination school may be identified for an original school if separate clusters of four or more students move into different transfer schools."}, {"section_title": "4-20", "text": "Language minority (LM) students, students with an Individualized Education Program (IEP), and students who had an Individualized Family Service Plan (IFSP). Students who were identified as language minority (LM) based on parent report of home language in the base year, as well as students identified as currently having an Individualized Education Program (IEP), or who had an Individualized Family Service Plan (IFSP) were followed at a rate of 100 percent in third grade. The IEP status of the child was obtained during the pre-assessment call when the team leader asked the school coordinator whether the child had an IEP or equivalent program on record with the school. The school records also may have indicated that a child had an IFSP when he or she was younger, even if the child did not have an IEP at the time of data collection, which the team leader could have noted during the call. Additionally, information about whether a child had had an IFSP prior to kindergarten was collected in the base-year parent interview. Due to an identification error before third grade, a number of these children who moved from their originally sampled school were not flagged to be followed with certainty in first grade and second grade. Despite this lack of sample protection, approximately 92 percent of the students who had had an IFSP were followed into second grade, either because they did not change schools, they had an IEP and became part of the protected group as a result of the IEP, or because they were part of the mover subsample that was followed at a rate of 50 percent. 7 In third grade, the identification error was corrected, and an additional 350 students who had had an IFSP were identified and followed with certainty. General procedures for all other movers. Fifty percent of students who did not meet one of the criteria described above (i.e., did not move to a destination school, were not LM, and did not have an IEP) were sampled with equal probability to be flagged as \"follow\" if they moved from their original sample school. If a student was flagged as \"do not follow,\" no data were collected for him or her. The subsampling process itself should not have introduced bias into the sample of IFSP children who were followed, because cases were randomly flagged to be followed. Additionally, the sampling weights developed for use with second-grade data account for this random subsampling. A comparison of key weighted estimates (such as school type, region of residence, school locale, percent of students in the school who were races other than White, and student race/ethnicity, gender, and year of birth) between kindergarten and first grade generally suggests the loss of those children who were not followed has little impact on the overall estimates for children who had IFSPs before age 3. Where slight differences between the kindergarten and first-grade estimates were noticed (for example, on the percent of students of race other than White in a school), the pattern with the sample of IFSP children is reflective of differences seen in the full ECLS-K:2011 sample. Also, it should be kept in mind that identifying a child to be followed with certainty does not necessarily mean that the child would have participated in the round(s) in which he or she was followed. Due to general sample attrition, the IFSP students who were not flagged to be followed with certainty constitute only about half of all IFSP children who did not participate in first grade and second grade. It is unlikely that differences in weighted estimates for the entire group of IFSP children (about 680) are due solely to the absence of the approximately 60 IFSP cases that were not followed neither in first grade nor in second grade. Nonparticipation of IFSP children in later rounds of the study for any reason does reduce the IFSP sample available for analysis. As is the case for analysis of any small subgroup, users should consider the size of their analytic sample and whether there is enough power in the data to make generalizations about the groups being examined."}, {"section_title": "4-21", "text": "Students flagged as \"do not follow\" were not sought for participation in any further data collection unless they were part of the fall subsample, as explained further below. If a student was flagged as \"follow,\" and 1. the student moved into a school in a study PSU: the student was included in all aspects of data collection (child assessment, child questionnaire, parent interview, school administrator questionnaire, and teacher questionnaires); 2. the student moved into a school outside a study PSU: only a parent interview was attempted; and 3. the student moved into a school outside the country: the student was out of scope and considered ineligible for continuation in the study. Procedures for students in the fall subsample. Fifty percent of all students in the subsample had their follow flag set to \"follow\" after the base-year data collection. Children were sampled with equal probability to be flagged as \"follow,\" meaning that if they transferred to a new school they would be followed into that new school for the fall first-and second-grade data collections. As explained in detail below, all students who were subsampled in the fall, regardless of their mover status, were followed in the spring data collections. As a result of these procedures, some subsample students were not followed in the fall collections, because their follow flag applicable to the fall collections was set to \"not follow,\" but they were followed in the spring collections. Procedures for students in the spring main sample. Fifty percent of the schools in the main sample were subsampled with equal probability to have follow flags (i.e., all students in the 50 percent subsample of schools have flags set to \"follow\") applicable for the spring data collections. All fall schools in the 30 sampled PSUs were included in the \"mover follow\" sample for the spring of first, second and third grade. An additional sample of schools that were not part of the fall subsample was selected to arrive at 50 percent of the entire sample of schools being included in the \"mover follow\" subsample in the spring first-, second-, and third-grade data collections. In this way, students who were originally sampled for fall data collections were included in the spring data collections with certainty. These fall subsample cases were followed for the spring data collections even if they were movers in the fall and had their fall mover flag set to \"not follow\" or they were nonrespondents in the fall. Also, this method allows fall subsample movers to continue to be followed in each subsequent round of data collection, as well as more clustering of the movers to be followed, thus cutting down on field costs."}, {"section_title": "4-22", "text": ""}, {"section_title": "Calculation and Use of Sample Weights", "text": "The ECLS-K:2011 data should be weighted to account for differential probabilities of selection at each sampling stage and to adjust for the effect nonresponse can have on the estimates. For the base year, weights were provided at the child and school levels. Estimates produced using the base-year child-level weights are representative of children who attended kindergarten or who attended an ungraded school or classroom and were of kindergarten age in the United States in the 2010-11 school year. Estimates produced using the base-year school-level weight are representative of schools with kindergarten programs or schools that educate children of kindergarten age in an ungraded setting. For the first-, second-, and third-grade data collections, weights are provided only at the child level, to produce estimates for the kindergarten cohort during the 2011-12 school year, the 2012-13 school year, and the 2013-14 school year, respectively. There are no school-level weights because the school sample is no longer nationally representative; it is not representative of schools with first-grade students, second-grade students, third-grade students, or ungraded schools serving children of first-grade, secondgrade, or third-grade age. The school sample is simply a set of schools attended by the children in the ECLS-K:2011 cohort during the 2011-12, the 2012-13, and the 2013-14 school years. The use of weights is essential to produce estimates that are representative of the cohort of children who were in kindergarten in 2010-11. Main sampling weights should be used to produce survey estimates. When testing hypotheses (e.g., conducting t tests, regression analyses, etc.) using weighted data from a study such as the ECLS-K:2011 that has a complex design, analysts also should use methods to adjust the standard errors. Two such methods are jackknife replication variance estimation and the Taylor series linearization method. Replicate weights are provided in the data file for use with the paired jackknife replication procedure, and PSU and stratum identifiers are provided for use with the Taylor series method."}, {"section_title": "Types of Sample Weights", "text": "Main sampling weights designed for use with data from a complex sample survey serve two primary purposes. When used in analyses, the main sampling weight weights the sample size up to the population total of interest. In the ECLS-K:2011, weighting produces national-level estimates. Also, the main sampling weight adjusts for differential nonresponse patterns that can lead to bias in the estimates. If people with certain characteristics are systematically less likely than others to respond to a survey, the collected data may not accurately reflect the characteristics and experiences of the nonrespondents, which can lead to bias. To adjust for this, respondents are assigned weights that, when applied, result in 4-23 respondents representing their own characteristics and experiences as well as those of nonrespondents with similar attributes. A sample weight could be produced for use with data from every component of the study (e.g., data from the third-grade parent interview, the third-grade child assessment and child questionnaire, the third-grade teacher questionnaire, or the third-grade school administrator questionnaire) and for every combination of components for the study (e.g., data from the third-grade child assessment with data from the third-grade school administrator questionnaire, or data from the spring kindergarten child assessment with data from the third-grade child assessment or child questionnaire and the third-grade parent interview). However, creating all possible weights for a study with as many components as the ECLS-K:2011 would be impractical, especially as the study progresses and the number of possible weights increases. In order to determine which weights would be most useful for researchers analyzing data from third grade, completion rates for each third-grade component (e.g., response to the child assessment and child questionnaire, the parent interview, various parts of the teacher questionnaire) were reviewed in combination with completion rates from the kindergarten, first-grade, second-grade, and third-grade years, and consideration was given to how analysts are likely to use the data. The best approach to choosing a sample weight for a given analysis is to select one that maximizes the number of sources of data included in the analyses for which nonresponse adjustments are made, which in turn minimizes bias in estimates, while maintaining as large an unweighted sample size as possible. Exhibit 4-1 shows the 16 weights computed for the analyses of third-grade data. It also identifies the survey component(s), or sources of data, for which nonresponse adjustments are made for each weight. Since every child who was assessed also had questionnaire data, the response rates have the same pattern. Therefore, nonresponse adjustments for the child questionnaire did not need to be made separately from nonresponse adjustments for the child assessment. Analyses that include either child assessment data or child questionnaire data should be done with a weight that includes the C7 component. "}, {"section_title": "W7C17P_2T27", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from both kindergarten rounds and spring third grade, as well as parent data from fall kindergarten or spring kindergarten, and either teacher-/classroom-or child-level teacher data from spring kindergarten (from a core or supplemental teacher questionnaire), spring first grade (from a first-grade or a kindergarten teacher questionnaire), spring second grade, and spring third grade (C1C2C7)(P1_P2)(T2T4T6T7)"}, {"section_title": "W7C17P_7T27A", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from both kindergarten rounds and spring third grade, as well as parent data from fall kindergarten or spring kindergarten, parent data from spring third grade, and either teacher-/classroom-or child-level teacher data from spring kindergarten (from a core or supplemental teacher questionnaire), spring first grade (from a first-grade or a kindergarten teacher questionnaire), spring second grade, and spring third grade (C1C2C7)(P1_P2))(P7)(T2T4T6T7) See notes at end of exhibit. "}, {"section_title": "W7CF7P_7", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from all seven rounds from kindergarten through third grade, as well as parent data from all seven rounds from kindergarten through third grade (C1C2C3C4C5C6C7)(P1_P2)(P3P4P5P6P7)"}, {"section_title": "W7CF7P_2T17", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from all seven rounds from kindergarten through third grade, as well as parent data from fall kindergarten or spring kindergarten, and either teacher-/classroom-or child-level teacher data from all seven rounds from kindergarten through third grade (C1C2C3C4C5C6C7)(P1_P2)(T1T2T3T4T5T6T7) NOTE: Having child assessment/child questionnaire data includes (1) having reading and/or mathematics and/or science scores, (2) having at least one executive function score, (3) having a height or weight measurement, or (4) being excluded from assessment due to lack of accommodation for a disability. In spring third grade, every child who was assessed also had questionnaire data. The weight designations (C1, C2, etc.) use the same prefixes that are used for other variables in the kindergarten-third grade data file. The prefixes are listed in exhibit 7-1. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), kindergarten-third grade (K-3) data file."}, {"section_title": "4-27", "text": "Exhibit 4-2, which presents the same information as exhibit 4-1 in matrix format, was developed to further assist researchers in deciding which weight to use for analyses. In exhibit 4-2, the components for which nonresponse adjustments are made for each weight are noted with a \"Yes.\" Researchers should choose a weight that has a \"Yes\" in the column(s) for the source(s) of data they are using in their analyses. The best weight would have a \"Yes\" for each and every source used and only those sources. For example, if a researcher is conducting an analysis that includes third-grade child assessment/child questionnaire data, and fall kindergarten or spring kindergarten parent interview data, the weight W7C7P_20 should be used since it adjusts for nonresponse on all of those components (i.e., exhibit 4-2 shows a \"Yes\" in the fall kindergarten and spring kindergarten parent columns and the spring thirdgrade child assessment/child questionnaire column; the italicized Yes indicates an \"or\" condition). However, for many analyses, there will be no weight that adjusts for nonresponse to all the sources of data that are included and for only those sources. When no weight corresponds exactly to the combination of components included in the desired analysis, researchers might prefer to use a weight that includes nonresponse adjustments for more components than they are using in their analysis (i.e., a weight with \"Yes\" in columns corresponding to components that are not included in their analyses) if that weight also includes nonresponse adjustments for the components they are using. Although such a weight may result in a smaller analytic sample than would be available when using a weight that corresponds exactly to the components from which the analyst is using data, it will adjust for the potential differential nonresponse associated with the components. If researchers instead choose a weight with nonresponse adjustments for fewer components than they are using in their analysis, missing data should be examined for potential bias. A case had to have either teacher/classroom-or child-level teacher data from the fall kindergarten data collection to have a valid weight. 2 A case had to have either teacher/classroom-or child-level teacher data from a teacher questionnaire or supplemental teacher questionnaire from the spring kindergarten data collection to have a valid weight. 3 A case had to have child-level teacher data from the fall first-grade data collection to have a valid weight. 4 A case had to have either teacher/classroom-or child-level teacher data from a first-grade or a kindergarten teacher questionnaire in the spring first-grade data collection to have a valid weight. 5 A case had to have child-level teacher data from the fall second-grade data collection to have a valid weight. 6 A case had to have either teacher/classroom-or child-level teacher data from the spring second-grade data collection to have a valid weight. 7 A case had to have either teacher/classroom-or child-level teacher data from the third-grade data collection to have a valid weight. NOTE: C indicates child assessment/child questionnaire data. P indicates parent interview data. T indicates teacher data. The weight designations (C1, C2, etc.) use the same prefixes that are used for other variables in the kindergarten-third grade data file. The prefixes are listed in exhibit 7-1. \"Yes\" indicates that the weight includes nonresponse adjustments for that component.  Main sampling weights (indicated by the suffix 0) and replicate weights (indicated by the suffixes 1 to 40 or 1 to 80) were computed and included in the data file. In the sections that follow, only the main sampling weight is discussed, but any adjustment done to the main sampling weight was done to the replicate weights as well."}, {"section_title": "Student Base Weights", "text": "Only base-year respondents were eligible to participate in the third-grade data collection. The thirdgrade student base weight is the base-year student base weight adjusted for base-year nonresponse. The adjustment factor for base-year nonresponse is the sum of the base weights of the eligible students in the base year divided by the sum of the base weights of the base-year respondents within nonresponse adjustment classes. 9 For a description of the computation of the base-year student base weights, see section  8 This was art of the school nonresponse adjustment that was done in the base year. 9 A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. 4-30"}, {"section_title": "Student Weights Adjusted for Mover Subsampling", "text": "The student base weight described in section 4.3.2.1 was adjusted to reflect the subsampling of movers described in section 4.2.3. For every student who is a base-year respondent, a \"follow\" flag was assigned a value of 0 (do not follow if student moves) or 1 (follow if student moves). A mover-subsampling adjustment factor was set to 1 if the student has never moved out of an original sampled school, 2 if the student moved out of the original sampled school at any time after the base year and was followed into his or her new school, and 0 if the student moved out of the original sampled school at any time after the base year and was not followed. The mover-subsampling adjusted weight is the product of the base weight described in section 4.3.2.1 and this mover-subsampling adjustment factor. Note that child assessments were not conducted and school staff questionnaires were not fielded for students who moved into nonsampled PSUs even if their flag was set to \"follow\"; such students are counted as nonrespondents in the adjustment for nonresponse on weights involving child assessment or teacher data. 10 However, an attempt was made to complete a parent interview for students who moved into nonsampled PSUs if their flag was set to \"follow\"; therefore, their parents would be counted as respondents in the adjustment for parent nonresponse if a parent interview was completed and as nonrespondents if a parent interview was not completed."}, {"section_title": "Student Nonresponse-Adjusted Weights", "text": "The mover-subsampling adjusted weight described in section 4.3.2.2 was adjusted for nonresponse to produce each of the student-level weights described in exhibit 4-1. For each weight, a response status was defined based on the presence of data for the particular component(s) and round(s) covered by the weight. For example, for the weight W7C7P_20, an eligible respondent is a base-year respondent who satisfies both of these criteria: (1) the student has child assessment/child questionnaire data 11 from third grade, and (2) the student has parent interview data from either the fall or spring of kindergarten. An ineligible student is one who moved out of the country or is deceased or moved to another school and was not assigned to be followed. A student of unknown eligibility is one who could not be located. The remaining students are eligible nonrespondents. Nonresponse adjustment was done in two steps: (1) adjustment for children whose eligibility was not determined (i.e., those who could not be located, or those who moved to another sampled PSU and who did not have parent interview data because the parent could not be contacted), and (2) adjustment for eligible nonrespondents. In the first step, a portion of cases with unknown eligibility was assumed to be ineligible. This proportion varied between 1.1 and 2.1 percent for the weights that do not include data from the fall collections, and between 1.6 and 3 percent for the weights that include data from the fall collections; it is highest for those weights that adjusted for teacher nonresponse. The latter is because children who were homeschooled were considered not eligible to have teacher data. Nonresponse classes were created using school and child characteristics and used in adjustments for both unknown eligibility and nonresponse."}, {"section_title": "Raking to Sample Control Totals", "text": "To reduce the variability due to the subsampling of movers and to ensure that the final weights continue to sum to the base-year population total, the student nonresponse-adjusted weights were raked to sample-based control totals using the third-grade student base weights. Raking is a calibration estimator that is closely related to poststratification. The poststratification adjustment procedure involves applying a ratio adjustment to the weights. Respondents are partitioned into groups, known as poststrata cells, and a single ratio adjustment factor is applied to the weights of all units in a given poststratification cell. The numerator of the ratio is a \"control total\" usually obtained from a secondary source; the denominator is a weighted total for the survey data. Therefore at the poststratum level, estimates obtained using the poststratified survey weights will correspond to the control totals used. If either the cell-level population counts are not available for all cells or the majority of the cell sample sizes are too small, raking is used to adjust the survey estimates to the known marginal totals of several categorical variables. Raking is essentially a multivariate poststratification. In the ECLS-K:2011, multiple background characteristics from schools, students, and parents were combined to create raking cells. The student records included in the file used for computing the control totals are records of base-year eligible children. The sum of the base weights from this file is the estimated number of children who were in kindergarten in 2010-11. Raking was done within raking cells (also known as raking dimensions). The raking dimensions were based on single characteristics (e.g., locale) or a combination of characteristics (e.g., age and race/ethnicity). Chi-Square Automatic Interaction Detector (CHAID) analysis was used to determine the best set of raking cells."}, {"section_title": "4-32", "text": "The final weight is the product of the raking factor and the student nonresponse-adjusted weight. The raking factor was computed as the ratio of the base-year sample control total for a raking cell over the sum of the nonresponse-adjusted third-grade weights in that raking cell."}, {"section_title": "Characteristics of Sample Weights", "text": "The statistical characteristics of the sample weights are presented in table 4-13. For each weight, the number of cases with a nonzero weight is presented along with the mean weight, the standard deviation, the coefficient of variation (i.e., the standard deviation as a percentage of the mean weight), the minimum weight, the maximum weight, the design effect of the final weight, the skewness, the kurtosis, and the sum of weights. The procedure for raking to control totals included respondents and ineligible cases. Afterwards, weights of ineligible cases were set to zero. Because a portion of children of unknown eligibility was assumed to be ineligible (as discussed in section 4.3.2.3) and this adjustment for unknown eligibility was done within adjustment cells, there are small differences in the sums of weights. "}, {"section_title": "4-33", "text": "A simple random sample (SRS) is completely self-weighting (i.e., no weights are necessary to produce estimates from this sample). In the ECLS-K:2011, the sample design is multistaged, with nonresponse encountered at both school and student levels. Weighting adjustments were necessary, but they tend to increase the variance of the estimates. As described in section 4.3, the design effect (DEFF)defined as the ratio of the variance estimate under the actual sample design to the variance estimate obtained with an SRS of the same sample size-shows an estimate of the variance increase. One way of approximating this increase due to weighting is by way of the coefficient of variation (CV): In table 4-13, design effect due to weighting is included for each weight. For example, for weight W7C7P_20, the design effect due to weighting is 1+(0.6609) 2 = 1.44, (i.e., the variance is increased by 44 percent due to weight adjustments). The design effect due to weighting varies between 1.42 to 1.66, with the highest design effects due to weighting for the fall subsample (weights W7CF7P_70 and W7CF7P_2T170). The fall subsample includes an additional sampling stage and is about 30 percent of the main sample (see section 4.2.1 for a discussion of the fall subsample)."}, {"section_title": "Variance Estimation", "text": "The precision of the sample estimates derived from a survey can be evaluated by estimating the variances of these estimates. For a complex sample design such as the one employed in the ECLS-K:2011, replication and Taylor Series methods have been developed to correctly estimate variance. These methods take into account the clustered, multistage sampling design and the use of differential sampling rates to oversample targeted subpopulations. For the ECLS-K:2011, in which the first-stage selfrepresenting sampling units (i.e., PSUs) were selected with certainty and the first-stage non-selfrepresenting sampling units were selected with two units per stratum, the paired jackknife replication method (JK2) is recommended. This section describes the JK2 and the Taylor series methods, which can be used to compute correct standard errors for any analysis."}, {"section_title": "Jackknife Method", "text": "The final main sampling and replicate weights can be used to compute estimates of variance for survey estimates using the jackknife method with two PSUs per stratum (JK2) using several software packages, including WesVar, AM, SUDAAN, SAS, Stata, and R. In the jackknife method, each survey 4-34 estimate of interest is calculated for the full sample as well as for each of the g replicates, where g is 80 for the spring weights, and 40 for the fall weights. The variation of the replicate estimates around the fullsample estimate is used to estimate the variance for the full sample. The variance estimator is computed as the sum of squared deviations of the replicate estimates from the full sample estimate: where \u03b8 is the survey estimate of interest, \u03b8 is the estimate of \u03b8 based on the full sample, G is the number of replicates, and \u03b8 ( ) is the g th replicate estimate of \u03b8 based on the observations included in the g th replicate. Each main sampling weight that does not include adjustments for nonresponse to components from the fall data collections has 80 corresponding replicate weights for use with the JK2 method. The replicate weights begin with the same characters as the main sampling weight and end with the numbers 1 to 80. For example, the replicate weights corresponding to weight W7C7P_20 are W7C7P_21 through W7C7P_280. For weights that include nonresponse adjustments for components from the fall data collections, there are 40 replicate weights. For example, weight W7CF7P_70 has W7CF7P_71 through W7CF7P_740 as replicate weights."}, {"section_title": "Taylor Series Method", "text": "Variance stratum and variance unit (first-stage sample unit [i.e., PSU]) identifiers were also created to be used in statistical software that computes variance estimates based on the Taylor series method (for example, AM, SUDAAN, SAS, SPSS, and Stata). In this method, a linear approximation of a statistic is formed and then substituted into the formula for calculating the variance of a linear estimate appropriate for the sample design."}, {"section_title": "If", "text": "is the corresponding vector of estimators based on a sample s of size n(s), \u03b8 = ( ) is the population parameter of interest, and \u03b8 = (\u0302) is an estimator of \u03b8, then where \u03b8 is the estimate of \u03b8 based on the full sample, \u03b8 is the survey estimate of interest, Y is a p-dimensional vector of population parameters, \u0302 is a p-dimensional vector of estimators, y is an element of the vector Y, and ( ) is an estimator of \u03b8. The Taylor series method relies on a simplified procedure for estimating the variance for a linear statistic even with a complex sample design and is valid when analyzing data from large samples in which the first-stage units are sampled with replacement. 12 The stratum and first-stage unit identifiers needed to use the Taylor series method were assigned as follows: all independent sampling strata were numbered sequentially from 1 to h; within each sampling stratum, first-stage sampling units were numbered from 1 to nh. Care was taken to ensure that there were at least two responding units in each stratum. For instances in which a stratum did not have at least two responding units, the stratum was combined with an adjacent stratum. Stratum and first-stage unit identifiers are provided in the data file. Each main sampling weight has corresponding stratum and PSU identifiers for use with the Taylor series method. The stratum and PSU identifiers begin with the same characters as the main sampling weight and end with either STR or PSU. For example, the stratum and PSU identifiers corresponding to weight W7C7P_20 are W7C7P_2STR and W7C7P_2PSU, respectively."}, {"section_title": "Specifications for Computing Standard Errors", "text": "For the jackknife replication method, the main sampling weight, the replicate weights, and the method of replication must be specified. All analyses of the ECLS-K:2011 data using the replication method should be done using JK2. As an example, an analyst using the main sample weight W7C7P_20 to compute child-level estimates of mean reading scores for third grade would need to specify W7C7P_20 as the main sampling weight, W7C7P_21 to W7C7P_280 as the replicate weights, and JK2 as the method of replication. Note that there are 40 replicate weights for each weight that involves the any of the fall data collections, and 80 replicate weights for each weight not involving any of the fall data collections. For the Taylor series method, the main sampling weight, the sample design, the nesting stratum, and PSU variables must be specified. As an example, an analyst using the main sample weight W7C7P_20 to compute child-level estimates of mean reading scores for third grade must specify the main sampling weight (W7C7P_20), the stratum variable (W7C7P_2STR), and the PSU variable (W7C7P_2PSU). The \"with replacement\" sample design option, WR, must also be specified if using SUDAAN."}, {"section_title": "Use of Design Effects", "text": "An important analytic device is to compare the statistical efficiency of survey estimates from where SE is the standard error of the estimate. As discussed above, jackknife replication and Taylor Series can be used to compute more precise standard errors for data from complex surveys. If statistical analyses are conducted using software packages that assume the data were collected using simple random sampling (i.e., adjustments are not made using jackknife replication or the Taylor series method), the standard errors will be calculated under this assumption and will be incorrect. They can be adjusted using the average DEFT, although this method is . 4-37 less precise than JK or Taylor series. 13 The standard error of an estimate under the actual sample design can be approximated as the product of the DEFT and the standard error assuming simple random sampling. In the ECLS-K:2011, a large number of data items were collected from children, parents, teachers, school administrators, and before-and after-school care providers. Each item has its own design effect that can be estimated from the survey data. Standard errors and design effects are presented in the tables below for selected items from the study to allow analysts to see the range of standard errors and design effects for the study variables. They were computed using the paired jackknife replication method in the statistical software package WesVar. However, as discussed in section 4.3.4, not all statistical analysis software packages have procedures to compute the variance estimate or standard error using the replication method, and some analysts may not have access to software packages that do have such procedures. In such situations the correct variance estimate or standard error can be approximated using the design effect or the root design effect. As the first step in the approximation of a standard error, the analyst should normalize the overall sample weights for packages that use the weighted population size (N) in the calculation of standard errors (SPSS but not SAS). The normalized weight will sum to the sample size (n) and is calculated as where n is the sample size (i.e., the number of cases with a valid main sampling weight) and N is the sum of weights. See table 4-13 for the sample size n and the sum of weights N. 13 Common procedures in SAS, SPSS, and Stata assume simple random sampling. Data analysts should use the SURVEY procedure (SAS), the Complex Samples module (SPSS), or the SVY command (Stata) to account for complex samples."}, {"section_title": "4-38", "text": "As the second step in the approximation, the standard errors produced by the statistical software, the test statistics, or the sample weight used in analysis can be adjusted to reflect the actual complex design of the study. To adjust the standard error of an estimate, the analyst should multiply the standard error produced by the statistical software by the square root of the DEFF or the DEFT as follows: A standard statistical analysis package can be used to obtain VARSRS and SESRS. The DEFF and DEFT used to make adjustments can be calculated for specific estimates, can be the median DEFF and DEFT across a number of variables, or can be the median DEFF and DEFT for a specific subgroup in the population. Adjusted standard errors can then be used in hypothesis testing, for example, when calculating t and F statistics. A second option is to adjust the t and F statistics produced by statistical software packages using unadjusted (i.e., SRS) standard errors. To do this, first conduct the desired analysis weighted by the normalized weight and then divide a t statistic by the DEFT or divide an F statistic by the DEFF. A third alternative is to create a new analytic weight variable in the data file by dividing the normalized analytic weight by the DEFF and using the adjusted weight in the analyses. Table 4-14 shows estimates, standard errors, and design effects for 50 means and proportions selected from the third-grade data collection. Table 4-15 shows the median design effects for the same items but for subgroups. For each survey item, table 4-14 presents the number of cases for which data are nonmissing, the estimate, the standard error taking into account the actual sample design (Design SE), the standard error assuming SRS (SRS SE), the root design effect (DEFT), and the design effect (DEFF). Standard errors (Design SE) were produced in WesVar using JK2 based on the actual ECLS-K:2011 complex design. For each survey item, the variable name as it appears in the data file is also provided in the table. In general, design effects for the third grade are comparable to design effects for the spring of second grade for similar items. As was the case in earlier years, design effects for the teacher-level data and the school-level data are quite large compared to the design effects of items coming from the child assessment or parent interview because the intraclass correlation is 100 percent for children in the same class with the same teacher and for children in the same school.  2 Estimates of variables from the teacher and school administrator questionnaires computed using weight W7C27P_7T70. 3 Estimates of variables from the parent interview computed using weight W7C27P_7A0. 4 This characteristic was created using the series of variables A7TSPNH, A7TSPNH, A7TVTNM, A7TCHNS, A7TJPNS, A7TKRN, A7TFLPN, A7ARBIC, A7TINDN, A7OTHLG (teacher questionnaire items A7 A13B to A7 A13K). NOTE: SE is the standard error based on the sample design. SEsrs is the standard error assuming simple random sampling. DEFT is the root design effect. DEFF is the design effect. Estimates produced with the restricted-use file. Due to top-and bottom-coding, the same estimates may not be obtained from the public-use file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2014. This page intentionally left blank."}, {"section_title": "DESIGN SRS SRS", "text": ""}, {"section_title": "SE DEFF VAR DEFT SE", "text": ""}, {"section_title": "4-40", "text": ""}, {"section_title": "4-41", "text": "5-1"}, {"section_title": "RESPONSE RATES", "text": "This chapter presents unit response rates and overall response rates for the different instruments included in the third-grade round of data collection (spring 2014) for the ECLS-K:2011. A unit response rate is the ratio of the number of units with a completed interview, questionnaire, or assessment (for example, the units are students with a completed assessment) to the number of units sampled and eligible for the interview, questionnaire, or assessment. Unit response rates are used to describe the outcomes of data collection activities and to measure the quality of the study. The overall response rate indicates the percentage of eligible units with a completed interview, questionnaire, or assessment, taking all survey stages into account."}, {"section_title": "Study Instruments", "text": "For the ECLS-K:2011 third-grade data collection, there were several survey instruments, as shown in exhibit 5-1. Exhibit 5-1 also indicates how much information had to be collected for each instrument for it to be considered \"complete\" and, therefore, for a case to be considered a respondent to that instrument for the purpose of calculating response rates. Response rates are presented in section 5.2 for all of these instruments."}, {"section_title": "5-2", "text": "Exhibit Yes School administrator completed at least one item in this questionnaire 1 In the third-grade data collection, there were two versions of the school administrator questionnaire. SAQ-A was given to administrators in schools that were new to the study or administrators in schools for which there was no previously completed SAQ. SAQ-B was given to administrators in schools that had a previously completed SAQ.. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2014."}, {"section_title": "Unit Response Rates and Overall Response Rates", "text": "The tables in this section present both weighted and unweighted response rates for the different components of data collection shown above in exhibit 5-1 (the child assessment, parent interview, general classroom teacher questionnaires, school administrator questionnaire (SAQ), and special education teacher questionnaires) computed at the student level. Response rates for all students and response rates by selected school and student background characteristics are provided."}, {"section_title": "5-3", "text": "Only weighted rates are discussed in this section. The unweighted rate provides a useful description of the success of the operational aspects of the survey. The weighted rate gives a better description of the success of the survey with respect to the population sampled since the weights allow for inference of the sample data (including response status) to the population level. Both rates are usually similar unless the probabilities of selection and the unit response rates in the categories with different selection probabilities vary considerably. All of the unit response rates discussed in this chapter are weighted unless noted specifically in the text, since the main purpose of this chapter is to describe the success of the survey with respect to the survey population. The weights used in the computation of the student-level unit response rates are the third-grade student base weights. For a description of these weights, see chapter 4. In order to compute response rates by different characteristics, the selected characteristics must be known for both respondents and nonrespondents. Multiple sources were used to obtain information on school characteristics in order to have data that were as complete as possible for the calculation of response rates. For respondents, data for school census region, school locale, school type, and school enrollment come from the composite variables derived for the data file. For nonrespondents, school characteristic variables were computed for use in the response rate calculations using the same process that Information on the child characteristics presented in the tables comes from the third-grade data collection. Information on student sex comes from the composite variable X_CHSEX_R (described in section 7.5.1.3). Information on student race/ethnicity comes from the composite variable X_RACETH_R (described in section 7.5.1.4). Information on student year of birth comes from the composite variable X_DOBYY_R (described in section 7.5.1.1). These composites were derived for all base-year respondents; therefore, they exist for third-grade respondents as well as nonrespondents. When necessary, comparisons in this chapter were examined to ensure that the differences discussed were statistically significant at the 95 percent level of confidence. For example, this was done for tables in sections 5.3 when comparing characteristics of the data using different weights, or when comparing data from different years. Significance tests were not conducted for statements related to response rates in 5-4 section 5.2 because the base weights were used to produce all rates, which are calculated over the same sample of eligible cases. The overall response rate indicates the percentage of possible interviews, questionnaires, or assessments completed, taking all survey stages into account. In the base-year data collection, children were identified for assessment in a two-stage process. The first stage involved the recruitment of sampled schools to participate in the study. Assessments were then conducted for the sampled children whose parents consented to the children's participation. In third grade, children were contacted for follow-up unless they 1became ineligible for the study because they had moved out of the country or had died, or (2) were movers who were not sampled for follow-up and, therefore, were excluded from data collection. The response rate for the child assessment is the percentage of sampled and eligible children not subsampled out as an unfollowed mover who completed the assessment. The overall weighted response rate is the product of the base-year before-substitution school response rate for all schools (62.7 percent) and the third-grade weighted child assessment response rate. The overall unweighted response rate is the product of the unweighted baseyear before-substitution response rate for all schools (61.3 percent) and the third-grade unweighted child assessment response rate. In the overall response rate tables, the response rates by characteristic are also a product of the third-grade response rate by the corresponding (weighted or unweighted) overall base-year rate. Because children were sampled in the base year and school participation after the base year was not required for the children to stay in the study, the school response rates used to calculate the studentlevel response rates in these tables are those from the base year (the base-year response rates are presented in  , hereinafter referred to as the base-year User's Manual). In the third-grade data collection, all 18,174 base-year respondents were part of the sample. Of these, about 160 became ineligible for the data collection because they had moved out of the country sometime between the base year and the start of the third-grade data collection and 5 had died. An additional 2,620 students were not included in the data collection because they were movers who were subsampled out of the study (see section 4.2.3 for information on mover subsampling). After these exclusions for ineligibility and subsampling, the number of children followed for data collection in third-grade was about 15,390. This number is the denominator used to calculate the unweighted parent interview response rate. This is also the basis of the denominator used to calculate the unweighted child assessment response rate. However, children who were excluded from the assessment because the study did not provide needed accommodations for a disability, such as an assessment in Braille, are not included in the calculation of response rates for the child assessment. Therefore, the denominator used to calculate the unweighted child 5-5 assessment response rate is about 15,310. The denominator used to calculate the teacher and the school administrator response rates is 12,990. This denominator is lower than the ones used to calculate response rates for the child assessment and parent interview because it excludes students who were not eligible for the teacher and administrator questionnaire components: homeschooled children and children who did not have either a complete child assessment score or parent interview (per the definition of complete provided in Exhibit 5-1) for the third-grade collection. The parent and teacher rates are computed at the student level, meaning they indicate the percentages of students for whom a parent interview was completed or for whom a teacher questionnaire was received. The school administrator rate is also computed at the student level and indicates the percentage of students whose school administrator completed a questionnaire. There were two versions of the administrator questionnaire but response rates are not calculated separately for each version since a student would only have data for one version. Table 5-1 presents weighted and unweighted response rates for the child assessment and the parent interview in the third-grade data collection by selected school characteristics. Response rates for the child questionnaire are the same as for the child assessment because all children with assessment data have child questionnaire data and vice-versa. Researchers should note that the \"unknown/homeschool group\" has a low response rate, in large part because this group includes unlocatable cases who are, by default, nonrespondents. This unknown/homeschool group (1,736 cases) is about 11 percent of the overall sample of eligible cases. Because their school characteristics are unknown, cases in this group cannot be included in a specific school characteristics category. This may have an impact on the calculation of the response rates by school characteristics that should be considered. Specifically, including these unlocatable cases in a separate category likely results in response rates by different school characteristics being higher than they would be if the unlocatable cases were included as nonrespondents when calculating response rates for the different school characteristic categories. Not including the \"unknown\" subgroups, the lowest response rate by school characteristics for the child assessment/child questionnaire was for students in non-Catholic private schools (83.5 percent). For other subgroups, response rates ranged from 88.9 to 96.4 percent. For the parent interview, the lowest response rate by school characteristics was for students in schools in the highest percent minority group (69.8 percent). Parent interview response rates ranged from 72.9 to 83.0 percent for all other subgroups.  1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, or a completed item from the Child Questionnaire."}, {"section_title": "5-6", "text": "2 Parent answered all applicable items in the family structure section of the questionnaire (FSQ) through item FSQ200 on current marital status. 3 School characteristics were taken from the third-grade school administrator questionnaire (SAQ) when available. When third-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 4 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 5 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: The weighted response rates were calculated using the third-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2014. Table 5-2 presents weighted and unweighted response rates for the child assessment and the parent interview in the third-grade data collection by selected student characteristics. For the child assessment, Hispanic students had the highest response rate at 84.4 percent. Among subgroups with a large number of sample members, Black students had a lower response rate (72.7 percent), while some subgroups with small sample sizes also had lower response rates: American Indians/Alaskan Natives (72.4 percent) and students born in 2006 (71.3 percent). For the parent interview, the highest response rate was for White students (74.5 percent), while the lowest parent response rates were for the following subgroups: Native Hawaiians/Other Pacific Islanders (57.8 percent) and Black students (58.6 percent). 1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, or a completed item from the Child Questionnaire."}, {"section_title": "5-8", "text": "2 Parent answered all applicable items in the Family Structure Questions (FSQ) section of the questionnaire through item FSQ200 on current marital status. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: The weighted response rates were calculated using the third-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2014. 91.9 percent, and 92.1 percent for the teacher-level teacher questionnaire, the teacher-level subject-area 5-9 questionnaire, and the student-level teacher questionnaire, respectively. The pattern of response rates is almost the same for all three teacher questionnaires. By school characteristics, the highest rates were between 96.6 and 97.9 percent for students in schools with fewer than 149 students, for students in schools with 15 percent or fewer students who were racial/ethnic minorities, for students in rural schools, and for students in Catholic schools. The lowest rates (between 86.1 and 88.0 percent) were for students in cities or students in schools with at least 86 percent of students who were racial/ethnic minorities.  1 School characteristics were taken from the third-grade school administrator questionnaire (SAQ) when available. When third-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data. The weighted response rates were calculated using the third-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2014. White students (95.0 percent, 94.4 percent, and 94.7 percent for the teacher-level teacher questionnaire, the teacher-level subject-area questionnaire, and student-level teacher questionnaire, respectively. The subgroups with the lowest rates were students born in 2006 (75.0 percent for all three questionnaires) and Asian students (between 80.3 and 80.9 percent for the three questionnaires). These patterns of response by subgroup are similar to the patterns seen in the spring of second grade.   1 School characteristics were taken from the third-grade school administrator questionnaire (SAQ) when available. When third-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file."}, {"section_title": "5-11", "text": ""}, {"section_title": "5-12", "text": "2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data. The weighted response rates were calculated using the third-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2014. 1 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. Table 5-7 presents weighted and unweighted overall response rates for the child assessment and the parent interview in the third-grade data collection by selected school characteristics. The overall response rate is the percentage of possible assessments, interviews, or questionnaires completed, taking into account all survey stages. Of the 2,684 original and transfer schools that were initially eligible for the thirdgrade data collection, 2,562 schools participated in the study, 33 schools refused, and 89 became ineligible because all ECLS-K:2011 students in the school had moved to other schools. The school response rates used in the overall rates are from the base year because children were sampled in the base year and are 5-15 eligible to stay in the study regardless of school participation after the base year. The overall weighted response rate is the product of the base-year before-substitution school response rate for all schools (62.7 percent) and the third-grade weighted response rate. The overall unweighted response rate is the product of the unweighted base-year before-substitution response rate for all schools (61.3 percent) and the third-grade unweighted response rate. In the overall response rate tables, the response rates by characteristic are also a product of the third-grade response rate by the corresponding (weighted or unweighted) overall base-year rate. The overall weighted response rate for the child assessment was 50.1 percent. For the parent interview, the overall weighted response rate was 44.0 percent. Because the driving factor of the overall response rate is the base-year school response rate for all schools, the pattern of overall response rates by subgroups is the same as the pattern of third-grade response rates.  1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, or a completed item from the Child Questionnaire."}, {"section_title": "5-16", "text": "2 Parent answered all applicable items in the family structure section of the questionnaire (FSQ) through item FSQ200 on current marital status. 3 School characteristics were taken from the third-grade school administrator questionnaire (SAQ) when available. When third-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 4 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 5 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: The weighted overall response rates were calculated using the school base weight for the school response rate component and the student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this  Table 5-8 and table 5-9 present weighted and unweighted overall response rates for three teacher questionnaires in the third-grade data collection, by selected school characteristics. The overall response rate for the teacher-level teacher questionnaire was 57.9 percent. The overall response rate for the teacher-level subject-area questionnaire was 57.6 percent. The overall response rate for the student-level teacher questionnaire was 57.7. The response rates by subgroup follow the same pattern as was seen for the thirdgrade teacher response rates. 1 School characteristics were taken from the third-grade school administrator questionnaire (SAQ) when available. When third-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file."}, {"section_title": "5-17", "text": "2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data. The weighted overall response rates were calculated using the school base weight for the school response rate component and the third-grade student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this   Midwest: Illinois,Indiana,Iowa,Kansas,Michigan,Minnesota,Missouri,Nebraska,North Dakota,Ohio,South Dakota,and Wisconsin. South: Alabama,Arkansas,Delaware,Florida,Georgia,Kentucky,Louisiana,Maryland,Mississippi,North Carolina,Oklahoma,South Carolina,Tennessee,Texas,Virginia,West Virginia,and the District of Columbia. West: Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data. The weighted overall response rates were calculated using the school base weight for the school response rate component and the third-grade student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this "}, {"section_title": "5-18", "text": ""}, {"section_title": "5-19", "text": "Table 5-10 presents the response rates for the two special education teacher questionnaires. Response rates are not presented by subgroup for the special education teacher questionnaires because of the relatively small number of students eligible for this component. The denominator for the special education teacher rates is 1,268. This denominator excludes children who did not have either a complete child assessment score or parent interview for the third-grade collection, even if they had special education teacher data. The two special education teacher questionnaires had the same response rates. Tables 5-11 and 5-12 present response rates for the school administrator questionnaire (SAQ) included in the third-grade data collection. In the base year, the school sample was representative of schools educating kindergartners and kindergarten-aged children, so the base-year User's Manual presented response rates at the school level. After the base year, the school sample is the set of schools attended by children in the ECLS-K:2011 and is no longer a nationally representative sample of schools. For this reason, response rates for the SAQ are presented only at the student level. Table 5-11 presents the weighted and unweighted response rates for the school administrator questionnaire by selected school characteristics. They are rates for students who were not homeschooled and are respondents in the third-grade data collection. 1 The weighted response rate for the school administrator questionnaire was 91.6 percent. The highest response rates by school characteristics for this questionnaire were between 97.2 and 97.7 percent for students in schools with fewer than 150 students, students in schools with zero to 15 percent of students who were racial/ethnic minorities, and students in rural areas. Aside from the \"Unknown\" categories, which had very low response rates due to their composition, the lowest response rates were for students in schools with at least 86 percent of students who were racial/ethnic minorities (85.4 percent) and in cities (88.5 percent). In this table, the \"unknown\" 5-20 categories include a small number of students with SAQ data, but for whom locale, school size, and/or minority enrollment data are missing.   Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned with at least one response, and the student had either child assessment or parent interview data. The weighted response rates were calculated using the third-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2014. Table 5-12 presents the weighted and unweighted response rates for the school administrator questionnaire by selected student characteristics. Excluding subgroups with small numbers of sampled students, the highest response rate was for White students (94.1 percent) and the lowest response rate was for Asian students (84.8 percent). 1 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned with at least one response, and the student had either child assessment or parent interview data. The weighted response rates were calculated using the third-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2014. Table 5-13 shows the overall response rates for the school administrator questionnaire. The overall weighted response rate was 57.4 percent. As with other overall response rates, the overall rates by subgroups have the same patterns as the third-grade response rates because the base-year school response rate is for all schools and, thus, the same for all subgroups.  Midwest: Illinois,Indiana,Iowa,Kansas,Michigan,Minnesota,Missouri,Nebraska,North Dakota,Ohio,South Dakota,and Wisconsin. South: Alabama,Arkansas,Delaware,Florida,Georgia,Kentucky,Louisiana,Maryland,Mississippi,North Carolina,Oklahoma,South Carolina,Tennessee,Texas,Virginia,West Virginia,and the District of Columbia. West: Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned with at least one response, and the student had either child assessment or parent interview data. The weighted overall response rates were calculated using the school base weight for the school response rate component and the third-grade student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because students with unknown school characteristics are not included in this 5-24"}, {"section_title": "5-22", "text": ""}, {"section_title": "5-23", "text": ""}, {"section_title": "Nonresponse Bias Analysis", "text": "NCES statistical standards require that any survey instrument with a weighted unit response rate less than 85 percent be evaluated for potential nonresponse bias. For the third-grade data collection, two study components had weighted response rates lower than 85 percent: the child assessment (79.9 percent, weighted, and 84.2 percent, unweighted), and the parent interview (70.1 percent, weighted, and 72.0 percent, unweighted). The effect of nonresponse is examined in two ways. Sections 5.3.1 and 5.3.2 discuss the effect of nonresponse on estimates produced from the child assessment data and the parent interview data, respectively. Section 5.3.3 compares estimates of selected base-year characteristics between base-year respondents and third-grade respondents. 2 A comparison of the study estimates to frame estimates, which pertain to schools with third grade and to third-graders in the United States, cannot be done because the sample of study schools is not a representative sample and the sample of study students is not representative of all third graders. After the base year, students in the ECLS-K:2011 can only be referred to as the cohort of children who attended kindergarten or of kindergarten age in the 2010-11 school year. For a comparison to frame estimates that was conducted in the base year of the study, see chapter 5 of the base-year User's Manual."}, {"section_title": "Effect of Nonresponse on Child Assessment Data", "text": "Estimates weighted by the nonresponse-adjusted weights are compared with estimates weighted by the base weights (which are referred to as unadjusted estimates). Large differences between the estimates weighted by the nonresponse-adjusted weights and the unadjusted weights may indicate the potential for bias in the unadjusted estimates. If the differences are small, then either there is very small bias in the estimates or the characteristics used in the adjustment process are not related to the survey estimates and, therefore, the adjustments do not introduce changes in the estimates. The unadjusted base weight only takes into account the selection probabilities of the sampling units and the subsampling of movers to be followed. The nonresponse-adjusted weights are the weights used to analyze ECLS-K:2011 data. The nonresponse adjusted weight used in this analysis of the effect of nonresponse on child assessment data is W7C7P_20, which is adjusted for nonresponse to the child assessment. For a discussion of how the weights were constructed, see chapter 4. 2 A base-year respondent has child data (scoreable assessment data, or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. A third-grade respondent has child data (scoreable assessment data, or executive function data, or child questionnaire data, or height or weight measurements or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from the third-grade round of data collection."}, {"section_title": "5-25", "text": "In the ECLS-K:2011, chi-square analyses were used to identify characteristics that are most related to nonresponse, and these characteristics were used in the adjustment. Therefore, the likelihood that the weighted estimates are biased as a result of nonresponse would be lower than if nonresponse adjustment was not implemented. This method of examining nonresponse bias provides an indication of the degree to which nonresponse adjustments are needed and how effective the adjustments are. an absolute difference between the nonresponse-adjusted and unadjusted estimates of 1.02 and a relative difference of 4.08. Proportionately there are fewer students who attended school in a town than students who attended school in the West; therefore, the relative difference is higher for students who went to school in a town even though the absolute difference is smaller for students in this group compared to students who attended school in the West. The differences found in the analyses show that there is some potential for nonresponse bias in the unweighted assessment data, but the weights used to produce estimates were adjusted for nonresponse and, thus, reduce that potential for bias. "}, {"section_title": "5-28", "text": ""}, {"section_title": "Effect of Nonresponse on Parent Interview Data", "text": "The adjusted weight used in the analysis of the effect of nonresponse on parent interview data is W7C27P_7A0. For a discussion of how the weights were constructed, see chapter 4. Table 5-16 shows estimates of selected items from the parent interview. Table 5-17 shows the differences between unweighted and weighted estimates, and between estimates produced using base weights (unadjusted estimates) and estimates produced using nonresponse adjusted weights. The range of absolute differences is 0 to 3.4, and the average is 0.64. The discussion of how to interpret the relative difference provide above in the section on the child assessment applies to the parent interview data as well. As noted above, the percent difference is sensitive not only to sample size but also to the prevalence of a particular characteristic. smaller for the groups of students with higher prevalence in the characteristic examined. As with the child assessment data, the differences found in the analyses show that there is some potential for nonresponse bias in the unweighted parent interview data, but the weights used to produce estimates were adjusted for nonresponse and, thus, reduce that potential for bias.  1 Unadjusted estimates are produced using the third-grade student base weight. The sample size is the count of cases with nonzero third-grade student base weight."}, {"section_title": "5-30", "text": "2 Adjusted estimates are produced using weight W7C27P_7A0. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas,Michigan,Minnesota,Missouri,Nebraska,North Dakota,Ohio,South Dakota,and Wisconsin. South: Alabama,Arkansas,Delaware,Florida,Georgia,Kentucky,Louisiana,Maryland,Mississippi,North Carolina,Oklahoma,South Carolina,Tennessee,Texas,Virginia,West Virginia,and the District of Columbia. West: Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: The sample sizes are the number of cases with a nonzero third-grade base weight and a nonmissing value for the characteristic or group of characteristics. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2014."}, {"section_title": "5-35", "text": ""}, {"section_title": "Effect of Nonresponse on Characteristics from the Base Year", "text": "In this section, the effect of nonresponse is explored by comparing estimates of selected baseyear characteristics between kindergarten respondents and third-grade respondents. 3 The estimates are unadjusted estimates (i.e., they are weighted by the base weights). Base-year characteristics of the kindergarten respondents are weighted by the base-year base weight that takes into account only the selection probabilities of the sampling units. Base-year characteristics of the third-grade respondents are weighted by the third-grade base weight that takes into account the selection probabilities and the subsampling of movers to be followed. Table 5-18 shows the differences in the unadjusted base-year estimates between the kindergarten respondents and the third-grade respondents. As noted above, the characteristics presented in this table are from the base year, since the purpose of this analysis is to detect large changes in the same estimates due to sample attrition between the two data collections. Because of missing values, the kindergarten sample size is smaller than 18,174, the number of base-year respondents. Similarly, the thirdgrade sample size is smaller than 13,579, the number of third-grade respondents. Each difference is shown as an absolute value and as a relative difference (i.e., the difference divided by the kindergarten estimate). The relative differences range from 0 percent to 14.71 percent, for an average of 3.28 percent. The largest relative difference is for the percentage of Black students. As in previous years, response rates for Black students are the lowest among the different race/ethnicity groups (not counting the Hawaiian Native/Pacific Islander and the American Indian/Alaska Native groups with very small sample sizes). The other relative differences that are larger than 5 percent are for students in towns (6.79 percent), students of two or more races (5.37 percent), and students in households with income below the poverty threshold (6.47 percent). Since locale and race/ethnicity are characteristics used to construct nonresponse cells for nonresponse adjustments, any potential bias would be reduced in estimates produced using weights adjusted for nonresponse.   Indiana,Iowa,Kansas,Michigan,Minnesota,Missouri,Nebraska,North Dakota,Ohio,South Dakota,and Wisconsin. South: Alabama,Arkansas,Delaware,Florida,Georgia,Kentucky,Louisiana,Maryland,Mississippi,North Carolina,Oklahoma,South Carolina,Tennessee,Texas,Virginia,West Virginia,and the District of Columbia. West: Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: The sample sizes for kindergarten are the number of base-year respondents with a nonmissing value for the kindergarten characteristic or group of characteristics. The sample sizes for third grade are the number of third-grade respondents with a nonmissing value for the kindergarten characteristic or group of characteristics. The reduction in potential bias is seen in table 5-19 where the relative differences between the adjusted estimates decrease to a range of 0 to 9.95 percent, with an average of 2.52 percent. For each group in table 5-19, the sample size is the number of records with nonzero final weights. Generally, a relative difference of more than 5 percent indicates that there may be potential bias in the estimate. There are no relative differences larger than 5 percent among the demographic groups. Relative differences larger than 5 percent are seen for children who regularly spoke a non-English language at home (9.95 percent) and student in households with income at or above the poverty threshold but below 200 percent of the poverty threshold (6.94 percent). As mentioned before, the relative difference is a function not only of the sample size but also of the prevalence of a particular characteristic. For example, only 8.24 percent of kindergartners and 7.42 percent of students in third grade regularly used a non-English language at home (representing a high relative difference), compared with 90.37 percent of kindergartners and 91.98 percent of students in third grade with at least one parent with a high school degree or higher (representing a low relative difference).  computer-assisted interviews and assessments (CAI) and self-administered paper forms (hard-copy questionnaires). As in kindergarten (i.e., the base year), first grade, and second grade, once data were collected, they were reviewed and prepared for release to analysts. The approaches used to prepare the data differed with the mode of data collection. The direct child assessments and parent interviews were conducted using CAI. Editing specifications were built into the CAI programs used by assessors or interviewers collecting these data. The teacher and school administrator hard-copy questionnaires were selfadministered. When these hard-copy questionnaires were returned to the data collector's home office, staff recorded the receipt of these forms into a project-specific forms tracking system. Data from the hard-copy questionnaires were then captured by scanning the completed forms. Before scanning, coders reviewed the questionnaires to ensure that responses were legible and had been written in appropriate response fields for transfer into an electronic format. After the data were scanned and reviewed for range and logical consistency, coding of open-ended 1 \"other, specify\" text responses into existing or new categories was implemented."}, {"section_title": "5-36", "text": ""}, {"section_title": "5-38", "text": "The following sections briefly describe the data preparation activities for both modes of data collection, focusing on the third-grade activities. More detailed information on these data preparation activities can be found in the base-year (kindergarten), kindergarten-first grade, and kindergarten-second 6-2"}, {"section_title": "Coding Text Responses", "text": "Additional coding was required for some of the items asked in the CAI parent interview once the data had been collected. These items included \"other, specify\" text responses and responses to questions asking about parent or guardian occupation, which interviewers had entered into the CAI system verbatim. Review of \"other, specify\" items. As in previous rounds, for third grade, trained data preparation staff reviewed respondents' verbatim \"other, specify\" text responses and coded responses into existing response categories as appropriate. These staff also reviewed the \"other, specify\" text to identify any responses that occurred with sufficient frequency to warrant the addition of a new response category. Beginning with the third-grade parent interview, one item required an additional category in order to categorize \"other, specify\" text responses that occurred with sufficient frequency. HEQ290 asked about subjects that the child was tutored in; the category \"Language Arts\" was added and used to categorize responses such as English, Spelling, Writing, Communication Arts, Grammar, Language Skills, and Vocabulary. Note that while this question was also asked in the spring first-grade and spring second-grade rounds, such responses were not upcoded to \"Language Arts\" until this third-grade round. New categories added as a result of a review of \"other, specify\" responses are noted as such in appendix A. Text responses that did not fit into any preexisting category and were not common enough to be coded into a new category were left coded as \"other\" in the data. There were no \"other, specify\" items in the child assessments. Parent occupation coding. In the third-grade data collection round, specifics related to a parent's occupation such as job title and employer were not asked in the parent interview. Details about parent occupation coding in earlier rounds can be found in the respective User's Manual for the round."}, {"section_title": "Household Roster Review", "text": "The third-grade parent interview included a household roster in which information on household composition was collected. Following protocols established during the previous rounds, three general types of checks were run on the household roster information to identify missing or inaccurate information that would require editing. were examined further. Information was corrected when the interview contained sufficient information to support a change. \uf06e Second, while it is possible to have more than one mother or more than one father in a household, households with more than one mother or more than one father were reviewed to ensure they were not cases of data entry error. Corrections were made whenever clear errors were identified and a clear resolution existed. \uf06e Third, the relationships of an individual in the household to both the study child and the respondent were examined, as there were cases in which the relationship of an individual to the study child conflicted with his or her status as the spouse/partner of the respondent. For example, in a household containing a child's grandparents but not the child's parents, the grandmother may be designated the \"mother\" figure, and the grandfather thus becomes the \"father\" figure for the purposes of some questions in the interview by virtue of his marriage to the grandmother. In this example, these cases would have been examined but left unchanged. Both the original-and correct (grandfather)-relationship data and the new \"parent-figure\" designation (father) that had been constructed were retained. In other situations, discrepancies in the reported relationships indicated an error, and the data were edited. For example, in a household containing two mothers, if a review of the audio recording from the interview indicated the relationship of the second mother was documented incorrectly by the interviewerthat the second female identified as a mother was not actually a mother to the focal child-the relationship of the second female would have been edited (corrected) to something other than mother. A flag on the data file (X7EDIT) identifies cases that were reviewed or edited for any of the reasons described above; the flag is set to 1 if the case was identified for review for any of these household roster checks. Note that a code of 1 does not necessarily indicate that the data were changed; if the data were reviewed and found to be as reported by the respondent or there was no clear error to be fixed, the reviewed data were left as is. There were 4,203 cases (37.9 percent) identified for review of the household roster from the spring of third grade. The number of cases identified for review was substantially higher in third grade than in previous rounds because age incrementation errors from prior rounds for persons in the household roster were fixed during editing of third-grade interviews. Of the 4,203 cases edited, the age incrementation edits impacted 2,663 cases. 3 6-4"}, {"section_title": "Partially Complete Parent Interviews", "text": "Parents did not have to complete an entire interview for the data collected from them to be included on the data file. However, parent interviews did have to be completed through a specified section of the interview for those data to be included. For the third-grade round, the respondent had to answer all applicable questions through the majority of the section on family structure (FSQ). There were 638 partially completed spring parent interviews for which the respondent answered applicable questions in the FSQ section but did not complete the entire interview. 4 All data derived from questions asked after the interview termination point for these partially completed interviews are set to -9 for \"not ascertained.\" 6.2 Receipt, Coding, and Editing of Hard-Copy Questionnaires"}, {"section_title": "Receipt Control", "text": "Receipt control was managed in the same manner for third grade as it had been in the earlier rounds of the ECLS-K:2011. Please refer to the base-year User's Manual for details."}, {"section_title": "Scanning of Hard-Copy Questionnaires", "text": "Scanning of hard-copy questionnaires was managed in the same manner for third grade as it had been in the earlier rounds of the ECLS-K:2011. Please refer to the base-year User's Manual for details."}, {"section_title": "Coding for Hard-Copy Questionnaires", "text": "Similar to the process described for the parent interview and identical to procedures used in earlier rounds, \"other, specify\" text responses were reviewed by the data preparation staff and coded at the instrument level. In the third-grade round, there were a small number of items for which additional categories were added to categorize \"other, specify\" text responses that occurred with sufficient frequency. For example, in response to the question about measures taken by the school to ensure the safety of children 6-5 in the school administrator questionnaires, a sufficient number of respondents indicated that cameras or video were used. A new response category was added to classify these responses. New categories added as a result of this review of \"other, specify\" responses are noted as such in appendix A. In addition, categories that were added during the coding process in earlier rounds that did not appear on the third-grade hard-copy questionnaires were again available to coders during the coding process for third grade. For example, in earlier rounds, a number of teachers provided \"other, specify\" responses to the question about what languages other than English are spoken by teachers or aides to the ELL children in the class. Although not listed on the questionnaire, several categories to classify these responses (e.g. African languages, Creole, French, German, Polish, Portuguese, Russian, and Hmong) were available to the coders for third grade as well. Text responses that did not fit into any preexisting category and were not common enough to be coded into new categories were left coded as \"other\" in the data."}, {"section_title": "Data Editing", "text": "The data editing process for hard-copy questionnaires was managed in the same manner for third grade as it had been in the earlier rounds of the ECLS-K:2011. The base-year User's Manual has more detail related to editing. As part of the editing process in third grade as well as in earlier rounds of the ECLS-K:2011, skip patterns were enforced. In cases in which respondents did not follow the skip instructions and proceeded to answer the questions that were supposed to be skipped, responses for the inapplicable dependent questions generally were deleted and the data were set to -1, the inapplicable code. There are two check boxes (shown below), one on the school administrator questionnaire given to administrators in schools that were new to the study or for which a completed school administrator questionnaire was not received in a prior data collection (i.e., SAQ-A) and one on the teacher-level teacher questionnaire, that were part of skip patterns that, in certain circumstances, were not enforced: When respondents marked these check boxes, they were directed to skip ahead in the questionnaire because a subset of subsequent, dependent questions were not applicable to them. In some cases, it was clear to the data editors that the check box was marked in error by the respondent and the responses to the dependent questions were valid, usable data. In such cases, the check box was edited (corrected) in order to retain responses to dependent questions in the data. Consequently, data for the two check boxes listed may not reflect the actual responses provided by the person completing the questionnaire."}, {"section_title": "DATA FILE CONTENT AND COMPOSITE VARIABLES", "text": "This chapter describes the contents of the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) kindergarten through third-grade (K-3) restricted-use data file. The data are accessible through software called the Electronic Codebook (ECB). The ECB allows data users to view variable frequencies, tag variables for extraction, and create the SAS, SPSS for Windows, or Stata code needed to create an extract file for analysis. The child data file on the ECB is referred to as a \"child catalog.\" Instructions for installing the ECB are provided in chapter 8. The K-3 file provides data at the child level and contains one record for each of the 18,174 children who participated, or whose parent participated, in at least one of the two kindergarten data collections. References to \"parents\" in this chapter include both parents and guardians. Each child record contains data from the various respondents associated with the child (i.e., the child herself or himself, a parent, one or more teachers, a school administrator and, if applicable, a nonparental care provider), weights and imputation flags, and administrative variables from the Field Management System (FMS), 1 for example, \"F7SCHZIP\" for the ZIP code of the school the child attended. The file includes cases with either child assessment data or parent interview data from at least one round of kindergarten data collection (fall 2010 or spring 2011). Among the 18,174 participants from kindergarten, the file includes fall 2011 data for those with a child assessment or parent interview in fall 2011, spring 2012 data for those with a child assessment or parent interview in spring 2012, fall 2012 data for those with a child assessment or parent interview in fall 2012, spring 2013 data for those with a child assessment or parent interview in spring 2013, and spring 2014 data for those with a child assessment or parent interview in spring 2014. The hearing evaluation data that were collected in the spring of third grade are not included in the kindergarten-third grade restricted-use file. Those data are provided in a separate restricted-use data file (NCES 2018-091) that has a separate user's manual focusing on the hearing evaluation component (NCES 2018-090). The raw data are provided in an ASCII data file named childK3.dat. To develop data files for statistical analyses, analysts should use the ECB software or the file record layout located in appendix B of the DVD. The ECB writes syntax files that must be run within a statistical software package to generate customized data files. Users should not access the ASCII data file directly, as any changes made to that file will alter the raw data obtained during data collection. 1 The Field Management System includes information collected about the study schools, school staff, and children from available administrative records or existing data sources (such as the Common Core of Data) or from conversations between data collection staff and school staff."}, {"section_title": "7-1", "text": "This chapter focuses primarily on the composite variables that were created from information obtained during the third-grade data collections. Most of the variables have been computed in the same way as those that were created using information collected in the base year (i.e., kindergarten), first grade, and second grade. However, a small number of the variables differ slightly either because the same exact information available in the earlier years of the study was not available in third grade or because it was determined that there was a better way to compute the composite after release of a previous data file. These differences are noted in the descriptions of the variables. To the extent feasible, the composite variables have also been computed in the same way as those created for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). This results in consistency between the two studies and facilitates comparisons between the two cohorts. However, some composites were created differently in the ECLS-K:2011 than in the ECLS-K. Documentation for both studies should be consulted before conducting cross-cohort analyses using composites. The user's manuals for kindergarten, first grade, and second grade should be consulted for detailed descriptions of the composite variables computed for rounds 1 through 6. As discussed in Appendix B, the public-use file is derived from the restricted-use file and is identical in format. However, masking techniques such as re-categorization and top-and bottom-coding have been applied to some data to make them suitable for public release. As a result of masking, some variables in the public-use file may not contain the exact same categories and values described in this chapter. Please see Appendix B for information on which variables are modified in the public-use file and see the public-use codebook for the exact categories and values provided in the public data. This chapter is divided into several sections. Sections 7.1 through 7.4 describe variable naming conventions, identification variables, missing values, and data flags. Section 7.5 provides details about the creation of composite variables, and section 7.6 focuses on the methodological variables. 7-2"}, {"section_title": "Variable Naming Conventions", "text": "Variables are named according to the data source (e.g., parent interview, teacher questionnaires about the teacher and child) and the data collection round to which they pertain. With the exception of the identification variables described in section 7.2, the first two or three characters of each variable (referred to as the variable prefix) include (1) a letter designating the source and (2) a number indicating the data collection round. For example, the number 7 is used for the data collection that took place in the spring of 2014. These variable naming conventions are used consistently in the data file. The prefixes used for third-grade variables in the kindergarten-third grade data file are listed in exhibit 7-1. Some variable names end with a suffix denoting a particular feature of the variable of which users should be aware. The suffix \"_R\" indicates that the variable has been updated or revised since its release in a prior data file. The suffix \"2\" is used for composites that are based on new questions or have new categories added relative to a prior round. The suffix \"_I\" indicates that missing data for the variable have been imputed or, in the case of a composite variable, that it is computed from imputed source variables. Imputation is discussed in sections 7.5.2.5 and 7.5.4.6. 7-3"}, {"section_title": "Identification Variables", "text": "The kindergarten through third-grade data file contains a child identification (ID) variable (CHILDID) that uniquely identifies each record. For children who have a twin who also participated in the study, TWIN_ID is the child identification number of the focal child's twin. The file also contains an ID for the parent (PARENTID). The parent ID number (PARENTID) is the same number as the child ID. Unlike in the ECLS-K, CHILDID is randomly generated, so it cannot be used to group children into classrooms or schools (that is, there is no commonality among IDs for children within the same school or classroom). The K-3 restricted-use data file does contain IDs for the child's general classroom teacher in each round, special education teacher (if applicable) in each round, school in each round, and beforeand after-school care provider in the kindergarten year (if the child was in before-or after-school care with one provider at least 5 hours per week). Users who wish to conduct hierarchical-level analyses with the school or classroom as additional levels can use these ID variables to group children within schools and classrooms. However, it should be noted that children change schools and classrooms over time, and this should be taken into account in any analysis of school or classroom effects. Additionally, as children change schools and classrooms over time, cluster sizes may become too small to support hierarchical analyses. The IDs available on the restricted-use file are listed in exhibit 7-2. Children's general classroom teachers in the 2013-14 school year are identified in the restricted-use file with the ID variable T7_ID, the spring 2014 teacher identification number. Study children who share the same general classroom teacher with other study children have the same value for the general classroom teacher ID. For children who had an Individualized Education Program (IEP) on record with the school that was identified as part of the process for determining accommodations for the child assessment, D7T_ID provides the identification number for their special education teacher or related service provider. For some students, the general classroom teacher was also the student's special education teacher. However, D7T_ID does not match T7_ID for these students. The ID variable S7_ID indicates the school the child attended at the time of the spring 2014 data collection. Each child has a school identification number for the two kindergarten data collections, the spring first-grade data collection, the spring second-grade data collection, and the spring third-grade data collection. Children selected for the fall subsamples also have school identification numbers for the fall 2011 and fall 2012 data collections. Not all identification numbers represent specific schools. Instead, certain identification numbers have been designated to identify children who were homeschoolers (9100), moved to a nonsampled county (9997), were unlocatable (9995), moved outside the United States (9993), were movers who were not subsampled to be followed into their new schools (9998), were deceased (9994), or whose parents asked for them to be removed from the study (9999). If a child did not have an IEP on record with the school that was identified as part of the process for determining accommodations for the child assessment, there is no special education teacher or related services provider associated with that child, and D7T_ID is missing. The D7T_ID would also be missing if the school records indicated that a child had an Individualized Family Service Plan (IFSP) when 7-5 he or she was younger, but did not have an IEP at the time of data collection. If a child had an IEP identified as part of the process for determining accommodations for the child assessment and, therefore, a special education teacher associated with him or her, there is an ID provided in D7T_ID whether or not the special education teacher responded to the spring 2014 special education teacher questionnaires. For both general classroom and special education teachers, there could be missing data for the child's teacher-level or child-level questionnaire (for example, if the general or special education teacher replied to only one of the two teacher questionnaires for the general or special education teacher, respectively, or did not fully complete the questionnaires) even though there is an assigned teacher ID. It is left to users to determine how they would like to set \"not applicable\" versus \"not ascertained\" codes when data for T7_ID or D7T_ID are missing. Note that if a teacher did not complete a teacher-level questionnaire, completed a child-level questionnaire for one child, and did not complete another child-level questionnaire for a child to whom the teacher was also linked, both children would have the same teacher identification number (e.g., T7_ID for the general classroom teacher or D7T_ID for the special education teacher). However, only the child for whom the teacher completed the child-level questionnaire would have data for those variables."}, {"section_title": "Missing Values", "text": "Variables on the ECLS-K:2011 data file use a standard scheme for identifying missing data. Missing value codes are used to indicate item nonresponse (when a question is not answered within an otherwise completed interview or questionnaire), legitimate skips (when a question was not asked or skipped because it did not pertain to the respondent), and unit nonresponse (when a respondent did not complete any portion of an interview or questionnaire) (see exhibit 7-3). Exhibit 7-3. Missing value codes used in the ECLS-K:2011 data file Not applicable, including legitimate skips -2 Data suppressed (public-use data file only) -4 Data suppressed due to administration error -5 Item not asked in School Administrator Questionnaire form B -7 Refused (a type of item nonresponse) -8 Don't know (a type of item nonresponse) -9 Not ascertained (a type of item nonresponse) (blank) System missing (unit nonresponse) SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K: 2011), kindergarten-third grade (K-3) data file."}, {"section_title": "7-6", "text": "The -1 (not applicable) code is used to indicate that a respondent did not answer a question due to skip instructions within the instrument. In the parent interview, \"not applicable\" is coded for questions that were not asked of the respondent because a previous answer made the question inapplicable to the particular respondent. For example, a question about a child's sibling's age is not asked when the respondent has indicated that the child has no siblings. For the teacher and school administrator selfadministered instruments, \"not applicable\" is coded for questions that the respondent left blank because the written directions instructed him or her to skip the question due to a certain response on a previous question that made the question inapplicable to the particular respondent. One example of the use of \"not applicable\" is found in the spring 2014 school administrator questionnaire version A (SAQ-A) questions F2 and F3ah. Question F1 asks whether the school received Title I funds for this school year. If the answer to question F1 is \"yes,\" the respondent is directed to continue to question F2 asking if the school was operating a Title I targeted assistance or schoolwide program, and then to F3a-h, which asks how the Title I funds are used. If the answer to question F1 is \"no,\" the respondent is supposed to skip to question F4 and questions F2 and F3a-h would be coded as -1 (not applicable). If questions F1, F2, and F3a-h are left blank by the respondent, and the respondent did not indicate that it is a private school (S7PRVSCH = 0), data for these questions are coded -9 (not ascertained), meaning the questions should have been answered but were not. If the respondent indicated that the school is private (S7PRVSCH = 1) and questions F1, F2, and F3a-h are left blank, data for these questions are coded -1 (not applicable) because they were supposed to be left blank given the school's designation as private. There are some exceptions to the standard use of -1 to indicate data are inapplicable for specific cases. For questions about the hours and minutes that the child spends watching television and playing video games, the questions about the number of minutes (P7TVMIN, P7VIDMIN) can be entered by interviewers as \"0\" or skipped if there are no minutes. If the questions about the number of minutes are skipped, they are coded -1 (not applicable). Another exception to the standard use of -1 is that for several round 7 variables (X7RTHETK3, X7MTHETK3, X7STHETK3), -1 is a valid value and should not be identified as missing data. Another use of -1 is in the teacher-level questionnaire about teacher background and the classroom. In this questionnaire, question B9 was \"How frequently do you or your students use computers or the following electronic devices for instructional purposes? Please include any desktop, laptop, or other computer-type devices.\" In addition to several discrete response categories for devices (a. Computer, b. LCD or DLP projector, c. Interactive whiteboard, etc.), a space was also provided for the respondent to enter text for \"h. Other electronic devices\" (A7OTHUSE). In many cases, a respondent did not indicate using any other type of electronic device (i.e., did not enter a text response for category h to specify another electronic device). In such cases, A7OTHUSE was set to -1 (not applicable) because providing a response for category h was essentially optional, and nonresponse (missing values) for category h could be expected. In order to protect the confidentiality of study participants, some data are suppressed in the public-use data file. The code -2 indicates the suppression of data for confidentiality. The suppression code -4 is used in rare instances in which there was a problem in the administration of an item that led to a high proportion of cases having missing or flawed data on the affected item, such that the data that were collected were not useful. For most variables that have a -4 assigned, the administration error did not typically affect all cases, but the -4 missing data code is assigned to all cases, including those not specifically affected by the error. An exception occurs with children's scores on the Dimensional Change Card Sort (DCCS) (Zelazo et al. 2013) for third grade. For these variables (C7DCCS1-30, C7GAME1-30, C7TARGRT1-30, X7DCCSSCR, X7CSDAC, X7CSNDAC, X7CSACC, X7CSNDRT), a code of -4 is only assigned to cases that were affected by a programming error that did not allow for sufficient practice with the DCCS task. These cases are flagged with X7DCCSFLG=7 and have all spring third-grade DCCS-related data set to -4. Information about a number of school characteristics that was collected in the SAQ-A (the school administrator questionnaire given to schools that were new to the study or had not previously completed an SAQ) was not collected in the SAQ-B (the school administrator questionnaire given to schools that had previously completed an SAQ). This data collection approach reduced respondent burden by eliminating questions about school characteristics that were unlikely to change from year-to-year, such as public/private control and the grade levels taught at the school. The code -5 is a special \"not applicable\" code indicating that a child does not have a value for the given school characteristic variable because it was not included in the abbreviated SAQ-B. The -7 (refused) code indicates that the respondent specifically told the interviewer that he or she would not answer the question. This, along with the -8 (don't know) code and the -9 (not ascertained) code, indicate item nonresponse. The -7 (refused) code is not used in the school or teacher data. The -8 (don't know) code indicates that the respondent specifically told the interviewer that he or she did not know the answer to the question. The -8 (don't know) code is not used in the school or teacher data. For questions where \"don't know\" is one of the options explicitly provided, a -8 is not coded for those who choose this option; instead the \"don't know\" response is coded as indicated in the value label information for the variable associated with that question."}, {"section_title": "7-8", "text": "The -9 (not ascertained) code indicates that the respondent left a question blank that he or she should have answered (or for which it is uncertain whether the item should have been answered or legitimately skipped because the respondent also left a preceding item blank). However, if a gate question 2 was left blank, but valid responses are provided to follow-up questions, the valid responses are included in the data file. For example, in the spring 2014 school administrator questionnaire version A (SAQ-A), question E1 asks, \"Do any of the children in this school come from a home where a language other than English is spoken?\" If the school administrator left E1 blank (i.e., unanswered), but then provided a valid response for question E2 which asks, \"What percentage of children in this school and in third grade are English language learners (ELL)?,\" E1 is coded -9 and the information from E2 is included in the data file as reported. If a gate question and its follow-up questions were left blank, all of the questions (gate and follow-up) are coded as -9 (not ascertained). For data that are not collected using the self-administered questionnaires (e.g., direct assessment scores), a -9 means that a value was not ascertained or could not be calculated due to nonresponse. The -9 (not ascertained) code is also used in the parent interview data when the interview ended before all applicable questions were asked. In these cases, the code of -9 is used for all variables associated with interview questions that came after the point at which the parent ended the interview. One exception to this coding scheme is the pointer variables. 3 Pointer variables are not set to -9 when the interview ended before all applicable questions were asked; instead they are set to the value corresponding to the household's parent figure(s). The -9 code is also used in the parent interview for questions that were edited 4 or inadvertently skipped in computer-assisted interviewing (CAI) programming. After editing, for complete interviews, the data for all questions that should have been asked but were not are coded as -9 (not ascertained), while the data for other skipped questions are coded as -1 (not applicable); codes -7 and -8 are used only when respondents stated a response of \"refused\" or \"don't know,\" and not as a result of editing or inadvertently skipping a question as a result of CAI programming. Missing values (-1, -7, -8, or -9) in questions that allow for more than one response are coded the same for all coding categories used for the question. For example, in the spring 2014 parent interview, if the question about subjects in which the child was tutored (HEQ290) has the answer of -8 (don't know), 2 A gate question is the first question in a series with skips to one or more follow-up questions. 3 Pointer variables indicate the household roster number of a person in the household who was the subject of questions about a parent figure. 4 Edits to household composition data that result in the addition or deletion of a parent or parent figure in the child's household sometimes result in -9 (not ascertained) codes being used for variables in multiple sections of the parent interview that have questions that are asked depending on the presence of specific parents or parent figures. The affected sections in the spring 2014 parent interview include FSQ (Family Structure), PLQ (Primary Language(s) Spoken), DWQ (Discipline, Warmth, and Emotional Supportiveness), NRQ (Nonresident Parents), COQ (Country of Origin for Nonresident Biological Parents), PPQ (Parent's Psychological Well-Being and Health), PEQ (Parent Education and Human Capital), and EMQ (Parent Employment). The -9 (not ascertained) code is used for both questions that are asked about specific parent/parent figures as well as those that are based on skips from those questions."}, {"section_title": "7-9", "text": "then all the subject variables associated with that question (e.g., reading, mathematics, science, and any categories that were added based on \"other, specify\" upcoding) are also coded as -8 (don't know). The \"system missing\" code appears as a blank when viewing codebook frequencies and in the ASCII data file. System missing codes (blanks) indicate that data for an entire instrument or assessment are missing due to unit nonresponse. For example, when a child's parent does not participate in the parent interview, all of the data associated with questions from the parent interview are coded \"system missing\" (blank) for that child. These blanks may be converted to another value when the data are extracted into specific processing packages. For instance, SAS converts these blanks into periods (\".\") for numeric variables. Codes used to identify missing values (-1, -7, -8, -9, or system missing) are not all identified as missing values by default in the data analysis software. Users will need to define these as missing values in the software they are using to analyze the data. Depending on the research question being addressed, in some instances users may want to assign a valid value to cases with missing values. For example, a teacher who reported that he or she did not have any English language learners in his or her classroom in the spring 2014 teacher-level questionnaire (Q A11) skipped the next question (Q A12) asking how many English language learners were in his or her classroom. An analyst interested in knowing the average number of English language learners in the classrooms of children in the ECLS-K:2011 may want to recode a value of -1 (not applicable) on the variable associated with Q A12 to a value of 0 (thereby indicating no English language learners in the classroom) in those instances where a teacher indicated in Q A11 that there were no English language learners in the classroom. It is advised that users crosstabulate all gate questions and follow-up questions before proceeding with any recodes or use of the data. Additionally, data users are encouraged to closely examine the distribution of their data and value labels to determine if values that appear to be missing value codes are valid data prior to any recoding. Composite variables may be derived using data from one or more instrument(s) in one round of data collection, from instrument data across multiple rounds, or from both instrument data and data from administrative records in one or more rounds. If a particular composite is inapplicable for a certain case, for example, as school composite variables are for children who are homeschooled, the variable is given a value of -1 (not applicable) for that case. In instances where a variable is applicable but complete information required to construct the composite is not available, the composite is given a value of -9 (not ascertained). The -7 (refused) code is not used for any of the composites except for the height and weight composites. The -8 (don't know) code is not used for any of the composites."}, {"section_title": "7-10", "text": "There is variation in the use of system missing for composite variables. Some child demographic variables (date of birth, sex, and race/ethnicity) are considered applicable to all 18,174 children who participated in the base year and are not assigned a value of system missing for any case. For composite variables using data from both a survey instrument and other administrative or school data sources, only nonparticipants in a given round of data collection are assigned values of system missing. For composite variables using data from only one instrument, (e.g., X7PAR1AGE, parent 1's age, is derived from the spring 2014 parent interview), a value of system missing is assigned if the instrument on which they are based was not completed; if the instrument was completed and an item used in the composite derivation was missing, the composite is assigned a value of -9 as described above."}, {"section_title": "Data Flags", "text": ""}, {"section_title": "Child Assessment Flags (X7RDGFLG, X7MTHFLG, X7SCIFLG, X7NRFLG, X7NRGEST, X7DCCSFLG, X7HGTFLG, X7WGTFLG, X7ASMTST, X7EXDIS)", "text": "There are many flags on the data file that indicate the presence or absence of child assessment data. X7RDGFLG denotes whether a child had scoreable reading assessment data in spring 2014, X7MTHFLG denotes whether a child had scoreable mathematics assessment data in spring 2014, X7SCIFLG denotes whether a child had scoreable science assessment data in spring 2014. 5 If a child answered fewer than 10 questions in any direct cognitive assessment domain (reading, mathematics, or science), the assessment was not considered scoreable. Only items actually attempted by the child counted toward the scoreability threshold. 6 A flag value of 1 indicates that the child responded to 10 or more questions in the assessment for that domain, and thus has the associated scores. A flag value of 0 indicates the child had fewer than 10 responses and does not have a score. X7NRFLG indicates the presence of Numbers Reversed scores and X7DCCSFLG indicates the presence of Dimensional Change Card Sort (DCCS) scores. X7HGTFLG and X7WGTFLG indicate the presence of data for height and weight in spring 2014, respectively. 5 For earlier rounds of data collection, these reading and mathematics flags took into account both the English and Spanish administrations of the assessments. (The science assessment was administered only in English.) In the fall 2012 and spring 2013 data collections, and the spring 2014 data collection, all children received the reading and mathematics assessments in English, so no language of administration is specified here. For more information on the language of administration, see section 2.1.1. 6 See chapter 3 for a complete discussion of assessment scoreability."}, {"section_title": "7-11", "text": "For the Numbers Reversed and DCCS assessments, as long as the child started the assessment task and answered at least one test question beyond the practice items, a W-ability score (for Numbers Reversed) or a computed overall score (for DCCS) was computed. Flags for each of the scores are coded 1 if the child has a W-ability score (for Numbers Reversed) or a computed overall score (for DCCS), coded 0 if the child participated in the child assessment but does not have a score, and set to system missing if the child did not participate in the child assessment. The DCCS flag has an additional code of -4, which is used for cases affected by a programming error. This error is described in more detail in section 3.2.1.1. The Numbers Reversed grade-normed scores are calculated using information about how far into the school year the assessment occurred. For some children the school year start and end dates are unavailable, so an estimate based on the mean of available data is used instead (information about the calculation of these grade-normed scores can be found in section 3.2.2). The data file includes a flag that indicates whether the assessment point was estimated for the Numbers Reversed grade-normed scores (X7NRGEST). This flag is set to 0 when actual school start and end dates are known, and set to 1 when the assessment point was estimated. The child's assessment status for the spring of 2014 is indicated by the composite X7ASMTST. The valid values include 1 for children who have any assessment data in the data file, 7 2 for those children who were excluded due to disability (and, therefore, do not have assessment data in the data file), and 3 for children who do not have assessment data in the data file and were not excluded due to disability. Note that those excluded due to disability (code 2) are considered to be participants in the data collection round even if they do not have any parent interview data either. In addition, there is a composite variable that uses FMS data to indicate whether the child was excluded from the assessment due to a disability: X7EXDIS. Study team leaders obtained information from school staff in the fall of 2013 and spring of 2014 about whether a child had an IEP on file and if any information in a child's IEP indicated that he or she would need Braille, large print, or sign language. It was also determined whether the IEP specifically prohibited the child from participating in standardized assessments such as those conducted in the ECLS-K:2011. If so, the child was not assessed, and XnEXDIS was coded 1 (child was excluded from the assessment due to a disability). Otherwise, XnEXDIS was coded 0 (child was not excluded from the assessment due to a disability). Students could have been excluded from taking the assessment for other reasons (e.g., lack of parental consent); these children are also coded 0 on XnEXDIS. The number of cases with system missing values varies across the seven XnEXDIS variables, 7 Having child assessment data includes (1) having reading and/or mathematics and/or science scores, (2) having at least one executive function score, or (3) having a height or weight measurement."}, {"section_title": "7-12", "text": "due to the sample for each round. The cases that are system missing on X1EXDIS are cases that were added to the sample in the spring of the base year and thus were not members of the sample in round 1. The cases that are system missing on X3EXDIS and X5EXDIS are those that were not selected for the fall subsample. There are no cases coded system missing on these variables in rounds 2, 4, 6, and 7."}, {"section_title": "Parent Data Flags (X7PARDAT, X7EDIT, X7BRKFNL)", "text": "There is one flag that indicates the presence of parent interview data. X7PARDAT is coded as 1 if there was a fully completed or partially completed interview in spring 2014. A partially completed interview in spring 2014 was one that ended before all applicable questions were answered, but that had answers to questions through FSQ200 (variable P7CURMAR) in section FSQ (family structure). The flag X7EDIT indicates whether, for a given case, household matrix data were reviewed or edited. It is coded as 1 if a parent interview household matrix was edited (e.g., if the age of a household member was reported incorrectly and had to be updated, or a person who was added to the household in error needed to be deleted from the household) or reviewed for editing even if no data were changed (e.g., if there were data that suggested a possible problem, but after examining the case the data were left as they were reported). This flag is included to make users aware that data cleaning or review of household matrix data was necessary for a particular case. If something about the household composition or characteristics of the household members seems unusual (e.g., the child is identified as having a 34-year-old brother in the household) and this flag is set to 1, this is an indication that the unusual data were reviewed and either edited to appear as they do in the data file or left as is because it was confirmed the data were accurate or there was no additional information indicating how the data could be edited accurately. When the flag is set to 1 and data (e.g., for the ages or relationships of household members) are corrected, the data are only changed in the variables for the round of the study to which the data flag pertains; no corrections are made to the data for the prior rounds to reflect the later corrections. Researchers who are using data about household composition from the parent interview household roster in their analyses should examine all rounds of household roster data closely, recognizing that for a limited number of cases corrected information from later rounds may need to be applied to earlier rounds. Before applying changes to earlierround data, researchers should ensure that they are making changes for the correct household member(s). It should also be ensured that any changes noted in the relationship variables are related to the correction of errors and not to real changes in the relationship of household members to the study child."}, {"section_title": "7-13", "text": "The composite variable X7BRKFNL indicates a final breakoff from the round 7 parent interview. A final breakoff occurs when a respondent stops in the middle of the interview before answering all applicable questions. These composites identify the variable associated with the last question answered by the parent. The breakoff point is provided only for those parent interviews with a status of partially complete. Cases for which a parent completed the interview have a value of -1, indicating that the case was not a breakoff."}, {"section_title": "Teacher Flags (X7TQCDAT, X7TQSUBDAT, X7TQTDAT, X7SETQA, X7SETQC)", "text": "Data were collected from teachers using three different questionnaires, two teacherclassroom-level questionnaires and a one child-level questionnaire. Teacher and classroom data were collected in two spring 2014 teacher-level questionnaires. A teacher-level questionnaire about teacher background and the classroom was completed by the child's primary teacher and includes information about the teacher (e.g., year of birth, ethnicity, race, education) and topics such as classroom characteristics, parent involvement, homework assignments, and criteria used to evaluate children's progress. A subject-specific teacher-level questionnaire was completed by the child's primary teacher, or by teachers for a specific subject if the subject was not taught by the primary teacher, and has questions about the instructional curriculum and time spent teaching reading and language arts, mathematics, science, and social studies. These questions were included in a separate subject-specific teacher-level questionnaire so that it would be easy for the primary teacher to share with other teachers if the primary teacher did not teach one or more of those subjects. Several variables that were derived from questions included in the teacher-level questionnaire about teacher background and the classroom administered in spring 2013 (e.g., A6CENMSG, A6MLT2NM, A6SENSOBS, A6STTMTR, A6HISTORY) have also been derived from the same questions included in the subject-specific teacher-level questionnaire administered in spring 2014. For spring 2014, these variables have the same name as in spring 2013, but have the prefix O7 (e.g., O7CENMSG) for data collected from the spring 2014 subject-specific teacher-level questionnaire. The teacher-level questionnaires were completed by a teacher linked to at least one ECLS-K:2011 child, and the data from the teacher-level questionnaires have been linked to every ECLS-K:2011 child in the teacher's class. Data which pertain to an individual study child, were collected from the teacher in the child-level questionnaire. Teachers were asked to complete a child-level questionnaire for each sampled child in their class in the spring of 2014."}, {"section_title": "7-14", "text": "The data file contains flag variables that can be used to determine whether data were obtained from a teacher. 8 There are separate flag variables corresponding to each of the teacher questionnaires (teacher-level and child-level) given to the specific teacher in the spring data collections (X7TQTDAT for the first teacher-level questionnaire described above, X7TQSUBDAT for the second teacher-level questionnaire described above, and X7TQCDAT for the child-level questionnaire). Two flags indicate the presence of data from each of the two special education teacher questionnaires for spring 2014 (X7SETQA for the teacher-level questionnaire and X7SETQC for the childlevel questionnaire). Cases linked to a special education teacher who did not complete a questionnaire and cases that were not linked to a special education teacher have a value of 0 on these flags. Users interested in information about whether special education teacher questionnaires were requested, regardless of whether special education questionnaires were completed in the spring of 2014, can use the composite variable X7SPECS, which is based on information from the FMS rather than the special education questionnaires. X7SPECS can be used with the flags for the presence of data for special education teacher questionnaires, X7SETQA and X7SETQC, to indicate whether special education questionnaires were requested and received. For example, if X7SETQA=0 and X7SPECS=1, this indicates that the case was linked to a special education teacher who did not complete a teacher-level special education questionnaire, but special education questionnaires were requested. If X7SETQA= 0 and X7SPECS=2, this indicates that the case was not linked to a special education teacher and special education questionnaires were not requested. X7SPECS is described further below in section 7.5.1.10."}, {"section_title": "School Administrator Data Flag (X7INSAQ)", "text": "There is a flag for the school administrator questionnaire (X7INSAQ) that is coded 1 if there are data from either version of the spring 2014 school administrator questionnaire (SAQ) and 0 if there are no data from the SAQ. 8 An identification number is provided in the teacher ID variable T6_ID as long as a child was linked to a general classroom teacher, even if the teacher did not complete any questionnaires. 7-15 7.4.5"}, {"section_title": "Destination School Flag (X7DEST)", "text": "The variable X7DEST is nonmissing for respondents in the spring round and is coded 1 if the child attended a school that became a destination school in the spring of 2014, and 0 otherwise. Destination schools are schools for which it was determined that at least four ECLS-K:2011 children moved into them during the same round of the study and from the same original school at which they were sampled for the study. This typically happened when children attended a school that ended with a particular grade (e.g., a school that only provided education through first grade) or a school that closed. Destination schools may be new to the ECLS-K:2011 or may have participated in a past round. A school already participating in the study could be designated a destination school if four children from the same original school move into that school. The composite X7DEST identifies schools that became destination schools in round 7. Users can identify schools that were ever designated as destination schools by looking at whether any of the X*DEST composites = 1."}, {"section_title": "Composite Variables", "text": "To facilitate analysis of the survey data, composite variables were derived and included in the data file. This section identifies the source variables and provides other details for the composite variables. Most composite variables were created using two or more variables that are also available in the data file, each of which is named in the text that explains the composite variable. Other composites, for example, X_CHSEX_R, were created using data from the Field Management System (FMS) and the sampling frame, which are not available in the data file. Note that some of these variables have been updated or revised since their release on the base-year data file. Such variables have an \"_R\" suffix in their name."}, {"section_title": "Child Composite Variables", "text": "There are many child-level composite variables in the child catalog. The nonassessment variables are described in further detail here. The child-level composites for the direct and indirect child assessment are described in chapter 3. 7-16 7.5.1.1"}, {"section_title": "Child's Date of Birth (X_DOBYY_R and X_DOBMM_R)", "text": "The composite variables for the child's date of birth are based on data from previous rounds of the study and are the same as the date of birth variables released in the K-2 longitudinal data file (X_DOBMM_R, X_DOBDD_R, 9 and X_DOBYY_R). The child's date of birth was not collected in the spring 2014 interview. Information about child's date of birth was collected from schools at the time of sampling and stored in the FMS, collected from parents in the fall kindergarten parent interview, and then collected or confirmed by parents in the spring kindergarten parent interview (parents confirmed the parent report from the fall or FMS data if the fall parent report was not obtained). Questions to collect date of birth information were only asked in the fall 2011, spring 2012, fall 2012, or spring 2013 interviews if data from the parent interview about the child's date of birth were missing due to unit or item nonresponse. In these rounds of the study, the parent was only asked child's date of birth if the parent had not confirmed FMSreported data (or had not reported date of birth if there were no FMS data) in a prior interview. In creating the composite, data from the most recent parent interview were given priority over data from other rounds because they were collected in the most recent interview and any data that were missing from the parent interview due to unit or item nonresponse had the potential to be updated in a subsequent data collection. month and is adjusted for leap years. The child assessment date was examined to ensure it was within the field period. If the assessment date fell outside the field period, the modal assessment date for the child's school was used to set the composite and was retained for the data file. 11 9 X_DOBDD_R indicates the child's exact day of birth. This is an administrative variable that is not included in the K-3 longitudinal data file for issues related to confidentiality. 10 X_DOBDD_R indicates the child's exact day of birth. This is an administrative variable that is not included in the K-3 longitudinal data file for reasons related to confidentiality. 11 Some assessments that were partially but not entirely completed during the field period were assigned a final status after the end of the data collection round. Thus, assessment dates after the end of the field period reflect the timing of the assignment of the final disposition, not the actual date of assessment. These cases were adjusted so that the assessment date reflects the modal date for the school."}, {"section_title": "7-17", "text": "Variables indicating the date of assessment (day, month, and year) in round 7 are also included in the kindergarten through third grade data file. The variable for the day of assessment (X7ASMTDD) provides a range of days in a month that the child was assessed and is coded 1 (days 1 through 7); 2 (days 8 through 15); 3 (days 16 through 22); 4 (day 23 or later); or -9 (not ascertained). The exact day of the month is not provided for reasons related to confidentiality. The variable for the month of assessment (X7ASMTMM) indicates the month that the child was assessed, and the variable for the year of assessment (X7ASMTYY) indicates the year that the child was assessed."}, {"section_title": "Child's Sex (X_CHSEX_R)", "text": "The composite variable for the child's sex is based on data from previous rounds of the study and is the same as the variables for the child's sex that were released in the K-2 longitudinal data file (X_CHSEX_R). The child's sex was not collected in the spring 2014 interview. Information about child's sex was collected from schools at the time of sampling and stored in the FMS, collected from parents in the fall kindergarten parent interview, and then collected or confirmed by parents in the spring kindergarten parent interview (parents confirmed the parent report from the fall or FMS data if the fall parent report was not obtained). Questions to collect information on the child's sex were only asked in the fall 2011, spring 2012, fall 2012, or spring 2013 interviews if data from the parent interview about the child's sex were missing due to unit or item nonresponse. In these rounds of the study, the parent was only asked the child's sex if the parent had not confirmed FMS reported data (or had not reported the child's sex if there were no FMS data) in a prior interview. In creating the composite, data from the most recent parent interview were given priority over data from other rounds because they were collected in the most recent interview and any data that were missing from the parent interview due to unit or item nonresponse had the potential to be updated in a subsequent data collection."}, {"section_title": "Race/Ethnicity (X_AMINAN_R, X_ASIAN_R, X_HAWPI_R, X_BLACK_R, X_WHITE_R, X_HISP_R, X_MULTR_R, X_RACETHP_R, X_RACETH_R)", "text": "There are three types of composite variables indicating child's race/ethnicity in the ECLS-K:2011 file: (1) dichotomous variables for each race/ethnicity category (X_AMINAN_R, X_ASIAN_R, X_HAWPI_R, X_BLACK_R, X_WHITE_R, X_HISP_R, X_MULTR_R) derived from data collected in the parent interview; (2) a single race/ethnicity composite derived from data collected in the parent 7-18 interview (X_RACETHP_R); and (3) a race/ethnicity composite that draws from either the parent-reported data about the child's race or the FMS (X_RACETH_R), with FMS data used only if parent responses about the child's race were missing. Parent interview responses about the races of the child's biological parents were not used in the creation of child race composite variables. Race/ethnicity information was updated in these composite variables for about 30 to about 100 cases based on information collected from parents in the spring 2014 parent interviews. Parents were asked about the child's ethnicity in the spring of 2014 if ethnicity in the parent interview items for the child were missing due to unit or item nonresponse. Specifically, parents were asked whether or not their child was Hispanic or Latino. Parents were also asked about the child's race in spring 2014 only if parent interview race data for the child were missing. Parents were asked to indicate to which of five race categories (White, Black or African American, Asian, Native Hawaiian or other Pacific Islander, American Indian or Alaska Native) their child belonged, and they were allowed to indicate more than one. From these responses, a series of five dichotomous race variables were created that indicate separately whether the child belonged to each of the five specified race groups. In addition, one additional dichotomous variable was created to identify those who had indicated that their child belonged to more than one race category. 12 The seven dichotomous ethnicity and race variables (X_HISP_R, X_AMINAN_R, X_ASIAN_R, X_HAWPI_R, X_BLACK_R, X_WHITE_R, X_MULTR_R) were created using parent data from spring 2014, or if those data were not asked in spring 2014 because they were asked in a previous round of the study, the dichotomous composites were set to the values of the second-grade dichotomous race composites that used parent data from the second grade, first grade, and base year (X_HISP_R, X_AMINAN_R, X_ASIAN_R, X_HAWPI_R, X_BLACK_R, X_WHITE_R, X_MULTR_R). Otherwise, the dichotomous ethnicity and race composites were set to -9 (not ascertained). Using the six dichotomous race variables and the Hispanic ethnicity variable, the race/ethnicity composite variables for the child (X_RACETHP_R, X_RACETH_R) were created. The categories for these variables are: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian, non-Hispanic; Native Hawaiian or other Pacific Islander, non-Hispanic; American Indian or Alaska Native, non-Hispanic; and more than one race specified, non-Hispanic. A child is classified as Hispanic if a parent indicated the child's ethnicity was Hispanic or Latino regardless of whether a race was identified and what that race was. If a child is not 12 Unlike the ECLS-K, in the ECLS-K:2011 \"other\" was not a permitted response for the race question."}, {"section_title": "7-19", "text": "reported to be Hispanic or Latino, the race/ethnicity categories (White, non-Hispanic; Black or African-American, non-Hispanic; Asian, non-Hispanic; Native Hawaiian or Other Pacific Islander, non-Hispanic; and American Indian or Alaska Native, non-Hispanic; More than one Race, non-Hispanic) are coded according to the child's reported race. If the report about whether the child was Hispanic or Latino was -7 (refused) or -8 (don't know), or if the child is not Hispanic/Latino and parent reported race is missing, X_RACETHP_R is coded -9 (not ascertained); if the report about whether the child was Hispanic or Latino is also missing from the FMS, or if the child is not Hispanic/Latino and race is also missing from the FMS, X_RACETH_R is coded -9 (not ascertained). The difference between X_RACETHP_R and X_RACETH_R is that if race or ethnicity data are missing from the spring 2014 parent interview, X_RACETH_R is set to the value used for the second-grade composite, also called X_RACETH_R, which uses both parent data and FMS data, while only parent-report data were used for the variable X_RACETHP_R. Thus, there are more missing data for X_RACETHP_R than for X_RACETH_R. About 100 cases have a value for X_RACETHP_R that is different in the K-3 longitudinal file than in the K-2 longitudinal file due to the collection of child race/ethnicity data in the spring 2014 parent interview. About 30 of these cases changed value from -9 (not ascertained) to a valid value and about 70 cases changed from code 4, Hispanic-no race reported, to code 3, Hispanic-race reported. About 85 cases have a changed value for X_RACETH_R due to the collection of child race/ethnicity data in the spring 2014 parent interview. Most of these cases, about 80, changed from code 4, Hispanic-no race reported, to code 3, Hispanic-race reported. The categories for X_RACETHP_R and X_RACETH_R are mutually exclusive, meaning that a child is coded as just one race/ethnicity. Users interested in the specific races of children who are identified as multiracial, or who are interested in identifying the race(s) of children who are identified as Hispanic, should use the dichotomous race variables discussed above."}, {"section_title": "Child's Height (X7HEIGHT)", "text": "To obtain accurate measurements, each child's height was measured twice in each data collection round. The height measurements were entered into the computer program used for the assessment, with a lower limit set at 35 inches and an upper limit set at 78 inches."}, {"section_title": "7-20", "text": "For the height composites, if the two height measurements (C7HGT1 and C7HGT2 for spring 2014) were less than 2 inches apart, the average of the two height values was computed and used as the composite value. If the two spring measurements were 2 inches or more apart, for X7HEIGHT (the child's height in spring 2014), the measurement that was closest to 52.83 inches for boys and 52.41 inches for girls was used as the composite value. This is the 50th percentile height for children who were 9 years old (109.25 months for boys and 108.66 months for girls: the average age at assessment in spring 2014 using the composite X7AGE). The height averages come from the 2000 Centers for Disease Control and Prevention (CDC) Growth Charts (www.cdc.gov/growthcharts/html_charts/statage.htm). 13 The two height measurements were 2 or more inches apart for 6 cases for X7HEIGHT. If one value for height was missing, the other value was used for the composite. If both the first and second measurements of height were coded as -8 (don't know), then the height composite was coded as -9 (not ascertained). Children who did not have their height measured due to a physical disability were coded as -8 (don't know) for both height measurements and, therefore, have a code of -9 on the composite. If both the first and second measurements of height were coded as -7 (refused), then the height composite was coded as -7 (refused). If both the first and second measurements of height were coded as -9 (not ascertained) because height data were missing as the result of a breakoff in the child assessment or the measurements had different missing values (e.g., one was -8 and the other was -9), then the height composite was coded as -9 (not ascertained). For 154 cases, the child's height in the spring of 2014 (X7HEIGHT) was shorter than in the spring of 2013 (X6HEIGHT). A difference of 1 inch or less (37 children) could be a function of things such as slouching versus standing upright or differences in shoes, hairstyle, thickness of socks, or a combination of these factors. However, 117 children were recorded as being more than 1 inch shorter in the spring of 2014 than in the spring of 2013, and 85 of those were recorded as being more than 2 inches shorter. These discrepancies may result from measurement error or recording error. Analysts should use their own judgment in how to use these cases in their analysis. 13 For calculating the median height, the composite X7AGE was used to determine children's average age at assessment. The average age at assessment in spring 2014 was 109.25 months for boys and 108.66 months for girls using the composite X7AGE. The closest value on the CDC Growth Chart was 109.5 for boys and 108.5 for girls. 7-21 7.5.1.6"}, {"section_title": "Child's Weight (X7WEIGHT)", "text": "To obtain accurate measurements, each child's weight was measured twice in each data collection round. The weight measurements were entered into the computer program used for the assessment, with a lower limit set at 20 pounds and an upper limit set at 250 pounds. Values outside the range that were documented in assessor comments as being valid measurements were included in the data file. For the weight composites, if the two weight measurements obtained within a round (i.e., C7WGT1 and C7WGT2 for spring 2014) were less than 5 pounds apart, the average of the two weight values was computed and used as the composite value. If the two measurements were 5 or more pounds apart, for X7WEIGHT the measurement that was closest to 63.81 pounds for boys or 64.25 pounds for girls was used as the composite value. These are the median weights for children who were 9 years old (109.25 months for boys and 108.66 months for girls: the average age at assessment in spring 2014 using the composite X7AGE). The weight averages come from the 2000 CDC Growth Charts (see www.cdc.gov/growthcharts/html_charts/wtage.htm). 14 The two weight measurements were 5 or more pounds apart in 4 cases for X7WEIGHT. If one value for weight was missing, the other value was used for the composite. If both the first and second measurements of weight were coded as -8 (don't know), the weight composite was coded as -9 (not ascertained). Children who did not have their weight measured due to a physical disability were coded as -8 (don't know) for both weight measurements and, therefore, have a code of -9 on the composite. If both the first and second measurement of weight in the child assessment were coded as -7 (refused), then the weight composite was coded as -7 (refused). If both the first and second measurements of weight in the child assessment were coded as -9 because weight data were missing as the result of a breakoff in the child assessment or the measurements had different missing values (e.g., one was -8 and the other was -9), then the weight composite was coded as -9 (not ascertained). There are approximately 40 children whose round 7 weights are more than 10 pounds lower than their round 6 weights; of these, about 10 change between 26 pounds and 76.6 pounds. It is possible that some of these changes result from measurement error. Analysts may wish to review such cases and determine how to account for these weight changes in their analysis."}, {"section_title": "7.5.1.7", "text": "Child's Body Mass Index (X7BMI) Composite body mass index (BMI) was calculated by multiplying the composite weight in pounds by 703.0696261393 and dividing by the square of the child's composite height in inches (Keys et al. 1972;Mei et al. 2002). Unrounded values of height and weight were used in the calculation of BMI. If either the height or weight composite was coded as -9 (not ascertained) or -7 (refused), the BMI composite was coded as not ascertained (-9). Values of \"don't know\" for height and weight were coded -9 (not ascertained) in the height and weight composites and also coded -9 (not ascertained) in the BMI composite."}, {"section_title": "Child's Disability Status (X7DISABL2, X7DISABL)", "text": "Two composite variables based on information obtained in the parent interview were created to indicate whether a child had a disability diagnosed by a professional. Note that these variables indicate either diagnosed disabilities that were identified for the first time in the round 7 parent interview or diagnoses reported in a previous interview for which the child also had a diagnosis reported in round 7. The variables must be used in conjunction with the disability composites from earlier rounds to identify the entire group of children who have ever had a disability diagnosed by a professional. Also, these two variables differ in how missing data were treated during their creation, as described below. Questions in the spring 2014 parent interview asked about the child's ability to be independent and take care of himself or herself, ability to pay attention and learn, coordination in moving arms and legs, overall activity level, overall behavior and ability to relate to adults and children, emotional or psychological difficulties, ability to communicate, difficulty in hearing and understanding speech, and eyesight. If parents indicated that their child had any issues or difficulties in response to these questions, follow-up questions asked whether the child had been evaluated by a professional for that particular issue and whether a diagnosis of a problem was obtained by a professional (CHQ120, CHQ125, CHQ215, CHQ245, CHQ246, CHQ300, CHQ301). A question was also asked about current receipt of therapy services or participation in a program for children with disabilities (CHQ340). The composite variable X7DISABL is coded 1 (yes) if the parent answered \"yes\" to at least one of the questions about diagnosis (indicating a diagnosis of a problem was obtained) or therapy services (indicating the child received services) (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) and the questions about the specific diagnoses (CHQ125, CHQ246, CHQ301) were not coded -7 (refused), -8 (don't 7-23 know), or -9 (not ascertained); or in the case of the vision diagnosis (CHQ301), the question was not coded as only nearsightedness (myopia), farsightedness (hyperopia), color blindness or deficiency, or astigmatism; or in the case of a hearing diagnosis (CHQ246), the question was not coded as only external ear canal ear wax. Using these criteria to calculate X7DISABL, a child could be coded as having a disability even if data for some of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) were missing. This is because a child is coded as not having a disability if there are data for at least one of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340), and the response was either 2 (no) or the item was -1 (inapplicable) (because the child did not have issues that indicated a question should be asked), even if data for some of these questions were missing. In addition to having \"no\" answers or \"inapplicable\" codes for the diagnoses or therapy services questions, if the child had a diagnosis, but the specific diagnosis was not reported (was refused, don't know, or not ascertained), X7DISABL was also coded 2 (no) because there was no reported disability. The composite was coded as missing only if all of the data for the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) were -7 (refused), -8 (don't know), or -9 (not ascertained), or if the items that skipped to these items were -7 (refused), -8 (don't know), or -9 (not ascertained). A more conservative approach when coding cases that had incomplete data for the diagnoses and services variables was used to derive the variable X7DISABL2. Whereas X7DISABL codes cases with missing data as \"no\" as long as all the information that was collected indicates the child does not have a diagnosed disability or receive services for a diagnosed disability, X7DISABL2 is coded -9 (not ascertained) when any of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) are -7 (refused), -8 (don't know), or -9 (not ascertained), or the items that skipped to these items are -7 (refused), -8 (don't know), or -9 (not ascertained). For X7DISABL2, if there are no \"yes\" answers for a disability, but any of the evaluation (CHQ115, CHQ210, CHQ235, CHQ290), diagnoses (CHQ120, CHQ215, CHQ245, CHQ300), or therapy questions (CHQ340) are -7 (refused), -8 (don't know), or -9 (not ascertained), 15 or if any of the evaluation, diagnosis, or therapy questions were not asked (were -1 for inapplicable) because of missing data for questions that skipped to those questions (and thus it is not known if they should have been asked), X7DISABL2 is coded -9 (not ascertained). In addition, if the parents indicated that a diagnosis had been obtained, but the specific diagnosis was coded as refused, don't know, 15 If CHQ340 was -9 (not ascertained) because the interview broke off after CHQ330, but all answers in CHQ330 and questions prior to CHQ330 indicated that CHQ340 would not have been applicable, X7DISABL2 and X2DISABL2 were coded 2 (no disability) because that question would not have been asked for those children."}, {"section_title": "7-24", "text": "or not ascertained, X7DISABL2 is coded as -9 (not ascertained). This approach is more conservative because it does not assume that the response for unanswered questions was \"no.\" Due to these differences in coding, the number of cases identified as not having a diagnosed disability is higher for X7DISABL than it is for X7DISABL2."}, {"section_title": "Student Grade Level (X7GRDLVL)", "text": "The X7GRDLVL composite indicates the child's grade level in the spring of 2014 as reported by the teacher or recorded in the FMS. This composite has valid values for the 13,579 cases that are respondents for round 7, that is the cases that have either child assessment or parent interview data. It is constructed using F7CLASS2 (child's grade in spring 2014 from the FMS). The values include 1 for first grade, 2 for second grade, 3 for third grade, and 4 for fourth grade. In all other cases the value is set to -9 for not ascertained. "}, {"section_title": "Child Linked to a Special Education Teacher (X7SPECS)", "text": "The composite variable X7SPECS indicates whether or not children were linked to a special education teacher and special education questionnaires were requested from teachers in the spring of 2014, based on the presence or absence of a link to a special education teacher or related service provider in the FMS. The value is 1 if special education questionnaires were requested and 2 if special education questionnaires were not requested. Study team leaders asked school staff if any accommodations were required for the study children to be assessed. During that discussion about assessment accommodations, team leaders were also supposed to record whether the child had an Individualized Education Program"}, {"section_title": "7-25", "text": "(IEP) on file with the school but did not require any accommodations for the study assessments. The link to a special education teacher was established automatically when information indicating a child needed an accommodation or had an IEP but did not require an accommodation was entered in the FMS by study team leaders. There are a few cases of a mismatch between X7SPECS and special education teacher reports about an IEP. In about 80 cases, there were FMS data indicating the child had an IEP on record at the school (and thus a special education teacher questionnaire was requested from the teacher and X7SPECS = 1), but the special education teacher indicated in the child-level questionnaire that the child did not have an IEP (E7RECSPE=2). During the spring 2014 parent interview, information on age, sex, and relationship to the study child was collected for all new household members. For certain existing household members, information was collected about whether their relationship to the study child had changed since the previous interview in which relationship data was collected. Change in relationship was asked for household members who were identified in a prior round interview as being a step-or foster mother or father, other male or female parent or guardian, boyfriend or girlfriend of the child's parent, relative, or nonrelative. Information about race and ethnicity were collected for specific household members who were new to the household and for specific previous household members with missing race or ethnicity data. Other key background"}, {"section_title": "Family and Household Composite Variables", "text": ""}, {"section_title": "7-26", "text": "information such as country of origin was also collected about parents in the spring of 2014 if it had not been collected in a prior round. The composite variables for parents (e.g., parent age, parent education) are for the parents who It should be noted that in spring 2014 there are not composite variables for parent occupation or employment. In spring 2014, questions about parent occupation were not asked and questions about parent employment were asked in a single question. Data for the single question for parent 1 and parent 2 (P7EMPSIT1_I, P7EMPSIT2_I) were imputed if they were missing using either longitudinal imputation, if appropriate, or hot deck imputation. 16"}, {"section_title": "Household Counts (X7HTOTAL, X7NUMSIB, X7LESS18, X7OVER18)", "text": "The composite variable X7HTOTAL provides a count of the total number of household members in the spring of 2014. For households for which household roster information had been collected in a prior round, this count is the number of household members who were previously rostered and reported to still be in the household plus any new persons added after the last interview in which roster information was collected. For a small number of households that did not participate in any of the prior parent interviews in which household composition information was collected (fall 2010, spring 2011, spring 2012, or spring 2013), X7HTOTAL is a count of the total number of persons identified by the respondent as household members in the spring 2014 parent interview. 16 Longitudinal imputation was conducted using the most recent composite variables for employment (X6PAR*EMP_I, X4PAR*EMP_I, or X1PAR1EMP). For example, if there were data available for spring 2013, if X6PAR*EMP_I = 2 (less than 35 hours per week), then P7EMPSIT*_I=1 (working part-time). If X6PAR*EMP_I = 1 (35 hours or more per week), then P7EMPSIT*_I=2 (working full-time). Otherwise, for parents who were not employed, the most recently collected value for P6DOW_*, P4DOW_*, or P1DOW_* was used as boundary variable in hot deck imputation to set the value of P7EMPSIT*_I=3 (a stay-at-home parent or guardian), and P7EMPSIT*_I=4 (not working)."}, {"section_title": "7-27", "text": "Two composite variables take the ages of the household members into account to indicate the total numbers of (1) adults and (2) children in the household in the spring of 2014. Information about household members' ages was collected in the household matrix, or roster, section of the parent interview. The composite X7NUMSIB indicates the total number of siblings (biological, step-, adoptive, or foster) living in the household with the study child. Siblings were identified by questions in the FSQ section of the parent interview that asked about the relationship of each household member to the study child. X7NUMSIB does not count children of the parent's boyfriend or girlfriend (identified by the code 5 in the variables associated with question FSQ180) as siblings."}, {"section_title": "Household Rosters", "text": "The ECLS-K:2011 data file includes rosters of the household members as collected in the parent interviews. The roster information appears as part of the block of Family Structure Questions (FSQ) for each round in which the FSQ section was included in the parent interview. Variable names begin with P1 for round 1 (fall kindergarten), P2 for round 2 (spring kindergarten), P4 for round 4 (spring 2012, when most children were in first grade), P6 for round 6 (spring 2013, when most children were in second grade), and P7 for round 7 (spring 2014, when most children were in third grade). No FSQ section was included in the brief round 3 or round 5 parent interviews. For each household member in each round, roster variables include the following, where * is the round number (1, 2, 4, 6 or 7) and # is the household roster number (1 through 25): If there is no parent interview completed in a given round, then the items for that round are assigned a value of system missing. Beginning in round 4, if a person has left the household (e.g., P4CUR_# = 2, not a current household member), the roster variables for that position are assigned a value of -1 for that round and subsequent rounds in which a parent interview is completed. In rare cases, only in rounds 4 and 6, there are roster positions for which all values are system missing or -1 across all rounds but P4CUR_# = 2 or P6CUR_# = 2 (not a current household member). This may occur because a new household member was the respondent for round 3 or 5, when there was no roster completion or confirmation in the parent interview, and that person had left the household before the next parent interview in which complete household composition information was collected. 18 Determining household membership in a given round. In round 1, respondents were not asked if persons were currently household members, because that was the first household enumeration for the study and all enumerated persons were household members at that time. For rounds 2, 4, 6, and 7, analysts can determine the current household membership at the time of the parent interview for the round by examining the variables P2CUR_#, P4CUR_#, P6CUR_#, and P7CUR_#, respectively. Analysts should not look for the first \"empty\" position in the roster series to determine the last person with roster data in the household, since, as noted above, all persons retain their household positions permanently; if person 3 leaves the household, then person 4 still remains in position 4. 18 Because there was not a household roster in the fall 2011 or fall 2012 parent interviews, there are potentially other household members who were present in the fall of 2011 or the fall of 2012 and had left the household by the time of the subsequent parent interviews. There would be no record of these household members in the study. 7-30"}, {"section_title": "7.5.2.3", "text": "Parent Identifiers and Type in the Household (X7IDP1, X7IDP2, X7HPAR1, X7HPAR2,"}, {"section_title": "X7HPARNT)", "text": "X7IDP1 and X7IDP2 indicate the positions in the household roster of the sampled child's residential parent/parent figure(s) in the spring of 2014. 19 The construction of parent identifiers and the household composition variables from the parent interview data was a multistep process. First, it was determined from household roster variables whether there was a mother (biological, adoptive, step-, or foster) and/or a father (biological, adoptive, step-, or foster) in the household. Using this information, the method described below was used to create X7IDP1 and X7IDP2 for the spring."}, {"section_title": "1.", "text": "If there was only one mother (of any type, including unknown type) and only one father (of any type, including unknown type) in the household, the mother was identified as parent 1 (X7IDP1) and the father was identified as parent 2 (X7IDP2).\nClose all applications on your computer."}, {"section_title": "2.", "text": "If there was only one mother (of any type, including unknown type) in the household and no other parent figure (of any type), the mother was identified as parent 1 and parent 2 is coded -1 (not applicable). If there was a mother and she had a male spouse/partner in the household who was not identified as a father (of any type, including unknown type), the spouse/partner was identified as parent 2.\nSelect the Settings menu and then the Control Panel folder icon.\nSelect the Control Panel tab.\nRun program \"InstallECLSECB.exe\"."}, {"section_title": "3.", "text": "If there was only one father (of any type, including unknown type) in the household and no other parent figure (of any type), the father was identified as parent 1 and parent 2 is coded -1 (not applicable). If there was a father and he had a female spouse/partner in the household who was not identified as a mother (of any type), the spouse/partner was identified as parent 1 and the father was identified as parent 2.\nIn the Control Panel window, click the Display icon.\nIn the Control Panel window, click the Display icon.\nThe screen shown in exhibit 8-1 indicates that the setup is being prepared. Exhibit 8-1. InstallShield Wizard"}, {"section_title": "4.", "text": "If there were two mothers (or a mother and female spouse/partner) in the household, an order of preference was used to identify one mother to be parent 1, with the order specified as biological, adoptive, step-, foster mother or female guardian, then other female parent or guardian. 20 The other mother was identified as parent 2. If there were two mothers of the same type (e.g., two adoptive mothers) or there were two mothers and the type for both was -7 (refused) or -8 (don't know), the mother with the lowest household roster number was identified as parent 1 and the other mother was identified as parent 2.\nSelect the Settings tab. 8-2\nSelect the Change display settings tab.\nYou will be prompted to continue with the installation in the Welcome window shown in exhibit 8-2. Click the Next button to continue."}, {"section_title": "5.", "text": "If there were two fathers in the household (or a father and male spouse/partner), an order of preference was used to identify one father to be parent 1, with the order specified as biological, adoptive, step-, foster father or male guardian, then other male parent or guardian. The other father was identified as parent 2. If there were two fathers of the same type (e.g., two adoptive fathers) or there were two fathers and the type for both was -7 19 In the ECLS-K, the parent identifiers were P*MOMID and P*DADID and specifically identified the mother/female guardian and father/male guardians, respectively, in the household. The format of the parent identifiers was changed in the ECLS-K:2011 to allow for more accurate identification of households with two mothers/female guardians or two fathers/male guardians. 20 There were new categories in the ECLS-K:2011 parent interview for \"Other female parent or guardian\" in FSQ140 and \"Other male parent or guardian\" in FSQ150 that were not included in the ECLS-K.\nSet the Desktop Area to 1024 x 768 pixels with the Desktop Area slidebar. If you have a Windows Vista or Windows 7 \u00ae operating system, you can check or set your desktop area as follows: 1. Click the Windows Start Button.\nSet the Desktop Area to 1024 x 768 pixels with the Desktop Area slidebar. As noted above, the ECB requires approximately 20 megabytes of available disk space on your hard drive. If 20 megabytes of space is not available, you may wish to delete unnecessary files from the drive to make space for the ECB."}, {"section_title": "7-31", "text": "(refused) or -8 (don't know), the father with the lowest household roster number was identified as parent 1 and the other father was identified as parent 2."}, {"section_title": "6.", "text": "If there was no one in the household identified as a mother or father, then a female respondent or the female spouse or partner of a male respondent was identified as parent 1. If the female parent figure had a male spouse or partner, the spouse/partner was identified as parent 2. If the respondent was male and had a female spouse or partner, she was designated as parent 1 and he was designated as parent 2. For example, if a child lived with his grandmother (the respondent) and grandfather, and neither his mother nor father lived in the household, then the grandmother was identified as parent 1 and the grandfather was identified as parent 2. If the grandfather lived in the household, but no grandmother or parents lived there, the grandfather respondent would be parent 1 and parent 2 would be coded -1. Demographic information such as age, race, and education was collected for these \"parent figures.\" Once parents/parent figures were identified, X7HPAR1 and X7HPAR2 were created to identify the specific relationship of parent 1 and parent 2 to the study child. 21 It should be noted, however, that for households in which the child lived with parent figures other than his or her mother and/or father, the parent figures identified in X7IDP1 and X7IDP2 were not defined as parents (meaning biological, step-, adoptive, or foster) for the construction of X7HPAR1 and X7HPAR2. For example, if there are a grandmother and grandfather and there are no parents listed in the household, X7HPAR1 and X7HPAR2 would be coded as category 15 (no resident parent). X7HPARNT indicates the type(s) of parents living in the household with the study child. The values for the X7HPARNT composite are as follows: When study children are living with parent figures (e.g., grandmother and grandfather), rather than biological, adoptive, step-, or foster parents, X7HPARNT is coded 4. The composite parent identifier variables X7IDP1 and X7IDP2 are used to determine which composite variables correspond to parent 1 and parent 2, respectively. These \"pointer\" variables indicate the household roster number of the person who was the subject of the questions being asked. All parent 21 These variables are a combination of P*HMOM and P*HDAD from the ECLS-K."}, {"section_title": "7-32", "text": "composite variables that include \"PAR\" and the number 1 in the variable name are associated with the person designated in X7IDP1, who is parent 1. All parent composite variables that include \"PAR\" and the number 2 in the variable name are associated with the person designated in X7IDP2, who is parent 2. In the third-grade parent interview, there are four sets of questions that were first asked about parent 1 and then asked about parent 2 if the household contained two parents."}, {"section_title": "7-33 \uf06e", "text": "The fourth set of questions about parent 1 and parent 2 asks about parent employment. There is also a set of \"pointer\" variables that hold the household roster number of the person who was the subject of the employment questions (P7EMPP1 and P7EMPP2). For the employment questions, the pointer variables are applicable to up to two parents in the household. If there are two parents in the household, P7EMPP1 and P7EMPP2 are the roster numbers of the first and second parent, respectively. If there is one parent in the household, P7EMPP1 is the roster number of the first parent and P7EMPP2 = -1 (not applicable). The value of employment pointer variables is the same as the value for the composite parent identifier variables. To illustrate how the pointer variables work, suppose there is a household with both a mother and a father who were listed as the third and fourth individuals in the household roster. According to the rules outlined above, household member #3, the mother, becomes parent 1 and X7IDP1 equals 3. All applicable pointer variables for parent 1 will subsequently take on the value 3. Similarly, household member #4, the father, becomes parent 2 and X7IDP2 equals 4. All applicable pointer variables for parent 2 will subsequently take on the value 4. Table 7-1 identifies the PLQ, PEQ, and EMQ section pointer variables included in the data file along with the interview items and variables associated with those pointer variables. The pointer variables are necessary to determine which parent should be assigned the answers to items about employment. Returning to the example above, the answers to the employment questions for the mother are stored in variables that end with the suffix \"1\" since the mother was identified as parent 1, and her household roster number is the value in X7IDP1. For example, P7EMPSIT1_I and P7EVRACTV1 indicate the mother's current employment situation and whether the mother has been on active duty in the military since the child was born, respectively. The answers to the employment questions for the father are stored in variables that end with the suffix \"2\" since the father was identified as parent 2, and his household roster number is the value in X7IDP2. For example, P7EMPSIT2_I and P7EVRACTV2 indicate the father's current employment situation and whether the father has been on active duty in the military since the child was born, respectively. "}, {"section_title": "7-34", "text": ""}, {"section_title": "Parent Demographic Variables (X7PAR1AGE, X7PAR2AGE, X7PAR1RAC, X7PAR2RAC)", "text": "X7PAR1AGE is a composite variable for the age of parent 1 from the household roster (the person whose roster number is indicated in X7IDP1) and X7PAR2AGE is the composite variable for the age of parent 2 from the household roster (the person whose roster number is indicated in X7IDP2). 22 The ages of all household members (other than the child) who had their ages collected in the fall of 2010 or spring of 2011 were automatically incremented by three years for the spring 2014 parent interview. Age was incremented by two years for household members who were living in the household in the spring of 2012 and had age information collected in that interview but who were not in the household in the fall of 2010 or the spring of 2011. Age was incremented by one year for household members who were living in the household in the spring of 2013 and had age information collected in that interview but who were not in the household in the fall of 2010, the spring of 2011, or the spring of 2012. For information about how the first and second parents were selected for these and other parent variables, see section 7.5.2.3 above. 22 These variables are a combination of P*HDAGE and P*HMAGE in the ECLS-K."}, {"section_title": "7-35", "text": "The composite variables for race/ethnicity for the parent/guardians were derived in the same way as those for the child, except that there are no variables that supplement parent-reported race/ethnicity with FMS data as was done for children. All data on parent race/ethnicity come from the parent interview. Race/ethnicity information collected for parents in the spring 2014 parent interview is provided in the data file in categorical race/ethnicity composites (X7PAR1RAC for parent 1, the person whose roster number is indicated in X7IDP1, and X7PAR2RAC for parent 2, the person whose roster number is indicated in X7IDP2). Race and ethnicity information was collected only once for each parent/guardian. If race and ethnicity information was collected in the fall of 2010, spring of 2011, spring of 2012, or spring of 2013, it was not collected again in the spring of 2014. The questions about race and ethnicity were only asked in the spring 2014 parent interview to collect this information for parents/guardians who were new to the household in that round or when this information was missing for parents/guardians who lived in the household at the time of the spring 2014 interview. Respondents were allowed to indicate that they, and the other parent figure when applicable, were Hispanic or Latino, and whether they belonged to one or more of the five race categories (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander). 23 From these responses, a person's race/ethnicity was classified into eight mutually exclusive categories. A person's race/ethnicity was classified as \"more than one race, non-Hispanic\" if more than one race was specified and the answer to the question about being Hispanic or Latino was 2 (no). A person's race/ethnicity was classified as \"Hispanic, race specified\" if the answer to the question about being Hispanic or Latino was 1 (yes) and at least one race was indicated in the question about race. If a person was Hispanic or Latino, but a race was not indicated, that person's race/ethnicity was classified as \"Hispanic, no race specified.\" The remaining race/ethnicity categories (White, non-Hispanic; Black or African-American, non-Hispanic; Asian, non-Hispanic; Native Hawaiian or Other Pacific Islander, non-Hispanic; and American Indian or Alaska Native, non-Hispanic) were coded according to the person's reported race when the person was not Hispanic or Latino. If the answer to the question about being Hispanic or Latino was -7, -8, or -9 (refused, don't know, or not ascertained respectively), or if the person was not Hispanic/Latino and the answer to the question about race was -7, -8, -9 (refused, don't know, or not ascertained, respectively), race/ethnicity was coded -9 (not ascertained). Parent race/ethnicity was obtained for all parents/guardians and spouses of respondent parents/guardians but may or may not have been collected for a parent's boyfriend or girlfriend. For 23 Unlike the ECLS-K, in the ECLS-K:2011 \"other\" was not a permitted response for the race question."}, {"section_title": "7-36", "text": "example, in a household with a birth mother and stepfather, the race/ethnicity of both parents was obtained. However, in a household with a birth mother and her boyfriend, the race/ethnicity of the mother was obtained but the race/ethnicity of the boyfriend was not unless he was the respondent. 24"}, {"section_title": "Household Income and Poverty (X7INCCAT_I, X7POVTY_I)", "text": "Household income data were collected in the spring 2014 parent interview. Parents were asked to report income by broad range ($25,000 or less or more than $25,000) and by detailed range as shown in table 7-2. 25 The composite X7INCCAT_I was created using the detailed income range information. X7INCCAT_I was set to the value of P7INCLOW_I (detailed income range for those who reported the broad income range in P7HILOW_I as $25,000 or less) or P7INCHIG (detailed income range for those who reported the broad income range in P6HILOW_I as more than $25,000). When data for the broad range variable (P7HILOW_I) or one of the detailed range variables (P7INCLOW_I, P7INCHIG_I) were missing (i.e., coded -7 (refused), -8 (don't know), or -9 (not ascertained)), income information was imputed.   26 24 In the spring of 2014, race/ethnicity information was collected for some persons who did not meet the criteria for having race and ethnicity questions asked in the spring of 2014 but did meet the criteria for having race and ethnicity collected in an earlier round of the study. Persons who have race and ethnicity on the file for spring 2014 include the study child, those with a relationship of mother/female guardian or father/male guardian in any round (P*REL_* = 1 or 2 or P*UNR = 3 or 4), those who were a respondent in any round (P*PER_* = 1), and persons who were spouse/partners of respondent parents in any round. 25 Starting at category 9 of the detailed income range, the categories for the income variable in the ECLS-K:2011 are different from those used in the ECLS-K. More narrow ranges of income were used at higher income levels in the ECLS-K:2011 in order to determine whether household income was near 200 percent of the federal poverty threshold given household size. If so, follow-up questions about exact income were asked. 26 No adjustment was made for inflation when household income was longitudinally imputed from a prior round. Where longitudinal imputation was not possible, missing values were imputed using the hot deck method in which similar respondents and nonrespondents are grouped or assigned to \"imputation cells,\" and a respondent's value is randomly \"donated\" to a nonrespondent within the same cell. Cells are defined by characteristics such as geographic region, school locale, school type, household type, age, race, education, and income. When information used to define the imputation cells was missing for any of these variables in third grade, information was used from second grade, first grade, or the base year, where available."}, {"section_title": "7-37", "text": "Imputation flag values for IFP7HILOW, IFP7INCLOW, and IFP7INCHIG identify cases for which longitudinal or hot deck imputation was conducted. There are no separate imputation flags for X7INCCAT_I and X7POVTY_I; imputation was done only in the source variables P7HILOW, P7INCLOW, and P7INCHIG, and is reflected in the imputation flags for those variables. "}, {"section_title": "7-38", "text": "Reported income was used to determine household poverty status in the spring of 2014, which is provided in variable X7POVTY_I. For some households, more detailed information about household income than the ranges described above was collected. Specifically, when parent respondents reported a detailed household income range suggesting the household income was close to or lower than 200 percent of the U.S. Census Bureau poverty threshold for a household of its size, the respondents were asked to report household income to the nearest $1,000 (referred to as exact income) in order to determine household poverty status more accurately. Table 7-4 shows the reported detailed income categories for households of a given size for which respondents were asked the exact income question. For example, a respondent in a household with two people would have been asked to provide an exact income if the respondent had indicated that the household income was less than or equal to $30,000. Table 7-4 also shows how the income categories compare to the value that is 200 percent of the weighted average 2013 poverty threshold. 27 The 2013 weighted poverty thresholds were used for the poverty composite because respondents in the spring of 2014 were asked about household income in the past year. At the time that the spring 2014 parent interview was finalized, the most updated poverty thresholds available were the weighted 2012 poverty thresholds. Poverty thresholds for 2013 were similar to the poverty thresholds for 2012. However, because of differences in one category, exact income should have been asked for one narrow range of incomes according to the 2013 thresholds, but it was not asked because the 2012 thresholds were used. Using the 2013 poverty thresholds rather than the 2012 poverty thresholds, cases with two household members and an income between $30,001 and $30,284 were not asked exact income when they should have been. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2014. 27 The CAPI program used to conduct the parent interview was programmed to only ask for exact income when parent respondents reported a detailed household income range suggesting the household income was close to or lower than 200 percent of the U.S. Census Bureau poverty threshold for a household of its size. Although the parent interview in which this information was collected was conducted in the spring of 2014, the 2012 poverty thresholds were used for instrument programming because they were the most recent thresholds available when programming was done. The question about exact income was asked for the following conditions: (NUMBER IN HH = 1 AND PAQ. "}, {"section_title": "7-39", "text": "When information about exact household income was available (P7TINCTH_I), it was used in conjunction with household size (X7HTOTAL) to calculate the poverty composite. When exact income was not available because the exact income question was not asked, the midpoint of the detailed income category (X7INCCAT_I) was used in conjunction with household size (X7HTOTAL). 28 Household poverty status in the spring of 2014 was determined by comparing total household income reported in the parent interview to the weighted 2013 poverty thresholds from the U.S. Census Bureau (shown in table 7-5), which vary by household size. Although the parent interview was conducted in the spring of 2014, the 2013 weighted poverty thresholds were used in the derivation of the poverty composite because respondents were asked about household income in the past year. Exact income (P7TINCTH_I) was asked in the parent interview or imputed for all persons in categories 1 and 2 of the poverty composite. Imputation of exact income was conducted according to thresholds in the parent interview. Households with an exact income that fell below the appropriate threshold were classified as category 1, \"below the poverty threshold,\" in the composite variable. Households with an exact income that was at or above the poverty threshold but below 200 percent of the poverty threshold were classified as category 2, \"at or above the poverty threshold, but below 200 percent of the poverty threshold,\" in the composite variable. Households with a total income (either exact or the income representing the midpoint of the detailed range reported by the composite) that was at or above 200 percent of the poverty threshold were classified as category 3, \"at or above 200 percent of the poverty threshold,\" in the composite variable. 29 For example, if a household contained two members and the household income was lower than $15,142, the household was considered to be below the poverty threshold and would have a value of 1 for the composite. If a household with two members had an income of $15,142 or more, but less than $30,284 (200 percent of the poverty threshold for a household of two), the composite would have a value of 2. If a household with two members had an income of $30,284 or more, the composite would have a value of 3. 28 Because exact income information was not collected from all parents, the ECLS-K:2011 provides an approximate but not exact measure of poverty. 29 In the ECLS-K:2011, there are three categories in the poverty composite rather than two categories for \"below poverty threshold\" and \"at or above poverty threshold\" as there were in the ECLS-K. The ECLS-K:2011 categories 2 and 3 can be combined to create a poverty composite variable comparable to the ECLS-K poverty composite variable. "}, {"section_title": "7-40", "text": ""}, {"section_title": "Creation of a Socioeconomic Status Variable", "text": "In the base-year of the study and the spring of 2012, a composite variable for socioeconomic status (SES) was created that combined occupation prestige scores, income, and education. The composite for socioeconomic status was not created in the spring of 2013 or 2014 because not all data for the composite were collected (in spring 2013, parents were not asked for education information; in spring 2014, parents were not asked for occupation information). Users who wish to create their own SES composite may take the spring 2014 education data for a case (X7PAR1ED_I and X7PAR2ED_I) and combine those data with spring 2014 household income (X7INCCAT_I) and spring 2013 variables for parent occupational prestige scores (X6PAR1SCR_I and X6PAR2SCR_I). The values of each SES component can then be normalized as z scores so that the component has a mean of 0 and a standard deviation of 1. In this normalization step,  , where m is the number of components. Note that for households with only one parent present and for parents who were retired or not currently in the labor force, not all the components would be defined. In these cases, the SES would be the average of the z scores of the available components."}, {"section_title": "Respondent ID and Relationship to Focal Child (X7RESID, X7RESREL2)", "text": "The respondent to the parent interview was a person identified as the household member who knew the most about the child's care, education, and health. X7RESID indicates the household roster number of the spring 2014 parent interview respondent. The relationship variables (P7REL_1-P7REL_25, P7MOM_1-P7MOM_25, P7DAD_1-P7DAD_25, and P7UNR_1-P7UNR_25) associated with the respondent's household roster number were used to code X7RESREL2. If the respondent was a biological mother or father, X7RESREL2 is coded as 1 (biological mother) or 4 (biological father), respectively. If the respondent was an adoptive, step-, or foster mother or father, or other female or male guardian, X7RESREL2 is coded as 2 (other mother type) or 5 (other father type), respectively. If the respondent was a mother or father but the type of mother (P7MOM_#) or father (P7DAD_#) was coded as -7 (refused), -8 (don't know), or -9 (not ascertained), X7RESREL2 is coded as 3 (mother of unknown type) or 6 (father of unknown type). 30 If the respondent was a grandparent, aunt, uncle, cousin, sibling, or other relative, X7RESREL2 is coded as 7 (nonparent relative). If the respondent was a girlfriend or boyfriend of the child's parent or guardian; a daughter or son of the child's parent's partner; other relative of the child's parent's partner; or another nonrelative, X7RESREL2 is coded as 8 (nonrelative). Otherwise, X7RESREL2 is coded as -9 (not ascertained). Because the interviewer initially asked to speak with the previous round respondent 30 Categories for mothers and fathers of unknown type were new for the spring 2012 composite. Mothers and fathers of unknown type were included in the categories \"other mother type\" and \"other father type\" for the fall 2010 and spring 2011 composites, X1RESREL and X2RESREL."}, {"section_title": "7-42", "text": "at the beginning of the spring 2014 parent interview, the respondent for previous interviews (X*RESID) was the same person for many cases."}, {"section_title": "Food Security Status", "text": "The food security status of the children's household was determined by responses to the 10 food security questions (P7WORRFD through P7NOTEA2) asked in section FDQ of the spring 2014 parent interview. The questions measured the households' experiences related to food insecurity and reduced food intake in the last 12 months. In spring 2011 and spring 2012, questions were asked about adults' experiences separately from the experiences of the children in the household. In spring 2014, to reduce respondent burden, a shorter 10-item version of this measure suggested by the United States Department of Agriculture (USDA) was used to measure adult food security. The adult food security measure can be used to predict child food security. The adult data were combined into scales using statistical methods based on the Rasch measurement model. The food security questions were developed by academic researchers using ethnographic and case-study methods with low-income women and families to identify natural language used to describe their situations and behaviors when they had difficulty obtaining enough food. The scales derived from the food security questions were validated using statistical methods based on item response theory and by comparing measured food security with other indicators of food adequacy. Composites were created that indicate the food security status of the adults (based on 10 household-and adult-referenced items X7FSADRA2 is the adult food security raw score, which is a simple count of the number of household-and adult referenced food security items affirmed by the parent, and ranges from 0 to 10. It is an ordinal-level measure of food insecurity. It can be used in analyses as an ordinal measure of food insecurity or to identify more severe or less severe categories of food insecurity than those identified in the categorical food security variables described in section 7.5.2.8.3. The raw score is ordinal, not interval, so it should not be used when a linear measure is required, such as for calculation of a mean. Responses to items skipped because of screening are assumed to be negative for the purpose of creating the score. For cases that have some missing data but at least some valid responses, missing responses were considered to be negatives. Cases with no valid responses to any of the 10 food security items are coded as missing -9 (not ascertained). Definitions for negative and affirmed values of food security items are shown in exhibit 7-4. Exhibit 7-4. Definitions of negative and affirmed values for the food security items in the ECLS-K:2011 kindergarten-third grade restricted-use data file Question number Negative responses (coded 0) Affirmative responses (coded 1) FDQ130A 3 (never true) 1 (often true); or 2 (sometimes true) FDQ130B 3 (never true) 1 (often true); or 2 (sometimes true) FDQ130C 3 (never true) 1 (often true); or 2 (sometimes true) FDQ140 2 (no); or screened out in previous questions and is coded -6. Under Rasch-model assumptions, the scale score for households that affirm no items (raw score = 0) is undefined. It is less than the lowest measured value, but its precise value is unknown and may vary substantially among households. For such cases, X7FSADSC2 is assigned a value of -6. These households are food secure, but the appropriate size of the interval between their score and the score of households that affirmed one item is not known and varies from household to household. If these cases (a substantial majority of all cases) are included in linear models, appropriate methods must be used. For example, if the food security scale score is a dependent variable, a selection model such as Tobit may be appropriate. If the food security scale score is a predictor variable, a value of 0 may be assigned to cases with a raw score of 0 and a dummy variable added to identify households with a raw score of 0."}, {"section_title": "Food Security Status: Categorical Measures (X7FSADST2)", "text": "X7FSADST2 is a categorical measure of adults' food security status based on the household's adult food security raw score, X7FSADRA2. X7FSADST2 identifies households as food secure (raw scores 0-2), having low food security among adults (raw scores 3-5), or having very low food security among adults (raw scores of 6 or more). Users may combine the latter two categories as indicating food insecurity among adults. This variable is appropriate for comparing percentages of households with food insecurity among adults and very low food security among adults across subpopulations."}, {"section_title": "Teacher Composite Variables", "text": "In addition to the teacher data flags discussed in section 7.4.3 above, there are several composite variables on the file that use data from teachers. For example, there are composite variables 7-45 about the child's closeness and conflict with the teacher (X7CLSNSS, X7CNFLCT). These two variables are described in chapter 3, along with other variables derived from teacher reports of children's social skills and working memory. Other variables that use teacher data are about the child's grade level (e.g.,"}, {"section_title": "X7GRDLVL", "text": ") and are discussed above in section 7.5.1 about the child composites."}, {"section_title": "School Composite Variables", "text": "Variables describing children's school characteristics were constructed using data from the teacher, the school administrator, and the sample frame. Details on how these variables were created are provided below. A change in approach to computing school composite variables was implemented for the spring 2014 data collection. ECLS-K:2011 data were prioritized over school master file 31 data in assigning values to school composites. As a result, data from the school administrator questionnaire were used for the current round or the most recent available prior round before using current school master file data to assign composite values. Because many children move from one school to another over the course of the study, the construction of school composites (e.g., school type) can be challenging when current-round data are missing or when items are not asked in the current round if the school submitted an SAQ in a prior round. Using the school value for a child from a prior round can be erroneous due to children moving. As a result, many school composites are constructed by combining data across years at the school level, calculating the composite value, and then assigning that value to participating children currently enrolled in the school."}, {"section_title": "School Type (X7SCTYP)", "text": "In the spring of 2014, the questionnaire given to administrators in schools that did not have previous round school data (SAQ-A) contained a question on school type that was used in the creation of the spring school type composite (X7SCTYP). The questionnaire given to administrators in schools that had provided school data in previous rounds (SAQ-B) did not contain the question used to create the school 31 The school master file was created for the ECLS-K:2011 from the Common Core of Data (CCD) for public schools, the Private School Universe Survey (PSS) for private schools, and other data sources. It is updated regularly as new files from those surveys become available."}, {"section_title": "7-46", "text": "type composite; therefore, for these schools data from the school administrator questionnaire in spring 2013, spring 2012, or spring 2011 were used. School master file data were used if school responses were not available from any ECLS-K:2011 round. X7SCTYP was created as follows when SAQ-A was given to school administrators: If question A5 in the SAQ-A (\"Which of the following characterizes your school?\") was answered as \"a regular public school (not including magnet school or school of choice)\" (S7REGPSK); \"a public magnet school\" (S7MAGSKL); or \"a charter school\" (S7CHRSKL); the school was coded as \"public.\" If the question was answered as \"a Catholic school\" of any type (S7CATHOL, S7DIOCSK, S7PARSKL, or S7PRVORS), the school was coded as \"Catholic.\" If the question was answered as \"other private school, religious affiliation\" (S7OTHREL), the school was coded as \"other religious.\" Otherwise, if the question was answered as \"private school, no religious affiliation\" (S7OTNAIS, S7OTHRNO), then the school was coded as \"other private.\" When questionnaire SAQ-B was given to school administrators, X7SCTYP was set based on school administrator questionnaire answers about school type provided in spring 2013, spring 2012, or spring 2011. If data about school type were missing from the SAQ-A for the current round or prior rounds, information about school type from the school master file (which included FMS and frame data) was used to create X7SCTYP. 32 Homeschooled children have a code of -1 (not applicable) on X7SCTYP. 33 Children who changed schools and were not followed and children who were not located in the spring of 2014 have a code of -9 (not ascertained) for X7SCTYP. The variable X7SCTYP is set to system missing for children who were not participants in the spring 2014 round. In addition, nonparticipants have a value of 990000000 on the variable F7CCDLEA. 32 X7SCTYP is constructed differently than previous versions of the same composite. For example, for the round 6 version of the composite, X6SCTYP, if spring 2013 school administrator data were missing, previous round composite values for school type (X4PUBPRI, X2PUBPRI) were used. If those data were missing, data from the school master file were used. 33 These children were enrolled in a school at the time of sampling in the base year, but were homeschooled during the spring of 2014. 7-47 7.5.4.2"}, {"section_title": "Public or Private School (X7PUBPRI)", "text": "X7PUBPRI is a broad indicator of school type with only two categories-public and private. X7PUBPRI, which is derived from the more detailed school type variable X7SCTYP described above, has valid values for the 13,579 cases that have either child assessment or parent interview data in round 7. This composite was created as follows: X7PUBPRI is coded 1 (public) if school type indicated in X7SCTYP is 4 (public). X7PUBPRI is coded 2 (private) if school type indicated in X7SCTYP is 1, 2, or 3 (Catholic, other religious, or other private). If the school identification number for spring 2014 indicated that the child was homeschooled, then X7PUBPRI is coded -1 (not applicable). X7PUBPRI is coded -9 (not ascertained) if data on school type are not available in the spring 2014 school master file. X7PUBPRI is set to system missing for children who did not participate in round 7."}, {"section_title": "School Enrollment (X7ENRLS)", "text": "There is a composite variable in the data file (X7ENRLS) that indicates total school enrollment on October 1, 2013 (or the date nearest to that date for which the school administrator had data available). This total school enrollment composite was created using the school enrollment variable from the school administrator questionnaire (S7ANUMCH). If school administrator data on total school enrollment were missing for spring 2014, enrollment data were obtained from the most recent round of the study with nonmissing school administrator data about school enrollment. If those data were missing, information from the Private School Universe Survey (PSS) for private schools and from the Common Core of Data (CCD) public school universe data for public schools were used. 34 In all other cases the variable is coded -9 (not ascertained)."}, {"section_title": "Percent Non-White Students in the School (X7RCETH)", "text": "The composite variable X7RCETH indicates the percentage of the student population that was not White in the spring of 2014. 35 The composite is derived from a question in the school administrator 34 X7ENRLS is constructed differently than previous versions of the same composite. For example, for the round 6 version of the composite, X6ENRLS, if spring 2013 school administrator data were missing, X6ENRLS was set using school master file data. If those data were missing, data from previous round composites (X4KENRLS, X2KENRLS) were used. 35 This variable was S*MINOR in the ECLS-K. In the ECLS-K:2011, there is a different variable factored into the composite that indicates the percentage of students classified as \"two or more races, non-Hispanic or Latino\" (S*MULTPT)."}, {"section_title": "7-48", "text": "questionnaire (question A8 in SAQ-A) that asked the number or percentage of students in the school who were the following race/ethnicities: Hispanic/Latino of any race; American Indian or Alaska Native, not School administrators were allowed to report their answers to the student racial/ethnic composition questions as either numbers or percentages. All answers provided as numbers were converted to percentages using the total enrollment variable S7TOTENR as the denominator before computing the composite variable. 36 The sum of the calculated percentages for each race/ethnicity category was allowed to be within +/-5 percent of 100 percent to allow for minor reporting errors of numbers that did not add to the reported total or percentages that did not add to 100 percent. In a few cases, this procedure resulted in a total sum of percentages that was slightly over 100 percent. Totals greater than 100 percent are top-coded to 100 percent. A flag for each individual race/ethnicity variable indicating whether the school administrator reported the information as a number or a percent is included in the data file. 37 Because the composite is calculated as a percent, these flags will not be needed by users unless they are interested in examining how 36 There were five recoding rules used for data with apparent errors: 1. If answers were reported as numbers and the total number of students in the school (S7TOTENR) was missing, the total from another question about total enrollment (Q3a S7ANUMCH) was used if the difference between the summed total of students in different race/ethnicity groups and the reported Q3a total was within +/-5 percent of 100 percent (95-105 percent). For example, if the number of students in each race/ethnicity group in the school added to 501 students, but the total number of students by race (S7TOTENR) was missing, and total enrollment from S7ANUMCH was 500 students, the sum of the number of students in the race/ethnicity categories (501) would be 100.2 percent of the value of 500 reported in S7ANUMCH. The value of 100.2 percent is within the 95-105 percent range of allowed errors, so S7ANUMCH is used as the denominator for calculating the percentage of students in each race/ethnicity category. 2. If the method of reporting was mixed (some as numbers, others as percentages), the race/ethnicity percentages were coded as -9 (not ascertained). 3. If percentages were recorded, with none of the above errors, and the summed total across categories was within +/-5 percent of 100 percent (95-105 percent) of the value in S7TOTENR, any race/ethnicity categories that the school administrator left blank were recoded to 0. 4. If the summed total of students in race/ethnicity categories was not +/-5 percent of 100 percent (95-105) percent of the sum reported in S7TOTENR or not 95-105 percent of total enrollment from another question (Q3a S7ANUMCH), the individually reported percentages and numbers were made -9 (not ascertained). 5. If numbers were reported, with none of the above errors, and the summed total across categories was within +/-5 percent of the reported total, any race/ethnicity categories that the school administrator left blank were recoded to 0. 37 There were also other questions in the school administrator questionnaire that allowed for answers to be recorded as either a number or percent. The flags for these variables are S7ADAFLG (average daily attendance reported as number or percent); S7ASIAFL (question about Asian or Pacific Islander teachers, not Hispanic or Latino, reported as number or percent); S7HISPFL (question about Hispanic teachers reported as number or percent); S7BLACFL (question about Black teachers, not Hispanic or Latino, reported as number or percent); S7WHITFL (question about White teachers, not Hispanic or Latino, reported as number or percent); S7AIANFL (question about American Indian or Alaska Native teachers, not Hispanic or Latino, reported as number or percent); S7HAWPFL (question about Native Hawaiian or Pacific Islander teachers, not Hispanic or Latino, reported as number or percent); and S7MULTFL (question about teachers of two or more races, not Hispanic or Latino, reported as number or percent). In all cases, the variables related to these flags provide information as numbers or percentages, with the flags indicating how the answers were originally reported by school administrators."}, {"section_title": "7-49", "text": "answers were reported. If the flag (S7ASIAFL, S7HISPFL, S7BLACFL, S7WHITFL, S7AIANFL, S7HAWPFL, and S7MULTFL) for each of the race/ethnicity variables (S7ASIAPT, S7HISPPT, S7BLACPT, S7WHITPT, S7AIANPT, S7HAWPPT, and S7MULTPT) is equal to 1, that indicates the information was reported by the school administrator as a percentage. If the flag (S7ASIAFL, S7HISPFL, S7BLACFL, S7WHITFL, S7AIANFL, S7HAWPFL, and S7MULTFL) for each of the race/ethnicity variables (S7ASIAPT, S7HISPPT, S7BLACPT, S7WHITPT, S7AIANPT, S7HAWPPT, and S7MULTPT) is equal to 2, that indicates the information was reported by the school administrator as a number. In some cases, the composite could not be derived from the school administrator questionnaire responses because some data used to compute it were missing or the data collected from administrators appeared to be in error (e.g., if school administrators reported both numbers and percents that were not consistent with one another and it was unclear which data were correct). If the composite could not be derived from the spring 2014 data, the percentage of non-White students in the school was obtained from school administrator questionnaire responses from spring 2013, spring 2012, or spring 2011. 38 If those data were also missing, the percentage of non-White students in the school was obtained from the CCD (for public schools) or the PSS (for private schools). If those data were also missing, X7RCETH is coded -9 (not ascertained). If the study child was homeschooled in the spring of 2014, X7RCETH is coded -1 (not applicable)."}, {"section_title": "Highest and Lowest Grade at the School (X7LOWGRD, X7HIGGRD)", "text": "Composite variables indicate the lowest grade taught at the school (X7LOWGRD) and the highest grade taught at the school (X7HIGGRD). They are derived from information collected from the school administrator during the spring 2014 data collection (for administrators in schools for which no previous SAQ had been submitted, who received questionnaire SAQ-A) or from the spring of 2013, spring of 2012, or the spring of 2011 (for administrators in schools for which an SAQ had previously been submitted and who received questionnaire SAQ-B). For administrators who submitted questionnaire SAQ-A, X7LOWGRD and X7HIGGRD were created by first coding answers of \"ungraded\" in question A4 (\"Mark all grade levels included in your school\") as category 15 (ungraded) and then coding the lowest grade in the school and the highest grade in the school, respectively. The grade level for children in transitional kindergarten, kindergarten, or pre-first grade is coded as category 2 (kindergarten). For schools 38 X7RCETH is constructed differently than previous versions of the same composite. For example, for the round 6 version of the composite, X6RCETH, if spring 2013 school administrator data were missing, X6RCETH was set using school master file data. If those data were missing, data from previous round composites (X4RCETH, X2KRCETH) were used."}, {"section_title": "7-50", "text": "whose administrators received questionnaire SAQ-B, or those who received questionnaire SAQ-A and had missing data for school grade levels, the composites X7HIGGRD and X7LOWGRD were set to the values reported in previous school administrator data in spring 2013, spring 2012, or spring 2011. Data from the school master file were used if information about the highest and lowest grade at the school was not collected in school administrator variables for any round. 39"}, {"section_title": "Students Eligible for Free or Reduced-Price School Meals (X7FRMEAL_I)", "text": "The composite variable X7FRMEAL_I indicates the percent of students in the school who were approved for free or reduced-price school meals. This composite has valid values for the 13,579 cases that have either child assessment or parent interview data in round 7. This composite differs from the school meal composites created in for the spring of 2011 and the spring of 2012 (X2FLCH2_I, X2RLCH2_I, X4FMEAL_I, and X4RMEAL_I) because the spring 2014 school administrator questionnaire did not include questions on USDA program participation or the numbers of students eligible for free and reduced priced meals (breakfast or lunch) that were used as the sources of the composite variables for spring 2011 and spring 2012. However, in the spring of 2014 and in previous rounds of the study, school administrators were asked for the percentage of children eligible for free or reduced-price lunch. This question and several other sources of information were used to create X7FRMEAL_I. Specifically, X7FRMEAL_I is derived from the percentage of children eligible for free or reduced-price lunch reported by the school administrator during the spring 2014 data collection, or imputed if the item was missing from the SAQ, using information collected from school administrators in the spring of 2013, the spring of 2012, or the spring of 2011, frame variables or hot deck imputation. 40 For schools where no SAQ was received for spring 2014 (and therefore SAQ missing values were not imputed), the composite was completed by assigning, in the following order, a value from prior rounds of the study, the school master file, or hot deck imputation. 41 X7FRMEAL_I, based on school administrator data about children eligible for free or reduced-price lunch, was imputed 39 X7LOWGRD and X7HIGGRD are constructed differently than previous versions of the same composite. For example, for the round 6 versions of the composites, X6LOWGRD and X6HIGGRD, if spring 2013 school administrator data were missing, previous round composite (X4HIGGRD and X4LOWGRD, X2HIGGRD, and X2LOWGRD) values were used to set the composites. If those data were missing, data from the school master file were used. 40 Both public schools and nonprofit private schools are eligible for the National School Lunch Program. 41 X7FRMEAL_I is constructed differently than previous versions of the same composite. For example, for the round 6 version of the composite, X6FRMEAL_I, data from the imputed spring 2013 school administrator questionnaire were used first to set the composite value, followed by variables in the following order of priority: unimputed school administrator data from the most recent previous round of the study available, data from the school master file, the sum of the spring 2012 composite for free school meals added to the spring 2012 composite for reduced-price school meals, and then the sum of the spring 2011 composite for free school meals added to the spring 2011 composite for reduced-price school meals. Finally, if X6FRMEAL_I did not have an assigned value following each of the above steps, the remaining missing values were imputed using hot deck imputation at the composite level."}, {"section_title": "7-51", "text": "with information from previous rounds about students eligible for free or reduced-price meals because children are approved for free or reduced-price meals generally, not just for lunch. Children who were homeschooled have X7FRMEAL_I set to -1. The percent of children reported by school administrators in spring 2014 to be eligible for free or reduced-price lunch (S7PCTFLN_I) was used as the first source of data for X7FRMEAL_I. 42 There are seven schools that appear to have reported a number of students rather than a percentage in S7PCTFLN_I; their values were retained for the composite and a flag (X7FRMEALFLG) was created to identify them. S7PCTFLN_I was imputed for all cases that had child assessment or parent interview data in the spring 2014 round and a completed SAQ, but for which the administrator did not provide free and reduced-price lunch information. Table 7-6 shows the level of missing data for the school administrator variable for the percent of children who were eligible for free or reduced-price lunch (S7PCTFLN) among the schools that had at least one child or parent respondent in the spring 2014 data collection. The imputation flag IFS7PCTFLN indicates whether the school administrator questionnaire variable S7PCTFLN_I was longitudinally imputed using spring 2013, spring 2012, or spring 2011 data, was filled with data from the CCD, was imputed using the hot deck method, or was not imputed. For cases with missing data on S7PCTFLN, longitudinal imputation was used first, if possible, taking a value from school administrator data in a previous round for the same school in spring 2013 (S6PCTFLN), spring 2012 (S4PCTFLN), or spring 2011 (S2LUNCH). If historical survey data were not available, then data from the CCD were used to impute for these missing S7PCTFLN_I values for public schools. The PSS does not have data on school meals that can be used to compute an imputed value for S7PCTFLN_I. If CCD data were not available, then the values of the meal composites from previous rounds were used to compute an imputed value for S7PCTFLN_I, where available, with the imputed value computed as X6FRMEAL_I, if 42 X7FRMEAL_I was top-coded to 100 percent, if necessary. This was done for 8 schools with 43 participating students."}, {"section_title": "7-52", "text": "this was available, the sum of X4FMEAL_I and X4RMEAL_I if these were available, and otherwise the sum of X2FLCH2_I and X2RLCH2_I, if available. If S7PCTFLN_I was still missing after data from previous rounds and the CCD were used, it was imputed using the hot deck method described above in section 7.5.2.5. Hot-deck imputation was done at the school level and the imputed value was then assigned to each child in the school. In hot-deck imputation, a school with a nonmissing value for a component has this value assigned or \"donated\" to a similar school with a missing value for the component. Schools are similar if they belong in the same imputation cell. Imputation cells were created using district poverty category (created from the district poverty variable X7DISTPOV described in section 7.5.7), census region, school type, the percentage of students in minority ethnic groups, and whether the school received Title I funding. Cases that did not have any data from the school administrator questionnaire in the spring of 2014 did not have a value for S7PCTFLN_I to set the value of the composite X7FRMEAL_I, so other sources were used to assign a value for the composite. X7FRMEAL_I was set to the percentage of students in the child's current school eligible for free or reduced-price lunch reported by the school administrator in the spring of 2013 (S6PCTFLN), if those data were available, or the spring of 2012 (S4PCTFLN), if those data were available. If spring 2012 data were not available but data from the spring of 2011 (S2LUNCH) were, the 2011 data were used. Otherwise, if the school master file had data for the school's total enrollment, the number of children approved for free meals, and the number of children approved for reduced-price meals, X7FRMEAL_I was set to the percentage of children approved for free meals plus the percentage of children approved for reduced-price meals. Finally, if X7FRMEAL_I did not have an assigned value following each of the above steps, the remaining missing values were imputed using hot deck imputation at the composite level. The imputation flag IFX7FRMEAL indicates whether X7FRMEAL_I was imputed longitudinally, was imputed using the hot deck method or was not imputed. In some cases, the children's schools are unknown because the child was unlocatable or the child moved to a nonsampled county and was not followed into his/her new school, but a parent interview was completed. In such cases, data were not imputed for X7FRMEAL_I because no information about the school was available (e.g., public or private control, school size, or even if the child was enrolled in a school). X7FRMEAL_I is coded as -9 for these cases. 7-53 7.5.4.7"}, {"section_title": "Geographic Region and Locality of the Child's School (X7REGION, X7LOCALE)", "text": "Composite variables indicating the geographic region (X7REGION) and locality type (X7LOCALE) of the child's school come from the PSS for private schools and the CCD for public schools. For the spring 2014 geographic region composite, X7REGION, if the geographic region was missing in the PSS and CCD files, then the state in which the school was located was used to assign region. If those data were missing and the geographic region for the school was identified in an earlier round, the composite was set to the value from the most recent round (as reported in X6REGION, X4REGION, X2REGION X7REGION is coded -9 (not ascertained) for children who were unlocatable or moved out of a sampled county and were not followed to new schools in the spring of 2014, but for whom there are parent interview data. Children who were homeschooled in the spring of 2014 have a code of -1 on X7REGION. X7REGION is set to system missing for those who did not participate in round 7. For the spring 2014 school locality variable, X7LOCALE, the categories correspond to the 2006 NCES system for coding locale (https://nces.ed.gov/ccd/rural_locales.asp). If data were not available for the child's school from the PSS or CCD, and locale data were available from an earlier round, the composites were set to the value from the most recent round (X6LOCALE, X4LOCALE, X2LOCALE, or X1LOCALE). Otherwise, the composites are coded -9 (not ascertained). Some -9 (not ascertained) values for X7LOCALE are associated with cases in which children who moved were unlocatable or moved out of a sampled county and were not followed to new schools in spring 2014, but for whom there are parent interview data. Children who were homeschooled in spring 2014 are coded as -1 on X7LOCALE. 43 X7REGION is constructed differently from all previous versions of the same composite. Although X7REGION uses the same data sources that were used to construct the composite in previous rounds, the order of the data sources used is different in round 7 than in previous rounds. For example, for the round 6 version of the composite, X6REGION, the state in which the school was located was used as a final step in assigning the composite value, if data from the CCD or PSS files and geographic location from a previous round (X4REGION, X2REGION, or X1REGION) were not available."}, {"section_title": "7-54", "text": "X7LOCALE is set to system missing for those who did not participate in round 7. Values for X7LOCALE are the following: 11 -City, Large: Territory inside an urbanized area and inside a principal city with population of 250,000 or more; 12 -City, Midsize: Territory inside an urbanized area and inside a principal city with population less than 250,000 and greater than or equal to 100,000; 13 -City, Small: Territory inside an urbanized area and inside a principal city with population less than 100,000; 21 -Suburb, Large: Territory outside a principal city and inside an urbanized area with population of 250,000 or more; 22 -Suburb, Midsize: Territory outside a principal city and inside an urbanized area with population less than 250,000 and greater than or equal to 100,000; 23 -Suburb, Small: Territory outside a principal city and inside an urbanized area with population less than 100,000; 31 -Town, Fringe: Territory inside an urban cluster that is less than or equal to 10 miles from an urbanized area; 32 -Town, Distant: Territory inside an urban cluster that is more than 10 miles and less than or equal to 35 miles from an urbanized area; 33 -Town, Remote: Territory inside an urban cluster that is more than 35 miles from an urbanized area; 41 -Rural, Fringe: Census-defined rural territory that is less than or equal to 5 miles from an urbanized area, as well as rural territory that is less than or equal to 2.5 miles from an urban cluster; 42 -Rural, Distant: Census-defined rural territory that is more than 5 miles but less than or equal to 25 miles from an urbanized area, as well as rural territory that is more than 2.5 miles but less than or equal to 10 miles from an urban cluster; and 43 -Rural, Remote: Census-defined rural territory that is more than 25 miles from an urbanized area and is also more than 10 miles from an urban cluster. Some schools have different values for X*LOCALE between the base year and subsequent rounds. The differences in values reflect changes in the PSS or CCD source data."}, {"section_title": "7-55", "text": "The classification of locale has undergone some changes since the ECLS-K study conducted with children in the kindergarten class of 1998-99. Information on these changes is available on the NCES website at https://nces.ed.gov/ccd/rural_locales.asp."}, {"section_title": "Field Management System (FMS) Composite Variables", "text": "Several composite variables were created from data stored in the FMS, which were obtained from school master file data as well as by field staff during visits to the schools and discussions with school staff."}, {"section_title": "School Year Start and End Dates (X7SCHBDD, X7SCHBMM, X7SCHBYY, X7SCHEDD, X7SCHEMM, X7SCHEYY)", "text": "The composite variables indicating school year start and end dates, which are listed below, were derived from information contained in the FMS. The composite variables for beginning and ending school dates are derived differently in spring 2014 than in previous rounds. In previous rounds of the study, the school administrator questionnaire data were used as the first source of data for creating the composites, followed by the use of FMS data if the questionnaire data were missing. In spring 2014, the school administrator questionnaire did not include a question about beginning and ending school dates, so the FMS data were used to derive the composites. For the spring 2014 composites, some 2013 school start dates were obtained after the data collection period. 7-56 7.5.5.2"}, {"section_title": "Year-Round Schools (X7YRRND)", "text": "The year-round school composite variable is based on information obtained from the school staff member who helps coordinate the data collection activities in the school (referred to as the school coordinator) about whether a school is a year-round school. This composite has valid values for the 13,579 cases that have child assessment or parent interview data in round 7. The values for this composite variable are 1 (year-round school) and 0 (not year-round school). If the child was homeschooled in the spring of 2014, the composite is coded as -1 (not applicable). If these data were not obtained in the spring of 2014 but information about being a year-round school was collected in an earlier round, the composite was set to the value from the most recent round (X6YRRND, X4YRRND, or X12YRRND). . There are 97 ECLS-K:2011 public schools with a missing value for X7DISTPOV because the values were missing in the SAIPE source data."}, {"section_title": "Methodological Variables", "text": "To facilitate methodological research, variables pertaining to aspects of the data collection work were extracted from the FMS and included in the data file. These include identifiers for parent interview work area (F7PWKARE), parent interviewer identification number (F7PINTVR), the month the parent interview was conducted (F7INTVMM), the year the parent interview was conducted (F7INTVYY), child assessment work area (F7CWKARE), and child assessor identification number (F7CASSOR). A \"work area\" is the group of schools that each team leader was assigned. Team leaders managed a group of 2 to 4 other individuals who worked as child assessors and parent interviewers for the sampled cases in the work area. If a case was not assigned to an interviewer (e.g., a child who moved and was not followed), Introduction This chapter provides specific instructions for using the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) Electronic Codebook (ECB). The functionality of the ECB, which is the same throughout the ECLS studies, is fully described in the Help File for the ECLS-K:2011 longitudinal kindergarten-fourth grade (K-4) ECB. The information in the ECB's Help File provides a comprehensive tour through the ECB and addresses all of the functions and capabilities of the program. These functions allow users to access the accompanying data catalog and view the data in various ways by performing customized searches and extractions. Using the ECB, the data user can create SAS, SPSS for Windows, and Stata syntax programs that can be run to generate an extract data file from the text (ASCII) data file."}, {"section_title": "Hardware and Software Requirements", "text": "The ECB program is designed to run under Windows 95 \u00ae , Windows 98 \u00ae , Windows 2000 \u00ae , Windows XP \u00ae , or Windows NT \u00ae 4.0 on a Pentium-class or higher personal computer (PC). The ECB has been successfully tested using current versions of Windows Vista and Windows 7. It has not been tested on Windows 10. The ECB is not designed for use on Apple Macintosh systems, but Mac users can create a data file using the file record layout. The PC should have a minimum of 20 megabytes of available disk space. The program will fit best visually on screens set to a desktop area of 1024 x 768 pixels. It will still work on other screen settings, but it may not make the best use of the available screen space. If you have a Windows NT \u00ae or earlier operating system, you can check or set your desktop area as follows: 1. Click the Windows Start button."}, {"section_title": "Installing, Starting, and Exiting the ECB", "text": "The ECB is intended to be installed and run from within the Windows 95 \u00ae , Windows 98 \u00ae , Windows 2000 \u00ae , Windows XP \u00ae , Windows NT \u00ae 4.0, Windows Vista, or Windows 7 \u00ae environment. The sections in this chapter provide you with step-by-step instructions for installing the program on your PC, starting the program, and exiting the program once you have completed your tasks."}, {"section_title": "Installing the ECB Program on Your Personal Computer", "text": "Program installation is initiated by running the \"InstallECLSECB.exe\" executable file."}, {"section_title": "How to Install the Program", "text": ""}, {"section_title": "8-3", "text": "Depending on your PC's configuration, you may encounter warning messages during installation. To respond, always keep the newer version of a file being copied and ignore any access violations that occur during file copying. If you are installing multiple ECBs (not different versions of the same ECB) on your PC, you may receive a message warning that Setup is about to replace pre-existing files. To respond, always opt to continue the installation although the default is to cancel the setup. When you get a follow-up message to confirm whether the installation should be continued, press Yes to continue, although the default is No."}, {"section_title": "DATA ANOMALIES, ERRATA, AND DATA CONSIDERATIONS", "text": "This appendix provides information on data anomalies, errata, and data considerations. Anomalies and errata listed here were identified during the editing and review of these data and are those known at the time this manual was prepared. Other anomalies and errata may exist in the data. This section also discusses additional categories that were added for coding some open-ended questions. The information presented here will be more easily understood, and is most useful, after the survey items or variables to be used in analyses have been identified. Each anomaly, error, or data consideration is associated with a specific survey question or variable in the data file (or both). Rather than read through this entire appendix, users may find it easier to identify any issues associated with their data of interest by searching for the survey question number, variable name, or keyword in this appendix. For example, an analyst who is interested in information about children's diagnoses of attention deficit hyperactivity disorder (ADHD) could search (1) CHQ125, which is the number of the question in which this information was asked in the parent interview; (2) P7ADHA, which is the name of the variable in which data from CHQ125 about ADHD are stored; and (3) \"Attention Deficit Hyperactivity Disorder\" or \"ADHD.\" These anomalies, errors, and considerations are noted so that users are aware these issues with the data exist. However, leaving the anomalous or erroneous data as they are will not significantly affect most analyses, because the number of cases affected is generally very small. An exception to this is the programming errors that affect entire groups of cases that should have been asked certain questions. Additionally, analyses focused on a small subpopulation or examining rare characteristics could be significantly affected by data issues with even a small number of cases. Therefore, analysts doing such analyses should consider the impact these data issues may have on their results."}, {"section_title": "A-2", "text": "This appendix is organized into the following sections: Some cases have household data about family members that were edited (e.g., if the age of a household member was reported incorrectly and had to be updated, or a person who was added to the household roster in error needed to be deleted from the household roster). These data were changed in the current round of the study, but not in previous rounds of the study. Researchers who are using data about household composition from the parent interview household roster in their analyses should examine all rounds of household roster data closely, recognizing that for a limited number of cases corrected information from later rounds may need to be applied to earlier rounds. Age changes were made to the following cases: for person 1 in the household, age changes were made to CHILDID=10000279, 10012797, 10003756, 10004230, 10005643, 10008515, 10008783, 10015171, 10015360, 10017528, 10017731; for person 3 in the household, age changes were made to CHILDID=10004230, 10001605, 10006227, 10007830; for person 4 in the household, age changes were made to CHILDID=10004230, 10014762; for person 5 in the household, age changes were made to CHILDID= 10017561, 10008501; and for person 6 in the household, age changes were made to CHILDID=10012545. Changes to a household member's sex and relationship to the child were made to the following cases: for person 1 in the household, CHILDID=10003947, 10004420, 10012797, 10014103; for person 3 in the household CHILDID=10003947; and for person 5 in the household CHILDID=10002026."}, {"section_title": "A-4", "text": "Parent Interview: Spring 2014 Errors in the CAI Programming \uf06e There were errors in the preloaded data used to determine which questions about education should be asked, given information collected in prior-round interviews. As a result, there are cases for which the data collected are inconsistent with the skip patterns documented in the interview specifications. In some instances, these errors caused education questions to be asked again for cases that already had education questions asked in an earlier round of the study and for which new education data should not have been collected. For parent 1, there were 86 cases that were asked about education in round 7 but had education data from round 4, and four cases that were asked about education again in round 7 but had education data from round 1. For parent 2, there were 66 cases that were asked about education in round 7 but had education data from round 4, and 13 cases that were asked about education again in round 7 but had education data from round 1. The round 7 education data have been kept on the file for these cases, along with the education data from previous rounds. There were 22 cases for parent 1 and 15 cases for parent 2 that were not asked education questions in section PEQ but should have been because education data were not collected for these parents in a prior round. Hot deck imputation was used to impute education data for these cases."}, {"section_title": "Parent Interview: Data Considerations", "text": ""}, {"section_title": "Hard-Copy Questionnaires", "text": "For the hard-copy instruments (school administrator questionnaires, teacher-level teacher questionnaire, and teacher child-level questionnaire), both range and consistency checks were performed."}, {"section_title": "Composite Variable Anomalies, Errata, and Considerations", "text": "Chapter 7 of this manual provides detailed information about the composite variables that were created and included in the data file. In this section, several data considerations related to the composite variables are described. Analysts are encouraged to carefully review the descriptions of the composite measures of interest to them in chapter 7. \uf06e Some households have an uncommon combination of parent/guardians. Cases (CHILDID=10018131, 10013049, 10014697, 10007530, 10002069, 10003018) were reviewed, and it was either confirmed that the parent/guardians listed are as reported during the parent interview or no information was provided to indicate the parent/guardians listed were incorrect."}, {"section_title": "The Dimensional Change Card Sort (DCCS) Error in the CAI Programming", "text": "There was a programming error in the round 7 DCCS that resulted in giving children credit for a correct response when the child did not provide a response to a trial. This error occurred in both the practice and test trials. Approximately 4 percent of cases were affected overall. As shown in Table A-1, in the practice trials, 281 out of 12,849 cases were affected. In the test trials, 250 out of 12,849 cases were affected. Because of the programming error, audio feedback given to a child for nonresponse during the practice trials was incorrect. The audio feedback that the child heard was \"That's right,\" even though the child did not provide a response. If the child did not respond to a trial, the trial was supposed to be scored as incorrect, and the audio feedback was supposed to indicate that the child responded with an incorrect answer and reteach the rule. The erroneous audio feedback during the practice trials could have confused the child about the rules of the game. It was important for the child to demonstrate a clear understanding of the rules of the game in the practice trials before progressing to the test trials to ensure that performance was not a reflection of failing to understand the instructions. Under some circumstances, having nonresponse scored as correct affected the number of practice trials administered. Errors that occurred during the test trials were corrected in the data. These errors did not affect the child's experience during the test, but only affected how the trial was recorded. A-7 Errors that occurred during the practice trials, however, did affect the child's experience during the test and, in some cases, resulted in insufficient opportunity for the child to demonstrate an understanding of the rules of the game. Cases affected by the programming error during practice were examined to determine whether they met the criteria for moving into the test trials based on the items for which they did provide a response (that is, whether they demonstrated sufficient understanding of the task despite receiving erroneous feedback). These cases, children who had at least one instance of nonresponse in the practice, are flagged as a 6 or 7 in the DCCS flag variable X7DCCSFLG depending on whether they met the criteria (exhibit A-1). Cases that have X7DCCSFLG=6 passed the practice trials with the responses they provided during the administration of the DCCS, despite erroneous feedback. These children successfully met the criterion on both the shape and color trials to advance to the test trials. Although the erroneous feedback may have confused the child in a way that would have affected his or her performance on the test, there are some notable trends in the performance on the practice trials that suggest that data for these children are valid and that the erroneous feedback had minimal impact on a child's understanding of the task. First, the nonresponse occurred most often on an item in the first set of shape trials during practice, which was the first set of practice trials the child received, as opposed to the color trials, which was the second set of practice trials the child received. Second, the practice trials appear to be easy for children this age. Approximately 83 percent of children with a score on the task (X7DCCSFLG=1) passed the shape and color practice with no errors and using the minimum number of practice trials required to advance, and 99.5 percent of children with a score passed the shape practice using the minimum number of practice trials required. In addition, a large percentage of children who had nonresponse in the practice (189 of 281 children) passed the practice by getting 3 or 4 trials correct, when excluding nonresponse trials. This means they were generally accurate when a response was provided. In addition, because the sorting rule was stated prior to every practice and test trial, the child had clear instructions for what to do on each trial, even if erroneous feedback had occurred on a previous practice trial. Because the practice trials seem to be easy for children this age because so many children were able to pass the practice despite the erroneous feedback, and because the child was constantly reminded of what sorting rule should be used for the trial, it is reasonable to conclude that receiving incorrect audio feedback produced minimal interference to children's understanding of the task. Cases that have X7DCCSFLG=7 did not demonstrate sufficient understanding of the task with the responses they provided and were not given sufficient practice per the administration protocols to have their scores included in the data file. They would not have advanced to the test trials if their nonresponse had not been recorded as correct. These cases have DCCS data set to -4 (suppressed due to programming error). Information is also provided on this error in section 3.2.1.1. "}, {"section_title": "\"Other, Specify\" Variables", "text": "As discussed in chapter 6, there were times when a sufficient number of cases provided the same \"other, specify\" response to warrant the addition of a new category to the response options for third grade. The categories added after data collection ended, during review of the data, are listed in exhibit A-2. Users should keep in mind that had these new categories been offered as response options to all respondents during data collection, it is possible that more respondents would have chosen them. This guide provides information specific to the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) kindergarten-fourth grade public-use data file, referred to hereinafter as the K-4 PUF, which includes data from the base-year (kindergarten) through fourth-grade data collections. This guide is a supplemental document that describes the edits made to the restricted-use file in order to produce the public-use file. This guide focuses on the variables associated with the third- The K-4 PUF is derived from the K-4 restricted-use file, or RUF, and is identical in format. All the variables from the K-4 restricted-use file are included in the same order on the K-4 public-use file. Like the RUF, the PUF is a child-level file that contains assessment data and parent, teacher, and school information collected for all 18,174 study children who are considered base-year respondents. Data masking techniques were applied to variables in the K-4 RUF to make it suitable for release to researchers without a restricteduse license. These masking techniques, which are described further in the next section, include suppression B-2 of sensitive data or variables that apply to only a small subset of study participants, collapsing variable categories, top-or bottom-coding values that are unusually low or unusually high, converting continuous variables to categorical variables and adding noise to school information from the study that is also present in the school sampling frame. These techniques are applied to the data to minimize the risk that any study participant can be identified using the information provided in the data file about them."}, {"section_title": "Masked Variables", "text": "As noted above, the masking techniques used to produce the ECLS-K:2011 public-use data file include variable recoding and suppression. The purpose of masking is to provide data in a format that minimizes the potential for a respondent to be identified because of that respondent's characteristics or a unique combination of characteristics. For example, there is potential for the principal of a school to be identified if the ZIP code of that school, the number of students in the school, and the age and race/ethnicity of that principal are all provided in the data file. To guard against this potential disclosure, ZIP code and principal race/ethnicity are suppressed (i.e., not provided) in the PUF, and the number of students in the school and principal age are provided in categories rather than as exact values. There are several types of modifications to variables in the K-4 PUF, as described below. \uf06e Outliers (that is, unusually high or unusually low values) are top-or bottom-coded to prevent identification of unique schools, teachers, parents, and children without affecting overall data quality. The category value labels for variables that are top-and bottom-coded in the PUF are edited versions of the RUF category labels and reflect the new highest and lowest categories. \uf06e Some continuous variables are converted into categorical variables, and some categorical variables have their categories collapsed in the K-4 PUF. Category value labels are provided for continuous variables that are converted into categorical variables."}]