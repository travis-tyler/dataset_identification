[{"section_title": "Abstract", "text": "Existing approaches for multivariate functional principal component analysis are restricted to data on the same one-dimensional interval. The presented approach focuses on multivariate functional data on different domains that may differ in dimension, e.g. functions and images. The theoretical basis for multivariate functional principal component analysis is given in terms of a Karhunen-Lo\u00e8ve Theorem. For the practically relevant case of a finite Karhunen-Lo\u00e8ve representation, a relationship between univariate and multivariate functional principal component analysis is established. This offers an estimation strategy to calculate multivariate functional principal components and scores based on their univariate counterparts. For the resulting estimators, asymptotic results are derived. The approach can be extended to finite univariate expansions in general, not necessarily orthonormal bases. It is also applicable for sparse functional data or data with measurement error. A flexible R implementation is available on CRAN. The new method is shown to be competitive to existing approaches for data observed on a common one-dimensional domain. The motivating application is a neuroimaging study, where the goal is to explore how longitudinal trajectories of a neuropsychological test score covary with FDG-PET brain scans at baseline. Supplementary material, including detailed proofs, additional simulation results and software is available online."}, {"section_title": "Introduction", "text": "Statistical methods for functional data have become increasingly important in recent years. Functional principal component analysis (FPCA) is one of the key techniques in functional data analysis, as it provides an easily interpretable exploratory analysis of the data. Further, it is an important building block for many statistical models (see e.g. Ramsay and Silverman, 2005) . The technical progress in many fields of application allows the collection of more and more data with functional features, often several kinds per observation unit. This encourages the study of multivariate functional data and new methods are required to reveal e.g. joint variation in the different elements.\nAs a simple motivating example, consider the gait cycle data (Ramsay and Silverman, 2005 ) shown in Fig. 1 . It contains 39 observations of hip and knee angle during a gait cycle on a standardized time interval. Both elements of this bivariate data can be described separately by their first three univariate eigenfunctions that explain 94.4% (hip) and 87.5% (knee) of the total variability in the data. The associated functional principal component scores, however, reveal that there is a non negligible correlation between almost all score pairs of the two elements. The separate FPCA thus captures joint variation between hip and knee angles only indirectly, which makes the interpretation of the FPCA results difficult. Correlated scores can also lead to multicollinearity issues in a subsequent regression analysis (functional principal component regression, e.g. M\u00fcller and Stadtm\u00fcller, 2005) . Multivariate FPCA, by contrast, directly adresses potential covariation between the hip and knee elements. The first three bivariate principal components shown in Fig. 1 , which explain 85.3% of the variability in the data, give insight into the main modes of joint variation in the overall gait movement. The corresponding scores do not only allow a more parsimonious representation of the data (one score value per bivariate principal component and per observation), but they are also uncorrelated by construction. Finally, the multivariate functional principal components are more natural to represent multivariate functional data in the sense that they have the same structure as each observation. The extension of FPCA to multivariate functional data is hence of high practical relevance.\nExisting approaches for multivariate functional principal component analysis (MFPCA) are restricted to functions observed on the same finite, one-dimensional interval (Ramsay and Silverman, 2005; Jacques and Preda, 2014; Chiou et al., 2014; Berrendero et al., 2011) . Except for Berrendero et al. (2011) , they all aim at a multivariate functional Karhunen-Lo\u00e8ve representation of the data. For data measured e.g. in different units, Jacques and Preda (2014) and Chiou et al. (2014) also discuss normalized versions of MFPCA based on a normalized covariance operator.\nThe key motivation for this paper is that in practical applications, multivariate functional data are neither restricted to lie on the same interval nor to have one-dimensional domains, e.g. data that consists of functions and images, as in our neuroimaging application. We start by extending the notion of multivariate functional data to the case of different (dimensional) domains for the different elements. Next, the theoretical foundations of MFPCA are provided in terms of a Karhunen-Lo\u00e8ve Theorem. For the practically relevant case of a finite or truncated Karhunen-Lo\u00e8ve representation, we establish a direct theoretical relationship between univariate and multivariate FPCA. This suggests a simple estimation strategy for multivariate functional principal components and scores based on their univariate counterparts. For data on higher dimensional domains (tensor data, e.g. images), principal component methods have originally been developed in the context of psychometrics (e.g. Tucker, 1966; Carroll and Chang, 1970) and have become particularly important in the machine learning literature (Coppi and Bolasco, 1989; Lu et al., 2013) . Recent approaches for functional or smooth principal component analysis for tensor data have been proposed e.g. in Allen (2013) . All these methods can be used as univariate building blocks for MFPCA. The resulting estimators for MFPCA are shown to be consistent under a given set of assumptions. In contrast to most of the existing methods for MFPCA, our new approach can be applied to sparse functional data and data with measurement error. It can be generalized to data available in arbitrary basis expansions and hence includes the MFPCA procedure proposed by Jacques and Preda (2014) as a special case. The new method further allows to incorporate weights for the elements, if they differ in domain, range or variation. The paper is organized as follows. Section 2 introduces multivariate functional data and gives the theoretical basis for MFPCA. In Section 3 we derive the estimation algorithm for MFPCA based on univariate basis expansions and investigate asymptotic properties of the resulting estimators. The performance of the new method is evaluated in Section 4 in a simulation with different levels of complexity. Section 5 contains the analysis of the motivating neuroimaging dataset. The paper concludes with a discussion and an outlook in Section 6. Supplementary material, containing detailed proofs of all propositions, more simulation results and R code is available online."}, {"section_title": "Theoretical Foundations of Multivariate Functional Data", "text": ""}, {"section_title": "Data Structure and Notation", "text": "This paper is concerned with multivariate functional data, i.e. each observation consists of p \u2265 2 functions X (1) , . . . , X (p) . They may be defined on different domains T 1 , . . . , T p with possibly different dimensions. Technically, T j must be compact sets in R d j , d j \u2208 N with finite (Lebesgue-) measure and each element X (j) : T j \u2192 R is assumed to be in L 2 (T j ).\nIn analogy to other approaches for multivariate functional data, the different functions are combined in a vector X with X(t) = X (1) (t 1 ), . . . , X (p) (t p ) \u2208 R p .\nNote that t := (t 1 , . . . , t p ) \u2208 T := T 1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 T p is a p-tuple of d 1 , . . . , d p -dimensional vectors and not a scalar. This is a main difference to earlier approaches, as it allows each element X (j) to have a different argument t j , even in the case of a common one-dimensional domain. In the following, it will be further assumed that \u00b5(t) := E (X(t)) = E X (1) (t 1 ) , . . . , E X (p) (t p ) = 0 \u2200 t \u2208 T .\nFor s, t \u2208 T , define the matrix of covariances C(s, t) := E (X(s) \u2297 X(t)) with elements\nAs noted in Ramsay and Silverman (2005, Chapter 8.5.) , a suitable inner product is the basis of all approaches for principal component analysis. For functions f = (f (1) , . . . , f (p) ) with elements f (j) \u2208 L 2 (T j ) define the space H := L 2 (T 1 ) \u00d7 . . . \u00d7 L 2 (T p ) and\nProposition 1. H is a Hilbert space with respect to the scalar product \u00b7, \u00b7 .\nProofs for all propositions are given in the online appendix. The norm induced by \u00b7, \u00b7 is denoted by |||\u00b7||| 1 . Next, define the covariance operator \u0393 : H \u2192 H with the j-th element of \u0393f, f \u2208 H given by (\u0393f ) (j) (t j ) :\nThe setting can be generalized to a weighted scalar product on H, i.e.\n2 , f, g \u2208 H\nfor some positive weights w 1 , . . . , w p , cf. Ramsay and Silverman (2005, Chapter 10.3 . in the context of hybrid data) or Chiou et al. (2014) . The associated weighted covariance operator \u0393 w is given by its elements (\u0393 w f ) (j) with f \u2208 H and (\u0393 w f ) (j) (t j ) = C \u00b7,j (\u00b7, t j ), f w , t j \u2208 T j .\n1 The L 2 -norm induced by \u00b7, \u00b7 2 on each L 2 (Tj) is denoted by ||\u00b7|| 2 . Further, ||\u00b7|| is the Euclidean norm for vectors and ||\u00b7|| T denotes a norm on T with ||t|| 2 T = p j=1 ||tj|| 2 for tj \u2208 Tj \u2282 R d j , j = 1, . . . , p.\nThe use of weights may be necessary if the elements have quite different domains or ranges or if they exhibit different amounts of variation, in order to obtain multivariate functional principal components that have a meaningful interpretation (Chiou et al., 2014) . A weighted scalar product corresponds to a (global) rescaling of the elements by w 1/2\nj . An alternative approach would be pointwise rescaling, e.g. by the inverse of the square root of the pointwise variance C jj (t j , t j ). This can be seen as normalizing the covariance operator (Chiou et al., 2014; Jacques and Preda, 2014) . However, this second approach does not consider the size of the different domains T j and would give equal variation per observation point t j rather than per element j. Moreover, rescaling with the pointwise variance would downweight areas in T j with stronger variation, hence areas that might contribute relevant information to the functional principal components. Therefore, only global rescaling by means of a weighted scalar product is considered in the following. The weights have to be chosen prior to the analysis. They can be specified based on expert knowledge or estimated from the data, e.g. based on the variation in each element (see references in Chiou et al., 2014) . A sensible choice will always depend on the specific application and the question of interest. One possible solution that is analogous to standardization in multivariate PCA is proposed in the application in Section 5. For the sake of better readability, all following theoretical results are derived for w 1 = . . . = w p = 1, but remain valid in the more general case of different weights. For the estimation algorithm discussed in Section 3.2, MFPCA based on the weighted scalar product is addressed again."}, {"section_title": "A Karhunen-Lo\u00e8ve Theorem for Multivariate Functional Data", "text": "In the following it is shown that under mild conditions, \u0393 has the same properties as the covariance operator in the univariate case and therefore a Karhunen-Lo\u00e8ve representation for multivariate functional data exists. The main difference to existing approaches for data with elements observed on the same (one-dimensional) domain is that in this special case, \u0393 is an integral operator with positive definite kernel C(s, t). This directly gives all of the desired properties (Saporta, 1981) . In the more general case of elements observed on different domains, this is not obviously the case and the properties are shown explicitly.\nProposition 2. The covariance operator \u0393 defined in (3) is a linear, self-adjoint and positive operator. If further for all i, j = 1, . . . , p there exist K ij < \u221e with\nand C ij is uniformly continuous in the sense that\nIn particular, since \u0393 is a positive operator, it may be assumed w.l.o.g. that \u03bd 1 \u2265 \u03bd 2 \u2265 . . . \u2265 0.\nSince \u03c8 m , m \u2208 N is an orthonormal basis of H and \u0393 is self-adjoint, by the Spectral Theorem (e.g. Werner, 2011, Thm. VI.3.2.) it holds that\nThe following proposition is a multivariate version of Mercer's Theorem (Mercer, 1909) . It plays a key role in the proof of the Karhunen-Lo\u00e8ve Theorem (Prop. 4).\nProposition 3 (Mercer's Theorem). For j = 1, . . . , p and s j , t j \u2208 T j it holds that\nwhere the convergence is absolute and uniform.\nProposition 4 (Multivariate Karhunen-Lo\u00e8ve Theorem). Under the assumptions of Prop. 2,\nwith zero mean random variables \u03c1 m = X, \u03c8 m and Cov(\u03c1 m , \u03c1 n ) = \u03bd m \u03b4 mn . Moreover\nThe multivariate Karhunen-Lo\u00e8ve representation has an analogous interpretation as in the univariate case (Ramsay and Silverman, 2005, Chapter 8.2.) . The eigenvalues \u03bd m represent the amount of variability in X explained by the single multivariate functional principal components \u03c8 m , while the multivariate functional principal component scores \u03c1 m serve as weights of \u03c8 m in the KarhunenLo\u00e8ve representation of X. As the eigenvalues \u03bd m decrease towards 0, leading eigenfunctions reflect the most important features of X. Truncated Karhunen-Lo\u00e8ve expansions, optimal M -dimensional approximations to X,\nare often used in practice. Single observations x i of X can then be characterized by their score vectors (\u03c1 i,1 , . . . , \u03c1 i,M ) with \u03c1 i,m = x i , \u03c8 m for further analysis, e.g. for regression (M\u00fcller and Stadtm\u00fcller, 2005) or clustering (Jacques and Preda, 2014 Given the Karhunen-Lo\u00e8ve representation of multivariate functional data X as in (6), a natural question is how this representation relates to the univariate Karhunen-Lo\u00e8ve representations of the single elements X (j) . The following proposition establishes a direct relationship between these two representations if they are both finite, based on the theory of integral equations (Zemyan, 2012) .\nProposition 5. The multivariate functional vector X = X (1) , . . . , X (p) in (6) has a finite KarhunenLo\u00e8ve representation if and only if all univariate elements X (1) , . . . , X (p) , have a finite KarhunenLo\u00e8ve representation. In this case, it holds:\n1. Given the multivariate Karhunen-Lo\u00e8ve representation (6), the positive eigenvalues \u03bb\ncorrespond to the positive eigenvalues of the matrix A (j) \u2208 R M \u00d7M with entries\nThe eigenfunctions of \u0393 (j) are given by\nwhere u (j) m denotes an (orthonormal) eigenvector of A (j) associated with eigenvalue \u03bb (j) m and [u (j) m ] n denotes the n-th entry of this vector. For the univariate scores\n2. Assuming the univariate Karhunen-Lo\u00e8ve representation\nThe eigenfunctions of \u0393 are given by their elements\nwhere [c m ] (j) \u2208 R M j denotes the j-th block of an (orthonormal) eigenvector c m of Z associated with eigenvalue \u03bd m . The scores are given by\nExtensions: The second part of Prop. 5 can be extended in a natural way if univariate elements are expanded in finitely many, not necessarily orthonormal basis functions b\nThis is a very likely situation in practice, e.g. due to pre-smoothing of noisy observations. Following analogous steps as in the proof of Prop. 5 results in an eigenanalysis problem BQc = \u03bdc as starting point for the MFPCA. Here B \u2208 R K + \u00d7K + with K + = p j=1 K j is a block diagonal matrix of scalar products b\nn 2 of univariate basis functions associated with each element X (j) . In the special case that all univariate bases are orthonormal (e.g. when using the univariate principal component bases as in Prop. 5), B equals the identity matrix. The symmetric block matrix Q with entries Q (jk)\nn ) corresponds to Z in Prop. 5. Although BQ is in general not symmetric, its eigenvectors c m and eigenvalues \u03bd m , which are at the same time the eigenvalues of \u0393, are real. This can be easily shown using the Cholesky decomposition of the symmetric matrix B = RR and solving R QRc = \u03bdc withc = R \u22121 c. The estimation algorithm for principal components \u03c8 m and associated scores \u03c1 m based on this general basis expansion is presented in the next section combined with the case of a weighted scalar product."}, {"section_title": "Estimation of Multivariate FPCA", "text": "Estimation based on univariate FPCA: The second part of Prop. 5. suggests a simple and natural approach for estimating the MFPCA. After calculation of unvariate FPCAs for each element, the estimates can be plugged into the formulae given in Prop. 5. Given de-meaned samples x 1 , . . . , x N of X, the proposed estimation procedure for MFPCA consists of four steps:\n1. For each element X (j) estimate a univariate FPCA based on the observations x i,m , i = 1, . . . , N, m = 1, . . . , M j for suitably chosen truncation lags M j . As there exist numerous estimation procedures, e.g. for irregularly sampled and sparse data with measurement error (Yao et al., 2005) , the multivariate method is also applicable to this kind of data.\n2. Define the matrix \u039e \u2208 R N \u00d7M + , where each row (\u03be\ni,Mp ) contains all estimated scores for a single observation. An estimate\u1e90 \u2208 R M + \u00d7M + of the block matrix Z in Prop. 5 is given by\u1e90 = (N \u2212 1) \u22121 \u039e \u039e.\n3. Perform a matrix eigenanalysis for\u1e90 resulting in eigenvalues\u03bd m and orthonormal eigenvector\u015d c m .\n4. Estimates for the multivariate eigenfunctions are given by their element\u015d\nand multivariate scores can be calculated vi\u00e2\nFinding an appropriate truncation lag M j in step 1 is a well-known issue in functional data analysis. Common approaches are based on the decrease of the estimated eigenvalues\u03bb (j) m (screeplot, Cattell, 1966) or the percentage of variance explained (e.g. Ramsay and Silverman, 2005, Chapter 8.2.) . An optimal number M \u2264 M + of multivariate functional principal components can basically be chosen with the same techniques, while the importance of a \"correct\" choice depends on the specific application: For simply exploratory aims it is less crucial than for subsequent analyses that ignore the information of the eigenvalues (and hence, the proportion of variance explained by the single components) and are based solely on multivariate eigenfunctions or scores, as e.g. clustering or functional principal component regression. For the latter, relevant eigenfunctions can also be selected using model-based approaches such as AIC or cross-validation. The goodness of the resulting MFPCA estimates of course depends on an appropriate choice of M j , which can also be used as a sensitivity check: If the first M j eigenfunctions capture all the relevant information in X (j) , increasing M j will add only little information and hence should have only little impact on the results. This relationship is analyzed in a simulation in the online appendix.\nExtensions: The estimation algorithm can easily be extended to elements X (j) available in general basis expansions as in (8) and to MFPCA based on a weighted scalar product as in (4). Given weights w 1 , . . . , w p > 0 and demeaned observations x 1 , . . . , x N of X with estimated basis function coefficients\u03b8 (j) i,m for each element, the eigenanalysis problem to solve is\nThe matrix B is the block diagonal matrix of basis scalar products as in Section 3.1 and D = diag(w 1 1/2 , . . . , w p 1/2 ) \u2208 R K + \u00d7K + accounts for the weights, where each w\ni,Kp ) corresponds to the matrix \u039e defined in step 2 of the original algorithm and (N \u2212 1) \u22121 \u0398 \u0398 is an estimate for Q introduced in Section 3.1. Given eigenvectors\u0109 m and eigenvalues\u03bd m for (11), estimated orthonormal eigenfunctions\u03c8 m of \u0393 w and associated scores\u03c1 i,m can be calculated in analogy to (9) and (10) wit\u0125 Q w = (N \u2212 1) \u22121 D\u0398 \u0398D:\nClearly, the original algorithm is obtained as a special case with \u0398 = \u039e, B = I (univariate FPCA for each element) and D = I (all weights equal to 1). Moreover, the extended algorithm allows to flexibly combine univariate FPCA and general basis expansions for different elements of the multivariate functional data. If all elements X (j) are defined on the same (one-dimensional) interval and D = I, expanding each element in a general basis is equivalent to the method of Jacques and Preda (2014) . The approach proposed in this paper, however, is more general, as it allows for different intervals as well as for higher-dimensional T j and thus basis functions b (j) m . Implementation: All presented variations of the MFPCA estimation algorithm are implemented in an R package MFPCA (Happ, 2016b) . Univariate basis expansions include univariate FPCA (1D), smooth tensor PCA (2D), spline bases (1D/2D) and cosine bases (2D/3D). New bases can be added easily and in a modular way. The MFPCA package is based on the package funData (Happ, 2016a) for representing (multivariate) functional data on potentially different dimensional domains."}, {"section_title": "Asymptotic Properties", "text": "The results of Prop. 5 and the estimators proposed in the previous section have been derived under the assumption of a finite sample size N and a finite Karhunen-Lo\u00e8ve representation for each element X (j) . This case is relevant in practice, since data is observable only in finite form (finitely many observations, finite resolution) and hence contains only finite information. In this case, the maximal number of principal components which can be estimated is limited to the number of observations N . For a growing number of observations, the truncation limits M j and thus M + may increase with N . All asympotic examinations hence have to consider the approximation error caused by truncating the univariate Karhunen-Lo\u00e8ve representations to finite sums as well as the estimation error. For the eigenfunctions (analogously for the eigenvalues and scores) one hence has the following decomposition:\nHere \u03c8 m is the true m-th eigenfunction of the covariance operator \u0393 and\u03c8 m is the estimator based on the assumption of a finite Karhunen-Lo\u00e8ve representation in each element. This assumption is reflected in \u03c8\nm , which denotes the m-th eigenfunction of the covariance operator \u0393 [M ] associated with X [M ] with elements equal to the truncated X (j) . These are really the eigenfunctions targeted with the estimation algorithm presented in Section 3.2. The first term on the right hand side of the inequality can be seen as a bias term caused by truncation. It depends on N only implicitly via M 1 , . . . , M p . The second term accounts for the estimation error, thus can be interpreted as a variance term. \nconverge in probability to \u03c1 m for all m \u2208 N.\nIn the remainder of this section, all nonzero eigenvalues \u03bd m are assumed to have multiplicity 1, as then the eigenfunctions \u03c8 m , m = 1, . . . , M j . In the following, let X 1 , . . . , X N be independent copies of X and assume for all j = 1, . . . , p\n(A1) -(A2) concern theoretical properties of X (j) and \u0393 (j) , while (A3) -(A5) depend on the univariate decompositions used. (A1) is a standard assumption in univariate FPCA (Bosq, 2000; Hall and Hosseini-Nasab, 2006) . It guarantees that the first M j univariate eigenvalues of each element all have multiplicity 1. With (A2), the integral operator with kernel\u0108 jk (s, t) :\ni (t) converges to the one with kernel C jk (s, t) with rate N \u22121/2 . (A2) is used in combination with (A5) to obtain a convergence rate for the maximal eigenvalue of Z \u2212\u1e90, which, in turn, affects the convergence of the eigenvectors\u0109 m to c m (Yu et al., 2015) . (A3) ensures that the operator\u0393 (j) , which is the basis of the univariate FPCA, converges to \u0393 (j) in the operator norm \u00b7 op induced by ||\u00b7|| 2 with a given rate r \u0393 N . For fully observed data, Hall and Horowitz (2007) show r \u0393 N = N \u22121/2 , while the approach of Yao et al. (2005) yields r \u0393 N = N \u22121/2 h \u22122 in the case of measurement error or irregularly sampled data for a certain bandwidth h. Together with (A1), r \u0393 N gives a convergence rate for the univariate eigenfunctions\u03c6 m , as eigenfunctions are defined only up to a sign change (Bosq, 2000; Hall and Hosseini-Nasab, 2006) . Finally, (A5) is used to formulate the convergence of the estimated scores in terms of convergence rates for the estimated eigenfunctions. If this assumption does not hold (e.g. in Yao et al., 2005) , convergence results can still be obtained e.g. by assuming a convergence rate for\u03be (j) i,m and replacing (A2) by an assumption on the rate of convergence for the maximal eigenvalue of Z \u2212\u1e90.\n, the maximal eigenvalue of Z \u2212\u1e90 can be characterized by\n. Using the same notation as in Prop. 6, it holds for all m = 1, . . . , M + that\nWhen combining the results of Prop. 6 and Prop. 7, the analogy to bias and variance again becomes apparent: For fixed N , higher values of M 1 , . . . , M p will reduce the approximation error, but simultaneously increase the estimation error, as both M max and \u2206 M increase with M j . If one assumes for example\n, and that the eigengaps fulfill \u03bb Hall and Horowitz, 2007) , the MFPCA estimators given in Section 3.2 are consistent for 0 < \u03b2 < (2\u03b1 + 5) \u22121 ."}, {"section_title": "Simulation", "text": "We illustrate the performance of our new MFPCA estimation procedure in three settings with increasing complexity:\n1. Densely observed bivariate functional data on the same one-dimensional interval.\n2. Trivariate functional data on different one-dimensional intervals with different levels of sparsity.\n3. Bivariate functional data on different dimensional domains (images and functions).\nThe first two settings deal with multivariate functional data on one-dimensional domains and are presented together in Section 4.1. Setting 3 is discussed separately in Section 4.2. Examples for simulated data and estimation results for all three settings are given in the online appendix, which also includes two additional simulations (cf. Sections 3.2 and 5). Unless specified otherwise, the MFPCA package (Happ, 2016b ) is used for all calculations.\nEach setting is based on 100 datasets with N = 250 observations of the form\nIn each case, we consider data without (\u03c3 2 = 0) and with (\u03c3 2 = 0.25) measurement error. The scores \u03c1 i,m are independent samples from N(0, \u03bd m ) for eigenvalues with exponential (\u03bd m is evaluated by the mean relative squared error"}, {"section_title": "Multivariate Functional Data on One-Dimensional Domains", "text": "Setting 1: For the first setting, the first M = 8 Fourier basis functions on [0, 2] are split into p = 2 parts. The pieces are shifted and multiplied by a random sign to form the elements \u03c8 m on T 1 = T 2 = [0, 1] (for technical details, see online appendix). The observations x i are sampled on an equispaced grid of S 1 = S 2 = 100 sampling points. The MFPCA is based on M 1 = M 2 = 8 univariate functional principal components that are calculated by the PACE algorithm (Yao et al., 2005) with penalized splines to smooth the covariance function, as implemented in the R package refund (Crainiceanu et al., 2014) . In this simple setting of a common, one-dimensional domain, the new approach can be compared to the method of Ramsay and Silverman (2005) , which is implemented in the R package fda (Ramsay et al., 2014) and in the following denoted by MFPCA RS . This method involves pre-smoothing of the elements with K = 15 cubic spline basis function. MFPCA RS computes score values\u03c1 The results for the first setting are shown in Fig. 2 and Table 1 . In total, the new approach can compete very well with the existing method of Ramsay and Silverman and gives nearly identical results for synthetic and real data (see online appendix for the gait cycle example). Both techniques mostly have higher errors in \u03c8 m for linearly decreasing eigenvalues, as in these cases, the eigenfunctions are more often confused, i.e.\u03c8 m is an estimate for e.g. \u03c8 m\u22121 or \u03c8 m+1 rather than for \u03c8 m . In the ideal case of no measurement error, MFPCA RS yields lower MRSE values than the new approach, which might be an effect of MFPCA RS expecting smooth or presmoothed data. For the practically relevant case of data with measurement error, both methods give almost the same prediction errors (cf. Table 1 ). Simulations based on Legendre polynomials gave very similar results (not shown here).\nSetting 2: Here we consider trivariate functional data on\n. The eigenfunctions are constructed according to the same scheme as in setting 1 by splitting the first M = 8 Fourier basis functions on [0, 2] into p = 3 parts, followed by a shift and multiplication with a random sign. The observations are sampled on equidistant grids with S 1 = S 3 = 50 and S 2 = 100 sampling points. We consider the dense observations as well as sparse variants with medium (50 \u2212 70%) and high (90 \u2212 95% missings) sparsity. The sparsification mechanism is analogous to Yao et al. (2005) and applied to each observation and each element separately. The MFPCA is calculated in the same way as in setting 1, using the PACE approach to estimate M 1 = M 2 = M 3 = 8 functional principal components for each element. For data with high sparsity, we set M 1 = M 3 = 3 and M 2 = 5 to make computation of the univariate FPCA feasible. The results are given in Fig. 2 and Table 1 . Here there is no available competitor. The performance of our MFPCA for full data is very similar to the simpler case of setting 1. Even for a moderate level of sparsity, the new method yields excellent results for most eigenvalues and eigenfunctions at the expense of somewhat higher reconstruction errors. For very sparse data, the leading eigenvalues and eigenfunctions are still estimated well, but the reconstruction error is considerably higher than for the full data. However, this is still acceptable (average MRSE is lower than 16% for all levels of sparsity), bearing in mind that data with high sparsity contains at most 10% of the original information. Again, simulations based on Legendre polynomials gave very similar results (not shown here)."}, {"section_title": "Multivariate Functional Data Consisting of Functions and Images", "text": "Setting 3: Observations are generated based on M = 25 principal components, where the image elements \u03c8 . The elements are weighted by random factors \u03b1 1/2 and (1 \u2212 \u03b1) 1/2 , respectively, with \u03b1 \u2208 (0.2, 0.8) to ensure orthonormality. For the scores, only exponentially decreasing eigenvalues are used. The observations are discretized using a grid of S 1 = 100 \u00d7 50 equidistant points for the image element and S 2 = 200 equidistant points for the functions.\nWe consider the new MFPCA approach based on univariate FPCA as well as non-orthogonal basis functions. In the first case, the eigendecomposition for the image data is calculated with the FCP-TPA algorithm for regularized tensor decomposition (Allen, 2013) . The smoothing parameters for penalizing second differences in both image directions are chosen via generalized cross-validation in [10 \u22125 , 10 5 ] (Allen, 2013; Huang et al., 2009 ). Multivariate FPCA is calculated based on M 1 = 20 eigenimages and M 2 = 15 univariate eigenfunctions. In the case of general basis functions, image elements are expanded in tensor products of K 1 = 10\u00d712 B-splines and the one-dimensional element is represented in terms of K 2 = 15 B-spline basis functions. In the presence of measurement error the univariate expansions are fit with appropriate smoothness penalties (Eilers and Marx, 1996) . The overall results for the first M = 12 eigenvalue/eigenvector pairs are given in Fig. 3 . Compared to the settings with one-dimensional domain, the errors are slightly higher, in particular for higher order eigenvalues and eigenfunctions. Exemplary results however, show that even in this case, the new approach is still able to capture the important features of the true eigenfunctions well (see online appendix). The results further show that the general approach with spline basis functions performs mostly better than the pure MFPCA approach. Moreover, the truncated Karhunen-Lo\u00e8ve representation with M = 12 (true M = 25) estimated eigenfunctions and scores gives an excellent reconstruction of the original data. The average MRSE is 1.382%/0.398% (PCA/splines) for data without measurement error and 2.233%/2.048% (PCA/splines) for data with measurement error."}, {"section_title": "Application -ADNI Study", "text": "In this section, the new method is applied to data from the Alzheimer's Disease Neuroimaging Initiative study (ADNI), which aims at identifying biomarkers for accurate diagnosis of Alzheimer's disease (AD) in an early stage (Mueller et al., 2005) . We use MFPCA to explore how longitudinal trajectories of a neuropsychological score (ADAS-Cog, a current standard for monitoring AD progression) covary with FDG-PET scans at baseline. The latter are used to assess the glucose metabolism in the brain, which is tightly coupled with neuronal function. As the brain images might be predictive of subsequent cognitive decline, common patterns between these two sources of information would be highly relevant.\nDataset: The dataset considered for MFPCA contains data from all N = 483 participants enrolled in ADNI1, having an FDG-PET scan at baseline and at least three ADAS-Cog measurements during follow-up. At baseline, 84 subjects were diagnosed with AD, 302 were suffering from mild cognitive impairment (MCI, in many cases an early stage of AD) and 97 were cognitively healthy elderly controls. The ADAS-Cog trajectories constitute the first element X (1) , where high values indicate a high level of cognitive impairment. The measurements contain missings, mostly in the second half of the study period and thus are sparse. The second element X (2) is an axial slice of 93 \u00d7 117 pixels (139.5 \u00d7 175.5 mm 2 ) of FDG-PET scans, containing the Precuneus and temporoparietal regions. Both are believed to show a strong relation between hypometabolism (reduced brain function) and AD (Blennow et al., 2006) . Exemplary data is shown in Fig. 4 .\nWeighted scalar product: As the ADAS-Cog trajectories and FDG-PET scans differ consid- erably in domain, range and variation (cf. Fig. 4 ), we use a weighted MFPCA with\nwhere\u0108 jj is estimated from the data. Using these weights, the integrated variance equals 1 for the rescaled elementsX (j) = w 1/2 j X (j) . All elements thus contribute equal amounts of variation to the analysis, similarly to multivariate PCA, where the data is usually standardized before the analysis. We believe that this a sensible choice for many applications, but there may of course be situations, in which other weighting schemes may be preferable. For example, one could think of data that has two image elements, representing brain regions of different size for the same imaging modality. Here variability is naturally on the same scale and it might be better to keep the information of the site of the individual domains by setting both weights to one. On the other hand, if the images stem from different imaging modalities on the same domain, it might be necessary to correct solely for differences in variation. As a general rule, the weights should be chosen in close coordination with practitioners, considering the objective of the analysis and the data at hand."}, {"section_title": "Results:", "text": "The results for the first two multivariate functional principal components, that account for 80.7% of the total weighted variance, are shown in Fig. 5 . For the univariate expansions, we use FPCA for X (1) with M 1 = 3 principal components (explaining 99.2% of the univariate variance) and 20 \u00d7 15 tensor product B-splines for the images X (2) . Fig. 5 further includes pointwise bootstrap confidence bands for the principal components based on 100 nonparametric bootstrap iterations on the level of subjects. The coverage of such confidence bands for data consisting of functions and images has been analyzed in a simulation study, which gave good results, even in the presence of measurement error (see online appendix). The entire analysis for the ADNI data took around 15 minutes on a standard laptop (2.7 GHz, 16 GB RAM) including the calculation of the bootstrap confidence bands and without parallelization.\nAlmost half of the variability in the data (46.7% of the weighted variance) is explained by the first functional principal component. The ADAS-Cog element -and hence the degree of cognitive impairment -is elevated relative to the mean and increases during follow-up. The FDG-PET element exhibits hypometabolism in the Precuneus and the temporo-parietal regions, i.e. this component reflects reduced brain activity in these regions already at baseline. In total, the first eigenfunction seems to be interpretable as an AD related effect, as the pattern for positive scores perfectly agrees with medical knowledge about AD progression. This interpretation is supported by the estimated scores, which are mainly positive for people diagnosed with AD by their last visit, while scores of subjects who remained cognitively normal during follow-up are nearly all negative. Persons with MCI have intermediate score values, which is in line with the hypothesis that this diagnosis can constitute a transitional phase between normal ageing and AD.\nFor the second functional principal component (explains 33.9% of weighted variance), the ADASCog element is nearly constant and has wide bootstrap confidence bands that include zero during the whole follow-up. In contrast, the FDG-PET element differs significantly from zero in almost all voxels (cf. Fig. 5 ). Hence, this principal component reflects variation in the FDG-PET scans at baseline. Plotting the overall mean plus or minus this component suggests that it can be interpreted as an effect of imperfect registration that manifests in different brain sizes, which are known to correlate with gender (Ruigrok et al., 2014) . This hypothesis is supported by the boxplots of the estimated scores in Fig. 5 , while scores do not differ notably by diagnosis (not shown here).\nDiscussion: The results show that MFPCA is able to capture important sources of variation in the data that have a meaningful interpretation from a medical and neuroimaging point of view. An important issue not addressed here is that for ADAS-Cog, there may well be an informative dropout of patients with high score values (cf. Fig. 4 ). While addressing informative missingness goes beyond the scope of this paper, interpretation of results should take this possibility into account. For instance, it is easily conceivable that\u03c8 \u03c8 1 , 2nd row:\u03c8 2 ). Estimates are given with pointwise 95% and 90% bootstrap confidence bands based on 100 nonparametric bootstrap iterations (ADAS-Cog, 1st column: Dashed lines; FDG-PET, 2nd and 3rd column: Pixels with pointwise 95% (left) and 90% (right) confidence bands not including zero in color). Boxplots of the scores (4th column) support the interpretation."}, {"section_title": "Discussion and Outlook", "text": "This paper introduces methodology and a practical estimation algorithm for multivariate functional principal component analysis. While other methods for MFPCA are restricted to observations on a common, one-dimensional interval, the new approach is suitable for data on different domains, which may also differ in dimension, such as functions and images. The key results are 1. a Karhunen-Lo\u00e8ve Theorem, that establishes the theoretical basis for MFPCA (Prop. 4), 2. an explicit relation between multivariate and univariate FPCA, which serves as a starting point for the estimation (Prop. 5) and 3. asymptotic results for the estimators (Prop. 6 and 7). The estimation algorithm can be extended to expansions of the univariate elements in not necessarily orthonormal bases. This allows to flexibly choose an appropriate basis for each element depending on the data structure, in particular also mixtures of univariate FPCA and general bases. The algorithm is applicable to sparse data or data with measurement error, as well as to images. Notably, the proposed method can be used to calculate smooth univariate functional principal components for data on higher dimensional domains and is hence an alternative to existing methods for tensor PCA (Allen, 2013) . The results of MFPCA give insights into simultaneous variation within the data and provide a natural tool for dimension reduction. Moreover, they can be used as a building block for further statistical analyses such as functional clustering methods or functional principal component regression with multiple covariates (cf. M\u00fcller and Stadtm\u00fcller, 2005 , for the univariate case). If the elements differ in domain, range or variation, the new method can incorporate weights, which should be chosen with respect to the question of interest and the data at hand.\nPossible extensions of the approach include normalization methods as an alternative to the weighted scalar product, following the ideas in Jacques and Preda (2014) or Chiou et al. (2014) for functions observed on a common interval. However, one should take into account that the domains may have different dimensions and sizes. The concept of MFPCA could further be extended to hybrid data, i.e. data consisting of a functional and a vector part (Ramsay and Silverman, 2005, Chapter 10.3.) . A natural starting point would be to extend the scalar product suggested by Ramsay and Silverman (2005) in this context to multivariate functional data as proposed in Prop. 1. However, transferring the results for MFPCA shown in this paper requires a careful revision of the concept of the covariance operator and related proofs. Finally, one could think of estimating the multivariate covariance operator directly without computing a univariate decomposition for each element. This operator is typically high-dimensional, making smoothing as well as an eigendecomposition hardly feasible, which is avoided in our two-step approach."}, {"section_title": "Supplementary Material", "text": "The online appendix contains detailed proofs for all propositions, some additional simulation results and R code for reproducing the analysis for the ADNI and gait cycle data based on the R packages fundata and MFPCA (Happ, 2016a,b) ."}, {"section_title": "Proofs of Propositions", "text": "Proof of Prop. 1. H is a direct sum of the Hilbert spaces L 2 (T j ), j = 1, . . . , p, with natural scalar product \u00b7, \u00b7 (cf. Reed and Simon, 1980, Chapter II.1.) Proof of Prop. 2.\n1. \u0393 is linear: Follows from the linearity of the scalar product in (3).\n2. \u0393 is self-adjoint: Follows from the symmetry C ij (s i , t j ) = C ji (t j , s i )."}, {"section_title": "\u0393 is positive: Let", "text": "4. \u0393 is compact: Let B := {f \u2208 H : |||f ||| 2 \u2264 B} be a bounded family in H for some constant\n2 \u2264 B for all j = 1, . . . , p. Define the image of B under \u0393 by Z = \u0393B = {g \u2208 H : \u2203 f \u2208 B such that g = \u0393f }, which has the following properties:\n\u2022 Z is uniformly bounded: Let g \u2208 Z and t \u2208 T . Define K := max i,j=1,...,p K ij with K ij as in (5). Then\n\u2022 Z is equicontinuous: Denote by \u03bb(T j ) the Lebesgue measure of T j and let T = max j=1,...,p \u03bb(T j ). For \u03b5 > 0, define\u03b5 := \u03b5 p(pT B) 1/2 . By the continuity assumption for C ij (s i , \u00b7), there exist \u03b4 ij > 0 such that\nfor all i, j = 1, . . . , p. Set \u03b4 := min i,j=1,...,p \u03b4 ij and let ||t \u2212 t * || T < \u03b4. Clearly, t j \u2212 t * j < \u03b4 for all j = 1, . . . , p and for g \u2208 Z it holds\nBy the Theorem of Arzel\u00e0-Ascoli (Reed and Simon, 1980, Thm. I.28 . and related notes for Chapter I), for each sequence {f n } n\u2208N in B there exists a convergent subsequence {g n(i) = \u0393f n(i) } i\u2208N of the corresponding sequence {g n } n\u2208N in Z, which implies that \u0393 is a compact operator (cf. Reed and Simon, 1980 , Chapter VI.5.).\nLemma 1. For fixed m \u2208 N and j \u2208 {1, . . . , p}, the j-th element \u03c8 . By the uniform continuity assumption for C ij , there exist \u03b4 ij > 0 such that for all i = 1, . . . , p (12) holds. Let \u03b4 j = min i=1,...,p \u03b4 ij and t j \u2212 t * j < \u03b4 j . Then, as \u03bd m > 0 and \u03c8\nProof of Prop. 3. The proof follows the idea in Werner (2011, Chapter VI.4.) for the proof of Mercer's Theorem in the univariate case. From the Spectral Theorem for compact self-adjoint operators (Werner, 2011, Thm. VI.3.2.) , it is known that\n. . , p} and\n) for some n \u2208 N, where\nis a closed ball in T j with center t * and radius 1 n , \u03bb(\u00b7) denotes the Lebesgue measure and 1 is the indicator function. Clearly, f (j) \u2208 L 2 (T j ) and f \u2208 H. Therefore\nby the Lebesgue Differentiation Theorem (Rudin, 1987, Thm. 7.10.) . As t * was arbitrary in T j , this implies that for all\nsince C jj is continuous and T j is compact, implying that C jj \u221e := sup t\u2208T j |C jj (t, t)| is finite. Using H\u00f6lder's inequality\nis absolutely convergent for all s, t \u2208 T j . In the following, assume t \u2208 T j to be fixed. For \u03b5 > 0 choose M \u2208 N such that\nThen, again by H\u00f6lder's inequality\nThe upper bound in (13) does not depend on s, henceC j (s, t) converges uniformly for fixed t. As the eigenfunctions \u03c8 Let now g (j) \u2208 L 2 (T j ) and define g := 0, . . . , 0, g (j) , 0, . . . , 0 , which is clearly in H. Therefore\naccording to the Spectral Theorem. Choosing g (j) = h j implies h j (s) = 0 for all s \u2208 T j , as h j is continuous in s. Therefore,\nBy Dini's Theorem (Werner, 2011, Thm. VI.4.6 .) the series C jj (t, t) = \u221e m=1 \u03bd m \u03c8 (j) m (t) 2 converges uniformly. Hence, M can be chosen independent of t in (13). This implies thatC j (s, t) converges absolutely and uniformly to C jj (s, t) for all s, t \u2208 T j . (Reed and Simon, 1980, Thm. VI.16.) , the (deterministic) eigenfunctions of \u0393 form an orthonormal basis of H, i.e. X can be written in the form X(t) = \u221e m=1 \u03c1 m \u03c8 m (t), t \u2208 T with random variables \u03c1 m = X, \u03c8 m . Hence for m, n \u2208 N"}, {"section_title": "Proof of Prop. 4. By the Hilbert-Schmidt Theorem", "text": "Proof of Prop. 5.\n1. Let X have a finite Karhunen-Lo\u00e8ve representation (7). Then, each element is given by\nwhich is a homogenous Fredholm integral equation of the second kind with separable ker-\nm (t) (cf. Lemma 1). Following the argumentation in Zemyan (2012, Chapter 1.3.) , (14) can be transformed into the matrix eigenequation\nwith a symmetric matrix A (j) \u2208 R M \u00d7M given by A \nTherefore, X (j) has a finite Karhunen-Lo\u00e8ve representation\nm with scores\n2. Assume the functional covariates X (1) , . . . , X (p) do each have a finite Karhunen-Lo\u00e8ve representation, i.e. for each j = 1, . . . , p :\nWith a similar argumentation as in Zemyan (2012, Chapter 1.3.) it holds\nfor m = 1, . . . , M j due to orthonormality of \u03c6 (j) m . Since m and j were arbitrarily chosen, this is equivalent to \uf8eb \uf8ec \uf8ed \nfor m = 1, . . . , M, j = 1, . . . , p. The eigenfunctions form an orthonormal system with respect to \u00b7, \u00b7 :\nThe Karhunen-Lo\u00e8ve decomposition of X is therefore given by X = M m=1 \u03c1 m \u03c8 m with scores\nProof of Prop. 6. For f \u2208 H, t \u2208 T and j = 1, . . . , p, the covariance operator \u0393 [M ] associated with\nIn the following, use C\n(j) (t j )) as short notation for the covariance functions (cf. the definition of C ij in (1) in the paper). Next, recall some well-known results for univariate functional data: By Mercer's Theorem (Mercer, 1909) \nThe univariate Karhunen-Lo\u00e8ve Theorem (e.g. Bosq, 2000, Thm 1.5.) states that\nconverges uniformly to 0 for t j \u2208 T j and M j \u2192 \u221e. As both X (j) and X [M ](j) have zero mean (X (j) by assumption and\nWith the assumptions of Prop. 2 it further holds (cf. proof of Prop. 3) that\nFor fixed s i \u2208 T i , t j \u2208 T j with i, j = 1, . . . , p, these three properties give\nHence it holds that C\nThe main proof is now in three steps:\n1. \u0393 [M ] converges in norm to \u0393 for M 1 , . . . , M p \u2192 \u221e: Let |||\u00b7||| op be the operator norm induced by |||\u00b7|||. Then\nwhere the last equality holds since f (i) 2 \u2264 1 for all f \u2208 H with |||f ||| = 1. \nThis upper bound is constant and therefore integrable over T i , which implies\nFor the outer integral in (20) the results of the norm term give\nwhere \u03bb(T i ) is the Lebesuge measure of T i as in the Proof of Prop. 2. The term on the right hand side is constant and hence integrable over T j , which gives that for M 1 , . . . , M p \u2192 \u221e, the limit M j \u2192 \u221e and the integral over T j in (20) can be interchanged. In summary, these results give that \u0393 [M ] converges to \u0393 in norm for M 1 , . . . , M p \u2192 \u221e."}, {"section_title": "\u0393 [M ]", "text": "is bounded: Let f \u2208 H. Clearly, f (i) 2 \u2264 |||f ||| for all i = 1, . . . , p and therefore\nfor T = max j=1,...,p \u03bb(T j ) and C = max j=1,...,p C jj \u221e . The value p 3/2 CT is constant and finite, hence \u0393 [M ] is bounded."}, {"section_title": "Convergence results for \u03bd", "text": "m : In Prop. 2, it was shown that \u0393 is compact, which implies that this operator is also bounded (Reed and Simon, 1980, Chapter VI.5.) . As \u0393 and \u0393 [M ] are both bounded, norm convergence is equivalent to convergence in the generalized sense (Kato, 1976, Chapter IV, \u00a72.6., Thm. 2.23) . This implies that the eigenvalues \u03bd [M ] m of \u0393 [M ] converge to the eigenvalues \u03bd m of \u0393 including multiplicity (if the multiplicity is finite, which holds for all nonzero eigenvalues, as \u0393 is compact (cf. Reed and Simon, 1980, Thm. VI.15.) ) and the associated total projections converge in norm (Kato, 1976, Chapter IV, \u00a73.5.) . If the m-th eigenvalue has multiplicity 1, then the projections on the eigenspaces spanned by \u03c8 m and \u03c8 [M ] m , respectively, are given by\nWithout loss of generality one may choose the orientation of \u03c8 m and \u03c8\nm both have norm 1,\nNorm convergence of the total projections hence implies \u03c8 m \u2212 \u03c8\nTo derive convergence of the scores \u03c1 [M ] m , note that for \u03b5 > 0 and c :=\ni.e. the norm of X is bounded in probability. Moreover,\n. . , M p \u2192 \u221e, as the expectation in the integral converges uniformly to 0 and is thus bounded (by univariate Karhunen-Lo\u00e8ve). As T j has finite measure, the overall integral converges to 0. Hence X \u2212 X [M ] converges in the second mean to 0, thus\n. Finally, this leads to\ni.e. \u03c1 [M ] m converges in probability to \u03c1 m .\nLemma 2. Under the assumptions of Prop. 7 it holds that\nwith Z as defined in Prop. 5,\u1e90 = (N \u2212 1) \u22121 \u039e \u039e as in Section 3.2, M max = max j=1,...,p M j and\nIf the X i are independent copies of the process X, it holds\nThe last step follows from the fact that the integral term does not depend on N and is finite by assumption (A2) and the conditions in Prop. 2 for C jk . This implies\ni,n for j, k = 1, . . . , p, l = 1, . . . , M j , n = 1, . . . , M k . As Z and\u1e90 are both symmetric matrices in R M + \u00d7M + it holds (cf. Horn and Johnson, 1991 , Chapter 3.7)\nLet now j, k = 1, . . . , p, l = 1, . . . , M j , n = 1, . . . , M k be fixed. Assumption (A5) gives\nThe rate for \u03c6 (k) n and \u03c6 (j) l in the last steps is shown at the beginning of the proof of Prop. 7. In total, equation (22)\nProof of Prop. 7. Under assumption (A4) and using the convention \u03bb \nBased on this result, Lemma 2 states that\nas the expression in square brackets converges to a constant C < \u221e (cf. Prop. 6 and the fact that \u03bd m is assumed to have multiplicity 1, see p. 18 in the main paper). Here \u03bb max (A) denotes the maximal eigenvalue of a symmetric matrix A. The second inequality follows from Corollary 1 in Yu et al. (2015) and the fact that \u03bd\n2. Eigenfunctions: Consider the j-th element of the m-th eigenfunctions:\nwhere the last inequality uses again Corollary 1 in Yu et al. (2015) . By definition of the norm, the result for the single elements implies"}, {"section_title": "Scores and reconstructedX: For\u03c1", "text": ",\u03c8 m as in Section 3.2 withX\nis bounded in probability using equation (16) m ). For the second term, note that\nThe univariate scores are uniformly bounded in probability: For m = 1, . . . , M j , let \u03b5 > 0 and\nIn total,"}, {"section_title": "Simulation -Additional Results", "text": ""}, {"section_title": "Construction of Eigenfunctions (Technical Details)", "text": "Setting 1 and 2: The first two settings of the simulation study consider multivariate functional data where each element has a one-dimensional domain (cf. Section 4.1). As a starting point for the construction of the multivariate eigenfunctions \u03c8 m with p elements, we use Fourier basis functions f 1 , . . . , f M on the interval [0, 2]. Next, choose split points 0 = T 1 < T 2 < . . . < T p < T p+1 = 2 and shift values \u03b7 1 , . . . , \u03b7 p \u2208 R such that T j = [T j + \u03b7 j , T j+1 + \u03b7 j ]. In the first setting with p = 2, one has T 1 = 0, T 2 = 1, T 3 = 2 and \u03b7 1 = 0, \u03b7 2 = 1, i.e. the functions are cut at T 2 = 1, and the second part is shifted to the left by 1 such that T 1 = T 2 = [0, 1]. Given random signs \u03c3 1 , . . . , \u03c3 p \u2208 {\u22121, 1}, the multivariate eigenfunctions are given by their elements\nThe constuction process is illustrated in Fig. 6 . Clearly,\nThe observations x i for the simulation are constructed as a truncated Karhunen-Lo\u00e8ve expansion, cf. the introduction of Section 4. Exemplary data for the second simulation setting including sparse data and data with measurement error is given in Fig. 7 \ni . Solid lines show the realizations x i , small points are the corresponding data with measurement error, big points mark measurements of the artificially sparsified data (high sparsity level).\nSetting 3: The data in the third setting consists of images and functions, hence multivariate functional data with elements having different dimensional domains (cf. Sectrion 4.2). The basic idea here is to find orthonormal bases for each of the domains and to construct the eigenfunctions as weighted combinations of those bases. Specifically, we use five Fourier basis functions f \nwith a random weight \u03b1 \u2208 (0, 1). This choice implies that \u03c8 m forms an orthonormal system in\n. In order to avoid extreme weights, \u03b1 is set to u 1 /(u 1 + u 2 ) with u 1 , u 2 \u223c U (0.2, 0.8). This construction restricts \u03b1 \u2208 (0.2, 0.8) and can easily be generalized to the simulation of multivariate functional data with p elements. Example data based on this type of eigenfunctions is shown in Fig. 8 . \ni , right) without (1st row) and with measurement error (2nd row). Table 2 . "}, {"section_title": "Example Fits", "text": ""}, {"section_title": "Sensitivity Analysis", "text": "As discussed in Section 3.2, the number M j of univariate eigenfunctions used for MFPCA clearly has an impact on the results, as they control how much of the information in the univariate elements is used for calculating the multivariate FPCA. A standard approach in functional data analysis for quantifying the amount of information contributed by single eigenfunctions \u03c6 (j) m is the percentage of variance explained (pve), which is the ratio of the associated eigenvalue \u03bb (j) m and the sum of all eigenvalues. The following simulation systematically examines the sensitivity of the MFPCA result based on the pve of the univariate eigenfunctions.\nSimulation Setup: The simulation is based on 100 replications with N = 250 observations of bivariate data on the unit interval (cf. setting 1 in Section 4.1), with M = 8 Fourier basis functions and exponentially decreasing eigenvalues for simulating the data. The number of univariate eigenfunctions M 1 , M 2 for MFPCA is chosen based on pve \u2208 {0.75, 0.90, 0.95, 0.99} for both elements and M 1 = M 2 = M = 8 for comparison. The number of multivariate principal component functions is then set to min{M 1 + M 2 , M }.\nResults: The results of the sensitivity analysis are shown in Fig. 12 and Table 3 . The number of estimated multivariate eigenvalues/eigenfunctions is for all 100 datasetsM = 4 for pve = 0.75, M = 6 for pve = 0.90 andM = 8 in all other cases. The results are as expected: Increasing the pve, and hence the information in the univariate FPCA, improves the estimation accuracy for both, multivariate eigenvalues and eigenfunctions. As a consequence, the reconstruction error reduces with increasing pve. Moreover, for a fixed m, the results show that there is a critical amount of information in univariate FPCA that is needed to describe the multivariate eigenvalues and eigenfunctions well. If this is reached (e.g. pve = 0.95 for m = 5, cf. Fig. 12 ), the additional benefit of using more univariate eigenfunctions (pve > 0.95) becomes negligible. If, in contrast, the univariate FPCA does not contain enough information (pve < 0.95), the error rates for the MFPCA estimates are considerably increased. For fixed pve, the error rates rise abruptly for the last pair of eigenfunctions (m \u2208 {M + \u2212 1,M + }). This is due to the fact that in this simulation, the multivariate functional principal components are derived from a Fourier basis. The last two eigenfunctions are hence sine and cosine functions with highest frequency and cannot be represented well by the univariate functions used, as they contain only functions with lower frequency, in other words, they do not contain enough information. In Section 5, pointwise bootstrap confidence bands were calculated for the multivariate functional principal components estimated from the ADNI data to quantify the variability in the estimates.\nThe following simulation study examines the coverage properties of such confidence bands. Simulation Setup: The data generating process is the same as in the simulation in Section 4.2, mimicking the ADNI data that consists of functions on a one-dimensional domain and images. In total, the simulation is based on 100 datasets, all having N = 250 observations. Each dataset is considered with and without measurement error. Both elements are represented in terms of Bspline basis functions with appropriate smoothness penalties in the presence of measurement error (cf. Section 4.2). For each dataset and each estimated eigenfunction, a pointwise 95% bootstrap confidence band is calculated based on 100 bootstrap samples on the level of subjects (cf. Section 5). The coefficients of the spline basis decompositions can efficiently be reused when bootstrapping, as the basis is fixed and does not depend on the bootstrap sample. In contrast, the univariate functional principal components for the ADAS-Cog trajectories in the ADNI application have to be re-estimated for each bootstrap sample. This computational aspect is taken into account in the bootstrap implementation in the MFPCA package (Happ, 2016b) . Finally, the confidence bands are calculated separately for each element as pointwise percentile bootstrap confidence intervals. For each eigenfunction and each observation point, the estimated coverage at one point t j \u2208 T j is the percentage of datasets for which the true eigenfunction \u03c8 (j) m evaluated at t j is enclosed in the bootstrap confidence band (up to a sign change of the whole function). Fig. 13 shows the estimated coverages of the elements of the eigenfunctions for data with and without measurement error aggregated over the observation points.\nResults: If the data is observed without measurement error, the pointwise confidence bands enclose the true functions fairly precisely in 95% of all cases with very little variation between the observation points. For the leading eigenfunctions, the same holds true if the data is observed with measurement error. For higher order eigenfunctions, that explain hardly any variation in the data, the estimated coverage decreases, especially for the second element (\u03c8 (2) m , one-dimensional domain) and shows a much higher variation between the observation points. On the one hand this may be caused by the fact that the true eigenfunctions \u03c8 (2) m have a stronger curvature for growing m (cf. Fig. 11 ). Severe undercoverage for higher-order eigenfunctions occurs mainly in regions of high curvature and slope of the eigenfunctions, where the low signal-to-noise level leads to oversmoothing (cf. Fig. 14) . On the other hand, the results of Section 4.2 show that the estimates for higher order eigenfunction elements become more inaccurate due to interchanging of eigenfunctions, hence the bootstrap confidence bands can be centered incorrectly. For the image elements \u03c8 (1) m , the bootstrapped confidence bands give much better results, except for some outliers that form spatially smooth outlying regions (see e.g. Fig. 14) . This reflects that the pointwise coverages are not independent, as the true eigenfunctions as well as the confidence bands are smooth: If the function \u03c8 (j) m lies within the bootstrap confidence band at a point t j , it is very likely that it will also be inside the confidence band at the neighbouring observation points (analogously for points outside the CI). This relation is highlighted in Fig. 14 , which illustrates the coverage rates for \u03c8 3 (having a good coverage) and \u03c8 9 (having a rather poor coverage) in the case of measurement error. In summary, the results of the simulation show that the bootstrapped confidence bands give reliable results, in particular for the leading eigenfunctions that explain most of the variation in the data. Moreover, smooth eigenfunctions will have a stabilizing effect for the coverage. However, when interpreting such pointwise confidence bands, one should keep in mind the dependence across neighbouring observation points due to the smoothness of the eigenfunctions. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q "}, {"section_title": "Applications -Gait Cycle Data", "text": "For comparison to an existing method in the special case of densely sampled bivariate data on the same one-dimensional interval, the new MFPCA approach is applied to the gait cycle data (cf. Fig. 1 in the main document) and compared to the method of Ramsay and Silverman (2005) as implemented in the R-package fda (Ramsay et al., 2014) . The results are shown in Fig. 15 . For the new approach, the multivariate principal components are calculated based on univariate FPCA with M 1 = M 2 = 5 principal components. For MFPCA RS , the observed functions are pre-smoothed using K = 15 cubic spline basis functions as in the simulation study (cf. Section 4.1). As for synthetic data, the two methods give nearly identical results. "}]