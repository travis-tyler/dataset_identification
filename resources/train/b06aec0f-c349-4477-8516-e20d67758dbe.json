[{"section_title": "", "text": "\u2022 On the reflect and evaluate reading literacy subscale, U.S. 15-year-olds had a higher average score than the OECD average. The U.S. average was lower than that of 5 OECD countries and higher than that of 23 OECD countries; it was lower than that of 8 countries and other education systems and higher than that of 51 countries and other education systems overall. On the other two subscales-access and retrieve and integrate and interpret-the U.S. average was not measurably different from the OECD average. \u2022 In reading literacy, 30 percent of U.S. students scored at or above proficiency level 4. Level 4 is the level at which students are \"capable of difficult reading tasks, such as locating embedded information, construing meaning from nuances of language and critically evaluating a text\" (OECD 2010a, p. 51). At levels 5 and 6 students demonstrate higher-level reading skills and may be referred to as \"top performers\" in reading. There was no measurable difference between the percentage of U.S. students and the percentage of students in the OECD countries on average who performed at or above level 4. \u2022 Eighteen percent of U.S. students scored below level 2 in reading literacy. Students performing below level 2 in reading literacy are below what OECD calls \"a baseline level of proficiency, at which students begin to demonstrate the reading literacy competencies that will enable them to participate effectively and productively in life\" (OECD 2010a, p. 52). There was no measurable difference between the percentage of U.S. students and the percentage of students in the OECD countries on average who demonstrated proficiency below level 2. \u2022 Female students scored higher, on average, than male students on the combined reading literacy scale in all 65 participating countries and other education systems. In the United States, the difference was smaller than the difference in the OECD countries, on average, and smaller than the differences in 45 countries and other education systems (24 OECD countries and 21 non-OECD countries and other education systems). \u2022 On the combined reading literacy scale, White (non-Hispanic) and Asian (non-Hispanic) students had higher average scores than the overall OECD and U.S. average scores, while Black (non-Hispanic) and Hispanic students had lower average scores than the overall OECD and U.S. average scores. The average scores of students who reported two or more races were not measurably different from the overall OECD or U.S. average scores. \u2022 Students in public schools in which half or more of students (50 to 74.9 percent and 75 percent or more) were eligible for free or reduced-price lunch (FRPLeligible) scored, on average, below the overall OECD and U.S. average scores in reading literacy. Students in Executive Summary schools in which less than 25 percent of students were FRPL-eligible (10 to 24.9 percent and less than 10 percent) scored, on average, above the overall OECD and U.S. average scores. The average scores of students in schools in which 25 to 49.9 percent were FRPLeligible were above the overall OECD average but not measurably different from the U.S. average. \u2022 There was no measurable difference between the average score of U.S. students in reading literacy in 2000, the last time in which reading literacy was the major domain assessed in PISA, and 2009PISA, and , or between 2003PISA, and and 2009. There also were no measurable differences between the U.S. average score and the OECD average score in 2000 or in 2009. 3 Mathematics Literacy \u2022 U.S. 15-year-olds had an average score of 487 on the mathematics literacy scale, which was lower than the OECD average score of 496. Among the 33 other OECD countries, 17 countries had higher average scores than the United States, 5 had lower average scores, and 11 had average scores not measurably different from the U.S. average. Among the 64 other OECD countries, non-OECD countries, and other education systems, 23 had higher average scores than the United States, 29 had lower average scores, and 12 had average scores not measurably different from the U.S. average score. \u2022 In mathematics literacy, 27 percent of U.S. students scored at or above proficiency level 4. This is lower than the 32 percent of students in the OECD countries on average that scored at or above level 4. Level 4 is the level at which students can complete higher order tasks such as \"solv [ing] problems that involve visual and spatial reasoning...in unfamiliar contexts\" and \"carry[ing] out sequential processes\" (OECD 2004, p. 55). Twenty-three percent of U.S. students scored below level 2. There was no measurable difference between the percentage of U.S. students and the percentage of students in the OECD countries on average demonstrating proficiency below level 2, what OECD calls a \"a baseline level of mathematics proficiency on the PISA scale at which students begin to demonstrate the kind of literacy skills that enable them to actively use mathematics\" (OECD 2004, p. 56). \u2022 The U.S. average score in mathematics literacy in 2009 was higher than the U.S. average in 2006 but not measurably different from the U.S. average in 2003, the earliest time point to which PISA 2009 performance can be compared in mathematics literacy. U.S. students' average scores were lower than the OECD average scores in each of these years. 4"}, {"section_title": "Science Literacy", "text": "\u2022 On the science literacy scale, the average score of U.S. students (502) was not measurably different from the OECD average (501). Among the 33 other OECD countries, 12 had higher average scores than the United States, 9 had lower average scores, and 12 had average scores that were not measurably different. Among the 64 other OECD countries, non-OECD countries, and other education systems, 18 had higher average scores, 33 had lower average scores, and 13 had average scores that were not measurably different from the U.S. average score. \u2022 Twenty-nine percent of U.S. students and students in the OECD countries on average scored at or above level 4 on the science literacy scale. Level 4 is the level at which students can complete higher order tasks such as \"select [ing] and integrat [ing] explanations from different disciplines of science or technology and link [ing] those explanations directly to...life situations\" (OECD 2007, p. 43). Eighteen percent of U.S. students and students in the OECD countries on average scored below level 2. Students performing below level 2 are below what OECD calls a \"baseline level of proficiency\u2026at which students begin to demonstrate the science competencies that will enable them to participate effectively and productively in life situations related to science and technology\" (OECD 2007, p. 44). There were no measurable differences between the percentages of U.S. students and students in the OECD countries on average that scored at the individual proficiency levels. \u2022 The U.S. average score in science literacy in 2009 List of Tables   Table  Page   1. Participation in PISA, by country: 2000PISA, by country: , 2003PISA, by country: , 2006PISA, by country: , and 2009 2. Percentage distribution of U.S. 15-year-old students, by grade level: 2009 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3. Average scores of 15-year-old students on combined reading literacy scale and reading literacy subscales, by country: 2009 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4. Average scores of 15-year-old female and male students on combined reading literacy scale, by country  T he Program for International Student Assessment (PISA) is an international assessment that measures the performance of 15-year-olds in reading literacy, mathematics literacy, and science literacy. Coordinated by the Organization for Economic Cooperation and Development (OECD), an intergovernmental organization of 34 member countries, PISA was first implemented in 2000 and is conducted every 3 years. PISA 2009 was the fourth cycle of the assessment. Each PISA data collection effort assesses one of the three subject areas in depth (considered the major subject area), although all three are assessed in each cycle (the other two subjects are considered minor subject areas for that assessment year). Assessing all three areas allows participating countries to have an ongoing source of achievement data in every subject area while rotating one area as the main focus over the years. In the fourth cycle of PISA, reading was the subject area assessed in depth, as it was in 2000 (figure 1). Sixty countries and 5 other education systems 1 participated as partners in PISA 2009 (figure 2 and table 1). This report focuses on the performance of U.S. students in the major subject area of reading literacy as assessed in PISA 2009. Achievement results for the minor subject areas of mathematics and science literacy in 2009 are also presented. Introduction Table 1. Participation in PISA, by country: 2000, 2003, 2006, and 2009   Country  2000  2003  2006  2009  Country  2000  2003  2006  2009  OECD "}, {"section_title": "Figure 1. PISA administration cycle", "text": "The Republics of Montenegro and Serbia were a united jurisdiction under the PISA 2003 assessment. NOTE: A \"\u2022\" indicates that the country participated in the Program for International Student Assessment (PISA) in the specific year. Because PISA is principally an Organization for Economic Cooperation and Development (OECD) study, non-OECD countries are displayed separately from the OECD countries. Eleven countries and other education systems-Albania, Argentina, Bulgaria, Chile, Hong Kong-China, Indonesia, Israel, Macedonia, Peru, Romania, andThailand-administered PISA 2000 in 2001. Italics indicate non-national entities. UAE refers to the United Arab Emirates. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2000(PISA), , 2003(PISA), , 2006(PISA), , and 2009."}, {"section_title": "Introduction", "text": "What PISA Measures P ISA assesses the application of knowledge in reading, mathematics, and science literacy to problems within a real-life context (OECD 1999). PISA uses the term \"literacy\" in each subject area to denote its broad focus on the application of knowledge and skills. For example, when assessing reading, PISA assesses how well 15-year-old students can understand, use, and reflect on written text for a variety of purposes and settings. In science, PISA assesses how well students can apply scientific knowledge and skills to a range of different situations they may encounter in their lives. Likewise, in mathematics, PISA assesses how well students analyze, reason, and interpret mathematical problems in a variety of situations. Scores on the PISA scales represent skill levels along a continuum of literacy skills. PISA provides ranges of proficiency levels associated with scores that describe what a student can typically do at each level (OECD 2006). The assessment of 15-year-old students allows countries to compare outcomes of learning as students near the end of compulsory schooling. PISA's goal is to answer the question \"What knowledge and skills do students have at age 15?\" In this way, PISA's achievement scores represent a \"yield\" of learning at age 15, rather than a direct measure of attained curriculum knowledge at a particular grade level. Fifteenyear-old students participating in PISA from the United States and other countries are drawn from a range of grade levels. Sixty-nine percent of the U.S. students were enrolled in grade 10, and another 20 percent were enrolled in grade 11 (table 2). In addition to participating in PISA, the United States has for many years conducted assessments of student achievement at a variety of grade levels and in a variety of subject areas through the National Assessment of Educational Progress (NAEP), the Trends in International Mathematics and Science Study (TIMSS), and the Progress in International Reading Literacy Study (PIRLS). These studies differ from PISA in terms of their purpose and design (see appendix D). NAEP reports information on the achievement of U.S. students using nationally established benchmarks of performance (i.e., basic, proficient, and advanced), based on the collaborative input of a wide range of experts and participants from government, education, business, and public sectors in the United States. Furthermore, the information is used to monitor progress in achievement over time, specific to U.S. students. To provide a critical external perspective on the mathematics, science, and reading achievement of U.S. students, the United States participates in PISA as well as TIMSS and PIRLS. TIMSS provides the United States with information on the mathematics and science achievement of 4th-and 8th-grade U.S. students compared to students in other countries. PIRLS allows the United States to make international comparisons of the reading achievement of students in the fourth grade. TIMSS and PIRLS seek to measure students' mastery of specific knowledge, skills, and concepts and are designed to broadly reflect curricula in the United States and other participating countries; in contrast, PISA does not focus explicitly on curricular outcomes but rather on the application of knowledge to problems in a real-life context. PISA 2009 was a 2-hour paper-and-pencil assessment of 15-year-olds collected from nationally representative samples of students in participating countries. 3 Like other large-scale assessments, PISA was not designed to provide individual student scores, but rather national and group estimates of performance. In PISA 2009, although each student was administered one test booklet, there were 13 test booklets in total. Each test booklet included either reading items only; reading and mathematics items; 2 The other members of the PISA Consortium are Analyse des syst\u00e8mes et des pratiques d'enseignement (aSPe, Belgium), cApStAn Linguistic Quality Control (Belgium), the German Institute for International Educational Research (DIPF), Educational Testing Service (ETS, United States), Institutt for Laererutdanning og Skoleu tvikling (ILS, Norway), Leibniz Institute for Science and Mathematics Education (IPN, Germany), the National Institute for Educational Policy Research (NIER, Japan), CRP Henri Tudor and Universit\u00e9 de Luxembourg -EMACS (Luxembourg), and Westat (United States). 3 Some countries also administered the PISA Electronic Reading Assessment, which was analyzed and reported separately from the paper-and-pencil assessment. The United States did not administer this optional component. reading and science items; or reading, mathematics, and science items. As such, all students answered reading items, but not every student answered mathematics and science items (for more information on the PISA 2009 design, see the technical notes in appendix B). PISA 2009 was administered in the United States between September and November 2009. The U.S. sample included both public and private schools, randomly selected and weighted to be representative of the nation. 4 In total, 165 schools and 5,233 students participated in PISA 2009 in the United States. The overall weighted school response rate was 68 percent before the use of replacement schools and 78 percent after the addition of replacement schools. The final weighted student response rate was 87 percent (see the technical notes in appendix B for additional details on sampling, administration, response rates, and other issues). This report provides results for the United States in relation to the other countries participating in PISA 2009, distinguishing OECD countries and non-OECD countries and other education systems. Differences described in this report have been tested for statistical significance at the .05 level, with no adjustments for multiple comparisons. Additional information on the statistical procedures used in this report is provided in the technical notes in appendix B. For further results from PISA 2009, see the OECD publications PISA 2009 Results (Volumes I-V) (OECD 2010a(OECD , 2010b(OECD , 2010c(OECD , 2010d and the NCES website at http://nces.ed.gov/surveys/pisa."}, {"section_title": "U.S. Performance in Reading Literacy", "text": "\nPISA's major focus in 2009 was reading literacy, which is defined as follows: Reading literacy is understanding, using, reflecting on and engaging with written texts, in order to achieve one's goals, to develop one's knowledge and potential, and to participate in society (OECD 2009, p. 23). In assessing students' reading literacy, PISA measures the extent to which students can construct, extend, and reflect on the meaning of what they have read across a wide variety of texts associated with a wide variety of situations. The PISA reading literacy assessment is built on three major task characteristics: \"situation -the range of broad contexts or purposes for which reading takes place; textthe range of material that is read; and aspect -the cognitive approach that determines how readers engage with a text\" (OECD 2009, p. 25). Text types include prose texts (such as stories, articles, and manuals) and noncontinuous texts (such as forms and advertisements) that reflect various uses or situations for which texts were constructed or the context in which knowledge and skills are applied. Reading aspects, or processes, include retrieving information; forming a broad understanding; developing an interpretation; reflecting on and evaluating the content of a text; and reflecting on and evaluating the form of a text. Sample reading literacy tasks are shown in appendix A. Since reading literacy was the major subject area for the 2009 cycle of PISA, results are shown for the combined reading literacy scale, as well as for the three reading literacy subscales that reflect the reading aspects or processes: accessing and retrieving information, integrating and interpreting, and reflecting and evaluating. Scores on the reading literacy scale (combined and subscales) range from 0 to 1,000. 5 5 The reading literacy scale was established in PISA 2000 to have a mean of 500 and a standard deviation of 100. The combined reading literacy scale is made up of all items in the three subscales. However, the combined reading scale and the three subscales are each computed separately through Item Response Theory (IRT) models. Therefore, the combined reading scale score is not the average of the three subscale scores.\nLevel 5   626At level 5, tasks involve retrieving information that require the reader to locate and organize several pieces of deeply embedded information, inferring which information in the text is relevant. Reflective tasks require critical evaluation or hypothesis, drawing on specialized knowledge. Both interpretative and reflective tasks require a full and detailed understanding of a text whose content or form is unfamiliar. For all aspects of reading, tasks at this level typically involve dealing with concepts that are contrary to expectations. Level 4   553At level 4, tasks involve retrieving information that require the reader to locate and organize several pieces of embedded information. Some tasks at this level require interpreting the meaning of nuances of language in a section of text by taking into account the text as a whole. Other interpretative tasks require understanding and applying categories in an unfamiliar context. Reflective tasks at this level require readers to use formal or public knowledge to hypothesize about or critically evaluate a text. Readers must demonstrate an accurate understanding of long or complex texts whose content or form may be unfamiliar. Level 3   480At level 3, tasks require the reader to locate, and in some cases recognize the relationship between, several pieces of information that must meet multiple conditions. Interpretative tasks at this level require the reader to integrate several parts of a text in order to identify a main idea, understand a relationship or construe the meaning of a word or phrase. They need to take into account many features in comparing, contrasting or categorizing. Often the required information is not prominent or there is much competing information; or there are other text obstacles, such as ideas that are contrary to expectation or negatively worded. Reflective tasks at this level may require connections, comparisons, and explanations, or they may require the reader to evaluate a feature of the text. Some reflective tasks require readers to demonstrate a fine understanding of the text in relation to familiar, everyday knowledge. Other tasks do not require detailed text comprehension but require the reader to draw on less common knowledge.\nIn reading literacy, 30 percent 6 of U.S. students scored at or above proficiency level 4, that is, at levels 4, 5, or 6, as shown in figure 3. Level 4 is the level at which students are \"capable of difficult reading tasks, such as locating embedded information, construing meaning from nuances of language and critically evaluating a text\" (OECD 2010a, p. 51). At levels 5 and students demonstrate higher-level reading skills and may be referred to as \"top performers\" in reading. While there was no measurable difference between the percentage of U.S. students and the percentage of students in the OECD countries on average who performed at or above level 4, a higher percentage of U.S. students performed at level 5 than the OECD average (8 versus 7 percent). In comparison to the United States, 7 OECD countries and 3 non-OECD countries and other education systems had higher percentages of students who performed at or above level 4 in reading literacy; 14 OECD countries and 27 non-OECD countries and other education systems had lower percentages of students who performed at or above level 4; and for 12 OECD countries and 1 non-OECD country, there were no measurable differences in the percentages of students who performed at or above level 4 (data shown in table R7A at http://nces. ed.gov/surveys/pisa/pisa2009tablefigureexhibit.asp). Eighteen percent of U.S. students scored below level 2 (that is, at levels 1a or 1b or below 1b). Students performing below level 2 are below what OECD calls \"a baseline level of proficiency, at which students begin to demonstrate the reading literacy competencies that will enable them to participate effectively and productively in life\" (OECD 2010a, p. 52). Students performing at levels 1a and 1b are able to perform only the least complex reading tasks on the PISA assessment such as locating explicitly stated information in the text and making simple connections between text and common knowledge (level 1a) or doing so in simple texts (level 1b), as described in exhibit 1. Students below level 1b are not able to routinely perform these tasks; this does not mean that they have no literacy skills but the PISA assessment cannot accurately characterize their skills. There was no measurable difference between the percentage of U.S. students and the percentage of students in the OECD countries on average demonstrating proficiency below level 2."}, {"section_title": "Performance of Students Overall", "text": "U.S. 15-year-olds had an average score of 500 on the combined reading literacy scale, not measurably different from the average score of 493 for the 34 OECD countries (table 3). Among the 33 other OECD countries, 6 countries had higher average scores than the United States, 13 had lower average scores, and 14 had average scores not measurably different from the U.S. average. Among the 64 other OECD countries, non-OECD countries, and other education systems, 9 had higher average scores than the United States, 39 had lower average scores, and 16 had average scores not measurably different from the U.S. average. On the reflect and evaluate subscale, U.S. 15-year-olds had a higher average score than the OECD average (512 versus 494). The U.S. average was lower than that of 5 OECD countries and higher than that of 23 OECD countries; it was lower than that of 8 countries and other education systems and higher than that of 51 countries and other education systems overall. On the other two subscalesaccess and retrieve and integrate and interpret-the U.S. average was not measurably different from the OECD average (492 versus 495 and 495 versus 493, respectively).\nU.S. 15-year-olds had an average score of 487 on the mathematics literacy scale, which was lower than the OECD average score of 496 (table 7). 9 Among the 33 other OECD countries, 17 countries had higher average scores than the United States, 5 had lower average scores, and 11 had average scores not measurably different from the U.S. average. Among the 64 other OECD countries, non-OECD countries, and other education systems, 23 had higher average scores than the United States, 29 had lower average scores, and 12 had average scores not measurably different from the U.S. average score. At level 6, students can conceptualize, generalize, and utilize information based on their investigations and modeling of complex problem situations. They can link different information sources and representations and flexibly translate among them. Students at this level are capable of advanced mathematical thinking and reasoning. These students can apply this insight and understandings along with a mastery of symbolic and formal mathematical operations and relationships to develop new approaches and strategies for attacking novel situations. Students at this level can formulate and precisely communicate their actions and reflections regarding their findings, interpretations, arguments, and the appropriateness of these to the original situations.\nOn the science literacy scale, the average score of U.S. students (502) was not measurably different from the OECD average (501) (table 8). 11 Among the 33 other OECD countries, 12 had higher average scores than the United States, 9 had lower average scores, and 12 had average scores that were not measurably different. Among the 64 other OECD countries, non-OECD countries, and other education systems, 18 had higher average scores, 33 had lower average scores, and 13 had average scores that were not measurably different from the U.S. average score. At level 6, students can consistently identify, explain and apply scientific knowledge and knowledge about science in a variety of complex life situations. They can link different information sources and explanations and use evidence from those sources to justify decisions. They clearly and consistently demonstrate advanced scientific thinking and reasoning, and they demonstrate willingness to use their scientific understanding in support of solutions to unfamiliar scientific and technological situations. Students at this level can use scientific knowledge and develop arguments in support of recommendations and decisions that center on personal, social or global situations."}, {"section_title": "Performance at PISA Proficiency Levels", "text": "In addition to reporting performance in terms of scale scores, PISA reports results in terms of the percentage of students at each of several proficiency levels. PISA's seven reading literacy proficiency levels, ranging from 1b to 6, are described in exhibit 1 (see appendix B for information about how the proficiency levels are created).  At level 6, tasks typically require the reader to make multiple inferences, comparisons and contrasts that are both detailed and precise. They require demonstration of a full and detailed understanding of one or more texts and may involve integrating information from more than one text. Tasks may require the reader to deal with unfamiliar ideas, in the presence of prominent competing information, and to generate abstract categories for interpretations. Reflect and evaluate tasks may require the reader to hypothesize about or critically evaluate a complex text on an unfamiliar topic, taking into account multiple criteria or perspectives, and applying sophisticated understandings from beyond the text. There are limited data about access and retrieve tasks at this level, but it appears that a salient condition is precision of analysis and fine attention to detail that is inconspicuous in the texts."}, {"section_title": "Level 2 407", "text": "At level 2, some tasks require the reader to locate one or more pieces of information, which may need to be inferred and may need to meet several conditions. Others require recognizing the main idea in a text, understanding relationships, or construing meaning within a limited part of the text when the information is not prominent and the reader must make low level inferences. Tasks at this level may involve comparisons or contrasts based on a single feature in the text. Typical reflective tasks at this level require readers to make a comparison or several connections between the text and outside knowledge, by drawing on personal experience and attitudes."}, {"section_title": "Level 1a 335", "text": "At level 1a, tasks require the reader to locate one or more independent pieces of explicitly stated information; to recognize the main theme or author's purpose in a text about a familiar topic, or to make a simple connection between information in the text and common, everyday knowledge. Typically the required information in the text is prominent and there is little, if any, competing information. The reader is explicitly directed to consider relevant factors in the task and in the text."}, {"section_title": "Level 1b 262", "text": "At level 1b, tasks require the reader to locate a single piece of explicitly stated information in a prominent position in a short, syntactically simple text with a familiar context and text type, such as a narrative or a simple list. The text typically provides support to the reader, such as repetition of information, pictures or familiar symbols. There is minimal competing information. In tasks requiring interpretation the reader may need to make simple connections between adjacent pieces of information. NOTE: To reach a particular proficiency level, a student must correctly answer a majority of items at that level. Students were classified into reading literacy levels according to their scores. Cut point scores in the exhibit are rounded; exact cut point scores are provided in appendix B. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2009."}, {"section_title": "Differences in Performance by Selected Student and School Characteristics", "text": "This section reports performance on the PISA combined reading literacy scale by selected characteristics of students: sex, racial/ethnic background, and the socioeconomic context of their schools. The results cannot be used to demonstrate a cause-and-effect relationship between these variables and student performance. Student performance can be affected by a complex mix of educational and other factors that are not accounted for in these analyses. NOTE: To reach a particular proficiency level, a student must correctly answer a majority of items at that level. Students were classified into reading literacy levels according to their scores. Exact cut point scores are as follows: below level 1b (a score less than or equal to 262.04); level 1b (a score greater than 262.04 and less than or equal to 334.75); level 1a (a score greater than 334.75 and less than or equal to 407.47); level 2 (a score greater than 407.47 and less than or equal to 480.18); level 3 (a score greater than 480.18 and less than or equal to 552.89); level 4 (a score greater than 552.89 and less than or equal to 625.61); level 5 (a score greater than 625.61 and less than or equal to 698.32); and level 6 (a score greater than 698.32). The Organization for Economic Cooperation and Development (OECD) average is the average of the national averages of the OECD member countries, with each country weighted equally. Detail may not sum to totals because of rounding. The standard errors of the estimates are shown in "}, {"section_title": "U.S. Performance in Reading Literacy Sex", "text": "Female students scored higher, on average, than male students on the combined reading literacy scale in all 65 participating countries and other education systems (table  4). The gender gap ranged from a difference of 9 scale score points in Colombia to 62 scale score points in Albania. In the United States, the difference (25 scale score points) was smaller than the difference in the OECD countries, on average (39 scale score points), and smaller than the differences in countries and other education systems (24 OECD countries and 21 non-OECD countries and other education systems).  "}, {"section_title": "U.S. Performance in Reading Literacy Race/Ethnicity", "text": "Racial and ethnic groups vary by country, so it is not possible to compare performance of students in individual countries by students' race/ethnicity. Therefore, only results for the United States are presented. On the combined reading literacy scale, White (non-Hispanic) and Asian (non-Hispanic) students had higher average scores (525 and 541, respectively) than the overall OECD and U.S. average scores, while Black (non-Hispanic) and Hispanic students had lower average scores (441 and 466, respectively) than the overall OECD and U.S. average scores (table 5). The average scores of students who reported two or more races (502) were not measurably different from the overall OECD or U.S. average scores. The average scores of White (non-Hispanic) students, Asian (non-Hispanic) students, and students who reported two or more races (525, 541, and 502, respectively) were in the range of PISA's proficiency level 3 (signifies a score of greater than 480 and less than or equal to 553), while the average scores of Black (non-Hispanic) and Hispanic students (441 and 466, respectively) were in the range of PISA's proficiency level 2 (signifies a score of greater than 407 and less than or equal to 480). These findings describe average performance and do not describe variation within the subgroup. Students at level 3 on the reading literacy scale are typically successful at \"reading tasks of moderate complexity, such as locating multiple pieces of information, making links between different parts of a text, and relating it to familiar everyday knowledge,\" as described in exhibit 1, and other tasks that might be expected to be commonly demanded of young and older adults across OECD countries in their everyday lives (OECD 2010a, p. 51). At level 2, which \"can be considered a baseline level of proficiency, at which students begin to demonstrate the reading literacy competencies that will enable them to participate effectively and productively in life\" (OECD 2010a, p. 52), students can typically locate information that meets several conditions, make comparisons or contrasts around a single feature, determine what a welldefined part of a text means even when the information is not prominent, and make connections between the text and personal experience. "}, {"section_title": "U.S. Performance in Reading Literacy School Socioeconomic Contexts", "text": "The percentage of students in a school who are eligible for free or reduced-price lunch (FRPL-eligible) through the National School Lunch Program is an indicator, in the United States, of the socioeconomic status of families served by the school. Other countries have different indicators of school socioeconomic context and thus only results for the United States are shown by the percentage of students in schools who are FRPL-eligible. Data are for public schools only. Students in public schools in which half or more of students were eligible for free or reduced-price lunch (50 to 74.9 percent and 75 percent or more) scored, on average, below the overall OECD and U.S. average scores (table   6). Students in schools in which less than 25 percent of students were FRPL-eligible (10 to 24.9 percent and less than 10 percent) scored, on average, above the overall OECD and U.S. average scores. The average scores of students in schools in which 25 to 49.9 percent were FRPL-eligible were above the overall OECD average but not measurably different from the U.S. average. The average scale score of students in schools with less than 10 percent of FRPL-eligible students (551) was at the upper end of proficiency level 3 (upper cut point is 553), while students in schools with 75 percent or more of FRPL-eligible students performed at the middle of level 2, with an average scale score of 446 (level 2 midpoint is 444), a difference of 105 scale score points. There was no measurable difference between the average score of U.S. students in reading literacy in 2000 (504), the last time in which reading literacy was the major domain assessed in PISA, and2009 (500), or between 2003 (495) and 2009 (figure 4). 7 There also were no measurable differences between the U.S. average score and the OECD average score in 2000 or in 2009 when the OECD averages were 496 and 495, respectively. The PISA 2000 and 2009 OECD averages used in the analysis of trends in reading literacy over time are based on the averages of the 27 countries that participated in both the 2000 and 2009 assessments and met all technical standards, and that are currently members of the OECD, even if they were not members when the PISA 2000 assessment was administered. 8 As a result, the reading literacy OECD average score for PISA 2000 differs from previously published reports and the reading literacy OECD average score for PISA 2009 differs from that reported in other tables in this report. The recalculated OECD averages are referred to as OECD trend scores. The U.S. averages in 2000 and 2009 are compared with OECD trend scores in 2000 and 2009 because reading literacy was the major domain assessed in those years. This presentation is consistent with the OECD's analysis of trends in performance on PISA (OECD 2010e). NOTE: PISA 2006 reading literacy results are not reported for the United States because of an error in printing the test booklets. For more details, see Baldi et al. 2007 (available at http://nces.ed.gov/pubsearch/pubsinfo.asp?pubid=2008016). The Organization for Economic Cooperation and Development (OECD) average is the average of the national averages of the OECD member countries, with each country weighted equally. There were no statistically significant differences between the U.S. average score and the OECD average score in 2000 or in 2009. The standard errors of the estimates are shown in  In PISA 2009, mathematics literacy is defined as follows: An individual's capacity to identify and understand the role that mathematics plays in the world, to make well-founded judgments and to use and engage with mathematics in ways that meet the needs of that individual's life as a constructive, concerned and reflective citizen (OECD 2009, p. 84)."}, {"section_title": "U.S. Performance in Mathematics Literacy", "text": "Level 5   607At level 5, students can develop and work with models for complex situations, identifying constraints and specifying assumptions. They can select, compare, and evaluate appropriate problem solving strategies for dealing with complex problems related to these models. Students at this level can work strategically using broad, well-developed thinking and reasoning skills, appropriate linked representations, symbolic and formal characterizations, and insight pertaining to these situations. They can reflect on their actions and formulate and communicate their interpretations and reasoning.\nIn mathematics literacy, 27 percent of U.S. students scored at or above proficiency level 4, that is, at levels 4, 5, or 6 (figure 5 and exhibit 2). This is lower than the 32 percent of students in the OECD countries on average that scored at or above level 4. Level 4 is the level at which students can complete higher order tasks such as \"solv [ing] problems that involve visual and spatial reasoning...in unfamiliar contexts\" and \"carry[ing] out sequential processes\" (OECD 2004, p. 55). A lower percentage of U.S. students performed at level 4 than the OECD average (17 percent versus 19 percent) and at level 6 (2 percent versus 3 percent). Twenty-three percent of U.S. students scored below level 2 (that is, at level 1 or below level 1), what OECD calls a \"a baseline level of mathematics proficiency on the PISA scale at which students begin to demonstrate the kind of literacy skills that enable them to actively use mathematics\" (OECD 2004, p. 56 NOTE: To reach a particular proficiency level, a student must correctly answer a majority of items at that level. Students were classified into mathematics literacy levels according to their scores. Exact cut point scores are as follows: below level 1 (a score less than or equal to 357.77); level 1 (a score greater than 357.77 and less than or equal to 420.07); level 2 (a score greater than 420.07 and less than or equal to 482.38); level 3 (a score greater than 482.38 and less than or equal to 544.68); level 4 (a score greater than 544.68 and less than or equal to 606.99); level 5 (a score greater than 606.99 and less than or equal to 669.30); and level 6 (a score greater than 669.30). The Organization for Economic Cooperation and Development (OECD) average is the average of the national averages of the OECD member countries, with each country weighted equally. Detail may not sum to totals because of rounding. The standard errors of the estimates are shown in of U.S. students and the percentage of students in the OECD countries on average demonstrating proficiency below level 2. A description of the general competencies and tasks 15-year-old students typically can do, by proficiency level, for the mathematics literacy scale is shown in exhibit 2. In comparison to the United States, 16 OECD countries and 6 non-OECD countries and other education systems had higher percentages of students who performed at or above level 4 in mathematics literacy; 5 OECD countries and 25 non-OECD countries and other education systems had lower percentages of students who performed at or above level 4; and for 12 OECD countries, there were no measurable differences in the percentage of students who performed at or above level 4 (data shown in table M4A at http://nces.ed.gov/surveys/pisa/pisa2009tablefigureexhibit. asp).  The U.S. average score in mathematics literacy in 2009 (487) was higher than the U.S. average in 2006 (474) but not measurably different from the U.S. average in 2003 (483), the earliest time point to which PISA 2009 performance can be compared in mathematics literacy (figure 6). U.S. students' average scores were lower than the OECD average scores in each of these years (2003 and 2009).  "}, {"section_title": "Level 4 545", "text": "At level 4, students can work effectively with explicit models for complex concrete situations that may involve constraints or call for making assumptions. They can select and integrate different representations, including symbolic ones, linking them directly to aspects of real-world situations. Students at this level can utilize well-developed skills and reason flexibly, with some insight, in these contexts. They can construct and communicate explanations and arguments based on their interpretations, arguments, and actions."}, {"section_title": "Level 3 482", "text": "At level 3, students can execute clearly described procedures, including those that require sequential decisions. They can select and apply simple problem solving strategies. Students at this level can interpret and use representations based on different information sources and reason directly from them. They can develop short communications reporting their interpretations, results and reasoning."}, {"section_title": "Level 2 420", "text": "At level 2, students can interpret and recognize situations in contexts that require no more than direct inference. They can extract relevant information from a single source and make use of a single representational mode. Students at this level can employ basic algorithms, formulae, procedures, or conventions. They are capable of direct reasoning and making literal interpretations of the results."}, {"section_title": "Level 1 358", "text": "At level 1, students can answer questions involving familiar contexts where all relevant information is present and the questions are clearly defined. They are able to identify information and to carry out routine procedures according to direct instructions in explicit situations. They can perform actions that are obvious and follow immediately from the given stimuli. "}, {"section_title": "U.S. Performance in Science Literacy", "text": "In PISA 2009, science literacy is defined as follows: \nLevel 5   633At level 5, students can identify the scientific components of many complex life situations, apply both scientific concepts and knowledge about science to these situations, and can compare, select and evaluate appropriate scientific evidence for responding to life situations. Students at this level can use well-developed inquiry abilities, link knowledge appropriately and bring critical insights to situations. They can construct explanations based on evidence and arguments based on their critical analysis."}, {"section_title": "Level 4 559", "text": "At level 4, students can work effectively with situations and issues that may involve explicit phenomena requiring them to make inferences about the role of science or technology. They can select and integrate explanations from different disciplines of science or technology and link those explanations directly to aspects of life situations. Students at this level can reflect on their actions and they can communicate decisions using scientific knowledge and evidence."}, {"section_title": "Level 3 484", "text": "At level 3, students can identify clearly described scientific issues in a range of contexts. They can select facts and knowledge to explain phenomena and apply simple models or inquiry strategies. Students at this level can interpret and use scientific concepts from different disciplines and can apply them directly. They can develop short statements using facts and make decisions based on scientific knowledge."}, {"section_title": "Level 2 410", "text": "At level 2, students have adequate scientific knowledge to provide possible explanations in familiar contexts or draw conclusions based on simple investigations. They are capable of direct reasoning and making literal interpretations of the results of scientific inquiry or technological problem solving."}, {"section_title": "Level 1 335", "text": "At level 1, students have such a limited scientific knowledge that it can only be applied to a few, familiar situations. They can present scientific explanations that are obvious and follow explicitly from given evidence. NOTE: To reach a particular proficiency level, a student must correctly answer a majority of items at that level. Students were classified into science literacy levels according to their scores. Exact cut point scores are as follows: below level 1 (a score less than or equal to 334.94); level 1 (a score greater than 334.94 and less than or equal to 409.54); level 2 (a score greater than 409.54 and less than or equal to 484.14); level 3 (a score greater than 484.14 and less than or equal to 558.73); level 4 (a score greater than 558.73 and less than or equal to 633.33); level 5 (a score greater than 633.33 and less than or equal to 707.93); and level 6 (a score greater than 707.93). The Organization for Economic Cooperation and Development (OECD) average is the average of the national averages of the OECD member countries, with each country weighted equally. Detail may not sum to totals because of rounding. There were no statistically significant differences between U.S. students and the OECD average in the percentages of students at each proficiency level. The standard errors of the estimates are shown in Twenty-nine percent of U.S. students and students in the OECD countries on average scored at or above level 4 on the science literacy scale, that is, at levels 4, 5, or 6. Level 4 is the level at which students can complete higher order tasks such as \"select[ing] and integrat [ing] explanations from different disciplines of science or technology\" and \"link[ing] those explanations directly to...life situations\" (OECD 2007, p. 43). Eighteen percent of U.S. students and students in the OECD countries on average scored below level 2, that is, at level 1 or below level 1 (figure 7). Students performing below level 2 are below what OECD calls a \"baseline level of proficiency\u2026at which students begin to demonstrate the science competencies that will enable them to participate effectively and productively in life situations related to science and technology\" (OECD 2007, p. 44). There also were no measurable differences between the percentages of U.S. students and students in the OECD countries on average that scored at the individual proficiency levels. In comparison to the United States, 13 OECD countries and 5 non-OECD countries and other education systems had higher percentages of students who performed at or above level 4 in science literacy; 11 OECD countries and 25 non-OECD countries and other education systems had lower percentages of students who performed at or above level 4; and for 9 OECD countries and 1 non-OECD education system, there were no measurable differences in the percentage of students who performed at or above level 4 (data shown in table S4A at http://nces.ed.gov/surveys/pisa/ pisa2009tablefigureexhibit.asp). The U.S. average score in science literacy in 2009 (502) was higher than the U.S. average in 2006 (489), the only time point to which PISA 2009 performance can be compared in science literacy (figure 8). While U.S. students scored lower than the OECD average in science literacy in 2006, the average score of U.S. students in 2009 was not measurably different from the 2009 OECD average. The PISA 2006 and 2009 OECD averages used in the analysis of trends in science literacy over time are based on the averages of the 34 countries that are currently members of the OECD, even if those countries were not members when the PISA 2006 assessment was administered (all 34 current OECD members participated in the 2006 assessment). As a result, the science literacy OECD average score for PISA 2006 differs from previously published reports and is referred to as the OECD trend score. Appendix A: Sample Reading Texts and Items From PISA 2009 After each administration of the Program for International Student Assessment (PISA), the Organization for Economic Cooperation and Development (OECD) releases to the public a subset of items in order to illustrate the content of the assessment. The remaining items are kept secure so they can be used again in a future PISA cycle to measure trends in performance. This appendix contains sample reading texts and items used in the U.S. administration of the PISA 2009 reading assessment. The items illustrate the different aspects of reading assessed by PISA as well as the PISA proficiency levels. The percentage of U.S. students who answered the item correctly is shown, along with the OECD average percentage correct for each item. Exhibit A-1 shows the PISA 2009 sample items organized by reading aspect and PISA proficiency level. For example, The Play's the Thing question 1 assesses the integrate and interpret aspect and is located on the PISA scale at level 6, indicating that it is of high difficulty. The access and retrieve aspect and the two lowest proficiency levels (level 1a and level 1b), as well as levels 2, 5 and 6 of reflect and evaluate were not covered by the released items on which U.S. students were assessed. Cutting down on commuting hours and reducing the energy consumption involved is obviously a good idea. But such a goal should be accomplished by improving public transportation or by ensuring that workplaces are located near where people live. The ambitious idea that telecommuting should be part of everyone's way of life will only lead people to become more and more self-absorbed. Do we really want our sense of being part of a community to deteriorate even further? Richard 1 \"Telecommuting\" is a term coined by Jack Nilles in the early 1970s to describe a situation in which workers work on a computer away from a central office (for example, at home) and transmit data and documents to the central office via telephone lines. Use \"Telecommuting\" above to answer the questions that follow."}, {"section_title": "Question 1: TELECOMMUTING", "text": "What is the relationship between \"The way of the future\" and \"Disaster in the making\"? A They use different arguments to reach the same general conclusion. B They are written in the same style but they are about completely different topics. C They express the same general point of view, but arrive at different conclusions. D They express opposing points of view on the same topic. The magnetic fields are incredibly weak, and so unlikely to affect cells in our body. 3. People who make long cell phone calls sometimes complain of fatigue, headaches, and loss of concentration. These effects have never been observed under laboratory conditions and may be due to other factors in modern lifestyles."}, {"section_title": "4.", "text": "Cell phone users are 2.5 times more likely to develop cancer in areas of the brain adjacent to their phone ears. Researchers admit it's unclear this increase is linked to using cell phones."}, {"section_title": "The International Agency for", "text": "Research on Cancer found a link between childhood cancer and power lines. Like cell phones, power lines also emit radiation. The radiation produced by power lines is a different kind of radiation, with much more energy than that coming from cell phones."}, {"section_title": "6.", "text": "Radio frequency waves similar to those in cell phones altered the gene expression in nematode worms. Worms are not humans, so there is no guarantee that our brain cells will react in the same way. A There is no danger involved in using cell phones. B There is a proven risk involved in using cell phones. C There may or may not be danger involved in using cell phones, but it is worth taking precautions. D There may or may not be danger involved in using cell phones, but they should not be used until we know for sure. E The Do instructions are for those who take the threat seriously, and the Don't instructions are for everyone else. TURAI I'm thinking about how difficult it is to begin a play. To introduce all the principal characters in the beginning, when it all starts. \u00c1D\u00c1M I suppose it must be hard."}, {"section_title": "Percentage of students answering correctly", "text": "\n"}, {"section_title": "TURAI", "text": "It is -devilishly hard. The play starts. The audience goes quiet. The actors enter the stage and the torment begins. It's an eternity, sometimes as much as a quarter of an hour before the audience finds out who's who and what they are all up to.\nThere aren't. I am a dramatist. That is my curse.\nIf you do not master it, you are its slave. There is no middle ground. Trust me, it's no joke starting a play well. It is one of the toughest problems of stage mechanics. Introducing your characters promptly. Let's look at this scene here, the three of us. Three gentlemen in tuxedoes. Say they enter not this room in this lordly castle, but rather a stage, just when a play begins. They would have to chat about a whole lot of uninteresting topics until it came out who we are. Wouldn't it be much easier to start all this by standing up and introducing ourselves? Stands up. Good evening. The three of us are guests in this castle. We have just arrived from the dining room where we had an excellent dinner and drank two bottles of champagne. My name is S\u00e1ndor Turai, I'm a playwright, I've been writing plays for thirty years, that's my profession. Full stop. Your turn."}, {"section_title": "G\u00c1L", "text": "Quite a peculiar brain you've got. Can't you forget your profession for a single minute? TURAI That cannot be done.\nNot half an hour passes without you discussing theater, actors, plays. There are other things in this world.\nYou shouldn't become such a slave to your profession.\nStands up. My name is G\u00e1l, I'm also a playwright. I write plays as well, all of them in the company of this gentleman here. We are a famous playwright duo. All playbills of good comedies and operettas read: written by G\u00e1l and Turai. Naturally, this is my profession as well. read: written by G\u00e1l and Turai. Naturally, this is my profession as well.  "}, {"section_title": "G\u00c1L and TURAI", "text": "\nTogether. And this young man \u2026 80 \u00c1D\u00c1M Stands up. This young man is, if you allow me, Albert \u00c1d\u00e1m, twenty-five years old, composer. I wrote the music for these kind gentlemen for their latest operetta. This is Question 3: THE PLAY'S THE THING A reader said, \"\u00c1d\u00e1m is probably the most excited of the three characters about staying at the castle.\" What could the reader say to support this opinion? Use the text to give a reason for your answer. He must be happy to be with the two guys who can make him famous. D He is using the characters to act out one of his own creative problems. "}, {"section_title": "International Requirements for Sampling, Data Collection, and Response Rates", "text": "To provide valid estimates of student achievement and characteristics, the sample of PISA students had to be selected in a way that represented the full population of 15-year-old students in each country. The international desired population in each country consisted of 15-year-olds attending both publicly and privately controlled schools in grade 7 and higher. A minimum of 4,500 students from a minimum of 150 schools was required in each country. The international guidelines specified that within schools, a sample of 35 students was to be selected in an equal probability sample unless fewer than 35 students age 15 were available (in which case all students were selected). International standards required that students in the sample be 15 years and 3 months to 16 years and 2 months at the beginning of the testing period. In the United States, sampled students were born between July 1, 1993, andJune 30, 1994. The international standard for the maximum length of the testing period was 42 days, but the United States requested and was granted permission to expand the testing window to 60 days (from September 21, 2009, to November 19, 2009 in order to accommodate school requests. 1 Each country collected its own data, following international guidelines and specifications. 1 Most countries conducted testing from March through August of 2009. The United States and the United Kingdom were given permission to move the testing dates to September through November in an effort to improve response rates. The range of eligible birthdates was adjusted so that the mean age remained the same (i.e., 15 years and 3 months to 16 years and 2 months at the beginning of the testing period). In 2003, the United States conducted PISA in the spring and fall and found no significant difference in student performance between the two time points. The school response rate target was 85 percent for all countries. A minimum of 65 percent of schools from the original sample of schools was required to participate for a country's data to be included in the international database. Countries were allowed to use replacement schools (selected during the sampling process) to increase the response rate once the 65 percent benchmark had been reached. PISA 2009 also required a minimum participation rate of 80 percent of sampled students from schools within each country. A student was considered to be a participant if he or she participated in the first testing session or a follow-up or makeup testing session. Data from countries not meeting this requirement could be excluded from international reports. PISA's intent was to be as inclusive as possible. The guidelines allowed schools to be excluded for approved reasons (for example, schools in remote regions, very small schools, or special education schools could be excluded). Schools used the following international guidelines on student exclusions: \u2022 Students with functional disabilities. These were students with a moderate to severe permanent physical disability such that they cannot perform in the PISA testing environment. \u2022 Students with intellectual disabilities. These were students with a mental or emotional disability and who have been tested as cognitively delayed or who are considered in the professional opinion of qualified staff to be cognitively delayed such that they cannot perform in the PISA testing environment. \u2022 Students with insufficient language experience. These were students who meet the three criteria of not being native speakers in the assessment language, having limited proficiency in the assessment language, and having less than one year of instruction in the assessment language. Overall estimated exclusions (including both school and student exclusions) were to be under 5 percent of the PISA target population. Quality monitors from the PISA Consortium visited a sample of schools in every country to ensure that testing procedures were conducted in a consistent manner."}, {"section_title": "Sampling, Data Collection, and Response Rates in the United States", "text": "The PISA 2009 school sample was drawn for the United States in July 2008 by the international PISA Consortium. The U.S. sample for 2009 was drawn using a twostage sampling process. The first stage was a sample of schools and the second stage was a sample of students within schools. The sample design for PISA 2009 was a stratified systematic sample, with sampling probabilities proportional to the estimated number of 15-year-old students in the school based on grade enrollments. The PISA sample was stratified into eight explicit groups based on control of school (public or private) and region of the country (Northeast, Central, West, Southeast). 2 Within each stratum, the frame was implicitly stratified (i.e., sorted for sampling) by five categorical stratification variables: grade range of the school (five categories); type of location relative to populous areas (city, suburb, town, rural); 3 first three digits of the zip code; combined percentage of Black, Hispanic, Asian, Pacific Islander, and American Indian/ Alaska Native students (above or below 15 percent); and estimated enrollment of 15-year-olds. The sampling employed techniques to minimize overlap with the High School Longitudinal Study of 2009 (which was collecting data in the same school year) and to undersample very small schools (those with an estimate of fewer than twentyone 15-year-old students). Following the PISA guidelines, at the same time as the PISA sample was selected, replacement schools were identified by assigning the two schools neighboring the sampled school in the frame as replacements. There were several constraints on the assignment of substitutes. One sampled school was not allowed to substitute for another, and a given school could not be assigned to substitute for more than one sampled school. Furthermore, substitutes were required to be in the same explicit stratum as the sampled school. If the sampled school was the first or 2 The Northeast region consists of Connecticut, Delaware, the District of Columbia, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. The Central region consists of Illinois,Indiana,Iowa,Kansas,Michigan,Minnesota,Missouri,Nebraska,North Dakota,Ohio,Wisconsin,and South Dakota. The West region consists of Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oklahoma,Oregon,Texas,Utah,Washington,and Wyoming. The Southeast region consists of Alabama,Arkansas,Florida,Georgia,Kentucky,Louisiana,Mississippi,North Carolina,South Carolina,Tennessee,Virginia,and West Virginia. 3 These types are defined as follows: (1) \"city\" is territory inside an urbanized area with a core population of 50,000 or more and inside a principal city; (2) \"suburb\" is territory inside an urbanized area with a core population of 50,000 or more and outside a principal city; (3) \"town\" is territory inside an urban cluster with a core population between 25,000 and 50,000; and (4) \"rural\" is territory not in an urbanized area or urban cluster. last school in the stratum, the second school following or preceding the sampled school was identified as the substitute. One school was designated a first replacement and the other a second replacement. If an original school refused to participate, the first replacement was then contacted. If that school also refused to participate, the second school was contacted. The U.S. PISA 2009 school sample consisted of 236 schools. This number was increased from the international minimum requirement of 150 to offset school nonresponse and reduce design effects. Schools were selected with probability proportionate to the school's estimated enrollment of 15-year-olds. The data for public schools were from the 2005-06 Common Core of Data (CCD), and the data for private schools were from the 2005-06 Private School Universe Survey (PSS). Any school containing at least one 7th-through 12th-grade class in school year 2005-06 was included in the school sampling frame. Participating schools provided a list of 15-year-old students (typically in August or September 2009), and a sample of 42 students was selected within each school in an equal probability sample. The overall sample design for the United States was intended to approximate a self-weighting sample of students as much as possible, with each 15-yearold student having an equal probability of being selected. In the United States, for a variety of reasons reported by school administrators (such as increased testing requirements at the national, state, and local levels; concerns about the timing of the PISA assessment; and loss of learning time), many schools in the original sample declined to participate. Of the 236 original sampled schools, 208 were eligible (22 schools did not have any 15-year-olds enrolled, 5 had closed, and 1 was ineligible because all of its students were also enrolled in other \"home\" schools), and 145 agreed to participate. The weighted school response rate before replacement was 68 percent, requiring the United States to conduct a nonresponse bias analysis, which was used by the PISA Consortium and the OECD to evaluate the quality of the final sample. 4 In addition to the 145 participating original schools, 20 replacement schools also participated, for a total of 165 participating schools (or a 78 percent overall school response rate). 5 4 NCES requires a nonresponse bias analysis for any survey with a weighted response rate below 85 percent. OECD requires a nonresponse bias analysis from countries with weighted school response rates between 65 and 85 percent. 5 Response rates reported here are based on the formula used in the international report and are not consistent with NCES standards. A more conservative way to calculate the response rate would be to include replacement schools that participated in the denominator as well as the numerator, and to add replacement schools that were hard refusals to the denominator. This results in a weighted school response rate of 64 percent. A total of 6,677 students were sampled for the assessment. Of these students, 273 were deemed ineligible because they had left the school between the samping and assessment date. Of the eligible 6,404 sampled students, an additional 339 were excluded using the decision criteria described earlier, for a weighted exclusion rate of 5 percent at the student level. Of the 6,065 remaining sampled students, a total of 5,233 participated in the assessment in the United States for an overall weighted student response rate of 87 percent. A bias analysis was conducted in the United States to address potential problems in the data owing to school nonresponse. To compare PISA participating schools and nonparticipating schools, it was necessary to match the sample of schools back to the sample frame to detect as many characteristics as possible that might provide information about the presence of nonresponse bias. Frame characteristics were taken from the 2005-06 CCD for public schools and from the 2005-06 PSS for private schools. The available school characteristics included affiliation (public or private), community type, region, number of age-eligible students, total number of students, and percentage of various racial/ethnic groups (Asian or Pacific Islander, non-Hispanic; Black, non-Hispanic; Hispanic; American Indian or Alaska Native, non-Hispanic; and White, non-Hispanic). The percentage of students eligible for free or reduced-price lunch was available for public schools only. Comparing frame characteristics for participating schools and nonparticipating schools is not always a good measure of nonresponse bias if the characteristics are unrelated or weakly related to more substantive items in the survey; however, this was the only approach available given that no comparable school-or student-level achievement data were available. For categorical variables, the hypothesis of independence between the characteristics and response status was tested using a chi-square statistic. For continuous variables, summary means were calculated and compared using t tests. In addition to these tests, logistic regression models were employed to identify whether any of the frame characteristics were significant in predicting response status. All analyses were performed using WesVar, a statistical software package. The school base weights used in these analyses did not include a nonresponse adjustment factor. The base weight for each original school was calculated as the reciprocal of the probability of selection times the number of eligible students in the school. The base weight for each replacement school was set equal to the base weight of the original school it replaced. The only variable for which there were statistically significant differences between participating schools and all sampled schools was the percentage of students at the school eligible for free or reduced-price lunch (t = 2.30, p = .02). On average, participating schools had a higher percentage of students from lower income families (mean = 35.4 percent, s.e.= 1.95) who were eligible for free or reduced-price lunch than did all sampled schools (mean = 34.1 percent, s.e.= 1.70)."}, {"section_title": "Test Development", "text": "The 2009 assessment instruments were developed by international experts and PISA Consortium test developers, and items were reviewed by representatives of each country for possible bias and relevance to PISA's goals. The assessment included items submitted by participating countries as well as items that were developed by the Consortium's test developers. The final assessment consisted of 102 reading items, 36 mathematics items, and 52 science items allocated to 13 test booklets. Each booklet was made up of 4 test clusters. Altogether there were 7 reading clusters, 3 mathematics clusters, and 3 science clusters. The clusters were allocated in a rotated design to the 13 booklets. The average number of items per cluster was 15 items for reading, 12 items for mathematics, and 17 items for science. Each cluster was designed to average 30 minutes of test material. Each student took one booklet, with about 2 hours worth of testing material. Approximately half of the items were multiple-choice, about 20 percent were closed or short response types (for which students wrote an answer that was simply either correct or incorrect), and about 30 percent were open constructed responses (for which students wrote answers that were graded by trained scorers using an international scoring guide). In PISA 2009, every student answered reading items. Not all students answered mathematics and/or science items. In addition to the cognitive assessment, students also received a 30-minute questionnaire designed to provide information about their backgrounds, attitudes, and experiences in school. Principals in schools where PISA was administered also received a 30-minute questionnaire about their schools."}, {"section_title": "Translation and Adaptation", "text": "Source versions of all instruments (assessment booklets, questionnaires, and manuals) were prepared in English and French and translated into the primary language or languages of instruction in each country. PISA recommended that countries prepare and consolidate independent translations from both source versions and provided precise translation guidelines that included a description of the features each item was measuring and statistical analysis from the field trial. In cases for which one source language was used, independent translations were required and discrepancies reconciled. In addition, it was sometimes necessary to adapt the instrument for cultural purposes, even in nations such as the United States that use English as the primary language of instruction. For example, words such as \"lift\" might be adapted to \"elevator\" for the United States. The PISA Consortium verified the national adaptation of all instruments. Electronic copies of printed materials were sent to the PISA Consortium for a final visual check prior to data collection."}, {"section_title": "Test Administration and Quality Assurance", "text": "PISA 2009 emphasized the use of standardized procedures in all countries. Each country collected its own data, based on a manual provided by the PISA Consortium (ACER 2008) to explain the survey's implementation, including precise instructions for the work of school coordinators and scripts for test administrators to use in testing sessions. Test administration in the United States was coordinated by professional staff trained according to the international guidelines. School staff members were asked to assist only with listing students, identifying space for testing in the school, and specifying any parental consent procedures needed for sampled students. Students were allowed to use calculators, and U.S. students were provided calculators; however, no information on the availability of calculators was collected internationally. At some schools, the PISA assessment was administered to students outside of normal school hours to address schools' concerns about the potential negative effect on students of the loss of instructional time. In the United States, tests were administered during normal school hours at 155 schools (94 percent), outside of normal school hours at 4 schools (2 percent), and on Saturdays at 6 schools (4 percent). Test administrations were observed in a sample of schools in each country by a PISA Quality Monitor (PQM) who was engaged by the PISA Consortium. The sample schools were selected jointly by the PISA Consortium and the PQM. In the United States, 7 schools were observed by the PQM. The PQM's primary responsibility was to document the extent to which testing procedures in schools were implemented in accordance with test administration procedures. The PQM's observations in U.S. schools indicated that international procedures for data collection were applied consistently."}, {"section_title": "Scoring", "text": "A significant proportion of the PISA assessment was devoted to items requiring constructed responses. The scoring of these responses was the responsibility of each country. The process of scoring these items was an important step in ensuring the quality and comparability of the PISA data. The PISA Consortium developed detailed scoring guides, scoring training materials, and scorer recruitment materials and led international training sessions on scoring. Those who attended the international training on scoring then led the training of national scoring teams. For each test item, the scoring guide described the intent of the question and how to score the students' responses. This description included the credit labels-full credit, partial credit, or no credit-attached to the possible categories of response. In addition, the scoring guides included real examples of students' responses accompanied by a rationale for their classification for purposes of clarity and illustration. To examine the consistency of this marking process in more detail within each country and to estimate the magnitude of the variance components associated with the use of scorers, the PISA Consortium conducted an interscorer reliability study on a subsample of assessment booklets. Homogeneity analysis was applied to the national sets of multiple scoring and compared with the results of the field trial. A full description of this process and the results can be found in the OECD's PISA 2009 Technical Report (forthcoming). facilitated the checking and correction of data by providing various data consistency checks. The data were then sent to ACER for cleaning. ACER's role at this point was to check that the international data structure was followed, check the identification system within and between files, correct single case problems manually, and apply standard cleaning procedures to questionnaire files. Results of the data cleaning process were documented and shared with the national project managers and included specific questions when required. The national project manager then provided ACER with revisions to coding or solutions for anomalies. ACER then compiled background univariate statistics and preliminary classical and Rasch item analysis. Detailed information on the entire data entry and cleaning process can be found in the OECD's PISA 2009 Technical Report (forthcoming)."}, {"section_title": "Weighting", "text": "The use of sampling weights is necessary for the computation of statistically sound, nationally representative estimates. Adjusted survey weights adjust for the probabilities of selection for individual schools and students, for school or student nonresponse, or for errors in estimating the size of the school or the number of 15-year-olds in the school at the time of sampling. Survey weighting for all countries and other education systems participating in PISA 2009 was coordinated by Westat, as part of the PISA Consortium. The school base weight was defined as the reciprocal of the school's probability of selection times the number of eligible students in the school. (For replacement schools, the school base weight was set equal to the original school it replaced.) The student base weight was given as the reciprocal of the probability of selection for each selected student from within a school. The product of these base weights was then adjusted for school and student nonresponse. The school nonresponse adjustment was done individually for each country using the explicit strata defined as part of the sample design. In the case of the United States, two variables were used: school control and census region. The student nonresponse adjustment was done within cells based first on their school nonresponse rate and their explicit stratum; within that, grade and sex were used when possible. Grade and sex were collected for students in all countries on the student tracking form. All PISA analyses were conducted using these adjusted sampling weights. For more information on the nonresponse adjustments, see the OECD's PISA 2009 Technical Report (forthcoming)."}, {"section_title": "Scaling of Student Test Data", "text": "Thirteen versions of the PISA test booklet were created, each containing a different subset of items. The fact that each student completed only a subset of items means that classical test scores, such as the percent correct, are not accurate measures of student performance. Instead, scaling techniques were used to establish a common scale for all students. For PISA 2009, item response theory (IRT) was used to estimate average scores for reading, mathematics, and science literacy for each country, as well as for three reading literacy subscales: accessing and retrieving information, integrating and interpreting, and reflecting and evaluating. 6 IRT identifies patterns of response and uses statistical models to predict the probability of answering an item correctly as a function of the students' proficiency in answering other questions. With this method, the performance of a sample of students in a subject area or subarea can be summarized on a simple scale or series of scales, even when students are administered different items. Scores for students are estimated as plausible values because each student completed only a subset of items. Five plausible values were estimated for each student for each scale. These values represent the distribution of potential scores for all students in the population with similar characteristics and identical patterns of item response. Statistics describing performance on the PISA reading, mathematics, and science literacy scales are based on plausible values. 7"}, {"section_title": "Proficiency Levels", "text": "In addition to a range of scale scores as the basic form of measurement, PISA describes student proficiency in terms of levels. Higher levels represent the knowledge, skills, and capabilities needed to perform tasks of increasing complexity. As a result, the findings are reported in terms of percentages of the student population at each of the predefined levels. To determine the performance levels and cut scores on the literacy scales, IRT techniques were used. With IRT techniques, it is possible to simultaneously estimate the ability of all students taking the PISA assessment, as well as the difficulty of all PISA items. Then estimates of student ability and item difficulty can be mapped on a single continuum. The relative ability of students taking a particular test can be estimated by considering the percentage of test items they get correct. The relative difficulty of items in a test can be estimated by considering the percentage of students getting each item correct. In PISA, all students within a level are expected to answer at least half of the items from that level correctly. Students at the bottom of a level are able to provide the correct answers to about 52 percent of all items from that level, have a 62 percent chance of success on the easiest items from that level, and have a 42 percent chance of success on the hardest items from that level. Students in the middle of a level have a 62 percent chance of correctly answering items of average difficulty for that level (an overall response probability of 62 percent). Students at the top of a level are able to provide the correct answers to about 70 percent of all items from that level, have a 78 percent chance of success on the easiest items from that level, and have a 62 percent chance of success on the hardest items from that level. Students just below the top of a level would score less than 50 percent on an assessment at the next higher level. Students at a particular level demonstrate not only the knowledge and skills associated with that level but also the proficiencies defined by lower levels. Patterns of responses for students below level 1b for reading literacy and below level 1 for mathematics and science literacy suggest that these students are unable to answer at least half of the items from those levels correctly. For details about the approach to defining and describing the PISA levels and establishing the cut scores, see the OECD's PISA 2009 Technical Report (forthcoming). The reading proficiency level ranges are below level 1b (a score less than or equal to 262.04); level 1b (a score greater than 262.04 and less than or equal to 334.75); level 1a (a score greater than 334.75 and less than or equal to 407.47); level 2 (a score greater than 407.47 and less than or equal to 480.18); level 3 (a score greater than 480.18 and less than or equal to 552.89); level 4 (a score greater than 552.89 and less than or equal to 625.61); level 5 (a score greater than 625.61 and less than or equal to 698.32); and level 6 (a score greater than 698.32). The math profiency level ranges are below level 1 (a score less than or equal to 357.77); level 1 (a score greater than 357.77 and less than or equal to 420.07); level 2 (a score greater than 420.07 and less than or equal to 482.38); level 3 (a score greater than 482.38 and less than or equal to 544.68); level 4 (a score greater than 544.68 and less than or equal to 606.99); level 5 (a score greater than 606.99 and less than or equal to 669.30); and level 6 (a score greater than 669.30). Science proficiency level ranges are below level 1 (a score less than or equal to 334.94); level 1 (a score greater than 334.94 and less than or equal to 409.54); level 2 (a score greater than 409.54 and less than or equal to 484.14); level 3 (a score greater than 484.14 and less than or equal to 558.73); level 4 (a score greater than 558.73 and less than or equal to 633.33); level 5 (a score greater than 633.33 and less than or equal to 707.93); and level 6 (a score greater than 707.93)"}, {"section_title": "Data Limitations", "text": "As with any study, there are limitations to PISA 2009 that should be taken into consideration. Estimates produced using data from PISA 2009 are subject to two types of error: nonsampling and sampling errors. Nonsampling errors can be due to errors made in the collection and processing of data. Sampling errors can occur because the data were collected from a sample rather than a complete census of the population."}, {"section_title": "Nonsampling Errors", "text": "\"Nonsampling error\" is a term used to describe variations in the estimates that may be caused by population coverage limitations, nonresponse bias, and measurement error, as well as data collection, processing, and reporting procedures. For example, the sampling frame was limited to regular public and private schools in the 50 states and the District of Columbia and cannot be used to represent Puerto Rico or other jurisdictions. The sources of nonsampling errors are typically problems such as unit and item nonresponse, the differences in respondents' interpretations of the meaning of survey questions, and mistakes in data preparation. Some of these issues (particularly school nonresponse) are discussed earlier in the section on U.S. sampling and data collection. There are four kinds of missing data at the item level. \"Nonresponse\" data occur when a respondent is expected to answer an item but no response is given. Responses that are \"missing or invalid\" occur in multiple-choice items for which an invalid response is given. (The missing or invalid code is not used for open-ended questions.) An item is \"not applicable\" when it is not possible for the respondent to answer the question. Finally, items that are \"not reached\" are consecutive missing values starting from the end of each test session. All four kinds of missing data are coded differently in the PISA 2009 database."}, {"section_title": "Sampling Errors", "text": "Sampling errors occur when a discrepancy between a population characteristic and the sample estimate arises because not all members of the target population are sampled for the survey. The size of the sample relative to the population and the variability of the population characteristics both influence the magnitude of sampling"}]