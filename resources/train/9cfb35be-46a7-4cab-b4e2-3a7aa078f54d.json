[{"section_title": "Abstract", "text": "One of the important objectives that the Alzheimer's Disease Neuroimaging Initiative (ADNI) tries to achieve is to understand how the human brain changes over the course of disease progression. We consider voxel-level analysis for the 18 F-Fluorodeoxyglucose positron emission tomography (FDG-PET) imaging study in ADNI for such a purpose. Traditional voxel-level multiple testing procedures in neuroimaging, which are mostly p-value based, often ignore the spatial correlations among neighboring voxels and thus suffer from substantial loss of power. We extend the local-significance-index based procedure, which aims to minimize the false nondiscovery rate subject to a constraint on the false discovery rate, to three-dimensional neuroimaging data using a hidden Markov random field model. A generalized expectation-maximization algorithm is proposed for estimating the model parameters. Extensive simulations show that the proposed approach is more powerful than conventional false discovery rate procedures. We apply the method to the comparison between mild cognitive impairment, a disease status with increased risk of developing Alzheimer's or another dementia, and normal controls in the ADNI's FDG-PET imaging study.\nKEY WORDS: Alzheimer's disease; False discovery rate; Generalized expectationmaximization algorithm; Ising model; Local significance index; Mild cognitive impairment. * Hai Shu is Ph.D. student (E-mail: haishu@umich.edu), and Bin Nan is Professor (E-mail: bnan@umich.edu), Department of Biostatistics, University of Michigan, Ann Arbor, MI 48109. Robert Koeppe is Professor (Email: koeppe@umich.edu), Department of Radiology, University of Michigan, Ann Arbor, MI 48109. The research is supported in part by NSF grant DMS-1007590 and NIH grant R01-AG036802. Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.ucla.edu/wpcontent/uploads/how to apply/ADNI Acknowledgement List.pdf."}, {"section_title": "INTRODUCTION", "text": "Alzheimer's disease (AD) is the most common cause of dementia in the elderly population. The worldwide prevalence of Alzheimer's disease was 26.6 million in 2006 and is predicted to be 1 in 85 persons by 2050 (Brookmeyer et al. 2007 ). Much progress has been made in the diagnosis of AD including clinical assessment and neuroimaging techniques. One such extensively used neuroimaging technique is 18 FFluorodeoxyglucose positron emission tomography (FDG-PET) imaging, which can be used to evaluate the cerebral metabolic rate of glucose (CMRgl). Numerous FDG-PET studies (Nestor et al. 2003; Mosconi et al. 2005 Mosconi et al. , 2008 Langbaum et al. 2009) have demonstrated significant reductions of CMRgl in brain regions in patients with AD and its prodromal stage mild cognitive impairment (MCI), compared with normal control (NC) subjects. These reduction could be used for the early detection of AD.\nVoxel-based multiple testing methods are common approaches to identify significant group differences in CMRgl.\nIn a seminal paper, Benjamini and Hochberg (1995) introduced false discovery rate (FDR) as an alternative measure of Type I error in multiple testing problems to the family-wise error rate (FWER) , and demonstrated that the control of FDR is more powerful than that of FWER. We denote Benjamini and Hochberg (1995) method as BH. FDR is defined as the expected proportion of false rejections among all rejections. The false nondiscovery rate (FNR; Genovese and Wasserman 2002) , the expected proportion of falsely accepted hypotheses, is the corresponding measure of Type II error. An FDR procedure is valid if it controls FDR at a prespecified level \u03b1, and optimal if it has the smallest FNR among all level-\u03b1 FDR procedures (Sun and Cai 2009 ). The traditional FDR procedures Hochberg 1995, 2000; Genovese and Wasserman 2004) rely heavily on the assumption that the test statistics are independent, which rarely holds in practice. Although these approaches are shown to be valid under certain dependence assumptions (Benjamini and Yekutieli 2001; Farcomeni 2007; Wu 2008) , they may suffer from severe loss of power when the dependence structure is ignored (Yekutieli and Benjamini 1999; Genovese et al. 2006; Benjamini and Heller 2007; Sun and Cai 2009) . Even under independence, the local FDR (Lfdr) procedure is optimal and thus more powerful than those p-value based traditional procedures (Efron 2007; Sun and Cai 2007) . Modeling the dependence structure using the hidden Markov chain (HMC) method, Sun and Cai (2009) proposed an optimal oracle procedure based on a new test statistic, the local index of significance (LIS), and corresponding asymptotic data-driven procedure. Following the work of Sun and Cai (2009) , Wei et al. (2009) "}, {"section_title": "developed a pooled LIS (PLIS)", "text": "procedure for multiple-group analysis where different groups have different HMC dependence structures, and proved the optimality of the PLIS procedure. Either the LIS procedure or the PLIS procedure only handles the one-dimensional dependency.\nHowever, problems with higher dimensional dependence are of particular practical interest. For example, Li et al. (2010) implemented the LIS procedure for genome-wide association studies.\nFDR procedures have been widely used in analyzing functional neuroimaging data Nichols and Hayasaka 2003; Chumbley and Friston 2009, among many others). The current version of the well-known functional neuroimaging software Statistical Parametric Mapping (SPM), SPM8, contains both the BH procedure at the voxel level and the topological FDR procedures (Chumbley and Friston 2009; Chumbley et al. 2010 ) that control the FDR of clusters or peaks in a way that takes into account the spatial structure among voxels. We focus on the voxel-level FDR procedures in this article by developing a LIS-based FDR procedures for threedimensional (3D) image data using a hidden Markov random field model (HMRF)\nfor the spatial dependency among multiple tests.\nHMRF model is a generalization of HMC model, which replaces the underlying Markov chain by Markov random field. In particular, the Ising model, a classical Markov random field, has been used to capture the spatial structure in imaging analysis (Bremaud 1999; Winkler 2003) . In this article, we consider the Brodmann's participation of the cerebral cortex and subcortical regions of the human brain (Garey 2006) . We assume that the unobservable states of voxels in a region on lattice S of the image grid, \u0398 = {\u0398 s : s \u2208 S}, follow a two-parameter Ising model, where \u0398 s = 1 if there is a signal at voxel s (i.e., hypothesis s is nonnull), and \u0398 s = 0 otherwise.\nThe observed data, test statistics X = {X s : s \u2208 S}, are independently generated from the standard normal distribution under the null, and a normal mixture under the nonnull. This HMRF model is a natural extension of the HMC model of Sun and Cai (2009) .\nA major difficulty in estimating parameters in an HMRF model is the intractable computation of the normalizing constant in the underlying distribution. To avoid the difficulty, Chalmond (1989) used the pseudo-likelihood method proposed by Besag (1974) to approximate the likelihood as a product of tractable conditional distributions. However, Geyer and Thompson (1992) pointed out that the pseudo-likelihood method may seriously overestimate the dependence. Zhang (1992) introduced the mean field theory from statistical mechanics (Chandler 1987) to solve the intractable problem, which turns out to be a generalized pseudo-likelihood that is sensitive to noise (Marroquin et al. 2003) . Both the algorithms of Chalmond (1989) and Zhang (1992) are based on the expectation-maximization (EM) algorithm (Dempster et al. 1977 ). Alternatively, Zhu et al. (2007) proposed the stochastic approximation EM algorithm for HMRFs based on maximum-likelihood estimation (MLE). Despite incorporating EM in the name, the algorithm is a gradient-type optimization per se, where the first-and second-order partial derivatives of the log-likelihood of the observed data are approximated by Monte Carlo averages, and unfortunately, the algorithm is restricted to the model where the conditional density P (X|\u0398) belongs to the exponential family.\nWe propose an EM-type algorithm to search for MLE or penalized MLE (Ridolfi 1997; Ciuperca et al. 2003 ) that prevents the degeneracy of MLE, and use Monte Carlo averages via Gibbs sampler (Geman and Geman 1984; Roberts and Smith 1994; Gilk et al. 1996) to overcome the intractability. In the M-step, we adapt a backtracking line search method in each iteration (Nocedal and Wright 2006) to find a point that increases the objective function, leading our algorithm to be a generalized EM algorithm (GEM; Dempster et al. 1977 "}, {"section_title": "A HIDDEN MARKOV RANDOM FIELD MODEL", "text": "Let S be a finite lattice of N voxels in an image grid, usually in a three-dimensional space. Let \u0398 = {\u0398 s \u2208 {0, 1} : s \u2208 S} denote the set of latent states for multiple testing in S, where \u0398 s = 1 if hypothesis s is nonnull and \u0398 s = 0 otherwise. Suppose \u0398 be generated from a two-parameter Ising model with the following probability distribution\nwhere\nand s, t denotes all the pairs (s, t) in S with s, t = t, s such that for any s, t is among the six nearest neighbors of voxel s. This model possesses the Markov property:\nwhere S \\ {s} denotes the set S after removing s, and N (s) \u2282 S is the nearest neighborhood of s in S.\nFor the above Ising model, it can also be shown that\nTherefore, if s and t are neighbors, \u03b2 is equal to a log odds ratio that describes the association between \u0398 s and \u0398 t conditional on all the other state variables being withheld. We can see that \u03b2 reflects how likely the same-state voxels are clustered together. Similarly,\nwhich is the log odds for \u0398 s = 1 given that \u0398 N (s) are all zero. We thus generally assume \u03b2 \u2265 0 and h \u2264 0 accounting for the nonnegative dependency of neighboring voxels that have the same state. In addition, for a voxel s with m nearest neighbors, we have log\nwhere n is an integer satisfying 0 \u2264 n \u2264 m, which reflects the log ratio of the cluster effect of signals (nonnulls) relative to the cluster effect of noises (nulls).\nConditional on \u0398, we assume that the observed test statistics X = {X s : s \u2208 S} are independent with\nwhere P \u03c6 (x s |\u03b8 s ) denotes the following distribution\nwith unknown parameters \u03c6 = {\u00b5 l , \u03c3 2 l , p l : l = 1, 2, ...L}. Here we assume that the test statistic X s follows the standard normal distribution under the null, and the nonnull distribution is set to be the normal mixture that can be used to approximate a large collection of distributions (Magder and Zeger 1996; Efron 2004) It is well known that any one-dimensional MC is an MRF, and any onedimensional stationary finite-valued MRF is proved to be a MC (Chandgotia et al. 2014 ). When S is taken to be one-dimensional, then the above approach based on (1), (5) and (6) reduces to the HMC method of Sun and Cai (2009) ."}, {"section_title": "HIDDEN MARKOV RANDOM FIELD LIS-BASED FDR PROCEDURES", "text": "Sun and Cai (2009) developed a compound decision theoretic framework for multiple testing under HMC dependence and proposed LIS-based oracle and data-driven testing procedures that aim to minimize the FNR subject to a constraint on FDR. We extend these procedures under HMRF for imaging data. The oracle LIS for hypothe-\nbe the ordered LIS values and H (1) , ..., H (N ) the corresponding null hypotheses. The oracle testing procedure operates as follows: For a prespecified FDR level \u03b1,\nParameters \u03a6 are unknown in practice. We can use the data-driven testing procedure that simply replaces LIS (i) (x) in (7) with We now provide details of the LIS-based data-driven procedure for 3D imaging data. First of all, the parameters of the HMRF model need to be estimated from observed test data."}, {"section_title": "A Generalized EM Algorithm", "text": "We propose an EM approach for the estimation of parameters in the HMRF model characterized by (1), (5) and (6) by maximizing the observed likelihood function\n. We first introduce unobservable categorical variables K = {K s :\nKs ), where K s \u2208 {0}1(\u0398 s = 0) + {l = 1, ..., L}1(\u0398 s = 1) with 1(\u00b7) being the indicator function. To estimate the HMRF parameters \u03a6 = {\u03c6, \u03d5}, (\u0398, K, X) are used as the complete data variables to construct the auxiliary function in the (t + 1)st iteration of EM algorithm given the observed data x and the current estimated parameters \u03a6 (t) :\nwhere\nfunction can be further written as follows\nwhere\nand\nTherefore, we can maximize Q(\u03a6|\u03a6 (t) ) for \u03a6 by maximizing Q 1 (\u03c6|\u03a6 (t) ) for \u03c6 and Q 2 (\u03d5|\u03a6 (t) ) for \u03d5, separately.\nMaximizing Q 1 (\u03c6|\u03a6 (t) ) under the constraint l p l = 1 by the method of Lagrange multipliers yields\nwhere\nFor Q 2 (\u03d5|\u03a6 (t) ), taking its first and second derivatives with respect to \u03d5, we obtain\nMaximizing Q 2 (\u03d5|\u03a6 (t) ) is then equivalent to solving the nonlinear equation:\nIt can be shown that equation (16) has a unique solution and can be solved by the Newton-Raphson (NR) method (Stoer and Bulirsch 2002) . However, a starting point that is not close enough to the solution may result in divergence of the NR method. Therefore, rather than searching for the solution of equation (16) over all \u03d5,\nwe choose a \u03d5 (t+1) that increases Q 2 (\u03d5|\u03a6 (t) ) over its value at \u03d5 = \u03d5 (t) . Together with the maximization of Q 1 (\u03c6|\u03a6 (t) ), the approach leads to\nwhich is termed a generalized EM (GEM) algorithm (Dempster et al. 1977) . To find such a \u03d5 (t+1) that increases the Q 2 -function, a backtracking line search algorithm (Nocedal and Wright 2006) is applied with a set of decreasing positive values \u03bb m in the following\nwhere m = 0, 1, ..., and \u03d5 (t+1) = \u03d5 (t+1,m) which is the first one satisfying the Armijo condition (Nocedal and Wright 2006 )\nSince I(\u03d5 (t) ) is positive-definite, the Armijo condition guarantees the increase of Q 2 -function. In practice, \u03b1 is chosen to be quite small. We adopt \u03b1 = 10 \u22124 , which is recommended by Nocedal and Wright (2006) , and halve the Newton-Raphson step length each time by using \u03bb m = 2 \u2212m . By the ergodic theorem of the Gibbs sampler (Roberts and Smith 1994) ,\nwhere {\u03b8 (t,1,x) , ..., \u03b8 (t,n,x) } are large n samples generated by the Gibbs sampler from\ns } s\u2208S being the normalizing constant, and {\u03b8 (1,\u03d5) , ..., \u03b8 (n,\u03d5) } are generated from P \u03d5 (\u03b8). Here for vector v, v \u22972 = vv T . Similarly,\nwhere C is the number of all possible configurations \u03b8 of \u0398. Then the difference between Q 2 -functions in the Armijo condition can be approximated by\nReaders interested in simulating the normalizing constant Z(\u03d5) are referred to the work of Gelman and Meng (1998) . Back to Q 1 (\u03c6|\u03a6 (t) ), the local conditional probability of \u0398 given x can also be approximated by the Gibbs sampler:\nWhen the number of components in the nonnull mixture L \u2265 2, if \u00b5 l = x s and \nwhere g is a proper probability density function that yields similar results with (11), (12) and (13). We choose g(\u03c3 2 l ) to be the inverse gamma distribution, i.e,\nwhere a > 0, b > 1. For a finite mixture of normal distributions, the penalized maximum likelihood estimator (PMLE) based on the inverse gamma distribution as the penalty function has strong consistency and asymptotic normality (Ciuperca et al. 2003; Chen et al. 2008 ), therefore we think the inverse gamma distribution would be a reasonable choice for our model, which performs well in our simulations. Formal proofs of consistency and asymptotic normality are still open questions. The penalized likelihood approach is equivalent to setting g(\u03c3 2 l ) to be the prior distribution for each \u03c3 2 l , leading to the auxiliary function for the penalized EM algorithm as follows:\nwhere\nand Q 2 (\u03d5|\u03a6 (t) ) is the same as equation (10). Then only equation (13) becomes\nand equations (11) and (12) for other parameters remain unchanged."}, {"section_title": "Implementation of the LIS-Based FDR Procedure", "text": "The algorithm for the LIS-based data-driven procedure, denoted as LIS for single group analysis, SLIS for separate analysis of multiple groups, and PLIS for pooled analysis for multiple groups, is given below:\n1. Set initial values \u03a6 (0) = {\u03c6 (0) , \u03d5 (0) } for the model parameters \u03a6 of each group; 2. Update \u03c6 (t) from equations (11), (12) and (13) 3. Update \u03d5 (t) from equations (17) and (18); 4. Iterate Steps 2 and 3 until convergence, then obtain the estimate\u03a6 of \u03a6;\n5. Plug-in\u03a6 to obtain the test statistics LIS from equation (19); 6. Apply the data-driven procedure (LIS, SLIS or PLIS).\nThe GEM algorithm is stopped when the following stopping rule\nwhere \u03a6 i is the ith coordinate of vector \u03a6, is satisfied for three consecutive regular\nNewton-Raphson iterations with m = 0 in (17), or the pre-specified maximum number of iterations is reached.\nStopping rule (21) was applied by Booth and Hobert (1999) to the Monte Carlo EM method, where they set 1 = 0.001, 2 between 0.002 and 0.005, and the rule to be satisfied for three consecutive iterations to avoid stopping the algorithm prematurely because of Monte Carlo error. We used 1 = 2 = 0.001 in simulation studies and real-data analysis. Constant \u03b1 = 10 \u22124 is recommended by Nocedal and Wright (2006) for the Armijo condition (18), and the Newton-Raphson step length in (17) is halved by using \u03bb m = 2 \u2212m . In practice, the Armijo condition (18) might not be satisfied when the step length \u03d5 (t+1,m) \u2212 \u03d5 (t) is very small. In this situation, the iteration within Step 3 is stopped by an alternative criterion "}, {"section_title": "SIMULATION STUDIES", "text": "The simulation setups are similar to those in Sun and Cai (2009) The BH procedure is implemented globally for the multiple-group simulation. For the Lfdr or CLfdr procedure, we use the proportion of the null cases generated from the Ising model with given parameters as the estimate of the probability of the null cases P (\u0398 s = 0), together with the given null and nonnull distributions without estimating their parameters. For the LIS-based data-driven procedures, the maximum number of GEM iterations is set to be 1,000 with 1 = 2 = 0.001, 3 = \u03b1 = 10 \u22124 , a = 1 and b = 2. For the Gibbs sampler (Roberts and Smith 1994; Gilk et al. 1996) , 5,000 samples are generated from 5,000 iterations after a burn-in period of 1,000 iterations. The MRF \u0398 = {\u0398 s : s \u2208 S} is generated from the Ising model (1) with parameters (\u03b2, h), and the observations X = {X s : s \u2208 S} are generated conditionally on \u0398 Figure 1 shows the comparisons of the performance of BH, Lfdr, OR and LIS. In Figure 1(1a-1c) , we fix h = \u22122.5, set \u00b5 1 = 2 and \u03c3 2 1 = 1, and plot FDR, FNR, and the average number of true positives (ATP) yielded by these procedures as functions of \u03b2. In Figure 1(2a-2c) , we fix \u03b2 = 0.8, set \u00b5 1 = 2 and \u03c3 2 1 = 1, and plot FDR, FNR and ATP as functions of h. In Figure 1(3a-3c) , we fix \u03b2 = 0.8 and h = \u22122.5, set \u03c3 2 1 = 1, and plot FDR, FNR and ATP as functions of \u00b5 1 . The initial values for the numerical algorithm are set at From Figure 1(1a-3a) , we can see that the FDR levels of all four procedures are controlled around 0.10 except one case of the LIS procedure in Figure 1(3a) with the lowest \u00b5 1 , whereas the BH and Lfdr procedures are generally conservative. In Figure   1(3a) , the obvious deviation of the FDR level from 0.10 by the LIS procedure for the case \u00b5 1 = 1 is likely caused by the small lattice size N . As a confirmation, additional simulations by increasing the lattice size N to 30 \u00d7 30 \u00d7 30 yield a FDR of 0.1018 for the same setup. From Figure 1(1b-3b) and (1c-3c) we can see that the two curves of OR and LIS procedures are almost identical, indicating that the data-driven LIS procedure works equally well as the OR procedure. These plots also show that the LIS procedure outperforms the two independent-case procedures (BH and Lfdr), with increased margin of performance in FNR and ATP as \u03b2 or h increases or \u00b5 1 is at a moderate level. Note that from (3) and (4), we can see that \u03b2 controls how likely the same-state cases cluster together, and \u03b2 and h together control the proportion of the aggregation of nonnulls relative to that of nulls."}, {"section_title": "Study 2: L = 2", "text": "We now consider the case where the nonnull distribution is a mixture of two normal distributions. The MRF is generated from the Ising model (1) with fixed parameters \u03b2 = 0.8 and h = \u22122.5. The nonnull distribution is a two-component normal mixture\n2 ) with fixed p 1 = p 2 = 0.5, \u00b5 2 = 2, and \u03c3 The results from both simulation studies are very similar to that in Sun and Cai (2009) for the one-dimensional case using HMC. It is clearly seen that, for dependent tests, incorporating dependence structure into a multiple-testing procedure improves efficiency dramatically. "}, {"section_title": "Multiple-Group Analysis", "text": "Voxels in a human brain can be naturally grouped into multiple functional regions. For simulations with grouped multiple tests, we consider two lattice groups each with size 15\u00d715\u00d715. The corresponding MRFs \u0398 1 = {\u0398 1s : s \u2208 S} and \u0398 2 = {\u0398 2s : s \u2208 S} are generated from the Ising model (1) with parameters (\u03b2 1 = 0.2, h 1 = \u22121) and (\u03b2 2 = 0.8, h 2 = \u22122.5), respectively. The observations X k = {X ks , s \u2208 S} are generated The simulation results are presented in Figure 3 , which are similar to that in Wei et al. (2009) for the one-dimensional case with multiple groups using HMCs. "}, {"section_title": "ADNI FDG-PET IMAGE DATA ANALYSIS", "text": "We apply the PLIS procedure with HMRFs to the analysis of FDG-PET image data obtained from the ADNI database, which is compared with the BH and CLfdr procedures. Each image is normalized by the average value of pons and cerebellar vermis.\nSixty-one brain regions of interest (ROIs) were included in the analysis, where the number of voxels in each region ranges from 149 to 20,680 with a median of 2,517.\nThe total number of voxels of these 61 ROIs is N = 251, 500. The goal is to identify voxels with different metabolic rates of glucose between AD and NC, AD and MCI, and MCI and NC. We only report the comparison of MCI and NC in this article, which has the weakest signals in general among all three comparisons.\nWe implement the BH procedure globally for the 61 ROIs, whereas treat each region as a group for the CLfdr and PLIS procedures. The null distribution is assumed to be the standard normal distribution. The nonnull distribution is assumed to be a two-component normal mixture for PLIS. For the BH procedure, a total number of N two-sample Welch's t-tests (Welch 1947) number of GEM iterations is set to be 5,000 with 1 = 2 = 0.001, 3 = \u03b1 = 10 \u22124 , a = 1 and b = 2. For the Gibbs sampler embedded in the GEM, 5,000 samples are generated from 5,000 iterations after a burn-in period of 1,000 iterations. In this data analysis, the GEM algorithm reaches the maximum iteration and is then claimed to be converged for five ROIs. The results are summarized in Figure 4 , where the z-values for all the signals found by each procedure are plotted. Since the FDG scans were normalized to the average of pons and cerebellar vermis, areas of the brain known to be least affected in AD, it was not surprising that almost all the signals are found with decreased CMRgl. Both PLIS and CLfdr procedures discovered significant metabolic reduction clustered (with a regional proportion of signals > 50%) in brain regions preferentially affected by AD, including the posterior cingulate (Mosconi et al. 2008; Langbaum et al. 2009 ), parietal cortex (Minoshima et al. 1995; Matsuda 2001) , temporal cortex (Alexander et al. 2002; Landau et al. 2011) , medial temporal cortex (Karow et al. 2010) , frontal cortex (Mosconi 2005) , insular cortex (Perneczky et al. 2007 ), amygdala (Nestor et al. 2003; Gray et al. 2012 ) and hippocampus . In regions also typically affected in AD, such as anterior cingulate (Fouquet et al. 2009 ) and occipital cortex (Langbaum et al. 2009 ), the proportions of signals found by PLIS are 49.6% and 39.0%, respectively, compared with 35.4% and 11.6% found by CLfdr as well as only 1.24% and 0.87% by BH.\nWith respect to the regions that are relatively spared from AD (Benson et al. 1983; Matsuda 2001; Ishii 2002) or rarely reported in the literature of the disease, caudate, thalamus and putamen are found with high proportions of signals by PLIS (> 45%) and CLfdr (> 25%) in each of these regions; signals in medulla, midbrain, cerebellar hemispheres, pre-motor cortex and primary somatosensory cortex are each claimed with a proportion greater than 20% by PLIS, but very sparse found by both CLfdr and BH. Since MCI as a group consists of a mix of patients, many of them will progress to AD but some will not which may include subjects with corticobasal degeneration (Ishii 2002) , frontotemporal dementia (Jeong et al. 2005 ), or Parkinsonism (Huang et al. 2007 Zeman et al. 2011; Ishii 2013) , it is not surprising that some areas not typical of AD patients were found to be abnormal in the MCI group."}]