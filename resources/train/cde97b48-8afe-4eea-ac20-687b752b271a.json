[{"section_title": "Abstract", "text": "Pre-symptomatic (or Preclinical) Alzheimer's Disease is defined by biomarker evidence of fibrillar amyloid beta pathology in the absence of clinical symptoms. Clinical trials in this early phase of disease are challenging due to the slow rate of disease progression as measured by periodic cognitive performance tests or by transition to a diagnosis of Mild Cognitive Impairment. In a multisite study, experts provide diagnoses by central chart review without the benefit of in-person assessment. We use a simulation study to demonstrate that models of repeated cognitive assessments detect treatment effects more efficiently compared to models of time-to-progression to an endpoint such as change in diagnosis. Multivariate continuous data are simulated from a Bayesian joint mixed effects model fit to data from the Alzheimer's Disease Neuroimaging Initiative. Simulated progression events are algorithmically derived from the continuous assessments using a random forest model fit to the same data. We find that power is approximately doubled with models of repeated continuous outcomes compared to the time-to-progression analysis. The simulations also demonstrate that a plausible informative missing data pattern can induce a bias which inflates treatment effects, yet 5% Type I error is maintained."}, {"section_title": "Introduction", "text": "Pre-symptomatic (or Preclinical) Alzheimer's Disease (PAD) is defined by evidence of abnormal levels of fibrillar amyloid beta in brain as measured by positron emission tomography (PET) brain scan or cerebrospinal fluid (CSF) assay [1] . Clinical trials have been initiated"}, {"section_title": "Data", "text": "ADNI is a prospective observational cohort study, led by Principal Investigator Michael W. Weiner, MD, which is tracking cognitive, imaging, and biofluid markers of Alzheimer's in volunteers diagnosed as cognitively normal (CN), subjective memory concern (SMC), mild cognitive impairment (MCI) and mild-to-moderate dementia. To simulate both longitudinal continuous markers and time-to-MCI for a Preclinical AD (PAD) clinical trial, we first model the disease markers and clinical diagnosis using data from PAD ADNI participants. The PAD population is defined by a diagnosis of CN or SMC at baseline, and florbetapir PET standardized uptake value ratio (SUVR) above 1.11 [10] or CSF amyloid beta (A\u03b2) below 950.6 pg/ml. The CSF threshold of 950.6 pg/ml was selected because it yields the same proportion of PAD as the 1.11 SUVR threshold. Follow-up observations, including a site clinician's diagnosis of CN, MCI, or dementia, are collected every three, six, or 12 months. For more information on the study design of ADNI, including protocols, see adni.loni. usc.edu.\nSensitive tests of cognition may show changes in PAD many years before the onset of functional decline [5, 11] . In this work, we focus on seven cognitive outcomes in the PAD population, namely:\n1. ADAS Delayed Word Recall (ADAS-DWR) [12] , 2. Logical Memory Paragraph Recall (LogMem) [13] , 3 . Trail Making Test Part B (Trails B) [14] , 4. Mini-Mental State Examination (MMSE) [15] , 5. Category Fluency -Animals, 6 . Clinical Dementia Rating -Sum of Boxes (CDRSB) [16] and 7. Functional Assessment Questionnaire (FAQ).\nBaseline covariates considered include age and carriage of an apolipoportein E4 (APOE\u03b54) allele. The PAD population includes a total of N =163 individuals, in which N =39 (23.9%) were observed to progress to MCI over a median follow-up time of 4.0 years (interquartile range 2.1 to 5.6 years; maximum 11.5 years). Baseline characteristics of the modeled PAD cohort are presented in Table 1 ."}, {"section_title": "Methods", "text": ""}, {"section_title": "Joint mixed-effects model for longitudinal data", "text": "To a derive a model to simulate plausible data, we first fit a model to observed ADNI data. We apply a joint (or multivariate) mixed-effects model (JMM) to simultaneous model continuous longitudinal data for disease markers in the PAD population. The model respects the within-subject correlation over time and among the battery outcomes.\nSuppose we have a set of n subjects followed over a time interval [0, \u03c4 ). The ith subject provides a set of longitudinal quantitative measurements {y ijk , j = 1, \u00b7 \u00b7 \u00b7 , n ik , k = 1, \u00b7 \u00b7 \u00b7 , p} at time points {t ijk , j = 1, \u00b7 \u00b7 \u00b7 , n i , k = 1, \u00b7 \u00b7 \u00b7 , p}. Linear mixed-effects models are commonly used to model continuous longitudinal data. The multivariate mixed-effects model is specified as y ijk = x ijk \u03b2 k + b 0ik + b 1ik t ijk + \u03b5 ijk , where \u03b2 k are fixed-effect regression coefficients, b 0ik and b 1ik are the subject-and outcome-specific random intercept and slope for individual i and outcome k. The random effects are assumed to follow a multivariate Gaussian distribution with mean vector 0 and variance-covariance matrix \u03a3 with dimension 2p, that is (\nThe model with multivariate random effects has the advantage of reflecting the dependency within subjects and among outcomes. The \u03b5 ijk \u223c N (0, \u03c3 2 k ) is a measurement error term, which accounts for outcome-specific variance. Since the outcomes are in different scales, we transform the raw outcome measures into a quantile scale ranging from 0 to 1 (least impaired to most severe dementia). Quantiles are calculated using the empirical cumulative distribution function using weights that are inversely proportional to the number of observations from each diagnostic category for each outcome. The quantiles were then transformed by the inverse Gaussian quantile function resulting an approximate Z-score before submitting to the model. When simulating data from these models, the simulated Z-scores can then be back transformed to the original scale, which is integer valued for some outcomes.\nBayesian estimation is performed via Markov Chain Monte Carlo (MCMC) sampling using the stan mvmer function in R package Rstanarm [17] . Because the stan mvmer function is limited to a maximum of three outcomes, we have coded our own version allowing up to twenty outcomes (available from github.com/mcdonohue/rstanarm)."}, {"section_title": "Random forest algorithm for diagnosis of MCI", "text": "In order to simulate a clinician's diagnosis of MCI or worse impairment, we first use ADNI data to learn an algorithm to approximate this decision. The random forest algorithm [18] is an ensemble learning method for classification and regression. It operates by generating several decision trees and aggregating them. It provides reasonable and easily interpretable model when a large number of predictors are present in the data and enables applications with mixed data-types such as continuous and categorical data.\nIn our application, clinician diagnosis of normal cognition versus MCI or worse impairment is the binary outcome variable, and the seven continuous markers, age and education are the predictors. The model is fit using the R package randomForest [19] . The fitted model is then applied to simulated continuous outcomes to predict a clinician's diagnosis."}, {"section_title": "3.3.", "text": "Competing clinical trial models for continuous and time-to-event outcomes in simulation study The simulated treatment effect on time-to-progression is modeled by the Cox proportional hazards model. For the continuous PACC, we consider MMRM and the the constrained longitudinal data analysis (cLDA) proposed by Liang and Zeger [9] . Like most likelihoodbased approaches for longitudinal data, all three models assume any missing data are missing at random (MAR).\nThe PACC is used as the continuous outcome measure for the PAD trials simulation study. The version of the PACC used in the study is a composite of four assessments: ADAS-DWR, LogMem, log transformation of Trails B, and MMSE. Each of the four component scores is first centered by subtracting the baseline sample mean and then divided by the baseline sample standard deviation of that component, to form standardized Z-scores. These Zscores are averaged to form the composite.\nThe MMRM for treats change from baseline in the PACC score as the outcome and baseline PACC as a predictor. It treats time as a categorical variable, which allows general mean trends in each group. MMRM has been extensively used for testing treatment effects at specific time points in clinical trials, since participants are often evaluated at a fixed and relatively small number of time points [20] . In our simulation study, the within-subject dependence is modeled by a first-order autoregressive covariance structure.\nWe also explore models that treat time as a continuous variable. In cLDA, the baseline outcome is treated as a response variable rather than a covariate, and constrained to have equal mean at baseline across treatment groups [21, 22] . We explore models with linear or quadratic time trends for each group."}, {"section_title": "Simulation set-up", "text": "We conduct a simulation study to evaluate the performance of the competing models described in Section 3.3. In each of 1000 simulated clinical trials with visits every 6 months from 0 to 8 years, a total of 1000 and 1500 patients are respectively randomized to either treatment or placebo in 1:1 ratio. We also assume the proportion of MCI progressors is 24% (based on ADNI data, as noted above).\nFor the placebo group, no changes will be made to the JMM fit to ADNI. For the treatment group, we will impose large (40% improvement on rate of change over the control), moderate (30% improvement), small (20% improvement) and null (same as the control) treatment effects on all outcomes. The PACC scores are calculated by taking the average of the four simulated component Z-scores.\nTo simulate non-ignorable missing data, three dropout categories are considered: intolerability, inefficacy and missing completed at random (MCAR). Participants having intolerability or inefficacy drop out from the study immediately after six and twelve months, respectively. For MCAR, we assume linear attrition rate of 5% per year for both the treatment and placebo groups. The simulated dropout rates are: -Treatment group: -Null: inefficacy (15%), intolerability (10%), MCAR (5%/year attrition rate); -Alternative: inefficacy (8%), intolerability (10%), MCAR (5%/year attrition rate); -Placebo group: inefficacy (15%), MCAR (5%/year attrition rate).\nIn order to assess bias due to missing data, we simulate complete data for every subject. The complete data is appropriately censored for the analysis of \"observed\" data, and left uncensored for analysis of the \"complete\" data. Completers and MCAR dropouts are assumed to have the same longitudinal mean profile within each treatment arm. Dropouts due to intolerability are simulated to have the expected benefit, on average, until dropout, followed by an \"unobserved\" benefit that is diminished by a factor of 15%. Dropouts due to inefficacy are simulated to have no benefit.\nThe four competing clinical trial models are MMRM, cLDA 1 (linear) and cLDA 2 (quadratic) for continuous PACC scores; and Cox for time-to-progression, with two baseline covariates: age at baseline and carriage of the APOE\u03b54allele. The Cox model will use all data observed out to 8 years until the last subject reaches the final scheduled visit under the common close design. We assume a linear enrollment rate such that enrollment is completed in 4 years and about half the subjects contribute \"extra\" common close follow-up in the 4.5 to 8 year range to the Cox model. The MMRM, cLDA 1 and cLDA 2 will only use data up to last scheduled visit, i.e., from 0 to 4.5 years.\nWe focus on \"treatment policy\" estimands of interest. The estimand will be the difference between randomized groups in the intention-to-treat population in terms of either: (I) Rate (hazard ratio) of progression to MCI/Dementia (Cox); (II) Group difference in PACC at final study time point (MMRM and cLDA 1 ); or (III) Area between mean PACC curves (cLDA 2 ). We show how to carry out the hypothesis test of case (III) in the Appendix. Let Y ijk denote the simulated PACC scores for subject i randomized to group j at time point k, where i = 1, \u00b7 \u00b7 \u00b7 , n j , j = D, P and k = 1, \u00b7 \u00b7 \u00b7 , T . And k = 0 represents the baseline time point, D is the treatment group and P is the placebo group. If the estimand of interest is the change from baseline at time T , i.e., Y ijT \u2212 Y ij0 . The object is to estimate the between-treatment difference \u03b4 = \u00b5 P \u2212 \u00b5 D , where \u00b5 j = E (Y ijT \u2212 Y ij0 ). A two-tailed test H 0 : \u03b4 = 0 versus H 1 : \u03b4 = 0 is carried out to evaluate whether treatment is different from placebo.\nFor each simulated dataset, we apply all four competing models to calculate point estimates of \u03b4 using the observed data (i.e., \u03b4 obs ) and the complete data (i.e., \u03b4 comp ). For each model, \"bias\" is calculated as the median of the 1000 point estimates of \u03b4 obs minus \u03b4 comp ; \"bias in percent\" is computed as the median of the 1000 points estimates of \u03b4 obs minus \u03b4 comp and then divided by \u03b4 comp . The interquartiles Q 1 and Q 3 are also summarized.\nIn a real clinical trial, the endpoint is measured for completers but is missing for those who either drop out from the study either due to inefficacy or intolerability or those who remain in the study after initiating rescue medication. Mehrotra, et al. [23] discussed that the commonly used MMRM with the embedded MAR assumption can deliver an exaggerated estimate of the aforementioned estimand of interest, in favor of the drug. This happens, in part, due to implicit imputation of an overly optimistic mean for dropouts in the treatment group. To remedy this, they proposed a formula-based two-step approach by treating the true endpoint distribution for treatment group as a mixture of distributions (one each for the completers and dropouts) rather than a single distribution. Their approach reduces the bias associated with the traditional MMRM while maintaining power. To increase the precision in estimating \u03b4, we apply their method to MMRM, cLDA 1 and cLDA 2 models in the simulation study."}, {"section_title": "Results", "text": ""}, {"section_title": "JMM and random forest fit to ADNI data", "text": "We fit a JMM for PAD participants who were observed to progress to MCI and a separate JMM for those who did not progress. Seven outcome measures described in Section 2 are included in the model. Fixed effect covariates for each outcome include age at baseline and carriage of the APOE\u03b54allele. Three parallel Markov chains are run for 4000 iterations and the first 2000 warm-up iterations are discarded. Every fourth value of the remaining part of each chain is stored to reduce correlation, yielding a total of 1500 samples for posterior analysis. Table 2 shows the posterior means and 95% credible intervals of the covariate-effect parameters. Figure 1 shows the subject-level observations and predictions according to time in years of the seven markers for all individuals, in which the blue and red lines are the curves using the LOESS smoother. The bottom panel shows that the predictions provide reasonable trends of the observations. The posterior estimates from JMM will be later used as the true parameter values to simulate the panel of continuous markers.\nFor the random forest, 500 trees are fitted and the number of variables selected at each split is 3. The node impurity of each tree is measured by the Gini index. The results show that CDRSB, LogMem and FAQ are three most important outcomes for determining the diagnosis of MCI. The model has a 6.19% out-of-bag error rate and 93.81% out-ofbag accuracy rate. Using the fitted random forest, the simulated cognitive status can be obtained from the simulated continuous markers. Figure 2 shows the Kaplan-Meier estimated progression rate of the ADNI-PAD population (black solid line) along with the progression rate from one large simulated placebo group (red dots). The simulated progression yields closer concordance with the Kaplan-Meier estimates at the earlier stage. Although we observe discrepancies between the two lines in the middle and the right tail, the red line still lies within the 95% confidence intervals. Both the subject-level trajectories and the progression rate illustrate that the simulated data plausibly mimics the observed data. Figure 3 shows the results of one simulated clinical trial with a 20% treatment effect and sample size n = 1000. The figure illustrates the group trends obtained by fitting the four different models."}, {"section_title": "Simulation results", "text": "Simulated power and Type I error are summarized in Table 3 . Under the null hypothesis (no treatment effect), the MMRM exhibits smaller than expected Type I error (about 2%), whereas the other models are closer to the expect 5% error rate. The Cox model consistently exhibits the weakest power of the four models. MMRM has the next best performance, followed by the quadratic (cLDA 2 ) and linear (cLDA 1 ) models. For example, with a trial of size N =1,000 subjects of drug with a 30% treatment effect, the simulated power is 33% for Cox, 79% for MMRM, 86% for cLDA 2 , and 96% for cLDA 1 . In comparing analysis of complete versus observed data, it seems the missing data does not increase Type I error, but it does inflate power. This suggests the bias is only an issue with an effective drug, in which case the effectiveness might appear inflated. Figure 4 shows the powers in all scenarios. Tables 4 and 5 further examine the bias induced by the missing data pattern. The tables summarize the median and interquartile ranges (Q 1 , Q 3 ) of the bias on the PACC scale (Tables 4) and as a percent of effect seen in complete data (5). The Cox model seems to have smaller bias with 20% treatment effect, but as the treatment grows, the bias is comparable for all models. The method proposed by Mehrotra, et al. [23] successfully shrinks the magnitude of bias, e.g. from 27% in favor of treatment to -4.4% in favor of placebo for MMRM with 20% treatment effect. The method appears to overcorrect the bias in favor of placebo in these simulations."}, {"section_title": "Discussion", "text": "We use Bayesian joint mixed effects models fit using ADNI data to simulate correlated longitudinal data that might plausibly arise in a PAD clinical trial. We used a random forest algorithm, also fit using ADNI, to algorithmically diagnose MCI in the simulated data so that we could compare models of the PACC to the Cox model of time-to-progression. The models of PACC consistently provide at least twice the power of the Cox model even when the Cox model has the benefit of considerably more follow-up under a common close design. Given this inefficiency, the time-to-progression analysis should be avoided in PAD.\nSome might still argue that the clinical meaningfulness of the time-to-progression is worth the cost of a larger, longer trial. However, given that the random forest provided a purely algorithmic diagnosis with 93.81% out-of-bag accuracy suggests that there is minimal additional value in the diagnosis. And again, while the progression outcome is more qualitative than the PACC on the subject level, the group level result is still quantitative (e.g. a hazard ratio) and requires additional interpretation to assign clinical meaning.\nOne might also argued that clinical diagnosis cannot be adequately modelled algorithmically using trial data. That is, clinical assessment and diagnosis by a trial site clinician may consider information not captured by trial measures. But the cognitive, clinical and functional assessments are designed to capture the relevant information, and clinicians generally rely on similar information obtained through less structured assessments. It seems questionable that a site clinician will gain much reliable information beyond the assessments; indeed, this is the justification for central expert panel adjudication of site diagnoses.\nThe Bayesian joint models are well-suited to simulating plausible panels of correlated longitudinal data necessary to compare clinical trial designs. This approach could be useful in many other contexts where one is interested in a fair comparison of different outcome measures, different combinations of correlated outcomes, or different models of treatment effect. Simulations which ignore the correlations among important outcomes will likely not provide reliable comparisons.\nAll of the models considered were susceptible to bias induced by a plausible missing data pattern. However, this bias seemed to only affect scenarios with an effective treatment and did not inflate Type I error under the null hypothesis. The Mehrotra method shows promise in correcting this bias, but it might overcorrect in favor of placebo, and it would be impossible to detect this overcorrection in practice. Given that Type I error is not inflated, we are inclined to suggest no change to the status quo approach in which the primary analysis is based on likelihood-based methods which are robust to MAR, and applying appropriate MNAR sensitivity analyses such as the delta method [24] ."}, {"section_title": "Conflicts of interest", "text": "The authors declare no potential conflicts of interest."}, {"section_title": "Acknowledgments", "text": "We are grateful to the ADNI study volunteers and their families. "}, {"section_title": "Appendix", "text": "For cLDA model with quadratic time effects, we can write the part of fixed effects as\nThe area between the curves of active group and placebo group is\nThe null hypothesis is H 0 : S trt\u2212pb = 0. We use the R package glht to carry out the hypothesis test. "}]