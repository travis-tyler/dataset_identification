[{"section_title": "Abstract", "text": "Joint models for longitudinal and time-to-event data are commonly used in longitudinal studies to forecast disease trajectories over time. Despite the many advantages of joint modeling, the standard forms suffer from limitations that arise from a fixed model specification and computational difficulties when applied to large datasets. We adopt a deep learning approach to address these limitations, enhancing existing methods with the flexibility and scalability of deep neural networks while retaining the benefits of joint modeling. Using data from the Alzheimer's Disease Neuroimaging Institute 1 , we show improvements in performance and scalability compared to traditional methods."}, {"section_title": "OVERVIEW", "text": "Effective clinical decision support often involves the dynamic forecasting of medical conditions based on clinically relevant variables collected over time. This involves jointly predicting the expected time to events of interest (e.g. death), biomarker trajectories, and other associated risks at different stages of disease progression.\nWith the prevalence of aging populations around the globe, Alzheimer's disease (AD) is a significant threat to public health -growing from being relatively rare at the start of the 20th century to having a case being reported every 7 seconds around the world [1] . Patients at risk of developing Alzheimer's disease are usually monitored over time based on longitudinal cognitive scores and MRI measurements [6] , which help doctors evaluate the severity of a patient's condition and formulate a diagnosis. As such, the ability to produce joint forecasts -such as those depicted in Figure 1 -Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD '18, August 2018, London, UK \u00a9 2018 Copyright held by the owner/author(s). ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. https://doi.org/10.1145/nnnnnnn.nnnnnnn would help doctors determine both the likelihood of developing dementia and the expected rate of deterioration of a given patient, potentially allowing for intervention at an early stage.\nGiven the promising initial results found in our companion paper for Cystic Fibrosis patients [5] , we investigate the application of the Disease-Atlas -a novel conception of the joint modeling framework using deep learning -to jointly predicting the expected time-to-transition to Alzheimer's disease and the values of longitudinal measurements, providing additional clinical decision support to doctors evaluating potential Alzheimer's disease patients. We start with an overview of the Disease-Atlas in Section 2 and 3, demonstrating performance gains for tests on data from the ADNI in Section 4. "}, {"section_title": "PROBLEM DEFINITION", "text": "For a given longitudinal study, let there be N patients with observations made at time t, for 0 \u2264 t \u2264 T cens where T cens denotes an administrative censoring time 2 . For the i th patient at time t, observations are made for a K-dimensional vector of longitudinal variables is defined to be the first time the event is observed after t, which allows us to model both repeated events and events that lead to censoring (e.g. death). The final observation for patient i occurs at\n), where {a i , . . . , a max } is the set of indices for events that censor observations."}, {"section_title": "DISEASE-ATLAS ARCHITECTURE", "text": "The Disease-Atlas captures the associations within the joint modeling framework, by learning shared representations between trajectories at different stages of the network, while retaining the same sub-model distributions captured by joint models. The network, as shown in Figure 2 , is conceptually divided into 3 sections: 1) A shared temporal layer to learn the temporal and cross-correlations between variables, 2) task-specific layers to learn shared representations between related trajectories, and 3) an output layer which computes parameters for predictive sub-model distributions for use in likelihood loss computations during training and generating predictive distributions at run-time.\nThe equations for each layer are listed in detail below. For notational convenience, we drop the subscript i for variables in this section, noting that the network is only applied to trajectories from one patient at time. While we focus on both the continuous valued and time-to-event predictions for tests in Section 4, we include descriptions of binary predictions here for completeness.\nShared Temporal Layer. We start with an RNN at the base of the network, which incorporates historical information into forecasts by updating its memory state over time. For the tests in Section 4, we adopt the use of a long-short term memory network (LSTM) in the base layer.\nWhere h t is the output of the RNN and m t its memory state. To generate uncertainty estimates for forecasts and retain consistency with joint models, we adopt the MC dropout approach described in [3] . Dropout masks are applied to the inputs, memory states and outputs of the RNN, and are also fixed across time steps. For memory updates, the RNN uses the Exponential Linear Unit (ELU) activation function.\nTask-specific Layers. For the task-specific layers, variables can be grouped according to the types of outputs, with layer z c,t for continuous-valued longitudinal variables, z b,t for binary longitudinal variables and z e,t for events. Dropout masks are also applied to the outputs of each layer here. At the inputs to the continuous and binary task layers, a prediction horizon \u03c4 is also concatenated with the outputs from the RNN. This allows the parameters of the predictive distributions at t + \u03c4 to be computed in the final layer, i.e.h t = [h t , \u03c4 ].\nOutput Layer. The final layer computes the parameter vectors of the predictive distribution, which are used to compute log likelihoods during training and dynamic predictions at run-time.\nSoftplus activation functions are applied to \u03c3 t+\u03c4 and p t+\u03c4 to ensure that we obtain valid (i.e. \u2265 0) standard deviations and binary probabilities. For simplicity, the exponential distribution is selected to model survival times, and predictive distributions are as below:"}, {"section_title": "Multitask Learning", "text": "From the above, the negative log-likelihood of the data given the network is:\nWhere f c (.), f b (.) are likelihood functions based on Equations 4 and W collectively represents the weights and biases of the entire network. For survival times, f T (.) is given as:\nWhich corresponds to event-free survival until time T before encountering the event [2] . While the negative log-likelihood can be directly optimized across tasks, the use of multitask learning can yield the following benefits:\nBetter Survival Representations. As shown in [4] , multitask learning problems which have one main task of interest can weight the individual loss contributions of each subtask to favor representations for the main problem. For our current architecture, where we group similar tasks into task-specific layers, our loss function corresponds to:\nGiven that survival predictions are the primary focus of many longitudinal studies, we set \u03b1 c = \u03b1 b = 1 and include \u03b1 T as an additional hyperparameter to be optimized. To train the network,"}, {"section_title": "Algorithm 1 Training Disease-Atlas", "text": "Input: Data \u2126 = {\u2126 1 , . . . , \u2126 Q }, max iterations J Output: Calibrated network weights W for count= 1 to J do Get minibatch M \u223c \u03b3 random samples from \u2126 Sample task loss function l \u223c {l c , l b , l T } Update W \u2190 Adam(l, M), using feed-forward passes with dropout applied end for patient trajectories are subdivided into Q sets of \u2126 q (i, \u03c1, \u03c4 ) = X i,0:\u03c1 , Y i, \u03c1+\u03c4 , T max,i , \u03b4 i , where \u03c1 is the length of the covariate history to use in training trajectories up to a maximum of \u03c1 max . Full details on the procedure can be found in Algorithm 1.\nHandling Irregularly Sampled Data. We address issues with irregular sampling by grouping variables that are measured together into the same task, and training the network with multitask learning. For instance, volumes of different parts of the brain (e.g. hippocampal, ventricular and intra-cranial volume) that are be measured together during the same MRI scan session can be grouped together in the same task. Given the completeness of the datasets we consider, we assume that task groupings match those defined by the task-specific layer of the network, and multitask learning is performed using Equation 8 and Algorithm 1.\nWe note, however, that in the extreme case where none of the trajectories are aligned, we can define each variable as a separate task with its own loss function l * . Algorithm 1 then samples loss functions for one variable at a time, and the network is trained using only actual observations as target labels. This could reduce errors in cases where multiple sample rates exist and simple imputation is used, which might result in the multioutput networks replicating the imputation process instead of making true predictions."}, {"section_title": "Forecasting Disease Trajectories", "text": "Dynamic prediction involves 2 key elements -1) calculating the expected longitudinal values and survival curves as described above, and 2) computing uncertainty estimates. To obtain these measures, we apply the Monte-Carlo dropout approach of [3] by approximating the posterior over network weights as:\nWhere we draw J samples\u0174 j using feed-forward passes through the network with the same dropout mask applied across time-steps. The samples obtained can then be used to compute expectations and uncertainty intervals for forecasts."}, {"section_title": "PERFORMANCE EVALUATION FOR ALZHEIMER'S DISEASE 4.1 Data Description", "text": "The Alzheimer's Disease Neuroimaging Initiative (ADNI) study data is a comprehensive dataset that tracks the progression of the Alzheimer's disease (AD) through 3 main states: normal brain function, mild cognitive impairment and the onset of either the disease or dementia. This data surveys 1737 patients for periods up to 10 years, capturing informative features extracted with Positron Emission Tomography (PET) regions of interest (ROI) scans -e.g. measures of cell metabolism, which are known to be reduced for AD patients -Magnetic Resonance (MRI) and Diffusion Tensor imaging (DTI) (for instance, ventricles volume), CSF and blood biomarkers, genetics, cognitive tests (ADAS-Cog), demographic and others. Observations were discretized to 6-month (or 0.5 year) intervals, and missing measurements were imputed using the previous value if present, and the population mean otherwise. In this investigation, we use a random selection of 60% of patients for our training data, 20% for validation and the final 20% for evaluation as per the CF tests. This was repeated 3 times to form 3 different partitions of the dataset, which were then used for cross-validation. The Disease-Atlas was used to jointly forecast longitudinal observations of clinical scores and scan measurements, treating the transition to Alzheimer's Disease from either mild cognitive impairment (MCI) or Cognitively Normal (CN) states as our event of interest. Hyperparameter optimization was performed with 20 iterations of random search."}, {"section_title": "Results & Discussion", "text": "To evaluate predictions of the event-of-interest -i.e. transitions to dementia -we compared the performance of the Disease-Atlas against simpler recurrent neural networks (i.e. LSTMs) and standard methods from biostatistics (i.e. landmarking [7] and joint models (JM) fitted with a two-step approximation [8] ).\nPrediction results for transitions to dementia -in terms of the area under the receiver operating characteristic (AUROC) and the precision-recall curve (AUPRC) -and MSE improvements for longitudinal forecasts can be found in Tables 1 and 2 respectively. From the cross-validation performance, we see that the Disease-Atlas consistently outperforms both the standard neural network and traditional benchmarks for survival analysis particularly on shortterm horizons -improving the LSTM by 10% and JM by 7% on average across all time steps.\nFor longitudinal predictions, we focus on both the Disease-Atlas and JM which are able to generate predictions at arbitrary time steps in the future. Once again, the Disease-Atlas outperforms joint models across the majority of longitudinal variables and time steps, with gains of 40% on average -highlighting the benefits of a deep learning approach to joint modeling."}, {"section_title": "CONCLUSIONS", "text": "In this paper, we investigate an application of the Disease-Atlas to forecasting longitudinal measurements and expected time-totransition to Dementia for patients at risk of Alzheimer's Disease. Using data from the ADNI, the Disease-Atlas [5] demonstrated performance gains over both simpler neural networks such as LSTMs and traditional methods from biostatistics -demonstrating the advantages of the Disease-Atlas as a method for joint modeling and highlighting its potential as a tool for clinical decision support. "}]