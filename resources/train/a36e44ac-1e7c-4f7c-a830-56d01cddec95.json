[{"section_title": "Abstract", "text": "Automated labeling of anatomical structures in medical images is very important in many neuroscience studies. Recently, patch-based labeling has been widely investigated to alleviate the possible mis-alignment when registering atlases to the target image. However, the weights used for label fusion from the registered atlases are generally computed independently and thus lack the capability of preventing the ambiguous atlas patches from contributing to the label fusion. More critically, these weights are often calculated based only on the simple patch similarity, thus not necessarily providing optimal solution for label fusion. To address these limitations, we propose a generative probability model to describe the procedure of label fusion in a multi-atlas scenario, for the goal of labeling each point in the target image by the best representative atlas patches that also have the largest labeling unanimity in labeling the underlying point correctly. Specifically, sparsity constraint is imposed upon label fusion weights, in order to select a small number of atlas patches that best represent the underlying target patch, thus reducing the risks of including the misleading atlas patches. The labeling unanimity among atlas patches is achieved by exploring their dependencies, where we model these dependencies as the joint probability of each pair of atlas patches in correctly predicting the labels, by analyzing the correlation of their morphological error patterns and also the labeling consensus among atlases. The patch dependencies will be further recursively updated based on the latest labeling results to correct the possible labeling errors, which falls to the Expectation Maximization (EM) framework. To demonstrate the labeling performance, we have comprehensively evaluated our patch-based labeling method on the whole brain parcellation and hippocampus segmentation. Promising labeling results have been achieved with comparison to the conventional patch-based labeling method, indicating the potential application of the proposed method in the future clinical studies."}, {"section_title": "Introduction", "text": "With the advent of magnetic resonance (MR) imaging technique, image analysis on MR images plays a very important role in quantitatively measuring the structure difference between either individuals or groups (Fennema-Notestine et al., 2009; Paus et al., 1999; Westerhausen et al., 2011) . In many neuroscience and clinic studies, some regions-of-interest (ROIs), e.g., hippocampus, in the human brain are specifically investigated due to their close relation to brain diseases such as dementia (Devanand et al., 2007; Dickerson et al., 2001; Holland et al., 2012) . Consequently, automatic accurate labeling and measurement of anatomical structures become significantly important in those studies to deal with large amount of clinical data. However, automatic labeling still remains a challenging problem because of the complicated brain structures and high inter-subject variability across individual brains.\nRecently, patch-based labeling methods have emerged as an important direction for the multi-atlas based segmentation (Coupe et al., 2011; Rousseau et al., 2011; Wang et al., 2012; Wang et al., 2011; Yan et al., 2013) . The basic assumption in these methods is that, if two image patches are similar in appearances, they should have the same anatomical label (Rousseau et al., 2011) . Most patch-based labeling methods perform label fusion in a non-local manner. Specifically, to label a patch in the target image, all possible candidate patches from different atlases are considered, with their contributions weighted according to the patch similarities w.r.t. the target patch. In this way, these non-local based labeling methods can alleviate the influences from the possible registration errors.\nAlthough patch-based labeling methods are effective in many applications, they still have several limitations:\n(1) All candidate patches from atlases contribute to the label fusion, according to their similarities to the target patch. However, even the atlas patches with high appearance similarity could still bear the wrong labels, thus undermining the label fusion result due to the lack of power in suppressing the misleading patches. (2) If a majority of candidate patches have wrong labels, those patches will dominate the conventional label fusion procedure and lead to incorrect labeling results (Wang et al., 2012) . The reason is that most label fusion methods independently treat each candidate patch during label fusion, thus allowing those highly correlated candidate patches to repeatedly produce the labeling errors. (3) The weights calculated from patch appearance are often directly applied for label fusion. Although these weights are optimal for patch representation, i.e., making the combination of candidate patches close to the target patch, these weights are not necessarily optimal for label fusion. (4) Most current label fusion methods complete the label fusion right after sequentially labeling each image point in the image domain, thus lacking a feedback mechanism to help correct possible labeling errors.\nIn this paper, we propose a novel patch-based labeling method, where a generative probabilistic model is presented to predict the labels based on the observations of registered atlas images. Specifically, the goals are (1) to seek for the best representation of the underlying target patch from a set of similar candidate atlas patches, and (2) to achieve the largest labeling consensus, among the entire candidate atlas patches, in predicting the label for each target point. For the first goal, we introduce the concept of sparse representation (Tibshirani, 1996; Vinje and Gallant, 2000; Zhang et al., 2012a Zhang et al., , 2012b Zhang et al., , 2012c by imposing a non-Gaussian sparsity prior (Seeger et al., 2007; Seeger, 2008) on the label fusion weights. Thus, our method, equipped with sparsity constraint, is able to alleviate the issue of ambiguous patches by representing each target patch with only a small number of atlas patches, instead of all candidate atlas patches. For the second goal, we propose to measure the labeling unanimity through the joint probability of patch dependencies, which encodes the risk for any pair of candidate patches to produce labeling error simultaneously. In our probability model, we describe the dependency probability in two ways. First, we measure the pairwise correlation of morphological error patterns for any pair of candidate patches, in order to penalize those candidate patches with simultaneously incorrect labels. Second, we further inspect whether the latest label fusion result achieves the largest labeling consensus among the candidate patches. Since the estimation of dependency probability is related with the currently estimated labels, our label fusion method offers the feedback mechanism by iteratively improving the label fusion result with the gradually refined estimation of the dependency probability. To this end, we present an efficient EM-based solution to infer the optimal labels for the target image.\nIn terms of joint label fusion, our work is close to (Wang et al., 2012) , which also measured the joint labeling risk between two patches. However, our generative probability model has several unique improvements. First, the joint distribution of patch dependency is measured by not only the error pattern but also the labeling consensus w.r.t. the latest estimated label. Second, the label fusion method in (Wang et al., 2012) lacks of the feedback mechanism as in our method to examine the current label fusion result and further refine the estimation of dependency. Third, our method takes advantages of sparsity constraint to obtain robust label fusion results to suppress misleading patches. As we will point out later, our method can be regarded as a generalized solution of most existing patch-based labeling methods (Artaechevarria et al., 2009; Coupe et al., 2011; Rousseau et al., 2011; Tong et al., 2012; Zhang et al., 2012a) .\nWe demonstrated the labeling performance on NIREP-NA0 dataset (Christensen et al., 2006) with 32 manually delineated ROIs and also the ADNI (Alzheimer's Disease Neuroimaging Initiative) dataset with manually labeled hippocampi. Compared to the conventional patch-based labeling method (Coupe et al., 2011; Rousseau et al., 2011) , our method achieves more accurate labeling results on both datasets. In the following, we first present our novel generative probability model for label fusion in Section 2. Then, we evaluate it in Section 3, by comparison with the conventional patch-based methods. Finally, we conclude the paper in Section 4."}, {"section_title": "Method", "text": "Let S be the set of N atlas images I = {I k |k = 1, . . ., N} and their corresponding label maps L = {L k |k = 1, . . ., N}, which have been already registered to the target image T (that will be labeled) by linear/non-linear registration methods (Vercauteren et al., 2008 (Vercauteren et al., , 2009 Wu et al., 2013 Wu et al., , 2007 Wu et al., , 2012a Wu et al., , 2010 . For each point\nis a binary vector of {0, 1}\nM representing the particular label at the point v, where M is the total number of labels. The goal of label fusion is to propagate the labels from the registered atlases to the target image T. For each point u e X T in the target image T, its label L * T \u00f0u\u00de will be estimated through the interaction between the target patch P T \u00f0u\u00de centered at point u and all possible candidate patches P k \u00f0v\u00de at the registered atlas image I k . The spatial location v is usually confined to a relatively small neighborhood n(u) & X T . Given the weight w k (u, v) for the pair of P T \u00f0u\u00de and \nIn the following, we will first introduce the conventional patchbased labeling method with non-local averaging in Section 2.1. Then, we will present our generative probability model for label fusion in Section 2.2. The inference of probability model will be presented in Section 2.3, followed by the discussion in Section 2.4. Our whole method will be summarized in Section 2.5.\nLeft Table 3 The mean and the standard deviation of hippocampus Dice ratios in three groups (NC, MCI, and AD) by Nonlocal-PBM (Coupe et al., 2011; Rousseau et al., 2011) , Sparse-PBM (Zhang et al., 2012a) , and our Joint-PBM. in Eq. (21)). Fig. 9 shows the evolution curve of Dice ratio by Nonlocal-PBM (blue curve), Sparse-PBM (green curve), and Joint-PBM (red curve), where the threshold degree e for pre-selection ranges from 0.60 to 0.95. It is apparent that, for three label fusion methods, the Dice ratios keep increasing as the threshold increases, indicating the importance of pre-selection procedure in label fusion. Although pre-selection can help improve the robustness of label fusion, if pre-selection criteria are too strict (i.e., e > 0.85 in Non-local PBM and e > 0.90 in both Sparse and Joint-PBM), the label fusion accuracy could be decreased as shown in the right part of Fig. 9 , since too strict pre-selection criteria could exclude the image patches with correct label.\n3.2. Experiment result of whole brain labeling on NIREP-NA40 dataset 3.2.1. Data description The NIREP dataset consists of 16 MR images of 8 normal male adults and 8 normal female adults, each with 32 manually delineated gray matter ROIs. All 16 MR images have been aligned according to the anterior and posterior commissures (AC and PC). The image size is 256 \u00c2 300 \u00c2 256, and the voxel dimension is 0.7 \u00c2 0.7 \u00c2 0.7 mm 3 . The following pre-processing steps have been applied on these NA40 dataset: (1) N4-based bias correction (Tustison et al., 2010) ; and (2) intensity standardization (Madabhushi and Udupa, 2006) ."}, {"section_title": "Conventional patch-based labeling method by non-local averaging", "text": "The principle of conventional patch-based labeling method is originated from the non-local strategy which is widely used in the computer vision area, such as image/video denoising (Buades et al., 2005) and super-resolution (Protter et al., 2009) . The applications in medical images can also be found in (Awate and Whitaker, 2006; Manj\u00f3n et al., 2011) . The overview of patched-based method is shown in Fig. 1(a) . Hereafter, for each target point u e X T , we use the column vector y * to vectorize the target patch P T \u00f0u\u00de centered at u (red box in Fig. 1(a) ). In order to account for the registration uncertainty, a set of candidate atlas patches (pink boxes in Fig. 1(a) ) are included in a search neighborhood n(u) (blue boxes in Fig. 1(a) ) from different atlas images. For clarity, we arrange each candidate patch P k \u00f0v\u00de into a column vector a * j and then assemble them into a dictionary matrix A \u00bc \u00bda \nwhere s is the decay parameter (Coupe et al., 2011) In this section, we first interpret the conventional patch-based label fusion methods in a deterministic probability model, which lacks of the dependency among candidate patches. Then, we propose to model the labeling dependency as the joint probability of all candidate patches in achieving the largest labeling consensus simultaneously. After integrating the labeling dependency, we further present a generative probability model to guide the label fusion procedure.\n2.2.1. The probability model for conventional patch-based label fusion method\nAs we mentioned early, the procedures of estimating the weighting vector w \nAssuming the residual between y * and A w * follows Gaussian distribution, the likelihood probability p\u00f0y * jA; w * \u00de is defined as:\nwhere s is related with the standard deviation of residual errors.\nDue to no prior knowledge on A, the conditional probabilities p\u00f0Aj w * \u00de is simplified to follow the uniform distribution. The label fusion methods with non-local averaging (Coupe et al., 2011; Rousseau et al., 2011) usually do not have the explicit constraint on prior p\u00f0w * \u00de, which derives the calculation of weights (Eq. (3)) in the scenario of Maximum Likelihood (ML) estimation. Recently, sparse coding has been introduced in label fusion by imposing the sparsity constraint upon the label fusion weights w * (Tong et al., 2012; Wu et al., 2012b; Zhang et al., 2012a) . That is, the weighting vector w * with a majority of elements approaching to zero is preferred a priori (Seeger et al., 2007) . The sparsity constraint can be achieved by assuming the independent Laplace distribution to each element w j in w\nwhere q > 0 is a scalar parameter measuring the distribution diversity.\nIt is worth noting that each weight is non-negative in Eq. (6), i.e., w j P 0\u00f08j\u00de. Intuitively, the probability of large w j is much smaller than that of small w j . Thus, the prior p\u00f0w * \u00de, encourages the value of every element in w * to approach to zero. Given the prior p\u00f0w * \u00de, the optimization of Eq. (4) turns to the Maximum-a-Posteriori (MAP) problem\nwhich is also the well-known non-negative LASSO problem (Tibshirani, 1996) . The advantage of sparsity constraint is demonstrated in Fig. 2 . In this example, we examine the candidate patches for the target patch (the pink patch in the top of Fig. 2 ), from each atlas image (I 1 to I 4 ) and in the respective searching neighborhoods (blue boxes). The last second row in Fig. 2 shows the weights for each candidate patch by non-local mean, where blue and red colors denote the candidate patches having different or same labels as the target patch, respectively. It is clear that some candidate patches, although having different labels, still have high patch similarity, which will mislead the label fusion procedure. The last row in Fig. 2 shows the weights computed by Eq. (7) (using the sparsity constraint). Since the sparsity constraint encourages using only a small number of candidate patches to represent the target patch, the influence from the misleading patches is suppressed, as observed with very fewer blue spikes in the last row of Fig. 2 . In this way, our label fusion method can be a i more robust than non-local mean.\nGraphic model is a useful tool in describing conditional dependence structure between random variables (Koller and Friedman, 2009 ). The graphic model of Eq. (7) is shown in Fig. 3(a) . It is clear that the estimation of the label g * is separated from the optimization of w * . In the following, we will propose a generative probability model by introducing the patch dependency probability next."}, {"section_title": "Modeling the labeling dependency of candidate patches in label fusion", "text": "As mentioned above, one limitation in the conventional patchbased label fusion method is that each candidate patch a j is independently considered. To address this issue, we propose to model Fig. 1 . The overview of the patch-based labeling method in the multi-atlas scenario. As shown in (a), the reference patch (red box) seeks for contributions from all possible candidate patches (pink boxes) in a small search neighborhood (blue box). The graph demonstrations by non-local averaging and our method are shown in (b) and (c), respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) the dependency as the joint probability of candidate patches A that achieve the highest labeling accuracy simultaneously. In order to make our model tractable, we measure the pairwise probability of labeling dependency by any pair of candidate patches. Here, we use the variable D = {d ij |i, j = 1, . . ., Q} to denote the pair of patches a i and a j which both label the target point u correctly. Then, the dependency probability p(d ij ) indicates the likelihood of making such agreement between a i and a j in label fusion.\nAlthough it is difficult to directly estimate the dependency probability p(d ij ), we can learn its conditional probability in two ways. First, given the observations A, K and the target patch y * , we regard that the conditional probability p\u00f0d ij jA; K; y * \u00de is related with the correlation of morphological error patterns (w.r.t. y * ) of each pair of a i and a j . To this end, two patches a i and a j have high chance to produce similar labeling error only if (1) their error patterns, i.e., e i \u00bc \u00f0a i \u00c0 y * \u00de and e j \u00bc \u00f0a j \u00c0 y * \u00de, are highly correlated, (2) they bear the same labels, i.e., d\u00f0k\nis the Dirac pulse function (d(0) = 1), and (3) the magnitudes of error patterns, i.e., both ke i k 2 2 and ke j k 2 2 , are large. Thus, the conditional probability p\u00f0d ij jA; K; y * \u00de is given by penalizing the above pairwise mislabeling risk as:\nwhere NCC(e i , e j ) is the normalized cross-correlation between e i and e j . b 1 > 0 is a scalar controlling the penalty for the pair of patches a \nand\nwhere b 2 > 0 is the scalar. It is apparent that p\u00f0d ij j g * \u00de has the lowest probability only if both candidate patch a * i and a * j bear different labels against g * . Here, we go one step further that the labeling dependency d ij is also related with the weights w i and w j . Combining Eqs. (8) and (9) \nwhere b = b 1 b 2 and 0 6 r 6 1 is the scalar balancing the impacts of / A ij and / K ij during label fusion. As we will explain later r = 0 in the beginning since there is no estimation of g * at that moment, and the degree of r will gradually increase to 0.5 at the end of label fusion procedure. Thus, the goal of our label fusion method is to seek for the representative candidate patches with not only the best representation for the target patch y * but also the largest joint labeling consensus, which is described in a generative model next."}, {"section_title": "A generative probability model for joint patch-based label fusion", "text": "Given the observations H \u00bc fA; K; y Fig. 3(b) . Compared with the graphic model of the conventional patch-based label fusion method Fig. 3(a) , the estimations of weighting vector w * and label fusion result g * are coupled in our generative model to guarantee that the estimated weighting vector w * is optimal for label fusion. More importantly, the concept of pairwise dependency is introduced in our model to describe the interaction between any pair of candidate patches in label fusion."}, {"section_title": "Optimization of generative probability model for label fusion", "text": "Obviously, there might be many settings of the variable H in Eq.\n(11) which can well explain the observation H. Note, the label fusion result g * is not only the consequence of weighting vector w * but also the parameter in our probability model which contributes to refine the estimation of dependency probability p\u00f0DjA; K; y * ; w * ; g * \u00de.\nIn light of this, our idea is to iteratively maximize the expectation of p\u00f0H; H\u00de by using the currently estimated g * , and then determine the hidden variable g * by maximizing the posteriori probability p\u00f0g \nwhere\nBy substituting Eqs. (5), (6), and (13) into Eq. (12) and maintaining the posteriori probability p\u00f0g * jK; w * \u00de, the log-likelihood p\u00f0H; H\u00de of is given as:\nWe use coordinate descent method (Wu and Lange, 2008 ) to efficiently solve this problem. The idea of coordinate descent is to go through each w j in w * and minimize the objective function in Eq. (14) along one w j at a time.\nSpecifically, Eq. (14) can be rewritten as follows:\njw j j; s:t:w j P 0; 8j:\nFor each w j , we discard all terms in Eq. (15) that are not related with w j and turn Eq. (15) int\u00f4 \nwhich turns to the classic lasso penalized l 2 regression problem (Wu and Lange, 2008 , the optimal solution to particular w * j is given as:\nThe entire optimization of Eq. (14) by coordinate descent method is summarized in Fig. 4 . In our experiment, we fix the iteration number as 200 in optimizing w * at each target image point u."}, {"section_title": "M-", "text": "Step: Estimate the label fusion result g * Given the estimated\u0175 * , the objective function to optimize g * is given as:\nHere we follow the strategy of local weighted voting (Artaechevarria et al., 2009) to solve this problem by sequentially applying Eqs.\n(1) and (2). That is, given\u011d * , we can refine the labeling dependency term U by Eqs. (8)- (10). Fig. 1(c) shows the major differences between our joint label fusion and the conventional non-local based method. First, only a small number of closest patches are considered in label fusion in our method, as indicated by the fewer arrows in Fig. 1(c) , compared with Fig. 1(b) . Second, the dependency within each pair of candidate patches in labeling the target patch is considered, as shown by the dashed lines in Fig. 1(c) . Third, the currently estimated label will feed back to the label fusion procedure to iteratively refine the labeling result, as designated by the two-end arrows in Fig. 1(c) ."}, {"section_title": "Discussions", "text": "\nIn the following, we randomly select 15 different subjects from those 198 subjects as the test samples to examine each parameter/ component in our label fusion method.\n3.1.3.1. Parameter selection of q and b. Here, we will evaluate each parameter in Joint-PBM method, in order to acquire the optimal parameter set. The ranges of b and q are (0.1$1.0) and (0.01-0.25), respectively. The Dice ratios w.r.t. b and q are shown in Fig. 5 , where b=0.5 and q=0.1 achieve the highest Dice ratios in our joint label fusion method. Thus, we fix b = 0.5 and q = 0.1 throughout the experiments.\n3.1.3.2. The role of dependency term U. Next, we specifically evaluate the role of two terms in modeling dependency, i.e., / A ij and / K ij , in our method. Fig. 6 shows the evolution curves of average Dice ratios by 198 subjects as the number of iteration increases. As the baseline method, we also plot the average Dice ratio by Sparse-PBM as a blue straight line in the figure. In the beginning of our joint label fusion, only the term / A ij contributes to the modeling of the dependency among candidate patches. Compared with Sparse-PBM which lacks of the dependency modeling, its average Dice ratio is 1% lower than our Joint-PBM method in the first iteration. After obtaining the initial estimation of labeling result, our Joint-PBM method is able to update the dependency probability by further inspecting whether two candidate patches achieve labeling consensus towards the latest label fusion results. As shown by the red curve in Fig. 6 , the Dice ratios increase to 0.887, indicating the effectiveness of / K ij in correcting possible mislabeling.\n3.1.3.3. The influence of the number of atlases. Here, we evaluate the evolution curves w.r.t. the atlas number in Fig. 7 by Nonlocal-PBM (red), Sparse-PBM (green), and Joint-PBM (red), respectively. It is clear that (1) the Dice ratios keep increasing for all three label fusion methods, as more and more atlases are included; and (2) our Joint-PBM method consistently achieves the highest Dice ratios in all cases.\n3.1.3.4. The influence of patch size and search range. Given the optimized parameters, we examine the influences of patch size and search range in Nonlocal-PBM, Sparse-PBM, and our Joint-PBM methods. Specifically, we perform these three label fusion methods with patch size from 3 \u00c2 3 \u00c2 3 to 11 \u00c2 11 \u00c2 11 and search range from 5 \u00c2 5 \u00c2 5 to 13 \u00c2 13 \u00c2 13. The Dice ratios w.r.t. different patch size and search range are shown in Fig. 8(a) and (b), with blue, green, and red curves representing the results by the Nonlocal-PBM, sparse-PBM, and our Joint-PBM methods, respectively.\n3.1.3.5. The influence of patch pre-selection. Patch pre-selection is able to not only speed up the label fusion procedure, but also improve the robustness of label fusion by excluding the unrelated patches. To demonstrate this point, we evaluate the label fusion accuracy when applying different thresholds in pre-selection (ss Table 2 The mean and the standard deviation of the Dice ratios on left and right hippocampi by Nonlocal-PBM (Coupe et al., 2011; Rousseau et al., 2011) , Sparse-PBM (Zhang et al., 2012a) , and our Joint-PBM."}, {"section_title": "Patch pre-selection", "text": "Pre-selecting patches in the search neighborhood n(u) is very important to speed up the procedure of label fusion and preclude the less similar candidate patches. In our implementation, we strictly follow the pre-selection procedure in (Coupe et al., 2011) by computing the structural similarity measurement ss:\nwhere (l y , r y ) and (l j , r j ) are the mean and stand deviation of target patch y * and candidate patch a we set the similarity threshold e = 0.9 to discard the candidate patch"}, {"section_title": "Multipoint estimation", "text": "In the previous sections, we presented the label fusion method for labeling particular point u in the target image T. However, it is straightforward to obtain the labels for the whole patch P T \u00f0u\u00de by following the estimated weighting vector w * at the target point u.\nAs the result, each point has the multiple estimates from the neighboring points. Here, we use the majority voting strategy to fuse these multiple estimates after finishing the estimation at all points, in order to make the labeling result spatially smooth."}, {"section_title": "Generalization of our probability model", "text": "As mentioned early, most of the current patch-based label fusion method can be regarded as the simplified version of our probability model. For example, if we discard the correlation term in Eq. (14), the objective function turns to Eq. (7). Thus, our method simplifies to the sparse patch-based labeling method (Zhang et al., 2012a) , which formulate the label fusion as the sparse representation problem with l 1 -norm constraint. Certainly, more advanced priors such as elastic net (Tong et al., 2012) , can be potentially incorporated in our probability model."}, {"section_title": "Summary", "text": "Given the registered atlas image I and associated label images L, our patch-based labeling method to label the target image T is summarized as follows:\n1. Suppose the number of total iteration is H. Set h = 0.\n2. Go through each point u 2 X T : 2.1. Collect the candidate patches from all atlas images, constructing matrix A. 2.2. Let r = 0.5 \u00c2 h/H (r is defined in Eq. (10))."}, {"section_title": "Compute the matrix of joint labeling risk U in Eq. (14)", "text": "based on the latest\u011d * . 2.4. Optimize the weighting vector w * for the point u according to the optimization algorithm in Fig. 4. "}, {"section_title": "Predict the labeln", "text": ""}, {"section_title": "Experiments", "text": "In this section, we apply our patch-based labeling method on NIREP-NA40 and ADNI datasets to evaluate the labeling performance. For comparison, we also apply the conventional patchbased method by non-local weighting (Nonlocal-PBM) (Rousseau et al., 2011) and the recently proposed sparse patch-based labeling method (Sparse-PBM) (Zhang et al., 2012a) on the same dataset. Since our method utilizes the joint distribution of labeling consensus to guide the label fusion, we call our method Joint-PBM for short in the following. To quantitatively evaluate the labeling accuracy, we use the Dice ratio to measure the overlap degree between ROI O 1 and ROI O 2\nwhere |\u00c1| means the volume of particular ROI."}, {"section_title": "Experiment result of hippocampus labeling on ADNI dataset", "text": "In many neuroscience studies, accurate delineation of hippocampus is very important for quantifying the convoluted intersubject anatomical difference and subtle intra-subject longitudinal change, since the structural change of hippocampus is closely related with dementias, such as Alzheimer's disease (AD)."}, {"section_title": "Data description", "text": "In this experiment, we randomly select 61 normal control (NC) subjects, 96 MCI (Mild Cognitive Impairment) subjects, and 41 AD subject from the ADNI dataset.\n1 The detailed subject information is shown in Table 1 . The following three pre-processing steps have been performed to all subject images: (1) Skull removal by a learning based meta-algorithm (Shi et al., 2012) ; (2) N4-based bias field correction (Tustison et al., 2010) ; (3) intensity standardization to normalize the intensity range (Madabhushi and Udupa, 2006) . Semi-automated hippocampal volumetry was carried out using a commercial available high dimensional brain mapping tools (Medtronic Surgical Navigation Technologies, Louisville, CO), which has been validated and compared to manual tracing of the hippocampus (Hsu et al., 2002) . In this experiment, we regard these hippocampal segmentations from ADNI as the ground truth."}, {"section_title": "Experiment results", "text": "As it is common in the evaluation of label fusion method, we use the leave-one-out strategy to compare the labeling performance of Nonlocal-PBM, Sparse-PBM, and our proposed Joint-PBM methods. In each leave-one-out case, affine registration is first performed by FLIRT in the FSL toolbox (Smith et al., 2004) with 12 degrees of freedom and the default parameters (normalized mutual information similarity metric and search range \u00b120 in all directions). Then, we use diffeomorphic Demons (Vercauteren et al., 2009) for the deformable registration upon the affine registration result, also with the default registration parameters (smoothing sigma 1.8 and iterations in low, middle, and high resolution as 20 \u00c2 20 \u00c2 20).\nThe common parameters, such as patch size and search range, are widely discussed in (Coupe et al., 2011; Rousseau et al., 2011; Tong et al., 2012; Wang et al., 2012) . Here, we fix the patch size as 7 \u00c2 7 \u00c2 7 and the search range 9 \u00c2 9 \u00c2 9 for all three label fusion methods. Specifically, we follow the patch pre-selection method and local adaptive selection of decay parameter (r in Eq. (3)) in (Coupe et al., 2011) for Nonlocal-PBM. As reported in (Zhang et al., 2012a) , we set the parameter for l 1 -norm strength as q=0.1 (Eq. (16)) for sparse-PBM. For our Joint-PBM method, the total number of iteration H in our Joint-PBM method is fixed to 5. Meanwhile, we set b=0.5 and q=0.1 throughout all the following experiments. Note, we will explain the way to determine the parameters b and q in our method after we show the overall labeling performance.\nThe overall Dice ratios on the left/right hippocampus by three label fusion results are shown in Table 2 , where our Joint-PBM method has achieved 4.7% and 2.2% improvements over Nonlocal-PBM and Sparse-PBM, respectively, in terms of labeling accuracy. Specifically, we show the average and the standard deviation of Dice ratios in NC, MCI, and AD groups by three label fusion methods in Table 3 . Again, our Joint-PBM method has the best labeling performance in each group. It is worth noting that the highest Dice ratio of hippocampus is 0.893 in (Wang et al., 2012) , which is comparable with our Joint-PBM label fusion method. However, only 57 NC and 82 MCI subjects are evaluated in (Wang et al., 2012) . As shown in Table 3 , labeling the hippocampus of AD subjects are more challenging than MCI and NC groups. Discarding the AD subject, the overall overlap ratio of NC and MCI subjects by our method is able to reach 0.896.\nLeave-one-out strategy is used in this experiment by alternatively taking one of the 16 NA40 images as the target image. For each leave-one-out case, affine and deformable registrations are sequentially deployed by FLIRT and Diffeomorphic Demons with the same registration parameters in labeling hippocampus. Since we have determined the optimal parameters in Section 3.1, here, we use these parameters in labeling 32 ROIs in NA40 dataset, i.e., patch size is 7 \u00c2 7 \u00c2 7 and search range is 9 \u00c2 9 \u00c2 9. Our Joint-PBM takes 5 iterations with q = 0.1 and b = 0.5. Table 4 displays the averaged Dice ratio in each brain structure (left and right combined) upon 16 leave-one-out cases. From second to the last column, we show the average Dice ratios by Nonlocal-PBM, Sparse-PBM, and our Joint-PBM, respectively. The overall Dice ratios across 32 ROIs are 0.792 by Nonlocal-PBM, 0.803 by Sparse-PBM, and 0.827 by Joint-PBM. It is clear that our Joint-PBM method achieves the best labeling accuracy over the other two methods. It is worth noting that the overall Dice ratio was reported as 0.823 by a non-local label fusion method in (Rousseau et al., 2011) , which is comparable with our method. However, they used SyN (Avants et al., 2008) as the deformable registration method before patch-based labeling and also STAPLE (Warfield et al., 2004) as the post-processing step to fuse the multiple estimates, where computational costs of both steps are very expensive.\n3.2.3. Discussions 3.2.3.1. The influence of deformable registration. Specifically, we evaluate the effect of deformable registration in patch-based labeling method. As we all know, there are two terms in the energy function of deformable registration, i.e., data fitting term and the regularization term. Generally, the fitting term aims to maximize the similarity between two images, and the regularization term makes the deformation field as smooth as possible. In patch-based label fusion method, it is desirable to deform the atlas images as similar as possible to the target image by deformable image registration. However, we argue that over-registration, which makes two images unreasonably similar but at the cost of very aggressive deformation field, will undermine the label fusion result since the over-registration might tear down the inherent topology of anatomical structure.\nTo demonstrate this point, we examine the evolution of Dice ratio with changes of parameters in deformable image registration. Fig. 8 . The evolution curves of Dice ratio by the three label fusion methods, with respective to patch size (a) and search range (b). Fig. 9 . The evolution curve of Dice ratio w.r.t. the pre-selection threshold e by three label fusion methods.\nIn general, the number of iterations and the size of the smoothing kernel are the two important parameters in the diffeomorphic Demons, for controling the smoothness of deformation field. In Fig. 10 , the horizontal and vertical axes denote the sigma for deformation smoothing and the iteration number, respectively, where the larger number of iterations and the smaller degree of sigma lead to more aggressive registration result. Taking precentral gyrus as example, we show the Dice ratios w.r.t. different parameters in diffeomorphic Demons in Fig. 10 . It is interesting to see that (1) the Dice ratio first increases as the registration becomes more and more accurate; and then (2) the Dice ratio decreases when the registration is too aggressive to preserve the topology of anatomical structures after warping the atlas image to the target image space.\n3.2.3.2. The convergence of our Joint-PBM method. Since our Joint-PBM method falls into the EM framework, we specifically evaluate the convergence of our label fusion method. Fig. 11 shows the evolution of Dice ratio during joint label fusion, where each curve denotes the average Dice ratio of 32 ROIs at different stages of one leave-one-out case. It is clear that the Dice ratio keeps increasing in the first half of the whole label fusion procedure and then quickly converges, which demonstrates the robustness of our iterative label fusion method."}, {"section_title": "Computation time", "text": ""}, {"section_title": "Computational complexity", "text": "Suppose there are Q candidate patches in labeling a particular point, where the length of each patch is P. Then the sizes of matrix A and U are P \u00c2 Q and Q \u00c2 Q, respectively. For classic LASSO problem (with the objective function shown in Eq. (7)), the computational complexity is O\u00f0P \u00c2 Q \u00c2 min\u00f0P; Q \u00de\u00de (Efron et al., 2004) .\nWith the quadratic term U in our energy function (Eq. (14)), the computation cost of our Joint-PBM method increases to O\u00f0P \u00c2 Q \u00c2 min\u00f0P; Q \u00de \u00fe Q 2 \u00de.\nIn the multi-atlas labeling scenario, the number of candidate patches is usually much larger than the length of patch, i.e., Q ) P. Thus, the computational complexities of Sparse-PBM and our Joint-PBM are approximately O\u00f0P 2 \u00de and O\u00f0Q 2 \u00de, respectively."}, {"section_title": "Experiment result", "text": "All the experiments are performed on our DELL computation server with 2 CPUs (each with four 2.0 GHz cores) and 32G memory. We utilize the OpenMP technique 2 to parallelize the labeling procedure independently for each point. The computation time of each step in labeling one image from NIREP-NA40 dataset, with other 15 images as the atlases, is shown in Table 5 . In general, our Joint-PBM takes 7.7 h to labeling the whole brain with 32 ROIs. It is worth noting that the computational times for labeling hippocampus by three labeling methods are comparable, i.e., 15 min by Nonlocal-PBM, 19 min by Sparse-PBM, and 21 min by Joint-PBM, since the volume of hippocampus is often small ($5000 voxels)."}, {"section_title": "Conclusion", "text": "In this paper, we present a novel probability model for multi-atlas label fusion. In general, we estimate the optimal weights for label fusion by seeking for not only the best representation but also the largest unanimity in labeling accuracy. To achieve it, sparsity is used as the prior on weighting vectors to suppress the contribution Table 4 The average Dice ratios of 32 ROIs in NIREP-NA40 dataset by Nonlocal-PBM, Sparse-PBM, and Joint-PBM. from ambiguous patches. Different from other conventional patching labeling methods, we explicitly model the labeling dependency of the entire candidate patches for achieving the highest labeling accuracy simultaneously. We further describe the label fusion procedure as the generative probability model, where the label fusion results are iteratively refined in the EM framework. Our joint label fusion method achieves better performance than conventional methods in terms of labeling accuracy and robustness, indicating its potential for future clinical applications. "}]