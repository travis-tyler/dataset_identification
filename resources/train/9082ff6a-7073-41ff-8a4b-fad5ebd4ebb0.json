[{"section_title": "Abstract", "text": "We propose nonparametric methods for functional linear regression which are designed for sparse longitudinal data, where both the predictor and response are functions of a covariate such as time.\nPredictor and response processes have smooth random trajectories, and the data consist of a small number of noisy repeated measurements made at irregular times for a sample of subjects. In longitudinal studies, the number of repeated measurements per subject is often small and may be modeled as a discrete random number, and accordingly only a finite and asymptotically non-increasing number of measurements are available for each subject or experimental unit. We propose a functional regression approach for this situation, using functional principal component analysis, where we es- "}, {"section_title": "Introduction", "text": "We develop a version of functional linear regression analysis, in which both the predictor and response variables are functions of some covariate which usually but not necessarily is time. Our approach extends the applicability of functional regression to typical longitudinal data where only very few and irregularly spaced measurements for predictor and response functions are available for most of the subjects. Examples of such data are discussed in Section 5 (see Figures 1 and 6 ).\nSince a parametric approach only captures features contained in the pre-conceived class of functions, nonparametric methods of functional data analysis are needed for the detection of new features and for the modeling of highly complex relationships. Functional principal component analysis (FPCA) is a basic methodology that has been studied in early work by Grenander (1950) and more recently by Rice and Silverman (1991) , Ramsay and Silverman (1997) , and many others. Background in probability on function spaces can be found in Grenander (1963) . James, Hastie and Sugar (2001) emphasized the case of sparse data by proposing a reduced rank mixed-effects model using B-spline functions. Nonparametric methods for unbalanced longitudinal data were studied by Boularan, Ferr\u00e9 and Vieu (1995) and Besse, Cardot and Ferraty (1997) . Yao, M\u00fcller and Wang (2005) proposed a FPCA procedure through a conditional expectation method, aiming at estimating functional principal component scores for sparse longitudinal data.\nIn the recent literature there has been increased interest in regression models for functional data, where both predictor and response are random functions. Our aim is to extend the applicability of such models to longitudinal data with their typically irregular designs, and to develop asymptotics for functional regression in sparse data situations. Practically all investigations to date are for the case of completely observed trajectories, where one assumes either entire trajectories or densely spaced measurements taken along each trajectory are observed; recent work includes , , and Ferraty and Vieu (2004) .\nIn this paper we illustrate the potential of functional regression for complex longitudinal data.\nIn functional data settings, Cardot, Ferraty and Sarda (1999) provided consistency results for the case of linear regression with functional predictor and scalar response, where the predictor functions are sampled at a regular grid for each subject, and discussed inference for the regression function. The case of a functional response was introduced by Ramsay and Dalzell (1991) , and for a summary of this and related work, we refer to Ramsay and Silverman (1997, Chap. 11) , and to Faraway (1997) for a discussion of relevant practical aspects. The theory for the case of fixed design and functional response in the densely sampled case was investigated by Cuevas, Febrero and Fraiman (2002) . studied functional regression models where the predictors are finite-dimensional vectors and the response is a function, using a quasi-likelihood approach. Applications of varying-coefficient modeling to functional data, including asymptotic inference, were presented in Fan and Lin (1998) and Fan and Zhang (1998) .\nThe proposed functional regression approach is flexible, and allows for varying patterns of timing in regard to the measurements of predictor and response functions. This is relevant since it is a common occurrence in longitudinal data settings that the measurement of either predictor or response is missing. The contributions of this paper are as follows: First, we extend the functional regression approach to longitudinal data, using a conditioning idea. This leads to improved prediction of the response trajectories, given sparse measurements of the predictor trajectories. Second, we provide a complete practical implementation of the proposed functional regression procedure and illustrate its utility for two longitudinal studies. Third, we obtain the asymptotic consistency of the estimated regression function of the functional linear regression model for the case of sparse and irregular data, including rates. Fourth, we construct asymptotic pointwise confidence bands for predicted response trajectories, based on asymptotic distribution results. Fifth, we introduce a consistent estimator for a proposed measure of association between the predictor and response functions in functional regression models, that provides an extension of the coefficient of determination R 2 in standard linear model theory to the functional case. The proposed functional coefficient of determination provides a useful quantification of the strength of the relationship between response and predictor functions, as it can be interpreted in a well-defined sense as the fraction of variance explained by the functional linear regression model, in analogy to the situation for the standard linear regression model. The paper is organized as follows. In Section 2, we introduce basic notions, the functional linear regression model, and describe the estimation of the regression function. In Section 3, we discuss the extension of the conditioning approach to the prediction of response trajectories in functional regression under irregular and sparse data. Pointwise confidence bands and the functional coefficient of determination R 2 are also presented in Section 3. Simulation results that illustrate the usefulness of the proposed method can be found in Section 4. This is followed by applications of the proposed functional regression approach to longitudinal PBC liver cirrhosis data and an analysis of the longitudinal relationship between blood pressure and body mass index, using data from the Baltimore Longitudinal Study on Aging in Section 5. Asymptotic consistency and distribution results are provided in Section 7, while proofs and auxiliary results are compiled in the Appendix."}, {"section_title": "Functional Linear Regression for Sparse and Irregular Data", "text": ""}, {"section_title": "Representing Predictor and Response Functions through Functional Principal Components", "text": "The underlying but unobservable sample consists of pairs of random trajectories (X i , Y i ), i = 1, . . . , n, with square integrable predictor trajectories X i and response trajectories Y i . These are realizations of smooth random processes (X, Y ), with unknown smooth mean functions EY (t) = \u00b5 Y (t),\nWe usually refer to the arguments of X(\u00b7) and Y (\u00b7) as time, with finite and closed intervals S and T as domains. We assume the existence of orthogonal expansions of G X and G Y (in the L 2 sense) in terms of eigenfunctions \u03c8 m and \u03c6 k with non-increasing eigenvalues \u03c1 m and \u03bb k , i.e.,\nWe model the actually observed data which consist of sparse and irregular repeated measurements of the predictor and response trajectories X i and Y i , contaminated with additional measurement errors (see Staniswalis and Lee, 1998; Rice and Wu, 2000) . To adequately reflect the irregular and sparse measurements, we assume that there is a random number of L i (respectively, N i ) random measurement times for X i (respectively, Y i ) for the ith subject, which are denoted as S i1 , . . . , S iL i (respectively, T i1 , . . . , T iN i ). The random variables L i and N i are assumed to be i.i.d. as L and N respectively, where L and N may be correlated but are independent of all other random variables.\nLet U il (respectively, V ij ) denote the observation of the random trajectory X i (respectively, Y i ) at a random time S il (respectively, T ij ), contaminated with measurement errors \u03b5 il (respectively, ij ),\n, and independent of functional principal component scores \u03b6 im (respectively, \u03be ik ) that satisfy\nThen we may represent predictor and response trajectories as follows,\nWe note that the response and predictor functions do not need to be sampled simultaneously, extending the applicability of the proposed functional regression model."}, {"section_title": "Functional Linear Regression Model and Estimation of the Regression Function", "text": "Consider a functional linear regression model in which both the predictor X and response Y are smooth random functions,\nHere the bivariate regression function \u03b2(s, t) is smooth and square integrable, i.e., T S \u03b2 2 (s, t)ds dt <\nOur aim is to predict an unknown response trajectory based on sparse and noisy observations of a new predictor function. This is the functional version of the classical prediction problem in a linear model where, given a set of predictors X, one aims at predicting the mean response Y by estimating E(Y |X) (see Draper and Smith (1998) given by\nThe convergence of the right hand side of (5) is discussed in Lemma 2 (Appendix). When referring to \u03b2, we always assume that the limit (5) exists in an appropriate sense. In a first step, smooth estimates of the mean and covariance functions for the predictor and response functions are obtained by scatterplot smoothing, see (30) and (31) We use two-dimensional scatterplot smoothing to obtain an estimate C(s, t) of the cross-covariance\n) be \"raw\" cross-covariances that serve as input for the two-dimensional smoothing step, see (36) in the Appendix. The smoothing parameters in the two coordinate directions can be chosen independently by one-curve-leave-out cross-validation procedures (Rice and Silverman, 1991) . From (6), we obtain estimates for\nWith estimates (33), the resulting estimate for \u03b2(s, t) i\u015d\nIn practice, the numbers M and K of included eigenfunctions can be chosen by one-curve-leaveout cross-validation (34), or by an AIC type criterion (35). For the asymptotic analysis, we consider 3 Prediction and Inference"}, {"section_title": "Predicting Response Trajectories", "text": "One of our central aims is to predict the trajectory Y * of the response for a new subject from sparse and irregular measurements of the predictor trajectory X * . In view of (4), the basis representation of \u03b2(s, t) in (5) and the orthonormality of the {\u03c8 m } m\u22651 , the prediction of the response function would be obtained via the conditional expectation\nwhere \u03b6 * m = S (X * (s) \u2212 \u00b5 X (s))\u03c8 m (s)ds is the m-th functional principal component score of the predictor trajectory X * . The quantities \u00b5 Y , \u03c6 k , \u03c3 km , and \u03c1 m can be estimated from the data, as described above. It remains to discuss the estimation of \u03b6 * m , and for this step we invoke Gaussian assumptions in order to handle the sparsity of the data.\nLet U * l be the lth measurement made for the predictor function X * at time S * l , according to (1), where l = 1, . . . , L * , with L * a random number. Assume that the functional principal component scores \u03b6 * m and the measurement errors \u03b5 * l for the predictor trajectories are jointly Gaussian. Following Yao et al. (2005) , the best prediction of the scores \u03b6 * m is then obtained through the best linear prediction, given the observations U * = (U * 1 , . . . , U * L * ), and the number and locations of these observations, L * and S * = (S * 1 , . . . , S * L * ) T . Let X * (S * l ) be the value of the predictor function X * at time S * l . Write\nwhere\nAccording to (10), estimates for the functional principal component scores \u03b6 * m are obtained by substituting estimates of \u00b5 * X , \u03c1 m and \u03c8 * m that are based on the entire data collection, leading t\u00f4\nwhere\nThe predicted trajectory is then obtained as\nIn the sparse situation, the Gaussian assumption is crucial. It allows us to obtain the best linear predictors in (10)- (12) "}, {"section_title": "Asymptotic Pointwise Confidence Bands for Response Trajectories", "text": "We construct asymptotic confidence bands for the response trajectory Y * (\u00b7) of a new subject, conditional on the sparse and noisy measurements that are available for the underlying predictor function.\nUnder the Gaussian assumption and conditioning on L * and S * , then\u03b6\n. As a consequence, the (1 \u2212 \u03b1) asymptotic pointwise interval for E[Y * (t)|X * ], the mean response at predictor level X * , is given by\nwhere \u03a6 is the standard Gaussian c.d.f.."}, {"section_title": "Coefficients of Determination for Functional Linear Regression", "text": "In standard linear regression, a measure to quantify the \"degree of linear association\" between predictor and response variables is the coefficient of determination R 2 (e.g., Draper and Smith (1998) \nThe analogous notion of total variation of\nSince\nAnother interpretation of the functional coefficient of determination R 2 (15) is as follows: Denoting by R 2 km the coefficients of determination of the simple linear regressions of the functional principal component scores\nkm is the coefficient of determination of regressing \u03be k on all \u03b6 m , m = 1, 2, . . ., simultaneously, by successively adding predictors \u03b6 m into the regression equation and observing that R 2 k is obtained as the sum of the R 2 km , as predictors \u03b6 m are uncorrelated. Then\nis seen to be a weighted average of these R 2 k , with weights provided by the \u03bb k . According to (15), a natural estimate R 2 for the functional coefficient of determination R 2 is\nwhere\u03c3 km are as in (7).\nBesides the functional R 2 (15) that provides a global measure of the linear association between processes X and Y , we also propose a local version of a functional coefficient of determination.\nThe corresponding function R 2 (t) may be considered a functional extension of the local R 2 measure that has been introduced in Doksum et al. (1994) and Doksum and Samarov (1995) . As shown above, for fixed t \u2208 T , the variation of Y (t) explained by the predictor process X is determined by\n. This motivates the following definition of a pointwise functional coefficient of determination R 2 (t):\nNote that R 2 (t) satisfies 0 \u2264 R 2 (t) \u2264 1 for all t \u2208 T .\nA second option to obtain an overall R 2 value is to extend the pointwise measure R 2 (t) to a global measure by taking its integral, which leads to an alternative definition of the global functional coefficient of determination, denoted by R 2 ,\nwhere |T | denotes the length of the time domain T . Natural estimates of R 2 (t) and R 2 are given by\nWe refer to Section 5 for further discussion of R 2 , R 2 (t) and R 2 in applications and to Theorem 4 in Section 7 regarding the asymptotic convergence of these estimates."}, {"section_title": "Simulation Studies", "text": "Simulation studies were based on 500 i.i.d. normal and 500 i.i.d. mixture samples, where 100 pairs of response and predictor trajectories were generated for each sample. Emulating very sparse and irregular designs, each pair of response and predictor functions was observed at different sets of time points. The number of measurements was randomly chosen for each predictor and each response trajectory, with equal probability from {3, 4, 5} for X i uniformly, and independently chosen from {3, 4, 5} for Y i uniformly, also with equal probability. This set-up reflects very sparse designs with at most five observations available per subject. Once their numbers were determined, the locations of the measurements were uniformly distributed on [0, 10] for both X i and Y i , respectively.\nThe predictor trajectories X i and associated sparse and noisy measurements U il (1) were generated as follows. The simulated processes X had the mean function \u00b5 X (s) = s + sin (s), with covariance function constructed from two eigenfunctions, \u03c8 1 (s) = \u2212 cos (\u03c0s/10)/ \u221a 5, and \u03c8 2 (s) = sin (\u03c0s/10)/ \u221a 5, 0 \u2264 s \u2264 10. We chose \u03c1 1 = 2, \u03c1 2 = 1 and \u03c1 m = 0, m \u2265 3 as eigenvalues, and \u03c3 2 X = 0.25 as the variance of the additional measurement errors \u03b5 il in (1), which were assumed to be normal with mean 0. For the 500 normal samples, the FPC scores \u03b6 im (m = 1, 2) were generated from N (0, \u03c1 m ), while the \u03b6 im for the non-normal samples were generated from a mixture of two normals, N ( \u03c1 m /2, \u03c1 m /2) with probability 1/2 and N (\u2212 \u03c1 m /2, \u03c1 m /2), also with probability 1/2.\nFor the response trajectories, letting b 11 = 2, b 12 = 2, b 21 = 1, b 22 = 2, the regression function was We investigated predicting response curves for new subjects. For each Monte Carlo simulation run, we generated 100 new predictor curves X * i , with noisy measurements taken at the same random time points as X i , and 100 associated response curves E[Y * i |X * i ]. Relative mean squared prediction error was used as evaluation criterion, given by\nwhere predicted trajectories Y * i,KM were obtained according to (11), (12) . This method is denoted as CE in Table 1 , and was compared with a \"classical\" functional regression approach, that was also based on (12), but with the conditional expectation replaced by the integral approximation\u03b6 Table 1 . The numbers of eigenfunctions K and M were chosen by the AIC criterion (35), separately for each simulation. We also included the case of irregular but non-sparse data, where the random numbers of the repeated measurements were chosen from {20, . . . , 30} for both X i and Y i with equal probability. From the results in Table 1 , we see that, for sparse data, the CE method improves the prediction errors by 57%/60% for normal/mixture distributions, while the gains for non-sparse data are not as dramatic, but nevertheless present. The CE method emerges as superior for the sparse data case."}, {"section_title": "Applications", "text": ""}, {"section_title": "Primary Biliary Cirrhosis Data", "text": "Primary biliary cirrhosis (Murtaugh et al., 1994) is a rare but fatal chronic liver disease of unknown cause, with a prevalence of about 50 cases per million population. The data were collected between January, 1974 and May, 1984 by the Mayo Clinic (see also Appendix D of Fleming and Harrington, 1991) . The patients were scheduled to have measurements of blood characteristics at six months, one year, and annually thereafter post diagnosis. However, since many individuals missed some of their scheduled visits, the data are sparse and irregular with unequal numbers of repeated measurements per subject and also different measurement times T ij per individual.\nTo demonstrate the usefulness of the proposed methods, we explore the dynamic relationship between albumin in mg/dl (predictor) and prothrombin time in seconds (response), which are both longitudinally measured. We include 137 female patients, and the measurements of albumin level and prothrombin time before 2500 days. For both albumin and prothrombin time, the number of observations ranged from 1 to 10, with a median of 5 measurements per subject. Individual trajectories of albumin and prothrombin time are shown in Figure 1 .\nThe smooth estimates of the mean function for both albumin and prothrombin time are also displayed in Figure 1 , indicating opposite trends. The AIC criterion leads to the choice of two eigenfunctions for both predictor and response functions, and smooth eigenfunction estimates are presented in Figure 2 . For both albumin and prothrombin time, the first eigenfunction reflects an overall level, and the second eigenfunction a contrast between early and late times. The estimate of the regression function \u03b2 is displayed in Figure 3 . Its shape implies that for the prediction of early prothrombin times, late albumin levels contribute positively, while early levels contribute negatively, whereas the prediction of late prothrombin times is based on a sharper contrast with highly positive weighting of early albumin levels and negative weighting of later levels.\nWe randomly selected four patients from the sample, for which the trajectory of prothrombin time was to be predicted solely from the sparse and noisy albumin measurements. For this prediction, the data of each subject to be predicted were omitted, the functional regression model was fitted from the remaining subjects, and then the predictor measurements were entered into the fitted model to obtain the predicted response trajectory, thus leading to genuine predictions. Predicted curves and 95% pointwise confidence bands are shown in Figure 4 . Note that these predictions of longitudinal trajectories are based on just a few albumin measurements, and the prothrombin time response measurements shown in the figures are not used.\nRegarding the functional coefficients of determination R 2 (15) and R 2 (18), we obtain very similar estimates R 2 = 0.37 and R 2 = 0.36, which we would interpret to mean that about 36% of the total functional variation of the prothrombin time trajectories is explained by the albumin data, indicating a reasonably strong functional regression relationship. The curve of estimated pointwise coefficients of determination R 2 (t) (19) is shown in Figure 5 , left panel, which describes the trend of the proportion of the variation of the prothrombin time, at each argument value, that is explained by the entire albumin trajectories. We find that the observations in the second half are better determined by the albumin trajectories than the values in the first half of the domain of prothrombin time."}, {"section_title": "Functional Regression of Systolic Blood Pressure on Body Mass Index", "text": "As a second example, we discuss a functional regression analysis of systolic blood pressure trajectories (responses) on body mass index trajectories (predictor), using anonymous data from the Baltimore Longitudinal Study of Aging (BLSA), a major longitudinal study of human aging (Shock et al., 1984) .\nThe data consist of 1590 male volunteers who were scheduled to visit the Gerontology Research Center bi-annually. Time corresponds to age of each subject and is measured in years. On each visit, systolic blood pressure (in mm Hg), and body mass index (BMI=weight in kg/height in m 2 ) were assessed.\nSince both measurements are highly variable, the data are noisy, and as many participants missed scheduled visits, or were seen at other than the scheduled times, the data are sparse with largely unequal numbers of repeated measurements and widely differing measurement times per subject.\nMore details about the study and data can be found in Pearson et al. (1997) , and for previous statistical approaches we refer to Morrell, Pearson and Brant (1997) .\nWe included the participants with measurements within the age range [60, 80] , and first checked for outliers based on standardized residuals of body mass index (BMI) and systolic blood pressure (SBP), respectively. The standardized residuals are defined as residuals divided by the pooled sample standard deviation, where residuals are the differences between measurements and the estimated mean function obtained by scatterplot smoothing, using the local linear smoother. We excluded subjects with standardized residuals larger (or less) than \u00b13, for either BMI or SBP. Individual trajectories of BMI and SBP for the included 812 subjects are shown in Figure 6 , along with the smooth estimated mean functions of BMI and SBP.\nWhile average BMI decreases after age 64, SBP throughout shows an increasing trend. Based on the AIC criterion, three eigenfunctions are used for the predictor (BMI) function, and four for the response (SBP) function; these are displayed in Figure 7 . The first eigenfunctions of both processes correspond to an overall mean effect, and the second eigenfunctions to a contrast between early and late ages, with further oscillations reflected in third and fourth eigenfunctions.\nThe estimated regression function in Figure 8 indicates that a contrast between late and early BMI levels forms the prediction of SBP levels at later ages, where late BMI levels are weighted positive and early levels negative. When predicting SBP at age 80, the entire BMI trajectory matters, and rapid overall declines in BMI lead to the lowest SBPs, where speed of decline between 60 and 65 and between 75 and 80 is critical. Similar patterns can be identified for predicting SBP at other ages.\nAs in the previous example, we randomly select four study participants and obtain predictions and 95% pointwise bands for each of these, based on one-leave-out functional regression analysis ( Figure   9 ). The predicted trajectories are found to be reasonably close to the observations, which are not used in the analysis.\nThe functional coefficients of determination R 2 (15) and R 2 (18) were both estimated as 0.13, indicating that the dynamics of body mass index explains 13% of the total variation of systolic blood pressure trajectories; the functional regression relationship is seen to be weaker than in the previous example. The curve of estimated pointwise coefficients of determination R 2 (t) (17) is displayed in Figure 5 , right panel, indicating generally weaker linear association at older ages (beyond 70 years) as compared to the earlier ages (60 to 70 years). The minimal linear association between predictor trajectories and the functional response is seen to occur around age 75.7."}, {"section_title": "Asymptotic Properties", "text": "In this section, we present the consistency of the regression function estimate (8) In what follows, we only consider the case that the processes X and Y are infinite dimensional.\nIf they are finite-dimensional, and there exist true finite K and M , such that G X and G Y are finite dimensional surfaces, then, as n tends to infinity, remainder terms such as \u03b8 n and \u03d1 n in (41) To define the convergence of the right hand side of (5) in the L 2 sense, we require that\nFurthermore, the right hand side of (5) converges uniformly on S \u00d7 T , provided that"}, {"section_title": "is continuous in s and t, and the function", "text": "The numbers M = M (n) and K = K(n) of included eigenfunctions are integer-valued sequences that depend on sample size n, see assumption (B5) in Appendix A.1. For simplicity, we suppress the dependency of M and K on n in the notation. The consistency of\u03b2 (8) "}, {"section_title": "[\u03b2(s, t) \u2212 \u03b2(s, t)]", "text": "2 ds dt = 0 in probability,\nand if (A1) is replaced with (A2),\nThe rate of convergence in (22) and (23) depends on specific properties of processes X and Y in the following way: If \u03c4 n , \u03c5 n and \u03c2 n are defined as in (B.5) of Appendix A.1, and \u03b8 n , \u03d1 n are defined as in (41) in Appendix A.2, then for (22) we obtain the rate\nand for (23) the rate is\nas n \u2192 \u221e. Here, \u03c2 n depends on bandwidths h 1 and h 2 that are used in the smoothing step (36) for the cross-covariance function C(s, t) = cov(X(s), Y (t)). These rates depend on specific properties of processes X and Y , such as the spacings of the eigenvalues of their autocovariance operators. We note that due to the sparsity of the data (at most finitely many observations are made per random trajectory), fast rates of convergence cannot be expected in this situation, in contrast to the case where entire trajectories are observed or are densely sampled. \nThis provides the consistency of the prediction Y * KM for the target trajectory Y * .\nFor the following results, we require Gaussian assumptions.\n(A5) For all 1 \u2264 i \u2264 n, m \u2265 1 and 1 \u2264 l \u2264 L i , the functional principal component scores \u03b6 im and the measurement errors \u03b5 il in (1) are jointly Gaussian.\nWe require existence of a limiting function and therefore the following analytical condition:\n(A6) There exists a continuous positive definite function \u03c9(t 1 , t 2 ) such that \u03c9 KM (t 1 , t 2 ) \u2192 \u03c9(t 1 , t 2 ),\nWe obtain the asymptotic distribution of { Y * KM (t) \u2212 E[Y * (t)|X * ]} as follows, providing inference for predicted trajectories."}, {"section_title": "Theorem 3. Under (A3)-(A6) and the assumptions of Lemma 1 and (B5) (see Appendix A.1), given", "text": "Considering the measure R 2 (15), R 2 is well-defined since\nFurthermore, the right hand side of (17) uniformly converges on t \u2208 T , provided that\nThe consistency of R 2 (16), R 2 (t) (19) and R 2 (20) is obtained as a consequence of Lemma 1, (A7) and (B5)."}, {"section_title": "Theorem 4. Under the assumptions of Lemma 1 and (B5) (see Appendix A.1),", "text": "and if (A7) is assumed,\nWe note that the rate of convergence in (26) is the same as in the remark after Theorem 1, and that the rate in (27) and (28) is given by O p (\u03c4 n + \u03c5 n + \u03c2 n + \u03c0 n ), where \u03c0 n is as in (29)."}, {"section_title": "Concluding Remarks", "text": "The functional regression method we are proposing applies to the situation where both predictors and responses are curves. Sparse data situations will occur also in other functional regression situations where responses could be functions and predictors vectors or responses are scalars and the predictors are curves. The ideas presented here can be extended to such situations.\nThe approach to functional regression we have proposed is quite flexible, and in simulations is seen to be robust to violations of assumptions such as the Gaussian assumption. It is a useful tool for data where both predictors and responses are contaminated by errors. We refer to Cui, He and Zhu (2002) for another approach and discussion of de-noising of single pairs of curves observed with measurement errors. Besides varying coefficient models, the available methodology for situations where one has a sample of predictor and response functions is quite limited. Common methods in longitudinal data analysis such as Generalized Estimating Equations and Generalized Linear Mixed Models are not suitable for this task.\nThe proposed methodology may prove generally useful for longitudinal data with missing measurements, where missingness would be assumed to be totally unrelated to the random trajectories and errors. Extensions to situations where missingness is correlated with the time courses would be of interest in many practical applications. There are also some limitations to the functional regression approach under sparse data. Our prediction methods target the trajectory conditional on the available data, while the response trajectory given the entire but unobservable predictor trajectory is not accessible. While in theory it is enough that the probability that one observes more than one measurement per random trajectory is positive, in practice there needs to be a substantial number of subjects with two or more observations. Sometimes a prediction for a response trajectory may be desired even if there is no observation at all available for that subject. In this case we predict the estimated mean response function as response trajectory. This is not unreasonable, since borrowing strength from other subjects to predict the response trajectory for a given subject is a key feature of the proposed method that will come more into play for subjects with very few measurements. Predictions for these subjects will often be relatively closer to the mean response than for subjects with many measurements.\nWe conclude by remarking that extensions to cases with more than one predictor function are of interest in a number of applications, and would be analogous to the extension of simple linear regression to multiple linear regression. Functional regression is only at its initial stages and much more work needs to be done."}, {"section_title": "Acknowledgments", "text": "We wish to thank two referees and an associate editor for most helpful and insightful remarks, and are indebted to Dr. C. H. Morrell, Loyola College, for his support and help to access an anonymous version of a subset of the BLSA data. This research was supported in part by NSF grants DMS99-71602, DMS00-79430, DMS02-04869, DMS03-54448 and DMS04-06430."}, {"section_title": "Appendix", "text": ""}, {"section_title": "A.1 Assumptions and notations", "text": "The data (S il , U il ) and ( (1) and (2), are assumed to have the same distribution as (S, U ) and (T, V ), with joint densities g 1 (s, u) and t, v) . Assume that the observation times S il are i.i.d. with marginal density f S (s). Dependence is allowed for observations U il 1 and U il 2 made for the same subject or cluster, and analogous properties hold for V ij , where T ij are i.i.d. with marginal density f T (t). We make the following assumptions for the number of observations L i and N i that are available for the ith subject:\n(B1.1) The number of observations L i and N i made for the ith subject or cluster are random variables \nLet K 1 (\u00b7) and K 2 (\u00b7, \u00b7) be nonnegative univariate and bivariate kernel functions that are used in the smoothing steps for the mean functions \u00b5 X , \u00b5 Y , covariance surfaces G X , G Y , and cross-covariance structure C. Assume that K 1 and K 2 are compactly supported densities with zero means and finite G Y (31) , and h 1 = h 1 (n), h 2 = h 2 (n) be the bandwidths for obtaining C (36). We develop asymptotics as the number of subjects n \u2192 \u221e, and require\n(B2.3) Without loss of generality, h 1 /h 2 \u2192 1, nh 6 1 \u2192 \u221e, and nh 8 1 < \u221e.\nDefine the Fourier transformations of\n(B3.2) \u03ba 2 (t, s) is absolutely integrable, i.e., |\u03ba 2 (t, s)|dtds < \u221e.\nAssume that the fourth moments of Y and U , centered at \u00b5 Y (T ) and \u00b5 X (S), are finite, i.e.,\nLet S 1 and S 2 be i.i.d. as S, and U 1 and U 2 be the repeated measurements of X made on the same subject, taken at S 1 and S 2 separately. Assume ( \nDefine the rank one operator f \u2297 g = f, h y, for f, h \u2208 H, and denote the separable Hilbert space of Hilbert-Schmidt operators on H by F \u2261 \u03c3 2 (H), endowed by T 1 , T 2 F = tr(T 1 T * 2 ) = j T 1 u j , T 2 u j H and T 2 F = T, T F , where T 1 , T 2 , T \u2208 F , and {u j : j \u2265 1} is any complete orthonormal system in H. The covariance operator G X (respectively, G X ) is generated by the\nThen G X and G X are Hilbert-Schmidt operators, and Theorem 1 in Yao et al. (2005) implies that\n. \nand analogously define sequences {\u03b4 Y j } and {A \u03b4 Y j } for the response process Y . We assume that the numbers M = M (n) and K = M (n) of included eigenfunctions depend on the sample size n, such\nThe main effect of this assumption is to impose certain constraints on the rate of K and M in relation to n and the bandwidths.\nWe denote the remainder in (A7) by"}, {"section_title": "A.2 Estimation procedures", "text": "We begin by summarizing the estimation procedures for the components of models (1) and (2) in the following, see Yao et al. (2005) for further details. Define the local linear scatterplot smoothers\nwith respect to\n, and define the local linear surface smoother for G X (s 1 , s 2 ) through minimizing\nwhere\nFor the estimation of \u03c3 2 X , we fit a local quadratic component orthogonal to the diagonal of G X , and a local linear component in the direction of the diagonal. Denote the diagonal of the resulting surface estimate by G X (s), and a local linear smoother focusing on diagonal values\nif\u03c3 2 X > 0, and\u03c3 2 X = 0 otherwise.\nThe estimates of {\u03c1 m , \u03c8 m } m\u22651 correspond to the solutions {\u03c1 m ,\u03c8 m } m\u22651 of the eigenequations,\nwith orthonormal constraints on {\u03c8 m } m\u22651 .\nLet\u03bc\nbe the estimated mean and eigenfunctions after removing the data for X i .\nOne-curve-leave-out cross-validation aims to minimize\nwith respect to M , where X\nim is obtained by (11). The AIC criterion as a function of M is given by\nwhere"}, {"section_title": "and", "text": "\u03b6 im is obtained by (11). We proceed analogously for the corresponding estimates for the components of model (2) regarding the response process Y .\nThe local linear surface smoother for the cross-covariance surface C(s, t) is obtained through\nwith regard to \u03b2 = (\u03b2 0 , \u03b2 11 , \u03b2 12 ), leading to C(s, t) =\u03b2 0 (s, t)."}, {"section_title": "A.3 Preliminary consistency results", "text": "Applying Theorems 1 and 2 of Yao et al. (2005) , we obtain uniform consistency of the estimate of the mean functions, covariance functions, eigenvalues, and eigenfunctions of the predictor and response processes. Under assumption (A2.3), this extends to the cross-covariance function."}, {"section_title": "Lemma 1. Under (B1.1)-(B5), and appropriate regularity assumptions for", "text": "Considering eigenvalues \u03c1 m and \u03bb k of multiplicity one respectively,\u03c8 m and\u03c6 k can be chosen such\nand furthermore\nAs a consequence of (38) and (39),\nwhere the O p (\u00b7) terms in (37), (38) and (40) hold uniformly over all k and m.\nProof of Lemma 1. Part of the proof follows that of Theorem 2 in Yao et al. (2005) . Additional arguments are needed to obtain the convergence rates in (38). Since\nTherefore, "}]