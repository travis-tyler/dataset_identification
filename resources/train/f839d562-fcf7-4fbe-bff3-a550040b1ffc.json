[{"section_title": "Abstract", "text": "Abstract-3D medical image registration is of great clinical importance. However, supervised learning methods require a large amount of accurately annotated corresponding control points (or morphing). The ground truth for 3D medical images is very difficult to obtain. Unsupervised learning methods ease the burden of manual annotation by exploiting unlabeled data without supervision. In this paper, we propose a new unsupervised learning method using convolutional neural networks under an end-to-end framework, Volume Tweening Network (VTN), to register 3D medical images. Three technical components ameliorate our unsupervised learning system for 3D end-to-end medical image registration: (1) We cascade the registration subnetworks; (2) We integrate affine registration into our network; and (3) We incorporate an additional invertibility loss into the training process. Experimental results demonstrate that our algorithm is 880x faster (or 3.3x faster without GPU acceleration) than traditional optimization-based methods and achieves state-of-theart performance in medical image registration."}, {"section_title": "I. INTRODUCTION", "text": "Image registration is the process of mapping images into the same coordinate system by finding the spatial correspondence between images (see Figure 1 ). It has a wide range of applications in medical image processing, such as aligning images of one subject taken at different times. Another example is to match an image of one subject to some predefined coordinate system, such as an anatomical atlas [1] .\nThere has not been an effective approach to generate ground-truth flows for medical images. Widely used supervised methods require accurately labeled ground truth with a vast number of instances. The quality of these labels directly affects the result of supervision, which entails much effort in traditional tasks such as classification and segmentation. But optical flows are dense and ambiguous quantities that are almost impossible to be labeled manually, and moreover, automatically generated dataset (e.g., the Flying Chairs dataset [2] ) which deviates from the realistic demands is not appropriate. Consequently, supervised methods are hardly applicable. In contrast, unlabeled medical images are universally available, and sufficient to advance the state of the art through our unsupervised framework shown in this paper.\nPreviously, there has been much effort on automating image registration. Tools like FAIR [3] , ANTs [4] and Elastix [5] have been developed for automated image registration [1] . Generally, these algorithms define a space of transformations and a metric of alignment quality, and then find the optimal transformation by iteratively updating the parameters. The optimization process is highly time-consuming, rendering such methods impractical for clinical applications.\nThere have been some works tackling medical image registration with CNNs [6] . Miao et al. [7] use CNN to perform 2D/3D registration, in which CNN regressors are used to estimate transformation parameters. Their CNNs are trained over synthetic data and the method is not end-to-end. Wu et al. [8] propose an unsupervised method to train CNN to extract features for 3D brain MRI registration. However, their method is patch-based and needs a feature-based registration method, thus is not end-to-end.\nOptical flow prediction is a closely related problem that aims to identify the correspondence of pixels in two images of the same scene taken from different perspectives. FlowNet [2] and its successor FlowNet 2.0 [9] are CNNs that predict optical flow from input images using end-to-end fully convolutional networks (FCN [10] ), which are capable of regressing pixellevel correspondence. FlowNet is trained on Flying Chairs, a synthetic dataset that consists of pairs of realistic images arXiv:1902.05020v1 [cs.CV] 13 Feb 2019 Figure 2 : Illustration of an overall structure of Volume Tweening Network (VTN) and how gradients back-propagate. Every registration subnetwork is responsible for finding the deformation field between the fixed image and the current moving image. The moving image is repeatedly warped according to the deformation field and fed into the next level of cascaded subnetworks. The current moving images are compared against the fixed image for a similarity loss function to guide training. There is also regularization loss, but not drawn for the sake of cleanness. The number of cascaded subnetworks may vary; only two are illustrated here. In the figure, yellow (lighter) indicates the similarity loss between the first warped image and the fixed image, and blue (darker) that between the second warped image and the fixed image. Solid bold arrows indicate how the loss is computed, and dashed bold arrows indicate how gradients back-propagate. Note that the second loss will propagate gradients to the first subnetwork as a consequence of the first warped image being a differentiable function of the first subnetwork. generated using computer graphics algorithms and the groundtruth flow. However, realistic medical images are hard to generate, which leaves us with the only option of using medical images captured by sensors. The ground-truth flow between medical images is very difficult to obtain, which impedes the employment of supervised learning-based methods.\nSpatial Transformer Networks (STN) [11] is a component in neural networks that spatially transforms feature maps to ease back-end tasks. It learns a localization net to produce an appropriate transformation to \"straighten\" the input image. The localization net is learnt without supervision, though back-end task might be supervised. Given a sampling grid and a transformation, STN applies the warping operation and outputs the warped image for further consumption by deeper networks. The warping operation deals with off-grid points by multi-linear interpolation, hence is differentiable and can back-propagate gradients.\nInspired by FlowNet and STN, we present a network structure (see Figure 2) , called Volume Tweening Network (VTN), enabling unsupervised training of end-to-end CNNs that perform voxel-level 3D medical image registration. Training does not require ground truth as FlowNet (2.0) does. The moving image is registered and warped, and the warped image is compared against the fixed image to form a similarity loss. There is a rich body of research into similarity losses [12] , [13] , [14] . The model is trained to minimize a combination of regularization loss and similarity loss. As the method is unsupervised, the performance potentially increases as it is trained with more unlabeled data. The network consists of several cascaded subnetworks, the number of which might vary, and each subnetwork is responsible for producing a transform that aligns the fixed image and the moving one. Deeper layers register moving images warped according to the output of previous layers with the initial fixed image. The final prediction is the composition of all intermediate flows.\nIt turns out that network cascading significantly improves the performance in the presence of large displacement between the input images. While the idea of cascading subnetworks is found in FlowNet 2.0, our approach does not include as much artificial intervention of network structures as FlowNet 2.0 does. Instead, we employ a natural dichotomy in a subnetwork structure consisting of affine and deformable registration processes, which is also found in traditional methods, including ANTs (affine and SyN for deformable) [4] and Elastix (affine and B-spline for deformable) [5] . Besides these structural innovations, we also introduce the invertibility loss, similar to left-right disparity consistency loss in [15] , to 3D medical image registration. Compared with traditional optimizationbased algorithms, ours is 880x faster (or 3.3x faster without GPU acceleration) and achieves state-of-the-art registration accuracy.\nTo summarize, we present a new unsupervised end-toend learning system using convolutional neural networks for deformation field prediction between 3D medical images. In this framework, we develop 3 technical components: (1) We cascade the registration subnetworks, which improves performance for registering largely displaced images without much slow-down; (2) We integrate affine registration into our network, which proves to be effective and faster than using a separate tool; (3) We incorporate an additional invertibility loss into the training process, which improves registration performance. The contributions of this work are closely related. An unsupervised approach is very suitable for this problem, as the images are abundant and the ground truth is costly to acquire. The use of the warping operation is crucial to our work, providing the backbone of unsupervision, network cascading and invertibility loss. Network cascading further allows us to plug in different subnetworks, and in this case, the affine registration subnetwork and the deformable ones, enabling us to adapt the natural structure of multiple stages in image registration. The efficient implementation of our algorithm gives a satisfactory speed."}, {"section_title": "II. RELATED WORKS", "text": ""}, {"section_title": "A. Directly Related Works", "text": "Several existing works that are directly related to ours are discussed below.\nFlowNet [2] is an end-to-end FCN [10] that predicts optical flow between realistic images. The network has an encoderdecoder structure that extracts features and predicts optical flow at progressively refined scales, in which skip connections are added to combine high-level and low-level features. FlowNet is trained in a supervised manner with Flying Chairs, a synthetic dataset with ground-truth flow. In contrast, our method is trained without any ground-truth flow, nor does it require synthesizing training data.\nFlowNet 2.0 [9] , the successor of FlowNet, cascades several carefully crafted versions of FlowNet into one large network. The approach has proven helpful in improving accuracy of estimation. All of the subnetworks in FlowNet 2.0 perform deformable registration and each of them is carefully handcrafted. Handcrafting the architecture makes the method prone to being task-specific. In contrast, we cascade two types of subnetworks, one performing affine registration, the other dense deformable, and we do not tweak the structure of our subnetworks.\nSpatial Transformer Network (STN) [11] is a neural network that performs class-wise alignment. Spatial transformation parameters can be learned by training STN to loss functions without supervision. STN contains a differentiable layer that warps feature maps. One can reconstruct the fixed image by warping the moving one with the flow. STN itself does not register images. We employ the warping operation in STN to enable unsupervised training as well as network cascading. Figure 3 compares STN and our subnetwork (unit of cascading). The localization net is comparable to the convolutional part of our network, which produces the parameters (the flow field). Our grid generator simply skews the lattice on which input images are defined according to the field. The sampler, also known as the warp operation, warps the moving image into the warped image. Both nets are unsupervised. A key difference is that our subnetwork takes two images as input whereas STN takes only one. This difference is not only in design but also philosophical, and adapts to the tasks for each network. Our subnetwork takes one image (the fixed) as the reference image, because the task is to align the other image (the moving) to it. The training is guided by the alignment between the warped image and the fixed image. In STN's settings, there is no such reference hence only one input image, and the task is to adjust the image to ease whatever task the back-end is to tackle. Its training is guided by the back-end task, which forces it to improve its warping strategy if the back-end network cannot solve the task well because working with the warped image is too hard for it. Philosophically, STN implicitly learns a reference \"atlas\" for the task at hand.\nIn an earlier work towards fast and accurate medical image registration by Shan et al. [16] , an unsupervised end-toend learning-based method for deformable medical image registration is proposed. The method in [16] registers 2D images. It is evaluated by registering corresponding sections (a) STN structure from [11] (b) fitting our subnetwork into STN parlance of MR brain images and CT liver scans. In contrast, our 3D algorithm directly performs 3D registration, fully exploiting information available from the 3D images. Moreover, we cascade subnetworks to improve the performance. With the help of cascaded subnetworks, affine registration is integrated into our network, which is done out-of-band in [16] .\nVoxelMorph, proposed by a concurrent work by Balakrishnan et al. [17] , is an unsupervised learning-based method for 3D medical image registration. VoxelMorph contains an encoder-decoder structure, uses warping operation to produce warped moving images and is trained to minimize the dissimilarity between the warped image and the fixed image. It is noticeable that both methods use the warp operation to train networks without supervision. This paper exploits the operation in a trio (namely enabling unsupervised training, enabling cascading, and implementing invertibility loss, which will be detailed later). Their method does not consider affine registration and assumes the input images are already affinely aligned, whereas ours embeds the process of affine registration as an integrated part of the network. Furthermore, their algorithm does not work well when large displacement between the images is present, which is common for liver CT scans. Finally, VoxelMorph is designed to take any two images as input, but [17] only evaluates it in the atlas-based registration scenario. Consider a clinical scenario where the brain of a patient is captured before and after an operation. It would be better, in terms of accuracy and convenience, to register these two images, instead of registering both to an atlas. In this paper, we present a more sophisticated and robust design that works well in the presence or absence of large displacement, and we evaluate the methods for general registration among images."}, {"section_title": "B. Traditional Algorithms", "text": "Over recent decades, many traditional algorithms have been developed and studied to register medical images [18] , [19] , [4] , [5] . Most of them define a hypothesis space of possible transforms, often described by a set of parameters, and a metric of the quality of such a transform, and then find the optimum by iteratively updating the parameters.\nSelecting an informative metric is crucial to registration quality. The metrics, also called loss functions, often consist of two parts, one measuring the level of correspondence between the input images implied by the transform, the other regularizing the transform itself. Examples of the former part include photo-metric difference, correlation coefficient and mutual information [13] , [14] among others [12] . Some of these measures, notably mutual information, require binning or quantizing, which makes gradients vanish thus continuous optimization techniques such as gradient descent inapplicable.\nThe transformation space can be either parametric or nonparametric. Affine transforms can be described by only a few real numbers, whereas a free-form dense deformable field specifies the displacement for each grid point. Though the latter contains all possible transforms, it is common to apply a multi-stage approach to the problem. For example, ANTs [4] registers the input images with a rigid transform, then a general affine transform, and finally a deformable transform modeled by SyN. In this paper, we also have components for affine/deformable registration, which are modeled by neural networks.\nTraditional methods have achieved good performance on several datasets, and are state-of-the-art, but their registration speed is barely practical for clinical applications. These methods do not exploit the patterns that exist in the registration task. In contrast, learning-based methods are generally faster. Computationally, they do not need to iterate over and over before producing the result. Conceptually, they learn those patterns represented by the parameters of the underlying learning model. The trained model is an efficient replacement of the optimization procedure."}, {"section_title": "C. Supervised Learning Methods", "text": "Lee et al. [20] employ Support Vector Machines (SVM) to learn the similarity measure for multi-modal image registration for brain scans. The training process of their algorithm requires pre-aligned image pairs thus the method is supervised. Sokooti et al. [21] develop a patch-based CNN to register chest CT scans and trains it with synthetic data. FlowNet [2] , developed by Dosovitskiy et al., is an FCN [10] for optical flow prediction. The network is trained with synthetic data and estimates pixel-level correspondence.\nWhile supervised learning methods achieve good performance, either abundant groud-truth alignment must be available, or synthetic data are used. Generation of synthetic data has to be carefully designed so that the generated data resemble the real ones."}, {"section_title": "D. Unsupervised Learning Methods", "text": "Obtaining the ground truth for registration between 3D medical images is very difficult. To work around this, unsupervised methods are to help.\nBoth inspired by FlowNet [2] , Ren et al. [22] and Yu et al. [23] use STN to warp images according to the optical flow produced by CNN. They use Charbonnier penalty of fixed and warped images to guide the optimization of CNN, eliminating the need for ground-truth flow. Targeting another task, single view depth estimation, Garg et al. [24] develop an FCN that predicts depth from one photo, and use the depth field to reconstruct an image from another perspective, which is then compared against the other photo. The reconstruction loss is used to train CNN without annotated depth data. The method gains a boost in performance with a left-right consistency check [15] ."}, {"section_title": "III. METHOD A. Problem Formulation", "text": "The input of an image registration problem consists of two images I 1,2 , both of which are functions \u2126 \u2192 R c , where \u2126 is a region of R n and c denotes the number of channels. Since this work focuses on 3D medical image registration, we confine ourselves to the case where \u2126 is a cuboid and c = 1 (grayscale image). Specifically, this means \u2126 \u2286 R 3 and each image is a function \u2126 \u2192 R. The objective of image registration is to find a displacement field (or flow field) f 12 : \u2126 \u2192 R 3 so that\nwhere the precise meaning of \"\u2248\" depends on specific application. The field f 12 is called the flow from I 1 to I 2 since it tells where each voxel in I 1 is in I 2 . We define warp (I 2 , f ) as the image I 2 warped according to f , i.e., warp (I 2 , f ) (x) = I 2 (x + f (x)). The above objective can be rephrased as finding f maximizing the similarity between I 1 and warp (I 2 , f ). The image I 1 is also called the fixed image, and I 2 the moving one. The term \"moving\" suggests that the image is transformed during the registration process.\nConsider warping an image twice, first with g 1 then with g 2 . What this procedure produces is\nThis motivates the definition of the composition of two flows. If we define the composition of the flow fields g 1 , g 2 to be\nEquation (2) can be restated as\nIt is noticeable that the warp operation in the above formulation should be further specified in practice. Real images as well as flow fields are only defined on lattice points. We continuate them onto the enclosing cuboid by trilinear interpolation as done in [25] . Furthermore, we deal with out-of-bound indices by nearest-point interpolation. That is, to evaluate a function defined on lattice points at any point x, we first move x to the nearest in the enclosing cuboid of those lattice points, then interpolate the value from the 8 nearest lattice points."}, {"section_title": "B. Unsupervised End-to-End Registration Network", "text": "Our network, called Volume Tweening Network (VTN), consists of several cascaded registration subnetworks, after each of which the moving image is warped. The unsupervised training of network parameters is guided by the dissimilarity between the fixed image and each of the warped images, with the regularization losses on the flows predicted by the subnetworks.\nThe warping operation, also known as the sampler in STN [25] , is differentiable due to the trilinear interpolation. The warping operation back-propagates gradients to both the input image and the input flow field, which is critical for training our cascaded networks. These subnetworks are trained to work cooperatively in a way that the moving image is successively and gradually aligned.\nFor better performance, it is common to apply an initial rigid transformation as a global alignment before predicting the dense flow field. Instead of prepending a time-consuming preprocessing stage with a tool like ANTs [4] as what is done in VoxelMorph [17] , we integrate this procedure as a top-level subnetwork. The integrated affine registration subnetwork not only works in negligible running time, but also outperforms the traditional affine stage."}, {"section_title": "C. Loss Functions", "text": "To train our model in an unsupervised manner, we measure the (dis)similarity between the moving images warped by the spatial transformer and the fixed image. There is a rich body of research in similarity metrics suitable for medical image registration [26] , [12] . Furthermore, regularization losses are introduced to prevent the flow fields from being unrealistic or overfitting. In the following paragraphs, \u2126 denotes the cuboid (or grid) on which the input images are defined. We will introduce the loss functions that we use in the experiments. Extended discussion of other possible loss functions is available in the appendices. a) Correlation Coefficient: The covariance between I 1 and I 2 is defined as\nand their correlation coefficient is defined as\nThe images are regarded as random variables whose sample space is the points on which voxel values are available. The range of correlation coefficient is [\u22121, 1], it measures how much the two images are linear related, and attains \u00b11 if and only if the two are linear function of each other. Applying a non-degenerate linear function to any of the images does not change their correlation coefficient, therefore, this measure is more robust than L 2 loss. For real-world images, the correlation coefficient should be non-negative (unless one of the images is a negative film). The correlation coefficient loss is defined as\nb) Orthogonality Loss: For the specific task discussed in this paper (medical image registration), it is usually the case that the input images need only a small scaling and a rotation before they are affinely aligned. We would like to penalize the network for producing overly non-rigid transform. To this end, we introduce a loss on the non-orthogonality of I + A, where A denotes the transform matrix produced by the affine registration network (see Section IV-B for more details). Let \u03bb 1,2,3 be the singular values of I + A, the orthogonality loss is\nThe more deviant I + A is from being an orthogonal matrix, the larger its orthogonality loss. If I + A is orthogonal, the value will be zero. c) Determinant Loss: We assume images are taken with the same chirality, therefore, an affine transform involving reflection is not allowed. This imposes the requirement that det (I + A) > 0. Together with the orthogonality requirement, we set the determinant loss to be\nd) Total Variation Loss (Smooth Term): For a dense flow field, we regularize it with the following loss that discourages discontinuity:\nwhere e 1,2,3 form the natural basis of R 3 . This is also known as the L2 smooth term."}, {"section_title": "IV. NETWORK ARCHITECTURE A. Cascading", "text": "Each subnetwork is responsible for aligning the fixed image and the current moving image. Following each subnetwork, the moving image is warped with the predicted flow, and the warped image is fed into the next cascaded subnetwork. The flow fields are composed to produce the final estimation. Figure 2 illustrates how the networks are cascaded, how the images are transformed and how each part contributes to the loss. All layers being differentiable, the gradient will backpropagate so that the subnetworks can be trained.\nIt might be tempting to compare our scheme with that of FlowNet 2.0 [9] . FlowNet 2.0 stacks subnetworks in a different way than our method. It performs two separate lines of flow estimations (large/small displacement flows) and fuses them into the final estimation. Each of its intermediate subnetworks has inputs of not only the warped moving image and the fixed image, but also the initial moving image, the current flow, and the brightness error. Its subnetworks are carefully crafted, having similar yet different structures and expected to solve specific problems (e.g., large/small displacement) in flow estimation. In contrast, our method does not involve two separate lines of registration, i.e., each subnetwork works on the fixed image and the warped moving image produced by the previous one, and intermediate subnetworks do not get more input than the initial one. We do not interfere much with the structures of subnetworks. Despite the initial affine subnetwork, we do not assign specific tasks to the remaining subnetworks since they share the same structure. The affine registration subnetwork aims to align the input image with an affine transform. It is only used as our first subnetwork. Its structure, as illustrated in Figure 4 , does not resemble an auto-encoder suggested by the sandglass shape in Figure 2 . This type of subnetwork progressively downsamples the input by doing strided 3D convolution, after which a fullyconnected layer is applied to produce 12 numeric parameters, which represents a 3 \u00d7 3 transform matrix A and a 3-dimensional displacement vector b. As a common practice, the number of channels doubles as the length of resolution halves. The flow field produced by this subnetwork is defined as"}, {"section_title": "B. Affine and Dense Deformable Subnetworks", "text": "The dense deformable registration subnetwork is used as all subsequent subnetworks, each of which refines the registration based on the output of the subnetwork preceeding it. Its structure, illustrated in Figure 5 , is an auto-encoder and similar to FlowNet [2] . We use strided 3D convolution to progressively downsample the image, and then use deconvolution (transposed convolution) [10] to recover spatial resolution. As suggested in [2] , skip paths connecting the deconvolution layer to the convolution layer with the same spatial resolution are added to help localizing estimation, which results in a structure similar to U-Net [27] . The subnetwork will output the dense flow field, a volume feature map with 3 channels (x, y, z displacements) of the same size as the input."}, {"section_title": "C. Invertibility", "text": "Given two images I 1,2 , going from a voxel in I 1 to the corresponding voxel in I 2 then back to I 1 should give zero displacement. Otherwise stated, the registration should be round-trip. In Figure 6 , we demonstrate the possible situations. The pair of solid arrows exemplifies round-trip registration, whereas the pair of dashed arrows exemplifies non-round-trip registration. If we have computed two flow fields (back and forth), f 12 and f 21 , the composed fields exhibit the roundtrip behavior of the registration, as illustrated by the magenta straight arrow in Figure 6 . Ideally, round-trip registration should satisfy the equations f 12 f 21 = f 21 f 12 = 0. We capture the round-tripness for a pair of images with the invertibility loss, namely\nThe larger the invertibility loss, the less round-trip the registration. For perfectly round-trip registration, the invertibility loss is zero. We come up with, formulate, and implement the invertibility loss independently of [15] . We use L2 invertibility loss whereas [15] uses L1 left-right disparity consistency loss, which is just a matter of choice. We are the first to incorporate the invertibility loss into 3D images to boost performance on medical image tasks. "}, {"section_title": "V. EXPERIMENT", "text": "We evaluate our algorithm with extensive experiments on liver CT datasets and brain MRI datasets. We compare our algorithm against state-of-the-art traditional registration algorithms including ANTs [4] and Elastix [5] , as well as VoxelMorph [17] . Our algorithm achieves state-of-the-art performance while being much faster. Our experiments prove that the performance of our unsupervised method is improved as more unlabeled data are used in training. We show that cascading subnetworks significantly improves the performance, and that integrating affine registration into the method is effective.\nWe evaluate the performance of algorithms with the following metrics:\n\u2022 Seg. IoU is the Jaccard coefficient between the warped liver segmentation and the ground truth. We warp the segmentation of the moving image by the predicted deformable field, and compute the Jaccard coefficient of the warped segmentation with the ground-truth segmentation of the fixed image. \"IoU\" means \"intersection over union\", i.e., |A\u2229B| |A\u222aB| , where A, B are the set of voxels the organ consists of.\n\u2022 Lm. Dist. is the average distance between warped landmarks (points of anatomical interest) and the ground truth.\n\u2022 Time is the average time taken for each pair of images to be registered. Some methods are implemented with GPU acceleration, therefore there are two versions of this metric (with or without GPU acceleration).\nOur model is defined and trained using TensorFlow [28] . We accelerate training with nVIDIA TITAN Xp and CUDA 8.0. We use the Adam optimizer [29] with the default parameters in "}, {"section_title": "A. Experiments on Liver Datasets 1) Settings:", "text": "The input to the algorithms are liver CT scans of size 128 3 . Affine subnetworks downsample the images to 4 3 before applying the fully-connected layer. Dense deformable subnetworks downsample the images to 2 3 before doing transposed deconvolution.\nWe cascade up to 4 registration subnetworks. The reason we need to cascade multiple subnetworks is that large displacement is very common among CT liver scans and that with more subnetworks, images with large displacement can be progressively aligned. Among these networks, the one with one affine registration subnetwork and three dense deformable registration networks (referred to as \"ADDD\") is used to be trained with different amount of data and compared with other algorithms. We use the correlation coefficient as our similarity loss, the orthogonality loss and the determinant loss as regularization losses for the affine subnetwork, and the total variation loss as that for dense deformable subnetworks. The ratio of losses for \"ADDD\" is listed in Table I . The performance is not very sensitive to the choice of hyperparameters. We suggest that each of the dense deformable subnetworks can automatically learn how to progressively align the images, and that only the final subnetwork and the affine subnetwork need to be trained with similarity loss.\n2) Datasets: We have three datasets available: 3 around the liver, permute (and reflect, if necessary) the axes so that the orientations of the images agree with each other, and normalize them by adjusting exposure so that the histograms of the images match each other. The preprocessing is necessary as the images come from different sources.\n3) Comparison among Methods: In Table II , \"ADDD\" is our model detailed in Section V-A1, and \"ADDD + inv\" is that model trained with additional term of invertibility loss in the central area (the beginning and the ending quaters of each side are removed) with relative weight 10 \u22123 . Learning-based methods (VTN and VoxelMorph) are trained on LITS and BFH datasets. All methods are evaluated on the MICCAI dataset. To prove the effectiveness of our unsupervised method, we also train \"ADDD\" supervised (the row \"supervised\"), where the output of ANTs is used as the ground truth (using end-point error [2] plus regularization term as the loss function). VoxelMorph-2 is trained (using code released by [17] ) with a batch size of 4 and an iteration count of 5000. Its performance with a batch size of 8 or 16 is worse so a batch size of 4 is used. After 5000 iterations, the model begins to overfit. The reason that it does not perform well on liver datasets might be that it is designed for brain registration thus cannot handle large displacement.\nThe results in Table II show the vast speed-up of learningbased methods against optimization-based methods. Our methods surpass state-of-the-art registration algorithms in terms of Segmentation IoU and Landmark Distance."}, {"section_title": "4) Performance with Different Amount of Data:", "text": "In Table III, the \"ADDD\" network (see Section V-A1) is trained with different amount of data. The result demonstrates that training with more unlabeled data improves the performance. Since we do not need any annotations during the training phase, there are abundant clinical data ready for use. "}, {"section_title": "B. Experiments on Brain Datasets a) Settings:", "text": "The input to the algorithms are brain MR images of size 128 3 . For some experiments, the selection of which will be detailed later, the brain scans are preprocessed to be aligned to a specific atlas from LONI [32] . We use ANTs for this purpose. There are two reasons we align the brain scans with ANTs. One is that VoxelMorph [17] requires the input to be affinely registered. The other is that we will compare the performance between \"ANTs + deformable registration subnetworks\" and \"affine registration subnetwork + deformable registration subnetworks\", i.e., to compare the effectiveness of integrating our affine registration subnetwork in place of ANTs.\nThe following methods will have ANTs-affine-aligned images as input: VoxelMorph methods and VTN without \"A\" (i.e., \"D\", \"DD\" and \"DDD\"). A more precise naming is \"ANTs + VoxelMorph\" and \"ANTs + VTN\". The following methods will not have ANTs-affine-aligned images as input: ANTs, Elastix, VTN with \"A\" (i.e., \"AD\", \"ADD\" and \"ADDD\"). Those methods have affine registration integrated. The comparison inside the former group focuses on dense deformable registration performance, that inside the latter group on overall performance, and that among the two groups benchmarks affine registration subnetwork versus ANTs affine registration in the context of a complete registration pipeline.\nWe will show 3 sets of comparisons similar to those for liver datasets. In the tables listed in later paragraphs, the time (with or without GPU) does not include the preprocessing with ANTs even if it is used. Preprocessing an image with ANTs costs 73.94 seconds on average. We will mention this fact again when such emphasis is needed.\nCare should be taken when evaluating methods with ANTs affine alignment. For the data to be comparable with those with affine registration integrated, the fixed image should be equivalent. Methods with ANTs affine alignment have both moving and fixed images aligned to an atlas. Those with integrated affine registration never move the fixed image. The affine transform produced by ANTs might not be orthogonal, which is the source of unfair comparison. If the affine transform is shrinking, methods with ANTs affine alignment gain advantage. Otherwise, methods with integrated affine alignment do.\nOne measure, Segmentation IoU, is not affected, because the volumes of all objects get multiplied by the determinant of the affine transform and the evaluation measure is homogeneous. For Landmark Distance, we perform the inverse of the linear part of the affine transform (which aligns the fixed image to the atlas) to the difference vector between warped landmark and landmark in the (aligned) fixed image, so that the length goes back to the coordinate defined by the original fixed image. This way, we minimize loss of precision to prevent unfairly underevaluating methods with ANTs affine alignment. Speaking of the actual data, the affine transformations produced by ANTs are slightly shrinking. Our correction restores a fair comparison among all methods. We acquire the second part of ABIDE after a while when the first part was downloaded and processed, thus the split. This only helps us to understand how performance improves as more data are used for training. For comparison among different methods, it is always the case that all the data mentioned above are used for training.\nRaw MR scans are cropped to 128 3 around the brain. Axes are permuted if necessary. The scans are normalized based on the histograms of their foreground color distribution, which might vary because they are captured on different sites.\nFor evaluation, we use 20 volumes from the LONI Probabilistic Brain Atlas (LPBA40) [32] . LONI consists of 40 volumes, 20 of which have tilted head positions and are discarded. For the remaining 20 volumes, 18 landmarks 2 are annotated by 3 experts and the average are taken as the ground truth.\nc) Comparison Among Methods: In Table V , we compare different methods on brain datasets. All neural networks are trained on all available training data, i.e., ADNI, ABIDE and ADHD. In the table, \"supervised\" is our \"ADD\" model supervised with ANTs as the ground truth. Its loss is the endpoint error [2] . Among these methods, our \"ADD\" achieves the lowest Landmark Distance with a competitive speed. If we compare \"ADD\" with \"DD\", we find that the integration of affine registration subnetwork significantly improves the Landmark Distance, compared to using ANTs for out-of-band affine alignment.\nd) Performance with Different Amount of Data: In Table VI, we summarize the performance of \"DD\" (with ANTs affine alignment) trained on different amount of unlabeled data. As more data are used to train the network, its performance in terms of Landmark Distance consistently increases. e) Network Cascading and Integration of Affine Registration: Table VII compares the performances of differently cascaded networks. The networks without \"A\" have ANTsaffine-aligned images as input, whereas the networks with \"A\" do not.\nAs one would expect, the performance in each group improves as the model gets more levels of cascaded subnetworks. While the methods with ANTs affine alignment have higher Segmentation IoU, integrating affine registration subnetwork yields better Landmark Distance. Worth mentioning is that the better Segmentation IoU comes at the price of a rather slow preprocessing phase (74 seconds)."}, {"section_title": "VI. CONCLUSION", "text": "In this paper, we present Volume Tweening Network (VTN), a new unsupervised end-to-end learning framework using convolutional neural networks for 3D medical image registration. The network is trained in an unsupervised manner without any ground-truth deformation. Experiments demonstrate that our method achieves state-of-the-art performance, and that it witnesses an 880x (or 3.3x without GPU acceleration) speed-up compared to traditional medical image registration methods. Our thorough experiments prove our contributions, each on its own being useful and forming a strong union when put together. Our networks can be cascaded. Cascading deformable subnetworks tackles the difficulty of registering images in the presence of large displacement. Network cascading also enables the integration of affine registration into the algorithm, resulting in a truly end-to-end method. The integration proves to be more effective than using out-of-band affine alignment. We also incorporate the invertibility loss into the training process, which further enhances the performance. Our methods can potentially be applied to various other medical image registration tasks. In Figure 7 , intermediate warped images as well as the fixed and moving images are projected and rendered, giving a straight-forward illustration of how the image is transformed step by step by a cascaded network. This explains the naming Volume Tweening Network -due to the cascaded nature of the network, the intermediate images look like frames in a shape tweening animation from the moving image to the fixed image."}, {"section_title": "APPENDIX B LOSS FUNCTIONS", "text": "This section lists some other loss functions that could be used for training the network.\nA. L 2 Loss L 2 loss (or mean square error) measures the difference between intensities of the images at each voxel, it is defined as\nL 2 loss is straight-forward and easy to implement, but it is not very informative if the fixing and moving images have different imaging parameters. The disadvantage manifests itself especially when the images are inter-subject, or are imaged using devices from different manufacturers."}, {"section_title": "B. Mutual Information", "text": "Again we will regard the images as random variables. The entropy of a discrete random variable X is\nand the mutual information of two discrete random variables X, Y is\nMutual information of two images measures how much the two variables are related by some function. Presumably the intensities of corresponding anatomical structures in the fixed and moving images should be related via some function, which might be non-linear. Therefore, mutual information should be more resistant to non-linear intensity variation. Indeed, variants of mutual information have been the gold standard among medical image registration metrics [19] , [18] , [14] , [13] . The downside of this family of loss functions is that adapting them in neural networks is not completely trivial. From the definition, computing mutual information reduces to computing entropies. Given a series of independent observations x 1 , . . . , x n to a random variable X, a na\u00efve estimation of H (X) is\nThough Equation (16) systematically underestimates the entropy and various improved algorithms have been developed [36] , [37] , it is still the simplest estimator. However, there is one obstacle keeping it from being used in neural networks, \"binning.\" Traditionally, binning is required for two reasons: that the metric is more robust against random fluctuations in intensities, which will be effaced by binning; and that directly estimating entropy of continuous random variable from unbinned observations is generally more difficult than the binned, discrete alternative. However, the downside of binning is that the operation makes gradient vanish almost everywhere, effectively making the problem of maximizing mutual information a discrete optimization problem, disqualifying gradient descent and other gradient-based optimizers. To tackle the problem, we rewrite Equation (16) as\nwhere z (x) is 1 if x = 0, and 0 otherwise. Now, we are ready to replace z with an approximation, e.g., z \u03bb (x) = e \u2212\u03bb|x| for \u03bb > 0. Furthermore, using this formula boosts the time of computation to the order of number of observations squared. Thus, we need to randomly sample a subset of locations to compute entropy."}, {"section_title": "APPENDIX C NOTES TO IMPLEMENTATION A. TensorFlow Custom Operations", "text": "We choose TensorFlow [28] as our platform. Several operations need to be implemented as custom operations to reduce memory footprint. These include functionalities to warp images, compute moments of random variables, approximate entropy, etc."}, {"section_title": "B. Warp and Domain Shrinking", "text": "Recall that the domain of a flow is the cuboid \u2126 on which the image is defined. Consider the flow f : \u2126 \u2192 R 3 , define its \"valid domain\" as\ni.e., those points that are kept in \u2126 when displaced according to the flow. Ideally the valid domain could be the same as \u2126. However, sometimes part of the moving image is cropped out in the fixed one, or due to algorithm's imperfectness, the valid domain might unavoidably be a proper subset of \u2126, which is called domain shrinking. In our network, an image might need to be warped multiple times. Warping with a domain-shrinking flow field will introduce glitches on the boundaries. This is especially undesirable if the warping is used to compose inverse flows, which makes values on the boundaries nonzero artefacts. To partially resolve the problem, we assume the central part of the composed flow is still valid, hence only compute invertibility loss in the central area."}, {"section_title": "C. Flow Composition", "text": "If we are composing two dense flows, there is nothing much interesting to do other than following the definition. However, if the first flow to be composed is actually an affine transform, alternative formula for composition could speed up computation, avoid interpolation and eliminate further domain shrinking. Simple calculation gives\nwhich does not involve warping at all."}, {"section_title": "D. Orthogonality Loss", "text": "Computing orthogonality loss involves signular values of I + A. The square of these singular values are exactly the eigenvalues of (I + A)\nT (I + A). Since the loss is a symmetric fraction of these eigenvalues, it can be rewritten as a fraction of the characteristic polynomial of (I + A) T (I + A) by Vi\u00e8te's theorem. The formula is cumbersome but not difficult to derive."}, {"section_title": "E. Command Lines for Traditional Methods", "text": "In this section we record the command lines used for traditional methods.\nFor registration (liver and brain) with ANTs [4] , the command we use is the following: For registration (liver and brain) with Elastix [5] , the command we use is the following: elastix -f <Fixed> -m <Moving> -out <OutFileSpec> -p Affine -p BSpline_1000\nFor affine pre-alignment (brain) with ANTs, the command we use is the following: Figure 8 compares the methods listed in Table II except for \"ADDD\" (which is similar to \"ADDD + inv\"), where three landmarks are selected and the sections of the volumes at the height of each landmark in the fixed image are rendered. This means the red crosses (landmarks in the moving and warped images) indicate the projections of the landmarks onto those planes. It should be noted that though the sections of the warped segmentations seem to be less overlapping with those of the fixed one, the Segmentation IoU is computed for the volume and not the sections. It might well be the case that the overlap is not so satisfactory when viewed from those planes yet is better when viewed as a volume. Similarly, overlapping red and yellow crosses do not necessarily imply overlapping fixed and warped landmarks as they might deviate along zaxis."}, {"section_title": "A. Figures for Liver Registration", "text": "In Figure 9 , we compare ADDD + inv, ADDD, ADD, AD and D, four differently cascaded networks, one with an extra loss term. The data prove cascading networks significantly improves the performance, because it better registers images with large displacement. Figure 10 illustrates the intermediate flows produced by ADDD network on two CT liver scans. Each subnetwork registers the images better, increasing Segmentation IoU and lowering Landmark Distance. Figure 11 exemplifies the methods on two brain scans. Comparison between our methods and traditional methods proves the applicability of our methods to 3D brain registration."}, {"section_title": "B. Figures for Brain Registration", "text": "Comparison between ADD and DD shows that integrating affine registration subnetwork is effective. Figure 12 compares unsupervised and supervised VTN. The figure clearly demonstrates that supervision by the groundtruth generated by traditional methods (ANTs here) does not yield better performance that using our unsupervised training method. Moreover, acquiring the ground-truth is timeconsuming, hence not worth the effort.\nIn Figure 13 , there are two dimensions of comparison. Comparing ADD and AD, or D and DD shows that performance is gained by cascading more subnetworks. Comparing ADD and DD , or AD and D shows that integrating affine registration into the method yields better registration accuracy. Figure 11 , except that the fixed image and the moving image are another pair of MR brain scans. Best viewed in color."}]