[{"section_title": "Abstract", "text": "Recently, multi-atlas patch-based label fusion has achieved many successes in medical imaging area. The basic assumption in the current state-of-the-art approaches is that the image patch at the target image point can be represented by a patch dictionary consisting of atlas patches from registered atlas images. Therefore, the label at the target image point can be determined by fusing labels of atlas image patches with similar anatomical structures. However, such assumption on image patch representation does not always hold in label fusion since (1) the image content within the patch may be corrupted due to noise and artifact; and (2) the distribution of morphometric patterns among atlas patches might be unbalanced such that the majority patterns can dominate label fusion result over other minority patterns. The violation of the above basic assumptions could significantly undermine the label fusion accuracy. To overcome these issues, we first consider forming label-specific group for the atlas patches with the same label. Then, we alter the conventional flat and shallow dictionary to a deep multi-layer structure, where the top layer (label-specific dictionaries) consists of groups of representative atlas patches and the subsequent layers (residual dictionaries) hierarchically encode the patchwise residual information in different scales. Thus, the label fusion follows the representation consensus across representative dictionaries. However, the representation of target patch in each group is iteratively optimized by using the representative atlas patches in each label-specific dictionary exclusively to match the principal patterns and also using all residual patterns across groups collaboratively to overcome the issue that some groups might be absent of certain variation patterns presented in the target image patch. Promising segmentation results have been achieved in labeling hippocampus on ADNI dataset, as well as basal ganglia and brainstem structures, compared to other counterpart label fusion methods."}, {"section_title": "Introduction", "text": "Automatically labeling regions of interest (ROIs) is the key step in many imaging-based studies [1] [2] [3] . Manual annotation of anatomical structures is tedious and very time-consuming, which makes it impractical in most of the current medical studies with generally a large amount of imaging data. Therefore, high-throughput and automated segmentation methods are highly desired.\nSince using multiple atlases is more capable of accommodating high structural variability than using single atlas, multi-atlas segmentation has emerged as a popular automated segmentation technique, by propagating the labels from annotated atlas images to the target image. In general, multi-atlas segmentation includes two steps, i.e., (1) Registration step, for aligning the selected atlases as well as their corresponding label images to the target image space [4] [5] [6] , and (2) label fusion step, for fusing the registered label maps of the selected atlases into a consensus segmentation of the target image [1, [7] [8] [9] [10] [11] [12] [13] [14] .\nAmong various proposed label fusion strategies, either voxelwise [1, 15] or patch-wise [9, 16] , they share a common assumption that similar anatomical structures should bear the same anatomical label. For example, the non-local label fusion method [9] computes patch-wise similarity between the target image patch and all possible atlas patches in a search neighbor. Intuitively, high similarity leads to large weight in label fusion. Eventually, the label with the largest weight wins for tagging the target image point. To\nContents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/pr reduce the risk of introducing ambiguous atlas patches, sparsity constraint is recently used to enforce selection of only a very small number of atlas patches for label fusion [17, 18] .\nIt is apparent that each atlas image patch is independently represented by a set of registered atlas patches in label fusion. Although pairwise correlation between any two atlas image patches is explored in [19, 20] , the entire atlas patches are treated as a whole and compete each other to represent the target image patch. Consequently, there are two limitations in the current patch-based label fusion methods. First, image patch could be complex and might include different-scale morphometric patterns and noise. Thus, representing the whole image patches with mixed information inside is challenging. Second, the distribution of atlas image patches is highly complex. For certain groups of atlas image patches tagged with the same label, they might lack of diversity to accurately represent the new instance. In other words, although the related pattern of underlying structure within the image patch might be well matched, the mismatches of unrelated structures can misguide the label fusion procedure.\nTo solve these issues, our solution is to break down the morphometric patterns in atlas image patches into two levels: labelspecific and residual patterns, which are organized into a tree-like dictionary. After that, we propose a hierarchical sparse label fusion method by efficiently representing the target image patch layer by layer in a competition-collaboration manner. Specifically, we consider that atlas image patches with the same anatomical label form a label-specific group. In the top layer, we construct a labelspecific dictionary for each label by using the representative image patches (i.e., cluster centers) in each group. From the second layer, we continue selecting representative image patches from the remaining patches for each group. Then, we build one residual dictionary for each layer by combining information from all groups, where each atom is the residual pattern between the representative image patch in the current layer and its parent patch in the previous layer.\nIn label fusion, we represent the principal part of the target image patch with each label-specific dictionary separately (via sparse constraint), with the remaining part collaboratively represented by the residual dictionaries. In the end, the group with the least representation error wins for tagging the target image point. In this way, only the label-specific dictionaries compete against others to represent the target image patch and vote for the label. Atoms in the residual dictionaries, regardless of being with the same label or not, collaborate to express the residual between the original target image patch and the weighted average of atoms in a certain label-specific dictionary, in order to overcome the issue that some groups might be absent of certain variation patterns presented in the target image patch. It is worth noting that the knowledge of common variations is allowed to transfer from one group to other groups, i.e., the variation patterns in the residual dictionary of each layer is shared across different anatomical groups. Importantly, since we alter the conventional flat and shallow dictionary into a deep tree-like structure, we eventually break down to solve a set of small-scale patch representation problems in each layer. In light of this, our method opts for solving the large-scale sparse patch representation problem in labeling each image point.\nWe have evaluated the performance of segmenting hippocampus from ADNI dataset and both basal ganglia and brainstem structures from other MR images. In all experiments, our hierarchical label fusion method achieves more accurate segmentation results than the conventional non-local [9, 16] and sparse label fusion methods [18] .\nThe remainder of the paper is organized as follows. In Section 2, we present our novel label fusion by deep sparse representation. In Section 3, we evaluate its performance by comparing with conventional patch-based methods, and we provide a brief conclusion in Section 4."}, {"section_title": "Method", "text": "The goal of multi-atlas label fusion is to propagate the labels from a set of registered atlas images to the target image T . Suppose we have N registered atlas images I s ( = \u2026 ) s N 1, , , along with their respective label maps L s ( = \u2026 ) s N 1, , . The conventional label fusion approaches estimate the target label f at each voxel \u2208\u03a9 x of target image T in a patch-wise manner. Denote \u03b1\u20d1 as a P-dimensional (column) vector containing the intensity values in the target image patch centered at voxel x, and matrix\nas a dictionary of M candidate atlas image patches (arranged into column vectors), which consists of all possible atlas patches in a search neighborhood of x. Following the same column order as the\nis a (column) vector of labels at the atlas patch centers, with each element\nindicating either the absence or the presence of a given structure at the center of the respective atlas patch \u03b2 i . For clarity, we only focus on single structure in this paper; however, it is straightforward to extend to multiple structures.\nNext, the label fusion procedure can be regarded as a patch representation problem, seeking for a linear combination of atlas patches \u20d1 Bw which can best fit the target image patch \u03b1\u20d1 . Here, \u20d1 w is a M-dimensional (column) vector, where each element indicates the influence in voting for the latent label f . Sparsity constraint upon the weighing vector \u20d1 w is proven a useful way to improve the label fusion accuracy [17, 18, 20] since encouraging more zero elements in the weighting vector can eventually reduce the risk of introducing misleading atlas patches. Thus, the objective function at each target image point x can be defined by:\nwhere \u03bb is the scalar controlling the sparsity degree. The intuition of encouraging sparsity on \u20d1 w is to suppress the spurious atlas patches in the dictionary B by using only a small number of good atlas patches, instead of all of them. Given the weighing vector \u20d1 w, the label f at each target image point x can be determined by:\nwhere ' \u2022 ' denotes the inner product of two vectors."}, {"section_title": "Limitation of Conventional Flat Dictionary", "text": "It is apparent that all atlas patches in the dictionary B are stacked, column by column, in a single layer. Thus, solving the sparse representation problem in Eq. 1 becomes very difficult if the number of atoms in B is beyond the affordable scale. Another critical issue is that each atom independently competes against other thousands of atoms. Due to possible large variations between target and atlas images and also image noise and artifacts within each patch, it is often too strict to enforce the exact and entire patch matching between two image patches.\nAs the toy example shown in Fig. 1(a) , the target image patch is a blue box face with two ears on the top. Conventional dictionary only has one layer, consisting of two kinds of faces: box faces (#1-#4 in green) with ear(s) on either left or right side, and round faces (#5-#15 in red) with ear(s) on the top. It is clear that the shape of face is the primary pattern and specific to the label (face type) in this toy example. The variations about ears are just the external patterns, which are not related to the task of recognizing faces.\nUnfortunately, none of the sample in the group of box faces has ear (s) on the top, while such variations of ear are abundant in the group of round faces. In the conventional patch representation scenario, it is highly possible to label the target face with round face since too many non-primary variations (the pattern of ears) presented in the dictionary may mislead the representation procedure unless being treated aside from the primary label-specific patterns."}, {"section_title": "Construction of Deep Tree-like Dictionary", "text": "In light of this, we propose to alter the flat dictionary into a deep tree-like structure. Here, we consider forming atlas patches with the same label into a group. Suppose there are R kinds of labels, denoted by \u03be \u03be \u03be \u2026 , , ,. . . ,\n. Thus we can divide the atlas patches\nonly keep the atlas patches with label \u03be r . In the following, we apply the hierarchical k-means [21] to each group G r to divide the atlas patches into several layers, where the variation patterns within the group is hierarchically encoded via the tree from majority to minority. In the beginning, we define the tree has H layers and the branching factors of the tree\n1 (with the last layer consisting of leaf nodes but no children), where b h specifies the number of children of each node at each layer. Then we start hierarchical k-means from the whole group G r . Specifically, we cluster all atlas patches of group G r into k r divisions based on patch appearance, where each division consists of the atlas image patches similar to a particular cluster center. Furthermore, we choose the atlas image patch closest to the cluster center to represent each cluster of group G r . The same procedure is then recursively applied to each cluster, splitting to \u00d7 b b 1 2 nodes in the second layer of tree. We repeat the above clustering procedure until\nleaf nodes are left in the bottom layer, where G r is the total number of image patches in the group G r .\nAfter we obtain the patch tree in each group, the next step is to construct the deep tree-like dictionary layer by layer. For each group G r , we stack top-level representative image patches, column by column, and build the label-specific dictionary D r in the first layer. From the second layer, we construct one residual dictionary E h ( \u2264 \u2264 h H 2 ) for each layer in two steps: (1) for each group G r , we compute the voxel-wise difference between each node (image patch in the current layer) with its parent node (cluster center in the previous layer); and (2) we stack the residual patches from all groups to build the residual-dictionary E h ( \u2264 \u2264 h H 2 ). The residual dictionaries { } E h are equally shared by each label-specific dictionary D r . After that, we can obtain tree-like dictionary TD r for each label as\n. It is worth noting that our deep dictionary allows both completion and collaboration. The label-specific patterns are not shared across different label-specific dictionary D r in the top layer. On the contrary, they compete each other to tag the underlying target image patch. From the second layer, the variation patterns are shared for all D r s since we consider the remaining information (after excluding the principle pattern) is not specific to any label.\nIt is worth noting that each atom in the label-specific dictionary D r represents one of the exemplar atlas patch in group G r . However, all tree-like dictionaries TD r share the same residual dictionaries from E 2 to E H , where each node only encodes the residual information w.r.t. the cluster center. The information in { }\nconveys the variations in different scales (from major to minor variations) as layer h increases. The toy example in Fig. 1(b) shows 2 layers. The two exemplars, one from box face (in green) and another one from round face (in red), form the label-specific dictionary D 1 and D 2 , respectively. The residual dictionary E 2 conveys various patterns of ears and is shared by D 1 and D 2 . Apparently, box face with ear(s) on the top is still box face, and vice versa. Since these variation patterns are not specific to labels (not related to box face or round face), this knowledge can be shared across different groups to represent the target image patch, as detailed next."}, {"section_title": "Hierarchical sparse patch label fusion", "text": "Since we alter the conventional flat dictionary B into a set of deep tree-like dictionary { } \u2026 TD TD , , R 1 , we develop the hierarchical sparse patch representation algorithm to represent target image patch \u03b1\u20d1 via certain dictionary TD r : with label \u03be = f r . The principle behind such hierarchical sparse patch label fusion is that we only allow the label-specific dictionaries D r to compete against each other to vote for the target image point x. Other variation patterns in the residual dictionaries collaborate, by sharing the variation patterns across groups, to fit the remaining part ( )\nAs shown in Fig. 1(b) , after well fitting the variations of ears by E 2 , we can accurately find the target face belongs to box face although the pattern of ear variations do not exist in the box face group. But the conventional sparse representation with flat dictionary B fails since it enforces the very strict whole-part matching such that the discrepancies of ears may mis-label the target face as round face.\nTo solve the minimization problem in Eq. (3), we resort to the Augmented Lagrange Multiplier (ALM) scheme [22, 23] "}, {"section_title": "Discussion", "text": "Tree-Guided Group Lasso [24, 25] (tree-lasso) also organizes the dictionary into the tree-like structure, in order to reflect the correlations among the dictionary atoms. However, our hierarchical sparse representation method has several significant differences from the tree-lasso: (1) Our method uses the residual image patches, instead of the original ones after the first layer, while every node in tree-lasso keeps the original information. (2) In tree-lasso, the children nodes only share the information with their own parent node. However, our method allows the knowledge of variation patterns to transfer across different groups, i.e., each atom in the label-specific dictionary can borrow the variation patterns derived from other groups to avoid the dilemma of mis-representation due to the lack of variation patterns presented in the target image patch. (3) In our method, we adaptively construct the tree by hierarchical k-means. In tree-lasso, the tree is usually manually constructed based on certain priori knowledge.\nNotice that we sequentially solve weighting vectors \u03bc\u20d1 r and\nfrom the roots to the leaf of the tree. Since we use sparsity constraint in each layer, it is highly possible that a large number of weights in each weighting vector are zero. For those atoms with zero weights, we do not include their children in the next layer. In this way, we dynamically form the new dictionary in transition to the next layer and keep solving the verysmall-scale sparse representation problem in each layer. Therefore, the computational cost is comparable to the conventional sparse representation methods that use flat and large dictionary."}, {"section_title": "Experiments", "text": "In the following experiments, we compare our label fusion with the non-local [9, 16] and sparse label fusion methods [18] . To label the target image, we first use FLIRT in FSL package to linearly register all atlas images onto the target image and then use diffeomorphic Demons [26] to compute the remaining local deformations. The main parameters for running diffeomorphic Demons are: 15, 10, and 5 iterations in low, middle, and high resolutions, respectively. The smoothing kernel size is 2.0. The patch size is \u00d7 \u00d7 9 9 9 for all the patch-based label fusion method. To assess label accuracy, the Dice ratio is used to measures the degree of overlap between two ROIs O 1 and O 2 as follows:\nwhere \u22c5 means the volume of the particular ROI."}, {"section_title": "Image Preprocessing and Parameter Setting", "text": "After register all atlases the to-be-segmented target image, histogram matching is performed on each registered atlas image, Table 1 The statistics of Dice ratios, average surface distance, and computation costs in hippocampus labeling by non-local, sparse, and our patch-based methods, with affine registration. (' n ' incidate that our proposed label fusion method achieves significant improvement over the underlying counterpart method under paired ttest with < p 0.05). Table 2 The statistics of Dice ratios, average surface distance, and computation costs in hippocampus labeling by non-local, sparse, and our patch-based methods, with deformable registration. (' n ' incidate that our proposed label fusion method achieves significant improvement over the underlying counterpart method under paired t-test with < p 0.05). in order to normalize the intensity range. The label fusion is independently deployed on each target image voxel. Since we have specifically evaluated the influence of patch size and search neighborhood in our previous work [20, 27] , we fix the patch size to \u00d7 \u00d7 5 5 5voxel in each direction and the search neighborhood is set to \u00d7 \u00d7 mm 5 5 5 3 in linear registration scenario and"}, {"section_title": "Non-Local", "text": "\n3 in non-linear registration scenario, which are found optimal in terms of labeling accuracy and computation time. As we will demonstrate in Fig. 3 , four layers ( = H 4) is optimal in balancing label fusion accuracy and computational cost. Thus, we use patch pre-selection procedure [9] to keep at least 1,000 candidate atlas patches. In building the patch tree by hierarchical k-means , respectively. It is worth noting that we will keep using the same parameter setting in the following experiment."}, {"section_title": "Experimental Result of Hippocampus Labeling", "text": "In this experiment, we randomly select 66 high resolution 3D T1-weithted MR images of elderly brains from ADNI dataset, 1 where the left and right hippocampi have been manually labeled for each brain. Specifically, these MR images were acquired on a 3.0T GE scanner in the sagittal plane using an IR-FSPGR pulse sequence, 8-channel coil, TR \u00bc650 ms, TE \u00bc min full, flip-angle \u00bc 8\u00b0, slice thickness \u00bc1.2 mm, resolution \u00bc256 \u00c2 256 mm, and FOV \u00bc26 cm. We evaluate label fusion performance in a leave-one-out manner. Specifically, we apply the label fusion accuracy by using affine registration and deformable registration separately. Here, we follow patch pre-selection condition in [9] to discard the less-similar image patches. Table 1 shows the mean and standard deviation of Dice ratio and surface distance on hippocampus (left and right combined) by non-local, sparse, and our hierarchical sparse label fusion method, where the atlas images are aligned to the target image by affine registration. Our method achieves the highest Dice ratio and lowest surface distance over other two counterpart methods, where the improvement of Dice ratio is significant. Similarly, as shown in Table 2 , our method beat other two in the deformable registration scenario, where we obtain 1.7% and 1.0% improvements of Dice ratio over non-local and sparse patch-based methods, respectively. The typical surface distance maps in labeling one individual subject are shown in Fig. 2 , where the red and blue denote for low and large surface distance. It is clear that our proposed label fusion method has smallest mean surface distance (Fig. 2(c) ) than non-local ( Fig. 2(a) ) and sparse patch-based label fusion methods ( Fig. 2(b) ). We test all label fusion methods on a workstation with 8 CPU cores (@3.0 G Hz) and 16 G memory. Since the label fusion procedure is independent at each target image point, we use OpenMP 2 to parallel the whole process. The computational cost for 3 label fusion methods by affine and deformable registrations (excluding the registration time) are shown in the bottom rows of Table 1 and Table 2 , respectively. Furthermore, we specifically evaluate the effect of tree layers in our sparse hierarchical label fusion method. Fig. 3 shows the curve of Dice ratio vs computation cost as the layer number H increases from 1 to 6. It is worth noting that only using the top layer ( = H 1) is the degraded version of our method. Without the support from the residual dictionaries, it is not surprising to see that the labeling accuracy is the lowest (84.6%). As H increases, the Dice ratio is improved significantly to 88.3% at = H 4, which indicates that the deep tree-like structure is very useful to deal with the complex patch representation problem in label fusion. Apparently, more layers can continue improving the Dice ratio, but the performance increase is marginal at the expense of computational cost. In all experiments, we find that 4 layers are sufficient to encode the label-specific and individual variations."}, {"section_title": "Experimental Result on Segmenting Basal Ganglia Structures and Brainstem", "text": "The main components in basal ganglia include dorsal striatum Fig. 2 . The surface distance in labeling one individual subject image using Non-local mean (a), sparse (b), and our proposed label fusion methods (c). Blue and red color denote for low and large surface distance respectively. Fig. 3 . The Dice ratio vs computation cost by using different number of layers in our hierarchical sparse label fusion method.\n(caudate and putamen), global pallidus, substantial nigra, and red nucleus. In terms of anatomy, brainstem can be partitioned to midbrain, pons, medulla oblongata and superior cerebellar peduncle. These 9 regions, as shown in Fig. 4 , are closely related with the development of Parkinson's disease. In this experiment, 3T T1-weighted MR images from 11 PD patients are used as the atlases, each of them have the above 9 ROIs manually delineated by two radiologists. The total scan time is expected to be in the range from between 20-30 min. The field of view must include the vertex, cerebellum and pons. The image size is \u00d7 \u00d7 512 512 176 and the image resolution is \u00d7 \u00d7 mm 0.5 0.5 1 3 . We evaluate the label fusion results in a leave-one-out manner. The overall Dice ratios and averaged surface distance in 9 ROIs are shown in Table 3 and Table 4 , respectively. It is clear that our proposed method has achieved the highest label fusion accuracy in all 9 ROIs."}, {"section_title": "Conclusion and future work", "text": "In this paper, we propose a novel hierarchical sparse representation method for multi-atlas patch-based label fusion. The main contribution of our work is that we alter the flat dictionary into a tree-like structure. Specifically, the most representative image patches within each group (label) form the top layer label-specific dictionary and the variation patterns across groups (label) are hierarchically encoded layer after layer. Since the data (candidate image patches) have been hierarchically organized into a major to minor manner, we substantially improve the representation power of sparse representation. In label fusion, we only allow the atoms in the label-specific dictionary delegating the whole group and competing against each other for voting the label. The variation patterns collected from different groups are shared across different groups and collaborate to alleviate the misrepresentation risk due to the lack of certain variation patterns in some groups. We have applied our new label fusion method to hippocampus segmentation and also the parcellation of basal ganglia and brainstem regions. Compared to the counterpart label fusion method, our proposed method has achieved more accurate labeling results."}]