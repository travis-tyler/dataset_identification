[{"section_title": "Abstract", "text": "Abstract. Artificial neural networks (ANNs) have become common data driven tools for modeling complex, nonlinear problems in science and engineering. Many previous applications have relied on gradient-based search techniques, such as the back propagation (BP) algorithm, for ANN training. Such techniques, however, are highly susceptible to premature convergence to local optima and require a trial-and-error process for effective design of ANN architecture and connection weights. This paper investigates the use of evolutionary programming (EP), a robust search technique, and a hybrid EP-BP training algorithm for improved ANN design. Application results indicate that the EP-BP algorithm may limit the drawbacks of using local search algorithms alone and that the hybrid performs better than EP from the perspective of both training accuracy and efficiency. In addition, the resulting ANN is used to replace the hydrologic simulation component of a previously developed multiobjective decision support model for watershed management. Due to the efficiency of the trained ANN with respect to the traditional simulation model, the replacement reduced the overall computational time required to generate preferred watershed management policies by 75%. The reduction is likely to improve the practical utility of the management model from a typical user perspective. Moreover, the results reveal the potential role of properly trained ANNs in addressing computational demands of various problems without sacrificing the accuracy of solutions."}, {"section_title": "Introduction", "text": "Following the demonstration of a mathematically rigorous theoretical framework known as the back propagation (BP) algorithm to machine learning (Rumelhart et al., 1986) , artificial neural networks (ANNs) have become increasingly popu lar for modeling complex, nonlinear problems in science and engineering. Within water resources related disciplines, ANNs have recently been used as data driven models in rainfall-runoff prediction (Gupta et al., 1997; Tokar and Johnson, 1999) , stream flow forecasting (Muttiah et al., 1997) , ground water simulation (Ranjithan et al., 1993; Yang et al., 1997) , water quality modeling (Maier and Dandy, 1996; Rogers and Dowla, 1994) , water demand forecasting (Jain et al., 2001) , reservoir operations (Cancelliere et al., 2002) and other applications (ASCE, 2000b) . The majority of these applications have relied on local search techniques, namely the BP algorithm, for ANN training (ASCE, 2000a) . Like other gradient-based algorithms, however, BP often suffers from premature convergence to local optima. Particu larly for complex problems, the success of training with BP depends on whether the modeler has sufficient knowledge of ANNs and of the problem at hand so as to design a compact and effective ANN. Yet such knowledge is often limited or unavailable for non-ANN experts facing realistic problems (Yao and Liu, 1998) . As a consequence, a trial-and-error procedure is often applied to determine the best performing, and yet simple and compact, ANN architecture (ASCE, 2000a) .\nThis study investigates the use of evolutionary programming (EP), a technique that belongs to a class of increasingly popular and robust search algorithms known as evolutionary algorithms (EAs), to reduce the shortcomings of gradient-based ANN training algorithms. However, similar to any other EA, if used independently, EP could be inefficient and ineffective in fine-tuning local searches for large prob lems. This lack of efficiency could be significantly improved by incorporating a local search procedure, such as BP, into the solution evolution. Specifically, this methodology would involve hybridizing EP's robust search ability with the finetuning ability of a gradient-based algorithm. Thus, EP could be used to determine the best region of the solution space, represented by ANN architecture and an initial weight surface, whereas BP could be used to determine optimal or near-optimal synaptic weights within this region.\nThe objective of this study is to compare the singular performance of EP with that of a hybrid EP-BP algorithm for training ANNs. BP is not included in the investigation since, if used independently, the algorithm ultimately involves highly subjective decisions and a trial-and-error approach for ANN construction, which are some of the same characteristics this study aims to reduce. Based on the compar ison, the ANN associated with the superior training algorithm is subsequently used as a replacement for the simulation component of a previously developed multiob jective decision support model for watershed management. This unique application satisfies a secondary objective of demonstrating the efficiency of an ANN in com parison to traditional simulation tools when used within a decision support model."}, {"section_title": "Background Information", "text": ""}, {"section_title": "MOTIVATION BASED ON RELATED RESEARCH", "text": "Human interferences with the natural environment in the form of mechanized agri culture, large-scale construction, deforestation, and overgrazing have tremendously increased rates of erosion and sedimentation from watersheds. Large percentages of sediment from such mistreated watersheds ultimately enter water bodies, causing environmental, economical, and social impacts. To assist in the assessment of the environmental impacts of erosion in large river basins, the U.S. Department of Agri culture (USDA) has developed a distributed hydrologic model known as the Soil and Water Assessment Tool (SWAT) (Arnold et al., 1998; ASCE, 1999) . The model operates on a daily time scale and on the spatial scale of a hydrologic response unit (HRU). Watersheds are typically subdivided into natural subbasins, which can be further divided to form HRUs depending on land-use and soil heterogeneity within the basin. SWAT can be used to simulate hydrologic processes such as surface runoff, percolation, lateral subsurface flow, ground water flow, potential evapotran spiration, snow melt, transmission losses, and sediment yield using simple and yet realistic techniques. Furthermore, it is capable of modeling crop growth and sub sequent crop yield while accounting for stresses on plants due to water shortages and inadequate fertilizer. In addition to these capabilities, SWAT is interfaced with c an ArcView \u00a9 Geographic Information System (GIS), thus simplifying processes of data extraction from a digital elevation model (DEM), digital land-use maps and digital soil maps. As a result, simulation models such as SWAT are generally sufficient for estimating the impacts of erosion in response to a particular land-use policy or activity. Like most hydrologic models, however, SWAT does not allow for an assessment of the economical or social impacts associated with such a policy. Moreover, by themselves, such models are incapable of directly identifying the best policy among various alternatives to reduce this anthropogenic threat and its adverse impacts.\nA better approach to address problems associated with erosion is the integra tive and systematic planning and management of watershed activities. Integral to the success of this approach is a comprehensive simulation technique to solve the cause-effect relationships that influence erosion and sedimentation, thereby re vealing the implication of management decisions on water quality and on aquatic ecology; a socioeconomic model to evaluate economic and social consequences of decisions on land owners in the watershed; and a systems approach that searches for the best decision among the many possible alternatives. Following this integra tive philosophy, the authors have developed a watershed decision support model designed to aid in reducing the impacts of erosion while considering social and economic dynamics of the watershed (Muleta and Nicklow, 2001, 2002; Nicklow and Muleta, 2001; Muleta, 2003) . Their model was based on coupling SWAT with an EA-based, multiobjective search method known as the Strength Pareto Evolu tionary Algorithm (SPEA). The SWAT-SPEA model was designed to search for optimal or near-optimal watershed landscapes, defined as the combination of land uses and farm management practices (i.e., decision variable) on the spatial scale of a farm field that simultaneously minimize sediment yield and maximize net agricul tural profit over a specified time horizon. The authors demonstrated the capabilities of the model using Big Creek watershed, a 130-km 2 drainage basin located in southern Illinois. Though their model was capable of solving this multiobjective problem, it was computationally intensive, requiring over 2.5 days of CPU time to \ufffd \ufffd identify preferred landscapes. This computational demand was primarily the result of required repeated application of SWAT for evaluating sediment and crop yield for each of the numerous landscapes identified in the search process. Motivated by the potential impact of large computational times, namely the reduced practical utility of this multiobjective watershed decision support tool, the authors are now focused on the integration of an ANN within the model. Specifically, they aim to investigate the use of an efficient ANN training algorithm and explore the suitability of the resulting data driven model as a replacement for SWAT in efforts to reduce overall computational time."}, {"section_title": "THEORETICAL FRAMEWORK OF ANNs", "text": "The multilayer feed forward network (FFNN), depicted in Figure 1 , is a particular type of ANN that is used in this study. For brief illustration of a FFNN, consider node i on Figure 1 , information from all four inputs (i.e., x k, for k = 1, . . . ,4) is passed to this node through the connection links. The strength of this information transfer is measured by connection weights (i.e., w ki, for k = 1, . . . ,4), and the output signal from the node, y i , is obtained by evaluating the value of an activation function, f , given as\nwhere b i is a nodal bias, or threshold value, which must be exceeded for the node to be activated. A commonly used activation function throughout neural network literature is the sigmoid function, which is a bounded, monotonic, strictly increasing and differentiable function expressed as where l represents the summation result from Equation (1). Sequentially, similar operations are applied to all nodes of the current and future layers until estimates are obtained from the output layer.\nAnalogous to the human brain, ANNs learn the system they model from exam ples presented to them. For a predefined FFNN architecture (i.e., number of hidden layers and number of nodes on each hidden layer), a training, or calibration, pro cess is applied to determine weight matrices, W , and bias vectors, B, that minimize the difference between the predicted output vector, Y = (y 1 , . . . ,y m ), and desired output vector, D = (d 1 , . . . ,d m ). The suitability of parameters is evaluated using a network error function, such as the sum of squared errors between the predicted and desired output vector, or\nwhere N is the number of training examples (i.e., input-output sets) provided for the learning scheme and m is the number of output nodes. Through training, it is hoped that the network learns, or generalizes, the nonlinear relationships that map inputs to outputs so that it can make reasonable estimates for an environment to which it was not exposed during the training process."}, {"section_title": "TRAINING WITH BACK PROPAGATION", "text": "BP is a gradient descent technique that can be used in efforts to minimize the network error function. When applied to ANN training, BP consists of two passes through the different layers of the network, a forward pass and a backward pass. In the forward pass, an input vector is applied to the sensory nodes of the network, and its effect propagates through the network according to the techniques described earlier.\nFinally, a set of outputs is produced and the value of the network error function is determined. During this stage of training, the synaptic weights are all fixed. In the backward pass, the error is propagated backward through the network against the direction of synaptic connections in such a way that it moves the actual response of the network closer to the desired response. During this backward pass, the synaptic weights are adjusted according to:\nwhere\nand w i j and w are the weights between node i and j during the mth and m-1th i j passes, respectively; V is the error function given in (3) and is implicitly dependent on the weights through (1); and \u03bc and \u03b1, both of which could take values ranging from 0 to 1 only, are the learning rate and momentum factor, respectively. The learning rate helps accelerate training in very flat regions of the error surface and helps prevent oscillations in connection weights. The momentum factor is used to reduce the probability of convergence to local minima. It should be noted, however, that there is still a high possibility for premature convergence to a non-global optima, in spite of the momentum factor. This statement is based on the fact that, as with any gradient-based technique, the quality of solutions obtained heavily depends on initially drawn solutions. Consequently, using BP for training involves a tedious and time-consuming trial-and-error process in which the effects of various initial weights and ANN architectures must be investigated. For further details regarding the BP algorithm, the reader is referred to Haykin (1999) ."}, {"section_title": "TRAINING WITH EVOLUTIONARY PROGRAMMING", "text": "A possible way to limit the drawbacks of gradient-based training techniques and improve the search for optimal or near-optimal solutions involves the adoption of a robust search algorithm. EAs refer to a class of population-based, stochas tic search algorithms that are developed from the principles of natural selection. They include genetic algorithms (GAs), EP and evolutionary strategies (ES), ge netic programming, and artificial life algorithms. EAs can generally handle large, complex, non-differentiable and multimodal spaces without requiring gradient in formation, making them a suitable candidate for evolving ANN connection weights and architecture. Although many of the features of various EAs are similar, EP and ES are dis tinctly different from GAs in that they rely on mutation as a primary search opera tor. In contrast, GAs use crossover, or mating of alternative solutions (i.e., chromo somes), as a basic operator. The use of mutation-based EAs has recently been found to be superior to the use of crossover-based EAs for training ANNs (Yao, 1999) . The reasoning associated with this finding involves the conception that crossover works best when building blocks (i.e., well-defined, low-order, and highly fit schema) ex ist. However, it is unclear what a building block might be in an ANN because they emphasize the distribution of knowledge among all of the weights. Recombining one part of an ANN with that of another ANN is therefore likely to destroy both parts (Yao, 1999) . The use of mutation-based EAs, however, can reduce the disrup tive features of the crossover operator's recombination process. As a result, EP is becoming more popular for training ANNs (Yao and Liu, 1997) .\nIn EP, it is assumed that whatever genetic information transformations occur, the resulting change in each behavioral trait will follow a Gaussian distribution with a zero mean and standard deviation of unity (Fogel, 1994) . When applied to real-valued function optimization, the EP methodology is implemented as follows:\n1. An initial population of N individuals is generated at random from uniformly distributed numbers. Each individual is a pair of real-valued vectors, ( p i j , s i j ), for i = 1, . . . ,n and j = 1, . . . ,N , for all n parameters of N alternatives.\nHere, p i j represents decision variable vectors to be optimized and s i j represents self-adaptive variance vectors for Gaussian mutations.\n; ;"}, {"section_title": "Each individual (", "text": "; ;\nwhere N (0,1) denotes a normally distributed, one-dimensional, random number with a mean of zero and variance of one. N j (0,1) indicates that the random number is newly generated for each value of j. The parameters \u03c4 and \u03c4 ; are \u221a \u221a \u221a commonly set to ( 2 n) \u22121 and ( 2n) \u22121 , respectively. 3. Determine the fitness of each individual, including both parents and offspring, based on the fitness measure (e.g., objective function to be optimized For additional details regarding EP, the reader is referred to Fogel (1999) and Porto (2000) .\nIt should be noted that although EP is often useful in a robust evaluation of the best region of a solution space, it may be inefficient and ineffective in fine-tuning the local search within that region. The impact of this inability to fine-tune could possibly be limited by integrating a gradient-based algorithm in the late stages of training, thus taking advantage of one algorithm's strength to compensate for the other's weaknesses."}, {"section_title": "Training Data", "text": "As the ANN developed herein is designed to reproduce SWAT estimates of sedi ment yield and agricultural profit, the data used for training must be generated us ing the same methodologies, constraints and assumptions implemented within the SWAT-based decision support model. Within this watershed management model, decision variables are represented as cropping and tillage practice combinations for a particular HRU. It is assumed that each HRU represents a particular farm field that is singularly or commonly owned, thus implying that a landowner's de cisions regarding their own property will have no influence on decisions made by neighboring landowners. This formulation allows each landowner within the water shed to make independent decisions, yet all decisions contribute toward the overall goal of reducing sediment yield from their watershed. This approach supports the Illinois Environmental Protection Agency's (ILEPA's) recognition that watershed planning and management begins with the shared responsibility of farmers and other landowners who have ownership rights within the watershed.\nFarm management decisions are also made under consideration of multi-year criteria, such as crop rotation, rather than strictly single-year concerns. Accordingly, it is assumed here that a decision policy dictates the seasonal sequence of crops to be grown on an individual farm field for a 3-year period. Furthermore, in the search process, only field crops are considered, and a maximum of two crops per year are permitted to grow. A crop year commences in January, and the second crop of the year can be planted only after the preceding crop is harvested. Therefore, within a 3-year rotation, a maximum of five crops can be grown. The first crop planted in the 3-year period is a warm season crop and is harvested in late September. A winter crop is then planted in early October and is harvested in June. Next, using a double cropping system, warm season crops, such as soybean, that can grow following harvest of winter crops are planted. The fourth crop is a warm season crop that is planted in March or April, and finally the fifth and the last crop of the sequence is a winter crop. In addition, once planted, perennial crops such as hay and pasture are allowed to remain in the field until the end of the three-year plan. These criteria together represent crop management constraints. Satisfaction of these constraints is checked for each sequence of decision variables through use of systematically assigned crop codes (see Table I ). For additional detail regarding formulation of the watershed management problem, the reader is referred to Muleta and Nicklow (2002) .\nTo be consistent with the SWAT-based decision support model, the ANN must be trained on a spatial scale of an HRU (i.e., farm field). Thus, for each HRU, a number of five-season sequences of crop types and management practice combi nations are randomly generated according to the previously described assumptions and constraints and are used as inputs for the ANN. A typical example of these five land use and management practice sequences (i.e. inputs of the ANN) may be CRNT, WWNT, SYWC, WWFT, SRST, with their associated codes (Table I) sequence were estimated by SWAT, which in turn represent the two desired outputs in the training process. Specifically, 500 potential decision policies (i.e., inputs) were generated for each HRU and their corresponding average annual sediment yields and average annual net profits (i.e., outputs) were estimated. Of the 500, 300 input-output sets were used as training data for determining optimal or nearoptimal connection weights and ANN architecture that would bring ANN estimates sufficiently close to the desired outputs. However, a cross-training procedure is usu ally recommended to limit the potential for overtraining, or overfitting. Overfitting occurs when the ANN starts to memorize the individual training examples rather than generalize the trends within the entire dataset. The objective in cross-training is to stop the learning process when the network starts to overfit. To do so, the network is allowed to determine values of the network error function using cross-training datasets, rather than training data, after one complete presentation of training data, or epoch. During the early stages of the search, errors for both the training and the cross-training dataset decrease. After a number of search iterations, however, the training data error may continue to be reduced while the cross-training data starts to increase, which is indicative of overfitting. This is a suitable point to stop training and consider the current weights and architectures as final solutions. One-hundred datasets that were different from the alternatives used during the training process were preserved for cross-training. In addition, performance of an ANN can be best evaluated by subjecting the trained ANN to new patterns that it has not seen during training or cross-training, a process known as verification. Another 100 datasets that were different from policies used in the training and cross-training activity were used to verify the trained ANNs. Haykin (1999) describes the advantages of input normalization, or standardiza tion, and recommends that it can be undertaken to accelerate the learning process, especially if BP is used in training. According to LeCun (1993) this can be achieved if each input variable is preprocessed so that its mean value, over the entire training set, is close to zero or is small compared to its standard deviation. In addition, it is important that values of the desired outputs of the system be standardized so that they lie within the range of the activation function used in training. There should be an offset by some amount away from the limiting values which otherwise tend to drive the parameters such as synaptic weights and bias to large values, and thereby slow the learning process. In this respect, the hidden nodes are essentially driven into saturation (Haykin, 1999) . Accordingly, for each input to be used in training, cross-training and verification in this study, the mean of the 500 input sets was eval uated and subtracted. The resulting inputs were further standardized in such a way that all inputs lie within a range of \u00b1 0.95. Also, because the activation function used in training is the sigmoid function, which is bounded between 0 and 1, the output datasets were standardized so that they lie within the range of 0.05 to 0.95, allowing an offset of 0.05 from both extremes. Once the data was generated and preprocessed, training was conducted, first using EP alone, and then by hybridizing the EP and BP algorithms."}, {"section_title": "Methodology", "text": ""}, {"section_title": "STAGE I-EP TRAINING", "text": "The logistical framework of the complete training process is depicted in Figure 2 . For application of EP, the authors adopted a population size of 1000 solutions, a maximum of 100 generations, a maximum of six hidden layer and a minimum of one hidden layer, a maximum of 15 nodes for each hidden layer and a minimum of one node, and a maximum and minimum weight of 2 and \u22122, respectively. These values are established on the basis of complexity of the problem and are guided by limits used and recommended in the literature. To begin, an initial pop ulation that consists of random architecture (i.e., random number of hidden layers and random number of nodes on each hidden layer), random weights, bias values, the EP strategic parameters, are generated. Hidden layer assignment is equitably distributed among the initial population. For example, if the maximum number of hidden layers is five and the minimum is one, and if the size of the initial popu lation is 1000, there will be 200 alternatives from each of the possible number of hidden layers. Numbers of nodes for each hidden layer, connection weights and EP strategic parameters within nodes of successive layers are randomly generated from uniformly distributed values. Following the EP methodology, each alternative solution is permitted to yield offspring by evolving its weight and self-adaptive mutation parameter according to Equation (6). For every parent and offspring, and after using the entire training dataset in an epoch, the network error function is eval uated. Tournament selection is then applied to choose solutions that become part of the next generation. The concept of elitism is also applied, which insures against losing the best performing alternative from previous generations. For the best solu tion of every generation, the error function is evaluated for overfitting tendencies by using the cross-training dataset. If error from the cross-training data increases for five subsequent generations while error from training dataset decreases, the search procedure is stopped and the final architecture and weights are those corresponding to the iteration immediately before that in which cross-training error began to grow. Finally, for the final solution obtained, the verification dataset is applied and the generalization ability of the model to a new environment is tested."}, {"section_title": "STAGE II-BP TRAINING", "text": "Unless the performance of the EP training is fully satisfactory, for relaxed con straints on weight limits, the BP algorithm is subsequently applied. The weight vectors obtained from EP are used as the initial weight surface from where the BP algorithm commences the fine-tuning process. Similar to EP, training, crosstraining and verification are essential to the application of the algorithm. For BP, there are two common modes of training, a sequential mode and a batch mode. In sequential training, weights are updated after the presentation of each training ex ample; whereas in the batch mode of training, they are updated only after all training datasets in an epoch have been presented. For large, difficult problems that have highly redundant training datasets, such as the watershed management problem, the sequential mode of BP learning is computationally faster than the batch mode since it takes advantage of the redundancy as the examples are presented. In addition, randomization of the order in which the training examples are presented from one epoch to the next makes the search region stochastic. This stochastic characteristic in turn makes BP less likely to be trapped in local optima (Haykin, 1999) . There fore, sequential training and a randomized presentation order are adopted in this study.\nIn application, the forward pass of BP algorithm is performed until outputs are estimated. Network error is then evaluated according to Equation (3), except that N now assumes a value of unity for the sequential mode of training. This error is propagated backward through the layers of the network by refining weights ac cording to Equation (4), and the forward-backward evaluation is repeated for the next training dataset. Following each epoch, the order of training data presenta tion is randomized, and the cross-training data is applied and corresponding error computed. Similar to EP, if the cross-training errors continue to increase for five subsequent iterations while the training error decreases, the search is halted and the final weight vectors are those corresponding to the iteration immediately before that in which cross-training error began to increase.\nTo accelerate convergence of BP, Haykin (1999) recommends using a different learning-rate for every adjustable network parameter (i.e., weights and bias values) and altering this rate from one iteration to the next. This suggestion has been implemented by assigning different learning rates for all connection weights and by updating those weights at a linearly decreasing rate from iteration to iteration. Specifically, weights are updated according to\nare learning rates between node i and node j at the initial iteration, final (Mth) iteration, iteration m, and iteration m + 1, respectively. Fur thermore, because layers near the output layer generally have larger gradients than those at the front end of the network, smaller learning rate values are assigned to end layers (i.e.,\u03bc i j 1 and \u03bc i j M ) so that all nodes in the network learn at similar rates. The momentum factor was, however, set to a constant of 0.2.\nWhen the convergence criterion, defined here as a maximum of 1000 iterations, is satisfied, resulting weights are applied to the verification dataset and performance of the model is tested. If its performance is not satisfactory, the search will restart by assigning a new learning rate (\u03bc i j 1 ), and the BP search will be repeated for that particular HRU until the convergence criterion is satisfied. After some prelimi nary testing, a maximum of five such repetitions are allowed in this study. After five attempts, the best result among the original and five additional attempts is accepted."}, {"section_title": "PERFORMANCE CRITERIA", "text": "There are various methods available for evaluating model performance, including graphical and numerical indicators. In this study, the authors use a scatter plot of simulated and desired outputs for calibration and verification of datasets. In addition, two numerical measures are used, namely the root mean square error (RMSE) and the Nash and Sutcliffe (1970) R 2 efficiency. These metrics can be expressed as\nHere, N is the total number of datasets, training or verification; d i is a desired output (i.e., SWAT estimate) for the ith dataset; y i is the ANN output; and \u0233 is the mean value of the desired output for the training data. An ideal value of RMSE is zero, in which case the R 2 efficiency index assumes a value of unity."}, {"section_title": "Results and Discussion", "text": "Big Creek watershed, located in the Cache River basin of southern Illinois and shown in Figure 3 , is used to test the hybrid training algorithm and evaluate the suitability of the resulting ANN as a replacement for SWAT in the watershed de cision support model. Because of its high sediment yield and influence on the Lower Cache River, multiple government agencies and private organizations have identified the Big Creek watershed as a priority area for targeted remediation. The area is undergoing extensive study as part of the Illinois' Pilot Watershed Program, through cooperation among the Illinois Department of Natural Resources (IDNR), the Illinois Department of Agriculture, ILEPA, and the U.S. Natural Resources Conservation Service (IDNR, 1998) . A 30-meter resolution U.S. Geological Survey (USGS) DEM, an IDNR landuse map, and a soils map were obtained for the region of study. The Big Creek watershed was delineated from the DEM and subdivided into 73 subbasins, and the land use and soils maps were superimposed over the subdivided watershed to identify HRUs. For this application, dominant soils types and land uses from each subbasin were used in establishing HRUs, a statement that implies that each farm field consists of a single soil type and land cover during any one season and that there are the same number of HRUs as subbasins. Observed data related to daily precipitation, daily maximum temperature and daily minimum temperature were obtained from the National Weather Service for Anna, IL, a nearby weather station.\nA database of 19 suitable cropping and tillage practice combinations was pre pared for the application. This database contains miscellaneous information on planting dates, harvesting dates, dates to apply tillage, fertilizer and pesticide types, application dates and dosages, heat units required for plant maturity, and runoff curve numbers. Information for the management database was collected from the Illinois Agronomy Handbook (UIUC, 2000) and from the National Agri cultural Statistics Service (USDA, 2000) . Additionally, an economic database was prepared that provides information on production expenses and selling prices for associated crop types. The production expenses were broadly classified as variable costs and fixed costs. Variable costs include expenses for seed, chemical, insurance and interest for machinery, labor and trucking. Fixed costs are related to the cost of owning land and machinery and were not used in the search process. Ten-year (i.e., 1990 Ten-year (i.e., -1999 averages of production expenses and selling price data for the study area were collected from various sources and were subsequently used in estimating the net economic benefit of implementing a potential decision policy. The major sources used in preparing the economic database were the University of Illinois at Urbana-Champaign (UIUC) Farm and Resource Management Laboratory (FaRM Lab) (UIUC, 1999), the Illinois Census of Agriculture (USDA, 1997a), and the Cost and Returns Estimator model (CARE) farm budget for Southern Illinois (USDA, 1997b) .\nUsing the operational management and economic databases, along with model inputs, SWAT was used to generate datasets for training, cross-training and verifica tion for the 73 HRUs. These datasets were based on randomly generated decisions, or combinations of land uses and management operations. The model inputs and outputs were then standardized, and datasets used for the cross-training and verifi cation were checked for uniqueness so that the generalization ability of the trained ANN to a new environment could be tested."}, {"section_title": "PERFORMANCE OF INDEPENDENT EP TRAINING", "text": "In assessing the performance of EP alone, the limiting values on synaptic connec tions and strategy parameters were relaxed to \u00b1 20 and \u00b1 10, respectively. The search was allowed to continue for 500 generations, with an initial population of 1000 solutions. For a typical HRU, a graphical comparison of desired outputs, pro vided by SWAT, and ANN outputs (also referred to as estimated outputs) are given in Figure 4a and Figure 4b for sediment yield and net profit, respectively. Similar plots are provided in Figure 5a and 5b for the verification data. Performance was also evaluated for both training and verification using the R 2 efficiency and RMSE; rather than reporting these indices for all HRUs, however, summarized statistics are presented in Table II . This summary table provides the worst, best, mean and, standard deviation of indices from all HRUs for both training and verification. It should be noted that, the extreme (i.e., worst and best) values do not necessarily correspond to a single HRU. The worst R 2 value for sediment yield in the training category may be for one HRU, while the same parameter for net profit may be for another HRU.\nA review of these results reveals that the EP-based training algorithm lacks a capability to fine-tune the search, even though it has a good generalization tendency. Its search result is not sufficient, especially for the verification data, where it yielded R 2 efficiencies as high as \u22120.8721. The negative R 2 efficiencies indicates that the model being tested is yielding verification outputs that are worse estimates than the mean of the outputs used in training. For this particular HRU, the search did not improve after approximately 175 generations, as shown in the error convergence plot for training and cross-training in Figure 6 . Furthermore, the computational demand required for this application makes the independent performance EP even less tolerable. The operation required about 42.2 hrs on a 1.3 GHz, Pentium IV processor. Therefore, even though the literature indicates that EP is generally computationally faster than a GA (Yao, 1999) , it was not sufficiently fast for the problem considered in this study."}, {"section_title": "HYBRID EP-BP PERFORMANCE", "text": "Dissatisfied by the capability of EP to adequately train the ANN, the performance of the hybrid EP-BP algorithm was subsequently evaluated. Once EP located a Figure 6 . Error convergence plot for training and cross-training data using the EP algorithm.\nnear-optimal ANN architecture and starting values of connection weights, the BP algorithm was used to fine-tune the search. Weights and Gaussian mutation terms for the EP algorithm were limited in a way that they do not affect the BP algorithm's performance. For example, the limits on weights were assigned primarily on the ba sis of the fact that the final weights obtained by EP are those that would be supplied as initial values for BP, which is often recommended to be in the range of \u00b11. Other wise, the BP algorithm may suffer from saturation of hidden layers and subsequent slowing of the search process (Haykin, 1999) . The number of search iterations for EP was also reduced to 100 to minimize computational demand. Search results for the hybrid algorithm are presented both graphically and numerically. Figure 7a shows comparison plots of desired and estimated outputs for sediment yield as evaluated using the training data, whereas Figure 7b shows a similar plot for net profit. Figures 8a and 8b present similar graphical measures for verification data. A statistical summary of R 2 and RMSE indices over all HRUs of the watershed is given in Table III , and an error convergence plot for the training and cross-training datasets is given in Figure 9 .\nAmong all HRU's, the largest number of hidden layers identified by EP as the favored architecture was two, with nine nodes on layer one and two nodes on the second hidden layer, thus representing a compact architecture. As indicated in both graphical and numerical results, the hybrid algorithm has significantly enhanced the overall search capability. Unlike the independent EP performance where R 2 efficiency as low as \u22120.8721 was obtained for verification of the net benefit, the worst R 2 efficiency obtained using the hybrid algorithm was 0.3518. Moreover, the average R 2 and RMSE are promising and low standard deviations are obtained, thus demonstrating the robustness of the hybrid algorithm. However, a brief comparison of Figure 6 and Figure 9 reveals that, for this particular HRU, the sum of square of errors obtained by the EP algorithm is lower than that obtained by the EP-BP algorithm; whereas a comparison of Figures 4, 5, 7, and 8, which are also obtained Figure 9 . Error Convergence plot for training and cross-training data using the BP-EP algorithm.\nfor the same HRU, proves otherwise. This raises an interesting question regarding suitability of using single criteria such as sum of square of errors, as a goodnessof-fit for models."}, {"section_title": "EFFICIENCY OF THE ANN-BASED DECISION SUPPORT MODEL", "text": "Impressed by performance of the EP-BP algorithm, the trained ANN was used as a replacement for SWAT in the multiobjectve watershed management model (Muleta and Nicklow, 2002) . For the Big Creek watershed, using a population of 100 solu tions and a maximum of 100 generations, the SWAT-based decision support model required 63.25 hrs of computational time on a 1.3 GHz, Pentium IV processor. The ANN-based decision support model, however, finished its execution in just 4.5 min, representing an extraordinary improvement. The solutions derived by the ANN-based and the SWAT-based multiobjective models are the same for 90% of the HRUs in the watershed, witnessing the capability of the developed ANN to replace SWAT. In examining execution times for the entire process including data generation (6.3 hrs), training (9.6 hrs), and the actual search process (4.5 min), the ANN-based model required just 16 hrs to locate optimal or near-optimal land use and management patterns. The latter represents approximately a 75% reduction in computational time when compared to the SWAT-based model. It should be noted that the data generation and training processes need only be performed once for the same watershed and simulation period, assuming other environmental variables are not changed. Therefore, following the initial execution, repeated searches could be performed in a matter of minutes. Considering the average user, this reduction in computational time could potentially improve the practical utility of the decision support model. After all, one of the many criteria often used by those in practice to select a particular model, whether in water resources engineering or other disci plines, is the feasibility of computational time required for execution. In addition, the results can be further generalized to studies that target performance comparisons between various search algorithms and search operators."}, {"section_title": "Conclusions", "text": "The EP-BP hybrid training algorithm adopted in this study is effective for calibrat ing an ANN to the highly nonlinear and complex processes of watershed erosion and sedimentation as a function of land use and management combinations. The sediment yields and the net economic benefits generated by the trained ANN and those generated by SWAT model, as a result of implementing a sequence of the land use and management practices given in Table I over span of five cropping seasons, are in excellent agreement (Table III, and Figures 7 and 8) . The hybrid algorithm limits some of the common drawbacks inherent to BP and other gradientbased algorithms. These include a heavy dependence on the skills of the modeler, convergence to local optima, and the typical trial-and-error procedure required for designing compact, effective ANN architectures. In addition, as demonstrated herein, the hybrid outperforms the independent application of EP for training. The EP-BP algorithm could be useful for solving a variety of complex problems apart from development of ANN-based hydrologic simulation models. The replacement of SWAT by an ANN within the watershed decision support model has resulted in a significant reduction in computational time. The resulting impact may be the improved practical utility of the overall model (Muleta and Nicklow, 2002) for solving erosion and sedimentation problems. Moreover, this study represents an example of the potential role of ANNs in addressing com putational demands of various problems without sacrificing accuracy of rigorous models. For example, the use of optimization-simulation tools that are based on theoretically justified techniques, such as distributed hydrologic models, are often impractical due to their large computational times, an issue that plagues many engineering systems applications. The application of ANNs, along with effective and efficient training algorithms such as the EP-BP hybrid algorithm, can poten tially alleviate this problem by providing quick and reasonable estimates of the theoretically inspired models."}]