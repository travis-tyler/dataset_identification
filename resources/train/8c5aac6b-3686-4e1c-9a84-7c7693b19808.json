[{"section_title": "Abstract", "text": "For large-scale testing with graph-associated data, we present an empirical Bayes mixture technique to score local false discovery rates. Compared to empirical Bayes procedures that ignore the graph, the proposed method gains power in settings where non-null cases form connected subgraphs, and it does so by regularizing parameter contrasts between testing units. Simulations also show that GraphMM controls the false discovery rate in a variety of settings. We apply GraphMM to magnetic resonance imaging data from a study of brain changes associated with the onset of Alzheimer's disease."}, {"section_title": "Introduction", "text": "The development of empirical Bayesian methods for large-scale hypothesis testing addresses important practical challenges to do with identifying distributional changes while controlling the false discovery rate. Often, these methods are applied relatively late in the data-analysis pipeline, after p-values, test statistics, or other summary statistics are computed for each testing unit. Essentially, the analyst performs univariate testing en masse, with the final unit-specific scores and discoveries dependent upon the choice of empirical Bayesian methodology. Empirical Bayes tools account for the collective properties of the univariate statistics and thereby gain their advantage (e.g., Storey (2003) , Efron (2010) , Stephens (2017) ). These methods may be underpowered in some applied problems when the underlying effects are relatively weak.\nMotivated by tasks in neuroscience and brain imaging, we describe an empirical-Bayesian approach that operates earlier in the data-analysis pipeline and that leverages regularities achieved through constraining the dimension of the parameter space. Our approach is restricted to data sets in which the variables constitute nodes of a known, undirected graph, which we use to guide regularization. We report simulation and empirical studies with structural magnetic resonance imaging to demonstrate the striking operating characteristics of the new methodology. We conjecture that power is gained for graph-associated data by moving upstream in the data reduction process and by recognizing low complexity parameter states.\nThe following toy problem illustrates in a highly simplified setting the phenomenon we leverage for improved power. Suppose we have two sampling conditions, and two variables measured each condition, say X 1 and X 2 in the first condition and Y 1 and Y 2 in the second. We aim to test the null hypothesis that X 1 and Y 1 have the same expected value; say H 0 : \u00b5 X 1 = \u00b5 Y 1 . Conditional upon target values \u00b5 X 1 , \u00b5 Y 1 and nuisance mean values \u00b5 X 2 and \u00b5 Y 2 , the four observations are mutually independent, with normal distributions and some constant, known variance \u03c3 2 . We further imagine that these four variables are part of a larger system, throughout which the distinct expected values themselves fluctuate, say according to a standard normal distribution. Within this structure, a test of H 0 may be based upon the local false discovery rate\nwhere we are mixing discretely over null (with probablity p 0 ) and non-null cases. Notice in this setting the across-system variation in expected values may be handled analytically and integrated out; thus in this predictive distribution g is the density of a mean 0 normal distribution with variance 1 + \u03c3 2 ; and f is the bivariate normal density with margins g and with correlation 1/(1 + \u03c3 2 ) between X 1 and Y 1 . In considering data X 2 and Y 2 on the second variable, it may be useful to suppose that the expected values here are no different from their counterparts on the first variable. We say the variables are blocked if both \u00b5 X 1 = \u00b5 X 2 and \u00b5 Y 1 = \u00b5 Y 2 , and we consider this a discrete possibility that occurs with probablity p block throughout the system, independently of H 0 . In the absence of blocking there is no information in X 2 and Y 2 that could inform the test of H 0 (considering the independence assumptions). In the presence of blocking, however, data on these second variables are highly relevant. Treating blocking as random variable across the system, we would score H 0 using the local false discovery rate lfdr 2 = P(H 0 |X 1 , X 2 , Y 1 , Y 2 ), which requires consideration of a 4-variate normal and joint discrete mixing over the blocking and null states for full evaluation. Fig. 1 shows the result of simulating a system with 10 4 variable pairs, where the marginal null frequency p 0 = 0.8, \u03c3 2 = 1/2, and the blocking rate p block varies over three possibilities. Shown is the false discovery rate of the list formed by ranking instances by either lfdr 1 or lfdr 2 . The finding in this toy problem is that power for detecting differences between \u00b5 X 1 and \u00b5 Y 1 increases by accounting for the blocking, since the list of discovered non-null cases by lfdr 2 is larger for a given false discovery rate than the list constructed using lfdr 1 . In other words, when the dimension of the parameter space is constrained by blocking, more data become relevant to the test of H 0 and power increases. False discovery rate (vertical) as a function of list size (horizontal) for various testing procedures. lfdr 1 refers to the procedure to list the unit if the local false discovery rate P(\u00b5 X 1 = \u00b5 Y 1 |X 1 , Y 1 ) is sufficiently small (black). Blue lines refer to the operating characteristics when using lfdr 2 which is P(\u00b5\n, for various probabilities p block that the two units share parameters. By accounting for blocking, we benefit through increased yield for a given false-discovery-rate.\nMaking a practical tool from the blocking observation requires that a number of modeling and computational issues be resolved. Others have recognized the potential, and have designed computationally intensive Bayesian approaches based on Markov chain Monte Carlo (Do and others (2005) , Dahl and Newton (2007) , Dahl and others (2008) , Kim and others (2009) ).\nWe seek simpler methodology that may be more readily adapted in various applications.\nIn many contexts data may be organized at nodes of an undirected graph, which will provide a basis for generalizing the concept of blocking using special graph-respecting partitions. Having replicate observations per group is a basic aspect of the data structure, but we must also account for statistical dependence among variables for effective methodology.\nIn the proposed mixture formulation we avoid the product-partition assumption which entails independencies that greatly simplify computations but at the expense of model validity and robustness; we gain numerical efficiency and avoid Markov chain Monte Carlo through a graph-localization of the mixture computations. The resulting tool we call GraphMM, for graph-based mixture model. It is deployed as a freely available open-source R package available at https://github.com/tienv/GraphMM/. We investigate its properties using a variety of synthetic-data scenarios, and we also apply it to identify statistically significant changes in brain structure associated with the onset of mild cognitive impairment."}, {"section_title": "Methods", "text": ""}, {"section_title": "Data structure and inference problem", "text": "Let G = (V, E) denote a simple, connected, undirected graph with vertex set V = {1, 2, ..., N} and edge set E, and consider partitions of V, such as \u03a8 = {b 1 , ..., b K }; that is, blocks (also called clusters) b k constitute non-empty disjoint subsets of V for which \u222a K k=1 b k = V. In the application in Section 3.2, vertices correspond to voxels at which brain-image data are measured, edges connect spatially neighboring voxels, and the partition conveys a dimension-reducing constraint. The framework is quite general and includes, for example, interesting problems from genomics and molecular biology. Recall that for any subset b \u2282 V, the induced subgraph\n, where E b contains all edges e = (v 1 , v 2 ) for which e \u2208 E and v 1 , v 2 \u2208 b. For use in constraining a parameter space, we introduce the following property: we say that \u03a8 respects G, or that \u03a8 is graph-respecting, if for all b k \u2208 \u03a8, the induced graph G b k is connected. Fig. 2 presents a simple illustration.\nIt happens that any graph-respecting partition may be encoded with a vector of binary edge variables Z = {Z e : e \u2208 E}, say with Z e \u2208 {0, 1}. Connected vertices in the same block have Z e = 1, and those in different blocks have Z e = 0. For general graphs not every binary edge vector corresponds to a graph-respecting partition, however if G is a tree then the graphrespecting partitions are in one-to-one correspondence with length-(N \u2212 1) binary vectors. In case the graph is complete, then the set of possible graph-respecting partitions is the same as the set of all partitions of V; i.e. the graph provides no reduction in the complexity of the set of partitions. It becomes relevant to statistical modeling that the size of the set of graphrespecting partitions, though large, still is substantially smaller than the set of all partitions as the graph itself becomes less complex. For example there are 21147 partitions of N = 9 objects (the 9th Bell number), but if these 9 objects are are arranged as vertices of a regular 3 \u00d7 3 lattice graph, then there are only 1434 graph-respecting partitions. In certain modeling settings, such as with Dirchlet-process mixture models, latent partitions allow for modeling parameter heterogeneity. We use the graph-respecting property to regularize the otherwise unwieldy set of such partitions.\nIn our setting, the graph G serves as a known object that provides structure to a data set being analyzed for the purpose of a two-group comparison. This is in contrast, for example, to graphical-modeling settings where the possibly unknown graph holds the dependency patterns of the joint distribution. We write the two-group data as X = ( and r, which we treat as identically distributed within group and mutually independent over m and r owing to the two-group, unpaired experimental design.\nOur methodology tests for changes between the two groups in the expected-value vectors:\nWe seek to gain statistical power over contemporary testing procedures by imposing a dimension constraint on the expected values. Although it is not required to be known or even estimated, we suppose there exists a graph-respecting partition \u03a8 = {b k } that constrains the expected values:\nAll vertices v in block b k have a common mean in the first group, say \u03d5 k , and a common mean \u03bd k in the second group. The contrast on test, then, is \u03b4 k = \u03bd k \u2212 \u03d5 k ; together with \u03a8, the binary requires that for each vertex we compute a local false discovery rate: \nand both the prior mass P(\u2206, \u03a8) and the prior predictive density f (X, Y|\u2206, \u03a8) need to be specified to compute inference summaries. Various modeling approaches present themselves. For example, we could reduce data per vertex to a test statistic (e.g., t-statistic) and model the predictive density nonparametrically, as in the R package locFDR (see Efron (2010) ). Alternatively, we could reduce data per vertex less severely, retaining effect estimates and estimated standard errors, as in adaptive shrinkage (Stephens (2017) ). In either case we would need to retain information about statistical dependencies between vertices; numerical experiments show that badly mis-specifying this dependence leads to inflated false discovery rate. The approach reported here takes an explicit parametric-model formulation for the predictive distribution of data given the discrete state (\u03a8, \u2206). This restricts the sampling model to be Gaussian, but allows general covariance among vertices and is not reliant on the product-partition assumption commonly used in partition-based models (Barry and Hartigan (1992) ).\nIn the present work we use a simple specification for P(\u03a8, \u2206); namely P(\u03a8) \u221d 1 and P(\u2206|\u03a8) encodes independent and identically distributed block-specific Bernoulli(p 0 ) indicators of a block shift. In numerical experiments we use univarite empirical-Bayes techniques to estimate\n2.2.2 Predictive density given discrete structure. We take a multivariate Gaussian sampling model:\nWe do not constrain the N \u00d7 N covariance matrices U and W, though we place a conjugage inverse Wishart prior distribution on them:\nOur predictive densities are conditional on the blocking and change patterns in \u03a8 and \u2206; in general there is no simple conjugate reduction owing to the less-than-full dimension of free parameters in \u00b5 X and \u00b5 Y . On these free parameters we further specify independent Gaussian forms:\nHyperparameters in GraphMM include scalars \u03b4 0 , \u00b5 0 , \u03c4 2 , \u03c3 2 , df and matrices A, B, which we estimate from data across the whole graph following the empirical Bayes approach (for details see Supplementary Material).\nModel (2.4) specifies the joint density f (X, Y, \u00b5 X , \u00b5 Y , U, W|\u2206, \u03a8). For the purpose of hypothesis testing, we need to marginalize most variables, since H 0,v is equivalent to \u2206 k = 0 and v \u2208 b k for block b k in partition \u03a8, and local false discovery rates require marginal posterior probabilities. Integrating out inverse Wishart distributions over the covariance matrices is possible analytically. We find:\n, \u0393 stands for multivariate gamma function\nIn the above, notation |.| denotes matrix determinant and X and Y are sample means\nNote that S 1 and T 1 are sample covariance matrices of X and Y respectively. We using Laplace approximation to numerically integrate the freely-varying means in order to obtain the marginal predictive density f (X, Y|\u2206, \u03a8). Our explicit formula is presented in Supplementary Material.\nNotably, by not constraining the sample covariance matrices U and V the GraphMM model does not adopt a product-partition form. In such, the predictive density would factor over blocks in the graph-respecting partition, and this would lead to simpler computations. We found in preliminary numerical experiments that various data sets are not consistent with this simplified dependence pattern, and we therefore propose the general form here. In working on relatively small local graphs the computations remain relatively straightforward in this case."}, {"section_title": "Data-driven simulations", "text": "Our primary evaluation of GraphMM is through an extensive set of simulations. As we have been motivated by a brain science problem (Section 3.2), we design these simulations to have summary empirical characteristics matching the empirical characteristics of our primary data set (Section 3.2). This data set comes from the Alzheimer's Disease Neuroimaging Initiative 2 (ADNI-2), and provides a template for the simulation. Briefly, we consider brain MRI data from a group of M X = 123 normal control subjects (group 1) and a second group of M Y = 148 subjects suffering from late-stage mild cognitive impairment (late MCI), a precursor to\nAlzheimer's disease. The simulation-guiding data involve a single coronal slice with N = 5236 voxels. Fig. 3 illustrates the general framework for all simulation scenarios; further details are in Algorithm 2, Supplementary Material. We use greedy clustering (Collins, 2014) on the empirical mean profiles to generate blocks for the synthetic expected values, and we adjust the empirical covariances by adding diagonal weight to assure invertibility. Three synthetic data sets are simulated in each scenario. The first three scenarios address the issue of block size; the next two investigate the role of the distribution of condition effects. To assess robustness, we also consider parameter settings where partitions are not graph respecting, and condition effects are not uniform over blocks. We also deploy two permutation experiments; the first uses sample label permutation to confirm the control of the false discovery rate, and the second uses voxel permutation to confirm that sensitivity drops when we disrupt the spatially coordiated signal.\nWhen applying GraphMM to each synthetic data set, we estimate hyperparameters for all distributional components and consider discoveries as L(c) = {v : l v c} for various thresholds c. We call the controlled FDR the mean\n, as this is the conditional expected rate of type-1 errors on the list, given data (and computable from data). We know the null status in each synthetic case, and so we also call the empirical FDR to be that rate Steps 1-4 make the correlation structure of synthetic data similar to that of MRI data. Steps 5-7 aim to mimic the mean structure and clustering pattern of MRI data. Steps 8-11 simulate data following multivariate normal distribution with specified correlation and mean structure.\ncounting latent null indicators; likewise the true positive rate counts the non-null indicators.\nWe compare GraphMM to several contemporary testing methods, including Benjamini-Hochberg correction (BH adj), Benjamini and Hochberg (1995) , local FDR, (locfdr) Efron (2007) , and qvalue (qvalue), Storey (2003) , that are all applied to voxel-specific t-tests. We also compare results to adaptive shrinkage, Stephens (2017) , both the local FDR statistic (ash lfdr) and the q-value (ash qval). These methods all work on summaries of voxel-specific tests; summaries may be p-values (for BH and q-value), or t-statistics (for locFDR), or effect estimates and estimated standard errors (for ASH). In any case, none of the methods leverages the graphical nature of the data in the way done by GraphMM."}, {"section_title": "Results", "text": ""}, {"section_title": "Operating characteristics of GraphMM", "text": "We first do a sanity check to confirm that statistical efficiency gains may arise by regularizing the expected values through the graph-respecting assumption. In a predictive simulation of veals that by enforcing regularity on the prior distribution over partitions (i.e., by enforcing the graph-respecting property), we tend to place greater posterior probability mass near the generative partition. In applications where the graph conveys structure of expected values, the graph-respecting assumption may usefully regularize the local FDR computations to benefit sensitivity. \nwhere OK is the event that the true partition \u03a8 * is graph-respecting. One analyst uses a prior that ignores the graph; the other uses a graph-respecting prior. The analyst who has regularized posterior computations tends to place more posterior probability near the generative partition.\nNext we address operating characteristics of the GraphMM methodology itself, aiming to find FDR-controlled lists of non-null vertices. Synthetic data sets mimic the structure and empirical characteristics of the brain MRI study data described in Section 3.2. The first three syntheticdata scenarios consider a single MRI brain slice measured on replicates from two conditions, with characteristics approximately matching the characteristics of observed data (Table 2) .\nThese scenarios vary the underlying size distribution of blocks, but follow the GraphMM model in having graph-respecting partitions of the underlying signal, block-level shifts between conditions, and multivariate Gaussian errors. The left panel of Figure 5 shows that all methods on test are able to control the false discovery rate. All methods display sensitivity for the signals, though GraphMM demonstrates superior power in the first two cases where blocks extend beyond the individual voxel. The high sensitivity in Scenario 2 may reflect that the prior distribution of block sizes used in the local GraphMM more closely matches the generative situation.\nNotably, even when this block-size distribution is not aligned with the GraphMM prior, we do not see an inflation of the false discovery rate.\nScenarios 4 and 5 are similar to the first cases, however they explore different forms of signals between the two groups; both have an average block size of 4 voxels, but in one case the changed block effects are fewer, relatively strong and in the other case they are more frequent, and relatively weaker (Table 3) . In both regimes, GraphMM retains its control of FDR and exhibits good sensitivity compared to other methods (Fig. 6 ).\nGraphMM is designed for the case where partition blocks are graph respecting and the changes between conditions affect entire blocks. Our next numerical experiment checks the robustness of GraphMM when this partition/change structure is violated Fig. 7 shows that GraphMM continues to control FDR and also retains a sensitivity advantage even when its underlying model is not fully correct.\nTo further assess the properties of GraphMM, we performed several permutation experiments.\nBoth started with the data set from Section 3.2. In the first, we simply permuted the sample labels of the 148 control subjects and 123 late MCI subjects, repeating for ten permuted sets. On each permuted set we applied various methods to detect differences. All discoveries are false discoveries in this null case. The left panel of Figure 8 shows that GraphMM and other methods are correctly recognizing the apparent signals as being consistent with the null hypothesis.\nThe second permutation experiment retains the sample-grouping information, but permutes the voxels within the brain slice on test. This permutation disrupts both spatial measurement dependencies and any spatial structure in the signal. Since GraphMM is leveraging spatially-coherent patterns in signal, we expect it to produce fewer statistically significant find- "}, {"section_title": "Analysis of Magnetic Resonance Images from the ADNI dataset", "text": "The data set used in this section comes from the Alzheimer's disease Neuroimaging Initiative-II (ADNI-2 * ). Our goal here was two fold:\n(i) evaluate the sensitivity of our proposal in identifying group-level differences between * The ADNI project was launched in 2003 by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, the Food and Drug Administration, private pharmaceutical companies, and nonprofit organizations. The overarching goal of ADNI study comprises of detecting Alzheimer's Disease (AD) at the earliest possible stage, identifying ways to track the disease progression with biomarkers and support advances in AD intervention, prevention and treatments. ADNI is the result of the efforts of many co-investigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. sample label permutation voxel permutation participants corresponding to different disease stages in a real scientific analysis task, and (ii) assess, via scientific interpretation of the results, the extent to which the findings from our analysis is corroborated by known results in the literature on aging and Azheimer's disease (AD).\nThe latter sub-goal is feasible because one could utilize standard pre-processing steps, described shortly, on our cohort of brain images and identify the brain regions pertinent to the clinical condition. Since the spatial location of these gray matter anatomical regions can be identified, we can evaluate whether the results are meaningful from the scientific perspective.\nDataset and pre-processing steps used. For the experimental analysis that is presented in this section, gray matter tissue probability maps derived from the co-registered T1-weighted magnetic resonance imaging (MRI) data, downloaded from ADNI using pre-processing steps provided in the commonly used voxel-based morphometry (VBM) toolbox in Statistical Parametric Mapping software (SPM, http://www.fil.ion.ucl.ac.uk/spm). Note that prior to registration to a common template or atlas (which provides a common coordinate system for conducting the analysis), standard artifact removal and other corrections were performed, consistent with existing literature Ithapu and others (2015) . Our dataset included brain images of 148 healthy control subjects (cognitively normal, abbreviated by CN) and 123 subjects with late mild cognitively impairment (LMCI). We also pre-filter voxels with very low marginal standard deviation (Bourgon and others, 2010) , which leaves 464441 voxels in total. We then apply rank-based inverse normal transformation in order to make data approximately normal."}, {"section_title": "Analysis task and baselines. Our goal is to detect or identify regions (i.e., voxels) where the distribution of gray matter intensities is significantly different across the clinical conditions, i.e., healthy controls (CN) and late stage Mild Cognitive impaired individuals (LMCI). To keep", "text": "the computational burden manageable, instead of processing the entire 3D image volume at once, we applied GraphMM (as well as a number of baseline methods) to 2D image slices in the coronal direction. Our baseline methods include Statistical non-parametric Mapping toolbox using Matlab SnPM, a popular image analysis method used by various groups, and q-value with adaptive shrinkage using R package ashr, which represents an advanced voxel-specific empirical-Bayes method.\nSummary of main experimental findings. Fig. 9 shows a representative example output of our analysis for a montage of 4 coronal slices extracted from the 3D image volume. The color bar (red to yellow), for each method presented, is a surrogate for the strength of some score describing the group-level difference: for instance, for SnPM, the color is scaled based on adjusted p-values, for the q-value method, it is scaled based on q-values, whereas for GraphMM, the color is scaled based on local false discovery rates l v . It is clear that while the regions reported as significantly different between CN and LMCI have some overlap between the different methods,\nGraphMM is able to identify many more significantly different voxels compared to baseline methods, at various FDR thresholds (Fig. 11) . A closer inspection of one case is informative (Fig.10) .\nVoxel v at coordinates (x = 31, y = 53, z = 23) is not found to be different between CN and MCI according to SnPM (adjusted p-value = 0.578) and q-value method (q-value = 0.138). But when we look at the results provided by GraphMM, the local FDR is 0.001. It is clear that part of the reason behind the increased sensitivity is that GraphMM is leveraging the consistent pattern of shifts among neighboring voxels. Clusters. Another statistical measure often reported in the neuroimaging literature is the size of significant clusters, where clusters refer to spatially connected set of voxels that are significantly altered by the sampling condition (e.g., clinical condition). The rational here is that stray voxels reported as significantly different are more likely to be an artifact relative to a group of anatomically clustered voxels. In the literature, results are often reported for clusters with size greater than or equal to 20. In the setting of graph-associated data, a significant cluster forms a connected component of the 3-dimensional lattice graph associated with the data. Fig. 12 shows a bar plot for the size of significant cluster. Here, we see that GraphMM performs favorably relative to the baseline methods and consistently reports larger clusters."}, {"section_title": "Scientific interpretation of the results. To better interpret the statistical results, we use", "text": "the Matlab package xjview to link anatomical information associated with those voxels that reported to be significantly different by the various methods. The xjview package maps the significant clusters from the analysis to the brain template or atlas (i.e., common coordinate system) using automated anatomical labeling information (Tzourio-Mazoyer and others, 2002) . This is helpful because one can then easily assign anatomically meaningful names to the voxel clusters that are significantly different. The results are summarized in Table 1 where we report the brain regions associated with significant findings from GraphMM as well as SnPM. Specifically, Column 3 in the table provides comparisons from both methods listing the number of significant voxels in the corresponding regions. This process provides additional evidence to assess if our analysis indeed revealed regions known to be associated with AD. We can answer this question by inspecting the neurological functions of the identified brain regions shown in column 4. Here, we only show the top 15 brain regions that contain most number of significant voxels. From Table 1 , we see that GraphMM discovers all the brain regions found by SnPM, with many more significant voxels in each region with more pronounced evidence of statistical significance. The only exception is the hippocampus where both methods identify a large number of voxels but GraphMM finds fewer significant voxels than SnPM. In addition, there are regions revealed to be significant by GraphMM but not by SnPM, including the precentral gyrus, middle frontal gyrus, inferior frontal gyrus opercular, insular, anterior cingulate, and supramarginal gyrus, which are relevant in the aging and AD literature. GraphMM consolidates known alterations between CN and LMCI and reveals potentially important new findings."}, {"section_title": "Discussion", "text": "Mass univariate testing remains the dominant approach to detect statistically significant changes in comparative brain-imaging studies (e.g., Groppe and others (2011) ). Here, a classical testing procedure, like the test of a contrast in a regression model, is applied in parallel over all testing has failed in some cases to reveal subtle structural changes between phenotypically distinct patient populations. The underlying problem is limited statistical power for relatively small effects, even with possibly hundreds of subjects per group. Empirical Bayes procedures improve power somewhat over BH by recognizing properties of the collection of tests. In case the data are associated with a graph and are consistent with a relatively low dimension of parameter states, we have shown one way to further enhance the empirical Bayes procedures.\nEmpirical Bayesian procedures trace their beneficial operating characteristics to \"information sharing\", or, equivalently, \"borrowing strength\". In isolation, the parameters governing data at a given testing unit are inferred locally from data at that unit. Having limited data at each unit limits any inferences we may try to make. But in treating the unit-level parameters -considered over many units -as draws from some common, system-level population of parameters, Bayesian theory provides a formal approach to link information between units (e.g., Bernardo and Smith (1994) ). Relatively elaborate information sharing is provided by highly flexible Bayesian models. For example, the Dirichlet process mixture (DPM) model engenders a clustering of the inference units, with units in the same cluster block if (and only if) they share the same parameter values. The DPM model has been effective at representing heterogeneity in a system of parameters (e.g., Muller and Quintana (2004) ), and in improving sensitivity in large-scale testing (e.g., Dahl and Newton (2007) , Dahl and others (2008) ). Benefits come at a high computational cost, since in principle the posterior summaries require averaging over all partitions of the units (e.g., Blei and Jordan (2006) ). There are also modeling costs: DPM's usu-ally have a product-partition form in which the likelihood function factors as a product over blocks of the partition (Hartigan (1990) ). In applications, such as brain imaging, we observe that independence between blocks is violated in a way that may lead to inflation of the actual false discovery rate over a target value.\nThe application of main focus in our research is structural magnetic resonance imaging (sMRI) data that arise in studies of brain structure and function. sMRI provides information to describe the shape, size, and integrity of brain structures. Briefly, the contrast and brightness of structural magnetic resonance images is determined by the characteristics of brain tissues.\nDepending on the types of sMRI sequences (e.g, T1-weighted, T2-weighted, proton densityweighted), different aspects of brain tissues are emphasized. We are particularly interested in T1-weighted sMRI, which provides good contrast between gray matter tissue (darker gray) and white matter tissue (lighter gray), while the cerebrospinal fluid is void of signal (black).\nBecause brain function depends to some extent on the integrity of brain structure, sMRI has become an integral part for clinical assessment of patients with suspected Alzheimer's disease (Vemuri and Jack (2010) ). By studying structural magnetic resonance images, we are able to make inference about gray matter atrophy in the brain, which has been shown to be one of classic symptoms for Alzheimer's disease (AD) in the neuroscience literature (e.g., Frisoni\nand others (2002), Scheltens and others (2002) , Moller and others (2013) ). Essentially, conclusions regarding the atrophy of gray matter may come from detecting changes in gray matter volume across different stages of Alzheimer's degenerative process. Furthermore, identifying subtle changes at early stages could help with early detection of gray matter atrophy, which ultimately facilitates AD diagnosis, interventions and treatments.\nThe crux of methodological research on large-scale testing in neuroimaging has been how to find thresholds on voxel-wise test statistics that control a specified false positive rate and maintain testing power. The two approaches that are most popular to neuroimaging researchers include: family-wise error control using Random Field Theory (e.g., Worsley and others (2004)) and false discovery rate control using Benjamin-Hochberg procedure (e.g., Genovese and others (2002) ). The former is based on additional assumptions about the spatial smoothness of the MRI signal. There have been criticisms. e.g., in Eklund and others (2016) , that these smoothness assumptions are usually not satisfied by the data, which would lead to alarmingly high degree of false positive. The voxel-wise test statistic can be either parametric (implemented in SPM Matlab package Penny and others (2007)) or non-parametric (implemented in Statistical Nonparametric mapping (SnPM) Matlab toolbox Nichols (2001)). A review of available methods for large-scale testing in neuroimaging inference can be found in Nichols (2012) and the references therein. Tansey and others (2018) presented an FDR tool that processes unit-specific test statistics in a way to spatially smooth the estimated prior proportions. As the clinical questions of interest move towards identifying early signs of AD, the changes in average brain profiles between conditions invariably become more subtle and increasingly hard to detect; the result is that very few voxels or brain regions may be detected as significantly different by standard methods. The methodology we study in this thesis aims to increase the sensitivity of large-scale tests for neuroimaging and related data.\nGraphs provide powerful tools for representing diverse patterns of interactions between entities of a system and have been widely used for modeling data arising in various fields of science (e.g., Gross and Yellen (2004) ). In the present work, vertices of the graph correspond to variables in a data set and the undirected edges convey relational information about the connected variables, due to associations with the context of the data set, such as temporal, functional, spatial, or anatomical information. The graphs we consider constitute an auxiliary part of observed data. For clarity, these graphs may or may not have anything to do with undirected graphical representations of the dependence in a joint distribution (e.g. Lauritzen (1996) ), as in the graphical models literature. For us, the graph serves to constrain patterns in the expected values of measurements. By limiting changes in expected values over the graph, we aim to capture low complexity of the system. An alternative way to model low-complexity is through \"smoothed/bandlimited\" signals (e.g., Ortega and others (2018) , Chen and others (2016) ). Comparisons between the approaches are warranted.'\nWe have advanced the idea of latent graph-respecting partitions that constrain expected values into low-dimensional space. The partition is paired with a vector of block-specific change indicators to convey the discrete part of the modeling specification. We used a uniform distribution over graph-respecting partitions in our numerical experiments, and have also considered more generally the distribution found by conditioning a product partition model (PPM) to be graph-respecting. In either case, two vertices that are nearby on the graph are more likely to\nshare expected values, in contrast to the exchangeability inherent in most partition models.\nGraph restriction greatly reduces the space of partitions; we simply enumerated all such partitions in our proposed graph-local computations and thereby avoided MCMC over partition space. When the generative situation is similarly graph restricted, we expect improved statistical properties; but we also showed that false discovery rates are controlled even if the generative situation is not graph respecting. Special cases of graph-restricted partitions have been studied by others. When G is a lattice graph, we have induces a spatial random partition distribution, which is the topic of study in Page and Quintana (2016) . When G is a decomposable graph, the random partition distribution proposed here and the one introduced in Caron and Doucet (2009) are similar in the sense that they are both restricted only on the set of graph-respecting partitions. For a general graph G, the distance dependent Chinese restaurant process (ddCPR) introduced in Blei and Frazier (2011) can induce a partition distribution that is also restricted on the graph-respecting partitions, though it differs from the distributions used here. When G is a complete graph there is no restriction and all partitions have positive mass. When G is a line graph the graph-respecting partition model matches Barry and Hartigan (1992) for change-point analysis.\nWorsley, Keith J., Taylor, Jonathan E., Tomaiuolo, Francesco and Lerch, Jason. (2004) .\nUnified univariate and multivariate random field theory. NeuroImage 23, S189 -S195. Mathematics in Brain Imaging."}, {"section_title": "Supplementary Material", "text": ""}, {"section_title": "Local approximation", "text": "Our model considers a general correlation structure. Therefore, it is impractical to apply the model on the entire graph due to high dimensionality. Instead, we derive a divide and conquer heuristic strategy, called local approximation, to overcome computational obstacle, which can take advantage of parallel computing. For each node v, consider a neighborhood N v of node v.\nN v is chosen such that it is a connected component of graph G. GraphMM, then, is applied to model the data in this neighborhood\nThe node-specific posterior probabilities (2.3) is approximated by\nThe local approximation procedure is illustrated by Fig. 13 . "}, {"section_title": "Algorithm 1 Local approximation", "text": "Input: pre-processed MRI data X, Y.\nOutput: per voxel posterior probability of differential structure.\nfor v in nodes do 3:\nfor (\u03a8, \u2206) in graph-respecting partitions of G local do 7: For results in Sections 3.1 and 3.2, we did local approximation on a 3 \u00d7 3 neighborhood of each node, as illustrated in Fig. 13 . In this case we can enumerate all the graph-respecting partitions (we devised a data-augmentation sampling scheme that makes use of spanning trees within the input graph; not shown). Then, we are able to enumerate all the pairs (\u03a8, \u2206) and compute the exact posterior distribution."}, {"section_title": "Algorithm 2 General framework for all the simulation scenarios", "text": "Input: MRI dataset for condition 1, G1, a N \u00d7 M X matrix; real dataset for condition 2, G2, a N \u00d7 M Y matrix. Output: synthetic dataset 1 X; synthetic dataset 2 Y.\nStep 1: S 1 \u2190 sample-covariance(G1).\nStep 2: S 2 \u2190 sample-covariance(G2).\nStep 3: V \u2190 S 1 + 0.5I # I is identity matrix.\nStep 4: W \u2190 S 2 + 0.5I # Add a small value to the diagonal of S 1 , S 2 to get positive definite matrices.\nStep 5: Avg \u2190 average-over-replicate(G1, G2) # Avg is a vector of length N ##### Implement for step 6 to 9 depends on specific scenario ##### Step 6:\n\u03d5 \u2190 simulated block means for condition 1 \u03b4 \u2190 simulated changed effects\n# \u03bd is simulated block means for condition 2. \u00b5 X \u2190 simulated node means for condition 1 \u00b5 Y \u2190 simulated node means for condition 2 # \u00b5 X , \u00b5 Y satisfy clustering constraints on the means w.r.t \u03a8 as in (2.2)."}, {"section_title": "##########", "text": "Step 10: X \u2190 Multivariate Normal (\u00b5 X , V)\nStep 11: Y \u2190 Multivariate Normal (\u00b5 Y , W)"}, {"section_title": "Simulation study details", "text": "The graph associated with data is a lattice graph representing spatial dependence, in which the vertices are the pixels and the edges connect neighboring voxels. The analysis of GraphMM involves estimating hyper parameters: prior null probability p 0 , prior mean \u00b5 0 and standard deviation \u03c4 2 of block mean of group 1, prior mean \u03b4 0 and standard deviation \u03c3 2 of difference in block mean between 2 groups, prior covariance matrix A for group 1 and matrix B for group 2. Different strategies for estimating hyperparameters have been considered,\n\u2022 Estimating prior null probability p 0 : We experimented with both qvalue or ahsr to get the estimated value of p 0 . Package qvalue produces conservative estimate of p 0 without any assumption on the distribution of effects. Hence it is a safe and conservative choice under general settings. Package ashr, on the other hand, provides conservative estimate under Table 2 : Description for simulation 1, 2 and 3. Text with blue color and figures emphasizes that these simulations differ in the average size of latent blocks. In the figures, area with magenta color shows changed-blocks. We can see that the size of changed-blocks decreases in simulation 1, 2 and 3. Especially simulation 3 has no clustering effect, i.e the block size is 1 for all blocks."}, {"section_title": "Scenario 1", "text": "Scenario 2 Scenario 3\nStep 6 * Use greedy clustering method [Collins (2014) ] * Partition is adjusted to respect lattice graph. * There are 1313 blocks, average block size is 3.9\nSame as scenario 1 * Each node itself is a block. * There are 5236 blocks, block size is 1\nStep 7\n* 50 blocks with size from 12 to 14 are chosen to be changed-block * Average size of changed-block is 13.6 * Percentage of changed-nodes: 14.3%\n* 300 blocks with size from 2 to 5 are chosen to be changed-block * Average size of changed-block is 2.6 * Percentage of changed-nodes: 14.9% * 15% of the nodes are chosen to be changed-nodes\nStep 8, 9 been discussed intensively in Stephens (2017) and has been argued to be both plausible and beneficial in many contexts. Furthermore, in our graph-based mixture model 2.2, the distribution of effects \u03b4 k was assumed to be a mixture of probability mass at 0 and normal distribution, which satisfies unimodal assumption. Therefore, using package ashr Table 3 : Description for simulation 4 and 5. Text with blue color and figures emphasizes that these simulations differ in the percentage of changed-nodes. The figures show histogram of block avergage shifts across 2 groups for all blocks (red area) and for changed-blocks (green area)."}, {"section_title": "Simulation 4", "text": "Simulation 5\nStep 6 * Use greedy clustering method [Collins (2014) ] * Partition is adjusted to respect lattice graph. * There are 1313 blocks, average block size is 3.9\nSame as Simulation 4\nStep 7 m x \u2190 block average of MRI data group 1 m y \u2190 block average of MRI data group 2 di f f \u2190 m y \u2212 m x prob \u2190 increasing function of |di f f | and belongs in (0,1) \u2206 \u223c Bernoulli(prob) * Percentage of changed-nodes: 16.4% * Similar to simulation 4, except that * Percentage of changed-nodes: 50.3%\nStep 8 & 9 * If block k is a changed-block:\nSame as Simulation 4\nFigure to estimate for p 0 meshes with our GraphMM. The estimation of p 0 is based on the whole dataset, computing prior to the local approximation procedure. In reported computations we used ashr package for p 0 .\n\u2022 Estimating other hyperparameters: We consider 3 approaches: global, local and mixed estimation. With global estimation, the hyperparameters are estimated using the whole dataset and computed prior to the local approximation procedure. With local estimation, hyperparameters are estimated for each neighborhood, during the local approximation procedure. With mixed estimation, all hyperparameters are estimated locally except for matrices A and B, which are estimated globally. These approaches, local, mixed and global provides increasingly conservative estimates in that order. In following simulation and application, we present results using mixed estimation."}, {"section_title": "Computing marginal likelihood", "text": "We derive the marginal likelihood using Laplace approximation. Consider the notations as in section 2.2. Let K \u03a8 be the number of blocks corresponding to partition \u03a8 and K diff be the number of changed blocks, which means\nDenote the ordered indices of changed blocks as (j 1 , j 2 , . . . , j K diff ). We re-parametrize the model in order to remove the clustering constraints on the means := (\u03b4 j 1 , \u03b4 j 2 , . . . , \u03b4 j K diff )\nThen, the free parameters are (\u03d5, ) and the marginal likelihood function can be written as f (X, Y|\u03a8, \u2206) = f (X, Y, \u03d5, |\u03a8, \u2206)dP(\u03d5)dP( )\nwhere In the next step, we derive the explicit formula for the gradient and Hessian matrix of F.\nLet L be the allocation matrix with size N \u00d7 K \u03a8 where c vk = 1 if and only iff node v belong to block k. Let R be a K \u03a8 \u00d7 K diff matrix such that column lth of R has value 1 at position j l and has value 0 at other postions. Then we can relate the mean vectors with the new parameters (\u03d5, ) as follows \u03b4 = R"}]