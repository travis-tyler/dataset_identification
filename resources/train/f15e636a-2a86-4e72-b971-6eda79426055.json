[{"section_title": "", "text": "international tests and frameworks for guidance and models. \"International benchmarking\" is the new buzz word. So is \"world class.\" But which international and national frameworks and tests are worth benchmarking to?"}, {"section_title": "our revieWerS and a SnapShot of What they found", "text": "To help answer these questions, we enlisted four top-notch experts, individuals who not only possess deep content expertise in their respective fields, but who have also rolled up their sleeves to work with state officials and classroom teachers to ensure that state standards are crafted and implemented with integrity. Sheila Byrd Carmichael served as lead reviewer for English language arts. She is an education consultant based in Washington, D.C., who has taught English in the District of Columbia Public Schools as well as Italy and Japan. She was the founding director of the American Diploma Project. Carol Jago served as external reviewer for English language arts. Ms. Jago is the Director of the Reading and Literature Project at UCLA and incoming President of the National Council of Teachers of English. W. Stephen Wilson served as lead reviewer for mathematics. He is Professor of Mathematics at the Johns Hopkins University and has served as Senior Advisor for Mathematics at the U.S. Department of Education. Richard Askey served as external reviewer for mathematics. Dr. Askey is Professor Emeritus of Mathematics at the University of Wisconsin-Madison, Fellow of American Academy of Arts and Sciences, and in 1999, was elected to the National Academy of Sciences. (For more on our reviewers, see \"Meet the Experts,\" page 5.)"}, {"section_title": "the eLa diLemma", "text": "That last point needs amplification, for it's crucial to understand that drafters of the Common Core standards in reading, etc., faced a dilemma. If they tried to set standards for the entirety of English language arts, they would be inviting unwinnable battles over reading lists, authors, multiculturalism, and such. If, on the other hand, they confined themselves to the essential \"skills\" associated with reading/writing/speaking/listening, they would be open to complaints that \"this isn't really English\" and \"reading cannot be taught sans content.\" So they made a prudential judgment. The actual standards they set forth are limited to key skills but they carefully explain the types and levels of reading materials (including but not limited to literature) that they judge to be suited to those skills; they offer a few well-chosen illustrative passages; they underscore the interdependence of skills and content; and they state clearly and bluntly that these standards need to be accompanied by a rich, content-based curriculum. But they don't supply that content themselves. Someone must. This is really, really important, particularly for reading. In addition to our reviewers' commentary on this point, consider these trenchant observations by University of Virginia psychologist Daniel Willingham, regarding the draft reading standards: WHAT'S THEIR VERDICT? k PISA \u2022 PISA strikes out. Neither in reading (literacy) nor in math does its content deserve better than a grade of \"D.\" This is not a promising benchmark for American K-12 education in these subjects. k NAEP \u2022 NAEP fares better, with a \"C\" for math and \"B\" grades in reading and writing. But it ought to be better than it is. k TIMSS \u2022 TIMSS does really well, earning an \"A\" in math. (Math and science are all that TIMSS touches.) k Common Core: Math \u2022 The draft Common Core end-of-high-school standards in math, mid-September version, are better than PISA and NAEP, not as good as TIMSS. Our reviewers give this draft a \"B\" and offer suggestions for improving the final version. k Common Core: ELA \u2022 The draft Common Core end-of-high-school standards in reading-writing-speaking-listening also earn a \"B\" from our reviewers-as well as much advice for strengthening and augmenting the final version."}, {"section_title": "STARS BY WHICH TO NAVIGATE? SCANNING NATIONAL AND INTERNATIONAL EDUCATION STANDARDS IN 2009", "text": "\n"}, {"section_title": "7", "text": "PART 1 > INTRODUCTION At first glance the 18 standards sound quite sensible: students should be able to determine what a text says, make inferences from it, discern the most important ideas, and so forth\u2026.The problem is that teachers and administrators are likely to read those 18 standards and to try to teach to them. But reading comprehension is not a \"skill\" that can be taught directly\u2026.The mainspring of comprehension is prior knowledge-the stuff readers already know that enables them to create understanding as they read\u2026. What happens if the reader doesn't have the prior knowledge the writer assumed she had? The reader will be confused and comprehension breaks down\u2026.If you take kids who score poorly on a reading test and ask them to read on a topic they know something about (baseball, say, or dinosaurs) all of a sudden their comprehension is terrific-better than kids who score well on reading tests but who don't know a lot about baseball or dinosaurs. In other words, kids who score well on reading tests are not really kids with good \"reading skills.\" The kids who score well on reading tests are ones who know a lot about the world-they have a lot of prior knowledge about a wide range of things-and so that whatever they are asked to read about on the test, they likely know something about it\u2026. How do students get prior knowledge? It accumulates through years of exposure to newspapers, serious magazines, books, conversations with knowledgeable people. It should also come from a content-rich curriculum in school\u2026. The new national standards actually say that\u2026.But the standards themselves don't recommend that we ensure that students \"have a strong content base\" as a way to ensure that they are good readers! Instead, the standards document lists things that students ought to be able to do (summarize, find the main idea, etc.) that invite states, districts, and teachers to design curricula emphasizing practice in those skills. The mistaken idea that reading is a skill-learn to crack the code, practice comprehension strategies and you can read anything-may be the single biggest factor holding back reading achievement in the country. Much work awaits the drafters, at least in reading/writing/speaking/listening, both as they revise the end-of-high-school standards and as they backward-map these to earlier grades. And even more work awaits those who will deploy these standards as part of-please, just part of-their own expectations for schools and students. We will, of course, have more to say on this topic when the revised versions come out. We naturally hope that the revision process will heed our reviewers' comments, criticisms, and advice."}, {"section_title": "the path ahead", "text": "The purpose of this report, then, is to assess the content, rigor, and clarity of the first public drafts of the CCSSI and-applying identical criteria-also to appraise these elements in America's current de facto national math and reading/writing tests (NAEP) as well as two influential international testing regimes (PISA and TIMSS) that many look to for \"benchmarking\" purposes. Our subject experts have compared these test frameworks and standards using the same metrics. (See \"Assumptions\" on page 8 for a discussion of why testing frameworks may be considered standards.) Our goal is to help readers and users determine which of these documents ought to influence their thinking, their standards, their aspirations, and their tests. Is one of these national/ international frameworks worthier of emulation than others? Are components of several stronger in different domains? But this is an interim report. What we review here are the first public drafts released by the Common Core project. Made available on September 21, the public has been invited to provide feedback on them through October 21 via the CCSSI website. And that's what we and our expert reviewers are doing. The common standards are still a work in progress; 1 hence, these reviews are not summative. That's true of NAEP, PISA, and TIMSS, too. Some of them are also in the process of being updated or revised. So we'll be back in the spring of 2010 with a follow-up report. It will include new reviews of standards and tests from organizations frequently regarded as key players in determining college-and career readiness-the ACT and the College Board (both of which are partners in the Common Core initiative). More specifically, the next report will round out our examination of math and English language arts standards. In math, we expect to add reviews of 1) the PISA 2009 Mathematics Frameworks (scheduled to be released in December 2009); 2) the TIMSS 2011 Mathematics Frameworks ( just released in September 2009); 3) the ACT Testing Specifications for Mathematics; 4) the In this way, the PISA framework has the same intention and performs essentially the same function as academic standards, and can reasonably be evaluated by the same criteria. Further, a number of American educators, policy makers, and standard setters look to PISA as a benchmark for what they should require and/or expect of American schools. (We think it's no coincidence that the head of PISA has just been appointed to the CCSSI \"validation committee,\" along with his boss at the OECD.) Much the same can be said for NAEP and TIMSS and the roles they play in American education generally and in standardssetting and benchmarking particularly. Since all these frameworks get used for these purposes, it is important to appraise their content. Hence, we use this opportunity to evaluate the NAEP, PISA, TIMSS, and Common Core reading/writing/ communications and math frameworks as content (as well as skill) standards. Further, it's long been observed that \"what gets tested is what gets taught\" so it's more than reasonable to review assessment items (which we've done here when there were enough of them) and assessment frameworks in light of their likely impact at the classroom level. That said, in the case of English language arts, reviewers confined their appraisals to the elements of ELA that the frameworks actually addressed. For instance, the Common Core standards are confined to reading, writing, and communications and do not attempt to encompass the full \"content\" of English language arts (e.g., literature and documents). Indeed, the drafters stipulate that their standards need to be accompanied by rich content-based curricula. Therefore, the draft Common Core standards are appraised as reading, writing, and communications standards, not ELA standards. The NAEP Reading and NAEP Writing Frameworks purport to be just what their names suggest-frameworks by which to design assessments of reading and writing performance-and are evaluated accordingly. Here, as in CCSSI, it will be very important for state officials to understand those limitations and to appreciate what else may be needed to generate complete standards and curricula for English language arts. iS it appropriate to demand that StandardS be \"evidenCe baSed\"? Our second assumption is that expert judgment is a vital component of appraising academic standards. We respect the yearning for all standards to be \"evidence-based\" and the criteria used to assess standards in this report reflect that impulse. When real evidence is available, we ought to use it; ACT, for one, and researchers like William Schmidt at Michigan State have done much to expand the evidentiary base for standards development. Examining readiness for college via actual college work samples is also promising. But real evidence to date is quite limited. Perhaps that's why CCSSI drafters cast such a wide evidentiary net. Yet merely finding and naming a country that has similar content in its standards or locating similar content in another test or framework (e.g., the ACT or American Diploma Project) is not compelling evidence. In fact, most of these determinations were originally derived from expert opinion, preference, or survey results. They were not an attempt to \"validate\" the standard per se, inasmuch as that means that the standard is necessary to accomplish a higher standard or end goal. In other words, much of the cited evidence in the common standards is suggestive, not dispositive. Frankly, we're not that optimistic that all academic standards can or must be \"validated,\" partly because we don't define a good education strictly in terms of college-and-career readiness, and partly because true validation studies are hard to do well. (We observe, too, that the recently-named \"Validation Committee\" is light on card-carrying mathematicians and literature experts.) Moreover, strict adherence to \"evidence\" may belie old fashioned common sense. Certainly the focus on \"college and career readiness\" is well-intended and legitimate as far as it goes. Students must be able to deal with the academic and occupational challenges that lie ahead. But young Americans must also be equipped to contribute in our democracy as citizen-participants. In her 1987 seminal text, Democratic Education, Amy Gutmann wrote: \"The democratic interpretation of equal education opportunity requires that all educable children learn enough to participate effectively in the democratic process.\" An education, then, is not only about preparation for what follows immediately after high school, namely college and/or job, but also about preparation for citizenship, community life, and cultural participation. If we define the purpose of education in narrow, utilitarian terms, and then build standards atop that definition, we could end up with a population that is highly skilled yet ignorant, self-absorbed, amoral, and uncultured. Why iS the CaLL for \"feWer, CLearer, higher StandardS\" more CompLiCated than it SeemS? There's been much talk about the need for \"fewer, higher, and clearer\" standards. At a glance, that's a reasonable-sounding call to action. Among the failings of the \"voluntary national standards\" projects of the early 1990's was their creators' tendency to put into them everything but the kitchen sink, resulting in whopping fat volumes that nobody could realistically expect to use in actual schools. Yet \"fewer\" does not necessarily lead to clearer or higher; one might have a few standards that are vague and lack depth. Further, we're inclined to believe that fewer, clearer, and higher standards work better in some disciplines than others. Math, for instance, is a single, cumulative discipline, quite unlike, say, English language arts, history, and science. A mathematics standard like \"Compare and order whole numbers\" is clear and explicit while a reading standard such as \"Read and analyze both literary and non-literary texts\" begs for clarification and explication. Perhaps it means that students need to evaluate the development of literary elements, or the effectiveness of rhetorical techniques, or the manipulation of stylistic devices? Perhaps they should describe the truth and/or validity of an argument or recognize and explain the presence of fallacious reasoning? Either way, it's going to take more detail to transform that lofty but nebulous standard into an explicit one that's actionable in the curriculum and the classroom. Fordham has been reviewing state content standards in core subject areas (English language arts, math, science, history) for more than a decade. But as America moves in the direction of common, multi-state, or national standards, and as state officials (and \"Common Core\" drafters) seek to benchmark, model, or justify their work on the basis of extant national and international standards and tests, it was necessary to go farther, to determine, in effect, whether the models deserve to be emulated. So we replicated at the national/international level the methods we have previously used to evaluate state standards. That meant relying once again on content experts. This time, we tapped four well-known mathematics and English language arts authorities, all of whom have worked with us in the past and have substantial experience working in the state standards arena (see \"Meet the Experts\" on page 5). Two served as lead reviewers; the other two judiciously responded to their work. 2 We asked the reviewers, based on the best current thinking about their discipline-and a modest but growing evidence base around what students need to know and be able to do in order to be prepared for college, work, and democratic citizenship-to devise a set of content expectations for students at grades four, eight, and twelve (as well as age fifteen). These are the grade levels (and ages) at which NAEP, PISA, and TIMSS are administered. We encouraged them to consider the most important elements of content that standards should cover. In other words, we asked them to set forth a minimal set of crucial content expectations, not to dump into the sink every stray fork and teacup in their fields. Multiple drafts were exchanged and revised before the content criteria were finalized. To facilitate apples-to-apples comparisons, we developed a common grading metric upon which all standards could be measured. In other words, reviewers would assess the various standards and tests according to content-specific criteria, but would need to render a bottom line grade based on a standardized scoring system. This meant that a \"B\" in mathematics would need to mean the same thing as a \"B\" in English language arts. The final versions of the subject-specific criteria and common grading metric can be found next in part 2."}, {"section_title": "PART 2 > GRADING", "text": "The Grading This section lays out the core or minimal content material that students in fourth, eighth, and twelfth grade should know, as well as fifteen-year-old students (above and beyond eighth-grade expectations). It also includes an explanation of the proper treatment of data analysis, statistics, and probability in K-12 education (see \"How much DASP do students need?\" page 12)."}, {"section_title": "fourth grade", "text": ""}, {"section_title": "Addition and subtraction", "text": "Students should know the single-digit addition facts and the corresponding subtraction facts."}, {"section_title": "1", "text": "They should be able to add and subtract whole numbers fluently and understand the algorithms they use.\nThey should understand the algorithms they use.\nIn addition to the above core that should be mastered, they should be given an introduction to other material. Students should 2 be introduced to fractions and decimals and be able to compare the magnitude of simple fractions and decimals. Students should be able to use commutativity and associativity.\n\nIn addition to the essentials above, students should have an elementary knowledge of data analysis, statistics, and probability.\nThe standards address systematic vocabulary development (e.g., basic prefixes and suffixes; common synonyms, antonyms, 2 and compound words; multiple meaning words; dictionary use). The standards outline specific expectations for reading and analyzing literary and non-literary texts (e.g., recognizing and 3 interpreting genres; structures; literary elements; and stylistic devices). The standards reflect the importance of reading grade-appropriate works of outstanding American literature that reflect our 4 common heritage. The standards describe the amount, quality, and complexity of both literary and non-literary texts to be studied through the 5 use of lists (authors and/or titles), sample passages, and/or commentary."}, {"section_title": "2", "text": "They should understand the inverse nature of addition and subtraction.\nStudents should be introduced to division and understand the inverse nature of multiplication and division.\n\n(See below, \"How much DASP do students need?\") They should understand absolute value and be able to solve elementary linear equations."}, {"section_title": "3", "text": "\n\nThey should be able to do elementary estimations."}, {"section_title": "Multiplication and division", "text": "Students should know the single-digit multiplication facts and be fluent with multiplying whole numbers."}, {"section_title": "Measurement", "text": "Students should understand some forms of measurement. In particular, they should know and understand the formulas for 1 the area and perimeter of a rectangle and know how to measure distances to the nearest centimeter and half-inch using rulers."}, {"section_title": "Problem solving", "text": "Students should be able to use the above essential material to solve complex multi-step exercises and word problems.\nStudents should be able to use the above essential material to solve complex multi-step exercises and word problems."}, {"section_title": "4", "text": "They need a basic vocabulary for geometry, including parallel and perpendicular lines.\nEach of the components is described and appraised below."}, {"section_title": "5", "text": "They should be exposed to very elementary statistics and probability including reading and making basic graphs and tables."}, {"section_title": "eighth grade", "text": ""}, {"section_title": "Arithmetic", "text": "Students should be fluent with the four arithmetic operations with rational numbers and decimals, without a calculator, and 1 they should understand the operations."}, {"section_title": "Ratios", "text": "Students should understand and be able to use rates, ratios, proportions, and percentages."}, {"section_title": "Geometry and measurement", "text": "Students should have a good geometry vocabulary, know about angles associated with triangles and parallel lines, understand similar and congruent triangles, and be able to compute areas, perimeters and volumes of various geometrical shapes, including circles. They should know the Pythagorean Theorem and the coordinate system."}, {"section_title": "fifteen year-oLdS", "text": "Fifteen year-olds should have mastered eighth-grade material and they should have the equivalent of a year of algebra as well. They should understand and be able to use roots, reciprocals, and powers. They should be able to solve equations and inequalities that are linear or involve the absolute value. Students should know about slope and forms of linear functions and how to graph them. Students should be fluent with the arithmetic of polynomials and rational expressions, including elementary factoring. Students should be able to use the above essential material to solve complex multi-step exercises and word problems. In addition to the essentials above, students should be introduced to quadratic equations and their solutions."}, {"section_title": "tWeLfth grade", "text": "Minimal twelfth grade expectations, in addition to the expectations for fifteen-year-olds, include a working knowledge of geometry containing constructions, definitions, and proofs of the major results in Euclidean geometry. Students should understand logarithmic and exponential functions as well as basic trigonometry and trigonometric functions. Students should also be able to completely analyze quadratic equations, inequalities, and functions. Students should know basic statistics and probability, particularly the counting arguments involving combinations and permutations. Students should be able to use this essential material to solve complex multi-step exercises and word problems."}, {"section_title": "HOW MUCH DASP DO STUDENTS NEED? s W. Stephen Wilson", "text": "Data Analysis, Statistics, and Probability (DASP), play a prominent role in many sets of standards. Certainly students should be able to read and construct various kinds of graphs and displays for data, compute elementary probabilities, and know the basic descriptors for data such as percentiles, mean, and median. In addition, they should be able to use combinations and permutations in their counting arguments for probability and have some concept of random variability. Many standards, however, go far beyond these basic needs. Justification for this is usually slim, frequently based on a vague notion that a good citizen should know this material. Presently, many standards include deep mathematical content that is far above the level of mathematics accessible to K-12 students. This results in superficial coverage that often lacks mathematical coherence. In addition, the \"grain size\" of DASP standards tends to be smaller than for other areas of math standards, i.e., more detail is devoted to them. This increases the number and percentage of standards dedicated to DASP and gives them more prominence than they deserve. A common example, frequently observed in middle school standards, is a variant on \"eye-balling\" a line of best fit for a scatter-plot. More specifically, the eighth grade NAEP framework has this standard: \"Visually choose the line that best fits given a scatterplot.\" \"Visually choosing\" (i.e., eye-balling) is not a mathematical activity. Further, the concept of \"least squares\" is mentioned in the NAEP twelfth grade standards, though it is inappropriate to include it. That's because most students do not have the requisite math background to see this for what it is: Finding the closest point on a plane in n-dimension Euclidean space to a point not on that plane using the Pythagorean Theorem to compute the distances. This is material from a college-level linear algebra or multivariable calculus course, courses that generally follow calculus. Another example is when students are exposed to the normal distribution, but its origin and connection to the binomial distribution-college-level material to be sure-is not covered, leading to superficial coverage of the former. There are many other examples where students lack mathematical understanding with regard to what is actually occurring in a DASP problem. "}, {"section_title": "13", "text": "PART 2 > GRADING Further, recent research has shown that DASP does not prepare students for placement exams in college or for college level material, 3 nor is it material requested by college mathematics teachers. 4 Certainly students headed for scientific and technological careers will be required to take college-level courses in statistics and probability, as will anyone else who needs this material in college. In the end, though, knowledge of arithmetic, ratios and algebra will be much more important to all students than DASP content void of a conceptual base. After all, students should possess the mathematical foundation that allows them to understand what they are doing."}, {"section_title": "English Language Arts", "text": "The following criteria contain examples in many places to clarify the type (and level) of concepts and skills that are expected in quality standards at the designated grade spans. The criteria and examples for each grade span should be understood to include the criteria and examples for the grade spans that precede them. 5 gradeS k-4"}, {"section_title": "Reading", "text": "The standards delineate explicit and systematic expectations in phonemic awareness, phonics, fluency, and comprehension skills.\nThe standards address vocabulary development (e.g., knowledge of roots and affixes; connotation and denotation; figurative 1 language; use of the dictionary for clarifying multiple meanings, etymology, and pronunciation). The standards specify strategies/skills for reading and analyzing both literary and non-literary texts (e.g., analysis of genres, 2 structures, literary elements, rhetorical techniques, and stylistic devices; strategies for comprehension and interpretation). The standards reflect the importance of reading grade-appropriate works of outstanding American literature that reflect our 3 common heritage. The standards describe the amount, quality, and complexity of both literary and non-literary texts to be studied through the 4 use of lists (authors and/or titles), sample passages, and/or commentary.\nThe standards address vocabulary development and skills for building content area vocabulary (e.g., applying knowledge of 1 roots and affixes to help determine meanings of words; tracing etymology; determining shades of meaning). The standards describe specific expectations for reading and analyzing both literary and non-literary texts (e.g., analyzing the 2 clarity of structures, the development of literary elements, the effectiveness of rhetorical techniques, and the manipulation of stylistic devices; describing the truth and/or validity of an argument; recognizing and explaining the presence of fallacious reasoning). The standards reflect the importance of reading grade-appropriate works of outstanding American literature that reflect our 3 common literary heritage. The standards describe the amount, quality, and complexity of both literary and non-literary texts to be studied through the 4 use of lists (authors and/or titles), sample passages, and/or commentary."}, {"section_title": "Writing", "text": "The standards delineate expectations for writing that address the characteristics and quality of writing products appropriate 6 to the grade level (e.g., organization of ideas and focus; introduction, body and conclusion; elements of a paragraph; evaluation and revision skills). The standards require students to recognize, explain, and produce writing that reflects the defining characteristics of various 7 grade appropriate writing genres (e.g., narration; exposition). The standards describe or reference the use of specific criteria for evaluating writing (e.g., logically organized and detailed 8 genre-or prompt-specific rubrics) that include examples regarding the quality of writing expected.\nThe standards delineate expectations for writing that address the characteristics and quality of writing products appropriate 5 to the grade level (increasingly sophisticated understanding of audience and purpose; clear organization and consistent focus; development of ideas through multi-paragraph essays; use of transitions; reflective peer review and revision processes). The standards require students to interpret and produce writing that reflects the defining characteristics of various grade-6 appropriate writing genres (e.g., argument). The standards describe or reference the use of specific criteria for evaluating writing (e.g., logically organized and detailed 7 genre-or prompt-specific rubrics) that include examples regarding the quality of writing expected.\nThe standards delineate expectations for writing that address the characteristics and quality of writing products appropriate 5 to the grade level (e.g., strong organization and development of ideas; facility with selection and blending of genres appropriate to audience and purpose; the use of sophisticated transitions, active rather than passive voice, and other stylistic elements for rhetorical effect). The standards require students to analyze and produce writing that reflects the defining characteristics of various grade-6 appropriate writing genres (e.g., persuasion). The standards describe or reference the use of specific criteria for evaluating writing (e.g., logically organized and detailed 7 genre-or prompt-specific rubrics) that include examples regarding the quality of writing expected."}, {"section_title": "Listening and Speaking", "text": "The standards clearly address active listening and effective speaking skills (e.g., summarizing information presented orally; 9 asking and answering relevant questions). The standards address the ability to make formal oral presentations (e.g., recitation; story retelling; and sequencing).\nThe standards clearly address active listening and effective speaking skills (e.g., give, restate, and execute multi-step 8 directions; convey ideas orally and interpret spoken ideas; make inferences from spoken information; ask and answer clarifying questions). The standards address the ability to make formal oral presentations (e.g., recitation; informative and persuasive presentations 9 that offer supporting details and evidence; address anticipated counterclaims and include a call to action when appropriate). The standards describe or reference the use of detailed criteria for evaluating formal oral presentations.\nThe standards clearly address active listening and effective speaking skills (e.g., interpret complex information and ideas 8 presented orally and convey complex information or ideas orally). The standards address the ability to make formal oral presentations (recitation; complex informative or persuasive oral 9 presentations that require a logical structure, well-chosen supporting evidence/details, skillful rhetorical techniques, and a strong presentation style). The standards describe or reference the use of detailed criteria for evaluating formal oral presentations."}, {"section_title": "10", "text": "The standards describe or reference the use of specific criteria for evaluating oral presentations (e.g., content; organization; 11 and presentation style). The standards include specific expectations for participation in group discussions (e.g., turn-taking; applying agreed-upon\nThe standards include specific expectations for participation in group discussions (e.g., designation of roles; eliciting and 11 considering suggestions).\nThe standards include specific expectations for participation in group discussions (e.g., tolerating ambiguity; building on the 11 ideas of others; reaching consensus). "}, {"section_title": "Oral and Written Language Conventions", "text": "The standards specify expectations for the correct use of standard English, describing a grade-appropriate facility with 13 the parts of speech, sentence structure, usage, and mechanics appropriate to the grade level (e.g., nouns, verbs, adjectives, adverbs, conjunctions, prepositions, and nominative/objective/interrogative pronouns; sentence types; complete/incomplete sentences; subject/verb (S/V) agreement; initial, internal, and ending punctuation; and basic spelling rules, such with as plurals, contractions, and inflections).\nThe standards specify expectations for the correct use of standard English, describing a grade-appropriate facility with the parts of speech, sentence structure, usage, and mechanics appropriate to the grade level (e.g., parts of the verb; interjections, possessive/demonstrative/relative/indefinite pronouns; tenses; analysis of sentence structure; types of phrases and clauses; fragments and run-on sentences; facility with mechanics grounded in understanding of sentence structure).\nThe standards specify expectations for the correct use of standard English, describing a grade-appropriate facility with 12 the parts of speech, sentence structure, usage, and mechanics (e.g., demonstrate control of sentence structure, usage, and mechanics)."}, {"section_title": "Research", "text": "The standards require students to learn the research process, outlining specific expectations for the essential components of 14 the process (e.g., identifying or finalizing a research question; locating information; evaluating and compiling information; presenting findings; and acknowledging sources using a standard format).\nThe standards require students to employ the research process, outlining specific expectations for the essential components 13 of the process (e.g., identifying and refining a research question; locating information; evaluating the quality of information/ sources; selecting information that supports a thesis; presenting findings; citing sources correctly using standard guidelines; and avoiding plagiarism).\nThe standards require students to conduct the research process, outlining specific expectations for the essential components 13 of the process (e.g., identifying and refining a research question; locating information; evaluating the quality of information/ sources; selecting information that supports a thesis; excluding extraneous information; presenting findings in a format appropriate for the audience and purpose; citing sources correctly in a standard format; and avoiding plagiarism)."}, {"section_title": "Media", "text": "The standards require students to analyze and evaluate information presented in multimedia formats (e.g., the effect of 15 various visual and aural techniques; how information presented in print is different from that which is presented through the use of multimedia). The standards require that students learn about multimedia techniques for presenting information.\nThe standards require students to analyze and evaluate information presented in multimedia formats (e.g., how information 14 presented in print is different from that which is presented through the use of multimedia; noting what is conveyed through the use of various visual and aural techniques, such as bias and propaganda). The standards require that students know how to use multimedia techniques to present information.\nThe standards require students to analyze and evaluate information presented in multimedia formats (e.g., noting instances of 14 manipulation, bias, propaganda, and potential fallacies). The standards require that students use multimedia techniques to present information when possible."}, {"section_title": "gradeS 5-8", "text": ""}, {"section_title": "gradeS 9-12", "text": ""}, {"section_title": "15", "text": ""}, {"section_title": "Common Grading Metric s Jointly developed by Fordham and the Reviewers", "text": "Reviewers compared each set of standards to subject-specific content expectations (see pages 11-16). 6 Based on that comparison, they assigned the standards two scores, one for \"content and rigor,\" the other for \"clarity and specificity.\" \"Content and rigor\" is scored on a 0-7-point scale while \"clarity and specificity\" is scored on a 0-3-point scale, as follows. Standards distinguish between more important and less important content and skills either directly (i.e., by articulating which are more or less important) OR via the number of standards dedicated to particular content and skills (i.e., more important content/skills have more standards while less important content/skills have fewer standards). The standards do not overemphasize topics of little importance or underemphasize topics of great importance. The level of rigor is appropriate for the targeted grade level(s). Students are expected to learn the content and skills in a sensible order and an appropriately increasing level of difficulty. The standards, taken as a whole, define a core literacy for all students in the subject under review; at the same time, the standards that run through grade 12 are sufficiently challenging to ensure that students who achieve proficiency by the final year of high school will be ready for college or work and citizenship. The standards do not overemphasize the importance of students' life experiences or \"real world\" problems. They do not embrace fads, suggest political bias, or teach moral dogma. They do not imply that all interpretations are equally valid (regardless of logic or the adequacy of supporting evidence). The standards also avoid other major subject-specific problems identified by the reviewers. While the standards are not perfect, any defects are marginal. PART 2 > GRADING 6: Standards fall short in one or more of the following ways: Some crucial content (as specified in the subject-specific content expectations) is missing (at least 5 percent and up to 20 percent). The content is covered satisfactorily but not in a high quality manner. Some of the content in the standards is unnecessary (at least 5 percent and up to 20 percent). Standards do not fully distinguish between more and less important content and skills (i.e., importance is neither expressly articulated nor conveyed via the number of standards dedicated to particular topics). In other words, the standards overemphasize no more than one or two topics of little importance or underemphasize no more than one or two topics of great importance. Standards at particular grade levels are not quite as rigorous as they could be, or are too rigorous (i.e., expectations are slightly too high or too low). There are minor problems or shortcomings (e.g., one or more of the problems listed in the last paragraph under score 7 affects the standards in a small way, or there are other minor subject-specific problems). 5: Standards fall short in one or more of the following ways: Some crucial content is missing (at least 20 percent and up to 35 percent). While most of the appropriate content is covered by the standards, the content is nonetheless covered in a manner that is not satisfactory (i.e., the standards cover the right material but do not cover that material robustly, thus, the material is shortchanged in some way). Some of the content in the standards is unnecessary (at least 20 percent and up to 35 percent). Standards do not distinguish between more and less important content and skills (i.e., importance is not articulated or conveyed in any way). The standards often overemphasize topics of little importance or underemphasize topics of great importance. Standards generally need to be more or less rigorous than they are at certain grade levels (i.e., expectations are too high or too low). There may be an important shortcoming (perhaps one of the problems listed in the last paragraph of score 7, or there are other subject-specific problems). 4: Standards fall short in one or more of the following ways: At least 35 percent and up to 50 percent of crucial content is missing. Some of the content in the standards is unnecessary (at least 35 percent, and up to 50 percent). There may be a few critical shortcomings (as listed above) although the standards contain no serious errors. 3: Standards fall short in one or more of the following ways: At least 50 percent and up to 65 percent of crucial content is missing. At least 50 percent and up to 65 percent of the content in the standards is unnecessary. There are serious problems or shortcomings or errors in the standards, although (the standards have some redeeming qualities and there is some evidence of rigor). 2: Standards fall short in one or more of the following ways: At least 65 and up to 80 percent of crucial content is missing. At least 65 percent and up to 80 percent of the content in the standards is unnecessary. There may be several serious problems, shortcomings, or errors (as listed above)."}, {"section_title": "18", "text": "PART 2 > GRADING 1: Standards fall short in one or more of the following ways: At least 80 percent of crucial content is missing. At least 80 percent of the content in the standards is unnecessary. There are numerous problems, shortcomings, or errors (as listed above). 0: Standards fall short in one or more of the following ways: The content of the standards does not address or barely addresses the subject-specific content expectations. The content is poorly chosen and fails to provide the level of rigor appropriate for the targeted grade level(s). Content is full of problems, shortcomings, and errors (as listed above)."}, {"section_title": "CLarity and SpeCifiCity", "text": "3: Standards are coherent, clear, and well organized. The scope and sequence of the material is apparent and sensible. They provide solid guidance to users (students, teachers, curriculum directors, test developers, textbook writers, etc.) about the content knowledge and skills required to do well on the exam. The right level of detail is provided. The document(s) are written in prose that the general public can understand and are mostly free from jargon. The standards describe things that are measurable (i.e., can lead to observable, comparable results across students and schools). The standards as a whole clearly illustrate the growth expected through the grades. 2: The standards are somewhat lacking in coherence, clarity, or organization. The scope and sequence of the material is not completely apparent or sensible. The standards do not quite provide a complete guide to users as to the content knowledge and skills required to do well on the exam (i.e., as a guide for users, there are shortcomings that were not already addressed by the content and rigor score). The standards provide insufficient detail. The prose is generally comprehensible but there is some jargon and some vague or unclear language. Some standards are not measurable. 1: The standards are somewhat coherent, clear, and organized. They offer limited guidance to users (students, teachers, curriculum directors, textbook writers, etc.) about the content knowledge and skills required to do well on the exam, but there are significant shortcomings (as a guide for users) that were not already addressed by the content and rigor score. The standards are seriously lacking in detail, and much of their language is vague enough to leave unclear what is being asked of students and teachers. 0: The standards are incoherent and/or disorganized. They are not helpful to users. The standards are sorely lacking in detail. Scope and sequence is a mystery."}, {"section_title": "finaL gradeS", "text": "A final grade for each set of standards is calculated by adding the \"content and rigor\" score to the \"clarity and specificity\" score. . Though the mathematics standards reviewed herein are not the final documents, they are the first to be released to the public for general review and comment. The standards are designed to \"define the knowledge and skills students should have to succeed in entry-level, credit-bearing, academic college course and in workforce training programs.\" As such, they are \"end of high school,\" or, more accurately, \"college readiness\" standards. The standards document is twenty-three pages, but the actual standards are broken down into ten content areas (such as number, equations, statistics), each neatly contained on one page and, on average, having five explicit \"core skills\" (i.e., standards, usually with some explanatory notes). With each content area there is a brief listing of \"core concepts\" that give an overview and a few paragraphs of commentary explaining what a \"coherent understanding\" of the area requires."}, {"section_title": "Introduction", "text": "The National Assessment of Educational Progress (NAEP) is known as \"the nation's report card.\" NAEP mathematics exams are administered at grades four, eight, and twelve. The Mathematics Framework is seventy-four pages long and includes an overview of the exam, a description of the test design, sample exam items, and item formats, as well as a detailed list of content items-in essence, standards-for grades four, eight, and twelve. The standards are organized by strand (such as Measurement and Geometry), with grade levels placed side-by-side to show how they should develop from fourth to eighth to twelfth grade. Breakdowns are given of the percentage of exam items for each strand. Sample exam items appear throughout the framework to illustrate some of the standards and clarify aspects of the test design.\nLike the 2009 Reading Framework, the 2011 NAEP Writing Framework proposes a workforce-oriented rationale for its contents. The framework explains that \"the impact of communications technologies has changed the way people write\" and asserts that \"writing in the 21 st century is defined by its frequency and efficiency\" rather than by its style, elegance, or even persuasiveness. More specifically, the framework has the following goals: To encourage student writers to move beyond prescriptive or formulaic approaches in their writing. To assess grade eight and twelve students' writing using word processing software with commonly available tools. To measure students' ability to respond to a writing task in an on-demand scenario. In short, the framework emphasizes students' ability to make choices about how to respond to a writing task-sometimes using a computer-and do it quickly."}, {"section_title": "Review", "text": "The NAEP mathematics framework is evaluated on two dimensions: \"content and rigor\" and \"clarity and specificity.\" This review addresses these dimensions in several sections below. First, the framework's content is compared with the content that should be covered by mathematics tests for fourth-, eighth-and twelfth-graders. (See \"Math Content-Specific Criteria,\" page 11.) Separate sections describe the content covered by the NAEP framework for each grade level, as well as what is missing from and problematic about it. The next section sums up the content and rigor of the NAEP framework across grade levels. Then the review appraises the clarity and specificity of the NAEP framework as a whole, and the last section offers an overall summary and final grade. In brief, most of the important content that we would expect students to learn is covered by the NAEP mathematics standards. But since the framework includes nearly 300 standards, it is impossible to distinguish the important standards from the less important ones. (For high school algebra, for instance, the heart of algebra-representing the abstract with symbols-is inappropriately deemphasized, while equal status is given to several non-symbolic approaches.) In addition, some important content is missing, and for other content that is included, inadequate detail is provided. A majority of the standards also have minor or major problems with clarity-meaning they are awkward, verbose or imprecise, or they contain meaningless phrases or verbs that do not make clear mathematical sense.\nThe TIMSS math framework is evaluated on two dimensions, \"content and rigor\" and \"clarity and specificity,\" and this review addresses these dimensions in several sections below. First, the content covered by the TIMSS framework is compared with the content that should be covered by a mathematics test for fourth-and eighth-graders (see \"Math Content-Specific Criteria,\" on page 11). A separate section describes the content covered by the TIMSS framework for each grade level, as well as what is missing from and problematic about it. The next section sums up the content and rigor of the TIMSS framework across both grade levels. Then the review appraises the clarity and specificity of the framework as a whole, and the last section offers an overall summary and final grade. In sum, the TIMSS standards are a model of clarity. Though they have a few gaps in content, the standards generally cover what is there well and provide sound guidance to readers.\nThe PISA framework is evaluated on two dimensions, \"content and rigor\" and \"clarity and specificity,\" and this review will address these dimensions under four sections. First, the content addressed by the PISA materials is compared with the content 7 The 2009 PISA Mathematics Framework is scheduled to be released by the end of 2009. We plan to review it in a subsequent report. \n\"Reading Informational and Literary Texts,\" \"Writing,\" and \"Speaking and Listening\" are the three strands identified in the Core Standards. Each of these strands is divided into two sections: 1) \"standards for range and content\" and 2) \"standards for student performance.\" The document also includes standards for two additional strands, \"Research\" and \"Media,\" combined in a section called \"Applications of the Core.\" Each strand is reviewed in the \"content and rigor\" section that follows. Using the \"English Language Arts Content-Specific Criteria\" for the upper grades (see page 15) and the \"Common Grading Metric\" (see page 16), a sub-grade for content and rigor is included at the end of that section. Following the \"content and rigor\" section is a similar discussion of the document's clarity and specificity (with a sub-grade also awarded). The review ends with a summary and final grade for the document.\n\"Reading\" and \"Writing\" are the only strands of English Language Arts assessed by NAEP, and each has its own framework. Thus, the reviews of each framework are presented separately here, with individual grades assigned. The 2009 Reading framework is addressed first, with a discussion of \"content and rigor\" that is organized into two subcategories: \"Coverage of Text Types\" and \"Types of Reading Skills\" assessed. A discussion of \"clarity and specificity\" follows; the review ends with a summary and final grade. The Writing Framework is organized in similar fashion except there the \"content and rigor\" section is subdivided into two categories titled \"Purposes for Writing\" and \"Features of Writing.\" Both Frameworks are evaluated against their stated purposes. That is, the NAEP Reading Framework is assessed as a \"reading\" framework only. It does not address writing, communication skills, research, media, or language, all worthy components of a robust English Language Arts curriculum-but not the territory that NAEP here seeks to occupy. Similarly, the NAEP Writing Framework is evaluated only as a \"writing\" framework. Therefore, the reading and writing criteria of the \"English Language Arts Content-Specific Criteria\" (see page 13) are the sole criteria applied to the respective reviews. Readers should bear in mind that the purpose of NAEP frameworks is to delineate the parameters and content of large scale, matrix sample assessments. In matrix sampling, no one student takes the whole test. Each test-taker responds to different sets of questions, which limits testing time yet still provides a picture of general student performance on a broad range of content and skills.\nThis review appraises PISA as a \"reading\" framework only, not as a framework for the entirety of English language arts. Missing from PISA are standards for writing, communication skills, research, media, and language-all worthy components of a robust English language arts curriculum, but apparently not within PISA's scope. Therefore, because PISA is a reading literacy assessment framework, the reading components of the \"English Language Arts Content-Specific Criteria\" are the only criteria used to evaluate it (see page 13). The PISA framework is evaluated on two dimensions, \"content and rigor\" and \"clarity and specificity.\" The content and rigor discussion is subdivided into four sections that mirror the organization of the PISA framework: 1) defining the conceptual basis for PISA's definition of reading literacy; 2) the organization of the reading literacy domain; 3) the types of skills required for addressing the tasks; and 4) task excerpts. Next, the clarity and specificity of the framework are appraised as a whole and the concluding section provides an overall summary and final grade. In short, although assessment frameworks must include good direction for test developers, the PISA framework is confusing in its detail and prioritizes the evaluation of metacognitive reading \"strategies\" over reading comprehension content."}, {"section_title": "Fourth Grade", "text": "\n"}, {"section_title": "Content Covered", "text": "Whole number arithmetic (and problem solving using whole number arithmetic) should make up the bulk of learning in mathematics in grades one through four. In the NAEP mathematics framework, there are standards that cover whole number arithmetic and its application to problem solving. The measurement of lengths in inches and meters, and the computation of perimeters and areas of rectangles, are also covered, as is problem solving. The core material requires a thorough understanding of linear and quadratic equations and functions. Both linear and quadratic equations come in multiple forms, and being able to algebraically (i.e., symbolically) translate between the various forms is important. Most of the linear material is covered by the eighth-grade standards, but there is some new material in this standard: Create and translate between different representations of algebraic expressions, equations, and inequalities (e.g., linear, quadratic, exponential, or trigonometric) using symbols, graphs, tables, diagrams, or written descriptions. This would also seem to cover much of quadratics. Eight standards in all mention quadratics, including standards covering inequalities. An important part of the study of quadratic functions is understanding that the graph is symmetrical and learning to locate the maximum or minimum of the function. This opens up a whole new world of problems that can be solved and requires solid technical work that allows a student to transform the standard form of a quadratic function into its vertex form. The closest standard to this (albeit one with inadequate detail) reads as follows: Recognize and analyze the general forms of linear, quadratic, rational, exponential, or trigonometric functions. Roots and exponents are already covered in the eighth-grade framework. Logarithms, exponentials, and basic trigonometric functions are all covered. Probability and statistics, including combinations and permutations, are more than adequately covered.\nWhole-number arithmetic and problem solving should make up the bulk of learning in grades one through four mathematics; both are addressed. The measurement of lengths and the computation of perimeters and areas of rectangles are also covered. \nGrades five through eight should thoroughly cover the arithmetic of rational numbers, including decimals, rates, ratios, proportions, and percentages. With the exception of rates, this material is well covered. In addition, the content standards cover similar triangles, perimeters, volumes, circles, and applications of the Pythagorean Theorem (in the geometry section). Problem solving is included throughout. Basic data analysis, statistics and probability, and basic linear equations are also well covered.\nAny mathematics assessment for fifteen year-olds should thoroughly cover the arithmetic of rational numbers, including decimals, and should also cover rates, ratios, proportions, and percentages. Fractions or rational numbers are not mentioned anywhere, but we do find that understanding the meaning of operations \"includes the ability to perform operations involving comparisons, ratios and percentages.\" The standard covering \"number sense\" does explicitly include proportional reasoning. Under the content category \"quantity,\" we find some standards that are potentially relevant: \"understanding the meaning of operations\" and \"elegant computations.\" These are much too vague, however, to assure coverage of the arithmetic of rational numbers. Typically, standards are self-explanatory. However, PISA standards require one to read the lengthy prose surrounding them to gain some insight into their meaning (more on this below). Volume, area, and perimeter are mentioned under the category \"quantity,\" and triangle appears in the preliminary discussion and in an example problem. \"Similar\" occurs in the discussion, but it is not the technical similarity of geometry. Coordinates are also mentioned in the discussion. The most guidance we get in the \"space and shape\" content category is from the rather vague standard: \"recognizing shapes and patterns.\" This, in principle, could cover much of the material of geometry, but the standard is far too general to be useful. Students should have covered a full year of algebra by age fifteen. Under the content category \"change and relationships,\" we have two potentially relevant standards: \"representing changes in a comprehensible form\" and \"understanding the fundamental types of change.\" These do not give clear guidance to readers, though. In the accompanying discussion, we find absolute value, linear equations, and inequalities and linear functions. Note that the discussion material included in the PISA framework is not an obvious attempt to clarify individual standards. Rather, it is a very general discussion about the uses of mathematics in the real world. Here is an example: Every natural phenomenon is a manifestation of change and the world around us displays a multitude of temporary and permanent relationships among phenomena. Examples are organisms changing as they grow, the cycle of seasons, the ebb and flow of tides, cycles of unemployment, weather changes and stock exchange indices. Some of these change processes involve and can be described or modeled by straightforward mathematical functions: linear, exponential, periodic or logistic, either discrete or continuous. But many relationships fall into different categories and data analysis is often essential to determine the kind of relationship that is present. Mathematical relationships often take the shape of equations or inequalities, but relations of a more general nature (e.g. equivalence, divisibility, inclusion, to mention but a few) may appear as well. Though PISA is given credit here for covering linear equations and inequalities, this credit might be considered quite generous. For one does not find explicit guidance about content so much as suggestions that certain content should be covered-linear equations for example."}, {"section_title": "Content miSSing", "text": "Among the content missing are geometric constructions. Explicit reference to the various forms of linear and quadratic equations, and translations between them, are also missing. Despite there being 120 standards for twelfth-grade mathematics (eight involving quadratics), significant details are missing, such as completing the square, the symmetry of the graph, and finding the maximum or minimum.\nSome important topics are missing, including the single-digit addition and multiplication facts and the corresponding subtraction and division facts; the inverse nature of addition and subtraction and of multiplication and division; the precision of measurement; the basic properties of operations such as commutativity and associativity; and elementary probability. In addition, though the sample questions use some of the addition, multiplication, subtraction, and division facts, the facts included are rudimentary.\nBoth rates (mentioned above) and absolute value are missing.\nThe core content of the arithmetic of rational numbers is missing, as is any coverage of rates. Most of the expected geometry is missing. There are no similar and congruent triangles; circles; angles associated with triangles and parallel lines; or computation of areas and perimeters of rectangles. The Pythagorean Theorem is missing. Roots, reciprocals, and powers are absent, as is the arithmetic of polynomials and rational expressions. Factoring is also missing. Quadratics are not mentioned. "}, {"section_title": "Content probLemS", "text": "The power of algebra is in its abstraction: the ability to reduce problems to symbols and solve them by manipulating the symbols. There are twenty-seven standards in the algebra strand for twelfth grade; however, algebra as abstraction, with the use of symbols, is not the emphasis. Consider again this twelfth-grade standard: Create and translate between different representations of algebraic expressions, equations, and inequalities (e.g., linear, quadratic, exponential, or trigonometric) using symbols, graphs, tables, diagrams, or written descriptions. The objects under discussion are symbolic: algebraic expressions, equations, and inequalities. Yet the symbolic approach to algebra is only one of five approaches (along with graphs, tables, etc). This de-emphasis of the symbolic misrepresents the nature of algebra.\nThe standards on patterns are easily misinterpreted. One reads, \"Extend patterns and find missing terms in them.\" Math problems addressing this overly nebulous standard must be carefully worded to avoid having multiple possible ways to extend a pattern.\nThe eighth-grade standards for patterns-like the standards for patterns in fourth grade-are also open to misinterpretation. Again, problems associated with these standards must take care to avoid having multiple possible solutions.\nPISA's content category \"uncertainty\" covers \"collecting data, data analysis and display/visualisation, probability and inference.\" It states: Relatively recent recommendations concerning school curricula are unanimous in suggesting that statistics and probability should occupy a much more prominent place than has been the case in the past. Although publications from 1982 to 2000 are cited to support this sentiment, \"unanimous\" is a dramatic overstatement. In the sidebar on data analysis, statistics, and probability, \"How much DASP do students need?\" on page 12 of this report, the case is made that these content areas should receive only limited attention. Simply put, there is an overemphasis on this content category in the PISA standards."}, {"section_title": "Content and Rigor Conclusion", "text": "The content covered by the NAEP standards at the fourth-, eighth-, and twelfth-grade levels would normally earn the standards a score of six; coverage is good, though there are some missing items. There are significant flaws in both the eighth-grade and PART 3 > REVIEWS > MATH > NAEP twelfth-grade standards, but the primary problem with the NAEP framework is its failure to set priorities. With nearly 300 standard-style items, all presented as equal, there is no way for the reader or user to determine which are the most important. Minor standards appear as equal to major ones. This is particularly evident in the algebra strand, where the words and phrases tables, diagrams, graphs, and written descriptions appear fifty-seven times while the more important focus for algebra, the symbolic, only appears thirteen times. Secondary content overshadows primary content simply because the secondary content appears so much more often. According to the \"Common Grading Metric\" (see page 16), a set of standards cannot score above a five for content if they \"do not distinguish between more important and less important content and skills.\" Consequently, a score of five is indicated.\nThough the content in both grades four and eight is covered nicely, there is just enough missing-and the pattern standards are problematic enough-to give the framework a content and rigor score of six. (Recall that a six means that no more than 20 percent of important content may be missing.)\nArithmetic, most of the geometry, and much of the algebra that is listed in the \"Math Content-Specific Criteria,\" (page 11) are missing from the PISA framework. More than half of the important content is never mentioned, which would normally result in a score of three for content and rigor-that is, if the framework were all that's being evaluated. However, the inclusion of the released items affects PISA's grade for content and rigor. Even though there are very few geometry problems in the released items, those that do appear require more geometry content than the standards suggest. This raises PISA's content and rigor score from three to four as it now appears that at least 50 percent of the content is covered, but certainly more than 35 percent is missing (see the \"Common Grading Metric,\" page 16).\nThese standards reflect laudable attempts to identify certain cross-disciplinary skills (typically taught in English classes) and explain why they are necessary for success in college and the workplace. The skills described are mostly reasonable but, because the standards focus on skills, some crucial content-particularly in the essential area of reading-is left unaddressed or is poorly addressed. The skills statements in reading try too hard to serve two masters, informational and literary text, and end up serving neither effectively. Although strong exemplars of the quality and complexity of expected reading are included, they function as suggestions rather than as requirements. This places a very heavy burden on others-including those who will develop the assessments needed to give these standards traction-to establish the nature, quality and content of what should actually be read (and how well). For these reasons, the standards earn a five for content.\nWhile the framework serves its designated purpose well, namely to be a fair starting point for assessing basic reading comprehension at fourth, eighth, and twelfth grades, it does not do four things that Americans should expect academic standards in reading to do: 1) address expectations for essential early reading skills; 2) detail more explicit aspects of vocabulary development; 3) prioritize students' exposure to outstanding American literature; and 4) detail choices about the quality and complexity of reading passages. The framework is missing crucial content by not placing enough emphasis on literature and English language conventions. The framework could also cover the content more carefully by distinguishing the skills targets at each grade span, as well as making them more rigorous and grade appropriate. These revisions would also help practitioners distinguish between more and less important content. Since the framework falls short in these ways, it earns a grade of five (see \"Common Grading Metric,\" page 16).\nSome of the content in the NAEP Writing Framework is covered satisfactorily. Specifically, it does an adequate job of identifying the appropriate purposes for writing and placing reasonable emphases on each purpose at the various grade levels. While the majority of the appropriate content is covered by the standards, however, it is shortchanged in important ways. First, more attention to conventions would help make the framework as rigorous as it should be. Second, since the rubrics repeat much of the same broadly-worded expectations from grade to grade, it is difficult to distinguish between more and less important content. It is also possible that the standards need to be more or less rigorous than they are at certain grade levels (i.e., expectations are too high or too low) but, given the one-size-fits-all expectations, it is impossible to tell. Because of these limitations, the writing framework receives an overall content and rigor grade of five (see \"Common Grading Metric,\" page 16). The writing framework scores rather well against our criteria for clarity. The guidelines for writing tasks are straightforward and coherent, and, as mentioned above, the sample prompts help clarify expectations. That said, illuminating a scope and sequence of writing skills by grade span would provide better guidance to educators. More specificity at these important benchmark grades would also help indicate what expectations should look like in the intervening grades. The framework's generic scoring rubrics could be strengthened by making them specific to a prompt. The framework could also include sample \"anchor papers\" (i.e., examples of student responses that are used to train scorers) so that practitioners can understand how the rubrics are applied (e.g., whether one area of the rubric appears to be more heavily weighted in scoring than another). Finally, rather than simply sketching the purposes for writing and offering generic rubrics for scoring the items, the framework could offer a matrix of more specific writing expectations by grade level, as it does for reading. It might mention the kinds of sentence structures, transitions, and stylistic devices it expects to see in writing samples at each grade span. Without offering sample acceptable responses at each cut score, it is difficult to determine the actual level of rigor of the items. This lack of specificity results in a two for clarity and specificity. Grade: NAEP Writing Policymakers could easily draw upon the conceptual frameworks delineated by NAEP for both reading and writing when developing national or state standards for English language arts. However, any such effort must keep in mind that actionable standards (at whatever level they are developed) must ultimately address with specificity all the domains of English language arts, and at each grade level, in order to be useful. Such an effort must, in particular, focus more than these frameworks do on literature (especially outstanding U.S. literature) and English language conventions. Using these NAEP frameworks as models for state standards might maximize students' performance on NAEP. But it may also lead to an English curriculum that slights both literature and English language conventions, jeopardizing students' chances of developing a strong foundation for a broad liberal arts education.\nAs an assessment of \"real world\" reading literacy, PISA does not meet many of the criteria that characterize rigorous English language arts standards (see \"English Language Arts Content-Specific Criteria,\" page 13). Even judged purely as a reading framework, which is how it has been assessed here, crucial content is missing: The PISA framework does not address vocabulary development, offer specific expectations for reading and analyzing literary and non-literary texts, include specific expectations for the correct use of English language conventions, recognize the importance of literary heritage, nor sufficiently define the amount, quality, and complexity of literary and informational texts. Further, much of the content of the PISA framework, focused on metacognitive strategies for accessing information, does not address reading comprehension and arguably doesn't belong there. This means that roughly 50 percent of reading content is missing, resulting in a score of three for content and rigor (see \"Common Grading Metric,\" page 16)."}, {"section_title": "Clarity and Specificity", "text": "The structure of the NAEP framework is acceptable. It shows the progression of content from fourth through twelfth grade. It is organized by content strand. However, it has a number of problems with clarity and specificity, most pertaining to nonmathematical verbs and unclear language.\nOverall, organizing the framework into multiple content domains for both the fourth-and eighth-grade standards works well, even though the domains are not the same for both grades. The number of standards is also appropriate-thirty-eight for fourth grade and fifty-five for eighth grade. Further, the content domains are organized into helpful sub-domains; for example, \"Number\" in the grade four standards is divided into four sections: whole numbers, fractions and decimals, number sentences, and patterns and relationships. This clarity of organization is complemented by clarity of language. In other words, almost all the standards are clearly written and easy to understand. Consider some examples from fourth-and eighth-grade math: Solve problems involving proportions. Compute with fractions and decimals. Solve problems involving percents and proportions. Though there are a few awkwardly phrased standards, they are the exception. The standards for both grades four and eight are models of clarity and appropriate balance. They focus on important material and ignore unimportant material. They provide excellent guidance to students and teachers. They easily merit a score of three.\nThe actual standards included in the PISA framework are non-specific and not testable. They give almost no guidance to readers and users-i.e., teachers, students, parents, curriculum designers, test makers, textbook developers, standards writers, policymakers, or others. The discussions around the standards are rambling accounts of mathematics in the real world, and although they mention various bits of content (for which coverage has been generously credited in this review), these discussions, too, give scant guidance to readers at any level. PART 3 > REVIEWS > MATH > PISA According to the \"Common Grading Metric\" (page 16), a score of zero for clarity and specificity means: The standards are incoherent and/or disorganized. The standards are not helpful to users. The standards are completely lacking in detail. It is as if the PISA framework were written to illustrate the kind of standards that should merit a score of zero. Accordingly, its score for clarity and specificity score is zero.\nThe core standards are mostly succinct, and their organization is relatively straightforward, but they lack coherence and clarity. The prose is generally comprehensible, but it is vague or convoluted in many places, leaving student and teacher expectations unclear. One primary example is the use of the word \"standards\" to refer to statements that are not standards, as is consistently the case with the \"standards for range and content.\" These statements are mostly rationales for why certain reading genres or characteristics of writing are important. Instead of outlining content in the genres, as they could, the developers suggest when and in what contexts graduates might need to call on certain reading, writing, speaking and listening skills. Two revisions could address this shortcoming: 1) strengthen the \"standards for range and content\" by including more specific content knowledge and 2) combine these new-and-improved content standards with the skills described in the student performance sections. In some places, the language of the performance standards (skills) could be much more specific and therefore clearer. For example, performance standards for reading include these three standards, back-to-back: (3): Discern the most important ideas, events, or information, and summarize them accurately and concisely. (4): Delineate the main ideas or themes in the text and the details that elaborate and support them. (5): Determine when, where, and why events unfold in the text, and explain how they relate to one another. The subtle differences and overlaps among these standards make it difficult to discern the gist and priority of each. More precise \nThe reading framework scores fairly well against our criteria for clarity. It is generally clear and usable. However, as noted above, it could be more specific and detailed in ways that would also improve the content. Despite its general clarity, the persistence of some vague language in the framework makes it difficult to give it the highest marks. For example, the definitions of the text types \"argumentation\" and \"persuasion\" are confusing. The framework implies that argumentation must, by definition, be persuasive text, and it links the two types together as one sub-category of informational text. Many practitioners, however, would distinguish between presenting an \"argument\" as a matter of exposition (i.e., making known one's position on an issue or one's interpretation of a text) and a persuasive essay or speech (i.e., attempting to sway the reader's opinion, often by including a call to action). The conflation of the two here is likely to confuse practitioners who have been teaching that not all arguments must be persuasive. The need for more specificity where the content is concerned and the confusion surrounding some definitions result in the framework earning a grade of two. Grade: NAEP Reading CONTENT CLARITY TOTAL\nThe 2009 PISA Reading Literacy Framework is both dense and murky. It includes multiple (and unnecessary) levels of detail about task types. Although these descriptions might be helpful to test item writers, they are not particularly useful as guides for developing national or state standards or for informing curriculum and instruction, even for the age level targeted by PISA (fifteen year-olds). Without any graphical explanation, these multiple layers of description make the framework very difficult to follow. Although the tasks themselves are described with excessive detail, the sample items are incomplete, making it difficult to understand exactly what is expected of students. Because much of the language in the framework does not provide sufficient guidance to users nor detail about what is expected of students, it receives a score of one (see \"Common Grading Metric,\" page 16)."}, {"section_title": "non-mathematiCaL verbS", "text": "The NAEP framework is expressly designed to prepare students for the NAEP math exams, yet words like identify, interpret, represent, and recognize occur well over 100 times. These are not words that would be used in a math problem; indeed, examples from the NAEP exam never start with a request for the student to identify, interpret, represent, or recognize. These words are broadly misused in the standards. On the other hand, mathematical words such as compute and algorithm never appear and calculate occurs only twice."}, {"section_title": "unCLear Language", "text": "A majority of the standards include awkward, verbose or imprecise, or meaningless phrases. For example, \"Use ratios to describe problem situations\" instead of \"Use ratios to solve problems.\" A grade four standard reads, \"Explore properties of paths between points.\" What this means for the fourth grade is not made clear. A twelfth-grade standard says, \"Evaluate algebraic expressions including polynomials and rational expressions.\" What does \"evaluate \ufffd x 2 \" mean? One can evaluate the function at a given value for x, but \"evaluate a polynomial\" makes no sense mathematically. While the problems with unclear language in individual standards can be minor, the general effect is cumulative and quite negative."}, {"section_title": "Clarity and Specificity Conclusion", "text": "Although the standards offer some guidance about the content to be covered by the assessments, the lack of clear priorities is a serious problem for the standards. Students and teachers will not be able to determine which content is important. What's more, the language used in a majority of the standards makes reading them difficult. The fact that the standards do not provide clear guidance to users results in a score of one for clarity and specificity."}, {"section_title": "Summary and Grade", "text": "The NAEP mathematics standards contain most of the important content we would expect students to learn, but it is hidden within the excessive number of standards (nearly 300). Less important material, such as data analysis, statistics, and probability, receives greater emphasis than more important material like symbolic algebra. Throughout the document, important content standards are bestowed the same status as secondary standards. Often standards containing crucial content are written in a way that also includes minor content, which makes it hard to identify core content. Over half of the standards would benefit from either major or minor rewriting. Many standards include unnecessary verbiage, mathematically meaningless language, or just a lack of focus. The other major problem-the lack of clear guidance about which content is more important and which is less-is not easy to correct, especially if such a large number of standards are to be retained. The Trends in International Mathematics and Science Study (TIMSS) conducts international comparisons of student achievement in mathematics and science. A project of the International Association for the Evaluation of Educational Achievement, TIMSS has administered math and science tests in roughly forty countries every four years since 1995. The assessment framework reviewed here is a 170-page document covering both mathematics and science, and provides some background information about the tests. Our focus is the nearly thirty pages that cover the TIMSS mathematics standards for the fourth-and eighth-grade tests, and the sample math test items in the framework's Appendix B. The mathematics framework is organized around content domains (the subject matter to be assessed-i.e., standards) and cognitive domains (\"the behaviors expected of students as they engage with the mathematics content\"). The content domains are listed separately for fourth and eighth grades, while the cognitive domains are discussed for both grades simultaneously. The fourth grade has three content domains: number (50 percent), geometric shapes and measures (35 percent), and data display (15 percent). The cognitive domains include knowing (40 percent), applying (40 percent), and reasoning (20 percent). (The numbers listed here represent the percentage of test items falling under each domain.) The eighth grade has four content domains: number (30 percent), algebra (30 percent), geometry (20 percent), and data and chance (20 percent). The cognitive domains include knowing (35 percent), applying (40 percent), and reasoning (25 percent).\nThe TIMSS standards for both fourth and eighth grade are clear, coherent, and well organized. They provide solid guidance to all readers and users, from students to textbook writers; they are also measurable and burdened by very little jargon. They cover nearly all of the required content, certainly over 80 percent, at appropriate levels of rigor. Excellent decisions have been made about what content to include and exclude, and there is little ambiguity about what is expected. Minor corrections to the pattern standards and the inclusion of some missing standards would raise the score for TIMSS to a perfect ten. Though the TIMSS architects do not have to address the more complicated world of twelfth-grade standards, standards for fourth and eighth grade are still difficult to do well-and here they have been handled quite admirably.\nThe PISA assessment tests mathematical literacy, not knowledge and understanding of grade-level mathematical content. The standards and their explanations do not cover the appropriate grade-level material and the released items indicate that the exam is quite weak in mathematical content. It is a problem-solving test, and although mathematics is used, it is somewhat incidental. Many problems have no apparent mathematical content at all and are, at best, small logic puzzles. Because of this low level of required content knowledge, the claim that PISA tests \"preparedness for further study\" is in doubt. The test itself is unbalanced, with glaring overemphasis on data display. Most of the content that is expected of a fifteen year-old in PISA is what younger students should have already mastered. As a serious problem solving test using elementary mathematics, the PISA assessment might function nicely, although its unbalanced nature would limit its usefulness here. Certainly schools should teach problem solving in their mathematics curricula, and it should be a major embarrassment for a country to perform poorly on this test. Still, results from PISA ought not to be used to interpret how successful a school system is at imparting grade-level mathematical knowledge and understanding, nor are the PISA framework and released items a suitable model for U.S. standards setters at any level. Developed by representatives from the ACT, the College Board, and Achieve, Inc. for the National Governors Association and the Council of Chief State School Officers, the Common Core standards identify \"essential college-and career-ready skills and knowledge in reading, writing, and speaking and listening across the disciplines\" for students at the end of high school. 8 The authors suggest that \"students who meet these standards will have the\u2026skills to flourish in the diverse, rapidly changing environments of colleges and careers.\" In many ways, these standards successfully define a set of college-and career-ready skills in the relevant domains. They outline expectations in strands typically represented in state English standards: reading, writing, and speaking and listening. They include a section called \"Applications of the Core,\" which defines standards for research and media; this is also common in state English standards. Despite these familiar strands, however, the authors acknowledge \"that the responsibility for teaching such skills must also extend to the other content areas,\" suggesting that these college-and career-ready skills are really designed to be cross-disciplinary. The draft standards do not profess to encompass the full content of English standards, including literature, as most state standards do. Hence we have evaluated them only in the skill domains that they profess to cover (reading, writing, speaking and listening, and the applications of research and media). The document does, however, include standards (in the reading strand) for reading literature and analyzing literary text, so we have examined that treatment. In reading (and to a lesser extent in other strands), the standards tend to shortchange the importance of content knowledge; instead, they function primarily as utilitarian, instrumental, broadly worded, skills-centered goals that are subject to multiple interpretations. The introduction assures readers that the standards \"should be accompanied by a comprehensive, content-rich curriculum\" but appears not to recognize that a state's English language arts (ELA) standards serve as the foundation of such a curriculum. Though the drafters intend that states and other users will indeed supplement these standards with ample, high quality content (and in several places signal that intention), this important responsibility is explicitly left for others to shoulder. Indeed the introduction states that \"many important decisions about curriculum will necessarily be left to states, districts, schools, teachers, professional organizations, and parents.\" Good curricula could be developed from these standards, yet poor curricula could just as easily be developed because the standards leave so much room for interpretation. One very helpful addition are the text \"exemplars\" included \"to illustrate the level of complexity that college-and career-ready students should be able to handle independently by the end of high school.\" They are, for the most part, well-chosen and indeed illustrative of a good ELA curriculum. In spite of this useful addition, however, no required or even suggested list of titles or authors is provided. The standards alone are not specific enough to provide sufficient guidance to curriculum developers and One is left to hope that others will indeed fill that troublesome void with the quality content that the standards-drafters envision but do not itemize.\nThese standards do a praiseworthy job of defining essential competencies in reading, writing, and speaking and listening to succeed in both college and the workplace. The authors are to be commended for not falling prey to spurious postmodern theories that disavow close reading and encourage interpretations of a text based solely on how it makes the reader feel. Further, the document properly acknowledges that essential communication skills must be embraced and addressed beyond the English classroom, which could lead to valuable collaboration among teachers and more consistent expectations across subjects. Although it is not the stated goal of these standards to do more than delineate \"college-and career-ready skills and knowledge,\" there is a risk that they will become de facto English standards in the states that adopt them. Requiring states to ensure that these standards comprise as much as 85 percent of the state's English standards would, if obeyed, imbue this document with responsibility to do more than just outline utilitarian skills that are directly applicable in some college courses and in the workplace. 13 Perhaps because they anticipated this reality, the authors have included standards for reading and analyzing literary texts. Indeed any proper state standards for English must do that and more. They must provide enough curricular guidance to help teachers instill not just useful skills, but also imagination, wonder, and a deep appreciation for our literary heritage. In this way, a strong foundation in English language and literature is an essential component of a content-rich, liberal arts curriculum, which is necessary for the preservation of a free society and robust culture. State standards should help cultivate not only successful college students and workers, but \"citizens,\" as Dana Gioia has suggested, \"who are awakened not only to their own humanity but also to [the] many possibilities of the human world they are about to enter.\" 14 Despite their many good qualities described above, these skills-based competencies do not yet serve as a strong framework for states to build the robust liberal arts curricula that will prepare their young people to thrive as citizens in a free society. States adopting these core standards must, therefore, be very careful about how they supplement them so as to achieve that goal. and work 16 -rather than also recognizing their vital role as the foundation of a robust liberal arts curriculum, for responsible adulthood, and for competent citizenship. Both NAEP frameworks seek to connect the content and skills they delineate to their ostensible application in the postsecondary worlds of college and work. While it is true-and important-that students should be prepared to succeed in college and the workplace, not all valuable educational experiences, such as the analysis of great literature, will have a direct or perpetual application in the workplace. (Some aren't necessarily applicable in college, either. A student may never take a \"poetry course\" in college, yet still should have learned to read poetry in high school.) Many things learned in school but not immediately useful after high school nevertheless remain essential for developing a student's cultural literacy, capacity for empathy, understanding of history, and potential for effective citizenship. Fortunately, these attempted connections to real world applications appear to function largely as rationales and do not completely eviscerate the substance of the NAEP assessment frameworks, which remains largely intact compared with previous iterations.\nPolicymakers who assume that poor U.S. performance on PISA means that we should make U.S. standards and curricula more PISA-like are making a risky leap. While we should not ignore the results of the PISA reading assessment, we must be cautious about the inferences we make from it, ever mindful that this test and its framework are not-nor do they profess to be-a model for reading standards, much less a complete English language arts curriculum. Like many, we're eager to get high-quality common standards into the hands of state, district, and local officials. We're eager for teachers to start using them and for youngsters to benefit from their use. Truth be told, we're also eager for our kids-and the adults they will become-to give those in Singapore, Hong Kong, and Finland a run for their money. But we won't allow eagerness to ruin a fifty-years-in-the-making opportunity. It's imperative that America get this done right. That means that states need to do their part to amplify the literary components of the Common Core Standards in reading, writing, listening and speaking. It also means that we must grapple with how to make these common standards stick and get traction-stick as an organizational entity, stick with the post-secondary and workplace worlds, and stick with policymakers, parents, principals, and teachers. This interim report finds that in late 2009 the United States has got some bright stars by which to navigate (as well as some solar dust to avoid). We need to keep our sights set on them even as gravitational pulls seek to lead us astray. But, done right, this could become quite a cosmos. The College and Career Readiness Standards for Reading, Writing, Speaking and Listening, and for Mathematics, currently in draft form, are products of the Common Core State Standards Initiative (CCSSI)-a joint project of the National Governors Association (NGA) and Council for Chief State School Officers (CCSSO), in collaboration with Achieve, ACT, and the College Board. In June 2009, forty-seven states, two territories, and the District of Columbia agreed to work together to develop a set of shared or common (some would say national) academic standards for the U.S. (One additional state joined in September, bringing that total to fifty-one.) This is a privately-funded, state-led effort-and the resulting standards are voluntary. What iS the purpoSe of the Common Core StandardS? CCSSI, recognizing the drawbacks of inter-state variation and the consequences of poor-quality standards in some states, is developing common standards that they believe will equip students to be career and college ready. \"It is time for us as states to challenge the education system and finally answer the question, 'What will it take for every child to be successful?'\" explains CCSSO Executive Director Gene Wilhoit. \"Fewer, clearer, and higher standards will help us get there.\""}, {"section_title": "Eighth Grade", "text": ""}, {"section_title": "SampLe probLemS", "text": "The sample problems provided in an appendix to the framework are generally quite reasonable."}, {"section_title": "CONTENT CLARITY TOTAL", "text": "6 3 9"}, {"section_title": "GRADE", "text": ""}, {"section_title": "A Introduction", "text": "The Programme for International Student Assessment (PISA) is governed and administered by the Organisation for Economic Co-operation and Development (OECD), an entity comprised of thirty countries. The PISA exams cover scientific, reading, and mathematical literacy, and have been administered every three years since 2000. This review covers the framework for the math assessment administered in 2006, 7 in which fifty-six OECD and non-OECD countries participated. The PISA test is administered to fifteen year-olds since this is the age \"in most OECD countries [where] students are approaching the end of compulsory schooling.\" PISA's objective is to test \"not so much in terms of mastery of the school curriculum, but in terms of important knowledge and skills needed in adult life.\" The PISA framework \"defines the contents [sic] that students need to acquire, the processes that need to be performed and the contexts in which knowledge and skills are applied.\" Though not intended exclusively for school-based education, this framework thus has the same intention and performs essentially the same function as a set of academic standards, and can reasonably be evaluated by the same standards-based criteria. Further, a number of American educators, policymakers, and standard setters look to PISA as a benchmark for what should be required and/or expected of American schools. Since the PISA standards are used for these purposes, it is important to appraise their content. The framework devotes over forty pages to summarizing what mathematical literacy should mean for fifteen year-olds. PISA breaks content into four categories: \"space and shape; change and relationships; quantity; and uncertainty.\" (In addition to those four categories, PISA discusses four \"situation types,\" eight \"competencies,\" and three \"competency clusters,\" but these are vague and do nothing to clarify the content that students should have mastered.) For each of the four content categories, there is a general discussion about the mathematics in the category, but again, these discussions do not describe content to be covered. Each category features a short list of \"key aspects\" that are as close to standards as the framework gets, but these, too, are vague and non-specific. They total only twenty-three and do not supply clear guidance for readers or users (e.g., teachers, curriculum developers, test developers, mathematicians). Sample problems are also supplied in each category, and the real content to be covered must often be inferred from these examples (as well as from released items from actual PISA exams, discussed below). The sample problems illustrate the very low level of mathematics content that is required. The exam is primarily a problem solving exam, and seldom requires the highest level of mathematics that a fifteen year-old would be expected to know. The second document reviewed here, the released items, is 106 pages long and contains fifty problems with descriptive names (such as Walking, The Best Car, and Postal Charges). Many of these problems consist of several distinct test items, all related to the same theme. All PISA problems are \"in context\" (i.e., they are intended as real world problems)."}, {"section_title": "DOCUMENTS REVIEWED", "text": ""}, {"section_title": "30", "text": "PART 3 > REVIEWS > MATH > PISA that should be covered by a mathematics test for fifteen year-olds (see \"Math Content-Specific Criteria,\" page 11), and the results are presented under \"content covered\" and \"content missing.\" Specific problems with the content are discussed in the third section, followed by a discussion of the released items in the fourth section. The remainder of the review sums up the content and rigor of the PISA framework and released items and considers the clarity and specificity of the former. The last section provides an overall summary and final grade."}, {"section_title": "reLeaSed itemS", "text": "In addition to the sample problems included in the PISA Framework, PISA also makes available a set of released items (test problems used in previous years). Because of the imprecise nature of the standards and the discussion surrounding them, it is important to review the released items to see if they give further guidance about content. The most striking feature of the released items is that thirty-five of the fifty involve the use of a picture, table, or graph. \"Data display\" falls under the content category of \"uncertainty,\" and this means that the \"uncertainty\" content category is highly overrepresented, even if the problems go on to test content covered by other categories. There are only a few real formulas and equations, and only about 10 percent of the released items use any algebra. There is only one question that expects students to produce a formula of their own from the information given in a problem. This question requires students simply to multiply a given formula by 0.8 in order to arrive at the new formula. The lack of algebra illustrates the low level of mathematical content knowledge expected by the PISA assessment. Nor are arithmetic skills in much demand in the problems. Even where such skills might be useful, calculators are allowed. The level of geometry used in a few of the released items is significantly higher than that suggested by the standards and the discussion surrounding them (as is the case with similarity and congruence). However, as with algebra, only a few test items call upon this level of geometry. Most of the released items are focused on problem solving and use fairly low-level mathematics content. However, the problems can be quite complex; about 20 percent of them are multi-step problems. So, even though the content is undemanding, the sample problems can be quite difficult due to the number of steps required to solve them. A significant number of problems could be taken to task for errors, misleading statements, or imprecise questions. Enumerating all of these issues is beyond the scope of this review."}, {"section_title": "Content and Rigor", "text": "Overall, the content and rigor of most of the core standards is reasonable (within the limits described above), but in several places, crucial content is missing or is not covered satisfactorily, especially in reading. It is sometimes difficult to distinguish between more and less important content within and among strands, and some content is not as rigorous as it should be for college and workplace readiness. The bulk of this discussion about content and rigor focuses on the reading standards because the other strands (writing, speaking and listening) are stronger.\nThe OECD notes that the PISA frameworks take \"an innovative approach to assessing knowledge, skills and attitudes by identifying the key concepts and processes that underpin each subject, rather than merely examining curriculum content.\" 25 In four opaque sections, the framework: defines the conceptual basis for its definition of reading literacy; 1 explains the organization of the reading literacy domain (i.e., the kinds of texts and tasks sampled on the assessment); 2 describes the types of skills required for addressing the tasks; and, finally, 3 offers excerpts from sample tasks."}, {"section_title": "Core StandardS for reading informationaL and Literary textS", "text": "As in the other two strands, this section describes both \"Standards for the Range and Content of Student Reading\" and \"Standards for Student Performance.\" Ostensibly, the former section addresses content and the latter section addresses skills, though only the performance standards outline specific student expectations. Separating content from skills weakens the overall rigor in every strand, but in reading neither the content nor the skills section describes expectations in sufficient detail to provide unequivocal guidance for curriculum developers."}, {"section_title": "StandardS for range and Content (reading)", "text": "An introductory \"How to Read the Document\" section notes that the \"range and content\" standards in each strand \"describe the contexts in which college-and career-ready students must be able to read, write, speak and listen,\" but in the case of reading, these \"standards\" take the form of general statements about the qualities and components of text types and reading activities. The \"How To\u2026\" section further states that \"these standards are themselves required and carry equal force,\" yet it is impossible to discern how these guidelines could be required unless the content outlined here were skillfully intertwined with targeted reading skills. (The reading skills are currently defined only very broadly in the \"Standards for Student Performance\" section of the strand.) The authors have made a laudable attempt to describe the kinds of texts they want students to read by explaining the desirable components of five characteristics of texts: For the most part, though, the descriptions read more like rationales for caring about the five characteristics in the first place. The description of complexity, for example, says that students should be able \"to comprehend complex texts independently\" and that students must \"read texts characterized by demanding vocabulary,\" among other admonitions that are subject to interpretation. The quality explanation says that texts should be \"rich in content\" and come from \"a variety of disciplines.\" All of this is praiseworthy as far as it goes, but of course the directive is subjective. U.S. educators also need to note that these standards make no mention of exceptional American literature, American literary heritage, or the importance of understanding that heritage as a matter of cultural literacy (see Reading Criterion 3, page 15). Other high performing countries are less abashed about giving special deference to the study of their own literature. 9 Vocabulary notes the importance of things like \"encountering and mastering a rich vocabulary.\" Some important aspects of vocabulary content, such as knowledge of roots and affixes or etymology, are not discussed (see Reading Criterion 1, page 15). Where quantity is concerned, students \"must have the capacity to handle independently the quantity of reading material\u2026.\" This description is again subject to interpretation, and no specific benchmarks for the desirable amount or lengths of reading materials is included (see Reading Criterion 4, page 15). Least satisfactory are the descriptions offered for range, where text types are addressed. Instead of defining text types and linking them to the applicable skills, the standards sketch very general rationales for why students should be exposed to various types of text. For example, regarding literature: \"By immersing themselves in literature, students enlarge their experiences and deepen their understanding of their own and other cultures.\" This certainly may serve as one justification for reading widely, but it is not a standard. It would be more helpful if this section addressed specific works and authors that would illustrate the desired quality and complexity, as has been attempted in the \"Illustrative Texts\" section. Better still would be to combine the content and skills standards into more specific expectations (yet still allow for teacher choice), as in the following twelfth-grade standards from Indiana: Evaluate the way in which authors have used archetypes (original models or patterns, such as best friend, champion, crusader, free spirit, nurturer, outcast, tyrant, and others) drawn from myth and tradition in literature, film, political speeches, and religious writings. Similar rationales for reading informational text and multimedia sources follow, and similar efforts to combine content with skills would render useful revisions here as well. The document appends exemplars of the kinds of texts that students should read. These mostly well-chosen excerpts reflect high expectations, but it is not clear how these exemplars will carry any real weight. They appear more as advisory recommendations that states, districts, and teachers could choose to ignore."}, {"section_title": "StandardS for Student performanCe (reading)", "text": "Eighteen statements comprise the performance standards for reading. They set forth such general reading skills as \"discern the most important ideas\u2026\" and \"determine what is meant by words and phrases in context\u2026\" The statements are intended to apply to both literary and informational texts, but that strategy works only at the most basic level. For example, while standards for literary analysis are hinted, as in number six, there is no discussion of specific literary elements and devices (see Reading Criterion 2, page 15). 10 Further, this standard \"applies to studying characters in fiction and figures in historical texts,\" two very different propositions requiring distinct analytical lenses. Where informational text is concerned, the standards address arguments only obliquely, as in number fourteen. 11 Specific structures of arguments and types of reasoning, especially fallacious reasoning, are not discussed (again, see Reading Criterion 2). Students are left with no specific benchmarks or vocabulary for understanding and explaining why arguments may or may not be valid, nor how dangerous specious arguments can be. This conflation of skills for reading literary and informational text will make it difficult for teachers to nurture the unique skills that are necessary for navigating certain text types. It also makes prioritization of content difficult to discern. As a result, the PART 3 > REVIEWS > ENGLISH LANGUAGE ARTS > COMMON CORE standards are less rigorous than they should be at the end of high school."}, {"section_title": "Core StandardS for Writing", "text": "The writing standards exhibit some of the same difficulties as the reading standards by conflating writing skills across genre types, but they do a better job than the reading standards of acknowledging (at least for informational and argumentative writing) the ways in which content and skills must be combined to define rigor."}, {"section_title": "StandardS for range and Content (Writing)", "text": "As in reading, these \"standards\" describe a set of characteristics of and contexts for writing. They emphasize the \"when and why\" of student writing rather than delineate specific student expectations for the content expected in various writing applications (genres). The characteristics are: Purpose (to make an argument or to inform/explain) The purposes are limited to making an argument and informing/explaining. Some may wonder why narrative writing is marginalized in a side note and why creative writing is not mentioned at all, but it appears that the authors chose to focus on the types of writing most common in both college and workplace settings. One must hope that narrative and creative writing will be addressed at earlier grades in the back-mapped standards to follow. The other range and content standards for writing provide explanations about when and why students must write often and well, but do not outline specific student expectations. It is particularly disappointing that they do not offer more detailed content for English language conventions. A footnote discusses their importance and even lists some particular concepts for \"formal writing\" that could be incorporated into these standards. Again, one hopes that specific standards for the correct use of grammar, usage, and mechanics will be detailed in the back-mapped standards to follow."}, {"section_title": "StandardS for Student performanCe (Writing)", "text": "For the most part, these standards do address components of good writing but, as in reading, they are fairly generic. Without explanation, the document identifies the skills specific only for \"writing to inform/explain\" and for \"writing to make an argument.\" It does not specify the skills that are important in other writing genres, which would provide better guidance to teachers. Even the argument description could be improved. For example, in standard number eighteen, it would be useful to include addressing or refuting counterclaims effectively in addition to \"acknowledging\" them. 12 Providing samples for writing and detailed scoring rubrics, as for reading, would also help to define and illuminate the expected level of rigor."}, {"section_title": "Core StandardS for Speaking and LiStening", "text": "These spare standards combine speaking and listening and focus on effective communication as an overarching goal. As with writing, the \"range and content\" standards address the \"when and why\" of speaking and listening, offering as contexts: Group and One-to-One Situations These contexts are appropriate but, as with the other strands, the statements are not really standards so much as rationales for the importance of speaking and listening skills in these situations. We may infer that they describe when and why one might use The \"Applications of the Core\" section describes \"the essential skills and knowledge that students need to apply to college and career tasks such as research and media.\" Rationales for inclusion of these applications are reasonable. In the case of research, the standards are actually standards from other strands presented in a coherent permutation of steps for formulating research questions, gathering information, analyzing research sources, and reporting findings. Perhaps in a subsequent draft the authors should try a similar experiment with combining skills (and content standards) in reading and apply them to specific types of texts. The standards for media include both standards for \"range and content\" and \"standards for student performance.\" These standards suffer from similar shortcomings as described above. They are not so much standards as descriptions of the kinds of multimedia that graduates will encounter and the contexts in which they will find them. The standards for student performance are taken from other strands-as for research-and presented as a new permutation for application to multimedia. They are fine for the most part, but would benefit from greater detail about the methods for presenting information, especially for combining written, visual and aural components-and identifying their unique effects, such as bias and propaganda in political or other advertising (see Media Criteria fourteen and fifteen, page 16). Including specific standards for developing multimedia presentations might also be useful, as it is an increasingly essential college-and career-readiness skill."}, {"section_title": "39", "text": "PART 3 > REVIEWS > ENGLISH LANGUAGE ARTS > COMMON CORE language would clarify whether the first standard is meant to focus on the skills of accurate and concise summary, and if the second is designed to underscore the skill of understanding how supporting details relate to main ideas (and, if so, how that differs from the expectation in number five). Skills in the other strands exhibit some of the same confusion. Finally, the purpose of some sections remains unclear. For example, the document includes an introductory section called \"Student Practices in Reading, Writing and Speaking and Listening.\" These practices are said to \"undergird and help unify the rest of the standards document,\" but it is unclear how they achieve that goal. The statements under each \"practice\" do not always align with their stated purpose, as in number two, \"They Build Strong Content Knowledge.\" The statements here read much like a continuation of the broadly worded description of students demonstrating \"independence\" as learners in number one. In short, the standards are somewhat lacking in coherence, clarity, and organization. For this reason and others, the standards earn a two for clarity and specificity (see \"Common Grading Metric,\" page 16)."}, {"section_title": "NAEP Reading Framework (2009)", "text": "Content and Rigor"}, {"section_title": "Coverage of text typeS", "text": "The content of the 2009 NAEP Reading Framework focuses on two types of texts: Literary texts (fiction, literary nonfiction, and poetry) and"}, {"section_title": "Informational texts", "text": "(exposition, argumentation, and persuasive text; and procedural text and documents). The framework prescribes the reporting of separate subscales for literary and informational text, which NAEP has never done before. The text types do not, however, differ much from the previous framework, which had defined three text types: \"literary, informational, and document.\" The document category in the previous framework has been subsumed under the new informational text category, while literary nonfiction (e.g., essays and speeches) has been moved from the informational category to the new literary text category. This redistribution is significant in the context of what the overall passage distribution looks like in the 2009 framework (see Exhibit 1). In the twelfth grade, significantly more weight is given to informational text than to literary text. 17 Despite the laudable addition of poetry at fourth grade, the framework maintains a disproportionate emphasis on informational texts. Insofar as NAEP is influential in the states, this shift could result in diminished emphasis in the field on teaching literary texts, both fiction and nonfiction. Further, it could have profound and detrimental effects on students' cultural literacy and their foundational skills in the humanities. The framework supplies details of the various types of text in easy-to-read matrices. For the most part, they represent a reasonable set of expectations for K-12 students, although there is room for improvement. First, some \"aspects of texts\" at eighth and twelfth grades might easily be introduced earlier. For example, rhythm patterns, point of view, and personification, now listed at eighth grade, could certainly be included at grade four. Aspects of fiction at grade eight could also be assessed at grade four, such as flashback and personification. Some aspects at twelfth grade also seem to come late, such as monologue, comic relief, and \"unconventional use of language.\" Second, the aspects of \"argumentation and persuasive text\" at twelfth grade are left undefined, despite this subcategory's previously stated significance. Finally, the elusive notion of voice is introduced as early as grade four in sub-categories of informational text, but how or why voice is distinct from style, tone, or mood is a much-debated proposition; the inclusion of voice seems unnecessary."}, {"section_title": "typeS of reading SkiLLS aSSeSSed", "text": "As for what students are asked to do with the texts, the framework defines three types of \"cognitive targets, or behaviors and skills\" to be assessed: Integrate/interpret; and Critique/evaluate. These skill categories do not differ significantly from those detailed in the previous NAEP reading framework, but specific skills are not delineated by grade level, which would help clarify expectations. The framework explains: The cognitive targets remain the same across all three grades on the assessment but the passages and documents about which items are developed will be of increasing sophistication at each grade. Because the \"targets\" do not change across the grade levels, the rigor of the NAEP reading assessment ultimately depends on how particular grade-specific items are constructed and the types of passages that are selected for students' consideration. Unfortunately, we do not have access to the full range of test items and associated passages at each grade span, but the released items found in \"Sample Questions\" booklets offer a glimpse of the assessment's rigor. The sample assessment items at each grade span seem appropriately challenging in terms of the cognitive skills listed above, but the sample reading passages themselves are largely mediocre and not particularly complex. As a result, the sample items overall are not sufficiently demanding. One welcome addition to the 2009 framework is \"a more systematic approach to vocabulary assessment,\" including the 17 The emphasis on informational text appears to be due to concerns about \"college-and career-readiness.\" Indeed the framework's Executive Summary notes that the National Assessment Governing Board (NAGB) adopted \"minor modifications to the 2009 Reading Framework at grade 12 based on a comprehensive analysis of the framework conducted by Achieve, Inc.\" The referenced 2005 report (Recommendations to the National Assessment Governing Board on Aligning 12 th Grade NAEP with College and Workplace Expectations: Reading) states: \"Because the reading demands that high school graduates face are overwhelmingly informational in nature and informational literacy is a vital 21 st century skill, NAEP should increase the percentage of informational text on the new NAEP from 60 to 70 percent.\" NAGB also cites the Progress in International Reading Literacy Survey (PIRLS) and PISA reading definitions as bases for the framework's rationale; both definitions emphasize what is \"valued by\" or \"necessary\" for \"participation in society.\" calculation of a vocabulary sub-score for the 2009 administration of the test. Although this new emphasis on vocabulary is commendable, the sample questions send mixed signals about rigor. Fourth and eighth grade look to be reasonably rigorous, but twelfth grade does not. For example, the words \"scornfully\" and \"subdued\" at fourth grade and \"deflect\" and \"abated\" at eighth grade are of an appropriate difficulty level; \"obligations\" and \"precedent\" at twelfth grade are not. Words taken from SAT prep lists, such as \"specious,\" \"attenuate\" or \"circuitous\" might be more appropriate."}, {"section_title": "Content and Rigor purpoSeS for Writing", "text": "The 2011 NAEP assessment measures three communicative \"purposes\" rather than the three \"modes\" of writing that previous Frameworks sought to gauge. The change is mostly semantic: The ultimate substance of the framework-what is assessedappears relatively unchanged. The three modes of the 1998-2007 NAEP writing assessment were: Persuasive mode: Writing to convince } The framework offers only very general descriptions of the three purposes. It discusses when and why people write for these purposes, but does not detail specific writing expectations by grade levels as the NAEP reading framework attempts to do. The relative percentages of items devoted to each purpose seem appropriate (see Exhibit 2). There need not be an emphasis on writing literature (i.e., \"writing to convey experience\") in the writing framework like there is on reading literature in the reading framework. In other words, students should not be expected to write sonnets or great literary pieces. Emphasizing persuasive and explanatory purposes for writing, then, makes sense. The emphasis is also consistent with the content criteria for writing (see \"English Language Arts Content-Specific Criteria,\" page 13) which state that students should \"\u2026produce writing that reflects the defining characteristics of various grade appropriate writing genres.\""}, {"section_title": "EXHIBIT 2. Percentage distribution of communicative purposes by grade", "text": "Compared to the previous writing framework, the emphasis on persuasive writing has increased slightly at grade four (30 percent in 2011 versus 25 percent in 1998). The framework notes that this addition was deliberate, in order to \"scaffold\" the emphasis on persuasive and informational writing skills at later grades. The \"English Language Arts Content-Specific Criteria\" place similar emphases across the three grade spans, moving from \"narration, exposition\" as an emphasis in kindergarten through fourth grade, to \"argument\" in five through eight, and \"persuasion\" at nine through twelve. The writing purposes and the time accorded to each in the NAEP writing framework are therefore sensible and on-target."}, {"section_title": "featureS of Writing", "text": "Within these types of writing, the framework identifies three very broad categories of features of writing: Further criteria for assessing writing in each of these \"broad domains\" are included, but they, too, are rather vague, mentioning only such aspects as \"depth and complexity\" (for number one above), \"coherence\" (for number two) and \"voice and tone\" (for number three). These terms alone are too broad to be useful. Further clarification is attempted in later prose descriptions, but alas, these, too, are nebulous and lack sample responses. For example, when describing \"Approaches to Thinking and Writing,\" an aspect of \"Development of Ideas,\" the framework provides this befuddling statement: Specific approaches to thinking and writing will not be specified on NAEP tasks, but responses will be evaluated for their use of effective approaches in relation to the development and organization of ideas. The framework appears to rely on sample writing tasks (or \"prompts\") more than on these descriptions to exemplify its expectations. This is to be expected since one of NAEP framework's primary roles is to inform assessment (see \"NAEP Overview\" in the Appendix, page 52). 18 Even more helpful, however, would be the addition of sample acceptable responses to the tasks (see related \"Clarity and Specificity\" discussion below). The prompts generally provide context and clear directions and many are text-based in some way (meaning the tasks are grounded in specific reference points and therefore provide a \"level playing field\" for responding). In terms of actual grading, however, the six-point scoring rubrics are \"holistic.\" This means that specific weights are not assigned to each of the criteria (many of which address the features of writing discussed above), making it difficult to discern where students are doing well and where they are struggling. For example, a student's response could be well-organized, yet inaccessible because of poor sentence structure and mechanics; another response might boast impressive style, but lack development of ideas. The holistic rubrics do not differentiate among these priorities; they could be made more useful if they were prompt-specific and accompanied by sample acceptable responses. One overall disappointment is the very general nature of the expectations for English language conventions. (They are absent from the reading framework altogether, even though careful analysis of sentence structure and grammar certainly aids reading comprehension.) Making matters worse, English language conventions-the most consistently noted concern of employers and postsecondary faculty-can easily be overlooked with holistic scoring. NAEP should find a way to articulate its expectations in this critically important area in much greater detail."}, {"section_title": "Reading Writing", "text": "reading. The new framework outlines a definition of \"reading literacy\" that includes what PISA calls \"reading engagement and metacognition.\" It emphasizes \"motivational attributes and behavioral characteristics of students' reading.\" 24 Because the focus of the PISA framework is on defining how reading skills may be used to meet \"real-life\" challenges, rather than delineating desired content for students, its usefulness as a model for national and state reading standards is limited-and its growing influence as a source for standards developers in the U.S. warrants serious scrutiny. The PISA framework describes the conceptual basis for an assessment, as well as the types of texts and tasks that students will encounter-and how the tasks will be scored. So, while PISA is not a curriculum-based or school-based framework, American educators, policymakers, and standard setters often look to it as a benchmark for what they should require and/or expect of American schools. Since PISA is being used for these purposes, it is important to examine its content carefully."}, {"section_title": "defining the ConCeptuaL baSiS for piSa'S definition of reading LiteraCy", "text": "The PISA framework states that the goal of education \"has shifted from the collection and memorization of information only to encompass a broader concept of knowledge.\" 26 It adds that \"the meaning of knowing has shifted from being able to remember information to being able to find and use it,\" which is obviously a disputable contention that drinks deeply from the well of so-called 21 st Century Skills. 27 In fact, it would be difficult to conjure educators who ever thought that the goal of education was simply to \"collect and memorize information\" in the first place, so the premise is faulty. Epistemologists would likewise recoil at the suggestion that \"to know\" is simply to \"find and use\" information, but these are the PISA assumptions. 27 For a discussion of the importance of content knowledge as it relates to reading comprehension, see Willingham, Daniel. 2006. \"How knowledge helps: It speeds and strengthens reading comprehension, learning-and thinking.\" American Educator, 30 No. 1. Spring. http://www.aft.org/pubs-reports/american_educator/issues/ spring06/willingham.htm In addition, the framework asserts that \"reading-related skills, attitudes, interests, habits and behaviors have been shown in a number of recent studies to be strongly correlated with reading proficiency.\" The evidence cited is the OECD's own finding that this correlation was stronger than the correlation between proficiency and socioeconomic status in the PISA 2000 administration. In short, the OECD seems more interested in why students perform well on PISA than in simply determining whether the students possess the requisite reading comprehension skills to answer questions correctly."}, {"section_title": "the organization of the reading LiteraCy domain", "text": "PISA's organization of the reading literacy domain is arguably the most complicated part of this complex framework. Assessment tasks are categorized by \"situation\" (private, public, occupational, or educational); \"text\" (medium, environment, format, and type); and \"aspect\" (access/retrieve, integrate/interpret, reflect/evaluate, and complex). In laymen's terms, this means that PISA categorizes its assessment tasks first by the situation in which the task might present itself (i.e., reading for pleasure, for school, etc.); then by text type (i.e., description, narration, exposition, etc.); and finally by skill (i.e., the type of cognitive skill demanded by the item, such as interpreting or evaluating information). 28 The framework notes that the categories may overlap. One important drawback to this organization of the reading literacy domain is its heavy emphasis on non-educational categories. For example, with regard to \"situation,\" only 28 percent of print text items and a mere 12 percent of electronic text items represent \"educational\" situations. 29 Forty-four percent of print items and 52 percent of electronic items are \"personal,\" \"occupational\" and \"public\" (meaning they are \"usually associated with out-of-school settings for children\"). While it is true that PISA seeks to assess students' ability to participate in society, a study that neglects educational tasks would surely thwart such a goal. 30"}, {"section_title": "aSpeCt: the typeS of SkiLLS required for addreSSing the taSkS", "text": "The framework's discussion of the kinds of skills that each task assesses is relatively straightforward. In fact, the PISA categories are consistent with common assessment classifications (such as those used in NAEP): access and retrieve; integrate and interpret; and reflect and evaluate. However, a fourth category, \"complex,\" is added for electronic text only; the framework suggests that none of the existing three skill types suffices to describe the skills demanded in electronic text (more on this below). The tasks include both multiple choice and constructed response items. Partial credit is awarded in some cases for constructed response items. \"Access and retrieve\" is much like \"locate and recall\" in NAEP: a simple skill that requires no complex inferences. \"Integrate and interpret\" parallels the next NAEP category exactly and \"focuses the reader on relationships within a text.\" Tasks that require students to \"reflect and evaluate\" ask them \"to draw primarily on knowledge outside the text and relate it to what they are reading.\" The framework includes examples of each type of task and makes distinctions about how these skills are employed when electronic text is involved. For example, the designers of the framework seem interested in exploring the various ways in which readers might access information (i.e., knowing where to click in an electronic text versus knowing what paragraph to look for in print text). In neither case is the priority simply assessing readers' comprehension skills. The distribution of the print tasks by skill seems even-handed (50 percent of the items are \"integrate and interpret\"; 25 percent are access and retrieve; 25 percent are \"reflect and evaluate\"), yet the distribution by level of difficulty is extraordinarily lopsided. None of the print items is deemed \"complex\"-not even one. This seems to be a task type reserved for electronic media. By restricting skill complexity to electronic media only, the framework implies that only electronic texts allow students to access, select, and retrieve information \"from a wide field of possible resources.\" Yet print media obviously may afford the same opportunities."}, {"section_title": "taSk exCerptS", "text": "As with any assessment framework, the sample reading passages, test items, and answers are the best indication of the exam's rigor. Unfortunately, the 2009 PISA Reading Framework has few sample task items. They include the description of an excerpt (i.e., not an actual excerpt) from Gabriel Garc\u00eda M\u00e1rquez's One Hundred Years of Solitude and two reproductions of 28 \"Skill\" is also addressed in (3) \"Aspect.\" 29 Recall that assessment tasks are categorized by \"situation\" first, which is broken down into private, public, occupational, or educational situations. 30 Curiously, the PISA framework also classifies literary text as \"personal\" since its architects say the \"original\" purpose of such text is reading for pleasure. If these were the only cases where non-academic text was highlighted, it might not be a problem. As it stands, the \"personal\" category includes \"personal letters\" and \"texts that are intended to satisfy curiosity, as a part of leisure or recreational activities.\" Moreover, electronic text \"includes personal emails, instant messaging and diarystyle blogs.\" "}, {"section_title": "50", "text": "PART 3 > REVIEWS > ENGLISH LANGUAGE ARTS > PISA \"Metrotransit\" maps, accompanied by instructions that ask students to use the maps to draw the shortest distance between two points. These meager examples do not provide much information about the tasks. In another excerpt, the stimulus for an open-ended response question is provided, but not the \"five short argumentative texts\" that the student would need to have read in order to answer the question, making it impossible to determine the item's rigor. Students are asked to state which of the five argumentative texts they agree with and why, citing their own opinion and the main ideas presented. The prompt provides just three lines for a response, and the sample answers are one or two sentences each, such as, \"I agree with him because he is concerned with the environment and he thinks we should leave space alone.\" The question begs for a longer and more defensible response than that provided by the rather simple-minded sample correct answer. In fact, the sample response lacks substance and fails to demonstrate the ability to make an argument. Noting the OECD's need for \"more differentiation at the lower and higher ends of student achievement,\" the samples deliberately include one \"very easy\" and one \"very difficult\" item. The \"very easy\" item simply asks students to name the company discussed in a product recall notice; the \"very difficult\" item asks students to discuss the possible purpose of Pericles's speech from The Peloponnesian War about democracy in Athens. We are told that the latter item is \"one of the most difficult administered in the 2009 PISA field trial,\" yet it is the kind of item that high school English teachers might expect many of their students to navigate successfully. The fact that it is identified as the most difficult suggests that the overall rigor of the PISA assessment is not particularly high. It also suggests that, regardless of what \"real world\" skills PISA may be assessing, it is not evaluating the menu of knowledge and skills that students should be mastering in English classes before graduating from U.S. high schools."}, {"section_title": "What do the StandardS Look Like?", "text": "The draft standards (released on September 21, 2009) address reading, writing, speaking and listening, and math. These subject/ skill areas are the foundation of current accountability systems under No Child Left Behind as well as foundational subjects that provide the basis for other subjects. The reading, writing, and speaking and listening standards are divided into two sub-categories: \"range and content\" and \"student performance.\" While the former has three to five broad categories, such as \"vocabulary\" (reading) and \"audience\" (writing)each explaining the areas in which career-and college-ready students should be competent-the latter are more specific. For example, student writers should be able to \"use varied sentence structures to engage the readers and achieve cohesion between sentences.\" The standards also include a section outlining how these core skills are applied to research and the media. In both sections, there is an emphasis on students' discerning accurate information and news from the plethora of information. The standards also include a number of sample texts that the developers consider to be high quality plus notes on how students should read and interact with the texts. The math standards take a somewhat different form. There are many more topics to address, such as number, quality, equations, etc., and each is explained in depth. The math topic standards delineate content (what \"students understand\") and skills (what \"students can and do\") in two side-by-side columns. For example, under the mathematical concept number, students should understand that \"quantities can be compared using division, yielding rates and ratios\" while also knowing \"when and how to use  31 The former, comprised of content experts from Achieve, ACT, and the College Board, were charged with initial development of the standards; they were aided by independent facilitators, writers, and advisors. The work groups' discussions were confidential during the process; now that the draft has been released, state and national education groups have the opportunity to comment until October 21, 2009. After the comment period ends, the standards will be submitted to a twenty-five-person validation committee for further review. 32 The extant draft deals with the end of high school but drafters are beginning work to \"backward-map\" these standards through the earlier grades of school. These K-12 standards will be released in January 2010. Once both sets of standards are finalized, participating states are expected to submit a timeline and process for approving them in 2010. CCSSO and NGA hope to present both sets of standards-the college and career readiness standards and the K-12 grade-specific standards-to states at the same time for approval."}, {"section_title": "the future of the StandardS", "text": "Much of this remains to be determined. Perhaps most important, while U. S. Education Secretary Arne Duncan has said that the federal government will underwrite the development of new assessments to be aligned with the Common Core standards, how and by whom (and when) that will be done is not yet known. PART 5 > APPENDIx"}, {"section_title": "Trends in International Mathematics and Science Study (TIMSS)", "text": "What iS it? Trends in International Mathematics and Science Study is an assessment of fourth-and eighth-grade math and science knowledge. 33 However, we only reviewed frameworks guiding the math portion in this report. It was created in 1995 by the International Association for the Evaluation of Educational Achievement (IEA), a sixty-eight-member (as of 2009) independent organization of national education research organizations and government education research agencies ; the IEA also oversees the Progress in International Reading Literacy Study (PIRLS). 34 The two tests are run by the TIMSS & PIRLS International Study Center at Boston College. The TIMSS assessment is given every four years. (The last administration occurred in 2007, with the next to occur in 2011)."}, {"section_title": "What iS the purpoSe of timSS?", "text": "TIMSS is a curriculum-based test of student achievement. It is concerned with how the curriculum is defined nationally, implemented in schools and classrooms, and absorbed by students. TIMSS calls these three pieces the intended curriculum (\"the mathematics and science that society intends for students to learn and how the education system should be organized to facilitate this learning\"), the implemented curriculum (\"what is actually taught in classrooms, who teaches it, and how it is taught\"), and the achieved curriculum (\"what it is that students have learned, and what they think about these subjects\"). In the end, TIMSS data address a key question: Are students actually learning what the curriculum says they should be? The test is also designed to capture achievement trends (though only through four testing cycles so far). A number of countries have been part of TIMSS since 1995 and can evaluate their achievement over that period. These trend data also capture achievement by grade cohort, since fourth-graders who take TIMSS in one testing cycle are eighth-graders in the next cycle."}, {"section_title": "Who takeS timSS?", "text": "Students at the end of four and eight years of schooling (typically fourth and eighth grade, though not necessarily) in participating countries take TIMSS. The number of participating countries has grown from forty-one in 1995 to more than fifty-nine countries and eight other entities (e.g., regional entities that are tested as if they were countries, such as the state of Massachusetts and Dubai, UAE) in 2007. The U.S. has been participating in TIMSS since its first administration. Almost seventy countries are expected to participate in 2011; forty of those countries will also participate in PIRLS, which is administered every five years, and happens to coincide with TIMSS in 2011."}, {"section_title": "What doeS the teSt Look Like?", "text": "Math and science are treated equally on the TIMSS assessment. In 2007, for example, there were 179 mathematics items and 174 science items (359 total) in fourth grade and 215 mathematics items and 214 science items (429 total) in eighth grade. Since there are many more questions that can possibly be answered by each individual student, the test uses a matrix-sampling approach whereby each student answers a subset of the total questions. (NAEP and PISA also use this strategy.) The test includes both multiple choice and constructed response questions (the latter are graded for content, not writing quality). In addition, students, teachers and principals fill out questionnaires that address home and school environments, instructional activities, topics covered, and teachers' own training. These questions are used to create background indices that accompany the raw achievement data. hoW iS the teSt deveLoped and adminiStered? Prior to 2003, the International Study Center overhauled both the math and science frameworks to better reflect what was actually being taught in participating countries and to make the test data more useful for them. This process included using the expert advice from content experts as well as intensive country surveys. The draft frameworks are reviewed by the National Research Coordinators (NRCs), a group of individuals chosen by each 33 TIMSS 1995 included a twelfth grade (or \"end of secondary schooling\") assessment. This was discontinued in subsequent administrations. 34 We plan to include a review of PIRLS in the follow-up report to Stars by Which to Navigate? Scanning National and International Education Standards in 2009, which is scheduled for publication in spring 2010. members in some of these groups, along with additional individuals in participating countries. All questions undergo further development by contractors and other experts. After much vetting, draft items are piloted with a large sample of students in multiple countries in the relevant age group; the most reliable items are reserved for the question bank."}, {"section_title": "the future of piSa", "text": "For the upcoming testing cycles (2012 and beyond), PISA has three goals: to examine the relationship between the process of education and achievement outcomes; to evaluate how students attain knowledge and skills over time; and to better use and integrate technology-both in how students learn and in how PISA can be adapted for computer-based administration."}, {"section_title": "Sources:", "text": "Adams "}]