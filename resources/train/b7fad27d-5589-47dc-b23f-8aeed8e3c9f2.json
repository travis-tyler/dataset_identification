[{"section_title": "Abstract", "text": "Abstract-We present an algorithm for creating high resolution anatomically plausible images consistent with acquired clinical brain MRI scans with large inter-slice spacing. Although large data sets of clinical images contain a wealth of information, time constraints during acquisition result in sparse scans that fail to capture much of the anatomy. These characteristics often render computational analysis impractical as many image analysis algorithms tend to fail when applied to such images. Highly specialized algorithms that explicitly handle sparse slice spacing do not generalize well across problem domains. In contrast, we aim to enable application of existing algorithms that were originally developed for high resolution research scans to significantly undersampled scans. We introduce a generative model that captures fine-scale anatomical structure across subjects in clinical image collections and derive an algorithm for filling in the missing data in scans with large inter-slice spacing. Our experimental results demonstrate that the resulting method outperforms state-of-the-art upsampling super-resolution techniques, and promises to facilitate subsequent analysis not previously possible with scans of this quality. Our implementation is freely available at https://github.com/adalca/papago."}, {"section_title": "I. INTRODUCTION", "text": "Increasingly open image acquisition efforts in clinical practice are driving dramatic increases in the number and size of patient cohorts in clinical archives. Unfortunately, clinical scans are typically of dramatically lower resolution than the research scans that motivate most methodological development. Specifically, while slice thickness can vary depending on the clinical study or scan, inter-slice spacing is often significantly larger than the in-plane resolution of individual slices. This results in missing voxels that are typically filled via interpolation.\nOur work is motivated by a study that includes brain MRI scans of thousands of stroke patients acquired within 48 hours of stroke onset. The study aims to quantify Mert R. Sabuncu is with the the School of Electrical and Computer Engineering, and Meinig School of Biomedical Engineering, Cornell University. Natalia S. Rost is with the Department of Neurology, Massachusetts General Hospital, HMS.\n*Data used in preparation of this article were obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how to apply/ ADNI Acknowledgement List.pdf The three panels display axial, sagittal and coronal slices, respectively. While axial in-plane resolution can be similar to that of a research scan, slice spacing is significantly larger. We visualize the saggital and coronal views using nearest neighbor interpolation.\nwhite matter disease burden [23] , necessitating skull stripping and deformable registration into a common coordinate frame [27] , [31] , [32] . The volumes are severely undersampled (0.85mm\u02c60.85mm\u02c66mm) due to constraints of acute stroke care (Fig. 1) . Such undersampling is typical of modalities, such as T2-FLAIR, that aim to characterize tissue properties, even in research studies like ADNI [15] .\nIn undersampled scans, the image is no longer smooth, and the anatomical structure may change substantially between consecutive slices (Fig. 1 ). Since such clinically acquired scans violate underlying assumptions of many algorithms, even basic tasks such as skull stripping and deformable registration present significant challenges, yet are often necessary for downstream analysis [4] , [7] , [12] , [15] , [23] , [30] , [31] .\nWe present a novel method for constructing high resolution anatomically plausible volumetric images consistent with the available slices in sparsely sampled clinical scans. Importantly, our method does not require any high resolution scans or expert annotations for training. It instead imputes the missing structure by learning solely from the available collection of sparsely sampled clinical scans. The restored images represent plausible anatomy. They promise to act as a medium for enabling computational analysis of clinical scans with existing techniques originally developed for high resolution, isotropic research scans. For example, although imputed data should not be used in clinical evaluation, the brain mask obtained through skull stripping of the restored scan can be applied to the original clinical scan to improve subsequent analyses."}, {"section_title": "A. Prior Work", "text": "Many image restoration techniques depend on having enough information in a single image to synthesize data. Traditional interpolation methods, such as linear, cubic or spline [28] , assume a functional representation of the image. They treat the low resolution voxels as samples, or observations, and estimate function parameters to infer missing voxel values. Patch-based superresolution algorithms use fine-scale arXiv:1808.05732v1 [cs.CV] 17 Aug 2018 redundancy within a single scan [10] , [11] , [19] , [20] , [22] . The key idea is to fill in the missing details by identifying similar image patches in the same image that might contain relevant detail [19] , [22] . This approach depends on having enough repetitive detail in a scan to capture and re-synthesize high frequency information. Unfortunately, clinical images are often characterized by sampling that is too sparse to adequately fit functional representations or provide enough fine-scale information to recover the lost detail. For example, 6mm slice spacing, typical of many clinical scans including our motivating example, is far too high to accurately estimate approximating functions without prior knowledge. In such cases, a single image is unlikely to contain enough fine-scale information to provide anatomically plausible reconstructions in the direction of slice acquisition, as we demonstrate later in the paper.\nAlternatively, one can use additional data to synthesize better images. Many superresolution algorithms use multiple scans of the same subject, such as multiple low resolution acquisitions with small shift differences to synthesize a single volume [2] , [16] , [22] . However, such acquisitions are not commonly available in the clinical setting.\nNonparametric and convolutional neural-network (CNN) based upsampling methods that tackle the problem of superresolution often rely on an external dataset of high resolution data or cannot handle extreme undersampling present in clinical scans. For example, some methods fill in missing data by matching a low resolution image patch from the input scan with a high resolution image patch from the training dataset [3] , [13] , [16] , [17] , [25] , [24] . Similarly, CNN-based upsampling methods approximate completion functions, but require high resolution scans for training [8] , [21] . A recent approach to improve resolution from a collection of scans with sparse slices jointly upsamples all images using non-local means [26] . However this method has only been demonstrated on slice spacing of roughly three times the in-plane resolution, and in our experience similar non-parametric methods fail to upsample clinical scans with more significant undersampling.\nOur work relies on a low dimensional embedding of image patches with missing voxels. Parametric patch methods and low dimensional embeddings have been used to model the common structure of image patches from full resolution images, but are typically not designed to handle missing data. Specifically, priors [33] and Gaussian Mixture Models [35] , [36] have been used in both medical and natural images for classification [1] and denoising [9] , [36] . The procedures used for training of these models rely on having full resolution patches with no missing data in the training phase.\nUnfortunately, high (full) resolution training datasets are not readily available for many image contrasts and scanners, and may not adequately represent pathology or other properties of clinical populations. Acquiring the appropriate high resolution training image data is often infeasible, and here we explicitly focus on the realistic clinical scenario where only sparsely sampled images are available."}, {"section_title": "B. Method Overview", "text": "We take advantage of the fact that local fine scale structure is shared in a population of medical images, and each scan with sparse slices captures some partial aspect of this structure. We borrow ideas from Gaussian Mixture Model (GMM) for image patch priors [36] , low dimensional Gaussian embeddings [14] , [34] , and missing data models [14] , [18] to develop a probabilistic generative model for sparse 3D image patches around a particular location using a low-dimensional GMM with partial observations. We derive the EM algorithm for maximum likelihood estimation of the model parameters and discuss related modeling choices. Given a new sparsely sampled scan, the maximum a posteriori estimate of the latent structure yields the imputed high resolution image. We evaluate our algorithm using scans from the ADNI cohort, and demonstrate its utility in the context of the motivating stroke study. We investigate the behaviour of our model under different parameter settings, and illustrate an example of potential improvements in the downstream analysis using an example task of skull stripping.\nThis paper extends the preliminary version of the method presented at the 2017 Conference on Information Processing in Medical Imaging [5] . Here, we improve model inference by removing parameter co-dependency between iterations and providing new parameter initialization. We provide detailed derivations and discuss an alternative related model. Finally, we provide an analysis of important model parameters, present results for more subjects, and illustrate more example reconstructions. The paper is organized as follows. Section II introduces the model and learning algorithm. Section III discusses implementation details. We present experiments and analysis of the algorithm's behavior in Section IV. We discuss important modeling aspects and related models in Section V. We include an Appendix and Supplementary Material with detailed derivations of the EM algorithm for the proposed models."}, {"section_title": "II. METHOD", "text": "In this section, we construct a generative model for sparse image patches, present the resulting learning algorithm, and describe our image restoration procedure.\nLet tY 1 , ..., Y N u be a collection of scans with large interslice spaces, roughly aligned into a common atlas space (we use affine transformations in our experiments). For each image Y i in the collection, only a few slices are observed. We seek to restore an anatomically plausible high resolution volume by imputing the missing voxel values.\nWe capture local structure using image patches. We assume a constant patch shape, and in our experiments use a 3D 11x11x11 shape. We use y i to denote a D-length vector that contains voxels of the image patch centered at a certain location in image Y i . We perform inference at each location independently and stitch the results into the final image as described later in this section. Fig. 2 provides an overview of the method."}, {"section_title": "A. Generative Model", "text": "We treat an image patch as a high dimensional manifestation of a low dimensional representation, with the intuition (d) Given a sparsely sampled scan, we infer the most likely cluster for each 3D patch, and restore the missing data using the learned model and the observed voxels. We form the final volume from overlapping restored patches. 2D images are shown for illustration only, the algorithms operate fully in 3D.\nthat the covariation within image patches has small intrinsic dimensionality relative to the number of voxels in the patch. To capture the anatomical variability across subjects, we employ a Gaussian Mixture Model (GMM) to represent local structure of 3D patches in the vicinity of a particular location across the entire collection. We then explicitly model the observed and missing information. We model the latent low dimensional patch representation x i of length d \u0103 D as a normal random variable\nwhere N p\u00b5, \u03a3q denotes the multivariate Gaussian distribution with mean \u00b5 and covariance \u03a3. We draw latent cluster assignment k from a categorical distribution defined by a length-K vector \u03c0 of cluster probabilities, and treat image patch y i as a high dimensional observation of x i drawn from a Kcomponent multivariate GMM. Specifically, conditioned on the drawn cluster k,\ni \" N p0, \u03c3 2 k I D\u02c6D q, and i |\u00f9\nVector \u00b5 k is the patch mean of cluster k, matrix W k shapes the covariance structure of y i , and \u03c3 2 k is the variance of image noise. This model implies I Ery i |ks \" \u00b5 k and\n, the likelihood of all patches Y \" ty i u at this location under the mixture model is\nIn our clinical images, only a few slices are known. To model sparse observations, we let O i be the set of observed voxels in patch y i , and y\nbe the corresponding vector of their intensity values:\nwhere\nwhere matrix\nextracts the rows and columns of C k that correspond to the observed voxel subset O i .\nWe do not explicitly model slice thickness, as in many clinical datasets this thickness is unknown or varies by site, scanner or acquisition. Instead, we simply treat the original data as high resolution thin planes and analyze the effects of varying slice thickness on the results in the experimental evaluation of the method.\nWe also investigated an alternative modeling choice where each missing voxel of patch y i is modelled as a latent variable. This assumption can optionally be combined with the latent low-dimensional patch representation. We discuss this alternative choice in Section V, and provide parameter updates in the Supplementary Material. Unfortunately, the resulting algorithm is prohibitively slow."}, {"section_title": "B. Learning", "text": "Given a collection of observed patches Y O , we seek the maximum likelihood estimates of the model parameters t\u00b5 k , W k , \u03c3 2 k u and \u03c0 under the likelihood (5). We derive the , , Expectation Maximization algorithm [6] in Appendix A, and present the update equations and their interpretations below.\nThe expectation step updates the class memberships:\nand the statistics of the low dimensional representation x i for each image patch y O i i as \"explained\" by cluster k:\nWe let P j be the set of patches in which voxel j is observed, and form the following normalized mean statistics:\nThe maximization step uses the observed voxels to update the model parameters. We let y j i be the j th element of vector y i , and update the cluster mean as a convex combination of observed voxels:\nThe covariance factors and image noise variance are updated based on the statistics of the low dimensional representation from (10) and (11):\nwhere W j k is the j th row of matrix W k . Finally, we update the cluster proportions:\nIntuitively, learning our model with sparse data is possible because each image patch provides a slightly different subset of voxel observations that contribute to the parameter estimation (Fig. 2) . In our experiments, all subject scans have the same acquisition direction. Despite different affine transformations to the atlas space for each subject, some voxel pairs are still never observed in the same patch, resulting in missing entries of the covariance matrix. Using a lowrank approximation for the covariance matrix regularized the estimates.\nUpon convergence of the EM updates, we compute the cluster covariance"}, {"section_title": "C. Imputation", "text": "To restore an individual patch y i , we compute the maximuma-posteriori (MAP) estimate of the image patch:\nDue to the high-dimensional nature of the data, most cluster membership estimates are very close to 0 or 1. We therefore first estimate the most likely cluster p k for patch y i by selecting the cluster with the highest membership \u03b3 ik . We estimate the low dimensional representation p x i p k given the observed voxels y O i i using (7), which yields the high resolution imputed patch:\nBy restoring the scans using this MAP solution, we perform conditional mean imputation (c.f. 17, Sec.4.2.2), and demonstrate the reconstructions in our experiments. In addition, our model enables imputation of each patch by sampling the posterior pp\u00a8|y\nproviding a better estimation of the residual noise. Depending on the desired downstream application, sampling-based imputation may be desired.\nWe average overlapping restored patches using standard techniques [18] to form the restored volume. "}, {"section_title": "NLM", "text": "Our method Ground truth Linear Subject 1 Subject 2"}, {"section_title": "III. IMPLEMENTATION", "text": "We work in the atlas space, and approximate voxels as either observed or missing in this space by thresholding interpolation weights. To limit interpolation effects due to affine alignment on the results, we set a higher threshold for regions with high image gradients than in regions with low gradients. Parameter estimation could be implemented to include transformation of the model parameters into the subject-specific space in order to optimally use the observed voxels, but this leads to computationally prohibitive updates.\nWe stack together the affinely registered sparse images from the entire collection. We learn a single set of mixture model parameters within overlapping subvolumes of 21\u02c621\u02c621 voxels in the isotropically sampled common atlas space. Subvolumes are centered 11 voxels apart in each direction. We use a cubic patch of size 11\u02c611\u02c611 voxels, and instead of selecting just one patch from each volume at a given location, we collect all overlapping patches within the subvolume centered at that location. This aggregation provides more data for each model, which is crucial when working with severely undersampled volumes. Moreover, including nearby voxels offers robustness in the face of image misalignment. Given the learned parameters at each location, we restore all overlapping patches within a subvolume.\nWhile learning is performed in the common atlas space, we restore each volume in its original image space to limit the effects of interpolation. Specifically, we apply the inverse of the estimated subject-specific affine transformation to the cluster statistics prior to performing subject-specific inference.\nOur implementation is freely available at https://github.com/ adalca/papago."}, {"section_title": "IV. EXPERIMENTS", "text": "We demonstrate the proposed imputation algorithm on two datasets and evaluate the results both visually and quantitatively. We also include an example of how imputation can aid in a skull stripping task."}, {"section_title": "A. Data: ADNI dataset", "text": "We evaluate our algorithm using 826 T1-weighted brain MR images from ADNI [15] 1 . We downsample the isotropic 1mm 3 images to slice separation of 6mm (1mm\u02c61mm inplane) in the axial direction to be of comparable quality with the clinical dataset. We use these low resolution images as input. All downsampled scans are affinely registered to a T1 atlas. The original images serve as the ground truth for quantitative comparisons. After learning model parameters using the data set, we evaluate the quality of the resulting imputations. 1 Data used in the preparation of this article were obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimers disease (AD). , and improvement over nearest neighbor interpolation using MSE (bottom). All statistics were computed over 50 scans randomly chosen from the ADNI dataset. Image intensities are scaled to a r0, 1s range."}, {"section_title": "B. Evaluation", "text": "We compare our algorithm to three upsampling methods: nearest neighbour (NN) interpolation, non-local means (NLM) upsampling, and linear interpolation [19] . We compare the reconstructed images to the original isotropic volumes both visually and quantitatively. We use the mean squared error,\nof the reconstructed image Z relative to the original high resolution scan Z o . We also compute the related peak signal to noise ratio,\nBoth metrics are commonly used in measuring the quality of reconstruction of compressed or noisy signals.\nC. Results   Fig. 4 illustrates representative restored images for subjects in the ADNI dataset. Our method produces more plausible structure. The method restores anatomical structures that are almost entirely missing in the other reconstructions, such as the dura or the sulci of the temporal lobe by learning about these structures from the image collection. We provide additional example results in the Supplementary Materials. Fig. 5 reports the error statistics in the ADNI data. Due to high variability of MSE among subject scans, we report improvements of each method over the nearest neighbor interpolation baseline in the same scan. Our algorithm offers significant improvement compared to nearest neighbor, NLM, and linear interpolation (p \u010f 10\u00b45, 10\u00b44\n2 , 10\u00b42 7 , respectively). Our method performs significantly better on all subjects. The improvement in MSE is observed in every single scan. Similarly, our method performs consistently better using the PSNR metric (not shown), with mean improvements of up to 1.4\u02d80.44 compared to the next best restored scan."}, {"section_title": "D. Parameter Setting", "text": "We analyze the performance of our algorithm while varying the values of the parameters, and the sparsity patterns of the observed voxels. For these experiments, we use four distinct subvolumes that encompass diverse anatomy from ADNI data, as illustrated in Fig. 6 . We start with isotropic data and use different observation masks as described in each experiment.\nHyper-parameters. We evaluate the sensitivity of our method under different hyper parameters: the number of clusters, k P r1, 2, 5, 10, 15s and the number of dimensions of the low dimensional embedding d P r10, 20, 30, 40, 50s. While different regions give optimal results with different settings, overall our algorithm produces comparable results for the middle range of these parameters. We run all of our experiments with k \" 5 and d \" 30.\nSparsity patterns. First, we evaluate how our algorithm performs under three different mask patterns, all of which allow for the same number of observed voxels. Specifically, we (i) use the true sparsely observed planes as in the first experiment; (ii) simulate random rotations of the observation planes mimicking acquisitions in different directions; and (iii) simulate random mask patterns. The latter setup is useful for denoising or similar tasks, and is instructive of the performance of our algorithm. Fig. 7 demonstrates that our algorithm performs better under acquisition with different directions, and similarly under truly random observations as more entries of the cluster covariance matrices are directly observed.This demonstrates a promising application of this model to other settings where different patterns of image voxels are observed. Slice Thickness. We also investigate the effects of slice thickness on the results. The model treats the original data as high resolution planes. Here, we simulate varying slice thickness by blurring isotropic data in the direction perpendicular to the slice acquisition direction. We then use the sampling masks of the scans used in the main experiments to identify observed, albeit blurred, voxels. Fig. 8 shows that although the algorithm performs worse with larger slice thickness, it provides plausible imputation results. For example, results show minimal noticeable differences, even for a blur kernel of \u03c3 \" 1mm, simulating a slice with significant signal contribution from 4mm away. Our method, which treats observed slices as thin, is nevertheless robust to slice thicknesses variations. "}, {"section_title": "E. Skull Stripping", "text": "We also illustrate how imputed data might facilitate downstream image analysis. Specifically, the first step in many analysis pipelines is brain extraction -isolating the brain from the rest of the anatomy. Typical algorithms assume that the brain consists of a single connected component separated from the skull and dura by cerebral spinal fluid [29] . Thus, they often fail on sparsely sampled scans that no longer include a clear contrast between these regions. Fig. 9 provides an example where the brain extraction fails on the original subject scan but succeeds on our reconstructed image."}, {"section_title": "F. Clinical Dataset", "text": "We also demonstrate our algorithm on a clinical set of 766 T2-FLAIR brain MR scans in a stroke patient cohort. These scans are severely anisotropic (0.85\u02c60.85mm in-plane, slice separation of 6mm). All subjects are affinely registered to an atlas and the intensity is normalized. Fig. 10 illustrates representative restoration improvements in T2-FLAIR scans from a clinical population. Our method produces more plausible structure, as can be especially seen in the close-up panels focusing on anatomical details. We provide additional example results in the Supplementary Materials. "}, {"section_title": "Ground truth", "text": "Our method Linear interpolation NLM"}, {"section_title": "V. DISCUSSION", "text": "Modeling Choices. We explicitly model and estimate a latent low-dimensional embedding for each patch. The likelihood model (5) does not include the latent patch representation x i , leading to observed patch covariance\nSince the set of observed voxels O i varies across subjects, the resulting Expectation Maximization algorithm [6] becomes intractable if we marginalize the latent representation out before estimation. Introducing the latent structure simplifies the optimization problem.\nWe investigated an alternative modeling choice that instead treats each missing voxel as a latent variable. In particular we consider the missing values of patch y i as latent variables, which can be optionally combined with the latent lowdimensional patch representation. These assumptions lead to an Expectation Conditional Maximization (ECM) [14] , [18] , a variant of the Generalized Expectation Maximization where parameter updates depend on the previous parameter estimates. The resulting algorithm estimates the expected missing voxel mean and covariance directly, and then updates the cluster parameters (see Supplementary Materials for a complete derivation). The most notable difference between this formulation and simpler algorithms that iteratively fill in missing voxels and then estimate GMM model parameters is in the estimation of the expected data covariance, which captures the covariance of the missing and observed data (c.f. [18] , Ch.8). We found that compared to the method presented in Section II, this variant often got stuck in local minima, had difficulty moving away from the initial missing voxel estimates, and was an order of magnitude slower than the presented method. We provide both implementations in our code.\nRestoration. Our restoration method assumes that the observed voxels are noisy manifestations of the low dimensional patch representation, and reconstructs the entire patch, including the observed voxels, leading to smoother images. This formulation assumes the original observed voxels are noisy observations of the true data. Depending on the downstream analyses, the original voxels could be kept in the reconstruction. In addition, we also investigated an alternative reconstruction method of filling in the missing voxels given the observed voxels as noiseless ground truth (not shown). This formulation leads to sharper but noisier results. The two restoration methods therefore yield images with different characteristics. This tradeoff is a function of the noise in the original acquisition: higher noise in the clinical acquisition leads to noisier reconstructions using the alternative method, whereas in the ADNI dataset the two methods perform similarly. In addition, imputation can be achieved by sampling the posterior distribution rather than using conditional mean estimation, enabling a better estimate of the residual noise for downstream analysis.\nUsability. Our model assumes that whether a voxel is observed is independent of the intensity of that voxel. Although the voxels missing in the sparsely sampled images clearly form a spatial pattern, we assume there is no correlation with the actual intensity of the voxels. The model can therefore be learned from data with varying sparseness patterns, including restoring data in all acquisition directions simultaneously.\nThe proposed method can be used for general image imputation using datasets of varying resolution. For example, although acquiring a large high resolution dataset for a clinical study is often infeasible, our algorithm will naturally make use of any additional image data available. Even a small number of acquisitions in different directions or higher resolution than the study scans promise to improve the accuracy of the resulting reconstruction.\nThe presented model depends on the image collection containing similar anatomical structures roughly aligned, such as affinely aligned brain or cardiac MR scans. Smaller datasets that contain vastly different scans, such as traumatic brain injuries or tumors, may not contain enough consistency to enable the model to learn meaningful covariation. However, a wide range of clinical datasets contain the anatomical consistency required, and can benefit from the proposed method.\nInitialization. We experimented with several initialization schemes, and provide them in our implementation. A natural initialization is to first learn a simple GMM from the linearly interpolated volumes, and use the resulting parameter values as initializations for our method. from the linearly interpolated volumes, and using the resulting means with diagonal covariances as an initial setting of the parameters. We start with a low dimensional representation to be of dimension 1, and grow it with every iteration up to the desired dimension. We found that this approach outperforms all other strategies."}, {"section_title": "VI. CONCLUSIONS", "text": "We propose an image imputation method that employs a large collection of low-resolution images to infer fine-scale anatomy of a particular subject. We introduce a model that captures anatomical similarity across subjects in large clinical image collections, and imputes, or fills in, the missing data in low resolution scans. The method produces anatomically plausible volumetric images consistent with sparsely sampled input scans.\nOur approach does not require high resolution scans or expert annotations for training. We demonstrate that our algorithm is robust to many data variations, including varying slice thickness. The resulting method enables the use of untapped clinical data for large scale scientific studies and promises to facilitate novel clinical analyses."}, {"section_title": "ACKNOWLEDGMENT", "text": "We acknowledge the following funding sources: NIH NINDS R01NS086905, NIH NICHD U01HD087211, NIH NIBIB NAC P41EB015902, NIH R41AG052246-01, 1K25EB013649-01, 1R21AG050122-01, NSF IIS 1447473, Wistron Corporation, and SIP.\nData collection and sharing for this project was funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from several agencies listed at http://adni.loni.usc.edu/about/."}, {"section_title": "APPENDIX A EXPECTATION MAXIMIZATION UPDATES", "text": "Following (5), the complete likelihood of our model is:\nwhere X \" tx i u. The expectation of this probability is then\nComputing this expectation requires evaluating I Erks, I Erx i |ks, and I Erx i x T i |ks, which is trivially done to obtain the expectation step updates (6) - (8) .\nFor the maximization step, we optimize (20) with respect to the model parameters.\nwhere \u03b4 and A are defined in (9) and (11), respectively. By combining (22) and (21), we obtain\nWe therefore update \u00b5 j k via (23), followed by W j k using (22) . Finally,\nSUPPLEMENTARY MATERIAL DERIVATION OF ALTERNATIVE MODEL In this section, we explore the parameter estimation for an alternative model. Specifically, letting M i be the set of missing voxels of patch y i , we treat y M i i as latent variables, instead of explicitly modeling a low-dimensional representation x. We show the maximum likelihood updates of the model parameters under the likelihood (5). We employ the Expectation Conditional Maximization (ECM) [14] , [18] variant of the Generalized Expectation Maximization, where parameter updates depend on the previous parameter estimates.\nThe complete data likelihood is ppY; \u03b8q \"\nThe expectation step updates the statistics of the missing data, computed based on covariates of the known and unknown voxels: where the correction in p s ijl can be interpreted as the uncertainty in the covariance estimation due to the missing values.\nGiven estimates for the missing data, the maximization step leads to familiar Gaussian Mixture Model parameters updates:\nwhere rS i s jl \" p s ijl . In additional to the latent missing voxels, we can still model each patch as coming from a low dimensional representation. We form C k \" W k W T k`\u03c3 2 k I as in (3), leading to the complete data likelihood:\nThe expectation steps are then unchanged from (26)- (28) with C k replacing \u03a3 k . The maximization steps are unchanged from (29)- (31), with \u03a3 k now the empirical covariance in (30) . We let U \u039bV T \" SVDp\u03a3 k q be the singular value decomposition of \u03a3 k , leading to the low dimensional updates\nFinally, we let C k \" W k W T k`\u03c3 2 k I. Unfortunately, both learning procedures involve estimating all of the missing voxel covariances, leading to a large and unstable optimization. "}]