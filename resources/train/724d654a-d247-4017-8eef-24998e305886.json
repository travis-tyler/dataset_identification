[{"section_title": "INTRODUCTION", "text": "The Program for the International Assessment of Adult Competencies (PIAAC) is a multicycle survey of adult skills and competencies sponsored by the Organization for Economic Cooperation and Development (OECD). The survey examines a range of basic skills in the information age and assesses these adult skills consistently across participating countries. The first cycle of PIAAC included three rounds: 24 countries participated in 2011-12 (round 1); 9 additional countries participated in 2014-15 (round 2); and 5 additional countries participated in 2017-18 (round 3). The United States has participated in all three rounds of the first cycle of PIAAC. The round 1 (PIAAC 2012) survey design was consistent with the international requirements (OECD 2016). In round 2 (PIAAC 2014), a supplemental sample was drawn to enhance the round 1 sample (Hogan et al. 2016). The combined PIAAC 2012/2014 sample is nationally representative of the U.S. adult population 16-74 years old. The round 3 (PIAAC 2017) data collection had two core objectives. First, it was designed to produce a nationally representative sample of the U.S. adult population 16-74 years old. Second, the sample was designed to arrive at a large enough sample size that, when combined with the 2012 and 2014 samples, can produce small area estimates for the U.S. counties. PIAAC is the sixth of a series of adult skills surveys, sponsored by the National Center for Education Statistics (NCES), that have been implemented in the United States. It is preceded by the Young Adult Literacy Survey (YALS) in 1985, the National Adult Literacy Survey (NALS) in 1992, the International Adult Literacy Survey (IALS) in 1994, the National Assessment of Adult Literacy (NAAL) in 2003, and the International Survey of Adult Literacy and Life Skills (ALL) in 2003. 1 In 2009, NCES published model-dependent estimates for states and counties using the NALS and NAAL data (available at https://nces.ed.gov/naal/estimates/Index.aspx). These estimates were called \"indirect\" estimates to distinguish them from standard or \"direct\" estimates that do not depend on the validity of a statistical model. The 2009 state and county estimates were produced using small area estimation (SAE) techniques that rely on survey data as well as data from other sources such as decennial censuses (1990 and 2000) for each of the two survey years. The demand for reliable small area estimates has greatly increased in the past decades (see for example, Czajka, Sukasih, and Maccarone 2014). Many federal agencies have been producing estimates for states and counties so that policymakers can plan and allocate resources and target interventions as necessary. Some examples include: Small Area Income and Poverty Estimates (SAIPE); Small Area Health 1 A history of the NCES-sponsored adult literacy surveys is available at https://nces.ed.gov/surveys/piaac/history.asp."}, {"section_title": "PIAAC Indirect Estimation Methodology", "text": "\n\n1-4 Table 1-1 provides a summary of the sample sizes in the 2012, 2014, and 2017 surveys, which results in 12,330 completed cases in the 2012/2014/2017 combined sample. As shown in table 1-2, of the 3,142 U.S. counties, 100 were at least a part of a sampled PSU in 2012/2014 and 103 in 2017. Table 1-2 also provides the number of counties in the 2012/2014 and 2017 samples with one or more completed cases. There are 99 counties in PIAAC rounds 1 and 2 with one or more completed cases, 99 counties with one or more completed cases in round 3, and a total of 185 unique counties with one or more completed cases. Table 1-3 provides a breakdown of the county sample sizes for the combined 2012/2014/2017 sample.  \n\n2-1\n\n\n\n\n\n\n\n5-7\n\n5-14\n\n\n\n\n\n\n\nA-7  \nA-9    \n\n\n\n\n\n\n\n\n\n\n\n"}, {"section_title": "1-2", "text": "Insurance Estimates (SAHIE); state and local area employment and unemployment statistics; county estimates of diabetes prevalence, incidence, and risk factors; state and county estimates of cancer risk factors and screening; and state estimates from the National Survey of Drug Use and Health. Similarly, access to information about proficiency levels of adults' literacy and numeracy skills at the state and county levels has been essential for policymakers, educators, and researchers to evaluate the distribution of proficiency levels across various areas, understand variables impacting literacy, and develop programs aimed at improving the proficiency levels of adults in the United States. See Czajka, Sukasih, and Maccarone (2014) for general descriptions of these SAE programs as well as additional examples of other programs across different agencies, and refer to sections 2.1 and 2.2 for further descriptions of a selected number of SAE programs across federal agencies. As the demand for reliable small area estimates has increased in the past decades, the SAE literature and research findings also grew rapidly, with significant enhancements made in the methodology and approaches (see Rao and Molina 2015) since the last round of SAE was produced using data from the 1992 NALS and 2003 NAAL surveys. This report describes the advanced statistical methodology used to produce state and county indirect estimates of various proficiency levels of adults for individual states and counties using data from the first cycle of PIAAC, using the SAE approach. As mentioned above, SAE is a model-dependent approach that produces indirect estimates for areas where survey data are insufficient for direct estimation. SAE models \"borrow strength\" across related small areas through auxiliary information to produce reliable \"indirect\" estimates for small areas. The SAE models rely on covariates available at the small area level and PIAAC survey data. In addition, small area models make specific allowance for between-area random effects that account for between-area variances beyond what is explained by covariates (e.g., percentage with less than a high school diploma) included in the model. Based on the results of these models, NCES derived small area estimates for all states and counties in the United States and produced a tool called the \"U.S. PIAAC Skills Map: State and County Indicators of Adult Literacy and Numeracy\" to view heat maps (i.e., data values represented by shades of colors) and to compare proficiency estimates across states or counties. The precision measures associated with each indirect estimate are based on the sophisticated statistical methodology that attempts to account for all sources of error. In the absence of any other literacy assessment data available for individual states and counties, the indirect estimates provide a general picture of literacy for all states and counties. Lacking these modeldependent estimates, covariates highly related to literacy and numeracy, such as educational attainment and poverty, have generally been used as proxy indicators of state and county proficiency levels. The estimates presented in the Skills Map website were developed using data from the actual assessments administered in the PIAAC survey and covariate data from the American Community Survey, which is administered by the Census Bureau. The estimates are thus predictions of how the adults in a state or county would have performed had they been administered the PIAAC assessment."}, {"section_title": "1-3", "text": "The remainder of this chapter contains a brief review of the PIAAC sample design, a description of proficiency measures in PIAAC, and the type of estimates and local areas for which small area estimates have been produced. A brief description of the website that hosts the small area estimates is also included in this chapter."}, {"section_title": "PIAAC Sample Design", "text": "Over the course of the years 2012 to 2017, PIAAC surveyed individuals 16-74 years old 2 in the United States as part of an international study involving over 35 countries. Data collected and combined over these years were, through the use of sophisticated statistical methods, used to produce indirect (also referred to as small area) estimates of literacy and numeracy proficiency. The combined household sample was created from three data collection efforts that took place in 2012, 2014, and 2017. In each year, a four-stage stratified area probability sample was selected. In the first stage, primary sampling units (PSUs) were selected, consisting of individual counties or groups of contiguous counties. The 2014 sample was designed as a supplement to 2012 and used the same sampled PSUs as 2012. The PSUs for 2017 were selected in such a way as to reduce overlap with the PSUs in 2012/2014. In the second stage, secondary sampling units (SSUs) were selected, consisting of Decennial Census blocks or block groups. In the third stage, dwelling units (DUs) were selected. A Screener interview was administered to each DU, and used to identify the eligible persons within selected DUs. At the fourth stage, a sampling algorithm was implemented within the computer-assisted personal interviewing (CAPI) system to select one or more sample persons among those identified to be eligible. Once selected, the Background Questionnaire (BQ) interview was completed. Upon completion of the BQ, the respondents were provided either the paper-and-pencil or computer-based assessment, based on whether they reported having any previous computer experience during the BQ interview or whether they refused the computerbased assessment as well as their performance on the Information and Communication Technology (ICT) core instrument. Together, the combined sample has 12,330 respondents from 185 counties. A set of weights for the combined PIAAC 2012/2014/2017 sample was created for the purpose of creating small area estimates. The weights for the PIAAC 2012/2014/2017 sample were created by combining the final PIAAC 2012/2014 and PIAAC 2017 weights and calibrating to population totals. The PIAAC Technical Report provides details of the sample designs for each data collection year in chapter 3, and discusses the weighting processes in chapter 8. 2 The PIAAC 2012 sample was limited to 16-to 65-year-olds."}, {"section_title": "1-5", "text": "Of the 50 states, plus the District of Columbia, 44 have completed cases in the combined 2012/2014/2017 sample. The number of completed cases by state ranges from just over 40 to almost 1,200. This is illustrated in figure 1-1. "}, {"section_title": "Proficiency Measures in PIAAC", "text": "The first cycle of PIAAC assessed three domains of cognitive skill: \uf06e literacy (including reading component); \uf06e numeracy; and \uf06e problem solving in technology-rich environment. The test design for PIAAC is based on an approach most common to the major large-scale surveys. This design, called matrix sampling, entails administering a subset of items from a larger item pool, with different groups of respondents answering different sets of items. This design allows a reduction in the response burden for an individual, and at the same time makes it inappropriate to use any statistic based PIAAC Indirect Estimation Methodology"}, {"section_title": "1-6", "text": "on the number of correct responses in reporting the survey results. This limitation is overcome through item response theory (IRT) scaling used to derive scores for each domain. To provide a measure of the uncertainty of the cognitive measurement, PIAAC uses 10 plausible values (multiple imputations) drawn from a posterior distribution based on the IRT scaling of the cognitive items with a latent regression model using information from the BQ in a population model. For more details about the IRT models and the model equations see the PIAAC Main Study technical report (Yamamoto, Khorramdel, and von Davier 2013)."}, {"section_title": "Types of Estimates, Areas of Interest, and Results Website", "text": "County-and state-level estimates of adult literacy and numeracy proficiency are produced for the proportion at or below Level 1, the proportion above Level 1 and below Level 3 (referred to as \"proportion at Level 2\"), proportion at Level 3 and above, and the average. Discussion and illustrations in the remaining chapters mainly focus on one estimate (percentage at or below Level 1 of literacy). Results pertaining to other small area estimates are given in appendix C. The county and state estimates are published in the Skills Map website at http://nces.ed.gov/pubsearch/ pubsinfo.asp?pubid=2020047. The website uses a visualization-based system that allows users access to the small area estimates through heat maps and summary card displays. This user-friendly website provides precision estimates and facilitates statistical comparisons among counties and states. The summary cards provide descriptions of the small areas in terms of all eight outcomes, and can show two selected states and/or two selected counties in a side-by-side comparison. There is also ability to see a comparison of distributions (from the American Community Survey) for select demographics to help in explaining the SAE model-based results. Summary text discusses results from statistical comparisons between areas on eight proficiency outcomes. That is, for both Literacy and Numeracy, comparisons will be conducted on the proportion at or below Level 1, proportion at Level 2, proportion at Level 3 and above, and the average score. The areas involved in the comparisons cover the following: \uf06e state-to-nation; \uf06e state-to-state; \uf06e county-to-state; \uf06e county-to-county within state; and \uf06e county-to-county across states."}, {"section_title": "1-7", "text": "Reports are available on the website that visually display the estimates for all counties within a state, and all states within the United States. The remainder of this state and county indirect estimates methodology report is divided into seven chapters and three appendixes. \uf06e Chapter 2 provides a review of the methodology (as relevant to PIAAC) with focus on new developments in the SAE area since the NAAL model development in 2003, and summarizes the results of our review of such recent literature, including both unit-level and area-level modeling. In addition, this section includes a literature review of the methodologies used in recently published SAE estimates produced for some major federal statistics in the United States, and discusses the relationship and relevance to PIAAC SAE. An important component of SAE modeling is the direct survey estimates. \uf06e Chapter 3 provides a summary of the approaches used in the computation of direct estimates, their variances, and the steps involved in preparing the estimates for the SAE modelling step including the application of survey regression estimation (SRE) approach for calibrating the weights to county level totals, pooling plausible values into one point estimate and one variance estimate to reflect the plausible value variations in the estimation, and smoothing the variance to improve the stability of the variance estimates. \uf06e Another key component of SAE modeling is access to predictor variables that are measured consistently across all counties and states, and that are effective predictors of the estimated proportion of adults at different levels or average of literacy and numeracy. Because of the small number of data points (counties with PIAAC sample) for the model development, a small number of covariates needed to be selected from the large pool of covariates. Chapter 4 contains a listing of the sources of covariates for model development, along with a description of the approach used for arriving at a final set of variables for the model development stage. \uf06e SAE model estimation, and prediction and aggregation are described in chapter 5. The model estimation is conducted on the small number of counties with PIAAC sample. The resulting model is used to make predictions for all 3,142 counties. Weighted aggregations of the county predictions lead to the state estimates. The model estimation and prediction process produce precision estimates that account for various sources of error, including sampling error, imputation error and modeling error. A brief description of a simulation study designed to evaluate the relative performance of estimates under a selected subset of various models and software is also given in this chapter. Often, users may be interested in conducting multiple comparisons to make simultaneous inferences. Therefore, the PIAAC state and county indirect estimates website provides comparisons between areas on eight outcomes. That is, for each of literacy and numeracy, comparisons are conducted on the proportion at or below Level 1, proportion at Level 2, proportion at Level 3 and above, and the average. Finally, section 5.6 contains a description of the methodology used for making the simultaneous inferences included in the PIAAC website. \uf06e Large-scale SAE programs generally employ an extensive model evaluation process since models are never a perfect fit to the data, and systematic errors can manifest themselves. It is especially important for PIAAC to conduct a thorough evaluation of the model development since over 90 percent of the county estimates rely solely on the model predictions. Chapter 6 contains the highlights of the full range of model diagnostics, sensitivity analysis, and evaluation results. In the chapter are some important evaluation results, such as in figure [6][7][8][9][10][11][12][13][14][15][16], which shows that the model predictions are generally close to the survey regression estimates. Another example is figure 6-17, which shows the reduction in mean square error from the model when compared to the survey regression estimates. The three appendixes include the list of potential covariates (appendix A), the simulation study results (appendix B), and the preliminary study results (appendix C). A list of references follows chapter 7."}, {"section_title": "BACKGROUND", "text": "The term small area estimation (SAE) refers to a variety of methods or statistical techniques to estimate information or, more precisely, parameters for subpopulations or smaller areas of interest. SAE uses survey data in combination with auxiliary data at the small area level from other sources to model the estimates of interest. A wide variety of models have been developed for generating small area estimates; the two major types of models are known as area level and unit level. 3 The area-level approach models the small area estimate of interest in terms of auxiliary data at the area level, whereas the unit-level approach models the underlying variable of interest in terms of unit-level auxiliary data, and then aggregating the individual predictions for each small area. Two major schools of thought underlie the proliferation of techniques and models: frequentist and Bayesian. Among the frequentist is the Empirical Best Linear Unbiased Predictor (EBLUP), which can be used to estimate random effects, such as through SAS Proc Mixed. The Linear Mixed Model (LMM) may be used when the dependent variable follows a normal distribution. The Generalized Linear Mixed Model (GLMM) is considered when the dependent variable has a nonnormal error distribution. Empirical Bayes is considered a frequentist approach and is conducted when the prior distribution is based on the data itself. Among the Bayesian approaches, Hierarchical Bayes (HB) is applied when there is a fixed prior distribution. The reader can find many details on the various types of SAE models in Rao and Molina (2015). A thorough literature review was conducted prior to developing the Program for the International Assessment of Adult Competencies (PIAAC) SAE methodology, in addition to organizing a U.S. PIAAC Summit of International SAE Experts (including William Bell, Partha Lahiri, Danny Pfefferman, Jon Rao) and those on the PIAAC team (e.g., Robert Fay) to discuss modeling issues relevant to PIAAC SAE (more discussion is in section 2.4). A summary of the literature review, including reviews of the federal SAE programs is given in Krenzke et al. (2018). The remainder of this chapter contains the highlights of the literature review as relevant to this report. First, a brief summary of model-based approaches applied to literacy data is given in section 2.1. Next, a short summary of our review of recent SAE developments and methodologies used to produce SAE official statistics published by some major federal statistical agencies in the United States is given in section 2.2. Section 2.3 provides a brief summary of the considerable amount of research and development in SAE methods that has taken place in the past decade. A systematic decisionmaking process was critical for the research and development of the PIAAC SAE models. It began with the literature review of recent advances in SAE models, followed by the U.S. PIAAC Summit of International SAE Experts, which provided various recommendations (e.g., determine level-of-effort for modeling different levels of proficiency, incorporate cross-validation into the simulation, investigate different models, look into variable selection through various approaches). Further investigation occurred, including a simulation study (described in appendix B) that provided key findings, and follow-ups led to decisions related to the recommendations made by the International SAE Experts. Section 2.4 provides more information about the topics that were addressed during the research and development that led to the final PIAAC SAE models."}, {"section_title": "Approaches Applied to Literacy Data", "text": "The National Center for Education Statistics (NCES) produced state-and county-level estimates of the proportions of adults lacking basic prose literacy skills for the 2003 National Assessment of Adult Literacy (NAAL) and 1992 National Adult Literacy Survey (NALS) (Mohadjer et al. 2009) using SAE methodology. The model was developed using a HB unmatched area-level model based on the 2003 NAAL and auxiliary data from the 2000 census (Mohadjer et al. 2011). 4 The aim of the NAAL SAE model was to estimate the true proportion of adults who are lacking basic prose literacy skills (as evaluated by the NAAL instrument) for all counties and states in the United States. The NAAL and NALS county and state estimates of the proportions of adults lacking basic prose literacy skills are available at https://nces.ed.gov/naal/estimates/Index.aspx. The website allows users to compare the proportions of adults lacking basic prose literacy skills across any two states or counties that are specified in advance, and across years for a single state or county. It should be pointed out that the proficiency assessment instruments and scales used in NAAL and NALS are different from those used in PIAAC, and thus the small area estimates for counties and states from NAAL and NALS SAE are not comparable with the corresponding estimates from PIAAC. The only two years available in the data tool are the 2003 NAAL and 1992 NALS. Other applications of model-based methods applied to literacy data include Gibson and Hewson (2012), where the authors used a unit-level, nonlinear (logistic) model, for literacy and numeracy binomial outcomes for the 2011 Skills for Life Survey that was conducted in the United Kingdom. Yamamoto (2014) presents a model-dependent approach to produce estimates of skill distribution for provinces based on population parameters derived from the Canadian PIAAC data and auxiliary information such as 4 The process was repeated for the 1992 National Adult Literacy Survey (NALS)."}, {"section_title": "2-3", "text": "census. In addition, Bijlsma et al. (2017) use the Netherlands' PIAAC data and focus on obtaining the literacy estimates at the municipality level in the Netherlands using model-based SAE techniques in an HB framework. They used a basic unit-level model originally proposed by Battese, Harter, and Fuller (1988) to model the average literacy score since literacy scores are continuous per individual and area and assumed to have a linear relation with individual-level covariates. An area-level model originally proposed by Fay and Herriot (1979) was used to model the proportion of low literates."}, {"section_title": "Review of Major Federal SAE Programs", "text": "As noted in Czajka, Sukasih, and Maccarone (2014), a number of federal statistical agencies have developed SAE methods and have published SAE estimates. This section contains brief reviews of a select number of these programs that are thought to have some relevance to the SAE efforts for the PIAAC survey, and thus could be helpful in guiding the decisions about final plans for PIAAC. The Census Bureau's Small Area Income and Poverty Estimates (SAIPE) program produces a number of income and poverty-related estimates for states, counties, and school districts using area-level models based on the American Community Survey (ACS) and small area auxiliary data from the Internal Revenue Service and other sources (https://www.census.gov/programs-surveys/saipe.html). Examples of the model-based small area estimates produced by the SAIPE program include median household income and the proportions and numbers of total persons, children, and school-age children below the poverty level. These annual estimates are used to allocate funding for many federal grant programs to state and local jurisdictions, including nutrition assistance, medical assistance, jobs training, housing, and education (see, for example, Citro and Kalton 2000, chapter 2). The Office of Applied Studies at the Substance Abuse and Mental Health Services Administration regularly produces substate-level estimates of substance abuse using unit-level models based on the National Survey of Drug Use and Health (NSDUH) and a wide variety of sources for the auxiliary data. 5 As in the NAAL, a multistage sample design was used for the NSDUH. The primary sampling units (PSUs) are collections of adjacent census block groups. The NSDUH small area model differs from the SAIPE and NAAL models in that it is an individual person-level (unit-level) model. Refer to Folsom, Shah, and Vaish (1999) for more detail about the NSDUH approach to SAE. Raghunathan et al. (2007) provide an application that includes a large national survey (National Health Interview Survey [NHIS]) and a supplementary survey (Behavior Risk Factor Surveillance Survey [BRFSS]) where small area parameters are jointly estimated in a small area model. Other specialized approaches include the SAE work that has been conducted for the National Crime Victimization Survey to produce state-level time series estimates of victimization and for large metropolitan areas. The sources for the covariates include the ACS and the Federal Bureau of Investigation's Uniform Crime Reports. More information can be found in Fay and Diallo (2015), Fay, Planty, and Diallo (2013), Fay and Diallo (2012), and Fay and Li (2011). 6 Also, as mentioned in Bauder, Luery, and Szelepka (2016), the Small Area Health Insurance Estimates (SAHIE) program at the Census Bureau produces SAE estimates of numbers and proportions of those with and without health insurance coverage for demographic groups within states and counties. The demographic groups are defined by age, sex, and income, and in addition, for states by race and ethnicity. Income groups are defined in terms of income-to-poverty ratio (IPR), which is the family income divided by the appropriate federal poverty level."}, {"section_title": "Review of Recent Developments in SAE", "text": "A considerable amount of research and development in SAE methods has taken place since the development of SAE procedures for the 2003 NAAL. The text by Rao (2003) presents a comprehensive overview of the methods, history, and applications of SAE methods. The book has since been updated (Rao and Molina 2015). As the demand for reliable small area estimates has greatly increased in the past decades, the SAE literature and research findings also grew rapidly. Pfefferman (2013)  informative sampling and nonresponse as well as model selection and checking. In the case of informative sampling or not-missing-at-random nonresponse, the model assumed for the population may not apply to the sample data. If not properly accounted for, the resulting predictions can be seriously biased. The model developed by Fay and Herriot (1979) has been one of the most widely-used models in SAE. Benavent and Morales (2016) Pfeffermann, Terryn, and Moura (2008) considered the situation that the target response variable is a continuous variable with a large peak of zero values. This occurs in the assessment of literacy proficiency in developing countries where zero outcomes indicate illiteracy and positive scores measure the level of literacy. Their estimates of interest are average literacy scores and the proportion of positive scores in small areas. A two-part random effect model was developed and fitted to the data with mixed distribution. Part one assumes a linear mixed model for positive responses, and part two assumes a generalized linear mixed model for the probabilities of positive responses. Nonzero correlations are allowed between the random effects in the two parts. This paper concluded that fitting a linear mixed model without differentiating the large frequency of zero values from other positive values will result in highly biased predictors and wrong coverage rates of credibility intervals. Accounting for the correlations between the random effects of the two parts is the best choice, but it may only improve the predictions marginally. The magnitude of the correlations and the importance of accounting for them in the model mainly depend on the predictive power of the available covariates."}, {"section_title": "Key Features of PIAAC Indirect Estimation Methodology", "text": "The U.S. PIAAC International SAE Experts discussed the importance of striving toward a more full picture of the distribution for local areas. Therefore, estimating proportions in more than one level was conducted, specifically for the proportion at or below Level 1, proportion at Level 2, and the proportion at or above Level 3, in both literacy and numeracy, and in addition estimating proficiency averages, resulted in eight outcome measures for each state and county. An area-level bivariate HB linear three-fold model was developed for proportions (discussed in section 5.2.1), and for averages, an area-level univariate HB linear three-fold model was developed (discussed in section 5.2.2). There were many models considered to estimate the outcome measures, and the final models resulted from the following preliminary steps: (1) review applications of SAE to literacy data, review federal programs, and review recent developments in literature (as discussed above); (2) U.S. PIAAC Summit of International SAE Experts held in December 2017; (3) simulation work as documented in appendix B; and (4) working through the various steps of the process using prefinal data. Key features of the final models include the following: \uf06e Informative sampling and nonresponse were incorporated. The sample of states and counties are not simple random samples; therefore, the sample design is informative. Also, weighting adjustments for nonresponse can reduce bias to the extent that the weighting variables are related to the proficiency scores. When nonresponse is not sufficiently explained by the weighting variables, informative nonresponse exists. The U.S. PIAAC International SAE Experts discussed being able to handle informative sampling and informative nonresponse because, otherwise, the process will lead toward biased estimates. The probability of selection of PSUs was included in the covariate selection process (described in chapter 4) but was not an important factor to include in the small area model. Relating to informative nonresponse, it was assumed that any literacy-related nonresponse was below Level 1, and for the estimation of averages, the first percentile of proficiency scores was imputed for literacy-related nonrespondents. This is a reasonable assumption because such nonrespondents could not complete the Background Questionnaire and assessment (conducted in English) due to a literacy-related reason (language barrier, reading/writing barrier, or mental disability). More discussion of literacy-related nonresponse is in section 3. \uf06e Area-level models were used. In an area-level model, direct estimates produced at the local area-level are the prime elements in the modeling process. One part of an area-level model is a \"sampling model,\" where survey-weighted estimates are produced for the small areas with sample-design based variance estimates. The regression model is developed using predictors at the small area level and could include variables at higher levels also. Unlike the area-level approach, the unit-level model is built at a much lower level such as individual persons or households. That is, a unit-level model uses covariates available at the person level to generate person-level values, which are aggregated to compute statistics at the area level. There is potential for smaller MSE and for producing estimates for a wide range of other subgroup of interest. The basic unit-level models ignore sample-design based variance estimates at this very low level. Extensions have included a random effect term as an attempt to capture the between area variation (see Rao and Molina 2015). In addition, the following discussion points occurred during the U.S. PIAAC Summit of International SAE Experts. If the model is linear, either the area-level or the unit-level approach could be used for a PIAAC small area program. The area-level approach is more design-based, since the basic building blocks are the sample (design-based) estimates at the targeted local level, as well as the sample-design based variance estimates at this level. Operationally, the area-level approach certainly works with a much simpler dataset, with one record for each local area rather than one record for each household or person, and in that sense is easier to work with in practice. While unit-level models were included in early research, for the various reasons stated above, a decision was made to move forward with the area-level models (details are provided in chapter 5). \uf06e HB was used. Another discussion point during the U.S. PIAAC Summit of International SAE Experts was the use of HB models, over alternatives such as Empirical Bayes. The estimation of HB models generates Markov Chain Monte Carlo (MCMC) samples, which provide the ability to obtain good estimates of the variability of the indirect estimates by accounting for several sources of error. The model is written using a hierarchical form (a sampling level for the direct estimates of proportions and a linking level for the relationship between the target proportions and the covariates), prior distributions are adopted for the model parameters, and the Bayesian approach is used for inference, where credible intervals can accompany all point estimates. More discussion about the benefits of using HB models is provided in section 5.2."}, {"section_title": "2-7", "text": "\uf06e Both univariate or multivariate models were used. The area-level HB bivariate linear three-fold model estimated PIAAC proportions for at or below Level 1, and at or above Level 3. For each MCMC sample, the results were combined to estimate the proportion at Level 2, and provided credible intervals for all point estimates. The model takes advantage of the covariance between domains, which may result in reduced MSE. Due to the demands on the model fitting and small number of data points, it was decided to fit the bivariate model for proportions separately for literacy and numeracy. Relating to estimating averages, consideration was being given to estimate averages for literacy and numeracy simultaneously; however, it was decided to have separate univariate models with the same covariates, as supported by the covariate selection process. \uf06e Three levels of random effects were used. Another feature of the model is the inclusion of three levels of random effects: county, state and census division. 7 The benefits of the threefold model are (1) benchmarking 8 the estimates will not be necessary as estimates are controlled through the random effects (e.g., the aggregation of county indirect estimates within a state should align with the state indirect estimate), (2) estimates for states without sample will have some contribution from the PIAAC sample because all census divisions, have PIAAC sample, and (3) associations of counties within states, and states within divisions will have some impact while the same random effect is applied to those areas. 3-1"}, {"section_title": "DIRECT ESTIMATION", "text": "As a result of the research and development phase (summarized in section 2.4), the indirect estimation production process for the U.S. Program for the International Assessment of Adult Competencies (PIAAC) 2012/2014/2017 started with area-level modeling. The area-level modeling process began with computing direct estimates for the areas of interest. In section 3.1, we provide a summary of the methods used for the production of direct estimates and associated variances. The variances of the direct estimates and variance estimates can be large in counties with small sample sizes. As presented in section 3.2, we implemented model-assisted methods as tools for (a) improving stability of the direct estimates (through survey regression estimation [SRE] [S\u00e4rndal and Hidiroglou 1989]) and (b) for smoothing the variances of the direct estimates (through generalized variance functions [Wolter 2007]). The direct estimates after the SRE and variance smoothing process served as inputs to the small area estimation (SAE) models. Variance estimates are intended to account for the error associated with the item response theory (IRT) modeling in addition to the sampling error. As described in chapter 10 of Hogan et al. (2016), different PIAAC respondents took different sets of items that could be of various levels of difficulty, and it would be inappropriate to base the proficiency estimates simply on the number of correct answers obtained. Therefore, large-scale assessments using matrix sampling rely on IRT models. The PIAAC IRT modeling resulted in 10 plausible values (PVs) (Mislevy 1991) for each respondent, reflecting the uncertainty in the respondents' proficiency estimate. To capture this uncertainty, we implemented a multiple imputation approach for calculating direct estimates and the associated variance estimates, using the PVs. This is described further in section 3.1. Another issue to account for in direct estimation is informative nonresponse; informative nonresponse is the condition when nonresponse is not sufficiently explained by the weighting variables. Being able to handle informative nonresponse needs to be considered and addressed because, otherwise, the process will lead toward biased estimates. For PIAAC, this means literacy-related nonresponse needs to be addressed, which is estimated to be about 5 percent of the population. Within a county, all cases receiving a final weight contribute to the direct estimate. This includes respondents to the background questionnaire (BQ) as well as sampled persons that did not respond to the BQ for a literacy-related reason (language barrier, reading/writing barrier, or mental disability). All respondents to the BQ have literacy scores, whereas the BQ literacy-related nonrespondents do not. For the estimation of proportions, we assumed any literacy-related nonresponse is below Level 1, and for the estimation of averages, we imputed a proficiency score for each PV using the first percentile of the respondents' scores for the corresponding PIAAC Indirect Estimation Methodology"}, {"section_title": "3-2", "text": "PV. The first percentile was chosen because it is similar to the average score for persons who completed the BQ but could not complete the assessment for a literacy-related reason."}, {"section_title": "Direct County Estimates", "text": "The first step in the area-level modeling process was to produce direct estimates for the 185 counties with sample. Under the multiple imputation approach, we first computed the survey estimate for the m-th PV for county k as: where = the final PIAAC 2012/2014/2017 national weight for person l in county k, is the proficiency score (for average) or an indicator variable for the proficiency level (for proportions), and = the number of cases in county k. Then the county-level direct estimate (\u0302) was calculated as: The multiple imputation estimate of the variance (Rubin 1987) is: where \u03022 is the within-imputation variance and \u03022 is the between-imputation variance. The withinimputation variance component was computed as the average of the sampling variance for each of the 10 PVs: where \u03022 is the sampling variance of the estimated mean or proportion for PV m. The betweenimputation component was calculated as: PIAAC Indirect Estimation Methodology"}, {"section_title": "3-3", "text": "Sampling variances were calculated using the Taylor series method (Wolter 2007, p. 234), with primary sampling units (PSUs) as strata and secondary sampling units (SSUs) as variance units (clusters), where PSUs and SSUs are defined in section 1.1. Details about the computation of sampling variances using the Taylor series method for estimated mean and proportion can be found in SAS online documentation. 9 Direct variance estimates could not be computed for 15 counties that only had one SSU with PIAAC data. The models described in section 3.3 were used to predict the variances for these counties. The remaining 170 counties had at least two SSUs and sample sizes of five or more. Table 3-1 shows the distribution of the proportion of variance for direct estimates attributed to imputation error, as measured through multiple imputation (i.e., between-imputation variance) across the 170 counties with at least two SSUs. For literacy skills, multiple imputation contributes on average 11 percent of total variance for the average score, 22 percent of total variance for the proportion at or below Level 1, 35 percent of total variance for the proportion at Level 2, and 20 percent of total variance for the proportion at Level 3 and above. Across counties, the contribution to the total variance from multiple imputation ranges from nearly 0 percent to 90 percent. The distribution of the proportion for numeracy is similar to that for literacy. This illustrates that imputation variance can be a significant portion of the variance and cannot be ignored when producing variance estimates. 9 https://support.sas.com/documentation/cdl/en/statug/63962/HTML/default/viewer.htm#statug_surveymeans_a0000000224.htm https://support.sas.com/documentation/cdl/en/statug/63962/HTML/default/viewer.htm#statug_surveyfreq_a0000000247.htm. Finally, to estimate the covariance for proportions, let \u03021 , \u03022 , and \u03023 be the estimated proportions at or below Level 1, at Level 2, and at Level 3 and above, respectively, and let \u03022 1 , \u03022 2 , and \u03022 3 be the corresponding variance estimates. Given that Var( 1 + 2 ) = Var( 3 ), the covariance of the proportion at or below Level 1 and the proportion at Level 2 can then be computed using the following formula. In Section 3.3 we will smooth covariance using this formula based on smoothed variances. Cov(\u03021 ,\u03022 ) = (\u03022 3 \u2212\u03022 1 \u2212\u03022 2 )/2. (3f)"}, {"section_title": "Survey Regression Estimation (SRE)", "text": "PIAAC was designed to be a nationally representative sample and does not produce efficient direct estimates at the county level; therefore, as is shown in table 3-2, the variances of the direct estimates can be large. This is particularly true for counties with small sample sizes. Therefore, we used SRE to reduce the variance associated with the survey estimates. Rao and Molina (2015, pp. 21-23) describe the use of these estimates in SAE, their derivation, and the usual Taylor series approach to estimating their variance. The SRE is a model-assisted approach that is used to bring survey estimated county population totals in line with county totals from a reliable external source and improve the stability of the survey estimates. The SRE process also helps to reduce variances that are used in the SAE modeling process. In the modified form proposed by S\u00e4rndal and Hidiroglou (1989), the survey regression estimate of the mth PV for county k can be written as: where is the vector of population totals in county k corresponding to the predictors in the unit-level regression model (defined below), is the vector of survey-weighted regression coefficients from the unit-level regression based on the whole sample for PV m, is the known size of the eligible population in the county, the = \u2212 are the unit-level residuals from the regression fit in county k for PV m, the are the corresponding survey weights, and is the sample in county k. In later sections, the survey regression estimates for proportions are denoted by and the survey regression estimates for averages are denoted by , for county k in state j of census division i. The original survey regression estimator for a mean or proportion would have been instead: which divides all terms in the original survey regression estimator of a total by . In other words, the modification replaced by a sample-based estimate \u2211 \u2208 in the correction term, giving the estimator generally better properties conditional on the estimated population size. In general, the sample estimate of the number of eligible respondents in a county would have been variable at the PSU level and particularly so at the county level in PSUs comprising more than one county. The predictors for the unit-level regression model were chosen based on the availability of population totals that had the same definition and coverage as the corresponding PIAAC variables. Predictors were further limited to PIAAC variables that had a low level of item nonresponse (less than 5 percent), and imputation was used to fill in the missing values. The models for the eight literacy/numeracy estimates used the same set of predictors, and the final set consisted of 15 indicator variables for the following 15 Each indicator takes a value of 1 if the person falls into the category or 0 otherwise. For example, a 16year-old would have a value of 0 for all 15 indicators. The corresponding county-level population totals were obtained from the American Community Survey (ACS) 2012-2016. 10 The age range for each indicator differs based on the availability of data in the ACS. The variance of the SRE for each PV was estimated using the Taylor series method by applying the standard variance expression to the residuals ( ), with PSUs as strata, and with SSUs as segments/clusters. Let , be the number of segments, , sampled in PSU , and let , be the number of segments in county within PSU . If PSU included more than one county, , is generally a random variable. Because the PSU is treated as a stratum, each county falls in a single stratum, simplifying the notation for the variance estimator somewhat. After exclusion of counties with only one sample segment, the sampling variance for each PV was estimated with is an indicator with value 1 if segment of PSU is in county and 0 otherwise. Rao and Molina (2015, p. 23) suggest this estimator. It is also similar and asymptotically equivalent to one of two versions offered by S\u00e4rndal, Swensson, and Wretman (1992)  As a remark, the variance estimators do not incorporate an explicit account for variability in the estimation of \u0302. An explanation is that the individual segment-level residuals are (1), that is, of order 1 in probability, and / is (1), that is, of order 1, while the error in \u0302, that is, \u0302\u2212 , is ( \u2212 1 2 ), and consequently this term does not contribute to the first-order Taylor expansion. In small counties, the variability in is a possible consideration. Because the ACS samples approximately 1 in 8 households over the course of 5 years, roughly 250 households would be sampled in a county with a population of 5,000. If a county that size was a sampled PSU, then the ACS sample would only be about three times as large as the PIAAC sample. But counties this small may have been grouped with others in forming PSUs and would not receive the full PSU-level sample. In addition, such counties represent less than half of 1 percent of the U.S. population, and so few if any would be sampled as PSUs by themselves. Consequently, ignoring the contribution of ACS variance on the variance of the SRE has a negligible effect on the overall analysis."}, {"section_title": "3-7", "text": "The multiple imputation formulas in section 3.1 were then applied to account for imputation variance, using \u0302in place of \u0302 in equations (3b) and (3e) to obtain the survey regression estimates and variances for the sampled counties. As with the direct estimates, the variance could not be estimated for the 15 single-SSU counties. The result of this step is one point estimate and one variance estimate for each of the eight outcomes. As shown in table 3-2, the SRE made a large impact on the variance estimates. That is, the median variance decreased substantially compared to the direct estimate for averages, the proportion at or below Level 1, and the proportion at Level 3 and above. For the proportion at Level 2, there was only a modest decline. The R 2 for the Level 2 models were of the order of 0.04 compared to 0.27 for the at or below Level 1 models, 0.25 for the Level 3 and above models, and 0.40 for averages."}, {"section_title": "Variance Smoothing", "text": "The Hierarchical Bayes models chosen for the PIAAC SAE process, discussed in chapter 5, assume that the variances of the SRE county estimates are known, whereas in practice they are unknown. Since the survey regression estimates of these variances are subject to substantial sampling error, the true variances have also been predicted using a modeling approach. An important feature of the development of the model for predicting the variances is that approximate values will suffice since the values of the variances affect the estimates of the PIAAC small area estimates in only a minor way. Their main impact is in stabilizing the widths of the credible intervals. Inspired by the generalized variance function (GVF) methods in chapter 7 of Wolter (2007), variance estimation smoothing models were developed separately for proportions (discussed first) and averages (discussed second). "}, {"section_title": "Variance Estimation Smoothing Model for Proportions", "text": "After evaluating a number of variance smoothing options (see appendix B), the variance of proportions was smoothed by fitting a weighted least square model for the effective sample size, which was computed as the sample size divided by the design effect. The number of clusters in a county and average cluster size in a county were used as the model predictors. And the weight was the number of clusters in a county minus 1. Note that counties with fewer than four clusters or SRE proportions less than 0.02 were excluded from the model (24 out of 185 counties); i.e., the model was estimated based on the remaining 161 counties. After the model was fit, the smoothed variance was derived for all 185 counties. The variance smoothing process started with the county-level survey regression estimates and variances from section 3.2, which had been combined over the PVs, and then smoothed the combined effective sample size. A separate model was fit for each of the six proportions (at or below Level 1, at Level 2, and at Level 3 and above for literacy and numeracy), where the dependent variable was ln( ), the natural log of the effective sample size for county k. Specifically, =\u0302(1 \u2212\u0302)/\u03022, where \u0302 was the SRE proportion and \u03022 was the SRE variance for county k. Then model was specified as where is the effective sample size for county k, is the number of clusters in county k, is the average cluster size for county k, and is an error term. The natural log was used to satisfy model assumptions based on some model diagnostics. The model was weighted by \u2212 1. The exponentiation of the predicted value from this model, \u0303, was used to derive the smoothed variance as \u03032 =\u0302(1 \u2212 )/\u0303. Covariances were calculated using formula (3f) and the smoothed variances. In later sections, the variance-covariance matrix after smoothing will be denoted by .  "}, {"section_title": "Variance Estimation Smoothing Model for Averages", "text": "As with proportions, variance smoothing for average proficiency scores needs to take into account the design effect (DEFF) associated with weight variation and clustering within the small areas. Using the combined variances from the SRE process, the variance is smoothed by fitting a weighted least square model as below: where is the residual variance for each county k, is the number of clusters in each county (SSU), is the average cluster size for each county k, and ^2 is the estimated population variance of the literacy/numeracy scores among each county k. The model is weighted by \u2212 1. The exponentiation of the predicted value from this model would be the smoothed variance. It should be noted that counties with PIAAC Indirect Estimation Methodology"}, {"section_title": "3-11", "text": "fewer than four clusters were excluded from the smoothing process (24 out of 185 counties), so the model is estimated based on the 161 counties. After the model was fit, the smoothed variance was derived for all 185 counties. In later sections, the smoothed variances are denoted by 2 . Table 3-4 provides the parameter estimates for the smoothing process for literacy and numeracy average. "}, {"section_title": "Initial Identification of County and State Covariates", "text": "Reliable data sources and variables that are potential covariates of proficiency levels were initially identified, and more than 70 county-level variables across five major variable types were obtained as potential predictors from eight data sources (see details in section 4.2). The major county-level variable types include variables related to demographic characteristics (i.e., race/ethnicity, age, gender, marital status), socioeconomic status (i.e., poverty, income, employment status, occupation), education (i.e., education, English-speaking ability), location (i.e., urbanicity, census division), immigration status (i.e., length of stay for foreign-born people, migration), and other (i.e., journey to work, housing unit tenure/phone service, plumbing facilities, health, tax). In addition, the primary sampling unit (PSU) selection probability was also initially included as a potential county-level covariate to account for the informative sampling design; however, it was not identified as a significant predictor through the covariate selection process described in section 4.3. These variable types were chosen because they were found to be related to the adult literacy skills in previous studies (Rampey et al. 2016;Goodman et al. 2013;Kirsch et al. 2002;Greenberg et al. 2001;Coley 1996;Weiss, Hart, and Pust 1991), and were available for all the counties in our sample.  (Vaish 2017). Therefore, we gathered variables from multiple sources and attempted to find the most suitable variables to fit the SAE model."}, {"section_title": "Initial Set of County and State Covariates Sources", "text": "The selected data sources have reliable data publicly available for all counties (or all states) and usually publish the updated data regularly (i.e., annually). In terms of using Big Data as possible covariates, Marchetti et al. (2015) state that Big Data derived from the digital crumbs that humans leave behind in their daily activities are providing more accurate proxies of social life, and using Big Data together with SAE techniques can provide more accurate estimation. In fact, researchers started to utilize mobile usage data to predict illiteracy in developing countries (Sundsoy 2016), or use the number of bookstores, newspaper circulation, library system, or periodical publishing resources to predict literacy levels of the U.S. cities (Miller 2016). However, we did not identify any clear Big Data source (such as social media and interactive platforms, or data publicly available on the Web) that was useful and added value beyond the data sources that had been already considered. 11 Available at: https://www.census.gov/programs-surveys/saipe/technical-documentation/methodology/counties-states/county-level.html."}, {"section_title": "4-4", "text": "The following subsections provide brief descriptions of the data sources and the variables chosen from each source. We begin with sources for county-level variables. More details about the variables are given in appendix A. "}, {"section_title": "Covariates Selection Process", "text": "A key step in model development involves selecting a smaller set of covariates from a large set of potential covariates. As mentioned above, for PIAAC SAE, more than 70 variables in the county-level and more than 20 variables in the state-level variable pools were identified as potential covariates. Each variable was examined against the outcome to identify if transformation was needed, and found that logtransformed income variables (i.e., median household income) have a more linear relationship with the outcome; thus, the three income variables were log transformed."}, {"section_title": "4-7", "text": "The process of selecting covariates was conducted in two phases. In the first phase, all the county-and state-level variables were considered as fixed effects and the number of variables was reduced through the variable selection methods. The implemented variable selection method (1) used a correlation matrix among the covariates themselves to identify highly correlated variables that led to dropping one variable in the highly correlated pair to avoid multicollinearity; (2) used the LASSO (Tibshirani 1996) method to select several sets of covariates for each of the four survey regression estimation (SRE) outcome models. The purpose of the screening (first step) based on the correlation was to avoid multicollinearity. The pairs of covariates with high correlation among themselves were examined, and it was decided whether to drop any of the covariates to avoid multicollinearity. In this case, the decision to drop covariates was based on whether they were correlated with other covariates, how many other covariates, and how high the correlation was. We also ran the LASSO regression using all the different sets of covariates (1. Include all the covariates; 2. Exclude those with 0.9 or higher correlation; 3. Exclude those with 0.8 or higher correlation; 4. Exclude those with 0.7 or higher correlation) and different lambda values, which resulted in a final set of covariates that was selected for all the models. Once the list of covariates was reduced to several sets of covariates, the second phase evaluated the different sets of covariates using a crossvalidation process (as recommended by the U.S. PIAAC International SAE Experts) adding the random effect estimation to arrive at the final list of covariates. This final list of covariates was used in modeling all the different SRE outcomes (i.e., literacy/numeracy proportion/average models)."}, {"section_title": "Phase 1-Covariates Reduction", "text": "This section describes the method used for the covariates reduction process with two steps: (1) a correlation matrix among the potential covariates themselves, as well as the covariates against the SRE outcomes; (2) a LASSO method (i.e., multivariate LASSO) using the county-and state-level covariates to predict the SRE outcomes. Details are provided below. Step 1. The correlation estimation is a natural first step of the overall process. between the covariates and each of the eight outcomes (SRE proportion at or below Level 1, proportion at Level 2, proportion at or above Level 3, and average proficiency score for both literacy and numeracy) through SAS. Covariates with high correlations with the SRE outcomes turned out to be the educationrelated variables (i.e., |\u03c1|=0.7 for proportion of population with lower than high school education vs. proportion at or below Level 1 literacy), poverty-related variables (i.e., |\u03c1|=0.6 for proportion of PIAAC Indirect Estimation Methodology 4-8 population lower than poverty threshold versus proportion at or below Level 1 literacy), employmentrelated variables (i.e., |\u03c1|=0.6 for proportion of population not in labor force vs. proportion at or below Level 1 literacy), and health-related variables (i.e., |\u03c1|=0.5 for proportion of population having no health insurance vs. proportion at or below Level 1 literacy). Covariates with high pairwise correlations (i.e., |\u03c1|>0.7) with other covariates were treated as with \"high multicollinearity.\" To avoid the potential effect of high multicollinearity on the model performance and prediction, one covariate from each pair of covariates with high multicollinearity was manually eliminated from the potential covariates pool, based on the correlations with the other covariates. In general, covariates with higher correlations with other covariates, or with lower correlations with the SRE outcomes, were generally eliminated first. In the cases where two highly correlated covariates were correlated by definition, and were found to have key impact on the SRE outcomes (i.e., proportion of population less than high school, proportion of population more than high school), both variables were kept for the following covariate reduction process. It should be noted that direct estimates are subject to sampling error, and the sample sizes that contribute to each county direct estimate vary widely; therefore, the correlation coefficients presented in this section are biased and attenuated. As pointed out in Lahiri and Suntornchost (2015), the true population correlations are higher. The correlation estimates could be improved if the sampling error is taken into account, as described in Lahiri and Suntornchost (2015). Step 2. LASSO estimation was carried out in R using the glmnet package (Friedman, Hastie, and Tibshirani 2010) to conduct the covariate selection. LASSO (Tibshirani 1996) is a method that applies shrinkage factors to regression coefficients, and thus can more efficiently perform stable covariate selection. The procedure can select a few covariates that are related to the dependent variable from a large amount of possible covariates. LASSO-based methods use \"penalized regression\" models that impose constraints on the estimated coefficients that tend to shrink the magnitude of the regression coefficients, often eliminating the covariates entirely by shrinking their coefficients to zero. Therefore, nonzero coefficients are estimated for true covariates, whereas the coefficients for irrelevant variables are zeroed out. The final covariate reduction process was based on applying the LASSO model with standardized covariates and LASSO penalty. To select covariates for the proportion estimates (proportion at or below Level 1, proportion at Level 2, proportion at or above Level 3), we used multivariate LASSO with the option \"family = \"mgaussian\" in glmnet to predict both SRE outcomes (proportion at or below Level 1, proportion at or above Level 3) at the same time. The second SRE outcome was not included as it could be derived from the other two SRE outcomes. This multitask learning method is useful when there are a number of correlated responses. When a variable is selected, a coefficient will be fit for each response, and a 'group-lasso' penalty was applied on the coefficients for each response. It will result in the selection PIAAC Indirect Estimation Methodology 4-9 of the same predictors across all responses. For average proficiency scores, we used univariate LASSO to do the variable selection. Because the proportion at or below Level 1 and proportion at or above Level 3 were modeled together in the same model (see section 5.2.1), there were a total of four SRE outcome models being analyzed: literacy/numeracy proportion model, literacy/numeracy average model). For each of the four SRE outcome models, we adjusted the lambda (the parameter that controls the overall strength of the penalty) using various values close in magnitude to the lambda that minimizes the mean cross-validated error (0.02 and 0.03 for the proportion model, and 2 and 3 for the average model) to arrive at two sets of covariates for each of the four models. We expected each of these sets contained less or equal to 10 covariates and there were some variation between the sets. The lambda adjustment and coefficients of the predictors were requested through the cv.glmnet function, from which two lambda values were applied and two lists of selected covariates were generated, one is more parsimonious and with fewer covariates and the other is more generous and with more covariates. Covariates with nonzero coefficients from the LASSO model were considered as potential covariates for the phase 1 lists of covariates to be included in the phase 2 cross validation process. Table 4-1 presents the list of selected phase 1 covariates, with the source, year, description and label. Table 4-2 presents the selected covariates with the marker \"X\" identifying the selected variables for each SRE outcome models (with two lambda options). It should be noted that for the numeracy proportion model, both lambda options (0.02 & 0.03) resulted in the same set of selected covariates.   Tables A-3 and A-4 in appendix A provide the listings of the county-and state-level selected variables with the correlation estimates, random forest importance score, and LASSO estimates, sorted descending by the correlations, respectively, for each SRE outcome."}, {"section_title": "4-10", "text": ""}, {"section_title": "Phase 2-Cross Validation", "text": "The SAE models (discussed in chapter 5) were used to make predictions for the nonsampled counties (the counties that have no PIAAC sample or have too few sampled cases to be usable). Once the set of covariates was reduced to a manageable number, the cross-validation approach was applied to validate the model using the area-level HB linear three-fold model, as well as determine the best set of predictors through trying different combinations, for all eight outcomes. A cross-validation analysis evaluates the prediction power of the model as compared to other models using alternative sets of covariates selected from the LASSO models through k-fold cross validation (Fushiki 2011). The k-fold cross validation was implemented in the following steps to select the best set of covariates for the bivariate model of literacy proportions, which models two proportions (P1, proportion at or below Level 1, and P3, proportion at or above Level 3) jointly."}, {"section_title": "4-12", "text": "\uf06e Sorted the 184 12 sampled counties from the largest to the smallest sample sizes and divided them into groups of 10 counties, with the last group having only 4 counties. There were 19 groups in total. \uf06e For each group of 10 counties, the counties were randomly assigned to 10 subsets, with each subset containing 1 county from the group. For the group with 4 counties, the counties were randomly assigned to four subsets. At the end of step 2, each subset contained 18 or 19 counties with varying sample sizes. \uf06e Excluding the counties in subset 1, the counties in the remaining 9 subsets were used to fit the bivariate SAE model (the model specification is discussed in chapter 5) for each given set of covariates and made predictions for the group of counties that were deleted (the prediction method is discussed in chapter 5). \uf06e Repeated step 3 by excluding subsets 2 through 10, one at a time. At the end of this process, the predicted proportions at or below Level 1, at Level 2, and at or above Level 3 were calculated for all the counties. \uf06e Compared the predicted proportions against the direct estimates for all 184 counties and only the counties with large sample sizes (sample size greater than 100). Calculated the sum of squared differences. The smaller the sum of squared differences, the better the set of covariates predicted the proportions for the counties that were excluded from modeling. For literacy proportions, five sets of covariates (all county level) were used to fit the models and to compare the predicted proportions against the direct estimates. The results are summarized in table 4-3. Table 4-3. Covariates used in cross validation for literacy proportions and results of summed squared differences between predicted proportions and direct estimates: 2012/2014/2017 For the cross validation analysis, scenarios 1 and 2 were chosen from the LASSO models with \u03bb=0.03 and \u03bb=0.02, respectively. Scenario 3 used the five predictors adopted by the Hierarchical Bayes model in the National Assessment of Adult Literacy (NAAL) study to predict the proportion of adults lacking basic prose literacy skills, and added the percentage of Hispanic as a predictor, which is highly correlated with proportion at or below Level 1. Compared to scenario 3, scenario 4 added another predictor, proportion of people with no health insurance coverage, which was shown to be significant in the LASSO models for predicting proportions and averages for both literacy and numeracy. Scenario 5 added an extra predictor, proportion in service occupation, to the set of covariates used on scenario 4 because this variable was shown to be a significant predictor in the LASSO models for predicting averages for both literacy and numeracy. The results in table 4-3 show that scenarios 2, 4, and 5 have similar performance and their sum of squared differences between model predictions and direct estimates are smaller for all three proportions than those from scenarios 1 and 3. Therefore, the variables in table 4-4 were considered as the base variables for the other three models for literacy averages, numeracy averages, and numeracy proportions. The goal was to include the same terms in each model to help retain the associations between the resulting eight outcomes. That being said, the covariate/cross-validation selection process looked at various alternative models PIAAC Indirect Estimation Methodology"}, {"section_title": "4-14", "text": "versus keeping the same variables in each. The results in tables C-1 through C-3 show that there were no alternative models that would justify a different set of covariates. Combining these results with the other cross validation results for literacy average and numeracy proportions and average (the results can be found in appendix C), a decision was made to use the seven county-level covariates from the 2013-2017 ACS data, as shown in table 4-4, in all four models fitted for proportions and averages for literacy and numeracy. Table 4-5 shows the correlation coefficients among these covariates. The seven covariates are highly correlated with the proportions and averages. For example, the adjusted R-square is 0.58 for the linear regression of literacy proportions at or below Level 1 on the seven covariates.  The sample of states and counties are not simple random samples; therefore, the sample design is informative. One approach to addressing informative sampling is to include the probability of selection of the primary sampling units (PSUs) in the model. However, the probability of selection of PSUs was included in the covariate selection process (described in chapter 4) but was not an important factor to include in the small area model. Also, weighting adjustments for nonresponse can reduce bias to the extent that the weighting variables are related to the proficiency scores. Being able to handle informative sampling needs to be considered and addressed because, otherwise, the process will lead toward biased estimates. \uf06e Accounting for Sources of Error. Another key discussion point made during the U.S. PIAAC Summit of International SAE Experts was that models need to account for all important sources of variability so that the reported estimated error reflects the true level of precision. For PIAAC, these sources of error include the following: -Sampling error results from probability sampling and that different results would occur for repeated samples. -Model error results from estimation of model parameters, such as area-level random effects. This type of error accounts for different results occurring for different runs of the modeling process due to its random mechanism in fitting the models. Hierarchical Bayes (HB) methods typically account for the noise contributions attributed to estimating model parameters (beta coefficients, and random effects variancecovariance parameters). -Measurement error occurs when using covariates that are subject to sampling or nonsampling error. The model can be as strong as its covariates' precision levels, and its strength of association with the small area estimate of interest. The covariates may have inaccurate measurements with possible systematic bias. Some models in the literature may rely on estimates in low levels of geography (e.g., full cross-tabulation of county-level variables from the American Community Survey [ACS], or tract-level data); however, these estimates are subject to substantial sampling error or measurement error. In order to allow for the propagation of the measurement error into the small area estimates, an approach such as introduced by Ybarra and Lohr (2008) could be incorporated; however, it may not be able to extend to this many variables. Therefore, we have not accounted for this source of measurement error. Related to the sampling error in county-level proportions from the ACS 5-year estimates, Bell et al. (2019) conclude that in the presence of measurement error"}, {"section_title": "5-2", "text": "(covariate sampling error) the model is misspecified if not accounted for, so it will misstate the prediction mean square error (MSE) except for counties with sampling error for the covariates equal to the average sampling error across the counties. In general, the resulting prediction error is an overestimate when the county's sampling error is below average, and more severely underestimates when above average. Because small counties could have less precise ACS data, we have notes on the state and county indirect estimates website to caution users that the covariates used in the model or predictions for counties with population less than 1,500 (2 percent of counties) may have higher associated uncertainty. -Prediction error results from making estimates from the final model for areas without sample cases. -Imputation error results from the generation of plausible values (PVs) and that different results would occur for replications of the imputation process. The PVs themselves come from a model, and that uncertainty has been accounted for in the SAE process. This section begins with a brief description of a simulation study designed to evaluate a selected subset of various models in section 5.1, and continues with technical details on the final SAE models in section 5.2. A summary of the model fitting process is provided in section 5.3, including a discussion of software considered and model estimation for proportions and average scores. Section 5.4 includes a description of how the predictions are computed, and section 5.5 provides a discussion of the precision measures. Finally, section 5.6 addresses the user's interest in simultaneous inference. In summary, the final models produced indirect estimates for eight outcomes, at or below Level 1, in Level 2, at or above Level 3, and averages for both literacy and numeracy. In addition, credible intervals were produced for each indirect estimate for states and counties."}, {"section_title": "Summary of Simulation Results", "text": "A simulation study (presented in appendix B) was conducted to evaluate the performance of possible small area estimators under conditions similar to those faced by the PIAAC application. By regarding a large sample from ACS as a true population and then drawing subsamples from it, the predictions from a small area estimator based on the subsamples were compared to the actual values in the full sample. A strength of the simulation approach is that it tests the small area estimators against actual data, rather than against artificial data created with the small area estimator in mind. The simulation study evaluated the HB approach implemented for National Assessment of Adult Literacy (NAAL) as well as several variants of it. One of the findings is that using survey regression estimators (SRE) and smoothed estimates of the variance can improve the performance of the small area estimates."}, {"section_title": "5-3", "text": "As a result, both SRE and variance smoothing were applied to the PIAAC county-level direct estimates before they were used in the SAE models. The comparisons of different SAE models showed some advantage to the bivariate models in predicting proportions over univariate models. The bivariate models are for two proportions (proportion of low education and proportion of high education) jointly whereas the univariate models only one proportion at a time, independently. It is then likely that the bivariate models provide more closely coordinated estimates of proportions concurrently than if they each were modeled in a univariate model. Moreover, the bivariate models support interval estimates for the sum of two proportions, equivalent in the PIAAC application to obtaining interval estimates for Level 2 (P2 = 1-P1-P3), where P1 represents the proportion at or below Level 1 and P3 represents the proportion Level 3 and above. Another finding is incorporating census division as a random effect together with state-and county-level random effects in the model may improve the results. Two software products, STAN and JAGS, were used in the simulation and produced essentially equivalent results."}, {"section_title": "Final SAE Models", "text": "The final SAE models for proportions and averages for literacy and numeracy are described in subsections 5.2.1 and 5.2.2, respectively."}, {"section_title": "Area-Level Bivariate HB Linear Three-Fold Model for Proportions", "text": "The progression of the previous research and simulation study has led to developing an area level bivariate HB linear three-fold model for estimating PIAAC proportions of literacy and numeracy proficiency at the county level. \uf06e Area-level-the model is fitted at the area level with the areas defined by counties, the input data being the sets of survey regression estimates and their associated variance estimates (smoothed), available at the county level. \uf06e Bivariate-two proportions (Level 1 and below and Level 3 and above) are modeled jointly whereas the third proportion (Level 2) is derived by subtracting the proportions of Level 1 and below and Level 3 and above from 1. \uf06e Linear-the linking model assumes linear relationship between the proportions and the predictors. \uf06e Three-fold-the model accounts for random effects at three nested levels defined by the county, census division, and state."}, {"section_title": "5-4", "text": "\uf06e HB model-the model is written using a hierarchical form (a sampling level for the direct estimates of proportions and a linking level for the relationship between the target proportions and the covariates), prior distributions are adopted for the model parameters, and the Bayesian approach is used for inference. Various sources of error are accounted for using the hierarchical model specification: smoothed sampling variances, and random effects at county, division and state levels. The use of Bayes methods over frequentist methods provides a straightforward framework for constructing summaries for the indirect estimates, including credible intervals, and for functions of the model parameters, such as the Level 2 proportion (defined as 1 minus the sum of the proportions at or below Level 1 and at or above Level 3). The use of HB methods over empirical Bayes methods provides a framework in which prior distributions for all the model parameters are adopted without using information from the observed data. For each Markov Chain Monte Carlo (MCMC) sample, the results can be combined to estimate the proportion at Level 2, and provide credible intervals for all point estimates. The model takes advantage of the covariance between the two set of proportions, i.e., proportions at or below Level 1 and proportions at Level 3 and above, which may result in reduced MSE of model estimates. Another feature of the model is the inclusion of three levels of random effects: county, state, and census division. The benefits of the three-fold model are that (1) benchmarking the estimates may not be necessary as estimates are controlled through the random effects (a consensus among the U.S. PIAAC International SAE Experts), 2estimates for states without sample will not be synthetic because all census divisions have PIAAC sample, and (3) associations of counties within states, and states within divisions will have some impact because the same random effect is applied to those areas. The model employs the traditional SAE structure, including a sampling model and a linking model, using matrix form notation to account for multiple domains, as follows: where is an index for the division, is an index for the state, is an index for the county, is a jointly normally 13 distributed bivariate vector of survey regression estimates for proportions at or below Level 1 and at or above Level 3, with associated estimated variance-covariance matrix , \u2032 is a vector of covariates, is a matrix of coefficients, and , , are county-level, state-level, and division-level random effects, respectively. 14 The estimated variance-covariance matrices are the result of smoothing functions of the variances for the survey regression estimates, and treated as fixed and known 13 Although the proportions are strictly between 0 and 1, their distributions can be approximated by normal distributions since is small and proportions are rarely close to 0 and 1. In case the predicted proportions from the models are outside the interval of [0,1], they are truncated at 0 and 1. 14 The county estimates are assumed to be independent of each other. For PIAAC, sampling occurred independently within PSUs. Among the 184 counties used in the modeling, 116 of them are from PSUs constructed from single PSUs, while the other 68 counties fall into 31 PSUs. The correlations between counties in the same PSUs are small and therefore ignored in all the SAE models in this section."}, {"section_title": "5-5", "text": "in the HB model. Independent priors are assumed for the regression coefficients and the random effects. Specifically, it is assumed ~(0,100), where the normal distribution specification uses the mean of 0 and the standard deviation of 10. It is also assumed that the random effects are mutually independent, following bivariate normal distributions, where , , and are 2 by 2 variance-covariance matrices. For example, in = ( ), the elements on the diagnol ,1,1 and ,2,2 are the variances of the random effects for P1 and P3, respectively, at the county level, and the elements off the diagonal ,2,1 and ,1,2 are identical and denote the covariance of the random effects for P1 and P3. The variance-covariance matrices , , and can be decomposed as follows: where , , and are diagonal matrices with standard deviations along the diagonal, and , , and are correlation matrices (with diagonal entries being equal to 1). In the model for literacy proportions, the prior distribution adopted for the standard deviation parameters (diagonal entries in , , and ) is Cauchy, with a location (median) hyper-parameter of 0 and a scale (half the interquartile range) hyperparameter of 5 ; the support of the distribution was restricted to the positive real line. An (1) prior (Lewandowski, Kurowicka, and Joe 2009) is adopted as the prior distribution for the correlation matrices , , and . In the model for numeracy proportions, (1) is adopted as the prior distribution for the Cholesky factors of correlation (lower triangular) matrices , , and , where = , = , and = , respectively. The idea behind the LKJ prior is based on the decomposition of the variance-covariance matrix, with priors adopted for its components. Some details are provided in STAN's guide and reference manual, available at http://blackwell.math.yorku.ca/ MATH6635/2016/stan-reference-2.12.0.pdf (STAN is the software used for PIAAC); see, for example, page 63 and page 72. More details on the specifications are provided in Lewandowski, Kurowicka and Joe (2009), whose authors' first initials are used to define 'LKJ,' too. Also, Alvarez, Niemi, and Simpson (2014) present a recent simulation study, on different priors for the variance-covariance matrices. PIAAC Indirect Estimation Methodology 5-6"}, {"section_title": "Modeling Averages-Area-Level Univariate HB Linear Three-Fold Model", "text": "An area-level univariate HB linear three-fold model was used for estimating PIAAC averages in domains. Similar to the HB models for proportions, the models for averages include three levels of random effects: county, state, and census division. The model is specified as follows: where is the survey regression estimate of average literacy or numeracy scores at the county level, normally distributed with associated estimated variance 2 , \u2032 is a vector of covariates, is a vector of coefficients, and , , are county-level, state-level, and division-level random effects, respectively. The estimated variances 2 are the result of smoothing functions of the variances for the survey regression estimates, and treated as fixed and known in the HB model. Independent priors are assumed for the regression coefficients and the random effects. Specifically, it is assumed ~(0,1000), where the normal distribution specification uses the mean and the standard deviation. It is also assumed that the random effects are mutually independent, following normal distributions, The variances of the random effects 2 , 2 , and 2 are assumed to follow a uniform prior distribution over a wide range, 0 to 1,000. The choice of vague (almost noninformative) priors for the model parameters ensures that little information about the values of the parameters is provided to the model, and hence the data (likelihood) have the major role in the posterior distribution."}, {"section_title": "Model Fitting", "text": "Two bivariate HB models and two univariate HB models were fitted for estimating proportions and averages for literacy and numeracy, respectively, based on the data from 184 counties in the PIAAC sample. One county was excluded due to its small sample size of 2 and negative survey regression estimate for proportion at or below Level 1 for literacy. To ensure reproducibility, the R and STAN (where STAN interfaces with R) starting points used in the generation of sequences of random numbers (seeds) were set equal to constants."}, {"section_title": "Software", "text": "The software chosen for the PIAAC state and county indirect estimates work was driven by model choice, which was driven mainly from the literature review. The review helped steer the decision toward a highly complex specification for proportions, which resulted in an area-level bivariate HB linear three-fold initial values for other model parameters were randomly generated within RSTAN) corresponding to the three independent MCMC chains and then updated all the parameter values of = ( , , , , , , , ) for proportions or = ( , , , , , 2 , 2 , 2 ) for averages in each chain. One set of the initial values of were created from running weighted linear regressions of the proportions or averages on the set of seven covariates, with the county sample size being the weight. The results are shown in table 5-1. The other two sets of initial values were derived by adding or subtracting a constant from the first set of the initial values.  After that point, for proportions models, 30,000 further iterations were produced for each of the three runs. Since the results from neighboring iterations after warm-up are correlated, they were \"thinned\" by taking a systematic sample of 1 in every 20; for averages models, 15,000 further iterations were produced for each of the three runs and were \"thinned\" by taking 1 in every 10. 16 Thus, over the three runs, a total of 4,500 iterations remained. These 4,500 final iterations (referred to as MCMC samples) then simulated the posterior distributions of all the parameters in . The means of the parameter estimates across the 4,500 MCMC samples are the HB estimates of the parameters. The results of the final HB models are shown in tables 5-2 and 5-3 for the parameters and the variances of the random effects. The signs of the regression coefficients are as expected in general. As an example for illustration, Education-LH is a significant predictor with a positive sign in the models for literacy and numeracy proportions P1. As another example, Black is a significant predictor with a positive sign in the models for literacy and numeracy proportions P1; and it is a significant predictor with a negative sign in the models for literacy and numeracy proportions P3 and averages. We note that Health insurance and service occupations are not significant (the 95 percent credible interval includes zero) in all the models. However, health insurance is marginally significant in the model for numeracy proportions P3 and service occupations is marginally significant in the models for literacy and numeracy averages. The posterior means of the intraclass correlations (ICCs) were computed for the four models for both literacy and numeracy. The ICCs are defined as: \u22121 , for the bivariate models for proportions, and as ( ) for the univariate models for averages. A summary of ICCs is as follows: 16 Thinning the sample reduces the autocorrelation between iterations. The \"thinning\" factor was chosen to ensure that the autocorrelation function computed from the \"thinned\" sample is close to zero."}, {"section_title": "5-10", "text": "For literacy averages: \u2022 County ranges from 0.04 to 0.47, with a mean of 0.23. \u2022 State ranges from 0.02 to 0.23, with a mean of 0.10. \u2022 Division ranges from 0.02 to 0.24, with a mean of 0.11. For literacy proportions at or below Level 1: \u2022 County ranges from 0.00 to 0.08, with a mean of 0.02. \u2022 State ranges from 0.00 to 0.11, with a mean of 0.03. \u2022 Division ranges from 0.00 to 0.08, with a mean of 0.03. For literacy proportions at or above Level 3: \u2022 County ranges from 0.00 to 0.16, with a mean of 0.04. \u2022 State ranges from 0.00 to 0.11, with a mean of 0.03. \u2022 Division ranges from 0.00 to 0.22, with a mean of 0.06."}, {"section_title": "For numeracy averages:", "text": "\u2022 County ranges from 0.04 to 0.45, with a mean of 0.22. \u2022 State ranges from 0.02 to 0.22, with a mean of 0.10. \u2022 Division ranges from 0.02 to 0.20, with a mean of 0.09. For numeracy proportions at or below Level 1: \u2022 County ranges from 0.00 to 0.10, with a mean of 0.03. \u2022 State ranges from 0.00 to 0.11, with a mean of 0.03. \u2022 Division ranges from 0.00 to 0.10, with a mean of 0.03. For numeracy proportions at or above Level 3: \u2022 County ranges from 0.01 to 0.27, with a mean of 0.08. \u2022 State ranges from 0.00 to 0.16, with a mean of 0.04. \u2022 Division ranges from 0.00 to 0.17, with a mean of 0.05. In general, the county ICCs tend to be larger than the state and division ICCs. Note that for the univariate models for averages three scalar ICCs were defined; however, for the bivariate models for proportions three ICC matrices (of dimension 2 x 2) were defined.   "}, {"section_title": "Predicted Values", "text": "As mentioned above, estimates of the parameters in each of the 4,500 MCMC samples were produced through the RSTAN software for sampled counties. Once the final model was processed and the model parameters estimated, the next step was to estimate the proportions and averages for the sampled counties, nonsampled counties, and for states and nation. The prediction process for sampled and nonsampled counties is described in sections 5.4.1 and 5.4.2, respectively. The process for making state-and nationallevel estimates is described in section 5.4.3."}, {"section_title": "Indirect Estimates for Sampled Counties", "text": "For the sampled counties, the posterior mean \u0302, which is also called the county-level HB model prediction (for proportions or averages), for sampled county k in state j and division i, is produced by the RSTAN software as: where the value of ( ) for MCMC sample b is obtained from"}, {"section_title": "Indirect Estimates for Nonsampled Counties", "text": "For the sampled counties, estimates of all the components on the right-hand side of equation 5bwere available. However, for all of the nonsampled counties, the values of ( ) were not available, and for the nonsampled counties in states without a sampled county, values of ( ) were not available either. To simulate the MCMC procedure, in cases where a component was not available, it was drawn at random from the appropriate normal distribution. Thus, following Rao (2003), for proportions, ( ) was drawn from (0, ( ) ) and for averages, ( ) was drawn from (0, 2( ) ). When necessary, ( ) was drawn from (0, ( ) ) for proportions and ( ) was drawn from (0, 2( ) ) for averages. For the nonsampled counties in states with one or more sampled counties, the estimated state effect was available from RSTAN. For such counties, the model prediction of ( ) was computed from is a random draw from (0, ( ) ) for proportions and from (0, 2( ) ) for averages."}, {"section_title": "5-15", "text": "For the nonsampled counties in states with no sampled county, the estimate of ( ) was computed from is a random draw from (0, ( ) ) for proportions and from (0, 2( ) ) for averages. In both cases, once the set of 4,500 values of ( ) was obtained, the posterior mean for nonsampled counties was computed using equation 5a. Due to the linearity of the model, an estimated proportion could be less than zero. This only occurred for the proportion at or above Level 3. Tables D-1 and D-2 in appendix D show one county with a negative value for the proportion at or above Level 3 in literacy, and eight counties with negative proportions at or above Level 3 in numeracy."}, {"section_title": "Indirect Estimates for States and Nation", "text": "The indirect estimates for states and nation were computed as weighted aggregates of small area county estimates, where the weights represent the proportion of the state's, division's, or nation's household population of adults aged 16-74 in each county. The county populations of the household residents 16-74 were obtained from the 2013-2017 ACS data. Table 5-4 compares the national-level model predictions with the direct estimates for proportions and averages for literacy and numeracy. The two sets of estimates are not significantly different. "}, {"section_title": "Measures of Precision for the Indirect Estimates", "text": "The primary measure of precision reported for each county-level indirect estimate is its credible interval, described in section 5.5.1. An alternative measure of uncertainty is the coefficient of variation (CV), discussed in section 5.5.2. An assessment of precision of the indirect estimates using both measures is provided in section 5.5.3."}, {"section_title": "Credible Intervals", "text": "A credible interval is a posterior probability interval used in Bayesian statistics for purposes similar to those of a confidence interval in frequentist statistics. A 95 percent credible interval is any interval with a probability under the posterior distribution of 0.95. For example, a statement such as \"following the model result, a 95 percent credible interval for is 7 percent to 21 percent\" means that the posterior probability that lies in the interval from 7 percent to 21 percent is 0.95. The 95 percent credible intervals for the county estimates \u0302 (or the state estimates \u0302, the division estimates \u0302, and the national estimate \u0302) were computed by calculating the 2.5 percent (lower bound) and 97.5 percent (upper bound) quantiles of ( ) (or ( ) , ( ) , and ( ) ), respectively, from the 4,500 MCMC samples that simulated the posterior distributions. Since these posterior distributions are skewed, the credible intervals are nonsymmetric around the estimate. Figure 6-15 in Section 6 shows the credible intervals of literacy proportions relative to the survey regression estimate for each sampled county. Due to the linearity of the model, the credible interval's lower bound of an estimated proportion could be less than zero. This occurred for the proportion at or below Level 1 and the proportion at or above Level 3 mostly for counties without PIAAC sample. Tables D-3, D-4, and D-5 show a list of counties where negative values occur for the credible interval's lower bound for literacy and numeracy, respectively."}, {"section_title": "Coefficient of Variation", "text": "The CV of the HB estimate for county k in state j and division i is computed as , PIAAC Indirect Estimation Methodology"}, {"section_title": "5-17", "text": "where the posterior variance Var(\u0302) is computed as The CV for states and nation can be computed similarly. Table 5-5 summarizes the distributions of the widths (the difference between the upper bound and the lower bound) of the credible intervals as well as the CVs for the 3,142 counties and 51 states in the United States, for literacy proportion at or below Level 1. Overall, the state predictions are more precise than the county predictions, and to a less extent, the counties with sample are more precise than counties without PIAAC sample (figure 6-17 in section 6"}, {"section_title": "Assessment of Precision Measures", "text": "shows that the indirect estimates of literacy proportions are more precise than the survey regression estimates for counties with PIAAC sample). For example, the median credible interval width for county predictions is 8.0 percent (i.e., percentage points), while the median is 6.1 percent for state predictions."}, {"section_title": "5-18", "text": "The table also shows that the median credible interval width is 7.2 percent for counties with PIAAC sample cases and 8.0 percent for counties without PIAAC sample. The CVs for the county-level model predictions are of the order of 10 percent. Estimates with CVs of this magnitude are considered precise. It is important for the users of these county predictions to recognize that for some counties the CVs can be large. While the state predictions are more precise, with a median CV of 8.1 percent, it is still important for users to consider the credible interval along with the model prediction."}, {"section_title": "Simultaneous Inference", "text": "As mentioned in chapter 1, the PIAAC state and county indirect estimates website is available to make comparisons between areas on eight outcomes. That is, for each of literacy and numeracy, comparisons are conducted on the proportion at or below Level 1, proportion at Level 2, proportion at Level 3 and above, and the average. The areas involved in the comparisons cover the following: \uf06e state-to-nation; \uf06e state-to-state; \uf06e county-to-state; and \uf06e county-to-county. Often, the user may be interested in conducting multiple comparisons to make simultaneous inferences. Initial interest was in pairwise comparisons rather than multiple testing. We acknowledge that both multiple comparisons and multiple testing are subareas of simultaneous inference. However, multiple comparisons apply to simultaneous comparisons of values for the same measurement/quantity of interest, and represent a common area in the literature; see, for example the book by Hsu (1996). For example, the comparisons of treatment means for a one-way model = + , = 1, . . . , and = 1, . . . , , falls under this category, of multiple comparisons. On the other hand, multiple testing applies to simultaneous tests of values for multiple measurements/quantities of interest. For example, the testing of measurements (weight, blood pressure, heart rate, etc.) taken on a random group of people, to make inference on the effect of a treatment, falls into the multiple testing category. Computations are conducted for one-to-one pairwise comparisons allowed between any two states or any two counties (i.e., within or across states).  "}, {"section_title": "5-20", "text": "The final interest is in multiple testing. Particularly to PIAAC, for a given pair of domains, (county1, county2) or (county1, state1) or (state1, state2) or (state1, nation), the interest is in testing simultaneously whether zero is significantly different from the differences between the domains in eight measurements mentioned above. Hence, the set consists of eight tests, denotes the parameter of interest for domain , with measure \u2208 { , } being literacy or numeracy, and characteristic \u2208 { 1, 2, 3, } being proportion at or below Level 1, proportion at Level 2, proportion at or above Level 3, or mean. For this, the distributions for the differences are constructed using the MCMC iterates available from the model fit, for the eight quantities: literacy proportions and averages, and numeracy proportions and averages. For the PIAAC SAE website, we applied a Bonferroni adjustment to the simultaneous 100(1 \u2212 ) credible intervals, and text appears with the results, conditional on chosen areas of interest. \u2022 indicator function, comparing the survey regression estimates and the corresponding predictions simulated from the posterior predictive distribution for the county-level quantities (proportions or averages); \u2022 order statistics function, comparing the order of the survey regression estimates and the order of the corresponding predictions simulated from the posterior predictive distribution for the county-level quantities (proportions or averages);"}, {"section_title": "6-3", "text": "\u2022 difference between the model predictions and the corresponding predictions simulated from the posterior predictive distribution for the county-level quantities (proportions or averages); and \u2022 difference and scaled (by the variance of the model prediction) difference between the survey regression estimates and the corresponding predictions simulated from the posterior predictive distribution for the county-level quantities (proportions or averages). \uf06e model sensitivity checks using initial values specification for nuisance parameters and random effects; hyperparameters specification for prior distribution parameters for the variance components; and changes in prior distributions for the variance components. \uf06e model specification using univariate versus bivariate model specification for the proportions; different software (stan) sampling algorithm (hamiltonian monte carlo/no-u-turn samplers) parameters (max_treedepth, adapt_delta, random seeds); -different distributions for the sampling model; and different link function for the linkage model. Internal model validation was conducted based on the set of 184 counties to which the models were fitted. For all the fits of the models compared against the final models, estimation and prediction are implemented in RSTAN, using the default parameters for the sampling algorithms and three MC chains. Most of the models were fit using 20,000 samples per MC chain, and thinned every 10 samples, after dropping the first 5,000 samples as burn-in. Hence, inference was conducted using 4,500 samples, from the three chains combined. Selected results are included in this section for the literacy proportions models. The results for numeracy proportions models, and for literacy and numeracy average scores models, are similar to the ones for literacy proportions models, and only briefly mentioned in each of the following subsections."}, {"section_title": "Convergence and Mixing Diagnostics", "text": "Convergence and mixing diagnostics were performed using functions from the R diagonal entry) or nearly zero (for the second diagonal entry), and leading to three undefined effective sample sizes (corresponding to the first diagonal entry). The cross-correlation is a matrix of dimension 897 by 897 and here we report summaries for all its entries, so the 1's correspond to the diagonal entries in that matrix. Convergence and mixing checks are conducted for the alternative model specifications for proportions, as well as for the various model specification for averages, and the diagnostics for the final models adopted do not indicate lack of convergence and mixing. "}, {"section_title": "Checks on Model Assumptions", "text": "To check some of the model assumptions, we conducted a collinearity test, residual analysis and a set of posterior predictive checks."}, {"section_title": "Collinearity Test", "text": "Collinearity may be detected among a set of covariates using the variance inflation factor (VIF). Each covariate is regressed on all the other covariates, and the resulting coefficients of determination, 2 , are used to construct the VIFs, where = 1, . . . ,7 denotes the covariate. Specifically, for covariate , = (1 \u2212 2 ) \u22121 . Large VIFs, typically larger than 10 (Erciulescu, Cruze, and Nandram [2019] used a similar threshold), indicate collinearity issues. Table 6-2 shows that there is no indication of collinearity among the set of covariates that were used in final SAE models. The variance inflation factors are all less than 10. The results in table 6-2 hold for all the different model specifications adopted for proportions and averages, because the same set of counties and the same set of covariates were used in all. PIAAC Indirect Estimation Methodology 6-6  The two components in each of the two residual sets correspond to proportions at or below Level 1 ( ,1 ) and at or above Level 3 ( ,2 ), respectively. Note that the smoothed sampling variances, and ,2,2 , are denoted without the hat symbol, because they were treated as fixed and known in the model fit. Also, note that the first set of residuals is inspired by the transformed residuals computed in PIAAC Indirect Estimation Methodology 6-7 Battese, Harter, and Fuller (1988); the authors constructed unit-level transformed residuals that are approximately independent (the correlation between any two residuals is not exactly zero) and identically distributed with mean zero and variance equal to the common constant across all the units, while we constructed area-level transformed residuals (Set 1) that are approximately independent (the correlation between any two residuals is not exactly zero) and identically distributed with mean zero and variance equal to one.  distributions of the residuals. As it will be described in section 6.1.4.3, we investigated this option and the estimated degrees of freedom of the assumed Student t-distribution was very large indicating a normal distribution is appropriate. The residuals plots with the fitted values being the x axis allow us to investigate the relationship between the residuals and the fitted values; the scatterplots indicate no significant departure from the homogeneous variance assumption for the conditional residuals considered here (not conditional on the random effects) or for the second set of residuals (conditional on the random effects). The two sets of residuals are very similar among each other because the variances of the random effects are very small; additional checks of the model predictions and their components: fitted values and predicted random effects indicated that the model predictions are nearly equal to the fitted values. The two sets of residuals defined above apply to proportions; the pairs reduce to one single residual for the averages. Initial residual checks for averages indicated departures from normality in one county of sample size two and hence it was removed from the set of final 184 counties to which the models are fit. Residual checks are conducted for the alternative model specifications for proportions, as well as for the various model specification for averages, and the diagnostics for the final models adopted do not indicate substantial lack of fit (most of the residuals values are between -2 and 2, and their sample quantiles are very close to the quantiles of a standard normal distributions, as indicated by the points falling close to the 45 degrees line in the residuals plots)."}, {"section_title": "Posterior Predictive Checks", "text": "To assess the adequate fit of the model, we conducted posterior predictive checks. Particularly, for a set of predefined statistics, we compare statistics based on their posterior predictive distribution to their corresponding values obtained using the original sample. The procedure to generate data from the posterior predictive distribution is as follows. Consider the posterior samples for , , , , , , and , denoted by and ( ) , respectively, for = 1, . . . , = 4,500. Construct ( ) and draw replicates ( ) following the model specified in chapter 5: For this application, we consider the following statistics: indicator, order indicator, mean deviation, unscaled residual constructed as deviation from the survey regression estimate, and scaled residual constructed as deviation from the survey regression estimate scaled by the posterior variance of the predictor using simulated values. For each proportion (at or below Level 1 and at or above Level 3), the corresponding posterior predictive quantities are where is the total number of counties (184), the subscript is used instead of , 1(2), for simplicity, and the subscript ( ) is used to denote the order statistics of the sample of county-level proportions, of size T. Posterior predictive p values are constructed for the first three statistics. By definition, the posterior predictive p value is the proportion of summary statistics calculated with samples generated from the posterior predictive distribution that exceed the corresponding value based on the original sample. The posterior predictive p values corresponding to the statistics indicator, order, and mean deviation are defined as , and . A p value close to 0.5 indicates that the model provides a reasonable fit to the sample data; see, for example, Gelman (2013). For the unscaled and scaled residuals, we report their ranges; values of the scaled residual between -1.96 and 1.96 and a global average of around 0 may be considered to be reasonable. The resulting posterior predictive p values constructed using the definitions in the previous paragraph (averages over the generated values for those statistics we constructed) are 0.474, 0.180 and 0.500, for the indicator, order and mean deviation statistics, respectively. Summary results for the posterior predictive statistics are provided in table 6-3, for literacy proportions at or below Level 1. The posterior predictive values for the ordered county-level proportions below Level 1 indicate that values generated from the posterior predictive distribution tend to be smaller than the sample values; the summaries in the last column of table 6-3 are all above zero. This result may be an effect of very small (and negative) model predictions. The posterior predictive values for the indicator test statistics are close to 0.5, the deviations and the unscaled residuals are close to zero, and the scaled residuals range is within -1.96 to 1.96. Therefore, overall there is no substantial indication for model lack of fit. The posterior predictive checks for the other quantities of interest (literacy proportions at or above Level 3, numeracy proportions, literacy and numeracy averages) do not indicate lack of fit for the models adopted. "}, {"section_title": "Model Sensitivity Checks", "text": "We conducted model sensitivity checks using a different specification for the prior distribution for the variance-covariance matrices, an alternative, classic, prior distribution for the variance-covariance matrices, different initial values specifications for nuisance parameters and random effects, as well as different hyperparameters specifications for the prior distribution for the variance-covariance matrices."}, {"section_title": "Changes in the Specification of the Prior Distribution for the Variance-Covariance", "text": ""}, {"section_title": "Matrices", "text": "In chapter 5, we described the model specification with the (1) adopted for the correlation matrices , , and ; see Lewandowski, Kurowicka, and Joe (2009). As a sensitivity check, we now consider an alternative model specification, adopting an containing the same information as our correlated 4,500 MCMC samples, providing more confidence for making inference using these sets of MCMC samples.   "}, {"section_title": "Changes in the Prior Distribution for the Variance-Covariance Matrices", "text": "The inverse-Wishart (IW) distribution has been a common choice of prior distribution adopted for the variance-covariance matrices in multivariate HB models, due to its conjugacy properties (i.e., the posterior distribution of the variance-covariance matrices is in the same probability distribution family as the prior distributions, in this case both the prior and the posterior are IW distributions). For example, Erciulescu, Berg, Cecere, and Ghosh (2019) Gelman (2006) has a discussion on the issues related to adopting inverse-Gamma priors versus using half-Cauchy priors or uniform priors for the variance components. Generalizing to the use of IW priors versus LKJ priors, similar issues may persist; see a discussion in Alvarez, Niemi, and Simpson (2014). Nevertheless, for sensitivity analysis we consider the IW prior distribution and fit an HB model assuming the following: One choice of initial value for the hyperparameters , , is a variance-covariance matrix inspired by the estimated variances in the simulation studies (see appendix B), = ( 0.001156 0.000098 0.000098 0.001156 ). For this alternative fit, we also provide initial values for the random effects parameters, again inspired by the predicted random effects in the simulation studies. Convergence and mixing diagnostics for this alternative model, with IW prior specification for the variance-covariance matrices, do not indicate lack of fit; see the results in table 6-5. Regression coefficients posterior means and county-level literacy proportions posterior means are similar for the different model specifications. However, as expected, the posterior standard deviations of these parameters are different. Figures 6-5 and 6-6 below facilitate these comparisons. Hence, the model specification is sensitive to the prior distribution adopted for the variance-covariance matrices. For the final models, priors adopted were suggested by the hierarchical Bayes modeling literature, less sensitive to specifications of hyperparameters or initial values, and well-performing in the simulation studies conducted.  "}, {"section_title": "6-17", "text": ""}, {"section_title": "Changes in Initial Values", "text": "In the previous model fit (section 6.1.3.2), somewhat arbitrary (previously constructed using simulation results under one of the scenarios considered in appendix B) initial values were provided for the variance parameters and for the random effects parameters. As an alternative model fit, we consider the HB model with IW prior on the variance matrix and use only initial values for the coefficients , letting the program generate initial values for the other parameters. This initial values scenario coincides with the one adopted for the final model, as presented in chapter 5. Convergence and mixing diagnostics for this alternative model, with IW prior specification for the variance-covariance matrices and no initial values for the variance components and random effects, do not indicate lack of fit; see the results in table 6-6. Regression coefficients posterior means and county-level literacy proportions posterior means are similar for the different model specifications. However, the posterior standard deviations of the county-level literacy proportions are different. Figures 6-7 and 6-8 below facilitate these comparisons. Hence, the model specification with IW prior adopted for the variance matrix is also sensitive to the set of initial values provided to the model parameters.  "}, {"section_title": "6-20", "text": ""}, {"section_title": "Changes in Hyperparameters Values", "text": "In the previous model fit (sections 6.1.3.2 and 6.1.3.3), somewhat arbitrary (previously constructed using simulation results under one of the scenarios considered in appendix B) hyperparameters were provided for the priors on the variance parameters. As an alternative model fit, we consider the HB model with IW prior on the variance matrix, using only initial values for the coefficients , letting the program generate initial values for the other parameters, and we replace the hyperparameters , , by , = 10 \u22123 ( 1 0 0 1 ) = ( 0.001 0 0 0.001 ). Convergence and mixing diagnostics for this alternative model, with IW prior specification for the variance-covariance matrices, no initial values for the variance components and random effects, and noninformative choice of hyperparameters for the IW priors, do not indicate lack of fit; see the results in table 6-7. Regression coefficients posterior means and county-level literacy proportions posterior means are similar for the different model specifications. However, the posterior standard deviations of the county-level literacy proportions are different. Figures 6-9 and 6-10 below facilitate these comparisons. Hence, the model specification with IW prior adopted for the variance matrix is also sensitive to the set of hyperparameter values provided to the model parameters.  The methods in section 6.1.3 apply to the bivariate models for the literacy proportions. As illustrated in subsection 6.1.3.2, the model that uses IW distribution as the prior distribution for the random effects variance-covariance matrices is sensitive to changes in initial values provided to the model parameters and to the hyperparameters provided to the model parameters, so it was not adopted as the prior distribution for the final model. The prior distribution adopted for the final model random effects variance-covariance matrices is the LKJ, as suggested in STAN's guide and reference manual, and recommended in recent literature studies, i.e., Alvarez, Niemi, and Simpson (2014). Similar methods were applied, and comparable results were obtained for the numeracy proportions and for the univariate models PIAAC Indirect Estimation Methodology"}, {"section_title": "6-23", "text": ""}, {"section_title": "6-25", "text": "for the averages. Because the models for averages are univariate, models using uniform and inversegamma prior distributions for the variance parameters are compared using different choices of initial values. Similar conclusions to the ones presented in this section hold for numeracy model evaluation. From the model evaluation for averages, it was observed that the models are not sensitive to the initial values, but are sensitive to the choice of prior distribution (leading to different posterior variances for the model parameters). Following the discussion in Gelman (2006) and the default choice in STAN, we adopted uniform priors on the random effects variances in the models for averages, on a wide range, 0 to 1,000, as shown to have better performance when compared to the classical inverse-Gamma priors for the variances, and because the number of counties we are modeling is larger than 3, the model would not be sensitive to the choice as upper bound on the uniform distribution range, in this case, 1,000. Alternative choices include half-Cauchy priors, again suggested in Gelman (2006) and STAN's guide and reference manual, and matching priors, as adopted, for example, by Datta, Rao, and Smith (2005) and Nandram, Erciulescu, and Cruze (2019). In the simulation studies we include results on the performance of the posterior variance under different prior choices, with respect to tracking the design MSE."}, {"section_title": "Changes in the Model Specification", "text": "We illustrate additional results, based on changes in the model specification. For this, univariate models were compared against the bivariate models adopted for the two proportions, different sets of software (STAN) parameters were used to fit the HB models, an alternative distribution (to the normal distribution) was assumed for the sampling model and an alternative link function (to the linear) was assumed for the linkage model."}, {"section_title": "Univariate Versus Bivariate Models for Literacy Proportions", "text": "We modeled the proportions using univariate models with uniform priors on the variances, on a wide range, 0 to 1,000, and compared against the bivariate model with LKJ on the correlation matrix (specification one in the report) and half-Cauchy on the standard deviations. Figures 6-11 and 6-12 illustrate the results for these comparisons for regression coefficients and for county-level literacy proportions under univariate and bivariate HB models, respectively. As expected, the posterior variances are reduced when the proportions are modeled using a bivariate model. However, more work could be done to assess whether the two modeling approaches were reasonable fits to the data and that the corresponding MCMC chains converged and mixed properly. Also, additional comparisons may be conducted using univariate models with inverse-gamma priors on the variances and compared against the PIAAC Indirect Estimation Methodology"}, {"section_title": "6-26", "text": "bivariate model with IW prior on the variance-covariance matrix, as well as univariate models with half-Cauchy priors on the variances and compared against the bivariate model with LKJ-type priors. "}, {"section_title": "6-28", "text": ""}, {"section_title": "Tuning Parameters in the Hamiltonian Monte Carlo and No-U-Turn Samplers", "text": ""}, {"section_title": "Algorithms", "text": "As a consequence of internal model validation results, the length of the MC chains used for the final bivariate models, for proportions, was increased to 30,000 (after dropping the first 5,000 samples), and the parameters 'max_treedepth' and 'adapt_delta' of the Hamiltonian Monte Carlo (HMC) and No-U-Turn Samplers (NUTS) algorithms, were increased above the default software values, to 13 and 0.99, respectively. The final univariate models, for averages, were fit using 20,000 MC samples (after dropping the first 5,000 samples) and the default HMC/NUTS parameters, 'max_treedepth' equal to 10 and 'adapt_delta' equal to 0.8. For example, table 6-8 shows the mean and quartiles of the diagnostic statistics for the fit of the final bivariate model for literacy proportions, using only 20,000 samples (after dropping the first 5,000) and the default parameters 'max_treedepth' and 'adapt_delta.' Note the smaller effective sample sizes and larger associated autocorrelations, when compared to the results in table 6-1 "}, {"section_title": "Relaxed Normality Assumptions in the Bivariate HB Models for Literacy Proportions", "text": "Alternative distributional and relational assumptions were made: for the sampling model a Student tdistribution is investigated, and for the linking model a logit link function is investigated. Unlike the specification used for the final model, where normal distribution is assumed at both the sampling level and the linking level (hence a matched model), these alternative specifications are referred to unmatched models. The results for the unmatched model using Student t-distribution at the sampling level are similar PIAAC Indirect Estimation Methodology"}, {"section_title": "6-29", "text": "to the results based on the matched normal-normal model. The results are as expected, given that the posterior means for the degrees of freedom parameter for the Student t-distribution is very large (approximately 50,000). The second unmatched model using logit link resulted in different model predictions, however, and model validation was left for future development. However, this is not a major concern because the models proposed were developed as the result of substantial simulation studies (presented in appendix B) and iterative steps with extensive internal validation checks (presented in section 6.1). In addition, the model estimates are further validated using external checks presented in the next section, 6.2. "}, {"section_title": "External Model Validation", "text": ""}, {"section_title": "Model Validation Graphs", "text": "The model validation conducted in this section is mainly graphical, as influenced by Khan et al. (2018). In general, the following graphs were conducted for the purpose of evaluation. \uf06e Histograms of differences between survey regression estimates and indirect estimates. The main objective of this plot is to take a first look at the results through reviewing the distribution of the differences. The graph can also indicate outliers that would need more investigation, especially to check the sample sizes in those small areas. \uf06e Bubble plots of survey regression estimates versus indirect estimates, with the sizes of bubbles being related to the sample sizes. One would expect to see from this plot that the large bubbles are close to the diagonal line, given that the direct estimates based on large sample sizes are fairly precise. The outliers would more likely be denoted by small bubbles indicating small sample sizes. \uf06e Shrinkage plots with arrows showing the direction from survey regression estimates to indirect estimates, by sample size. The main purpose of this plot is to show how the model impacts the estimates. There should be some shrinkage; that is, the predictions are pulled toward the average, if the predictions are more dependent on the model than the direct estimates. The longer arrows show larger impact from the model, which should occur for areas with smaller sample sizes than others."}, {"section_title": "6-30", "text": "\uf06e Interval coverage plots, showing the credible interval of the survey regression and indirect estimates, by sample size. These plots help show whether or not the resulting credible interval from the model covers the survey regression estimate. Again, the focus is solely on the areas with the largest samples. If the interval does not cover the survey regression estimates in the areas with largest sample sizes, it may indicate that the modeling process can be improved. \uf06e Variance plots showing the resulting smoothed variances and model variances, by sample size. While the aforementioned graphs are used to review the point estimates, this plot shows the impact on precision. This review is a common aspect of SAE evaluations, and as an example, Bijlsma et al. (2017) also reviewed the decrease in standard errors from the SAE approach. Erciulescu, Cruze, and Nandram (2017) reviewed the outcomes to ensure (1) the negative correlation between coefficient of variance (CV) and sample size, and (2) the impact of the SAE approach on the CV. Mohadjer et al. (2011) reviewed the credible interval widths and CVs (direct estimates and model predictions) for sampled counties and nonsampled counties to ensure the impact of direct estimates on the model predictions' variances for counties with sample. In general, the posterior variances associated with the model predictions should be smaller than the smoothed sampling variances associated with survey regression estimates. If not, it may be due to weak covariates used in the models. The evaluation results discussed below are based on the model predictions for literacy proportions. Other results from the evaluation on the models for literacy averages, and numeracy proportions and averages are available in appendix C."}, {"section_title": "Histograms of Differences", "text": "The differences between survey regression estimates and indirect estimates are shown in the histograms in figure 6-13 for literacy proportions. The means and medians of the differences are around zero. The majority of the differences are within 20 percentage points. The outliers in the plots show that a few model predictions deviate from the survey regression estimates by about 20-40 percentage points. These large deviations are not a concern because they are mostly observed for counties with small sample sizes, for which the direct survey estimates are less reliable than the corresponding estimates for counties with large sample sizes. "}, {"section_title": "Shrinkage Plots", "text": "Shrinkage towards the means can be observed in figure 6-14. As expected, the shrinkages are more significant in areas with smaller sample sizes than those in areas with larger sample sizes. The model predictions and the survey regression estimates become much more similar when the sample sizes are  "}, {"section_title": "Interval Coverage Plots", "text": "The interval coverage plots in figure 6-15 show that, for a majority of the small areas, the credible intervals generated from the models cover the survey regression estimates, especially for areas with large sample sizes. When the sample sizes are less than 50, sometimes the credible intervals from the models do not cover the survey regression estimates. This is as expected because the survey regression estimates contribute less to the indirect estimates if the survey regression estimates are derived from samples of smaller sizes (i.e., less reliable).  Figure 6-16 shows the survey regression estimates versus indirect estimates. The majority of the points are around the 45-degree line, indicating that the model predictions are close to the survey regression estimates. Larger bubbles (i.e., counties with larger sample sizes) have closer estimates than smaller bubbles. Some of the small bubbles, with the sizes of bubbles being proportional to the sample sizes in the small areas, are farther away from the 45-degree lines. Similar as above, this is as expected due to higher sampling errors for the survey regression estimates constructed using samples with small sizes."}, {"section_title": "6-33", "text": ""}, {"section_title": "Bubble Plots of Survey Regression Estimates and Indirect Estimates", "text": "The proportion at or below Level 1 (P1) and proportion Level 3 and above (P3) have closer estimates than proportion at Level 2 (P2), which could be a result of using P1 and P3 in the model fit and estimation. Similar results are shown for each outcome in appendix C.  Figure 6-17 shows the smoothed standard errors of the survey regression estimates and the posterior standard deviations from the small area model. For these plots, while in general the resulting model predictions are similar to the survey regression estimates, keep in mind that the standard errors of proportions depend on the sizes of the estimated proportions. Therefore if the model proportion is different from the survey regression proportion, the variance will in theory be different. The plot shows that the model produces smaller posterior standard deviations than the smoothed standard errors, especially for areas of very small sample sizes. "}, {"section_title": "6-34", "text": ""}, {"section_title": "Smoothed and Small Area Model Variances", "text": ""}, {"section_title": "Comparison of Aggregates of Model Predictions and Direct Estimates", "text": "Another method for the external model validation is to compare the model predictions with the corresponding direct estimates (the direct estimates referred in this section is the actual direct estimates, not the survey regression estimates) at various aggregate geographical levels for which the direct estimates are reasonably reliable. The county-level model predictions were aggregated to a number of domains using county-level characteristics, including various three-level categorizations of county variables relating to variables used in the model: race/ethnicity, education attainment, poverty, as well as covariates not used in the model, such as Beale codes (Rural-Urban Continuum Codes), population size, language, foreign-born status, and mortality rate. The direct estimates were computed from the unit-level sample data for the same domains. The direct estimates and model predictions of the literacy/numeracy proficiency outcomes are comparable, and table 6-9 below shows the literacy proportion at or below Level 1. Similar tables for PIAAC Indirect Estimation Methodology"}, {"section_title": "6-36", "text": "other outcomes are presented in appendix C. In general, the relative differences are within 10 percent. The side-by-side comparisons in table 6-9 should be viewed with caution because the direct estimates cannot be thought of as the truth, rather the comparison is the difference between two estimates. The PIAAC sample was not designed for estimates within classes of counties, so the representativeness is in question.      "}, {"section_title": "A-10", "text": ""}, {"section_title": "A-11", "text": ""}, {"section_title": "A-12", "text": ""}, {"section_title": "B-1 APPENDIX B SIMULATION STUDY RESULTS", "text": "Simulation studies have frequently been used to evaluate the performance of small area estimators. Although small area estimators are usually supported by well-elaborated statistical models, the models rest on specific assumptions. Simulation studies offer a means to assess the consequences of departures from these assumptions. One strategy in the design of simulation studies is to generate simulated populations based on alternative statistical models. Samples can be drawn from the generated populations and the performance of the small area estimators assessed. An alternative strategy is to treat a very large dataset, such as a census, as the reference population, to draw samples from it, and then to compare the actual performance of the resulting small area estimators to the true area-level values of reference population. The natural appeal of the second strategy, which is the one used here, is that it attempts to represent the possible challenges of applying the small area estimators to data in \"the real world\" that might not exactly fit any specific set of statistical assumptions. A possible limitation of this approach, however, is in the ability to generalize from the findings of the simulation to the application at hand if the population chosen for the simulation does not approximate key features of the application. In the summer of 2017, work began on the design of a simulation study based on the 2015 5-year public use file from the American Community Survey (ACS) to evaluate the performance of possible small area estimators under conditions similar to those that will be faced by the Program for the International Assessment of Adult Competencies (PIAAC) application. Most of the initially planned results were obtained by the spring of 2018, before the 2017 PIAAC data were available. A smaller scale supplemental study was conducted in the spring of 2019 to investigate two specific issues that arose during the final implementation of the small area models to the 2012/2014/2017 PIAAC data."}, {"section_title": "B.1 Overall Design of the Simulation Study", "text": "Because of confidentiality restrictions, the geographic detail on ACS public use files is restricted to Public Use Microdata Areas (PUMAs), which are required to have a minimum population size. ACS estimates for each state and most large counties can be assembled by summing the PUMAs within them, but the ACS data for small counties is comingled with other counties, in which case they cannot be individually estimated. In the simulation, the focus on states and counties in PIAAC was modified to evaluate the PIAAC Indirect Estimation Methodology B-2 estimates for states, the large counties that could be assembled from PUMAs, and the remaining ACS PUMAs. The four stages of sampling in PIAAC were \uf06e sampling primary sampling units (PSUs) within strata, except for a small number of large, certainty PSUs; \uf06e sampling segments within sampled PSUs; \uf06e sampling households within segments; and \uf06e selecting persons within households. The simulation approximated the PIAAC sampling, combining the second and third stages by artificially forming ACS segments of size equal to the average number of households sampled in the PIAAC third stage, so that the simulation consisted of \uf06e sampling PSUs within strata, except for a small number of large, certainty PSUs; \uf06e sampling ACS segments of households formed for purposes of the simulation; and \uf06e selecting persons within households. Although the ACS public use file includes weights, the simulation treated this file as the reference population. The simulation weights reflected only the simulated sampling, not the original ACS weights. In both the PIAAC design and the simulation, the last step of sampling is the primary source of variation in the weights. Further details on both the PIAAC and simulation sample designs are presented in the last section. While the PIAAC universe includes 16-through 74-year-olds in housing units and most noninstitutional group quarters (excluding military barracks and other dwellings on military bases), the simulation was restricted to ACS persons aged 19-74 in housing units-that is, persons in group quarters were excludedgiving a count of 2,089,949 individuals in 1,112,755 households. Households without eligible individuals were excluded from the simulation. Educational attainment was chosen as the dependent variable for the study. To approximate the proportion of adults with literacy Level 1 or below, persons whose highest educational attainment was a GED were combined with those with less than a high school education; together, they are 14.29 percent of the population in the public use file. As an approximation to literacy at Level 2, a second group was formed from the remaining high school graduates plus those who started but PIAAC Indirect Estimation Methodology B-3 did not complete a full year of post-secondary education, representing 29.86 percent of the population in the public use file. Let be the set of sampled individuals, , in sampled households, that are in division , state , and PSU . Direct H\u00e1jek (1971) PSU-level estimates, \u0302, of the proportions in an education group, such as low education, were formed using the weights where is an indicator function for low education for sample individual in division , state , and PSU , and is the survey weight derived as the inverse of the probability of selection considering all stages of selection. The simulations used the sequence number, 1 to 500, as the seed of the random number generator in R. The random samples based on a given seed are reproducible, so that the simulation can compare the performance of different small area estimators for the same generated samples. For each seed, a sample of PSUs was selected, then a random sample of segments within the PSU was selected with replacement. One or more respondents within each of the households in a sampled segment were selected, and weights were determined for each case. Direct within-PSU variance estimates were computed using segment as the ultimate cluster within the PSU and the weights described above, using the standard linearization formula for a ratio estimator. The variance calculation was performed with the function svymean() from the survey package in R, https://cran.r-project.org/web/packages/survey/survey.pdf. All predictors are based on the unweighted full ACS public use file, so they have no sampling error in the simulation setting. Some models also introduce census division, primarily as random effects but also as"}, {"section_title": "B-5", "text": "\uf06e The survey regression estimate provided a more effective starting point for the area-level models than the usual direct survey estimate; \uf06e A bivariate version of a multivariate model jointly modeling low education and high school education generally improved the predictions for low education compared to a univariate model for low education; and \uf06e The best performing model, which included census division as a random effect, was more effective than a similar model treating division as a fixed effect. One model, M1, was designed to closely follow the unmatched model used for NAAL. First, a fixedeffect logistic regression was used to predict the proportions \u0303 of the true proportions of low education for sampled PSUs in state , division based on the direct estimates \u0302. Then, the log of the observed relvar (relative variance), var(\u0302)/\u03022 , was predicted in a weighted regression with terms log(\u0303), log(1-\u0303) and log( with the same set of predictors."}, {"section_title": "B-6", "text": "Except for three models, the models first applied some form of smoothing to the direct estimates of the sampling variance within PSU. The variance smoothing for M1 was described above. A second model, M2, was similar to M1 but used a different form of smoothing. In place of a model for relvar, a model for the log of the effective sample size, =\u0302(1 \u2212\u0302)/var(\u0302) based on the direct variance estimate var(\u0302), was fitted with a weighted regression using ln(\u0303) and ln( ), where is the number of sampled individuals in the PSU, ln( ) = 0 + 1 ln(\u0303) + ln( ) + \u2032 , and where \u2032 is an error term with mean 0. The regression used weights proportional to , \u2212 1. The variance smoothing for the matched linear models based on the direct estimates \u0302 was similar. In place of logistic regression, a fixed effect linear regression is used to predict \u0303 for each sampled PSU, converting any predicted value < 0.01 to 0.01. Similar to M2, the log of the observed effective sample size, =\u0302(1 \u2212\u0302)/var(\u0302) was modeled with a weighted regression using ln(\u0303) and The true within-PSU sampling variance was estimated by simulating the sampling for 2000 samples in each PSUs and computing the variance of the estimated proportion. Two models (M3 and M5 described later) used these estimated true variances in place of modeled variances. Some of the linear models, including the best performing one, used a survey regression estimate (or \"modified generalized regression [GREG] estimate\"), \u030c, of the PSU proportion in place of the simple direct estimate, \u0302. Rao and Molina (2015, pp. 21-23) described the use of these estimates in small area estimation, their derivation, and the usual Taylor series approach to estimating their variance. In addition to the predictors available for the sampled cases, the values of the population totals in state and PSU must be available for this estimator. In the simulation, these were provided by the full ACS. An indicator of low education at the person level was modeled by a linear model using indicators for working, black, Hispanic, U.S. born, and indicators for intervals of household income (loss-$19,999, $20,000-$29,999, $30,000-$39,999, $40,000-$49,999, $50,000-$59,999, $60,000-$69,999, $70,000-$79,999, $80,000-$89,999, $90,000-$99,999, $100,000-$124,999, and $125,000 and above.) The indicator variables for income were selected to replace the log of mean income at the area level because individual income can be negative or zero. Low education did not appear to vary with income below $20,000."}, {"section_title": "B-7", "text": "In the modified form proposed by S\u00e4rndal and Hidiroglou (1989), the survey regression estimate can be where \u0302 is the vector of survey-weighted regression coefficients from the unit-level regression based on the whole sample, is the known size of the eligible population in the PSU, the are the unit-level residuals from the regression fit in state and PSU , and the are the corresponding survey weights. The Taylor series variance estimate is based on applying the standard variance expression (in this case appropriate for the simple random sampling with replacement [SRSWR] sampling of segments within PSU) to the . As is well known, GREG estimators can occasionally yield negative weights in some cases, leading to negative estimates of nonnegative characteristics (Rao and Molina 2015, p.14). The survey regression estimator did so during the simulations for a small number of PSUs, and in the simulations negative estimates were replaced by small positive quantities or excluded from the modeling. The variance modeling was modified slightly for the survey regression estimate. The observed effective sample size =\u030c(1 \u2212\u030c)/var(\u030c) was again modeled with a weighted regression using ln(\u0303) and ln( ), with weights proportional to , \u2212 1. The true sampling variance of the PSU-level survey regression estimates was estimated by simulating the sampling 40,000 times, including the estimation of \u0302, and finding the variance of the PSU-level estimates for those samples including the PSU. Figure B-1 compares the variances of the estimates of \u0302 and \u030c. On average, the variance of the survey regression estimate is about 14 percent lower. The ratio of average variance for the two estimators is shown in table B-1, disaggregated by groupings of PSUs. The first distinction is between the 803 PSUs of individual PUMAs that were not uniquely assigned to counties, many of which contained multiple small counties, and the 431 PSUs corresponding to large individual counties, divided by size into two noncertainty groups and the 10 certainty PSUs in the simulation. The table indicates that the survey regression estimate yields the greatest relative improvement in the largest counties, but average improvements are present in the other groups of PSUs."}, {"section_title": "B-8", "text": "Comparisons are also made according to the level of low education in two ways. In the first, the PSUs are brings more of a modeling perspective to the comparisons. Note that the distribution of PSUs by ACS low-education level is somewhat more dispersed than the distribution of the modeled estimates, as might be expected. These two perspectives produce relatively similar outcomes for the ratios of average variance in table B-1, showing variance reductions for all of the groups. The small area model for NAAL produced estimates for a single characteristic, at or below Level 1 literacy, but a multivariate approach permits expansion of the analysis to simultaneously produce estimates for the proportions at or below Level 1 literacy, for Level 2 literacy, and for Level 3 literacy and above. As previously noted, high school education is used as a proxy for Level 2 literacy in the simulations. The multivariate models used the same predictors for both education levels, but the beta coefficients for the two levels were not forced to be equal and indeed were quite different."}, {"section_title": "B-10", "text": "All of the models were implemented through HB using MCMC methods in the same basic way. Models were fitted in R by calling either the JAGS or STAN software. For JAGS, the protocol for NAAL was followed: three chains with burn-in of 10,000 and total of 100,000 for each chain, which was sampled at 1 in 10 for analysis. All models used diffuse uniform (-1000, 1000) priors on the betas and gamma priors on the precision (reciprocal variance) of the state and PSU variances with shape and inverse scale parameters 0.001. For STAN, the three chains were shortened to a burn-in of 2,000 and total of 20,000, with again 1 in 10 sampling. Univariate models and multivariate used diffuse N(0, 100) priors on the betas. Univariate models used inverse gamma priors for the state and count variances with shape and scale parameters 0.001 for the variance components. Multivariate models used inverse Wishart distributions each with 2 degrees of freedom; the scale matrix for the PSU covariance was taken to have elements .001156, .000098, .000098, .001156. For states and divisions, the elements were .000576, .000289, .000289, .000961. Models considered. Models M1, the closest analogue to the NAAL model, and M2, with the alternative variance smoothing, have already been introduced. Except where noted, models M1-M7 were fitted with JAGS and the remaining models with STAN. The other models are as follows: \uf06e M3: the same estimator as M1 or M2 if the within-sampling variances are known, using the variance estimates from the 2,000 simulated samples in each PSU; \uf06e M4: the linear model based on \u0302; \uf06e M4S: M4 implemented in STAN; \uf06e M5: M4 using the variance estimates from the 2,000 simulated samples in each PSU; \uf06e M6: the linear model implemented on the survey regression estimate, \u030c; PIAAC Indirect Estimation Methodology"}, {"section_title": "B-11", "text": "\uf06e M6S: M6 implemented in STAN; \uf06e M7: M6 using the within-PSU variance estimates from the 40,000 simulated samples; \uf06e M8: the multivariate extension of M4, with modifications to the variance smoothing noted below; \uf06e M9: the multivariate extension of M6; \uf06e M10: an extension of M4 with random effects for census division; \uf06e M11: a multivariate version of M10; \uf06e M12: an extension of M6 with random effects for census division; \uf06e M13: a multivariate version of M12; \uf06e M14: an extension of M6 with fixed effects for census division; and \uf06e M15: a multivariate version of M14. Except for the procedure to smooth the variances, table B-2 summarizes these choices. Table B-2. Features of the models studied, comparing unmatched, \"x\" (vs. matched, \".\"), use of the estimated true variance, \"x\" (vs. smoothed \".\"), STAN, \"x\" (vs. JAGS, \".\"), use of the survey regression estimate, \"x\" (vs. H\u00e1jek \".\"), multivariate, \"x\" (vs. univariate, \".\") and inclusion, \"fixed\" or \"random\" of census division effect in the model (vs. not \".\") In the multivariate models an additional predictor was added to the individual-level regression used in the survey regression estimator: an indicator variable for household income $125,000-$174,999. The motivation for the addition was to further reflect the income distribution when modeling higher education levels."}, {"section_title": "Model", "text": "The multivariate models required estimates of the sampling covariance matrix within PSU for the two components of education. The variance for each of the three education proportions was modeled separately, giving (1) for the modeled variance of low education, (2) for the modeled variance of high school education, and (3) for the modeled variance of higher education, that is, 1 or more years of college. In general, if three proportions with respective variances (1) , and (3) sum to 1, then The variance of the third proportion must equal the variance of the sum of the first two, or (3) = (1) + (2) + 2 (12) . A modeled covariance was derived by After some initial experimentation, each of the variance models were simplified to modeling the log of the effective sample size as ln( ) = 0 + 1 ln( ) + \u2032 . Two previous variance models including a term or terms involving occasionally produced covariance matrices that were not positive definite for one or more of the simulated samples. The simplified approach yielded satisfactory results for all 500 simulation samples."}, {"section_title": "B.2 Results", "text": "Mean Square Error (MSE) Results at the PSU Level.    Because the results are based on only 500 simulated samples, many but not all comparisons between models are statistically significant. Furthermore, in some instances where one model gives better overall performance, the results are not uniform across subsets of the PSUs. Two results stand out immediately, however, \uf06e Even though some of the differences between M4 and M4S and between M6 and M6S are statistically significant, the MSE results are for all practical purposes identical, so these two available comparisons suggest that JAGS and STAN were producing essentially equivalent results for these models. \uf06e Comparisons between M4 and M6, between M8 and M9, between M10 and M12, and between M11 and M13 each reflect the effect of the direct versus the survey regression estimator. The comparisons favor the survey regression estimator for each of the 4 pairs and each of the 17 comparisons within each pair. The values of the 68 t-statistics range from a low of 4.04 to a high of 35.1; in other words, all are highly significant. The simulation results uniformly favor use of the survey regression estimate over the direct estimate."}, {"section_title": "B-16", "text": "Three other results show overall differences but somewhat inconsistent results: \uf06e Comparison of M4 and M8, M6 and M9, M10 and M11, and M12 and M13 each assess the impact of using a multivariate model on the quality of the estimates for low education. There is a significant overall gain from the multivariate model, particularly when census division is a random effect, and gains appear in most but not all of the groups, with exceptions for the certainty PSUs (not significant) and for the highest proportions of low education (significantly worse). \uf06e Including random effects for census divisions appears to improve the estimation in some circumstances but not others. Here, the pairings are M4 with M10, M6 with M12, M8 with M11, and M9 with M13. The first two pairings of univariate models yield nonsignificant overall comparisons and significant differences for some subgroups with both positive and negative signs. The overall improvements are significant for the pairings of multivariate models, M8 with M11 and M9 with M13, and most but not all of the significant findings for subgroups of PSUs are in the direction of showing a benefit for the use of random effects at the division level. \uf06e Expressing the division effect as a random effect in the model rather than a fixed effect is substantially better overall and for most subsets of the PSUs, except for differences that are not significant for certainty PSUs and for differences that are not significant or reversed for PSUs with the highest levels of low education. Two other results seem entirely mixed: \uf06e The comparison of M1 and M2 is not statistically significant overall and produces a mixture of positive, negative, and not significant differences for the subgroups considered. \uf06e Somewhat surprisingly, the substitution of estimated true variances for modeled variances does not consistently improve the results over the modeled variance. In fact, overall M2 is statistically significantly better than M3, but the comparison reverses in some of the subgroups. Similarly, M4 outperforms M5 for overall MSE, as does M6 over M7, but again the comparison reverses for some of the subgroups. The simulation results do not indicate an appreciable loss from using smoothed variances in place of the unknown true variances. The unmatched model has an advantage over the matched linear model by guaranteeing positive estimates when applied to the direct survey estimates. The chance for negative estimates may be increased further by use of the survey regression estimate, which itself may give negative estimates. For model M13, which includes use of the survey regression estimates, the average number of negative HB estimates among the 147 sampled PSUs was 0.5, and the average was 1.1 among all of the 1,234 PSU HB estimates. The simulation results suggest that the magnitude of the problem is small, but that it should be addressed in advance. One approach would be to assign any negative estimate a value known to be small in the population, such as 0.02, or alternatively the smallest positive HB estimate among the entire set of 1,234. This one disadvantage of the matched model should be assessed in the context of its other advantages, such PIAAC Indirect Estimation Methodology B-17 as its possible use of the survey regression estimator and extensions to multivariate estimation. (It may be possible to develop a multivariate extension of the unmatched model, but this was not attempted.) For the 147 PSUs in sample, one might ask how often the HB estimate is better than the sample estimate. In fact, the simulation shows that the conditional variance of a sampled PSU is smaller than the estimated  14.17 percent, 14.17 percent, 14.17 percent, 14.15 percent, 14.15 percent, 14.17 percent, 14.15 percent, 14.17 percent, 14.15 percent, 14.17 percent, and 14.15 percent, for M1, M2, M4, M4S, M6, M6S, M8, M9, M10, M11, M12, M13, M14, and M15, respectively.) Interval results. Because the simulation is based on a single population, the proportion of times the HB credible interval covers an individual true value will be different from the nominal level. The true value for some PSUs may lie close to the model prediction, and for these PSUs the credible intervals may cover at an average rate above the nominal value. In the case of PSUs with outlier values compared to the assumed model, the credible intervals may cover the true values at a rate lower than the nominal level. But from a frequentist perspective, a set of intervals may be judged as successful for a fixed population if their actual coverage is close to the nominal coverage when averaged over groups of areas. Table B-7 presents the average coverage of the 95 percent credible intervals from the MCMC overall and for PSUs classified by PSU type and population. In this table, the performance appears relatively satisfactory for all of the models. The large, noncertainty PSUs are covered at a rate above the nominal level, whereas coverage is a bit below for the certainty. M2 appears to improve on M1 in terms of coverage. Tables B-8 and B-9 provide the same groupings of PSUs as tables B-4 and B-5. Table B-8 shows that the unmatched HB models provide low true coverage for areas with extremely low levels of low education."}, {"section_title": "B-19", "text": "For the same PSUs, the matched models cover at rates far above the nominal level, suggesting that their intervals are too wide there. At the other end of the scale, when the proportion of low education is 25 percent or more, the coverage of intervals for matched models is also poor, where the unmatched results are somewhat better. The range 20-24 percent remains challenging for all of the methods, and the best performance is seen in the middle of the distribution. The results in table B-9 appear somewhat better, with improvements for the unmatched models for PSUs with modeled estimates less than 5 percent and for matched models for PSUs with modeled estimates of 25 percent or more. The difference between table B-8 and table B-9 can be understood as a consequence of regression to the mean. In other words, PSUs with extreme values will be more difficult to model. When PSUs are averaged over groups based on their predicted values under a model, the regression effect is mitigated.   Table B-10 shows the average coverage rates of the state estimates for all models to be satisfactory. Remarks on Two Aspects of the Simulation Results. Two simple analyses of the ACS data offer some insight into results from the simulation. One concerns the finding that the matched model outperformed the unmatched model in the simulation. A re-examination of the characteristics of the simulation population suggests how population characteristics might influence the relative performance of the small area models. Figures B-2 and B-3 contrast the predictions of a logistic and linear model for the PSU loweducation proportions, using only fixed effects, based on the entire ACS sample used in the simulation. The graphs indicate that both model expressions capture a large proportion of the underlying variation, but the logistic fit appears to slant away from the 45-degree line. Very few predicted values from the logistic model are below 0.05, while there is a larger number of actual values below 0.05. This last observation may be a consequence of using population proportions as predictors of the logit of a probability, as is the case in this application with all of the variables representing proportions except the log of income.    "}, {"section_title": "B-22", "text": ""}, {"section_title": "B.3 Supplemental Simulation", "text": "The simulation results reported in the previous section were obtained in the winter and spring of 2018, before the results from the 2017 PIAAC sample were available. The results set a general direction for the modeling of the PIAAC results without dictating every detail. One major change in the final model was to replace the simulation study's use of inverse Wishart distributions as priors for the variance-covariance matrices of the random effects at the county, state, and division levels. Instead, an (1) prior, introduced in section 5.2, was used. This section reports on a small simulation study to investigate whether the revised prior would substantially alter the conclusions of the previous simulation. If the PIAAC Indirect Estimation Methodology"}, {"section_title": "B-25", "text": "performance changed substantially with the use of the LKJ prior, then the preceding simulation results from 2018 would be of diminished value. A simulation using the first 100 samples was conducted with the LKJ prior using the default parameters used by STAN in R. The default parameters permitted considerably faster runs than the settings used in the final model. Figure B-5 compares the average MSE using two different priors for M13, the inverse Wishart used in the original simulation and the LKJ prior with the default parameters used by STAN. Figure B-5 indicates that revising the prior has a relatively small effect, generally no more than a few percentage points, on the average MSE, with no clear trend. In contrast, figure B-6 compares the average posterior variance from the simulation, showing generally greater observed differences between the results from the two priors and a tendency for the LKJ prior to produce somewhat more dispersed posterior variances, either systematically higher or systematically lower than the inverse Wishart, depending on the sample.    Figures B-5 through B-8 show that the inverse Wishart and LKJ priors yield relative similar results, suggesting that the simulation findings can still be a useful guide for the PIAAC application. A remaining point, however, is that the 100 simulations were based on the default settings rather than the final settings used for PIAAC, adapt_delta=.99 and max_treedepth=13. There was sufficient time to run 20 simulations using the final settings. In one case, replicate sample 17, the running time was exceptionally long. Figure   B-9 compares the average MSE of the LKJ prior with the default against the results with the final settings. The result for replicate sample 17 appears in this graph as a distinct outlier. As a check on the computation, all of the random seeds were fixed at the same values, and the same result was exactly reproduced when it was rerun. In an additional run, by changing the value of the random seed just before the call to stan(), the revised finding fell in line with other results as shown in figure B-10. A possible interpretation is that the initial MCMC results for replicate sample 17 were unsatisfactory, illustrating the usefulness of diagnostics to review MCMC results. With the revised results, figure B-10 indicates that findings from the LKJ and inverse Wishart priors are quite similar.  "}, {"section_title": "B.4 Summary and Discussion", "text": "The HB model implemented for NAAL performs well when applied to a similar application based on treating an entire ACS sample as a population. Nonetheless, several variants of the approach are possible, and the simulations show that it is possible to improve on the original approach. \uf06e It was possible to create a survey regression estimator that uniformly improved the performance of the small area estimates over various subsets of the PSUs. \uf06e In the simulation, most models were based on smoothed estimates of the variance. The models generally performed well relative to use of the estimated true variances obtained from simulation. This conclusion must be tempered by the observation that, because the withinsampling variances were affected by the method to form segments rather than arising naturally from information in the ACS data, the apparent conclusion that variance smoothing had a negligible effect might not hold in some other situations. \uf06e The comparisons of models here showed some advantage to the multivariate models in predicting low education, with M9 and M13 producing the generally best results. The multivariate models have other important advantages. First, it is likely that they provide more closely coordinated estimates of high school education concurrently than if high school education were modeled in a univariate model. Second, the multivariate models will support"}, {"section_title": "B-30", "text": "interval estimates for the sum of low and high school education, equivalent in the PIAAC application to obtaining interval estimates for literacy Level 2 and below as well as for Level 1 and below and for Level 2. \uf06e Incorporating division as a random effect in the model improved the overall results, but the simulation findings are not entirely consistent on this point. The simulation suggested including division as a random effect rather than a fixed effect was more effective. \uf06e The data used for modeling the NAAL, PIAAC, or each simulated sample in the simulation covered a relatively small fraction of the total number of counties. This differs from many of the applications in the small area literature in which most or all of the small areas are represented by at least some sample. The current HB model does not explicitly account for PSU-level variation in estimating the coefficients of the area-level regression model. The generally reasonable average coverage rates of the credible intervals suggest that this limitation did not substantially affect the simulation findings."}, {"section_title": "B.5 Additional Details on the Design of the Simulation", "text": "Major Features of the PIAAC 2012/2014/2017 Sample Design. Although the PIAAC design is described elsewhere in the report and by Hogan et al. (2016), a summary of key features here will facilitate comparison between the PIAAC design and the simulation. The PIAAC sample was drawn through a multistage design, beginning with a first-stage selection of PSUs. Each PSU was a county or a group of counties. PIAAC divided the country into 1,949 PSUs, of which 1,213 were single counties. Most of the largest counties were individual PSUs, including four counties included with certainty. Sampling for the 2012/2014/2017 PIAAC involved two stratified samples of PSUs. For the 2012/2014, a stratified sample of 80 PSUs was selected, with the 2014 sample using the same PSUs that were selected in 2012. Of the 80 PSUs in the 2012/2014 sample, four were self-representing, that is, selected with certainty. The remaining 1,945 PSUs were grouped into 18 major strata. Each major stratum was divided into an even number of minor strata, and a single PSU was sampled with probability proportional to size from each minor stratum. For 2017, the PIAAC PSUs were restratified into 11 major strata and then into new minor strata, and a first-stage sample of 4 certainty and 76 noncertainty PSUs was selected. Except for the four certainty PSUs, the sampling procedure was designed to minimize the overlap of the 2017 sample with the 2012/2014 sample. As a result, the large PSUs with a probability of selection of 0.5 or more in the original 2012/2014 sample that were not selected then had a conditional probability of selection close to 1.0 into the 2017 sample. Across the combined 2012/2014/2017 samples, 147 unique PSUs were selected for PIAAC."}, {"section_title": "B-31", "text": "At the second stage of selection, dwelling units in the sampled PSUs were divided into segments by grouping census blocks. An average of approximately 11 segments were sampled in each PSU. In PSUs composed of more than one county, counties received only a share of the segments allotted to the PSU, occasionally only one or none. Dwelling units were sampled within the segment as the third stage of selection. Finally, the fourth stage of selection sampled eligible respondents within sampled dwellings. A single individual was sampled if three or fewer persons were eligible, and two individuals were sampled otherwise. The Simulated Population. The public use file from the 2015 ACS provided the data for the simulation study. The simulation used only variables available from the ACS, so the unweighted ACS data can be regarded as an unknown but true population. Ideally, the simulation would have used ACS data at the county level, but many counties cannot be identified on the public use file. For reasons of protecting the confidentiality of ACS respondents, the Census Bureau restricts the geographic information released on the public use file to a set of PUMAs. There are 2,531 such areas on the 2015 file. PUMAs are allowed to cross county but not state boundaries, and in defining PUMAs, small counties were combined with others in order to satisfy a minimum population requirement of 100,000 people. Generally, however, the largest counties are exactly divided into one or more PUMAs, so that estimates for these counties can be based on reassembling their constituent PUMAs. As Minor strata were not assigned in the simulation."}, {"section_title": "B-32", "text": "Simulation of Sampling. The sample of PSUs was meant to approximate but not fully mimic the more complex sampling procedure used for PIAAC. In place of the combination of 4 certainty PSUs and the sampling of large PSUs by a procedure to reduce overlap, the simulation simplified this procedure by treating 10 counties, all large, as self-representing. The number of PSUs selected from each of the 11 major strata was adjusted accordingly and varied between 8 and 15. The PSUs were selected through random systematic sampling using the function UPrandomsystematic() in the R sampling package. The ACS population aged 19-74 was used as a measure of size, but the measure of size was reduced in a few noncertainty PSUs to ensure a probability of selection less than 1. A total of 147 PSUs was sampled in each simulation, the number of unique PSUs in the PIAAC 2012/2014/2017 design. To simulate the second and third stages of selection for PIAAC, the ACS households in each PSU were grouped into segments of size 7. An interclass correlation of 0.25 corresponding to a design effect of about 2.5 (=1+0.25\u00d7(7-1)) was induced by sorting the households by the number of low education adults and assigning a linearly increasing set of initial probabilities, with the first probability approximately onesixth the last. (The choice of slope for the increase was determined by trial and error.) The design effect was approximately achieved by drawing a household sample of the same size without replacement using these initial probabilities, thereby permuting the sorted households in the PSU. The resulting permutation was partitioned into segments. This assignment of ACS households to segments was performed once and kept fixed throughout the simulation. The unconditional sampling rate for households was set at 0.0105, approximating the overall sampling rate for the combined 2012/2014/2017 PIAAC sample. A simplified procedure was implemented to approximate the sampling of eligible respondents within households, which is the fourth stage of selection in PIAAC. A single respondent was drawn for households of 3 or fewer, two respondents for 4-6, three respondents for 7-9, etc. Although PIAAC capped the number of respondents at two, the weights for large households were trimmed in PIAAC, a process not replicated in the simulation. The sampling of eligible respondents within sampled households is a primary source of weight variation in the PIAAC design. The simulated samples were weighted by assigning each sampled individual the reciprocal of the product of the following factors: \uf06e (1) = the probability of selection of PSU in state . This probability was 1 for the 10 certainty PSUs; otherwise when the PSU is in stratum \u210e where \u2032 \u2032 is the measure of size for each PSU \u2032 \u2032 in the same major stratum-typically equal to the ACS estimated population aged 19-74 but in a few cases a value to prevent (1) from exceeding 1-and \u210e is the number of PSUs sampled from the major stratum \u210e; \uf06e (2) = the expected number of hits for an individual segment resulting from SRSWR within the PSU, computed as the ratio of the number of segments assigned to be sampled with replacement from the PSU to the number of segments in the PSU; and \uf06e \u2113 (3) = \u2113 / \u2113 , where \u2113 is the number of eligible persons in the household \u2113 and \u2113 is the number sampled, that is, \u2113 = 1 if \u2113 \u2264 3; \u2113 = 2 if 4 \u2264 \u2113 \u2264 6; etc. The number of segments to sample in each PSU was determined to keep variation in (1) (2) to a minimum, that is, to produce an approximately self-weighting sample of households. When applied to the simulation samples, the weights for sampled person in household \u2113 in state and PSU , = 1/( (1) (2) \u2113 (3) ), can be used to form unconditionally design unbiased estimates of unweighted ACS PSU-level totals from the ACS public use file. Analysis of state and national rates. Although unweighted proportions from the full ACS were the reference population in the simulation, a small complication was introduced in analyzing rates for states and the U.S, although not for the rates for PSUs. ACS-weighted estimates of the eligible population by PSU were used to aggregate the PSU-level rates to state and national rates. This approach was used consistently throughout the analysis. (In hindsight, a simpler alternative would have been to form aggregates using the unweighted PSU counts.) With this approach, 14.22 percent were estimated to fall in the low education group instead of the 14.29 percent in the public use file. Similarly, 29.13 percent were in the high school education group instead of 29.86 percent on the public use file. The same weighting was used to aggregate HB PSU-level estimates to the state and national levels. Thus, the effect on the analysis can be assumed to be small."}, {"section_title": "C-10 Additional Chapter 5 Tables and Graphs", "text": "The appendix provides the results for all other outcomes not covered in chapter 5. Tables C-1 through C-3 provide the different sets of covariates for literacy average, numeracy proportions, and numeracy average, respectively. Table C-4 provides distribution of credible interval widths and coefficients of variation for small area estimates, for each outcome.                               "}]